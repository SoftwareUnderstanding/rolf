{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I-BERT has been developed as part of the following paper. We appreciate it if you would please cite the following paper if you found the library useful for your work:\n\n```text\n@article{kim2021bert,\n  title={I-BERT: Integer-only BERT Quantization},\n  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},\n  journal={International Conference on Machine Learning (Accepted)},\n  year={2021}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{kim2021bert,\n  title={I-BERT: Integer-only BERT Quantization},\n  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},\n  journal={International Conference on Machine Learning (Accepted)},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8614677240022502
      ],
      "excerpt": "Github Link: https://github.com/huggingface/transformers/tree/master/src/transformers/models/ibert \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kssteven418/I-BERT",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nLicense\nBy contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq),\nyou agree that your contributions will be licensed under the LICENSE file in\nthe root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-22T15:47:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T12:51:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.991365411892791,
        0.8506761159145684
      ],
      "excerpt": "I-BERT is also available in the master branch of HuggingFace! \nVisit the following links for the HuggingFace implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "Model Links:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9871472895115884
      ],
      "excerpt": "This branch is identical to the master branch of the original Fairseq repo, except for some loggings and run scripts that are irrelevant to the functionality. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8255274726292926
      ],
      "excerpt": "Run the following commands to fetch and move to the ibert-base branch: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[ICML'21] I-BERT: Integer-only BERT Quantization",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kssteven418/I-BERT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Wed, 29 Dec 2021 03:58:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kssteven418/I-BERT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kssteven418/I-BERT",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/kssteven418/I-BERT/tree/ibert/docs",
      "https://github.com/kssteven418/I-BERT/tree/ibert/examples/simultaneous_translation/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/scripts/compound_split_bleu.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/scripts/sacrebleu.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/language_model/prepare-wikitext-103.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/multilingual/finetune_multilingual_model.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/multilingual/multilingual_fairseq_gen.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/multilingual/train_multilingual_model.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/speech_recognition/datasets/prepare-librispeech.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/byte_level_bpe/get_data.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/roberta/preprocess_GLUE_tasks.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/roberta/preprocess_RACE.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/roberta/commonsense_qa/download_cqa_data.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/backtranslation/sacrebleu.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/backtranslation/tokenized_bleu.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/backtranslation/prepare-wmt18en2de.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/backtranslation/prepare-de-monolingual.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/translation/prepare-wmt14en2fr.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/translation/prepare-iwslt14.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/translation/prepare-iwslt17-multilingual.sh",
      "https://raw.githubusercontent.com/kssteven418/I-BERT/ibert/examples/translation/prepare-wmt14en2de.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can find more detailed installation guides from the Fairseq repo: https://github.com/pytorch/fairseq\n\n**1. Fairseq Installation**\n\nReference: [Fairseq](https://github.com/pytorch/fairseq)\n* [PyTorch](http://pytorch.org/) version >= 1.4.0\n* Python version >= 3.6\n* Currently, I-BERT only supports training on GPU\n\n```bash\ngit clone https://github.com/kssteven418/I-BERT.git\ncd I-BERT\npip install --editable ./\n```\n\n**2. Download pre-trained RoBERTa models**\n\nReference: [Fairseq RoBERTa](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md)\n\nDownload pretrained RoBERTa models from the links and unzip them.\n* RoBERTa-Base: [roberta.base.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz)\n* RoBERTa-Large: [roberta.large.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz)\n```bash\n#: In I-BERT (root) directory\nmkdir models && cd models\nwget {link}\ntar -xvf roberta.{base|large}.tar.gz\n```\n\n\n**3. Download GLUE datasets**\n\nReference: [Fairseq Finetuning on GLUE](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md)\n\nFirst, download the data from the [GLUE website](https://gluebenchmark.com/tasks). Make sure to download the dataset in I-BERT (root) directory.\n```bash\n#: In I-BERT (root) directory\nwget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\npython download_glue_data.py --data_dir glue_data --tasks all\n```\n\nThen, preprocess the data. \n\n```bash\n#: In I-BERT (root) directory\n./examples/roberta/preprocess_GLUE_tasks.sh glue_data {task_name}\n```\n`task_name` can be one of the following: `{ALL, QQP, MNLI, QNLI, MRPC, RTE, STS-B, SST-2, CoLA}` .\n`ALL` will preprocess all the tasks.\nIf the command is run propely, preprocessed datasets will be stored in `I-BERT/{task_name}-bin`\n\nNow, you have the models and the datasets ready, so you are ready to run I-BERT!\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8809611468979547
      ],
      "excerpt": "Github Link: https://github.com/huggingface/transformers/tree/master/src/transformers/models/ibert \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8271602839766401
      ],
      "excerpt": "If you already have finetuned models, you can skip this part. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9395696914546919,
        0.8210853507111112
      ],
      "excerpt": "git fetch \ngit checkout -t origin/ibert-base \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8005168985375024
      ],
      "excerpt": "Then, run the script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8662551457762397
      ],
      "excerpt": "By default, models are trained according to the task-specific hyperparameters specified in [Fairseq Finetuning on GLUE](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md). However, you can also specify the hyperparameters with the options (use the option-h` for more details). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9206633769338113
      ],
      "excerpt": "git checkout ibert \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8010525124534313
      ],
      "excerpt": "Then, run the script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8752226565127433,
        0.8752226565127433
      ],
      "excerpt": ": CUDA_VISIBLE_DEVICES={device} python run.py --arch {roberta_base|roberta_large} --task {task_name} \nCUDA_VISIBLE_DEVICES=0 python run.py --arch roberta_base --task MRPC \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8752226565127433
      ],
      "excerpt": "#: CUDA_VISIBLE_DEVICES={device} python run.py --arch {roberta_base|roberta_large} --task {task_name} \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8789686439194798
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python run.py --arch roberta_base --task MRPC --restore-file ckpt-best.pt --lr 1e-6 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kssteven418/I-BERT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Lua",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Facebook, Inc. and its affiliates.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "I-BERT: Integer-only BERT Quantization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "I-BERT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kssteven418",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kssteven418/I-BERT/blob/ibert/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can find more detailed installation guides from the Fairseq repo: https://github.com/pytorch/fairseq\n\n**1. Fairseq Installation**\n\nReference: [Fairseq](https://github.com/pytorch/fairseq)\n* [PyTorch](http://pytorch.org/) version >= 1.4.0\n* Python version >= 3.6\n* Currently, I-BERT only supports training on GPU\n\n```bash\ngit clone https://github.com/kssteven418/I-BERT.git\ncd I-BERT\npip install --editable ./\n```\n\n**2. Download pre-trained RoBERTa models**\n\nReference: [Fairseq RoBERTa](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md)\n\nDownload pretrained RoBERTa models from the links and unzip them.\n* RoBERTa-Base: [roberta.base.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz)\n* RoBERTa-Large: [roberta.large.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz)\n```bash\n#: In I-BERT (root) directory\nmkdir models && cd models\nwget {link}\ntar -xvf roberta.{base|large}.tar.gz\n```\n\n\n**3. Download GLUE datasets**\n\nReference: [Fairseq Finetuning on GLUE](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md)\n\nFirst, download the data from the [GLUE website](https://gluebenchmark.com/tasks). Make sure to download the dataset in I-BERT (root) directory.\n```bash\n#: In I-BERT (root) directory\nwget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\npython download_glue_data.py --data_dir glue_data --tasks all\n```\n\nThen, preprocess the data. \n\n```bash\n#: In I-BERT (root) directory\n./examples/roberta/preprocess_GLUE_tasks.sh glue_data {task_name}\n```\n`task_name` can be one of the following: `{ALL, QQP, MNLI, QNLI, MRPC, RTE, STS-B, SST-2, CoLA}` .\n`ALL` will preprocess all the tasks.\nIf the command is run propely, preprocessed datasets will be stored in `I-BERT/{task_name}-bin`\n\nNow, you have the models and the datasets ready, so you are ready to run I-BERT!\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 91,
      "date": "Wed, 29 Dec 2021 03:58:14 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "natural-language-processing",
      "quantization",
      "efficient-model",
      "efficient-neural-networks",
      "transformer",
      "bert",
      "model-compression"
    ],
    "technique": "GitHub API"
  }
}