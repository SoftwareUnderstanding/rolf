{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Dai, Zihang, et al. \u201cTransformer-XL: Attentive Language Models Beyond a Fixed-Length Context.\u201d ArXiv.org, Cornell University, 2 June 2019, arxiv.org/abs/1901.02860.\n\nDevlin, Jacob, et al. \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.\u201d ArXiv.org, Cornell University, 24 May 2019, arxiv.org/abs/1810.04805.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cmunnis/BERT_vs_Transformer-XL",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-15T13:35:50Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-10T20:41:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Comparison of Two NLP Frameworks for General Research Purposes",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cmunnis/BERT_vs_Transformer-XL/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 23:15:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cmunnis/BERT_vs_Transformer-XL/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cmunnis/BERT_vs_Transformer-XL",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cmunnis/BERT_vs_Transformer-XL/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT_vs_Transformer-XL",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT_vs_Transformer-XL",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cmunnis",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cmunnis/BERT_vs_Transformer-XL/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 27 Dec 2021 23:15:49 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**2/14/20**:\n- We've uploaded our abstract to the MassURC website and specified our needs for presentation. We created the GitHub repository for this project.\n\n**2/22/20**:\n- We updated the abstract to have quotes from the two research papers we're going to reference in our project. Ed has been working on setting up his company's server to run BERT, and will be providing access to Vincent and Connor soon. Vincent and Connor have been researching how to use/understand the results from both algorithms.\n\n**2/28/20**: \n- It was a bit close, but we have successfully been able to implement BERT onto our server, and are able to demonstrate it executing and working. With the structure of BERT implemented, our goal now shifts from the basics of BERT to changing its dataset manually.\n\n**3/6/20**: \n- Alongside the change in data set, we are also in the process of modifying the function to output, and save the generated results, the input, and the amount of time, in milliseconds into a text file with a similar syntax to that of a JSON file. Both this, and changing our bert's data set (from the IMDB Database to the Wikipedia Database), are still a work in progress, but we are getting closer to its completion.\n   + We've converted our TF BERT program to operate on Tensorflow 2, improving functionality and allowing use with Keras. \n   + We've installed transformer-xl onto our server and are writing a keras script for building, finetuning and testing our transformer-xl model. \n   \n**4/2/20**: \n- *Overview*: \n   + Amongst other goals, scripts are being developed to significantly speed-up the testing and comparing process, to hopefully increase development efficiency.\n- *Edward*:   \n   + Implemented entity-recognition.py for the Bert model, which can be viewed within our repository. This script is the counterpart to our entity recognition script for transformer xl. Both scripts read through a prepared script and identify entities that are containned within the scripts. We can use the combination of accuracy of the two models ability to find entities as well as the specificity of the entity categorization to judge the efficiency of the two models against each other. Our testing process will allow us to see how each model responds as the text used for entity analysis grows longer. \n   + Additionally, background research will need to be done to see if bias is introduced due to the entity-recognition script utilizing pytorch rather than tensorflow-2, to see if it would be valuable to refactor the pipeline transformer-xl function to utilize pytorch as well.    \n- *Vincent*:   \n   + Currently in the process of creating intuitive testing and comparing scripts that will allow for increased productivity, as well as reformat the output of both BERT and Transformer-XL in a way that will allow both to be comparible to each other, and their output sent to different columns of a spreadsheet file, along with the base input, for ease of recording and comparing.\n- *Connor*:    \n   + Continued to: read up on Machine Learning; learn Python to be able to help make the necessary changes; and look up examples of how to find/understand output for each model.\n   \n**4/12/20**:   \n- *Edward*:    \n   + Implemented BERT question answering, need to increase data pool for question answering to more than 512 tokens, or build a token delivery system instead. Will replicate BERT question answering into transformer xl. \n- *Vincent*:   \n   + Comparable data sourced from bert is currently functionally saved. However, further work is needed to encapsulate all of the required information, as well as identical formatting\n- *Connor*:    \n   + Worked with Vincent on having the BERT and Transformer-XL outputs made readable and itemized.\n\n- **Goal for Next Week**:\n   + We need to implement spreadsheet input functionality, make a unified spreadsheet input and output function with an increased data pool size past 512 tokens, and implement question answering on Transformer-XL to mirror the question answering on BERT.\n\n**4/18/20**:\n- *Edward*:\n  + Implemented an improved BERT QA system that can be found within bert-qa-advanced.py. This file allows us to run large batch tests on BERT's question answering, it also has an improved output style.\n  + Implemented a basic QA pipeline for transformers-xl, also reviewed documentation on how others have utilized transformer-xl for QA: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15766157.pdf, as this type of behavior isn't what transformer-xl is designed/optimized for. \n  + Fixed our transformer-summarization model to not use BERT tokenization, read this source material to see how text summarization could be improved with transformer-xl https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15776950.pdf\n   + For next week I will be changing our other tests (Named Entity Recognition, Next Sentence Prediction, Token Recognition and Summarization) to utilize the same testing structure. \n- *Vincent*:\n   + Generated a series of edge-case a logical-understanding cases for the Q&A testing.\n   + Also worked on creating a more dynamic and comparitively logical system for the BERT data saving and generation. Unfortunately, it is not at a stage which is entirely finished yet, due to development difficulties. However, it is significantly closer as of now.\n- *Connor*:\n   + Per Ed's suggestion, gathered test cases for bert-qa-advanced.py for fine-tuning.\n   \n**4/25/20**:\n- *Edward*:\n   + Researched methods of implementing transformer qa based on QANet + Transformer XL information included in the above Stanford Article\n   + Implementing pretrained wikipedia edited QA model that includes transformer-xl attention management to properly test against BERT\n   + Rewrote pipeline for transformer-xl to accept test cases similar to bert-qa-advanced\n   \n- *Vincent*:\n   + Upgraded the output of BERT's Q&A system to now output a CSV file containing 3 columns, one for the associated question, one for BERT's answer to that question, and one (Which is temporarily filled with \"NULL\"s) that will contain Transformer-XL's answer to the question, too.\n   + Also implemented JSON file output, which is a more universal and consistant method of data storage. (The JSON contains an array of \"QandA\" objects, with a \"Question\", \"BERT Answer\", and \"Transformer-XL\" attribute for each of them.\n   + Re-wrote old comments in the BERT Q&A for clarity-purposes.\n- *Connor*:\n   + Increased number of Q&A test cases from 18 to 50. \n   + Attempted to find a way to have the results of the Transformer-XL put into a specific column in the CSV file Vincent set up, but was unsuccessful.\n   \n**5/2/20**:\n- *Edward*:\n   + Additional implementation of xlnet on the transformer server. Additional research on using xlnet for QA. Basic implementation of sentence generation with xlnet/transformer-xl \n- *Vincent*:\n   + Progress made towards xlnet Q&A's repair. Debugging unusual tuple structure issue with the output tokens.\n   + Working on data visualization utilizing MatPlotLib 3.2\n- *Connor*:\n   + Worked on getting the skeleton of the research paper ready: abstract, works cited, and introduction.\n",
      "technique": "Header extraction"
    }
  ]
}