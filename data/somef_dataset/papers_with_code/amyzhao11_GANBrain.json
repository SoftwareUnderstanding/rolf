{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.06434 [cs], Jan. 2016, https://arxiv.org/abs/ 1511.06434. [Online]. Available:\nhttp://arxiv.org/abs/1511.06434\n\n[2] TensorFlow, \"Deep Convolutional Generative Adversarial Network\", [Online]. Available: https://www.tensorflow.org/tutorials/generative/dcgan",
      "https://arxiv.org/abs/ 1511.06434. [Online]. Available:\nhttp://arxiv.org/abs/1511.06434\n\n[2] TensorFlow, \"Deep Convolutional Generative Adversarial Network\", [Online]. Available: https://www.tensorflow.org/tutorials/generative/dcgan"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9988937211090503
      ],
      "excerpt": "Generative Adversarial Networks,\u201d arXiv:1511.06434 [cs], Jan. 2016, arXiv: 1511.06434. [Online]. Available: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amyzhao11/GANBrain",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-13T01:12:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-18T23:11:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "An important field within computer vision is medical imaging. However, a main problem within this area of research is that it is difficult to obtain a large sample of training images. Limitations to obtaining brain MRIs include: the low availability of participants, the time it takes to obtain and process high resolution MRI brain images, as well as the fact that participants have to stay still for long periods of time (whichkes it difficult to obtain a good image). Therefore it is useful to implement a generative adversarial network (GAN) that can be trained on existing brain MRIs and then if trained successfully, it can generate an infinite number of plausible brain MRIs. This would aid the training of computer vision techniques such as brain segmentation which would require much more expansive datasets that may otherwise not exist without many man-hours of medical imaging. \n\nIn particular, I have implemented a deep convolutional generative adversarial network (DCGAN) with reference DCGAN specifications in the paper written by Radford, Metz and Chintala [1]. In the DCGAN, the use of convolutional layers allows higher quality feature mapping and recognition relative to the traditional GAN which is only connected by dense layers. In my GAN implementation, I followed specifications such as:\n* using LeakyReLU in the discriminator\n* using strided convolutions in the discriminator and fractional-strided convolutions in the generator\n* using batchnorm in both the generator and discriminator\n* remove fully connected hidden layers\n* scaling training image pixel values from -1 to 1\n* in LeakyReLU, the slope of the leak was set to 0.2\n* using an Adam optimiser with learning rate of 0.0002 for both generator and discriminator (I used 0.0002 for the generator and 0.0001 for the discriminator)\n* no pooling layers\n\nI also did not follow several specifications as I found they either did not work or produced lower quality results:\n* They suggested the use of a ReLU activation function in the generator, however, I found LeakyReLU worked better as they are more effective in preventing vanishing gradients. They also suggested the use of a Tanh activation function in the final layer of the generator, however, I found my model worked better without any activation functions in the generator and discriminator.\n* Instead of using a batch size of 128, I used a batch size of 10 (i.e. 10 real and 10 fake images in each batch). I found larger batch sizes would overload the GPU.\n* The paper suggested the use of beta_1=0.5 for the Adam optimiser, however I found that using the default beta_1=0.9 worked fine.\n* I decided to use a latent space of 256 instead of 100 for no real reason and this worked quite well\n* For the Conv2DTranspose layers, when using the depicted kernel size of (5,5) with stride (2,2) (Figure 1 in [1]) I got very aliased generator images with grid artifacts. This was remedied by using a kernel size of (4,4) with stride (2,2)\n* Also in reference to Figure 1 in [1], I tried using four fractionally-strided convolutions (Conv2DTranspose) layers with one convolutional layer after and ended up with mode collapse. My model was working and produced very high quality brain images (SSIM>0.6) however, my generator would only produce the same images regardless of the noise input. I later fixed this by using three Conv2DTranspose layers and two convolutional layers instead.\n* I used dropout layers in my discriminator to make my discriminator learn more slowly. I did not try running the GAN without dropout layers so I'm not sure if this had any real effect, but the current model is quite effective.\n* In contrast to the number of filters in the generator in Figure 1 in [1] (which had filters 1024, 512, 256, 128 for the Conv2DTranspose layers), I used a maximum of 256 for the filters in my layers. I originally implemented the same number of filters in my generator as the paper, however, I found that my GPU would run out of memory due to the large number of filters. Also the brain MRI images are quite simple so may not require the larger number of filters.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8509039022540942,
        0.8233485666141159,
        0.9694895907171567,
        0.9461925254549899,
        0.9725861150694427
      ],
      "excerpt": "Data consists of all the non-segmented OASIS data (9664 training images, 544 test images, 1120 validation images). The size of these images are 256x256 and are greyscale with pixel values ranging from 0 to 255. This is a sample of the training images: \nThe model script contains 2 functions, a generator and discriminator \nThe generator generates 256 x 256 images and is designed to take an input noise vector with latent space of size 256. Batchnorm is used after every convolutional layer except the last one. Each layer uses LeakyReLU with slope of 0.2 for each layer except the last which has no activation function (when no activation function is specified the default activation is linear). \nThe discriminator takes an input image size of 256 x 256 and returns one output value. Its main objective is to classify the input images as real or fake by trying to minimise its loss. Batchnorm is used after every convolutional layer except the last one. I also used dropout layers with a dropout of 0.4. Each layer uses LeakyReLU with slope of 0.2 for each layer except the last which has no activation function (when no activation function is specified the default activation is linear). \nThe OASIS dataset contains 6 folders, however, only 3 of those folders are relevant to this project as they contain non-segmented brain MRI images. These are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8253807698816283,
        0.9561838088899294,
        0.9218066857928325,
        0.954768621836401,
        0.9212416232593749
      ],
      "excerpt": "As per the paper [1], I scaled the image pixel values so that they were between -1 and 1. I did not change the image dimensions. \nThe loss for the discriminator was defined as the sum of the binary crossentropy of classifying the real images and the binary crossentropy of classifying the fake images. Which means that loss is minimised for the discriminator if it is able to correctly classify real images as real and fake images as fake. The loss for the generator was defined as the binary crossentropy of fake images with real image labels. This means the loss for the generator is minimised if its generated images are classified as real.  \nI used the Adam optimiser with learning rate 0.0002 for the generator and a learning rate of 0.0001 for the discriminator as I found the generator loss was not growing as quickly as I would have liked when the discriminator learning rate was 0.0002. \nI defined a training function with gradient tape with help from the tensorflow website [2]. I initially tried using train_on_batch however this appears to work very differently between tensorflow 2.1 (lab GPU computers) and 2.3 (google colab). So I have found gradient tape to be a more stable technique across both tensorflow versions. \nI used a batch size of 10 and ran my model for 200 epochs, this took about 6 hours. However, it would be sufficient to get decent images by running it for 20 or more epochs. The training data was shuffled and partitioned into 1133 batches (total number of images divided by batch size) using tf.data.Dataset.from_tensor_slices.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9543845829618159,
        0.9491927519623766,
        0.8623947504009093,
        0.9010928251628461
      ],
      "excerpt": "Here is a plot of the generator(blue) and discriminator(red) losses for the first 40 epochs (I forgot to record the losses when I ran it for 200 epochs). At around 25 epochs the training appears to stabilise and converge. \nAfter generating images from the trained generator, you can choose an image to calculate the SSIM for. Only one image is chosen since the calculation involves iterating over the entire training set and calculating and storing the SSIM value which is computational expensive as there are over 11000 training images. The maximum SSIM is then displayed along with the corresponding training image which is closest in structural similarity to the generated image. With 200 epochs, the SSIM should be above 0.64 with some images reaching 0.68. \nThe following example was generated after 40 epochs with an SSIM of 0.68. \n[1] A. Radford, L. Metz, and S. Chintala, \u201cUnsupervised Representation Learning with Deep Convolutional \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "DCGAN using OASIS brain MRI images",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amyzhao11/GANBrain/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 20:59:32 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amyzhao11/GANBrain/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "amyzhao11/GANBrain",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Here, I install all the relevant packages which include tensorflow and keras which are involved in the creation of the GAN model. I also installed numpy, PIL, glob and os to help with loading the training images into an array from a specific directory. Matplotlib was used for image visualisation and sys was used to check whether a GPU was available as GAN training requires a lot of computational power and I had a lot of trouble with GPU availability on the lab computers. I also call my generator and discriminator functions from modelscript.py.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8428898559283207
      ],
      "excerpt": "Data consists of all the non-segmented OASIS data (9664 training images, 544 test images, 1120 validation images). The size of these images are 256x256 and are greyscale with pixel values ranging from 0 to 255. This is a sample of the training images: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8499828272142841
      ],
      "excerpt": "I used a batch size of 10 and ran my model for 200 epochs, this took about 6 hours. However, it would be sufficient to get decent images by running it for 20 or more epochs. The training data was shuffled and partitioned into 1133 batches (total number of images divided by batch size) using tf.data.Dataset.from_tensor_slices.  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amyzhao11/GANBrain/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DCGAN on OASIS data",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "GANBrain",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "amyzhao11",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amyzhao11/GANBrain/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "My driver script requires the following packages to be installed in the environment\n* tensorflow-gpu (version 2.1)\n* keras\n* python (version 3.7)\n* jupyter notebook\n* scikit-image\n* matplotlib\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 20:59:32 GMT"
    },
    "technique": "GitHub API"
  }
}