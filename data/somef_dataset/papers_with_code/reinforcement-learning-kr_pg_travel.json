{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We referenced the codes from below repositories.\n* [OpenAI Baseline](https://github.com/openai/baselines/tree/master/baselines/trpo_mpi)\n* [Pytorch implemetation of TRPO](https://github.com/ikostrikov/pytorch-trpo)\n* [RLCode Actor-Critic](https://github.com/rlcode/reinforcement-learning-kr/tree/master/2-cartpole/2-actor-critic)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8906174419333412
      ],
      "excerpt": "* Trust Region Policy Optimization [5] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "* Unity ml-agent: https://github.com/Unity-Technologies/ml-agents \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.839187750457435
      ],
      "excerpt": "* [1] R. Sutton, et al., \"Policy Gradient Methods for Reinforcement Learning with Function Approximation\", NIPS 2000. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9945537322088419
      ],
      "excerpt": "* [2] D. Silver, et al., \"Deterministic Policy Gradient Algorithms\", ICML 2014. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9000493788164815
      ],
      "excerpt": "* [3] T. Lillicrap, et al., \"Continuous Control with Deep Reinforcement Learning\", ICLR 2016. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "* [4] S. Kakade, \"A Natural Policy Gradient\", NIPS 2002. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9980807442543781
      ],
      "excerpt": "* [5] J. Schulman, et al., \"Trust Region Policy Optimization\", ICML 2015. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9468095625576742
      ],
      "excerpt": "* [6] J. Schulman, et al., \"High-Dimensional Continuous Control using Generalized Advantage Estimation\", ICLR 2016. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999404797011227
      ],
      "excerpt": "* [7] J. Schulman, et al., \"Proximal Policy Optimization Algorithms\", arXiv, https://arxiv.org/pdf/1707.06347.pdf. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/reinforcement-learning-kr/pg_travel",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-15T02:24:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-25T14:01:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9482121478477153
      ],
      "excerpt": "This repository contains PyTorch (v0.4.0) implementations of typical policy gradient (PG) algorithms. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8806013782566557
      ],
      "excerpt": "For reference, solid reviews of below papers related to PG (in Korean) are located in https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/. Enjoy! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.809638933310309
      ],
      "excerpt": "* [3] T. Lillicrap, et al., \"Continuous Control with Deep Reinforcement Learning\", ICLR 2016. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "Table of Contents \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "Modify the hyperparameters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "Modify the hyperparameters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8764704486528211,
        0.9044141046409281
      ],
      "excerpt": "* Note that the results of trainings are automatically saved in logs folder. \n* TensorboardX is the Tensorboard-like visualization tool for Pytorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8892032871488672
      ],
      "excerpt": "We have trained the agents with four different PG algortihms using Hopper-v2 env. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8348945726081531
      ],
      "excerpt": "    * +1000 for reaching the target \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9738512068386392,
        0.8492936860970658
      ],
      "excerpt": "    * When the body parts other than the right and left foots of the walker agent touch the ground or walls \n    * When the walker agent reaches the target \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8976386408254349
      ],
      "excerpt": "Pass the hyperparameter arguments according to your preference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9113078411671868
      ],
      "excerpt": "We have trained the agents with PPO using plane and curved envs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Policy Gradient algorithms (REINFORCE, NPG, TRPO, PPO)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/reinforcement-learning-kr/pg_travel/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 70,
      "date": "Sat, 25 Dec 2021 14:56:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/reinforcement-learning-kr/pg_travel/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "reinforcement-learning-kr/pg_travel",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Ubuntu](https://github.com/reinforcement-learning-kr/pg_travel/wiki/Installing-Unity-ml-agents-on-Linux)\n* [Windows](https://github.com/reinforcement-learning-kr/pg_travel/wiki/Installing-Unity-ml-agents-on-Windows)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Ubuntu](https://github.com/reinforcement-learning-kr/pg_travel/wiki/Installing-Mujoco-py-on-Linux)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name=\"1\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name=\"2\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name=\"3\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name=\"4\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name=\"5\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name=\"6\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name=\"7\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9769420979004304
      ],
      "excerpt": "1. Installation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9769420979004304
      ],
      "excerpt": "1. Installation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8089856058707676
      ],
      "excerpt": "* TensorboardX is the Tensorboard-like visualization tool for Pytorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8159698584129703
      ],
      "excerpt": "We have modified Walker environment provided by Unity ml-agents. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name=\"1\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name=\"2\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name=\"3\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name=\"4\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name=\"5\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name=\"6\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name=\"7\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9029939477674856
      ],
      "excerpt": "<!-- @import \"[TOC]\" {cmd=\"toc\" depthFrom=1 depthTo=6 orderedList=false} --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.897176225947782
      ],
      "excerpt": "2. Train \nBasic Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917254123495214
      ],
      "excerpt": "Test the pretrained model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.897176225947782
      ],
      "excerpt": "3. Train \nBasic Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917254123495214
      ],
      "excerpt": "Test the pretrained model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.913477538235006
      ],
      "excerpt": "python main.py --load_model ckpt_736.pth.tar \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.879591013490689
      ],
      "excerpt": "python test_algo.py --load_model ckpt_736.pth.tar --iter 5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8494174195948629,
        0.8494174195948629,
        0.8494174195948629,
        0.8494174195948629
      ],
      "excerpt": "| Vanilla PG |  | <img src=\"img/vanilla_pg.gif\" height=\"180px\" width=\"250px\"/> | \n| NPG |  | <img src=\"img/npg.gif\" height=\"180px\" width=\"250px\"/> | \n| TRPO |  | <img src=\"img/trpo.gif\" height=\"180px\" width=\"250px\"/> | \n| PPO |  | <img src=\"img/ppo.gif\" height=\"180px\" width=\"250px\"/> | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481942365069672,
        0.8191873617298402
      ],
      "excerpt": "| Walker | <img src=\"img/walker.png\" alt=\"walker\" width=\"100px\"/> | \n| Plane Env | <img src=\"img/plane-unity-env.png\" alt=\"plane\" width=\"200px\"/> | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.941132828213137
      ],
      "excerpt": "python main.py --load_model ckpt_736.pth.tar --train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8854309789864122
      ],
      "excerpt": "python main.py --render --load_model ckpt_736.pth.tar \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8768516369807537
      ],
      "excerpt": "See main.py for default hyperparameter settings. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/reinforcement-learning-kr/pg_travel/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Woongwon Lee\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Policy Gradient (PG) Algorithms",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pg_travel",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "reinforcement-learning-kr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/reinforcement-learning-kr/pg_travel/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 314,
      "date": "Sat, 25 Dec 2021 14:56:59 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Train the agent with `PPO` using `Hopper-v2` without rendering.\n~~~\npython main.py\n~~~\n* Note that models are saved in `save_model` folder automatically for every 100th iteration.\n\nTrain the agent with `TRPO` using `HalfCheetah` with rendering\n~~~\npython main.py --algorithm TRPO --env HalfCheetah-v2 --render\n~~~\n* **algorithm**: PG, TNPG, TRPO, **PPO**(default)\n* **env**: Ant-v2, HalfCheetah-v2, **Hopper-v2**(default), Humanoid-v2, HumanoidStandup-v2, InvertedPendulum-v2, Reacher-v2, Swimmer-v2, Walker2d-v2\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Train walker agent with `PPO` using `Plane` environment without rendering.\n~~~\npython main.py --train\n~~~\n* The PPO implementation is for multi-agent training. Collecting experiences from multiple agents and using them for training the global policy and value networks ([brain](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Brains.md)) are included. Refer to `pg_travel/mujoco/agent/ppo_gae.py` for just single-agent training.\n* See arguments in main.py. You can change hyper parameters for the ppo algorithm, network architecture, etc.\n* Note that models are saved in `save_model` folder automatically for every 100th iteration.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}