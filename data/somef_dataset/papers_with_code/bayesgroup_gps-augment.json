{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.09103",
      "https://arxiv.org/abs/2002.09103"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you found our code or algorithm useful, please cite our paper:\n\n```\n@article{molchanov2020greedy,\n  title={Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation},\n  author={Molchanov, Dmitry and Lyzhov, Alexander and Molchanova, Yuliya and Ashukha, Arsenii and Vetrov, Dmitry},\n  journal={arXiv preprint arXiv:2002.09103},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{molchanov2020greedy,\n  title={Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation},\n  author={Molchanov, Dmitry and Lyzhov, Alexander and Molchanova, Yuliya and Ashukha, Arsenii and Vetrov, Dmitry},\n  journal={arXiv preprint arXiv:2002.09103},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8484338661904762
      ],
      "excerpt": "Folder CIFAR-C should contain folders CIFAR-10-C, CIFAR-100-C. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "--policy trained_pols/CIFAR10-VGG16BN-M45-policy.npz \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "--policy imagenet/trained_pols/resnet50_ll.npy \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SamsungLabs/gps-augment",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-28T12:42:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-05T10:56:35Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8218148559660557
      ],
      "excerpt": "In this repo we reproduce experiments from our paper \"Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation\" (UAI 2020) by Dmitry Molchanov, Alexander Lyzhov, Yuliya Molchanova, Arsenii Ashukha, Dmitry Vetrov. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9950563837909933
      ],
      "excerpt": "Test-time data augmentation&mdash;averaging the predictions of a machine learning model across multiple augmented samples of data&mdash;is a widely used technique that improves the predictive performance. While many advanced learnable data augmentation techniques have emerged in recent years, they are focused on the training phase. Such techniques are not necessarily optimal for test-time augmentation and can be outperformed by a policy consisting of simple crops and flips. The primary goal of this paper is to demonstrate that test-time augmentation policies can be successfully learned too. We introduce greedy policy search (GPS), a simple but high-performing method for learning a policy of test-time augmentation. We demonstrate that augmentation policies learned with GPS achieve superior predictive performance on image classification problems, provide better in-domain uncertainty estimation, and improve the robustness to domain shift. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.855431079700706
      ],
      "excerpt": "We trained VGG16BN on a single GPU, and PreResNet110 and WideResNet28x10 on two GPUs (multi-GPU support out of the box). Hyperparameters for CIFAR10 and CIFAR100 are the same. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908721220618372
      ],
      "excerpt": "We use 4 different magnitudes of augmentations: 500 samples with M=45, 500 samples with M=20, 100 samples with M=0 (basic crop/flip augmentation) and one sample with no agumentation (central crop). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9720395337669603,
        0.8426654237820947,
        0.9810039285292232,
        0.9535372771037838
      ],
      "excerpt": "For example, in order to evaluate Deep Ensembles, just supply the --models argument with a list of models. \nDifferent augmentations can be obtained by varying the parameters --N and --M of RandAugment, and flag --no_tta disables test-time augmentation. \nSee the list of the arguments of get_predictions_randaugment.py for further details. \n--select_by states for metric that is optimized by the algorithm: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "--model resnet50 --select_by ll --select_only 20 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9316976113830605
      ],
      "excerpt": "--corruptions are indices of corruptions to evaluate policy on, check ood_transforms in https://github.com/da-molchanov/advanced-tta/blob/master/imagenet/utils_imagenet.py for the full list. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "--use_val --report_ens --num_tta 20 --fix_sign --model resnet50 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8722085634069551,
        0.952634126563745,
        0.9385282735519221,
        0.9902224603716232,
        0.9161440387650273,
        0.9943770008843196,
        0.9729618710613447,
        0.9346259001116398,
        0.956227442168845
      ],
      "excerpt": "We have modified the original RandAugment training procedure to achieve better performance. \nMost importantly, the original implementation of RandAugment does not keep the semantics of the magnitude parameter M. \nThe effective magnitude of some transformations decreases in M or has some other non-monotonic dependency on M (see e.g. a related issue). \nWe have modified the mapping from the magnitude parameter M to the actual transformations such that larger M would mean a larger augmentation intensity for all transformations, with M=0 generally corresponding to no additional augmentations (with the exception of an occasional AutoContrast which does not have a magnitude). \nFor bidirectional transformations like Brightness or Sharpness the direction of the augmentation (e.g. make the image darker or brighter) is chosen at random (one can override it by setting randomize_sign=False in utils/randaugment.py). \nWe have also found that using mirror padding for geometric transformations like Rotate and Shear instead of the default black padding improves the stability of training, decreases the shift of the moments of the images (likely important for Batch Normalization layers) and results in overall better performance. \nAlso, longer training is crucial for learning with large-magnitude augmentations (we train for 2000 epochs with N=3 and M=45). \nWe have also removed the Invert and Equalize transforms for CIFAR models since they did not provide a significant improvement on a validation set. \nThese modifications allowed us to significantly improve the perfromance of WideResNet28x10 on CIFAR-100: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9903892365801893
      ],
      "excerpt": "The samples for different magnitudes of our modification of RandAugment are presented below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Simple but high-performing method for learning a policy of test-time augmentation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bayesgroup/gps-augment/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Fri, 24 Dec 2021 20:44:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SamsungLabs/gps-augment/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "SamsungLabs/gps-augment",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/bayesgroup/gps-augment/master/notebooks/ood_visualize_cifar.ipynb",
      "https://raw.githubusercontent.com/bayesgroup/gps-augment/master/notebooks/CIFAR%20plots.ipynb",
      "https://raw.githubusercontent.com/bayesgroup/gps-augment/master/notebooks/ImageNet%20plots.ipynb",
      "https://raw.githubusercontent.com/bayesgroup/gps-augment/master/notebooks/ood_visualize_imagenet.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`--mode` stands for augmentation mode:\n- `cc`: rescale and central crop (no augmentation is used)\n- `cf`: conventional scale/crop/flip augmentation \n- `ra`: RandAugment with parameters N and M \n- `5c`: resize and five crop augmentation central crop and four corner crops\n- `10c`: resize and ten crop augmentation (five crops, two flips each)\n\n`--model` works with the folowing models:\n- `resnet50`\n- `efficientnet_b2`\n- `tf_efficientnet_b5`\n- `tf_efficientnet_l2_ns`\n- `tf_efficientnet_l2_ns_475`\n\nThese models are pre-trained and will be loaded automatically. More info on these models can be found at [https://github.com/rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models).\n\n```(bash)\nipython3 -- imagenet/get_preds_randaug_imagenet.py \\\n--model resnet50 --mode ra --N 2 --M 20 \\\n--num_samples 20 --batch_size 128 --fix_sign \\\n--logits_dir ./logits/ --log_dir ./logs/ \\\n--data_path ~/imagenet/raw-data --num_workers 10\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The following allows to create and to run a python environment with all required dependencies using [miniconda](https://docs.conda.io/en/latest/miniconda.html): \n\n```(bash)\nconda env create -f condaenv.yml\nconda activate megabayes\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8242452826116361
      ],
      "excerpt": "In order to train with a fixed train-validation split, use flag --valid_size 5000. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276296527534612,
        0.9276296527534612,
        0.9303593008357236
      ],
      "excerpt": "ipython -- ./train/train_randaugment.py --dir=../augres --dataset=CIFAR10 --data_path=~/data --epochs=2000 --model=VGG16BN --lr_init=0.1 --wd=3e-4 --fname=CIFAR10-VGG16BN-randaugment --N 3 --M 45 --num_tta 1 --randaugment \nipython -- ./train/train_randaugment.py --dir=../augres --dataset=CIFAR10 --data_path=~/data --epochs=2000 --model=PreResNet110 --lr_init=0.1 --wd=3e-4 --fname=CIFAR10-PreResNet110-randaugment --N 3 --M 45 --num_tta 1 --randaugment \nipython -- ./train/train_randaugment.py --dir=../augres --dataset=CIFAR10 --data_path=~/data --epochs=2000 --model=WideResNet28x10 --lr_init=0.1 --wd=5e-4 --fname=CIFAR10-WideResNet28x10-randaugment --N 3 --M 45 --num_tta 1 --randaugment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8334341852651624
      ],
      "excerpt": "We use 4 different magnitudes of augmentations: 500 samples with M=45, 500 samples with M=20, 100 samples with M=0 (basic crop/flip augmentation) and one sample with no agumentation (central crop). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9001729856355344,
        0.870812546808713,
        0.8820853986647169,
        0.8580778588728148
      ],
      "excerpt": "ipython -- get_predictions_randaugment.py --dataset=CIFAR10 --data_path ~/data --models ~/models/CIFAR10-VGG16BN-stratvalid.pt --fname Preds --N 3 --M 45 --log_dir validpreds --num_tta 500 --fix_sign --true_m0 --verbose --valid \nipython -- get_predictions_randaugment.py --dataset=CIFAR10 --data_path ~/data --models ~/models/CIFAR10-VGG16BN-stratvalid.pt --fname Preds --N 3 --M 20 --log_dir validpreds --num_tta 500 --fix_sign --true_m0 --verbose --valid \nipython -- get_predictions_randaugment.py --dataset=CIFAR10 --data_path ~/data --models ~/models/CIFAR10-VGG16BN-stratvalid.pt --fname Preds --N 3 --M 0 --log_dir validpreds --num_tta 100 --fix_sign --true_m0 --verbose --valid \nipython -- get_predictions_randaugment.py --dataset=CIFAR10 --data_path ~/data --models ~/models/CIFAR10-VGG16BN-stratvalid.pt --fname Preds --N 0 --M 0 --log_dir validpreds --num_tta 1 --fix_sign --true_m0 --verbose --no_tta --valid \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589851910927314
      ],
      "excerpt": "ipython -- get_predictions_randaugment.py --dataset=CIFAR10 --data_path ~/data --models ~/models/CIFAR10-VGG16BN-randaugment.pt --fname Preds --log_dir logs --num_tta 100 --fix_sign --true_m0 --verbose --silent --policy CIFAR10-VGG16BN.npz \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8000262199455452
      ],
      "excerpt": "--data_path ~/imagenet/raw-data --batch_size 500  \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131833634086569
      ],
      "excerpt": "ipython -- get_predictions_randaugment_ood.py --num_tta 100 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248248734366212
      ],
      "excerpt": "<img width=\"80%\" src=\"randaugment.jpg\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SamsungLabs/gps-augment/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/bayesgroup/gps-augment/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 2-Clause License\\n\\nCopyright (c) 2020 Samsung AI Center Moscow, Dmitry Molchanov, Alexander Lyzhov, Yuliya Molchanova, Arsenii Ashukha, Dmitry Vetrov\\nAll rights reserved.\\n\\nParts of this software are based on the following repositories:\\n- Pytorch Ensembles https://github.com/bayesgroup/pytorch-ensembles, Copyright (c) 2020, Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, Dmitry Vetrov\\n- Stochastic Weight Averaging (SWA), https://github.com/timgaripov/swa, Copyright (c) 2018, Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson\\n- PyTorch Examples, https://github.com/pytorch/examples/tree/ee964a2/imagenet, Copyright (c) 2017\\n- PyTorch, https://github.com/pytorch/pytorch, Copyright (c) 2016-present, Facebook Inc\\n- PyTorch RandAugment, https://github.com/ildoonet/pytorch-randaugment, Copyright (c) 2019 Ildoo Kim\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Learnable test-time augmentation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "gps-augment",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "SamsungLabs",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SamsungLabs/gps-augment/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 28,
      "date": "Fri, 24 Dec 2021 20:44:23 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "uncertainty",
      "data-augmentation",
      "test-time-augmentation",
      "ensembles",
      "deep-ensemble",
      "pytorch",
      "out-of-domain",
      "domain-shift",
      "imagenet-c",
      "robustness",
      "deep-learning",
      "uai2020"
    ],
    "technique": "GitHub API"
  }
}