{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.06434 (2015).](https://arxiv.org/pdf/1511.06434.pdf%C3%AF%C2%BC%E2%80%B0)\n\nSample outputs for random digits:\n\n![Random Digits](chapter4-gan/images/dcgan_mnist.gif)\n\n2. [Conditional (GAN)](chapter4-gan/cgan-mnist-4.3.1.py)\n\n[Mirza, Mehdi, and Simon Osindero. \"Conditional generative adversarial nets.\" arXiv preprint https://arxiv.org/abs/1411.1784 (2014).](https://arxiv.org/pdf/1411.1784)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter4-gan/images/cgan_mnist.gif)\n## [Chapter 5 - Improved GAN](chapter5-improved-gan)\n1. [Wasserstein GAN (WGAN)](chapter5-improved-gan/wgan-mnist-5.1.2.py)\n\n[Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein GAN.\" arXiv preprint https://arxiv.org/abs/1701.07875 (2017).](https://arxiv.org/pdf/1701.07875)\n\nSample outputs for random digits:\n\n![Random Digits](chapter5-improved-gan/images/wgan_mnist.gif)\n\n2. [Least Squares GAN (LSGAN)](chapter5-improved-gan/lsgan-mnist-5.2.1.py)\n\n[Mao, Xudong, et al. \"Least squares generative adversarial networks.\" 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017.](http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf)\n\nSample outputs for random digits:\n\n![Random Digits](chapter5-improved-gan/images/lsgan_mnist.gif)\n\n3. [Auxiliary Classfier GAN (ACGAN)](chapter5-improved-gan/acgan-mnist-5.3.1.py)\n\n[Odena, Augustus, Christopher Olah, and Jonathon Shlens. \"Conditional image synthesis with auxiliary classifier GANs. Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.\"](http://proceedings.mlr.press/v70/odena17a.html)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter5-improved-gan/images/acgan_mnist.gif)\n## [Chapter 6 - GAN with Disentangled Latent Representations](chapter6-disentangled-gan)\n1. [Information Maximizing GAN (InfoGAN)](chapter6-disentangled-gan/infogan-mnist-6.1.1.py)\n\n[Chen, Xi, et al. \"Infogan: Interpretable representation learning by information maximizing generative adversarial nets.\" \nAdvances in Neural Information Processing Systems. 2016.](http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter6-disentangled-gan/images/infogan_mnist.gif)\n\n2. [Stacked GAN](chapter6-disentangled-gan/stackedgan-mnist-6.2.1.py)\n\n[Huang, Xun, et al. \"Stacked generative adversarial networks.\" IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Vol. 2. 2017](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter6-disentangled-gan/images/stackedgan_mnist.gif)\n\n## [Chapter 7 - Cross-Domain GAN](chapter7-cross-domain-gan)\n1. [CycleGAN](chapter7-cross-domain-gan/cyclegan-7.1.1.py)\n\n[Zhu, Jun-Yan, et al. \"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.\" 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017.](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)\n\nSample outputs for random cifar10 images:\n\n![Colorized Images](chapter7-cross-domain-gan/images/cifar10_colorization.gif)\n\nSample outputs for MNIST to SVHN:\n\n![MNIST2SVHN](chapter7-cross-domain-gan/images/MNIST2SVHN.png)\n\n## [Chapter 8 - Variational Autoencoders (VAE)](chapter8-vae)\n\n1. [VAE MLP MNIST](chapter8-vae/vae-mlp-mnist-8.1.1.py)\n2. [VAE CNN MNIST](chapter8-vae/cvae-cnn-mnist-8.2.1.py)\n3. [Conditional VAE and Beta VAE](chapter8-vae/cvae-cnn-mnist-8.2.1.py)\n\n[Kingma, Diederik P., and Max Welling. \"Auto-encoding Variational Bayes.\" arXiv preprint https://arxiv.org/abs/1312.6114 (2013).](https://arxiv.org/pdf/1312.6114.pdf)\n\n[Sohn, Kihyuk, Honglak Lee, and Xinchen Yan. \"Learning structured output representation using deep conditional generative models.\" Advances in Neural Information Processing Systems. 2015.](http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf)\n\n[I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. \u03b2-VAE: Learning basic visual concepts with a constrained variational framework. ICLR, 2017.](https://openreview.net/pdf?id=Sy2fzU9gl)\n\nGenerated MNIST by navigating the latent space:\n\n![MNIST](chapter8-vae/images/digits_over_latent.png)\n\n## [Chapter 9 - Deep Reinforcement Learning](chapter9-drl)\n\n1. [Q-Learning](chapter9-drl/q-learning-9.3.1.py)\n2. [Q-Learning on Frozen Lake Environment](chapter9-drl/q-frozenlake-9.5.1.py)\n3. [DQN and DDQN on Cartpole Environment](chapter9-drl/dqn-cartpole-9.6.1.py)\n\nMnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529\n\nDQN on Cartpole Environment:\n\n![Cartpole](chapter9-drl/images/cartpole.gif)\n\n## [Chapter 10 - Policy Gradient Methods](chapter10-policy)\n\n1. [REINFORCE, REINFORCE with Baseline, Actor-Critic, A2C](chapter10-policy/policygradient-car-10.1.1.py)\n\n[Sutton and Barto, Reinforcement Learning: An Introduction ](http://incompleteideas.net/book/bookdraft2017nov5.pdf)\n\n[Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International conference on machine learning. 2016.](http://proceedings.mlr.press/v48/mniha16.pdf)\n\n\nPolicy Gradient on MountainCar Continuous Environment:\n\n![Car](chapter10-policy/images/car.gif)\n\n\n## Citation\nIf you find this work useful, please cite:\n\n```\n@book{atienza2018advanced,\n  title={Advanced Deep Learning with Keras",
      "https://arxiv.org/abs/1411.1784 (2014).](https://arxiv.org/pdf/1411.1784)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter4-gan/images/cgan_mnist.gif)\n## [Chapter 5 - Improved GAN](chapter5-improved-gan)\n1. [Wasserstein GAN (WGAN)](chapter5-improved-gan/wgan-mnist-5.1.2.py)\n\n[Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein GAN.\" arXiv preprint https://arxiv.org/abs/1701.07875 (2017).](https://arxiv.org/pdf/1701.07875)\n\nSample outputs for random digits:\n\n![Random Digits](chapter5-improved-gan/images/wgan_mnist.gif)\n\n2. [Least Squares GAN (LSGAN)](chapter5-improved-gan/lsgan-mnist-5.2.1.py)\n\n[Mao, Xudong, et al. \"Least squares generative adversarial networks.\" 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017.](http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf)\n\nSample outputs for random digits:\n\n![Random Digits](chapter5-improved-gan/images/lsgan_mnist.gif)\n\n3. [Auxiliary Classfier GAN (ACGAN)](chapter5-improved-gan/acgan-mnist-5.3.1.py)\n\n[Odena, Augustus, Christopher Olah, and Jonathon Shlens. \"Conditional image synthesis with auxiliary classifier GANs. Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.\"](http://proceedings.mlr.press/v70/odena17a.html)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter5-improved-gan/images/acgan_mnist.gif)\n## [Chapter 6 - GAN with Disentangled Latent Representations](chapter6-disentangled-gan)\n1. [Information Maximizing GAN (InfoGAN)](chapter6-disentangled-gan/infogan-mnist-6.1.1.py)\n\n[Chen, Xi, et al. \"Infogan: Interpretable representation learning by information maximizing generative adversarial nets.\" \nAdvances in Neural Information Processing Systems. 2016.](http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter6-disentangled-gan/images/infogan_mnist.gif)\n\n2. [Stacked GAN](chapter6-disentangled-gan/stackedgan-mnist-6.2.1.py)\n\n[Huang, Xun, et al. \"Stacked generative adversarial networks.\" IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Vol. 2. 2017](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter6-disentangled-gan/images/stackedgan_mnist.gif)\n\n## [Chapter 7 - Cross-Domain GAN](chapter7-cross-domain-gan)\n1. [CycleGAN](chapter7-cross-domain-gan/cyclegan-7.1.1.py)\n\n[Zhu, Jun-Yan, et al. \"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.\" 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017.](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)\n\nSample outputs for random cifar10 images:\n\n![Colorized Images](chapter7-cross-domain-gan/images/cifar10_colorization.gif)\n\nSample outputs for MNIST to SVHN:\n\n![MNIST2SVHN](chapter7-cross-domain-gan/images/MNIST2SVHN.png)\n\n## [Chapter 8 - Variational Autoencoders (VAE)](chapter8-vae)\n\n1. [VAE MLP MNIST](chapter8-vae/vae-mlp-mnist-8.1.1.py)\n2. [VAE CNN MNIST](chapter8-vae/cvae-cnn-mnist-8.2.1.py)\n3. [Conditional VAE and Beta VAE](chapter8-vae/cvae-cnn-mnist-8.2.1.py)\n\n[Kingma, Diederik P., and Max Welling. \"Auto-encoding Variational Bayes.\" arXiv preprint https://arxiv.org/abs/1312.6114 (2013).](https://arxiv.org/pdf/1312.6114.pdf)\n\n[Sohn, Kihyuk, Honglak Lee, and Xinchen Yan. \"Learning structured output representation using deep conditional generative models.\" Advances in Neural Information Processing Systems. 2015.](http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf)\n\n[I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. \u03b2-VAE: Learning basic visual concepts with a constrained variational framework. ICLR, 2017.](https://openreview.net/pdf?id=Sy2fzU9gl)\n\nGenerated MNIST by navigating the latent space:\n\n![MNIST](chapter8-vae/images/digits_over_latent.png)\n\n## [Chapter 9 - Deep Reinforcement Learning](chapter9-drl)\n\n1. [Q-Learning](chapter9-drl/q-learning-9.3.1.py)\n2. [Q-Learning on Frozen Lake Environment](chapter9-drl/q-frozenlake-9.5.1.py)\n3. [DQN and DDQN on Cartpole Environment](chapter9-drl/dqn-cartpole-9.6.1.py)\n\nMnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529\n\nDQN on Cartpole Environment:\n\n![Cartpole](chapter9-drl/images/cartpole.gif)\n\n## [Chapter 10 - Policy Gradient Methods](chapter10-policy)\n\n1. [REINFORCE, REINFORCE with Baseline, Actor-Critic, A2C](chapter10-policy/policygradient-car-10.1.1.py)\n\n[Sutton and Barto, Reinforcement Learning: An Introduction ](http://incompleteideas.net/book/bookdraft2017nov5.pdf)\n\n[Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International conference on machine learning. 2016.](http://proceedings.mlr.press/v48/mniha16.pdf)\n\n\nPolicy Gradient on MountainCar Continuous Environment:\n\n![Car](chapter10-policy/images/car.gif)\n\n\n## Citation\nIf you find this work useful, please cite:\n\n```\n@book{atienza2018advanced,\n  title={Advanced Deep Learning with Keras",
      "https://arxiv.org/abs/1701.07875 (2017).](https://arxiv.org/pdf/1701.07875)\n\nSample outputs for random digits:\n\n![Random Digits](chapter5-improved-gan/images/wgan_mnist.gif)\n\n2. [Least Squares GAN (LSGAN)](chapter5-improved-gan/lsgan-mnist-5.2.1.py)\n\n[Mao, Xudong, et al. \"Least squares generative adversarial networks.\" 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017.](http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf)\n\nSample outputs for random digits:\n\n![Random Digits](chapter5-improved-gan/images/lsgan_mnist.gif)\n\n3. [Auxiliary Classfier GAN (ACGAN)](chapter5-improved-gan/acgan-mnist-5.3.1.py)\n\n[Odena, Augustus, Christopher Olah, and Jonathon Shlens. \"Conditional image synthesis with auxiliary classifier GANs. Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.\"](http://proceedings.mlr.press/v70/odena17a.html)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter5-improved-gan/images/acgan_mnist.gif)\n## [Chapter 6 - GAN with Disentangled Latent Representations](chapter6-disentangled-gan)\n1. [Information Maximizing GAN (InfoGAN)](chapter6-disentangled-gan/infogan-mnist-6.1.1.py)\n\n[Chen, Xi, et al. \"Infogan: Interpretable representation learning by information maximizing generative adversarial nets.\" \nAdvances in Neural Information Processing Systems. 2016.](http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter6-disentangled-gan/images/infogan_mnist.gif)\n\n2. [Stacked GAN](chapter6-disentangled-gan/stackedgan-mnist-6.2.1.py)\n\n[Huang, Xun, et al. \"Stacked generative adversarial networks.\" IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Vol. 2. 2017](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf)\n\nSample outputs for digits 0 to 9:\n\n![Zero to Nine](chapter6-disentangled-gan/images/stackedgan_mnist.gif)\n\n## [Chapter 7 - Cross-Domain GAN](chapter7-cross-domain-gan)\n1. [CycleGAN](chapter7-cross-domain-gan/cyclegan-7.1.1.py)\n\n[Zhu, Jun-Yan, et al. \"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.\" 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017.](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)\n\nSample outputs for random cifar10 images:\n\n![Colorized Images](chapter7-cross-domain-gan/images/cifar10_colorization.gif)\n\nSample outputs for MNIST to SVHN:\n\n![MNIST2SVHN](chapter7-cross-domain-gan/images/MNIST2SVHN.png)\n\n## [Chapter 8 - Variational Autoencoders (VAE)](chapter8-vae)\n\n1. [VAE MLP MNIST](chapter8-vae/vae-mlp-mnist-8.1.1.py)\n2. [VAE CNN MNIST](chapter8-vae/cvae-cnn-mnist-8.2.1.py)\n3. [Conditional VAE and Beta VAE](chapter8-vae/cvae-cnn-mnist-8.2.1.py)\n\n[Kingma, Diederik P., and Max Welling. \"Auto-encoding Variational Bayes.\" arXiv preprint https://arxiv.org/abs/1312.6114 (2013).](https://arxiv.org/pdf/1312.6114.pdf)\n\n[Sohn, Kihyuk, Honglak Lee, and Xinchen Yan. \"Learning structured output representation using deep conditional generative models.\" Advances in Neural Information Processing Systems. 2015.](http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf)\n\n[I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. \u03b2-VAE: Learning basic visual concepts with a constrained variational framework. ICLR, 2017.](https://openreview.net/pdf?id=Sy2fzU9gl)\n\nGenerated MNIST by navigating the latent space:\n\n![MNIST](chapter8-vae/images/digits_over_latent.png)\n\n## [Chapter 9 - Deep Reinforcement Learning](chapter9-drl)\n\n1. [Q-Learning](chapter9-drl/q-learning-9.3.1.py)\n2. [Q-Learning on Frozen Lake Environment](chapter9-drl/q-frozenlake-9.5.1.py)\n3. [DQN and DDQN on Cartpole Environment](chapter9-drl/dqn-cartpole-9.6.1.py)\n\nMnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529\n\nDQN on Cartpole Environment:\n\n![Cartpole](chapter9-drl/images/cartpole.gif)\n\n## [Chapter 10 - Policy Gradient Methods](chapter10-policy)\n\n1. [REINFORCE, REINFORCE with Baseline, Actor-Critic, A2C](chapter10-policy/policygradient-car-10.1.1.py)\n\n[Sutton and Barto, Reinforcement Learning: An Introduction ](http://incompleteideas.net/book/bookdraft2017nov5.pdf)\n\n[Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International conference on machine learning. 2016.](http://proceedings.mlr.press/v48/mniha16.pdf)\n\n\nPolicy Gradient on MountainCar Continuous Environment:\n\n![Car](chapter10-policy/images/car.gif)\n\n\n## Citation\nIf you find this work useful, please cite:\n\n```\n@book{atienza2018advanced,\n  title={Advanced Deep Learning with Keras",
      "https://arxiv.org/abs/1312.6114 (2013).](https://arxiv.org/pdf/1312.6114.pdf)\n\n[Sohn, Kihyuk, Honglak Lee, and Xinchen Yan. \"Learning structured output representation using deep conditional generative models.\" Advances in Neural Information Processing Systems. 2015.](http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf)\n\n[I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. \u03b2-VAE: Learning basic visual concepts with a constrained variational framework. ICLR, 2017.](https://openreview.net/pdf?id=Sy2fzU9gl)\n\nGenerated MNIST by navigating the latent space:\n\n![MNIST](chapter8-vae/images/digits_over_latent.png)\n\n## [Chapter 9 - Deep Reinforcement Learning](chapter9-drl)\n\n1. [Q-Learning](chapter9-drl/q-learning-9.3.1.py)\n2. [Q-Learning on Frozen Lake Environment](chapter9-drl/q-frozenlake-9.5.1.py)\n3. [DQN and DDQN on Cartpole Environment](chapter9-drl/dqn-cartpole-9.6.1.py)\n\nMnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529\n\nDQN on Cartpole Environment:\n\n![Cartpole](chapter9-drl/images/cartpole.gif)\n\n## [Chapter 10 - Policy Gradient Methods](chapter10-policy)\n\n1. [REINFORCE, REINFORCE with Baseline, Actor-Critic, A2C](chapter10-policy/policygradient-car-10.1.1.py)\n\n[Sutton and Barto, Reinforcement Learning: An Introduction ](http://incompleteideas.net/book/bookdraft2017nov5.pdf)\n\n[Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International conference on machine learning. 2016.](http://proceedings.mlr.press/v48/mniha16.pdf)\n\n\nPolicy Gradient on MountainCar Continuous Environment:\n\n![Car](chapter10-policy/images/car.gif)\n\n\n## Citation\nIf you find this work useful, please cite:\n\n```\n@book{atienza2018advanced,\n  title={Advanced Deep Learning with Keras"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this work useful, please cite:\n\n```\n@book{atienza2018advanced,\n  title={Advanced Deep Learning with Keras},\n  author={Atienza, Rowel},\n  year={2018},\n  publisher={Packt Publishing Ltd}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@book{atienza2018advanced,\n  title={Advanced Deep Learning with Keras},\n  author={Atienza, Rowel},\n  year={2018},\n  publisher={Packt Publishing Ltd}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.99807139452412
      ],
      "excerpt": "Radford, Alec, Luke Metz, and Soumith Chintala. \"Unsupervised representation learning with deep convolutional generative adversarial networks.\" arXiv preprint arXiv:1511.06434 (2015). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999761492995796
      ],
      "excerpt": "Mirza, Mehdi, and Simon Osindero. \"Conditional generative adversarial nets.\" arXiv preprint arXiv:1411.1784 (2014). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999482520348485
      ],
      "excerpt": "Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein GAN.\" arXiv preprint arXiv:1701.07875 (2017). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999999998955076
      ],
      "excerpt": "Mao, Xudong, et al. \"Least squares generative adversarial networks.\" 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999800396584985
      ],
      "excerpt": "Odena, Augustus, Christopher Olah, and Jonathon Shlens. \"Conditional image synthesis with auxiliary classifier GANs. Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.997054489633101,
        0.9627825314486358
      ],
      "excerpt": "Chen, Xi, et al. \"Infogan: Interpretable representation learning by information maximizing generative adversarial nets.\"  \nAdvances in Neural Information Processing Systems. 2016. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999999999960636
      ],
      "excerpt": "Huang, Xun, et al. \"Stacked generative adversarial networks.\" IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Vol. 2. 2017 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999999999864286
      ],
      "excerpt": "Zhu, Jun-Yan, et al. \"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.\" 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999276015529297,
        0.8161852097721835
      ],
      "excerpt": "Kingma, Diederik P., and Max Welling. \"Auto-encoding Variational Bayes.\" arXiv preprint arXiv:1312.6114 (2013). \nSohn, Kihyuk, Honglak Lee, and Xinchen Yan. \"Learning structured output representation using deep conditional generative models.\" Advances in Neural Information Processing Systems. 2015. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8827840915603409
      ],
      "excerpt": "Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999852338885462
      ],
      "excerpt": "Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International conference on machine learning. 2016. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/soyoung9306/Keras-advansed2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-11T10:51:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-16T03:15:55Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9957740380206893,
        0.9656246274633424,
        0.8982274721455666
      ],
      "excerpt": "This is the code repository for Advanced Deep Learning with Keras, published by Packt. It contains all the supporting project files necessary to work through the book from start to finish. \nThis book covers advanced deep learning techniques to create successful AI. Using MLPs, CNNs, and RNNs as building blocks to more advanced techniques, you\u2019ll study deep neural network architectures, Autoencoders, Generative Adversarial Networks (GANs), Variational AutoEncoders (VAEs), and Deep Reinforcement Learning (DRL) critical to many cutting-edge AI results. \nAll of the code is organized into folders. Each folder starts with a number followed by the application name. For example, chapter2-deep-networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9006935005897961,
        0.9528137161429222,
        0.8875325166294367,
        0.8372396012058242,
        0.9786490849789912,
        0.8372396012058242
      ],
      "excerpt": "Deep Reinforcement Learning Hands-On \nDeep Learning with Keras \nReinforcement Learning with TensorFlow \nMLP on MNIST \nCNN on MNIST \nRNN on MNIST \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882824607646918,
        0.8372396012058242
      ],
      "excerpt": "ResNet v1 and v2 on CIFAR10 \nDenseNet on CIFAR10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9627487715650112
      ],
      "excerpt": "VAE CNN MNIST \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9438144838120519
      ],
      "excerpt": "Generated MNIST by navigating the latent space: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9508915433312637
      ],
      "excerpt": "Sutton and Barto, Reinforcement Learning: An Introduction  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "\uc0dd\uc131\ubaa8\ub378 \ubc0f \uac15\ud654\ud559\uc2b5",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/soyoung9306/GAN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 10:25:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/soyoung9306/Keras-advansed2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "soyoung9306/Keras-advansed2",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8202918942955407
      ],
      "excerpt": "The code will look like the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8297235760342162,
        0.8014074056533506
      ],
      "excerpt": "Q-Learning on Frozen Lake Environment \nDQN and DDQN on Cartpole Environment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9614187234762808
      ],
      "excerpt": "DQN on Cartpole Environment: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "                  instance_norm=True): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8183744054171672
      ],
      "excerpt": "Sample outputs for random cifar10 images: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8183744054171672
      ],
      "excerpt": "Sample outputs for random cifar10 images: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/soyoung9306/Keras-advansed2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Packt\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Advanced Deep Learning with Keras",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Keras-advansed2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "soyoung9306",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/soyoung9306/Keras-advansed2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 10:25:09 GMT"
    },
    "technique": "GitHub API"
  }
}