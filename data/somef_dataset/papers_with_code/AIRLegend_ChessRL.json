{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AIRLegend/ChessRL",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-21T08:07:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-04T11:19:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9276794455260774,
        0.9789205108968169,
        0.9634418434786923,
        0.8922253887084584,
        0.9368084692745213,
        0.9868959562026839,
        0.9541931653907323,
        0.901886083235268
      ],
      "excerpt": "Personal project to build a chess engine based using reinforcement learning. \nThe idea is to some sort replicate the system built by DeepMind with AlphaZero. I'm \naware that the computational resources to achieve their results is huge, but my aim \nit's simply to reach an amateur chess level performance (about 1200-1400 Elo), not \nstate of the art. \nAt this moment, the approach I'm using is based on pre-training a model using self-play data of a Stockfish  \nalgorithm. Later, the idea is to put two models to play agaisnt each other and make selection/merges of weights (RL part). \nIf you want to reuse this code on your project, and have any doubt here you will find some explanation about the most important classes. Also, feel free to open an issue on this repo to ask. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9173398476317861,
        0.8865846676624355
      ],
      "excerpt": "DISCLAIMER: This is under development and can still contains bugs or  inefficiencies and modifications are being made. \nDISCLAIMER 2: As soon as I get acceptable results, I will also share weights/datasets with this code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8800362603828651
      ],
      "excerpt": "The main purpose of this part is to pre-train the model to make the policy head of the network to reliably predict the game outcome. This will be useful during the self-play phase as the MCTS will make better move policies (reducing the training time). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9747807043301028
      ],
      "excerpt": "Once we have a pretrained model, we can move to the self-play phase. The incharged of this process is the selfplay.py script, which will fire up a instance of the model which play agaisnt itself and after each one, makes a training round (saving the model and the results). Please, take a look at the possible arguments. However, here you have an example. (Keep in mind that this is an expensive process which takes a considerable amount of time per move). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9690842523156655
      ],
      "excerpt": "(And set the \"Horizontal axis\" to \"WALL\" for viewing all the diferent runs.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9549567345793379
      ],
      "excerpt": "contains the recorded training games of the model. With this, we can study its \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8736567539226423
      ],
      "excerpt": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Deep Reinforcement Learning for Chess",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AIRLegend/ChessRL/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 03:11:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AIRLegend/ChessRL/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "AIRLegend/ChessRL",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/AIRLegend/ChessRL/master/res/get_stockfish.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src/chessrl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src/chessrl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src/chessrl \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8629400568140789,
        0.943845960184624
      ],
      "excerpt": "cd src/chessrl \npython gen_data_stockfish.py ../../data/dataset_stockfish.json --games 100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9081945941908224
      ],
      "excerpt": "cd src/chessrl \npython supervised.py ../../data/models/model1 ../../data/dataset_stockfish.json --epochs 2 --bs 4 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9365771407782385
      ],
      "excerpt": "cd src/chessrl \npython selfplay.py ../../data/models/model1-superv --games 100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8697041331729144
      ],
      "excerpt": "tensorboard --logdir data/models/model1/train \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AIRLegend/ChessRL/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "JavaScript",
      "HTML",
      "CSS",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright 2019 Chris Oakman\\n\\nPermission is hereby granted, free of charge, to any person obtaining\\na copy of this software and associated documentation files (the\\n\"Software\"), to deal in the Software without restriction, including\\nwithout limitation the rights to use, copy, modify, merge, publish,\\ndistribute, sublicense, and/or sell copies of the Software, and to\\npermit persons to whom the Software is furnished to do so, subject to\\nthe following conditions:\\n\\nThe above copyright notice and this permission notice shall be\\nincluded in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Neural chess <br> Reinforcement Learning based chess engine.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ChessRL",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "AIRLegend",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AIRLegend/ChessRL/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The necessary python packages (ATM) are listed in the requirements file.\nYou can install them with\n\n```bash\npip3 install -r requirements.txt\n```\n\nTensorflow is also needed, but you must install either `tensorflow` or `tensorflow-gpu` (for the development I used >= TF 2.0).\n\nAlso, you need to download the specific \n[stockfish binary](https://stockfishchess.org/download/) for your platform,\nfor automating this made a script to automatically download it.\n\n```bash\ncd res\nchmod +x get_stockfish.sh\n./get_stockfish.sh linux   #:or \"mac\", depending of your platform. \n```\nIf you want to download it manually, you have to put the stockfish executable under `res/stockfish-10-64` path, in order to the training script to detect it.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Thu, 23 Dec 2021 03:11:52 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "chess",
      "alpha-zero",
      "chess-engine",
      "deep-learning",
      "neural-network"
    ],
    "technique": "GitHub API"
  }
}