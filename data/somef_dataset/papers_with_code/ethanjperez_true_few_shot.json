{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2105.11447",
      "https://arxiv.org/abs/2005.14165",
      "https://arxiv.org/abs/2103.11955",
      "https://arxiv.org/abs/2103.11955",
      "https://arxiv.org/abs/2105.11447}\n}\n```"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n@article{perez2021true,\n  author = {Ethan Perez and Douwe Kiela and Kyunghyun Cho},\n  title = {True Few-Shot Learning with Language Models},\n  journal={NeurIPS},\n  year = {2021},\n  url = {https://arxiv.org/abs/2105.11447}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{perez2021true,\n  author = {Ethan Perez and Douwe Kiela and Kyunghyun Cho},\n  title = {True Few-Shot Learning with Language Models},\n  journal={NeurIPS},\n  year = {2021},\n  url = {https://arxiv.org/abs/2105.11447}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9287913210266059
      ],
      "excerpt": "wget https://www.cis.uni-muenchen.de/~poerner/blobs/e-bert/LAMA_UHN.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9917074309379053
      ],
      "excerpt": "torch_version_suffix=\"+cu110\" #: Use \"+cu100\" for CUDA 10.0, \"+cu101\" for CUDA 10.1, and \"\" for CUDA 10.2 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ethanjperez/true_few_shot",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-21T22:46:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T11:44:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8903609585111212,
        0.9087084457631385,
        0.9901779864797746,
        0.8639776679028092
      ],
      "excerpt": "This codebase supports using language models (LMs) for true few-shot learning: learning to perform a task using a limited number of examples from a single task distribution. \nWe choose prompts and hyperparameters for few-shot learning methods using no additional held-out data via methods like cross-validation and minimum description length. \nThe code reproduces the results in our paper and supports two forms of few-shot learning: \n1. \"In-context\" learning using LMs similar to GPT-3. Here, we format a few training examples as input to the LM using a natural language \"prompt,\" and we use the LM to predict the next token. We include the code for in-context learning primarily in the top-level directory (largely in eval_lm.py).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.924447236197831
      ],
      "excerpt": "You can run this codebase with GPT-2/DistilGPT-2 (using HuggingFace Transformers) or GPT-3 (if you have a key from OpenAI). The underlying model you use is abstracted away using a common API. Below, we describe how to reproduce our results, as well as how to download our precomputed results that we used to produce our paper's plots. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8979411005071259
      ],
      "excerpt": "mv data/* . \nrmdir data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8979411005071259,
        0.8979411005071259,
        0.8979411005071259
      ],
      "excerpt": "unzip data.zip \nmv data/* . \nrmdir data \nrm data.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.809705181828744
      ],
      "excerpt": "At this point, you'll need to get the results from LM(s) on LAMA or SuperGLUE, in order to later evaluate MDL/CV/Test Accuracy. You can get these results by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458134228833952,
        0.9739052551549471
      ],
      "excerpt": "After either of the above, we'll describe how you can plot the results in our paper. \nWe save a lot of statistics about the predictions made by different GPT models using the eval_lm.py command above, which would make it time-consuming to load all of the data in every time we'd like to plot different results. Thus, we first extract the stats that we care about (e.g., stats we need to compute cv/mdl) and save them to smaller files like so: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8116444739582804,
        0.9558913519376918
      ],
      "excerpt": "DATA_NAME=\"TREx\"     #: Use the \"TREx\" split of LAMA-UHN \nFIELDS=\"nlls ranks\"  #: names of stats we use to compute cv/mdl/acc (\"nlls\" for LM Negative Log-Likelihood, \"ranks\" to get the rank of the true answer, according to the LM -- we will convert this to accuracy later on)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.904598068370949
      ],
      "excerpt": "FIELDS=\"verbalizer_nlls verbalize_accs\"  #: names of stats we use to compute cv/mdl/acc (\"verbalizer_nlls\" to get the NLL of the true answer after eliminating tokens that aren't \"verbalizer\" tokens or class names; \"verbalizer_accs\" to get the accuracy when only consider the probabilities of classes with class names instead of all possible tokens)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8866979703711901
      ],
      "excerpt": "You can load the results from our GPT-2 evaluation runs, to avoid having to evaluate all of the models yourself: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9531366401692347
      ],
      "excerpt": "We do not have permission from OpenAI to release our GPT-3 results (please send us an email if this is an issue for you). However, our pre-computed GPT-2 results will still allow you to move on to plotting our results for GPT-2 models below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550300649961301
      ],
      "excerpt": "    <td> <b> Plots results for... </b> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8818232107851858
      ],
      "excerpt": "    <td> various model selection criteria (all criteria described in the Appendix) </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9893711350820269
      ],
      "excerpt": "Here, we describe how to choose hyperparameters for ADAPET, a few-shot learning method that finetunes a language model using a classification loss alongside an auxiliary masked language modeling objective. We use a modified version of their original code, which we include in this repo. We now detail how to setup and run our version of the repo to reproduce our results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9934660038434662
      ],
      "excerpt": "We shuffle FewGLUE and generate the 3 additional random subsets of SuperGLUE used in our paper with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8482179311806728,
        0.830662505765935
      ],
      "excerpt": "TN=\"WiC\"     #: For other tasks, change to \"CB\" \"COPA\" \"BoolQ\" \"RTE\" \"WSC\" \"MultiRC\" \"ReCoRD\" \nTSS=0        #: Random seed for sampling training set. Use TSS=0 for FewGLUE. We used TSS in 0 1 2 3 for our 4 training sets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.872700046819868
      ],
      "excerpt": "Evaluate the K=8 fold cross-validation loss for the above dataset/hyperparameters by training 8 models as follows (the code refers to \"folds\" as \"blocks\", following terminology in MDL): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8658245335883024
      ],
      "excerpt": "SM=\"cv\"      #: selection method: \"cv\" for cross-validation or \"mdl\" for minimum description length \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055027039707183
      ],
      "excerpt": "for BN in $(seq 0 $((NB-1))); do  #: iterate over all block (fold) numbers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9957578527875662
      ],
      "excerpt": "For more details on how to train/evaluate/test ADAPET, please see the README of the original ADAPET repo, which we used for our code (with only minor modifications). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8393926004113166
      ],
      "excerpt": "rm -r fewglue  #: delete any existing results, so we can replace them with our pre-computed results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.919664719740372
      ],
      "excerpt": "TSSS=\"0 1\"  #: Train Set Seeds to show mean/std. dev. results for (we used 0 1 2 3 in our paper) \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for the paper \"True Few-Shot Learning in Language Models\" (https://arxiv.org/abs/2105.11447)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ethanjperez/true_few_shot/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Tue, 28 Dec 2021 00:38:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ethanjperez/true_few_shot/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ethanjperez/true_few_shot",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ethanjperez/true_few_shot/main/bin/test.sh",
      "https://raw.githubusercontent.com/ethanjperez/true_few_shot/main/bin/init.sh",
      "https://raw.githubusercontent.com/ethanjperez/true_few_shot/main/bin/train.sh",
      "https://raw.githubusercontent.com/ethanjperez/true_few_shot/main/bin/setup.sh",
      "https://raw.githubusercontent.com/ethanjperez/true_few_shot/main/bin/dev.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone this repo into your current working directory. Then, follow the general setup instructions below:\n```bash\ncd true_few_shot    #: Move into cloned directory\nexport BASE=$PWD    #: Store top-level directory location\nmkdir data exp_out  #: Make directories for data and experiment results, or symlink to a location that can host large files\n```\n\nContinue below to reproduce our prompt selection experiments (with GPT models on LAMA or SuperGLUE). Skip to [Hyperparameter Selection](#true-few-shot-hyperparameter-selection-for-adapet) to reproduce our ADAPET results.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.936628297057198,
        0.954950449355536,
        0.9770335174395833,
        0.9074526300204816
      ],
      "excerpt": "First, create a virtual Python 3.7+ environment. We installed and activated a Python 3.7 with Anaconda 3 (downloadable from docs.anaconda.com) like so: \nconda create -y -n true_few_shot python=3.7 \nconda activate true_few_shot \n: To deactivate the environment, use conda deactivate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9529317201571271,
        0.897236615707984,
        0.9954427483597357
      ],
      "excerpt": "Next, install the dependencies for this repo: \ncd $BASE \npip install -r requirements_prompt.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8096422022825122
      ],
      "excerpt": ": Download LAMA-UHN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8096422022825122
      ],
      "excerpt": ": Download LAMA vocab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880928426990838
      ],
      "excerpt": ": Download original LAMA (to get manual prompts) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977318542907875
      ],
      "excerpt": "To experiment with GPT2/DistilGPT2 models, you'll then need to install PyTorch to use transformers models (PyTorch download instructions at pytorch.org). (PyTorch installation is not required for GPT3 models.) We used PyTorch 1.7.1 (with CUDA 11.0.194 for GPU inference), installed with the below command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.983740791831395,
        0.9133375897388751
      ],
      "excerpt": "pip install torch==1.7.1${torch_version_suffix} torchvision==0.8.2${torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html \nTo experiment with GPT3 models, if you have GPT3 access, set your API key as a bash variable (otherwise, you can still run this repo using GPT2 models): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897236615707984
      ],
      "excerpt": "cd $BASE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897236615707984
      ],
      "excerpt": "cd $BASE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8725549602927676
      ],
      "excerpt": "tar -xzvf eval_lm_results.tar.gz \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481145459886135
      ],
      "excerpt": "rm eval_lm_results.tar.gz \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897236615707984
      ],
      "excerpt": "cd $BASE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9552998903651976,
        0.9701647175716385
      ],
      "excerpt": "ADAPET trains on GPU, so you'll need to ensure that CUDA is installed for NVIDIA GPUs. \nFirst, move to the main directory (cd $BASE) and deactivate any existing virtual environments (e.g. conda deactivate if using conda). Then run source bin/init.sh, which will automatically:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9880321676300292,
        0.9961625397812048,
        0.8193579827725417,
        0.9770335174395833,
        0.9979947896609701,
        0.9889076037524084
      ],
      "excerpt": "- Install and setup environment with correct dependencies into a virtual environment. \nIf you run into issues installing the virtual env with the source bin/init.sh, you can also use conda (as we did) to run experiments instead. We installed Python 3.7.10 using Anaconda 4.8.3, as well as the required dependencies (including PyTorch 1.5) like so: \nconda create -y -n adapet python=3.7.10 \nconda activate adapet \npip install -r requirements.txt \nTo train with an AMD GPU instead of NVIDIA, you'll need to install ROCm (AMD's CUDA) and then install a ROCm-compatible version of PyTorch 1.8+ instead of PyTorch 1.5 as done above (see pytorch.org for instructions). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8725549602927676,
        0.8481145459886135
      ],
      "excerpt": "tar -xzvf eval_train_labels.tar.gz \nrm eval_train_labels.tar.gz \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8688029732130124
      ],
      "excerpt": "mkdir -p $checkpoint_dir  #: Make directory for saving results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "    mkdir -p $checkpoint_dir \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897236615707984
      ],
      "excerpt": "cd $BASE/exp_out \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8725549602927676,
        0.8481145459886135
      ],
      "excerpt": "tar -xzvf fewglue_adapet_results.tar.gz \nrm fewglue_adapet_results.tar.gz \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897236615707984
      ],
      "excerpt": "cd $BASE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897236615707984
      ],
      "excerpt": "cd $BASE \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.810870454768102
      ],
      "excerpt": ": Download LAMA-UHN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082022887640018
      ],
      "excerpt": "unzip LAMA_UHN.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017285540429218,
        0.810870454768102
      ],
      "excerpt": "rm LAMA_UHN.zip \n: Download LAMA vocab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8505159152472684
      ],
      "excerpt": "unzip data.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.854816245009357
      ],
      "excerpt": "rm data.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082022887640018,
        0.8017285540429218
      ],
      "excerpt": "unzip lpaqa.zip \nrm lpaqa.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.827786547106459
      ],
      "excerpt": "python extract_fields.py --data_name $DATA_NAME --keys $FIELDS  #: see the command line flags in this script, if you'd like to just extract results for a subset of models, training seeds, etc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.827786547106459
      ],
      "excerpt": "python extract_fields.py --data_name $DATA_NAME --keys $FIELDS  #: see the command line flags in this script, if you'd like to just extract results for a subset of models, training seeds, etc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8887588172932509
      ],
      "excerpt": "python plot_results.py --exp 'TREx-vary_models' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8122024735526998
      ],
      "excerpt": "To plot our other results, change the value for the experiment flag --exp: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8331892592288173
      ],
      "excerpt": "    <td> various numbers of training examples (all sizes up to 6.7B; figures 3, 4) </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python subsample_superglue.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9787631423339477
      ],
      "excerpt": "python src/train.py -c config/$TN.json -k \"exp_name='$checkpoint_dirname'\" \"mask_alpha=$MA\" \"fixed_mask_ratio=$FMR\" \"train_set_seed=$TSS\" \"save_model=True\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9703350355936583
      ],
      "excerpt": "    python src/train.py -c config/$TN.json -k \"exp_name='$checkpoint_dirname'\" \"selection_method='$SM'\" \"num_blocks=$NB\" \"block_no=$BN\" \"mask_alpha=$MA\" \"fixed_mask_ratio=$FMR\" \"train_set_seed=$TSS\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052805227076848
      ],
      "excerpt": ": Download training run results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8265094701666488
      ],
      "excerpt": "python adapet.py \nThe above command will print a latex table showing the results for the best/worst/mean/median hyperparameters, as well as the CV/MDL-chosen hyperparameters. You can also show a subset of results (e.g., if you haven't trained on all SuperGLUE tasks), by using command line flags: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python adapet.py --tns $TNS --tsss $TSSS --sms $SMS \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ethanjperez/true_few_shot/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "True Few-Shot Learning with Language Models",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "true_few_shot",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ethanjperez",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ethanjperez/true_few_shot/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First, move to the top-level directory in this repo (e.g., run `cd $BASE`). From there, the following command will run inference with DistilGPT2 on LAMA-UHN:\n```bash\npython eval_lm.py --engine distilgpt2 --num_train 5 --seeds 0\n```\nThis command chooses 5 random examples from LAMA-UHN as training examples, randomly orders them in 5!=120 different ways, and appends an unseen (test) example from LAMA-UHN to the end. Then, the code evaluates the log-probability of the correct answer for each train/test example, which we'll use later to compute CV/MDL/Test Accuracy. Below we describe each command line flag:\n- `--engine`: specifies the model you'd like to evaluate with\n- `--num_train`: specifies the number of training examples you'd like to use (here, 5); using more than 5 will result in evaluating MDL/CV/test accuracy using only a subset of all possible training example permutations, as described in our paper experiments when we use >5 training examples\n- `--seeds`: takes a list of integers (e.g., `0 1 2`) and uses each integer as a random seed for sampling a new LAMA training set (per relation); so if you run with `--seeds 0 1 2`, you'll run on LAMA 3 times in total (useful for calculating mean and std. error over several runs).\n\nWe use a similar command to run on CB, RTE, and WiC, just adding a couple extra flags:\n```bash\npython eval_lm.py --engine distilgpt2 --num_train 5 --seeds 0 --data_name super_glue --rels cb rte wic\n```\nThe `--data_name` flag specifies that you want to run on SuperGLUE datasets, and the `--rels` flags specifies which SuperGLUE datasets you'd like to run on. BoolQ is also supported (using `--rels boolq`), but be warned that the inputs are quite long, so which can make running GPT2 models time-consuming and running GPT3 models costly. Other datasets require some extra modification to use their respective few-shot approach described in the GPT3 paper.\n\nAt this point, if you've run one of the above `eval_lm.py` commands, you can skip to [Post-processing GPT Results](#post-processing-gpt-results). Below, we show the specific commands we used to reproduce different sets of results in our paper.\n\nFor reference, we include a full bash loop you can run to reproduce all of our LAMA experiments that use 5 training examples:\n```bash\nfor ENGINE in 'distilgpt2' 'gpt2' 'gpt2-medium' 'gpt2-large' 'gpt2-xl' 'ada' 'babbage' 'curie' 'davinci'; do  #: Different models, in order: DistilGPT2, GPT2 (117M, 345M, 782M, 1.5B), and GPT3 (2.7B, 6.7B, 13B, 175B) \nfor NT in 5; do  #: 5 training examples\nfor SEED in 0 1 2 3 4; do  #: 5 random seeds to sample different training sets\npython eval_lm.py --engine $ENGINE --num_train $NT --seeds $SEED\n#: Results will be saved to $BASE/data/rel2template2results.data_name-TREx_UHN.engine-$ENGINE.num_train-$NT.sort_by_weight-False/seed-$SEED\ndone\ndone\ndone\n```\nYou'll probably want to parallelize the calls to `eval_lm.py`, since the above will take a while on a single GPU. Running with GPT3 models (`ada`, `babbage`, `curie`, `davinci`) will query the OpenAI API (so you'll be charged for these queries).\n\nSimilarly, we include the full bash loop you can run to reproduce our results for true few-shot prompt selection varying the number of training examples used for selection:\n```bash\nfor ENGINE in 'distilgpt2' 'gpt2' 'gpt2-medium' 'gpt2-large' 'gpt2-xl' 'ada' 'babbage'; do  #: Here, we don't use curie (13B) and davinci (175B) models for cost reasons \nfor NT in 5 10 15 20 30 40; do  #: you won't need to run with NT=5 if you ran the above bash loop\nfor SEED in 0 1 2 3 4; do  #: 5 random seeds to sample different training sets\npython eval_lm.py --engine $ENGINE --num_train $NT --seeds $SEED\n#: Results will be saved to $BASE/data/rel2template2results.data_name-TREx_UHN.engine-$ENGINE.num_train-$NT.sort_by_weight-False/seed-$SEED\ndone\ndone\ndone\n```\n\nLastly, we include the bash loop for reproducing our SuperGLUE experiments:\n```bash\nfor ENGINE in 'distilgpt2' 'gpt2' 'gpt2-medium' 'gpt2-large' 'gpt2-xl' 'ada' 'babbage' 'curie' 'davinci'; do \nfor NT in 5; do\nfor SEED in 0 1 2 3 4; do\npython eval_lm.py --engine $ENGINE --num_train $NT --seeds $SEED --data_name super_glue --rels cb rte wic \n#: Results will be saved to $BASE/data/rel2template2results.data_name-super_glue_UHN.engine-$ENGINE.num_train-$NT.sort_by_weight-False/seed-$SEED\ndone\ndone\ndone\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 98,
      "date": "Tue, 28 Dec 2021 00:38:38 GMT"
    },
    "technique": "GitHub API"
  }
}