{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo borrows code from several repos, like [second.pytorch](https://github.com/traveller59/second.pytorch) and [F-PointNet](https://github.com/charlesq34/frustum-pointnets).\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.10187"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find our work useful in your research, please consider citing:\n```\n@article{Yang2020ssd,\n  author    = {Yang, Zetong and Sun, Yanan and Liu, Shu and Jia, Jiaya}\n  title     = {3DSSD: Point-based 3D Single Stage Object Detector},\n  journal   = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  year      = {2020},\n}\n```\n \nand / or\n\n```\n@inproceedings{Yang2019std,\n  author    = {Yang, Zetong and Sun, Yanan and Liu, Shu and Shen, Xiaoyong and Jia, Jiaya}\n  title     = {{STD:} Sparse-to-Dense 3D Object Detector for Point Cloud},\n  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}\n               2019, Seoul, Korea (South), October 27 - November 2, 2019},\n  year      = {2019},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Yang2019std,\n  author    = {Yang, Zetong and Sun, Yanan and Liu, Shu and Shen, Xiaoyong and Jia, Jiaya}\n  title     = {{STD:} Sparse-to-Dense 3D Object Detector for Point Cloud},\n  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}\n               2019, Seoul, Korea (South), October 27 - November 2, 2019},\n  year      = {2019},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Yang2020ssd,\n  author    = {Yang, Zetong and Sun, Yanan and Liu, Shu and Jia, Jiaya}\n  title     = {3DSSD: Point-based 3D Single Stage Object Detector},\n  journal   = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  year      = {2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "            <td>83.30</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "            <td>78.37</td> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dvlab-research/3DSSD",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you have any questions or suggestions about this repo, please feel free to contact me (tomztyang@gmail.com).\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-16T15:00:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T07:04:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Currently, there have been many kinds of voxel-based 3D single stage detectors, while point-based single stage methods are still underexplored. In this paper, we first present a lightweight and effective point-based 3D single stage object detector, named 3DSSD, achieving a good balance between accuracy and efficiency. In this paradigm, all upsampling layers and refinement stage, which are indispensable in all existing point-based methods, are abandoned to reduce the large computation cost. We novelly propose a fusion sampling strategy in downsampling process to make detection on less representative points feasible. A delicate box prediction network including a candidate generation layer, an anchor-free regression head with a 3D center-ness assignment strategy is designed to meet with our demand of accuracy and speed. Our paradigm is an elegant single stage anchor-free framework, showing great superiority to other existing methods. We evaluate 3DSSD on widely used KITTI dataset and more challenging nuScenes dataset. Our method outperforms all state-of-the-art voxel-based single stage methods by a large margin, and has comparable performance to two stage point-based methods as well, with inference speed more than 25 FPS, 2x faster than former state-of-the-art point-based methods.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9475109607726772
      ],
      "excerpt": "This is the official implementation of 3DSSD: Point-based 3D Single Stage Object Detector&nbsp; (CVPR 2020 Oral), a single-stage 3D point-based object detector. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9552659397039474
      ],
      "excerpt": " <p><font size=\"2\">3DSSD consists of three components. (a) Backbone network. It takes the raw point cloud as input, and generates global features for all representative points through several SA layers with fusion sampling (FS) strategy. (b) Candidate generation layer (CG). It downsamples, shifts and extracts features for representative points after SA layers. (c) Anchor-free prediction head.</font></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364517215200747
      ],
      "excerpt": "Some pretrained models including 3DSSD and PointRCNN are listed below with their performance, which are trained on 3,717 training samples and evaluated on 3,769 testing samples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9277312214835278
      ],
      "excerpt": "[ ] Visualization tools \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "3DSSD: Point-based 3D Single Stage Object Detector (CVPR 2020)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Jia-Research-Lab/3DSSD/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 64,
      "date": "Thu, 23 Dec 2021 12:10:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dvlab-research/3DSSD/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dvlab-research/3DSSD",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/Jia-Research-Lab/3DSSD/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Jia-Research-Lab/3DSSD/master/compile_all.sh",
      "https://raw.githubusercontent.com/Jia-Research-Lab/3DSSD/master/lib/utils/tf_ops/nms/build.sh",
      "https://raw.githubusercontent.com/Jia-Research-Lab/3DSSD/master/lib/builder/voxel_generator/build.sh",
      "https://raw.githubusercontent.com/Jia-Research-Lab/3DSSD/master/mayavi/mayavi_install.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Currently we only support KITTI dataset, and NuScenes dataset will be supported as soon as possible. \n\n(1) Please download the KITTI dataset and arrange it as below (please also download road planes at [here](https://drive.google.com/file/d/1d5mq0RXRnvHPVeKx6Q612z0YRO1t2wAp/view), which are useful in data augmentation).\n\n```\n.                                           (root directory)\n|-- lib                                     (3DSSD library file)\n|-- configs                                 (model configurations folder)\n|-- ...\n|-- dataset\n    |-- KITTI                               (dataset directory)\n    |   |-- object\n    |       |-- train.txt                   (KITTI train images list (3712 samples))                              \n    |       |-- val.txt                     (KITTI val images list (3769 samples))\n    |       |-- test.txt                    (KITTI test images list (7518 samples))\n    |       |-- trainval.txt                (KITTI images list (7481 samples))\n    |       |-- training\n    |       |   |-- calib\n    |       |   |-- image_2\n    |       |   |-- label_2\n    |       |   |-- planes\n    |       |   |-- velodyne\n    |       |-- testing\n    |-- NuScenes\n    |   |-- ...\n    |-- ...\n```\n\n(2) Choose the target configure file in **configs** directory (i.e., configs/kitti/3dssd/3dssd.yaml) and preprocess original dataset by\n\n\n```\npython lib/core/data_preprocessor.py --cfg configs/kitti/3dssd/3dssd.yaml --split training --img_list train\n\npython lib/core/data_preprocessor.py --cfg configs/kitti/3dssd/3dssd.yaml --split training --img_list val\n\npython lib/core/data_preprocessor.py --cfg configs/kitti/3dssd/3dssd.yaml --split testing --img_list test\n```\n\nThe preprocessed data will be saved in directory **data/KITTI**. You can also modify the dataset path and saving path in configure file:\n\n```\nDATASET:\n  TYPE: 'DATASET_NAME'           \n  ...\n  KITTI:\n    CLS_LIST: ('Car', )\n    ...\n    BASE_DIR_PATH: 'PATH/TO/DATASET'\n    TRAIN_LIST: 'PATH/TO/DATASET/train.txt'\n    VAL_LIST: 'PATH/TO/DATASET/val.txt'\n    ...\n    SAVE_NUMPY_PATH: 'PATH/TO/SAVE_PATH'\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "(1) Clone this repository.\n```\ngit clone https://github.com/tomztyang/3DSSD\ncd 3DSSD\n```\n\n(2) Setup Python environment.\n```\nconda create -n 3dssd python=3.6\nsource activate 3dssd\npip install -r requirements.txt\n```\n\nDownload and install tensorflow-1.4.0 [here](https://drive.google.com/file/d/142fwmiq8skVUcEqxXny9zA4bNG7YULGn/view?usp=sharing) which is compiled with CUDA-9.0 and CuDNN-7.0.0.\n```\npip install tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\n```\n\n(3) Compile and install 3DSSD library. Note that only GCC no later than version 5.0 can compile CUDA-9.0 code, make sure you install gcc-5. \n```\nbash compile_all.sh /path/to/tensorflow /path/to/cuda\n```\n\n(4) Add **3DSSD/lib** to PYTHONPATH.\n\n```\nexport PYTHONPATH=$PYTHONPATH:/path/to/3DSSD/lib:/path/to/3DSSD/mayavi\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8637738812225821
      ],
      "excerpt": "<p align=\"center\"> <img src=\"docs/demo.gif\" width=\"90%\"></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8448618155613022,
        0.8589534893990137
      ],
      "excerpt": "To train a model with multiple GPU, you have to set the available GPU number and batch size for each GPU in configure files. \nTRAIN:           \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117007451991057
      ],
      "excerpt": "Then run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8520031928910748
      ],
      "excerpt": "In order to test a model and store its prediction results, run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8398776326968947
      ],
      "excerpt": "[x] Reduce training GPU memory usage \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dvlab-research/3DSSD/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "C",
      "Makefile",
      "CMake",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Zetong Yang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "3DSSD",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "3DSSD",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dvlab-research",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dvlab-research/3DSSD/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All the codes are tested in the following environment:\n* Ubuntu 16.04\n* Python 3.6\n* tensorflow 1.4.0\n* CUDA 9.0 & CuDNN 7.0.0\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 303,
      "date": "Thu, 23 Dec 2021 12:10:41 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "3d-detection"
    ],
    "technique": "GitHub API"
  }
}