{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We are thankful to the [GDE program](https://developers.google.com/programs/experts/) for providing us GCP credits.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1612.00593.\n\n## Acknowledgements\n\nWe are thankful to the [GDE program](https://developers.google.com/programs/experts/"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation; Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas;\nCVPR 2017; https://arxiv.org/abs/1612.00593.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8150733245482739
      ],
      "excerpt": "<img src=\"https://github.com/soumik12345/point-cloud-segmentation/workflows/tests/badge.svg\" alt=\"build-failing\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8940389409941903
      ],
      "excerpt": "you should only use TPUs for the following object categories: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/soumik12345/point-cloud-segmentation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-30T14:38:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T20:27:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9923376981948051,
        0.8584263464548425
      ],
      "excerpt": "This repository provides a TF2 implementation of PointNet<sup>1</sup> for segmenting point clouds. Our implementation is fully supported on \nTPUs allowing you to train models faster. Distributed training (single-device multi-worker) on GPUs is also supported and so is single-GPU \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.932576834378128
      ],
      "excerpt": "To get an understanding of PointNet for segmentation, follow this blog post from keras.io: Point cloud segmentation with PointNet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9498963044821467
      ],
      "excerpt": "Update November 16, 2021: We won the #TFCommunitySpolight award for this project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741860791331839
      ],
      "excerpt": "training using TPUs is usually recommended when you have sufficient amount of data. Therefore,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.971795372406521
      ],
      "excerpt": "As such we only provide results and models for these categories.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8332131976262305
      ],
      "excerpt": "We also provide notebooks for training and testing the models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9679640230294881
      ],
      "excerpt": "We track our training results using Weights and Biases (WandB). For the hyperparameter \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "TF2 implementation of PointNet for segmenting point clouds",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/soumik12345/point-cloud-segmentation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Fri, 24 Dec 2021 10:38:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "soumik12345/point-cloud-segmentation",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/soumik12345/point-cloud-segmentation/main/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/soumik12345/point-cloud-segmentation/main/notebooks/train_gpu.ipynb",
      "https://raw.githubusercontent.com/soumik12345/point-cloud-segmentation/main/notebooks/train_tpu.ipynb",
      "https://raw.githubusercontent.com/soumik12345/point-cloud-segmentation/main/notebooks/keras-tuner.ipynb",
      "https://raw.githubusercontent.com/soumik12345/point-cloud-segmentation/main/notebooks/run_inference.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* The `batch_size` in the configuration denotes local batch size. If you are using single-host multi-worker distributed training,\nthe `batch_size` denoted here will be multiplied by the number of workers you have. \n* Using a Google Cloud Storage (GCS) based `artifact_location` is not a requirement if you are using GPU(s). But for \nTPUs, it's a requirement. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8934309321212723
      ],
      "excerpt": "This part is only required if you would like to train models using TPUs. Be advised that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8261861876778845
      ],
      "excerpt": "you should only use TPUs for the following object categories: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8169921814855521
      ],
      "excerpt": "notebooks/train_gpu.ipynb lets you train using GPU(s). If you are using multiple GPUs in the single machine it will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850164110436357
      ],
      "excerpt": "notebooks/run_inference.ipynb lets you test the models on GPU(s) on individual object categories. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9459306480564178,
        0.9059990302124403
      ],
      "excerpt": "Usage: create_tfrecords.py [OPTIONS] \n  python create_tfrecords.py --experiment_configs configs/shapenetcore.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9459306480564178
      ],
      "excerpt": "Usage: train_shapenet_core.py [OPTIONS] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8119341489430466
      ],
      "excerpt": "  --wandb_project_name    Project Name (DEFAULT: pointnet_shapenet_core) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9059990302124403
      ],
      "excerpt": "  python train_shapenet_core.py --experiment_configs configs/shapenetcore.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8094073202072359
      ],
      "excerpt": "| <h3>Object Category</h3> | <h3>Training Result</h3> | <h3>Final Model</h3> | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Soumik Rakshit\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Point Cloud Segmentation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "point-cloud-segmentation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "soumik12345",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/soumik12345/point-cloud-segmentation/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "sayakpaul",
        "body": "",
        "dateCreated": "2021-10-31T12:25:44Z",
        "datePublished": "2021-11-01T08:33:19Z",
        "html_url": "https://github.com/soumik12345/point-cloud-segmentation/releases/tag/v0.3",
        "name": "Pre-trained models",
        "tag_name": "v0.3",
        "tarball_url": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/tarball/v0.3",
        "url": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/releases/52431375",
        "zipball_url": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/zipball/v0.3"
      },
      {
        "authorType": "User",
        "author_name": "soumik12345",
        "body": "Pre-release for hosting ShapeShapeNetCore TFRecord files along with all original part annotations and an additional metadata file for easy data parsing. The point clouds and corresponding label clouds were sampled at 1024 points per point cloud.\r\n\r\n> ShapeNet is an ongoing effort to establish a richly annotated, large-scale dataset of 3D shapes. We provide researchers around the world with this data to enable research in computer graphics, computer vision, robotics, and other related disciplines. ShapeNet is a collaborative effort between researchers at Princeton, Stanford, and TTIC.\r\n\r\nShapeNetCore is a subset of the full ShapeNet dataset with single clean 3D models and manually verified category and alignment annotations. It covers 55 common object categories with about 51,300 unique 3D models. The 12 object categories of [PASCAL 3D+](http://cvgl.stanford.edu/projects/pascal3d.html), a popular computer vision 3D benchmark dataset, are all covered by ShapeNetCore.\r\n\r\nRefer to [https://shapenet.org/](https://shapenet.org/) for more information on the Shapenet dataset.\r\n",
        "dateCreated": "2021-10-26T04:53:54Z",
        "datePublished": "2021-10-26T13:59:41Z",
        "html_url": "https://github.com/soumik12345/point-cloud-segmentation/releases/tag/v0.2",
        "name": "ShapenetCore TFRecords",
        "tag_name": "v0.2",
        "tarball_url": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/tarball/v0.2",
        "url": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/releases/52060797",
        "zipball_url": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/zipball/v0.2"
      },
      {
        "authorType": "User",
        "author_name": "soumik12345",
        "body": "Pre-release for hosting ShapeShapeNetCore Dataset along with all original part annotations and an additional metadata file for easy data parsing.\r\n\r\n> ShapeNet is an ongoing effort to establish a richly annotated, large-scale dataset of 3D shapes. We provide researchers around the world with this data to enable research in computer graphics, computer vision, robotics, and other related disciplines. ShapeNet is a collaborative effort between researchers at Princeton, Stanford, and TTIC.\r\n\r\nShapeNetCore is a subset of the full ShapeNet dataset with single clean 3D models and manually verified category and alignment annotations. It covers 55 common object categories with about 51,300 unique 3D models. The 12 object categories of [PASCAL 3D+](http://cvgl.stanford.edu/projects/pascal3d.html), a popular computer vision 3D benchmark dataset, are all covered by ShapeNetCore.\r\n\r\nRefer to [https://shapenet.org/](https://shapenet.org/) for more information on the Shapenet dataset.\r\n",
        "dateCreated": "2021-09-30T14:38:34Z",
        "datePublished": "2021-10-02T14:52:18Z",
        "html_url": "https://github.com/soumik12345/point-cloud-segmentation/releases/tag/v0.1",
        "name": "Shapenet Dataset",
        "tag_name": "v0.1",
        "tarball_url": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/tarball/v0.1",
        "url": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/releases/50679578",
        "zipball_url": "https://api.github.com/repos/soumik12345/point-cloud-segmentation/zipball/v0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Build image using `docker build -t point-cloud-image .`\n\n- Run Jupyter Server using `docker run -it --gpus all -p 8888:8888 -v $(pwd):/usr/src/point-cloud-segmentation point-cloud-image`\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 20,
      "date": "Fri, 24 Dec 2021 10:38:03 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "point-cloud",
      "segmentation",
      "tensorflow2",
      "keras",
      "tpu",
      "deep-learning"
    ],
    "technique": "GitHub API"
  }
}