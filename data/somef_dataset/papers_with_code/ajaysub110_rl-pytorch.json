{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- **Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor**, (2018) [[bib]](./bibtex.bib#L9-L15)  by *Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel and Sergey Levine*\n\n- **Proximal Policy Optimization Algorithms**, (2017) [[bib]](./bibtex.bib#L25-L31)  by *John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford and Oleg Klimov*\n\n- **Benchmarking Deep Reinforcement Learning for Continuous Control**, (2016) [[bib]](./bibtex.bib#L17-L23)  by *Yan Duan, Xi Chen, Rein Houthooft, John Schulman and Pieter Abbeel*\n\n- **Playing Atari with Deep Reinforcement Learning**, (2013) [[bib]](./bibtex.bib#L1-L7)  by *Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra and Martin A. Riedmiller*\n\n- **Using Confidence Bounds for Exploitation-Exploration Trade-offs**, (2002) [[bib]](./bibtex.bib#L42-L49)  by *Peter Auer*\n\n- **Eligibility Traces for Off-Policy Policy Evaluation**, (2000) [[bib]](./bibtex.bib#L60-L65)  by *Doina Precup, Richard S. Sutton and Satinder P. Singh*\n\n- **Policy Gradient Methods for Reinforcement Learning with Function Approximation**, (1999) [[bib]](./bibtex.bib#L67-L72)  by *Richard S. Sutton, David A. McAllester, Satinder P. Singh and Yishay Mansour*\n\n- **Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning**, (1992) [[bib]](./bibtex.bib#L33-L40)  by *Ronald J. Williams*\n\n- **Q-learning**, (1992) [[bib]](./bibtex.bib#L51-L58)  by *Chris Watkins and Peter Dayan*\n\n- **Deterministic Policy Gradient Algorithms**, (2014) [[bib]](../bibtex.bib#L74-L79)  by *David Silver, Guy Lever, Nicolas Manfred Otto Heess, Thomas Degris, Daan Wierstra and Martin A. Riedmiller*",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8772692606136239,
        0.8189540139458567
      ],
      "excerpt": "Vanilla Policy Gradient (Actor-Critic) [Results] \nProximal Policy Optimization (PPO) [Results] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajaysub110/RLin200Lines",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-20T17:40:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-20T07:29:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8909374356991845,
        0.8352355318554535
      ],
      "excerpt": "PyTorch implementations of Reinforcement Learning algorithms in less than 200 lines. \nDeep Reinforcement Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8205565849369291
      ],
      "excerpt": "Proximal Policy Optimization (PPO) [Results] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073334045027941
      ],
      "excerpt": "Classical MDP Control \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8470392850672136
      ],
      "excerpt": "Report on Classical MDP control algorithms \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch implementations of Reinforcement Learning algorithms in less than 200 lines",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajaysub110/rl-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 10:36:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ajaysub110/RLin200Lines/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ajaysub110/RLin200Lines",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ajaysub110/RLin200Lines/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "TeX"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "RL in 200 Lines",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "RLin200Lines",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ajaysub110",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajaysub110/RLin200Lines/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- PyTorch\n- Tensorboard\n- OpenAI Gym\n- Numpy\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Wed, 29 Dec 2021 10:36:10 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-reinforcement-learning",
      "reinforcement-learning-algorithms",
      "pytorch-implementations",
      "ppo",
      "soft-actor-critic",
      "dqn",
      "policy-gradient",
      "machine-learning",
      "reinforcement-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Clone the repository.\n- Run experiments on an algorithm by running either <name>.py or main.py within its directory.\n- Tensorboard of my experiments can be viewed by using the 'Result' links given above.\n\n",
      "technique": "Header extraction"
    }
  ]
}