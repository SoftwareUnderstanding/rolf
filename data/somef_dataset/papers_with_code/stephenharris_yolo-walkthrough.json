{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Most of this applies when you're running it locally or on some other cloud service, but details may vary slightly).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The process for training digit recongition is essentially identical to the above, but for changes in configuration process.\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.8056401260211428,
        0.8032488167390682
      ],
      "excerpt": "https://github.com/AlexeyAB/darknet - Implementation of Yolo used \nhttp://arxiv.org/abs/1506.02640 (YOLO paper, explains some of the configuration parameters) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9369211462220431
      ],
      "excerpt": "https://blog.francium.tech/custom-object-training-and-detection-with-yolov3-darknet-and-opencv-41542f2ff44e (blog explaining the entire process, uses labelling tool: https://github.com/tzutalin/labelImg) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/stephenharris/yolo-walkthrough",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-30T22:02:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-14T18:59:49Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9373062777592697
      ],
      "excerpt": "This repository details how to set up a convolutional neural network to automatically extract meter readings from photos of meters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295256908165859,
        0.8425811894155114
      ],
      "excerpt": "I then renamed the images of meters with digital displays by appending a d to the filename to ensure that my training, testing and validation set had similar proportion of digital to non-digital displays. \nFor each image there needs to be .txt file, with the same filename, which labels the data (in particular, the width, height and co-ordinates of the centre of each bounding box (all given as ratios of the image's height/width), and the class number of the object contained by the box). Each line in this file corresponds to an object in the corresponding image (for this step, it should just be the one line). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510344831014137
      ],
      "excerpt": "xhost +local:docker \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8532111617947126
      ],
      "excerpt": "This step involves splitting your data set into three groups: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9397696105126133,
        0.9920760549653399,
        0.9158806424303683
      ],
      "excerpt": "testing - A portion of the dataset used to evaluate different models and their parameters \nvalidation - The portion of the dataset reserved to give an indication of performance of your final model. \nThe following will create a training, testing and validation folders and move each image and its annotations into one of them according to the proportions set in the code (I've gone for a 60-20-20 split) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029136808773207
      ],
      "excerpt": "Create the data file (see cfg/spark-counter.data) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9134158886561079
      ],
      "excerpt": "Your meter images are already split into training, testing and validation. The next step is to apply your counter-detection model to generate images of counter regions, preserving that split. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132937652190707
      ],
      "excerpt": "{filename}-prediction.jpg - the meter image with a bounding box drawn around the predicted counter region (incase of multiple predictions, the one with the highest confidence score is shown) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8461227504064482,
        0.9259549377171508
      ],
      "excerpt": "Configuration follows as before, with the following changes: \nAdd flip=0 in your config file (cfg/spark-digits-yolov3-tiny.cfg) at the top above learning_rate. This is to prevent the software from flipping the image while performing data augmentation - this is clearly not suitable when training for digit recognition). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9205575655145383
      ],
      "excerpt": "Generate a new set of anchors for your new data set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8486643251923202
      ],
      "excerpt": "http://arxiv.org/abs/1506.02640 (YOLO paper, explains some of the configuration parameters) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Example walkthrough of training YOLO to identify classes of objects",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/stephenharris/yolo-walkthrough/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 20 Dec 2021 22:49:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/stephenharris/yolo-walkthrough/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "stephenharris/yolo-walkthrough",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/stephenharris/yolo-walkthrough/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/stephenharris/yolo-walkthrough/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/stephenharris/yolo-walkthrough/master/util/bounding-box-rato.ipynb",
      "https://raw.githubusercontent.com/stephenharris/yolo-walkthrough/master/util/data-aug-notes.ipynb",
      "https://raw.githubusercontent.com/stephenharris/yolo-walkthrough/master/AMR/Introduction.ipynb",
      "https://raw.githubusercontent.com/stephenharris/yolo-walkthrough/master/AMR/Results.ipynb",
      "https://raw.githubusercontent.com/stephenharris/yolo-walkthrough/master/AMR/Method.ipynb",
      "https://raw.githubusercontent.com/stephenharris/yolo-walkthrough/master/AMR/analysis.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/stephenharris/yolo-walkthrough/master/terraform/setup.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "See `terraform`. This creates:\n\n - EC2 instance (`p2.xlarge`, with CUDA installed)\n - 2 S3 buckets to store training data, initial weights and configs\n\nAdditionally for deploying our trained model\n\n - 1 S3 bucket to store images passed to the API, along with your model's predictions\n - Lambda which runs our inference\n - API Gateway which provides an API interface for the lambda\n\n(and all the necessary security groups / policies etc)\n\nThe `setup.sh` script (which should be automatically run, should install the necessary dependencies)\n\nTo test\n\n    ./darknet\n\nYou should get the output:\n\n    usage: ./darknet <function>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "For this you will need source images of meters. Once collected, I ran the following to give standardised names to the image files:\n\n    docker exec -it yolo_yolo_1 bash\n\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "On the local machine run\n\n    xhost +local:docker\n\n    docker-compose up --build\n\nThen, to test, on the running container\n\n    cd darknet\n    ./darknet\n\nYou should get the output:\n\n    usage: ./darknet <function>\n\nTo test the OpenCV/Display run\n\n    ./darknet imtest data/eagle.jpg\n\n(you should see eagles - but this will fail if you're running it on a headless EC2 instance.)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8397022560787323
      ],
      "excerpt": "find /absolute/path/to/testing/ -name \".jpg\" > /path/to/train.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8457805517875979
      ],
      "excerpt": "train  = /path/to/train.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8161993237941593
      ],
      "excerpt": "./darknet partial /path/to/cfg/spark-counter-yolov3-tiny.cfg /path/to/pretrained.weights pretrained.conv.11 11 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8541165738468425
      ],
      "excerpt": "Configuration follows as before, with the following changes: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9946078424784638
      ],
      "excerpt": "https://pjreddie.com/darknet/install/ (Installation guide for darknet) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8045921079025995
      ],
      "excerpt": "https://medium.com/@manivannan_data/how-to-train-yolov3-to-detect-custom-objects-ccbcafeb13d2 (updated version of below for yolov2) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8816263441001799
      ],
      "excerpt": "python util/rename-files.py /path/to/images/of/meters/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python labelImg.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9540562667822831
      ],
      "excerpt": "python util/split-train-test-data.py /path/to/images/of/meters/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8098057232782067,
        0.8715684849742674,
        0.8749055337402262,
        0.8686214875009791,
        0.8431261627843716
      ],
      "excerpt": "Create the names file (listing labels for objects to detect), see cfg/spark-counter.names \nCreate train.txt and text.txt, which list paths to your training and test images. \nfind /absolute/path/to/training/ -name \".jpg\" > /path/to/train.txt \nfind /absolute/path/to/testing/ -name \".jpg\" > /path/to/train.txt \nCreate the data file (see cfg/spark-counter.data) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8632529304359361,
        0.8054413111914455,
        0.8424449300751454
      ],
      "excerpt": "train  = /path/to/train.txt \nvalid  = /path/to/test.txt \nnames = cfg/counters.names \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359991881242311,
        0.801204049322105
      ],
      "excerpt": "Generate anchors for your training data \n./darknet detector calc_anchors /path/to/spark-counter.data -num_of_clusters 6 -width 416 -height 416 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837696578500303
      ],
      "excerpt": "python test_counter_detection.py /path/to/dataset/ /path/to/output/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8377010335568151,
        0.8627839380444282,
        0.8218091770724872
      ],
      "excerpt": "Set filters on line 128 and 172 to 45  \nCreate names file listing digits 0-9 (see cfg/spark-digits.names) \nSet classes to 10 in data file and update path to files listed (see cfg/spark-digits.data). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8945113896999972,
        0.8920438529000969
      ],
      "excerpt": "python test_digit_detection.py /path/to/dataset/ \nThis will apply your trained model (again you'll need to configure the parameters passed to YoloModel) and print out the image filename, the actual reading (according to the annotations) and the predicted value. The last line will print a number between 0 and 1, indicating the percentage of reading correctly extracted. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/stephenharris/yolo-walkthrough/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "HCL",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The Python Imaging Library (PIL) is\\n\\n    Copyright \\xc2\\xa9 1997-2011 by Secret Labs AB\\n    Copyright \\xc2\\xa9 1995-2011 by Fredrik Lundh\\n\\nPillow is the friendly PIL fork. It is\\n\\n    Copyright \\xc2\\xa9 2010-2020 by Alex Clark and contributors\\n\\nLike PIL, Pillow is licensed under the open source PIL Software License:\\n\\nBy obtaining, using, and/or copying this software and/or its associated\\ndocumentation, you agree that you have read, understood, and will comply\\nwith the following terms and conditions:\\n\\nPermission to use, copy, modify, and distribute this software and its\\nassociated documentation for any purpose and without fee is hereby granted,\\nprovided that the above copyright notice appears in all copies, and that\\nboth that copyright notice and this permission notice appear in supporting\\ndocumentation, and that the name of Secret Labs AB or the author not be\\nused in advertising or publicity pertaining to distribution of the software\\nwithout specific, written prior permission.\\n\\nSECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS\\nSOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS.\\nIN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR BE LIABLE FOR ANY SPECIAL,\\nINDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM\\nLOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE\\nOR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR\\nPERFORMANCE OF THIS SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Automatic Meter Reading",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "yolo-walkthrough",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "stephenharris",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/stephenharris/yolo-walkthrough/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "See `terraform`. This creates:\n\n - EC2 instance (`p2.xlarge`, with CUDA installed)\n - 2 S3 buckets to store training data, initial weights and configs\n\nAdditionally for deploying our trained model\n\n - 1 S3 bucket to store images passed to the API, along with your model's predictions\n - Lambda which runs our inference\n - API Gateway which provides an API interface for the lambda\n\n(and all the necessary security groups / policies etc)\n\nThe `setup.sh` script (which should be automatically run, should install the necessary dependencies)\n\nTo test\n\n    ./darknet\n\nYou should get the output:\n\n    usage: ./darknet <function>\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": " \nDownload some weights:\n\n    mkdir weights\n    wget -O weights/yolov3.weights https://pjreddie.com/media/files/yolov3.weights\n\nThen, in `/home/jovyan/darknet`\n\n    ./darknet detect cfg/yolov3.cfg ../weights/yolov3.weights data/dog.jpg\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    nohup darknet detector train /path/to/spark-counter.data /path/to/spark-counter-yolov3-tiny.cfg pretrained.conv.11 -dont_show -mjpeg_port 8090 -map > /path/to/darknet.counters.log &\n\n**Note** You will need to download some convolutional weights (e.g. for tiny yolo [yolov3-tiny.conv.11](https://drive.google.com/file/d/18v36esoXCh-PsOKwyP2GWrpYDptDY8Zf/view?usp=sharing)) - or create your own from pretrained weights, see step 6 above.\n\n\nYou can then check progress by \n\n    tail -10 /var/log/darknet.counters.log.\n    \nor \n\n    grep \"avg\" /var/log/darknet.counters.log\n\nIf you EC2 instance is public then you'll be able to view a graph of the training on port `8090`.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 20 Dec 2021 22:49:10 GMT"
    },
    "technique": "GitHub API"
  }
}