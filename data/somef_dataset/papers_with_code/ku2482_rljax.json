{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1802.09477",
      "https://arxiv.org/abs/1801.01290",
      "https://arxiv.org/abs/1812.05905",
      "https://arxiv.org/abs/1710.10044",
      "https://arxiv.org/abs/1806.06923",
      "https://arxiv.org/abs/1911.02140",
      "https://arxiv.org/abs/1910.07207",
      "https://arxiv.org/abs/1511.05952",
      "https://arxiv.org/abs/2003.07305",
      "https://arxiv.org/abs/1910.01741",
      "https://arxiv.org/abs/1907.00953",
      "https://arxiv.org/abs/2010.09163",
      "https://arxiv.org/abs/2005.04269",
      "https://arxiv.org/abs/1707.06347 (2017).\n\n[[2]](https://arxiv.org/abs/1509.02971) Lillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint https://arxiv.org/abs/1509.02971 (2015).\n\n[[3]](https://arxiv.org/abs/1802.09477) Fujimoto, Scott, Herke Van Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" arXiv preprint https://arxiv.org/abs/1802.09477 (2018).\n\n[[4]](https://arxiv.org/abs/1801.01290) Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint https://arxiv.org/abs/1801.01290 (2018).\n\n[[5]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint https://arxiv.org/abs/1812.05905 (2018).\n\n[[6]](https://www.nature.com/articles/nature14236?wm=book_wap_0005) Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" nature 518.7540 (2015): 529-533.\n\n[[7]](https://arxiv.org/abs/1710.10044) Dabney, Will, et al. \"Distributional reinforcement learning with quantile regression.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n\n[[8]](https://arxiv.org/abs/1806.06923) Dabney, Will, et al. \"Implicit quantile networks for distributional reinforcement learning.\" arXiv preprint. 2018.\n\n[[9]](https://arxiv.org/abs/1911.02140) Yang, Derek, et al. \"Fully Parameterized Quantile Function for Distributional Reinforcement Learning.\" Advances in Neural Information Processing Systems. 2019.\n\n[[10]](https://arxiv.org/abs/1910.07207) Christodoulou, Petros. \"Soft Actor-Critic for Discrete Action Settings.\" arXiv preprint https://arxiv.org/abs/1910.07207 (2019).\n\n[[11]](https://arxiv.org/abs/1511.05952) Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint https://arxiv.org/abs/1511.05952 (2015).\n\n[[12]](https://arxiv.org/abs/2003.07305) Kumar, Aviral, Abhishek Gupta, and Sergey Levine. \"Discor: Corrective feedback in reinforcement learning via distribution correction.\" arXiv preprint https://arxiv.org/abs/2003.07305 (2020).\n\n[[13]](https://arxiv.org/abs/1910.01741) Yarats, Denis, et al. \"Improving sample efficiency in model-free reinforcement learning from images.\" arXiv preprint https://arxiv.org/abs/1910.01741 (2019).\n\n[[14]](https://arxiv.org/abs/1907.00953) Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" arXiv preprint https://arxiv.org/abs/1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/1509.02971 (2015).\n\n[[3]](https://arxiv.org/abs/1802.09477) Fujimoto, Scott, Herke Van Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" arXiv preprint https://arxiv.org/abs/1802.09477 (2018).\n\n[[4]](https://arxiv.org/abs/1801.01290) Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint https://arxiv.org/abs/1801.01290 (2018).\n\n[[5]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint https://arxiv.org/abs/1812.05905 (2018).\n\n[[6]](https://www.nature.com/articles/nature14236?wm=book_wap_0005) Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" nature 518.7540 (2015): 529-533.\n\n[[7]](https://arxiv.org/abs/1710.10044) Dabney, Will, et al. \"Distributional reinforcement learning with quantile regression.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n\n[[8]](https://arxiv.org/abs/1806.06923) Dabney, Will, et al. \"Implicit quantile networks for distributional reinforcement learning.\" arXiv preprint. 2018.\n\n[[9]](https://arxiv.org/abs/1911.02140) Yang, Derek, et al. \"Fully Parameterized Quantile Function for Distributional Reinforcement Learning.\" Advances in Neural Information Processing Systems. 2019.\n\n[[10]](https://arxiv.org/abs/1910.07207) Christodoulou, Petros. \"Soft Actor-Critic for Discrete Action Settings.\" arXiv preprint https://arxiv.org/abs/1910.07207 (2019).\n\n[[11]](https://arxiv.org/abs/1511.05952) Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint https://arxiv.org/abs/1511.05952 (2015).\n\n[[12]](https://arxiv.org/abs/2003.07305) Kumar, Aviral, Abhishek Gupta, and Sergey Levine. \"Discor: Corrective feedback in reinforcement learning via distribution correction.\" arXiv preprint https://arxiv.org/abs/2003.07305 (2020).\n\n[[13]](https://arxiv.org/abs/1910.01741) Yarats, Denis, et al. \"Improving sample efficiency in model-free reinforcement learning from images.\" arXiv preprint https://arxiv.org/abs/1910.01741 (2019).\n\n[[14]](https://arxiv.org/abs/1907.00953) Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" arXiv preprint https://arxiv.org/abs/1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/1802.09477 (2018).\n\n[[4]](https://arxiv.org/abs/1801.01290) Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint https://arxiv.org/abs/1801.01290 (2018).\n\n[[5]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint https://arxiv.org/abs/1812.05905 (2018).\n\n[[6]](https://www.nature.com/articles/nature14236?wm=book_wap_0005) Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" nature 518.7540 (2015): 529-533.\n\n[[7]](https://arxiv.org/abs/1710.10044) Dabney, Will, et al. \"Distributional reinforcement learning with quantile regression.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n\n[[8]](https://arxiv.org/abs/1806.06923) Dabney, Will, et al. \"Implicit quantile networks for distributional reinforcement learning.\" arXiv preprint. 2018.\n\n[[9]](https://arxiv.org/abs/1911.02140) Yang, Derek, et al. \"Fully Parameterized Quantile Function for Distributional Reinforcement Learning.\" Advances in Neural Information Processing Systems. 2019.\n\n[[10]](https://arxiv.org/abs/1910.07207) Christodoulou, Petros. \"Soft Actor-Critic for Discrete Action Settings.\" arXiv preprint https://arxiv.org/abs/1910.07207 (2019).\n\n[[11]](https://arxiv.org/abs/1511.05952) Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint https://arxiv.org/abs/1511.05952 (2015).\n\n[[12]](https://arxiv.org/abs/2003.07305) Kumar, Aviral, Abhishek Gupta, and Sergey Levine. \"Discor: Corrective feedback in reinforcement learning via distribution correction.\" arXiv preprint https://arxiv.org/abs/2003.07305 (2020).\n\n[[13]](https://arxiv.org/abs/1910.01741) Yarats, Denis, et al. \"Improving sample efficiency in model-free reinforcement learning from images.\" arXiv preprint https://arxiv.org/abs/1910.01741 (2019).\n\n[[14]](https://arxiv.org/abs/1907.00953) Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" arXiv preprint https://arxiv.org/abs/1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/1801.01290 (2018).\n\n[[5]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint https://arxiv.org/abs/1812.05905 (2018).\n\n[[6]](https://www.nature.com/articles/nature14236?wm=book_wap_0005) Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" nature 518.7540 (2015): 529-533.\n\n[[7]](https://arxiv.org/abs/1710.10044) Dabney, Will, et al. \"Distributional reinforcement learning with quantile regression.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n\n[[8]](https://arxiv.org/abs/1806.06923) Dabney, Will, et al. \"Implicit quantile networks for distributional reinforcement learning.\" arXiv preprint. 2018.\n\n[[9]](https://arxiv.org/abs/1911.02140) Yang, Derek, et al. \"Fully Parameterized Quantile Function for Distributional Reinforcement Learning.\" Advances in Neural Information Processing Systems. 2019.\n\n[[10]](https://arxiv.org/abs/1910.07207) Christodoulou, Petros. \"Soft Actor-Critic for Discrete Action Settings.\" arXiv preprint https://arxiv.org/abs/1910.07207 (2019).\n\n[[11]](https://arxiv.org/abs/1511.05952) Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint https://arxiv.org/abs/1511.05952 (2015).\n\n[[12]](https://arxiv.org/abs/2003.07305) Kumar, Aviral, Abhishek Gupta, and Sergey Levine. \"Discor: Corrective feedback in reinforcement learning via distribution correction.\" arXiv preprint https://arxiv.org/abs/2003.07305 (2020).\n\n[[13]](https://arxiv.org/abs/1910.01741) Yarats, Denis, et al. \"Improving sample efficiency in model-free reinforcement learning from images.\" arXiv preprint https://arxiv.org/abs/1910.01741 (2019).\n\n[[14]](https://arxiv.org/abs/1907.00953) Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" arXiv preprint https://arxiv.org/abs/1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/1812.05905 (2018).\n\n[[6]](https://www.nature.com/articles/nature14236?wm=book_wap_0005) Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" nature 518.7540 (2015): 529-533.\n\n[[7]](https://arxiv.org/abs/1710.10044) Dabney, Will, et al. \"Distributional reinforcement learning with quantile regression.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n\n[[8]](https://arxiv.org/abs/1806.06923) Dabney, Will, et al. \"Implicit quantile networks for distributional reinforcement learning.\" arXiv preprint. 2018.\n\n[[9]](https://arxiv.org/abs/1911.02140) Yang, Derek, et al. \"Fully Parameterized Quantile Function for Distributional Reinforcement Learning.\" Advances in Neural Information Processing Systems. 2019.\n\n[[10]](https://arxiv.org/abs/1910.07207) Christodoulou, Petros. \"Soft Actor-Critic for Discrete Action Settings.\" arXiv preprint https://arxiv.org/abs/1910.07207 (2019).\n\n[[11]](https://arxiv.org/abs/1511.05952) Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint https://arxiv.org/abs/1511.05952 (2015).\n\n[[12]](https://arxiv.org/abs/2003.07305) Kumar, Aviral, Abhishek Gupta, and Sergey Levine. \"Discor: Corrective feedback in reinforcement learning via distribution correction.\" arXiv preprint https://arxiv.org/abs/2003.07305 (2020).\n\n[[13]](https://arxiv.org/abs/1910.01741) Yarats, Denis, et al. \"Improving sample efficiency in model-free reinforcement learning from images.\" arXiv preprint https://arxiv.org/abs/1910.01741 (2019).\n\n[[14]](https://arxiv.org/abs/1907.00953) Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" arXiv preprint https://arxiv.org/abs/1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/1910.07207 (2019).\n\n[[11]](https://arxiv.org/abs/1511.05952) Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint https://arxiv.org/abs/1511.05952 (2015).\n\n[[12]](https://arxiv.org/abs/2003.07305) Kumar, Aviral, Abhishek Gupta, and Sergey Levine. \"Discor: Corrective feedback in reinforcement learning via distribution correction.\" arXiv preprint https://arxiv.org/abs/2003.07305 (2020).\n\n[[13]](https://arxiv.org/abs/1910.01741) Yarats, Denis, et al. \"Improving sample efficiency in model-free reinforcement learning from images.\" arXiv preprint https://arxiv.org/abs/1910.01741 (2019).\n\n[[14]](https://arxiv.org/abs/1907.00953) Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" arXiv preprint https://arxiv.org/abs/1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/1511.05952 (2015).\n\n[[12]](https://arxiv.org/abs/2003.07305) Kumar, Aviral, Abhishek Gupta, and Sergey Levine. \"Discor: Corrective feedback in reinforcement learning via distribution correction.\" arXiv preprint https://arxiv.org/abs/2003.07305 (2020).\n\n[[13]](https://arxiv.org/abs/1910.01741) Yarats, Denis, et al. \"Improving sample efficiency in model-free reinforcement learning from images.\" arXiv preprint https://arxiv.org/abs/1910.01741 (2019).\n\n[[14]](https://arxiv.org/abs/1907.00953) Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" arXiv preprint https://arxiv.org/abs/1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/2003.07305 (2020).\n\n[[13]](https://arxiv.org/abs/1910.01741) Yarats, Denis, et al. \"Improving sample efficiency in model-free reinforcement learning from images.\" arXiv preprint https://arxiv.org/abs/1910.01741 (2019).\n\n[[14]](https://arxiv.org/abs/1907.00953) Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" arXiv preprint https://arxiv.org/abs/1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/1910.01741 (2019).\n\n[[14]](https://arxiv.org/abs/1907.00953) Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" arXiv preprint https://arxiv.org/abs/1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint https://arxiv.org/abs/2005.04269 (2020)",
      "https://arxiv.org/abs/2005.04269 (2020)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[[1]](https://arxiv.org/abs/1707.06347) Schulman, John, et al. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017).\n\n[[2]](https://arxiv.org/abs/1509.02971) Lillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint arXiv:1509.02971 (2015).\n\n[[3]](https://arxiv.org/abs/1802.09477) Fujimoto, Scott, Herke Van Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" arXiv preprint arXiv:1802.09477 (2018).\n\n[[4]](https://arxiv.org/abs/1801.01290) Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018).\n\n[[5]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).\n\n[[6]](https://www.nature.com/articles/nature14236?wm=book_wap_0005) Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" nature 518.7540 (2015): 529-533.\n\n[[7]](https://arxiv.org/abs/1710.10044) Dabney, Will, et al. \"Distributional reinforcement learning with quantile regression.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n\n[[8]](https://arxiv.org/abs/1806.06923) Dabney, Will, et al. \"Implicit quantile networks for distributional reinforcement learning.\" arXiv preprint. 2018.\n\n[[9]](https://arxiv.org/abs/1911.02140) Yang, Derek, et al. \"Fully Parameterized Quantile Function for Distributional Reinforcement Learning.\" Advances in Neural Information Processing Systems. 2019.\n\n[[10]](https://arxiv.org/abs/1910.07207) Christodoulou, Petros. \"Soft Actor-Critic for Discrete Action Settings.\" arXiv preprint arXiv:1910.07207 (2019).\n\n[[11]](https://arxiv.org/abs/1511.05952) Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint arXiv:1511.05952 (2015).\n\n[[12]](https://arxiv.org/abs/2003.07305) Kumar, Aviral, Abhishek Gupta, and Sergey Levine. \"Discor: Corrective feedback in reinforcement learning via distribution correction.\" arXiv preprint arXiv:2003.07305 (2020).\n\n[[13]](https://arxiv.org/abs/1910.01741) Yarats, Denis, et al. \"Improving sample efficiency in model-free reinforcement learning from images.\" arXiv preprint arXiv:1910.01741 (2019).\n\n[[14]](https://arxiv.org/abs/1907.00953) Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" arXiv preprint arXiv:1907.00953 (2019).\n\n[[15]](https://arxiv.org/abs/2010.09163) Sinha, Samarth, et al. \"D2RL: Deep Dense Architectures in Reinforcement Learning.\" arXiv preprint arXiv:2010.09163 (2020).\n\n[[16]](https://arxiv.org/abs/2005.04269) Kuznetsov, Arsenii, et al. \"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.\" arXiv preprint arXiv:2005.04269 (2020).",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| SAC+DisCor[12]| Continuous | :heavy_check_mark: | - | -                  | :heavy_check_mark: | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ku2482/rljax",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-24T15:34:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T18:20:06Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.969461250237736
      ],
      "excerpt": "Rljax is a collection of RL algorithms written in JAX. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A collection of RL algorithms written in JAX.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ku2482/rljax/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Thu, 23 Dec 2021 13:42:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ku2482/rljax/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ku2482/rljax",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can install dependencies simply by executing the following. To use GPUs, CUDA (10.0, 10.1, 10.2 or 11.0) must be installed.\n```bash\npip install https://storage.googleapis.com/jax-releases/`nvcc -V | sed -En \"s/.* release ([0-9]*)\\.([0-9]*),.*/cuda\\1\\2/p\"`/jaxlib-0.1.55-`python3 -V | sed -En \"s/Python ([0-9]*)\\.([0-9]*).*/cp\\1\\2/p\"`-none-manylinux2010_x86_64.whl jax==0.2.0\npip install -e .\n```\n\nIf you don't have a GPU, please execute the following instead.\n```bash\npip install jaxlib==0.1.55 jax==0.2.0\npip install -e .\n```\n\nIf you want to use a [MuJoCo](http://mujoco.org/) physics engine, please install [mujoco-py](https://github.com/openai/mujoco-py).\n```bash\npip install mujoco_py==2.0.2.11\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ku2482/rljax/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Toshiki Watanabe\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rljax",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rljax",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ku2482",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ku2482/rljax/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 47,
      "date": "Thu, 23 Dec 2021 13:42:59 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All algorithms can be trained in a few lines of code.\n\n<details>\n<summary>Getting started</summary>\n\nHere is a quick example of how to train DQN on `CartPole-v0`.\n\n```Python\nimport gym\n\nfrom rljax.algorithm import DQN\nfrom rljax.trainer import Trainer\n\nNUM_AGENT_STEPS = 20000\nSEED = 0\n\nenv = gym.make(\"CartPole-v0\")\nenv_test = gym.make(\"CartPole-v0\")\n\nalgo = DQN(\n    num_agent_steps=NUM_AGENT_STEPS,\n    state_space=env.observation_space,\n    action_space=env.action_space,\n    seed=SEED,\n    batch_size=256,\n    start_steps=1000,\n    update_interval=1,\n    update_interval_target=400,\n    eps_decay_steps=0,\n    loss_type=\"l2\",\n    lr=1e-3,\n)\n\ntrainer = Trainer(\n    env=env,\n    env_test=env_test,\n    algo=algo,\n    log_dir=\"/tmp/rljax/dqn\",\n    num_agent_steps=NUM_AGENT_STEPS,\n    eval_interval=1000,\n    seed=SEED,\n)\ntrainer.train()\n```\n\n</details>\n\n<details>\n<summary>MuJoCo(Gym)</summary>\n\nI benchmarked my implementations in some environments from MuJoCo's `-v3` task suite, following [Spinning Up's benchmarks](https://spinningup.openai.com/en/latest/spinningup/bench.html) ([code](https://github.com/ku2482/rljax/blob/master/examples/mujoco)). In TQC, I set num_quantiles_to_drop to 0 for HalfCheetath-v3 and 2 for other environments. Note that I benchmarked with 3M agent steps, not 5M agent steps as in TQC's paper.\n\n<img src=\"https://user-images.githubusercontent.com/37267851/97766058-2d89a700-1b58-11eb-9266-29c3605f7d6c.png\" title=\"HalfCheetah-v3\" width=400><img src=\"https://user-images.githubusercontent.com/37267851/97766061-2e223d80-1b58-11eb-94a0-44efb7e5d9b7.png\" title=\"Walker2d-v3\" width=400>\n<img src=\"https://user-images.githubusercontent.com/37267851/97766056-2c587a00-1b58-11eb-9844-d704657857f8.png\" title=\"Swimmer-v3\" width=400><img src=\"https://user-images.githubusercontent.com/37267851/97766062-2ebad400-1b58-11eb-8cf1-6d3bd338c414.png\" title=\"Ant-v3\" width=400>\n\n</details>\n\n<details>\n<summary>DeepMind Control Suite</summary>\n\nI benchmarked SAC+AE and SLAC implementations in some environments from DeepMind Control Suite ([code](https://github.com/ku2482/rljax/blob/master/examples/dm_control)). Note that the horizontal axis represents the environment step, which is obtained by multiplying agent_step by action_repeat. I set action_repeat to 4 for cheetah-run and 2 for walker-walk.\n\n<img src=\"https://user-images.githubusercontent.com/37267851/97359828-b7c7d600-18e0-11eb-8c79-852624dfa1e8.png\" title=\"cheetah-run\" width=400><img src=\"https://user-images.githubusercontent.com/37267851/97359825-b696a900-18e0-11eb-88e2-b532076de7e8.png\" title=\"walker-walk\" width=400>\n\n</details>\n\n<details>\n<summary>Atari(Arcade Learning Environment)</summary>\n\nI benchmarked SAC-Discrete implementation in `MsPacmanNoFrameskip-v4` from the Arcade Learning Environment(ALE) ([code](https://github.com/ku2482/rljax/blob/master/examples/atari)). Note that the horizontal axis represents the environment step, which is obtained by multiplying agent_step by 4.\n\n<img src=\"https://user-images.githubusercontent.com/37267851/97410160-0e193100-1942-11eb-8056-df445eb6f5e9.png\" title=\"MsPacmanNoFrameskip-v4\" width=400>\n\n</details>\n\n",
      "technique": "Header extraction"
    }
  ]
}