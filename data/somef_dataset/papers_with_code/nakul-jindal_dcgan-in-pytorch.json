{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* **Title**: UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS\n* **Authors**: Alec Radford, Luke Metz, Soumith Chintala\n* **Link**: https://arxiv.org/pdf/1511.06434.pdf\n* **Tags**: Neural Network, Generative Networks, GANs\n* **Year**: 2015\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "<td> DCGAN after 10 epochs </td> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nakul-jindal/dcgan-in-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-18T23:23:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-08T22:55:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Deep Convolution Generative Adversarial Networks (DCGANs) belong to a set of algorithms called generative models, which are widely used for unupervised learning tasks which aim to learn the underlying structure of the given data. \n\n* Simple GANs allow you to generate new unseen data that mimic the actual given real data. However, GANs pose problems in training and require carefullly tuned hyperparameters.\n\n* DCGAN aims to solve this problem by explicitly using convolutional and convolutional-transpose layers in the discriminator and generator, respectively. \n\n* DCGANs basically convert the laplacian pyramid technique (many pairs of G and D to progressively upscale an image) to a single pair of G and D.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9430290411926087,
        0.8515912868339199,
        0.9758704738718605,
        0.9717494308843808,
        0.8052911705410011
      ],
      "excerpt": "* The generator G is designed to map the latent space vector z (random noise) to data-space (images same as training images)  \n* involves a series of transpose Conv2d layers, each with BatchNorm2d and relu activation. \n* The output of the generator is fed through a tanh function to return it to the input data range of [-1,1]. \nThe discriminator D is a binary classification network that takes an image as input and outputs a scalar probability that the input image is real or fake.  \nD involves a series of Conv2d, BatchNorm2d, and LeakyReLU layers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.979502017195961,
        0.8876149742487113
      ],
      "excerpt": "The DCGAN paper mentions it is a good practice to use strided convolution rather than pooling to downsample because it lets the network learn its own pooling function. Also batch norm and leaky relu functions promote healthy gradient flow which is critical for the learning process of both G and D. \nReplace all pooling layers with strided convolutions for the downsampling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549909087517874
      ],
      "excerpt": "Hyperparameters are chosen as given in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643,
        0.9677802883508364
      ],
      "excerpt": "* slope of leak of LeakyReLU: 0.2 \n* For the optimizer Adam (with beta2 = 0.999) has been used instead of SGD as described in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.954489498094286
      ],
      "excerpt": "proposes and evaluates Deep Convolutional GANs (DCGAN) which are a set of constraints on the architectural topology of Convolutional \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8267242467587494,
        0.9568937105131488,
        0.971415965341642,
        0.9840868599732396
      ],
      "excerpt": "use of trained discriminators for image classification tasks, showing competitive performance with other unsupervised algorithms. \nvisualize the filters learnt by GANs and empirically show that specific filters have learned to draw specific objects. \nshow that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples. \nThis paper shows how convolutional layers can be used with GANs and provides a series of additional architectural guidelines for doing this. The paper also discusses topics such as Visualizing GAN features, Latent space interpolation, using discriminator features to train classifiers, and evaluating results. The paper contains many examples of images generated by final and intermediate layers of the network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465691412615707
      ],
      "excerpt": "DCGAN can learn an interesting hierarchy of features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "dcgan pytorch implementation MNIST dataset",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nakul-jindal/dcgan-in-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 04:57:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nakul-jindal/dcgan-in-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "nakul-jindal/dcgan-in-pytorch",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.9001373101481486,
        0.9001373101481486
      ],
      "excerpt": "<td><img src = 'images/raw_MNIST.png'> \n<td><img src = 'images/MNIST_DCGAN_10.png'> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nakul-jindal/dcgan-in-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DCGAN implementation in pytorch on MNIST",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "dcgan-in-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "nakul-jindal",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nakul-jindal/dcgan-in-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 04:57:30 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nusage: main.py [-h] [--num_epochs NUM_EPOCHS]\n               [--batch_size  BATCH_SIZE] \n               [--channels_noise CHANNELS_NOISE] \n               [--lr_g LR_G][--lr_d LR_D]\n               [--beta1 BETA1]\n\noptional arguments:\n  -h, --help                           show this help message and exit\n  --num_epochs NUM_EPOCHS              no. of epochs : default=10\n  --batch_size  BATCH_SIZE             batch size : default=128\n  --channels_noise CHANNELS_NOISE      size of noise vector : default=100\n  --lr_g LR_G                          learning rate generator : default=0.0002\n  --lr_d LR_D                          learning rate discriminator : default=0.0002\n  --beta1 BETA1                        bet1 value for adam optimizer\n                        \n  ```\n\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ python3 main.py --num_epochs 10\n```\n> **_NOTE:_** on Colab Notebook use following command:\n```python\n!git clone link-to-repo\n%run main.py --num_epochs 10\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}