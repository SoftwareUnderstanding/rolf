{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Code is inspired by [pytorch-DCGAN](https://github.com/pytorch/examples/tree/master/dcgan).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.10593"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite our papers.\n```\n@article{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  journal={arXiv preprint arXiv:1703.10593},\n  year={2017}\n}\n\n@article{pix2pix2016,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  journal={arxiv},\n  year={2016}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{pix2pix2016,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  journal={arxiv},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  journal={arXiv preprint arXiv:1703.10593},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8497496570404576
      ],
      "excerpt": "The code was written by Jun-Yan Zhu and Taesung Park. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997516492998463
      ],
      "excerpt": "If you use this code for your research, please cite: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9878673543585983,
        0.9817276665716111
      ],
      "excerpt": "Jun-Yan Zhu*,  Taesung Park*, Phillip Isola, Alexei A. Efros \nIn arxiv, 2017. (* equal contributions)   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9878673543585983,
        0.97474157841955,
        0.8227893355900447
      ],
      "excerpt": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros  \nIn CVPR 2017. \n<p><a href=\"https://github.com/leehomyc/cyclegan-1\"> [Tensorflow]</a> (by Harry Yang), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227893355900447,
        0.9432788912470228
      ],
      "excerpt": "<a href=\"https://github.com/kaonashi-tyc/zi2zi\">[Tensorflow (zi2zi)]</a> (by Yuchen Tian), \n<a href=\"https://github.com/pfnet-research/chainer-pix2pix\">[Chainer]</a> (by mattya), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9952474852238322
      ],
      "excerpt": "If you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper Collection: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lukeandshuo/IR2VI_journal",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-20T02:42:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-07T02:46:42Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9793143942208359
      ],
      "excerpt": "This is our ongoing PyTorch implementation for both unpaired and paired image-to-image translation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8099033291246154
      ],
      "excerpt": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8552078246538597
      ],
      "excerpt": "Image-to-Image Translation with Conditional Adversarial Networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8260642699300744
      ],
      "excerpt": "- To view training results and loss plots, run `python -m visdom.server` and click the URL http://localhost:8097. To see more intermediate results, check out `./checkpoints/maps_cyclegan/web/index.html` \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8260642699300744
      ],
      "excerpt": "- To view training results and loss plots, run `python -m visdom.server` and click the URL http://localhost:8097. To see more intermediate results, check out  `./checkpoints/facades_pix2pix/web/index.html` \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828710842013515
      ],
      "excerpt": "If you would like to apply a pre-trained model to a collection of input photos (without image pairs), please use --dataset_mode single and --model test options. Here is a script to apply a model to Facade label maps (stored in the directory facades/testB). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9811057153305932
      ],
      "excerpt": "Note: We currently don't have all pretrained models using PyTorch. This is in part because the models trained using Torch and PyTorch produce slightly different results, although we were not able to decide which result is better. If you would like to generate the same results that appeared in our paper, we recommend using the pretrained models in the Torch codebase. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.943834925937405
      ],
      "excerpt": "Note that we specified --which_direction BtoA to accomodate the fact that the Facades dataset's A to B direction is photos to labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9294116103022102,
        0.9329852525954802
      ],
      "excerpt": "Visualization: during training, the current results can be viewed using two methods. First, if you set --display_id > 0, the results and loss plot will appear on a local graphics web server launched by visdom. To do this, you should have visdom installed and a server running by the command python -m visdom.server. The default server URL is http://localhost:8097. display_id corresponds to the window ID that is displayed on the visdom server. The visdom display functionality is turned on by default. To avoid the extra overhead of communicating with visdom set --display_id 0. Second, the intermediate results are saved to [opt.checkpoints_dir]/[opt.name]/web/ as an HTML file. To avoid this, set --no_html. \nPreprocessing: images can be resized and cropped in different ways using --resize_or_crop option. The default option 'resize_and_crop' resizes the image to be of size (opt.loadSize, opt.loadSize) and does a random crop of size (opt.fineSize, opt.fineSize). 'crop' skips the resizing step and only performs random cropping. 'scale_width' resizes the image to have width opt.fineSize while keeping the aspect ratio. 'scale_width_and_crop' first resizes the image to have width opt.loadSize and then does random cropping of size (opt.fineSize, opt.fineSize). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833797416181476,
        0.8172075594923899,
        0.9599855590514081
      ],
      "excerpt": "- summer2winter_yosemite: 1273 summer Yosemite images and 854 winter Yosemite images were downloaded using Flickr API. See more details in our paper. \n- monet2photo, vangogh2photo, ukiyoe2photo, cezanne2photo: The art images were downloaded from Wikiart. The real photos are downloaded from Flickr using the combination of the tags landscape and landscapephotography. The training set size of each class is Monet:1074, Cezanne:584, Van Gogh:401, Ukiyo-e:1433, Photographs:6853. \n- iphone2dslr_flower: both classes of images were downlaoded from Flickr. The training set size of each class is iPhone:1813, DSLR:3316. See more details in our paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9573305914890285
      ],
      "excerpt": "We provide a python script to generate pix2pix training data in the form of pairs of images {A,B}, where A and B are two different depictions of the same underlying scene. For example, these might be pairs {label map, photo} or {bw image, color image}. Then we can learn to translate A to B or B to A: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807756918396975
      ],
      "excerpt": "Once the data is formatted this way, call: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8099033291246154,
        0.8290512791172416
      ],
      "excerpt": "CycleGAN: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks \npix2pix: Image-to-image translation with conditional adversarial nets \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lukeandshuo/IR2VI_journal/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 04:38:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lukeandshuo/IR2VI_journal/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lukeandshuo/IR2VI_journal",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lukeandshuo/IR2VI_journal/master/scripts/test_cyclegan.sh",
      "https://raw.githubusercontent.com/lukeandshuo/IR2VI_journal/master/scripts/test_pix2pix.sh",
      "https://raw.githubusercontent.com/lukeandshuo/IR2VI_journal/master/scripts/test_single.sh",
      "https://raw.githubusercontent.com/lukeandshuo/IR2VI_journal/master/scripts/train_cyclegan.sh",
      "https://raw.githubusercontent.com/lukeandshuo/IR2VI_journal/master/scripts/train_pix2pix.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install PyTorch and dependencies from http://pytorch.org\n- Install Torch vision from the source.\n```bash\ngit clone https://github.com/pytorch/vision\ncd vision\npython setup.py install\n```\n- Install python libraries [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate).\n```bash\npip install visdom\npip install dominate\n```\n- Clone this repo:\n```bash\ngit clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\ncd pytorch-CycleGAN-and-pix2pix\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8547506928230144,
        0.9259408091992843
      ],
      "excerpt": "<a href=\"https://github.com/yunjey/mnist-svhn-transfer\">[Minimal PyTorch]</a> (by yunjey), \n<a href=\"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN\">[Mxnet]</a> (by Ldpe2G), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.946109459978882
      ],
      "excerpt": "<a href=\"https://github.com/taey16/pix2pixBEGAN.pytorch\">[Pytorch]</a> (by taey16) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8554479836060276
      ],
      "excerpt": "- Test the model (`bash ./scripts/test_pix2pix.sh`):bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8608750546969411
      ],
      "excerpt": "More example scripts can be found at scripts directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "``` bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8816116861990372,
        0.9465718491881494
      ],
      "excerpt": "You can download a few pretrained models from the authors. For example, if you would like to download horse2zebra model,  \nbash pretrained_models/download_cyclegan_model.sh horse2zebra \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8499854636927169,
        0.9465718491881494
      ],
      "excerpt": "Download the pre-trained models using ./pretrained_models/download_pix2pix_model.sh. For example, if you would like to download label2photo model on the Facades dataset, \nbash pretrained_models/download_pix2pix_model.sh facades_label2photo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481878710019339
      ],
      "excerpt": "Also, the models that are currently available to download can be found by reading the output of bash pretrained_models/download_pix2pix_model.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102530092827479
      ],
      "excerpt": "CPU/GPU (default --gpu_ids 0): set--gpu_ids -1 to use CPU mode; set --gpu_ids 0,1,2 for multi-GPU mode. You need a large batch size (e.g. --batchSize 32) to benefit from multiple GPUs.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917133644880719
      ],
      "excerpt": "bash ./datasets/download_cyclegan_dataset.sh dataset_name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917133644880719
      ],
      "excerpt": "bash ./datasets/download_pix2pix_dataset.sh dataset_name \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8260262293211895
      ],
      "excerpt": "<img src=\"https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg\" width=\"900\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9154330685897901
      ],
      "excerpt": "<img src='imgs/edges2cats.jpg' width=\"600px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737873112808705
      ],
      "excerpt": "Train a model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9373628356670933
      ],
      "excerpt": "python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --no_dropout \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8574618856000782
      ],
      "excerpt": "- Test the model:bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9387946937211802
      ],
      "excerpt": "python test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --phase test --no_dropout \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8094468517814178,
        0.8640668950388781
      ],
      "excerpt": "The test results will be saved to a html file here:./results/maps_cyclegan/latest_test/index.html`. \nDownload a pix2pix dataset (e.g.facades): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737873112808705
      ],
      "excerpt": "Train a model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9661124720133236
      ],
      "excerpt": "python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --which_model_netG unet_256 --which_direction BtoA --lambda_A 100 --dataset_mode aligned --no_lsgan --norm batch --pool_size 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.96210143898156
      ],
      "excerpt": "python test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --which_model_netG unet_256 --which_direction BtoA --dataset_mode aligned --norm batch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8094468517814178
      ],
      "excerpt": "The test results will be saved to a html file here:./results/facades_pix2pix/latest_val/index.html`. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9508760236814469
      ],
      "excerpt": "python test.py --dataroot ./datasets/facades/testA/ --name {my_trained_model_name} --model test --dataset_mode single \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833339116157716
      ],
      "excerpt": "The pretrained model is saved at ./checkpoints/{name}_pretrained/latest_net_G.pth.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9296928648806009
      ],
      "excerpt": "python test.py --dataroot datasets/horse2zebra/testA --checkpoints_dir ./checkpoints/ --name horse2zebra_pretrained --no_dropout --model test --dataset_mode single --loadSize 256 --results_dir {directory_path_to_save_result} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8373508929508664
      ],
      "excerpt": "Download the pre-trained models using ./pretrained_models/download_pix2pix_model.sh. For example, if you would like to download label2photo model on the Facades dataset, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.96210143898156
      ],
      "excerpt": "python test.py --dataroot ./datasets/facades/ --which_direction BtoA --model pix2pix --name facades_label2photo_pretrained --dataset_mode aligned --which_model_netG unet_256 --norm batch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8988969803425948
      ],
      "excerpt": "Flags: see options/train_options.py and options/base_options.py for all the training flags; see options/test_options.py and options/base_options.py for all the test flags. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8528240424939154
      ],
      "excerpt": "To train a model on your own datasets, you need to create a data folder with two subdirectories trainA and trainB that contain images from domain A and B. You can test your model on your training set by setting --phase train in test.py. You can also create subdirectories testA and testB if you have test data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8875489604619244,
        0.8967576975011664
      ],
      "excerpt": "Create folder /path/to/data with subfolders A and B. A and B should each have their own subfolders train, val, test, etc. In /path/to/data/A/train, put training images in style A. In /path/to/data/B/train, put the corresponding images in style B. Repeat same for other data splits (val, test, etc). \nCorresponding images in a pair {A,B} must be the same size and have the same filename, e.g., /path/to/data/A/train/1.jpg is considered to correspond to /path/to/data/B/train/1.jpg. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8419365417525021
      ],
      "excerpt": "python datasets/combine_A_and_B.py --fold_A /path/to/data/A --fold_B /path/to/data/B --fold_AB /path/to/data \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lukeandshuo/IR2VI_journal/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/lukeandshuo/IR2VI_journal/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2017, Jun-Yan Zhu and Taesung Park\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\n\\n--------------------------- LICENSE FOR pix2pix --------------------------------\\nBSD License\\n\\nFor pix2pix software\\nCopyright (c) 2016, Phillip Isola and Jun-Yan Zhu\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n----------------------------- LICENSE FOR DCGAN --------------------------------\\nBSD License\\n\\nFor dcgan.torch software\\n\\nCopyright (c) 2015, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n\\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n\\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n\\nNeither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CycleGAN and pix2pix in PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "IR2VI_journal",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lukeandshuo",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lukeandshuo/IR2VI_journal/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Linux or macOS\n- Python 2 or 3\n- CPU or NVIDIA GPU + CUDA CuDNN\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 04:38:05 GMT"
    },
    "technique": "GitHub API"
  }
}