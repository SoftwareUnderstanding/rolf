{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1708.02182",
      "https://arxiv.org/abs/1712.05877"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Janus-Shiau/awd-lstm-tensorflow",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Code work by Jia-Yau Shiau <jiayau.shiau@gmail.com>.\\\nQuantization code work is advised and forked from Peter Huang <peter124574@gmail.com>\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-24T14:04:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-13T07:00:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9199963457963034,
        0.9139838612812551,
        0.9556259212579163,
        0.8620035712977688
      ],
      "excerpt": "AWD-LSTM from (\"Regularizing and Optimizing LSTM Language Models\") for tensorflow. \nTraining-award quantization for integer-arithmetic-only inference (\"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\") is also provided. \nThis code is implemmented and tested with tensorflow 1.11.0. and 1.13.0. \nThe main idea of AWD-LSTM is the drop-connect weights and concatinated inputs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.913181629730322
      ],
      "excerpt": "I have conduct experiments on a many-to-many recursive task this implementation and carry out a better results than simple LSTMCell. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.966919310918162,
        0.9616391332639576
      ],
      "excerpt": "I also provided a tensorflow implementation of variational dropout, which is more flexible than DropoutWrapper in tensorflow. \nThe usage is similar to using WeightDropLSTMCell: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "with tf.control_dependencies(vd.get_update_mask_op()): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549691390780828,
        0.979330069924774
      ],
      "excerpt": "Provide the regulization utilities mentioned in the paper. \nMaybe there is some more elegant way to implement variational dropout. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "AWD-LSTM from \"Regularizing and Optimizing LSTM Language Models\" with training-award quantization support for tensorflow.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Janus-Shiau/awd-lstm-tensorflow/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 00:44:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Janus-Shiau/awd-lstm-tensorflow/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Janus-Shiau/awd-lstm-tensorflow",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src=\"doc/vd2.png\"  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src=\"doc/vd1.png\"  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555262106879538
      ],
      "excerpt": "    is_quant=True, is_train=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from variational_dropout import VariationalDropout \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Janus-Shiau/awd-lstm-tensorflow/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "AWD-LSTM (Weight Drop LSTM) with training-award quantization in Tensorflow",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "awd-lstm-tensorflow",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Janus-Shiau",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Janus-Shiau/awd-lstm-tensorflow/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Fri, 24 Dec 2021 00:44:15 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "language-model",
      "recurrent-neural-networks",
      "tensorflow"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Simply initial AWD-LSTM, it's a standard [`LayerRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerRNNCell).\n```\nfrom weight_drop_lstm import WeightDropLSTMCell\n\nlstm_cell = WeightDropLSTMCell(\n    num_units=CELL_NUM, weight_drop_kr=WEIGHT_DP_KR, \n    use_vd=True, input_size=INPUT_SIZE)\n```\nArguments are define as follows:\n> `num_units`: the number of cell in LSTM layer. [ints]\\\n> `weight_drop_kr`: the number of steps that fast weights go forward. [int]\\\n> `use_vd`: If true, using variational dropout on weight drop-connect, standard dropout otherwise. [bool]\\\n> `input_size`: If `use_vd=True`, input_size (dimension of last channel) should be provided. [int]\n\nThe remaining keyword arguments is exactly the same as [`tf.nn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell). \n\nNoted that, if the weight_drop_kr is not provided or provided with 1.0, `WeightDropLSTMCell` is reducted as `LSTMCell`.\n\n2. Insert update operation of dropout kernel to the place you want.\n\n```\n#: By simply sess.run in each training step\nsess.run(lstm_cell.get_vd_update_op())\n\n#: Or use control_dependencies\nvd_update_ops = lstm_cell.get_vd_update_op() \nwith tf.control_dependencies(vd_update_ops):\n    tf.train.AdamOptimizer(learning_rate).minimize(loss)\n```\n\nYou can also add `get_vd_update_op()` to [`GraphKeys.UPDATE_OPS`](https://www.tensorflow.org/api_docs/python/tf/GraphKeys) when calling `WeightDropLSTMCell`.\n\nNoted that, if you use [`control_dependencies`](https://www.tensorflow.org/api_docs/python/tf/control_dependencies), please be careful for the order of execution.\\\nThe variational dropout kernel should not be update before the optimizer step.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}