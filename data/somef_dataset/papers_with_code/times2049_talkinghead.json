{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1905.08233\n\n![Fake1](https://github.com/vincent-thevenin/Realistic-Neural-Talking-Head-Models/blob/master/examples/1%201.png \"Fake 1\""
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9881895859982757
      ],
      "excerpt": "My implementation of Few-Shot Adversarial Learning of Realistic Neural Talking Head Models (Egor Zakharov et al.). https://arxiv.org/abs/1905.08233 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/times2049/talkinghead",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-17T06:55:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-12T12:23:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9152385298746912
      ],
      "excerpt": "Inference after 5 epochs of training on the smaller test dataset, due to a lack of compute ressources I stopped early (author did 75 epochs with finetuning method and 150 with feed-forward method on the full dataset). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8793425576268525
      ],
      "excerpt": "IR to Pytorch code and weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.918650400483497
      ],
      "excerpt": "Pytorch code and weights to Pytorch model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8145399309746906,
        0.9239866923223571
      ],
      "excerpt": "I followed the architecture guidelines from the paper on top of details provided by M. Zakharov. \nThe images that are fed from voxceleb2 are resized from 224x224 to 256x256 by using zero-padding. This is done so that spatial dimensions don't get rounded when passing through downsampling layers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8680610084504055,
        0.9283140420996283
      ],
      "excerpt": "The embedder uses 6 downsampling residual blocks with no normalisation. A self-attention layer is added in the middle. \nThe output from the last residual block is resized to a vector of size 512 via maxpooling. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9019507632521282,
        0.9228586310777879,
        0.984721198417652,
        0.8793171308678972,
        0.8631048581420915
      ],
      "excerpt": "The downsampling part of the generator uses the same architecture as the embedder with instance normalization added at each block following the paper. \nThe same dimension residual part uses 5 blocks. These blocks use adaptive instance normalization. Unlike the AdaIN paper(Xun Huang et al.) where the alpha and beta learnable parameters from instance normalisation are replaced with mean and variance of the input style, the adaptative parameters (mean and variance) are taken from psi. With psi = P*e, P the projection matrix and e the embedding vector calculated by the embedder. \n(P is of size 2*(512*2*5 + 512*2 + 512*2+ 512+256 + 256+128 + 128+64 + 64+3) x 512 = 17158 x 512) \nThere are then 6 upsampling residual blocks. The final output is a tensor of dimensions 3x224x224. I rescale the image using a sigmoid and multiplying by 255. There are two adaIN layers in each upsampling block (they replace the normalisation layers from the Biggan paper). \nSelf-attention layers are added both in the downsampling part and upsampling part of the generator. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/times2049/talkinghead/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 00:21:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/times2049/talkinghead/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "times2049/talkinghead",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9162898779493325
      ],
      "excerpt": "Follow these instructions to install the VGGFace from the paper (https://arxiv.org/pdf/1703.07332.pdf): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8644500257064217,
        0.9852239031488754,
        0.999746712887969
      ],
      "excerpt": "$ tar xvzf vgg_face_caffe.tar.gz \n$ sudo apt install caffe-cuda \n$ pip install mmdnn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9794300954391058
      ],
      "excerpt": "If you have a problem with pickle, delete your numpy and reinstall numpy with version 1.16.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9143054177750861
      ],
      "excerpt": "cv2 (opencv-python) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8049066583339194,
        0.8852592574765727
      ],
      "excerpt": "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/F2vms-eUrYs/0.jpg\"  \nalt=\"IMAGE ALT TEXT HERE\" width=\"240\" height=\"180\" border=\"10\" /></a> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/times2049/talkinghead/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Realistic-Neural-Talking-Head-Models",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "talkinghead",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "times2049",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/times2049/talkinghead/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 00:21:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- modify paths in params folder to reflect your path\n- preprocess.py: preprocess our data for faster inference and lighter dataset\n- train.py: initialize and train the network or continue training from trained network\n- embedder_inference.py: (Requires trained model) Run the embedder on videos or images of a person and get embedding vector in tar file \n- fine_tuning_trainng.py: (Requires trained model and embedding vector) finetune a trained model\n- webcam_inference.py: (Requires trained model and embedding vector) run the model using person from embedding vector and webcam input, just inference\n- video_inference.py: just like webcam_inference but on a video, change the path of the video at the start of the file\n\n\n",
      "technique": "Header extraction"
    }
  ]
}