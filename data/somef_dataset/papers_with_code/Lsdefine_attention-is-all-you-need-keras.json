{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Some model structures and some scripts are borrowed from [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Lsdefine/attention-is-all-you-need-keras",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-03-16T06:07:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T04:01:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.933978815880939,
        0.9533166117300753,
        0.9687252164186767
      ],
      "excerpt": "This task is same as in jadore801120/attention-is-all-you-need-pytorch: WMT'16 Multimodal Translation: Multi30k (de-en) (http://www.statmt.org/wmt16/multimodal-task.html). We borrowed the data preprocessing step 0 and 1 in the repository, and then construct the input file en2de.s2s.txt \nThe code achieves near results as in the repository: about 70% valid accuracy.  \nIf using smaller model parameters, such as layers=2 and d_model=256, the valid accuracy is better since the task is quite small. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9234738841973491
      ],
      "excerpt": "For larger number of layers, the special learning rate scheduler reported in the papar is necessary. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Keras+TensorFlow Implementation of the Transformer: Attention Is All You Need",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Lsdefine/attention-is-all-you-need-keras/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 194,
      "date": "Tue, 28 Dec 2021 19:06:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Lsdefine/attention-is-all-you-need-keras/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Lsdefine/attention-is-all-you-need-keras",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8046438710702787
      ],
      "excerpt": "In pinyin_main.py, I tried another method to train the deep network. I train the first layer and the embedding layer first, then train a 2-layers model, and then train a 3-layers, etc. It works in this task. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Lsdefine/attention-is-all-you-need-keras/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "The Transformer model in Attention is all you need\uff1aa Keras implementation.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "attention-is-all-you-need-keras",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Lsdefine",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Lsdefine/attention-is-all-you-need-keras/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 651,
      "date": "Tue, 28 Dec 2021 19:06:01 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "attention-is-all-you-need",
      "attention-seq2seq",
      "keras-tensorflow",
      "keras"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please refer to *en2de_main.py* and *pinyin_main.py*\n",
      "technique": "Header extraction"
    }
  ]
}