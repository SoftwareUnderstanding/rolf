{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1606.00915"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{KrahenbuhlK11,\n  title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},\n  author={Philipp Kr{\\\"{a}}henb{\\\"{u}}hl and Vladlen Koltun},\n  booktitle={NIPS},\n  year={2011}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{CP2015Semantic,\n  title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs},\n  author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n  booktitle={ICLR},\n  year={2015}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{PC2015Weak,\n  title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation},\n  author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille},\n  booktitle={ICCV},\n  year={2015}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{CB2016Semantic,\n  title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform},\n  author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille},\n  booktitle={CVPR},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{CY2016Attention,\n  title={Attention to Scale: Scale-aware Semantic Image Segmentation},\n  author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille},\n  booktitle={CVPR},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{CP2016Deeplab,\n  title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},\n  author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n  journal={arXiv:1606.00915},\n  year={2016}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/open-cv/deeplab-v2",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing\nIssues\nSpecific Caffe design and development issues, bugs, and feature requests are maintained by GitHub Issues.\nPlease do not post usage, installation, or modeling questions, or other requests for help to Issues.\nUse the caffe-users list instead. This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.\nWhen reporting a bug, it's most helpful to provide the following information, where applicable:\n\nWhat steps reproduce the bug?\nCan you reproduce the bug using the latest master, compiled with the DEBUG make option?\nWhat hardware and operating system/distribution are you running?\nIf the bug is a crash, provide the backtrace (usually printed by Caffe; always obtainable with gdb).\n\nTry to give your issue a title that is succinct and specific. The devs will rename issues as needed to keep track of them.\nPull Requests\nCaffe welcomes all contributions.\nSee the contributing guide for details.\nBriefly: read commit by commit, a PR should tell a clean, compelling story of one improvement to Caffe. In particular:\n\nA PR should do one clear thing that obviously improves Caffe, and nothing more. Making many smaller PRs is better than making one large PR; review effort is superlinear in the amount of code involved.\nSimilarly, each commit should be a small, atomic change representing one step in development. PRs should be made of many commits where appropriate.\nPlease do rewrite PR history to be clean rather than chronological. Within-PR bugfixes, style cleanups, reversions, etc. should be squashed and should not appear in merged PR history.\nAnything nonobvious from the code should be explained in comments, commit messages, or the PR description, as appropriate.",
    "technique": "File Exploration"
  },
  "contributors": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributors\nCaffe is developed by a core set of BVLC members and the open-source community.\nWe thank all of our contributors!\nFor the detailed history of contributions of a given file, try\ngit blame file\n\nto see line-by-line credits and\ngit log --follow file\n\nto see the change log even across renames and rewrites.\nPlease refer to the acknowledgements on the Caffe site for further details.\nCopyright is held by the original contributor according to the versioning history; see LICENSE.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-22T06:36:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-24T15:41:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "DeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).\n\nIt combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks, (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views, and (3) densely connected conditional random fields (CRF) as post processing.\n\nThis distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915).\nThis version also supports the experiments (DeepLab v1) in our ICLR'15. You only need to modify the old prototxt files. For example, our proposed atrous convolution is called dilated convolution in CAFFE framework, and you need to change the convolution parameter \"hole\" to \"dilation\" (the usage is exactly the same). For the experiments in ICCV'15, there are some differences between our argmax and softmax_loss layers and Caffe's. Please refer to [DeepLabv1](https://bitbucket.org/deeplab/deeplab-public/) for details.\n\nPlease consult and consider citing the following papers:\n\n    @article{CP2016Deeplab,\n      title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},\n      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n      journal={arXiv:1606.00915},\n      year={2016}\n    }\n\n    @inproceedings{CY2016Attention,\n      title={Attention to Scale: Scale-aware Semantic Image Segmentation},\n      author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille},\n      booktitle={CVPR},\n      year={2016}\n    }\n\n    @inproceedings{CB2016Semantic,\n      title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform},\n      author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille},\n      booktitle={CVPR},\n      year={2016}\n    }\n\n    @inproceedings{PC2015Weak,\n      title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation},\n      author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille},\n      booktitle={ICCV},\n      year={2015}\n    }\n\n    @inproceedings{CP2015Semantic,\n      title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs},\n      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n      booktitle={ICLR},\n      year={2015}\n    }\n\n\nNote that if you use the densecrf implementation, please consult and cite the following paper:\n\n    @inproceedings{KrahenbuhlK11,\n      title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},\n      author={Philipp Kr{\\\"{a}}henb{\\\"{u}}hl and Vladlen Koltun},\n      booktitle={NIPS},\n      year={2011}\n    }\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9346798788893937,
        0.9928104927573357,
        0.9136713126584173
      ],
      "excerpt": "DeepLabv2 currently achieves 79.7% on the challenging PASCAL VOC 2012 semantic image segmentation task -- see the leaderboard.  \nPlease refer to our project website for details. \nWe have released several trained models and corresponding prototxt files at here. Please check it for more model details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "deeplab v2",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/open-cv/deeplab-v2/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Check [FAQ](http://liangchiehchen.com/projects/DeepLab_FAQ.html) if you have some problems while using the code.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 13:22:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/open-cv/deeplab-v2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "open-cv/deeplab-v2",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/open-cv/deeplab-v2/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/02-fine-tuning.ipynb",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/net_surgery.ipynb",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/detection.ipynb",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/01-learning-lenet.ipynb",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/brewing-logreg.ipynb",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/00-classification.ipynb",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/siamese/mnist_siamese.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/scripts/deploy_docs.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/scripts/build_docs.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/scripts/download_model_from_gist.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/scripts/upload_model_to_gist.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/scripts/gather_examples.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/scripts/travis/travis_setup_makefile_config.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/scripts/travis/travis_build_and_test.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/scripts/travis/travis_install.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/imagenet/create_imagenet.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/imagenet/resume_training.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/imagenet/train_caffenet.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/imagenet/make_imagenet_mean.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/cifar10/create_cifar10.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/cifar10/train_quick.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/cifar10/train_full_sigmoid.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/cifar10/train_full_sigmoid_bn.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/cifar10/train_full.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/mnist/train_lenet_adam.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/mnist/train_lenet_consolidated.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/mnist/train_mnist_autoencoder_adagrad.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/mnist/train_lenet_rmsprop.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/mnist/create_mnist.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/mnist/train_mnist_autoencoder_nesterov.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/mnist/train_lenet.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/mnist/train_mnist_autoencoder.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/mnist/train_mnist_autoencoder_adadelta.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/siamese/create_mnist_siamese.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/examples/siamese/train_mnist_siamese.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/tools/extra/parse_log.sh",
      "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/tools/extra/launch_resize_and_crop_images.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. The scripts we used for our experiments can be downloaded from this [link](https://ucla.box.com/s/4grlj8yoodv95936uybukjh5m0tdzvrf):\n    1. run_pascal.sh: the script for training/testing on the PASCAL VOC 2012 dataset. __Note__ You also need to download sub.sed script.\n    2. run_densecrf.sh and run_densecrf_grid_search.sh: the scripts we used for post-processing the DCNN computed results by DenseCRF.\n2. The image list files used in our experiments can be downloaded from this [link](https://ucla.box.com/s/rd9z2xvwsfpksi7mi08i2xqrj7ab4keb):\n    * The zip file stores the list files for the PASCAL VOC 2012 dataset.\n3. To use the mat_read_layer and mat_write_layer, please download and install [matio](http://sourceforge.net/projects/matio/files/matio/1.5.2/).\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/open-cv/deeplab-v2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Python",
      "Cuda",
      "MATLAB",
      "CMake",
      "Makefile",
      "Shell",
      "C",
      "Limbo"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/open-cv/deeplab-v2/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'COPYRIGHT\\n\\nAll new contributions compared to the original Caffe branch:\\nCopyright (c) 2015, 2016, Liang-Chieh Chen (UCLA, Google), George Papandreou (Google),\\nIasonas Kokkinos (CentraleSup\\xc3\\xa9lec /INRIA), Jonathan T. Barron(Google),\\nYi Yang (Baidu), Jiang Wang (Baidu), Wei Xu (Baidu),\\nKevin Murphy (Google), and Alan L. Yuille (UCLA, JHU)\\nAll rights reserved.\\n\\nAll contributions by the University of California:\\nCopyright (c) 2014, 2015, The Regents of the University of California (Regents)\\nAll rights reserved.\\n\\nAll other contributions:\\nCopyright (c) 2014, 2015, the respective contributors\\nAll rights reserved.\\n\\nCaffe uses a shared copyright model: each contributor holds copyright over\\ntheir contributions to Caffe. The project versioning records all such\\ncontribution and copyright details. If a contributor wants to further mark\\ntheir specific copyright on a particular contribution, they should indicate\\ntheir copyright solely in the commit message of the change when it is\\ncommitted.\\n\\nLICENSE\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met: \\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer. \\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution. \\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\nCONTRIBUTION AGREEMENT\\n\\nBy contributing to the BVLC/caffe repository through pull-request, comment,\\nor otherwise, the contributor releases their content to the\\nlicense and copyright terms herein.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# DeepLab v2",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deeplab-v2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "open-cv",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/open-cv/deeplab-v2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "There are several variants of DeepLab. To begin with, we suggest DeepLab-LargeFOV, which has good performance and faster training time.\n\nSuppose the codes are located at deeplab/code\n\n1. mkdir deeplab/exper (Create a folder for experiments)\n2. mkdir deeplab/exper/voc12 (Create a folder for your specific experiment. Let's take PASCAL VOC 2012 for example.)\n3. Create folders for config files and so on.\n    1. mkdir deeplab/exper/voc12/config  (where network config files are saved.)\n    2. mkdir deeplab/exper/voc12/features  (where the computed features will be saved (when train on train))\n    3. mkdir deeplab/exper/voc12/features2 (where the computed features will be saved (when train on trainval))\n    4. mkdir deeplab/exper/voc12/list (where you save the train, val, and test file lists)\n    5. mkdir deeplab/exper/voc12/log (where the training/test logs will be saved)\n    6. mkdir deeplab/exper/voc12/model (where the trained models will be saved)\n    7. mkdir deeplab/exper/voc12/res (where the evaluation results will be saved)\n4. mkdir deeplab/exper/voc12/config/deeplab_largeFOV (test your own network. Create a folder under config. For example, deeplab_largeFOV is the network you want to experiment with. Add your train.prototxt and test.prototxt in that folder (you can check some provided examples for reference).)\n5. Set up your init.caffemodel at deeplab/exper/voc12/model/deeplab_largeFOV. You may want to soft link init.caffemodel to the modified VGG-16 net. For example, run \"ln -s vgg16.caffemodel init.caffemodel\" at voc12/model/deeplab_largeFOV.\n6. Modify the provided script, run_pascal.sh, for experiments. You should change the paths according to your setting. For example, you should specify where the caffe is by changing CAFFE_DIR. Note You may need to modify sub.sed, if you want to replace some variables with your desired values in train.prototxt or test.prototxt.\n7. The computed features are saved at folders features or features2, and you can run provided MATLAB scripts to evaluate the results (e.g., check the script at code/matlab/my_script/EvalSegResults).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 21 Dec 2021 13:22:13 GMT"
    },
    "technique": "GitHub API"
  }
}