{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1811.08383",
      "https://arxiv.org/abs/1711.07971",
      "https://arxiv.org/abs/1608.00859>",
      "https://arxiv.org/abs/1811.08383"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find our paper and repo useful, please cite our paper. Thanks!\n\n```\n@article{lin2018temporal,\n    title={Temporal Shift Module for Efficient Video Understanding},\n    author={Lin, Ji and Gan, Chuang and Han, Song},\n    journal={arXiv preprint arXiv:1811.08383},\n    year={2018}\n}  \n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{lin2018temporal,\n    title={Temporal Shift Module for Efficient Video Understanding},\n    author={Lin, Ji and Gan, Chuang and Han, Song},\n    journal={arXiv preprint arXiv:1811.08383},\n    year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8984312557015187
      ],
      "excerpt": "| method     | n-frame | acc (1-crop) | acc (10-crop) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9696971001562653
      ],
      "excerpt": "Change to --test_crops=10 for 10-crop evaluation. With the above scripts, you should get around 68.8% and 71.2% results respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "       --gd 20 --lr 0.001 --lr_steps 10 20 --epochs 25 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "       --gd 20 --lr 0.001 --lr_steps 10 20 --epochs 25 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/WavesUR/embedded_TSM",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-29T01:56:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-22T06:45:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9630530293722905,
        0.9942026895833965,
        0.9425653279771917
      ],
      "excerpt": "We release the PyTorch code of the Temporal Shift Module. \nThis code is based on the TSN codebase. The core code to implement the Temporal Shift Module is ops/temporal_shift.py. It is a plug-and-play module to enable temporal reasoning, at the cost of zero parameters and zero FLOPs. \nHere we provide a naive implementation of TSM. It can be implemented with just several lines of code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8619089259402867
      ],
      "excerpt": ": shape of x: [N, T, C, H, W] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9642399867141984,
        0.9592909963222506
      ],
      "excerpt": "Note that the naive implementation involves large data copying and increases memory consumption during training. It is suggested to use the in-place version of TSM to improve speed (see ops/temporal_shift.py Line 12 for the details.) \nTraining on Kinetics is computationally expensive. Here we provide the pretrained models on Kinetics for fine-tuning. To get the pretrained model, run from the root folder: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9862154153830333,
        0.9287442472215485
      ],
      "excerpt": "In the current version of our paper, we reported the results of TSM trained and tested with I3D dense sampling (Table 1&4, 8-frame and 16-frame), using the same training and testing hyper-parameters as in Non-local Neural Networks paper to directly compare with I3D. Here we provide the 8-frame version checkpoint TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e100_dense.pth that achieves 74.1% Kinetics accuracy. We also provide a model trained with Non-local module TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e100_dense_nl.pth to form NL TSM. \nWe compare the I3D performance reported in Non-local paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9674073786842188,
        0.9838451222748981,
        0.9842055057497991
      ],
      "excerpt": "TSM outperforms I3D under the same dense sampling protocol. NL TSM model also achieves better performance than NL I3D model. Non-local module itself improves the accuracy by 1.5%. \nWe also provide the checkpoints of TSN and TSM models using uniform sampled frames as in Temporal Segment Networks paper, which is very useful for fine-tuning on other datasets. We provide the pretrained ResNet-50 for TSN and our TSM (8 and 16 frames), including TSM_kinetics_RGB_resnet50_avg_segment5_e50.pth, TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth, TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment16_e50.pth. \nThe performance on Kinetics is measured as (using only 1 clip): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8380696880298945
      ],
      "excerpt": "Our TSM module improves consistently over the TSN baseline. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9844061064182301
      ],
      "excerpt": "To get the Kinetics performance of our dense sampling model under Non-local protocol, run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9368701804908895
      ],
      "excerpt": "We provide the log files of above testing examples in folder logs. For other datasets and trained models, refer to the code for details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068756609271439
      ],
      "excerpt": "After getting the Kinetics pretrained models, we can fine-tune on other datasets using the Kinetics pretrained models. For example, we can fine-tune 8-frame Kinetics pre-trained model on UCF-101 dataset using uniform sampling by running: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "cs231n project",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/WavesUR/embedded_TSM/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 08:40:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/WavesUR/embedded_TSM/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "WavesUR/embedded_TSM",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/WavesUR/embedded_TSM/master/scripts/train_tsn_kinetics_rgb_5f.sh",
      "https://raw.githubusercontent.com/WavesUR/embedded_TSM/master/scripts/train_tsm_kinetics_rgb_16f.sh",
      "https://raw.githubusercontent.com/WavesUR/embedded_TSM/master/scripts/train_tsm_jester_rgb_8f.sh",
      "https://raw.githubusercontent.com/WavesUR/embedded_TSM/master/scripts/train_tsm_kinetics_rgb_8f.sh",
      "https://raw.githubusercontent.com/WavesUR/embedded_TSM/master/scripts/test_tsm_kinetics_rgb_8f.sh",
      "https://raw.githubusercontent.com/WavesUR/embedded_TSM/master/scripts/test_tsm_jester_resnet50_rgb_8f.sh",
      "https://raw.githubusercontent.com/WavesUR/embedded_TSM/master/scripts/finetune_tsm_ucf101_rgb_8f.sh",
      "https://raw.githubusercontent.com/WavesUR/embedded_TSM/master/scripts/test_tsm_jester_MobileNetV2_rgb_8f.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We need to first extract videos into frames for fast reading. Please refer to [TSN](https://github.com/yjxiong/temporal-segment-networks) repo for the detailed guide of data pre-processing.\n\nWe have successfully trained on [Kinetics](https://deepmind.com/research/open-source/open-source-datasets/kinetics/), [UCF101](http://crcv.ucf.edu/data/UCF101.php), [HMDB51](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/), [Something-Something-V1](https://20bn.com/datasets/something-something/v1) and [V2](https://20bn.com/datasets/something-something/v2), [Jester](https://20bn.com/datasets/jester) datasets with this codebase. Basically, the processing of video data can be summarized into 3 steps:\n\n- Extract frames from videos (refer to [tools/vid2img_kinetics.py](tools/vid2img_kinetics.py) for Kinetics example and [tools/vid2img_sthv2.py](tools/vid2img_sthv2.py) for Something-Something-V2 example)\n- Generate annotations needed for dataloader (refer to [tools/gen_label_kinetics.py](tools/gen_label_kinetics.py) for Kinetics example, [tools/gen_label_sthv1.py](tools/gen_label_sthv1.py) for Something-Something-V1 example, and [tools/gen_label_sthv2.py](tools/gen_label_sthv2.py) for Something-Something-V2 example)\n- Add the information to [ops/dataset_configs.py](ops/dataset_configs.py)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8788827344341016
      ],
      "excerpt": "Make sure you download the pretrained Checkpoint files to /pretrained/  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917551972412043
      ],
      "excerpt": "Make sure dataset directory is as the following structure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.829557141570049
      ],
      "excerpt": "run following shell in master folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023697225149864,
        0.9023697225149864
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 test_tsm_jester_MobileNetV2_rgb_8f.sh \n\u2502   \u2514\u2500\u2500 test_tsm_jester_resnet50_rgb_8f.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9675155705296752
      ],
      "excerpt": "bash pretrained/download.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9425339501450113
      ],
      "excerpt": "For example, to test the downloaded pretrained models on Kinetics, you can run scripts/test_tsm_kinetics_rgb_8f.sh. The scripts will test both TSN and TSM on 8-frame setting by running: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9469241685289078,
        0.8793671372991584
      ],
      "excerpt": "To train on Kinetics from ImageNet pretrained models, you can run scripts/train_tsm_kinetics_rgb_8f.sh, which contains: \n  #: You should get TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8354493523192837
      ],
      "excerpt": "Make sure you download the pretrained Checkpoint files to /pretrained/  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.847094757043326
      ],
      "excerpt": "Copy *.txt files to jester folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8607474522911361,
        0.8350169214006999,
        0.8827864380226305
      ],
      "excerpt": "Training on Kinetics is computationally expensive. Here we provide the pretrained models on Kinetics for fine-tuning. To get the pretrained model, run from the root folder: \nbash pretrained/download.sh \nIt will download the models into pretrained folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076904413800101
      ],
      "excerpt": "For example, to test the downloaded pretrained models on Kinetics, you can run scripts/test_tsm_kinetics_rgb_8f.sh. The scripts will test both TSN and TSM on 8-frame setting by running: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664,
        0.9298336810758577,
        0.8148043622917135
      ],
      "excerpt": ": test TSN \npython test_models.py kinetics \\ \n    --weights=pretrained/TSM_kinetics_RGB_resnet50_avg_segment5_e50.pth \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664,
        0.9298336810758577,
        0.8148043622917135
      ],
      "excerpt": ": test TSM \npython test_models.py kinetics \\ \n    --weights=pretrained/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9298336810758577,
        0.8148043622917135
      ],
      "excerpt": "python test_models.py kinetics \\ \n    --weights=pretrained/TSM_kinetics_RGB_resnet50_avg_segment5_e50.pth \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9298336810758577,
        0.8148043622917135
      ],
      "excerpt": "python test_models.py kinetics \\ \n    --weights=pretrained/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e100_dense.pth \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9298336810758577,
        0.8148043622917135
      ],
      "excerpt": "python test_models.py kinetics \\ \n    --weights=pretrained/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e100_dense_nl.pth \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207504233295029
      ],
      "excerpt": "  python main.py kinetics RGB \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8559395774122353
      ],
      "excerpt": "       --batch-size 128 -j 16 --dropout 0.5 --consensus_type=avg --eval-freq=1 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8843046402447461,
        0.876976154230365
      ],
      "excerpt": "After getting the Kinetics pretrained models, we can fine-tune on other datasets using the Kinetics pretrained models. For example, we can fine-tune 8-frame Kinetics pre-trained model on UCF-101 dataset using uniform sampling by running: \npython main.py ucf101 RGB \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109532309113273,
        0.8257168972922656
      ],
      "excerpt": "       --gd 20 --lr 0.001 --lr_steps 10 20 --epochs 25 \\ \n       --batch-size 64 -j 16 --dropout 0.8 --consensus_type=avg --eval-freq=1 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8509653106547419
      ],
      "excerpt": "       --tune_from=pretrained/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8438291436061371
      ],
      "excerpt": "  python main.py something RGB \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109532309113273,
        0.8257168972922656
      ],
      "excerpt": "       --gd 20 --lr 0.001 --lr_steps 10 20 --epochs 25 \\ \n       --batch-size 64 -j 16 --dropout 0.5 --consensus_type=avg --eval-freq=1 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/WavesUR/embedded_TSM/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Note",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "embedded_TSM",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "WavesUR",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/WavesUR/embedded_TSM/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code is built with following libraries:\n\n- [PyTorch](https://pytorch.org/) 1.0\n- [TensorboardX](https://github.com/lanpa/tensorboardX)\n- [tqdm](https://github.com/tqdm/tqdm.git)\n\nFor video data pre-processing, you may need [ffmpeg](https://www.ffmpeg.org/).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 23 Dec 2021 08:40:20 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We have build an online hand gesture recognition demo using our TSM. The model is built with MobileNetV2 backbone and trained on Jester dataset. \n\n- Recorded video of the live demo [[link]](https://hanlab.mit.edu/projects/tsm/#live_demo)\n- Code of the live demo on Jeston TX2: [TODO]\n",
      "technique": "Header extraction"
    }
  ]
}