# Question Answering Model
REFERENCES
1)	https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html 
2)	https://jalammar.github.io/illustrated-transformer/ 
3)	https://blog.floydhub.com/attention-mechanism/ 
4)	https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b 
5)	https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/
6)	https://arxiv.org/pdf/1506.03340.pdf
7)	https://www.coursera.org/learn/nlp-sequence-models/lecture/ftkzt/recurrent-neural-network-model
8)	https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
9)	https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16084/15738
10)	https://arxiv.org/abs/1611.01603
11)	https://rajpurkar.github.io/mlx/qa-and-squad/
12)	https://pypi.org/project/bert-embedding/
13)	https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b
14)	https://github.com/google-research/bert


