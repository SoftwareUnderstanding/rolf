{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980",
      "https://arxiv.org/abs/1412.6980",
      "https://arxiv.org/abs/1207.0580](https://arxiv.org/abs/1207.0580",
      "https://arxiv.org/abs/1207.0580",
      "https://arxiv.org/abs/1412.6980\n[2]: https://arxiv.org/abs/1207.0580\n",
      "https://arxiv.org/abs/1207.0580\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[[1]] Diederik P. Kingma, Jimmy Ba. \"Adam: A Method for Stochastic\n         Optimization\", ICLR 2015. [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n\n[[2]] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. \"Improving neural networks by preventing co-adaptation of feature detectors.\" arXiv preprint. [https://arxiv.org/abs/1207.0580](https://arxiv.org/abs/1207.0580)\n\n\n[1]: https://arxiv.org/abs/1412.6980\n[2]: https://arxiv.org/abs/1207.0580\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DylanMuir/fmin_adam",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-02-15T13:44:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-09T13:29:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9893243326633955
      ],
      "excerpt": "This is a Matlab implementation of the Adam optimiser from Kingma and Ba [[1]], designed for stochastic gradient descent. It maintains estimates of the moments of the gradient independently for each parameter. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9494184287779504,
        0.8344937726393095
      ],
      "excerpt": "fun must be deterministic in its calculation of fCost and vfCdX, even if mini-batches are used. To this end, fun can accept a parameter nIter which specifies the current iteration of the optimisation algorithm. fun must return estimates over identical problems for a given value of nIter. \nSteps that do not lead to a reduction in the function to be minimised are not taken. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741328224286306
      ],
      "excerpt": "2: The norm of the current step was less than TolX. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8727399309192793
      ],
      "excerpt": ".stepsize \u2014 Norm of current parameter step \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9111632521189462
      ],
      "excerpt": "The optional parameters stepSize, beta1, beta2 and epsilon are  parameters of the Adam optimisation algorithm (see [[1]]). Default values  of {1e-3, 0.9, 0.999, sqrt(eps)} are reasonable for most problems. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8797502206684678
      ],
      "excerpt": "The optional argument options is used to control the optimisation process (see optimset). Relevant fields: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Matlab implementation of the Adam stochastic gradient descent optimisation algorithm",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DylanMuir/fmin_adam/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 23,
      "date": "Tue, 28 Dec 2021 17:36:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DylanMuir/fmin_adam/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DylanMuir/fmin_adam",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DylanMuir/fmin_adam/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "MATLAB"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Adam optimiser",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fmin_adam",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DylanMuir",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DylanMuir/fmin_adam/blob/master/Readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 33,
      "date": "Tue, 28 Dec 2021 17:36:36 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "matlab",
      "optimization",
      "optimization-algorithms",
      "gradient-descent",
      "stochastic-gradient-descent"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "` [x, fval, exitflag, output] = fmin_adam(fun, x0 <, stepSize, beta1, beta2, epsilon, nEpochSize, options>)`\n\n`fmin_adam` is an implementation of the Adam optimisation algorithm (gradient descent with Adaptive learning rates individually on each parameter, with Momentum) from Kingma and Ba [[1]]. Adam is designed to work on stochastic gradient descent problems; i.e. when only small batches of data are used to estimate the gradient on each iteration, or when stochastic dropout regularisation is used [[2]].\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "###Simple regression problem with gradients\n\nSet up a simple linear regression problem: ![$$$y = x\\cdot\\phi_1 + \\phi_2 + \\zeta$$$](https://latex.codecogs.com/svg.latex?%5Cinline%20y%20%3D%20x%5Ccdot%5Cphi_1%20&plus;%20%5Cphi_2%20&plus;%20%5Czeta), where ![$$$\\zeta \\sim N(0, 0.1)$$$](https://latex.codecogs.com/svg.latex?%5Cinline%20%5Czeta%20%5Csim%20N%280%2C%200.1%29). We'll take ![$$$\\phi = \\left[3, 2\\right]$$$](https://latex.codecogs.com/svg.latex?%5Cinline%20%5Cphi%20%3D%20%5Cleft%5B3%2C%202%5Cright%5D) for this example. Let's draw some samples from this problem:\n\n```matlab\nnDataSetSize = 1000;\nvfInput = rand(1, nDataSetSize);\nphiTrue = [3 2];\nfhProblem = @(phi, vfInput) vfInput .* phi(1) + phi(2);\nvfResp = fhProblem(phiTrue, vfInput) + randn(1, nDataSetSize) * .1;\nplot(vfInput, vfResp, '.'); hold;\n```\n\n<img src=\"images/regression_scatter.png\" />\n\nNow we define a cost function to minimise, which returns analytical gradients:\n\n```matlab\nfunction [fMSE, vfGrad] = LinearRegressionMSEGradients(phi, vfInput, vfResp)\n   % - Compute mean-squared error using the current parameter estimate\n   vfRespHat = vfInput .* phi(1) + phi(2);\n   vfDiff = vfRespHat - vfResp;\n   fMSE = mean(vfDiff.^2) / 2;\n   \n   % - Compute the gradient of MSE for each parameter\n   vfGrad(1) = mean(vfDiff .* vfInput);\n   vfGrad(2) = mean(vfDiff);\nend\n```\n\nInitial parameters `phi0` are Normally distributed. Call the `fmin_adam` optimiser with a learning rate of 0.01.\n\n```matlab\nphi0 = randn(2, 1);\nphiHat = fmin_adam(@(phi)LinearRegressionMSEGradients(phi, vfInput, vfResp), phi0, 0.01)\nplot(vfInput, fhProblem(phiHat, vfInput), '.');\n````\n\nOutput:\n\n     Iteration   Func-count         f(x)   Improvement    Step-size\n    ----------   ----------   ----------   ----------   ----------\n          2130         4262       0.0051        5e-07      0.00013\n    ----------   ----------   ----------   ----------   ----------\n\n    Finished optimization.\n       Reason: Function improvement [5e-07] less than TolFun [1e-06].\n\n    phiHat =\n        2.9498\n        2.0273\n\n<img src=\"images/regression_fit.png\" />\n\n###Linear regression with minibatches\n\nSet up a simple linear regression problem, as above.\n\n```matlab\nnDataSetSize = 1000;\nvfInput = rand(1, nDataSetSize);\nphiTrue = [3 2];\nfhProblem = @(phi, vfInput) vfInput .* phi(1) + phi(2);\nvfResp = fhProblem(phiTrue, vfInput) + randn(1, nDataSetSize) * .1;\n```\n\nConfigure minibatches. Minibatches contain random sets of indices into the data.\n\n```matlab\nnBatchSize = 50;\nnNumBatches = 100;\nmnBatches = randi(nDataSetSize, nBatchSize, nNumBatches);\ncvnBatches = mat2cell(mnBatches, nBatchSize, ones(1, nNumBatches));\nfigure; hold;\ncellfun(@(b)plot(vfInput(b), vfResp(b), '.'), cvnBatches);\n```\n<img src=\"images/regression_minibatches.png\" />\n       \nDefine the function to minimise; in this case, the mean-square error over the regression problem. The iteration index `nIter` defines which mini-batch to evaluate the problem over.\n\n```matlab\nfhBatchInput = @(nIter) vfInput(cvnBatches{mod(nIter, nNumBatches-1)+1});\nfhBatchResp = @(nIter) vfResp(cvnBatches{mod(nIter, nNumBatches-1)+1});\nfhCost = @(phi, nIter) mean((fhProblem(phi, fhBatchInput(nIter)) - fhBatchResp(nIter)).^2);\n```\nTurn off analytical gradients for the `adam` optimiser, and ensure that we permit sufficient function calls.\n\n```matlab\nsOpt = optimset('fmin_adam');\nsOpt.GradObj = 'off';\nsOpt.MaxFunEvals = 1e4;\n```\n\nCall the `fmin_adam` optimiser with a learning rate of `0.1`. Initial parameters are Normally distributed.\n\n```matlab\nphi0 = randn(2, 1);\nphiHat = fmin_adam(fhCost, phi0, 0.1, [], [], [], [], sOpt)\n```\nThe output of the optimisation process (which will differ over random data and random initialisations):\n\n    Iteration   Func-count         f(x)   Improvement    Step-size\n    ----------   ----------   ----------   ----------   ----------\n           711         2848          0.3       0.0027      3.8e-06\n    ----------   ----------   ----------   ----------   ----------\n\n    Finished optimization.\n       Reason: Step size [3.8e-06] less than TolX [1e-05].\n\n    phiHat =\n        2.8949\n        1.9826\n    \n",
      "technique": "Header extraction"
    }
  ]
}