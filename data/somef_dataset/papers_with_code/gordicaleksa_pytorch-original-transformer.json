{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I found these resources useful (while developing this one):\n\n* [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n* [PyTorch official implementation](https://github.com/pytorch/pytorch/blob/187e23397c075ec2f6e89ea75d24371e3fbf9efa/torch/nn/modules/transformer.py)\n\nI found some inspiration for the model design in the The Annotated Transformer but I found it hard to understand, and\nit had some bugs. It was mainly written with researchers in mind. Hopefully this repo opens up\nthe understanding of transformers to the common folk as well! :nerd_face:\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this code useful, please cite the following:\n\n```\n@misc{Gordi\u01072020PyTorchOriginalTransformer,\n  author = {Gordi\u0107, Aleksa},\n  title = {pytorch-original-transformer},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/gordicaleksa/pytorch-original-transformer}},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{Gordi\u01072020PyTorchOriginalTransformer,\n  author = {Gordi\u0107, Aleksa},\n  title = {pytorch-original-transformer},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/gordicaleksa/pytorch-original-transformer}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "Machine translation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8715509475085587
      ],
      "excerpt": "Input: Ich bin ein guter Mensch, denke ich. (\"gold\": I am a good person I think) <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654671031158477,
        0.8654671031158477,
        0.8028046190715653,
        0.8028046190715653
      ],
      "excerpt": "| Baseline transformer (EN-DE) | 27.8 | IWSLT val | \n| Baseline transformer (DE-EN) | 33.2 | IWSLT val | \n| Baseline transformer (EN-DE) | x | WMT-14 val | \n| Baseline transformer (DE-EN) | x | WMT-14 val | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302556419090275
      ],
      "excerpt": "<a href=\"https://www.youtube.com/watch?v=cbYxHkgkSVs\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/cbYxHkgkSVs/0.jpg\"  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gordicaleksa/pytorch-original-transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-19T19:30:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T21:17:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9535332760059066,
        0.9181846987525246
      ],
      "excerpt": "This repo contains PyTorch implementation of the original transformer paper (:link: Vaswani et al.). <br/> \nIt's aimed at making it easy to start playing and learning about transformers. <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8883153990411549
      ],
      "excerpt": "is that they showed that you don't have to use recurrent or convolutional layers and that simple architecture coupled with attention is super powerful. It \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9819580794353217,
        0.8876178206580563,
        0.9560264405257571
      ],
      "excerpt": "This repo is supposed to be a learning resource for understanding transformers as the original transformer by itself is not a SOTA anymore. \nFor that purpose the code is (hopefully) well commented and I've included the playground.py where I've visualized a couple \nof concepts which are hard to explain using words but super simple once visualized. So here we go! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "<img src=\"data/readme_pics/positional_encoding_visualized.jpg\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9267538904370215,
        0.9458287417388371
      ],
      "excerpt": "Depending on the position of your source/target token you \"pick one row of this image\" and you add it to it's embedding vector, that's it. \nThey could also be learned, but it's just more fancy to do it like this, obviously! :nerd_face: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8527674811222534
      ],
      "excerpt": "Noup? So I thought, here it is visualized: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8487437820571014
      ],
      "excerpt": "It's super easy to understand now. Now whether this part was crucial for the success of transformer? I doubt it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8700369727804105
      ],
      "excerpt": "Note: model dimension is basically the size of the embedding vector, baseline transformer used 512, the big one 1024 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.986695581462939
      ],
      "excerpt": "to a one-hot. Meaning 1 position out of 30k (or whatever your vocab size is) is set to 1. probability and everything else to 0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8862965956000496,
        0.881776066413868,
        0.9945021917548229,
        0.9546089193232696
      ],
      "excerpt": "In label smoothing instead of placing 1. on that particular position you place say 0.9 and you evenly distribute the rest of \nthe \"probability mass\" over the other positions  \n(that's visualized as a different shade of purple on the image above in a fictional vocab of size 4 - hence 4 columns) \nNote: Pad token's distribution is set to all zeros as we don't want our model to predict those! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9380796487308051,
        0.8809402517700294
      ],
      "excerpt": "What I did (for now) is I trained my models on the IWSLT dataset, which is much smaller, for the \nEnglish-German language pair, as I speak those languages so it's easier to debug and play around. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240097416123793
      ],
      "excerpt": "Some short translations from my German to English IWSLT model: <br/><br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8771137795178549,
        0.881296413674432,
        0.898953133420355
      ],
      "excerpt": "Which is actually pretty good! Maybe even better IMO than Google Translate's \"gold\" translation. \nThere are of course failure cases like this: <br/><br/> \nInput: Hey Alter, wie geht es dir? (How is it going dude?) <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8913361579638003,
        0.8237302296167657,
        0.9801241850881912,
        0.993546070178737
      ],
      "excerpt": "Which is actually also not completely bad! Because: \n* First of all the model was trained on IWSLT (TED like conversations) \n* \"Alter\" is a colloquial expression for old buddy/dude/mate but it's literal meaning is indeed age. \nSimilarly for the English to German model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.888665813116056
      ],
      "excerpt": "* --batch_size - this is important to set to a maximum value that won't give you CUDA out of memory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.941641452772086
      ],
      "excerpt": "Note: data loading is slow in torch text, and so I've implemented a custom wrapper which adds the caching mechanisms \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.976050736255501,
        0.849472168090565
      ],
      "excerpt": "The second part is all about playing with the models and seeing how they translate! <br/> \nTo get some translations start the translation_script.py, there is a couple of settings you'll want to set: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8168822728429154,
        0.8439775144758124
      ],
      "excerpt": "* --model_name - one of the pretrained model names: iwslt_e2g, iwslt_g2e or your model(*) \n* --dataset_name - keep this in sync with the model, IWSLT if the model was trained on IWSLT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723919823191312,
        0.9368537680836508
      ],
      "excerpt": "I'll link IWSLT pretrained model links here as well: English to German and German to English. \nThat's it you can also visualize the attention check out this section. for more info. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774829471226694
      ],
      "excerpt": "BLEU is an n-gram based metric for quantitatively evaluating the quality of machine translation models. <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8053456377550158,
        0.8794612792373779,
        0.9117446260792531
      ],
      "excerpt": "I got these using greedy decoding so it's a pessimistic estimate, I'll add beam decoding soon. \nImportant note: Initialization matters a lot for the transformer! I initially thought that other implementations \nusing Xavier initialization is again one of those arbitrary heuristics and that PyTorch default init will do - I was wrong: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8278077723609334,
        0.9309160788442655
      ],
      "excerpt": "That would give you some qualitative insight into how the transformer is doing, although I didn't do that. <br/> \nA similar thing is done when you have hard time quantitatively evaluating your model like in GANs and NST fields. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8763187810819286,
        0.8800892477249945,
        0.9373129463232899
      ],
      "excerpt": "model was \"paying attention to\" in the source and target sentences. \nHere are the attentions I get for the input sentence Ich bin ein guter Mensch, denke ich. \nThese belong to layer 6 of the encoder. You can see all of the 8 multi-head attention heads. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8579857273220579,
        0.946972081900051,
        0.9680003955572032
      ],
      "excerpt": "The 3rd type of MHA module is the source attending one and it looks similar to the plot you saw for the encoder. <br/> \nFeel free to play with it at your own pace! \nNote: there are obviously some bias problems with this model but I won't get into that analysis here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.856347014667896
      ],
      "excerpt": "* Beam decoding (turns out it's not that easy to implement this one!) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167013708048408
      ],
      "excerpt": "I have some more videos which could further help you understand transformers: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8194348163386976,
        0.8854574607277305
      ],
      "excerpt": "* Another overview of the paper (a bit higher level) \n* A case study of how this project was developed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "My implementation of the original transformer model (Vaswani et al.). I've additionally included the playground.py file for visualizing otherwise seemingly hard concepts. Currently included IWSLT pretrained models.",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://torchtext.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gordicaleksa/pytorch-original-transformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 82,
      "date": "Wed, 22 Dec 2021 23:32:34 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gordicaleksa/pytorch-original-transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "gordicaleksa/pytorch-original-transformer",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/gordicaleksa/pytorch-original-transformer/main/The%20Annotated%20Transformer%20%2B%2B.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "So we talked about what transformers are, and what they can do for you (among other things). <br/>\nLet's get this thing running! Follow the next steps:\n\n1. `git clone https://github.com/gordicaleksa/pytorch-original-transformer`\n2. Open Anaconda console and navigate into project directory `cd path_to_repo`\n3. Run `conda env create` from project directory (this will create a brand new conda environment).\n4. Run `activate pytorch-transformer` (for running scripts from your console or set the interpreter in your IDE)\n\nThat's it! It should work out-of-the-box executing environment.yml file which deals with dependencies. <br/>\nIt may take a while as I'm automatically downloading SpaCy's statistical models for English and German.\n\n-----\n\nPyTorch pip package will come bundled with some version of CUDA/cuDNN with it,\nbut it is highly recommended that you install a system-wide CUDA beforehand, mostly because of the GPU drivers. \nI also recommend using Miniconda installer as a way to get conda on your system.\nFollow through points 1 and 2 of [this setup](https://github.com/Petlja/PSIML/blob/master/docs/MachineSetup.md)\nand use the most up-to-date versions of Miniconda and CUDA/cuDNN for your system.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8864574716181884
      ],
      "excerpt": "Hardware requirements \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8210987140860949
      ],
      "excerpt": "You probably heard of transformers one way or another. GPT-3 and BERT to name a few well known ones :unicorn:. The main idea \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9927186821877624
      ],
      "excerpt": "Just do pip uninstall pywin32 and then either pip install pywin32 or conda install pywin32 should fix it! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8236076731187497
      ],
      "excerpt": "() Note: after you train your model it'll get dumped into models/binaries see what it's name is and specify it via \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8163974329216299
      ],
      "excerpt": "You can see here 3 runs, the 2 lower ones used PyTorch default initialization (one used mean for KL divergence \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8078581544916071
      ],
      "excerpt": "Just run tensorboard --logdir=runs from your Anaconda console and you can track your metrics during the training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326538498047761,
        0.8840059974749735
      ],
      "excerpt": "The repo already has everything it needs, these are just the bonus points. I've tested everything \nfrom environment setup, to automatic model download, etc. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9078564040380478
      ],
      "excerpt": "<img src=\"data/readme_pics/transformer_architecture.PNG\" width=\"350\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052924254802468
      ],
      "excerpt": "<img src=\"data/readme_pics/positional_encoding_formula.PNG\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017477711066736
      ],
      "excerpt": "Neither can I. Running the visualize_positional_encodings() function from playground.py we get this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9344641499119093
      ],
      "excerpt": "<img src=\"data/readme_pics/positional_encoding_visualized.jpg\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052924254802468
      ],
      "excerpt": "<img src=\"data/readme_pics/lr_formula.PNG\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052924254802468
      ],
      "excerpt": "<img src=\"data/readme_pics/custom_learning_rate_schedule.PNG\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078564040380478
      ],
      "excerpt": "<img src=\"data/readme_pics/label_smoothing.PNG\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8685948998720379
      ],
      "excerpt": "To run the training start the training_script.py, there is a couple of settings you will want to specify: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8998274345595775
      ],
      "excerpt": "python training_script.py --batch_size 1500 --dataset_name IWSLT --language_direction G2E \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8210266215504783
      ],
      "excerpt": "* Dump checkpoint .pth models into models/checkpoints/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8160731142741525
      ],
      "excerpt": "* Periodically write some training metadata to the console \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8447178455139249
      ],
      "excerpt": "* --model_name - one of the pretrained model names: iwslt_e2g, iwslt_g2e or your model(*) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078564040380478
      ],
      "excerpt": "<img src=\"data/readme_pics/bleu_score_xavier_vs_default_pt_init.PNG\" width=\"450\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078564040380478
      ],
      "excerpt": "<img src=\"data/readme_pics/attention_enc_self.PNG\" width=\"850\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078564040380478
      ],
      "excerpt": "<img src=\"data/readme_pics/attention_dec_self.PNG\" width=\"850\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gordicaleksa/pytorch-original-transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# The Original Transformer (PyTorch) :computer: = :rainbow:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-original-transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "gordicaleksa",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gordicaleksa/pytorch-original-transformer/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You really need a decent hardware if you wish to train the transformer on the **WMT-14** dataset.\n\nThe authors took:\n* **12h on 8 P100 GPUs** to train the baseline model and **3.5 days** to train the big one.\n\nIf my calculations are right that amounts to ~19 epochs (100k steps, each step had ~25000 tokens and WMT-14 has ~130M src/trg tokens)\nfor the baseline and 3x that for the big one (300k steps).\n\nOn the other hand it's much more feasible to train the model on the **IWSLT** dataset. It took me:\n* 13.2 min/epoch (1500 token batch) on my RTX 2080 machine (8 GBs of VRAM)\n* ~34 min/epoch (1500 token batch) on Azure ML's K80s (24 GBs of VRAM)\n\nI could have pushed K80s to 3500+ tokens/batch but had some CUDA out of memory problems.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 525,
      "date": "Wed, 22 Dec 2021 23:32:34 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "transformer",
      "transformers",
      "pytorch-transformer",
      "pytorch-transformers",
      "attention",
      "attention-mechanism",
      "attention-is-all-you-need",
      "pytorch",
      "python",
      "jupyter",
      "transformer-tutorial",
      "deeplearning",
      "deep-learning",
      "original-transformer"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You just need to link the Python environment you created in the [setup](#setup) section.\n\n",
      "technique": "Header extraction"
    }
  ]
}