{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1801.07698",
      "https://arxiv.org/abs/1905.00641"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find *InsightFace* useful in your research, please consider to cite the following related papers:\n\n```\n@inproceedings{deng2019retinaface,\ntitle={RetinaFace: Single-stage Dense Face Localisation in the Wild},\nauthor={Deng, Jiankang and Guo, Jia and Yuxiang, Zhou and Jinke Yu and Irene Kotsia and Zafeiriou, Stefanos},\nbooktitle={arxiv},\nyear={2019}\n}\n\n@inproceedings{guo2018stacked,\n  title={Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment},\n  author={Guo, Jia and Deng, Jiankang and Xue, Niannan and Zafeiriou, Stefanos},\n  booktitle={BMVC},\n  year={2018}\n}\n\n@article{deng2018menpo,\n  title={The Menpo benchmark for multi-pose 2D and 3D facial landmark localisation and tracking},\n  author={Deng, Jiankang and Roussos, Anastasios and Chrysos, Grigorios and Ververas, Evangelos and Kotsia, Irene and Shen, Jie and Zafeiriou, Stefanos},\n  journal={IJCV},\n  year={2018}\n}\n\n@inproceedings{deng2018arcface,\ntitle={ArcFace: Additive Angular Margin Loss for Deep Face Recognition},\nauthor={Deng, Jiankang and Guo, Jia and Niannan, Xue and Zafeiriou, Stefanos},\nbooktitle={CVPR},\nyear={2019}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{deng2018arcface,\ntitle={ArcFace: Additive Angular Margin Loss for Deep Face Recognition},\nauthor={Deng, Jiankang and Guo, Jia and Niannan, Xue and Zafeiriou, Stefanos},\nbooktitle={CVPR},\nyear={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{deng2018menpo,\n  title={The Menpo benchmark for multi-pose 2D and 3D facial landmark localisation and tracking},\n  author={Deng, Jiankang and Roussos, Anastasios and Chrysos, Grigorios and Ververas, Evangelos and Kotsia, Irene and Shen, Jie and Zafeiriou, Stefanos},\n  journal={IJCV},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{guo2018stacked,\n  title={Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment},\n  author={Guo, Jia and Deng, Jiankang and Xue, Niannan and Zafeiriou, Stefanos},\n  booktitle={BMVC},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{deng2019retinaface,\ntitle={RetinaFace: Single-stage Dense Face Localisation in the Wild},\nauthor={Deng, Jiankang and Guo, Jia and Yuxiang, Zhou and Jinke Yu and Irene Kotsia and Zafeiriou, Stefanos},\nbooktitle={arxiv},\nyear={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8650239057784687
      ],
      "excerpt": "By Jia Guo and Jiankang Deng \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9984258754360601,
        0.9861581105155569
      ],
      "excerpt": "2019.08.10: We achieved 2nd place at WIDER Face Detection Challenge 2019. \n2019.05.30: Presentation at cvmart \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966095042717282
      ],
      "excerpt": "2019.04.14: We will launch a Light-weight Face Recognition challenge/workshop on ICCV 2019. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9892740741085043,
        0.9196814658466638,
        0.8343848002233065
      ],
      "excerpt": "2019.02.08: Please check https://github.com/deepinsight/insightface/tree/master/recognition/ArcFace for our parallel training code which can easily and efficiently support one million identities on a single machine (8* 1080ti). \n2018.12.13: Inference acceleration TVM-Benchmark. \n2018.10.28: Light-weight attribute model Gender-Age. About 1MB, 10ms on single CPU core. Gender accuracy 96% on validation set and 4.1 age MAE. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "Deep Face Recognition \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298,
        0.8090016440670298,
        0.8090016440670298
      ],
      "excerpt": "       lfw.bin \n       cfp_fp.bin \n       agedb_30.bin \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283216015784888
      ],
      "excerpt": "| Method  | LFW(%) | CFP-FP(%) | AgeDB-30(%) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283216015784888
      ],
      "excerpt": "| Method           | m1   | m2   | m3   | LFW   | CFP-FP | AgeDB-30 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "TensorRT: wang-xinyu/tensorrtx \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vladimirwest/insightface_cinematic",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n[Jia Guo](guojia[at]gmail.com)\n[Jiankang Deng](jiankangdeng[at]gmail.com)\n```\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-25T20:12:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-22T17:09:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In this module, we provide training data, network settings and loss designs for deep face recognition.\nThe training data includes, but not limited to the cleaned MS1M, VGG2 and CASIA-Webface datasets, which were already packed in MXNet binary format.\nThe network backbones include ResNet, MobilefaceNet, MobileNet, InceptionResNet_v2, DenseNet, etc..\nThe loss functions include Softmax, SphereFace, CosineFace, ArcFace, Sub-Center ArcFace and Triplet (Euclidean/Angular) Loss.\n\nYou can check the detail page of our work [ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/ArcFace)(which accepted in CVPR-2019) and [SubCenter-ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/SubCenter-ArcFace)(which accepted in ECCV-2020).\n\n![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)\n\nOur method, ArcFace, was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this module, you can simply achieve LFW 99.83%+ and Megaface 98%+ by a single model. This module can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "InsightFace is an open source 2D&3D deep face analysis toolbox, mainly based on MXNet. \n\nThe master branch works with **MXNet 1.2 to 1.6**, with **Python 3.x**.\n\n\n ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8192505441185383,
        0.9455480971959047
      ],
      "excerpt": "By Jia Guo and Jiankang Deng \n2021-01-20: OneFlow based implementation of ArcFace and Partial-FC, here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8616908589111525
      ],
      "excerpt": "2020-08-01: We released lightweight facial landmark models with fast coordinate regression(106 points). See detail here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9265025199893884
      ],
      "excerpt": "2020.02.21: Instant discussion group created on QQ with group-id: 711302608. For English developers, see install tutorial here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9261702495439417
      ],
      "excerpt": "2019.04.30: Our Face detector (RetinaFace) obtains state-of-the-art results on the WiderFace dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8968302389720788
      ],
      "excerpt": "report (name: Imperial-000 and Imperial-001). Our solution is based on [MS1MV2+DeepGlintAsian, ResNet100, ArcFace loss].  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8450070941881584
      ],
      "excerpt": "2018.10.28: Light-weight attribute model Gender-Age. About 1MB, 10ms on single CPU core. Gender accuracy 96% on validation set and 4.1 age MAE. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906128909950556
      ],
      "excerpt": "- Introduction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815444321573754
      ],
      "excerpt": "This model can achieve LFW 99.83+ and MegaFace 98.3%+. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278860269777196
      ],
      "excerpt": "(4). Fine-turn the above Softmax model with Triplet loss. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9583229686355454
      ],
      "excerpt": "Please check Model-Zoo for more pretrained models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806386701014948
      ],
      "excerpt": "Results by using MS1M-IBUG(MS1M-V1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8617599490313199
      ],
      "excerpt": "In this part, we assume you are in the directory $INSIGHTFACE_ROOT/deploy/. The input face image should be generally centre cropped. We use RNet+ONet of MTCNN to further align the image before sending it to the feature embedding network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8927898519411741
      ],
      "excerpt": "Put the model under $INSIGHTFACE_ROOT/models/. For example, $INSIGHTFACE_ROOT/models/model-r100-ii. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9796814996914888
      ],
      "excerpt": "For single cropped face image(112x112), total inference time is only 17ms on our testing server(Intel E5-2660 @ 2.00GHz, Tesla M40, LResNet34E-IR). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152404983275265
      ],
      "excerpt": "RetinaFace is a practical single-stage SOTA face detector which is initially introduced in arXiv technical report and then accepted by CVPR 2020. We provide training code, training dataset, pretrained models and evaluation scripts.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9474057792153507,
        0.9293035924876928,
        0.9926479071459969
      ],
      "excerpt": "RetinaFaceAntiCov is an experimental module to identify face boxes with masks. Please check RetinaFaceAntiCov for detail. \nPlease check the Menpo Benchmark and our Dense U-Net for detail. We also provide other network settings such as classic hourglass. You can find all of training code, training dataset and evaluation scripts there. \nOn the other hand, in contrast to heatmap based approaches, we provide some lightweight facial landmark models with fast coordinate regression. The input of these models is loose cropped face image while the output is the direct landmark coordinates. See detail at alignment-coordinateReg. Now only pretrained models available. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vladimirwest/insightface_cinematic/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 22:08:35 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vladimirwest/insightface_cinematic/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "vladimirwest/insightface_cinematic",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/vladimirwest/insightface_cinematic/tree/main/recognition/partial_fc/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/evaluation/IJB/IJBB_Evaluation_MS1MV2.ipynb",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/evaluation/IJB/IJBB_Evaluation_VGG2.ipynb",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/evaluation/IJB/IJBC_Evaluation_VGG2.ipynb",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/evaluation/IJB/IJBC_Evaluation_MS1MV2.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/oneflow_face/tools/model_convert/mobilefacenet/of_model_2_mx.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/oneflow_face/tools/model_convert/mobilefacenet/mx_model_2_of.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/oneflow_face/tools/model_convert/resnet100/of_model_2_mx.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/oneflow_face/tools/model_convert/resnet100/mx_model_2_of.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/oneflow_face/tools/dataset_convert/bin_2_ofrecord.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/partial_fc/mxnet/config.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/partial_fc/mxnet/run.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/partial_fc/mxnet/setup-utils/install-horovod.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/partial_fc/mxnet/setup-utils/install-mpi.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/partial_fc/mxnet/evaluation/example.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/partial_fc/pytorch/run.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/recognition/partial_fc/pytorch/IJB/example.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/evaluation/Megaface/run.sh",
      "https://raw.githubusercontent.com/vladimirwest/insightface_cinematic/main/evaluation/IJB/example.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9977752388963443,
        0.998394942772833
      ],
      "excerpt": "Install MXNet with GPU support (Python 3.X). \npip install mxnet-cu101 #: which should match your installed cuda version \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9799489993452795
      ],
      "excerpt": "git clone --recursive https://github.com/deepinsight/insightface.git \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8385975937262083
      ],
      "excerpt": "For training with m1=1.0, m2=0.3, m3=0.2, run following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322609392449874,
        0.9322609392449874
      ],
      "excerpt": "PyTorch: InsightFace_Pytorch \nPyTorch: arcface-pytorch \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8668937975765546,
        0.8589534893990137,
        0.8245539886860519
      ],
      "excerpt": "- Training Data \n- Train \n- Pretrained Models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8287107764023556
      ],
      "excerpt": "Please check src/data/face2rec2.py on how to build a binary face dataset. You can either choose MTCNN or RetinaFace to align the faces. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.8589534893990137
      ],
      "excerpt": "       train.idx \n       train.rec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8927905239057822,
        0.8300446489733351
      ],
      "excerpt": "cp sample_config.py config.py \nvim config.py #: edit dataset path etc.. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8278121020599903
      ],
      "excerpt": "(1). Train ArcFace with LResNet100E-IR. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850032781798308,
        0.8366887577325904
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore \nIt will output verification results of LFW, CFP-FP and AgeDB-30 every 2000 batches. You can check all options in config.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8278121020599903
      ],
      "excerpt": "(2). Train CosineFace with LResNet50E-IR. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850032781798308,
        0.8278121020599903
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r50 --loss cosface --dataset emore \n(3). Train Softmax with LMobileNet-GAP. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850032781798308
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network m1 --loss softmax --dataset emore \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8847134236737256
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network m1 --loss triplet --lr 0.005 --pretrained ./models/m1-softmax-emore,1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9209739772085854
      ],
      "excerpt": "You can use $INSIGHTFACE/src/eval/verification.py to test all the pre-trained models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184235793538223
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss combined --dataset emore \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8667726685541095,
        0.9095062552484893
      ],
      "excerpt": "Put the model under $INSIGHTFACE_ROOT/models/. For example, $INSIGHTFACE_ROOT/models/model-r100-ii. \nRun the test script $INSIGHTFACE_ROOT/deploy/test.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8420227656543805
      ],
      "excerpt": "TensorFlow: tf-insightface \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vladimirwest/insightface_cinematic/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "C",
      "Cuda",
      "C++",
      "Shell",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 MTlab, Meitu Inc.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "InsightFace: 2D and 3D Face Analysis Project",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "insightface_cinematic",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "vladimirwest",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vladimirwest/insightface_cinematic/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 22:08:35 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[![ArcFace Demo](https://github.com/deepinsight/insightface/blob/master/resources/facerecognitionfromvideo.PNG)](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s)\n\nPlease click the image to watch the Youtube video. For Bilibili users, click [here](https://www.bilibili.com/video/av38041494?from=search&seid=11501833604850032313).\n\n",
      "technique": "Header extraction"
    }
  ]
}