{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1911.00720"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use or extend our work, please cite our paper at Findings of EMNLP-2020.\n\n```\n@inproceedings{tian-etal-2020-improving,\n    title = \"Improving Constituency Parsing with Span Attention\",\n    author = \"Tian, Yuanhe and Song, Yan and Xia, Fei and Zhang, Tong\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    pages = \"1691--1703\",\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{tian-etal-2020-improving,\n    title = \"Improving Constituency Parsing with Span Attention\",\n    author = \"Tian, Yuanhe and Song, Yan and Xia, Fei and Zhang, Tong\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    pages = \"1691--1703\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9782722018136596
      ],
      "excerpt": "Please contact us at yhtian@uw.edu or cuhksz.nlp@gmail.com if you have any questions. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cuhksz-nlp/SAPar",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-12T05:36:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-19T13:25:06Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9986335121614994
      ],
      "excerpt": "This is the implementation of Constituency Parsing with Span Attention at Findings of EMNLP2020. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9129870479204509
      ],
      "excerpt": "To preprocess the data, please go to data_processing directory and follow the instruction to process the data. You need to obtain the official datasets yourself before running our code. \n",
      "technique": "Supervised classification"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In our paper, we use [BERT](https://www.aclweb.org/anthology/N19-1423/), [ZEN](https://arxiv.org/abs/1911.00720), and [XLNet](https://arxiv.org/pdf/1906.08237.pdf) as the encoder.\n\nFor BERT, please download pre-trained BERT model from [Google](https://github.com/google-research/bert) and convert the model from the TensorFlow version to PyTorch version. \n* For Arabic, we use MulBERT-Base, Multilingual Cased.\n* For Chinese, we use BERT-Base, Chinese;\n* For English, we use BERT-Large, Cased and BERT-Large, Uncased.\n\nFor ZEN, you can download the pre-trained model from [here](https://github.com/sinovation/ZEN).\n\nFor XLNet, you can download the pre-trained model from [here](https://github.com/zihangdai/xlnet).\n\nFor our pre-trained model, you can download them from [Baidu Wangpan](https://pan.baidu.com/s/1iSUcfRHccrgGmc2GEsDDBw) (passcode: 2o1n) or [Google Drive](https://drive.google.com/drive/folders/1-wINl7lLtlT0WEX88NPwyBHZOr4yKnCK?usp=sharing).\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cuhksz-nlp/SAPar/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 09:31:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cuhksz-nlp/SAPar/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cuhksz-nlp/SAPar",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/cuhksz-nlp/SAPar/master/run.sh",
      "https://raw.githubusercontent.com/cuhksz-nlp/SAPar/master/data_processing/get_atb.sh",
      "https://raw.githubusercontent.com/cuhksz-nlp/SAPar/master/data_processing/get_ctb.sh",
      "https://raw.githubusercontent.com/cuhksz-nlp/SAPar/master/data_processing/get_ptb.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8836317475360358
      ],
      "excerpt": "You can find the command lines to train and test models on a specific dataset in run.sh. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8045490032128066
      ],
      "excerpt": "Ideally, all data will appear in ./data directory. The data with gold POS tags are located in folders whose name is the same as the dataset name (i.e., ATB, CTB, and PTB); the data with predicted POS tags are located in folders whose name has a \"_POS\" suffix (i.e., ATB_POS, CTB_POS, and PTB_POS). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cuhksz-nlp/SAPar/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C",
      "Shell",
      "Scilab",
      "Makefile",
      "Perl"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'This is free and unencumbered software released into the public domain.\\n\\nAnyone is free to copy, modify, publish, use, compile, sell, or\\ndistribute this software, either in source code form or as a compiled\\nbinary, for any purpose, commercial or non-commercial, and by any\\nmeans.\\n\\nIn jurisdictions that recognize copyright laws, the author or authors\\nof this software dedicate any and all copyright interest in the\\nsoftware to the public domain. We make this dedication for the benefit\\nof the public at large and to the detriment of our heirs and\\nsuccessors. We intend this dedication to be an overt act of\\nrelinquishment in perpetuity of all present and future rights to this\\nsoftware under copyright law.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\\nIN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR\\nOTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\\nOTHER DEALINGS IN THE SOFTWARE.\\n\\nFor more information, please refer to http://unlicense.org/\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SAPar",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SAPar",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cuhksz-nlp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cuhksz-nlp/SAPar/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* `python 3.6`\n* `pytorch 1.1`\n\nInstall python dependencies by running:\n\n`\npip install -r requirements.txt\n`\n\n`EVALB` and `EVALB_SPMRL` contain the code to evaluate the parsing results for English and other languages. Before running evaluation, you need to go to the `EVALB` (for English) or `EVALB_SPMRL` (for other languages) and run `make`.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To train a model on a small dataset, run:\n\n`\n./run.sh\n`\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Thu, 30 Dec 2021 09:31:49 GMT"
    },
    "technique": "GitHub API"
  }
}