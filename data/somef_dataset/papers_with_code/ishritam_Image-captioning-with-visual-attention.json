{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Tensorflow Tutorials: https://www.tensorflow.org/tutorials/text/image_captioning\n* CS231n: Convolutional Neural Networks for Visual Recognition by Andrej Karpathy: https://www.youtube.com/watch?v=NfnWJUyUJYU&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC\n* Show, Attend and Tell: Neural Image Caption\nGeneration with Visual Attention: https://arxiv.org/pdf/1502.03044.pdf\n* Seq to seq model by Andrew Ng: https://www.youtube.com/watch?v=Q8ys8YnDRXM&list=PL1w8k37X_6L_s4ncq-swTBvKDWnRSrinI\n* Attention Is All You Need: https://arxiv.org/pdf/1706.03762v5.pdf\n\n* Show, Attend and Tell Paper presentation: https://www.youtube.com/watch?v=ENVGHs3yw7k&t=454s\n\n* https://distill.pub/2016/augmented-rnns/\n* https://fairyonice.github.io/Develop_an_image_captioning_deep_learning_model_using_Flickr_8K_data.html#Add-start-and-end-sequence-tokens\n\n* BLEU score: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ishritam/Image-captioning-with-visual-attention",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-11T09:45:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-16T18:44:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9475255809584737,
        0.9930998712534362
      ],
      "excerpt": "To build networks capable of perceiving contextual subtleties in images, to relate observations to both the scene and the real world, and to output succinct and accurate image descriptions; all tasks that we as people can do almost effortlessly. \nDeep Learning is a very rampant field right now \u2013 with so many applications coming out day by day. In this case study, I have made an Image Captioning refers to the process of generating textual description from an image \u2013 based on the objects and actions in the image. For example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8968096904851869
      ],
      "excerpt": "Incorrect caption could impact the negative impression on user. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9705863397947325
      ],
      "excerpt": "Flilckr8K contains 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333888449109152
      ],
      "excerpt": "Flickr8k_text: Contains all the captions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9963543117661952,
        0.9635478021036623
      ],
      "excerpt": "The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is.  \nBLEU is a well-acknowledged metric to measure the similarly of one hypothesis sentence to multiple reference sentences. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "To build networks capable of perceiving contextual subtleties in images, to relate observations to both the scene and the real world, and to output succinct and accurate image descriptions; all tasks that we as people can do almost effortlessly.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ishritam/Image-captioning-with-visual-attention/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Image captioning is an interesting problem, where we can learn both computer vision techniques and natural language processing techniques. In case study I have followed Show, Attend and Tell: Neural Image Caption Generation with Visual Attention and create an image caption generation model using Flicker 8K data. This model takes a single image as input and output the caption to this image and read that predicted caption.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To accomplish this, we'll use an attention-based model, which enables us to see what parts of the image the model focuses on as it generates a caption.\n\u201c**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**\u201d by Xu et al. (2015) \u2014 the first paper, to our knowledge, that introduced the concept of attention into image captioning. The work takes inspiration from attention\u2019s application in other sequence and image recognition problems.\n![image](https://user-images.githubusercontent.com/40149802/70611854-dd49e380-1c2b-11ea-9890-cdcb691d11ff.png)\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 29 Dec 2021 07:42:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ishritam/Image-captioning-with-visual-attention/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ishritam/Image-captioning-with-visual-attention",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ishritam/Image-captioning-with-visual-attention/master/Image%20Captioning%20Tensorflow%202.0.ipynb",
      "https://raw.githubusercontent.com/ishritam/Image-captioning-with-visual-attention/master/final.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.844186078448035
      ],
      "excerpt": "Predict a correct caption as per the input image. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ishritam/Image-captioning-with-visual-attention/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Image-captioning-with-visual-attention",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Image-captioning-with-visual-attention",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ishritam",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ishritam/Image-captioning-with-visual-attention/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3\n* Tensorflow 2.0\n* gtts\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 07:42:33 GMT"
    },
    "technique": "GitHub API"
  }
}