{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1607.05666\n\n<img src=\"images/PCEN and DB.png\" width=\"70%\"/>\n\n## Network Architecture\nThis solution borrows heavily from an approach of the winners of the 2018 competition. Their technical report is here:\nhttp://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Jeong_102.pdf\n\nTheir implementation here:\nhttps://github.com/finejuly/dcase2018_task2_cochlearai\n\nEssentially the architecture is a densely connected CNN with Squeeze-Exitation blocks.\n\n<img src=\"images/high_level_arch.png\" width=\"25%\"/><img src=\"images/block_arch.png\" width=\"70%\"/>\n\nThe Squeeze and Exitation block described here: https://arxiv.org/abs/1709.01507\n<img src=\"images/se_arch.png\" width=\"70%\"/>\n\n## Data augmentation\nThe following data augmentations were used:\n1. Random selection of 4 second subset of audio clip\n2. MixUp - https://arxiv.org/abs/1710.09412\n3. Mild zooming and warping\n4. Test Time Augmentation\n\n## Training Approach\n1. Train a CNN on the curated set using 6-fold cross validation\n2. Use this model to predict the entire noisy dataset\n3. Pick a class-balanced subset of examples from the noisy dataset where the predictions are reasonably good\n4. Finetune the model trained on the curated set on the combination of curated and selected noisy subset\n\n## Submission\nThis competition was run as a kernels only competition. The kernel had a maximum runtime of one hour. Available disk space was limited to ~5Gb and there was ~14Gb of memory available.\n\nIn order for the kernel to run more quickly spectrograms are loaded into memory in a shared dictionary. Memory usage is monitored and once it reaches 95% usage, spectrograms are written to disk instead of being added to memory.\n\n## Result\n44th of 808 participants - Top 5% - Silver Medal :",
      "https://arxiv.org/abs/1709.01507\n<img src=\"images/se_arch.png\" width=\"70%\"/>\n\n## Data augmentation\nThe following data augmentations were used:\n1. Random selection of 4 second subset of audio clip\n2. MixUp - https://arxiv.org/abs/1710.09412\n3. Mild zooming and warping\n4. Test Time Augmentation\n\n## Training Approach\n1. Train a CNN on the curated set using 6-fold cross validation\n2. Use this model to predict the entire noisy dataset\n3. Pick a class-balanced subset of examples from the noisy dataset where the predictions are reasonably good\n4. Finetune the model trained on the curated set on the combination of curated and selected noisy subset\n\n## Submission\nThis competition was run as a kernels only competition. The kernel had a maximum runtime of one hour. Available disk space was limited to ~5Gb and there was ~14Gb of memory available.\n\nIn order for the kernel to run more quickly spectrograms are loaded into memory in a shared dictionary. Memory usage is monitored and once it reaches 95% usage, spectrograms are written to disk instead of being added to memory.\n\n## Result\n44th of 808 participants - Top 5% - Silver Medal :",
      "https://arxiv.org/abs/1710.09412\n3. Mild zooming and warping\n4. Test Time Augmentation\n\n## Training Approach\n1. Train a CNN on the curated set using 6-fold cross validation\n2. Use this model to predict the entire noisy dataset\n3. Pick a class-balanced subset of examples from the noisy dataset where the predictions are reasonably good\n4. Finetune the model trained on the curated set on the combination of curated and selected noisy subset\n\n## Submission\nThis competition was run as a kernels only competition. The kernel had a maximum runtime of one hour. Available disk space was limited to ~5Gb and there was ~14Gb of memory available.\n\nIn order for the kernel to run more quickly spectrograms are loaded into memory in a shared dictionary. Memory usage is monitored and once it reaches 95% usage, spectrograms are written to disk instead of being added to memory.\n\n## Result\n44th of 808 participants - Top 5% - Silver Medal :"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8821565366821446
      ],
      "excerpt": "Acoustic Scenes and Events (DCASE) 2019: http://dcase.community/challenge2019/task-audio-tagging \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9656206675494673
      ],
      "excerpt": "The Squeeze and Exitation block described here: https://arxiv.org/abs/1709.01507 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944484218006108
      ],
      "excerpt": "2. MixUp - https://arxiv.org/abs/1710.09412 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simongrest/kaggle-freesound-audio-tagging-2019",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-30T16:43:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-15T11:12:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9106694148557912
      ],
      "excerpt": "The task in this competition is drawn from the second task from the Detection and Classification of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8645988874966074,
        0.9637767694077353
      ],
      "excerpt": "The task is a multi-label classification problem, audio samples need to be tagged with one or more of 80 labels drawn from Google's AudioSet Ontology. \nThis competition is based on two datasets: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877462096314007,
        0.9584546184597504,
        0.9774077202745377,
        0.9512722516379237
      ],
      "excerpt": "A larger dataset with approximately 20,000 audio files drawn from videos on Flickr and labelled automatically based on tags and other meta-data. \nThe key part of this competition is to figure out how to use effectively use the larger noisy dataset. In order to do so, one needs to address the noisy labels and differences in the domain between the datasets. From the DCASE task description: \nThe main research question addressed in this task is how to adequately exploit a small amount of reliable, manually-labeled data, and a larger quantity of noisy web audio data in a multi-label audio tagging task with a large vocabulary setting. In addition, since the data comes from different sources, the task encourages domain adaptation approaches to deal with a potential domain mismatch. \nFor this solution, audio files were transformed to a spectral or frequency representation. The mel-scale is used for the spectrograms. Instead of a decibel front-end a PCEN front-end is used - see https://arxiv.org/abs/1607.05666 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.945827921842526
      ],
      "excerpt": "This solution borrows heavily from an approach of the winners of the 2018 competition. Their technical report is here: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9340749852293957
      ],
      "excerpt": "Essentially the architecture is a densely connected CNN with Squeeze-Exitation blocks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8390785956318405
      ],
      "excerpt": "1. Random selection of 4 second subset of audio clip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "3. Mild zooming and warping \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.950957569117392,
        0.8821355750111672,
        0.9671487108858715,
        0.9088633456253394
      ],
      "excerpt": "Finetune the model trained on the curated set on the combination of curated and selected noisy subset \nThis competition was run as a kernels only competition. The kernel had a maximum runtime of one hour. Available disk space was limited to ~5Gb and there was ~14Gb of memory available. \nIn order for the kernel to run more quickly spectrograms are loaded into memory in a shared dictionary. Memory usage is monitored and once it reaches 95% usage, spectrograms are written to disk instead of being added to memory. \n44th of 808 participants - Top 5% - Silver Medal :) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "44th Place solution to the Freesound Audio Tagging 2019 Competition",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simongrest/kaggle-freesound-audio-tagging-2019/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 20 Dec 2021 15:27:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simongrest/kaggle-freesound-audio-tagging-2019/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "simongrest/kaggle-freesound-audio-tagging-2019",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/simongrest/kaggle-freesound-audio-tagging-2019/master/training.ipynb",
      "https://raw.githubusercontent.com/simongrest/kaggle-freesound-audio-tagging-2019/master/submission.ipynb",
      "https://raw.githubusercontent.com/simongrest/kaggle-freesound-audio-tagging-2019/master/make_predictions_for_entire_noisy_set.ipynb",
      "https://raw.githubusercontent.com/simongrest/kaggle-freesound-audio-tagging-2019/master/trainable-pcen-frontend-in-pytorch.ipynb",
      "https://raw.githubusercontent.com/simongrest/kaggle-freesound-audio-tagging-2019/master/network_structure.ipynb",
      "https://raw.githubusercontent.com/simongrest/kaggle-freesound-audio-tagging-2019/master/exploratory_data_analysis.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8714100753555479
      ],
      "excerpt": "<img src=\"http://d33wubrfki0l68.cloudfront.net/98c159a16704dac8b3861c3a5c7672bb5ce15656/696eb/images/tasks/challenge2019/task2_freesound_audio_tagging.png\" width=\"50%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589672000755753
      ],
      "excerpt": "<img src=\"images/PCEN and DB.png\" width=\"70%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061608775733352
      ],
      "excerpt": "<img src=\"images/high_level_arch.png\" width=\"25%\"/><img src=\"images/block_arch.png\" width=\"70%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8760414441734843
      ],
      "excerpt": "<img src=\"images/se_arch.png\" width=\"70%\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simongrest/kaggle-freesound-audio-tagging-2019/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Kaggle Freesound Audio Tagging Competition",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "kaggle-freesound-audio-tagging-2019",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "simongrest",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simongrest/kaggle-freesound-audio-tagging-2019/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Mon, 20 Dec 2021 15:27:14 GMT"
    },
    "technique": "GitHub API"
  }
}