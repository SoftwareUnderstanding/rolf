{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.02325",
      "https://arxiv.org/abs/1409.1556"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [1] Wei Liu, et al. SSD: Single Shot MultiBox Detector. [ECCV2016]((http://arxiv.org/abs/1512.02325)).\n- [2] S. Saha, G. Singh, M. Sapienza, P. H. S. Torr, and F. Cuzzolin, Deep learning for detecting multiple space-time action tubes in videos. BMVC 2016 \n- [3] X. Peng and C. Schmid. Multi-region two-stream R-CNN for action detection. ECCV 2016\n- [4] G. Singh, S Saha, M. Sapienza, P. H. S. Torr and F Cuzzolin. Online Real time Multiple Spatiotemporal Action Localisation and Prediction. ICCV, 2017.\n- [5] Kalogeiton, V., Weinzaepfel, P., Ferrari, V. and Schmid, C., 2017. Action Tubelet Detector for Spatio-Temporal Action Localization. ICCV, 2017.\n- [Original SSD Implementation (CAFFE)](https://github.com/weiliu89/caffe/tree/ssd)\n- A huge thanks to Max deGroot, Ellis Brown for Pytorch implementation of [SSD](https://github.com/amdegroot/ssd.pytorch)\n \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If this work has been helpful in your research please consider citing [1] and [4]\n\n      @inproceedings{singh2016online,\n        title={Online Real time Multiple Spatiotemporal Action Localisation and Prediction},\n        author={Singh, Gurkirt and Saha, Suman and Sapienza, Michael and Torr, Philip and Cuzzolin, Fabio},\n        jbooktitle={ICCV},\n        year={2017}\n      }\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9872118697333594
      ],
      "excerpt": "<a href='#citation'>Citation</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9697530585149285
      ],
      "excerpt": "    <td align=\"left\">Peng et al [3] RGB+BroxFLOW </td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894110301647197
      ],
      "excerpt": "    <td align=\"left\">Saha et al [2] RGB+BroxFLOW </td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    <td>36.37</td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9553055233818778
      ],
      "excerpt": "    <td align=\"left\">Singh et al [4] RGB+FastFLOW </td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "    <td>14.10</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9553055233818778
      ],
      "excerpt": "    <td align=\"left\">Singh et al [4] RGB+BroxFLOW </td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    <td>46.30</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    <td> 91.12 </td>   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "    <td>64.35</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    <td>12.23</td> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gurkirt/realtime-action-detection",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-07-30T21:42:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-03T16:19:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9839915462165749,
        0.9601170799978603,
        0.9004995594206522
      ],
      "excerpt": "NEWS Feb 2021: I released 3D-RetineNet, which is purly pytorch and python code. Linking and trimming is also implemented in python, however on actioness scores. Hope on to 3D-RetineNet repo, it might be more usful than this one for UCF24 dataset in some cases. \nAn implementation of our work (Online Real-time Multiple Spatiotemporal Action Localisation and Prediction) published in ICCV 2017. \nOriginally, we used Caffe implementation of SSD-V2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9619548916501272,
        0.9603512840602028,
        0.9920132518196928
      ],
      "excerpt": "This implementation is bit off from original work. It works slightly, better on lower IoU and higher IoU and vice-versa. \nTube generation part in original implementations as same as this. I found that this implementation of SSD is slight worse @ IoU greater or equal to 0.5 in context of the UCF24 dataset.  \nI decided to release the code with PyTorch implementation of SSD,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9013791698131022,
        0.926231569209506
      ],
      "excerpt": "We build on Pytorch implementation of SSD by Max deGroot, Ellis Brown. \nWe made few changes like (different learning rate for bias and weights during optimization) and simplified some parts to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9506449299937187
      ],
      "excerpt": "Not everything is verified with current except single stream rgb training and testing, but everything should work alright. Here is the link to the previous version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490841536791229
      ],
      "excerpt": "UCF24DETECTION is a dataset loader Class in data/ucf24.py that inherits torch.utils.data.Dataset making it fully compatible with the torchvision.datasets API. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8291745798936508
      ],
      "excerpt": "During training checkpoint is saved every 10K iteration also log it's frame-level frame-mean-ap on a subset of 22k test images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8739462440069292
      ],
      "excerpt": "To evaluate on optical flow models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8281982630693765
      ],
      "excerpt": "NOTE: I01onlineTubes and I02genFusedTubes not only produce video-level mAP; they also produce video-level classification accuracy on 24 classes of UCF24. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915155772541204,
        0.9955316701809707,
        0.9531191821363978
      ],
      "excerpt": "but their ap computation from precision and recall is slightly different. \nThe table below is similar to table 1 in our paper. It contains more info than \nthat in the paper, mostly about this implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9709809087059526
      ],
      "excerpt": "There is an effect due to the choice of learning rate and the number of iterations the model is trained. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.901478540444451,
        0.9860976308411931
      ],
      "excerpt": "lower IoU threshold, which is done in this case. \nIn original work using caffe implementation of SSD, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9272859491121677,
        0.8734078151571438,
        0.8379210228862942,
        0.8220665962782716,
        0.916210305124579,
        0.8003220580808791
      ],
      "excerpt": "In this implementation, all the models are trained for 120K \niterations, the initial learning rate is set to 0.0005 and learning is dropped by the factor of 5 after 70K and 90K iterations. \nKalogeiton et al. [5] make use mean fusion, so I thought we could try in our pipeline which was very easy to incorporate. \nIt is evident from above table that mean fusion performs better than other fusion techniques. \nAlso, their method relies on multiple frames as input in addition to post-processing of bounding box coordinates at tubelet level. \nThis implementation is mainly focused on producing the best numbers (mAP) in the simplest manner, it can be modified to run faster. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9225930060956147
      ],
      "excerpt": " - Most of the time spent during tube generations is taken by disc operations; which can be eliminated completely. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8315343901356778,
        0.8512885411396205
      ],
      "excerpt": "I presented the timing of individual components in the paper, which still holds true. \nThanks to Zhujiagang, a matlab version of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repository  host the code for real-time action detection paper ",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Currently, we provide the following PyTorch models: \n    * SSD300 trained on ucf24 ; available from my [google drive](https://drive.google.com/drive/folders/1Z42S8fQt4Amp1HsqyBOoHBtgVKUzJuJ8?usp=sharing)\n      - appearence model trained on rgb-images (named `rgb-ssd300_ucf24_120000`)\n      - accurate flow model trained on brox-images (named `brox-ssd300_ucf24_120000`)\n      - real-time flow model trained on fastOF-images (named `fastOF-ssd300_ucf24_120000`)    \n- These models can be used to reproduce above table which is almost identical in our [paper](https://arxiv.org/pdf/1611.08563.pdf) \n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gurkirt/realtime-action-detection/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 97,
      "date": "Sat, 25 Dec 2021 07:16:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gurkirt/realtime-action-detection/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "gurkirt/realtime-action-detection",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install [PyTorch](http://pytorch.org/)(version v0.3) by selecting your environment on the website and running the appropriate command.\n- Please install cv2 as well. I recommend using anaconda 3.6 and it's opnecv package.\n- You will also need Matlab. If you have distributed computing license then it would be faster otherwise it should also be fine. \nJust replace `parfor` with simple `for` in Matlab scripts. I would be happy to accept a PR for python version of this part.\n- Clone this repository. \n  * Note: We currently only support Python 3+ with Pytorch version v0.2 on Linux system.\n- We currently only support [UCF24](http://www.thumos.info/download.html) with [revised annotaions](https://github.com/gurkirt/corrected-UCF101-Annots) released with our paper, we will try to add [JHMDB21](http://jhmdb.is.tue.mpg.de/) as soon as possible, but can't promise, you can check out our [BMVC2016 code](https://bitbucket.org/sahasuman/bmvc2016_code) to get started your experiments on JHMDB21.\n- To simulate the same training and evaluation setup we provide extracted `rgb` images from videos along with optical flow images (both `brox flow` and `real-time flow`) computed for the UCF24 dataset.\nYou can download it from my [google drive link](https://drive.google.com/file/d/1o2l6nYhd-0DDXGP-IPReBP4y1ffVmGSE/view?usp=sharing)\n- We also support [Visdom](https://github.com/facebookresearch/visdom) for visualization of loss and frame-meanAP on subset during training.\n  * To use Visdom in the browser: \n  ```Shell\n  #: First install Python server and client \n  pip install visdom\n  #: Start the server (probably in a screen or tmux)\n  python -m visdom.server --port=8097\n  ```\n  * Then (during training) navigate to http://localhost:8097/ (see the Training section below for more details).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8223564515868579
      ],
      "excerpt": "for publication. I have forked the version of SSD-CAFFE which I used to generate results for paper, you try that if you want to use caffe. You can use that repo if like caffe other I would recommend using this version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8287955725858237
      ],
      "excerpt": "We build on Pytorch implementation of SSD by Max deGroot, Ellis Brown. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589169901347673
      ],
      "excerpt": "The previous version was in pytorch 0.2. The current one works on pytorch 1.2.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9287467277036058
      ],
      "excerpt": "<a href='#installation'>Installation</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8943583843711341
      ],
      "excerpt": "By default, we assume that you have downloaded that dataset.     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9229896420955709
      ],
      "excerpt": "Let's assume that you extracted dataset in /home/user/ucf24/ directory then your train command from the root directory of this repo is going to be:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9574979005657257,
        0.8773952612615806
      ],
      "excerpt": "For instructions on Visdom usage/installation, see the <a href='#installation'>Installation</a> section. By default, it is off. \nIf you don't like to use visdom then you always keep track of train using logfile which is saved under save_root directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8381552309669075
      ],
      "excerpt": "To compute frame-mAP you can use frameAP.m script. You will need to specify data_root, data_root. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9267542305056462
      ],
      "excerpt": " - NMS is performed once in python then again in Matlab; one has to do that on GPU in python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8766527512859791
      ],
      "excerpt": "Also, Feynman27 pushed a python version of the incremental_linking \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "<a href='#training-ssd'>Training SSD</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717718307432916
      ],
      "excerpt": "To train SSD using the training script simply specify the parameters listed in train-ucf24.py as a flag or manually change them. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8560616667916674
      ],
      "excerpt": "To eval SSD using the test script simply specify the parameters listed in test-ucf24.py as a flag or manually change them. for e.g.: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597
      ],
      "excerpt": "    <td>15.86</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8641699875385698
      ],
      "excerpt": "    <td>75.01</td> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gurkirt/realtime-action-detection/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "MATLAB",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/gurkirt/realtime-action-detection/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Gurkirt Singh\\nThis is an adaption of Max deGroot, Ellis Brown originl code of SSD for VOC dataset\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Real-time online Action Detection: ROAD",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "realtime-action-detection",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "gurkirt",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gurkirt/realtime-action-detection/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 282,
      "date": "Sat, 25 Dec 2021 07:16:53 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "action-recognition",
      "action-detection",
      "ssd",
      "pytorch",
      "online",
      "real-time",
      "ucf101",
      "detection"
    ],
    "technique": "GitHub API"
  }
}