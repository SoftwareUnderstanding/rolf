{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1903.07803",
      "https://arxiv.org/abs/1505.04597",
      "https://arxiv.org/abs/1903.07803",
      "https://arxiv.org/abs/1505.04597",
      "https://arxiv.org/abs/1903.07803"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. J. Staal, M. Abramoff, M. Niemeijer, M. Viergever, and B. van Ginneken, \u201cRidge based vessel segmentation in color\nimages of the retina,\u201d IEEE Transactions on Medical Imaging 23, 501\u2013509 (2004)\n2. O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d inMICCAI,(2015)\n3. Dynamic Deep Networks for Retinal Vessel Segmentation, https://arxiv.org/abs/1903.07803\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9989922930825649
      ],
      "excerpt": "deepdyn/testarch Full end to end working u-net(Olaf Ronneberger et al.) and MINI-UNET as per Deep Dynamic(https://arxiv.org/abs/1903.07803) paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.92644608324469
      ],
      "excerpt": "Epochs[1/40] Batch[10/34] loss:0.34364 pre:0.584 rec:0.638 f1:0.610 acc:0.912 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326585122547221
      ],
      "excerpt": "Epochs[2/40] Batch[10/34] loss:0.27762 pre:0.999 rec:0.025 f1:0.049 acc:0.916 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sraashis/deepdyn",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-09-11T19:33:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-08T09:42:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8112546733189808
      ],
      "excerpt": "deepdyn/torchtrainer framework core. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8186982349464621
      ],
      "excerpt": "for more robust retinal image segmentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.931785920846569
      ],
      "excerpt": "Original image and respective ground-truth image. Ground-truth is a binary image with each vessel pixel(white) 255 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9220421327951328
      ],
      "excerpt": "Figure above is the validation F1 and Accuracy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "pytorch implementation of paper https://www.frontiersin.org/articles/10.3389/fcomp.2020.00035/full",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sraashis/deepdyn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 20,
      "date": "Sun, 26 Dec 2021 09:48:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sraashis/deepdyn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sraashis/deepdyn",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sraashis/deepdyn/master/NewF1.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8231352644448148
      ],
      "excerpt": "Along with example of unet for DRIVE dataset segmentation [1]. DRIVE dataset is composed of 40 retinal fundus images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333384803827206,
        0.8934745846565956
      ],
      "excerpt": "workstation$ python main.py  \nTotal Params: 31042434 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8239847268959134
      ],
      "excerpt": "Epochs[1/40] Batch[15/34] loss:0.22827 pre:0.804 rec:0.565 f1:0.664 acc:0.939 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8071363535973313,
        0.8068494514088058,
        0.8068494514088058,
        0.8068494514088058,
        0.8068494514088058,
        0.8068494514088058
      ],
      "excerpt": "Running validation.. \n21_training.tif  PRF1A [0.66146, 0.37939, 0.4822, 0.93911] \n39_training.tif  PRF1A [0.79561, 0.28355, 0.41809, 0.93219] \n37_training.tif  PRF1A [0.78338, 0.47221, 0.58924, 0.94245] \n35_training.tif  PRF1A [0.83836, 0.45788, 0.59228, 0.94534] \n38_training.tif  PRF1A [0.64682, 0.26709, 0.37807, 0.92416] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8239847268959134
      ],
      "excerpt": "Epochs[2/40] Batch[15/34] loss:0.25742 pre:0.982 rec:0.049 f1:0.093 acc:0.886 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8071363535973313,
        0.8068494514088058,
        0.8068494514088058,
        0.8068494514088058,
        0.8068494514088058,
        0.8068494514088058
      ],
      "excerpt": "Running validation.. \n21_training.tif  PRF1A [0.95381, 0.45304, 0.6143, 0.95749] \n39_training.tif  PRF1A [0.84353, 0.48988, 0.6198, 0.94837] \n37_training.tif  PRF1A [0.8621, 0.60001, 0.70757, 0.95665] \n35_training.tif  PRF1A [0.86854, 0.64861, 0.74263, 0.96102] \n38_training.tif  PRF1A [0.93073, 0.28781, 0.43966, 0.93669] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.869908292163965
      ],
      "excerpt": "The network is trained for 40 epochs with 15 training images, 5 validation images and 20 test images. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sraashis/deepdyn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Aashis Khanal\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "A pytorch based framework for medical image processing with Convolutional Neural Network.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepdyn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sraashis",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sraashis/deepdyn/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We need python3, numpy, pandas, pytorch, torchvision, matplotlib and PILLOW packages\n\n```\npip install -r deepdyn/assets/requirements.txt\n```\n\n![Flow](assets/flow_ature.png)\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 49,
      "date": "Sun, 26 Dec 2021 09:48:20 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "computer-vision",
      "convolutional-neural-networks",
      "graph-algorithms",
      "image-procesing",
      "u-net",
      "unet",
      "vessel-segmentation",
      "machine-learning",
      "fundus-image",
      "pytorch",
      "pytorch-implementation",
      "pytorch-vizualization",
      "multiple-gpu",
      "retinal-vessel-segmentation",
      "universal-pytorch-framework",
      "centralized-image-processing",
      "custom-data-loader-pytorch",
      "cross-validation",
      "custom-dataloader"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Example **main.py**\n```python\nimport testarch.unet as unet\nimport testarch.unet.runs as r_unet\nimport testarch.miniunet as mini_unet\nimport testarch.miniunet.runs as r_miniunet\nimport torchvision.transforms as tmf\n\n\ntransforms = tmf.Compose([\n    tmf.ToPILImage(),\n    tmf.ToTensor()\n])\n\nif __name__ == \"__main__\":\n    unet.run([r_unet.DRIVE], transforms)\n    mini_unet.run([r_miniunet.DRIVE], transforms)\n```\nWhere ***testarch.unet.runs*** file consist a predefined configuration  ***DRIVE*** with all necessary parameters.\n```python\nimport os\nsep = os.sep\nDRIVE = {\n    'Params': {\n        'num_channels': 1,\n        'num_classes': 2,\n        'batch_size': 4,\n        'epochs': 250,\n        'learning_rate': 0.001,\n        'patch_shape': (388, 388),\n        'patch_offset': (150, 150),\n        'expand_patch_by': (184, 184),\n        'use_gpu': True,\n        'distribute': True,\n        'shuffle': True,\n        'log_frequency': 5,\n        'validation_frequency': 1,\n        'mode': 'train',\n        'parallel_trained': False,\n    },\n    'Dirs': {\n        'image': 'data' + sep + 'DRIVE' + sep + 'images',\n        'mask': 'data' + sep + 'DRIVE' + sep + 'mask',\n        'truth': 'data' + sep + 'DRIVE' + sep + 'manual',\n        'logs': 'logs' + sep + 'DRIVE' + sep + 'UNET',\n        'splits_json': 'data' + sep + 'DRIVE' + sep + 'splits'\n    },\n\n    'Funcs': {\n        'truth_getter': lambda file_name: file_name.split('_')[0] + '_manual1.gif',\n        'mask_getter': lambda file_name: file_name.split('_')[0] + '_mask.gif',\n        'dparm': lambda x: np.random.choice(np.arange(1, 101, 1), 2)\n    }\n}\n```\nSimilarly, ***testarch.miniunet.runs*** file consist a predefined configuration  ***DRIVE*** with all necessary parameters. \n***NOTE: Make sure it picks up probability-maps from the logs of previous run.***\n\n```python\nimport os\nsep = os.sep\nDRIVE = {\n    'Params': {\n        'num_channels': 2,\n        'num_classes': 2,\n        'batch_size': 4,\n        'epochs': 100,\n        'learning_rate': 0.001,\n        'patch_shape': (100, 100),\n        'expand_patch_by': (40, 40)\n        'use_gpu': True,\n        'distribute': True,\n        'shuffle': True,\n        'log_frequency': 20,\n        'validation_frequency': 1,\n        'mode': 'train',\n        'parallel_trained': False\n    },\n    'Dirs': {\n        'image': 'data' + sep + 'DRIVE' + sep + 'images',\n        'image_unet': 'logs' + sep + 'DRIVE' + sep + 'UNET',\n        'mask': 'data' + sep + 'DRIVE' + sep + 'mask',\n        'truth': 'data' + sep + 'DRIVE' + sep + 'manual',\n        'logs': 'logs' + sep + 'DRIVE' + sep + 'MINI-UNET',\n        'splits_json': 'data' + sep + 'DRIVE' + sep + 'splits'\n    },\n\n    'Funcs': {\n        'truth_getter': lambda file_name: file_name.split('_')[0] + '_manual1.gif',\n        'mask_getter': lambda file_name: file_name.split('_')[0] + '_mask.gif'\n    }\n}\n```\n\n- **num_channels**: Input channels to the CNN. We are only feeding the green channel to unet.\n- **num_classes**: Output classes from CNN. We have vessel, background.\n- **patch_shape, expand_patch_by**: Unet takes 388 * 388 patch but also looks at 184 pixel on each dimension equally to make it 572 * 572. We mirror image if we run to image edges when expanding.\n So 572 * 572 goes in 388 * 388 * 2 comes out.\n- **patch_offset**: Overlap between two input patches. We get more data doing this.\n- **distribute**: Uses all gpu in parallel if set to True. [WARN]torch.cuda.set_device(1) Mustn't be done if set to True.\n- **shuffle**: Shuffle train data after every epoch.\n- **log_frequency**: Just print log after this number of batches with average scores. No rocket science :).\n- **validation_frequency**: Do validation after this number of epochs. We also persist the best performing model.\n- **mode**: train/test.\n- **parallel_trained**: If a resumed model was parallel trained or not.\n- **logs**: Dir for all logs\n- **splits_json**: A directory that consist of json files with list of files with keys 'train', 'test'\n'validation'. (https://github.com/sraashis/deepdyn/blob/master/utils/auto_split.py) takes a folder with all images and does that automatically. This is handy when we want to do k-fold cross validation. We jsut have to generate such k json files and put in splits_json folder. \n- **truth_getter, mask_getter**: A custom function that maps input_image to its ground_truth and mask respectively.\n\n",
      "technique": "Header extraction"
    }
  ]
}