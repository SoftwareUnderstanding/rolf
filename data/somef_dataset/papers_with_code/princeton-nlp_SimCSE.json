{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2104.08821",
      "https://arxiv.org/abs/2012.12624",
      "https://arxiv.org/abs/1908.10084"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite our paper if you use SimCSE in your work:\n\n```bibtex\n@inproceedings{gao2021simcse,\n   title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},\n   author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},\n   booktitle={Empirical Methods in Natural Language Processing (EMNLP)},\n   year={2021}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{gao2021simcse,\n   title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},\n   author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},\n   booktitle={Empirical Methods in Natural Language Processing (EMNLP)},\n   year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8939426072597875
      ],
      "excerpt": "Naming rules: unsup and sup represent \"unsupervised\" (trained on Wikipedia corpus) and \"supervised\" (trained on NLI datasets) respectively. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/princeton-nlp/SimCSE",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-16T02:57:04Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T11:04:06Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.991901645665181
      ],
      "excerpt": "This repository contains the code and pre-trained models for our paper SimCSE: Simple Contrastive Learning of Sentence Embeddings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9425621725009653
      ],
      "excerpt": "<!-- Thanks for your interest in our repo! --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515072506379995
      ],
      "excerpt": "Wait a minute! The authors are working day and night \ud83d\udcaa, to make the code and models available, so you can explore our state-of-the-art sentence embeddings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9269542596646467,
        0.986823542649154
      ],
      "excerpt": "8/31: Our paper has been accepted to EMNLP! Please check out our updated paper (with updated numbers and baselines).  \n5/12: We updated our unsupervised models with new hyperparameters and better performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8108783436821375,
        0.9110775546600983,
        0.9047222399920153
      ],
      "excerpt": "4/23: We released our training code. \n4/20: We released our model checkpoints and evaluation code. \n4/18: We released our paper. Check it out! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9954271036390143,
        0.8405421530189204,
        0.860059181823877
      ],
      "excerpt": "We propose a simple contrastive learning framework that works with both unlabeled and labeled data. Unsupervised SimCSE simply takes an input sentence and predicts itself in a contrastive learning framework, with only standard dropout used as noise. Our supervised SimCSE incorporates annotated pairs from NLI datasets into contrastive learning by using entailment pairs as positives and contradiction pairs as hard negatives. The following figure is an illustration of our models. \nOur released models are listed as following. You can import these models by using the simcse package or using HuggingFace's Transformers.  \n|              Model              | Avg. STS | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9839125154693362
      ],
      "excerpt": "Note that the results are slightly better than what we have reported in the current version of the paper after adopting a new set of hyperparameters (for hyperparamters, see the training section). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9545088640041206,
        0.986949329356484
      ],
      "excerpt": "In the following section, we describe how to train a SimCSE model by using our code. \nOur evaluation code for sentence embeddings is based on a modified version of SentEval. It evaluates sentence embeddings on semantic textual similarity (STS) tasks and downstream transfer tasks. For STS tasks, our evaluation takes the \"all\" setting, and report Spearman's correlation. See our paper (Appendix B) for evaluation details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8988007079633777
      ],
      "excerpt": "which is expected to output the results in a tabular format: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8547755619698303,
        0.887161739872543
      ],
      "excerpt": "dev: Report the development set results. Note that in STS tasks, only STS-B and SICK-R have development sets, so we only report their numbers. It also takes a fast mode for transfer tasks, so the running time is much shorter than the test mode (though numbers are slightly lower). \nfasttest: It is the same as test, but with a fast mode so the running time is much shorter, but the reported numbers may be lower (only for transfer tasks). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586286498861544
      ],
      "excerpt": "sts (default): Evaluate on STS tasks, including STS 12~16, STS-B and SICK-R. This is the most commonly-used set of tasks to evaluate the quality of sentence embeddings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8563310317084577
      ],
      "excerpt": "full: Evaluate on both STS and transfer tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9334354109917629
      ],
      "excerpt": "--tasks: Specify which dataset(s) to evaluate on. Will be overridden if --task_set is not na. See the code for a full list of tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8249830755271322,
        0.8042477671016393,
        0.808191761914143,
        0.8924934957526178
      ],
      "excerpt": "* --model_name_or_path: Pre-trained checkpoints to start with. For now we support BERT-based models (bert-base-uncased, bert-large-uncased, etc.) and RoBERTa-based models (RoBERTa-base, RoBERTa-large, etc.). \n* --temp: Temperature for the contrastive loss. \n* --pooler_type: Pooling method. It's the same as the --pooler_type in the evaluation part. \n* --mlp_only_train: We have found that for unsupervised SimCSE, it works better to train the model with MLP layer but test the model without it. You should use this argument when training unsupervised SimCSE models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9684915575131307
      ],
      "excerpt": "  * --mlm_weight: Weight for the MLM objective. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9469698109635905,
        0.9328010326464178
      ],
      "excerpt": "All the other arguments are standard Huggingface's transformers training arguments. Some of the often-used arguments are: --output_dir, --learning_rate, --per_device_train_batch_size. In our example scripts, we also set to evaluate the model on the STS-B development set (need to download the dataset following the evaluation section) and save the best checkpoint. \nFor results in the paper, we use Nvidia 3090 GPUs with CUDA 11. Using different types of devices or different versions of CUDA/other softwares may lead to slightly different performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8740703641471835,
        0.9174787095030196
      ],
      "excerpt": "If you have any questions related to the code or the paper, feel free to email Tianyu (tianyug@cs.princeton.edu) and Xingcheng (yxc18@mails.tsinghua.edu.cn). If you encounter any problems when using the code, or want to report a bug, you can open an issue. Please try to specify the problem with details so we can help you better and quicker! \nWe thank the community's efforts for extending SimCSE! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9214939858121882
      ],
      "excerpt": "AK391 integrated to Huggingface Spaces with Gradio. See demo:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "EMNLP'2021: SimCSE: Simple Contrastive Learning of Sentence Embeddings",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/princeton-nlp/SimCSE/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 203,
      "date": "Thu, 23 Dec 2021 17:55:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/princeton-nlp/SimCSE/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "princeton-nlp/SimCSE",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/princeton-nlp/SimCSE/main/run_sup_example.sh",
      "https://raw.githubusercontent.com/princeton-nlp/SimCSE/main/run_unsup_example.sh",
      "https://raw.githubusercontent.com/princeton-nlp/SimCSE/main/SentEval/data/downstream/download_dataset.sh",
      "https://raw.githubusercontent.com/princeton-nlp/SimCSE/main/demo/run_demo_example.sh",
      "https://raw.githubusercontent.com/princeton-nlp/SimCSE/main/data/download_wiki.sh",
      "https://raw.githubusercontent.com/princeton-nlp/SimCSE/main/data/download_nli.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9289108637405007
      ],
      "excerpt": "Our released models are listed as following. You can import these models by using the simcse package or using HuggingFace's Transformers.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8080002376980483,
        0.9465718491881494
      ],
      "excerpt": "cd SentEval/data/downstream/ \nbash download_dataset.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8101577749986222
      ],
      "excerpt": "--model_name_or_path: The name or path of a transformers-based pre-trained checkpoint. You can directly use the models in the above table, e.g., princeton-nlp/sup-simcse-bert-base-uncased. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8477354107464127
      ],
      "excerpt": "We provide example training scripts for both unsupervised and supervised SimCSE. In run_unsup_example.sh, we provide a single-GPU (or CPU) example for the unsupervised version, and in run_sup_example.sh we give a multiple-GPU example for the supervised version. Both scripts call train.py for training. We explain the arguments in following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.816558081713355
      ],
      "excerpt": "Jianlin Su has provided a Chinese version of SimCSE. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "Train SimCSE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.840469567290978
      ],
      "excerpt": "python evaluation.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8438769870689441
      ],
      "excerpt": "    --mode test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "------ test ------ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/princeton-nlp/SimCSE/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD License\\n\\nFor SentEval software\\n\\nCopyright (c) 2017-present, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n * Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n * Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n * Neither the name Facebook nor the names of its contributors may be used to\\n   endorse or promote products derived from this software without specific\\n   prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# SimCSE: Simple Contrastive Learning of Sentence Embeddings",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SimCSE",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "princeton-nlp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/princeton-nlp/SimCSE/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "gaotianyu1350",
        "body": "* Update pooling methods for unsupervised models (cls -> cls-before-pooler)\r\n* Fix a faiss bug.",
        "dateCreated": "2021-05-12T05:37:28Z",
        "datePublished": "2021-05-12T05:38:26Z",
        "html_url": "https://github.com/princeton-nlp/SimCSE/releases/tag/0.4",
        "name": "0.4",
        "tag_name": "0.4",
        "tarball_url": "https://api.github.com/repos/princeton-nlp/SimCSE/tarball/0.4",
        "url": "https://api.github.com/repos/princeton-nlp/SimCSE/releases/42815293",
        "zipball_url": "https://api.github.com/repos/princeton-nlp/SimCSE/zipball/0.4"
      },
      {
        "authorType": "User",
        "author_name": "gaotianyu1350",
        "body": "",
        "dateCreated": "2021-05-11T14:07:42Z",
        "datePublished": "2021-05-11T14:07:59Z",
        "html_url": "https://github.com/princeton-nlp/SimCSE/releases/tag/0.3",
        "name": "0.3",
        "tag_name": "0.3",
        "tarball_url": "https://api.github.com/repos/princeton-nlp/SimCSE/tarball/0.3",
        "url": "https://api.github.com/repos/princeton-nlp/SimCSE/releases/42774915",
        "zipball_url": "https://api.github.com/repos/princeton-nlp/SimCSE/zipball/0.3"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First, install PyTorch by following the instructions from [the official website](https://pytorch.org). To faithfully reproduce our results, please use the correct `1.7.1` version corresponding to your platforms/CUDA versions. PyTorch version higher than `1.7.1` should also work. For example, if you use Linux and **CUDA11** ([how to check CUDA version](https://varhowto.com/check-cuda-version/)), install PyTorch by the following command,\n\n```bash\npip install torch==1.7.1+cu110 -f https://download.pytorch.org/whl/torch_stable.html\n```\n\nIf you instead use **CUDA** `<11` or **CPU**, install PyTorch by the following command,\n\n```bash\npip install torch==1.7.1\n```\n\n\nThen run the following script to install the remaining dependencies,\n\n```bash\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1573,
      "date": "Thu, 23 Dec 2021 17:55:21 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "sentence-embeddings"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide an easy-to-use sentence embedding tool based on our SimCSE model (see our [Wiki](https://github.com/princeton-nlp/SimCSE/wiki) for detailed usage). To use the tool, first install the `simcse` package from PyPI\n```bash\npip install simcse\n```\n\nOr directly install it from our code\n```bash\npython setup.py install\n```\n\nNote that if you want to enable GPU encoding, you should install the correct version of PyTorch that supports CUDA. See [PyTorch official website](https://pytorch.org) for instructions.\n\nAfter installing the package, you can load our model by just two lines of code\n```python\nfrom simcse import SimCSE\nmodel = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n```\nSee [model list](#model-list) for a full list of available models. \n\nThen you can use our model for **encoding sentences into embeddings**\n```python\nembeddings = model.encode(\"A woman is reading.\")\n```\n\n**Compute the cosine similarities** between two groups of sentences\n```python\nsentences_a = ['A woman is reading.', 'A man is playing a guitar.']\nsentences_b = ['He plays guitar.', 'A woman is making a photo.']\nsimilarities = model.similarity(sentences_a, sentences_b)\n```\n\nOr build index for a group of sentences and **search** among them\n```python\nsentences = ['A woman is reading.', 'A man is playing a guitar.']\nmodel.build_index(sentences)\nresults = model.search(\"He plays guitar.\")\n```\n\nWe also support [faiss](https://github.com/facebookresearch/faiss), an efficient similarity search library. Just install the package following [instructions](https://github.com/princeton-nlp/SimCSE/wiki/Installation) here and `simcse` will automatically use `faiss` for efficient search.\n\n**WARNING**: We have found that `faiss` did not well support Nvidia AMPERE GPUs (3090 and A100). In that case, you should change to other GPUs or install the CPU version of `faiss` package.\n\nWe also provide an easy-to-build [demo website](./demo) to show how SimCSE can be used in sentence retrieval. The code is based on [DensePhrases](https://arxiv.org/abs/2012.12624)' [repo](https://github.com/princeton-nlp/DensePhrases) and [demo](http://densephrases.korea.ac.kr) (a lot of thanks to the authors of DensePhrases). \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Besides using our provided sentence embedding tool, you can also easily import our models with HuggingFace's `transformers`:\n```python\nimport torch\nfrom scipy.spatial.distance import cosine\nfrom transformers import AutoModel, AutoTokenizer\n\n#: Import our models. The package will take care of downloading the models automatically\ntokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n\n#: Tokenize input texts\ntexts = [\n    \"There's a kid on a skateboard.\",\n    \"A kid is skateboarding.\",\n    \"A kid is inside the house.\"\n]\ninputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n\n#: Get the embeddings\nwith torch.no_grad():\n    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n\n#: Calculate cosine similarities\n#: Cosine similarities are in [-1, 1]. Higher means more similar\ncosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\ncosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n\nprint(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\nprint(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))\n```\n\nIf you encounter any problem when directly loading the models by HuggingFace's API, you can also download the models manually from the above table and use `model = AutoModel.from_pretrained({PATH TO THE DOWNLOAD MODEL})`.\n\n",
      "technique": "Header extraction"
    }
  ]
}