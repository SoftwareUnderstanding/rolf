{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bhuyanamit986/FlowerClassification",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-23T13:49:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-22T13:33:49Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\nHere I have taken a flower classification dataset from http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html. After some preprocessing of images I took InceptionNet as my model and fine tuned it's last 50 layers and freezed all the first 50 but batch normalization layers.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9753621427530058
      ],
      "excerpt": "The Inception network , was complex (heavily engineered). It used a lot of tricks to push performance; both in terms of speed and accuracy. Its constant evolution lead to the creation of several versions of the network. The popular versions are as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.896970721217292
      ],
      "excerpt": "Each version is an iterative improvement over the previous one. Understanding the upgrades can help us to build custom classifiers that are optimized both in speed and accuracy. Also, depending on your data, a lower version may actually work better. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9929996758135538,
        0.9545031987498199,
        0.8019568656353729,
        0.8925807862624384,
        0.9648660412750686,
        0.9391690282645258
      ],
      "excerpt": "Because of this huge variation in the location of the information, choosing the right kernel size for the convolution operation becomes tough. A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally. \nVery deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network. \nNaively stacking large convolution operations is computationally expensive. \nWhy not have filters with multiple sizes operate on the same level? The network essentially would get a bit \u201cwider\u201d rather than \u201cdeeper\u201d. The authors designed the inception module to reflect the same. \nThe naive inception model performs convolution on an input, with 3 different sizes of filters (1x1, 3x3, 5x5). Additionally, max pooling is also performed. The outputs are concatenated and sent to the next inception module. https://miro.medium.com/max/1400/1*DKjGRDd_lJeUfVlY50ojOA.png  \nAs stated before, deep neural networks are computationally expensive. To make it cheaper, the authors limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions. Though adding an extra operation may seem counterintuitive, 1x1 convolutions are far more cheaper than 5x5 convolutions, and the reduced number of input channels also help. Do note that however, the 1x1 convolution is introduced after the max pooling layer, rather than before.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.896933282154596,
        0.9921178122228597,
        0.9580077159303695
      ],
      "excerpt": "GoogLeNet has 9 such inception modules stacked linearly. It is 22 layers deep (27, including the pooling layers). It uses global average pooling at the end of the last inception module. \nNeedless to say, it is a pretty deep classifier. As with any very deep network, it is subject to the vanishing gradient problem. \nTo prevent the middle part of the network from \u201cdying out\u201d, the authors introduced two auxiliary classifiers (The purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss. Weight value used in the paper was 0.3 for each auxiliary loss. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8861197993643757,
        0.8576107344143247,
        0.8645910245180466
      ],
      "excerpt": "Inception v2 and Inception v3 were presented in the same paper. The authors proposed a number of upgrades which increased the accuracy and reduced the computational complexity. \nReduce representational bottleneck. The intuition was that, neural networks perform better when convolutions didn\u2019t alter the dimensions of the input drastically. Reducing the dimensions too much may cause loss of information, known as a \u201crepresentational bottleneck\u201d \nUsing smart factorization methods, convolutions can be made more efficient in terms of computational complexity. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8832706871553885
      ],
      "excerpt": "Moreover, they factorize convolutions of filter size nxn to a combination of 1xn and nx1 convolutions. For example, a 3x3 convolution is equivalent to first performing a 1x3 convolution, and then performing a 3x1 convolution on its output. They found this method to be 33% more cheaper than the single 3x3 convolution. https://miro.medium.com/max/1196/1*hTwo-hy9BUZ1bYkzisL1KA.png \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8225438125904296,
        0.8371216934755207,
        0.8641094080443851,
        0.9380106688962548
      ],
      "excerpt": "The above three principles were used to build three different types of inception modules (Let\u2019s call them modules A,B and C in the order they were introduced. \nThe authors noted that the auxiliary classifiers didn\u2019t contribute much until near the end of the training process, when accuracies were nearing saturation. They argued that they function as regularizes, especially if they have BatchNorm or Dropout operations. \nPossibilities to improve on the Inception v2 without drastically changing the modules were to be investigated. \nInception Net v3 incorporated all of the above upgrades stated for Inception v2, and in addition used the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911478160601604,
        0.8826690918135751,
        0.9763503310311066
      ],
      "excerpt": " - BatchNorm in the Auxillary Classifiers. \n - Label Smoothing (A type of regularizing component added to the loss formula that prevents the network from becoming too confident about a class. Prevents over fitting). \nMake the modules more uniform. The authors also noticed that some of the modules were more complicated than necessary. This can enable us to boost performance by adding more of these uniform modules. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8339743692688234
      ],
      "excerpt": "The \u201cstem\u201d of Inception v4 was modified. The stem here, refers to the initial set of operations performed before introducing the Inception blocks. https://miro.medium.com/max/1292/1*cYjhQ05zLXdHn363TsPrLQ.jpeg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "# Flower Classification",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bhuyanamit986/FlowerClassification/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 05:42:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bhuyanamit986/FlowerClassification/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "bhuyanamit986/FlowerClassification",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/bhuyanamit986/FlowerClassification/master/FlowerClassification.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8247694759619252
      ],
      "excerpt": "Make the modules more uniform. The authors also noticed that some of the modules were more complicated than necessary. This can enable us to boost performance by adding more of these uniform modules. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bhuyanamit986/FlowerClassification/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FlowerClassification",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "FlowerClassification",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "bhuyanamit986",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bhuyanamit986/FlowerClassification/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 30 Dec 2021 05:42:01 GMT"
    },
    "technique": "GitHub API"
  }
}