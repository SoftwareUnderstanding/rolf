{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**nmtpy** includes code from the following projects:\n\n - [dl4mt-tutorial](https://github.com/nyu-dl/dl4mt-tutorial)\n - Scripts from [subword-nmt](https://github.com/rsennrich/subword-nmt)\n - Ensembling and alignment collection from [nematus](https://github.com/rsennrich/nematus)\n - `multi-bleu.perl` from [mosesdecoder](https://github.com/moses-smt/mosesdecoder)\n - METEOR v1.5 JAR from [meteor](https://github.com/cmu-mtlab/meteor)\n - Sorted data iterator, coco eval script and LSTM from [arctic-captions](https://github.com/kelvinxu/arctic-captions)\n - `pycocoevalcap` from [coco-caption](https://github.com/tylin/coco-caption)\n\nSee [LICENSE](LICENSE.md) file for license information.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1608.05859",
      "https://arxiv.org/abs/1605.09186",
      "https://arxiv.org/abs/1609.03976",
      "https://arxiv.org/abs/1609.04621",
      "https://arxiv.org/abs/1605.09186 (2016).](https://arxiv.org/abs/1605.09186)\n\n[Caglayan, Ozan, Lo\u00efc Barrault, and Fethi Bougares. \"Multimodal Attention for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1609.03976 (2016).](https://arxiv.org/abs/1609.03976)\n\nThe models are separated into 8 files implementing their own multimodal CGRU differing in the way the attention is formulated in the decoder (4 ways) x the way the multimodal contexts are fusioned (2 ways: SUM/CONCAT). These models also use a different data iterator, namely `WMTIterator` that requires converting the textual data into `.pkl` as in the [multimodal example](examples/wmt16-mmt-task1).\n\nThe `WMTIterator` only knows how to handle the ResNet-50 convolutional features that we provide in the examples page. If you would like to use FC-style fixed-length vectors or other types of multimodal features, you need to write your own iterator.\n\n### Factored NMT: `attention_factors.py`\n\nThe model file `attention_factors.py` corresponds to the following paper:\n\n[Garc\u00eda-Mart\u00ednez, Mercedes, Lo\u00efc Barrault, and Fethi Bougares. \"Factored Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1609.04621 (2016).](https://arxiv.org/abs/1609.04621)\n\nIn the examples folder of this repository, you can find data and a configuration file to run this model.\n\n### RNNLM: `rnnlm.py`\n\nThis is a basic recurrent language model to be used with `nmt-test-lm` utility.\n\n## Requirements\n\nYou need the following Python libraries installed in order to use **nmtpy**:\n  - numpy\n  - Theano >= 0.9\n\n- We recommend using Anaconda Python distribution which is equipped with Intel MKL (Math Kernel Library) greatly\n  improving CPU decoding speeds during beam search. With a correct compilation and installation, you should achieve\n  similar performance with OpenBLAS as well but the setup procedure may be difficult to follow for inexperienced ones.\n- nmtpy only supports Python 3.5+, please see [pythonclock.org](http://pythonclock.org)\n- Please note that METEOR requires a **Java** runtime so `java` should be in your `$PATH`.\n\n#### Additional data for METEOR\n\nBefore installing **nmtpy**, you need to run `scripts/get-meteor-data.sh` to download METEOR paraphrase files.\n\n#### Installation\n\n```\n$ python setup.py install\n```\n\n**Note:** When you add a new model under `models/` it will not be directly available in runtime\nas it needs to be installed as well. To avoid re-installing each time, you can use development mode with `python setup.py develop` which will directly make Python see the `git` folder as the library content.\n\n## Ensuring Reproducibility in Theano\n\n(Update: Theano 1.0 includes a configuration option `deterministic = more` that obsoletes the below patch.)\n\nWhen we started to work on **dl4mt-tutorial**, we noticed an annoying reproducibility problem where\nmultiple runs of the same experiment (same seed, same machine, same GPU) were not producing exactly\nthe same training and validation losses after a few iterations.\n\nThe solution that was [discussed](https://github.com/Theano/Theano/issues/3029) in Theano\nissues was to replace a non-deterministic GPU operation with its deterministic equivalent. To achieve this,\nyou should **patch** your local Theano v0.9.0 installation using [this patch](patches/00-theano-advancedinctensor.patch) unless upstream developers add a configuration option to `.theanorc`.\n\n## Configuring Theano\n\nHere is a basic `.theanorc` file (Note that the way you install CUDA, CuDNN\nmay require some modifications):\n\n```\n[global]\n# Not so important as nmtpy will pick an available GPU\ndevice = gpu0\n# We use float32 everywhere\nfloatX = float32\n# Keep theano compilation in RAM if you have a 7/24 available server\nbase_compiledir=/tmp/theano-%(user)s\n# For Theano >= 0.10, if you want exact same results for each run\n# with same seed\ndeterministic=more\n\n[cuda]\nroot = /opt/cuda-8.0\n\n[dnn]\n# Make sure you use CuDNN as well\nenabled = auto\nlibrary_path = /opt/CUDNN/cudnn-v5.1/lib64\ninclude_path = /opt/CUDNN/cudnn-v5.1/include\n\n[lib]\n# Allocate 95% of GPU memory once\ncnmem = 0.95\n```\n\nYou may also want to try the new GPU backend after\ninstalling [libgpuarray](https://github.com/Theano/libgpuarray). In order to do so,\npass `GPUARRAY=1` into the environment when running `nmt-train`:\n\n```\n$ GPUARRAY=1 nmt-train -c <conf file> ...\n```\n\n### Checking BLAS configuration\n\nRecent Theano versions can automatically detect correct MKL flags. You should obtain a similar output after running the following command:\n\n```\n$ python -c 'import theano; print theano.config.blas.ldflags'\n-L/home/ozancag/miniconda/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lm -lm -Wl,-rpath,/home/ozancag/miniconda/lib\n```\n\n## Acknowledgements\n\n**nmtpy** includes code from the following projects:\n\n - [dl4mt-tutorial](https://github.com/nyu-dl/dl4mt-tutorial)\n - Scripts from [subword-nmt](https://github.com/rsennrich/subword-nmt)\n - Ensembling and alignment collection from [nematus](https://github.com/rsennrich/nematus)\n - `multi-bleu.perl` from [mosesdecoder](https://github.com/moses-smt/mosesdecoder)\n - METEOR v1.5 JAR from [meteor](https://github.com/cmu-mtlab/meteor)\n - Sorted data iterator, coco eval script and LSTM from [arctic-captions](https://github.com/kelvinxu/arctic-captions)\n - `pycocoevalcap` from [coco-caption](https://github.com/tylin/coco-caption)\n\nSee [LICENSE](LICENSE.md) file for license information.",
      "https://arxiv.org/abs/1609.03976 (2016).](https://arxiv.org/abs/1609.03976)\n\nThe models are separated into 8 files implementing their own multimodal CGRU differing in the way the attention is formulated in the decoder (4 ways) x the way the multimodal contexts are fusioned (2 ways: SUM/CONCAT). These models also use a different data iterator, namely `WMTIterator` that requires converting the textual data into `.pkl` as in the [multimodal example](examples/wmt16-mmt-task1).\n\nThe `WMTIterator` only knows how to handle the ResNet-50 convolutional features that we provide in the examples page. If you would like to use FC-style fixed-length vectors or other types of multimodal features, you need to write your own iterator.\n\n### Factored NMT: `attention_factors.py`\n\nThe model file `attention_factors.py` corresponds to the following paper:\n\n[Garc\u00eda-Mart\u00ednez, Mercedes, Lo\u00efc Barrault, and Fethi Bougares. \"Factored Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1609.04621 (2016).](https://arxiv.org/abs/1609.04621)\n\nIn the examples folder of this repository, you can find data and a configuration file to run this model.\n\n### RNNLM: `rnnlm.py`\n\nThis is a basic recurrent language model to be used with `nmt-test-lm` utility.\n\n## Requirements\n\nYou need the following Python libraries installed in order to use **nmtpy**:\n  - numpy\n  - Theano >= 0.9\n\n- We recommend using Anaconda Python distribution which is equipped with Intel MKL (Math Kernel Library) greatly\n  improving CPU decoding speeds during beam search. With a correct compilation and installation, you should achieve\n  similar performance with OpenBLAS as well but the setup procedure may be difficult to follow for inexperienced ones.\n- nmtpy only supports Python 3.5+, please see [pythonclock.org](http://pythonclock.org)\n- Please note that METEOR requires a **Java** runtime so `java` should be in your `$PATH`.\n\n#### Additional data for METEOR\n\nBefore installing **nmtpy**, you need to run `scripts/get-meteor-data.sh` to download METEOR paraphrase files.\n\n#### Installation\n\n```\n$ python setup.py install\n```\n\n**Note:** When you add a new model under `models/` it will not be directly available in runtime\nas it needs to be installed as well. To avoid re-installing each time, you can use development mode with `python setup.py develop` which will directly make Python see the `git` folder as the library content.\n\n## Ensuring Reproducibility in Theano\n\n(Update: Theano 1.0 includes a configuration option `deterministic = more` that obsoletes the below patch.)\n\nWhen we started to work on **dl4mt-tutorial**, we noticed an annoying reproducibility problem where\nmultiple runs of the same experiment (same seed, same machine, same GPU) were not producing exactly\nthe same training and validation losses after a few iterations.\n\nThe solution that was [discussed](https://github.com/Theano/Theano/issues/3029) in Theano\nissues was to replace a non-deterministic GPU operation with its deterministic equivalent. To achieve this,\nyou should **patch** your local Theano v0.9.0 installation using [this patch](patches/00-theano-advancedinctensor.patch) unless upstream developers add a configuration option to `.theanorc`.\n\n## Configuring Theano\n\nHere is a basic `.theanorc` file (Note that the way you install CUDA, CuDNN\nmay require some modifications):\n\n```\n[global]\n# Not so important as nmtpy will pick an available GPU\ndevice = gpu0\n# We use float32 everywhere\nfloatX = float32\n# Keep theano compilation in RAM if you have a 7/24 available server\nbase_compiledir=/tmp/theano-%(user)s\n# For Theano >= 0.10, if you want exact same results for each run\n# with same seed\ndeterministic=more\n\n[cuda]\nroot = /opt/cuda-8.0\n\n[dnn]\n# Make sure you use CuDNN as well\nenabled = auto\nlibrary_path = /opt/CUDNN/cudnn-v5.1/lib64\ninclude_path = /opt/CUDNN/cudnn-v5.1/include\n\n[lib]\n# Allocate 95% of GPU memory once\ncnmem = 0.95\n```\n\nYou may also want to try the new GPU backend after\ninstalling [libgpuarray](https://github.com/Theano/libgpuarray). In order to do so,\npass `GPUARRAY=1` into the environment when running `nmt-train`:\n\n```\n$ GPUARRAY=1 nmt-train -c <conf file> ...\n```\n\n### Checking BLAS configuration\n\nRecent Theano versions can automatically detect correct MKL flags. You should obtain a similar output after running the following command:\n\n```\n$ python -c 'import theano; print theano.config.blas.ldflags'\n-L/home/ozancag/miniconda/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lm -lm -Wl,-rpath,/home/ozancag/miniconda/lib\n```\n\n## Acknowledgements\n\n**nmtpy** includes code from the following projects:\n\n - [dl4mt-tutorial](https://github.com/nyu-dl/dl4mt-tutorial)\n - Scripts from [subword-nmt](https://github.com/rsennrich/subword-nmt)\n - Ensembling and alignment collection from [nematus](https://github.com/rsennrich/nematus)\n - `multi-bleu.perl` from [mosesdecoder](https://github.com/moses-smt/mosesdecoder)\n - METEOR v1.5 JAR from [meteor](https://github.com/cmu-mtlab/meteor)\n - Sorted data iterator, coco eval script and LSTM from [arctic-captions](https://github.com/kelvinxu/arctic-captions)\n - `pycocoevalcap` from [coco-caption](https://github.com/tylin/coco-caption)\n\nSee [LICENSE](LICENSE.md) file for license information.",
      "https://arxiv.org/abs/1609.04621 (2016).](https://arxiv.org/abs/1609.04621)\n\nIn the examples folder of this repository, you can find data and a configuration file to run this model.\n\n### RNNLM: `rnnlm.py`\n\nThis is a basic recurrent language model to be used with `nmt-test-lm` utility.\n\n## Requirements\n\nYou need the following Python libraries installed in order to use **nmtpy**:\n  - numpy\n  - Theano >= 0.9\n\n- We recommend using Anaconda Python distribution which is equipped with Intel MKL (Math Kernel Library) greatly\n  improving CPU decoding speeds during beam search. With a correct compilation and installation, you should achieve\n  similar performance with OpenBLAS as well but the setup procedure may be difficult to follow for inexperienced ones.\n- nmtpy only supports Python 3.5+, please see [pythonclock.org](http://pythonclock.org)\n- Please note that METEOR requires a **Java** runtime so `java` should be in your `$PATH`.\n\n#### Additional data for METEOR\n\nBefore installing **nmtpy**, you need to run `scripts/get-meteor-data.sh` to download METEOR paraphrase files.\n\n#### Installation\n\n```\n$ python setup.py install\n```\n\n**Note:** When you add a new model under `models/` it will not be directly available in runtime\nas it needs to be installed as well. To avoid re-installing each time, you can use development mode with `python setup.py develop` which will directly make Python see the `git` folder as the library content.\n\n## Ensuring Reproducibility in Theano\n\n(Update: Theano 1.0 includes a configuration option `deterministic = more` that obsoletes the below patch.)\n\nWhen we started to work on **dl4mt-tutorial**, we noticed an annoying reproducibility problem where\nmultiple runs of the same experiment (same seed, same machine, same GPU) were not producing exactly\nthe same training and validation losses after a few iterations.\n\nThe solution that was [discussed](https://github.com/Theano/Theano/issues/3029) in Theano\nissues was to replace a non-deterministic GPU operation with its deterministic equivalent. To achieve this,\nyou should **patch** your local Theano v0.9.0 installation using [this patch](patches/00-theano-advancedinctensor.patch) unless upstream developers add a configuration option to `.theanorc`.\n\n## Configuring Theano\n\nHere is a basic `.theanorc` file (Note that the way you install CUDA, CuDNN\nmay require some modifications):\n\n```\n[global]\n# Not so important as nmtpy will pick an available GPU\ndevice = gpu0\n# We use float32 everywhere\nfloatX = float32\n# Keep theano compilation in RAM if you have a 7/24 available server\nbase_compiledir=/tmp/theano-%(user)s\n# For Theano >= 0.10, if you want exact same results for each run\n# with same seed\ndeterministic=more\n\n[cuda]\nroot = /opt/cuda-8.0\n\n[dnn]\n# Make sure you use CuDNN as well\nenabled = auto\nlibrary_path = /opt/CUDNN/cudnn-v5.1/lib64\ninclude_path = /opt/CUDNN/cudnn-v5.1/include\n\n[lib]\n# Allocate 95% of GPU memory once\ncnmem = 0.95\n```\n\nYou may also want to try the new GPU backend after\ninstalling [libgpuarray](https://github.com/Theano/libgpuarray). In order to do so,\npass `GPUARRAY=1` into the environment when running `nmt-train`:\n\n```\n$ GPUARRAY=1 nmt-train -c <conf file> ...\n```\n\n### Checking BLAS configuration\n\nRecent Theano versions can automatically detect correct MKL flags. You should obtain a similar output after running the following command:\n\n```\n$ python -c 'import theano; print theano.config.blas.ldflags'\n-L/home/ozancag/miniconda/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lm -lm -Wl,-rpath,/home/ozancag/miniconda/lib\n```\n\n## Acknowledgements\n\n**nmtpy** includes code from the following projects:\n\n - [dl4mt-tutorial](https://github.com/nyu-dl/dl4mt-tutorial)\n - Scripts from [subword-nmt](https://github.com/rsennrich/subword-nmt)\n - Ensembling and alignment collection from [nematus](https://github.com/rsennrich/nematus)\n - `multi-bleu.perl` from [mosesdecoder](https://github.com/moses-smt/mosesdecoder)\n - METEOR v1.5 JAR from [meteor](https://github.com/cmu-mtlab/meteor)\n - Sorted data iterator, coco eval script and LSTM from [arctic-captions](https://github.com/kelvinxu/arctic-captions)\n - `pycocoevalcap` from [coco-caption](https://github.com/tylin/coco-caption)\n\nSee [LICENSE](LICENSE.md) file for license information."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{nmtpy2017,\n  author    = {Ozan Caglayan and\n               Mercedes Garc\\'{i}a-Mart\\'{i}nez and\n               Adrien Bardet and\n               Walid Aransa and\n               Fethi Bougares and\n               Lo\\\"{i}c Barrault},\n  title     = {NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems},\n  journal   = {Prague Bull. Math. Linguistics},\n  volume    = {109},\n  pages     = {15--28},\n  year      = {2017},\n  url       = {https://ufal.mff.cuni.cz/pbml/109/art-caglayan-et-al.pdf},\n  doi       = {10.1515/pralin-2017-0035},\n  timestamp = {Tue, 12 Sep 2017 10:01:08 +0100}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9876016946072088
      ],
      "excerpt": "If you use nmtpy, you may want to cite the following paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999983810533197,
        0.9999839944386729
      ],
      "excerpt": "Caglayan, Ozan, et al. \"Does Multimodality Help Human and Machine for Translation and Image Captioning?.\" arXiv preprint arXiv:1605.09186 (2016). \nCaglayan, Ozan, Lo\u00efc Barrault, and Fethi Bougares. \"Multimodal Attention for Neural Machine Translation.\" arXiv preprint arXiv:1609.03976 (2016). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999858666351105
      ],
      "excerpt": "Garc\u00eda-Mart\u00ednez, Mercedes, Lo\u00efc Barrault, and Fethi Bougares. \"Factored Neural Machine Translation.\" arXiv preprint arXiv:1609.04621 (2016). \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lium-lst/nmtpy",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-03-12T16:01:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-29T16:44:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**nmtpy** is a suite of Python tools, primarily based on the starter code provided in [dl4mt-tutorial](https://github.com/nyu-dl/dl4mt-tutorial) for training neural machine translation networks using Theano. The basic motivation behind forking **dl4mt-tutorial** was to create a framework where it would be easy to implement a new model by just copying and modifying an existing model class (or even inheriting from it and overriding some of its methods).\n\nTo achieve this purpose, **nmtpy** tries to completely isolate training loop, beam search,\niteration and model definition:\n  - `nmt-train` script to start a training experiment\n  - `nmt-translate` to produce model-agnostic translations. You just pass a trained model's\n  checkpoint file and it does its job.\n  - `nmt-rescore` to rescore translation hypotheses using an nmtpy model.\n  - An abstract `BaseModel` class to derive from to define your NMT architecture.\n  - An abstract `Iterator` to derive from for your custom iterators.\n\nA non-exhaustive list of differences between **nmtpy** and **dl4mt-tutorial** is as follows:\n\n  - No shell script, everything is in Python\n  - Overhaul object-oriented refactoring of the code: clear separation of API and scripts that interface with the API\n  - INI style configuration files to define everything regarding a training experiment\n  - Transparent cleanup mechanism to kill stale processes, remove temporary files\n  - Simultaneous logging of training details to stdout and log file\n  - Supports out-of-the-box BLEU, METEOR and COCO eval metrics\n  - Includes [subword-nmt](https://github.com/rsennrich/subword-nmt) utilities for training and applying BPE model (NOTE: This may change as the upstream subword-nmt moves forward as well.)\n  - Plugin-like text filters for hypothesis post-processing (Example: BPE, Compound, Char2Words for Char-NMT)\n  - Early-stopping and checkpointing based on perplexity, BLEU or METEOR (Ability to add new metrics easily)\n  - Single `.npz` file to store everything about a training experiment\n  - Automatic free GPU selection and reservation using `nvidia-smi`\n  - Shuffling support between epochs:\n    - Simple shuffle\n    - [Homogeneous batches of same-length samples](https://github.com/kelvinxu/arctic-captions) to improve training speed\n  - Improved parallel translation decoding on CPU\n  - Forced decoding i.e. rescoring using NMT\n  - Export decoding informations into `json` for further visualization of attention coefficients\n  - Improved numerical stability and reproducibility\n  - Glorot/Xavier, He, Orthogonal weight initializations\n  - Efficient SGD, Adadelta, RMSProp and ADAM: Single forward/backward theano function without intermediate variables\n  - Ability to stop updating a set of weights by recompiling optimizer\n  - Several recurrent blocks:\n    - GRU, Conditional GRU (CGRU) and LSTM\n    - Multimodal attentive CGRU variants\n  - [Layer Normalization](https://github.com/ryankiros/layer-norm) support for GRU\n  - 2-way or 3-way [tied target embeddings](https://arxiv.org/abs/1608.05859)\n  - Simple/Non-recurrent Dropout, L2 weight decay\n  - Training and validation loss normalization for comparable perplexities\n  - Initialization of a model with a pretrained NMT for further finetuning\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8312143739322544
      ],
      "excerpt": "This codebase is no longer maintained as we moved towards nmtpytorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8976130142270278,
        0.9902179233562308,
        0.9771814672437811,
        0.9477763136291659
      ],
      "excerpt": "attention_factors_seplogits.py is removed and its functionality is added to attention_factors model as a configuration switch: sep_h2olayer: True. \ntied_trg_emb: True/False is replaced with tied_emb: False/2way/3way to also support the sharing of \"all\" embeddings throughout the network. \nIt is advised to check the actual model implementations for the most up-to-date informations as what is written may become outdated. \nThis is the basic attention based NMT from dl4mt-tutorial improved in different ways: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.855181874941621
      ],
      "excerpt": "This model uses the simple BitextIterator i.e. it directly reads plain parallel text files as defined in the experiment configuration file. Please see this monomodal example for usage. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9657642386459445,
        0.8934377315789858
      ],
      "excerpt": "The models are separated into 8 files implementing their own multimodal CGRU differing in the way the attention is formulated in the decoder (4 ways) x the way the multimodal contexts are fusioned (2 ways: SUM/CONCAT). These models also use a different data iterator, namely WMTIterator that requires converting the textual data into .pkl as in the multimodal example. \nThe WMTIterator only knows how to handle the ResNet-50 convolutional features that we provide in the examples page. If you would like to use FC-style fixed-length vectors or other types of multimodal features, you need to write your own iterator. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9397857581378877
      ],
      "excerpt": "This is a basic recurrent language model to be used with nmt-test-lm utility. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9746576935498075
      ],
      "excerpt": "When we started to work on dl4mt-tutorial, we noticed an annoying reproducibility problem where \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8049620487330661
      ],
      "excerpt": "issues was to replace a non-deterministic GPU operation with its deterministic equivalent. To achieve this, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "nmtpy is a Python framework based on dl4mt-tutorial to experiment with Neural Machine Translation pipelines.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lium-lst/nmtpy/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 33,
      "date": "Thu, 30 Dec 2021 11:05:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lium-lst/nmtpy/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lium-lst/nmtpy",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/lium-lst/nmtpy/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lium-lst/nmtpy/master/scripts/get-meteor-data.sh",
      "https://raw.githubusercontent.com/lium-lst/nmtpy/master/scripts/prep-charnmt.sh",
      "https://raw.githubusercontent.com/lium-lst/nmtpy/master/examples/wmt16-mmt-task2/scripts/01-tokenize.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n$ python setup.py install\n```\n\n**Note:** When you add a new model under `models/` it will not be directly available in runtime\nas it needs to be installed as well. To avoid re-installing each time, you can use development mode with `python setup.py develop` which will directly make Python see the `git` folder as the library content.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.978938989664606,
        0.8460297901732785
      ],
      "excerpt": "Before installing nmtpy, you need to run scripts/get-meteor-data.sh to download METEOR paraphrase files. \n(Update: Theano 1.0 includes a configuration option deterministic = more that obsoletes the below patch.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9667405601418223,
        0.9601586898384449
      ],
      "excerpt": "you should patch your local Theano v0.9.0 installation using this patch unless upstream developers add a configuration option to .theanorc. \nHere is a basic .theanorc file (Note that the way you install CUDA, CuDNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8120445866302624
      ],
      "excerpt": ": Make sure you use CuDNN as well \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lium-lst/nmtpy/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'## MIT License\\n\\nCopyright (c) 2017 - University of Le Mans - Language and Speech Technology (LST) Lab\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n--\\n\\nnmtpy includes code from the following projects which have their own licenses:\\n\\n - dl4mt-tutorial [BSD-3-Clause]\\n - Ensembling and alignment collection from nematus [Same as dl4mt-tutorial]\\n - Scripts from subword-nmt [MIT]\\n - multi-bleu.perl from mosesdecoder [LGPL-2.1]\\n - METEOR v1.5 JAR from meteor [LGPL-2.1]\\n - Sorted data iterator, coco eval script and LSTM from arctic-captions [Revised BSD-3-Clause]\\n - pycocoevalcap from coco-caption [BSD-2-Clause]\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## UNMAINTAINED",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "nmtpy",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lium-lst",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lium-lst/nmtpy/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "ozancaglayan",
        "body": "Initial public release.\n",
        "dateCreated": "2017-02-06T10:03:27Z",
        "datePublished": "2017-02-06T10:55:18Z",
        "html_url": "https://github.com/lium-lst/nmtpy/releases/tag/v1.0.0",
        "name": "v1.0.0",
        "tag_name": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/lium-lst/nmtpy/tarball/v1.0.0",
        "url": "https://api.github.com/repos/lium-lst/nmtpy/releases/5368936",
        "zipball_url": "https://api.github.com/repos/lium-lst/nmtpy/zipball/v1.0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You need the following Python libraries installed in order to use **nmtpy**:\n  - numpy\n  - Theano >= 0.9\n\n- We recommend using Anaconda Python distribution which is equipped with Intel MKL (Math Kernel Library) greatly\n  improving CPU decoding speeds during beam search. With a correct compilation and installation, you should achieve\n  similar performance with OpenBLAS as well but the setup procedure may be difficult to follow for inexperienced ones.\n- nmtpy only supports Python 3.5+, please see [pythonclock.org](http://pythonclock.org)\n- Please note that METEOR requires a **Java** runtime so `java` should be in your `$PATH`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 125,
      "date": "Thu, 30 Dec 2021 11:05:04 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nmt",
      "theano",
      "python",
      "neural-machine-translation",
      "neural-mt",
      "deep-learning"
    ],
    "technique": "GitHub API"
  }
}