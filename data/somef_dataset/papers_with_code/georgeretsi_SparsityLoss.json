{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2006.02768",
      "https://arxiv.org/abs/1308.3432",
      "https://arxiv.org/abs/2006.02768",
      "https://arxiv.org/abs/1605.07146"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{retsinas2021weight,\n  title={Weight Pruning via Adaptive Sparsity Loss},\n  author={Retsinas, George and Elafrou, Athena and Goumas, Georgios and Maragos, Petros},\n  booktitle={2021 IEEE international conference on image processing (ICIP)},\n  year={2021},\n  organization={IEEE}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{retsinas2021weight,\n  title={Weight Pruning via Adaptive Sparsity Loss},\n  author={Retsinas, George and Elafrou, Athena and Goumas, Georgios and Maragos, Petros},\n  booktitle={2021 IEEE international conference on image processing (ICIP)},\n  year={2021},\n  organization={IEEE}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "      0, & \\text{if}\\ |w| < b \\\\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/georgeretsi/SparsityLoss",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-04T05:54:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-16T03:46:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8415748782105436,
        0.9923004682446721,
        0.855468417193278
      ],
      "excerpt": "This repository contains the code for pruning any given architecture according to a sparsity percentage target, as proposed in the following paper: \"Weight Pruning via Adaptive Sparsity Loss\" [arxiv] \nThe main idea is to introduce a single trainable parameter per layer which controls the sparsity of the layer. This extra parameter acts as the threshold of a pruning operation (any weight under this threshold is pruned) and is optimized with respect to a multi-task loss consisted of the task loss and the sparsity controlling loss (user provides a requested target sparsity).  \nThe sparsity controlling loss, the main novelty of \"Weight Pruning via Adaptive Sparsity Loss\", relies on the assumption of a Gaussian distribution over the weights at each layer.  Such assumption, retained by the application of Straight Through Estimator [arxiv] at each pruning operation, enables us to formulate the sparsity at each layer as an analytic function w.r.t to first order statistics (mean value and standard deviation) and the trainable pruning parameter, using the erf function. For a detailed description of the adaptive sparsity loss formulation, see the paper [arxiv]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810375021337262
      ],
      "excerpt": "The sparsity loss can be formulated according to the user's needs (see paper) and the basic tools for sparsifying any architecture are provided at sparse_utils.py. An example of using these sparsity tools is also provided for the setting of Wide ResNets [arxiv] and the CIFAR100 dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9281361255096106
      ],
      "excerpt": "- starget: the requested sparsity (e.g. .9 for 90% sparsity). Used for fixed/budget alternatives. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8889641378164693
      ],
      "excerpt": "(existing scheduler is Cosine Annealing with warm restarts - 1 restart @ epochs/2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "(Unstructured) Weight Pruning via Adaptive Sparsity Loss",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/georgeretsi/SparsityLoss/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 21 Dec 2021 05:18:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/georgeretsi/SparsityLoss/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "georgeretsi/SparsityLoss",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.987347996718735
      ],
      "excerpt": "Tested on PyTorch 1.3 (torch, torchvision & scipy packages are required) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241778926939864
      ],
      "excerpt": "gpu: select GPU device (by id, e.g. 0) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8714792968086912
      ],
      "excerpt": "<!---***sparsity:*** $s  = \\text{erf}(\\frac{b}{\\sigma \\sqrt{2}})$ ,  where  $\\text{erf}(x) = \\frac{1}{\\sqrt{\\pi}}\\int_{-x}^{x}e^{-t^2}dt$     *(error function)*---> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8350846847410327
      ],
      "excerpt": "      w, & \\text{otherwise} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241595775912176
      ],
      "excerpt": " - sparse_example.py (example script: WRNet-16-8 & CIFAR100) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8728112819943259
      ],
      "excerpt": "batch-size: input batch size for training (default: 128) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/georgeretsi/SparsityLoss/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 georgeretsi\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Weight Pruning via Adaptive Sparsity Loss",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SparsityLoss",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "georgeretsi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/georgeretsi/SparsityLoss/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Tue, 21 Dec 2021 05:18:49 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "machine-learning",
      "weight-pruning",
      "sparsification",
      "unstructured-pruning",
      "deep-learning",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    python3 sparse_example.py --gpu 0 --sparsity adaptive --starget .9 --lv 10.0\n\n-------------------------------------------------------------------------\n",
      "technique": "Header extraction"
    }
  ]
}