{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.5602. See: http://www.davidqiu.com:8888/research/nature14236.pdf\n\nThe agent learns to play the MsPacman-v0 Gym environment.\n\n![Alt text](mspacman.jpg?raw=true \"Title\")\n\nI modified the example found from https://keon.io/deep-q-learning/ by implementing the CNN model, target model logic, and frame averaging (among other things).\n\nHyperparameters were chosen according to the original paper as well as from https://github.com/ageron/tiny-dqn which also provided the image preprocessing method.  One key trick I found lead to better policies was introducing a fixed penalty of -1 at each action which did not naturally have a reward.\n\n---\n# Instructions\nThis is a Python 3 project and you should have a local Python 3 `venv`.\n1. Install libraries: `$ pip install -r requirements.txt`\n2. Run notebooks: `$ jupyter notebook`\n\n---\n# Double Deep Q-network\nDouble Deep Q-network (DDQN.ipynb) is implemented.  See: https://arxiv.org/pdf/1509.06461.pdf\n\n---\n# Future Work\n* Try different Atari environments\n* Experiment with hyperparameters\n* Classical algorithm problems (learn functions)"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/moduIo/Deep-Q-network",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-02T20:28:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-14T01:45:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8309647715997317,
        0.8647435950402517
      ],
      "excerpt": "Keras implementation of DQN (DQN.ipynb) for MsPacman-v0 from OpenAI Gym. \nImplements Deep Q-network (DQN) in Keras following the architecture proposed in the 2013 paper by V. Mnih et al., \"Playing Atari with Deep Reinforcement Learning\": arXiv:1312.5602. See: http://www.davidqiu.com:8888/research/nature14236.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9416661914791169,
        0.8969134785322757
      ],
      "excerpt": "I modified the example found from https://keon.io/deep-q-learning/ by implementing the CNN model, target model logic, and frame averaging (among other things). \nHyperparameters were chosen according to the original paper as well as from https://github.com/ageron/tiny-dqn which also provided the image preprocessing method.  One key trick I found lead to better policies was introducing a fixed penalty of -1 at each action which did not naturally have a reward. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Keras implementation of DQN for the MsPacman-v0 OpenAI Gym environment.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/moduIo/Deep-Q-network/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sat, 25 Dec 2021 03:28:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/moduIo/Deep-Q-network/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "moduIo/Deep-Q-network",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/moduIo/Deep-Q-network/master/DQN.ipynb",
      "https://raw.githubusercontent.com/moduIo/Deep-Q-network/master/DDQN.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9993358611190515,
        0.8509008817452535
      ],
      "excerpt": "1. Install libraries: $ pip install -r requirements.txt \n2. Run notebooks: $ jupyter notebook \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/moduIo/Deep-Q-network/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Q-network",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep-Q-network",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "moduIo",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/moduIo/Deep-Q-network/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 23,
      "date": "Sat, 25 Dec 2021 03:28:38 GMT"
    },
    "technique": "GitHub API"
  }
}