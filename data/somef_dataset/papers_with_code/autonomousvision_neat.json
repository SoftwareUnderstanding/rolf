{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This implementation primarily extends the existing [TransFuser repository](https://github.com/autonomousvision/transfuser).\n\nIf you found our work interesting, check out these other papers on autonomous driving from our group.\n- [Prakash et al. - Multi-Modal Fusion Transformer for End-to-End Autonomous Driving (CVPR'21)](http://www.cvlibs.net/publications/Prakash2021CVPR.pdf)\n- [Behl et al. - Label efficient visual abstractions for autonomous driving (IROS'20)](https://arxiv.org/pdf/2005.10091.pdf)\n- [Ohn-Bar et al. - Learning Situational Driving (CVPR'20)](https://openaccess.thecvf.com/content_CVPR_2020/papers/Ohn-Bar_Learning_Situational_Driving_CVPR_2020_paper.pdf)\n- [Prakash et al. - Exploring Data Aggregation in Policy Learning for Vision-based Urban Autonomous Driving (CVPR'20)](https://openaccess.thecvf.com/content_CVPR_2020/papers/Prakash_Exploring_Data_Aggregation_in_Policy_Learning_for_Vision-Based_Urban_Autonomous_CVPR_2020_paper.pdf)",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Chitta2021ICCV,\n  author = {Chitta, Kashyap and Prakash, Aditya and Geiger, Andreas},\n  title = {NEAT: Neural Attention Fields for End-to-End Autonomous Driving},\n  booktitle = {International Conference on Computer Vision (ICCV)},\n  year = {2021}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/autonomousvision/neat/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/autonomousvision/neat",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to CARLA\nWe are more than happy to accept contributions!\nHow can I contribute?\n\nReporting bugs\nFeature requests\nImproving documentation\nCode contributions\n\nReporting bugs\nUse our issue section on GitHub. Please check before that the\nissue is not already reported, and make sure you have read our CARLA\nDocumentation and FAQ.\nFeature requests\nPlease check first the list of feature requests. If it is not there\nand you think is a feature that might be interesting for users, please submit\nyour request as a new issue.\nImproving documentation\nIf you feel something is missing in the documentation, please don't hesitate to\nopen an issue to let us know. Even better, if you think you can improve it\nyourself, it would be a great contribution to the community!\nWe build our documentation with MkDocs based on the\nMarkdown files inside the \"Docs\" folder. You can either directly modify them on\nGitHub or locally in your machine.\nOnce you are done with your changes, please submit a pull-request.\nTIP: You can build and serve it locally by running mkdocs in the project's\nmain folder\n$ sudo pip install mkdocs\n$ mkdocs serve\n\nCode contributions\nSo you are considering making a code contribution, great! we love to have\ncontributions from the community.\nBefore starting hands-on on coding, please check out our\nissue board to see if we are already working on that, it would\nbe a pity putting an effort into something just to discover that someone else\nwas already working on that. In case of doubt or to discuss how to proceed,\nplease contact one of us (or send an email to carla.simulator@gmail.com).\nWhat should I know before I get started?\nCheck out the \"CARLA Documentation\" to get an idea on CARLA. In\naddition you may want to check the Getting started document.\nCoding standard\nPlease follow the current coding standard when submitting\nnew code.\nPull-requests\nOnce you think your contribution is ready to be added to CARLA, please submit a\npull-request.\nTry to be as descriptive as possible when filling the pull-request description.\nAdding images and gifs may help people to understand your changes or new\nfeatures.\nPlease note that there are some checks that the new code is required to pass\nbefore we can do the merge. The checks are automatically run by the continuous\nintegration system, you will see a green tick mark if all the checks succeeded.\nIf you see a red mark, please correct your code accordingly.\nChecklist\n\n[ ] Your branch is up-to-date with the master branch and tested with latest changes\n[ ] Extended the README / documentation, if necessary\n[ ] Code compiles correctly",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-18T12:59:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T07:51:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9833432895310777,
        0.9895959772223591
      ],
      "excerpt": "This repository is for the ICCV 2021 paper NEAT: Neural Attention Fields for End-to-End Autonomous Driving. \nThe training data is generated using leaderboard/team_code/auto_pilot.py. Data generation requires routes and scenarios. Each route is defined by a sequence of waypoints (and optionally a weather condition) that the agent needs to follow. Each scenario is defined by a trigger transform (location and orientation) and other actors present in that scenario (optional). We provide several routes and scenarios under leaderboard/data/. The TransFuser repository and leaderboard repository provide additional routes and scenario files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9250393890205598,
        0.8130813750243573
      ],
      "excerpt": "With Docker: \nInstructions for setting up docker are available here. Pull the docker image of CARLA 0.9.10.1 docker pull carlasim/carla:0.9.10.1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317112657769747
      ],
      "excerpt": "Additional baselines are available in the TransFuser repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[ICCV'21] NEAT: Neural Attention Fields for End-to-End Autonomous Driving",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://mmcv.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/autonomousvision/neat/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Sun, 26 Dec 2021 03:23:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/autonomousvision/neat/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "autonomousvision/neat",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/autonomousvision/neat/main/scenario_runner/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/autonomousvision/neat/tree/main/leaderboard/docs",
      "https://github.com/autonomousvision/neat/tree/main/scenario_runner/Docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/autonomousvision/neat/main/leaderboard/scripts/code_check_and_formatting.sh",
      "https://raw.githubusercontent.com/autonomousvision/neat/main/leaderboard/scripts/run_evaluation.sh",
      "https://raw.githubusercontent.com/autonomousvision/neat/main/leaderboard/scripts/make_docker.sh",
      "https://raw.githubusercontent.com/autonomousvision/neat/main/scenario_runner/srunner/utilities/code_check_and_formatting.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please follow the installation instructions from our [TransFuser repository](https://github.com/autonomousvision/transfuser) to set up the CARLA simulator. The conda environment required for NEAT can be installed via:\n```Shell\nconda env create -f environment.yml\nconda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia\n```\n\nFor running the AIM-VA baseline, you will additionally need to install [MMCV](https://mmcv.readthedocs.io/en/latest/get_started/installation.html) and [MMSegmentation](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/get_started.md#installation).\n```Shell\npip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\npip install mmsegmentation\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "mkdir model_ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881951064247234
      ],
      "excerpt": "Spin up a CARLA server (described above) and run the required agent. The required variables need to be set in leaderboard/scripts/run_evaluation.sh. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8822742303355483
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 ./leaderboard/scripts/run_evaluation.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8720996757212075
      ],
      "excerpt": "<img src=\"neat/assets/neat_clip.GIF\" height=\"270\" hspace=30> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8396958255203512,
        0.8348952530996053
      ],
      "excerpt": "unzip model_ckpt/models.zip -d model_ckpt/ \nrm model_ckpt/models.zip \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/autonomousvision/neat/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "XSLT",
      "HTML",
      "Shell",
      "Dockerfile",
      "CSS",
      "JavaScript",
      "Ruby",
      "SCSS"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 CARLA\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "NEAT: Neural Attention Fields for End-to-End Autonomous Driving",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "neat",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "autonomousvision",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/autonomousvision/neat/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Once the CARLA server is running, rollout the autopilot to start data generation.\n```Shell\n./leaderboard/scripts/run_evaluation.sh\n```\nThe expert agent used for data generation is defined in ```leaderboard/team_code/auto_pilot.py```. Different variables which need to be set are specified in ```leaderboard/scripts/run_evaluation.sh```. The expert agent is originally based on the autopilot from [this codebase](https://github.com/bradyz/2020_CARLA_challenge).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 169,
      "date": "Sun, 26 Dec 2021 03:23:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "autonomous-driving",
      "imitation-learning",
      "implicit-representations",
      "birds-eye-view",
      "iccv2021"
    ],
    "technique": "GitHub API"
  }
}