{
  "citation": [
    {
      "confidence": [
        0.9330586062571771
      ],
      "excerpt": "Pattern-based tokenization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8760087310584409,
        0.8618528462889394
      ],
      "excerpt": "| wbd.bin | Default Tokenization Model    | Pattern-based | src |  \n| sbd.bin | Default model for Sentence breaking    | Pattern-based | src |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298,
        0.9554441738822752
      ],
      "excerpt": "| bert_chinese.bin       | BERT Chinese                                | WordPiece | src |  \n| bert_multi_cased.bin       | BERT Multi Lingual Cased                                | WordPiece | src |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "| xlm_roberta_base.bin | XLM Roberta Tokenization | Unigram LM | src |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9594652697765655
      ],
      "excerpt": "Response Center (MSRC) at secure@microsoft.com. You should \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/BlingFire",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n[This document](https://github.com/microsoft/BlingFire/wiki/How-to-train-better-language-detection-with-Bling-Fire-and-FastText) describes how to improve [FastText](https://fasttext.cc/) language detection model with Bling Fire and achive 99% accuracy in language detection task for 365 languages.\r\n\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-13T19:04:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T04:37:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nIf you simply want to use it in Python, you can install the latest release using [pip](https://pypi.org/project/pip/):\r\n\r\n`pip install -U blingfire`\r\n\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nHi, we are a team at Microsoft called Bling (Beyond Language Understanding), we help Bing be smarter. Here we wanted to share with all of you our **FI**nite State machine and **RE**gular expression manipulation library (**FIRE**). We use Fire for many linguistic operations inside Bing such as Tokenization, Multi-word expression matching, Unknown word-guessing, Stemming / Lemmatization just to mention a few.\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9727178620004757
      ],
      "excerpt": "Bling Fire Tokenizer provides state of the art performance for Natural Language text tokenization. Bling Fire supports the following tokenization algorithms: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9689955003486823,
        0.9669666565389196
      ],
      "excerpt": "Bling Fire provides uniform interface for working with all four algorithms so there is no difference for the client whether to use tokenizer for XLNET, BERT or your own custom model. \nModel files describe the algorithms they are built for and are loaded on demand from external file. There are also two default models for NLTK-style tokenization and sentence breaking, which does not need to be loaded. The default tokenization model follows logic of NLTK, except hyphenated words are split and a few \"errors\" are fixed.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8839857950715786,
        0.8996514737700367
      ],
      "excerpt": "Bling Fire Tokenizer high level API designed in a way that it requires minimal or no configuration, or initialization, or additional files and is friendly for use from languages like Python, Ruby, Rust, C#, JavaScript (via WASM), etc. \nWe have precompiled some popular models and listed with the source code reference below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8542743481427121,
        0.9169190104893509,
        0.8746019878383624,
        0.901426830050148
      ],
      "excerpt": "| laser(100k\\|250k\\|500k).bin | Trained on balanced by language WikiMatrix corpus of 80+ languages | Unigram LM | src |  \n| uri(100k\\|250k\\|500k).bin | URL tokenization model trained on a large set of random URLs from the web | Unigram LM | src | \n| gpt2.bin | Byte-BPE tokenization model for GPT-2 | byte BPE | src | \n| roberta.bin | Byte-BPE tokenization model for Roberta model | byte BPE | src | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9948415507382607
      ],
      "excerpt": "Oh yes, it is also the fastest! We did a comparison of Bling Fire with tokenizers from Hugging Face, Bling Fire runs 4-5 times faster than Hugging Face Tokenizers, see also Bing Blog Post. We did comparison of Bling Fire Unigram LM and BPE implementaion to the same one in SentencePiece library and our implementation is ~2x faster, see XLNET benchmark and BPE benchmark. Not to mention our default models are 10x faster than the same functionality from SpaCy, see benchmark wiki and this Bing Blog Post. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468576834790875
      ],
      "excerpt": "If you want to create your own tokenization or any other finite-state model, you need to compile the C++ tools first. Then use these tools to compile linugusitc resources from human readble format into binary finite-state machines. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9701508395282978,
        0.9606249356594774,
        0.9606249356594774
      ],
      "excerpt": "Adding BERT-like tokenization model is describing how to add new tokenization model similar to BERT. \nHow to add a new Unigram LM model. \nHow to add a new BPE model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9395030198738896
      ],
      "excerpt": "Bling Fire is supported for Windows, Linux and Mac (Thanks to Andrew Kane!) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073737513954784,
        0.9783267603835809
      ],
      "excerpt": "This project has adopted the Microsoft Open Source Code of Conduct. \nFor more information see the Code of Conduct FAQ or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828225204827971
      ],
      "excerpt": "To contribute directly to code base, you should create a personal fork and create feature branches there when you need them. This keeps the main repository clean and your personal workflow out of sight. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314727105326414
      ],
      "excerpt": "However, you don't have to do this up-front. You can simply clone, fork, and submit your pull-request as usual. When your pull-request is created, it is classified by a CLA bot. If the change is trivial (i.e. you just fixed a typo) then the PR is labelled with  cla-not-required. Otherwise, it's classified as  cla-required. In that case, the system will also tell you how you can sign the CLA. Once you have signed a CLA, the current and all future pull-requests will be labelled as  cla-signed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.837070489968152
      ],
      "excerpt": "email to ensure we received your original message. Further information, including the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A lightning fast Finite State machine and REgular expression manipulation library.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/BlingFire/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 97,
      "date": "Sun, 26 Dec 2021 04:46:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/BlingFire/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft/BlingFire",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/BlingFire/master/doc/Bling%20Fire%20Tokenizer%20Demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9618621571567432
      ],
      "excerpt": "Setup your environment, once. You need to do this step once, it compiles retail version of the tools and adds the build directory to the PATH. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8996688920420451
      ],
      "excerpt": "However, you don't have to do this up-front. You can simply clone, fork, and submit your pull-request as usual. When your pull-request is created, it is classified by a CLA bot. If the change is trivial (i.e. you just fixed a typo) then the PR is labelled with  cla-not-required. Otherwise, it's classified as  cla-required. In that case, the system will also tell you how you can sign the CLA. Once you have signed a CLA, the current and all future pull-requests will be labelled as  cla-signed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8096331902379353
      ],
      "excerpt": "Response Center (MSRC) at secure@microsoft.com. You should \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8905278505328039
      ],
      "excerpt": "| bert_chinese.bin       | BERT Chinese                                | WordPiece | src |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9047198310975773,
        0.8446570154224069,
        0.9342534888004778,
        0.8905278505328039
      ],
      "excerpt": "| xlnet.bin | XLNET Tokenization Model    | Unigram LM | src |  \n| xlnet_nonorm.bin | XLNET Tokenization Model /wo normalization    | Unigram LM | src |  \n| bpe_example.bin | A model to test BPE tokenization    | BPE | src |  \n| xlm_roberta_base.bin | XLM Roberta Tokenization | Unigram LM | src |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8046575867781294
      ],
      "excerpt": "| roberta.bin | Byte-BPE tokenization model for Roberta model | byte BPE | src | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/BlingFire/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Batchfile",
      "Perl",
      "Python",
      "C#",
      "Gnuplot",
      "C",
      "Yacc",
      "CMake",
      "JavaScript",
      "HTML",
      "Makefile",
      "PHP",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Facebook, Inc. and its affiliates.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Bling Fire",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BlingFire",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/BlingFire/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "SergeiAlonichau",
        "body": "1. added IdsToText API for all models which return Ids, for example see https://github.com/microsoft/BlingFire/blob/master/scripts/blingfire_example.py ",
        "dateCreated": "2021-09-24T03:22:20Z",
        "datePublished": "2021-09-24T19:39:59Z",
        "html_url": "https://github.com/microsoft/BlingFire/releases/tag/v0.1.8",
        "name": "Bling Fire v0.1.8",
        "tag_name": "v0.1.8",
        "tarball_url": "https://api.github.com/repos/microsoft/BlingFire/tarball/v0.1.8",
        "url": "https://api.github.com/repos/microsoft/BlingFire/releases/50230154",
        "zipball_url": "https://api.github.com/repos/microsoft/BlingFire/zipball/v0.1.8"
      },
      {
        "authorType": "User",
        "author_name": "SergeiAlonichau",
        "body": "1. added no_dummy_prefix configuration and API to change the existing model configuration\r\n2. fixed the offset of the dummy prefix is now always -1, the first token may have start/end offset -1 it means dummy prefix is included\r\n3. change compilation options for Windows code",
        "dateCreated": "2021-05-25T05:22:22Z",
        "datePublished": "2021-05-25T06:07:04Z",
        "html_url": "https://github.com/microsoft/BlingFire/releases/tag/v0.1.7",
        "name": "Bling Fire v0.1.7",
        "tag_name": "v0.1.7",
        "tarball_url": "https://api.github.com/repos/microsoft/BlingFire/tarball/v0.1.7",
        "url": "https://api.github.com/repos/microsoft/BlingFire/releases/43502625",
        "zipball_url": "https://api.github.com/repos/microsoft/BlingFire/zipball/v0.1.7"
      },
      {
        "authorType": "User",
        "author_name": "SergeiAlonichau",
        "body": "- Added byte BPE algorithm support\r\n- Added GPT2, Roberta tokenization models\r\n- Added hyphenation / syllabification APIs and a sample model: syllab\r\n- Added URL tokenization models: uri100k, uri250k, uri500k\r\n- Some small changes in the C# interface (it should be backwards compatible), uses Span<byte> instead of byte[] to allow on stack allocations of input and output  buffers\r\n",
        "dateCreated": "2021-03-12T20:39:26Z",
        "datePublished": "2021-03-12T21:14:05Z",
        "html_url": "https://github.com/microsoft/BlingFire/releases/tag/v0.1.5",
        "name": "Bling Fire v0.1.5",
        "tag_name": "v0.1.5",
        "tarball_url": "https://api.github.com/repos/microsoft/BlingFire/tarball/v0.1.5",
        "url": "https://api.github.com/repos/microsoft/BlingFire/releases/39739072",
        "zipball_url": "https://api.github.com/repos/microsoft/BlingFire/zipball/v0.1.5"
      },
      {
        "authorType": "User",
        "author_name": "SergeiAlonichau",
        "body": "Four tokenization algorithms supported: patterns, word-piece, unigram lm, bpe. Added space normalization api, Added a few more popular models, added unigram lm tokenization models trained on uniformly represented ~84 languages from wikimatrix set. Bug fixes, parity fixes.",
        "dateCreated": "2020-06-25T17:11:20Z",
        "datePublished": "2020-06-25T17:27:23Z",
        "html_url": "https://github.com/microsoft/BlingFire/releases/tag/v0.1.3",
        "name": "blingfire pypi package v0.1.3",
        "tag_name": "v0.1.3",
        "tarball_url": "https://api.github.com/repos/microsoft/BlingFire/tarball/v0.1.3",
        "url": "https://api.github.com/repos/microsoft/BlingFire/releases/27927817",
        "zipball_url": "https://api.github.com/repos/microsoft/BlingFire/zipball/v0.1.3"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1582,
      "date": "Sun, 26 Dec 2021 04:46:41 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [Rust wrapper](https://github.com/reinfer/blingfire-rs)\r\n2. [Ruby wrapper](https://github.com/ankane/blingfire)\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\r\nfrom blingfire import *\r\n\r\ntext = 'After reading this post, you will know: What \"natural language\" is and how it is different from other types of data. What makes working with natural language so challenging. [1]'\r\n\r\nprint(text_to_sentences(text))\r\nprint(text_to_words(text))\r\n```\r\n\r\nExpected output:\r\n```\r\nAfter reading this post, you will know: What \"natural language\" is and how it is different from other types of data.\r\nWhat makes working with natural language so challenging. [1]\r\nAfter reading this post , you will know : What \" natural language \" is and how it is different from other types of data . What makes working with natural language so challenging . [ 1 ]\r\n```\r\n\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n```python\r\nfrom blingfire import *\r\n\r\n#: load a custom model from file\r\nh = load_model(\"./wbd_chuni.bin\")\r\n\r\ntext = 'This is the Bling-Fire tokenizer. 2007\u5e749\u6708\u65e5\u5386\u8868_2007\u5e749\u6708\u519c\u5386\u9633\u5386\u4e00\u89c8\u8868-\u4e07\u5e74\u5386'\r\n\r\n#: custom model output\r\nprint(text_to_words_with_model(h, text))\r\n\r\n#: default model output\r\nprint(text_to_words(text))\r\n\r\nfree_model(h)\r\n```\r\n\r\nExpected output:\r\n```\r\nThis is the Bling - Fire tokenizer . 2007 \u5e74 9 \u6708 \u65e5 \u5386 \u8868 _2007 \u5e74 9 \u6708 \u519c \u5386 \u9633 \u5386 \u4e00 \u89c8 \u8868 - \u4e07 \u5e74 \u5386\r\nThis is the Bling - Fire tokenizer . 2007\u5e749\u6708\u65e5\u5386\u8868_2007\u5e749\u6708\u519c\u5386\u9633\u5386\u4e00\u89c8\u8868 - \u4e07\u5e74\u5386\r\n```\r\n\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "On one thread, it works 14x faster than orignal BERT tokenizer written in Python. Given this code is written in C++ it can be called from multiple threads without blocking on global interpreter lock thus achieving higher speed-ups for batch mode.\r\n\r\n```python\r\nimport os\r\nimport blingfire\r\n\r\ns = \"\u042dpple pie. How do I renew my virtual smart card?: /Microsoft IT/ 'virtual' smart card certificates for DirectAccess are valid for one year. In order to get to microsoft.com we need to type pi@1.2.1.2.\"\r\n\r\n#: one time load the model (we are using the one that comes with the package)\r\nh = blingfire.load_model(os.path.join(os.path.dirname(blingfire.__file__), \"bert_base_tok.bin\"))\r\nprint(\"Model Handle: %s\" % h)\r\n\r\n#: use the model from one or more threads\r\nprint(s)\r\nids = blingfire.text_to_ids(h, s, 128, 100)  #: sequence length: 128, oov id: 100\r\nprint(ids)                                   #: returns a numpy array of length 128 (padded or trimmed)\r\n\r\n#: free the model at the end\r\nblingfire.free_model(h)\r\nprint(\"Model Freed\")\r\n```\r\n\r\nExpected output:\r\n```\r\nModel Handle: 2854016629088\r\n\u042dpple pie. How do I renew my virtual smart card?: /Microsoft IT/ 'virtual' smart card certificates for DirectAccess are valid for one year. In order to get to microsoft.com we need to type pi@1.2.1.2.\r\n[ 1208  9397  2571 11345  1012  2129  2079  1045 20687  2026  7484  6047\r\n  4003  1029  1024  1013  7513  2009  1013  1005  7484  1005  6047  4003\r\n 17987  2005  3622  6305  9623  2015  2024  9398  2005  2028  2095  1012\r\n  1999  2344  2000  2131  2000  7513  1012  4012  2057  2342  2000  2828\r\n 14255  1030  1015  1012  1016  1012  1015  1012  1016  1012     0     0\r\n     0     0     0     0     0     0     0     0     0     0     0     0\r\n     0     0     0     0     0     0     0     0     0     0     0     0\r\n     0     0     0     0     0     0     0     0     0     0     0     0\r\n     0     0     0     0     0     0     0     0     0     0     0     0\r\n     0     0     0     0     0     0     0     0     0     0     0     0\r\n     0     0     0     0     0     0     0     0]\r\nModel Freed\r\n```\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nSince hyphenation API's take one word at a time with the limit of 300 Unicode characters, we need to break the text into words first and then run hyphenation for each token.\r\n\r\n```python\r\nimport os\r\nimport blingfire\r\n\r\n#: load a provided with the package model\r\nh = blingfire.load_model(os.path.join(os.path.dirname(blingfire.__file__), \"syllab.bin\"))\r\n\r\n#: get a text\r\ntext = \"Like Curiosity, the Perseverance rover was built by engineers and scientists at NASA's Jet Propulsion Laboratory in Pasadena, California. Roughly 85% of Perseverance's mass is based on Curiosity \\\"heritage hardware,\\\" saving NASA time and money and reducing risk considerably, agency officials have said.  \u041a\u0430\u043a \u0438 Curiosity, \u043c\u0430\u0440\u0441\u043e\u0445\u043e\u0434 Perseverance \u0431\u044b\u043b \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d \u0438\u043d\u0436\u0435\u043d\u0435\u0440\u0430\u043c\u0438 \u0438 \u0443\u0447\u0435\u043d\u044b\u043c\u0438 \u0438\u0437 \u041b\u0430\u0431\u043e\u0440\u0430\u0442\u043e\u0440\u0438\u0438 \u0440\u0435\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0434\u0432\u0438\u0436\u0435\u043d\u0438\u044f \u041d\u0410\u0421\u0410 \u0432 \u041f\u0430\u0441\u0430\u0434\u0435\u043d\u0435, \u041a\u0430\u043b\u0438\u0444\u043e\u0440\u043d\u0438\u044f. \u041f\u043e \u0441\u043b\u043e\u0432\u0430\u043c \u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043b\u0438\u0446 \u0430\u0433\u0435\u043d\u0442\u0441\u0442\u0432\u0430, \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e 85% \u043c\u0430\u0441\u0441\u044b Perseverance \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043e \u043d\u0430 \u00ab\u0442\u0440\u0430\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u043e\u043c \u043e\u0431\u043e\u0440\u0443\u0434\u043e\u0432\u0430\u043d\u0438\u0438\u00bb Curiosity, \u0447\u0442\u043e \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0442 \u0432\u0440\u0435\u043c\u044f \u0438 \u0434\u0435\u043d\u044c\u0433\u0438 NASA \u0438 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0441\u043d\u0438\u0436\u0430\u0435\u0442 \u0440\u0438\u0441\u043a\u0438.\"\r\n\r\n#: break text into words with default model and hyphenate each word\r\noutput = \" \".join([blingfire.word_hyphenation_with_model(h, w) for w in blingfire.text_to_words(text).split(' ')])\r\nprint(output)\r\n\r\n#: free the model after we are all done\r\nblingfire.free_model(h)\r\n```\r\n\r\nThe output should be something like this: \r\n```\r\nLi-ke Cu-rios-i-ty , the Per-se-ve-rance ro-ver was built by en-gi-neers and sci-en-tists at NASA 's Jet Pro-pul-sion La-bo-ra-to-ry in Pa-sa-dena , Cali-for-nia . Roughly 85 % of Per-se-ve-rance 's mass is ba-se-d on Cu-rios-i-ty \" he-r-i-tage hard-ware , \" sa-ving NASA time and money and re-du-c-ing risk con-si-de-r-ably , agen-cy of-fi-cials ha-ve said . \u041a\u0430-\u043a \u0438 Cu-rios-i-ty , \u043c\u0430\u0440-\u0441\u043e-\u0445\u043e\u0434 Per-se-ve-rance \u0431\u044b-\u043b \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d \u0438\u043d-\u0436\u0435-\u043d\u0435-\u0440\u0430\u043c\u0438 \u0438 \u0443-\u0447\u0435-\u043d\u044b-\u043c\u0438 \u0438\u0437 \u041b\u0430-\u0431\u043e\u0440\u0430-\u0442\u043e-\u0440\u0438\u0438 \u0440\u0435-\u0430\u043a\u0442\u0438\u0432-\u043d\u043e\u0433\u043e \u0434\u0432\u0438-\u0436\u0435-\u043d\u0438\u044f \u041d\u0410\u0421\u0410 \u0432 \u041f\u0430-\u0441\u0430-\u0434\u0435\u043d\u0435 , \u041a\u0430-\u043b\u0438-\u0444\u043e\u0440-\u043d\u0438\u044f . \u041f\u043e \u0441\u043b\u043e-\u0432\u0430\u043c \u043e\u0444\u0438-\u0446\u0438-\u0430\u043b\u044c-\u043d\u044b\u0445 \u043b\u0438-\u0446 \u0430\u0433\u0435\u043d\u0442-\u0441\u0442\u0432\u0430 , \u043f\u0440\u0438-\u043c\u0435\u0440\u043d\u043e 85 % \u043c\u0430\u0441-\u0441\u044b Per-se-ve-rance \u043e\u0441\u043d\u043e-\u0432\u0430\u043d\u043e \u043d\u0430 \u00ab \u0442\u0440\u0430-\u0434\u0438-\u0446\u0438-\u043e\u043d-\u043d\u043e\u043c \u043e\u0431\u043e-\u0440\u0443-\u0434\u043e-\u0432\u0430-\u043d\u0438\u0438 \u00bb Cu-rios-i-ty , \u0447\u0442\u043e \u044d\u043a\u043e-\u043d\u043e-\u043c\u0438\u0442 \u0432\u0440\u0435-\u043c\u044f \u0438 \u0434\u0435\u043d\u044c\u0433\u0438 NASA \u0438 \u0437\u043d\u0430-\u0447\u0438-\u0442\u0435-\u043b\u044c\u043d\u043e \u0441\u043d\u0438-\u0436\u0430-\u0435\u0442 \u0440\u0438\u0441\u043a\u0438 .\r\n```\r\n\r\nNote you can specify any other Unicode character as a hyphen that API inserts into the output string.\r\n\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nNote, everything that is supported in Python is supported by C",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n```csharp\r\nusing System;\r\nusing BlingFire;\r\n\r\nnamespace BlingUtilsTest\r\n{\r\n    class Program\r\n    {\r\n        static void Main(string[] args)\r\n        {\r\n            // load XLM Roberta tokenization model\r\n            var h = BlingFireUtils.LoadModel(\"./xlm_roberta_base.bin\");\r\n\r\n\r\n            // input string\r\n            string input = \"Autophobia, also called monophobia, isolophobia, or eremophobia, is the specific phobia of isolation. I saw a girl with a telescope. \u042f \u0443\u0432\u0438\u0434\u0435\u043b \u0434\u0435\u0432\u0443\u0448\u043a\u0443 \u0441 \u0442\u0435\u043b\u0435\u0441\u043a\u043e\u043f\u043e\u043c.\";\r\n            // get its UTF8 representation\r\n            byte[] inBytes = System.Text.Encoding.UTF8.GetBytes(input);\r\n\r\n\r\n            // allocate space for ids and offsets\r\n            int[] Ids =  new int[128];\r\n            int[] Starts =  new int[128];\r\n            int[] Ends =  new int[128];\r\n\r\n            // tokenize with loaded XLM Roberta tokenization and output ids and start and end offsets\r\n            outputCount = BlingFireUtils.TextToIdsWithOffsets(h, inBytes, inBytes.Length, Ids, Starts, Ends, Ids.Length, 0);\r\n            Console.WriteLine(String.Format(\"return length: {0}\", outputCount));\r\n            if (outputCount >= 0)\r\n            {\r\n                Console.Write(\"tokens from offsets: [\");\r\n                for(int i = 0; i < outputCount; ++i)\r\n                {\r\n                    int startOffset = Starts[i];\r\n                    int surfaceLen = Ends[i] - Starts[i] + 1;\r\n\r\n                    string token = System.Text.Encoding.UTF8.GetString(new ArraySegment<byte>(inBytes, startOffset, surfaceLen));\r\n                    Console.Write(String.Format(\"'{0}'/{1} \", token, Ids[i]));\r\n                }\r\n                Console.WriteLine(\"]\");\r\n            }\r\n\r\n            // free loaded models\r\n            BlingFireUtils.FreeModel(h);\r\n        }\r\n    }\r\n}\r\n```\r\nThis code will print the following output:\r\n```\r\nreturn length: 49\r\ntokens from offsets: ['Auto'/4396 'pho'/22014 'bia'/9166 ','/4 ' also'/2843 ' called'/35839 ' mono'/22460 'pho'/22014 'bia'/9166 ','/4 ' is'/83 'olo'/7537 'pho'/22014 'bia'/9166 ','/4 ' or'/707 ' '/6 'eremo'/102835 'pho'/22014 'bia'/9166 ','/4 ' is'/83 ' the'/70 ' specific'/29458 ' pho'/53073 'bia'/9166 ' of'/111 ' '/6 'isolation'/219488 '.'/5 ' I'/87 ' saw'/24124 ' a'/10 ' girl'/23040 ' with'/678 ' a'/10 ' tele'/5501 'scope'/70820 '.'/5 ' \u042f'/1509 ' \u0443\u0432\u0438\u0434\u0435\u043b'/79132 ' \u0434\u0435\u0432'/29513 '\u0443'/105 '\u0448\u043a\u0443'/46009 ' \u0441'/135 ' \u0442\u0435\u043b\u0435'/18293 '\u0441\u043a\u043e\u043f'/41333 '\u043e\u043c'/419 '.'/5 ]\r\n```\r\nSee this project for more C",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": " \r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nThe goal of integration with JavaScript is ability to run the code in a browser with ML frameworks like TensorFlow.js and FastText web assembly.\r\n\r\nNote: this work is still in progress, we are likely to make some changes/improvements there.\r\n\r\n```javascript\r\nimport { GetVersion, TextToWords, TextToSentences, LoadModel, FreeModel, TextToIds } from './blingfire_wrapper.js';\r\n\r\n$(document).ready(function() {\r\n\r\n  var text = \"I saw a girl with a telescope. \u042f \u0432\u0438\u0434\u0435\u043b \u0434\u0435\u0432\u0443\u0448\u043a\u0443 \u0441 \u0442\u0435\u043b\u0435\u0441\u043a\u043e\u043f\u043e\u043c.\";\r\n\r\n  var modelHandle1 = null;\r\n\r\n  $(\"#:btn4\").click(function () {\r\n    if(modelHandle1 == null) {\r\n      (async function () {\r\n        modelHandle1 = await LoadModel(\"./bert_base_tok.bin\");\r\n        console.log(\"Model handle: \" + modelHandle1);\r\n      })();\r\n    }\r\n  });\r\n\r\n  $(\"#:btn5\").click(function () {\r\n    if(modelHandle1 != null) {\r\n      FreeModel(modelHandle1);\r\n      modelHandle1 = null;\r\n      console.log(\"Model Freed!\");\r\n    }\r\n  });\r\n\r\n  $(\"#:btn6\").click(function () {\r\n    if(modelHandle1 != null) {\r\n      console.log(TextToIds(modelHandle1, text, 128));\r\n    } else {\r\n      console.log(\"Load the model first!\");\r\n    }\r\n  });\r\n\r\n});\r\n```\r\n\r\nFull example code can be found [here](https://github.com/microsoft/BlingFire/blob/master/wasm/example.html). Details of the API are described in the [wasm](https://github.com/microsoft/BlingFire/tree/master/wasm) folder.\r\n\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n[This notebook](/doc/Bling%20Fire%20Tokenizer%20Demo.ipynb) demonstrates how Bling Fire tokenizer helps in Stack Overflow posts classification problem.\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n[This document](https://github.com/microsoft/BlingFire/wiki/How-to-train-better-language-detection-with-Bling-Fire-and-FastText) describes how to improve [FastText](https://fasttext.cc/) language detection model with Bling Fire and achive 99% accuracy in language detection task for 365 languages.\r\n\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}