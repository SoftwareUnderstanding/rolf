{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.3215",
      "https://arxiv.org/abs/1409.0473"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9898689759446914
      ],
      "excerpt": "https://dl.acm.org/citation.cfm?id=1246450,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8437025776904489
      ],
      "excerpt": "Based on: https://nlp.stanford.edu/pubs/emnlp15_attn.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9583785825568012
      ],
      "excerpt": "    #: (equation (10) in https://nlp.stanford.edu/pubs/emnlp15_attn.pdf) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "if load.lower() == 'y': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    best_loss = 2**30 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if i % display_step == 0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if i % 500 == 0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221899747000617
      ],
      "excerpt": "                if word == '&lt;EOS&gt;': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "                    print(word,end=\" \") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221899747000617
      ],
      "excerpt": "                if word == '&lt;EOS&gt;': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "                    print(word,end=\" \") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if i%100==0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "    if (avg_val_loss &lt; best_loss): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "    if impatience &gt; patience: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "Iter 1500, Cost= 0.608, Acc = 29.32% \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JRC1995/Abstractive-Summarization",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-10-04T15:40:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T09:18:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9287942022083909,
        0.8471597874509949,
        0.9234677930986261
      ],
      "excerpt": "Based on Seq2seq learning \nwith attention mechanism, specifically local attention. \nThe Data is preprocessed in Data_Pre-Processing.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "idx2vocab = {v:k for k,v in vocab2idx.items()} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016,
        0.829901958723446
      ],
      "excerpt": "Instructions for updating: \nnon-resource variables are not supported in the long term \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8886034359529978
      ],
      "excerpt": "More info:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "with tf.variable_scope(scope,reuse=tf.AUTO_REUSE): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.952967601322711
      ],
      "excerpt": "The final output is a combination (in this case, a concatenation) of the forward encoded text and the backward encoded text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8619089259402867
      ],
      "excerpt": ":shape of embd_text: [N,S,embd_dim] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.949759956164784
      ],
      "excerpt": "Given a sequence of encoder states ($H_s$) and the decoder hidden state ($H_t$) of current timestep $t$, the equation for computing attention score is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "with tf.variable_scope(scope,reuse=tf.AUTO_REUSE): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "with tf.variable_scope(scope,reuse=tf.AUTO_REUSE): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8281668711377833,
        0.9716326508984031
      ],
      "excerpt": "#: ps = (soft-)predicted starting position of attention window \npt = ps+D #: pt = center of attention window where the whole window length is 2*D+1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9399631341443562
      ],
      "excerpt": "#: It tells the decoder that it is about to decode the first word of the output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "regularization = tf.reduce_sum([tf.nn.l2_loss(var) for var \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9459964997420707
      ],
      "excerpt": ": Masking to ignore the effect of pads while calculating accuracy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "            for word in predicted_summary: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "            for word in actual_summary: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8434521412067304
      ],
      "excerpt": "i love my starbucks sumatra first thing in the morning . i was not always up early enough to take the detour to starbucks and now i do n't have to ! these &lt;UNK&gt; are perfect and delicious . now i can have my fav coffee even before i take off my slippers ! i love this product ! it 's easy to order - arrived quickly and the price was good . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "good substitute for coffee . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8266442518938039
      ],
      "excerpt": "can no longer find this product locally anymore . i purchased it previously at a warehouse club but costco , bj ` s and sam ` s club no longer stock it in my area stores . my two golden retriever ` s love this gravy when added to their mix of both dry and moist dog food . hope it stays on the market ... &lt;UNK&gt; ! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9741250212234257
      ],
      "excerpt": "i want to start out by saying that i thought at first that a bag with only 120 calories and 4 grams of fat ( no saturated or trans ) for every 20 chips was going to taste like crap . i must say that not only was i wrong , that this is my favorite bbq chip on the market today . they are light and you can not taste any fat or grease after eating them . that 's because they are n't baked or fried , just popped as their name suggests . these chips are very easy to dip as well . fantastic product ! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9872385612101005
      ],
      "excerpt": "this &lt;UNK&gt; of 7-ounce `` taster 's choice french roast '' canisters , is a good buy . the coffee is flavored differently than original flavor , but the difference is very subtle , and refreshingly good . overall , this taster 's choice coffee is a bargain , and highly recommended . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of abstractive summarization using LSTM in the encoder-decoder architecture with local attention.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JRC1995/Abstractive-Summarization/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 56,
      "date": "Sun, 26 Dec 2021 05:53:24 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JRC1995/Abstractive-Summarization/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "JRC1995/Abstractive-Summarization",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/JRC1995/Abstractive-Summarization/master/Summarization.ipynb",
      "https://raw.githubusercontent.com/JRC1995/Abstractive-Summarization/master/Data_Pre-Processing.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8644849407710178,
        0.858204411280099
      ],
      "excerpt": "WARNING:tensorflow:From /home/jishnu/miniconda3/envs/ML/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version. \nInstructions for updating: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8206851346323828
      ],
      "excerpt": "husband loves this tea especially in the &lt;UNK&gt; recommend using the large cup setting on your keurig brewer unless you prefer your tea extra strong . \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9180062578030207,
        0.8932891814861686
      ],
      "excerpt": "import json \nwith open('Processed_Data/Amazon_Reviews_Processed.json') as file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9006394831022142,
        0.8172342705654815,
        0.8188774533332797,
        0.8633506362289554,
        0.8293630417981156,
        0.8188774533332797,
        0.8128131768734507
      ],
      "excerpt": "import tensorflow.compat.v1 as tf  \nembd_dim = len(embd[0]) \ntf_text = tf.placeholder(tf.int32, [None, None]) \ntf_embd = tf.placeholder(tf.float32, [len(vocab2idx),embd_dim]) \ntf_true_summary_len = tf.placeholder(tf.int32, [None]) \ntf_summary = tf.placeholder(tf.int32,[None, None]) \ntf_train = tf.placeholder(tf.bool) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8069275751695314
      ],
      "excerpt": "    return tf.cond(tf_train, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "embd_text = tf.nn.embedding_lookup(tf_embd, tf_text) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8098308255451723,
        0.8589102878178917,
        0.8186790943648705
      ],
      "excerpt": "with tf.variable_scope(scope,reuse=tf.AUTO_REUSE): \n    w = tf.get_variable(\"w\", shape=[4,input_dim,hidden_size], \n                                dtype=tf.float32, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8589102878178917,
        0.8186790943648705
      ],
      "excerpt": "                                initializer=tf.glorot_uniform_initializer()) \n    u = tf.get_variable(\"u\", shape=[4,hidden_size,hidden_size], \n                        dtype=tf.float32, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8589102878178917,
        0.8186790943648705
      ],
      "excerpt": "                        initializer=tf.glorot_uniform_initializer()) \n    b = tf.get_variable(\"bias\", shape=[4,1,hidden_size], \n                dtype=tf.float32, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8123763140827432,
        0.8123763140827432,
        0.8123763140827432,
        0.8123763140827432
      ],
      "excerpt": "                initializer=tf.zeros_initializer()) \ninput_gate = tf.nn.sigmoid( tf.matmul(x,w[0]) + tf.matmul(hidden_state,u[0]) + b[0]) \nforget_gate = tf.nn.sigmoid( tf.matmul(x,w[1]) + tf.matmul(hidden_state,u[1]) + b[1]) \noutput_gate = tf.nn.sigmoid( tf.matmul(x,w[2]) + tf.matmul(hidden_state,u[2]) + b[2]) \ncell_ = tf.nn.tanh( tf.matmul(x,w[3]) + tf.matmul(hidden_state,u[3]) + b[3]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "hidden_state = output_gate*tf.tanh(cell) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8892590354894273,
        0.8718383063938133
      ],
      "excerpt": "S = tf.shape(embd_text)[1] #:text sequence length \nN = tf.shape(embd_text)[0] #:batch_size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8538773884280993,
        0.8538773884280993,
        0.8530158888138687
      ],
      "excerpt": "hidden=tf.zeros([N, hidden_size], dtype=tf.float32) \ncell=tf.zeros([N, hidden_size], dtype=tf.float32) \nhidden_forward=tf.TensorArray(size=S, dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "embd_text_t = tf.transpose(embd_text,[1,0,2])  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230254167766119
      ],
      "excerpt": "hidden_forward = hidden_forward.write(i, hidden) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8538773884280993,
        0.8538773884280993,
        0.8530158888138687
      ],
      "excerpt": "hidden=tf.zeros([N, hidden_size], dtype=tf.float32) \ncell=tf.zeros([N, hidden_size], dtype=tf.float32) \nhidden_backward=tf.TensorArray(size=S, dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230254167766119
      ],
      "excerpt": "    hidden_backward = hidden_backward.write(i, hidden) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.84760898352076,
        0.8123763140827432
      ],
      "excerpt": "encoder_states = tf.concat([hidden_forward,hidden_backward],axis=-1) \nencoder_states = tf.transpose(encoder_states,[1,0,2]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8743745540667157
      ],
      "excerpt": "final_encoded_state = dropout(tf.concat([hidden_forward[-1],hidden_backward[-1]],axis=-1),rate=0.3,training=tf_train) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8098308255451723,
        0.8589102878178917,
        0.8186790943648705
      ],
      "excerpt": "with tf.variable_scope(scope,reuse=tf.AUTO_REUSE): \n    Wa = tf.get_variable(\"Wa\", shape=[2*hidden_size,2*hidden_size], \n                                dtype=tf.float32, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "                                initializer=tf.glorot_uniform_initializer()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8151670105623777
      ],
      "excerpt": "encoder_states = tf.reshape(tf.matmul(encoder_states,Wa),[N,S,2*hidden_size]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8267978215883168
      ],
      "excerpt": "return tf.reshape(tf.matmul(encoder_states,decoder_hidden_state),[N,S]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8098308255451723,
        0.8845186727393237,
        0.8186790943648705
      ],
      "excerpt": "with tf.variable_scope(scope,reuse=tf.AUTO_REUSE): \n    Wp = tf.get_variable(\"Wp\", shape=[2*hidden_size,128], \n                                dtype=tf.float32, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8845186727393237,
        0.8186790943648705
      ],
      "excerpt": "                                initializer=tf.glorot_uniform_initializer()) \n    Vp = tf.get_variable(\"Vp\", shape=[128,1], \n                        dtype=tf.float32, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "                        initializer=tf.glorot_uniform_initializer()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "ps = positions*tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(decoder_hidden_state,Wp)),Vp)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8530158888138687,
        0.8370241539307991
      ],
      "excerpt": "gaussian_position_based_scores = tf.TensorArray(size=S,dtype=tf.float32) \nsigma = tf.constant(D/2,dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8439631742160844
      ],
      "excerpt": "    score = tf.exp(-((tf.square(tf.cast(i,tf.float32)-pt))/(2*tf.square(sigma))))  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "gaussian_position_based_scores = tf.transpose(gaussian_position_based_scores,[1,0]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589102878178917,
        0.8186790943648705
      ],
      "excerpt": "    SOS = tf.get_variable(\"sos\", shape=[1,embd_dim], \n                                dtype=tf.float32, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "                                initializer=tf.glorot_uniform_initializer()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589102878178917,
        0.8186790943648705
      ],
      "excerpt": "Wc = tf.get_variable(\"Wc\", shape=[4*hidden_size,embd_dim], \n                        dtype=tf.float32, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8953903300454644
      ],
      "excerpt": "                        initializer=tf.glorot_uniform_initializer()) \nSOS = tf.tile(SOS,[N,1]) #:now SOS shape: [N,embd_dim] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8538773884280993,
        0.8530158888138687,
        0.8318846814185316
      ],
      "excerpt": "cell=tf.zeros([N, 2*hidden_size], dtype=tf.float32) \ndecoder_outputs=tf.TensorArray(size=max_summary_len, dtype=tf.float32) \noutputs=tf.TensorArray(size=max_summary_len, dtype=tf.int32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.84760898352076
      ],
      "excerpt": "encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.84760898352076
      ],
      "excerpt": "inp = tf.concat([inp,encoder_context_vector],axis=-1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.84760898352076,
        0.84760898352076,
        0.8123763140827432,
        0.8123763140827432
      ],
      "excerpt": "encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1) \nconcated = tf.concat([hidden,encoder_context_vector],axis=-1) \nlinear_out = tf.nn.tanh(tf.matmul(concated,Wc)) \ndecoder_output = tf.matmul(linear_out,tf.transpose(tf_embd,[1,0]))  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230254167766119
      ],
      "excerpt": "decoder_outputs = decoder_outputs.write(i,decoder_output) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "next_word = tf.nn.embedding_lookup(tf_embd, next_word_vec) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "decoder_outputs = tf.transpose(decoder_outputs,[1,0,2]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537312864908407,
        0.8605821333926196,
        0.8710603055174466,
        0.8856502825202467
      ],
      "excerpt": "filtered_trainables = [var for var in tf.trainable_variables() if \n                       not(\"Bias\" in var.name or \"bias\" in var.name \n                           or \"noreg\" in var.name)] \nregularization = tf.reduce_sum([tf.nn.l2_loss(var) for var \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.814789656623384,
        0.8123763140827432
      ],
      "excerpt": "epsilon = tf.constant(1e-9, tf.float32) \ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "pad_mask = tf.sequence_mask(tf_true_summary_len, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8186790943648705
      ],
      "excerpt": "                            dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "cost = tf.reduce_mean(masked_cross_entropy) + \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "cross_entropy = tf.reduce_mean(masked_cross_entropy) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "                     tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "pad_mask = tf.sequence_mask(tf_true_summary_len, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8186790943648705,
        0.8123763140827432
      ],
      "excerpt": "                            dtype=tf.bool) \nmasked_comparison = tf.boolean_mask(comparison, pad_mask) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8936954105699045
      ],
      "excerpt": "all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) \noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8074286628742857
      ],
      "excerpt": "capped_gvs = [(tf.clip_by_norm(grad, 5), var) for grad, var in gvs] #: Gradient Clipping \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214055523315815,
        0.8722253495125046,
        0.8383163063168253
      ],
      "excerpt": "import pickle \nimport random \nwith tf.Session() as sess:  #: Start Tensorflow Session \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.812546482972794,
        0.8936954105699045
      ],
      "excerpt": "load = input(\"\\nLoad checkpoint? y/n: \") \nsaver = tf.train.Saver() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.858539997398074
      ],
      "excerpt": "    print('Loading pre-trained weights for the model...') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8670539095623045,
        0.8670539095623045
      ],
      "excerpt": "    sess.run(tf.global_variables()) \n    sess.run(tf.tables_initializer()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589185899045798
      ],
      "excerpt": "    print('\\nRESTORATION COMPLETE\\n') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8670539095623045
      ],
      "excerpt": "    sess.run(tf.tables_initializer()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9446546964994679
      ],
      "excerpt": "    print(\"\\n\\nSTARTING TRAINING\\n\\n\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "            acc, _ = sess.run([cross_entropy, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "                                         tf_train: True}) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9163086562459459
      ],
      "excerpt": "            print(\"Iter \"+str(i)+\", Cost= \" + \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8339556183526199
      ],
      "excerpt": "                  \"{:.2f}%\".format(acc*100)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216008240134867
      ],
      "excerpt": "            idx = random.randint(0,len(train_batches_text[j])-1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9442417739250801,
        0.9442417739250801,
        0.8815517450553388
      ],
      "excerpt": "            print(\"\\nSample Text\\n\") \n            print(text) \n            print(\"\\nSample Predicted Summary\\n\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785
      ],
      "excerpt": "            print(\"\\n\\nSample Actual Summary\\n\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785,
        0.8992931915840741
      ],
      "excerpt": "            print(\"\\n\\n\") \n    print(\"\\n\\nSTARTING VALIDATION\\n\\n\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9461021381405468
      ],
      "excerpt": "            print(\"Validating data #: {}\".format(i)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "            acc = sess.run([cross_entropy, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815,
        0.9294184993106162,
        0.9173340317180552,
        0.9200679377396996,
        0.8702954393371198,
        0.9055346347773922
      ],
      "excerpt": "    avg_val_loss = total_val_loss/len(val_batches_text) \n    print(\"\\n\\nEpoch: {}\\n\\n\".format(epoch+covered_epochs)) \n    print(\"Average Training Loss: {:.3f}\".format(total_train_loss/len(train_batches_text))) \n    print(\"Average Training Accuracy: {:.2f}\".format(100*total_train_acc/len(train_batches_text))) \n    print(\"Average Validation Loss: {:.3f}\".format(avg_val_loss)) \n    print(\"Average Validation Accuracy: {:.2f}\".format(100*total_val_acc/len(val_batches_text))) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.82991399582984
      ],
      "excerpt": "        print(\"\\nModel saved\\n\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "STARTING TRAINING \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "Sample Text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample Actual Summary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "Sample Text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample Actual Summary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "Sample Text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample Actual Summary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "Sample Text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample Actual Summary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "Sample Text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample Actual Summary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597,
        0.8823867403514597
      ],
      "excerpt": "Iter 2500, Cost= 0.900, Acc = 21.15% \nSample Text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample Actual Summary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "Sample Text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample Actual Summary \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JRC1995/Abstractive-Summarization/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Jishnu Ray Chowdhury\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Abstractive Summarization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Abstractive-Summarization",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "JRC1995",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JRC1995/Abstractive-Summarization/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 137,
      "date": "Sun, 26 Dec 2021 05:53:24 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "abstractive-text-summarization",
      "nlp",
      "deep-learning",
      "encoder-decoder",
      "attention-mechanism",
      "local-attention",
      "tensorflow",
      "lstm"
    ],
    "technique": "GitHub API"
  }
}