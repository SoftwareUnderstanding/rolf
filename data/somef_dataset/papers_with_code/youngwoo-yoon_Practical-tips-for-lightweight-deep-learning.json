{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/youngwoo-yoon/Practical-tips-for-lightweight-deep-learning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-08T09:54:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-03T11:31:21Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.942377913484029,
        0.8955656344539618,
        0.899593720467488
      ],
      "excerpt": "Here I'd like to share some tips after struggling to run a deep learning model on desktop CPUs and mobile phones. \nThere is no silver bullet working best for all environments in my experiences. Try different frameworks and compare the performance in your target environment. And use the latest version of the frameworks. \nWhen you try different frameworks, train the model again in the frameworks. There are model conversion tools like ONNX, but they are not optimized yet. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/youngwoo-yoon/Practical-tips-for-lightweight-deep-learning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 20:42:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/youngwoo-yoon/Practical-tips-for-lightweight-deep-learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "youngwoo-yoon/Practical-tips-for-lightweight-deep-learning",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8866024484240201
      ],
      "excerpt": "Some optimization options might be disabled in the precompiled pip versions. Try to enable all optimizations to get maximum performances. You have to compile the frameworks by yourself. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/youngwoo-yoon/Practical-tips-for-lightweight-deep-learning/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Practical tips for lightweight deep learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Practical-tips-for-lightweight-deep-learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "youngwoo-yoon",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/youngwoo-yoon/Practical-tips-for-lightweight-deep-learning/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 20:42:16 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Mobilenet V2 is a safe choice. It have been tested in various problems, and you can find implementations and pretrained models for most deep learning frameworks. Also, the operations in Mobilenet are highly optimized in the frameworks. \n\nYou should be careful when you try a new architecture. A new architecture having fewer parameters can be slower when the architecture is not optimized for CPUs and mobile environments. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "GPU is definitely faster than CPU. It is true for mobile phones. TF-lite [https://www.tensorflow.org/lite/performance/gpu] supports mobile GPUs.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Weight quantization is a good way to reduces inference latency without much loss in accuracy. See links below. \n\nTF lite [https://www.tensorflow.org/lite/performance/post_training_quantization]  \nQNNPACK for Caffe2 [https://code.fb.com/ml-applications/qnnpack/]\n\nBe aware that the operations in your network should have implementations for quantization. \n\n",
      "technique": "Header extraction"
    }
  ]
}