{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.5602v1",
      "https://arxiv.org/abs/1509.06461",
      "https://arxiv.org/abs/1511.06581",
      "https://arxiv.org/abs/1511.05952"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [DRL_paper_summary](https://github.com/RPC2/DRL_paper_summary)\n- [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.854586659907898,
        0.9911063870772613,
        0.9995778566230304,
        0.9966508017704312
      ],
      "excerpt": "Deep Q Network (DQN) from Playing Atari with Deep Reinforcement Learning, Mnih et al, 2013. [arxiv] [summary] \nDouble DQN Deep Reinforcement Learning with Double Q-learning, Hasselt et al 2015. [arxiv] [summary] \nDueling DQN Dueling Network Architectures for Deep Reinforcement Learning, Wang et al, 2015. [arxiv] [summary] \nPrioritized Experience Replay (PER) Prioritized Experience Replay, Schaul et al, 2015.  [arxiv] [summary] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if dueling: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "UPDATE_FREQ = 10 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kmdanielduan/DQN_Family_PyTorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-23T01:44:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-09T02:31:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9966793584267761
      ],
      "excerpt": "This is a repository of DQN and its variants implementation in PyTorch based on the original papar. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9816659668542487
      ],
      "excerpt": "The Q-network I use here is 3-hidden-layer perceptrons(MLP). The hidden_size is 32. The option of dueling network is also included. Double and PER are implemented in the agent codes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9751882087545173,
        0.8268135115888302
      ],
      "excerpt": "When evaluating the performance of the model, I wrote a class method called demo. demo basically plays the game for 100 times by exploiting the actions generated by the policy network (equivalent to <img src=\"https://latex.codecogs.com/svg.latex?\\large&space;\\epsilon\" title=\"\\large \\epsilon\" /> = 1.0), and get the average score of the games as the score of the current policy network. \nThe policy network scores, and average scores of the past 10 versions of policy network, as well as the current episode duration are plotted in the result.png. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382800719816385
      ],
      "excerpt": "In additional to the settings above, the augmented integrated agent also has the following settings: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8329616626349666,
        0.9127819163381757
      ],
      "excerpt": "The loss function(after accumulating weight change) will be multiplied with abs_errors in order to further scale up the gradients of the prioritized transitions  \nThe greedy actions were sampled by the target net, with the hope to stablize the training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933079502735492,
        0.8799488524219832
      ],
      "excerpt": "|  2   |   1e-4    |    0.99    |     32     |   0.99    |  | Small learning rate makes the network hard to learn anything. Not converging at all. | \n|  3   |   1e-3    |    0.99    |     32     |   0.99    |  | Solved the game after around 200 episodes. Displayed a pattern of high probability of divergence due to high learning rate. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.890371602093251,
        0.916096658511949,
        0.8342437043004592,
        0.9142248847117204
      ],
      "excerpt": "|  5   |     5e-4      |    0.99    |   16   |   0.99    |  | Smaller batch size makes the  policy stay in high performance but much more noise. | \n|  6   |     5e-4      |    0.99    |   8    |   0.99    |  | Smaller batch size makes the  policy stay in high performance but much more noise. | \n|  7   |     5e-4      |    0.99    |     16     | 0.999 |  | Higher gamma means preservation of the past learned knowledge. Solved the game around 100 episodes, and stayed there for around 80 episodes. | \n|  8   |     5e-4      |    0.99    |     16     |  0.9  |  | Lower gamma leads to high variations of the performance.     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is a repository of DQN and its variants implementation in PyTorch based on the original papar.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kmdanielduan/DQN_Family_PyTorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 04:47:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kmdanielduan/DQN_Family_PyTorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kmdanielduan/DQN_Family_PyTorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8233588558014837
      ],
      "excerpt": "        abs_errors_ = abs_errors.numpy()  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.882144032723717
      ],
      "excerpt": "Environment: \"CartPole-v1\" \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "MIN_EPS = 0.01 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228662911826681
      ],
      "excerpt": "PER = False         #: prioritized replay \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.849277257638252
      ],
      "excerpt": "|  91  |     5e-4(step: 400)     |   True    |   True    |  True    |  | Solved the game after 1050 episodes. | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kmdanielduan/DQN_Family_PyTorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Q-Learning in PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DQN_Family_PyTorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kmdanielduan",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kmdanielduan/DQN_Family_PyTorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sat, 25 Dec 2021 04:47:05 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "| No.  | learning rate |  double   | dueling |    PER    | result.png                       | Comments                                                     |\n| :--: | :-----------: | :-------: | :-----: | :-------: | -------------------------------- | ------------------------------------------------------------ |\n|  11  |     25e-5     |   False   |   False    |   False    | ![11-result](assets/11-result.png) | High variance in training, but making stable progress. |\n|  21   |   25e-5    |  **True** |   False    |   False    | ![21-result](assets/21-result.png) | Double Q-learning decreases abrupt performance degradation. |\n|  32   |   5e-4(step:200)    |   False    |  **True** |   False    | ![32-result](assets/32-result.png) | Deuling network seems to make it worse :) |\n|  42  | 5e-4(step:200) |   False    | False  |  **True** | ![42-result](assets/42-result.png) | PER makes the network boost to high performance quickly, but followed by huge degradation. |\n|  51   |     25e-5      |   False    | **True** |  **True** | ![51-result](assets/51-result.png) | The performance was maintained on a high level after 770 episodes, but constantly harmed by variance. |\n|  61   |     25e-5      |  **True** |  False  |  **True** | ![61-result](assets/61-result.png) | The training seems more stable, but takes longer to get to high performance. |\n|  72   |     5e-4(step:200)      |  **True** |  **True** | False | ![72-result](assets/72-result.png) | The agent achieved high performance at 100 episodes, but quickly degrades after that. |\n|  81   |     25e-5      |  **True** |  **True** |  **True**  | ![81-result](assets/81-result.png) | Relatively steady growth, but high variance when training. |\n\n",
      "technique": "Header extraction"
    }
  ]
}