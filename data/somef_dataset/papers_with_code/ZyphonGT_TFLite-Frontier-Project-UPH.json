{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Google CodeLab](https://codelabs.developers.google.com)\n* [Google Images Download](https://google-images-download.readthedocs.io) by [hardikvasa](https://github.com/hardikvasa)\n* [Autocrop](https://github.com/leblancfg/autocrop) by [leblancfg](https://github.com/leblancfg)\n* [A Basic Introduction to Separable Convolutions](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728) by [Chi-Feng Wang]\n* [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision\nApplications](https://arxiv.org/pdf/1704.04861.pdf) by (Andrew G. Howard Menglong Zhu Bo Chen Dmitry Kalenichenko\nWeijun Wang Tobias Weyand Marco Andreetto Hartwig Adam) \n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n[build-shield]: https://img.shields.io/badge/build-passing-brightgreen.svg?style=flat-square\n[build-url]: #\n[contributors-shield]: https://img.shields.io/badge/contributors-1-orange.svg?style=flat-square\n[contributors-url]: https://github.com/othneildrew/Best-README-Template/graphs/contributors\n[license-shield]: https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square\n[license-url]: https://choosealicense.com/licenses/mit\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&logo=linkedin&colorB=555\n[linkedin-url]: https://linkedin.com/in/othneildrew\n[product-screenshot]: https://raw.githubusercontent.com/othneildrew/Best-README-Template/master/screenshot.png\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.9972508553966268,
        0.9061005469315193,
        0.9061005469315193,
        0.8834554552694481
      ],
      "excerpt": "Source: https://arxiv.org/pdf/1704.04861.pdf \nRicky - 00000020025 - rickygani10@gmail.com - Informatics 2016 Universitas Pelita Harapan \nWilbert Nathaniel - 00000019924 - wilbert.wijaya@yahoo.com - Informatics 2016 Universitas Pelita Harapan \nProject Link: https://github.com/ZyphonGT/TFLite-Frontier-Project-UPH \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ZyphonGT/TFLite-Frontier-Project-UPH",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-18T02:13:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-23T03:46:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9975828797856402
      ],
      "excerpt": "This project is about building an Android-based image classification program using a pre-trained graph, MobileNet, which is provided by TensorFlow. This project was done with the help of \"TensorFlow for Poets 2: TFLite Android\" and \"TensorFlow For Poets\", which are tutorials from CodeLabs by Google. With this program, the user will be given a label (and the percentage of the confidence) of the object recorded real-time using the device camera. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8393001656451802,
        0.972799168372178,
        0.9990681940270504,
        0.9917100360086288
      ],
      "excerpt": "MobileNet is a small efficient convolution neural network. \"convolution\" means that the same calculation are performed at each location in the image. \nMobileNet is build on Depthwise Separable Convolution, which is divided into 2 kind of operation: a 3x3 depthwise convolution and a 1x1 a pointwise convolusion. the architecture is different than the \"traditional\" CNN's which instead using a 3x3 convolution layer. \nA few things that MobileNet is more favorable beacuse they're insanely small, fast, remarkably accurate, and and easy to tune for resources vs.accuracy. which is the reason why it is so important for our project, the mobile deep learning task are mostly performed in the cloud, and this is change quickly. it is more practical to use a system that has no requirement of internet connection, it is more efficient and faster. \nThe standard convolutional layer is parameterized by convolution kernel K of size DK \u00d7 DK \u00d7 M \u00d7 N where DK is the spatial dimension of the kernel assumed to be square and M is number of input channels and N is the number of output channels as defined previously. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8642603905626798,
        0.9473097826334665
      ],
      "excerpt": "left: standard Convolutional layer with batchnorm and ReLU. Right: Depthwise Separable convolutions with Depthwise and Pointwise layer combine with batchnorm and ReLU. \nthe MobileNet neural network has been tested before by using it in FaceRecognition, as it is written inside its paper. The FaceNet model is a face recognition model, it builds the face embeddingsbased on triplet loss. using the FaceNet model, the reserahcer use distillation to train by minimizing the squared differences of the output. below is the result: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ZyphonGT/TFLite-Frontier-Project-UPH/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 10:47:51 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ZyphonGT/TFLite-Frontier-Project-UPH/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ZyphonGT/TFLite-Frontier-Project-UPH",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Retraining The Pre-Trained Models\n```sh\npy -m scripts.retrain --bottleneck_dir=tf_files/bottlenecks --how_many_training_steps=500 --model_dir=tf_files/models/ --summaries_dir=tf_files/training_summaries/mobilenet_0.50_224 --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --architecture=mobilenet_0.50_224 --image_dir=tf_files/...\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone the repo\n```sh\ngit clone https://github.com/ZyphonGT/TFLite-Frontier-Project-UPH\n```\n2. Open Android Studio\n3. Choose `Open Existing Project`\n4. Open the project file `Project_File/android/tflite`\n5. After Android Studio finishes loading, click on `Sync Project with Gradle Files`\n6. Run the app\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ZyphonGT/TFLite-Frontier-Project-UPH/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Java",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# About The Project",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "TFLite-Frontier-Project-UPH",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ZyphonGT",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ZyphonGT/TFLite-Frontier-Project-UPH/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This program was build using these versions of framework (Older or Newer version may or may not work properly) :\n* Python ver 3.7.4\n* Tensorflow ver 1.14.0\n```sh\npip install --upgrade \"tensorflow==1.7.*\"\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 10:47:51 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To get a local copy up and running follow these simple example steps.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "For development purposes, it is also possible to test the trained model using image input (.jpeg, .jpg, .gif) via Command Prompt\n\n1. Open your Command Prompt and Navigate to the project folder\n2. Enter the following command\n```sh\npy -m scripts.label_image --graph=tf_files/retrained_graph.pb  --image=PATH_TO_YOUR_TEST_IMAGE\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}