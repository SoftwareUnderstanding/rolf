{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2107.03974",
      "https://arxiv.org/abs/1903.03698",
      "https://arxiv.org/abs/1807.04742",
      "https://arxiv.org/abs/1802.09081",
      "https://arxiv.org/abs/1707.01495",
      "https://arxiv.org/abs/1509.06461.pdf",
      "https://arxiv.org/abs/1801.01290",
      "https://arxiv.org/abs/1812.05905",
      "https://arxiv.org/abs/1802.09477",
      "https://arxiv.org/abs/2006.09359",
      "https://arxiv.org/abs/2110.06169",
      "https://arxiv.org/abs/1903.03698",
      "https://arxiv.org/abs/2107.03974",
      "https://arxiv.org/abs/1903.03698",
      "https://arxiv.org/abs/1807.04742",
      "https://arxiv.org/abs/1802.09081",
      "https://arxiv.org/abs/1707.01495",
      "https://arxiv.org/abs/1509.06461",
      "https://arxiv.org/abs/1812.05905",
      "https://arxiv.org/abs/1801.01290",
      "https://arxiv.org/abs/1802.09477"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository was initially developed primarily by [Vitchyr Pong](https://github.com/vitchyr), until July 2021, at which point it was transferred to the RAIL Berkeley organization and is primarily maintained by [Ashvin Nair](https://github.com/anair13).\nOther major collaborators and contributions:\n - [Murtaza Dalal](https://github.com/mdalal2020)\n - [Steven Lin](https://github.com/stevenlin1111)\n\nA lot of the coding infrastructure is based on [rllab](https://github.com/rll/rllab).\nThe serialization and logger code are basically a carbon copy of the rllab versions.\n\nThe Dockerfile is based on the [OpenAI mujoco-py Dockerfile](https://github.com/openai/mujoco-py/blob/master/Dockerfile).\n\nThe SMAC code builds off of the [PEARL code](https://github.com/katerakelly/oyster), which built off of an older RLKit version.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The algorithms are based on the following papers\n\n[Offline Meta-Reinforcement Learning with Online Self-Supervision](https://arxiv.org/abs/2107.03974)\nVitchyr H. Pong, Ashvin Nair, Laura Smith, Catherine Huang, Sergey Levine. arXiv preprint, 2021.\n\n[Skew-Fit: State-Covering Self-Supervised Reinforcement Learning](https://arxiv.org/abs/1903.03698).\nVitchyr H. Pong*, Murtaza Dalal*, Steven Lin*, Ashvin Nair, Shikhar Bahl, Sergey Levine. ICML, 2020.\n\n[Visual Reinforcement Learning with Imagined Goals](https://arxiv.org/abs/1807.04742).\nAshvin Nair*, Vitchyr Pong*, Murtaza Dalal, Shikhar Bahl, Steven Lin, Sergey Levine. NeurIPS 2018.\n\n[Temporal Difference Models: Model-Free Deep RL for Model-Based Control](https://arxiv.org/abs/1802.09081).\nVitchyr Pong*, Shixiang Gu*, Murtaza Dalal, Sergey Levine. ICLR 2018.\n\n[Hindsight Experience Replay](https://arxiv.org/abs/1707.01495).\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba. NeurIPS 2017.\n\n[Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461).\nHado van Hasselt, Arthur Guez, David Silver. AAAI 2016.\n\n[Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236).\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis. Nature 2015.\n\n[Soft Actor-Critic Algorithms and Applications](https://arxiv.org/abs/1812.05905).\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, Sergey Levine. arXiv preprint, 2018.\n\n[Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290).\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. ICML, 2018.\n\n[Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477)\nScott Fujimoto, Herke van Hoof, David Meger. ICML, 2018.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8511535834563841
      ],
      "excerpt": "    - Double Q-learning paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8786044417622563
      ],
      "excerpt": "    - TensorFlow implementation from author \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rail-berkeley/rlkit",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-01-25T00:34:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T05:06:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8928184645607082
      ],
      "excerpt": "Reinforcement learning framework and algorithms implemented in PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9035864984985441,
        0.9581664900574234
      ],
      "excerpt": " - Reinforcement Learning with Imagined Goals (RIG) \n    - See this version of this repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9580424623649095
      ],
      "excerpt": "    - Only implemented in v0.1.2 of RLkit. See Legacy Documentation section below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.968121635817269
      ],
      "excerpt": "    - Includes the \"min of Q\" method, the entropy-constrained implementation, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9035535316600947
      ],
      "excerpt": "Fix SAC bug to account for future entropy (#41, #43) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9335475820879757,
        0.8273176937597491
      ],
      "excerpt": "   - Makes code more amenable to parallelization. \n   - Implementing the online-version is straightforward. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9654276399288944
      ],
      "excerpt": "a method for performing goal-directed exploration to maximize the entropy of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654592913977991
      ],
      "excerpt": " - Update soft actor-critic to more closely match TensorFlow implementation: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9650088486046394
      ],
      "excerpt": "Overall, the refactors are intended to make the code more modular and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9065783160702385
      ],
      "excerpt": "Upgraded to PyTorch v0.4 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9249045331743831
      ],
      "excerpt": " - &lt;foldername&gt; is auto-generated and based off of exp_prefix. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735367419092809
      ],
      "excerpt": "depending on whether or not the policy is goal-conditioned. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8300538379566658,
        0.8947498112258397
      ],
      "excerpt": "using rllab's viskit, described at \nthe bottom of this page \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8833634134452215
      ],
      "excerpt": "to visualize all experiments with a prefix of exp_prefix. To only visualize a single run, you can do \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471003502303927,
        0.9743632774723759
      ],
      "excerpt": "Implement model-based algorithms. \nFor Temporal Difference Models (TDMs) and the original implementation of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Collection of reinforcement learning algorithms",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "http://rllab.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rail-berkeley/rlkit/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The `run_experiment` function makes it easy to run Python code on Amazon Web\nServices (AWS) or Google Cloud Platform (GCP) by using\n[this fork of doodad](https://github.com/vitchyr/doodad/tree/v0.2.1).\n\nIt's as easy as:\n```\nfrom rlkit.launchers.launcher_util import run_experiment\n\ndef function_to_run(variant):\n    learning_rate = variant['learning_rate']\n    ...\n\nrun_experiment(\n    function_to_run,\n    exp_prefix=\"my-experiment-name\",\n    mode='ec2',  #: or 'gcp'\n    variant={'learning_rate': 1e-3},\n)\n```\nYou will need to set up parameters in config.py (see step one of Installation).\nThis requires some knowledge of AWS and/or GCP, which is beyond the scope of\nthis README.\nTo learn more, more about `doodad`, [go to the repository](https://github.com/vitchyr/doodad/), which is based on [this original repository](https://github.com/justinjfu/doodad/).\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 434,
      "date": "Wed, 22 Dec 2021 00:25:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rail-berkeley/rlkit/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rail-berkeley/rlkit",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rail-berkeley/rlkit/master/environment/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/rail-berkeley/rlkit/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install and use the included Ananconda environment\n```\n$ conda env create -f environment/[linux-cpu|linux-gpu|mac]-env.yml\n$ source activate rlkit\n(rlkit) $ python examples/ddpg.py\n```\nChoose the appropriate `.yml` file for your system.\nThese Anaconda environments use MuJoCo 1.5 and gym 0.10.5.\nYou'll need to [get your own MuJoCo key](https://www.roboti.us/license.html) if you want to use MuJoCo.\n\n2. Add this repo directory to your `PYTHONPATH` environment variable or simply\nrun:\n```\npip install -e .\n```\n\n3. (Optional) Copy `conf.py` to `conf_private.py` and edit to override defaults:\n```\ncp rlkit/launchers/conf.py rlkit/launchers/conf_private.py\n```\n\n4. (Optional) If you plan on running the Skew-Fit experiments or the HER\nexample with the Sawyer environment, then you need to install\n[multiworld](https://github.com/vitchyr/multiworld).\n\nDISCLAIMER: the mac environment has only been tested without a GPU.\n\nFor an even more portable solution, try using the docker image provided in `environment/docker`.\nThe Anaconda env should be enough, but this docker image addresses some of the rendering issues that may arise when using MuJoCo 1.5 and GPUs.\nThe docker image supports GPU, but it should work without a GPU.\nTo use a GPU with the image, you need to have [nvidia-docker installed](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9100131090408023
      ],
      "excerpt": "    - Requires multiworld to be installed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717106327039013
      ],
      "excerpt": "    version \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8876301297464343,
        0.8614774180432472
      ],
      "excerpt": "To get started, checkout the example scripts, linked above. \nUse new multiworld code that requires explicit environment registration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8726464403172065
      ],
      "excerpt": "The initial release for 0.2 has the following major changes: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9140106505443276
      ],
      "excerpt": "Upgraded to PyTorch v0.4 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.96121632401258
      ],
      "excerpt": "You can use a GPU by calling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8711227677659502
      ],
      "excerpt": "If you are using doodad (see below), simply use the use_gpu flag: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8764322114558676
      ],
      "excerpt": " - LOCAL_LOG_DIR is the directory set by rlkit.launchers.config.LOCAL_LOG_DIR. Default name is 'output'. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8354002730376242
      ],
      "excerpt": "If you have rllab installed, you can also visualize the results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8114104018235266
      ],
      "excerpt": "to visualize all experiments with a prefix of exp_prefix. To only visualize a single run, you can do \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8466803416040211
      ],
      "excerpt": "    - example script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466803416040211
      ],
      "excerpt": "    - example script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466803416040211
      ],
      "excerpt": "    - example script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466803416040211
      ],
      "excerpt": "    - example script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466803416040211
      ],
      "excerpt": "    - example script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466803416040211
      ],
      "excerpt": "    - example script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8084787269340844
      ],
      "excerpt": "run_experiment(..., use_gpu=True) \nDuring training, the results will be saved to a file called under \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.835676549630452
      ],
      "excerpt": " - LOCAL_LOG_DIR is the directory set by rlkit.launchers.config.LOCAL_LOG_DIR. Default name is 'output'. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "tl;dr run \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rail-berkeley/rlkit/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Vitchyr Pong\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "RLkit",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rlkit",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rail-berkeley",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rail-berkeley/rlkit/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1759,
      "date": "Wed, 22 Dec 2021 00:25:05 GMT"
    },
    "technique": "GitHub API"
  }
}