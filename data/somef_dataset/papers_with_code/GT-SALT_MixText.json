{
  "citation": [
    {
      "confidence": [
        0.9987741895065713
      ],
      "excerpt": "If you would like to refer to it, please cite the paper mentioned above. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GT-SALT/MixText",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-22T21:33:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T01:56:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9664297223725389,
        0.9863189077339927
      ],
      "excerpt": "This repo contains codes for the following paper:  \nJiaao Chen, Zichao Yang, Diyi Yang: MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification. In Proceedings of the 58th Annual Meeting of the Association of Computational Linguistics (ACL'2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8807516709996661
      ],
      "excerpt": "| data/ \n        | yahoo_answers_csv/ --> Datasets for Yahoo Answers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.824539701188826
      ],
      "excerpt": "        | transformers/ --> Codes copied from huggingface/transformers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8497438939604111
      ],
      "excerpt": "        | normal_bert.py --> Codes for BERT baseline model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9678512805870135
      ],
      "excerpt": "        | mixtext.py --> Codes for our proposed TMix/MixText model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752251725687604,
        0.9033850402423151
      ],
      "excerpt": "Note that for AG News and DB Pedia, we only utilize the content (without titles) to do the classifications, and for IMDB we do not perform any pre-processing. \nWe utilize Fairseq to perform back translation on the training dataset. Please refer to ./data/yahoo_answers_csv/back_translate.ipynb for details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8129335908340293
      ],
      "excerpt": "These section contains instructions for training models on Yahoo Answers using 10 labeled data per class for training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please download the dataset and put them in the data folder. You can find Yahoo Answers, AG News, DB Pedia [here](https://github.com/LC-John/Yahoo-Answers-Topic-Classification-Dataset), IMDB [here](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GT-SALT/MixText/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 52,
      "date": "Wed, 29 Dec 2021 00:12:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/GT-SALT/MixText/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "GT-SALT/MixText",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/GT-SALT/MixText/master/data/yahoo_answers_csv/back_translate.ipynb",
      "https://raw.githubusercontent.com/GT-SALT/MixText/master/data/yahoo_answers_csv/Pre-process.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.915575152171742,
        0.9074871165481111
      ],
      "excerpt": "            | train.csv --> Original training dataset \n            | test.csv --> Original testing dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8116852041564827
      ],
      "excerpt": "        | read_data.py --> Codes for reading the dataset; forming labeled training set, unlabeled training set, development set and testing set; building dataloaders \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8086550210487228
      ],
      "excerpt": "        | normal_train.py --> Codes for training BERT baseline model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101541122929124
      ],
      "excerpt": "        |__ train.py --> Codes for training/testing TMix/MixText  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8380052447366588,
        0.8371668195593059,
        0.832184336617044,
        0.9052767993286118
      ],
      "excerpt": "Here, we have put two examples of back translated data, de_1.pkl and ru_1.pkl, in ./data/yahoo_answers_csv/ as well. You can directly use them for Yahoo Answers or generate your own back translated data followed the ./data/yahoo_answers_csv/back_translate.ipynb. \nThese section contains instructions for training models on Yahoo Answers using 10 labeled data per class for training. \nPlease run ./code/normal_train.py to train the BERT baseline model (only use labeled training data): \npython ./code/normal_train.py --gpu 0,1 --n-labeled 10 --data-path ./data/yahoo_answers_csv/ \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8871066025498366,
        0.9342871685411396
      ],
      "excerpt": "Please run ./code/train.py to train the TMix model (only use labeled training data): \npython ./code/train.py --gpu 0,1 --n-labeled 10 --data-path ./data/yahoo_answers_csv/ \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8390579885006946,
        0.9234378473993997,
        0.8321355475193181
      ],
      "excerpt": "Please run ./code/train.py to train the MixText model (use both labeled and unlabeled training data): \npython ./code/train.py --gpu 0,1,2,3 --n-labeled 10 \\ \n--data-path ./data/yahoo_answers_csv/ --batch-size 4 --batch-size-u 8 --epochs 20 --val-iteration 1000 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/GT-SALT/MixText/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 GT-NLP\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "MixText",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MixText",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "GT-SALT",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GT-SALT/MixText/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3.6 or higher\n* Pytorch >= 1.3.0\n* Pytorch_transformers (also known as transformers)\n* Pandas, Numpy, Pickle\n* Fairseq\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 250,
      "date": "Wed, 29 Dec 2021 00:12:41 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "mixtext",
      "textclassification",
      "semisupervised-learning",
      "dataaugmentation",
      "interpolation",
      "computation",
      "natural-language-processing",
      "textgeneration",
      "machine-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "These instructions will get you running the codes of MixText.\n\n",
      "technique": "Header extraction"
    }
  ]
}