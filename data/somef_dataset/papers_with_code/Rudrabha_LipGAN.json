{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Part of the MATLAB code is taken from the an implementation of the [Talking Face Generation](https://github.com/Hangz-nju-cuhk/Talking-Face-Generation-DAVS) implementation. We thank the authors for releasing their code.\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The software is licensed under the MIT License. Please cite the following paper if you have use this code:\n\n```\n@inproceedings{KR:2019:TAF:3343031.3351066,\n  author = {K R, Prajwal and Mukhopadhyay, Rudrabha and Philip, Jerin and Jha, Abhishek and Namboodiri, Vinay and Jawahar, C V},\n  title = {Towards Automatic Face-to-Face Translation},\n  booktitle = {Proceedings of the 27th ACM International Conference on Multimedia}, \n  series = {MM '19}, \n  year = {2019},\n  isbn = {978-1-4503-6889-6},\n  location = {Nice, France},\n   = {1428--1436},\n  numpages = {9},\n  url = {http://doi.acm.org/10.1145/3343031.3351066},\n  doi = {10.1145/3343031.3351066},\n  acmid = {3351066},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n  keywords = {cross-language talking face generation, lip synthesis, neural machine translation, speech to speech translation, translation systems, voice transfer},\n}\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{KR:2019:TAF:3343031.3351066,\n  author = {K R, Prajwal and Mukhopadhyay, Rudrabha and Philip, Jerin and Jha, Abhishek and Namboodiri, Vinay and Jawahar, C V},\n  title = {Towards Automatic Face-to-Face Translation},\n  booktitle = {Proceedings of the 27th ACM International Conference on Multimedia}, \n  series = {MM '19}, \n  year = {2019},\n  isbn = {978-1-4503-6889-6},\n  location = {Nice, France},\n   = {1428--1436},\n  numpages = {9},\n  url = {http://doi.acm.org/10.1145/3343031.3351066},\n  doi = {10.1145/3343031.3351066},\n  acmid = {3351066},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n  keywords = {cross-language talking face generation, lip synthesis, neural machine translation, speech to speech translation, translation systems, voice transfer},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.854836641318266
      ],
      "excerpt": "[Paper] | [Project Page]  | [Demonstration Video] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rudrabha/LipGAN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-02T16:52:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T19:27:15Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8534416184111477
      ],
      "excerpt": "Generate realistic talking faces for any human speech and face identity. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9286963271616389,
        0.9421422396587171,
        0.9849854184286401
      ],
      "excerpt": "A new, improved work that can produce significantly more accurate and natural results on moving talking face videos is available here: https://github.com/Rudrabha/Wav2Lip \nCode without MATLAB dependency is now available in fully_pythonic branch. Note that the models in both the branches are not entirely identical and either one may perform better than the other in several cases. The model used at the time of the paper's publication is with the MATLAB dependency and this is the one that has been extensively tested. Please feel free to experiment with the fully_pythonic branch if you do not want to have the MATLAB dependency.  \nA Google Colab notebook is also available for the fully_pythonic branch. [Credits: Kirill] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9299339405662084
      ],
      "excerpt": "Can handle speech in any language and is robust to background noise. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9577139484744998
      ],
      "excerpt": "LipGAN takes speech features in the form of MFCCs and we need to preprocess our input audio file to get the MFCC features. We use the create_mat.m script to create .mat files for a given audio.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9432254975835753
      ],
      "excerpt": "Here, we are given an audio input (as .mat MFCC features) and a video of an identity speaking something entirely different. LipGAN can synthesize the correct lip motion for the given audio and overlay it on the given video of the speaking identity (Example #1, #2 in the above image). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9275873410891473
      ],
      "excerpt": "Refer to example #3 in the above picture. Given an audio, LipGAN generates a correct mouth shape (viseme) at each time-step and overlays it on the input image. The sequence of generated mouth shapes yields a talking face video. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.822142464132381
      ],
      "excerpt": "Please use the --pads argument to correct for inaccurate face detections such as not covering the chin region correctly. This can improve the results further.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9161059231395228
      ],
      "excerpt": "We illustrate the training pipeline using the LRS2 dataset. Adapting for other datasets would involve small modifications to the code.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8948256786413458
      ],
      "excerpt": "We need to do two things: (i) Save the MFCC features from the audio and (ii) extract and save the facial crops of each frame in the video.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999531193718416
      ],
      "excerpt": "|   \u251c\u2500\u2500 list of folders \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9430616344625097
      ],
      "excerpt": "Saving the MFCC features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061334619929298
      ],
      "excerpt": "Saving the Face Crops of all Video Frames \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999531193718416
      ],
      "excerpt": "|   \u251c\u2500\u2500 list of folders \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174538851604484
      ],
      "excerpt": "|   \u2502   |    \u251c\u2500\u2500 0.npz, 1.npz .... (mfcc features corresponding to each frame) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repository contains the codes for LipGAN. LipGAN was published as a part of the paper titled \"Towards Automatic Face-to-Face Translation\".",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rudrabha/LipGAN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 85,
      "date": "Wed, 22 Dec 2021 19:38:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rudrabha/LipGAN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rudrabha/LipGAN",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9823710416934183
      ],
      "excerpt": "cd matlab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd .. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823710416934183
      ],
      "excerpt": "cd matlab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd .. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8971823465573899
      ],
      "excerpt": "python batch_inference.py --help \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8467570507971127
      ],
      "excerpt": "LRS2 dataset folder structure \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8386321471074475
      ],
      "excerpt": "preprocess_mat('../filelists/train.txt', 'mvlrs_v1/main/') # replace with appropriate file paths for other datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8877398661093764
      ],
      "excerpt": "python preprocess.py --split [train|pretrain|val] --videos_data_root mvlrs_v1/ --final_data_root <folder_to_store_preprocessed_files> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8971823465573899
      ],
      "excerpt": "python preprocess.py --help \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8092473267164679
      ],
      "excerpt": "Final preprocessed folder structure \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511207861997153
      ],
      "excerpt": "|   \u2502   |    \u251c\u2500\u2500 0.jpg, 1.jpg .... (extracted face crops of each frame) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8459886616662089
      ],
      "excerpt": "As training LipGAN is computationally intensive, you can just train the generator alone for quick, decent results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_unet.py --data_root <path_to_preprocessed_dataset> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8971823465573899
      ],
      "excerpt": "python train_unet.py --help \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "Train LipGAN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py --data_root <path_to_preprocessed_dataset> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9355986061254542
      ],
      "excerpt": "python train.py --help \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rudrabha/LipGAN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "MATLAB"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) [year] [fullname]\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "LipGAN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "LipGAN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rudrabha",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rudrabha/LipGAN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python >= 3.5\n- ffmpeg: `sudo apt-get install ffmpeg`\n- Matlab R2016a (for audio preprocessing, this dependency will be removed in later versions)\n- Install necessary packages using `pip install -r requirements.txt`\n- Install keras-contrib `pip install git+https://www.github.com/keras-team/keras-contrib.git`\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 367,
      "date": "Wed, 22 Dec 2021 19:38:00 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download checkpoints of the folowing models into the `logs/` folder\n\n- CNN Face detection using dlib: [Link](http://dlib.net/files/mmod_human_face_detector.dat.bz2)\n- LipGAN [Google Drive](https://drive.google.com/open?id=1ZTIt0XII4ZPulMNZbq2yg0x7zQBG6n9e)\n\n",
      "technique": "Header extraction"
    }
  ]
}