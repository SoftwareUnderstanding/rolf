{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Code borrows from [tjwei](https://github.com/tjwei/GANotebooks), [eriklindernoren](https://github.com/eriklindernoren/Keras-GAN/blob/master/aae/adversarial_autoencoder.py), [fchollet](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.5-introduction-to-gans.ipynb), [keras-contrib](https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py) and [reddit user deepfakes' project](https://pastebin.com/hYaLNg1T). The generative network is adopted from [CycleGAN](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix). Weights and scripts of MTCNN are from [FaceNet](https://github.com/davidsandberg/facenet). Illustrations are from [irasutoya](http://www.irasutoya.com/).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1805.08318"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9185620941897606
      ],
      "excerpt": "Face tracking/alignment using MTCNN and Kalman filter in video conversion:  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shaoanlu/faceswap-GAN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-12-23T08:40:32Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T02:43:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9472970621860044
      ],
      "excerpt": "| 2018-07-25 \u00a0 \u00a0  | Data preparation: Add a new notebook for video pre-processing in which MTCNN is used for face detection as well as face alignment.|  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9099010163225938,
        0.972262315222716
      ],
      "excerpt": "| 2018-06-25 \u00a0 \u00a0  | New version: faceswap-GAN v2.2 has been released. The main improvements of v2.2 model are its capability of generating realistic and consistent eye movements (results are shown below, or Ctrl+F for eyes), as well as higher video quality with face alignment.| \n| 2018-06-06 \u00a0 \u00a0  | Model architecture: Add a self-attention mechanism proposed in SAGAN into V2 GAN model. (Note: There is still no official code release for SAGAN, the implementation in this repo. could be wrong. We'll keep an eye on it.)| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9444739530483265,
        0.9097637324817864
      ],
      "excerpt": "Notebook for model training of faceswap-GAN model version 2.2. \nThis notebook also provides code for still image transformation at the bottom. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9017596514580445,
        0.9161365882902768
      ],
      "excerpt": "Notebook for video conversion of faceswap-GAN model version 2.2. \nFace alignment using 5-points landmarks is introduced to video conversion. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9085463544034255,
        0.8539399440469464
      ],
      "excerpt": "Notebook for training data preprocessing. Output binary masks are save in ./binary_masks/faceA_eyes and ./binary_masks/faceB_eyes folders. \nRequire face_alignment package. (An alternative method for generating binary masks (not requiring face_alignment and dlib packages) can be found in MTCNN_video_face_detection_alignment.ipynb.)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108005153681167,
        0.876548441197804
      ],
      "excerpt": "Detected faces are saved in ./faces/raw_faces and ./faces/aligned_faces for non-aligned/aligned results respectively. \nCrude eyes binary masks are also generated and saved in ./faces/binary_masks_eyes. These binary masks can serve as a suboptimal alternative to masks generated through prep_binary_masks.ipynb.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.84044315340046
      ],
      "excerpt": "    - You can skip this pre-processing step by (1) setting use_bm_eyes=False in the config cell of the train_test notebook, or (2) use low-quality binary masks generated in step 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8148359759403915
      ],
      "excerpt": "An all-in-one notebook for demostration purpose that can be run on Google colab. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164277031504592,
        0.8932219483424203,
        0.8699496416600908
      ],
      "excerpt": "VGGFace perceptual loss: Perceptual loss improves direction of eyeballs to be more realistic and consistent with input face. It also smoothes out artifacts in the segmentation mask, resulting higher output quality. \nAttention mask: Model predicts an attention mask that helps on handling occlusion, eliminating artifacts, and producing natrual skin tone. \nConfigurable input/output resolution (v2.2): The model supports 64x64, 128x128, and 256x256 outupt resolutions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9703005202200203
      ],
      "excerpt": "MTCNN is introduced for more stable detections and reliable face alignment (FA).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A denoising autoencoder + adversarial losses and attention mechanisms for face swapping.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shaoanlu/faceswap-GAN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 810,
      "date": "Sun, 26 Dec 2021 20:39:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/shaoanlu/faceswap-GAN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "shaoanlu/faceswap-GAN",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/MTCNN_video_face_detection_alignment.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/FaceSwap_GAN_v2.2_train_test.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/FaceSwap_GAN_v2.2_video_conversion.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/prep_binary_masks.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/colab_demo/faceswap-GAN_colab_demo.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/legacy/faceswap_GAN_keras.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/legacy/FaceSwap_GAN_v2_train.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/legacy/FaceSwap_GAN_v2_test_img.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/legacy/FaceSwap_GAN_github.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/legacy/FaceSwap_GAN_v2_test_video_MTCNN.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/legacy/FaceSwap_GAN_v2_sz128_train.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/legacy/FaceSwap_GAN_v2.1_train.ipynb",
      "https://raw.githubusercontent.com/shaoanlu/faceswap-GAN/master/legacy/faceswap_WGAN-GP_keras_github.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8452001178093744
      ],
      "excerpt": "3. Run FaceSwap_GAN_v2.2_train_test.ipynb to train  models. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8660259414152656
      ],
      "excerpt": "3. Run FaceSwap_GAN_v2.2_train_test.ipynb to train  models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338038317063932
      ],
      "excerpt": "Images will be resized to 256x256 during training. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/shaoanlu/faceswap-GAN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "faceswap-GAN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "faceswap-GAN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "shaoanlu",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shaoanlu/faceswap-GAN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* keras 2.1.5\n* Tensorflow 1.6.0 \n* Python 3.6.4\n* OpenCV\n* [keras-vggface](https://github.com/rcmalli/keras-vggface)\n* [moviepy](http://zulko.github.io/moviepy/)\n* [prefetch_generator](https://github.com/justheuristic/prefetch_generator) (required for v2.2 model)\n* [face-alignment](https://github.com/1adrianb/face-alignment) (required as preprocessing for v2.2 model)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3098,
      "date": "Sun, 26 Dec 2021 20:39:12 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Here is a [playground notebook](https://colab.research.google.com/github/shaoanlu/faceswap-GAN/blob/master/colab_demo/faceswap-GAN_colab_demo.ipynb) for faceswap-GAN v2.2 on Google Colab. Users can train their own model in the browser.\n\n[Update 2019/10/04] There seems to be import errors in the latest Colab environment due to inconsistent version of packages. Please make sure that the Keras and TensorFlow follow the version number shown in the requirement section below.\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "face-swap",
      "generative-adversarial-network",
      "gan",
      "gans",
      "image-manipulation"
    ],
    "technique": "GitHub API"
  }
}