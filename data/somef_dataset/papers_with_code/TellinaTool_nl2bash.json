{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.0473",
      "https://arxiv.org/abs/1603.06393"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use the data or source code in your work, please cite\n```\n@inproceedings{LinWZE2018:NL2Bash, \n  author = {Xi Victoria Lin and Chenglong Wang and Luke Zettlemoyer and Michael D. Ernst}, \n  title = {NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System}, \n  booktitle = {Proceedings of the Eleventh International Conference on Language Resources\n               and Evaluation {LREC} 2018, Miyazaki (Japan), 7-12 May, 2018.},\n  year = {2018} \n}\n```\n\nRelated paper: [Lin et. al. 2017. Program Synthesis from Natural Language Using Recurrent Neural Networks](http://victorialin.net/pubs/tellina_tr170510.pdf). \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{LinWZE2018:NL2Bash, \n  author = {Xi Victoria Lin and Chenglong Wang and Luke Zettlemoyer and Michael D. Ernst}, \n  title = {NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System}, \n  booktitle = {Proceedings of the Eleventh International Conference on Language Resources\n               and Evaluation {LREC} 2018, Miyazaki (Japan), 7-12 May, 2018.},\n  year = {2018} \n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9790223733437481
      ],
      "excerpt": "a stage-wise NL\u27f6Bash model using argument filling heuristics (Lin et. al. 2017). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9869969418366171
      ],
      "excerpt": "  <img src=\"http://victorialin.net/img/github/nl2bash-utility-dist2.png\" width=\"320\" title=\"NL2Bash utility distribution\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9983318754511967
      ],
      "excerpt": "      <td>Tellina (Lin et. al. 2017)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9983318754511967
      ],
      "excerpt": "      <td>Tellina (Lin et. al. 2017)</td> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TellinaTool/nl2bash",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-01-29T20:19:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T03:11:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9922171631875517
      ],
      "excerpt": "This repository contains the data and source code release of the paper: NL2Bash: A Corpus and Semantic Parser for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8084903380623844
      ],
      "excerpt": "Specifically, it contains the following components: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8811568344442877
      ],
      "excerpt": "Tensorflow implementations of the following translation models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9366599593503736,
        0.8889609978164584
      ],
      "excerpt": "A set of domain-specific natural language processing tools, including a regex-based sentence tokenizer and a domain specific named entity recognizer. \nYou may visit  http://tellina.rocks to interact with our pretrained model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920805532725988
      ],
      "excerpt": "Our corpus contains a diverse set of Bash utilities and flags: 102 unique utilities, 206 unique flags and 15 reserved tokens. (Browse the raw data collection here.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9595494358377477
      ],
      "excerpt": "The statistics of the data split is tabulated below. (A command template is defined as a Bash command with all of its arguments replaced by their semantic types.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9640627608645247
      ],
      "excerpt": "The frequency of the top 50 most frequent Bash utilities in the corpus is illustrated in the following diagram. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8855278279722686
      ],
      "excerpt": "Top-k full command accuracy and top-k command template accuracy judged by human experts. Please refer to section 4 of the paper for the exact procedures we took to run manual evaluation.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "      <td>Model</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882115599518425
      ],
      "excerpt": "      <td>Sub-token CopyNet (this work)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723134601896912
      ],
      "excerpt": "In addition, we also report character-based BLEU and a self-defined template matching score as the automatic evaluation metrics used to approximate the true translation accuracy. Please refer to appendix C of the paper for the metrics definitions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "      <td>Model</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882115599518425
      ],
      "excerpt": "      <td>Sub-token CopyNet (this work)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9573829324695015
      ],
      "excerpt": "To change the data-processing workflow, go to data and modify the utility scripts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9673762157770233,
        0.8634204727534538
      ],
      "excerpt": "We provide evaluation scripts to evaluate the performance of any new model.  \nTo do so please save your model output to a file (example). We assume the file is of the following format: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278494033012064
      ],
      "excerpt": "Apr 24, 2020 The dataset data/bash is separately licensed under the terms of the MIT license. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Generating bash command from natural language https://arxiv.org/abs/1802.08979",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TellinaTool/nl2bash/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 54,
      "date": "Tue, 28 Dec 2021 23:58:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TellinaTool/nl2bash/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "TellinaTool/nl2bash",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/TellinaTool/nl2bash/master/scripts/bash-copy-partial-token.sh",
      "https://raw.githubusercontent.com/TellinaTool/nl2bash/master/scripts/bash-token.sh",
      "https://raw.githubusercontent.com/TellinaTool/nl2bash/master/scripts/bash-partial-token.sh",
      "https://raw.githubusercontent.com/TellinaTool/nl2bash/master/scripts/bash-run.sh",
      "https://raw.githubusercontent.com/TellinaTool/nl2bash/master/scripts/bash-copy-char.sh",
      "https://raw.githubusercontent.com/TellinaTool/nl2bash/master/scripts/bash-copy.sh",
      "https://raw.githubusercontent.com/TellinaTool/nl2bash/master/scripts/bash-char.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In our experiment, we conduct manual evaluation as the correctness of a Bash translation cannot simply be determined by mapping it to a set of ground truth.\nWe suggest the following practices for future work to generate comparable results and to accelerate the development cycle.\n1. If you plan to run your own manual evaluation, please annotate the output of both your system(s) and the baseline systems you compared to. This is to ensure that the newly proposed system(s) and the baselines are judged by the same group of annotators.\n2. If you run manual evaluation, please release [the examples annotated with their annotations](https://github.com/TellinaTool/nl2bash/tree/master/data/bash/manual_judgements). This helps others to replicate the results and reuse these annotations.\n3. During model development you could annotate a small subset of the dev examples (50-100 is likely enough) to estimate the true dev set accuracy. We released a script which saves any previous annotations and opens a commandline interface for judging any unseen predictions ([manual_eval.md](https://github.com/TellinaTool/nl2bash/blob/master/manual_eval.md)).\n\nThe motivation for the practices above is detailed in issue [#6](https://github.com/TellinaTool/nl2bash/issues/6).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Dev set evaluation\n```\n./bash-run.sh --data bash --prediction_file <path_to_your_model_output_file> --manual_eval\n```\n\nTest set evaluation\n```\n./bash-run.sh --data bash --prediction_file <path_to_your_model_output_file> --manual_eval --test\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To reproduce our experiments, please install TensorFlow 2.0. The experiments can be reproduced on machines with or without GPUs. However, training with CPU only is extremely slow and we do not recommend it. Inference with CPU only is slow but tolerable.\n\nWe suggest following the [official instructions](https://www.tensorflow.org/install/) to install the library. The code has been tested on Ubuntu 16.04 + CUDA 10.0 + CUDNN 7.6.3.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8906348405570123,
        0.9714207429223904
      ],
      "excerpt": "Then enter the scripts directory. \ncd scripts \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8904214639314836
      ],
      "excerpt": "make train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474895321345809
      ],
      "excerpt": "make decode \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474895321345809
      ],
      "excerpt": "make gen_manual_evaluation_table \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "      <td>Train</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "      <td>Test</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8702815774991463
      ],
      "excerpt": "      <td><strong>0.45</strong></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.868798926633235
      ],
      "excerpt": "1. The i-th line of the file contains predictions for example i in the dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8355611875199558
      ],
      "excerpt": "Decode the pre-trained models and print the evaluation summary table. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TellinaTool/nl2bash/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "NewLisp",
      "Python",
      "Java",
      "Shell",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "GNU General Public License v3.0",
      "url": "https://api.github.com/licenses/gpl-3.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'  \\nMIT License\\n\\nCopyright (c) 2020 NL2Bash dataset\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "NL2Bash",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "nl2bash",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "TellinaTool",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TellinaTool/nl2bash/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Once TensorFlow is installed, use the following commands to set up the Python path and main experiment dependencies.\n```\nexport PYTHONPATH=`pwd`\n\n(sudo) make\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 356,
      "date": "Tue, 28 Dec 2021 23:58:45 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "copynet",
      "tensorflow",
      "seq2seq",
      "programming-by-natural-language",
      "nl2bash",
      "natural-language-interface"
    ],
    "technique": "GitHub API"
  }
}