{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1909.10852",
      "https://arxiv.org/abs/1705.03122",
      "https://arxiv.org/abs/1506.03340"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{li-etal-2019-conclusion,\n    title = \"In Conclusion Not Repetition: Comprehensive Abstractive Summarization with Diversified Attention Based on Determinantal Point Processes\",\n    author = \"Li, Lei  and\n      Liu, Wei  and\n      Litvak, Marina  and\n      Vanetik, Natalia  and\n      Huang, Zuying\",\n    booktitle = \"Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)\",\n    month = nov,\n    year = \"2019\",\n    address = \"Hong Kong, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/K19-1077\",\n    doi = \"10.18653/v1/K19-1077\",\n    pages = \"822--832\",\n    abstract = \"Various Seq2Seq learning models designed for machine translation were applied for abstractive summarization task recently. Despite these models provide high ROUGE scores, they are limited to generate comprehensive summaries with a high level of abstraction due to its degenerated attention distribution. We introduce Diverse Convolutional Seq2Seq Model(DivCNN Seq2Seq) using Determinantal Point Processes methods(Micro DPPs and Macro DPPs) to produce attention distribution considering both quality and diversity. Without breaking the end to end architecture, DivCNN Seq2Seq achieves a higher level of comprehensiveness compared to vanilla models and strong baselines. All the reproducible codes and datasets are available online.\",\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{li-etal-2019-conclusion,\n    title = \"In Conclusion Not Repetition: Comprehensive Abstractive Summarization with Diversified Attention Based on Determinantal Point Processes\",\n    author = \"Li, Lei  and\n      Liu, Wei  and\n      Litvak, Marina  and\n      Vanetik, Natalia  and\n      Huang, Zuying\",\n    booktitle = \"Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)\",\n    month = nov,\n    year = \"2019\",\n    address = \"Hong Kong, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/K19-1077\",\n    doi = \"10.18653/v1/K19-1077\",\n    pages = \"822--832\",\n    abstract = \"Various Seq2Seq learning models designed for machine translation were applied for abstractive summarization task recently. Despite these models provide high ROUGE scores, they are limited to generate comprehensive summaries with a high level of abstraction due to its degenerated attention distribution. We introduce Diverse Convolutional Seq2Seq Model(DivCNN Seq2Seq) using Determinantal Point Processes methods(Micro DPPs and Macro DPPs) to produce attention distribution considering both quality and diversity. Without breaking the end to end architecture, DivCNN Seq2Seq achieves a higher level of comprehensiveness compared to vanilla models and strong baselines. All the reproducible codes and datasets are available online.\",\n}",
      "technique": "Regular expression"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/thinkwee/DPP_CNN_Summarization/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/thinkwee/DPP_CNN_Summarization",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to FAIR Sequence-to-Sequence Toolkit (PyTorch)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nCoding Style\nWe try to follow the PEP style guidelines and encourage you to as well.\nLicense\nBy contributing to FAIR Sequence-to-Sequence Toolkit, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-09T09:14:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-11T08:56:12Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9198490105021464
      ],
      "excerpt": "        Construction of matrix L \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9855487817921208,
        0.9842351503181854,
        0.9484383509142129,
        0.9892136044466792
      ],
      "excerpt": "This repository contains PyTorch code for the CoNLL 2019 accepted paper \"In Conclusion Not Repetition:Comprehensive Abstractive Summarization With Diversified Attention Based On Determinantal Point Processes\"[pdf].  \nOur ConvS2S model is based on Convolutional Sequence to Sequence Learning. The original code from their paper can be found as a part of Facebook AI Research Sequence-to-Sequence Toolkit. We delete some irrelevant code. Full code and more models can be found at fairseq repository. \nWe use the ConvS2S model on the CNNDM dataset as baseline and propose Diverse CNN Seq2Seq(DivCNN Seq2Seq) model for comprehensive abstractive summarization. \nSome datasets have millions of samples and we just use part of these data. Details can be found in our paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589148925648018
      ],
      "excerpt": "You can register your own task under fairseq/fairseq/tasks . The task control how to preprocess the dataset. We just set up six tasks corresponding to six datasets. More tasks set please see fairseq tasks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "code for the paper \"In Conclusion Not Repetition:Comprehensive Abstractive Summarization With Diversified Attention Based On Determinantal Point Processes\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/thinkwee/DPP_CNN_Summarization/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 21:37:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/thinkwee/DPP_CNN_Summarization/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "thinkwee/DPP_CNN_Summarization",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/thinkwee/DPP_CNN_Summarization/master/check_embeddings.ipynb",
      "https://raw.githubusercontent.com/thinkwee/DPP_CNN_Summarization/master/extractive_summarization.ipynb",
      "https://raw.githubusercontent.com/thinkwee/DPP_CNN_Summarization/master/check_attention.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/thinkwee/DPP_CNN_Summarization/master/fairseq/scripts/compound_split_bleu.sh",
      "https://raw.githubusercontent.com/thinkwee/DPP_CNN_Summarization/master/fairseq/scripts/sacrebleu_pregen.sh",
      "https://raw.githubusercontent.com/thinkwee/DPP_CNN_Summarization/master/fairseq/raw_dataset/bpe-summarization.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "-   For now you can change the ratio of KL loss or Det Loss in the fairseq_model.py, in which the raio is defined in class FairseqDetModel and class FairseqKLModel. The ratio is connected to the dpp_macro_loss.py and dpp_micro_loss.py in criterions.\n-   Other hyperparameters can be set directly in the fconv_dpp_macro.py and fconv_dpp_micro.py. We will improve the configuration soon. \n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8283918897954949
      ],
      "excerpt": "NOTE you need to modify bpe-summarization.sh and truncate.py if you want to use BPE and truncate your dataset. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8228420580052977
      ],
      "excerpt": "Choose your own checkpoints save dir and output result dir by passing --save-dir in the fairseq-train and --results-path in the fairseq-generate. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/thinkwee/DPP_CNN_Summarization/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Lua",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD License\\n\\nFor fairseq software\\n\\nCopyright (c) 2017-present, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n * Redistributions of source code must retain the above copyright notice, this\\n    list of conditions and the following disclaimer.\\n\\n * Redistributions in binary form must reproduce the above copyright notice,\\n    this list of conditions and the following disclaimer in the documentation\\n       and/or other materials provided with the distribution.\\n\\n * Neither the name Facebook nor the names of its contributors may be used to\\n    endorse or promote products derived from this software without specific\\n       prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "In Conclusion Not Repetition:Comprehensive Abstractive Summarization With Diversified Attention Based On Determinantal Point Processes",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DPP_CNN_Summarization",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "thinkwee",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/thinkwee/DPP_CNN_Summarization/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 29 Dec 2021 21:37:53 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "cnn-seq2seq",
      "determinantal-point-processes",
      "conll",
      "summarization",
      "pytorch",
      "natural-language-processing",
      "natural-language-generation"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "-   **Example**: **Conv Vanilla Model** on **CNNDM** dataset with default settings.\n-   **Fairseq Install**: clone this repo and run \n    -   ```cd fairseq``` \n    -   ```pip install --editable .```\n-   **Dataset Format**: download cleaned dataset of [CNNDM](https://arxiv.org/abs/1506.03340) and extract it under ```/fairseq/raw_dataset/cnndm/raw/```(make a new cnndm folder) , you should get 7 files here:\n    -   train.src\n    -   test.src\n    -   valid.src\n    -   train.tgt\n    -   test.tgt\n    -   valid.tgt\n    -   corpus_total.txt (which is an combination of train.src and train.tgt)\n    Other datasets should be preprocessed in the same way(7 files)\n-   **BPEncoding and Truncate**: run\n    -   ```cd /raw_dataset/cnndm```\n    -   ```mkdir bpe-output bpe-truncate``` \n    -   ```cd .. && bash ./bpe-summarization.sh cnndm``` to generate [BPE](https://arxiv.org/pdf/1508.07909.pdf) code list(located in \"./raw/code\") and apply bpe on 6 dataset files. You will get six Byte Pair Encoded files in \"./bpe-output\"\n    -   ```python truncate.py -sl 600 -tl 70 -d cnndm``` . after Byte Pair Encoding the length of sentence may be longer than before. What's more the next fairseq-preprocess step do not truncate sentence(but it may make sentences longer than before) in fixed length, so we should truncate the original text and summaries. In our paper we truncate original text for 600 words and summary for 70 words.\n-   **Binarizing Data**: run\n    -   ```cd .. && mkdir -p data-bin/cnndm```\n    -   ```TEXT=raw_dataset/cnndm/bpe-truncate```\n    -   ```fairseq-preprocess --source-lang src --target-lang tgt --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data_bin/cnndm --nwordssrc 50000 --nwordstgt 20000 --memory-efficient-fp16 --task summ_cnndm``` and should output the following information\n        ```\n        | [src] Dictionary: 49999 types\n\n        | [src] raw_dataset/cnndm/bpe-truncate/train.src: 287227 sents, 154092057 tokens, 1.19% replaced by <unk>\n\n        | [src] Dictionary: 49999 types\n\n        | [src] raw_dataset/cnndm/bpe-truncate/valid.src: 13368 sents, 7048430 tokens, 1.16% replaced by <unk>\n\n        | [src] Dictionary: 49999 types\n\n        | [src] raw_dataset/cnndm/bpe-truncate/test.src: 11490 sents, 6078757 tokens, 1.21% replaced by <unk>\n\n        | [tgt] Dictionary: 19999 types\n\n        | [tgt] raw_dataset/cnndm/bpe-truncate/train.tgt: 287227 sents, 15092804 tokens, 4.55% replaced by <unk>\n\n        | [tgt] Dictionary: 19999 types\n\n        | [tgt] raw_dataset/cnndm/bpe-truncate/valid.tgt: 13368 sents, 758025 tokens, 4.53% replaced by <unk>\n\n        | [tgt] Dictionary: 19999 types\n\n        | [tgt] raw_dataset/cnndm/bpe-truncate/test.tgt: 11490 sents, 632533 tokens, 4.7% replaced by <unk>\n\n        | Wrote preprocessed data to data_bin/cnndm\n        ```\n    -   **Pretrained Embeddings**: If you want to use pretrained embedding, check the parse_embedding functions in [utils.py](https://github.com/pytorch/fairseq/blob/master/fairseq/utils.py). \n        -   In our paper we use [fasttext](https://fasttext.cc/) to train the embedding and the output .vec file is exactly the format that fairseq needed. Put the .vec file under \"./data_bin/cnndm\" to use the pretrained embeddings. For reference our settings are:\n            -   ```./fasttext skipgram -input ../DPPs_Conv_Summarization/fairseq/raw_dataset/cnndm/raw/corpus_total.txt -output model_cnndm_256 -loss hs -ws 5 -epoch 5 -lr 0.05 -dim 256 ``` . \n        -   **NOTE** we train the embedding on the raw corpus not BPEncoded corpus so there only part of words have pretrained embedding, specifically:\n\n        ```\n        | Found 44025/50000 types in embedding file.\n\n        | Found 18750/20000 types in embedding file.\n        ```\n        \n    -   **NOTE**:  you should make ```--max-source-positions``` and ```--max-target-positions``` larger than actual to ensure that no training samples are skipped because after fairseq-preprocess the sample will get longer than before. In our experiment we set the two values to 620 and 80\n    -   **Train**: start training by running:\n        -   ``` TEXT = cnndm ```\n        -   ``` mkdir -p checkpoints/summarization_vanilla_$TEXT ```\n        -    ```CUDA_VISIBLE_DEVICES=0 fairseq-train data_bin/$TEXT  --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 10000 --arch summ_vanilla --save-dir checkpoints/summarization_vanilla_$TEXT  --task summ_$TEXT --memory-efficient-fp16 --skip-invalid-size-inputs-valid-test --keep-last-epochs 3``` \n        -   **NOTE** If you want to train the DPP Model you should add ```--criterion dpp_micro_loss``` or ```--criterion dpp_macro_loss```\n        -   it will cost about 6 GB GPU memories\n        -   We train the model on a single RTX2070 and it takes about 24 minutes for one epoch. After training for 22 epoches the result is as follows:\n            ```\n            | epoch 022 | loss 4.290 | ppl 19.56 | wps 10476 | ups 6 | wpb 1852.327 | bsz 35.251 | num_updates 179247 | lr 2.5e-05 | gnorm 0.321 | clip 1.000 | oom 0.000 | loss_scale 64.000 | wall 32337 | train_wall 31350\n\n            | epoch 022 | valid on 'valid' subset | loss 4.008 | ppl 16.09 | num_updates 179247 | best_loss 4.00763 \n\n            | saved checkpoint checkpoints/summarization_vanilla/checkpoint22.pt (epoch 22 @ 179247 updates) (writing took 0.3943181037902832 seconds)\n\n            | done training in 32352.1 seconds\n            ```\n    -   **Generate** run\n        -   ``` fairseq-generate data_bin/cnndm  --path checkpoints/summarization_vanilla_cnndm/checkpoint_best.pt  --batch-size 256 --beam 5 --skip-invalid-size-inputs-valid-test --remove-bpe --quiet --task summ_cnndm --memory-efficient-fp16 --results-path recent-output --print-alignment```\n        -   then the systems and models folder under ``/recent-output-nounk`` can be directly used to evaluate ROUGE\n        -   the model generated readable (keep unk) summaries are under ``/recent-output/models``\n        -   attention alignments are generated under ``/recent-output/alignments``\n        -   the infer speed on RTX 2070 is as follows:\n            \n            ```\n            | Summarized 11490 articles (484468 tokens) in 73.3s (156.80 articles/s, 6611.44 tokens/s)\n            ```\n            \n        -   **NOTE** The original fairseq-generate runs BLEU test on each generated sample but we removed it\n        -   **NOTE** If the infer speed is too slow try to set ``--beam`` option lower.\n    -   **Interactive**: run\n        -   ```fairseq-interactive data_bin/cnndm  --path checkpoints/summarization_vanilla_cnndm/checkpoint_best.pt --beam 5 --remove-bpe --task summ_cnndm --memory-efficient-fp16``` and type in truncated testset article to get the results or you can type in any news article shorter than ```--max-source-positions```. If you paste news artcle on Internet instead from dataset you need remove the ``\\n`` in the article and lowercase all words.\n\n",
      "technique": "Header extraction"
    }
  ]
}