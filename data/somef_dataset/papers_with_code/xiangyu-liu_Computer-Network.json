{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.6114\n.. _Importance Weighting: https://arxiv.org/abs/1509.00519\n.. _Weight Normalization: https://arxiv.org/abs/1602.07868\n.. _Dropout: https://arxiv.org/abs/1207.0580\n.. _API Documentation: https://vae.readthedocs.io/en/latest/modules.html",
      "https://arxiv.org/abs/1509.00519\n.. _Weight Normalization: https://arxiv.org/abs/1602.07868\n.. _Dropout: https://arxiv.org/abs/1207.0580\n.. _API Documentation: https://vae.readthedocs.io/en/latest/modules.html",
      "https://arxiv.org/abs/1602.07868\n.. _Dropout: https://arxiv.org/abs/1207.0580\n.. _API Documentation: https://vae.readthedocs.io/en/latest/modules.html",
      "https://arxiv.org/abs/1207.0580\n.. _API Documentation: https://vae.readthedocs.io/en/latest/modules.html"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9201896952023112
      ],
      "excerpt": ".. image:: https://readthedocs.org/projects/vae/badge/?version=latest \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9110973112225068
      ],
      "excerpt": ".. image:: https://img.shields.io/badge/License-MIT-blue.svg \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xiangyu-liu/Characterizing-Hosts-on-the-Internet-Based-on-Deep-Learning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-11T04:04:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-29T06:11:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9802514779904724
      ],
      "excerpt": "This package contains an implementation of a variational autoencoder in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A New Framework for Characterizing Hosts on the Internet Based on Deep Learning",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://vae.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xiangyu-liu/Computer-Network/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 00:34:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xiangyu-liu/Characterizing-Hosts-on-the-Internet-Based-on-Deep-Learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "xiangyu-liu/Characterizing-Hosts-on-the-Internet-Based-on-Deep-Learning",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/xiangyu-liu/Computer-Network/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Install using:\n\n.. code-block:: sh\n\n    python setup.py install\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8328635853330358
      ],
      "excerpt": ".. image:: https://readthedocs.org/projects/vae/badge/?version=latest \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xiangyu-liu/Characterizing-Hosts-on-the-Internet-Based-on-Deep-Learning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Characterizing-Hosts-on-the-Internet-Based-on-Deep-Learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "xiangyu-liu",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xiangyu-liu/Characterizing-Hosts-on-the-Internet-Based-on-Deep-Learning/blob/master/README.rst",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 00:34:14 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "vae",
      "malicious-url-detection",
      "xgboost",
      "machine-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The following example shows how to instantiate and train a VAE model, save the\ntrained model, and then load it for evaluating samples.\n\nFirst we instantitate a ``VAE`` object and use some training data (a NumPy\narray or a SciPy sparse matrix containing real-valued/binary observations) to\ntrain the model.\n\n.. code-block:: python\n\n    import dill\n    import vae\n\n    model = vae.VAE(\n        n_inputs=train_data.shape[1],\n        n_latent=2,\n        n_encoder=[1000, 1000],\n        n_decoder=[1000, 1000],\n        visible_type='binary',\n        nonlinearity=tf.nn.relu,\n        weight_normalization=True,\n        importance_weighting=False,\n        optimizer='Adam',\n        learning_rate=0.001,\n        model_dir='vae'\n    )\n\n    with open('vae/model.pkl', 'wb') as f:\n        dill.dump(model, f)\n\n    model.fit(\n        train_data,\n        validation_data=validation_data,\n        epochs=10,\n        shuffle=True,\n        summary_steps=100,\n        init_feed_dict={'batch_size': 1000},\n        batch_size=100,\n        n_samples=10\n    )\n\nNote that ``VAE`` object can be serialized using ``dill``, however separate\nTensorFlow checkpoint files are created after training each epoch in the\nprovided directory for saving the trained weights/biases.\n\nOne can also monitor training using TensorBoard:\n\n.. code-block:: sh\n\n    tensorboard --logdir vae\n\nWe can then restore the trained model and use it to evaluate samples:\n\n.. code-block:: python\n\n    with open('vae/model.pkl', 'rb') as f:\n        model = dill.load(f)\n\n    Z_mean, Z_sd = model.evaluate(data, tensors=['z_mean', 'z_sd'])\n\nSee the `API documentation`_ for more detailed usage.\n\n.. _Variational Autoencoder: https://arxiv.org/abs/1312.6114\n.. _Importance Weighting: https://arxiv.org/abs/1509.00519\n.. _Weight Normalization: https://arxiv.org/abs/1602.07868\n.. _Dropout: https://arxiv.org/abs/1207.0580\n.. _API Documentation: https://vae.readthedocs.io/en/latest/modules.html\n",
      "technique": "Header extraction"
    }
  ]
}