{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Thanks to Google for the generous cloud credit donations.\n\n[SyncBN](https://github.com/vacancy/Synchronized-BatchNorm-PyTorch) by Jiayuan Mao and Tete Xiao.\n\n[Progress bar](https://github.com/Lasagne/Recipes/tree/master/papers/densenet) originally from Jan Schl\u00fcter.\n\nTest metrics logger from [VoxNet.](https://github.com/dimatura/voxnet)\n\nPyTorch [implementation of cov](https://discuss.PyTorch.org/t/covariance-and-gradient-support/16217/2) from Modar M. Alfadly.\n\nPyTorch [fast Matrix Sqrt](https://github.com/msubhransu/matrix-sqrt) for FID from Tsung-Yu Lin and Subhransu Maji.\n\nTensorFlow Inception Score code from [OpenAI's Improved-GAN.](https://github.com/openai/improved-gan)\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1809.11096"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{\nbrock2018large,\ntitle={Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},\nauthor={Andrew Brock and Jeff Donahue and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xsqj09Fm},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8779212639294134
      ],
      "excerpt": "The author's officially unofficial PyTorch BigGAN implementation. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajbrock/BigGAN-PyTorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-20T18:50:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T13:53:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8714662260208549,
        0.9583906057228229
      ],
      "excerpt": "This repo contains code for 4-8 GPU training of BigGANs from Large Scale GAN Training for High Fidelity Natural Image Synthesis by Andrew Brock, Jeff Donahue, and Karen Simonyan. \nThis code is by Andy Brock and Alex Andonian. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207598412616808
      ],
      "excerpt": "The logs folder contains scripts to process these logs and plot the results using MATLAB (sorry not sorry). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9602109135793768,
        0.9720181582988568,
        0.8929315005592773,
        0.9785354040040248,
        0.9146792544001067
      ],
      "excerpt": "By default, everything is saved to weights/samples/logs/data folders which are assumed to be in the same folder as this repo. \nYou can point all of these to a different base folder using the --base_root argument, or pick specific locations for each of these with their respective arguments (e.g. --logs_root). \nWe include scripts to run BigGAN-deep, but we have not fully trained a model using them, so consider them untested. Additionally, we include scripts to run a model on CIFAR, and to run SA-GAN (with EMA) and SN-GAN on ImageNet. The SA-GAN code assumes you have 4xTitanX (or equivalent in terms of GPU RAM) and will run with a batch size of 128 and 2 gradient accumulations. \nThis repo uses the PyTorch in-built inception network to calculate IS and FID.  \nThese scores are different from the scores you would get using the official TF inception code, and are only for monitoring purposes! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9822675323962952,
        0.9600071488365405,
        0.8658994133287329
      ],
      "excerpt": "We include two pretrained model checkpoints (with G, D, the EMA copy of G, the optimizers, and the state dict): \n- The main checkpoint is for a BigGAN trained on ImageNet at 128x128, using BS256 and 8 gradient accumulations, taken just before collapse, with a TF Inception Score of 97.35 +/- 1.79: LINK \n- An earlier checkpoint of the first model (100k G iters), at high performance but well before collapse, which may be easier to fine-tune: LINK \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978694820326569
      ],
      "excerpt": "This repo also contains scripts for porting the original TFHub BigGAN Generator weights to PyTorch. See the scripts in the TFHub folder for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8247077160724317
      ],
      "excerpt": "We include the full training and metrics logs here for reference. I've found that one of the hardest things about re-implementing a paper can be checking if the logs line up early in training, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8772969525437406
      ],
      "excerpt": "We include an accelerated FID calculation--the original scipy version can require upwards of 10 minutes to calculate the matrix sqrt, this version uses an accelerated PyTorch version to calculate it in under a second. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8761500387398763
      ],
      "excerpt": "By default, we only compute the top singular value (the spectral norm), but this code supports computing more SVs through the --num_G_SVs argument. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8952422271727373,
        0.9597902098188115,
        0.8775212879301938
      ],
      "excerpt": "The two variants we tried (a custom, naive one and the one included in this repo) have slightly different gradients (albeit identical forward passes) from the built-in BatchNorm, which appear to be sufficient to cripple training. \nGradient accumulation means that we update the SV estimates and the BN statistics 8 times more frequently. This means that the BN stats are much closer to standing stats, and that the singular value estimates tend to be more accurate. \nBecause of this, we measure metrics by default with G in test mode (using the BatchNorm running stat estimates instead of computing standing stats as in the paper). We do still support standing stats (see the sample.sh scripts). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8446193077972769,
        0.8484839749930204,
        0.9484168688500721,
        0.9599766064441685,
        0.8741728529840352,
        0.951857120667088,
        0.9936686421270488,
        0.9204434847803754,
        0.9367210138294699
      ],
      "excerpt": "The currently provided pretrained models were not trained with orthogonal regularization. Training without ortho reg seems to increase the probability that models will not be amenable to truncation, \nbut it looks like this particular model got a winning ticket. Regardless, we provide two highly optimized (fast and minimal memory consumption) ortho reg implementations which directly compute the ortho reg. gradients. \nThis code is designed from the ground up to serve as an extensible, hackable base for further research code.  \nWe've put a lot of thought into making sure the abstractions are the right thickness for research--not so thick as to be impenetrable, but not so thin as to be useless. \nThe key idea is that if you want to experiment with a SOTA setup and make some modification (try out your own new loss function, architecture, self-attention block, etc) you should be able to easily do so just by dropping your code in one or two places, without having to worry about the rest of the codebase. \nThings like the use of self.which_conv and functools.partial in the BigGAN.py model definition were put together with this in mind, as was the design of the Spectral Norm class inheritance. \nWith that said, this is a somewhat large codebase for a single project. While we tried to be thorough with the comments, if there's something you think could be more clear, better written, or better refactored, please feel free to raise an issue or a pull request. \nWant to work on or improve this code? There are a couple things this repo would benefit from, but which don't yet work. \nSynchronized BatchNorm (AKA Cross-Replica BatchNorm). We tried out two variants of this, but for some unknown reason it crippled training each time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8781743857723475,
        0.902552958738768
      ],
      "excerpt": "Mixed precision training and making use of Tensor cores. This repo includes a naive mixed-precision Adam implementation which works early in training but leads to early collapse, and doesn't do anything to activate Tensor cores (it just reduces memory consumption). \n  As above, integrating apex into this code and employing its mixed-precision training techniques to take advantage of Tensor cores and reduce memory consumption could yield substantial speed gains. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The author's officially unofficial PyTorch BigGAN implementation.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajbrock/BigGAN-PyTorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 433,
      "date": "Wed, 29 Dec 2021 07:49:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ajbrock/BigGAN-PyTorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ajbrock/BigGAN-PyTorch",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/launch_BigGAN_deep.sh",
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/launch_cifar_ema.sh",
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/launch_BigGAN_bs256x8.sh",
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/launch_BigGAN_bs512x4.sh",
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/launch_SAGAN_bs128x2_ema.sh",
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/sample_cifar_ema.sh",
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/launch_SNGAN.sh",
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/sample_BigGAN_bs256x8.sh",
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/launch_BigGAN_ch64_bs256x8.sh",
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/utils/prepare_data.sh",
      "https://raw.githubusercontent.com/ajbrock/BigGAN-PyTorch/master/scripts/utils/duplicate.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8132295497588068
      ],
      "excerpt": "Run sample.py on your model, with the --sample_npz argument, then run inception_tf13 to calculate the actual TensorFlow IS. Note that you will need to have TensorFlow 1.3 or earlier installed, as TF1.4+ breaks the original IS code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8079028678733686
      ],
      "excerpt": "  We have not tried the apex SyncBN as my school's servers are on ancient NVIDIA drivers that don't support it--apex would probably be a good place to start.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8022578882992878
      ],
      "excerpt": "During training, this script will output logs with training metrics and test metrics, will save multiple copies (2 most recent and 5 highest-scoring) of the model weights/optimizer params, and will produce samples and interpolations every time it saves weights. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401678012986594
      ],
      "excerpt": "After training, one can use sample.py to produce additional samples and interpolations, test with different truncation values, batch sizes, number of standing stat accumulations, etc. See the sample_BigGAN_bs256x8.sh script for an example. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8110222990075986
      ],
      "excerpt": "Run sample.py on your model, with the --sample_npz argument, then run inception_tf13 to calculate the actual TensorFlow IS. Note that you will need to have TensorFlow 1.3 or earlier installed, as TF1.4+ breaks the original IS code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.862441587106091
      ],
      "excerpt": "To prep your own dataset, you will need to add it to datasets.py and modify the convenience dicts in utils.py (dset_dict, imsize_dict, root_dict, nclass_dict, classes_per_sheet_dict) to have the appropriate metadata for your dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.896517179674144
      ],
      "excerpt": "To use your own training function (e.g. train a BigVAE): either modify train_fns.GAN_training_function or add a new train fn and add it after the if config['which_train_fn'] == 'GAN': line in train.py. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "MATLAB"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Andy Brock\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BigGAN-PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BigGAN-PyTorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ajbrock",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajbrock/BigGAN-PyTorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2465,
      "date": "Wed, 29 Dec 2021 07:49:49 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "biggan",
      "pytorch",
      "deep-learning",
      "neural-networks",
      "gans",
      "dogball"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You will need:\n\n- [PyTorch](https://PyTorch.org/), version 1.0.1\n- tqdm, numpy, scipy, and h5py\n- The ImageNet training set\n\nFirst, you may optionally prepare a pre-processed HDF5 version of your target dataset for faster I/O. Following this (or not), you'll need the Inception moments needed to calculate FID. These can both be done by modifying and running\n\n```sh\nsh scripts/utils/prepare_data.sh\n```\n\nWhich by default assumes your ImageNet training set is downloaded into the root folder `data` in this directory, and will prepare the cached HDF5 at 128x128 pixel resolution.\n\nIn the scripts folder, there are multiple bash scripts which will train BigGANs with different batch sizes. This code assumes you do not have access to a full TPU pod, and accordingly\nspoofs mega-batches by using gradient accumulation (averaging grads over multiple minibatches, and only taking an optimizer step after N accumulations). By default, the `launch_BigGAN_bs256x8.sh` script trains a\nfull-sized BigGAN model with a batch size of 256 and 8 gradient accumulations, for a total batch size of 2048. On 8xV100 with full-precision training (no Tensor cores), this script takes 15 days to train to 150k iterations.\n\nYou will first need to figure out the maximum batch size your setup can support. The pre-trained models provided here were trained on 8xV100 (16GB VRAM each) which can support slightly more than the BS256 used by default.\nOnce you've determined this, you should modify the script so that the batch size times the number of gradient accumulations is equal to your desired total batch size (BigGAN defaults to 2048).\n\nNote also that this script uses the `--load_in_mem` arg, which loads the entire (~64GB) I128.hdf5 file into RAM for faster data loading. If you don't have enough RAM to support this (probably 96GB+), remove this argument.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}