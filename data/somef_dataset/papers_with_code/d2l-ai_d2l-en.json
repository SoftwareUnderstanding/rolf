{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2106.11342"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhang2021dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    journal={arXiv preprint arXiv:2106.11342},\n    year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9993424490694317,
        0.9916572200183664
      ],
      "excerpt": "Descending through a Crowded Valley--Benchmarking Deep Learning Optimizers. R. Schmidt, F. Schneider, P. Hennig. International Conference on Machine Learning, 2021 \nUniversal Average-Case Optimality of Polyak Momentum. D. Scieur, F. Pedregosan. International Conference on Machine Learning, 2020 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0,
        0.9971223382559767
      ],
      "excerpt": "GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing. J. Guo, H. He, T. He, L. Lausen, M. Li, H. Lin, X. Shi, C. Wang, J. Xie, S. Zha, A. Zhang, H. Zhang, Z. Zhang, Z. Zhang, S. Zheng, and Y. Zhu. Journal of Machine Learning Research, 2020 \nDetecting Human Driver Inattentive and Aggressive Driving Behavior Using Deep Learning: Recent Advances, Requirements and Open Challenges. M. Alkinani, W. Khan, Q. Arshad. IEEE Access, 2020 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9463055632955055,
        0.9941879527649682,
        0.9999987220041571,
        0.9936456596442305,
        0.9999999996816484
      ],
      "excerpt": "1. [**Diagnosing Parkinson by Using Deep Autoencoder Neural Network**](https://link.springer.com/chapter/10.1007/978-981-15-6325-6_5). U. Kose, O. Deperlioglu, J. Alzubi, B. Patrut. *Deep Learning for Medical Decision Support Systems, 2020* \n1. [**Deep Learning Architectures for Medical Diagnosis**](https://link.springer.com/chapter/10.1007/978-981-15-6325-6_2). U. Kose, O. Deperlioglu, J. Alzubi, B. Patrut. *Deep Learning for Medical Decision Support Systems, 2020* \n1. [**ControlVAE: Tuning, Analytical Properties, and Performance Analysis**](https://arxiv.org/pdf/2011.01754.pdf). H. Shao, Z. Xiao, S. Yao, D. Sun, A. Zhang, S. Liu, T. Abdelzaher. \n1. [**Potential, challenges and future directions for deep learning in prognostics and health management applications**](https://reader.elsevier.com/reader/sd/pii/S0952197620301184?token=7261E56B97513C5D621B9B5F43CAABEC2860AE3036278C3E5264707C32DCB658077B2AFA6ED6D5CD0FB7B16770828080). O. Fink, Q. Wang, M. Svens\u00e9n, P. Dersin, W-J. Lee, M. Ducoffe. *Engineering Applications of Artificial Intelligence, 2020* \n1. [**Learning User Representations with Hypercuboids for Recommender Systems**](https://arxiv.org/pdf/2011.05742.pdf). S. Zhang, H. Liu, A. Zhang, Y. Hu, C. Zhang, Y. Li, T. Zhu, S. He, W. Ou. *ACM International Conference on Web Search and Data Mining, 2021* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999685608968402
      ],
      "excerpt": "If you find this book useful, please star (\u2605) this repository or cite this book using the following bibtex entry: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9257135145668539
      ],
      "excerpt": "<b>&mdash; Jensen Huang, Founder and CEO, NVIDIA</b> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9822278878921976
      ],
      "excerpt": "<b>&mdash; Jiawei Han, Michael Aiken Chair Professor, University of Illinois at Urbana-Champaign</b> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/d2l-ai/d2l-en/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/d2l-ai/d2l-en",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to This Book\n:label:sec_how_to_contribute\nContributions by readers help us improve this book. If you find a typo, an outdated link, something where you think we missed a citation, where the code does not look elegant or where an explanation is unclear, please contribute back and help us help our readers. While in regular books the delay between print runs (and thus between typo corrections) can be measured in years, it typically takes hours to days to incorporate an improvement in this book. This is all possible due to version control and continuous integration (CI) testing. To do so you need to submit a pull request to the GitHub repository. When your pull request is merged into the code repository by the authors, you will become a contributor.\nSubmitting Minor Changes\nThe most common contributions are editing one sentence or fixing typos. We recommend you to find the source file in the GitHub repository and edit the file directly. For example, you can search the file through the Find file button (:numref:fig_edit_file) to locate the source file (a markdown file). Then you click the \"Edit this file\" button on the upper-right corner to make your changes in the markdown file.\n\n:width:300px\n:label:fig_edit_file\nAfter you are done, fill in your change descriptions in the \"Propose file change\" panel on the page bottom and then click the \"Propose file change\" button. It will redirect you to a new page to review your changes (:numref:fig_git_createpr). If everything is good, you can submit a pull request by clicking the \"Create pull request\" button.\nProposing Major Changes\nIf you plan to update a large portion of text or code, then you need to know a little bit more about the format this book is using. The source file is based on the markdown format with a set of extensions through the d2lbook package such as referring to equations, images, chapters, and citations. You can use any markdown editors to open these files and make your changes.\nIf you would like to change the code, we recommend you to use the Jupyter Notebook to open these markdown files as described in :numref:sec_jupyter. So that you can run and test your changes. Please remember to clear all outputs before submitting your changes, our CI system will execute the sections you updated to generate outputs.\nSome sections may support multiple framework implementations.\nIf you add a new code block not for the default implementation, which is MXNet, please use #@tab to mark this block on the beginning line. For example, #@tab pytorch for a PyTorch code block, #@tab tensorflow for a TensorFlow code block, or #@tab all a shared code block for all implementations. You may refer to the d2lbook package for more information.\nSubmitting Major Changes\nWe suggest you to use the standard Git process to submit a major change. In a nutshell the process works as described in :numref:fig_contribute.\n\n:label:fig_contribute\nWe will walk you through the steps in detail. If you are already familiar with Git you can skip this section. For concreteness we assume that the contributor's user name is \"astonzhang\".\nInstalling Git\nThe Git open source book describes how to install Git. This typically works via apt install git on Ubuntu Linux, by installing the Xcode developer tools on macOS, or by using GitHub's desktop client. If you do not have a GitHub account, you need to sign up for one.\nLogging in to GitHub\nEnter the address of the book's code repository in your browser. Click on the Fork button in the red box at the upper-right of :numref:fig_git_fork, to make a copy of the repository of this book. This is now your copy and you can change it any way you want.\n\n:width:700px\n:label:fig_git_fork\nNow, the code repository of this book will be forked (i.e., copied) to your username, such as astonzhang/d2l-en shown at the upper-left of :numref:fig_git_forked.\n\n:width:700px\n:label:fig_git_forked\nCloning the Repository\nTo clone the repository (i.e., to make a local copy) we need to get its repository address. The green button in :numref:fig_git_clone displays this. Make sure that your local copy is up to date with the main repository if you decide to keep this fork around for longer. For now simply follow the instructions in :ref:chap_installation to get started. The main difference is that you are now downloading your own fork of the repository.\n\n:width:700px\n:label:fig_git_clone\n```\nReplace your_github_username with your GitHub username\ngit clone https://github.com/your_github_username/d2l-en.git\n```\nEditing and Pushing\nNow it is time to edit the book. It is best to edit it in the Jupyter Notebook following instructions in :numref:sec_jupyter. Make the changes and check that they are OK. Assume that we have modified a typo in the file ~/d2l-en/chapter_appendix_tools/how-to-contribute.md.\nYou can then check which files you have changed.\nAt this point Git will prompt that the chapter_appendix_tools/how-to-contribute.md file has been modified.\n```\nmylaptop:d2l-en me$ git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\nmodified:   chapter_appendix_tools/how-to-contribute.md\n\n```\nAfter confirming that this is what you want, execute the following command:\ngit add chapter_appendix_tools/how-to-contribute.md\ngit commit -m 'fix typo in git documentation'\ngit push\nThe changed code will then be in your personal fork of the repository. To request the addition of your change, you have to create a pull request for the official repository of the book.\nSubmitting Pull Requests\nAs shown in :numref:fig_git_newpr, go to your fork of the repository on GitHub and select \"New pull request\". This will open up a screen that shows you the changes between your edits and what is current in the main repository of the book.\n\n:width:700px\n:label:fig_git_newpr\nFinally, submit a pull request by clicking the button as shown in :numref:fig_git_createpr. Make sure to describe the changes you have made in the pull request.\nThis will make it easier for the authors to review it and to merge it with the book. Depending on the changes, this might get accepted right away, rejected, or more likely, you will get some feedback on the changes. Once you have incorporated them, you are good to go.\n\n:width:700px\n:label:fig_git_createpr\nSummary\n\nYou can use GitHub to contribute to this book.\nYou can edit the file on GitHub directly for minor changes.\nFor a major change, please fork the repository, edit things locally, and only contribute back once you are ready.\nPull requests are how contributions are being bundled up. Try not to submit huge pull requests since this makes them hard to understand and incorporate. Better send several smaller ones.\n\nExercises\n\nStar and fork the d2l-ai/d2l-en repository.\nIf you spot anything that needs improvement (e.g., missing a reference), submit a pull request. \nIt is usually a better practice to create a pull request using a new branch. Learn how to do it with Git branching.\n\nDiscussions",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-09T01:04:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T04:27:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This open source book is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See [LICENSE](LICENSE) file.\n\nThe sample and reference code within this open source book is made available under a modified MIT license. See the [LICENSE-SAMPLECODE](LICENSE-SAMPLECODE) file.\n\n[Chinese version](https://github.com/d2l-ai/d2l-zh) | [Discuss and report issues](https://discuss.d2l.ai/) | [Code of conduct](CODE_OF_CONDUCT.md) | [Other Information](INFO.md)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.939533612740075
      ],
      "excerpt": "<h5 align=\"center\"><i>The best way to understand deep learning is learning by doing.</i></h5> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8824615292817931,
        0.969403910973792
      ],
      "excerpt": "This open-source book represents our attempt to make deep learning approachable, teaching you the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. \nOur goal is to offer a resource that could \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9016079964899572,
        0.9738971315776672
      ],
      "excerpt": "1. allow for rapid updates, both by us and also by the community at large; \n1. be complemented by a forum for interactive discussion of technical details and to answer questions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.883320092804424,
        0.9348745364152056
      ],
      "excerpt": "Universal Average-Case Optimality of Polyak Momentum. D. Scieur, F. Pedregosan. International Conference on Machine Learning, 2020 \n2D Digital Image Correlation and Region-Based Convolutional Neural Network in Monitoring and Evaluation of Surface Cracks in Concrete Structural Elements. M. S\u0142o\u0144ski, M. Tekieli. Materials, 2020 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9040982208839282
      ],
      "excerpt": "1. [**Potential, challenges and future directions for deep learning in prognostics and health management applications**](https://reader.elsevier.com/reader/sd/pii/S0952197620301184?token=7261E56B97513C5D621B9B5F43CAABEC2860AE3036278C3E5264707C32DCB658077B2AFA6ED6D5CD0FB7B16770828080). O. Fink, Q. Wang, M. Svens\u00e9n, P. Dersin, W-J. Lee, M. Ducoffe. *Engineering Applications of Artificial Intelligence, 2020* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9946317981170554
      ],
      "excerpt": "<p>\"In less than a decade, the AI revolution has swept from research labs to broad industries to every corner of our daily life.  Dive into Deep Learning is an excellent text on deep learning and deserves attention from anyone who wants to learn why deep learning has ignited the AI revolution: the most powerful technology force of our time.\"</p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9954718894858755
      ],
      "excerpt": "<p>\"This is a timely, fascinating book, providing with not only a comprehensive overview of deep learning principles but also detailed algorithms with hands-on programming code, and moreover, a state-of-the-art introduction to deep learning in computer vision and natural language processing. Dive into this book if you want to dive into deep learning!\"</p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9768546942216868
      ],
      "excerpt": "<p>\"This is a highly welcome addition to the machine learning literature, with a focus on hands-on experience implemented via the integration of Jupyter notebooks. Students of deep learning should find this invaluable to become proficient in this field.\"</p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594497275416512
      ],
      "excerpt": "This open source book has benefited from pedagogical suggestions, typo corrections, and other improvements from community contributors. Your help is valuable for making the book better for everyone. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Interactive deep learning book with multi-framework code, math, and discussions. Adopted at 300 universities from 55 countries including Stanford, MIT, Harvard, and Cambridge.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/d2l-ai/d2l-en/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2769,
      "date": "Mon, 27 Dec 2021 07:35:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/d2l-ai/d2l-en/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "d2l-ai/d2l-en",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/d2l-ai/d2l-en/master/graffle/convert.sh",
      "https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/cache.sh",
      "https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/clean_img.sh",
      "https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/build_html.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.9105170275933254,
        0.9167565967623973,
        0.8895014054083792,
        0.8872012052170378
      ],
      "excerpt": "  <img width=\"200\"  src=\"static/frontpage/_images/eq.jpg\"> \n  <img width=\"200\"  src=\"static/frontpage/_images/figure.jpg\"> \n  <img width=\"200\"  src=\"static/frontpage/_images/code.jpg\"> \n  <img width=\"200\"  src=\"static/frontpage/_images/notebook.gif\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8337070632228121
      ],
      "excerpt": "  <img width=\"600\"  src=\"static/frontpage/_images/map.png\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/d2l-ai/d2l-en/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "TeX",
      "HTML",
      "Shell",
      "CSS"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/d2l-ai/d2l-en/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Creative Commons Attribution-ShareAlike 4.0 International Public License\\n\\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\\n\\nSection 1 \\xe2\\x80\\x93 Definitions.\\n\\t\\n     a.\\tAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\\n\\t\\n     b.\\tAdapter\\'s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\\n\\t\\n     c.\\tBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\\n\\t\\n     d.\\tCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\\n\\t\\n     e.\\tEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\\n\\t\\n     f.\\tExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\\n\\t\\n     g.\\tLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\\n\\t\\n     h.\\tLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\\n\\t\\n     i.\\tLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\\n\\t\\n     j.\\tLicensor means the individual(s) or entity(ies) granting rights under this Public License.\\n\\t\\n     k.\\tShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\\n\\t\\n     l.\\tSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\\n\\t\\n     m.\\tYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\\n\\nSection 2 \\xe2\\x80\\x93 Scope.\\n\\t\\n     a.\\tLicense grant.\\n\\t\\n          1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\\n\\n               A. reproduce and Share the Licensed Material, in whole or in part; and\\t\\n\\n               B. produce, reproduce, and Share Adapted Material.\\n\\t\\n          2. Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\\n\\t\\n          3. Term. The term of this Public License is specified in Section 6(a).\\n\\t\\n          4. Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\\n\\t\\n          5. Downstream recipients.\\n\\n               A. Offer from the Licensor \\xe2\\x80\\x93 Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\\n\\t\\n               B. Additional offer from the Licensor \\xe2\\x80\\x93 Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter\\xe2\\x80\\x99s License You apply.\\n\\t\\n               C. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\\n\\t\\n          6. No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\\n\\t\\n     b.\\tOther rights.\\n\\t\\n          1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\\n\\t\\n          2. Patent and trademark rights are not licensed under this Public License.\\n\\t\\n          3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\\n\\nSection 3 \\xe2\\x80\\x93 License Conditions.\\n\\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\\n\\t\\n     a.\\tAttribution.\\n\\t\\n          1. If You Share the Licensed Material (including in modified form), You must:\\n\\n               A. retain the following if it is supplied by the Licensor with the Licensed Material:\\n\\n                    i.\\tidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\\n\\n                    ii.\\ta copyright notice;\\n\\n                    iii. a notice that refers to this Public License;\\n\\n                    iv.\\ta notice that refers to the disclaimer of warranties;\\n\\n                    v.\\ta URI or hyperlink to the Licensed Material to the extent reasonably practicable;\\n\\n               B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\\n\\n               C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\\n\\t\\n          2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\\n\\t\\n          3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\\n\\t\\n     b.\\tShareAlike.In addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\\n\\t\\n          1. The Adapter\\xe2\\x80\\x99s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\\n\\t\\n          2. You must include the text of, or the URI or hyperlink to, the Adapter\\'s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\\n\\t\\n          3. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter\\'s License You apply.\\n\\nSection 4 \\xe2\\x80\\x93 Sui Generis Database Rights.\\n\\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\\n\\t\\n     a.\\tfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\\n\\t\\n     b.\\tif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\\n\\t\\n     c.\\tYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\\n\\nSection 5 \\xe2\\x80\\x93 Disclaimer of Warranties and Limitation of Liability.\\n\\t\\n     a.\\tUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\\n\\t\\n     b.\\tTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\\n\\t\\n     c.\\tThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\\n\\nSection 6 \\xe2\\x80\\x93 Term and Termination.\\n\\t\\n     a.\\tThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\\n\\t\\n     b.\\tWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\\n\\t\\n          1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\\n\\t\\n          2. upon express reinstatement by the Licensor.\\n\\t\\n     c.\\tFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\\n\\t\\n     d.\\tFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\\n\\t\\n     e.\\tSections 1, 5, 6, 7, and 8 survive termination of this Public License.\\n\\nSection 7 \\xe2\\x80\\x93 Other Terms and Conditions.\\n\\t\\n     a.\\tThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\\n\\t\\n     b.\\tAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\\n\\nSection 8 \\xe2\\x80\\x93 Interpretation.\\n\\t\\n     a.\\tFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\\n\\t\\n     b.\\tTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\\n\\t\\n     c.\\tNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\\n\\t\\n     d.\\tNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "D2L.ai: Interactive Deep Learning Book with Multi-Framework Code, Math, and Discussions",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "d2l-en",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "d2l-ai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/d2l-ai/d2l-en/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "astonzhang",
        "body": "This release supports running the book with [SageMaker Studio Lab](https://studiolab.sagemaker.aws/import/github/d2l-ai/d2l-pytorch-sagemaker-studio-lab/blob/main/GettingStarted-D2L.ipynb) for free and introduces several fixes:\r\n\r\n* Fix data synchronization for multi-GPU training in PyTorch (https://github.com/d2l-ai/d2l-en/pull/1978)\r\n* Fix token sampling in BERT datasets (https://github.com/d2l-ai/d2l-en/pull/1979/)\r\n* Fix semantic segmentation normalization in PyTorch (https://github.com/d2l-ai/d2l-en/pull/1980/)\r\n* Fix mean square loss calculation in PyTorch and TensorFlow (https://github.com/d2l-ai/d2l-en/pull/1984)\r\n* Fix broken paragraphs (https://github.com/d2l-ai/d2l-en/commit/8e0fe4ba54b6e2a0aa0f15f58a1e81f7fef1cdd7)",
        "dateCreated": "2021-12-08T19:41:35Z",
        "datePublished": "2021-12-08T20:42:23Z",
        "html_url": "https://github.com/d2l-ai/d2l-en/releases/tag/v0.17.1",
        "name": "Release v0.17.1",
        "tag_name": "v0.17.1",
        "tarball_url": "https://api.github.com/repos/d2l-ai/d2l-en/tarball/v0.17.1",
        "url": "https://api.github.com/repos/d2l-ai/d2l-en/releases/54901761",
        "zipball_url": "https://api.github.com/repos/d2l-ai/d2l-en/zipball/v0.17.1"
      },
      {
        "authorType": "User",
        "author_name": "astonzhang",
        "body": "*Dive into Deep Learning* is now available on [arxiv](https://arxiv.org/abs/2106.11342)!\r\n\r\n# Framework Adaptation \r\n\r\nWe have added TensorFlow implementations up to Chapter 11 (Optimization Algorithms). \r\n\r\n# Towards v1.0\r\n\r\nThe following chapters have been significantly improved for v1.0:\r\n\r\n* Optimization (the first 4 sections)\r\n* Computational Performance\r\n* Computer Vision\r\n* Natural Language Processing: Pretraining\r\n* Natural Language Processing: Applications\r\n\r\nFinalized chapters are being translated into Chinese ([d2l-zh v2](https://github.com/d2l-ai/d2l-zh/tree/v2))\r\n\r\n# Other Improvements\r\n\r\n* Add BLEU uniform weights from the original paper\r\n* Revise the normalization trick in LogSumExp\r\n* Revise data standardization\r\n* Prove convexity using second derivatives for one-dimensional and multi-dimensional cases\r\n* Improve d2l.train_2d function\r\n* Improve convergence analysis of Newton's method\r\n* Improve SGD convergence analysis for convex objectives 1\r\n* Improve Convergence Analysis for Convex Objectives\r\n* Reorganize comparisons of network partitioning, layer-wise partitioning, and data parallelism\r\n* Improve d2l.box_iou function\r\n* Improve the \"Labeling Classes and Offsets\" subsection\r\n* Add discussions of issues of non-maximum suppression\r\n* Reorganize multiscale anchor boxes and multiscale detection\r\n* Highlight layerwise representations via deep nets in multiscale object detection\r\n* Connect SSD downsampling blocks to VGG blocks\r\n* Refer to YOLO and a recent survey on object detection\r\n* Fix legend issues in Kaggle CIFAR-10 and ImageNet Dogs\r\n* Improve performance on the Kaggle small-scale CIFAR-10 dataset\r\n* Improve performance on the Kaggle small-scale ImageNet Dog dataset\r\n* Improve the function to build the mapping from RGB to class indices for VOC labels\r\n* Revise motivations for transposed convolution\r\n* Rewrite basic transposed convolution operation\r\n* Add relations between transposed convolution and regular convolution implementations\r\n* Improve explanations of the pretrained backbone for the fully convolutional network\r\n* Improve the output synthesized image of style transfer\r\n* Add d2l.show_list_len_pair_hist\r\n* Fix d2l.get_negatives\r\n* Improve efficiency of d2l.Vocab\r\n* Exclude unknown tokens when training word embeddings\r\n* Add self-supervised learning\r\n* Add discussions of self-supervised learning in NLP\r\n* Revise the notation table\r\n",
        "dateCreated": "2021-07-25T09:43:06Z",
        "datePublished": "2021-07-26T05:04:20Z",
        "html_url": "https://github.com/d2l-ai/d2l-en/releases/tag/v0.17.0",
        "name": "Release v0.17.0",
        "tag_name": "v0.17.0",
        "tarball_url": "https://api.github.com/repos/d2l-ai/d2l-en/tarball/v0.17.0",
        "url": "https://api.github.com/repos/d2l-ai/d2l-en/releases/46745015",
        "zipball_url": "https://api.github.com/repos/d2l-ai/d2l-en/zipball/v0.17.0"
      },
      {
        "authorType": "User",
        "author_name": "astonzhang",
        "body": "# Brand-New Attention Chapter\r\n\r\nWe have added the brand-new Chapter: Attention Mechanisms:\r\n\r\n* Attention Cues\r\n  * Attention Cues in Biology\r\n  * Queries, Keys, and Values\r\n  * Visualization of Attention\r\n\r\n* Attention Pooling: Nadaraya-Watson Kernel Regression\r\n  * Generating the Dataset\r\n  * Average Pooling\r\n  * Nonparametric Attention Pooling\r\n  * Parametric Attention Pooling\r\n\r\n* Attention Scoring Functions\r\n  * Masked Softmax Operation\r\n  * Additive Attention\r\n  * Scaled Dot-Product Attention\r\n\r\n* Bahdanau Attention\r\n  * Model\r\n  * Defining the Decoder with Attention\r\n  * Training\r\n\r\n* Multi-Head Attention\r\n  * Model\r\n  * Implementation\r\n\r\n* Self-Attention and Positional Encoding\r\n  * Self-Attention\r\n  * Comparing CNNs, RNNs, and Self-Attention\r\n  * Positional Encoding\r\n\r\n* Transformer\r\n  * Model\r\n  * Positionwise Feed-Forward Networks\r\n  * Residual Connection and Layer Normalization\r\n  * Encoder\r\n  * Decoder\r\n  * Training\r\n\r\n\r\n# PyTorch Adaptation Completed\r\n\r\nWe have completed PyTorch implementations for Vol.1 (Chapter 1--15). \r\n\r\n\r\n# Towards v1.0\r\n\r\nThe following chapters have been significantly improved for v1.0:\r\n\r\n* Introduction\r\n* Modern Recurrent Neural Networks\r\n\r\n\r\n# Chinese Translation\r\n\r\nThe following chapters have been translated into Chinese ([d2l-zh v2 Git repo](https://github.com/d2l-ai/d2l-zh/tree/v2), [Web preview](http://preview.d2l.ai.s3-website-us-west-2.amazonaws.com/d2l-zh/v2/)):\r\n\r\n* Introduction\r\n* Preliminaries\r\n* Linear Neural Networks\r\n* Multilayer Perceptrons\r\n* Deep Learning Computation\r\n* Convolutional Neural Networks\r\n* Modern Convolutional Neural Networks\r\n\r\n# Turkish Translation \r\n\r\nThe community are translating the book into Turkish ([d2l-tr Git repo](https://github.com/d2l-ai/d2l-tr), [Web preview](http://preview.d2l.ai/d2l-tr/master/)). The first draft of Chapter 1--7 is complete.\r\n\r\n\r\n\r\n",
        "dateCreated": "2021-01-06T07:22:20Z",
        "datePublished": "2021-01-06T08:06:56Z",
        "html_url": "https://github.com/d2l-ai/d2l-en/releases/tag/v0.16.0",
        "name": "Release v0.16.0",
        "tag_name": "v0.16.0",
        "tarball_url": "https://api.github.com/repos/d2l-ai/d2l-en/tarball/v0.16.0",
        "url": "https://api.github.com/repos/d2l-ai/d2l-en/releases/36034380",
        "zipball_url": "https://api.github.com/repos/d2l-ai/d2l-en/zipball/v0.16.0"
      },
      {
        "authorType": "User",
        "author_name": "astonzhang",
        "body": "# Framework Adaptation \r\n\r\nWe have added PyTorch implementations up to Chapter 11 (Optimization Algorithms). Chapter 1--7 and Chapter 11 have also been adapted to TensorFlow.\r\n\r\n# Towards v1.0\r\n\r\nThe following chapters have been significantly improved for v1.0:\r\n\r\n* Linear Neural Networks\r\n* Multilayer Perceptrons\r\n* Deep Learning Computation\r\n* Convolutional Neural Networks\r\n* Modern Convolutional Neural Networks\r\n* Recurrent Neural Networks\r\n\r\nFinalized chapters are being translated into Chinese ([d2l-zh v2](https://github.com/d2l-ai/d2l-zh/tree/v2))\r\n\r\n\r\n# Other Improvements\r\n\r\n* Fixed issues of not showing all the equation numbers in the HTML and PDF\r\n* Consistently used f-string\r\n* Revised overfitting experiments\r\n* Fixed implementation errors for weight decay experiments\r\n* Improved layer index style\r\n* Revised \"breaking the symmetry\"\r\n* Revised descriptions of covariate and label shift\r\n* Fixed mathematical errors in covariate shift correction\r\n* Added true risk, empirical risk, and (weighted) empirical risk minimization\r\n* Improved variable naming style for matrices and tensors\r\n* Improved consistency of mathematical notation for tensors of order two or higher\r\n* Improved mathematical descriptions of convolution\r\n* Revised descriptions of cross-correlation\r\n* Added feature maps and receptive fields\r\n* Revised mathematical descriptions of batch normalization\r\n* Added more details to Markov models\r\n* Fixed implementations of k-step-ahead predictions in sequence modeling\r\n* Fixed mathematical descriptions in language modeling\r\n* Improved the `d2l.Vocab` API\r\n* Fixed mathematical descriptions and figure illustrations for deep RNNs\r\n* Added BLEU\r\n* Improved machine translation application results\r\n* Improved the animation plot function in the all the training loops\r\n",
        "dateCreated": "2020-10-23T10:08:36Z",
        "datePublished": "2020-10-23T11:04:17Z",
        "html_url": "https://github.com/d2l-ai/d2l-en/releases/tag/v0.15.0",
        "name": "Release v0.15.0",
        "tag_name": "v0.15.0",
        "tarball_url": "https://api.github.com/repos/d2l-ai/d2l-en/tarball/v0.15.0",
        "url": "https://api.github.com/repos/d2l-ai/d2l-en/releases/32970828",
        "zipball_url": "https://api.github.com/repos/d2l-ai/d2l-en/zipball/v0.15.0"
      },
      {
        "authorType": "User",
        "author_name": "mli",
        "body": "# Highlights\r\n\r\nWe have added both PyTorch and TensorFlow implementations up to Chapter 7 (Modern CNNs). \r\n\r\n# Improvements\r\n\r\n- We updated the text to be framework neutral, such as now we call `ndarray` as tensor. \r\n- Readers can click the tab in the HTML version to switch between frameworks, both colab button and discussion thread will change properly. \r\n- We changed the release process, d2l.ai will host the latest release (i.e. the release branch), instead of the contents from the master branch. We unified the version number of both text and the `d2l` package. That's why we jumped from v0.8 to v0.14.0 \r\n- The notebook zip contains three folders, `mxnet`, `pytorch` and `tensorflow` (though we only build the PDF for mxnet yet). \r\n",
        "dateCreated": "2020-07-02T16:23:05Z",
        "datePublished": "2020-07-08T17:53:30Z",
        "html_url": "https://github.com/d2l-ai/d2l-en/releases/tag/v0.14.0",
        "name": "Release v0.14.0",
        "tag_name": "v0.14.0",
        "tarball_url": "https://api.github.com/repos/d2l-ai/d2l-en/tarball/v0.14.0",
        "url": "https://api.github.com/repos/d2l-ai/d2l-en/releases/28369576",
        "zipball_url": "https://api.github.com/repos/d2l-ai/d2l-en/zipball/v0.14.0"
      },
      {
        "authorType": "User",
        "author_name": "astonzhang",
        "body": "# Highlights\r\n\r\nD2L is now runnable on [Amazon SageMaker](https://d2l.ai/chapter_appendix-tools-for-deep-learning/sagemaker.html) and [Google Colab](https://d2l.ai/chapter_appendix-tools-for-deep-learning/colab.html). \r\n\r\n# New Contents\r\n\r\nThe following chapters are re-organized:\r\n\r\n* Natural Language Processing: Pretraining\r\n* Natural Language Processing: Applications\r\n\r\nThe following sections are added:\r\n\r\n* Subword Embedding (Byte-pair encoding)\r\n* Bidirectional Encoder Representations from Transformers (BERT)\r\n* The Dataset for Pretraining BERT\r\n* Pretraining BERT\r\n* Natural Language Inference and the Dataset\r\n* Natural Language Inference: Using Attention\r\n* Fine-Tuning BERT for Sequence-Level and Token-Level Applications\r\n* Natural Language Inference: Fine-Tuning BERT\r\n\r\n# Improvements\r\n\r\nThere have been many light revisions and improvements throughout the book.",
        "dateCreated": "2020-05-30T06:35:12Z",
        "datePublished": "2020-05-30T06:51:42Z",
        "html_url": "https://github.com/d2l-ai/d2l-en/releases/tag/v0.8.0",
        "name": "Release v0.8.0",
        "tag_name": "v0.8.0",
        "tarball_url": "https://api.github.com/repos/d2l-ai/d2l-en/tarball/v0.8.0",
        "url": "https://api.github.com/repos/d2l-ai/d2l-en/releases/27054994",
        "zipball_url": "https://api.github.com/repos/d2l-ai/d2l-en/zipball/v0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "astonzhang",
        "body": "# Highlights\r\n\r\n* D2L is now based on the NumPy interface. All the code samples are rewritten.\r\n\r\n# New Contents\r\n\r\n* Recommender Systems\r\n  * Overview of Recommender Systems\r\n  * The MovieLens Dataset\r\n  * Matrix Factorization\r\n  * AutoRec: Rating Prediction with Autoencoders\r\n  * Personalized Ranking for Recommender Systems\r\n  * Neural Collaborative Filtering for Personalized Ranking\r\n  * Sequence-Aware Recommender Systems\r\n  * Feature-Rich Recommender Systems\r\n  * Factorization Machines\r\n  * Deep Factorization Machines\r\n\r\n* Appendix: Mathematics for Deep Learning\r\n  * Geometry and Linear Algebraic Operations\r\n  * Eigendecompositions\r\n  * Single Variable Calculus\r\n  * Multivariable Calculus\r\n  * Integral Calculus\r\n  * Random Variables\r\n  * Maximum Likelihood\r\n  * Distributions\r\n  * Naive Bayes\r\n  * Statistics\r\n  * Information Theory\r\n\r\n* Attention Mechanisms\r\n  * Attention Mechanism\r\n  * Sequence to Sequence with Attention Mechanism\r\n  * Transformer\r\n\r\n* Generative Adversarial Networks\r\n  * Generative Adversarial Networks\r\n  * Deep Convolutional Generative Adversarial Networks\r\n\r\n* Preliminaries\r\n  * Data Preprocessing\r\n  * Calculus\r\n\r\n\r\n# Improvements\r\n\r\n* The Preliminaries chapter is improved.\r\n* More theoretical analysis is added to the Optimization chapter. \r\n\r\n\r\n# Preview Version\r\n\r\nHard copies of a D2L preview version based on this release (excluding chapters of Recommender Systems and Generative Adversarial Networks) are distributed at AWS re:Invent 2019 and NeurIPS 2019.",
        "dateCreated": "2019-12-09T06:57:25Z",
        "datePublished": "2019-12-18T00:46:54Z",
        "html_url": "https://github.com/d2l-ai/d2l-en/releases/tag/v0.7.0",
        "name": "Release v0.7.0",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/d2l-ai/d2l-en/tarball/v0.7.0",
        "url": "https://api.github.com/repos/d2l-ai/d2l-en/releases/22307323",
        "zipball_url": "https://api.github.com/repos/d2l-ai/d2l-en/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "astonzhang",
        "body": "Change of Contents\r\n=================\r\n\r\nWe heavily revised the following chapters, especially during teaching [STAT 157 at Berkeley](https://courses.d2l.ai/berkeley-stat-157/index.html).\r\n\r\n* Preface\r\n* Installation\r\n* Introduction\r\n* The Preliminaries: A Crashcourse\r\n* Linear Neural Networks\r\n* Multilayer Perceptrons\r\n* Recurrent Neural Networks\r\n\r\n\r\nThe Community Are Translating D2L into Korean and Japanese\r\n=================\r\n\r\n[d2l-ko in Korean](https://github.com/d2l-ai/d2l-ko) (website: ko.d2l.ai) joins d2l.ai! Thank [Muhyun Kim](https://github.com/muhyun), [Kyoungsu Lee](https://github.com/yikster), [Ji hye Seo](https://github.com/jihys), [Jiyang Kang](https://github.com/jamiekang) and [many other contributors](https://github.com/d2l-ai/d2l-ko/graphs/contributors)!\r\n\r\n[d2l-ja in Japanese](https://github.com/d2l-ai/d2l-ja) (website: ja.d2l.ai) joins d2l.ai! Thank [Masaki Samejima](https://github.com/harusametime)!\r\n\r\n\r\nThanks to Our Contributors\r\n=================\r\n@alxnorden, @avinashingit, @bowen0701, @brettkoonce, Chaitanya Prakash Bapat, @cryptonaut, Davide Fiocco, @edgarroman, @gkutiel, John Mitro, Liang Pu, Rahul Agarwal, @mohamed-ali, @mstewart141, Mike M\u00fcller, @NRauschmayr, @Prakhar Srivastav, @sad-, @sfermigier, Sheng Zha, @sundeepteki, @topecongiro, @tpdi, @vermicelli, Vishaal Kapoor, @vishwesh5, @YaYaB, Yuhong Chen, Evgeniy Smirnov, @lgov, Simon Corston-Oliver, @IgorDzreyev, @trungha-ngx, @pmuens, @alukovenko, @senorcinco, @vfdev-5, @dsweet, Mohammad Mahdi Rahimi, Abhishek Gupta, @uwsd, @DomKM, Lisa Oakley, @vfdev-5, @bowen0701, @arush15june, @prasanth5reddy. \r\n",
        "dateCreated": "2019-04-11T17:13:15Z",
        "datePublished": "2019-04-11T18:00:21Z",
        "html_url": "https://github.com/d2l-ai/d2l-en/releases/tag/v0.6.0",
        "name": "Release v0.6.0",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/d2l-ai/d2l-en/tarball/v0.6.0",
        "url": "https://api.github.com/repos/d2l-ai/d2l-en/releases/16708454",
        "zipball_url": "https://api.github.com/repos/d2l-ai/d2l-en/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "astonzhang",
        "body": "Contents\r\n=================\r\n\r\n* Translated contents from https://github.com/d2l-ai/d2l-zh, including the following chapters\r\n    * Introduction\r\n    * A Taste of Deep Learning\r\n    * Deep Learning Basics\r\n    * Deep Learning Computation\r\n    * Convolutional Neural Networks\r\n    * Recurrent Neural Networks\r\n    * Optimization Algorithms\r\n    * Computational Performance\r\n    * Computer Vision\r\n    * Natural Language Processing\r\n    * Appendix\r\n\r\n* Added new contents in the following chapters\r\n    * Introduction\r\n    * A Taste of Deep Learning\r\n    * Deep Learning Basics\r\n    * Deep Learning Computation\r\n    * Convolutional Neural Networks\r\n\r\n\r\nStyle\r\n=================\r\n* Improved HTML styles\r\n* Improved PDF styles\r\n\r\nChinese Version\r\n=================\r\nv1.0.0-rc0 is released: https://github.com/d2l-ai/d2l-zh/releases/tag/v1.0.0-rc0\r\nThe physical book will be published soon.\r\n\r\nThanks to Our Contributors\r\n=================\r\nalxnorden, avinashingit, bowen0701, brettkoonce, Chaitanya Prakash Bapat, cryptonaut, Davide Fiocco, edgarroman, gkutiel, John Mitro, Liang Pu, Rahul Agarwal, mohamed-ali, mstewart141, Mike M\u00fcller, NRauschmayr, Prakhar Srivastav, sad-, sfermigier, Sheng Zha, sundeepteki, topecongiro, tpdi, vermicelli, Vishaal Kapoor, vishwesh5, YaYaB\r\n",
        "dateCreated": "2019-01-25T07:23:32Z",
        "datePublished": "2019-01-25T07:45:32Z",
        "html_url": "https://github.com/d2l-ai/d2l-en/releases/tag/v0.5.0",
        "name": "Release v0.5.0",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/d2l-ai/d2l-en/tarball/v0.5.0",
        "url": "https://api.github.com/repos/d2l-ai/d2l-en/releases/15175988",
        "zipball_url": "https://api.github.com/repos/d2l-ai/d2l-en/zipball/v0.5.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11942,
      "date": "Mon, 27 Dec 2021 07:35:02 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "machine-learning",
      "book",
      "notebook",
      "computer-vision",
      "natural-language-processing",
      "python",
      "kaggle",
      "data-science",
      "mxnet",
      "pytorch",
      "tensorflow",
      "keras"
    ],
    "technique": "GitHub API"
  }
}