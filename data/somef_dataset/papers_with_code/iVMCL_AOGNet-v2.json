{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1711.05847",
      "https://arxiv.org/abs/1908.01259",
      "https://arxiv.org/abs/1908.01259"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please consider citing the AOGNets or Attentive Normalization papers in your publications if it helps your research.\n```\n@inproceedings{li2019aognets,\n  title={AOGNets: Compositional Grammatical Architectures for Deep Learning},\n  author={Li, Xilai and Song, Xi and Wu, Tianfu},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={6220--6230},\n  year={2019}\n}\n\n@article{li2019attentive,\n  title={Attentive Normalization},\n  author={Li, Xilai and Sun, Wei and Wu, Tianfu},\n  journal={arXiv preprint arXiv:1908.01259},\n  year={2019}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{li2019attentive,\n  title={Attentive Normalization},\n  author={Li, Xilai and Sun, Wei and Wu, Tianfu},\n  journal={arXiv preprint arXiv:1908.01259},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{li2019aognets,\n  title={AOGNets: Compositional Grammatical Architectures for Deep Learning},\n  author={Li, Xilai and Song, Xi and Wu, Tianfu},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={6220--6230},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8746044245410667
      ],
      "excerpt": "Please check our refactorized code at iVMCL-Release. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920391031109833
      ],
      "excerpt": "(CVPR 2019) and Attentive Normalization. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/iVMCL/AOGNet-v2",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please feel free to report issues and any related problems to Xilai Li (xli47 at ncsu dot edu), and Tianfu Wu (twu19 at ncsu dot edu).\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-18T04:07:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-28T09:07:36Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8968081197175185,
        0.9806160063399251
      ],
      "excerpt": "Please check our refactorized code at iVMCL-Release. \nThis project provides source code for AOGNets: Compositional Grammatical Architectures for Deep Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555128307665404
      ],
      "excerpt": "| ResNet-50-BN | 25.56M | 4.09G | 23.01 | 6.68 | Google Drive | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.887345116730098,
        0.8555128307665404,
        0.887345116730098
      ],
      "excerpt": "| ResNet-50-AN (w/ BN) | 25.76M | 4.09G | 21.59 | 5.88 | Google Drive | \n| ResNet-101-BN | 44.57M | 8.12G | 21.33 | 5.85 | Google Drive| \n| ResNet-101-AN (w/ BN) | 45.00M | 8.12G | 20.61 | 5.41 |Google Drive | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172760706738814
      ],
      "excerpt": "| MobileNet-v2-AN (w/ BN) | 3.56M | 0.34G | 26.67 | 8.56 | Google Drive| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172760706738814
      ],
      "excerpt": "| AOGNet-12M-AN (w/ BN) | 12.37M | 2.19G | 21.28 | 5.76 | Google Drive | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8015411343962561
      ],
      "excerpt": "| AOGNet-40M-AN (w/ BN) | 40.39M | 7.51G | 19.33 | 4.72 | Google Drive | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555128307665404,
        0.8750678942814064,
        0.8494182485032163,
        0.8682985444770366
      ],
      "excerpt": "| ResNet-50-BN | 25.56M | 4.09G | 21.08 | 5.56 | Google Drive | \n| ResNet-50-AN (w/ BN) | 25.76M | 4.09G | 19.92 | 5.04 | Google Drive | \n| ResNet-101-BN | 44.57M | 8.12G | 19.71 | 4.89 | Google Drive| \n| ResNet-101-AN (w/ BN) | 45.00M | 8.12G | 18.85 | 4.63 |Google Drive | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9495011806190475
      ],
      "excerpt": "Remarks: The accuracy from the pretrained models might be slightly different than that reported during the training (used in our papers). The reason is still unclear. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079352540987015
      ],
      "excerpt": "e.g. For training AOGNet-12M with AN  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Official implementation of AOGNets and Attentive Normalization. ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/iVMCL/AOGNet-v2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Thu, 23 Dec 2021 06:03:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/iVMCL/AOGNet-v2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "iVMCL/AOGNet-v2",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/iVMCL/AOGNet-v2/master/scripts/train_fp16.sh",
      "https://raw.githubusercontent.com/iVMCL/AOGNet-v2/master/scripts/test_fp16.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Download the [ImageNet dataset](http://image-net.org/download) to YOUR_IMAGENET_PATH and move validation images to labeled subfolders\n    - The [script](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh) may be helpful.\n\n- Create a datasets subfolder under your cloned AOGNet-v2 and a symbolic link to the ImageNet dataset\n\n```\n$ cd AOGNet-v2\n$ mkdir datasets\n$ ln -s PATH_TO_YOUR_IMAGENET ./datasets/\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Create a conda environment, and install all dependencies.\n```\nconda create -n aognet-v2 python=3.7\nconda activate aognet-v2\ngit clone https://github.com/iVMCL/AOGNet-v2\ncd AOGNet-v2\npip install -r requirements.txt\n```\n2. Install PyTorch 1.0+, follows [https://pytorch.org/](https://pytorch.org/)\n\n3. Install apex, follows [https://github.com/NVIDIA/apex](https://github.com/NVIDIA/apex)\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9906248903846466,
        0.8902627162932362,
        0.8510584444209074
      ],
      "excerpt": "$ cd AOGNet-v2 \nmkdir pretrained_models \nDownload pretrained models from the links provided in the tables below, unzip it and place into the pretrained_models directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "$ cd AOGNet-v2 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8433201755977864
      ],
      "excerpt": "<img align=\"center\" src=\"images/an-comparison.png\" alt=\"drawing\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8038848982775663
      ],
      "excerpt": "Download pretrained models from the links provided in the tables below, unzip it and place into the pretrained_models directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8757963654314309,
        0.8258393085257714
      ],
      "excerpt": "| ResNet-50-BN | 25.56M | 4.09G | 23.01 | 6.68 | Google Drive | \n| ResNet-50-GN | 25.56M | 4.09G | 23.52 | 6.85 | - | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8304652573913092
      ],
      "excerpt": "Models trained with advanced training setup: 200 epochs, 0.1 label smoothing, 0.2 mixup \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/iVMCL/AOGNet-v2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/iVMCL/AOGNet-v2/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'RESEARCH ONLY LICENSE\\nCopyright (c) 2018-2019 North Carolina State University.\\nAll rights reserved.\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided\\nthat the following conditions are met:\\n1. Redistributions and use are permitted for internal research purposes only, and commercial use\\nis strictly prohibited under this license. Inquiries regarding commercial use should be directed to the\\nOffice of Research Commercialization at North Carolina State University, 919-215-7199,\\nhttps://research.ncsu.edu/commercialization/contact/, commercialization@ncsu.edu .\\n2. Commercial use means the sale, lease, export, transfer, conveyance or other distribution to a\\nthird party for financial gain, income generation or other commercial purposes of any kind, whether\\ndirect or indirect. Commercial use also means providing a service to a third party for financial gain,\\nincome generation or other commercial purposes of any kind, whether direct or indirect.\\n3. Redistributions of source code must retain the above copyright notice, this list of conditions and\\nthe following disclaimer.\\n4. Redistributions in binary form must reproduce the above copyright notice, this list of conditions\\nand the following disclaimer in the documentation and/or other materials provided with the\\ndistribution.\\n5. The names \\xe2\\x80\\x9cNorth Carolina State University\\xe2\\x80\\x9d, \\xe2\\x80\\x9cNCSU\\xe2\\x80\\x9d and any trade-name, personal name,\\ntrademark, trade device, service mark, symbol, image, icon, or any abbreviation, contraction or\\nsimulation thereof owned by North Carolina State University must not be used to endorse or promote\\nproducts derived from this software without prior written permission. For written permission, please\\ncontact trademarks@ncsu.edu.\\nDisclaimer: THIS SOFTWARE IS PROVIDED \\xe2\\x80\\x9cAS IS\\xe2\\x80\\x9d AND ANY EXPRESSED OR IMPLIED WARRANTIES,\\nINCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NORTH CAROLINA STATE UNIVERSITY BE\\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\\nOTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\nPOSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "AOGNet-v2",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "AOGNet-v2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "iVMCL",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/iVMCL/AOGNet-v2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 37,
      "date": "Thu, 23 Dec 2021 06:03:14 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We performed object detection and instance segmentation task on COCO with our models pretrained on ImageNet. We implement it based on the [mmdetection](https://github.com/open-mmlab/mmdetection) framework. The code is released in [https://github.com/iVMCL/AttentiveNorm_Detection/tree/master/configs/attn_norm](https://github.com/iVMCL/AttentiveNorm_Detection/tree/master/configs/attn_norm).\n\n",
      "technique": "Header extraction"
    }
  ]
}