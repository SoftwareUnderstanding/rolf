{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1603.05027",
      "https://arxiv.org/abs/1603.05027"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wenxinxu/resnet-in-tensorflow",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-10-09T21:05:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T05:47:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9664033038502533,
        0.863644484579493
      ],
      "excerpt": "This implementation of resnet and its variants  is designed to be straightforward and friendly to new ResNet users. You can train a resnet on cifar10 by downloading and running the code. There are screen outputs, tensorboard statistics and tensorboard graph visualization to help you monitor the training process and visualize the model. \nNow the code works with tensorflow 1.0.0 and 1.1.0, but it's no longer compatible with earlier versions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8978070660765585
      ],
      "excerpt": "cifar10_train.py is responsible for the training and validation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9190422085660204
      ],
      "excerpt": "The following sections expain the codes in details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8277383892680628
      ],
      "excerpt": "There are five categories of hyper-parameters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8360685800026658
      ],
      "excerpt": "train_ema_decay: float. The tensorboard will record a moving average of batch train errors, besides the original ones. This decay factor is used to define an ExponentialMovingAverage object in tensorflow with tf.train.ExponentialMovingAverage(FLAGS.train_ema_decay, global_step). Essentially, the recorded error = train_ema_decay * shadowed_error + (1 - train_ema_decay) * current_batch_error. The larger the train_ema_decay is, the smoother the training curve will be. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042078101206009
      ],
      "excerpt": "lr_decay_factor: float. The decaying factor of learning rate. The learning rate will become lr_decay_factor * current_learning_rate every time it is decayed.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9834806004499459,
        0.9613825926208118
      ],
      "excerpt": "weight_decay: float. The weight decay used to regularize the network. Total_loss = train_loss + weight_decay* sume of sqaures of the weights \npadding_size: int. padding_size is numbers of zero pads to add on each side of the image. Padding and random cropping during training can prevent overfitting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8668593311343732
      ],
      "excerpt": "Here we use the latest version of ResNet. The structure of the residual block looks like ref: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8404984574358593,
        0.9875126737283245,
        0.882501682408181
      ],
      "excerpt": "The inference() function is the main function of resnet.py. It will be used twice in both building the training graph and validation graph.  \n<!--The inference() function is the main function of resnet.py. It takes three arguments: input_tensor_batch, n and resue. input_tensor_batch is a 4D tensor with shape of [batch_size, img_height, img_width, img_depth]. n is the num_residual_blocks. Reuse is a boolean, indicating the graph is build for train or validation data. \nTo enable the different sizes of validation batch to train batch, I use two different sets of placeholders for train and validation data, and build the graphs separately, and the validation graph shares the same weights with the train graph. In this situation, we are passing reuse=True to each variable scope of train graph to fetch the weights. To read more about variable scope, see [variable scope](https://www.tensorflow.org/versions/master/how_tos/variable_scope/index.html) --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8564374445834044
      ],
      "excerpt": "<!--(We do want to validate before training, so that we can check the original errors and losses with the theoretical value.)--> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9591856633161792
      ],
      "excerpt": "Placeholders can be viewed as tensors that must be fed with real data on every execution. If you want to change the \"values\" of certain tensors on each step of training, placeholders are the most straightforward way. For example, we train the model with different batches of data on each step by feeding different batches of numpy array into the image_placeholder and label_placeholder. A feed dict looks like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8406599777402759
      ],
      "excerpt": "**a) Summarize the tensors of interest** \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009512079859675,
        0.897081230534774
      ],
      "excerpt": "**b) Merge all summaries** \nAfter you set up all the scalar summaries, type `summary_op = tf.merge_all_summaries()`. This command merge all the summarizing operations into a single operation, which means that running summary_op is equivalent to running all the scalar summaries together. --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008269076462375
      ],
      "excerpt": ": predictions is the predicted softmax array. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Re-implement Kaiming He's deep residual networks in tensorflow. Can be trained with cifar10.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wenxinxu/resnet_in_tensorflow/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The lowest valdiation errors of ResNet-32, ResNet-56 and ResNet-110 are 6.7%, 6.5% and 6.2% respectively. You can change the number of the total layers by changing the hyper-parameter num_residual_blocks. Total layers = 6 * num_residual_blocks + 2\n\nNetwork | Lowest Validation Error\n------- | -----------------------\nResNet-32 | 6.7%\nResNet-56 | 6.5%\nResNet-110 | 6.2%\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 282,
      "date": "Tue, 28 Dec 2021 11:13:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wenxinxu/resnet-in-tensorflow/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "wenxinxu/resnet-in-tensorflow",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can run cifar10_train.py and see how it works from the screen output (the code will download the data for you if you don't have it yet). It\u2019s better to speicify version identifier before running, since the training logs, checkpoints, and error.csv file will be saved in the folder with name logs_$version. You can do this by command line: `python cifar10_train.py --version='test'`. You may also change the version number inside the hyper_parameters.py file\n\nThe training and validation error will be output on the screen. They can also be viewed using tensorboard. Use `tensorboard --logdir='logs_$version'` command to pull them out. (For e.g. If the version is \u2018test\u2019, the logdir should be \u2018logs_test\u2019.) \nThe relevant statistics of each layer can be found on tensorboard.  \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8469876268623551
      ],
      "excerpt": "Run the following commands in the command line: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8074866589141254
      ],
      "excerpt": "Training curves \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807940611943144,
        0.8072005108134348
      ],
      "excerpt": "There are four python files in the repository. cifar10_input.py, resnet.py, cifar10_train.py, hyper_parameters.py. \ncifar10_input.py includes helper functions to download, extract and pre-process the cifar10 images.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.839098730884667,
        0.8158712007857872
      ],
      "excerpt": "cifar10_train.py is responsible for the training and validation. \nhyper_parameters.py defines hyper-parameters related to train, resnet structure, data augmentation, etc.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8942233818742708,
        0.8221595349605718
      ],
      "excerpt": "report_freq: int. How many batches to run a full validation and print screen output once. Screen output looks like: \ntrain_ema_decay: float. The tensorboard will record a moving average of batch train errors, besides the original ones. This decay factor is used to define an ExponentialMovingAverage object in tensorflow with tf.train.ExponentialMovingAverage(FLAGS.train_ema_decay, global_step). Essentially, the recorded error = train_ema_decay * shadowed_error + (1 - train_ema_decay) * current_batch_error. The larger the train_ema_decay is, the smoother the training curve will be. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8551175957884292,
        0.8448669831330348
      ],
      "excerpt": "is_full_validation: boolean. If you want to use all the 10000 validation images to run the validation (True), or you want to randomly draw a batch of validation data (False) \ntrain_batch_size: int. Training batch size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8001652498644691
      ],
      "excerpt": "num_residual_blocks: int. The total layers of the ResNet = 6 * num_residual_blocks + 2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8196474668512671
      ],
      "excerpt": "The class Train() defines all the functions regarding training process, with train() being the main function. The basic idea is to run train_op for FLAGS.train_steps times. If step % FLAGS.report_freq == 0, it will valdiate once, train once and wrote all the summaries onto the tensorboard.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136509394491014
      ],
      "excerpt": "The test() function in the class Train() help you predict. It returns the softmax probability with shape [num_test_images, num_labels]. You need to prepare and pre-process your test data and pass it to the function. You may either use your own checkpoints or the pre-trained ResNet-110 checkpoint I uploaded. You may wrote the following lines at the end of cifar10_train.py file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "train = Train() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8887338038698807
      ],
      "excerpt": "predictions = train.test(test_image_array) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8628153825751562
      ],
      "excerpt": "python cifar10_train.py --test_ckpt_path='model_110.ckpt-79999' \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wenxinxu/resnet-in-tensorflow/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Alexia Xu\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ResNet in Tensorflow",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "resnet-in-tensorflow",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "wenxinxu",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wenxinxu/resnet-in-tensorflow/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "pandas, numpy , opencv, tensorflow(1.0.0)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 828,
      "date": "Tue, 28 Dec 2021 11:13:58 GMT"
    },
    "technique": "GitHub API"
  }
}