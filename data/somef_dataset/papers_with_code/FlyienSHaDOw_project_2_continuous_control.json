{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.02971\n\n[2] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M.A. (2014"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Lillicrap, T. Hunt, J. Pritzel, A. Heess, N. Erez, T. Tassa, Y. Silver, D. & Wierstra, D. (2016). Continuous Control with Reinforcement Learning, In Proceedings of ICLR. https://arxiv.org/abs/1509.02971\n\n[2] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M.A. (2014). Deterministic Policy Gradient Algorithms. ICML. https://dl.acm.org/doi/10.5555/3044805.3044850\n\n[3] Watkins, C.J., Dayan, P. Technical Note: Q-Learning. Machine Learning 8, 279\u2013292\n(1992). https://doi.org/10.1023/A:102267672231\n\n[4] Sutton R, Barto A, Reinforcement Learning: An Introduction, The MIT Press, 2018.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8528760958819365
      ],
      "excerpt": "<center>Figure 1 DDPG learning process\u2014actor</center> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8528760958819365
      ],
      "excerpt": "<center>Figure 2 DDPG learning process\u2014critic</center> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/FlyienSHaDOw/continuous_control",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-14T14:41:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-01T11:54:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9702659890603152,
        0.953267467709156,
        0.955567331212743,
        0.9960386749031088
      ],
      "excerpt": "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible. \nThe observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between -1 and 1. \nIt is required that we can achieve an average score of +30. \nThe deep deterministic policy gradient (DDPG) method [1] is a model free reinforcement learning algorithm, and it is an extension of the deterministic policy gradient (DPG) method [2]. The difference between the two method is that, DPG considers the deterministic policies which considers that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.859711867402063,
        0.9914881440780738,
        0.8428019519915685
      ],
      "excerpt": "where a is the action, <img src=\"https://latex.codecogs.com/gif.latex?\\mu\"/> is the policy, <img src=\"https://latex.codecogs.com/gif.latex?\\theta\"/> is the parameters and s is the state. \nThe DDPG method adopts the actor-critic approach with Deep Q Network[3] to form a model-free, off-policy reinforcement learning algorithm for the learning of optimal policies in high-dimensional and continuous action spaces problems, such as autonomous driving and robotics, etc. For the example problems, their actuators receives continuous command, such as throttle and joint torques. The DQN method can only handle discrete action space, for that reason, its application is limited. \nThe DDPG uses stochastic policy for the agent, i.e. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9193088520419724,
        0.9519040568875727
      ],
      "excerpt": "For this problem, the stochastic actor-critic method is applied. the actor is applied to find the optimal <img src=\"https://latex.codecogs.com/gif.latex?\\theta^*\"/> in order to approach the optimal policy <img src=\"https://latex.codecogs.com/gif.latex?\\pi^*\"/>, that's to say, <img src=\"https://latex.codecogs.com/gif.latex?\\pi_{\\theta}(a|s)\\rightarrow\\pi^*_{\\theta}(a|s)\"/>. For policy gradient method, the state-value function has to be estimated as well. In this approach, the critic is applied to adjust the parameter vector to approximate the sate-value function <img src=\"https://latex.codecogs.com/gif.latex?Q^{\\pi}(s,a)\"/>. Then, an approach similar to DQN method is applied for both actor-critic networks. \nA thematic diagram for this approach is shown in Fig 1 and Fig 2.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9782618269664244,
        0.9002557308047141,
        0.9799360704707007
      ],
      "excerpt": "The update of critic network is even more complex. First of all, since we prefer to get the Expected action-value function, we use the state of the next time step <img src=\"https://latex.codecogs.com/gif.latex?s_{t+1}\"/>. An next time step action is guessed via the Target network of the actor. And The expected value function is generated via Target network of the critic, and the action value function is generated via Local network of critic. Then, the Bellman equation is calculated with the value function, and the mean-square-error loss function is applied for the update of Local network of the critic. \nBear in mind that, for both actor and critic network, the Target network are slowly converged to the Local network through soft update. \nThe ReplayBuffer class is a container which stores the past experiences. In the learn procedure, the past experiences are stochastically chosen and are fed into the two Q-networks. One Q-network is fixed as Q-target, it is denoted by<img src=\"https://latex.codecogs.com/gif.latex?\\theta^-\"/>. This Q-network is 'detached' in the training process, in order to achieve better stability. As a consequence, the change in weights can be expressed as  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.992771889639684
      ],
      "excerpt": "DDPG is an off-policy algorithm, as a matter of fact, the exploration procedure can be conducted independently. This procedure is kind of policy gradient method. An stochastic actor is determined by the current policy, and noise generated by the Uhlenbeck & Ornstein method is added to it for searching the gradient direction, until it approaches the optimal policy. Thus the actor policy can be expressed as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9596475727825495,
        0.9809260607054462
      ],
      "excerpt": "In this project, the Q-net is constructed by three fully connected layers. The architecture is the same as the network described in the paper [1]. But the units are reduced to reduce the computational time, since the problem is simpler.  In this case, the hidden layers are with 128 and 256 units respectively. For the input layer, the number of input node is the same as the number of states of the agent.  Finally, for the output layer, the number of output layer is the same as the action size of the agent. For the input layer and the out put layer, the output value is activated by the Rectified Linear Unit (ReLU) function. Since this is a continuous control problem, we have to use tanh function for the output of final layer. The network for the critic has the same structure as the actor network. however, the critic approximates the action-value function, its input should be states and action, consequently, the number of node for the input layer is the number of states plus the number of actions. \nThe hyper parameters for the learning process are generally utilized the parameters provided by  the paper [1]. However, some modifications are conducted for both convergence and stability. The WEIGHT_DECAY is set as 0. And I conduct one training process in very 25 time steps. In my  hyper-parameters tuning experience, TRAIN_EVERY influence the convergence significantly. At one training step, I set NUM_TRAINS as 5 to conduct 5 trains at a time. Other difference  is that I increase the minibatch size to 128 to allow more past experiences to be used for one training. Another improvement is that, I reduce the exploration noise a decay rate (say 0.999) to achieve better stability. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8990166231237909,
        0.8932746024607822,
        0.8932746024607822
      ],
      "excerpt": "TAU = 1e-3              #: for soft update of target parameters \nLR_ACTOR = 1e-4         #: learning rate of the actor  \nLR_CRITIC = 1e-3        #: learning rate of the critic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8574686076054017
      ],
      "excerpt": "TRAIN_EVERY = 25        #: how often to update the network \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9888565025791903
      ],
      "excerpt": "At one time step of an episode, the process is generally depicted in Figure 3.  The agent choose an action corresponding to  the current state via the Local network of the actor. And the action is applied to the environment, generates the reward of the action and the state, and the transmission of the next state. Than, they are stored in the Experience Replay Buffer for the training process. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9575679831794084
      ],
      "excerpt": "The animation shown in Figure 4 demonstrates the effectiveness of the trained network, and the Figure 5 shows the learning procedure. With the prescribed structure and hyper parameters, the networks converges to the 'optimal policy' nicely with little oscillations. And the agent reaches the target average score 30 in 150 episodes, which means the network structure and the hyper parameters defined find a good balance point between exploration and exploitation. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/FlyienSHaDOw/project_2_continuous_control/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 30 Dec 2021 04:58:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/FlyienSHaDOw/continuous_control/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "FlyienSHaDOw/continuous_control",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/FlyienSHaDOw/project_2_continuous_control/master/project_2_solution.ipynb",
      "https://raw.githubusercontent.com/FlyienSHaDOw/project_2_continuous_control/master/.ipynb_checkpoints/project_2_solution-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8300928104049362,
        0.834296455458036
      ],
      "excerpt": "BUFFER_SIZE = int(1e6)  #: replay buffer size \nBATCH_SIZE = 128        #: minibatch size \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/FlyienSHaDOw/continuous_control/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Continuous Control",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "continuous_control",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "FlyienSHaDOw",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/FlyienSHaDOw/continuous_control/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Thu, 30 Dec 2021 04:58:05 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/).\n\n\n```python\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom unityagents import UnityEnvironment\nimport numpy as np\nimport random\nimport copy\nfrom collections import namedtuple, deque\nimport os\nimport time\nimport sys\n\nfrom time import sleep\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cpu\")\n```\n\nNext, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n\n- **Mac**: `\"path/to/Reacher.app\"`\n- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n\nFor instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n```\nenv = UnityEnvironment(file_name=\"Reacher.app\")\n```\n\n\n```python\nenv = UnityEnvironment(file_name='./Reacher_Linux/Reacher.x86_64',\n                       no_graphics = False)\n#: get the default brain\nbrain_name = env.brain_names[0]\nbrain = env.brains[brain_name]\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}