{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.05365",
      "https://arxiv.org/abs/1412.2007"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If our ELMo gave you nice improvements, please cite us.\n\n```\n@InProceedings{che-EtAl:2018:K18-2,\n  author    = {Che, Wanxiang  and  Liu, Yijia  and  Wang, Yuxuan  and  Zheng, Bo  and  Liu, Ting},\n  title     = {Towards Better {UD} Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation},\n  booktitle = {Proceedings of the {CoNLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},\n  month     = {October},\n  year      = {2018},\n  address   = {Brussels, Belgium},\n  publisher = {Association for Computational Linguistics},\n  pages     = {55--64},\n  url       = {http://www.aclweb.org/anthology/K18-2005}\n}\n```\n\nPlease also cite the \n[NLPL Vectors Repository](http://wiki.nlpl.eu/index.php/Vectors/home)\nfor hosting the models.\n```\n@InProceedings{fares-EtAl:2017:NoDaLiDa,\n  author    = {Fares, Murhaf  and  Kutuzov, Andrey  and  Oepen, Stephan  and  Velldal, Erik},\n  title     = {Word vectors, reuse, and replicability: Towards a community repository of large-text resources},\n  booktitle = {Proceedings of the 21st Nordic Conference on Computational Linguistics},\n  month     = {May},\n  year      = {2017},\n  address   = {Gothenburg, Sweden},\n  publisher = {Association for Computational Linguistics},\n  pages     = {271--276},\n  url       = {http://www.aclweb.org/anthology/W17-0237}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{fares-EtAl:2017:NoDaLiDa,\n  author    = {Fares, Murhaf  and  Kutuzov, Andrey  and  Oepen, Stephan  and  Velldal, Erik},\n  title     = {Word vectors, reuse, and replicability: Towards a community repository of large-text resources},\n  booktitle = {Proceedings of the 21st Nordic Conference on Computational Linguistics},\n  month     = {May},\n  year      = {2017},\n  address   = {Gothenburg, Sweden},\n  publisher = {Association for Computational Linguistics},\n  pages     = {271--276},\n  url       = {http://www.aclweb.org/anthology/W17-0237}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{che-EtAl:2018:K18-2,\n  author    = {Che, Wanxiang  and  Liu, Yijia  and  Wang, Yuxuan  and  Zheng, Bo  and  Liu, Ting},\n  title     = {Towards Better {UD} Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation},\n  booktitle = {Proceedings of the {CoNLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},\n  month     = {October},\n  year      = {2018},\n  address   = {Brussels, Belgium},\n  publisher = {Association for Computational Linguistics},\n  pages     = {55--64},\n  url       = {http://www.aclweb.org/anthology/K18-2005}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --max_epoch 10 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HIT-SCIR/ELMoForManyLangs",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-10T00:31:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T08:33:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9895120418463983
      ],
      "excerpt": "and the character CNN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9511398088340457,
        0.857752394737983,
        0.9618402787360741
      ],
      "excerpt": "on a set of 20-million-words data randomly \nsampled from the raw text released by the shared task (wikidump + common crawl) for each language. \nWe largely based ourselves on the code of AllenNLP, but made the following changes: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9193189229096393,
        0.8175677868789509
      ],
      "excerpt": "as negative samples and it shows better performance in our preliminary experiments. \nThe training of ELMo on one language takes roughly 3 days on an NVIDIA P100 GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8712946473238022
      ],
      "excerpt": "    -  -1 for an average of 3 layers. (default) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616159130335191
      ],
      "excerpt": "to get more details about the ELMo training.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8936527836843264,
        0.9577356680959053
      ],
      "excerpt": "need to add that the training process is not very stable. \nIn some cases, we end up with a loss ofnan`. We are actively working on that and hopefully \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pre-trained ELMo Representations for Many Languages",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "|   |   |   |   |\n|---|---|---|---|\n| [Arabic](http://vectors.nlpl.eu/repository/11/136.zip) | [Bulgarian](http://vectors.nlpl.eu/repository/11/137.zip) | [Catalan](http://vectors.nlpl.eu/repository/11/138.zip) | [Czech](http://vectors.nlpl.eu/repository/11/139.zip) |\n| [Old Church Slavonic](http://vectors.nlpl.eu/repository/11/140.zip) | [Danish](http://vectors.nlpl.eu/repository/11/141.zip) | [German](http://vectors.nlpl.eu/repository/11/142.zip) | [Greek](http://vectors.nlpl.eu/repository/11/143.zip) |\n| [English](http://vectors.nlpl.eu/repository/11/144.zip) | [Spanish](http://vectors.nlpl.eu/repository/11/145.zip) | [Estonian](http://vectors.nlpl.eu/repository/11/146.zip) | [Basque](http://vectors.nlpl.eu/repository/11/147.zip) |\n| [Persian](http://vectors.nlpl.eu/repository/11/148.zip) | [Finnish](http://vectors.nlpl.eu/repository/11/149.zip) | [French](http://vectors.nlpl.eu/repository/11/150.zip) | [Irish](http://vectors.nlpl.eu/repository/11/151.zip) |\n| [Galician](http://vectors.nlpl.eu/repository/11/152.zip) | [Ancient Greek](http://vectors.nlpl.eu/repository/11/153.zip) | [Hebrew](http://vectors.nlpl.eu/repository/11/154.zip) | [Hindi](http://vectors.nlpl.eu/repository/11/155.zip) |\n| [Croatian](http://vectors.nlpl.eu/repository/11/156.zip) | [Hungarian](http://vectors.nlpl.eu/repository/11/157.zip) | [Indonesian](http://vectors.nlpl.eu/repository/11/158.zip) | [Italian](http://vectors.nlpl.eu/repository/11/159.zip) |\n| [Japanese](http://vectors.nlpl.eu/repository/11/160.zip) | [Korean](http://vectors.nlpl.eu/repository/11/161.zip) | [Latin](http://vectors.nlpl.eu/repository/11/162.zip) | [Latvian](http://vectors.nlpl.eu/repository/11/163.zip) |\n| [Norwegian Bokm\u00e5l](http://vectors.nlpl.eu/repository/11/165.zip) | [Dutch](http://vectors.nlpl.eu/repository/11/164.zip) | [Norwegian Nynorsk](http://vectors.nlpl.eu/repository/11/166.zip) | [Polish](http://vectors.nlpl.eu/repository/11/167.zip) |\n| [Portuguese](http://vectors.nlpl.eu/repository/11/168.zip) | [Romanian](http://vectors.nlpl.eu/repository/11/169.zip) | [Russian](http://vectors.nlpl.eu/repository/11/170.zip) | [Slovak](http://vectors.nlpl.eu/repository/11/171.zip) |\n| [Slovene](http://vectors.nlpl.eu/repository/11/172.zip) | [Swedish](http://vectors.nlpl.eu/repository/11/173.zip) | [Turkish](http://vectors.nlpl.eu/repository/11/174.zip) | [Uyghur](http://vectors.nlpl.eu/repository/11/175.zip) |\n| [Ukrainian](http://vectors.nlpl.eu/repository/11/176.zip) | [Urdu](http://vectors.nlpl.eu/repository/11/177.zip) | [Vietnamese](http://vectors.nlpl.eu/repository/11/178.zip) | [Chinese](http://vectors.nlpl.eu/repository/11/179.zip) |\n\nThe models are hosted on the [NLPL Vectors Repository](http://wiki.nlpl.eu/index.php/Vectors/home).\n\n**ELMo for Simplified Chinese**\n\nWe also provided [simplified-Chinese ELMo](http://39.96.43.154/zhs.model.tar.bz2).\nIt was trained on xinhua proportion of [Chinese gigawords-v5](https://catalog.ldc.upenn.edu/ldc2011t13),\nwhich is different from the Wikipedia for traditional Chinese ELMo.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HIT-SCIR/ELMoForManyLangs/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 244,
      "date": "Thu, 23 Dec 2021 10:08:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HIT-SCIR/ELMoForManyLangs/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "HIT-SCIR/ELMoForManyLangs",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "After unzip the model, you will find a JSON file `${lang}.model/config.json`.\nPlease change the `\"config_path\"` field to the relative path to \nthe model configuration `cnn_50_100_512_4096_sample.json`.\nFor example, if your ELMo model is `zht.model/config.json` and your model configuration\nis `zht.model/cnn_50_100_512_4096_sample.json`, you need to change `\"config_path\"`\nin `zht.model/config.json` to `cnn_50_100_512_4096_sample.json`.\n\nIf there is no configuration `cnn_50_100_512_4096_sample.json` under `${lang}.model`,\nyou can copy the `elmoformanylangs/configs/cnn_50_100_512_4096_sample.json` into `${lang}.model`,\nor change the `\"config_path\"` into  `elmoformanylangs/configs/cnn_50_100_512_4096_sample.json`.\n\nSee [issue 27](https://github.com/HIT-SCIR/ELMoForManyLangs/issues/27) for more details. \n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You need to install the package to use the embeddings with the following commends\n```\npython setup.py install\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9411306984221938,
        0.9322609392449874,
        0.867018643427208
      ],
      "excerpt": "must python >= 3.6 (if you use python3.5, you will encounter this issue https://github.com/HIT-SCIR/ELMoForManyLangs/issues/8) \npytorch 0.4 \nother requirements from allennlp \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921748492701493
      ],
      "excerpt": "$ python -m elmoformanylangs.biLM train -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921748492701493
      ],
      "excerpt": "$ python -m elmoformanylangs.biLM train \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8725153287135207
      ],
      "excerpt": "$ python -m elmoformanylangs.biLM train -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594670415830261
      ],
      "excerpt": "Here is an example for training English ELMo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8725153287135207
      ],
      "excerpt": "$ python -m elmoformanylangs.biLM train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857129346874625
      ],
      "excerpt": "    --model output/en \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HIT-SCIR/ELMoForManyLangs/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 \\xe5\\x93\\x88\\xe5\\xb7\\xa5\\xe5\\xa4\\xa7\\xe7\\xa4\\xbe\\xe4\\xbc\\x9a\\xe8\\xae\\xa1\\xe7\\xae\\x97\\xe4\\xb8\\x8e\\xe4\\xbf\\xa1\\xe6\\x81\\xaf\\xe6\\xa3\\x80\\xe7\\xb4\\xa2\\xe7\\xa0\\x94\\xe7\\xa9\\xb6\\xe4\\xb8\\xad\\xe5\\xbf\\x83\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pre-trained ELMo Representations for Many Languages",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ELMoForManyLangs",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "HIT-SCIR",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HIT-SCIR/ELMoForManyLangs/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1373,
      "date": "Thu, 23 Dec 2021 10:08:48 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "elmo",
      "multilingual"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Prepare your input file in the [conllu format](http://universaldependencies.org/format.html), like\n```\n1   Sue    Sue    _   _   _   _   _   _   _\n2   likes  like   _   _   _   _   _   _   _\n3   coffee coffee _   _   _   _   _   _   _\n4   and    and    _   _   _   _   _   _   _\n5   Bill   Bill   _   _   _   _   _   _   _\n6   tea    tea    _   _   _   _   _   _   _\n```\nFileds should be separated by `'\\t'`. We only use the second column and space (`' '`) is supported in\nthis field (for Vietnamese, a word can contains spaces).\nDo remember tokenization!\n\nWhen it's all set, run\n\n```\n$ python -m elmoformanylangs test \\\n    --input_format conll \\\n    --input /path/to/your/input \\\n    --model /path/to/your/model \\\n    --output_prefix /path/to/your/output \\\n    --output_format hdf5 \\\n    --output_layer -1\n```\n\nIt will dump an hdf5 encoded `dict` onto the disk, where the key is `'\\t'` separated\nwords in the sentence and the value is it's 3-layer averaged ELMo representation.\nYou can also dump the cnn encoded word with `--output_layer 0`,\nthe first layer of the LsTM with `--output_layer 1` and the second layer\nof the LSTM with `--output_layer 2`.  \nWe are actively changing the interface to make it more adapted to the \nAllenNLP ELMo and more programmatically friendly.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Thanks @voidism for contributing the API.\nBy using `Embedder` python object, you can use ELMo into your own code like this:\n\n```python\nfrom elmoformanylangs import Embedder\n\ne = Embedder('/path/to/your/model/')\n\nsents = [['\u4eca', '\u5929', '\u5929\u6c23', '\u771f', '\u597d', '\u963f'],\n['\u6f6e\u6c34', '\u9000', '\u4e86', '\u5c31', '\u77e5\u9053', '\u8ab0', '\u6c92', '\u7a7f', '\u8932\u5b50']]\n#: the list of lists which store the sentences \n#: after segment if necessary.\n\ne.sents2elmo(sents)\n#: will return a list of numpy arrays \n#: each with the shape=(seq_len, embedding_size)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}