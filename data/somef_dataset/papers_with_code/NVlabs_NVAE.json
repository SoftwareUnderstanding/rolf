{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2007.03898",
      "https://arxiv.org/abs/2007.03898"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{vahdat2020NVAE,\n  title={{NVAE}: A Deep Hierarchical Variational Autoencoder},\n  author={Vahdat, Arash and Kautz, Jan},\n  booktitle={Neural Information Processing Systems (NeurIPS)},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8854398367006624
      ],
      "excerpt": "  <a href=\"http://jankautz.com/\" target=\"_blank\">Jan&nbsp;Kautz</a>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.849892928323584
      ],
      "excerpt": "on this issue https://github.com/NVlabs/NVAE/issues/2 . \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NVlabs/NVAE",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-18T18:59:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T00:28:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.852390653417266,
        0.853236328536213
      ],
      "excerpt": "NVAE is a deep hierarchical variational autoencoder that enables training SOTA  \nlikelihood-based generative models on several image datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8469584472823105
      ],
      "excerpt": "Several users have reported issues building CelebA 64 or have encountered NaN at the beginning of training on this dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336458588122001
      ],
      "excerpt": "One of the main challenges in training very deep hierarchical VAEs is training instability that we discussed in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9125190309781304,
        0.9909546849178461,
        0.8197730411707467
      ],
      "excerpt": "In some very rare cases, we observed that training freezes after 2-3 days of training. We believe the root cause \nof this is because of a racing condition that is happening in one of the low-level libraries. If for any reason the training  \nis stopped, kill your current run, and use the exact same commend with the addition of `--cont_training` \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9606452524058223
      ],
      "excerpt": "Set `--data` to the same argument that was used when training NVAE (our example is for MNIST). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955908630415793
      ],
      "excerpt": "where `--temp` sets the temperature used for sampling and `--readjust_bn` enables readjustment of the BN statistics \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955908630415793
      ],
      "excerpt": "where `--temp` sets the temperature used for sampling and `--readjust_bn` enables readjustment of the BN statistics \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9606452524058223
      ],
      "excerpt": "Set `--data` to the same argument that was used when training NVAE (our example is for MNIST). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295761013810665,
        0.9781361770829484,
        0.9910602012307486
      ],
      "excerpt": "For CIFAR10, we provide two checkpoints as we observed that a multiscale NVAE provides better qualitative \nresults than a single scale model on this dataset. The multiscale model is only slightly worse in terms \nof log-likelihood (0.01 bpd). We also observe that one of our early models on CelebA HQ 256 with 0.01 bpd  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8072598974663103
      ],
      "excerpt": "In the commands above, we are constructing big NVAE models that require several days of training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9366380038926818,
        0.966385986086806
      ],
      "excerpt": "of initial channels in the bottom-up and top-down networks respectively. Recall that we halve the \nnumber of channels with every spatial downsampling layer in the bottom-up network, and we double the number of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8756315765917538
      ],
      "excerpt": "Reduce the number of residual cells in the hierarchy: --num_cell_per_cond_enc and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9759194278903628,
        0.8778929971099889
      ],
      "excerpt": "group in the bottom-up and top-down networks respectively. In most of our experiments, we are using \ntwo cells per group for both networks. You can reduce the number of residual cells to one to make the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.94563570580699,
        0.8113538219367958,
        0.8465940580187252,
        0.8647106824372033
      ],
      "excerpt": "An equal number of groups: This is set by --num_groups_per_scale which indicates the number of groups  \nin each scale of latent variables. Reduce this number to have a small NVAE. \nAn adaptive number of groups: This is enabled by --ada_groups. In this case, the highest \nresolution of latent variables will have --num_groups_per_scale groups and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8866249263251423
      ],
      "excerpt": "when --ada_groups is enabled. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9927220476048457,
        0.8182265375795389
      ],
      "excerpt": "We can generate images by traversing in the latent space of NVAE. This sequence is generated using our model \ntrained on CelebA HQ, by interpolating between samples generated with temperature 0.6.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The Official PyTorch Implementation of \"NVAE: A Deep Hierarchical Variational Autoencoder\" (NeurIPS 2020 spotlight paper)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NVlabs/NVAE/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 104,
      "date": "Sun, 26 Dec 2021 12:12:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/NVlabs/NVAE/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "NVlabs/NVAE",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/NVlabs/NVAE/master/scripts/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We have examined NVAE on several datasets. For large datasets, we store the data in LMDB datasets\nfor I/O efficiency. Click below on each dataset to see how you can prepare your data. Below, `$DATA_DIR` indicates\nthe path to a data directory that will contain all the datasets and `$CODE_DIR` refers to the code directory:\n\n<details><summary>MNIST and CIFAR-10</summary>\n\nThese datasets will be downloaded automatically, when you run the main training for NVAE using `train.py`\nfor the first time. You can use `--data=$DATA_DIR/mnist` or `--data=$DATA_DIR/cifar10`, so that the datasets\nare downloaded to the corresponding directories.\n</details>\n\n<details><summary>CelebA 64</summary>\nRun the following commands to download the CelebA images and store them in an LMDB dataset:\n\n```shell script\ncd $CODE_DIR/scripts\npython create_celeba64_lmdb.py --split train --img_path $DATA_DIR/celeba_org --lmdb_path $DATA_DIR/celeba64_lmdb\npython create_celeba64_lmdb.py --split valid --img_path $DATA_DIR/celeba_org --lmdb_path $DATA_DIR/celeba64_lmdb\npython create_celeba64_lmdb.py --split test  --img_path $DATA_DIR/celeba_org --lmdb_path $DATA_DIR/celeba64_lmdb\n```\nAbove, the images will be downloaded to `$DATA_DIR/celeba_org` automatically and then then LMDB datasets are created\nat `$DATA_DIR/celeba64_lmdb`.\n</details>\n \n<details><summary>ImageNet 32x32</summary>\n\nRun the following commands to download tfrecord files from [GLOW](https://github.com/openai/glow) and to convert them\nto LMDB datasets\n```shell script\nmkdir -p $DATA_DIR/imagenet-oord\ncd $DATA_DIR/imagenet-oord\nwget https://storage.googleapis.com/glow-demo/data/imagenet-oord-tfr.tar\ntar -xvf imagenet-oord-tfr.tar\ncd $CODE_DIR/scripts\npython convert_tfrecord_to_lmdb.py --dataset=imagenet-oord_32 --tfr_path=$DATA_DIR/imagenet-oord/mnt/host/imagenet-oord-tfr --lmdb_path=$DATA_DIR/imagenet-oord/imagenet-oord-lmdb_32 --split=train\npython convert_tfrecord_to_lmdb.py --dataset=imagenet-oord_32 --tfr_path=$DATA_DIR/imagenet-oord/mnt/host/imagenet-oord-tfr --lmdb_path=$DATA_DIR/imagenet-oord/imagenet-oord-lmdb_32 --split=validation\n```\n</details>\n\n<details><summary>CelebA HQ 256</summary>\n\nRun the following commands to download tfrecord files from [GLOW](https://github.com/openai/glow) and to convert them\nto LMDB datasets\n```shell script\nmkdir -p $DATA_DIR/celeba\ncd $DATA_DIR/celeba\nwget https://storage.googleapis.com/glow-demo/data/celeba-tfr.tar\ntar -xvf celeba-tfr.tar\ncd $CODE_DIR/scripts\npython convert_tfrecord_to_lmdb.py --dataset=celeba --tfr_path=$DATA_DIR/celeba/celeba-tfr --lmdb_path=$DATA_DIR/celeba/celeba-lmdb --split=train\npython convert_tfrecord_to_lmdb.py --dataset=celeba --tfr_path=$DATA_DIR/celeba/celeba-tfr --lmdb_path=$DATA_DIR/celeba/celeba-lmdb --split=validation\n```\n</details>\n\n\n<details><summary>FFHQ 256</summary>\n\nVisit [this Google drive location](https://drive.google.com/drive/folders/1WocxvZ4GEZ1DI8dOz30aSj2zT6pkATYS) and download\n`images1024x1024.zip`. Run the following commands to unzip the images and to store them in LMDB datasets:\n```shell script\nmkdir -p $DATA_DIR/ffhq\nunzip images1024x1024.zip -d $DATA_DIR/ffhq/\ncd $CODE_DIR/scripts\npython create_ffhq_lmdb.py --ffhq_img_path=$DATA_DIR/ffhq/images1024x1024/ --ffhq_lmdb_path=$DATA_DIR/ffhq/ffhq-lmdb --split=train\npython create_ffhq_lmdb.py --ffhq_img_path=$DATA_DIR/ffhq/images1024x1024/ --ffhq_lmdb_path=$DATA_DIR/ffhq/ffhq-lmdb --split=validation\n```\n</details>\n\n<details><summary>LSUN</summary>\n\nWe use LSUN datasets in our follow-up works. Visit [LSUN](https://www.yf.io/p/lsun) for \ninstructions on how to download this dataset. Since the LSUN scene datasets come in the\nLMDB format, they are ready to be loaded using torchvision data loaders.\n\n</details>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9123499839978982,
        0.8487965356536347
      ],
      "excerpt": "If you face similar issues on this dataset, you can download this dataset manually and build LMDBs using instructions \non this issue https://github.com/NVlabs/NVAE/issues/2 . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216427668989986
      ],
      "excerpt": "We have verified that the settings in the commands above can be trained in a stable way. If you modify the settings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd $CODE_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd $CODE_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.864511158660792
      ],
      "excerpt": "You can compute the FID score using 50K samples. To do so, you will need to create \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd $CODE_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd $CODE_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.805939349282506
      ],
      "excerpt": "We don't let the number of groups go below --min_groups_per_scale. You can reduce \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8481942365069672
      ],
      "excerpt": "    <img src=\"img/celebahq.png\" width=\"800\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142788572031961
      ],
      "excerpt": "Above, `$CHECKPOINT_DIR` and `$EXPR_ID` are the same variables used for running the main training script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8749788831063199
      ],
      "excerpt": "python evaluate.py --checkpoint $CHECKPOINT_DIR/eval-$EXPR_ID/checkpoint.pt --data $DATA_DIR/mnist --eval_mode=evaluate --num_iw_samples=1000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8218514874273772
      ],
      "excerpt": "`$CHECKPOINT_DIR` and `$EXPR_ID` are the same variables used for running the main training script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.90562084696497
      ],
      "excerpt": "python evaluate.py --checkpoint $CHECKPOINT_DIR/eval-$EXPR_ID/checkpoint.pt --eval_mode=sample --temp=0.6 --readjust_bn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172465137665468
      ],
      "excerpt": "(i.e., BN layers will use running mean and variances extracted during training). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8250207272706537
      ],
      "excerpt": "a mean and covariance statistics file on the training data using a command like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.878513612797134
      ],
      "excerpt": "python scripts/precompute_fid_statistics.py --data $DATA_DIR/cifar10 --dataset cifar10 --fid_dir /tmp/fid-stats/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9172740742821691
      ],
      "excerpt": "python evaluate.py --checkpoint $CHECKPOINT_DIR/eval-$EXPR_ID/checkpoint.pt --data $DATA_DIR/cifar10 --eval_mode=evaluate_fid  --fid_dir /tmp/fid-stats/ --temp=0.6 --readjust_bn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172465137665468,
        0.8142788572031961
      ],
      "excerpt": "(i.e., BN layers will use running mean and variances extracted during training). \nAbove, `$CHECKPOINT_DIR` and `$EXPR_ID` are the same variables used for running the main training script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481942365069672
      ],
      "excerpt": "    <img src=\"img/model_diagram.png\" width=\"900\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/NVlabs/NVAE/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/NVlabs/NVAE/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'NVIDIA Source Code License for NVAE\\n\\n1. Definitions\\n\\n\\xe2\\x80\\x9cLicensor\\xe2\\x80\\x9d means any person or entity that distributes its Work.\\n\\n\\xe2\\x80\\x9cSoftware\\xe2\\x80\\x9d means the original work of authorship made available under this License.\\n\\n\\xe2\\x80\\x9cWork\\xe2\\x80\\x9d means the Software and any additions to or derivative works of the Software that are made available under\\nthis License.\\n\\nThe terms \\xe2\\x80\\x9creproduce,\\xe2\\x80\\x9d \\xe2\\x80\\x9creproduction,\\xe2\\x80\\x9d \\xe2\\x80\\x9cderivative works,\\xe2\\x80\\x9d and \\xe2\\x80\\x9cdistribution\\xe2\\x80\\x9d have the meaning as provided under\\nU.S. copyright law; provided, however, that for the purposes of this License, derivative works shall not include\\nworks that remain separable from, or merely link (or bind by name) to the interfaces of, the Work.\\n\\nWorks, including the Software, are \\xe2\\x80\\x9cmade available\\xe2\\x80\\x9d under this License by including in or with the Work either\\n(a) a copyright notice referencing the applicability of this License to the Work, or (b) a copy of this License.\\n\\n2. License Grant\\n\\n2.1 Copyright Grant. Subject to the terms and conditions of this License, each Licensor grants to you a perpetual,\\nworldwide, non-exclusive, royalty-free, copyright license to reproduce, prepare derivative works of, publicly\\ndisplay, publicly perform, sublicense and distribute its Work and any resulting derivative works in any form.\\n\\n3. Limitations\\n\\n3.1 Redistribution. You may reproduce or distribute the Work only if (a) you do so under this License, (b) you\\ninclude a complete copy of this License with your distribution, and (c) you retain without modification any\\ncopyright, patent, trademark, or attribution notices that are present in the Work.\\n\\n3.2 Derivative Works. You may specify that additional or different terms apply to the use, reproduction, and\\ndistribution of your derivative works of the Work (\\xe2\\x80\\x9cYour Terms\\xe2\\x80\\x9d) only if (a) Your Terms provide that the use\\nlimitation in Section 3.3 applies to your derivative works, and (b) you identify the specific derivative works\\nthat are subject to Your Terms. Notwithstanding Your Terms, this License (including the redistribution\\nrequirements in Section 3.1) will continue to apply to the Work itself.\\n\\n3.3 Use Limitation. The Work and any derivative works thereof only may be used or intended for use\\nnon-commercially. Notwithstanding the foregoing, NVIDIA and its affiliates may use the Work and any derivative\\nworks commercially. As used herein, \\xe2\\x80\\x9cnon-commercially\\xe2\\x80\\x9d means for research or evaluation purposes only.\\n\\n3.4 Patent Claims. If you bring or threaten to bring a patent claim against any Licensor (including any claim,\\ncross-claim or counterclaim in a lawsuit) to enforce any patents that you allege are infringed by any Work, then\\nyour rights under this License from such Licensor (including the grant in Section 2.1) will terminate immediately.\\n\\n3.5 Trademarks. This License does not grant any rights to use any Licensor\\xe2\\x80\\x99s or its affiliates\\xe2\\x80\\x99 names, logos,\\nor trademarks, except as necessary to reproduce the notices described in this License.\\n\\n3.6 Termination. If you violate any term of this License, then your rights under this License (including the\\ngrant in Section 2.1) will terminate immediately.\\n\\n4. Disclaimer of Warranty.\\n\\nTHE WORK IS PROVIDED \\xe2\\x80\\x9cAS IS\\xe2\\x80\\x9d WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING\\nWARRANTIES OR CONDITIONS OF M ERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT. YOU\\nBEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER THIS LICENSE.\\n\\n5. Limitation of Liability.\\n\\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER IN TORT (INCLUDING\\nNEGLIGENCE), CONTRACT, OR OTHERWISE SHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT,\\nINDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF OR RELATED TO THIS LICENSE, THE USE OR\\nINABILITY TO USE THE WORK (INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION, LOST PROFITS OR\\nDATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER COMM ERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN\\nADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "The Official PyTorch Implementation of \"NVAE: A Deep Hierarchical Variational Autoencoder\" [(NeurIPS 2020 Spotlight Paper)](https://arxiv.org/abs/2007.03898)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "NVAE",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "NVlabs",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NVlabs/NVAE/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "NVAE is built in Python 3.7 using PyTorch 1.6.0. Use the following command to install the requirements:\n```\npip install -r requirements.txt\n``` \n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We use the following commands on each dataset for training NVAEs on each dataset for \nTable 1 in the [paper](https://arxiv.org/pdf/2007.03898.pdf). In all the datasets but MNIST\nnormalizing flows are enabled. Check Table 6 in the paper for more information on training\ndetails. Note that for the multinode training (more than 8-GPU experiments), we use the `mpirun` \ncommand to run the training scripts on multiple nodes. Please adjust the commands below according to your setup. \nBelow `IP_ADDR` is the IP address of the machine that will host the process with rank 0 \n(see [here](https://pytorch.org/tutorials/intermediate/dist_tuto.html#initialization-methods)). \n`NODE_RANK` is the index of each\u00a0node among all the nodes that are running the job.\n\n<details><summary>MNIST</summary>\n\nTwo 16-GB V100 GPUs are used for training NVAE on dynamically binarized MNIST. Training takes about 21 hours.\n\n```shell script\nexport EXPR_ID=UNIQUE_EXPR_ID\nexport DATA_DIR=PATH_TO_DATA_DIR\nexport CHECKPOINT_DIR=PATH_TO_CHECKPOINT_DIR\nexport CODE_DIR=PATH_TO_CODE_DIR\ncd $CODE_DIR\npython train.py --data $DATA_DIR/mnist --root $CHECKPOINT_DIR --save $EXPR_ID --dataset mnist --batch_size 200 \\\n        --epochs 400 --num_latent_scales 2 --num_groups_per_scale 10 --num_postprocess_cells 3 --num_preprocess_cells 3 \\\n        --num_cell_per_cond_enc 2 --num_cell_per_cond_dec 2 --num_latent_per_group 20 --num_preprocess_blocks 2 \\\n        --num_postprocess_blocks 2 --weight_decay_norm 1e-2 --num_channels_enc 32 --num_channels_dec 32 --num_nf 0 \\\n        --ada_groups --num_process_per_node 2 --use_se --res_dist --fast_adamax \n```\n</details>\n\n<details><summary>CIFAR-10</summary>\n\nEight 16-GB V100 GPUs are used for training NVAE on CIFAR-10. Training takes about 55 hours.\n\n```shell script\nexport EXPR_ID=UNIQUE_EXPR_ID\nexport DATA_DIR=PATH_TO_DATA_DIR\nexport CHECKPOINT_DIR=PATH_TO_CHECKPOINT_DIR\nexport CODE_DIR=PATH_TO_CODE_DIR\ncd $CODE_DIR\npython train.py --data $DATA_DIR/cifar10 --root $CHECKPOINT_DIR --save $EXPR_ID --dataset cifar10 \\\n        --num_channels_enc 128 --num_channels_dec 128 --epochs 400 --num_postprocess_cells 2 --num_preprocess_cells 2 \\\n        --num_latent_scales 1 --num_latent_per_group 20 --num_cell_per_cond_enc 2 --num_cell_per_cond_dec 2 \\\n        --num_preprocess_blocks 1 --num_postprocess_blocks 1 --num_groups_per_scale 30 --batch_size 32 \\\n        --weight_decay_norm 1e-2 --num_nf 1 --num_process_per_node 8 --use_se --res_dist --fast_adamax \n```\n</details>\n\n<details><summary>CelebA 64</summary>\n\nEight 16-GB V100 GPUs are used for training NVAE on CelebA 64. Training takes about 92 hours.\n\n```shell script\nexport EXPR_ID=UNIQUE_EXPR_ID\nexport DATA_DIR=PATH_TO_DATA_DIR\nexport CHECKPOINT_DIR=PATH_TO_CHECKPOINT_DIR\nexport CODE_DIR=PATH_TO_CODE_DIR\ncd $CODE_DIR\npython train.py --data $DATA_DIR/celeba64_lmdb --root $CHECKPOINT_DIR --save $EXPR_ID --dataset celeba_64 \\\n        --num_channels_enc 64 --num_channels_dec 64 --epochs 90 --num_postprocess_cells 2 --num_preprocess_cells 2 \\\n        --num_latent_scales 3 --num_latent_per_group 20 --num_cell_per_cond_enc 2 --num_cell_per_cond_dec 2 \\\n        --num_preprocess_blocks 1 --num_postprocess_blocks 1 --weight_decay_norm 1e-1 --num_groups_per_scale 20 \\\n        --batch_size 16 --num_nf 1 --ada_groups --num_process_per_node 8 --use_se --res_dist --fast_adamax\n```\n</details>\n\n<details><summary>ImageNet 32x32</summary>\n\n24 16-GB V100 GPUs are used for training NVAE on ImageNet 32x32. Training takes about 70 hours.\n\n```shell script\nexport EXPR_ID=UNIQUE_EXPR_ID\nexport DATA_DIR=PATH_TO_DATA_DIR\nexport CHECKPOINT_DIR=PATH_TO_CHECKPOINT_DIR\nexport CODE_DIR=PATH_TO_CODE_DIR\nexport IP_ADDR=IP_ADDRESS\nexport NODE_RANK=NODE_RANK_BETWEEN_0_TO_2\ncd $CODE_DIR\nmpirun --allow-run-as-root -np 3 -npernode 1 bash -c \\\n        'python train.py --data $DATA_DIR/imagenet-oord/imagenet-oord-lmdb_32 --root $CHECKPOINT_DIR --save $EXPR_ID --dataset imagenet_32 \\\n        --num_channels_enc 192 --num_channels_dec 192 --epochs 45 --num_postprocess_cells 2 --num_preprocess_cells 2 \\\n        --num_latent_scales 1 --num_latent_per_group 20 --num_cell_per_cond_enc 2 --num_cell_per_cond_dec 2 \\\n        --num_preprocess_blocks 1 --num_postprocess_blocks 1 --num_groups_per_scale 28 \\\n        --batch_size 24 --num_nf 1 --warmup_epochs 1 \\\n        --weight_decay_norm 1e-2 --weight_decay_norm_anneal --weight_decay_norm_init 1e0 \\\n        --num_process_per_node 8 --use_se --res_dist \\\n        --fast_adamax --node_rank $NODE_RANK --num_proc_node 3 --master_address $IP_ADDR '\n```\n</details>\n\n<details><summary>CelebA HQ 256</summary>\n\n24 32-GB V100 GPUs are used for training NVAE on CelebA HQ 256. Training takes about 94 hours.\n\n```shell script\nexport EXPR_ID=UNIQUE_EXPR_ID\nexport DATA_DIR=PATH_TO_DATA_DIR\nexport CHECKPOINT_DIR=PATH_TO_CHECKPOINT_DIR\nexport CODE_DIR=PATH_TO_CODE_DIR\nexport IP_ADDR=IP_ADDRESS\nexport NODE_RANK=NODE_RANK_BETWEEN_0_TO_2\ncd $CODE_DIR\nmpirun --allow-run-as-root -np 3 -npernode 1 bash -c \\\n        'python train.py --data $DATA_DIR/celeba/celeba-lmdb --root $CHECKPOINT_DIR --save $EXPR_ID --dataset celeba_256 \\\n        --num_channels_enc 30 --num_channels_dec 30 --epochs 300 --num_postprocess_cells 2 --num_preprocess_cells 2 \\\n        --num_latent_scales 5 --num_latent_per_group 20 --num_cell_per_cond_enc 2 --num_cell_per_cond_dec 2 \\\n        --num_preprocess_blocks 1 --num_postprocess_blocks 1 --weight_decay_norm 1e-2 --num_groups_per_scale 16 \\\n        --batch_size 4 --num_nf 2 --ada_groups --min_groups_per_scale 4 \\\n        --weight_decay_norm_anneal --weight_decay_norm_init 1. --num_process_per_node 8 --use_se --res_dist \\\n        --fast_adamax --num_x_bits 5 --node_rank $NODE_RANK --num_proc_node 3 --master_address $IP_ADDR '\n```\n\nIn our early experiments, a smaller model with 24 channels instead of 30, could be trained on only 8 GPUs in \nthe same time (with the batch size of 6). The smaller models obtain only 0.01 bpd higher \nnegative log-likelihood.\n</details>\n\n<details><summary>FFHQ 256</summary>\n\n24 32-GB V100 GPUs are used for training NVAE on FFHQ 256. Training takes about 160 hours. \n\n```shell script\nexport EXPR_ID=UNIQUE_EXPR_ID\nexport DATA_DIR=PATH_TO_DATA_DIR\nexport CHECKPOINT_DIR=PATH_TO_CHECKPOINT_DIR\nexport CODE_DIR=PATH_TO_CODE_DIR\nexport IP_ADDR=IP_ADDRESS\nexport NODE_RANK=NODE_RANK_BETWEEN_0_TO_2\ncd $CODE_DIR\nmpirun --allow-run-as-root -np 3 -npernode 1 bash -c \\\n        'python train.py --data $DATA_DIR/ffhq/ffhq-lmdb --root $CHECKPOINT_DIR --save $EXPR_ID --dataset ffhq \\\n        --num_channels_enc 30 --num_channels_dec 30 --epochs 200 --num_postprocess_cells 2 --num_preprocess_cells 2 \\\n        --num_latent_scales 5 --num_latent_per_group 20 --num_cell_per_cond_enc 2 --num_cell_per_cond_dec 2 \\\n        --num_preprocess_blocks 1 --num_postprocess_blocks 1 --weight_decay_norm 1e-1  --num_groups_per_scale 16 \\\n        --batch_size 4 --num_nf 2  --ada_groups --min_groups_per_scale 4 \\\n        --weight_decay_norm_anneal --weight_decay_norm_init 1. --num_process_per_node 8 --use_se --res_dist \\\n        --fast_adamax --num_x_bits 5 --learning_rate 8e-3 --node_rank $NODE_RANK --num_proc_node 3 --master_address $IP_ADDR '\n```\n\nIn our early experiments, a smaller model with 24 channels instead of 30, could be trained on only 8 GPUs in \nthe same time (with the batch size of 6). The smaller models obtain only 0.01 bpd higher \nnegative log-likelihood.\n</details>\n\n**If for any reason your training is stopped, use the exact same commend with the addition of `--cont_training`\nto continue training from the last saved checkpoint. If you observe NaN, continuing the training using this flag\nusually will not fix the NaN issue.**\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 717,
      "date": "Sun, 26 Dec 2021 12:12:11 GMT"
    },
    "technique": "GitHub API"
  }
}