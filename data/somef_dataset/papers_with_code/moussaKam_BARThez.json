{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2010.12321]\n\n## Introduction\nA french sequence to sequence pretrained model based on [BART](https://github.com/pytorch/fairseq/tree/master/examples/bart",
      "https://arxiv.org/abs/2010.12321"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{eddine2020barthez,\n  title={BARThez: a Skilled Pretrained French Sequence-to-Sequence Model},\n  author={Eddine, Moussa Kamal and Tixier, Antoine J-P and Vazirgiannis, Michalis},\n  journal={arXiv preprint arXiv:2010.12321},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9502570162300228
      ],
      "excerpt": "A french sequence to sequence pretrained model. [https://arxiv.org/abs/2010.12321] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992494165864036
      ],
      "excerpt": "| BARThez             |  BARThez fine-tuned on abstract generation | BARThez fine-tuned on title generation | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --num_train_epochs 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999997781873693
      ],
      "excerpt": "text_sentence = \"Citant les pr\u00e9occupations de ses clients d\u00e9non\u00e7ant des cas de censure apr\u00e8s la suppression du compte de Trump, un fournisseur d'acc\u00e8s Internet de l'\u00c9tat de l'Idaho a d\u00e9cid\u00e9 de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clients m\u00e9contents de la politique de ces r\u00e9seaux sociaux.\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "   --num_train_epochs 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "print(\"positive\" if predict.argmax(dim=-1).item()==1 else \"negative\")  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195926162616405
      ],
      "excerpt": "sent2 = \"L\u2019\u00e9quipe effectua \u00e9galement une tourn\u00e9e en Australie en 1953 et en Asie en ao\u00fbt 1959.\" \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/moussaKam/BARThez/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/moussaKam/BARThez",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nLicense\nBy contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq),\nyou agree that your contributions will be licensed under the LICENSE file in\nthe root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-22T19:06:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T20:12:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To generate the summaries use `generate_summary.py` script:\n```\npython generate_summary.py \\\n    --model_path experiments/checkpoints/translation/summarization_title_fr/barthez/ms4096_mu60000_lr1e-04_me50_dws1/1/checkpoint_best.pt \\\n    --output_path experiments/checkpoints/translation/summarization_title_fr/barthez/ms4096_mu60000_lr1e-04_me50_dws1/1/output.txt \\ \n    --source_text summarization_data_title_barthez/test-article.txt \\\n    --data_path summarization_data_title_barthez/data-bin/ \\\n    --sentence_piece_model barthez.base/sentence.bpe.model\n```\nwe use [rouge-score](https://pypi.org/project/rouge-score/) to compute ROUGE score. No stemming is applied before evaluation.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "A french sequence to sequence pretrained model based on [BART](https://github.com/pytorch/fairseq/tree/master/examples/bart). <br>\nBARThez is pretrained by learning to reconstruct a corrupted input sentence. A corpus of 66GB of french raw text is used to carry out the pretraining. <br>\nUnlike already existing BERT-based French language models such as CamemBERT and FlauBERT, BARThez is particularly well-suited for generative tasks, since not only its encoder but also its decoder is pretrained. \n\nIn addition to BARThez that is pretrained from scratch, we continue the pretraining of a multilingual BART [mBART](https://github.com/pytorch/fairseq/tree/master/examples/mbart) which boosted its performance in both discriminative and generative tasks. We call the french adapted version mBARThez.\n\n| Model         | Architecture  | #layers | #params | Link  |\n| ------------- |:-------------:| :-----:|:-----:|:-----:|\n| BARThez       | BASE          | 12     | 216M  | [Link](https://www.dropbox.com/s/a1y5avgb8uh2v3s/barthez.base.zip?dl=1) |\n| mBARThez      | LARGE         | 24     | 561M  |[Link](https://www.dropbox.com/s/oo9tokh09rioq0m/mbarthez.large.zip?dl=1) |\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8991953063778082
      ],
      "excerpt": "Our models are now on Hugging face! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8616786669337639
      ],
      "excerpt": "It is possible to use BARThez for text classification tasks, such as sentiment analysis. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979676381184016
      ],
      "excerpt": "For inference: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230964640962787
      ],
      "excerpt": "It's time to train the model.  <br>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810706751001248
      ],
      "excerpt": "1 refers to the seed <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8688357964177088
      ],
      "excerpt": "In addition to text generation, BARThez can perform discriminative tasks. For example to fine-tune the model on PAWSX task: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for SENT in $SENTS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "    for SPLIT in $SPLITS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "        spm_encode --model ../../barthez.base/sentence.bpe.model < $SPLIT.$SENT > $SPLIT.spm.$SENT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810706751001248
      ],
      "excerpt": "1 refers to the seed <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    sentencepiece_vocab='barthez.base/sentence.bpe.model', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A french sequence to sequence pretrained model",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/moussaKam/BARThez/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sat, 25 Dec 2021 08:42:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/moussaKam/BARThez/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "moussaKam/BARThez",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/moussaKam/BARThez/tree/main/fairseq/docs",
      "https://github.com/moussaKam/BARThez/tree/main/fairseq/examples/simultaneous_translation/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/FLUE/get-data-xnli.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/summarization_data_title_barthez/binarize.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/summarization_data_title_barthez/encode_spm.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/experiments/PAWSX/experiment_barthez.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/experiments/title_generation/barthez_summarization_title.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/scripts/compound_split_bleu.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/scripts/sacrebleu_pregen.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/language_model/prepare-wikitext-103.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/speech_recognition/datasets/prepare-librispeech.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/byte_level_bpe/get_data.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/roberta/preprocess_GLUE_tasks.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/roberta/preprocess_RACE.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/roberta/commonsense_qa/download_cqa_data.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/backtranslation/sacrebleu.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/backtranslation/tokenized_bleu.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/backtranslation/prepare-wmt18en2de.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/backtranslation/prepare-de-monolingual.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/translation/prepare-wmt14en2fr.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/translation/prepare-iwslt14.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/translation/prepare-iwslt17-multilingual.sh",
      "https://raw.githubusercontent.com/moussaKam/BARThez/main/fairseq/examples/translation/prepare-wmt14en2de.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\ngit clone https://github.com/moussaKam/BARThez\ncd BARThez/fairseq\npip install --editable ./\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8662167724323339,
        0.999746712887969
      ],
      "excerpt": "First make sure that you have sentencepiece installed: \npip install sentencepiece \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8743482350653297,
        0.8603499673592618
      ],
      "excerpt": "Make sure that your dataset files are in the required format. \nFor inference you can use the following code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9800347430776957
      ],
      "excerpt": "Install sentencepiece from here <br>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8785077071534921
      ],
      "excerpt": "You can refer to summarization_data_title_barthez/encode_spm.sh script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811319816230116,
        0.97258628462344,
        0.9465718491881494
      ],
      "excerpt": "Use the script in experiments/title_generation/barthez_summarization_title.sh <br>  \ncd experiments/title_generation/ \nbash barthez_summarization_title.sh 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362,
        0.9906248903846466
      ],
      "excerpt": "mkdir discriminative_tasks_data/ \ncd discriminative_tasks_data/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd PAWSX \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941112352797852,
        0.97258628462344,
        0.9465718491881494
      ],
      "excerpt": "Use the script experiments/PAWSX/experiment_barthez.sh <br>  \ncd experiments/PAWSX/ \nbash experiment_barthez.sh 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8603499673592618
      ],
      "excerpt": "For inference you can use the following code: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8216270093103228,
        0.9563277988753164
      ],
      "excerpt": "For example: \npython examples/seq2seq/run_seq2seq.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8897107595245954
      ],
      "excerpt": "    --train_file ../OrangeSumTransformers/abstract_generation/train.csv \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from transformers import ( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    [barthez_tokenizer.encode(text_sentence, add_special_tokens=True)] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8797144750738146,
        0.9028449554668395
      ],
      "excerpt": "predict = barthez_model.generate(input_ids, max_length=100)[0] \nbarthez_tokenizer.decode(predict, skip_special_tokens=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_glue.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from transformers import ( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    [barthez_tokenizer.encode(text_sentence, add_special_tokens=True)] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8268874990071704
      ],
      "excerpt": "print(\"positive\" if predict.argmax(dim=-1).item()==1 else \"negative\")  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8437434863611334
      ],
      "excerpt": "Encode the data using spm_encode. In total there will be 6 files to tokenize. <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python ../FLUE/prepare_pawsx.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8635670535437081
      ],
      "excerpt": "SPLITS=\"train test valid\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "  --trainpref train.spm.sent1 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "  --testpref test.spm.sent1 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "  --trainpref train.spm.sent2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "  --testpref test.spm.sent2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900486270063179
      ],
      "excerpt": "from fairseq.models.bart import BARTModel \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/moussaKam/BARThez/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Cuda",
      "C++",
      "Cython",
      "Lua",
      "Batchfile",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Facebook, Inc. and its affiliates.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BARThez",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BARThez",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "moussaKam",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/moussaKam/BARThez/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 30,
      "date": "Sat, 25 Dec 2021 08:42:30 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please follow the steps [here](https://github.com/moussaKam/OrangeSum) to get OrangeSum. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Use the script `compute_mean_std.py`:\n```\npython compute_mean_std.py --path_events experiments/tensorboard_logs/sentence_prediction/PAWSX/barthez/ms32_mu23200_lr1e-04_me10_dws1/\n```\nIn case you ran the training for multiple seeds, this script helps getting the mean, the median and the standard deviation of the scores. The valid score corresponds to the best valid score across the epochs, and the test score corresponds to the test score of the epoch with the best valid score.\n\n",
      "technique": "Header extraction"
    }
  ]
}