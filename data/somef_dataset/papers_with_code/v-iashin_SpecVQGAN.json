{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Funding for this research was provided by the Academy of Finland projects 327910 & 324346. The authors acknowledge CSC \u2014 IT Center for Science, Finland, for computational resources for our experimentation.\n\nWe also acknowledge the following work:\n- The code base is built upon an amazing [taming-transformers](https://github.com/CompVis/taming-transformers) repo.\nCheck it out if you are into high-res image generation.\n- The implementation of some evaluation metrics is partially borrowed and adapted from [torch-fidelity](https://github.com/toshas/torch-fidelity).\n- The feature extraction pipeline for BN-Inception relies on the baseline implementation [RegNet](https://github.com/PeihaoChen/regnet).\n- MelGAN training scripts are built upon the [official implementation for text-to-speech MelGAN](https://github.com/descriptinc/melgan-neurips).\n- Thanks [AK391](https://github.com/AK391) for adapting our neural audio codec demo as a\nGradio app at [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/SpecVQGAN_Neural_Audio_Codec)\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2012.09841",
      "https://arxiv.org/abs/1711.00937"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our paper was accepted as an oral presentation for the BMVC 2021.\nPlease, use this bibtex if you would like to cite our work\n```\n@InProceedings{SpecVQGAN_Iashin_2021,\n  title={Taming Visually Guided Sound Generation},\n  author={Iashin, Vladimir and Rahtu, Esa},\n  booktitle={British Machine Vision Conference (BMVC)},\n  year={2021}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{SpecVQGAN_Iashin_2021,\n  title={Taming Visually Guided Sound Generation},\n  author={Iashin, Vladimir and Rahtu, Esa},\n  booktitle={British Machine Vision Conference (BMVC)},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "Taming Visually Guided Sound Generation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "vggsound_meta['end_seconds'] = vggsound_meta['start_seconds'] + 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "  - freetype=2.10.2=h5ab3b9f_0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "  - libpng=1.6.37=hbc83047_0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "  - tk=8.6.10=hbc83047_0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "    - sk-video==1.1.8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.882905965933972
      ],
      "excerpt": "| Trained on | Evaluated on | FID \u2193 | Avg. MKL \u2193 |                                                                                                                                                          Link / MD5SUM | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593299683604384
      ],
      "excerpt": "| 212 Feats |     ResNet50 | 10.5 |       6.9 |          11.8 | b222cc0e7aeb419f533d5806a08669fe | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/v-iashin/SpecVQGAN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-17T11:20:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-15T23:33:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9591963772075245,
        0.9047062276590283
      ],
      "excerpt": "The codebook is trained on spectrograms similarly to VQGAN (an upgraded VQVAE). \nWe refer to it as Spectrogram VQGAN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317952075784933
      ],
      "excerpt": "Once the spectrogram codebook is trained, we can train a transformer (a variant of GPT-2) to autoregressively sample the codebook entries as tokens conditioned on a set of visual features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790972281596987
      ],
      "excerpt": "This approach allows training a spectrogram generation model which produces long, relevant, and high-fidelity sounds while supporting tens of data classes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "VGGish-ish, Melception, and MelGAN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.887883544036865
      ],
      "excerpt": "<!-- The link to this section is used in demo.ipynb --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9766885248512236
      ],
      "excerpt": "In this project, we used VAS and VGGSound datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9047310317763173,
        0.9550730095807468,
        0.9004202626253757
      ],
      "excerpt": "For BN Inception features, we employ the same procedure as RegNet. \nFor ResNet50 features, we rely on video_features (branch specvqgan) \nrepository and used these commands: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    --output_path ./data/vggsound/feature_resnet50_dim2048_21.5fps \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8563640805669209,
        0.9159529446117765
      ],
      "excerpt": "Similar to `BN Inception`, we need to \"tile\" (cycle) a video if it is shorter than 10s. For \n`ResNet50` we achieve this by tiling the resulting frame-level features up to 215 on temporal dimension, e.g. as follows:python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9138914695383004
      ],
      "excerpt": "We also add 3 lines with ` \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8477644734212683
      ],
      "excerpt": "It will take some time. In my case it was one week, including reruns on shuffled data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9311243186410093,
        0.8584736770781901
      ],
      "excerpt": "Run Sampling Tool to see the reconstruction results for available data. \nThe setting (a): the transformer is trained on VGGSound to sample from the VGGSound codebook: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911060041832677
      ],
      "excerpt": "Evaluating a model on a larger number of samples per video is an expensive procedure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8584736770781901
      ],
      "excerpt": "The setting (b): the transformer is trained on VAS to sample from the VGGSound codebook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911060041832677
      ],
      "excerpt": "Evaluating a model on a larger number of samples per video is an expensive procedure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8584736770781901
      ],
      "excerpt": "The setting (c): the transformer is trained on VAS to sample from the VAS codebook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911060041832677
      ],
      "excerpt": "Evaluating a model on a larger number of samples per video is an expensive procedure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8494188498269273,
        0.8584736770781901
      ],
      "excerpt": "We also provide pre-trained models for all three settings: \nThe setting (c): the transformer is trained on VAS to sample from the VAS codebook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9541976679887039
      ],
      "excerpt": "The reference performance of VGGish-ish and Melception: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144815396079746
      ],
      "excerpt": "The training is done in two stages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8336254811151496
      ],
      "excerpt": "The first and the second stages can be trained on the same or separate datasets as long as the process of spectrogram extraction is the same. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9645067009167936
      ],
      "excerpt": "data.params.batch_size=8 for the codebook and =16 for the transformer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9378021401493769
      ],
      "excerpt": "data.params.spec_dir_path, data.params.rgb_feats_dir_path, and data.params.flow_feats_dir_path \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891912720011952
      ],
      "excerpt": "A transformer (GPT-2) is trained to sample from the spectrogram codebook given a set of frame-level visual features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9766957315474967
      ],
      "excerpt": ": with the VAS codebook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8041407325901113,
        0.9587893230635326
      ],
      "excerpt": "    model.params.first_stage_config.params.ckpt_path=./logs/2021-06-06T19-42-53_vas_codebook/checkpoints/epoch_259.ckpt \n: or with the VGGSound codebook which has 1024 codes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "    model.params.transformer_config.params.GPT_config.vocab_size=1024 \\ \n    model.params.first_stage_config.params.n_embed=1024 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8815458363993088,
        0.9938166959878378
      ],
      "excerpt": "The size of the visual condition is controlled by two arguments in the config file. \nThe feat_sample_size is the size of the visual features resampled equidistantly from all available features (212) and block_size is the attention span. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9082270207706271,
        0.9013135377775329,
        0.911931425300574,
        0.9060741583630505
      ],
      "excerpt": "For instance, for feat_sample_size=212 the block_size=477. \nHowever, the longer the condition, the more memory and more timely the sampling. \nBy default, the configs are using feat_sample_size=212 for VAS and 5 for VGGSound. \nFeel free to tweak it to your liking/application for example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.8979411005071259,
        0.8041407325901113,
        0.9168311986561436
      ],
      "excerpt": "    model.params.transformer_config.params.GPT_config.block_size=318 \\ \n    data.params.feat_sampler_cfg.params.feat_sample_size=53 \\ \n    model.params.first_stage_config.params.ckpt_path=./logs/2021-06-06T19-42-53_vas_codebook/checkpoints/epoch_259.ckpt \nThe No Feats settings (without visual condition) are trained similarly to the settings with visual conditioning where the condition is replaced with random vectors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.8979411005071259,
        0.8041407325901113
      ],
      "excerpt": "    model.params.transformer_config.params.GPT_config.block_size=266 \\ \n    data.params.feat_sampler_cfg.params.feat_sample_size=1 \\ \n    model.params.first_stage_config.params.ckpt_path=./logs/2021-06-06T19-42-53_vas_codebook/checkpoints/epoch_259.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8979411005071259,
        0.8979411005071259
      ],
      "excerpt": "        data.params.spec_dir_path=$SPEC_DIR_PATH \\ \n        data.params.rgb_feats_dir_path=$RGB_FEATS_DIR_PATH \\ \n        data.params.flow_feats_dir_path=$FLOW_FEATS_DIR_PATH \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8990636973179837
      ],
      "excerpt": "For interactive sampling, we rely on the Streamlit library. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009424207176827
      ],
      "excerpt": ": go to localhost:5555 in your browser \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9618832774431968
      ],
      "excerpt": "We also alternatively provide a similar notebook in ./generation_demo.ipynb to play with the demo on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Source code for \"Taming Visually Guided Sound Generation\" (Oral at the BMVC 2021)",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The scripts will download features, check the `md5` sum, unpack, and do a clean-up for each part of the dataset:\n```bash\ncd ./data\n#: 24GB\nbash ./download_vas_features.sh\n#: 420GB (+ 420GB if you also need ResNet50 Features)\nbash ./download_vggsound_features.sh\n```\nThe unpacked features are going to be saved in `./data/downloaded_features/*`.\nMove them to `./data/vas` and `./data/vggsound` such that the folder structure would match the structure of the demo files.\nBy default, it will download `BN Inception` features, to download `ResNet50` features uncomment the lines in scripts `./download_*_features.sh`\n\nIf you wish to download the parts manually, use the following URL templates:\n\n- `https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/specvqgan_public/vas/*.tar`\n- `https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/specvqgan_public/vggsound/*.tar`\n\nAlso, make sure to check the `md5` sums provided in [`./data/md5sum_vas.md5`](./data/md5sum_vas.md5) and [`./data/md5sum_vggsound.md5`](./data/md5sum_vggsound.md5) along with file names.\n\nNote, we distribute features for the VGGSound dataset in 64 parts.\nEach part holds ~3k clips and can be used independently as a subset of the whole dataset (the parts are not class-stratified though).\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/v-iashin/SpecVQGAN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Thu, 23 Dec 2021 19:23:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/v-iashin/SpecVQGAN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "v-iashin/SpecVQGAN",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/v-iashin/SpecVQGAN/main/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/v-iashin/SpecVQGAN/main/generation_demo.ipynb",
      "https://raw.githubusercontent.com/v-iashin/SpecVQGAN/main/neural_audio_codec_demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/v-iashin/SpecVQGAN/main/data/download_vas_features.sh",
      "https://raw.githubusercontent.com/v-iashin/SpecVQGAN/main/data/download_vggsound_features.sh",
      "https://raw.githubusercontent.com/v-iashin/SpecVQGAN/main/evaluation/sbatch_sample.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "During experimentation, we used Linux machines with `conda` virtual environments, PyTorch 1.8 and CUDA 11.\n\nStart by cloning this repo\n```bash\ngit clone https://github.com/v-iashin/SpecVQGAN.git\n```\n\nNext, install the environment.\nFor your convenience, we provide both `conda` and `docker` environments.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "BMVC 2021 \u2013 Oral Presentation\n\n\u2022 [[Project Page](https://v-iashin.github.io/SpecVQGAN)]\n\u2022 [[ArXiv](http://arxiv.org/abs/2110.08791)]\n\u2022 [[BMVC Proceedings](https://www.bmvc2021-virtualconference.com/conference/papers/paper_1213.html)]\n\u2022 [[Poster (for PAISS)](https://v-iashin.github.io/images/specvqgan/poster.pdf)]\n\u2022 [[Presentation on YouTube](https://www.youtube.com/watch?v=Bucb3nAa398)] ([Can't watch YouTube?](https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/specvqgan_public/SpecVQGAN%20YouTube.mp4))\n\u2022\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pxTIMweAKApJZ3ZFqyBee3HtMqFpnwQ0?usp=sharing)\n\n<img src=\"https://github.com/v-iashin/v-iashin.github.io/raw/master/images/specvqgan/specvqgan_vggsound_samples.jpg\" alt=\"Generated Samples Using our Model\" width=\"900\">\n\nListen for the samples on our [project page](https://v-iashin.github.io/SpecVQGAN).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "Environment Preparation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8809874272079831,
        0.9575182603544689,
        0.9770335174395833
      ],
      "excerpt": "conda env create -f conda_env.yml \nTest your environment \nconda activate specvqgan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "    python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023387805154013
      ],
      "excerpt": "or build it yourselfbash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "name: down_audioset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9511302806636455
      ],
      "excerpt": "  - conda-forge \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9351267081877779,
        0.8837680365796365
      ],
      "excerpt": "  - pip=20.1.1=py38_1 \n  - python=3.8.3=hcff3b4d_2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9736629876602173
      ],
      "excerpt": "  - pip: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8841036387318887
      ],
      "excerpt": "    - scipy==1.5.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9940522740266574
      ],
      "excerpt": "Install the environment: conda env create -f down_audioset.yml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8239725935425682,
        0.8239725935425682
      ],
      "excerpt": "| 212 Feats | BN Inception |  20.5 |        6.0 |          11.8 | 1c4e56077d737677eac524383e6d98d3 | \n| 212 Feats |     ResNet50 | 20.8 |       6.2 |          11.8 | 6e553ea44c8bc7a3310961f74e7974ea | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8455679244896503
      ],
      "excerpt": "We run our experiments on a relatively expensive hardware setup with four 40GB NVidia A100 but the models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554385253536647,
        0.9923498545687789
      ],
      "excerpt": "Run it on a 12GB GPU as \ncd ./specvqgan/modules/losses/vggishish \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd ./vocoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8455795716091624
      ],
      "excerpt": "We provide a multi-gpu command which can easily be applied on a multi-node setup by replacing --master_addr to your main machine and --node_rank for every worker's id (also see an sbatch script in ./evaluation/sbatch_sample.sh if you have a SLURM cluster at your disposal): \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8245539886860519
      ],
      "excerpt": "Pretrained Models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313,
        0.8174540907975313
      ],
      "excerpt": "Training a Spectrogram Codebook \nTraining a Transformer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training MelGAN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": ": True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021772751119439
      ],
      "excerpt": "docker run \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": ": True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333384803827206
      ],
      "excerpt": "    python main.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333384803827206
      ],
      "excerpt": "python main.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8013803951358645
      ],
      "excerpt": "    --output_path ./data/vggsound/feature_resnet50_dim2048_21.5fps \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8575497988150836
      ],
      "excerpt": "feats = pickle.load(open(path, 'rb')).astype(np.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891006132389237,
        0.8537887978149701
      ],
      "excerpt": "feats = np.tile(feats, (reps, 1))[:215, :] \nwith open(new_path, 'wb') as file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8525447642357499,
        0.8525447642357499,
        0.809547384061538
      ],
      "excerpt": "VGGSOUND_PATH = './data/vggsound.csv' \nVGGSOUND_REF_PATH = './data/vggsound_ref.csv' \nvggsound_meta = pd.read_csv(VGGSOUND_PATH, names=['YTID', 'start_seconds', 'positive_labels', 'split']) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "name: down_audioset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142835995138061
      ],
      "excerpt": "  - _libgcc_mutex=0.1=main \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130584857348769
      ],
      "excerpt": "  - mad=0.15.1b=he1b5a44_0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8197708039605266,
        0.801419485107265
      ],
      "excerpt": "(down_audioset) $ python download_audioset.py ./vggsound -e ./vggsound_ref.csv -n 8 -nr 2 -f $(which ffmpeg) -fp $(which ffprobe) -lp ./log.txt --verbose \nThe script will download the dataset into ./vggsound folder (mp4 videos and flac audios). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8016176574591641
      ],
      "excerpt": "cp vggsound_ref.csv vggsound_noshuf.csv &amp;&amp; shuf -o vggsound_ref.csv vggsound_ref_notshuf.csv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8294827978602012
      ],
      "excerpt": "Unpack the pre-trained models to ./logs/ directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221518510459593
      ],
      "excerpt": "First, a spectrogram codebook should be trained. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8422500427304016
      ],
      "excerpt": "data.params.spec_dir_path, data.params.rgb_feats_dir_path, and data.params.flow_feats_dir_path \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240727200309153
      ],
      "excerpt": "python train.py --base configs/vas_codebook.yaml -t True --gpus 0, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240727200309153
      ],
      "excerpt": "python train.py --base configs/vggsound_codebook.yaml -t True --gpus 0, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240727200309153,
        0.8464981155397245
      ],
      "excerpt": "python train.py --base configs/vas_transformer.yaml -t True --gpus 0, \\ \n    model.params.first_stage_config.params.ckpt_path=./logs/2021-06-06T19-42-53_vas_codebook/checkpoints/epoch_259.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240727200309153,
        0.8347190929786527,
        0.8347190929786527,
        0.8534708947894585
      ],
      "excerpt": "python train.py --base configs/vas_transformer.yaml -t True --gpus 0, \\ \n    model.params.transformer_config.params.GPT_config.vocab_size=1024 \\ \n    model.params.first_stage_config.params.n_embed=1024 \\ \n    model.params.first_stage_config.params.ckpt_path=./logs/2021-05-19T22-16-54_vggsound_codebook/checkpoints/epoch_39.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240727200309153,
        0.8534708947894585
      ],
      "excerpt": "python train.py --base configs/vggsound_transformer.yaml -t True --gpus 0, \\ \n    model.params.first_stage_config.params.ckpt_path=./logs/2021-05-19T22-16-54_vggsound_codebook/checkpoints/epoch_39.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240727200309153,
        0.8347190929786527,
        0.8383773530064831,
        0.8464981155397245
      ],
      "excerpt": "python train.py --base configs/vas_transformer.yaml -t True --gpus 0, \\ \n    model.params.transformer_config.params.GPT_config.block_size=318 \\ \n    data.params.feat_sampler_cfg.params.feat_sample_size=53 \\ \n    model.params.first_stage_config.params.ckpt_path=./logs/2021-06-06T19-42-53_vas_codebook/checkpoints/epoch_259.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240727200309153,
        0.9163182072930159,
        0.8347190929786527,
        0.8383773530064831,
        0.8464981155397245
      ],
      "excerpt": "python train.py --base configs/vas_transformer.yaml -t True --gpus 0, \\ \n    data.params.replace_feats_with_random=true \\ \n    model.params.transformer_config.params.GPT_config.block_size=266 \\ \n    data.params.feat_sampler_cfg.params.feat_sample_size=1 \\ \n    model.params.first_stage_config.params.ckpt_path=./logs/2021-06-06T19-42-53_vas_codebook/checkpoints/epoch_259.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": ": Sample \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8146987923390707
      ],
      "excerpt": "        evaluation/generate_samples.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550162597498284,
        0.8550162597498284,
        0.8550162597498284
      ],
      "excerpt": "        data.params.spec_dir_path=$SPEC_DIR_PATH \\ \n        data.params.rgb_feats_dir_path=$RGB_FEATS_DIR_PATH \\ \n        data.params.flow_feats_dir_path=$FLOW_FEATS_DIR_PATH \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.852118605582617
      ],
      "excerpt": "    evaluate.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "SPLITS=\"\\\"[test, ]\\\"\" \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/v-iashin/SpecVQGAN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Vladimir Iashin\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Taming Visually Guided Sound Generation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SpecVQGAN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "v-iashin",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/v-iashin/SpecVQGAN/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 131,
      "date": "Thu, 23 Dec 2021 19:23:54 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "transformer",
      "vqvae",
      "gan",
      "pytorch",
      "audio-generation",
      "video-features",
      "melgan",
      "multi-modal",
      "video-understanding",
      "vggsound",
      "vas",
      "bmvc",
      "evaluation-metrics",
      "audio",
      "video"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "While the Spectrogram VQGAN was never designed to be a neural audio codec but\nit happened to be highly effective for this task.\nWe can employ our Spectrogram VQGAN pre-trained on an open-domain dataset as a\nneural audio codec without a change\n\nIf you wish to apply the SpecVQGAN for audio compression for arbitrary audio,\nplease see our Google Colab demo:\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1K_-e6CRQFLk9Uq6O46FOsAYt63TeEdXf?usp=sharing).\n\nIntegrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/SpecVQGAN_Neural_Audio_Codec)\n\nWe also alternatively provide a similar notebook in `./neural_audio_codec_demo.ipynb` to play with the demo on\na local machine.\n\n",
      "technique": "Header extraction"
    }
  ]
}