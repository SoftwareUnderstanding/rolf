{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1910.10683",
      "https://arxiv.org/abs/1910.10683",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/2002.08910"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "WMT 19 English-German machine \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    seqio.TfdsDataSource(tfds_name=\"wmt19_translate/de-en:1.0.0\"), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "registry, if desired. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "  if isinstance(targets[0], list): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "    shard_info=seqio.ShardInfo(index=0, num_shards=10), \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/seqio",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "How to Contribute\nWe'd love to accept your patches and contributions to this project. There are\njust a few small guidelines you need to follow.\nContributor License Agreement\nContributions to this project must be accompanied by a Contributor License\nAgreement. You (or your employer) retain the copyright to your contribution;\nthis simply gives us permission to use and redistribute your contributions as\npart of the project. Head over to https://cla.developers.google.com/ to see\nyour current agreements on file or to sign a new one.\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\nCode reviews\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose. Consult\nGitHub Help for more\ninformation on using pull requests.\nCommunity Guidelines\nThis project follows\nGoogle's Open Source Community Guidelines.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-15T23:42:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T11:23:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9871168610082475
      ],
      "excerpt": "SeqIO is a library for processing sequential data to be fed into downstream \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8702729956153206
      ],
      "excerpt": "to create scalable data pipelines but requires minimal use of TensorFlow. In \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.927548496393815
      ],
      "excerpt": "numpy iterator and hence it is fully compatible with other frameworks such as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9054431649606565
      ],
      "excerpt": "Currently, SeqIO assumes that the dataset is a sequence, i.e., each feature is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9035421172345814,
        0.999428065671609,
        0.8340277080709482
      ],
      "excerpt": "in the future in order to support higher dimensional data. \nSeqIO is a refactor of the t5.data \nlibrary used (in conjunction with the Mesh Tensorflow \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9854704495787344
      ],
      "excerpt": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9297119664264268
      ],
      "excerpt": "The most important class in SeqIO is the Task. It is an abstraction that combines: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9314985535603534,
        0.8732668771252966,
        0.940304197231725
      ],
      "excerpt": "a vocabulary to tokenize/detokenize each preprocessed feature for the model \na postprocessor to convert detokenized model outputs into a format for evaluation \none or more metrics to evaluate with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8330928045329387
      ],
      "excerpt": "translation task. In the end, our Task will look like this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9548265023455312,
        0.8235345680262228
      ],
      "excerpt": "We typically add the Task to the global registry when we define it (as shown \nabove) to make it easier to use with model configs and flags. Thus, it  must \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8443463832689365,
        0.9383967430805376
      ],
      "excerpt": "data in many formats as a tf.data.Dataset. \nAll data sources are subclasses of the DataSource base class and are defined in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024856887710108,
        0.9147999351745626
      ],
      "excerpt": "FunctionDataSource for providing an custom function that returns a tf.data.Dataset. \nIn our example, we are using the TfdsDataSource. We specify the name of the WMT dataset in TFDS (\"wmt19_translate\"), the specific config for the language pair that excludes the context for the open domain setting (\"de-en\"), and the version number (\"1.0.0\"). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9553153771791841
      ],
      "excerpt": "to seqio.Feature objects. This defines what the Task is expected to produce \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8777856072572323
      ],
      "excerpt": "add_eos, which specifies whether the feature should end with the vocabulary's EOS token. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9876084664440847,
        0.9485844494858511
      ],
      "excerpt": "The tasks used in T5 all produce \"inputs\" and \"targets\" features to be consumed by the text-to-text model. For a decoder-only language model, only a single feature (e.g., \"targets\") would be necessary. \nNevertheless, SeqIO is flexible enough to generate arbitrary output features what will be converted into model features by the FeatureConverter later in the pipeline. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8159705671529597
      ],
      "excerpt": "Preprocessors are functions that transform one tf.data.Dataset into a new tf.data.Dataset. Typically this involves executing a map over the given dataset. The preprocessors provided to the Task will be executed sequentially. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8855165817003802
      ],
      "excerpt": "  {'de': 'Das ist gut.', 'en': 'That is good.'} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9796235604028144
      ],
      "excerpt": "  'targets': 'That is good.'} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9077463748741773
      ],
      "excerpt": "  target_language: target language code (e.g. 'de') to translate to. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "tgt_str = f' to {target_language}: ' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9824163717778739,
        0.9743530715405261
      ],
      "excerpt": "'Das ist gut.', 'en': 'That is good.'}. We convert this to \"inputs\" and \n\"targets\" with the appropriate prompt to inform the model of the task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9729410551073475,
        0.977397170780252
      ],
      "excerpt": "Mapping functions operate on and return tf.Tensors using TensorFlow operations, although it is possible to take advantage of automatic AutoGraph conversion for numpy or use tf.py_function to wrap arbitrary Python code. See tf.data.Dataset documentation for more details. \nWhen calling map, it is important to always set num_parallel_calls=tf.data.experimental.AUTOTUNE to avoid creating a bottleneck. The seqio.map_over_dataset decorator helps enforce this as follows. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8855165817003802
      ],
      "excerpt": "  {'de': 'Das ist gut.', 'en': 'That is good.'} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9796235604028144
      ],
      "excerpt": "  'targets': 'That is good.'} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9077463748741773
      ],
      "excerpt": "  target_language: target language code (e.g. 'de') to translate to. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "tgt_str = f' to {target_language}: ' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213147172236186
      ],
      "excerpt": "  seqio.map_over_dataset decorates it to a function that takes in a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8218061478194976
      ],
      "excerpt": "  \"\"\"Takes a random chunk out of each feature the size ofsequence_length`.\"\"\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "  for k, v in ex.items(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9497650877945217
      ],
      "excerpt": "In our \"wmt_19_ende\" task, we also use the predefined preprocessors seqio.preprocessors.tokenize and seqio.preprocessors.append_eos. The former uses each Feature.vocabulary to tokenize it, and the the latter appends Feature.vocabulary.eos_id to the feature if the Feaure.add_eos is True. See preprocessors.py for their implementations and other useful preprocessors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9485692352853415,
        0.873764073290348
      ],
      "excerpt": "Since the postprocess function is used on both the model output and the targets, it is passed an is_target boolean in case the behavior should be different. It is also passed the fully preprocessed example, including fields that were excluded from output_features. \nFor the \"wmt19_ende\", we don't need any postprocessors. See \"trivia_qa_open\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9370275464674861
      ],
      "excerpt": "Metrics are functions that are passed (by the Evaluator) the fully-materialized list of postprocessed model outputs (or scores) and targets and return a mapping from string names to MetricValue objects containing their values. These are most commonly floating-point scalars, but may also be text, images, audio, histograms, etc (see metrics.py for the full list). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9041114872960345
      ],
      "excerpt": "Prediction metrics are computed using the postprocessed targets and model outputs (predictions). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.931925988809801
      ],
      "excerpt": "for the translation task is BLEU and we use sacrebleu implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341821984618547
      ],
      "excerpt": "    targets: list of strings or list of list of strings if multiple references \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999531193718416
      ],
      "excerpt": "    predictions: list of strings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.826523778575567
      ],
      "excerpt": "    targets = [[x for x in target] for target in targets] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9320917537064896
      ],
      "excerpt": "Score metrics are computed using the postprocessed targets and their log-likelihood scores according to the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8578365212519491,
        0.84685527166809,
        0.8370347244726851
      ],
      "excerpt": "As an example, Multilingual T5 uses a Mixture of per-language \nTasks with tail languages up-weighted in the mixture. \nThere are 3 ways to specify the tasks and their rates: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183380282618422
      ],
      "excerpt": "Now that your Task (and/or Mixture) is defined, its primary functionality is to use it to generate a dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for punc in string.punctuation: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076,
        0.9560187895509076
      ],
      "excerpt": "targets = [[_normalize_answer(t) for t in u] for u in targets] \n  predictions = [_normalize_answer(p) for p in predictions] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "      for pred, ground_truths in zip(predictions, targets) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Task-based datasets, preprocessing, and evaluation for sequence models.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/seqio/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Thu, 30 Dec 2021 08:00:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/google/seqio/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "google/seqio",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/google/seqio/tree/main/seqio/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.999746712887969,
        0.9667204341478239,
        0.9906248903846466,
        0.999746712887969
      ],
      "excerpt": "pip install seqio \ngit clone https://github.com/google/seqio.git \ncd seqio \npip install -e . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8133903354206835
      ],
      "excerpt": "At a high level, we use SeqIO with the following steps. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9136737714764279
      ],
      "excerpt": "one or more preprocessing steps \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8217609181262783
      ],
      "excerpt": "For the \"wmt19_ende\", we don't need any postprocessors. See \"trivia_qa_open\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033238233071944
      ],
      "excerpt": "  The example below will produce identical mixing rates as the previous one. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8147537886195232
      ],
      "excerpt": "You may first need to use seqio.get_mixture_or_task(mixture_or_task_name) to access your dataset provider from the registry. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8478137829489665
      ],
      "excerpt": "sequence models. It uses tf.data.Dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8472396971604007
      ],
      "excerpt": ":#: Usage Tutorial \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8682752838346358
      ],
      "excerpt": "     tf.data.Dataset instance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577095268530011
      ],
      "excerpt": "           add_eos=False, dtype=tf.int32 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917323503348322
      ],
      "excerpt": "           add_eos=True, dtype=tf.int32 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8541179693050871,
        0.8877627123512003,
        0.9423971377338328
      ],
      "excerpt": "TfdsDataSource for loading examples from TensorFlow Datasets. \nTextLineDataset for loading examples from text files (e.g., tsv). \nTFExampleDataSource for loading tf.train.Example protos from a file (e.g. a TFRecord file.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8325329218732374
      ],
      "excerpt": "in its output examples. The output examples may contain additional fields, but \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815617893039517
      ],
      "excerpt": "The output dtype which must be a tf.dtypes.DType. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008288576901689
      ],
      "excerpt": "def translate(dataset: tf.data.Dataset, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8972447453508822
      ],
      "excerpt": "              target_language: str) -> tf.data.Dataset: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.867449815215923
      ],
      "excerpt": "For example, say the dataset returns examples of this format: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.800246505937448
      ],
      "excerpt": "  A preprocessed example with the format listed above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.867449815215923
      ],
      "excerpt": "For example, say the dataset returns examples of this format: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.800246505937448
      ],
      "excerpt": "  A preprocessed example with the format listed above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8682752838346358
      ],
      "excerpt": "  tf.data.Dataset instance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.864861331957199
      ],
      "excerpt": "    dataset: tf.data.Dataset, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8202561346525306,
        0.8380777606997525
      ],
      "excerpt": "      start_idx = tf.random.stateless_uniform( \n         (), seed, 0, tf.size(v) - (length + 1)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8462996359017466
      ],
      "excerpt": "    \"perplexity\": seqio.metrics.Scalar(np.exp(np.mean(scores))) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8501070052733798
      ],
      "excerpt": "Examples will then be sampled from each task in proportion to its rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.8594142235991984
      ],
      "excerpt": "    split=\"train\", \n    shuffle=True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8226440209499781
      ],
      "excerpt": "return text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594979903743593
      ],
      "excerpt": "em = np.mean([ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/google/seqio/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SeqIO: Task-based datasets, preprocessing, and evaluation for sequence models.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "seqio",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "google",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/seqio/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 125,
      "date": "Thu, 30 Dec 2021 08:00:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "for _, ex in zip(range(5), dataset.as_numpy_iterator()):\n  print(ex)\n```\n\nSome notes on a few the arguments:\n\n  * `sequence_length`: An *optional* mapping from feature name to *maximum* length. Will be passed to the preprocessors with a `sequence_length` argument. If not `None`, the final example features will be truncated if they exceed the specified length. Note that this value may be required to be set if any of the preprocessors use the `sequence_length` argument and do not handle the `None` case.\n  * `num_epochs`: The number of times to repeat the source dataset. Preprocessing will be re-applied with new seeds to enable new samples from stochastic steps. Note that if the `CacheDatasetPlaceholder` is included (see below) preprocessing is only re-applied after that step.\n  * `shard_info`: An optional sharding specification for loading a deterministic subset of the dataset. Loading will be most efficient if the number of shards evenly divides the number of shards in the raw data source.\n  * `use_cached`: Specifies whether to load from a pre-cached task for increased performance or to do the preprocessing on-the-fly. See the [following section](#:optional-offline-caching) for details on how to cache your task, which must be done before this can be set to `True`.\n  * `seed`: An optional seed to use for deterministic shuffling and (stateless) stochastic ops. These operations will still be pseudorandom but will be reproducible with the same seed. Set to `None` if determinism is not desired.\n\n#:#:#: (Optional) Offline Caching\n\nFor improved performance at load time and avoid redundant computations for commonly used tasks, you can pre-cache your `Task` with all or part of the preprocessing done in advance of training.\n\nThe first step to doing so is to add a `seqio.CacheDatasetPlaceholder(required=False)` as one of the steps in your preprocessing pipeline. All steps before the placeholder will be cached offline and all steps after will be executed on the fly at load time. You may set `required=True` if you want `get_dataset` to fail unless `use_cached=True`.\n\nCaveats:\n\n* Any stochastic operations that you wish to be re-run when `num_epochs > 1` or with a different `seed` *should* go after the placeholder since only a single sample will be cached.\n* Any preprocessing steps that use the `sequence_length` argument *must* come after the `seqio.CacheDatasetPlaceholder` preproessor since this is only known at runtime, or an exception will be raised. If you wish to cache for a specific sequence length, you can use [`seqio.experimental.add_fully_cached_task`](https://github.com/google/seqio/tree/main/seqio/experimental.py).\n\nOnce your `Task` is registered, you can run [`cache_tasks_main`](scripts/cache_tasks_main.py) to execute the offline preprocessing, providing it with the module containing your task definitions via the `--module_import` flag. For very large datasets, it's recommended you run this [Apache Beam](https://beam.apache.org/) script on a distributed framework like [Google Cloud DataFlow](https://beam.apache.org/documentation/runners/dataflow/).\n\nFinally, you are ready to load the cached version of your `Task` (or `Mixture`) containing it. You will need to add the path to the directory you passed to `--output_cache_dir` via `seqio.add_global_cache_dirs([\"/my/cache/dir\"])`. Now when you call `task_or_mixture.get_dataset(..., use_cached=True)`, the data will be loaded from the cache directory instead of the raw data source.\n\n#:#:#: Feature Converters\n\nThe role of `Task` is to provide the dataset object with as little\nmodel-specific features (e.g., generic \"inputs\" and \"targets\") while the Feature\nConverters transform the model-agnostic features to model-specific features\n(e.g., \"encoder_input_tokens\"). We refer to the former as \"task features\" and\nthe latter as \"model features\".\n\n\nLet's use machine translation (English to German) as a running example.\n\nThe raw data consists of sentence pairs such as\n\n```\n\"That is good\\tDas ist gut.\"\n```\n\nA task registered to `Task` (e.g.,\n[wmt_t2t_ende_v003](t5/data/tasks.py?l=156&rcl=337594707))\nreads these sentence pairs from the data source and applies a series of\n[preprocessors](t5/data/preprocessors.py?rcl=343354647).\nOne of the internal representations looks like\n\n```python\n{\"inputs\": \"translate English to German: That is good.\",\n \"targets\": \"Das ist gut.\"}\n```\n\nThe final output from the `Task` is a tokenized version of the parallel\nsentences.  In the following toy example (the token ids do not correspond to the\nabove string example), the dataset consists of 2 examples.\n\n```python\ndataset = [{\"inputs\": [7, 8, 5], \"targets\": [3, 9]},\n           {\"inputs\": [8, 4, 9, 3], \"targets\": [4]}]\n```\n\nThe format is in the `tf.data.Dataset` (i.e., each example is a dictionary with\n\"inputs\" and \"targets\" fields.\n\nThe `FeatureConverter` then takes this as an input and converts to the\nmodel-specific features. In addition, the feature converter performs padding and\noptionally packing (for model implementations that support it) for efficiency.\nFor example, let's assume that we are using the standard Transformer\narchitecture with an encoder and a decoder. The output of the feature converter\nis\n\n```python\nconverted_dataset = [{\n    \"encoder_input_tokens\": [7, 8, 5, 1, 8, 4, 9, 3, 1, 0],\n     \"encoder_segment_ids\": [1, 1, 1, 1, 2, 2, 2, 2, 2, 0],\n       \"encoder_positions\": [0, 1, 2, 3, 0, 1, 2, 3, 4, 0],\n   \"decoder_target_tokens\": [3, 9, 1, 4, 1, 0, 0],\n    \"decoder_input_tokens\": [0, 3, 9, 0, 4, 0, 0],\n    \"decoder_loss_weights\": [1, 1, 1, 1, 1, 0, 0],\n       \"decoder_positions\": [0, 1, 2, 0, 1, 0, 0],\n     \"decoder_segment_ids\": [1, 1, 1, 2, 2, 0, 0],\n}]\n```\n\nIn this case, two task examples are packed into one. `*_segment_id` and\n`*_position` are the fields used to denote the membership and position of packed\ntoken in the original sequence. The EOS ids (i.e., 1) are appended. In addition,\neach fields is padded to the specified length.\n\nWe will look at the details of this example in Encoder-decoder architecture:\n`seqio.EncDecFeatureConverter` section.\n\n\n#:#:#:#: Feature converters provided out of the box\n\nWe provide feature converters for three common architectures: encoder-decoder,\ndecoder-only and encoder-only. Here we describe how users can use the feature\nconverters for each of these architectures out of the box as a part of the\nSeqIO library.\n\nIn the SeqIO library, each architecture has a class defining how the task\nfeatures are converted to model features. Since these feature converters are\nalready implemented, it is straightforward to use them by providing the class as a\n`feature_converter` argument of the `seqio.get_dataset` function. The\nfollowing sections will show the example usage of `seqio.get_dataset`.\n\n\n#:#:#:#:#: Encoder-decoder architecture: `seqio.EncDecFeatureConverter`\nThis is the architecture of the original Transformer paper. For the\nEnglish-to-German translation task, the following function call retrieves the\n`tf.data.Dataset` object with the model features.\n\n```python\ndataset: tf.data.Dataset = seqio.get_dataset(\n    mixture_or_task_name=\"wmt_t2t_ende_v003\",\n    task_feature_lengths={\"inputs\": 32, \"targets\": 32},\n    dataset_split=\"train\",\n    shuffle=True,\n    feature_converter=seqio.EncDecFeatureConverter(pack=True)\n)\n```\n\nThe resulting dataset object has the following 7 fields\n\n|Feature name          | Explanation                |\n|----------------------|---------------------------|\n|`encoder_input_tokens` | Input tokens to the encoder. |\n|`encoder_positions`    | Position index in the sequence before packing.|\n|`encoder_segment_ids`  | Sequence membership before packing. Two positions with the same positive integer mean that they belong to the same sequence before packing. |\n|`decoder_input_tokens` | Input tokens to the decoder. |\n|`decoder_target_tokens`| Output tokens from the decoder. |\n|`decoder_loss_weights` | A weight on each position that can be used as a mask. |\n|`decoder_positions`    | Position index in the sequence before packing. |\n|`decoder_segment_ids`  | Same as `encoder_segment_ids` but for decoder.|\n\n\n#:#:#:#:#: Decoder-only architecture\n\nThis architecture consists of a single autoregressive stack, which we denote as\na \"decoder\".\n\nA decoder autoregressively produces an output sequence.\nTherefore, it can be used as a standard language model if the task dataset has\nonly \"targets\" features, i.e., self-supervised. If the task dataset also has an\n\"inputs\" field, e.g., supervised machine translation, the decoder can still be\nused by concatenating the inputs and targets fields. See [Raffel et al.\n(2020)](https://arxiv.org/abs/1910.10683), Section 3.2.1 for more detailed take\non this topic.\n\nWe support both uses cases and refer to the former as *standard language model*\nand the latter as *prefix language model*. Each of these models is described\nseparately below.\n\nNote that we do not provide special features to denote how the dataset should be\nconsumed. For example, a Transformer-based fully autoregressive decoder has a\nfully-causal self-attention layer. Since there are many ways of implementing the\nmasking pattern for such attention layer and, more importantly, SeqIO is not\nlimited to attention-based models, we leave it up to the model implementations\nto apply the masking pattern. There is one exception, and we cover this in\nthe Prefix LM section below.\n\nA common use pattern is to pretrain a decoder model with the left-to-right\nlanguage modeling objective (unsupervised) using `seqio.LMFeatureConverter` and\nthen fine-tune (supervised) using `seqio.PrefixLMFeatureConverter`.\n\n\n#:#:#:#:#:#: Standard LM\n\nFor the standard language model, the task dataset only has \"targets\" field.\nTherefore, the sequence length specification only needs to specify targets.\n\n```python\ndataset: tf.data.Dataset = seqio.get_dataset(\n    mixture_or_task_name=\"standard_lm\",\n    task_feature_lengths={\"targets\": 32},\n    dataset_split=\"train\",\n    shuffle=True,\n    feature_converter=seqio.LMFeatureConverter(pack=True)\n)\n```\n\nNote that \"standard_lm\" is not a registered task in the codebase. It is the\nleft-to-right language modeling task, i.e., predict the next token given the\nprevious tokens on some language corpus (e.g.,\n[C4](https://www.tensorflow.org/datasets/catalog/c4)).\n\nThe output dataset has the following model features.\n\n|Feature name          | Explanation                |\n|----------------------|---------------------------|\n|`decoder_target_tokens`| Output tokens from the decoder |\n|`decoder_input_tokens` | Input tokens to the decoder |\n|`decoder_loss_weights` | Binary mask to indicate where the loss should be taken |\n|`decoder_positions`    | Position index in the sequence before packing|\n|`decoder_segment_ids`  | Sequence membership before packing. Two positions with the same positive integer mean that they belong to the same sequence before packing. |\n\nThe `decoder_target_tokens` is a shifted version of `decoder_input_tokens` for the\nstandard teacher-forced autoregressive training.\n\n\n\n#:#:#:#:#:#: Prefix LM: `seqio.PrefixLMFeatureConverter`\n\nIf the input dataset has a notion of \"inputs\" and \"targets\", we can concatenate\nthem so that we can still use a single stack decoder. Therefore, the output only\ncontains \"targets\" just like standard LM case.\n\nWe use the same toy example for English-to-German translation task as a running\nexample:\n\n```\n{\"inputs\": \"translate English to German: That is good.\",\n \"targets\": \"Das ist gut.\"}\n```\n\nTo be consumed by the decoder-only stack, `seqio.PrefixLMFeatureConverter`\nconcatenates them form the new \"targets\". Consider 2-layer decoder architecture\nwhose activations are shown below\n\n```\n\nThat  is  good <EOS> Das ist gut <EOS>\n |    |    |    |    |   |    |   |\n u1   u2   u3   u4   u5  u6   u7  u8\n |    |    |    |    |   |    |   |\n v1   v2   v3   v4   v5  v6   v7  v8\n |    |    |    |    |   |    |   |\n<BOS> That is  good <EOS> Das ist gut\n\n```\n\nLet's us denote the first layer's activation in the `i`th position as `vi`.\nSimilarly, let `ui` denote the activation of the second layer in the `i`th\nposition.\n\n\nFor attention-based sequence models such as Transformer decoders, the\nself-attention layer is used to encode contextualized representation of the\nsequence. At a given layer, each position's representation is computed as a\nfunction of the representations of the tokens *before* its position in the\nprevious layer.\n\nReferring to the toy example, when computing `u2` with fully-causing masking, we\ndo not use `v3`. This results in a representation `u2` of the word \"is\" that\ndoes not take into account the word \"good\", which is unnecessarily limiting.\n\nFor Prefix LM, this issue is resolved by having the fully visible masking\npattern for the inputs portion only. For example, when computing `u2`, `v1`,\n`v2`, `v3`, `v4` and `v5` are all visible and taken into account. For the tokens in\nthe \"targets\" of the `Task` dataset, we use the causal masking. For example,\nwhen computing `u6`, all `vi` for `i <= 6` are taken into account but not `v7`.\n\n<details>\n  <summary>Why `v5` is included in the inputs attention pattern</summary>\n  In the same translation example, we note that when computing `u2`, the\n  activation corresponding to the position where \\<EOS\\> token was input (i.e.,\n  `v5`) was visible. This doesn't count as \"cheating\" because the model doesn't\n  see the next word \"Das\". This can provide additional context in building the\n  representation for \"good\". In this case, `u4` has the context that \"good\" is\n  the last word in the sentence.\n</details>\n\n`seqio.PrefixLMFeatureConverter` provides a feature `decoder_causal_attention`\nto encode this information. For the above example, we have\n\n\n```\ndecoder_causal_attention = [1, 1, 1, 1, 1, 0, 0, 0]\n```\n\nindicating that the non-causal attention can be applied to the first five\npositions. Note that this feature seems trivial, but for a packed dataset\nthe inputs and targets boundary are more nuanced.\n\n\nA final consideration for the prefix LM is that because we concatenate \"inputs\"\nand \"targets\", which tokens are used for the loss computation is a modeling\ndecision. For example, we can penalize the models only for the \"targets\" tokens\nor we may choose to penalize building the representation for \"inputs\" tokens.\nThis is controlled by `loss_on_targets_only` argument (defaults to `True`) to\n`seqio.PrefixLMFeatureConverter` constructor. In the above example, we would get\n\n```\ndecoder_loss_weights = [0, 0, 0, 0, 1, 1, 1, 1]\n```\n\nThis indicates that the last 4 positions are used for the loss computation.\n\nTo get the dataset with prefix LM features, we can use\n\n```python\ndataset: tf.data.Dataset = seqio.get_dataset(\n    mixture_or_task_name=\"wmt_t2t_ende_v003\",\n    task_feature_lengths={\"inputs\": 32, \"targets\": 32},\n    dataset_split=\"train\",\n    shuffle=True,\n    feature_converter=seqio.PrefixLMFeatureConverter(\n        pack=True,\n        loss_on_targets_only=True)\n)\n```\n\nThe resulting features have length 64 because it concatenates inputs and targets\neach with length 32.\n\nThe output dataset has the following model features. Note that the only\nadditional feature is `decoder_causal_attention`.\n\n|Feature name          | Explanation                |\n|----------------------|---------------------------|\n|`decoder_target_tokens`| Output tokens from the decoder |\n|`decoder_input_tokens` | Input tokens to the decoder |\n|`decoder_loss_weights` | Binary mask to indicate where the loss should be taken |\n|`decoder_positions`    | Position index in the sequence before packing|\n|`decoder_segment_ids`  | Sequence membership before packing. Two positions with the ` same positive integer mean that they belong to the same sequence before packing. |\n|`decoder_causal_attention`| Binary mask denoting which tokens are in the non-causal masking region.|\n\n\n\n\n#:#:#:#:#:#: Encoder-only architecture\nLike decoder-only architecture, this one is a single stack, but not\nautoregressive.\n\nOne notable assumption is that the inputs and targets are *aligned*, i.e., they\nhave the same sequence length and `i`th position in the targets correspond to\nthe output representation of the `i`th token in the inputs.\n\nA common model using encoder-only architecture is\n[BERT](https://arxiv.org/abs/1810.04805). We provide `Encoder` feature converter\nclass to support the Masked Language Modeling (MLM) objective from BERT.\n\nWe assume that a unique sentinel such as `[MASK]` token is used to mask some\nfraction of the input text and the task is to recover the original text.\nTherefore, the \"targets\" is naturally defined as the original text whereas\n\"inputs\" are the masked text.\n\nEncoder-only models are often used for classification tasks. In BERT, a special\ntoken `[CLS]` is prepended to the input sequence. The last layer's activation\ncorresponding to this sentinel token is the contextualized representation of the\nsequence. We assume that such \"classification\" sentinel is prepended.\n\nConsider the following example for the MLM task. The input dataset has two\nexamples, which is packed to one example. We assume that `mask_id = 9` and the\n`[CLS]` token has id of 8.\n\n```py\ndataset = [{\"inputs\": [8, 9, 9, 3, 4], \"targets\": [8, 7, 4, 3, 4]},\n           {\"inputs\": [8, 3, 9], \"targets\": [8, 3, 6]}]\n\nconverted_dataset = {\n     \"encoder_input_tokens\": [8, 9, 9, 3, 4, 1, 8, 3, 9, 1, 0],\n    \"encoder_target_tokens\": [8, 7, 4, 3, 4, 1, 8, 3, 6, 1, 0],\n      \"encoder_segment_ids\": [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0],\n        \"encoder_positions\": [0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 0],\n     \"encoder_loss_weights\": [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n}\n```\n\nNote that the packed sequence has `[CLS]` token at the beginning of each\nsequences. Also note that the loss is taken only on the masked position.\n\nTo use the pre-defined `EncoderFeatureConverter`, provide `mask_id` as an\nargument.\n\n```py\ndataset: tf.data.Dataset = seqio.get_dataset(\n    mixture_or_task_name=\"some mlm task\",\n    task_feature_lengths={\"inputs\": 32, \"targets\": 32},\n    dataset_split=\"train\",\n    shuffle=True,\n    feature_converter=seqio.EncoderFeatureConverter(\n        pack=True,\n        mask_id=9)\n)\n```\n\nThe resulting dataset object has the following 5 fields\n\n|Feature name          | Explanation                |\n|----------------------|---------------------------|\n|`encoder_input_tokens` | Input tokens to the encoder |\n|`encoder_positions`    | Position index in the sequence before packing|\n|`encoder_segment_ids`  | Sequence membership before packing. Two positions with the ` same positive integer mean that they belong to the same sequence before packing. |\n|`encoder_target_tokens`| Output tokens from the encoder |\n|`encoder_loss_weights` | Binary mask to indicate where the loss should be taken |\n\n\n#:#:#:#:#:#: Custom architectures\nFor a model architectures, you would need to create a subclass of\n`FeatureConverter` and override two methods `_convert_features` and\n`get_model_feature_lengths` to define how task features are mapped to the model\nfeatures including the length relationships. The existing feature converters\n(e.g., `seqio.EncDecFeatureConverter`) follows the same pattern. So this can be\nuseful starting point.\n\n\n\n#:#:#: `Evaluator`\n\nTODO(hwchung)\n\n#:#: Differences from `t5.data`\n\nThe original `t5` library introduced and implemented the `t5.data.Task` abstraction for specifying preprocessing and evaluation metrics for text-to-text tasks. When creating a task, users specify a source dataset of raw text, some preprocessing steps, a vocabulary for tokenization, and evaluation metrics. The fully-specified Task can then be used to pre-train or fine-tune a encoder-decoder transformer model. However, the design included many baked-in assumptions about the types of tasks users could specify.\n\nSeqIO removes some of the constraints of this abstraction:\n\n* Inputs and outputs are no longer required to be strings (e.g., it may be images or audio).\n* Architectures other than the original encoder-decoder are supported (e.g., decoder-only languaged models like GPT or encoder-only models like BERT).\n* Users can control at which stage of the pipeline offline caching occurs.\n* Users can control when and where EOS tokens are added.\n\nFurthermore, SeqIO has been made more modular with respect to the Mesh TensorFlow Transformer. This allows it to be used with other model implementations with more consistency and much less code duplication.\n\n\n#:#: Advanced Postprocessing `Task`\n\n#:#:#: TriviaQA (Closed-book, open-domain version)\nThis version of TriviaQA was introduced in [Roberts et al.\n2020](https://arxiv.org/abs/2002.08910).\n\n\n```py\nseqio.TaskRegistry.add(\n    \"trivia_qa_open\",\n    source=seqio.TfdsDataSource(\n      tfds_name=\"trivia_qa/unfiltered.nocontext:1.1.0\",\n      splits={\n          \"train\": \"train[:90%]\",\n          \"validation\": \"train[90%:]\",\n          \"test\": \"validation\"\n      }),\n    preprocessors=[\n        tqa_open_preprocessor,\n        seqio.preprocessors.tokenize,\n        seqio.preprocessors.append_eos,\n    ],\n    output_features={\n        \"inputs\": seqio.Feature(\n           seqio.SentencePieceVocabulary(\"/path/to/inputs/vocab\"),\n           add_eos=False, dtype=tf.int32\n        ),\n        \"targets\": seqio.Feature(\n           seqio.SentencePieceVocabulary(\"/path/to/targets/vocab\"),\n           add_eos=True, dtype=tf.int32\n        ),\n    },\n    postprocess_fn=tqa_open_postprocessor,\n    metric_fns=[tqa_metric])\n```\n\nIn this example, we are using the `TfdsDataSource`. We specify the name of the TriviaQA dataset in TFDS ([`\"trivia_qa\"`](https://www.tensorflow.org/datasets/catalog/trivia_qa)), the specific config that excludes the context for the open domain setting (`\"unfiltered.nocontext\"`), and the version number (`\"1.1.0\"`). We also override the default splits to match what is commonly used for the open domain setting. Specifically, we set our \"test\" split to be the TFDS \"validation\" split, and create a small pseudo-\"validation\" set by taking examples out of the TFDS \"train\" split.\n\nThe preprocessor `tqa_open_preprocessor` is defined as follows.\n\n```py\ndef trivia_qa_open(\n    dataset: tf.data.Dataset,\n    prefix:str = \"trivia_qa question: \"\n  ) -> tf.data.Dataset:\n  \"\"\"Convert TriviaQA dataset to open domain qa examples.\n\n  The function takes the trivia_qa TFDS dataset and emits examples of the\n  form:\n  {\n    \"inputs\": \"trivia_qa question: What are the names of the Olsen Twins?\"\n    \"targets\": \"Mary-Kate and Ashley\",\n    \"answers\": [\"Mary-Kate and Ashley\", \"Ashley and Mary-Kate\"]\n  }\n\n  Args:\n    dataset: a tf.data.Dataset to process.\n    prefix: str, prefix to prepend to the inputs.\n\n  Returns:\n    a tf.data.Dataset\n  \"\"\"\n  def tqa_map(ex):\n    \"\"\"Map TriviaQA example to text-to-text example.\"\"\"\n    return {\n        \"inputs\": prefix + ex[\"question\"],\n        \"targets\": ex[\"answer\"][\"value\"],\n        \"answers\": ex[\"answer\"][\"aliases\"],\n    }\n\n  return dataset.map(tqa_map, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n```\n\nOr with the `seqio.map_overdataset` decorator, we have\n\n```py\ndef trivia_qa_open(\n  dataset: tf.data.Dataset,\n  prefix: str = \"trivia_qa question: \"\n) -> tf.data.Dataset:\n\n  @seqio.map_over_dataset\n  def tqa_map(ex: Mapping[str, tf.Tensor]) -> Mapping[str, tf.Tensor]:\n    \"\"\"Map TriviaQA example to text-to-text example.\"\"\"\n    return {\n        \"inputs\": prefix + ex[\"question\"],\n        \"targets\": ex[\"answer\"][\"value\"],\n        \"answers\": ex[\"answer\"][\"aliases\"],\n    }\n\nreturn tqa_map(dataset)\n```\n\nHere we made a thin wrapper to emphasize that the function decorated by\n`seqio.map_over_dataset` takes in an instance of `tf.data.Dataset`. In practice,\nthis wrapper is not necessary.\n\n\nThe postprocessor for this example is `tqa_open_postprocessor`, which is defined\nas follows:\n\n```py\ndef tqa_open_postprocessor(output_or_target, example=None, is_target=False):\n  \"\"\"Returns output as answer, or all answers if the full example is provided.\"\"\"\n  if is_target:\n    return [a.decode(\"utf-8\") for a in example[\"answers\"]]\n  else:\n    return output_or_target.decode(\"utf-8\")\n```\n\nWhen processing the target, we ignore `output_or_target` (equivalent to `example[\"targets\"]`) since it is just selecting a single answer in `trivia_qa_open`. Instead, we extract the full list of answers from the example and convert them from bytes to text. When handling the model output, we simply convert it to text from detokenized bytes.\n\nThe metric function `tqa_metric` is defined as:\n\n```py\ndef tqa_metric(\n  targets: Sequence[Sequence[str]],\n  predictions: Sequence[str]\n) -> Mapping[str, seqio.metrics.MetricValueValue]:\n  \"\"\"Computes official TriviaQA metrics.\n\n  Args:\n    targets: list of lists of strings\n    predictions: list of strings\n\n  Returns:\n    dict with score_key: squad score across all targets and predictions\n  \"\"\"\n\n  if len(targets) != len(predictions):\n    raise ValueError(\"Number of targets and predictions must match.\")\n\n  def _normalize_answer(text):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    ",
      "technique": "Header extraction"
    }
  ]
}