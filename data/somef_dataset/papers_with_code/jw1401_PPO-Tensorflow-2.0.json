{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8816140590976245
      ],
      "excerpt": "Blueprint is the PPO algorithm develped by OpenAI (https://arxiv.org/abs/1707.06347).  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jw1401/PPO-Tensorflow-2.0",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-14T18:43:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-30T07:10:06Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.971371811444757
      ],
      "excerpt": "Deep Reinforcement Learning is a really interesting modern technology and so I decided to implement an PPO (from the family of Policy Gradient Methods) algorithm in Tensorflow 2.0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.904336314681917
      ],
      "excerpt": "For test reasons I designed four simple training environments with Unity 3D and ML-Agents. You can use this algorithm with executable Unity 3D files and in the Unity 3D Editor. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Proximal Policy Optimization with Tensorflow 2.0",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jw1401/PPO-Tensorflow-2.0/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Thu, 23 Dec 2021 05:59:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jw1401/PPO-Tensorflow-2.0/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jw1401/PPO-Tensorflow-2.0",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jw1401/PPO-Tensorflow-2.0/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "CSS",
      "HTML"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Proximal Policy Optimization (PPO) with Tensorflow 2.0",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PPO-Tensorflow-2.0",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jw1401",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jw1401/PPO-Tensorflow-2.0/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "        python main.py --runner=run-ppo --working_dir=./__WORKING_DIRS__/CartPole/ --config=CartPole.yaml\n\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "        python main.py --runner=run-ppo --working_dir=./__WORKING_DIRS__/RollerBall/ --config=RollerBall.yaml\n            \n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "         python main.py --runner=run-ppo --working_dir=./__WORKING_DIRS__/BallSorter/ --config=BallSorter.yaml\n\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "        python main.py --runner=run-ppo --working_dir=./__WORKING_DIRS__/BallSorterVisualObs/ --config=BallSorterVisualObs.yaml\n        \n6. Watch the agent learn \n\n7. Experiment with the environments \n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 24,
      "date": "Thu, 23 Dec 2021 05:59:21 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "ppo",
      "proximal-policy-optimization",
      "tensorflow2",
      "policy-gradient",
      "ppo2",
      "reinforcement-learning-algorithms"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone [PPO Repo](https://github.com/jw1401/PPO-Tensorflow-2.0) and run pip install -e in the PPO folder\n\n2. Clone [Environments Repo](https://github.com/jw1401/Environments)\n\n    Put the repos in an project-folder. You shold have following file structure.\n\n    ```\n    Project\n        |\n        Envs \n        |     \n        PPO     \n    ```\n\n3. (Optional) If you are familiar with ML-Agents you can also clone this [Repo](https://github.com/jw1401/UnitySDK-MLAgents-Environments) and run from the Unity 3D Editor.\n\n4. Set the configs in the *.yaml file that you want to use\n\n        Standard config = `__Example__.yaml` (is loaded by default if no config is specified) \n        Standard directory = __WORKING_DIRS__/__STANDARD__/__EXAMPLE__.yaml\n\n    - Set **env_name** (path + filename) to the Unity 3D executeable\n    - Set **nn_architecure** based on the environment to train (Vec Obs, Visual Obs, mixed, ...)\n    - Set **training and policy parameters** (lr, hidden sizes of network, ...)\n\n5. Run python main.py and specify --runner=run-ppo --working_dir=./path/to/your/working_dir --config=your_config.yaml\n\n\n    ",
      "technique": "Header extraction"
    }
  ]
}