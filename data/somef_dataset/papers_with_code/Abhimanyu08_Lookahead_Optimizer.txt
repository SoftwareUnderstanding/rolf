# Lookahead_Optimizer
 An implementation of the lookahead optimizer proposed by Zhang et.al in the paper "Lookahead Optimizer: k steps forward, 1 step back" (https://arxiv.org/abs/1907.08610). The code is based on the fastaiv2 library(https://github.com/fastai/fastai2). The main idea behind the paper is to first make two copies of the parameters viz. slow weights and fast weights and then wrap a vanilla optimizer in a function which linearly interpolates the slow weights in the direction where the fast weights move by letting the vanilla optimizer run for k steps. (see citation for more details). A vanilla optimizer (sgd with momentum) and it's lookahead counterpart are tested on the imagenette dataset from fastai (https://github.com/fastai/imagenette). The wandb report for the runs can be seen here -> https://app.wandb.ai/a_bhimanyu/learning/reports/Lookahead-optimizer-trials--VmlldzoxNjMyOTM (TL;DR The lookahead performs better at fitting and generalizing)
