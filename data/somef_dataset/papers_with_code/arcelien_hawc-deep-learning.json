{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1601.06759"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/arcelien/hawc-deep-learning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-13T18:21:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-11T19:50:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8232156006414938,
        0.8007593633125436
      ],
      "excerpt": "Deep learning models on HAWC simulation dataset \nPhysics Simulation Data             |  Generated by Neural Network \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139191617832102
      ],
      "excerpt": "parse_hawc.py reads in data from the $HAWC folder and generates the training and testing datasets for our experiments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9357683914528991
      ],
      "excerpt": ": generate the coordinates of the PMT by reading a XCDF file (for visualization later) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.83814721108692
      ],
      "excerpt": "For PMT data from grid hit events, we use a mapping of PMTs to specific coordinates of a 40x40 grid, defined in squaremapping.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9864423167697481
      ],
      "excerpt": "We can visualize the actual structure of the HAWC grid of PMTs. It's clear that some data is unintialized and we clean it during processing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9909062321384913
      ],
      "excerpt": "Here are some visualizations of data from our dataset, and an example of a conversion from the raw cosmic ray event data to the 40x40 image we use in our models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241276854138852,
        0.955783525422964
      ],
      "excerpt": "It is important to note that this last command will generally not work when in an anaconda enviroment so it is suggested to exit any enviroment before running the bash command. \nThis GAN is a simple MLP (Multi Layer Perceptron) model that has been trained exclusively on the gamma ray simulation.  It accepts a vector of random draws from an 8D standard spherical gaussian.  The output of this model is  rec.logNPE, log(rec.nHit), rec.nTankHit, rec.zenith, rec.azimuth, rec.coreX, rec.coreY, and rec.CxPE40. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8655284417954836
      ],
      "excerpt": "Histograms for the generated and actual distributions will be written to the paramGANplots folder in the same directory. An example of a generated histogram is shown below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.97752003206107,
        0.9792524110896306
      ],
      "excerpt": "This model trains to near completion in less than an hour on a GeForce GTX 1080 Ti. \nBelow is an comparison of the CDF between the real and fake distributions for rec.zenith. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9716929649557582
      ],
      "excerpt": "The 1D parameter GAN can be modified slightly, allowing for conditional inputs.  Along with the 8D entropy vector, another set of input parameters can be appended.  In this case, these parameters are log(SimEvent.energyTrue), SimEvent.thetaTrue, and SimEvent.phiTrue.  These values are also included as an input to the discriminator.  During training, the GAN samples the input params from the real simulation, and generates an output.  The sampled params and the generated output are passed to the discriminator.  The output of this model is as before; rec.logNPE, log(rec.nHit), rec.nTankHit, rec.zenith, rec.azimuth, rec.coreX, rec.coreY, and rec.CxPE40.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8281753277467745
      ],
      "excerpt": "The effects of the conditional inputs can be seen in the following tests.  First, we left the conditional variables free and sampled the inputs from uniform distributions.  This shows how much variation the generative model can express. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9606008816485132
      ],
      "excerpt": "We also passed input values from the simulation directly to the generative model, which illustrates just how well the model is able to capture the source distribution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9413514792083593,
        0.9018125821952107,
        0.897780769660323
      ],
      "excerpt": "Differences here, especially in the distributions with hard cutoffs, comes from a combination of using only gaussians as the input entropy source, and training time.   \nThis model was trained to near completion in less than an hour on a GTX 1080 Ti \nWe can extend the 1D GAN to the 2D domain via 2D GANs and WGANs. To increase stability, we use the WGAN model which entails simply changing the loss function. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8355219409386547
      ],
      "excerpt": "Here is an example of the current 2-channel GAN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9449345941180304
      ],
      "excerpt": "We then extend our PixelCNN model to generate a simulation event including both the charge and hit time recorded at each PMT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9122535251337256,
        0.9509844986650733,
        0.9938221371274921
      ],
      "excerpt": "Because the model converges and can generate very realistic iamges after only 3-6 epochs of training, it takes less than two hours to train this model. A small additional improvement in the metric (bits per dimension) can be realized after training for a full day, but images look equally realistic to the human eye. \nUnfortunately, PixelCNN is not a fast model for sampling, and it takes 272 seconds to generate a batch of 16 images (17 seconds per image) \nHere is an example of generated samples from PixelCNN, where the first channel is log charge, and the second is hit time (normalized). From inspection, it seems as if the PixelCNN model learns to generate a distribution of samples that is representative of the varying sparsity between hits, and the smooth falloff of charge from a specific point indicative of gamma data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9578948282805817,
        0.938837414381133,
        0.9884239163503906
      ],
      "excerpt": "We can apply a method to cache results while generating images (https://github.com/PrajitR/fast-pixel-cnn) to speed up the two channel image sampling process significantly. This new sampling technique gets an initial, signficiant speedup over the original while using the same batch size, and it scales with batch scale in near constant time - increasing the batch size does not increase sampling time. \nOn a GTX 1080 ti, it took less than 82 seconds to generate a batch of 512 images (0.16 seconds / image). Compared to the 272 seconds to generate a batch of 16 on a Teslta V100 from before (17 seconds / image), this generation technique is over 100 times faster. \nThe number of images per batch is limited by the ammount of GPU memory, and we ran out of memory when trying a batch of size 1024. On a GPU with 32 GBs of memory (new Telsa V100), it shoould be able to generate a batch of size 32/11*512 ~ 1500 in the same amount of time (0.05 seconds / image).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9530043461329566
      ],
      "excerpt": "Here is a sample of 16 generated by the fast version of PixelCNN. From inspection, the results look very close to the original. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9636356268700643
      ],
      "excerpt": "Similiar to the approach to condition a 1D GAN on labeled inputs, we can condition the samples generated by a PixelCNN model by passing in image-label pairs, where the labels will be included in the latent space and passed through the network.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9854439728769716,
        0.9364826482799409,
        0.8423989587045319
      ],
      "excerpt": "Conditioning on variables allows us to generate events corresponding to an arbitrary set of variables - we can condition on any labeled variable associcated with the events in our dataset. \nHere are visualizations of a PixelCNN model conditioned on rec.azimuth, where we cherry picked large events for clarity. Azimuth represents the angle that the cosmic ray shower crosses the grid. We see the effect clearly in the time channel, where all events with the same rec.azimuth value have the same gradient. \nOutput of a PixelCNN model conditioned on rec.azimuth = 0.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423989587045319
      ],
      "excerpt": "Output of a PixelCNN model conditioned on rec.azimuth = 3.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reproducing physics simulations on HAWC data with deep learning",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/arcelien/hawc-deep-learning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 22:02:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/arcelien/hawc-deep-learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "arcelien/hawc-deep-learning",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```shell\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh ./Miniconda3-latest-Linux-x86_64.sh -b -p\nexport PATH=\"$HOME/miniconda/bin:$PATH\"\n\nconda create --name hawc python=2.7\nsource activate hawc\nconda install cython matplotlib numpy scipy imageio pytorch torchvision cuda90 -c pytorch\npip install tensorflow-gpu==1.8.0\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8571712400644512
      ],
      "excerpt": "To run PixelCNN on the 40x40 images generated from above, run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808635265798662
      ],
      "excerpt": "cd pixel-cnn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8491972909138085
      ],
      "excerpt": "Run the fast generation with the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd .. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8331194320028589,
        0.8424800678046704
      ],
      "excerpt": "parse_hawc.py reads in data from the $HAWC folder and generates the training and testing datasets for our experiments. \nTo generate the dataset, run  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8032833601952275,
        0.8460086649917223
      ],
      "excerpt": "python parse_hawc.py --hawc-dir $HAWC --save-dir [dir to save data in] --gen layout \n: generate the specified data files \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811234604408922
      ],
      "excerpt": "For PMT data from grid hit events, we use a mapping of PMTs to specific coordinates of a 40x40 grid, defined in squaremapping.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874045737975337
      ],
      "excerpt": "<img src=\"./plots/pmt_vis_1.png\" width=\"400px\"/> <img src=\"./plots/pmt_vis_2.png\" width=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874045737975337,
        0.874045737975337,
        0.8540118600433757
      ],
      "excerpt": "<img src=\"./plots/ground_truth/single_ground_truth.png\" width=\"400px\"/> <img src=\"./plots/ground_truth/single_ground_truth_pmts.png\" width=\"400px\"/> \n<img src=\"./plots/ground_truth/ground_truth_grid.png\" width=\"800px\"/> \nTo visualize a 2 channel image (must have time as the sceond channel) as a video, use visualize.py. First load and reference a .npy file containing the image as a numpy array. If multiple images are in the array, specify the image that is to be converted. Then create folder in same directory level called \"imgs\". Run visualize.py, which should fill \"imgs\" with around 100 images of the event. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.887760704453646
      ],
      "excerpt": "ffmpeg -start_number 0 -i imgs/img%00d.jpg -vcodec mpeg4 test.mp4 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8224904722482199
      ],
      "excerpt": "To run param-gen/parameterGAN.py, run gen_gamma_params(\"/path/to/gamma\") in parse_hawc and specify paramters to collect in the function. Then run in param-gen/Vanilla 1DGAN/:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python parameterGAN.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874045737975337
      ],
      "excerpt": "<img src=\"./plots/1DGAN/1Dhist.png\" width=\"600px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874045737975337
      ],
      "excerpt": "<img src=\"./plots/1DGAN/cdf_dist_zenith.png\" width=\"600px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python parameterCGAN.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471249509282781
      ],
      "excerpt": "<img src=\"./plots/1DGAN/1dgan_uniform labels.png\" width=\"600px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874045737975337
      ],
      "excerpt": "<img src=\"./plots/1DGAN/1dgan_reallabels.png\" width=\"600px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785,
        0.8443391904502446
      ],
      "excerpt": "python train.py \nto train the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8626338829305042
      ],
      "excerpt": "<img src=\"./plots/2DGAN/2dgan_2channel.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8218541636336627
      ],
      "excerpt": "python train.py --save_dir [dir to save pixel-cnn output] --data_dir [where processed data was saved to] --save_interval 3 --dataset [hawc1 or hawc2] (--nosample if no matplotlib) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9074401260433635
      ],
      "excerpt": "python plot.py --num [epoch number of checkpoint] --chs [1 or 2] --data-path [dir of processed HAWC data (layout.npy)] --save-path [dir of pixel-cnn output] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874045737975337
      ],
      "excerpt": "<img src=\"./plots/pixelcnn/pixelcnn_grid.png\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023718011150058
      ],
      "excerpt": "python generate.py --checkpoint [PixelCNN save directory]/params_hawc2.ckpt --save_dir [location to save images] --batch-size [max size that fits on GPU] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8931515437939825
      ],
      "excerpt": "python plot.py --num [iteration number to view] --chs 2 --data-path [dir of processed HAWC data] --save-path [save dir from above] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874045737975337
      ],
      "excerpt": "<img src=\"./plots/pixelcnn/pixelcnn_grid_fast.png\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8797956780636391
      ],
      "excerpt": "Run the model by passing in an additonal command line argument of -c to the PixelCNN train file. See pixel-cnn/train-cond.sh for an example. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874045737975337
      ],
      "excerpt": "<img src=\"./plots/pixelcnn/pixelcnn_grid_azimuth_0.png\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/arcelien/hawc-deep-learning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "HAWC Deep Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "hawc-deep-learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "arcelien",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/arcelien/hawc-deep-learning/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- A HAWC simulation dataset should be downloaded and placed in `$HAWC`\n- XCDF from https://github.com/jimbraun/XCDF should be complied to `$XCDF`\n    - contents of the compiled `$XCDF/lib/` folder should be placed in the root directory, or link in `$LD_LIBRARY_PATH`\n    - Make sure you're using python 2 and cython 2 with XCDF\n\nTo download this repository, run \n```shell\ngit clone --recurse-submodules https://github.com/arcelien/hawc-deep-learning.git\ncd hawc-deep-learning\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Thu, 23 Dec 2021 22:02:16 GMT"
    },
    "technique": "GitHub API"
  }
}