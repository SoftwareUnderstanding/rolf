{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Part of code was adapted from the following projects:\n\n- https://github.com/keithito/tacotron\n- https://github.com/facebookresearch/fairseq-py\n\nBanner and logo created by [@jraulhernandezi](https://github.com/jraulhernandezi) ([#76](https://github.com/r9y9/deepvoice3_pytorch/issues/76))\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1710.07654",
      "https://arxiv.org/abs/1710.08969",
      "https://arxiv.org/abs/1710.08969",
      "https://arxiv.org/abs/1710.08969",
      "https://arxiv.org/abs/1710.07654](https://arxiv.org/abs/1710.07654): Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning.\n2. [https://arxiv.org/abs/1710.08969](https://arxiv.org/abs/1710.08969): Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention.\n\nAudio samples are available at https://r9y9.github.io/deepvoice3_pytorch/.\n\n## Folks\n\n- https://github.com/hash2430/dv3_world: DeepVoice3 with WORLD vocoder support. [#166](https://github.com/r9y9/deepvoice3_pytorch/issues/166)\n\n## Online TTS demo\n\nNotebooks supposed to be executed on https://colab.research.google.com are available:\n\n- [DeepVoice3: Multi-speaker text-to-speech demo](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/DeepVoice3_multi_speaker_TTS_en_demo.ipynb)\n- [DeepVoice3: Single-speaker text-to-speech demo](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/DeepVoice3_single_speaker_TTS_en_demo.ipynb)\n\n## Highlights\n\n- Convolutional sequence-to-sequence model with attention for text-to-speech synthesis\n- Multi-speaker and single speaker versions of DeepVoice3\n- Audio samples and pre-trained models\n- Preprocessor for [LJSpeech (en)](https://keithito.com/LJ-Speech-Dataset/), [JSUT (jp)](https://sites.google.com/site/shinnosuketakamichi/publication/jsut) and [VCTK](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html) datasets, as well as [carpedm20/multi-speaker-tacotron-tensorflow](https://github.com/carpedm20/multi-Speaker-tacotron-tensorflow) compatible custom dataset (in JSON format)\n- Language-dependent frontend text processor for English and Japanese\n\n### Samples\n\n- [Ja Step000380000 Predicted](https://soundcloud.com/user-623907374/ja-step000380000-predicted)\n- [Ja Step000370000 Predicted](https://soundcloud.com/user-623907374/ja-step000370000-predicted)\n- [Ko_single Step000410000 Predicted](https://soundcloud.com/user-623907374/ko-step000410000-predicted)\n- [Ko_single Step000400000 Predicted](https://soundcloud.com/user-623907374/ko-step000400000-predicted)\n- [Ko_multi Step001680000 Predicted](https://soundcloud.com/user-623907374/step001680000-predicted)\n- [Ko_multi Step001700000 Predicted](https://soundcloud.com/user-623907374/step001700000-predicted)\n\n## Pretrained models\n\n**NOTE**: pretrained models are not compatible to master. To be updated soon.\n\n | URL | Model      | Data     | Hyper paramters                                  | Git commit | Steps  |\n |-----|------------|----------|--------------------------------------------------|----------------------|--------|\n | [link](https://www.dropbox.com/s/5ucl9remrwy5oeg/20180505_deepvoice3_checkpoint_step000640000.pth?dl=0) | DeepVoice3 | LJSpeech | [link](https://www.dropbox.com/s/0ck82unm0bo0rxd/20180505_deepvoice3_ljspeech.json?dl=0) | [abf0a21](https://github.com/r9y9/deepvoice3_pytorch/tree/abf0a21f83aeb451b918f867bc23378f1e2e608b)| 640k |\n |  [link](https://www.dropbox.com/s/1y8bt6bnggbzzlp/20171129_nyanko_checkpoint_step000585000.pth?dl=0)   | Nyanko     | LJSpeech | `builder=nyanko,preset=nyanko_ljspeech`     | [ba59dc7](https://github.com/r9y9/deepvoice3_pytorch/tree/ba59dc75374ca3189281f6028201c15066830116) | 585k |\n  |  [link](https://www.dropbox.com/s/uzmtzgcedyu531k/20171222_deepvoice3_vctk108_checkpoint_step000300000.pth?dl=0)   | Multi-speaker DeepVoice3     | VCTK | `builder=deepvoice3_multispeaker,preset=deepvoice3_vctk`     | [0421749](https://github.com/r9y9/deepvoice3_pytorch/tree/0421749af908905d181f089f06956fddd0982d47) | 300k + 300k |\n\nTo use pre-trained models, it's highly recommended that you are on the **specific git commit** noted above. i.e.,\n\n```\ngit checkout ${commit_hash",
      "https://arxiv.org/abs/1710.08969](https://arxiv.org/abs/1710.08969): Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention.\n\nAudio samples are available at https://r9y9.github.io/deepvoice3_pytorch/.\n\n## Folks\n\n- https://github.com/hash2430/dv3_world: DeepVoice3 with WORLD vocoder support. [#166](https://github.com/r9y9/deepvoice3_pytorch/issues/166)\n\n## Online TTS demo\n\nNotebooks supposed to be executed on https://colab.research.google.com are available:\n\n- [DeepVoice3: Multi-speaker text-to-speech demo](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/DeepVoice3_multi_speaker_TTS_en_demo.ipynb)\n- [DeepVoice3: Single-speaker text-to-speech demo](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/DeepVoice3_single_speaker_TTS_en_demo.ipynb)\n\n## Highlights\n\n- Convolutional sequence-to-sequence model with attention for text-to-speech synthesis\n- Multi-speaker and single speaker versions of DeepVoice3\n- Audio samples and pre-trained models\n- Preprocessor for [LJSpeech (en)](https://keithito.com/LJ-Speech-Dataset/), [JSUT (jp)](https://sites.google.com/site/shinnosuketakamichi/publication/jsut) and [VCTK](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html) datasets, as well as [carpedm20/multi-speaker-tacotron-tensorflow](https://github.com/carpedm20/multi-Speaker-tacotron-tensorflow) compatible custom dataset (in JSON format)\n- Language-dependent frontend text processor for English and Japanese\n\n### Samples\n\n- [Ja Step000380000 Predicted](https://soundcloud.com/user-623907374/ja-step000380000-predicted)\n- [Ja Step000370000 Predicted](https://soundcloud.com/user-623907374/ja-step000370000-predicted)\n- [Ko_single Step000410000 Predicted](https://soundcloud.com/user-623907374/ko-step000410000-predicted)\n- [Ko_single Step000400000 Predicted](https://soundcloud.com/user-623907374/ko-step000400000-predicted)\n- [Ko_multi Step001680000 Predicted](https://soundcloud.com/user-623907374/step001680000-predicted)\n- [Ko_multi Step001700000 Predicted](https://soundcloud.com/user-623907374/step001700000-predicted)\n\n## Pretrained models\n\n**NOTE**: pretrained models are not compatible to master. To be updated soon.\n\n | URL | Model      | Data     | Hyper paramters                                  | Git commit | Steps  |\n |-----|------------|----------|--------------------------------------------------|----------------------|--------|\n | [link](https://www.dropbox.com/s/5ucl9remrwy5oeg/20180505_deepvoice3_checkpoint_step000640000.pth?dl=0) | DeepVoice3 | LJSpeech | [link](https://www.dropbox.com/s/0ck82unm0bo0rxd/20180505_deepvoice3_ljspeech.json?dl=0) | [abf0a21](https://github.com/r9y9/deepvoice3_pytorch/tree/abf0a21f83aeb451b918f867bc23378f1e2e608b)| 640k |\n |  [link](https://www.dropbox.com/s/1y8bt6bnggbzzlp/20171129_nyanko_checkpoint_step000585000.pth?dl=0)   | Nyanko     | LJSpeech | `builder=nyanko,preset=nyanko_ljspeech`     | [ba59dc7](https://github.com/r9y9/deepvoice3_pytorch/tree/ba59dc75374ca3189281f6028201c15066830116) | 585k |\n  |  [link](https://www.dropbox.com/s/uzmtzgcedyu531k/20171222_deepvoice3_vctk108_checkpoint_step000300000.pth?dl=0)   | Multi-speaker DeepVoice3     | VCTK | `builder=deepvoice3_multispeaker,preset=deepvoice3_vctk`     | [0421749](https://github.com/r9y9/deepvoice3_pytorch/tree/0421749af908905d181f089f06956fddd0982d47) | 300k + 300k |\n\nTo use pre-trained models, it's highly recommended that you are on the **specific git commit** noted above. i.e.,\n\n```\ngit checkout ${commit_hash"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9105843271952955
      ],
      "excerpt": "Multi-speaker and single speaker versions of DeepVoice3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": " | link | DeepVoice3 | LJSpeech | link | abf0a21| 640k | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9449187648369126
      ],
      "excerpt": "  |  link   | Multi-speaker DeepVoice3     | VCTK | builder=deepvoice3_multispeaker,preset=deepvoice3_vctk     | 0421749 | 300k + 300k | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9519011109412632
      ],
      "excerpt": "Binary divergence (described in https://arxiv.org/abs/1710.08969) seems stabilizes training particularly for deep (> 10 layers) networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "vctk (en, multi-speaker) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "nikl_m (ko, multi-speaker) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/r9y9/deepvoice3_pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-10-31T12:31:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T13:15:43Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9443482964742551,
        0.8093809126974536,
        0.8357162263411534
      ],
      "excerpt": "PyTorch implementation of convolutional networks-based text-to-speech synthesis models: \narXiv:1710.07654: Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning. \narXiv:1710.08969: Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8821169798639457,
        0.843689071529961
      ],
      "excerpt": "Convolutional sequence-to-sequence model with attention for text-to-speech synthesis \nMulti-speaker and single speaker versions of DeepVoice3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8493057456493799,
        0.9617915134530133
      ],
      "excerpt": "Preprocessor for LJSpeech (en), JSUT (jp) and VCTK datasets, as well as carpedm20/multi-speaker-tacotron-tensorflow compatible custom dataset (in JSON format) \nLanguage-dependent frontend text processor for English and Japanese \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8791596422888195
      ],
      "excerpt": "NOTE: pretrained models are not compatible to master. To be updated soon. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9377286055226952
      ],
      "excerpt": "Then follow the \"Synthesize from a checkpoint\" section in the README of the specific git commit. Please notice that the latest development version of the repository may not work. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9775590741788959
      ],
      "excerpt": "Hyper parameters described in DeepVoice3 paper for single speaker didn't work for LJSpeech dataset, so I changed a few things. Add dilated convolution, more channels, more layers and add guided attention loss, etc. See code for details. The changes are also applied for multi-speaker model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9835086414076637
      ],
      "excerpt": "There are many hyper parameters to be turned depends on what model and data you are working on. For typical datasets and models, parameters that known to work good (preset) are provided in the repository. See presets directory for details. Notice that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903158379432229
      ],
      "excerpt": "instead of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9709162406179287,
        0.9512732920617745
      ],
      "excerpt": "When this is done, you will see extracted features (mel-spectrograms and linear spectrograms) in ./data/ljspeech. \nBuilding your own dataset, with metadata in JSON format (compatible with carpedm20/multi-speaker-tacotron-tensorflow) is currently supported. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9067218312146312
      ],
      "excerpt": "(e.g. VCTK, although this is covered in vctk_preprocess) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9391767134256334
      ],
      "excerpt": "- Prepare phoneme alignments for all utterances \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9845572437960282
      ],
      "excerpt": "Preliminary results show that while HTK/festival/merlin-based method in vctk_preprocess/prepare_vctk_labels.py works better on VCTK, Gentle is more stable with audio clips with ambient noise. (e.g. movie excerpts) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8498696426051949
      ],
      "excerpt": "Pleae check this in advance and follow the commands below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8989757042124807,
        0.8463812923621048,
        0.8148855740615636,
        0.9248475262878879
      ],
      "excerpt": "Once upon a time there was a dear little girl who was loved by every one who looked at her, but most of all by her grandmother, and there was nothing that she would not have given to the child. \nA text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. \nVCTK and NIKL are supported dataset for building a multi-speaker model. \nSince some audio samples in VCTK have long silences that affect performance, it's recommended to do phoneme alignment and remove silences according to vctk_preprocess. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8769498867570773
      ],
      "excerpt": "--speaker-id=&lt;N&gt;: It specifies what speaker of data is used for training. This should only be specified if you are using multi-speaker dataset. As for VCTK, speaker id is automatically assigned incrementally (0, 1, ..., 107) according to the speaker_info.txt in the dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8269143569341659
      ],
      "excerpt": "This may happen depending on backends you have for matplotlib. Try changing backend for matplotlib and see if it works as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9740315713888458
      ],
      "excerpt": "In #78, engiecat reported that changing the backend of matplotlib from Tkinter(TkAgg) to PyQt5(Qt5Agg) fixed the problem. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch implementation of convolutional neural networks-based text-to-speech synthesis models",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- LJSpeech (en): https://keithito.com/LJ-Speech-Dataset/\n- VCTK (en): http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html\n- JSUT (jp): https://sites.google.com/site/shinnosuketakamichi/publication/jsut\n- NIKL (ko) (**Need korean cellphone number to access it**): http://www.korean.go.kr/front/board/boardStandardView.do?board_id=4&mn_id=17&b_seq=464\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/r9y9/deepvoice3_pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 419,
      "date": "Sun, 26 Dec 2021 07:25:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/r9y9/deepvoice3_pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "r9y9/deepvoice3_pytorch",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/r9y9/deepvoice3_pytorch/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/r9y9/deepvoice3_pytorch/master/release.sh"
    ],
    "technique": "File Exploration"
  },
  "identifier": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://zenodo.org/badge/latestdoi/108992863",
      "technique": "Regular expression"
    }
  ],
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please install packages listed above first, and then\n\n```\ngit clone https://github.com/r9y9/deepvoice3_pytorch && cd deepvoice3_pytorch\npip install -e \".[bin]\"\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.805938744229563
      ],
      "excerpt": "https://github.com/hash2430/dv3_world: DeepVoice3 with WORLD vocoder support. #166 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9034127225011366,
        0.9206633769338113
      ],
      "excerpt": "To use pre-trained models, it's highly recommended that you are on the specific git commit noted above. i.e., \ngit checkout ${commit_hash} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9206633769338113
      ],
      "excerpt": "git checkout 4357976 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.840266970859276
      ],
      "excerpt": "Suppose you build a DeepVoice3-style model using LJSpeech dataset, then you can train your model by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066128589755514
      ],
      "excerpt": "Logs are dumped in ./log directory by default. You can monitor logs by tensorboard: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8967610306216423
      ],
      "excerpt": "Now that you have data prepared, then you can train a multi-speaker version of DeepVoice3 by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055802262920922
      ],
      "excerpt": "If you want to reuse learned embedding from other dataset, then you can do this instead by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8967610306216423
      ],
      "excerpt": "Now that you have data prepared, then you can train a multi-speaker version of DeepVoice3 by: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8117610934722751
      ],
      "excerpt": "Preprocessor for LJSpeech (en), JSUT (jp) and VCTK datasets, as well as carpedm20/multi-speaker-tacotron-tensorflow compatible custom dataset (in JSON format) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874742116990516
      ],
      "excerpt": ": pretrained model (20180505_deepvoice3_checkpoint_step000640000.pth) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9038375602254317
      ],
      "excerpt": "python synthesis.py --preset=20180505_deepvoice3_ljspeech.json \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9418908076833293,
        0.932615055374559
      ],
      "excerpt": "python preprocess.py --preset=presets/deepvoice3_ljspeech.json ljspeech ~/data/LJSpeech-1.0 \npython train.py --preset=presets/deepvoice3_ljspeech.json --data-root=./data/ljspeech \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9382892056106165
      ],
      "excerpt": "python preprocess.py ljspeech ~/data/LJSpeech-1.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.932615055374559
      ],
      "excerpt": "python train.py --preset=presets/deepvoice3_ljspeech.json --data-root=./data/ljspeech \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372828227895622
      ],
      "excerpt": "python preprocess.py --preset=presets/deepvoice3_ljspeech.json ljspeech ~/data/LJSpeech-1.0/ ./data/ljspeech \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088162342764607,
        0.8232609204438228,
        0.8990568378988332
      ],
      "excerpt": "You may need to modify pre-existing preset JSON file, especially n_speakers. For english multispeaker, start with presets/deepvoice3_vctk.json. \nAssuming you have dataset A (Speaker A) and dataset B (Speaker B), each described in the JSON metadata file ./datasets/datasetA/alignment.json and ./datasets/datasetB/alignment.json, then you can preprocess  data by: \npython preprocess.py json_meta \"./datasets/datasetA/alignment.json,./datasets/datasetB/alignment.json\" \"./datasets/processed_A+B\" --preset=(path to preset json file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8373101547221833
      ],
      "excerpt": "python gentle_web_align.py -w \"datasetA/wavs/*.wav\" -t \"datasetA/txts/*.txt\" --server_addr=localhost --port=8567 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8590587710032429
      ],
      "excerpt": "python gentle_web_align.py --nested-directories=\"datasetB\" --server_addr=localhost --port=8567 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.932615055374559
      ],
      "excerpt": "python train.py --preset=presets/deepvoice3_ljspeech.json --data-root=./data/ljspeech/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9418908076833293,
        0.9375117727961259
      ],
      "excerpt": "python preprocess.py nikl_s ${your_nikl_root_path} data/nikl_s --preset=presets/deepvoice3_nikls.json \npython train.py --data-root=./data/nikl_s --checkpoint-dir checkpoint_nikl_s --preset=presets/deepvoice3_nikls.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9113449520803272
      ],
      "excerpt": "Given a list of text, synthesis.py synthesize audio signals from trained model. Usage is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8565567048953707
      ],
      "excerpt": "Example test_list.txt: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9382892056106165
      ],
      "excerpt": "python preprocess.py vctk ${your_vctk_root_path} ./data/vctk \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210949072497312
      ],
      "excerpt": "python train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210949072497312
      ],
      "excerpt": "python train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9382892056106165
      ],
      "excerpt": "python preprocess.py nikl_m ${your_nikl_root_path} data/nikl_m \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210949072497312
      ],
      "excerpt": "python train.py --data-root=./data/nikl_m  --checkpoint-dir checkpoint_nikl_m \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210949072497312
      ],
      "excerpt": "python train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk_adaptation \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "MPLBACKEND=Qt5Agg python train.py ${args...} \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/r9y9/deepvoice3_pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The deepvoice3_pytorch package is licensed under the MIT \"Expat\" License:\\n\\n> Copyright (c) 2017: Ryuichi Yamamoto.\\n>\\n> Permission is hereby granted, free of charge, to any person obtaining\\n> a copy of this software and associated documentation files (the\\n> \"Software\"), to deal in the Software without restriction, including\\n> without limitation the rights to use, copy, modify, merge, publish,\\n> distribute, sublicense, and/or sell copies of the Software, and to\\n> permit persons to whom the Software is furnished to do so, subject to\\n> the following conditions:\\n>\\n> The above copyright notice and this permission notice shall be\\n> included in all copies or substantial portions of the Software.\\n>\\n> THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\\n> EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\\n> MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\\n> IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\\n> CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\n> TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\\n> SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n> # Part of code was adapted from https://github.com/facebookresearch/fairseq-py\\n> # Copyright (c) 2017-present, Facebook, Inc.\\n> # Thier licenses apply.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deepvoice3_pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepvoice3_pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "r9y9",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/r9y9/deepvoice3_pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "r9y9",
        "body": "First citable release. This should have a DOI for citation.",
        "dateCreated": "2018-10-27T07:05:37Z",
        "datePublished": "2018-10-27T07:06:49Z",
        "html_url": "https://github.com/r9y9/deepvoice3_pytorch/releases/tag/v0.1.0",
        "name": "v0.1.0 release",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/r9y9/deepvoice3_pytorch/tarball/v0.1.0",
        "url": "https://api.github.com/repos/r9y9/deepvoice3_pytorch/releases/13689200",
        "zipball_url": "https://api.github.com/repos/r9y9/deepvoice3_pytorch/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python >= 3.5\n- CUDA >= 8.0\n- PyTorch >= v1.0.0\n- [nnmnkwii](https://github.com/r9y9/nnmnkwii) >= v0.0.11\n- [MeCab](http://taku910.github.io/mecab/) (Japanese only)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1654,
      "date": "Sun, 26 Dec 2021 07:25:19 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tts",
      "speech-synthesis",
      "end-to-end",
      "speech-processing",
      "machine-learning",
      "pytorch",
      "python",
      "multi-speaker"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Notebooks supposed to be executed on https://colab.research.google.com are available:\n\n- [DeepVoice3: Multi-speaker text-to-speech demo](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/DeepVoice3_multi_speaker_TTS_en_demo.ipynb)\n- [DeepVoice3: Single-speaker text-to-speech demo](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/DeepVoice3_single_speaker_TTS_en_demo.ipynb)\n\n",
      "technique": "Header extraction"
    }
  ]
}