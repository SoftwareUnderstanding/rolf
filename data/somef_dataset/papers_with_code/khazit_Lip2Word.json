{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Advice given by my supervisor, Clement Chatelain, has been a great help in this project and I would like to thank him for his valuable and constructive suggestions.\nI\u2019m also grateful to Rob Cooper at BBC Research & Development for his help in obtaining the dataset.\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.9256381480761728
      ],
      "excerpt": "Paper : Joon Son Chung and Andrew Zisserman, \u201cLip Reading in the Wild\u201d \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8028046190715653
      ],
      "excerpt": "| Human experts           | ~30%            | -               | -                      | (years?)      | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/khazit/Lip2Word",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-02T19:22:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T12:28:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project was conducted as part of my engineering degree. The goal was to build a lip reading AI that could output words or sentences from a silent video input.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9748566772750548,
        0.9943750542493638,
        0.973995357215691
      ],
      "excerpt": "There is different ways to tackle this problem (sorted from the lowest to the highest level of abstraction) : \n  * Lip reading on the phoneme level: For every frame of the input, predict the corresponding phoneme. The classification problem is easier (only 44 different phonemes in English), but going up to a higher level to form words or sentences can be challenging : (1) a phoneme can be spread over multiple frames, (2) and some phonemes are impossible to differentiate (from a lip movement perspective, there is no way to distinguish between and \u201cp\u201d and a \u201cb\u201d for example). \n  * Lip reading on the word level: Parse the video sequence into different subsequences with each one of them containing a single word. Then classify those sequences using a predefined dictionary. This classification problem is more difficult, given that the dictionary should contain a lot of words (>100). But also because we first need to parse the input into different subsequences. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8380801347744793,
        0.8514032445586199,
        0.9871418687530608
      ],
      "excerpt": "Here I chose to work on the word level because even if a high accuracy is not achieved, the output can still be used to enhance speech recognition models. \nFor humans, adding sight of the speaker to heard speeches improves speech processing. In the same way, a lip reading AI can be used to enhance some already existing speech recognition models, especially if the audio is noisy (low quality, music in the background, etc.) \nThe dataset consists of ~1000 utterances of 500 different words, spoken by different speakers. All videos are 29 frames in length, and the word occurs in the middle of the video. The frames were cropped around the speaker\u2019s mouth and downsized to 64x64. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9576419399489611,
        0.964114175934083,
        0.8484209626657624,
        0.9707018700387451,
        0.9912489830245154,
        0.9165246266053485,
        0.9918926232827657,
        0.9772126496813793,
        0.9851699512873286
      ],
      "excerpt": "This repository contains the source code for two different architectures : \nThe Multiple Towers architecture is largely inspired by the VGG-M architecture, but adapted to a video input. A convolutional layer and a pooling layer is first applied on every frame. We then concatenate all the outputs into a single 3D matrix. We finally apply a set a convolutions/poolings (see paper for more details) \nPaper : Joon Son Chung and Andrew Zisserman, \u201cLip Reading in the Wild\u201d \nThe other model is a slightly modified Inception-v4 architecture. This model is based on several very small convolutions, grouped in \u201cblocks\u201d, in order to drastically reduce the number of parameters Here, multiple frames pass through the same layers in the \u201cstem\u201d block because of the video input. We then concatenate the output in the same way that we did with the Multiple Towers architecture. \nThe main advantage of this architecture is to allow us to have a very deep model with multiple blocks and layers without bearing the weight of a huge number of parameters. \nPaper : C.Szegedy, S.Ioffe, V.Vanhoucke, A.Alemi, \u201cInception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\u201d \nOne of the most important, but also time consuming aspect of this project was setting up a good data pipeline. Given the fact that the dataset couldn\u2019t fit in memory, the performance of the pipeline was very important : at every iteration, it needed to fetch a batch of training examples from the disk, apply preprocessing on it, data-augmentation, and finally feed it to the neural network. \nTo achieve that I chose to use Tensorflow\u2019s data input pipeline. It allow us to do everything mentioned above, but also to achieve a peak level of performance by using the CPU and GPU at the same time. As a result the data for the next step is ready before the current step has finished. \nPipelining overlaps the preprocessing and model execution of a training step. While the accelerator is performing training step N, the CPU is preparing the data for step N+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract and transform the data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060755072930152
      ],
      "excerpt": "The two networks were trained on a Nvidia GTX 1080 Ti GPU and an Intel Xeon CPU for 25 epochs or until the validation loss started increasing, whichever come first. The best results were obtained using Momentum SGD and Adam. The hyperparameters for the fine tuned models are stored in .json files (hyperparameter directory, see repo). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9037254083944254,
        0.806696664995858
      ],
      "excerpt": "The following table summarizes the results obtained and compares them with other methods. \n|                         |  Top-1 accuracy | Top-10 accuracy | Size of the model      | Training time | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.983450130794769,
        0.8708533862344704
      ],
      "excerpt": "The main goal of this project was to build an end-to-end lipreader generic enough to be used on any video. The preprocessing required to go from the input to a 64x64x29 matrix gives rise to two problems : (1) how to reduce the spacial dimension of the video, ie cropping it around the speaker's mouth, but also (2) how to reduce the temporal dimension, ie going from x numbers of frames to 29. \nThe first problem is solved by using Adam Geitgey's face recognition Python API (see lipReader.py for more interesting details). The solution to the second one is pretty straightforward : we just select 29 evenly spaced frames from the input video. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8386890569601161,
        0.9692081379879661
      ],
      "excerpt": "  * A video that represents the input that is fed to the neural network (it was used a lot during debugging). \n  * A bar graph that summarises the output of the model. For the word \"Change\" for example, the following graph is obtained: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757561202759059,
        0.8878576536057269
      ],
      "excerpt": "The results were very promising. The model is however tricky when used on videos that are poorly framed or videos with low contrast and high brightness. \nWhen tested on videos that were not part of the initial dataset (Demo video), the model did pretty good, but showed the following flaws : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936297878939982,
        0.977722876295077,
        0.8637749321834873,
        0.9342605827639687,
        0.984626984851628
      ],
      "excerpt": "  * Even though everytime, the ground truth was in the top-5 predictions, the model couldn't achieve a top-1 accuracy comparable to that of the dataset (~64% accuracy on the validation and test sets). \nHowever in every example, the model did recognize nearly all the phonemes. But it had trouble with the temporal aspect, giving a nearly equal probability to the words that contain one of those phonemes. \nThe Inception-v4 architecture achieved SOTA in both top-1 and top-10 accuracies. However the margin is small. There appears to be a plateau in the accuracy results, which can be attributed to different factors : \n  * Some words in the dataset that are nearly homophones (\u201cgroups\u201d and \u201ctroops\u201d, or \u201cground\u201d and \u201caround\u201d). \n  * The distinction between the singular and plural form is also difficult to establish (as in \u201creport\u201d and \u201creports\u201d which are considered different words in the dataset). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "End-to-end pipeline for lip reading at the word level using a tensorflow CNN implementation.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/khazit/Lip2Word/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Thu, 23 Dec 2021 23:21:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/khazit/Lip2Word/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "khazit/Lip2Word",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/khazit/Lip2Word/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Lip2Word",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "khazit",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/khazit/Lip2Word/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 19,
      "date": "Thu, 23 Dec 2021 23:21:03 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "lip-reading",
      "tensorflow",
      "deep-learning",
      "lipreading",
      "classification"
    ],
    "technique": "GitHub API"
  }
}