{
  "citation": [
    {
      "confidence": [
        0.9977994744046882,
        0.953227424668473
      ],
      "excerpt": "- Paper: https://arxiv.org/pdf/1603.08511.pdf?fbclid=IwAR1wo-0xutFu7ZurZJQwkZ4RDjxyaLaavW3A1tldAXyy8uLBprpSuvkS9Ps \n- Github Repo: https://github.com/richzhang/colorization/tree/815b3f7808f8f2d9d683e9ed6c5b0a39bec232fb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9279628677675321,
        0.8218013304355039
      ],
      "excerpt": "- https://github.com/foamliu/ImageNet-Downloader \n- http://image-net.org/download-imageurls \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hsalhab/Coloring-in-the-Deep",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-19T20:48:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-02T08:14:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This paper\u2019s objectives are to produce vibrant and realistic colorizations of grayscale photographs.  We chose this paper because it utilizes architecture we already know (CNNs) and the project seemed like an interesting one to everyone in the group. This is a supervised learning problem where  our inputs are the grey scale images and the labels are the corresponding original colored images. We hope to predict the colors for every pixel on the image, based on training our model to generate artificially coloured images that are similar to the real coloured images from a corresponding grayscale image.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9958463715295018
      ],
      "excerpt": "This project is based on the Colorful Image Colorization paper linked below. The original implementation is in Caffe, but we will try to implement this in TensorFlow. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9290481434397541
      ],
      "excerpt": "The dataset we plan to use is called imagenet. Imagenet is a dataset organized according to the WordNet hierarchy of nouns. The dataset includes 1.3 million images which fall in a range of subsets called synonym sets.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9780608951684918
      ],
      "excerpt": "- We will be truncating the dataset and using a subset of it for initial training and model development. Once the model is good-to-go, we will look into the possibility of training it on the entire dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9604065356903478,
        0.9661772180112522,
        0.9888164922770878,
        0.9773112635056123,
        0.9833701687786908,
        0.9364722441020483,
        0.9507572432164103,
        0.988881820414832,
        0.9782618413557504,
        0.9500530614209702,
        0.9635040445748938
      ],
      "excerpt": "- Our model follows a standard CNN architecture, with a few changes. It consists of 8 layer \u201cblocks\u201d, and a softmax layer at the end to get probability distributions. Each of these blocks consists of 2 to 3 repeated Convolution and ReLU activation layers, followed by a Batch Normalization layer. There are no Pooling layers used in this architecture. \n- We\u2019ll be training our model for 8000-10000 epochs on the ImageNet dataset. Basically batching and shuffling the data at each epoch, then doing a forward pass to obtain our probabilities. We will then calculate our loss and perform our backwards pass, doing gradient descent and updating the trainable parameters. \n- Calculating the model\u2019s loss is probably going to be the hardest part of implementation. Since there is no right or wrong with image colorization, it would be hard to tell our network how well it\u2019s performing during training. We could use a euclidean loss to find how \u201cdifferent\u201d the colored image is from the original one, but this is not optimal since it would favor \u201cgrayer\u201d images. Instead, we will perform a multinomial classification that uses a multinomial cross-entropy loss, where we compare the probability distribution of colours for each pixel with the true colour value using KNN. \nWe plan to conduct experiments where we show colored images to participants and ask them to identify whether the image is original or was colorized using our model. \nThe notion of accuracy does apply to our project. We are comparing the image that is colorized using our model to the original colored image. However, even if a colorized image does not look similar to the original image, the image can still look accurate with different colors.  \nThe authors of the paper were originally hoping to see how realistic the colorized image is. They did this using a \u201ccolorization Turing Test\u201d, where they showed the real image vs the colored image for real life participants to select which one was the fake image. 40 Participants were given 10 practice trials and 40 tests pairs. \nWhat is your dataset? Are there any concerns about how it was collected, or labeled? Is it representative? What kind of underlying historical or societal biases might it contain? \n- This dataset is incredibly representative for our purposes as it contains so many different types of imagery to colorize, yet recently there has been a push to rebalance the people category in imagenet. Since this September, researchers have made a valiant effort to remedy non-imageable concepts, concept vocabulary, and the diversity of images. \n- Non-imageable concepts \u2014> These include concepts that mat not necessarily be offensive, but are not suitable for image classification. An example is race. Race can be assumed by color of the skin but that is not necessarily true. Classifying someone as one race could be offensive when they are not. This creates bias. To fix this, researchers determined the imageability of the synsets and removed any with low imageability. \n- Concept Vocabulary \u2014> this involves cleansing the wordnet dataset of offensive and derogatory terms. Researchers have manually annotated unsafe and sensitive words that could insult others \n- Diversity of Images\u2014> since images were collected from search engine queries. These have shown bias in the words that may not have a gender attached to them (such as banker). To remedy this, researchers have searched in different languages, expanded their queries, and have combined multiple search engines. The filtering of non-imageable synonym sets helps out with this as well.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9677408890286143,
        0.9644298149497684,
        0.9093187971817557,
        0.9737090132482132
      ],
      "excerpt": "- The model could use wrong colors, which in certain contexts would give the image a different meaning (e.g miscoloring people\u2019s skin). We will take this into account when evaluating the success of the model. \nWe have identified three key parts to the project: \n- Image tools and preprocessing (e.i grayscale testing, grayscale conversion, and image resizing) \n- Loss function, which is a fairly complicated task since there is no intuitive way of telling how \u201cwell\u201d our model colors grayscale images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Convolutional Neural Net for colorizing gray scale images",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hsalhab/Coloring-in-the-Deep/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 03:52:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hsalhab/Coloring-in-the-Deep/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hsalhab/Coloring-in-the-Deep",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/hsalhab/Coloring-in-the-Deep/master/download.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.914176848008594
      ],
      "excerpt": "- Github Repo: https://github.com/richzhang/colorization/tree/815b3f7808f8f2d9d683e9ed6c5b0a39bec232fb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179742997827968
      ],
      "excerpt": "- https://github.com/foamliu/ImageNet-Downloader \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8170869886538761
      ],
      "excerpt": "- 1.3 million images (100 gb)  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hsalhab/Coloring-in-the-Deep/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Coloring in the Deep: Using Deep Learning for Image Colorization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Coloring-in-the-Deep",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hsalhab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hsalhab/Coloring-in-the-Deep/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 03:52:15 GMT"
    },
    "technique": "GitHub API"
  }
}