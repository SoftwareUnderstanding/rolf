{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- <https://github.com/huggingface/transformers>\n- <https://medium.com/huggingface/distilbert-8cf3380435b5>\n- <https://arxiv.org/pdf/1910.01108.pdf>\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "  --per_gpu_train_batch_size 12 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/askaydevs/distillbert-qa",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-06T05:35:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-04T21:32:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9950860090840568,
        0.9250672542407598,
        0.9794289877952984
      ],
      "excerpt": "DistilBERT is a small, fast, cheap and light Transformer model based on Bert architecture. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving 97% of BERT's performance as measured on the GLUE language understanding benchmark. DistilBERT is trained using knowledge distillation, a technique to compress a large model called the teacher into a smaller model called the student. By distillating Bert, we obtain a smaller Transformer model that bears a lot of similarities with the original BERT model while being lighter, smaller and faster to run. DistilBERT is thus an interesting option to put large-scaled trained Transformer model into production. \nTransformers - Hugging Face repository \nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9044813240702859
      ],
      "excerpt": "The SQuAD fine-tuned model is available here. Download the model by following the google drive link place the downloaded model in model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9194528285125134
      ],
      "excerpt": "You can also make predictions by using the provided demo which is deployed with Flask to handle the interactions with the UI. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of paper \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" by Victor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/askaydevs/distillbert-qa/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 19:17:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/askaydevs/distillbert-qa/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "askaydevs/distillbert-qa",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/askaydevs/distillbert-qa/master/train/DistilBERT%20QA.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you are testing this on your own machine I would recommend you do the setup in a virtual environment, as not to affect the rest of your files.\n\nIn Python3 you can set up a virtual environment with\n\n```bash\npython3 -m venv /path/to/new/virtual/environment\n```\n\nOr by installing virtualenv with pip by doing\n```bash\npip3 install virtualenv\n```\nThen creating the environment with\n```bash\nvirtualenv venv\n```\nand finally activating it with\n```bash\nsource venv/bin/activate\n```\n\nYou must have Python3\n\nInstall the requirements with:\n```bash\npip3 install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.809187830523061
      ],
      "excerpt": "You can do this with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8349509675516925
      ],
      "excerpt": "The data for SQuAD can be downloaded with the following links and should be saved in a $SQUAD_DIR directory. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8026967625304323
      ],
      "excerpt": "alternatively inside the model.py file you can specify the type of model you wish to use, the one I have provided, or a Hugging Face fine-tuned SQuAD model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_squad.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8445146240188418
      ],
      "excerpt": "  --train_file $SQUAD_DIR/train-v1.1.json \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/askaydevs/distillbert-qa/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "HTML",
      "CSS"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DistilBERT-SQuAD",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "distillbert-qa",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "askaydevs",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/askaydevs/distillbert-qa/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Mon, 27 Dec 2021 19:17:20 GMT"
    },
    "technique": "GitHub API"
  }
}