{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1710.03957",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Csaky:2019,\n    title = \"Improving Neural Conversational Models with Entropy-Based Data Filtering\",\n    author = \"Cs{\\'a}ky, Rich{\\'a}rd and Purgai, Patrik and Recski, G{\\'a}bor\",\n    booktitle = \"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P19-1567\",\n    pages = \"5650--5669\",\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ricsinaruto/dialog-eval",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-30T13:14:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T13:15:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9371948907643342
      ],
      "excerpt": "A lightweight repo for automatic evaluation of dialog models using 17 metrics. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289233470380999
      ],
      "excerpt": "  :floppy_disk: &nbsp; Metrics are saved in a pre-defined easy to process format \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8146837975225817,
        0.976789087005021,
        0.9384513283258701,
        0.9528516850607044,
        0.950290186268333,
        0.9581809976628467,
        0.9482602620914595,
        0.8155119795357714
      ],
      "excerpt": "Response length: Number of words in the response. \nPer-word entropy: Probabilities of words are calculated based on frequencies observed in the training data. Entropy at the bigram level is also computed. \nUtterance entropy: The product of per-word entropy and the response length. Also computed at the bigram level. \nKL divergence: Measures how well the word distribution of the model responses approximates the ground truth distribution. Also computed at the bigram level (with bigram distributions). \nEmbedding: Embedding average, extrema, and greedy are measured. average measure the cosine similarity between the averages of word vectors of response and target utterances. extrema constructs a representation by taking the greatest absolute value for each dimension among the word vectors in the response and target utterances and measures the cosine similarity between them. greedy matches each response token to a target token (and vica versa) based on the cosine similarity between their ebeddings and averages the total score across all words.  \nCoherence: Cosine similarity of input and response representations (constructed with the average word embedding method). \nDistinct: Distinct-1 and distinct-2 measure the ratio of unique unigrams/bigrams to the total number of unigrams/bigrams in a set of responses. \nBLEU: Measures n-gram overlap between response and target (n = [1,2,3,4]). Smoothing method can be choosen in the arguments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8385150997686607,
        0.9918116965641143
      ],
      "excerpt": "A file will be saved to the directory where the response file(s) is. The first row contains the names of the metrics, then each row contains the metrics for one file. The name of the file is followed by the individual metric values separated by spaces. Each metric consists of three numbers separated by commas: the mean, standard deviation, and confidence interval. You can set the t value of the confidence interval in the arguments, the default is for 95% confidence. \nInterestingly all 17 metrics improve until a certain point and then stagnate with no overfitting occuring during the training of a Transformer model on DailyDialog. Check the appendix of the paper for figures. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9171954371440919
      ],
      "excerpt": "TRF is the Transformer model evaluated at the validation loss minimum and TRF-O is the Transformer model evaluated after 150 epochs of training, where the metrics start stagnating. RT means randomly selected responses from the training set and GT means ground truth responses. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.884779860161828
      ],
      "excerpt": "TRF is the Transformer model, while RT means randomly selected responses from the training set and GT means ground truth responses. These results are on measured on the test set at a checkpoint where the validation loss was minimal. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.884779860161828,
        0.9308445996659249,
        0.8783145052577583
      ],
      "excerpt": "TRF is the Transformer model, while RT means randomly selected responses from the training set and GT means ground truth responses. These results are on measured on the test set at a checkpoint where the validation loss was minimal. \nNew metrics can be added by making a class for the metric, which handles the computation of the metric given data. Check BLEU metrics for an example. Normally the init function handles any data setup which is needed later, and the update_metrics updates the metrics dict using the current example from the arguments. Inside the class you should define the self.metrics dict, which stores lists of metric values for a given test file. The names of these metrics (keys of the dictionary) should also be added in the config file to self.metrics. Finally you need to add an instance of your metric class to self.objects. Here at initialization you can make use of paths to data files if your metric requires any setup. After this your metric should be automatically computed and saved.   \nHowever, you should also add some constraints to your metric, e.g. if a file required for the computation of the metric is missing the user should be notified, as here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Evaluate your dialog model with 17 metrics! (see paper)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ricsinaruto/dialog-eval/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Wed, 22 Dec 2021 18:06:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ricsinaruto/dialog-eval/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ricsinaruto/dialog-eval",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/ricsinaruto/dialog-eval/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Run this command to install required packages:\n```\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ricsinaruto/dialog-eval/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Richard Csaky\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "dialog-eval &middot;",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "dialog-eval",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ricsinaruto",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ricsinaruto/dialog-eval/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 74,
      "date": "Wed, 22 Dec 2021 18:06:43 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dialog",
      "chatbot",
      "entropy",
      "embeddings",
      "evaluation",
      "metrics",
      "conversation"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The main file can be called from anywhere, but when specifying paths to directories you should give them from the root of the repository.\n```\npython code/main.py -h\n```\n<a><img src=\"https://github.com/ricsinaruto/dialog-eval/blob/master/docs/help.png\" align=\"top\" height=\"500\" ></a>\n\nFor the complete documentation visit the [wiki](https://github.com/ricsinaruto/dialog-eval/wiki).\n\n",
      "technique": "Header extraction"
    }
  ]
}