{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1610.02391][1]\n\n\n**[PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation][2]**  \nCharles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas  \n[https://arxiv.org/abs/1610.02391][2]\n\n\n**[Saliency Maps: Learning deep features for discriminative localization][7]**  \nTianhang Zheng, Changyou Chen, Junsong Yuan, Kui Ren  \n[http://arxiv.org/abs/1812.01687][7]\n\n## 3rd Party dependancies\n\n- [Pointnet][3]\n- [Open3D][4]\n- [Python][5]\n- [Anaconda][6]\n\n[1]: https://arxiv.org/abs/1610.02391\n[2]: https://arxiv.org/pdf/1612.00593\n[3]: https://github.com/charlesq34/pointnet\n[4]: http://open3d.org/docs/index.html\n[5]: https://www.python.org/downloads/\n[6]: https://www.anaconda.com/products/individual\n[7]: http://arxiv.org/abs/1812.01687\n[8]: https://github.com/Fragjacker/Pointcloud-grad-CAM/files/5371140/Master_Thesis_paper___p-grad-CAM.pdf",
      "https://arxiv.org/abs/1610.02391][2]\n\n\n**[Saliency Maps: Learning deep features for discriminative localization][7]**  \nTianhang Zheng, Changyou Chen, Junsong Yuan, Kui Ren  \n[http://arxiv.org/abs/1812.01687][7]\n\n## 3rd Party dependancies\n\n- [Pointnet][3]\n- [Open3D][4]\n- [Python][5]\n- [Anaconda][6]\n\n[1]: https://arxiv.org/abs/1610.02391\n[2]: https://arxiv.org/pdf/1612.00593\n[3]: https://github.com/charlesq34/pointnet\n[4]: http://open3d.org/docs/index.html\n[5]: https://www.python.org/downloads/\n[6]: https://www.anaconda.com/products/individual\n[7]: http://arxiv.org/abs/1812.01687\n[8]: https://github.com/Fragjacker/Pointcloud-grad-CAM/files/5371140/Master_Thesis_paper___p-grad-CAM.pdf",
      "https://arxiv.org/abs/1610.02391\n[2]: https://arxiv.org/pdf/1612.00593\n[3]: https://github.com/charlesq34/pointnet\n[4]: http://open3d.org/docs/index.html\n[5]: https://www.python.org/downloads/\n[6]: https://www.anaconda.com/products/individual\n[7]: http://arxiv.org/abs/1812.01687\n[8]: https://github.com/Fragjacker/Pointcloud-grad-CAM/files/5371140/Master_Thesis_paper___p-grad-CAM.pdf"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**[Identifying salient regions using point cloud gradient Class Activation Mapping][8]**  \nDennis Struhs\n\n**[Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization][1]**  \nRamprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra  \n[https://arxiv.org/abs/1610.02391][1]\n\n\n**[PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation][2]**  \nCharles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas  \n[https://arxiv.org/abs/1610.02391][2]\n\n\n**[Saliency Maps: Learning deep features for discriminative localization][7]**  \nTianhang Zheng, Changyou Chen, Junsong Yuan, Kui Ren  \n[http://arxiv.org/abs/1812.01687][7]\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Fragjacker/Pointcloud-grad-CAM",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-25T11:44:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-04T11:57:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9912427081737288,
        0.9965273697635534,
        0.9918082134210392
      ],
      "excerpt": "This is the repository for pointcloud grad-CAM (p-grad-CAM)[[8]], which was originally inspired by the Gradient Class Activation Mapping (Grad-CAM)[[1]] approach. It was implemented utlizing the Pointnet[[2]] network and Tensorflow. For testing the validity of our approach it was compared to a similar algorithm, called Saliency Maps[[7]]. This algorithm was slightly rewritten to fit our testing environent and was renamed ASM, it is also included in this repository. \nThe goal of this application is to yield a similar grade of transparency for pointcloud neuronal networks as grad-CAM does for 2D image convolutional neuronal networks (CNN). Usually the user is granted no deeper insight as of why a current input yields a particular results delivered by the network. This is particularly an issue if the output does not yield the expected result. In order to increase the confidence and trust in the performance of neuronal networks it is of virtue to be able to understand the reasoning behind the networks decisions. \nWe seek to achieve this transparency by finding the salient regions of the input pointcloud in order to be able to identify the areas of interest for the underlying neuronal network. This should yield a visual representation of the important areas that were responsible for the predicion result of the network. The pointcloud data is then visualized using the Open3D[[4]] library. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Fragjacker/Pointcloud-grad-CAM/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 08:45:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Fragjacker/Pointcloud-grad-CAM/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Fragjacker/Pointcloud-grad-CAM",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the the code Python 3.5[[5]] or better and Anaconda[[6]] is required. To use this algorithm follow the next steps:\n\n1. Install Anaconda[[6]] for Python 3.5[[5]] on your system.\n\n2. Open the Anaconda prompt and `cd` into the cloned repo directory.\n\n3. Create the environment with all necessary dependencies with the following command:\n\n`conda env create -f \"..\\Pointcloud-grad-CAM\\Anaconda\\p-grad-CAM.yml\"`\n    \n4. Activate the virtual environment by typing the following command:\n\n`conda activate p-grad-CAM`\n\n5. Train the network:\n\n`python train.py`\n\n6. Run either p-grad-CAM or ASM on it:\n\n`python p_grad_CAM.py` or `python ASM.py`\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Fragjacker/Pointcloud-grad-CAM/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Dennis S.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pointcloud grad-CAM",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pointcloud-grad-CAM",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Fragjacker",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Fragjacker/Pointcloud-grad-CAM/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sat, 25 Dec 2021 08:45:04 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the the code Python 3.5[[5]] or better and Anaconda[[6]] is required. To use this algorithm follow the next steps:\n\n1. Install Anaconda[[6]] for Python 3.5[[5]] on your system.\n\n2. Open the Anaconda prompt and `cd` into the cloned repo directory.\n\n3. Create the environment with all necessary dependencies with the following command:\n\n`conda env create -f \"..\\Pointcloud-grad-CAM\\Anaconda\\p-grad-CAM.yml\"`\n    \n4. Activate the virtual environment by typing the following command:\n\n`conda activate p-grad-CAM`\n\n5. Train the network:\n\n`python train.py`\n\n6. Run either p-grad-CAM or ASM on it:\n\n`python p_grad_CAM.py` or `python ASM.py`\n\n",
      "technique": "Header extraction"
    }
  ]
}