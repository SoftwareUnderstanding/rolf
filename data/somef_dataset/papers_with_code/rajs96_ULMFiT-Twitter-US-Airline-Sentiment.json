{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1801.06146\n- \"Regularizing and Optimizing LSTM Language Models\": https://arxiv.org/abs/1708.02182\n- \"A Comparison of Pre-processing Techniques for Twitter Sentiment Analysis\": https://link.springer.com/chapter/10.1007/978-3-319-67008-9_31\n- \"How transferable are features in deep neural networks?\": https://arxiv.org/abs/1411.1792\n- \"A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay\": https://arxiv.org/abs/1803.09820\n- \"Grokking Deep Learning\" - https://www.manning.com/books/grokking-deep-learning\n- Andrew Ng's Deep Learning course: https://www.coursera.org/specializations/deep-learning\n- Stanford's Deep Learning for NLP course: https://cs224d.stanford.edu/\n- fastai documentation: https://docs.fast.ai/\n- fastai course: https://course.fast.ai/\n- Finding an optimal learning rate: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n\n\n## Some key corrections\n### ulmfit_runthrough_explanation\n- When fine-tuning the language model, when we initialize the Learner with 'language_model_learner', the comment above should read \"pass in **drop_mult=0.3**\" to specify that our dropouts for the model are with p = 0.3.\n- In the code cell under \"Getting back to the big picture, the comment should read \"as discussed before, we choose **'1e-2'** because it's slightly **smaller** than the minimum loss LR\"",
      "https://arxiv.org/abs/1708.02182\n- \"A Comparison of Pre-processing Techniques for Twitter Sentiment Analysis\": https://link.springer.com/chapter/10.1007/978-3-319-67008-9_31\n- \"How transferable are features in deep neural networks?\": https://arxiv.org/abs/1411.1792\n- \"A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay\": https://arxiv.org/abs/1803.09820\n- \"Grokking Deep Learning\" - https://www.manning.com/books/grokking-deep-learning\n- Andrew Ng's Deep Learning course: https://www.coursera.org/specializations/deep-learning\n- Stanford's Deep Learning for NLP course: https://cs224d.stanford.edu/\n- fastai documentation: https://docs.fast.ai/\n- fastai course: https://course.fast.ai/\n- Finding an optimal learning rate: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n\n\n## Some key corrections\n### ulmfit_runthrough_explanation\n- When fine-tuning the language model, when we initialize the Learner with 'language_model_learner', the comment above should read \"pass in **drop_mult=0.3**\" to specify that our dropouts for the model are with p = 0.3.\n- In the code cell under \"Getting back to the big picture, the comment should read \"as discussed before, we choose **'1e-2'** because it's slightly **smaller** than the minimum loss LR\"",
      "https://arxiv.org/abs/1411.1792\n- \"A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay\": https://arxiv.org/abs/1803.09820\n- \"Grokking Deep Learning\" - https://www.manning.com/books/grokking-deep-learning\n- Andrew Ng's Deep Learning course: https://www.coursera.org/specializations/deep-learning\n- Stanford's Deep Learning for NLP course: https://cs224d.stanford.edu/\n- fastai documentation: https://docs.fast.ai/\n- fastai course: https://course.fast.ai/\n- Finding an optimal learning rate: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n\n\n## Some key corrections\n### ulmfit_runthrough_explanation\n- When fine-tuning the language model, when we initialize the Learner with 'language_model_learner', the comment above should read \"pass in **drop_mult=0.3**\" to specify that our dropouts for the model are with p = 0.3.\n- In the code cell under \"Getting back to the big picture, the comment should read \"as discussed before, we choose **'1e-2'** because it's slightly **smaller** than the minimum loss LR\"",
      "https://arxiv.org/abs/1803.09820\n- \"Grokking Deep Learning\" - https://www.manning.com/books/grokking-deep-learning\n- Andrew Ng's Deep Learning course: https://www.coursera.org/specializations/deep-learning\n- Stanford's Deep Learning for NLP course: https://cs224d.stanford.edu/\n- fastai documentation: https://docs.fast.ai/\n- fastai course: https://course.fast.ai/\n- Finding an optimal learning rate: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n\n\n## Some key corrections\n### ulmfit_runthrough_explanation\n- When fine-tuning the language model, when we initialize the Learner with 'language_model_learner', the comment above should read \"pass in **drop_mult=0.3**\" to specify that our dropouts for the model are with p = 0.3.\n- In the code cell under \"Getting back to the big picture, the comment should read \"as discussed before, we choose **'1e-2'** because it's slightly **smaller** than the minimum loss LR\""
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- \"Universal Language Model Fine-tuning for Text Classification\": https://arxiv.org/abs/1801.06146\n- \"Regularizing and Optimizing LSTM Language Models\": https://arxiv.org/abs/1708.02182\n- \"A Comparison of Pre-processing Techniques for Twitter Sentiment Analysis\": https://link.springer.com/chapter/10.1007/978-3-319-67008-9_31\n- \"How transferable are features in deep neural networks?\": https://arxiv.org/abs/1411.1792\n- \"A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay\": https://arxiv.org/abs/1803.09820\n- \"Grokking Deep Learning\" - https://www.manning.com/books/grokking-deep-learning\n- Andrew Ng's Deep Learning course: https://www.coursera.org/specializations/deep-learning\n- Stanford's Deep Learning for NLP course: https://cs224d.stanford.edu/\n- fastai documentation: https://docs.fast.ai/\n- fastai course: https://course.fast.ai/\n- Finding an optimal learning rate: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rajs96/ULMFiT-Twitter-US-Airline-Sentiment",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-02T02:02:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-12T16:04:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.968727021241009,
        0.9486439946054281,
        0.8963580678545761,
        0.9853436551880841,
        0.8926189947475828
      ],
      "excerpt": "The truth is, there isn't one solution. You'll learn a lot from grinding through paper implementations yourself. But there are certain strategies that can make the process easier and more fun: that's what I've tried to illustrate in this repository. In particular, I'm going to walk through how I would go about interpreting a ML research method and how I'd apply it to a new dataset. \nMore specifically, we're going to apply a ULMFiT approach to predict sentiment towards an airline based on a Tweet. The ULMFiT approach was devised by Jeremy Howard and Sebastian Ruder, and it's essentially a transfer learning approach for NLP. \nThis repo focuses on the problem-solving, intuitions, and actual process of applying ULMFiT to a novel problem. \nImportant note: This repo was not created to undermine or replace the work that Howard and Ruder have done. It's simply my own interpretation/explanation of their method, which I hope will help others gain a better intuition of cutting-edge machine learning research. \nThere are three notebooks in this repo that are of interest: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A repo that provides an intuitive analysis of ULMFiT, as well as a code walkthrough on how to apply it.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rajs96/ULMFiT-Twitter-US-Airline-Sentiment/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Many people are interested in getting into machine learning research, but in practice, it can be difficult.  There are several reasons for this:\n- Reading research papers is hard.\n- It can be hard to find well-explained code implementations. Very high barrier to entry.\n- As more complex methods are used, interpretability flies out the window. We start to forget why we're doing things, which is a very bad thing.\n- Reading papers usually assumes background knowledge, which leads to the reader to another dilemma of \"what do I need to understand to understand this concept, and what resources to I need to visit to gain this understanding?\"\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "(Background_info.ipynb)\nIn this notebook, we'll talk about domain of NLP, more specifically text classification. We'll understand what the initial approaches were, and why a ULMFiT approach is better. Reading this is key to understanding why language modeling is an effective transfer learning approach for NLP.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 15:22:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rajs96/ULMFiT-Twitter-US-Airline-Sentiment/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rajs96/ULMFiT-Twitter-US-Airline-Sentiment",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rajs96/ULMFiT-Twitter-US-Airline-Sentiment/master/ulmfit_results.ipynb",
      "https://raw.githubusercontent.com/rajs96/ULMFiT-Twitter-US-Airline-Sentiment/master/Background_info.ipynb",
      "https://raw.githubusercontent.com/rajs96/ULMFiT-Twitter-US-Airline-Sentiment/master/ulmfit_runthrough_explanation.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rajs96/ULMFiT-Twitter-US-Airline-Sentiment/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Demystifying ULMFiT - A state-of-the-art transfer learning approach for NLP",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ULMFiT-Twitter-US-Airline-Sentiment",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rajs96",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rajs96/ULMFiT-Twitter-US-Airline-Sentiment/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Fri, 24 Dec 2021 15:22:00 GMT"
    },
    "technique": "GitHub API"
  }
}