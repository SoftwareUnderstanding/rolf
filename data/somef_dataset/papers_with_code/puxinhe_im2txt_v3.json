{
  "citation": [
    {
      "confidence": [
        0.8368253919629696
      ],
      "excerpt": "object recognition and detection. Our particular choice of network is the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "approximately 10 times slower. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "bazel-bin/im2txt/run_inference \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9446287103301529
      ],
      "excerpt": "  1) a person riding a surf board on a wave (p=0.017452) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/puxinhe/im2txt_v3",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "***Author:*** Chris Shallue\n\n***Pull requests and issues:*** @cshallue\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-23T10:14:03Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-25T14:20:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The *Show and Tell* model is a deep neural network that learns how to describe\nthe content of images. For example:\n\n![Example captions](g3doc/example_captions.jpg)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8219383339686415
      ],
      "excerpt": "Fine Tune the Inception v3 Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560215490858932
      ],
      "excerpt": "The Show and Tell model is an example of an encoder-decoder neural network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088566151582217,
        0.9461978048572688,
        0.9926079396880679,
        0.911946652893942
      ],
      "excerpt": "and then \"decoding\" the representation into a natural language description. \nThe image encoder is a deep convolutional neural network. This type of \nnetwork is widely used for image tasks and is currently state-of-the-art for \nobject recognition and detection. Our particular choice of network is the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254749566248652
      ],
      "excerpt": "pretrained on the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9357612130020833,
        0.8512332954073805,
        0.9429664424507224,
        0.9641784076207379,
        0.9386761851494397,
        0.970792255491975
      ],
      "excerpt": "The decoder is a long short-term memory (LSTM) network. This type of network is \ncommonly used for sequence modeling tasks such as language modeling and machine \ntranslation. In the Show and Tell model, the LSTM network is trained as a \nlanguage model conditioned on the image encoding. \nWords in the captions are represented with an embedding model. Each word in the \nvocabulary is associated with a fixed-length vector representation that is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9780919394546375
      ],
      "excerpt": "are the words of the caption and {w<sub>e</sub>s<sub>0</sub>, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9568418515367135,
        0.9612604414513911
      ],
      "excerpt": "p<sub>2</sub>, ..., p<sub>N</sub>} of the LSTM are probability \ndistributions generated by the model for the next word in the sentence. The \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9537676267200477,
        0.9943627401522747,
        0.902694072116933
      ],
      "excerpt": "correct word at each step; the negated sum of these terms is the minimization \nobjective of the model. \nDuring the first phase of training the parameters of the Inception v3 model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9804012771125524,
        0.9638642001200546,
        0.978125586769832,
        0.9864248332908727,
        0.8860838948273546,
        0.8106338529029739,
        0.8472910205360822
      ],
      "excerpt": "layer is added on top of the Inception v3 model to transform the image \nembedding into the word embedding vector space. The model is trained with \nrespect to the parameters of the word embeddings, the parameters of the layer on \ntop of Inception v3 and the parameters of the LSTM. In the second phase of \ntraining, all parameters - including the parameters of Inception v3 - are \ntrained to jointly fine-tune the image encoder and the LSTM. \nGiven a trained model and an image we use beam search to generate captions for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207034031546512,
        0.9418458368534709,
        0.9354137665088141,
        0.8541854456331047,
        0.9199822900257212
      ],
      "excerpt": "the set of sentences already generated with length t - 1 to generate a new set \nof sentences with length t. We keep only the top k candidates at each step, \nwhere the hyperparameter k is called the beam size. We have found the best \nperformance with k = 3. \nThe time required to train the Show and Tell model depends on your specific \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9141710460566589
      ],
      "excerpt": "training on a single machine with a GPU. In our experience on an NVIDIA Tesla \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944617733849807
      ],
      "excerpt": "a cluster of machines with GPUs, but that is not covered in this guide. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934376615681449
      ],
      "excerpt": "phase of training. Try it out! (See Generating Captions). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8928499119349111,
        0.8893018082567579
      ],
      "excerpt": "second training phase to jointly fine-tune the parameters of the Inception v3 \nimage submodel and the LSTM. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195906778825398,
        0.9141948214860429
      ],
      "excerpt": "improve by a small amount for a long time. We have found that it will improve \nslowly for an additional 2-2.5 million steps before it begins to overfit. This \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8761697923713465
      ],
      "excerpt": "Your trained Show and Tell model can generate captions for any JPEG image! The \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9050024509620794
      ],
      "excerpt": ": point to a directory with just a copy of a model checkpoint: in that case, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466776957618867,
        0.9745549110669556
      ],
      "excerpt": "Captions for image COCO_val2014_000000224477.jpg: \n  0) a man riding a wave on top of a surfboard . (p=0.040413) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.970696976983273
      ],
      "excerpt": "  2) a man riding a wave on a surfboard in the ocean . (p=0.005743) \n",
      "technique": "Supervised classification"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The *Show and Tell* model requires a pretrained *Inception v3* checkpoint file\nto initialize the parameters of its image encoder submodel.\n\nThis checkpoint file is provided by the\n[TensorFlow-Slim image classification library](https://github.com/tensorflow/models/tree/master/research/slim#tensorflow-slim-image-classification-library)\nwhich provides a suite of pre-trained image classification models. You can read\nmore about the models provided by the library\n[here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models).\n\n\nRun the following commands to download the *Inception v3* checkpoint.\n\n```shell\n#: Location to save the Inception v3 checkpoint.\nINCEPTION_DIR=\"${HOME}/im2txt/data\"\nmkdir -p ${INCEPTION_DIR}\n\nwget \"http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\"\ntar -xvf \"inception_v3_2016_08_28.tar.gz\" -C ${INCEPTION_DIR}\nrm \"inception_v3_2016_08_28.tar.gz\"\n```\n\nNote that the *Inception v3* checkpoint will only be used for initializing the\nparameters of the *Show and Tell* model. Once the *Show and Tell* model starts\ntraining it will save its own checkpoint files containing the values of all its\nparameters (including copies of the *Inception v3* parameters). If training is\nstopped and restarted, the parameter values will be restored from the latest\n*Show and Tell* checkpoint and the *Inception v3* checkpoint will be ignored. In\nother words, the *Inception v3* checkpoint is only used in the 0-th global step\n(initialization) of training the *Show and Tell* model.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/puxinhe/im2txt_v3/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 07:22:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/puxinhe/im2txt_v3/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "puxinhe/im2txt_v3",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/puxinhe/im2txt_v3/master/run2.sh",
      "https://raw.githubusercontent.com/puxinhe/im2txt_v3/master/run.sh",
      "https://raw.githubusercontent.com/puxinhe/im2txt_v3/master/im2txt/data/download_and_preprocess_mscoco.sh",
      "https://raw.githubusercontent.com/puxinhe/im2txt_v3/master/im2txt/data/download_and_preprocess_flickr8.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To train the model you will need to provide training data in native TFRecord\nformat. The TFRecord format consists of a set of sharded files containing\nserialized `tf.SequenceExample` protocol buffers. Each `tf.SequenceExample`\nproto contains an image (JPEG format), a caption and metadata such as the image\nid.\n\nEach caption is a list of words. During preprocessing, a dictionary is created\nthat assigns each word in the vocabulary to an integer-valued id. Each caption\nis encoded as a list of integer word ids in the `tf.SequenceExample` protos.\n\nWe have provided a script to download and preprocess the [MSCOCO](http://mscoco.org/) image captioning data set into this format. Downloading\nand preprocessing the data may take several hours depending on your network and\ncomputer speed. Please be patient.\n\nBefore running the script, ensure that your hard disk has at least 150GB of\navailable space for storing the downloaded and processed data.\n\n```shell\n#: Location to save the MSCOCO data.\nMSCOCO_DIR=\"${HOME}/im2txt/data/mscoco\"\n\n#: Build the preprocessing script.\ncd research/im2txt\nbazel build //im2txt:download_and_preprocess_mscoco\n\n#: Run the preprocessing script.\nbazel-bin/im2txt/download_and_preprocess_mscoco \"${MSCOCO_DIR}\"\n```\n\nThe final line of the output should read:\n\n```\n2016-09-01 16:47:47.296630: Finished processing all 20267 image-caption pairs in data set 'test'.\n```\n\nWhen the script finishes you will find 256 training, 4 validation and 8 testing\nfiles in `DATA_DIR`. The files will match the patterns `train-?????-of-00256`,\n`val-?????-of-00004` and `test-?????-of-00008`, respectively.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "First ensure that you have installed the following required packages:\n\n* **Bazel** ([instructions](http://bazel.io/docs/install.html))\n* **TensorFlow** 1.0 or greater ([instructions](https://www.tensorflow.org/install/))\n* **NumPy** ([instructions](http://www.scipy.org/install.html))\n* **Natural Language Toolkit (NLTK)**:\n    * First install NLTK ([instructions](http://www.nltk.org/install.html))\n    * Then install the NLTK data package \"punkt\" ([instructions](http://www.nltk.org/data.html))\n* **Unzip**\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9982473972247818
      ],
      "excerpt": "Install Required Packages \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.831808035298714
      ],
      "excerpt": "Download the Inception v3 Checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8175236367082749
      ],
      "excerpt": "cd research/im2txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8081667737070849,
        0.9442731012828861
      ],
      "excerpt": "Note that you may run out of memory if you run the evaluation script on the same \nGPU as the training script. You can run the command \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184563601384902
      ],
      "excerpt": ": Ignore GPU devices (only necessary if your GPU is currently memory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8762121807756895
      ],
      "excerpt": ": you will need to pass the checkpoint path explicitly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8175236367082749
      ],
      "excerpt": "cd research/im2txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184563601384902
      ],
      "excerpt": ": Ignore GPU devices (only necessary if your GPU is currently memory \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8483819940059046,
        0.8088743896423801
      ],
      "excerpt": "Training a Model \nInitial Training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8013989054729467
      ],
      "excerpt": "During the first phase of training the parameters of the Inception v3 model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090969936049344
      ],
      "excerpt": "K20m GPU the initial training phase takes 1-2 weeks. The second training phase \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8523251450748818
      ],
      "excerpt": "Run the training script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828127170060198
      ],
      "excerpt": ": Directory containing preprocessed MSCOCO data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8210484581844435
      ],
      "excerpt": ": Inception v3 checkpoint file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8071045472720387
      ],
      "excerpt": ": Directory to save the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8523251450748818,
        0.8387578162363503
      ],
      "excerpt": ": Run the training script. \nbazel-bin/im2txt/train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "  --train_dir=\"${MODEL_DIR}/train\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.83339440497809
      ],
      "excerpt": "GPU as the training script. You can run the command \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211888162517073
      ],
      "excerpt": ": constrained, for example, by running the training script). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "  --checkpoint_dir=\"${MODEL_DIR}/train\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779721155481044,
        0.8387578162363503
      ],
      "excerpt": ": Restart the training script with --train_inception=true. \nbazel-bin/im2txt/train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.8594142235991984
      ],
      "excerpt": "  --train_dir=\"${MODEL_DIR}/train\" \\ \n  --train_inception=true \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8278425440238237
      ],
      "excerpt": "following command line will generate captions for an image from the test set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8618030955812921
      ],
      "excerpt": ": Path to checkpoint file or a directory containing checkpoint files. Passing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8409295466762761
      ],
      "excerpt": ": point to a directory with just a copy of a model checkpoint: in that case, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211888162517073
      ],
      "excerpt": ": constrained, for example, by running the training script). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9129617675176488
      ],
      "excerpt": "Example output: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/puxinhe/im2txt_v3/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Show and Tell: A Neural Image Caption Generator",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "im2txt_v3",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "puxinhe",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/puxinhe/im2txt_v3/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 07:22:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A TensorFlow implementation of the image-to-text model described in the paper:\n\n\"Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning\nChallenge.\"\n\nOriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan.\n\n*IEEE transactions on pattern analysis and machine intelligence (2016).*\n\nFull text available at: http://arxiv.org/abs/1609.06647\n\n",
      "technique": "Header extraction"
    }
  ]
}