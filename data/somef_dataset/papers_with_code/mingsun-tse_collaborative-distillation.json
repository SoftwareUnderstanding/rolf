{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In this code we refer to the following implementations: [PytorchWCT](https://github.com/sunshineatnoon/PytorchWCT), [UniversalStyleTransfer](https://github.com/Yijunmaverick/UniversalStyleTransfer), [pytorch-AdaIN](https://github.com/naoto0804/pytorch-AdaIN), [AdaIN-style](https://github.com/xunhuang1995/AdaIN-style). Great thanks to them!\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.08436"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite this in your publication if our work helps your research. Should you have any questions, welcome to reach out to Huan Wang (wang.huan@northeastern.edu).\n\n    @inproceedings{wang2020collaborative,\n      Author = {Wang, Huan and Li, Yijun and Wang, Yuehai and Hu, Haoji and Yang, Ming-Hsuan},\n      Title = {Collaborative Distillation for Ultra-Resolution Universal Style Transfer},\n      Booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n      Year = {2020}\n    }\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{wang2020collaborative,\n  Author = {Wang, Huan and Li, Yijun and Wang, Yuehai and Hu, Haoji and Yang, Ming-Hsuan},\n  Title = {Collaborative Distillation for Ultra-Resolution Universal Style Transfer},\n  Booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  Year = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9497315756634387
      ],
      "excerpt": "<center><img src=\"UHD_stylized.jpg\" width=\"1000\" hspace=\"10\"></center> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MingSun-Tse/Collaborative-Distillation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-10T16:56:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T06:57:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9954569660867852
      ],
      "excerpt": "Official PyTorch code for our CVPR-20 poster paper \"Collaborative Distillation for Ultra-Resolution Universal Style Transfer\", where we propose a new knowledge distillation method to reduce VGG-19 filters, realizing the ultra-resolution universal style transfer on a single 12GB GPU. We focus on model compression instead of new stylization schemes. For stylization, our method builds upon WCT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8883205616414814,
        0.8215346612640883
      ],
      "excerpt": "- Since the ultra-res images can be quite large, we only place two samples in this repo. For more ultra-res contents and styles presented in our paper, please download them from this google drive. \nImage copyrights: We use the UHD images from this wallpaper website. All copyrights are attributed to them and thanks to them! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233140378470369,
        0.971479156131366
      ],
      "excerpt": "- For original WCT: Download the unpruned models (which are from the official WCT implementation). Unzip and place them under trained_models/original_wct_models. \n- For ultra-resolution WCT: We use our pruned VGG-19. The models are already in the trained_models/wct_se_16x_new (for encoders) and trained_models/wct_se_16x_new_sd (for decoders). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9925587351486407
      ],
      "excerpt": "- --pretrained_init is to indicate using base models for initialization, which are obtained by pruning the filters with the least L1-norms (see also 2017-ICLR-Filter Pruning) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[CVPR'20] PyTorch code for our paper \"Collaborative Distillation for Ultra-Resolution Universal Style Transfer\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mingsun-tse/collaborative-distillation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Sat, 25 Dec 2021 22:12:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MingSun-Tse/Collaborative-Distillation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "MingSun-Tse/Collaborative-Distillation",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8479529696138558,
        0.9685908622475755,
        0.9971319979249904
      ],
      "excerpt": "OS: Linux (Ubuntu 1404 and 1604 checked. It should be all right for most linux platforms. Windows and MacOS not checked.) \npython==3.6.9 (conda to manage environment is suggested) \nThe needed libraries are summarized in requirements.txt. Simply install them by pip install -r requirements \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9792849384495262
      ],
      "excerpt": "git clone https://github.com/MingSun-Tse/Collaborative-Distillation.git \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8324933867638727
      ],
      "excerpt": "<center><img src=\"UHD_stylized.jpg\" width=\"1000\" hspace=\"10\"></center> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8375561605451
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python WCT.py --debug --mode original \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8375561605451
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python WCT.py --debug --mode original --UHD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432291481150119
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python WCT.py --debug --mode 16x \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432291481150119
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python WCT.py --debug --mode 16x --UHD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8037047320408043
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python WCT.py --debug --mode 16x --UHD --content_size 3000 --style_size 2000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432291481150119
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python WCT.py --debug --mode 16x --UHD --picked_content_mark green_park --picked_style_mark Vincent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8673119728924031,
        0.8673119728924031,
        0.8673119728924031,
        0.8673119728924031,
        0.8673119728924031
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python main.py --mode wct_se --pretrained_init --screen --stage 5 -p wct_se_stage5 \nCUDA_VISIBLE_DEVICES=0 python main.py --mode wct_se --pretrained_init --screen --stage 4 -p wct_se_stage4 \nCUDA_VISIBLE_DEVICES=0 python main.py --mode wct_se --pretrained_init --screen --stage 3 -p wct_se_stage3 \nCUDA_VISIBLE_DEVICES=0 python main.py --mode wct_se --pretrained_init --screen --stage 2 -p wct_se_stage2 \nCUDA_VISIBLE_DEVICES=0 python main.py --mode wct_se --pretrained_init --screen --stage 1 -p wct_se_stage1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8316277241163492,
        0.8316277241163492,
        0.8316277241163492,
        0.8316277241163492,
        0.8316277241163492
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python main.py --mode wct_sd --pretrained_init --screen --lw_perc 0.01 --stage 5 -p wct_sd_stage5 --SE &lt;SE path&gt; \nCUDA_VISIBLE_DEVICES=0 python main.py --mode wct_sd --pretrained_init --screen --lw_perc 0.01 --stage 4 -p wct_sd_stage4 --SE &lt;SE path&gt; \nCUDA_VISIBLE_DEVICES=0 python main.py --mode wct_sd --pretrained_init --screen --lw_perc 0.01 --stage 3 -p wct_sd_stage3 --SE &lt;SE path&gt; \nCUDA_VISIBLE_DEVICES=0 python main.py --mode wct_sd --pretrained_init --screen --lw_perc 0.01 --stage 2 -p wct_sd_stage2 --SE &lt;SE path&gt; \nCUDA_VISIBLE_DEVICES=0 python main.py --mode wct_sd --pretrained_init --screen --lw_perc 0.01 --stage 1 -p wct_sd_stage1 --SE &lt;SE path&gt; \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MingSun-Tse/Collaborative-Distillation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Huan Wang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Collaborative-Distillation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Collaborative-Distillation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "MingSun-Tse",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MingSun-Tse/Collaborative-Distillation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 160,
      "date": "Sat, 25 Dec 2021 22:12:01 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "model-compression",
      "knowledge-distillation",
      "neural-style-transfer",
      "low-level-vision"
    ],
    "technique": "GitHub API"
  }
}