{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Paper: \u201cResidual Attention Network for Image Classification\u201d https://arxiv.org/pdf/1704.06904.pdf <br>\nAuthors: Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou <br>\nGithub: https://github.com/fwang91/residual-attention-network\n<br>\n<br>\n\nPaper: \"Deep Residual Learning for Image Recognition\" https://arxiv.org/pdf/1512.03385.pdf <br>\nPaper: \"Identity Mappings in Deep Residual Networks\" https://arxiv.org/pdf/1603.05027.pdf <br>\nAuthors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun\n<br>\n<br>\n\nGithub: https://github.com/qubvel/residual_attention_network\n- Though our final implementation is ultimately different, I was able to get a more solid understanding of the network thanks to the github user, qubvel. Thanks!\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deontaepharr/Residual-Attention-Network",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-03T01:00:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T08:15:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9894549588262489,
        0.9278805242283353
      ],
      "excerpt": "The progress thus far are preliminary results. I am still in the process of rigorous validation of the efficiency of this deep learning model  \nI came across this network while studying about Attention mechanisms and found the architecture really intriguing. After reading the paper, \"Residual Attention Network for Image Classification\", by Fei Wang et al., I put myself to task to implement it to understand the network more in-depth. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.98074394998921
      ],
      "excerpt": "To give a brief summary of the paper, a Residual Attention Network (RAN) is a convolutional neural network that incorporates both attention mechanism and residual units, a component that utilizes skip-connections to jump over 2\u20133 layers with nonlinearities (e.g. ReLU in CNNs) and batch normalizations. It's prime feature is the attention module. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.950937232849627
      ],
      "excerpt": "The RAN is built by stacking Attention Modules, which generate attention-aware features that adaptively change as layers move deeper into the network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9480259182379944
      ],
      "excerpt": "The composition of the Attention Module includes two branches: the trunk branch and the mask branch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9189034500206923,
        0.829244094732555,
        0.824363578172273
      ],
      "excerpt": "Mask Branch uses bottom-up top-down structure softly weight output features with the goal of improving trunk branch features \nBottom-Up Step: collects global information of the whole image by downsampling (i.e. max pooling) the image  \nTop-Down Step: combines global information with original feature maps by upsampling (i.e. interpolation) to keep the output size the same as the input feature map \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9885460377382358
      ],
      "excerpt": "Once the actions are completed, the features extracted from the respective branches are combined together using the team's novel Attention Residual Learning formula. This is used to train very deep Residual Attention Networks so that it can be easily scaled up to hundreds of layers without a drop in performance. Thus, increasing Attention Modules leads to consistent performance improvement, as different types of attention are captured extensively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Keras Implementation Residual Attention Network",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deontaepharr/Residual-Attention-Network/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16,
      "date": "Sat, 25 Dec 2021 17:13:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deontaepharr/Residual-Attention-Network/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "deontaepharr/Residual-Attention-Network",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/deontaepharr/Residual-Attention-Network/master/Notebooks/Residual%20Attention%20Network%20Implementation%20-%20Cats%20vs%20Dogs%20Example.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deontaepharr/Residual-Attention-Network/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Residual Attention Networks",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Residual-Attention-Network",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "deontaepharr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deontaepharr/Residual-Attention-Network/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 30,
      "date": "Sat, 25 Dec 2021 17:13:16 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "keras",
      "deep-learning",
      "neural-network",
      "residual-attention-network",
      "deep-neural-networks",
      "tensorflow",
      "convolutional-neural-networks",
      "residual-networks",
      "resnet",
      "machine-learning"
    ],
    "technique": "GitHub API"
  }
}