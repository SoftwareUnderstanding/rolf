{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1811.04533"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8670498468297771
      ],
      "excerpt": "By Kushajveer Singh (https://kushajveersingh.github.io) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8840467439138965
      ],
      "excerpt": "    f = open('save_name.txt', 'wb') \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/KushajveerSingh/Unsupervised-Parking-Lot-Detection",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-27T19:19:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-13T16:33:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Parking Space Detection is a very important task in the field of computer vision as there is a general dearth of parking spaces toady and it takes time to explore the parking spaces as parking spots start filling up. By solving this problem we can reduce the car emissions in urban centers by reducing the need for people to needlesly circle city blocks for parking. It also permits cities to carefully manage their parking supply and finally it reduces the daily stress associated with parking spaces.\n\nMy implementation is based on the methodology that with the continous advances in the field of computer vision a lot of new and more efficient and accurate models are being introduced and we should be able to use these models directly off the shelf without any fine-tuning. So I present my unsupervised method of doing parking detection.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9842447987641115,
        0.8165150805589418
      ],
      "excerpt": "This repo is the official pytorch implementation of \"Upsupervised Learning for Parking Detection\". It is based on a modular approach where you can change the blocks according to your need. \nCurrently, this repo implements the following \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9354233726206008,
        0.9659324737405302
      ],
      "excerpt": "* Color of car in a parking space \n* Pose of car in a parking space \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265597283311195,
        0.8517885111328753
      ],
      "excerpt": "Details of various modules \nHow to extend these modules \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9830881999235109,
        0.8154222631651388,
        0.9845956590289122,
        0.9878163146959951,
        0.956100329158093
      ],
      "excerpt": "Label Processing Module :- As out model is not finetuned, there are some tricks that I add to overcome these limitations \nClassification Module :- Use the processed labels/bounding_boxes to tell if that parking space is occupied or not. \nObject Detection Module :- This module is responsible for detecting the cars in an image. Why is this important? Because it is the cheapest way of getting the location of parking spaces directly from an image. This module only assumes that the images that you provide for learning the parking spaces should not contain images of cars that are not in a parking space. \nThis assumption is necessary so as to reduce the complexity of the task. If we were to remove this assumption than techniques like motion tracking would have to be used and also as we are not using fine-tuned models there would be problems with that also. Or using a count based approach, but that would significantly increase the labeling cost. \nWhy not use fine-tuned model? You can finetune your model. But in order to show the generalization power of this approach, I refrained from finetuning my model. So to overcome the limitations of pretrained models, I use the following approaches: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9149027076884826,
        0.9572173707196776,
        0.9847310335675822
      ],
      "excerpt": "* Split the image into a 3x3 grid of overlapping regions so as to get more occurate bounding boxes.  \nBy using the above two methods the need for fine-tuning is essentially removed and as a result of this you don't need any data for Parking Lot Detection thus making it an example of unsupervised learning. \nBy using this technique there is no need to do any kind of feature engineering for getting the parking spaces, as the earlier research focused on using lane markings as an indicator of parking spaces, but as you can see in the bottom right of the figure there are some cases, where there are no lane markings but they are considered as standard parking spaces. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8870878027687966
      ],
      "excerpt": "Combine bounding boxes from the multiple splits of an image with the original image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752908638147985,
        0.9419128356066154,
        0.8768904358381197
      ],
      "excerpt": "First module is responsible for the classification of patches (that we get after the Label Processing Module) as occupied or not. This is a Resnet50 with a custom head. \nSecond module is responsible for getting the color of the car in that patch. The colors include, White-0, Silver-1, Black-2, Grey-3, Blue-4, Red-5, Brown-6, Green-7, Others-8. This is a Resnet50 with a custom head. \nThird module is responsible for getting the pose of the car in the patch. There are three poses possible,front facing (0), back facing (1) or side ways (2). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.923862724501183
      ],
      "excerpt": "Refer to the model instructions on how to do inference for your images. The final output should be a dictionary such that locs[img_path] = [list of bounding boxes for that image] and the bounding boxes should be (x_min, y_min, x_max, y_max) i.e. topleft corner and bottom right corner. And the dictionary should be stored as binary pickled object as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882974374306674
      ],
      "excerpt": "    To use another model you just need to define a function that returns your model. The code for my get_color_model() is as follows \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9327350806884891,
        0.860059181823877
      ],
      "excerpt": "        #: By default the model is loaded on CPU \n        model = resnet50() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877,
        0.9949320156342584
      ],
      "excerpt": "        model.eval() \n        return model \nEfficientNet:- It can be considered as the next revolutionary model after ResNet. It shows promising results. It is officially implemented in Tensorflow, but work is going on to reproduce the training results on Imagenet in PyTorch. This model with one-eight of parameters as Resnet-152 has better top-1 accuracy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9689294823908151,
        0.9519076479317692
      ],
      "excerpt": "I have previously tested with YOLO and RefineNet models for my Object Detection Module also, and the results are similar. It is possibly due to the powerful Label Processing Module. \nNote: The only reason I used M2Det model here, was because it was a new paper at that time and I was very impressed by the technique that they discussed in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9069330364093133,
        0.9794538746704036
      ],
      "excerpt": "Pose contains the custom dataset that I used for detecting the pose of a car in an image. \nimages contains out.mp4 and test.jpg. out.mp4 is used to get the locations of parking spaces in an image and test.jpg is used to test our model for that parking space location. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192559532037754
      ],
      "excerpt": "The temporary datasets are also created in this directory. (Details for it are available further) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8578707557474552
      ],
      "excerpt": "Extras: These are the images that I included in the presentation and README. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802514250684476
      ],
      "excerpt": "m2det This if a github clone of the official repo, except the code has been converted from PyTorch0.4 to PyTorch 1.1 and all the redundant code has been removed. A custom inference script, detect_parking_spaces.py has also been created to meet my specific needs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8436863215004116,
        0.8299739219219086
      ],
      "excerpt": "Scripts for creating the models are also placed here as *_classifier.py \ninference.py The inference script I used to get the final predictions from the models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9118923290585764
      ],
      "excerpt": "model.py. Main script that contains wrapper for the complete project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Detect parking spaces from a video of the parking lot. It will then merge the detections from multiple image and from multiple scales into the final prediction.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/KushajveerSingh/Unsupervised-Parking-Lot-Detection/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Mon, 27 Dec 2021 13:01:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/KushajveerSingh/Unsupervised-Parking-Lot-Detection/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "KushajveerSingh/Unsupervised-Parking-Lot-Detection",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/KushajveerSingh/Unsupervised-Parking-Lot-Detection/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/KushajveerSingh/Unsupervised-Parking-Lot-Detection/master/training/notebooks/get_pose_dataset.ipynb",
      "https://raw.githubusercontent.com/KushajveerSingh/Unsupervised-Parking-Lot-Detection/master/training/notebooks/train_color.ipynb",
      "https://raw.githubusercontent.com/KushajveerSingh/Unsupervised-Parking-Lot-Detection/master/training/notebooks/train_pose.ipynb",
      "https://raw.githubusercontent.com/KushajveerSingh/Unsupervised-Parking-Lot-Detection/master/training/notebooks/download_color_cars_data.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/KushajveerSingh/Unsupervised-Parking-Lot-Detection/master/training/m2det/make.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8019077590812459
      ],
      "excerpt": "By Kushajveer Singh (https://kushajveersingh.github.io) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8397331684428114,
        0.8203986657686064
      ],
      "excerpt": "Directory Structure \nTesting Environment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8632133128599745
      ],
      "excerpt": "Refer to the Directory Structure for details on how to setup up your directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "    python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "    python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374923879987805
      ],
      "excerpt": "requirements.txt. The main requirements for this project include \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9143054177750861
      ],
      "excerpt": "Python OpenCV \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365,
        0.8412265219762219,
        0.8966655221085083
      ],
      "excerpt": "Python 3.7.3 \nNumpy 1.16.4 \nmatplotlib 3.1.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649682819662753,
        0.9322609392449874
      ],
      "excerpt": "imageio 2.5.0 \nPyTorch 1.1.0 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8380993936803871
      ],
      "excerpt": "<img src='docs/Extra/fe_drawback_edit.jpg' width=70%> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.812553390870241
      ],
      "excerpt": "        model.load_state_dict(torch.load('training/color_classifier.pth')) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8622910462291907,
        0.809770431024045
      ],
      "excerpt": "data folder: \ncolor and Color. These folders contain the custom dataset that I used to train my model for color detection.  I first trained on color and then fine-tuned on Color. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8203417270641279
      ],
      "excerpt": "annotations contains annotations.json which contains the label file for test.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8770921085779495
      ],
      "excerpt": "training folder: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/KushajveerSingh/Unsupervised-Parking-Lot-Detection/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "C",
      "Cuda",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Unsupervised Learning for Parking Detection",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Unsupervised-Parking-Lot-Detection",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "KushajveerSingh",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/KushajveerSingh/Unsupervised-Parking-Lot-Detection/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You have to manually download the object-detection model weights. You can download the weights from this [link](https://drive.google.com/file/d/1NM1UDdZnwHwiNDxhcP-nndaWj24m-90L/view). After downloading them place the weights in `training/m2det/weights`.\n\n`model.py` takes the following arguments. These arguments are same for the `Model` class defined in the `model.py`.\n\n| Name | default | description|\n| -- | -- | -- |\n| dir | data/temp | Name of directory to store intermediate results like detection result images|\n| make_splits | False | If true, then all your images would be split into 3x3 grid and the training would be done on those 3x3 + 1(original image) |\n| gpu | False | If True, then the training is done on GPU |\n| detection_thresh | 0.2 | The detection thresold for the object detection model, so as to detect the cars in the image |\n| detection_save | False | If True save the images with predicted bounded boxes to disk (location is `dir/detection_images`) |\n| detection_show | False | If True show the images with the predicted bounded boxes in a new window (there is some delay also) |\n| label_thresh | 0.5 | The threshold for the maximum relative overlap between two bounding boxes that is allowed |\n| label_save | False | If True save the images with the merged bounding boxes to disk (location is `dir/labels_images`)\n| label_show | False | If True show the image with the merged bounding boxes in a new window |\n\nAs an example, I already have the video and image input in the `data/images` folder. So to run my program on this data, use this command\n```\npython model.py --video-path=data/images/out.mp4 \n```\n\nThe `Model` class follows the same structure. Ideally, you don't need to change any values in the constructor. You can directly call the `Model.predict()` method\n```python\n    def predict(self, video_path, x:np.ndarray=None):\n```\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 26,
      "date": "Mon, 27 Dec 2021 13:01:14 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "object-detection",
      "deep-learning",
      "computer-vision",
      "pytorch"
    ],
    "technique": "GitHub API"
  }
}