{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Structural Defects Network (SDNET) 2018, Concrete Cracks Image Dataset, https://www.kaggle.com/aniruddhsharma/structural-defects-network-concrete-crack-images/version/1?select=Pavements\n\n[2] Very deep convolutional networks for large-scale image recognition, K. Simonyan and A. Zisserman, https://arxiv.org/pdf/1409.1556.pdf\n\n[3] Batch Normalization: accelerating deep network training by reducing internal covariate shift, S. Ioffe and C. Szegedy, https://arxiv.org/pdf/1502.03167.pdf\n\n[4] Understanding the Disharmony between Dropout and Batch Normalization by\nVariance Shift, X. Li at al,  https://arxiv.org/pdf/1801.05134.pdf\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8532197979556304
      ],
      "excerpt": "Capstone project for the Springboard Machine Learning Engineering Career Track \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simo-bat/Crack_detection",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-27T03:33:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-06T11:45:35Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "notebooks/ contains an example of data augmentation (DataAugmentation), the training of the model (ModelTraining) and the testing of the trained model (ModelTesting)\n\napp/ contains all the files to run the application: the trained model with the weights, the Flask application, the Dockerfile and the requirements file\n\ntest_images/ contains few images from the test subset that can be used to test the app\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Computer vision is used for surface defects inspection in multiple fields, like manufacturing and civil engineering. In this project, the problem of detecting cracks in a concrete surface has been tackled using a deep learning model.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9755643762719545
      ],
      "excerpt": "Capstone project for the Springboard Machine Learning Engineering Career Track \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9865526727273765,
        0.8980900260921186
      ],
      "excerpt": "The architecture used for this project is based on convolutional neural networks (CNN) and it is inspired by the many architectures reported in literature, especially VGG16 [2]. It consists of the sequence of 5 CNN blocks, the first three blocks have a convolutional layer, followed by a batch normalization, relu activation function and a max pool layer. The last two blocks have two consecutives convolutional+batch normalization+ReLu layers before the max pool. This approach allows to increase the effective receptive field, limiting the number of trainable parameters and accelerating the training [2]. \nThe stack of convolutional layers is followed by two fully connected layers, with a final softmax activation function that performs the binary classification. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9963115200681371,
        0.9776936815165526,
        0.9669484435597506,
        0.9462547342403353,
        0.8996183110588133,
        0.8502590880630383,
        0.9390550079491531,
        0.8578712281799221,
        0.9915296037445233,
        0.8360269840687711
      ],
      "excerpt": "Hyperparameters tuning is critical to optimize the performance of a neural network and is very time consuming, since there are infinite combinations of hyperparameters that can be tested. In this section, the impact of few hyperparameters is summarized, namely learning rate, padding and regularization. \nLearning rate is the first hyperparameter that has been optimized. In general, constant values do not perform very well, as shown in the figure above (dotted lines). The loss of the validation data fluctuates significantly with the default value of 1e-3 and for larger learning rates (e.g. 1e-2). A smaller learning rate (e.g. 1e-4) works better but tends to overfit: the loss on train data decreases below 0.1 and the loss on validation data is around 0.4 for few epochs, then it increases a little bit. \nBetter results can be achieved gradually decreasing the learning rate during the training. In this case, an exponential decay has been used: learning_rate=lr0 * decay_rate ^ (step/decay), with decay_rate = 0.92. Similar results were obtained using lr0 = 1e-2 and 1e-3, decay = 35, with loss on validation data stable around 0.24. A more aggressive decay (decay=100) leads to more overfitting, smaller loss on training data and bigger loss on validation data. \nAn improvement of the loss on both training and validation data was achieved using same padding in the convolutional layers. Same padding helps to keep the information from the pixels close to the edges and avoids the shrinking of the images due to convolutional layers (assuming stride=1). At the same time, it increases significantly the number of trainable parameters from 1.5M to 2.7M. \nRegularization was used to reduce the overfitting. The figure above shows the impact of L2 and Dropout regularization. The regularization was applied only to the first fully connected layer after the stack of convolutional layers: experimental results showed that regularization on convolutional layers does not improve the loss.  \nWithout regularization, blue dotted line in the figure above, the loss on training data tends to zero after 10 epochs, while the loss pn validation data continues to oscillate. Dropout regularization stabilizes the val-loss and a rate of 15% shows the lowest val-loss. L2 regularization does not really improve the loss, it is almost negligible for very small regularization factors (e.g. 1e-4) or it increases the loss for larger factors (e.g. 1e-3). \nThe network has been trained with a GPU P5000, using Adam optimizer (default values of beta_1 = 0.9, beta_2 = 0.999 and epsilon = 1e-7) and binary crossentropy loss. The learning rate has been decreased exponentially, from an initial value of 1e-3, with a decay step of 35 and decay rate of 0.92. \nAfter 10 epochs (batches of 128 images), the train loss is stable around 0.105 and the validation loss is around 0.199, which correspond to a ROC AUC of 0.992 and 0.918 respectively. \nThe training of the model is saved in notebook/ModelTraining. \nThe model has been tested on the dedicated test set, that showed a loss of 0.183, similar to the validation set. To convert the probability to class labels, an optimal threshold has been extracted from the validation set through the  expression: optimal_threshold = argmin(TruePositiveRate - (1-FalsePositiveRate)) and used on both validation and test set. The optimal threshold results in the following metrics: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9009275016200543
      ],
      "excerpt": "False positive examples show common features like stripes, granules, stains and concrete discontinuity. False negative examples show common features like very small and shallow cracks and out of focus or dark images. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simo-bat/Crack_detection/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 20:09:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simo-bat/Crack_detection/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "simo-bat/Crack_detection",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/simo-bat/Crack_detection/main/app/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/simo-bat/Crack_detection/main/notebooks/ModelTesting.ipynb",
      "https://raw.githubusercontent.com/simo-bat/Crack_detection/main/notebooks/HyperparametersDependences.ipynb",
      "https://raw.githubusercontent.com/simo-bat/Crack_detection/main/notebooks/DataAugmentation.ipynb",
      "https://raw.githubusercontent.com/simo-bat/Crack_detection/main/notebooks/ModelTraining.ipynb",
      "https://raw.githubusercontent.com/simo-bat/Crack_detection/main/notebooks/.ipynb_checkpoints/ModelTesting-checkpoint.ipynb",
      "https://raw.githubusercontent.com/simo-bat/Crack_detection/main/notebooks/.ipynb_checkpoints/FinalModelTesting-checkpoint.ipynb",
      "https://raw.githubusercontent.com/simo-bat/Crack_detection/main/notebooks/.ipynb_checkpoints/ModelTraining-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8100672557677833
      ],
      "excerpt": "SDNET2018 is an annotated dataset of concrete images with and without cracks from bridge decks, walls and pavements [1]. The pavements subset, which includes 2600 positive images (with crack) and 21700 negative images (without crack), has been used to train and test this model. First, the data have been divided into three sets, namely train (80%), validation (10%) and test (10%), then, only for the train subset, new images with cracks have been created and saved to balance the two classes. Here an example of 9 images generated using different augmentation techniques. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "|Test | 0.921 | 0.366 | 0.824 | 0.507 | 0.659 | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simo-bat/Crack_detection/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Surface cracks detection in pavements",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Crack_detection",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "simo-bat",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simo-bat/Crack_detection/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 20:09:52 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Here are a few examples of corrected classification on test data for both positive and negative examples. The title of each image indicates the actual class and the probability that the image contains a crack. Note that the optimal threshold, evaluated on validation data, is equal to 0.08 (i.e. p < 0.08 \u2192 Non-cracked, p >= 0.08 \u2192 Cracked)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Here are a few misclassified images for both false positive and false negative examples.\n\n",
      "technique": "Header extraction"
    }
  ]
}