{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1812.08466",
      "https://arxiv.org/abs/1902.08710?"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9977994744046882,
        0.9944484218006108
      ],
      "excerpt": "[Wasserstein earth-mover distance] (https://arxiv.org/pdf/1704.00028.pdf) \n[GANSynth] (https://arxiv.org/abs/1902.08710?) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonce/sonyGanFork",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-22T04:33:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-07T04:06:33Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8456968850672905
      ],
      "excerpt": "This repo is a fork of Comparing-Representations-for-Audio-Synthesis-using-GANs.  The requirements and install procedure are updated a bit from the original so that it works and is sharable through containerization.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9820804621732441
      ],
      "excerpt": "The Singularity version is for internal use (we use an image built on our university computers that run on a rack of V100s), but it is built with same libraries and packages as found in the requirements.txt and docker file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891125296658835
      ],
      "excerpt": "This section is for running in an nvidia-docker container. For running in a Singularity container (used on NUS atlas machines), see <span style=\"color:maroon\"> SINGULARITY </span> below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329327025881562
      ],
      "excerpt": "This section is for running in a Singularity container. For running in a Docker container (e.g. on your local machine), see <span style=\"color:maroon\"> DOCKER </span> below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8259273494374455
      ],
      "excerpt": "Note: you have to be in a container (Docker or Singularity) to generate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8844050038105764,
        0.8595625188215595,
        0.8890389789769589,
        0.9645552262258532
      ],
      "excerpt": "Output is written to a sub/sub/sub/sub/ folder of output_path. \nscale -gerates a bunch of files generated using latent vectors with the conditioned pitch value set. \ninterpolation - generates a bunch of files at a given pitch interpolating from one random (?) point in the latent space to another.  \nrandom - generates a bunch of files from random points in the latent space letting you get an idea of the achievable variation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8199103808926055
      ],
      "excerpt": "The --pm flag is for optionally using the paramManager metadata file format rather than the python .pt files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repo contains code for running a pytorch version of GANSynth.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonce/sonyGanFork/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n $ qsub runscripts/runtrain.pb\n #: Or for running the small test non-interactively:\n $ qsub runscripts/runtrainTEST.pb\n```\nUnfortuneately, you can't pass args to a scripit that you are submitting with qsub. Thus you will need to edit these scripts to set the output folder name and the config file you want to use.  You will also have to set the output_path, the data_path, and the att_dict_path values in the config file.\n\nYou'll notice that I use rsynch to move the data to the /scratch directory on the machine the system allocates to run the job (you don't have control over which machine that is). I normally do this to speed up the I/O between the GPU and the disk, but for sonyGAN, the preprocessing step writes the preprocessed data to the output_path which is probably on your local disk anyway, subverting any attempts to speed things up this way. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can run a notebook on atlas9 while interacting with it through a browser on your local machine. :\n```\n $ qsub -I -l select=1:mem=50GB:ncpus=10:ngpus=1 -l walltime=01:00:00 -q volta_login\n #: WAIT FOR INTERACTIVE JOB TO START, then create container running jupyter and exposing a port:\n $ singularity exec $image jupyter notebook --no-browser --port=8889 --ip=0.0.0.0\n #:Then, BACK ON YOUR LOCAL MACHINE:\n $ ssh -L 8888:volta01:8889 user_name@atlas9\n #: Then just point your browser to: http://localhost:8888\n```\n\n___\n___\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 10:30:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lonce/sonyGanFork/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lonce/sonyGanFork",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/SOM.ipynb",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/SOM-Gaussian-Demo.ipynb",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/shell_scripts/plotPickle_C.ipynb",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/shell_scripts/plotPickle_L.ipynb",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/shell_scripts/old/plotLosses.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/shell_scripts/run_test_configs_local.sh",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/shell_scripts/fad.sh",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/shell_scripts/old/stderr2data.sh",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/runscripts/C/runtrain.sh",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/runscripts/C/keeprun.sh",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/runscripts/C/runtrainTEST.sh",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/runscripts/C/runtrainTEST_pb.sh",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/runscripts/L/runtrain.sh",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/runscripts/L/keeprun.sh",
      "https://raw.githubusercontent.com/lonce/sonyGanFork/master/runscripts/L/runtrainTEST.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "0) Have an account on NUS high-performance computing. Run on the atlas machines. \n1) There is already an image built with all the libs and python packages we need. You can assign it to an environment variable in a bash shellor bash script:\n```\n   $ image=/app1/common/singularity-img/3.0.0/user_img/freesound-gpst-pytorch_1.7_ngc-20.12-py3.simg\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "0) install [nvidia-docker](https://github.com/NVIDIA/nvidia-docker)\n1) Build the image foo with tag bar:\n```\n   $ cd docker\n   $ docker image build --build-arg USER_ID=$(id -u) --build-arg GROUP_ID=$(id -g) --file Dockerfile --tag foo:bar ../\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9190911809052892
      ],
      "excerpt": "Now your can train by executing a script: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8650885208126502
      ],
      "excerpt": "You can run some test using the same dataset used by the Sony folks:  the Nsynth datasaet. Scroll down to 'Files' and grab the json/wav version of the 'Train' set (A training set with 289,205 examples). It's big, something approaching 20Gb. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9063243845912854
      ],
      "excerpt": "Run jupyter notebook, and open plotPickle.ipynb. In the notebook, set the 'infile' to be the xxx_losses.pkl file in your output directory. Then just run all the cells in the notebook. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9069579772661432,
        0.9213573783611988
      ],
      "excerpt": "$ python generate.py 2D4pt --z0 vector_19.pt --z1 vector_21.pt  --d0 11  --pm True --d1nvar=3 --d1var .1 -d output/oreilly2 \n$ python generate.py 2D4pt --z0 vector_19.pt --z1 vector_21.pt --z2 vector_88.pt --z3 vector_89.pt --d0 21 --d1 21 --d1nvar 1 --d1var 0.03 --pm True -d output/oreilly2 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lonce/sonyGanFork/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "sonyGanForked",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sonyGanFork",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lonce",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonce/sonyGanFork/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The first thing you need to do to use this code is fire up a container from the image *from the sonyGanForked directory* (don't change the first -v mounting arg). The second mounting arg needs the full path to your data directory before the colon, leave the name 'mydata' as it is. 'foo' and 'bar' are whatever name and tag you gave to your docker image.\n```\n $ docker run  --shm-size=10g --gpus \"device=0\" -it -v $(pwd):/sonyGan -v /full/path/to/datadir:/mydata --rm foo:bar\n```\n(You'll see a few warnings about depcrated calls that seem to be harmless for the time being). \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This runs a small configuration, creating output in output/outname:\n```\nrunscripts/runtrainTEST.sh  -n outname config_files/new/testconfig.json\n```\nYou will see that the script ouputs some text, the last line gives you the command to run to watch the stderr output flow (tail -f logsdir.xxxx/stderr.txt). Copy and paste it to the shell. \nDepending on your machine, it could take 20 minutes to run, but that is long enough to then generate a discernable, if noisy musical scale from nsynth data. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "I think you have to be logged in on atlas9.nus.edu.sg. This example runs the smallish test run on nsynth using mel and just a few iterations per progressive gan scale.\n```\n $ qsub -I -l select=1:mem=50GB:ncpus=10:ngpus=1 -l walltime=01:00:00 -q volta_login\n #: WAIT FOR INTERACTIVE JOB TO START, then create container:\n $ singularity exec $image bash\n #: Now you can run your python code:\n $ python train.py --restart  -n \"mytestoutdir\" -c $configfile -s 500 -l 200\n```\n(You'll have to set the ouput_path, the data_path, and the att_dict_path values in the config file first. The -n arg is the name of the folder inside the output_path where your checkpoints will be written). \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can run a notebook on atlas9 while interacting with it through a browser on your local machine. :\n```\n $ qsub -I -l select=1:mem=50GB:ncpus=10:ngpus=1 -l walltime=01:00:00 -q volta_login\n #: WAIT FOR INTERACTIVE JOB TO START, then create container running jupyter and exposing a port:\n $ singularity exec $image jupyter notebook --no-browser --port=8889 --ip=0.0.0.0\n #:Then, BACK ON YOUR LOCAL MACHINE:\n $ ssh -L 8888:volta01:8889 user_name@atlas9\n #: Then just point your browser to: http://localhost:8888\n```\n\n___\n___\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Fri, 24 Dec 2021 10:30:55 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The experiments are defined in a configuration file with JSON format.\n```\n#: This is the config file for the 'best' nsynth run in the Sony Gan paper (as far as I can tell). \n{\n\n\t#:name - of outputfolder in ouput_path for checkpoints, and generation. **THIS SHOULD BE CHANGED FOR EVERY RUN YOU DO!!**  (unless you want to start your run from the latest checkpoint here). This field should actually be provided by a flag - not stuck in a configuration file!\n    \"name\": \"myTestOut\", \n    \n    \"comments\": \"fixed alpha_configuration\",\n    \n    #: output_path - used for preprocessed data and checkpoints. These files can be **BIG** - make sure you have space whereever you put it. On NUS machines, consider using /hpctmp/user_name/foo...\n    \"output_path\": \"output\",   \n    \n    \"loaderConfig\": {\n        \"data_path\": \"/mydata/nsynth-train/audio\", #: Path to audio data. ('mydata' matches the mount point in the 'docker run' command above.)\n        \"att_dict_path\": \"/mydata/nsynth-train/examples.json\", #: Path to meta data file\n        \"filter\": [\"acoustic\"],\n        \"instrument_labels\": [\"brass\", \"flute\", \"guitar\", \"keyboard\", \"mallet\"],\n        \"shuffle\": false,\n\n        #: I *think* this is just used to filter the files used for training\n        \"attribute_list\": [\"pitch\"], #:the 'conditioning' param used for the GAN\n        \"pitch_range\": [44, 70], #: only data with conditioning param labels in this range will be used for training\n        \"load_metadata\": true,\n\n        #: not sure why this is here. Instances that match filter critera are counted in code\n        \"size\": 24521   #:the number of training examples(???)\n    },\n        \n    \"transformConfig\": {\n\t\t\"transform\": \"specgrams\", #:the 'best' performer from the sony paper\n        \"fade_out\": true,\n        \"fft_size\": 1024,\n        \"win_size\": 1024,\n        #: I think n_frames and hop_size  are essentially specifying how long the wave files are. \n        \"n_frames\": 64,\n        \"hop_size\": 256,\n        \"log\": true,\n        \"ifreq\": true,          #: instantaneous frequency (??)\n        \"sample_rate\": 16000,\n        \"audio_length\": 16000\n    },\n    \"modelConfig\": {\n        \"formatLayerType\": \"gansynth\",\n        \"ac_gan\": true,  #:if false, latent vector is not augmented with labels\n        \"downSamplingFactor\": [\n            [16, 16],\n            [8, 8],\n            [4, 4],\n            [2, 2],\n            [1, 1]\n        ],\n        \"imagefolderDataset\": true, #: ?? what does this param do?\n        \"maxIterAtScale\": [200000, 200000, 200000, 300000, 300000 ],\n        \"alphaJumpMode\": \"linear\",\n        #: alphaNJumps*alphaSizeJumps is the number of Iters it takes for alpha to go to zero. The product should typically be about half of the corresponding maxIterAtScale number above.\n        \"alphaNJumps\": [3000, 3000, 3000, 3000, 3000],\n        \"alphaSizeJumps\": [32, 32, 32, 32, 32],\n        \"transposed\": false,\n                \"depthScales\": [ \n            128,\n            64,\n            64,\n            64,\n            32\n        ],\n        \"miniBatchSize\": [12, 12, 12, 8, 8],\n        \"dimLatentVector\": 64,\n        \"perChannelNormalization\": true,\n        \"lossMode\": \"WGANGP\",\n        \"lambdaGP\": 10.0,\n        \"leakyness\": 0.02,\n        \"miniBatchStdDev\": true,\n        \"baseLearningRate\": 0.0008,\n        \"dimOutput\": 2,\n\n        #: from original AC-GAN paper nad Nistal paper\n        \"weightConditionG\": 10.0, #:in AC-GAN, weight of the classification loss applied to the generator\n        \"weightConditionD\": 10.0, #:in AC-GAN, weight of the classification loss applied to the discriminator\n        #: not sure why this is necessary....\n        \"attribKeysOrder\": {\n            \"pitch\": 0\n        },\n        \"startScale\": 0,\n        \"skipAttDfake\": []\n    }\n}\n\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "[Here](https://sites.google.com/view/audio-synthesis-with-gans/p%C3%A1gina-principal) you can listen to audios synthesized with models trained on a variety of audio representations, includeing the raw audio waveform and several time-frequency representations.\n\n",
      "technique": "Header extraction"
    }
  ]
}