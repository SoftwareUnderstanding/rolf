{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2106.01342\n\n> NB: This implementation uses [Pytorch-lightning](https://pytorch-lightning.readthedocs.io/en/latest/"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9965271881743462
      ],
      "excerpt": "Paper Reference: https://arxiv.org/abs/2106.01342 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ogunlao/saint",
    "technique": "GitHub API"
  },
  "contributor": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Ahmed A. Elhag](https://github.com/Ahmed-A-A-Elhag)\n- [Aisha Alaagib](https://github.com/AishaAlaagib)\n- [Amina Rufai](https://github.com/Aminah92)\n- [Amna Ahmed Elmustapha](https://github.com/AMNAALMGLY)\n- [Jamal Hussein](https://github.com/engmubarak48)\n- [Mohammedelfatih Salah](https://github.com/mohammedElfatihSalah)\n- [Ruba Mutasim](https://github.com/ruba128)\n- [Sewade Olaolu Ogun](https://github.com/ogunlao)\n\n(names in alphabetical order)",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-18T15:46:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-25T04:19:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9619229231973393,
        0.9917196133579851
      ],
      "excerpt": "NB: This implementation uses Pytorch-lightning for setting up experiments and Hydra for configuration. For an earlier release of this code which does not use hydra, check the branch saint-orig \nWe decided to create an implementation of saint that can work with any tabular dataset, not jsut those mentioned in the paper. This implementation can be run with any tabular data for binary or multiclass classification. Regression is not supported. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9737891997691791
      ],
      "excerpt": "Embeddings for tabular data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9676873738892562
      ],
      "excerpt": "For easy configuration, we decided to organize the code in a structured way \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9750282310660647
      ],
      "excerpt": "The datasets should live in the data directory. It is necessary to provide the absolute paths to the data folder in the data configs. Also, the datasets have to be pre-processed before running experiments. These are recommendations from the paper e.g data transforms using z-transform. Other recommendations are design decisions made by us. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713138603797757,
        0.9295174988721305,
        0.9396691410063958
      ],
      "excerpt": "It is required that categorical columns are separated from numerical columns. In particular, we track the total categorical columns in the dataframe by the number of columns. Therefore, you need split the data into categorical columns and numerical columns, compute some statistics as explained below, then merge them to form a new dataframe which can be understood by the model. Note that all categorical columns appear before the numerical columns in the data. The cls column is expected the be the first column and it is counted as a categorical column. \nCalculate the number of categorical columns (including 'cls' column) and numerical columns. Include these statistics under data_stats in the data configs of your particular dataset. \nAlso, you will need to provide the number of categories in each categorical column in the data config. These are required to build proper embeddings for the model. It should also be provided as an array in the data.data_stats.cats parameter or hard coded in the data config. Note that the cls column has 1 category and is always indicated as 1 in the array. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Unofficial Pytorch implementation of SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pretraining https://arxiv.org/abs/2106.01342",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ogunlao/saint/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sun, 26 Dec 2021 03:14:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ogunlao/saint/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ogunlao/saint",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ogunlao/saint/main/notebooks/Bank_Dataset.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Activate your virtual environment. It is advisable to use a virtual environment to setup this code.\n2. Install dependencies using the `requirements.txt` file provided\n\n```python\npip3 install -r requirements.txt \n```\n\n3. Update the config.yaml file with your hyperparameters. Alternatively, you can provide your settings on the command-line while running experiments. A good knowledge of hydra might be required.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8577094315219677
      ],
      "excerpt": "Another config directory is dedicated to house all datasets configurations. This is the data sub-directory inside the configs directory It includes hyperparameters like train, validation and test data paths and other data statistics. Samples of configs for supervised (indicated with names ending in sup) and self-supervised training (indicated with names ending in sup) are provided for bank dataset. They can be replicated for other custom datasets as well. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8903498241176446,
        0.8329912424598748
      ],
      "excerpt": "Save your processed files as train, val and test csvs in data folder.  \nA sample function named preprocess.py is included under src &gt; dataset.py that explains the preprocessing strategy. You may need to modify this function depending on the dataset. Also, tutorial notebooks are provided in notebooks folder to showcase how to preprocess custom datasets and run experiments. Look at Bank_Dataset.ipynb \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ogunlao/saint/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Sewade Ogun\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pretraining",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "saint",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ogunlao",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ogunlao/saint/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Examples\n\n1. To train saint-intersample (saint-i) model in self-supervised mode using bank dataset, run;\n\n```bash\npython main.py experiment=self-supervised \\\n    experiment.model=saint_i \\\n    data=bank_ssl \\\n    data.data_folder=/content/saint/data\n```\n\n2. To train saint model in supervised mode using bank dataset, run;\n\n```bash\npython main.py experiment=supervised \\\n    experiment.model=saint \\\n    data=bank_sup \\\n    data.data_folder=/content/saint/data\n```\n\n3. To make prediction using saint model in supervised mode using bank dataset, run;\n\n```bash\n!python saint/predict.py experiment=predict \\\n    experiment.model=saint \\\n    experiment.pretrained_checkpoint=[\"PATH_TO_SAVED_CKPT\"] \\\n    experiment.pred_sav_path=[\"PATH_TO_SAVE_PREDICTION.csv\"] \\\n    data=bank_sup \\\n    data.data_folder=/content/saint/data\n```\n\n> You may need to run some hyperparameter search to determine the best model for your task. Hydra provides this functionality out of the box with [multirun](https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Sun, 26 Dec 2021 03:14:54 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "transformer",
      "tabular-data",
      "semi-supervised-learning"
    ],
    "technique": "GitHub API"
  }
}