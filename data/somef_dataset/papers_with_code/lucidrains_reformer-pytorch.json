{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2104.09864\">rotary embeddings</a>.\n\nHowever, <a href=\"https://github.com/AranKomat\">Aran</a> has informed me that the Reformer team used axial position embeddings with great results on longer sequences.\n\nYou can turn on axial positional embedding and adjust the shape and dimension of the axial embeddings by following the instructions below.\n\n```python\nimport torch\nfrom reformer_pytorch import ReformerLM\n\nmodel = ReformerLM(\n    num_tokens= 20000,\n    dim = 1024,\n    depth = 12,\n    max_seq_len = 8192,\n    ff_chunks = 8,\n    attn_chunks = 2,\n    causal = True,\n    axial_position_emb = True,         # set this to True\n    axial_position_shape = (128, 64",
      "https://arxiv.org/abs/2002.05202}    \n}\n```\n\n```bibtex\n@misc{roy*2020efficient,\n    title   = {Efficient Content-Based Sparse Attention with Routing Transformers},\n    author  = {Aurko Roy* and Mohammad Taghi Saffar* and David Grangier and Ashish Vaswani},\n    year    = {2020},\n    url     = {https://openreview.net/forum?id=B1gjs6EtDr}\n}\n```\n\n```bibtex\n@misc{bachlechner2020rezero,\n    title   = {ReZero is All You Need: Fast Convergence at Large Depth},\n    author  = {Thomas Bachlechner and Bodhisattwa Prasad Majumder and Huanru Henry Mao and Garrison W. Cottrell and Julian McAuley},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2003.04887}\n}\n```\n\n```bibtex\n@misc{lample2019large,\n    title   = {Large Memory Layers with Product Keys},\n    author  = {Guillaume Lample and Alexandre Sablayrolles and Marc'Aurelio Ranzato and Ludovic Denoyer and Herv\u00e9 J\u00e9gou},\n    year    = {2019},\n    eprint  = {1907.05242},\n    archivePrefix = {arXiv}\n}\n```\n\n```bibtex\n@misc{bhojanapalli2020lowrank,\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\n    year    = {2020},\n    eprint  = {2002.07028}\n}\n```\n\n```bibtex\n@misc{dong2021attention,\n    title   = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth}, \n    author  = {Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},\n    year    = {2021},\n    eprint  = {2103.03404}\n}\n```\n\n```bibtex\n@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{vaswani2017attention,\n    title   = {Attention Is All You Need},\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    year    = {2017},\n    eprint  = {1706.03762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n[\u2665](https://www.youtube.com/watch?v=GUo2XuqMcCU",
      "https://arxiv.org/abs/2003.04887}\n}\n```\n\n```bibtex\n@misc{lample2019large,\n    title   = {Large Memory Layers with Product Keys},\n    author  = {Guillaume Lample and Alexandre Sablayrolles and Marc'Aurelio Ranzato and Ludovic Denoyer and Herv\u00e9 J\u00e9gou},\n    year    = {2019},\n    eprint  = {1907.05242},\n    archivePrefix = {arXiv}\n}\n```\n\n```bibtex\n@misc{bhojanapalli2020lowrank,\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\n    year    = {2020},\n    eprint  = {2002.07028}\n}\n```\n\n```bibtex\n@misc{dong2021attention,\n    title   = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth}, \n    author  = {Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},\n    year    = {2021},\n    eprint  = {2103.03404}\n}\n```\n\n```bibtex\n@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{vaswani2017attention,\n    title   = {Attention Is All You Need},\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    year    = {2017},\n    eprint  = {1706.03762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n[\u2665](https://www.youtube.com/watch?v=GUo2XuqMcCU",
      "https://arxiv.org/abs/1910.05895"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@inproceedings{kitaev2020reformer,\n    title       = {Reformer: The Efficient Transformer},\n    author      = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},\n    booktitle   = {International Conference on Learning Representations},\n    year        = {2020},\n    url         = {https://openreview.net/forum?id=rkgNKkHtvB}\n}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-01470,\n    author    = {Sainbayar Sukhbaatar and\n               Edouard Grave and\n               Guillaume Lample and\n               Herv{\\'{e}} J{\\'{e}}gou and\n               Armand Joulin},\n    title     = {Augmenting Self-attention with Persistent Memory},\n    journal   = {CoRR},\n    volume    = {abs/1907.01470},\n    year      = {2019},\n    url       = {http://arxiv.org/abs/1907.01470}\n}\n```\n\n```bibtex\n@article{1910.05895,\n    author  = {Toan Q. Nguyen and Julian Salazar},\n    title   = {Transformers without Tears: Improving the Normalization of Self-Attention},\n    year    = {2019},\n    eprint  = {arXiv:1910.05895},\n    doi     = {10.5281/zenodo.3525484},\n}\n```\n\n```bibtex\n@inproceedings{fan2020reducing,\n    title     = {Reducing Transformer Depth on Demand with Structured Dropout},\n    author    = {Angela Fan and Edouard Grave and Armand Joulin},\n    booktitle = {International Conference on Learning Representations},\n    year      = {2020},\n    url       = {https://openreview.net/forum?id=SylO2yStDr}\n}\n```\n\n```bibtex\n@article{Shazeer2019FastTD,\n    title   = {Fast Transformer Decoding: One Write-Head is All You Need},\n    author  = {Noam Shazeer},\n    journal = {ArXiv},\n    year    = {2019},\n    volume  = {abs/1911.02150}\n}\n```\n\n```bibtex\n@misc{shazeer2020glu,\n    title   = {GLU Variants Improve Transformer},\n    author  = {Noam Shazeer},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2002.05202}    \n}\n```\n\n```bibtex\n@misc{roy*2020efficient,\n    title   = {Efficient Content-Based Sparse Attention with Routing Transformers},\n    author  = {Aurko Roy* and Mohammad Taghi Saffar* and David Grangier and Ashish Vaswani},\n    year    = {2020},\n    url     = {https://openreview.net/forum?id=B1gjs6EtDr}\n}\n```\n\n```bibtex\n@misc{bachlechner2020rezero,\n    title   = {ReZero is All You Need: Fast Convergence at Large Depth},\n    author  = {Thomas Bachlechner and Bodhisattwa Prasad Majumder and Huanru Henry Mao and Garrison W. Cottrell and Julian McAuley},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2003.04887}\n}\n```\n\n```bibtex\n@misc{lample2019large,\n    title   = {Large Memory Layers with Product Keys},\n    author  = {Guillaume Lample and Alexandre Sablayrolles and Marc'Aurelio Ranzato and Ludovic Denoyer and Herv\u00e9 J\u00e9gou},\n    year    = {2019},\n    eprint  = {1907.05242},\n    archivePrefix = {arXiv}\n}\n```\n\n```bibtex\n@misc{bhojanapalli2020lowrank,\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\n    year    = {2020},\n    eprint  = {2002.07028}\n}\n```\n\n```bibtex\n@misc{dong2021attention,\n    title   = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth}, \n    author  = {Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},\n    year    = {2021},\n    eprint  = {2103.03404}\n}\n```\n\n```bibtex\n@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{vaswani2017attention,\n    title   = {Attention Is All You Need},\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    year    = {2017},\n    eprint  = {1706.03762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n[\u2665](https://www.youtube.com/watch?v=GUo2XuqMcCU)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{vaswani2017attention,\n    title   = {Attention Is All You Need},\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    year    = {2017},\n    eprint  = {1706.03762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{dong2021attention,\n    title   = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth}, \n    author  = {Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},\n    year    = {2021},\n    eprint  = {2103.03404}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{bhojanapalli2020lowrank,\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\n    year    = {2020},\n    eprint  = {2002.07028}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{lample2019large,\n    title   = {Large Memory Layers with Product Keys},\n    author  = {Guillaume Lample and Alexandre Sablayrolles and Marc'Aurelio Ranzato and Ludovic Denoyer and Herv\u00e9 J\u00e9gou},\n    year    = {2019},\n    eprint  = {1907.05242},\n    archivePrefix = {arXiv}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{bachlechner2020rezero,\n    title   = {ReZero is All You Need: Fast Convergence at Large Depth},\n    author  = {Thomas Bachlechner and Bodhisattwa Prasad Majumder and Huanru Henry Mao and Garrison W. Cottrell and Julian McAuley},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2003.04887}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{roy*2020efficient,\n    title   = {Efficient Content-Based Sparse Attention with Routing Transformers},\n    author  = {Aurko Roy* and Mohammad Taghi Saffar* and David Grangier and Ashish Vaswani},\n    year    = {2020},\n    url     = {https://openreview.net/forum?id=B1gjs6EtDr}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{shazeer2020glu,\n    title   = {GLU Variants Improve Transformer},\n    author  = {Noam Shazeer},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2002.05202}    \n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Shazeer2019FastTD,\n    title   = {Fast Transformer Decoding: One Write-Head is All You Need},\n    author  = {Noam Shazeer},\n    journal = {ArXiv},\n    year    = {2019},\n    volume  = {abs/1911.02150}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{fan2020reducing,\n    title     = {Reducing Transformer Depth on Demand with Structured Dropout},\n    author    = {Angela Fan and Edouard Grave and Armand Joulin},\n    booktitle = {International Conference on Learning Representations},\n    year      = {2020},\n    url       = {https://openreview.net/forum?id=SylO2yStDr}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{1910.05895,\n    author  = {Toan Q. Nguyen and Julian Salazar},\n    title   = {Transformers without Tears: Improving the Normalization of Self-Attention},\n    year    = {2019},\n    eprint  = {arXiv:1910.05895},\n    doi     = {10.5281/zenodo.3525484},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-1907-01470,\n    author    = {Sainbayar Sukhbaatar and\n               Edouard Grave and\n               Guillaume Lample and\n               Herv{\\'{e}} J{\\'{e}}gou and\n               Armand Joulin},\n    title     = {Augmenting Self-attention with Persistent Memory},\n    journal   = {CoRR},\n    volume    = {abs/1907.01470},\n    year      = {2019},\n    url       = {http://arxiv.org/abs/1907.01470}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{kitaev2020reformer,\n    title       = {Reformer: The Efficient Transformer},\n    author      = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},\n    booktitle   = {International Conference on Learning Representations},\n    year        = {2020},\n    url         = {https://openreview.net/forum?id=rkgNKkHtvB}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9793901823005043
      ],
      "excerpt": "The default positional embedding uses <a href=\"https://arxiv.org/abs/2104.09864\">rotary embeddings</a>. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    depth = 12, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    depth = 12, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    depth = 12, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    depth = 12, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479,
        0.9105368110547479
      ],
      "excerpt": "Routing Transformer - https://github.com/lucidrains/routing-transformer \nSinkhorn Transformer - https://github.com/lucidrains/sinkhorn-transformer \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/reformer-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-09T20:42:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T14:16:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9392651979105677,
        0.8436069252530253
      ],
      "excerpt": "This is a Pytorch implementation of Reformer https://openreview.net/pdf?id=rkgNKkHtvB \nIt includes LSH attention, reversible network, and chunking. It has been validated with an auto-regressive task (enwik8). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823909737650497
      ],
      "excerpt": "This repository supports masks on the input sequence input_mask (b x i_seq), the context sequence context_mask (b x c_seq), as well as the rarely used full attention matrix itself input_attn_mask (b x i_seq x i_seq), all made compatible with LSH attention. Masks are made of booleans where False denotes masking out prior to the softmax. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = ReformerLM( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "y = model(x, keys = c, input_mask = i_mask, context_mask = c_mask) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8754001619735257
      ],
      "excerpt": "You can turn on axial positional embedding and adjust the shape and dimension of the axial embeddings by following the instructions below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = ReformerLM( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "y = model(x) #: (1, 8192, 20000) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8141086081019006
      ],
      "excerpt": "By popular demand, I have coded up a wrapper that removes a lot of the manual work in writing up a generic Reformer encoder / decoder architecture. To use, you would import the ReformerEncDec class. Encoder keyword arguments would be passed with a enc_ prefix and decoder keyword arguments with dec_. The model dimension (dim) must be prefix free and will be shared between encoder and decoder. The framework will also take care of passing the encoder input mask to the decoder context mask, unless explicitly overridden. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474500043348784
      ],
      "excerpt": ": evaluate with the following \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9623699414161203
      ],
      "excerpt": "samples = enc_dec.generate(eval_seq_in, eval_seq_out_start, seq_len = EN_SEQ_LEN, eos_token = 1) #: assume 1 is id of stop token \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9838541693009993
      ],
      "excerpt": "To see the benefits of using PKM, the learning rate of the values must be set higher than the rest of the parameters. (Recommended to be 1e-2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = ReformerLM( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "y = model(x) #: (1, 8192, 20000) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278232093540073
      ],
      "excerpt": "To access the attention weights and bucket distribution, simply wrap the instantiated model with the Recorder wrapper class. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = Reformer( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = Recorder(model) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.8973867101885499,
        0.860059181823877
      ],
      "excerpt": "y = model(x) \nmodel.recordings[0] #: a list of attention weights and buckets for the first forward pass \nmodel.turn_off() #: stop recording \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302287581602335,
        0.939453217394393
      ],
      "excerpt": "model.clear() #: clear the recordings \nmodel = model.eject() #: recover the original model and remove all listeners \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8004945338410508
      ],
      "excerpt": "Reformer comes with a slight drawback that the sequence must be neatly divisible by the bucket size * 2. I have provided a small helper tool that can help you auto-round the sequence length to the next best multiple. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = ReformerLM( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = Autopadder(model) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "y = model(x, keys = keys) #: (1, 7777, 20000) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.989101000353534
      ],
      "excerpt": "A lot of users are only interested in an auto-regressive language model (like GPT-2). Here is a training wrapper to make it easy to both train and evaluate on arbitrarily lengthed sequences of encoded tokens. You will have to take care of the encoding and decoding yourself. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = ReformerLM( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9464206812632462,
        0.860059181823877
      ],
      "excerpt": ": 0 is used for padding and no loss to be calculated on it \nmodel = TrainingWrapper(model, ignore_index = 0, pad_value = 0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8574741569479365
      ],
      "excerpt": ": when evaluating, just use the generate function, which will default to top_k sampling with temperature of 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8333539611066315
      ],
      "excerpt": "<a href=\"https://github.com/andreabac3\">Andrea</a> has uncovered that using O2 optimization level when training with mixed precision can lead to instability. Please use O1 instead, which can be set with the amp_level in Pytorch Lightning, or opt_level in Nvidia's Apex library. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reformer, the efficient Transformer, in Pytorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/reformer-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 222,
      "date": "Tue, 21 Dec 2021 13:25:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lucidrains/reformer-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lucidrains/reformer-pytorch",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lucidrains/reformer-pytorch/master/pretraining/self-supervised.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install reformer_pytorch\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9573822402267661
      ],
      "excerpt": "Since version 0.17.0, and some corrections to the reversible network, Reformer Pytorch is compatible with Microsoft's Deepspeed! If you have multiple local GPUs, you can follow the instructions / example <a href=\"https://github.com/lucidrains/reformer-pytorch/tree/master/examples/enwik8_deepspeed\">here</a>. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9179688463369292
      ],
      "excerpt": "You can follow the instructions here to set it correctly https://github.com/lucidrains/product-key-memory#learning-rates \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406,
        0.8918974083095406,
        0.9667326416418838
      ],
      "excerpt": "Routing Transformer - https://github.com/lucidrains/routing-transformer \nSinkhorn Transformer - https://github.com/lucidrains/sinkhorn-transformer \nPerformer - https://github.com/lucidrains/performer-pytorch \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img src=\"./lsh_attention.png\" width=\"500\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from reformer_pytorch import ReformerLM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "    depth = 1, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    causal = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from reformer_pytorch import ReformerLM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.821964837177328,
        0.8401420007903249
      ],
      "excerpt": "    causal = True, \n    axial_position_emb = True,         #: set this to True \n    axial_position_shape = (128, 64),  #: the shape must multiply up to the max_seq_len (128 x 64 = 8192) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from reformer_pytorch import ReformerEncDec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920388670233877
      ],
      "excerpt": "print(samples.shape) #: (1, <= 1024) decode the tokens \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from reformer_pytorch import ReformerLM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "    depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from reformer_pytorch import Reformer, Recorder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    causal = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from reformer_pytorch import ReformerLM, Autopadder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    causal = True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516,
        0.8801854956928516
      ],
      "excerpt": "from reformer_pytorch import ReformerLM \nfrom reformer_pytorch.generative_tools import TrainingWrapper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    causal = True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8625674545850888,
        0.8190200549205122
      ],
      "excerpt": ": when training, set return_loss equal to True \nloss = model(x_train, return_loss = True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828223137865405,
        0.9335358509042444
      ],
      "excerpt": "sample = model.generate(initial, 100, temperature=1., filter_thres = 0.9, eos_token = 1) #: assume end token is 1, or omit and it will sample up to 100 \nprint(sample.shape) #: (1, <=100) token ids \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lucidrains/reformer-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Phillip Wang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Reformer, the Efficient Transformer, in Pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "reformer-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lucidrains",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/reformer-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-11-06T23:08:22Z",
        "datePublished": "2021-11-06T23:08:36Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.4.4",
        "name": "1.4.4",
        "tag_name": "1.4.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.4.4",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/52855928",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.4.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-25T11:57:32Z",
        "datePublished": "2021-08-25T11:57:49Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.4.3",
        "name": "1.4.3",
        "tag_name": "1.4.3",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.4.3",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/48423114",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.4.3"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-05-09T14:49:27Z",
        "datePublished": "2021-05-09T14:49:45Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.4.2",
        "name": "1.4.2",
        "tag_name": "1.4.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.4.2",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/42666011",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.4.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-05-05T20:02:44Z",
        "datePublished": "2021-05-05T20:03:01Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.4.1",
        "name": "1.4.1",
        "tag_name": "1.4.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.4.1",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/42500651",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.4.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-04-21T19:07:51Z",
        "datePublished": "2021-04-21T19:08:05Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.4.0",
        "name": "1.4.0",
        "tag_name": "1.4.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.4.0",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/41797617",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.4.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-03-08T14:51:06Z",
        "datePublished": "2021-03-08T14:51:30Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.2.6",
        "name": "1.2.6",
        "tag_name": "1.2.6",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.2.6",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/39445207",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.2.6"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-02-27T02:29:46Z",
        "datePublished": "2021-02-27T02:29:55Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.2.5",
        "name": "1.2.5",
        "tag_name": "1.2.5",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.2.5",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/38838841",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.2.5"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-12-06T23:07:20Z",
        "datePublished": "2020-12-06T23:07:32Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.2.4",
        "name": "1.2.4",
        "tag_name": "1.2.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.2.4",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/34904368",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.2.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-11-05T07:30:06Z",
        "datePublished": "2020-11-05T07:30:22Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.2.3",
        "name": "1.2.3",
        "tag_name": "1.2.3",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.2.3",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/33488033",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.2.3"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-11-04T20:16:48Z",
        "datePublished": "2020-11-04T20:17:01Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.2.2",
        "name": "1.2.2",
        "tag_name": "1.2.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.2.2",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/33471950",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.2.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-11-04T16:22:00Z",
        "datePublished": "2020-11-04T16:22:16Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.2.1",
        "name": "1.2.1",
        "tag_name": "1.2.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.2.1",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/33461949",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.2.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-10-29T17:06:26Z",
        "datePublished": "2020-10-29T17:06:42Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.2.0",
        "name": "1.2.0",
        "tag_name": "1.2.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.2.0",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/33231442",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.2.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-10-04T20:40:25Z",
        "datePublished": "2020-10-04T20:40:50Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.1.5",
        "name": "1.1.5",
        "tag_name": "1.1.5",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.1.5",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/32143754",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.1.5"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-09-17T01:19:11Z",
        "datePublished": "2020-09-17T01:19:28Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.1.4",
        "name": "1.1.4",
        "tag_name": "1.1.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.1.4",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/31436655",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.1.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-08-05T20:01:14Z",
        "datePublished": "2020-08-05T20:01:34Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.1.3",
        "name": "1.1.3",
        "tag_name": "1.1.3",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.1.3",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/29380673",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.1.3"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-07-23T18:07:04Z",
        "datePublished": "2020-07-23T18:07:43Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.1.2",
        "name": "1.1.2",
        "tag_name": "1.1.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.1.2",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/28883949",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.1.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-07-23T16:39:24Z",
        "datePublished": "2020-07-23T16:39:52Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.1.1",
        "name": "1.1.1",
        "tag_name": "1.1.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.1.1",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/28880750",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.1.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "Add `dim_head` keyword for fixing dimension of each head",
        "dateCreated": "2020-07-13T21:40:48Z",
        "datePublished": "2020-07-13T21:41:36Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.1.0",
        "name": "1.1.0",
        "tag_name": "1.1.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.1.0",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/28520299",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.1.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-07-05T20:12:00Z",
        "datePublished": "2020-07-05T20:12:43Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.0.5",
        "name": "1.0.5",
        "tag_name": "1.0.5",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.0.5",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/28235475",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.0.5"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2020-07-03T23:24:41Z",
        "datePublished": "2020-07-03T23:25:13Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.0.4",
        "name": "1.0.4",
        "tag_name": "1.0.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.0.4",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/28212939",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.0.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "TPU compatible",
        "dateCreated": "2020-06-29T21:23:54Z",
        "datePublished": "2020-06-29T21:24:33Z",
        "html_url": "https://github.com/lucidrains/reformer-pytorch/releases/tag/1.0.3",
        "name": "1.0.3",
        "tag_name": "1.0.3",
        "tarball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/tarball/1.0.3",
        "url": "https://api.github.com/repos/lucidrains/reformer-pytorch/releases/28042021",
        "zipball_url": "https://api.github.com/repos/lucidrains/reformer-pytorch/zipball/1.0.3"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1644,
      "date": "Tue, 21 Dec 2021 13:25:17 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "artificial-intelligence",
      "transformers",
      "attention-mechanism",
      "machine-learning",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A simple Reformer language model\n\n```python\n#: should fit in ~ 5gb - 8k tokens\n\nimport torch\nfrom reformer_pytorch import ReformerLM\n\nmodel = ReformerLM(\n    num_tokens= 20000,\n    dim = 1024,\n    depth = 12,\n    max_seq_len = 8192,\n    heads = 8,\n    lsh_dropout = 0.1,\n    ff_dropout = 0.1,\n    post_attn_dropout = 0.1,\n    layer_dropout = 0.1,  #: layer dropout from 'Reducing Transformer Depth on Demand' paper\n    causal = True,        #: auto-regressive or not\n    bucket_size = 64,     #: average size of qk per bucket, 64 was recommended in paper\n    n_hashes = 4,         #: 4 is permissible per author, 8 is the best but slower\n    emb_dim = 128,        #: embedding factorization for further memory savings\n    dim_head = 64,        #: be able to fix the dimension of each head, making it independent of the embedding dimension and the number of heads\n    ff_chunks = 200,      #: number of chunks for feedforward layer, make higher if there are memory issues\n    attn_chunks = 8,      #: process lsh attention in chunks, only way for memory to fit when scaling to 16k tokens\n    num_mem_kv = 128,       #: persistent learned memory key values, from all-attention paper\n    full_attn_thres = 1024, #: use full attention if context length is less than set value\n    reverse_thres = 1024,   #: turn off reversibility for 2x speed for sequence lengths shorter or equal to the designated value\n    use_scale_norm = False,  #: use scale norm from 'Transformers without tears' paper\n    use_rezero = False,      #: remove normalization and use rezero from 'ReZero is All You Need'\n    one_value_head = False,  #: use one set of values for all heads from 'One Write-Head Is All You Need'\n    weight_tie = False,           #: tie parameters of each layer for no memory per additional depth\n    weight_tie_embedding = False, #: use token embedding for projection of output, some papers report better results\n    n_local_attn_heads = 2,       #: many papers suggest mixing local attention heads aids specialization and improves on certain tasks\n    pkm_layers = (4,7),           #: specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best\n    pkm_num_keys = 128,           #: defaults to 128, but can be increased to 256 or 512 as memory allows\n    use_full_attn = False    #: only turn on this flag to override and turn on full attention for all sequence lengths. for comparison with LSH to show that it is working\n).cuda()\n\nx = torch.randint(0, 20000, (1, 8192)).long().cuda()\ny = model(x) #: (1, 8192, 20000)\n```\n\nThe Reformer (just a stack of reversible LSH attention)\n\n```python\n#: should fit in ~ 5gb - 8k embeddings\n\nimport torch\nfrom reformer_pytorch import Reformer\n\nmodel = Reformer(\n    dim = 512,\n    depth = 12,\n    heads = 8,\n    lsh_dropout = 0.1,\n    causal = True\n).cuda()\n\nx = torch.randn(1, 8192, 512).cuda()\ny = model(x) #: (1, 8192, 512)\n```\n\nSelf Attention with LSH\n\n```python\nimport torch\nfrom reformer_pytorch import LSHSelfAttention\n\nattn = LSHSelfAttention(\n    dim = 128,\n    heads = 8,\n    bucket_size = 64,\n    n_hashes = 8,\n    causal = False\n)\n\nx = torch.randn(10, 1024, 128)\ny = attn(x) #: (10, 1024, 128)\n```\n\nLSH (locality sensitive hashing) Attention\n\n```python\nimport torch\nfrom reformer_pytorch import LSHAttention\n\nattn = LSHAttention(\n    bucket_size = 64,\n    n_hashes = 16,\n    causal = True\n)\n\nqk = torch.randn(10, 1024, 128)\nv = torch.randn(10, 1024, 128)\n\nout, attn, buckets = attn(qk, v) #: (10, 1024, 128)\n#: attn contains the unsorted attention weights, provided return_attn is set to True (costly otherwise)\n#: buckets will contain the bucket number (post-argmax) of each token of each batch\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "A full Reformer sequence \u2192 sequence, say translation\n\n```python\nimport torch\nfrom reformer_pytorch import ReformerLM\n\nDE_SEQ_LEN = 4096\nEN_SEQ_LEN = 4096\n\nencoder = ReformerLM(\n    num_tokens = 20000,\n    emb_dim = 128,\n    dim = 1024,\n    depth = 12,\n    heads = 8,\n    max_seq_len = DE_SEQ_LEN,\n    fixed_position_emb = True,\n    return_embeddings = True #: return output of last attention layer\n).cuda()\n\ndecoder = ReformerLM(\n    num_tokens = 20000,\n    emb_dim = 128,\n    dim = 1024,\n    depth = 12,\n    heads = 8,\n    max_seq_len = EN_SEQ_LEN,\n    fixed_position_emb = True,\n    causal = True\n).cuda()\n\nx  = torch.randint(0, 20000, (1, DE_SEQ_LEN)).long().cuda()\nyi = torch.randint(0, 20000, (1, EN_SEQ_LEN)).long().cuda()\n\nenc_keys = encoder(x)               #: (1, 4096, 1024)\nyo = decoder(yi, keys = enc_keys)   #: (1, 4096, 20000)\n```\n\nA full Reformer image \u2192 caption\n\n```python\nimport torch\nfrom torch.nn import Sequential\nfrom torchvision import models\nfrom reformer_pytorch import Reformer, ReformerLM\n\nresnet = models.resnet50(pretrained=True)\nresnet = Sequential(*list(resnet.children())[:-4])\n\nSEQ_LEN = 4096\n\nencoder = Reformer(\n    dim = 512,\n    depth = 6,\n    heads = 8,\n    max_seq_len = 4096\n)\n\ndecoder = ReformerLM(\n    num_tokens = 20000,\n    dim = 512,\n    depth = 6,\n    heads = 8,\n    max_seq_len = SEQ_LEN,\n    causal = True\n)\n\nx  = torch.randn(1, 3, 512, 512)\nyi = torch.randint(0, 20000, (1, SEQ_LEN)).long()\n\nvisual_emb = resnet(x)\nb, c, h, w = visual_emb.shape\nvisual_emb = visual_emb.view(1, c, h * w).transpose(1, 2) #: nchw to nte\n\nenc_keys = encoder(visual_emb)\nyo = decoder(yi, keys = enc_keys) #: (1, 4096, 20000)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}