# Hidden layer Kernel Flow regularized Neural Networks

This code is an implementation of https://arxiv.org/abs/2002.08335.  Code is used from https://github.com/dalgu90/wrn-tensorflow and https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd.

This work introduces the application of Kernel Flows to Neural Networks.  Kernel Flows is a Gaussian Process Regression (GPR) technique which learns a generalizable kernel, i.e. one for which the GPR classifier using a small subset of the data is similar to the classifier using larger subsets of data.  In practice, Kernel Flows is applied to a family of kernels, and applications to NN's use RBF kernels on the  outputs of selected hidden layers.  Such a kernel is parameterized by the weights of the network, which Kernel Flows learns to create a generalizable output.  This leads to a loss function dependent on hidden layer outputs on randomly selected batches, which we add to the traditional cross-entropy loss on the NN output.  Hence, we simultaneously train the hidden layer generalizability and the accuracy of the final output and observe improved accuracy and robustness to distributional shift.  The mathematical set-up can be found in the paper in Chapter 2 on pages 3-4.  Further implementation details and testing errors as well as other statistics can be found in Chapter 3.

This is implemented in CNN_MNIST.py, CNN_CIFAR10.py, and CNN_CIFAR100.py, corresponding to a CNN for MNIST and WRN's for CIFAR 10 and 100 respectively.  In the MNIST case, we use validation to estimate learning rate, dropout rate, and KF-regularization parameters (\lambda and \gamma in our notation).  The codes control_bn.py, control_kf.py, control_do.py, and control_kfdo.py execute the code for BN, BN+KF, BN+DO, and BN+KF+DO trained NNs respectively for augmented data MNIST.  Original MNIST and QMNIST are found in control_bn0.py and control_qbn.py respectively.  In the WRN for CIFAR, we use learning and dropout rates as found in https://arxiv.org/abs/1605.07146 with cross-validation.  We use validation to learn the KF-regularization parameters.  The codes control_10bn.py, etc. and control_100bn.py, etc. executes the code for CIFAR-10 and 100 training and testing.  The KF loss calculations occur from lines 1046-1128 in CNN_MNIST.py and from lines 113-162 in resnet10021.py in the WRN example.

My apologies in advance if the code is not so good, I'll try to answer any questions that arises.  Thank you for your interest!
