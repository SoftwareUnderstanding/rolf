{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1908.10084",
      "https://arxiv.org/abs/2004.09813",
      "https://arxiv.org/abs/2010.08240",
      "https://arxiv.org/abs/2012.14210",
      "https://arxiv.org/abs/2104.06979",
      "https://arxiv.org/abs/2104.08663",
      "https://arxiv.org/abs/1908.10084",
      "https://arxiv.org/abs/1908.10084\",\n}\n```\n\n\nIf you use one of the multilingual models, feel free to cite our publication [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://arxiv.org/abs/2004.09813",
      "https://arxiv.org/abs/2004.09813",
      "https://arxiv.org/abs/2004.09813\",\n}\n```\n\nPlease have a look at [Publications](https://www.sbert.net/docs/publications.html"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repository helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):\n```bibtex \n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n```\n\n\nIf you use one of the multilingual models, feel free to cite our publication [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://arxiv.org/abs/2004.09813):\n```bibtex\n@inproceedings{reimers-2020-multilingual-sentence-bert,\n    title = \"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/2004.09813\",\n}\n```\n\nPlease have a look at [Publications](https://www.sbert.net/docs/publications.html) for our different publications that are integrated into SentenceTransformers.\n\n\nContact person: [Nils Reimers](https://www.nils-reimers.de), [info@nils-reimers.de](mailto:info@nils-reimers.de)\n\nhttps://www.ukp.tu-darmstadt.de/\n\n\nDon't hesitate to send us an e-mail or report an issue, if something is broken (and it shouldn't be) or if you have further questions.\n\n> This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication.\n\n\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{reimers-2020-multilingual-sentence-bert,\n    title = \"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/2004.09813\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9719743352453015
      ],
      "excerpt": "- Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (EMNLP 2019) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440963974496585
      ],
      "excerpt": "- TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning (arXiv 2021) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9838285174506147
      ],
      "excerpt": "- Multi-Lingual and multi-task learning \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UKPLab/sentence-transformers",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-24T10:53:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T15:10:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9982786343166085,
        0.9347401562544557,
        0.970572311122744,
        0.9194592901915939
      ],
      "excerpt": "This framework provides an easy method to compute dense vector representations for sentences, paragraphs, and images. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various task. Text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity. \nWe provide an increasing number of state-of-the-art pretrained models for more than 100 languages, fine-tuned for various use-cases. \nFurther, this framework allows an easy  fine-tuning of custom embeddings models, to achieve maximal performance on your specific task. \nFor the full documentation, see www.SBERT.net. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9165863257163631
      ],
      "excerpt": "- Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks (NAACL 2021) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8625288961340939,
        0.9408957734203827,
        0.8582713489219483
      ],
      "excerpt": "We provide a large list of Pretrained Models for more than 100 languages. Some models are general purpose models, while others produce embeddings for specific use cases. Pre-trained models can be loaded by just passing the model name: SentenceTransformer('model_name'). \n\u00bb  Full list of pretrained models \nThis framework allows you to fine-tune your own sentence embedding methods, so that you get task-specific sentence embeddings. You have various options to choose from in order to get perfect sentence embeddings for your specific task.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474720989357001
      ],
      "excerpt": "- Support of various transformer networks including BERT, RoBERTa, XLM-R, DistilBERT, Electra, BART, ... \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Multilingual Sentence & Image Embeddings with BERT",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UKPLab/sentence-transformers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1310,
      "date": "Mon, 27 Dec 2021 20:34:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/UKPLab/sentence-transformers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "UKPLab/sentence-transformers",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/UKPLab/sentence-transformers/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/examples/applications/image-search/Image_Duplicates.ipynb",
      "https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/examples/applications/image-search/Image_Classification.ipynb",
      "https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/examples/applications/image-search/Image_Search.ipynb",
      "https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/examples/applications/image-search/Image_Clustering.ipynb",
      "https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/examples/applications/image-search/Image_Search-multilingual.ipynb",
      "https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We recommend **Python 3.6** or higher, **[PyTorch 1.6.0](https://pytorch.org/get-started/locally/)** or higher and **[transformers v4.6.0](https://github.com/huggingface/transformers)** or higher. The code does **not** work with Python 2.7.\n\n\n\n\n**Install with pip**\n\nInstall the *sentence-transformers* with `pip`:\n```\npip install -U sentence-transformers\n```\n\n**Install from sources**\n\nAlternatively, you can also clone the latest version from the [repository](https://github.com/UKPLab/sentence-transformers) and install it directly from the source code:\n````\npip install -e .\n```` \n\n**PyTorch with CUDA**\nIf you want to use a GPU / CUDA, you must install PyTorch with the matching CUDA Version. Follow\n[PyTorch - Get Started](https://pytorch.org/get-started/locally/) for further details how to install PyTorch.\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/UKPLab/sentence-transformers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright 2019 Nils Reimers\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\nlimitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sentence Transformers: Multilingual Sentence, Paragraph, and Image Embeddings using BERT & Co.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sentence-transformers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "UKPLab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UKPLab/sentence-transformers/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "This is a smaller release with some new features\r\n\r\n### MarginMSELoss\r\n[MarginMSELoss](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MarginMSELoss.py) is a great method to train embeddings model with the help of a cross-encoder model. The details are explained here: [MSMARCO - MarginMSE Training](https://www.sbert.net/examples/training/ms_marco/README.html#marginmse)\r\n\r\nYou pass your training data in the format: \r\n```python\r\nInputExample(texts=[query, positive, negative], label=cross_encoder.predict([query, positive])-cross_encoder.predict([query, negative])\r\n```\r\n### MultipleNegativesSymmetricRankingLoss\r\nMultipleNegativesRankingLoss computes the loss just in one way: Find the correct answer for a given question.\r\n\r\n[MultipleNegativesSymmetricRankingLoss](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesSymmetricRankingLoss.py) also computes the loss in the other direction: Find the correct question for a given answer.\r\n\r\n### Breaking Change: CLIPModel\r\nThe CLIPModel is now based on the `transformers` model. \r\n\r\nYou can still load it like this:\r\n```python\r\nmodel = SentenceTransformer('clip-ViT-B-32')\r\n```\r\n\r\nOlder SentenceTransformers versions are now longer able to load and use the 'clip-ViT-B-32' model.\r\n\r\n\r\n### Added files on the hub are automatically downloaded\r\nPR #1116 checks if you have all files in your local cache or if there are added files on the hub. If this is the case, it will automatically download them.\r\n\r\n### `SentenceTransformers.encode()` can return all values\r\n\r\nWhen you set `output_value=None` for the `encode` method, all values (token_ids, token_embeddings, sentence_embedding) will be returned.",
        "dateCreated": "2021-10-01T08:33:41Z",
        "datePublished": "2021-10-01T09:10:30Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v2.1.0",
        "name": "v2.1.0 - New Loss Functions",
        "tag_name": "v2.1.0",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v2.1.0",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/50615208",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v2.1.0"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "## Models hosted on the hub \r\nAll pre-trained models are now hosted on the [Huggingface Models hub](https://huggingface.co/models). \r\n\r\nOur pre-trained models can be found here: [https://huggingface.co/sentence-transformers](https://huggingface.co/sentence-transformers)\r\n\r\nBut you can easily share your own sentence-transformer model on the hub and have other people easily access it. Simple upload the folder and have people load it via:\r\n```\r\nmodel = SentenceTransformer('[your_username]/[model_name]')\r\n```\r\n\r\nFor more information, see: [Sentence Transformers in the Hugging Face Hub](https://huggingface.co/blog/sentence-transformers-in-the-hub)\r\n\r\n## Breaking changes\r\n\r\nThere should be no breaking changes. Old models can still be loaded from disc. However, if you use one of the provided pre-trained models, it will be downloaded again in version 2 of sentence transformers as the cache path has slightly changed.\r\n\r\n## Find sentence-transformer models on the Hub\r\n\r\nYou can filter the hub for sentence-transformers models: [https://huggingface.co/models?filter=sentence-transformers](https://huggingface.co/models?filter=sentence-transformers)\r\n\r\nAdd the `sentence-transformers` tag to you model card so that others can find your model.\r\n\r\n## Widget & Inference API\r\nA widget was added to sentence-transformers models on the hub that lets you interact directly on the models website:\r\nhttps://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2\r\n\r\nFurther, models can now be used with the [Accelerated Inference API](https://api-inference.huggingface.co/docs/python/html/index.html): Send you sentences to the API and get back the embeddings from the respective model.\r\n\r\n## Save Model to Hub\r\n\r\nA new method was added to the `SentenceTransformer` class: `save_to_hub`.\r\n\r\nProvide the model name and the model is saved on the hub. \r\n\r\nHere you find the explanation from transformers how the hub works: [Model sharing and uploading\r\n](https://huggingface.co/transformers/model_sharing.html)\r\n\r\n## Automatic Model Card\r\n\r\nWhen you save a model with `save` or `save_to_hub`, a `README.md` (also known as model card) is automatically generated with basic information about the respective SentenceTransformer model.\r\n\r\n\r\n## New Models\r\n- Several new sentence embedding models have been added, which are much better than the previous model: [Sentence Embedding Models](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models)\r\n- Some new models for semantic search based on MS MARCO have been added: [MSMARCO Models](https://www.sbert.net/docs/pretrained-models/msmarco-v3.html)\r\n- The training script for these MS MARCO models have been released as well: [Train MS MARCO Bi-Encoder v3](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/ms_marco/train_bi-encoder-v3.py)\r\n",
        "dateCreated": "2021-06-24T15:01:58Z",
        "datePublished": "2021-06-24T16:16:11Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v2.0.0",
        "name": "v2.0.0 - Integration into Huggingface Model Hub",
        "tag_name": "v2.0.0",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v2.0.0",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/45181400",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v2.0.0"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "Final release of version 1: Makes v1 of sentence-transformers forward compatible with models from version 2 of sentence-transformers.",
        "dateCreated": "2021-06-22T09:52:28Z",
        "datePublished": "2021-06-24T14:20:43Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v1.2.1",
        "name": "v1.2.1 - Forward compatibility with version 2",
        "tag_name": "v1.2.1",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v1.2.1",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/45173058",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v1.2.1"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "\r\n# Unsupervised Sentence Embedding Learning\r\n\r\nNew methods integrated to train sentence embedding models without labeled data. See [Unsupervised Learning](https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning) for an overview of all existent methods. \r\n\r\nNew methods:\r\n- **[CT](https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/CT)**: Integration of [Semantic Re-Tuning With Contrastive Tension (CT)](https://openreview.net/pdf?id=Ov_sMNau-PF)  to tune models without labeled data\r\n- **[CT_In-Batch_Negatives](https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/CT_In-Batch_Negatives)**: A modification of CT using in-batch negatives\r\n- **[SimCSE](https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/SimCSE)**: An unsupervised sentence embedding learning method by [Gao et al.](https://arxiv.org/abs/2104.08821)\r\n\r\n# Pre-Training Methods\r\n- **[MLM](https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/MLM):** An example script to run Masked-Language-Modeling (MLM). Running MLM on your custom data before supervised training can significantly improve the performances. Further, MLM also works well for domain trainsfer: You first train on your custom data, and then train with e.g. NLI or STS data.\r\n\r\n\r\n# Training Examples\r\n- **[Paraphrase Data](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/paraphrases):** In our paper  [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://arxiv.org/abs/2004.09813) we have shown that training on paraphrase data is powerful. In that folder we provide collections of different paraphrase datasets and scripts to train on it.\r\n- **[NLI with MultipleNegativeRankingLoss](https://www.sbert.net/examples/training/nli/README.html#multiplenegativesrankingloss)**: A dedicated example how to use MultipleNegativeRankingLoss for training with NLI data, which leads to a significant performance boost.\r\n\r\n\r\n\r\n\r\n# New models\r\n- **[New NLI & STS models](https://www.sbert.net/docs/pretrained_models.html#semantic-textual-similarity):** Following the [Paraphrase Data training example](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/paraphrases) we published new models trained on NLI and NLI+STS data. Training code is available: [training_nli_v2.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli_v2.py).\r\n  \r\n    | Model-Name | STSb-test performance |\r\n    | --- | :---: |\r\n    | *Previous best models* | |\r\n    | nli-bert-large | 79.19 |\r\n    | stsb-roberta-large | 86.39 |\r\n    | *New v2 models* | |\r\n    | nli-mpnet-base-v2 | 86.53 |\r\n    | stsb-mpnet-base-v2 | 88.57 |\r\n  \r\n- **[New MS MARCO model for Semantic Search](https://www.sbert.net/docs/pretrained-models/msmarco-v3.html)**: [Hofst\u00e4tter et al.](https://arxiv.org/abs/2104.06967) optimized the training procedure on the [MS MARCO dataset](https://www.sbert.net/examples/training/ms_marco/README.html). The resulting model is integrated as **msmarco-distilbert-base-tas-b** and improves the performance on the MS MARCO dataset from 33.13 to 34.43 MRR@10\r\n\r\n# New Functions\r\n- `SentenceTransformer.fit()` **Checkpoints**: The fit() method now allows to save checkpoints during the training at a fixed number of steps. [More info](https://www.sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.fit)\r\n- **Pooling-mode as string**: You can now pass the pooling-mode to `models.Pooling()` as string: \r\n  ```python\r\n  pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='mean')\r\n  ```\r\n  Valid values are mean/max/cls.\r\n- **[NoDuplicatesDataLoader](https://www.sbert.net/docs/package_reference/datasets.html#noduplicatesdataloader)**: When using the [MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss), one should avoid to have duplicate sentences in the same sentence. This data loader simplifies this task and ensures that no duplicate entries are in the same batch.~~~~\r\n\r\n",
        "dateCreated": "2021-05-12T13:11:27Z",
        "datePublished": "2021-05-12T13:14:10Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v1.2.0",
        "name": "v1.2.0 - Unsupervised Learning, New Training Examples, Improved Models",
        "tag_name": "v1.2.0",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v1.2.0",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/42839844",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v1.2.0"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "## Unsupervised Sentence Embedding Learning\r\nThis release integrates methods that allows to learn sentence embeddings without having labeled data:\r\n- **[TSDAE](https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/TSDAE)**: TSDAE is using a denoising auto-encoder to learn sentence embeddings. The method has been presented in our [recent paper](https://arxiv.org/abs/2104.06979) and achieves state-of-the-art performance for several tasks.\r\n- **[GenQ](https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/query_generation)**: GenQ uses a pre-trained T5 system to generate queries for a given passage. It was presented in our [recent BEIR paper](https://arxiv.org/abs/2104.08663) and works well for domain adaptation for (semantic search)[https://www.sbert.net/examples/applications/semantic-search/README.html]\r\n\r\n## New Models - SentenceTransformer\r\n-  [MSMARCO Dot-Product Models](https://www.sbert.net/docs/pretrained-models/msmarco-v3.html): We trained models using the dot-product instead of cosine similarity as similarity function. As shown in our [recent BEIR paper](https://arxiv.org/abs/2104.08663), models with cosine-similarity prefer the retrieval of short documents, while models with dot-product prefer retrieval of longer documents. Now you can choose what is most suitable for your task.\r\n-  [MSMARCO MiniLM Models](https://www.sbert.net/docs/pretrained-models/msmarco-v3.html):  We uploaded some models based on [MiniLM](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased): It uses just 384 dimensions, is faster than previous models and achieves nearly the same performance\r\n\r\n## New Models - CrossEncoder\r\n- [MSMARCO Re-ranking-Models v2](https://www.sbert.net/docs/pretrained-models/ce-msmarco.html): We trained new significantly faster and significantly better CrossEncoder re-ranking models on the MSMARCO dataset. It outperforms BERT-large models in terms of accuracy while being 18 times faster. [Trainingcode is available](https://www.sbert.net/examples/training/ms_marco/README.html#cross-encoder)\r\n\r\n## New Features\r\n- You can now pass to the CrossEncoder class a `default_activation_function`, that is applied on-top of the output logits generated by the class.\r\n- You can now pre-process images for the [CLIP Model](https://www.sbert.net/examples/applications/image-search/README.html). Soon I will release a tutorial how to fine-tune the CLIP Model with your data.",
        "dateCreated": "2021-04-21T12:43:25Z",
        "datePublished": "2021-04-21T13:12:31Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v1.1.0",
        "name": "Unsupervised Sentence Embedding Learning",
        "tag_name": "v1.1.0",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v1.1.0",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/41776524",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v1.1.0"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "It was not possible to fine-tune and save the CLIPModel. This release fixes it. CLIPModel can now be saved like any other model by calling `model.save(path)`",
        "dateCreated": "2021-04-01T06:32:45Z",
        "datePublished": "2021-04-01T06:35:11Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v1.0.4",
        "name": "v1.0.4 - Patch CLIPModel.save",
        "tag_name": "v1.0.4",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v1.0.4",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/40801247",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v1.0.4"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "v1.0.3 - Patch for util.paraphrase_mining method",
        "dateCreated": "2021-03-22T08:12:33Z",
        "datePublished": "2021-03-22T08:15:39Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v1.0.3",
        "name": "v1.0.3 - Patch  util.paraphrase_mining",
        "tag_name": "v1.0.3",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v1.0.3",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/40144766",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v1.0.3"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "v1.0.2 - Patch for CLIPModel, new Image Examples\r\n- Bugfix in CLIPModel: Too long inputs raised a RuntimeError. Now they are truncated.\r\n- New util function: util.paraphrase_mining_embeddings, to find most similar embeddings in a matrix\r\n- **Image Clustering** and **Duplicate Image Detection** examples added: [more info](https://www.sbert.net/examples/applications/image-search/README.html#examples)\r\n",
        "dateCreated": "2021-03-19T21:33:18Z",
        "datePublished": "2021-03-19T21:44:45Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v1.0.2",
        "name": "v1.0.2 - Patch CLIPModel",
        "tag_name": "v1.0.2",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v1.0.2",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/40089331",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v1.0.2"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "This release brings many new improvements and new features. Also, the version number scheme is updated. Now we use the format x.y.z with x: for major releases, y: smaller releases with new features, z: bugfixes\r\n\r\n## Text-Image-Model CLIP\r\nYou can now encode text and images in the same vector space using the OpenAI CLIP Model. You can use the model like this:\r\n```python\r\nfrom sentence_transformers import SentenceTransformer, util\r\nfrom PIL import Image\r\n\r\n#Load CLIP model\r\nmodel = SentenceTransformer('clip-ViT-B-32')\r\n\r\n#Encode an image:\r\nimg_emb = model.encode(Image.open('two_dogs_in_snow.jpg'))\r\n\r\n#Encode text descriptions\r\ntext_emb = model.encode(['Two dogs in the snow', 'A cat on a table', 'A picture of London at night'])\r\n\r\n#Compute cosine similarities \r\ncos_scores = util.cos_sim(img_emb, text_emb)\r\nprint(cos_scores)\r\n```\r\n[More Information](https://www.sbert.net/examples/applications/image-search/README.html) \r\n[IPython Demo](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/image-search/Image_Search.ipynb)\r\n[Colab Demo](https://colab.research.google.com/drive/16OdADinjAg3w3ceZy3-cOR9A-5ZW9BYr#scrollTo=xTFNbzmG3erx)\r\n\r\nExamples how to train the CLIP model on your data will be added soon.\r\n\r\n## New Models\r\n- Add v3 models trained for semantic search on MS MARCO: [MS MARCO Models v3](https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/msmarco-v3.md)\r\n- First models trained on Natural Questions dataset for Q&A Retrieval: [Natural Questions Models v1](https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nq-v1.md)\r\n- Add DPR Models from Facebook for Q&A Retrieval: [DPR-Models](https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/dpr.md)\r\n\r\n## New Features\r\n- The [Asym Model](https://github.com/UKPLab/sentence-transformers/releases/tag/v0.4.1) can now be used as the first model in a SentenceTransformer modules list.\r\n- Sorting when encoding changes: Previously, we encoded from short to long sentences. Now we encode from long to short sentences. Out-of-memory errors will then happen at the start. Also the approximation on the duration of the encode process is more precise\r\n- Improvement of the util.semantic_search method: It now uses the much faster torch.topk function. Further, you can define which scoring function should be used\r\n- New util methods: `util.dot_score` computes the dot product of two embedding matrices. `util.normalize_embeddings` will normalize embeddings to unit length\r\n- New parameter for `SentenceTransformer.encode` method: `normalize_embeddings` if set to true, it will normalize embeddings to unit length. In that case the faster `util.dot_score` can be used instead of `util.cos_sim` to compute cosine similarity scores.\r\n- If you specify in `models.Transformer(do_lower_case=True)` when creating a new SentenceTransformer, then all input will be lower cased.\r\n\r\n\r\n## New Examples\r\n- Add example for model quantization on CPUs (smaller models, faster run-time): [model_quantization.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_quantization.py)\r\n- Start to add example how to train SBERT models without training data: [unsupervised learning](https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning). We start with an example for [Query Generation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/query_generation) to train a semantic search model.\r\n\r\n## Bugfixes\r\n- Encode method now correctly returns token_embeddings if `output_value='token_embeddings'` is defined\r\n- Bugfix of the `LabelAccuracyEvaluator`\r\n- Bugfix of removing tensors off the CPU if you specified `encode(sent, convert_to_tensor=True)`. They now stay on the GPU\r\n\r\n## Breaking changes:\r\n- SentenceTransformer.encode-Methode: Removed depcreated parameters is_pretokenized and num_workers",
        "dateCreated": "2021-03-18T20:34:43Z",
        "datePublished": "2021-03-18T20:57:58Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v1.0.0",
        "name": "v1.0.0 - Improvements, New Models, Text-Image Models",
        "tag_name": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v1.0.0",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/40031270",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v1.0.0"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "**Refactored Tokenization**\r\n- Faster tokenization speed: Using batched tokenization for training & inference - Now, all sentences in a batch are tokenized simoultanously.\r\n- Usage of the `SentencesDataset` no longer needed for training. You can pass your train examples directly to the DataLoader:\r\n```python\r\ntrain_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\r\n    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\r\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\r\n```\r\n- If you use a custom torch DataSet class: The dataset class must now return `InputExample` objects instead of tokenized texts\r\n- Class `SentenceLabelDataset` has been updated to new tokenization flow: It returns always two or more `InputExamples` with the same label\r\n\r\n**Asymmetric Models**\r\nAdd new `models.Asym` class that allows different encoding of sentences based on some tag (e.g. *query* vs *paragraph*). Minimal example:\r\n\r\n```python\r\nword_embedding_model = models.Transformer(base_model, max_seq_length=250)\r\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\r\nd1 = models.Dense(word_embedding_model.get_word_embedding_dimension(), 256, bias=False, activation_function=nn.Identity())\r\nd2 = models.Dense(word_embedding_model.get_word_embedding_dimension(), 256, bias=False, activation_function=nn.Identity())\r\nasym_model = models.Asym({'QRY': [d1], 'DOC': [d2]})\r\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model, asym_model])\r\n\r\n##Your input examples have to look like this:\r\ninp_example = InputExample(texts=[{'QRY': 'your query'}, {'DOC': 'your document text'}], label=1)\r\n\r\n##Encoding (Note: Mixed inputs are not allowed)\r\nmodel.encode([{'QRY': 'your query1'}, {'QRY': 'your query2'}])\r\n```\r\n\r\nInputs that have the key 'QRY' will be passed through the `d1` dense layer, while inputs with they key 'DOC' through the `d2` dense layer.\r\nMore documentation on how to design asymmetric models will follow soon.\r\n\r\n\r\n**New Namespace & Models for Cross-Encoder**\r\nCross-Encoder are now hosted at [https://huggingface.co/cross-encoder](https://huggingface.co/cross-encoder). Also, new [pre-trained models](https://www.sbert.net/docs/pretrained_cross-encoders.html) have been added for: NLI & QNLI.\r\n\r\n**Logging**\r\nLog messages now use a custom logger from `logging` thanks to PR #623. This allows you which log messages you want to see from which components.\r\n\r\n**Unit tests**\r\nA lot more unit tests have been added, which test the different components of the framework.",
        "dateCreated": "2021-01-04T14:01:33Z",
        "datePublished": "2021-01-04T14:04:01Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.4.1",
        "name": "v0.4.1 - Faster Tokenization & Asymmetric Models",
        "tag_name": "v0.4.1",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.4.1",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/35951845",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.4.1"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "- Updated the dependencies so that it works with Huggingface Transformers version 4. Sentence-Transformers still works with huggingface transformers version 3, but an update to version 4 of transformers is recommended. Future changes might break with transformers version 3.\r\n- New naming of pre-trained models. Models will be named: {task}-{transformer_model}. So 'bert-base-nli-stsb-mean-tokens' becomes  'stsb-bert-base'. Models will still be available under their old names, but newer models will follow the updated naming scheme.\r\n- New application example for [information retrieval and question answering retrieval](https://www.sbert.net/examples/applications/information-retrieval/README.html). Together with respective pre-trained models\r\n\r\n",
        "dateCreated": "2020-12-22T13:25:11Z",
        "datePublished": "2020-12-22T13:42:13Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.4.0",
        "name": "v0.4.0 - Upgrade Transformers Version",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.4.0",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/35614825",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "This release only include some smaller updates:\r\n- Code was tested with transformers 3.5.1, requirement was updated so that it works with transformers 3.5.1\r\n- As some parts and models require Pytorch >= 1.6.0, requirement was updated to require at least pytorch 1.6.0.  Most of the code and models will work with older pytorch versions.\r\n- model.encode() stored the embeddings on the GPU, which required quite a lot of GPU memory when encoding millions of sentences. The embeddings are now moved to CPU once they are computed.\r\n- The CrossEncoder-Class now accepts a max_length parameter to control the truncation of inputs\r\n- The Cross-Encoder predict method has now a apply_softmax parameter, that allows to apply softmax on-top of a multi-class output. ",
        "dateCreated": "2020-11-18T08:19:12Z",
        "datePublished": "2020-11-18T08:25:11Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.3.9",
        "name": "v0.3.9 - Small updates",
        "tag_name": "v0.3.9",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.3.9",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/34103033",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.3.9"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "- Add support training and using [CrossEncoder](https://www.sbert.net/docs/usage/cross-encoder.html)\r\n- Data Augmentation method [AugSBERT](https://www.sbert.net/examples/training/data_augmentation/README.html) added\r\n- New model trained on large scale paraphrase data. Models works on internal benchmark much better than previous models: **distilroberta-base-paraphrase-v1** and **xlm-r-distilroberta-base-paraphrase-v1**\r\n- New model for Information Retrieval trained on MS Marco: **distilroberta-base-msmarco-v1**\r\n- Improved MultipleNegativesRankingLoss loss function: Similarity function can be changed and is now cosine similarity (was dot-product before), further, similarity scores can be multiplied by a scaling factor. This allows the usage of NTXentLoss / InfoNCE loss.\r\n- New [MegaBatchMarginLoss](https://www.sbert.net/docs/package_reference/losses.html#megabatchmarginloss), inspired from the paper [ParaNMT-Paper](https://www.aclweb.org/anthology/P18-1042/).\r\n\r\n**Smaller changes:**\r\n- Update InformationRetrievalEvaluator, so that it can work with large corpora (Millions of entries). Removed the query_chunk_size parameter from the evaluator\r\n- SentenceTransformer.encode method detaches tensors from compute graph\r\n- SentenceTransformer.fit() method - Parameter output_path_ignore_not_empty deprecated. No longer checks that target folder must be empty\r\n",
        "dateCreated": "2020-10-19T14:11:31Z",
        "datePublished": "2020-10-19T14:23:08Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.3.8",
        "name": "v0.3.8 - CrossEncoder, Data Augmentation, new Models",
        "tag_name": "v0.3.8",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.3.8",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/32759129",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.3.8"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "- Upgrade transformers dependency, transformers 3.1.0, 3.2.0 and 3.3.1 are working\r\n- Added example code for model distillation: Sentence Embeddings models can be drastically reduced to e.g. only 2-4 layers while keeping 98+% of their performance. Code can be found in examples/training/distillation\r\n- Transformer models can now accepts two inputs ['sentence 1', 'context for sent1'], which are encoded as the two inputs for BERT.\r\n\r\n\r\nMinor changes:\r\n- Tokenization in the multi-processes encoding setup now happens in the child processes, not in the parent process.\r\n- Added models.Normalize() to allow the normalization of embeddings to unit length\r\n",
        "dateCreated": "2020-09-29T20:14:26Z",
        "datePublished": "2020-09-29T20:17:02Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.3.7",
        "name": "v0.3.7 - Upgrade transformers, Model Distillation Example, Multi-Input to Transformers Model",
        "tag_name": "v0.3.7",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.3.7",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/31956861",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.3.7"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "Hugginface Transformers version 3.1.0 had a breaking change with previous version 3.0.2\r\n\r\nThis release fixes the issue so that Sentence-Transformers is compatible with Huggingface Transformers 3.1.0. Note, that this and future version will not be compatible with transformers < 3.1.0.",
        "dateCreated": "2020-09-11T08:04:28Z",
        "datePublished": "2020-09-11T08:06:16Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.3.6",
        "name": "v0.3.6 - Update transformers to v3.1.0",
        "tag_name": "v0.3.6",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.3.6",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/31110275",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.3.6"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "- The old FP16 training code in model.fit() was replaced by using Pytorch 1.6.0 automatic mixed precision (AMP). When setting `model.fit(use_amp=True)`, AMP will be used. On suitable GPUs, this leads to a significant speed-up while requiring less memory.\r\n- Performance improvements in paraphrase mining & semantic search by replacing np.argpartition with torch.topk\r\n-  If a sentence-transformer model is not found, it will fall back to huggingface transformers repository and create it with mean pooling.\r\n- Fixing huggingface transformers to version 3.0.2. Next release will make it compatible with huggingface transformers 3.1.0\r\n- Several bugfixes: Downloading of files, mutli-GPU-encoding",
        "dateCreated": "2020-09-01T13:04:25Z",
        "datePublished": "2020-09-01T13:09:07Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.3.5",
        "name": "v0.3.5 - Automatic Mixed Precision & Bugfixes",
        "tag_name": "v0.3.5",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.3.5",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/30509087",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.3.5"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "- The documentation is substantially improved and can be found at: [www.SBERT.net](https://www.sbert.net) - Feedback welcome\r\n- The dataset to hold training InputExamples (dataset.SentencesDataset) now uses lazy tokenization, i.e., examples are tokenized once they are needed for a batch. If you set `num_workers` to a positive integer in your `DataLoader`, tokenization will happen in a background thread. This substantially increases the start-up time for training.\r\n- `model.encode()` uses also a PyTorch DataSet + DataLoader. If you set `num_workers` to a positive integer, tokenization will happen in the background leading to faster encoding speed for large corpora.\r\n- Added functions and an example for [mutli-GPU encoding](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/computing_embeddings_mutli_gpu.py) - This method can be used to encode a corpus with multiple GPUs in parallel. No multi-GPU support for training yet.\r\n- Removed parallel_tokenization parameters from encode & SentencesDatasets - No longer needed with lazy tokenization and DataLoader worker threads.\r\n- Smaller bugfixes \r\n\r\nBreaking changes:\r\n- Renamed evaluation.BinaryEmbeddingSimilarityEvaluator to evaluation.BinaryClassificationEvaluator",
        "dateCreated": "2020-08-24T16:14:56Z",
        "datePublished": "2020-08-24T16:24:24Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.3.4",
        "name": "v0.3.4 - Improved Documentation, Improved Tokenization Speed, Mutli-GPU encoding",
        "tag_name": "v0.3.4",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.3.4",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/30062092",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.3.4"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "### New Functions\r\n- Multi-process tokenization (Linux only) for the model encode function. Significant speed-up when encoding large sets\r\n- Tokenization of datasets for training can now run in parallel (Linux Only)\r\n- New example for Quora Duplicate Questions Retrieval: See examples-folder\r\n- Many small improvements for training better models for Information Retrieval\r\n- Fixed LabelSampler (can be used to get batches with certain number of matching labels. Used for BatchHardTripletLoss). Moved it to DatasetFolder\r\n- Added new Evaluators for ParaphraseMining and InformationRetrieval\r\n- evaluation.BinaryEmbeddingSimilarityEvaluator no longer assumes a 50-50 split of the dataset. It computes the optimal threshold and measure accuracy\r\n- model.encode - When the convert_to_numpy parameter is set, the method returns a numpy matrix instead of a list of numpy vectors\r\n- New function: util.paraphrase_mining to perform paraphrase mining in a corpus. For an example see examples/training_quora_duplicate_questions/\r\n- New function: util.information_retrieval to perform information retrieval / semantic search in a corpus. For an example see examples/training_quora_duplicate_questions/\r\n\r\n### Breaking Changes\r\n- The evaluators (like EmbeddingSimilarityEvaluator) no longer accept a DataLoader as argument. Instead, the sentence and scores are directly passed. Old code that uses the previous evaluators needs to be changed. They can use the class method from_input_examples(). See examples/training_transformers/training_nli.py how to use the new evaluators.",
        "dateCreated": "2020-08-06T08:01:26Z",
        "datePublished": "2020-08-06T08:16:50Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.3.3",
        "name": "v0.3.3 - Multi-Process Tokenization and Information Retrieval Improvements",
        "tag_name": "v0.3.3",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.3.3",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/29397826",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.3.3"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "This is a minor release. There should be no breaking changes.\r\n\r\n- **ParallelSentencesDataset**: Datasets are tokenized on-the-fly, saving some start-up time\r\n- **util.pytorch_cos_sim** - Method. New method to compute cosine similarity with pytorch. About 100 times faster than scipy cdist. semantic_search.py example has been updated accordingly.\r\n- **SentenceTransformer.encode**: New parameter: *convert_to_tensor*. If set to true, encode returns one large pytorch tensor with your embeddings\r\n\r\n",
        "dateCreated": "2020-07-23T14:50:17Z",
        "datePublished": "2020-07-23T15:03:59Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.3.2",
        "name": "v0.3.2 - Lazy tokenization for Parallel Sentence Training & Improved Semantic Search",
        "tag_name": "v0.3.2",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.3.2",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/28876574",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.3.2"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "This is a minor update that changes some classes for training & evaluating multilingual sentence embedding methods.\r\n\r\nThe examples for training multi-lingual sentence embeddings models have been significantly extended. See [docs/training/multilingual-models.md](https://github.com/UKPLab/sentence-transformers/blob/master/docs/training/multilingual-models.md) for details. An automatic script that downloads suitable data and extends sentence embeddings to multiple languages has been added.\r\n\r\nThe following classes/files have been changed:\r\n- datasets/ParallelSentencesDataset.py: The dataset with parallel sentences is encoded on-the-fly, reducing the start-up time for extending a sentence embedding model to new languages. An embedding cache can be configure to store previously computed sentence embeddings during training.\r\n\r\nNew evaluation files:\r\n- evaluation/MSEEvaluator.py - **breaking change**. Now, this class expects lists of strings with parallel (translated) sentences. The old class has been renamed to MSEEvaluatorFromDataLoader.py\r\n- evaluation/EmbeddingSimilarityEvaluatorFromList.py - Semantic Textual Similarity data can be passed as lists of strings & scores\r\n- evaluation/MSEEvaluatorFromDataFrame.py - MSE Evaluation of teacher and student embeddings based on data in a data frame\r\n- evaluation/MSEEvaluatorFromDataLoader.py - MSE Evaluation if data is passed as a data loader\r\n\r\n\r\n**Bugfixes:**\r\n- model.encode() failed to sort sentences by length. This function has been fixed to boost encoding speed by reducing overhead of padding tokens.",
        "dateCreated": "2020-07-22T13:45:37Z",
        "datePublished": "2020-07-22T13:54:26Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.3.1",
        "name": "v0.3.1 - Updates on Multilingual Training",
        "tag_name": "v0.3.1",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.3.1",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/28828695",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.3.1"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "This release updates HuggingFace transformers to v3.0.2. Transformers did some breaking changes to the tokenization API. This (and future) versions will not be compatible with HuggingFace transfomers v2. \r\n\r\nThere are no known breaking changes for existent models or existent code. Models trained with version 2 can be loaded without issues.\r\n\r\n# New Loss Functions\r\nThanks to PR #299 and #176 several new loss functions: Different triplet loss functions and ContrastiveLoss",
        "dateCreated": "2020-07-09T15:09:55Z",
        "datePublished": "2020-07-09T15:11:23Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.3.0",
        "name": "v0.3.0 - Transformers Updated to Version 3",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.3.0",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/28406470",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "The release update huggingface/transformers to the release v2.8.0.\r\n\r\n## New Features\r\n- **models.Transformer**: The Transformer-Model can now load any huggingface transformers model, like BERT, RoBERTa, XLNet, XLM-R, Elextra... It is based on the AutoModel from HuggingFace. You now longer need the architecture specific models (like models.BERT, models.RoBERTa) any more. It also works with the community models.\r\n- **Multilingual Training**: Code is released for making mono-lingual sentence embeddings models mutli-lingual. See training_multilingual.py for an example. More documentation and details will follow soon.\r\n- **WKPooling**: Adding a pytorch implementation of SBERT-WK. Note, due to an inefficient implementation in pytorch of QR decomposition, WKPooling can only be run on the CPU, which makes it about 40 slower than mean pooling. For some models WKPooling improves the performance, for other don't.\r\n- **WeightedLayerPooling**: A new pooling layer that uses representations from all transformer layers and learns a weighted sum of them. So far no improvement compared to only averaging the last layer.\r\n- **New pre-trained models** released. Every available model is document in a google Spreadsheet for an easier overview. \r\n\r\n## Minor changes\r\n- Clean-up of the examples folder. \r\n- Model and tokenizer arguments can now be passed to the according transformers models.\r\n- Previous version had some issues with RoBERTa and XLM-RoBERTa, that the wrong special characters were added. Everything is fixed now and relies on huggingface transformers for the correct addition of special characters to the input sentences. \r\n\r\n## Breaking changes\r\n- *STSDataReader*: The default parameter values have been changed, so that it expects the sentences in the first two columns and the score in the third column. If you want to load the STS benchmkark dataset, you can use the STSBenchmarkDataReader.",
        "dateCreated": "2020-04-16T13:58:33Z",
        "datePublished": "2020-04-16T14:12:34Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.2.6",
        "name": "v0.2.6 - Transformers Update - AutoModel - WKPooling",
        "tag_name": "v0.2.6",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.2.6",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/25573306",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.2.6"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "huggingface/transformers was updated to version 2.3.0\r\n\r\nChanges:\r\n- ALBERT works (bug was fixed in transformers). Does not yield improvements compared to BERT / RoBERTA\r\n- T5 added (does not run on GPU due to a bug in transformers). Does not yield improvements compared to BERT / RoBERTA\r\n- CamemBERT added\r\n- XML-RoBERTa added",
        "dateCreated": "2020-01-10T09:29:49Z",
        "datePublished": "2020-01-10T09:30:10Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.2.5",
        "name": "v0.2.5 - Transformers updates, T5 and XML-RoBERTa added",
        "tag_name": "v0.2.5",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.2.5",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/22736517",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.2.5"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "This version update the underlying HuggingFace Transformer package to v2.2.1.\r\n\r\n**Changes:**\r\n- DistilBERT and ALBERT modules added\r\n- Pre-trained models for RoBERTa and DistilBERT uploaded\r\n- Some smaller bug-fixes",
        "dateCreated": "2019-12-06T14:08:24Z",
        "datePublished": "2019-12-06T14:12:01Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.2.4",
        "name": "v0.2.4 - Transformer Update - DistilBERT and ALBERT added",
        "tag_name": "v0.2.4",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.2.4",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/22029276",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.2.4"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "No breaking changes. Just update with ```pip install -U sentence-transformers```\r\n\r\nBugfixes:\r\n- SentenceTransformers can now be used with Windows (threw an exception before about invalid tensor types before)\r\n- Outputs a warning if seq. length for BERT / RoBERTa is too long\r\n\r\nImprovements:\r\n- A flag can be set to hide the progress bar when a dataset is convert or an  evaluator is executed",
        "dateCreated": "2019-08-20T17:14:33Z",
        "datePublished": "2019-08-20T17:21:51Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.2.3",
        "name": "v0.2.3 - Windows bugfixes",
        "tag_name": "v0.2.3",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.2.3",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/19413217",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.2.3"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "Updated pytorch-transformers to v1.1.0. Adding support for RoBERTa model.\r\n\r\nBugfixes:\r\n- Critical bugfix for SoftmaxLoss: Classifier weights were not optimized in previous version\r\n- Minor fix for including the timestamp of the output folders",
        "dateCreated": "2019-08-19T14:28:01Z",
        "datePublished": "2019-08-19T14:38:06Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.2.2",
        "name": "v0.2.2 - RoBERTa support",
        "tag_name": "v0.2.2",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.2.2",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/19380213",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.2.2"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "This is a minor fix: Packages were not correctly defined for pypi",
        "dateCreated": "2019-08-16T21:16:46Z",
        "datePublished": "2019-08-16T21:19:12Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.2.1",
        "name": "v0.2.1 - Bugfix pypi",
        "tag_name": "v0.2.1",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.2.1",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/19349411",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "v0.2.0 completely changes the architecture of sentence transformers. \r\n\r\nThe new architecture is based on a sequential architecture: You define individual models that transform step-by-step a sentence to a fixed sized sentence embedding. \r\n\r\nThe modular architecture allows to easily swap different components. You can choose between different embedding methods (BERT, XLNet, word embeddings), transformations (LSTM, CNN), weighting & pooling methods as well as adding deep averaging networks.\r\n\r\nNew models in this release:\r\n- Word Embeddings (like GloVe) for computation of average word embeddings\r\n- Word weighting, for example, with tf-idf values\r\n- BiLSTM and CNN encoder, for example, to re-create the InferSent model\r\n- Bag-of-Words (BoW) sentence representation. Optionally also with tf-idf weighting.\r\n\r\nThis release has many breaking changes with the previous release. If you need help with the migration, open a new issue.\r\n\r\nNew model storing procedure: Each sub-module is stored in its own subfolder. If you need to migrate old models, it is best to create the subfolder structure by the system (model.save()) and then to copy the pytorch_model.bin into the correct subfolder.",
        "dateCreated": "2019-08-16T08:07:48Z",
        "datePublished": "2019-08-16T08:14:24Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.2.0",
        "name": "v0.2.0 - New Architecture & Models",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.2.0",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/19331897",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "nreimers",
        "body": "First release of sentence transformers framework",
        "dateCreated": "2019-07-25T07:59:51Z",
        "datePublished": "2019-07-25T08:02:27Z",
        "html_url": "https://github.com/UKPLab/sentence-transformers/releases/tag/v0.1.0",
        "name": "v0.1.0",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/tarball/v0.1.0",
        "url": "https://api.github.com/repos/UKPLab/sentence-transformers/releases/18854302",
        "zipball_url": "https://api.github.com/repos/UKPLab/sentence-transformers/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6730,
      "date": "Mon, 27 Dec 2021 20:34:19 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "See [Quickstart](https://www.sbert.net/docs/quickstart.html) in our documenation.\n\n\n[This example](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/computing-embeddings/computing_embeddings.py) shows you how to use an already trained Sentence Transformer model to embed sentences for another task.\n\nFirst download a pretrained model.\n````python\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n````\nThen provide some sentences to the model.\n````python\nsentences = ['This framework generates embeddings for each input sentence',\n    'Sentences are passed as a list of string.', \n    'The quick brown fox jumps over the lazy dog.']\nsentence_embeddings = model.encode(sentences)\n````\nAnd that's it already. We now have a list of numpy arrays with the embeddings.\n````python\nfor sentence, embedding in zip(sentences, sentence_embeddings):\n    print(\"Sentence:\", sentence)\n    print(\"Embedding:\", embedding)\n    print(\"\")\n````\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can use this framework for:\n- [Computing Sentence Embeddings](https://www.sbert.net/examples/applications/computing-embeddings/README.html)\n- [Semantic Textual Similarity](https://www.sbert.net/docs/usage/semantic_textual_similarity.html)\n- [Clustering](https://www.sbert.net/examples/applications/clustering/README.html)\n- [Paraphrase Mining](https://www.sbert.net/examples/applications/paraphrase-mining/README.html)\n - [Translated Sentence Mining](https://www.sbert.net/examples/applications/parallel-sentence-mining/README.html)\n - [Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html)\n - [Retrieve & Re-Rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) \n - [Text Summarization](https://www.sbert.net/examples/applications/text-summarization/README.html) \n- [Multilingual Image Search, Clustering & Duplicate Detection](https://www.sbert.net/examples/applications/image-search/README.html)\n\nand many more use-cases.\n\n\nFor all examples, see [examples/applications](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications).\n\n",
      "technique": "Header extraction"
    }
  ]
}