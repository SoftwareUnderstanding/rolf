{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1905.02244",
      "https://arxiv.org/abs/1905.02244"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.839740509351351
      ],
      "excerpt": "I'm grateful for advice from: Ching Hung, Eric Viscito, Franklyn Wang, Jagadeesh Sankaran, and Zoran Nikolic. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ekzhang/fastseg",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-22T22:11:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T03:38:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9553788638965055
      ],
      "excerpt": "This respository aims to provide accurate real-time semantic segmentation code for mobile devices in PyTorch, with pretrained weights on Cityscapes. This can be used for efficient segmentation on a variety of real-world street images, including datasets like Mapillary Vistas, KITTI, and CamVid. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9654579879015338
      ],
      "excerpt": "The models are implementations of MobileNetV3 (both large and small variants) with a modified segmentation head based on LR-ASPP. The top model was able to achieve 72.3% mIoU accuracy on Cityscapes val, while running at up to 37.3 FPS on a GPU. Please see below for detailed benchmarks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8294863499260788
      ],
      "excerpt": "Export models for production with ONNX. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "Exporting to ONNX \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8994318634422148
      ],
      "excerpt": "Added pretrained weights for MobileV3Small with 256 filters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95747504251014,
        0.9651507878230197
      ],
      "excerpt": "Implementations of MobileV3Large and MobileV3Small with LR-ASPP \nPretrained weights for MobileV3Large with 128/256 filters, and MobileV3Small with 64/128 filters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9929764156320994,
        0.9816433166907822,
        0.8905213351389448,
        0.981126792206765,
        0.941714310447468
      ],
      "excerpt": "This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small, which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. \nFor the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. \nMobileNetV3-Large LRASPP is 34% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. \nThis project tries to faithfully implement MobileNetV3 for real-time semantic segmentation, with the aims of being efficient, easy to use, and extensible. \nI was able to train a few models close to or exceeding the accuracy described in the original Searching for MobileNetV3 paper. Each was trained only on the gtFine labels from Cityscapes for around 12 hours on an Nvidia DGX-1 node, with 8 V100 GPUs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9419089107319085
      ],
      "excerpt": "The accuracy is within 0.3% of the original paper, which reported 72.6% mIoU and 3.6M parameters on the Cityscapes val set. Inference was tested on a single V100 GPU with full-resolution 2MP images (1024 x 2048) as input. It runs roughly 4x faster on half-resolution (512 x 1024) images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.8192720384262456
      ],
      "excerpt": "  --model MODEL, -m MODEL \n                        the model to export (default MobileV3Large) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8417021069159972
      ],
      "excerpt": "                        the number of filters in the segmentation head \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9681429743888365
      ],
      "excerpt": "Please see the ekzhang/semantic-segmentation repository for the training code used in this project, as well as documentation about how to train your own custom models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "\ud83d\udcf8 PyTorch implementation of MobileNetV3 for real-time semantic segmentation, with pretrained weights & state-of-the-art performance",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ekzhang/fastseg/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 27,
      "date": "Mon, 27 Dec 2021 05:53:37 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ekzhang/fastseg/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ekzhang/fastseg",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ekzhang/fastseg/master/demo/fastseg-semantic-segmentation.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8458415567237834
      ],
      "excerpt": "Currently, you can do the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8096300466379475
      ],
      "excerpt": "If you have any feature requests or questions, feel free to leave them as GitHub issues! \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from fastseg import MobileV3Large \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8222660997317658
      ],
      "excerpt": "Load pretrained MobileNetV3 semantic segmentation models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158759802076209,
        0.8360821950873047,
        0.856495151102712
      ],
      "excerpt": "| MobileV3Large |  LR-ASPP, F=128   |    3.2M    | 72.3% | 25.7 FPS  | 37.3 FPS |    \u2714     | \n| MobileV3Small |  LR-ASPP, F=256   |    1.4M    | 67.8% | 30.3 FPS  | 39.4 FPS |    \u2714     | \n| MobileV3Small |  LR-ASPP, F=128   |    1.1M    | 67.4% | 38.2 FPS  | 52.4 FPS |    \u2714     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8544647686727336
      ],
      "excerpt": "The onnx_export.py script can be used to convert a pretrained segmentation model to ONNX. You should specify the image input dimensions when exporting. See the usage instructions below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8971823465573899,
        0.9235286844008953
      ],
      "excerpt": "$ python onnx_export.py --help \nusage: onnx_export.py [-h] [--model MODEL] [--num_filters NUM_FILTERS] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8332522070684228
      ],
      "excerpt": "Command line script to export a pretrained segmentation model to ONNX. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8985124046952064
      ],
      "excerpt": "  OUTPUT_FILENAME       filename of output model (e.g., \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863782080368037
      ],
      "excerpt": "                        filename of the weights checkpoint .pth file (uses \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ekzhang/fastseg/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Fast Semantic Segmentation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fastseg",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ekzhang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ekzhang/fastseg/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "ekzhang",
        "body": "Pretrained weights for MobileNetV3-Large (F=256, F=128) and MobileNetV3-Small (F=128, F=64), on Cityscapes.",
        "dateCreated": "2020-08-11T23:16:15Z",
        "datePublished": "2020-08-11T23:45:31Z",
        "html_url": "https://github.com/ekzhang/fastseg/releases/tag/v0.1-weights",
        "name": "Pretrained weights",
        "tag_name": "v0.1-weights",
        "tarball_url": "https://api.github.com/repos/ekzhang/fastseg/tarball/v0.1-weights",
        "url": "https://api.github.com/repos/ekzhang/fastseg/releases/29583854",
        "zipball_url": "https://api.github.com/repos/ekzhang/fastseg/zipball/v0.1-weights"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This code requires Python 3.7 or later. It has been tested to work with PyTorch versions 1.5 and 1.6. To install the package, simply run `pip install fastseg`. Then you can get started with a pretrained model:\n\n```python\n#: Load a pretrained MobileNetV3 segmentation model in inference mode\nfrom fastseg import MobileV3Large\nmodel = MobileV3Large.from_pretrained().cuda()\nmodel.eval()\n\n#: Open a local image as input\nfrom PIL import Image\nimage = Image.open('street_image.png')\n\n#: Predict numeric labels [0-18] for each pixel of the image\nlabels = model.predict_one(image)\n```\n\n![Example image segmentation](https://i.imgur.com/WspmlwN.jpg)\n\nMore detailed examples are given below. As an alternative, instead of installing `fastseg` from pip, you can clone this repository and install the [`geffnet` package](https://github.com/rwightman/gen-efficientnet-pytorch) (along with other dependencies) by running `pip install -r requirements.txt` in the project root.\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The easiest way to get started with inference is to clone this repository and use the `infer.py` script. For example, if you have street images named `city_1.png` and `city_2.png`, then you can generate segmentation labels for them with the following command.\n\n```shell\n$ python infer.py city_1.png city_2.png\n```\n\nOutput:\n```\n==> Creating PyTorch MobileV3Large model\n==> Loading images and running inference\nLoading city_1.png\nGenerated colorized_city_1.png\nGenerated composited_city_1.png\nLoading city_2.png\nGenerated colorized_city_2.png\nGenerated composited_city_2.png\n```\n\n|               Original               |              Colorized               |              Composited              |\n| :----------------------------------: | :----------------------------------: | :----------------------------------: |\n| ![](https://i.imgur.com/74vqz0q.png) | ![](https://i.imgur.com/HRr16YC.png) | ![](https://i.imgur.com/WVd5a6Z.png) |\n| ![](https://i.imgur.com/MJA7VMN.png) | ![](https://i.imgur.com/FqoxHzR.png) | ![](https://i.imgur.com/fVMvbRv.png) |\n\nTo interact with the models programmatically, first install the `fastseg` package with pip, as described above. Then, you can import and construct models in your own Python code, which are instances of PyTorch `nn.Module`.\n\n```python\nfrom fastseg import MobileV3Large, MobileV3Small\n\n#: Load a pretrained segmentation model\nmodel = MobileV3Large.from_pretrained()\n\n#: Load a segmentation model from a local checkpoint\nmodel = MobileV3Small.from_pretrained('path/to/weights.pt')\n\n#: Create a custom model with random initialization\nmodel = MobileV3Large(num_classes=19, use_aspp=False, num_filters=256)\n```\n\nTo run inference on an image or batch of images, you can use the methods `model.predict_one()` and `model.predict()`, respectively. These methods take care of the preprocessing and output interpretation for you; they take PIL Images or NumPy arrays as input and return a NumPy array.\n\n(You can also run inference directly with `model.forward()`, which will return a tensor containing logits, but be sure to normalize the inputs to have mean 0 and variance 1.)\n\n```python\nimport torch\nfrom PIL import Image\nfrom fastseg import MobileV3Large, MobileV3Small\n\n#: Construct a new model with pretrained weights, in evaluation mode\nmodel = MobileV3Large.from_pretrained().cuda()\nmodel.eval()\n\n#: Run inference on an image\nimg = Image.open('city_1.png')\nlabels = model.predict_one(img) #: returns a NumPy array containing integer labels\nassert labels.shape == (1024, 2048)\n\n#: Run inference on a batch of images\nimg2 = Image.open('city_2.png')\nbatch_labels = model.predict([img, img2]) #: returns a NumPy array containing integer labels\nassert batch_labels.shape == (2, 1024, 2048)\n\n#: Run forward pass directly\ndummy_input = torch.randn(1, 3, 1024, 2048, device='cuda')\nwith torch.no_grad():\n    dummy_output = model(dummy_input)\nassert dummy_output.shape == (1, 19, 1024, 2048)\n```\n\nThe output labels can be visualized with colorized and composited images.\n\n```python\nfrom fastseg.image import colorize, blend\n\ncolorized = colorize(labels) #: returns a PIL Image\ncolorized.show()\n\ncomposited = blend(img, colorized) #: returns a PIL Image\ncomposited.show()\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 257,
      "date": "Mon, 27 Dec 2021 05:53:37 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "computer-vision",
      "semantic-segmentation",
      "mobilenetv3",
      "efficientnet",
      "deep-learning",
      "cityscapes",
      "kitti-dataset",
      "mapillary-vistas-dataset",
      "edge-computing",
      "pytorch",
      "aspp",
      "deeplabv3"
    ],
    "technique": "GitHub API"
  }
}