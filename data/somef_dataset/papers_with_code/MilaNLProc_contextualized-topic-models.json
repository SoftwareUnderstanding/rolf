{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.03974"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{bianchi-etal-2021-pre,\n    title = \"Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence\",\n    author = \"Bianchi, Federico  and\n      Terragni, Silvia  and\n      Hovy, Dirk\",\n    booktitle = \"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.acl-short.96\",\n    doi = \"10.18653/v1/2021.acl-short.96\",\n    pages = \"759--766\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{bianchi-etal-2021-cross,\n    title = \"Cross-lingual Contextualized Topic Models with Zero-shot Learning\",\n    author = \"Bianchi, Federico and Terragni, Silvia and Hovy, Dirk  and\n      Nozza, Debora and Fersini, Elisabetta\",\n    booktitle = \"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume\",\n    month = apr,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2021.eacl-main.143\",\n    pages = \"1676--1683\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": ".. _Cookiecutter: https://github.com/audreyr/cookiecutter \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479,
        0.9626356225854676,
        0.9105368110547479
      ],
      "excerpt": ".. audreyr/cookiecutter-pypackage: https://github.com/audreyr/cookiecutter-pypackage \n.. Stephen Carrow : https://github.com/estebandito22 \n.. rbo : https://github.com/dlukes/rbo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8715509475085587
      ],
      "excerpt": ".. _Silvia Terragni: https://silviatti.github.io/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": ".. _SBERT: https://www.sbert.net/docs/pretrained_models.html \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/MilaNLProc/contextualized-topic-models/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MilaNLProc/contextualized-topic-models",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-04T19:11:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T10:19:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8403800105633719,
        0.9907305951519533
      ],
      "excerpt": "a different set of test documents, this will create a BoW of a different size. Thus, the best \nway to do this is to pass just the text that is going to be given in input to the contexual model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483383539719263,
        0.9522027199710354
      ],
      "excerpt": "you can use this simple pipeline to predict the topics for documents in a different language (as long as this language \nis covered by paraphrase-multilingual-mpnet-base-v2). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9869491995001283,
        0.9739073946148652
      ],
      "excerpt": "Advanced Notes: We do not need to pass the Spanish bag of word: the bag of words of the two languages will not be comparable! We are passing it to the model for compatibility reasons, but you cannot get \nthe output of the model (i.e., the predicted BoW of the trained language) and compare it with the testing language one. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8566402996976146,
        0.9774948015152181
      ],
      "excerpt": "that are empty after training. The preprocess method will return the preprocessed and the unpreprocessed documents. \nWe generally use the unpreprocessed for BERT and the preprocessed for the Bag Of Word. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.946639054358905
      ],
      "excerpt": "Silvia Terragni_ &#115;&#46;&#116;&#101;&#114;&#114;&#97;&#103;&#110;&#105;&#52;&#64;&#99;&#97;&#109;&#112;&#117;&#115;&#46;&#117;&#110;&#105;&#109;&#105;&#98;&#46;&#105;&#116; University of Milan-Bicocca \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8223959695387696
      ],
      "excerpt": "Software Details \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9618529640464686
      ],
      "excerpt": "Super big shout-out to Stephen Carrow_ for creating the awesome https://github.com/estebandito22/PyTorchAVITM package from which we constructed the foundations of this package. We are happy to redistribute this software again under the MIT License. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723243105691277,
        0.957274130979436
      ],
      "excerpt": "This package was created with Cookiecutter_ and the audreyr/cookiecutter-pypackage project template. \nTo ease the use of the library we have also included the rbo package, all the rights reserved to the author of that package. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8328050787938486
      ],
      "excerpt": "Remember that this is a research tool :) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A python package to run contextualized topic modeling. CTMs combine contextualized embeddings (e.g., BERT) with topic models to get coherent topics. Published at EACL and ACL 2021.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MilaNLProc/contextualized-topic-models/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 87,
      "date": "Tue, 28 Dec 2021 00:30:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MilaNLProc/contextualized-topic-models/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "MilaNLProc/contextualized-topic-models",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/MilaNLProc/contextualized-topic-models/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8768243844808817
      ],
      "excerpt": "If you use ZeroShotTM you do not need to use the testing_text_for_bow because if you are using \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002226445455036
      ],
      "excerpt": "Do you need a quick script to run the preprocessing pipeline? We got you covered! Load your documents \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9283560026429053,
        0.8918974083095406,
        0.8274642073325015
      ],
      "excerpt": ".. pytorch: https://pytorch.org/get-started/locally/ \n.. _Cookiecutter: https://github.com/audreyr/cookiecutter \n.. _preprocessing: https://github.com/MilaNLProc/contextualized-topic-models#preprocessing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": ".. audreyr/cookiecutter-pypackage: https://github.com/audreyr/cookiecutter-pypackage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": ".. rbo : https://github.com/dlukes/rbo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732089505944296
      ],
      "excerpt": ".. _Dirk Hovy: https://dirkhovy.com/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8491737638821341
      ],
      "excerpt": ".. _HuggingFace: https://huggingface.co/models \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8835282915431637,
        0.8215170768466401
      ],
      "excerpt": "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing \ndocuments = [line.strip() for line in open(\"unpreprocessed_documents.txt\").readlines()] \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MilaNLProc/contextualized-topic-models/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Bjarte Mehus Sunde\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "contextualized-topic-models",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "MilaNLProc",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MilaNLProc/contextualized-topic-models/blob/master/README.rst",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    ctm.get_topics(2)\n\n\n**Advanced Notes:** Combined TM combines the BoW with SBERT, a process that seems to increase\nthe coherence of the predicted topics (https://arxiv.org/pdf/2004.03974.pdf).\n\nZero-Shot Topic Model\n~~~~~~~~~~~~~~~~~~~~~\n\nOur ZeroShotTM can be used for zero-shot topic modeling. It can handle words that are not used during the training phase.\nMore interestingly, this model can be used for cross-lingual topic modeling (See next sections)! See the paper (https://www.aclweb.org/anthology/2021.eacl-main.143)\n\n.. code-block:: python\n\n    from contextualized_topic_models.models.ctm import ZeroShotTM\n    from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n    from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file\n\n    text_for_contextual = [\n        \"hello, this is unpreprocessed text you can give to the model\",\n        \"have fun with our topic model\",\n    ]\n\n    text_for_bow = [\n        \"hello unpreprocessed give model\",\n        \"fun topic model\",\n    ]\n\n    qt = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\")\n\n    training_dataset = qt.fit(text_for_contextual=text_for_contextual, text_for_bow=text_for_bow)\n\n    ctm = ZeroShotTM(bow_size=len(qt.vocab), contextual_size=768, n_components=50)\n\n    ctm.fit(training_dataset) ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    ctm.get_topics(2)\n\n\nAs you can see, the high-level API to handle the text is pretty easy to use;\n**text_for_bert** should be used to pass to the model a list of documents that are not preprocessed.\nInstead, to **text_for_bow** you should pass the preprocessed text used to build the BoW.\n\n**Advanced Notes:** in this way, SBERT can use all the information in the text to generate the representations.\n\nUsing The Topic Models\n----------------------\n\nGetting The Topics\n~~~~~~~~~~~~~~~~~~\n\nOnce the model is trained, it is very easy to get the topics!\n\n.. code-block:: python\n\n    ctm.get_topics()\n\nPredicting Topics For Unseen Documents\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe **transform** method will take care of most things for you, for example the generation\nof a corresponding BoW by considering only the words that the model has seen in training.\nHowever, this comes with some bumps when dealing with the ZeroShotTM, as we will se in the next section.\n\nYou can, however, manually load the embeddings if you like (see the Advanced part of this documentation).\n\nMono-Lingual Topic Modeling\n===========================\n\nIf you use **CombinedTM** you need to include the test text for the BOW:\n\n.. code-block:: python\n\n    testing_dataset = qt.transform(text_for_contextual=testing_text_for_contextual, text_for_bow=testing_text_for_bow)\n\n    ",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 740,
      "date": "Tue, 28 Dec 2021 00:30:55 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "topic-modeling",
      "bert",
      "transformer",
      "embeddings",
      "text-as-data",
      "topic-coherence",
      "multilingual-topic-models",
      "multilingual-models",
      "neural-topic-models",
      "nlp",
      "nlp-library",
      "nlp-machine-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    testing_dataset = qt.transform(testing_text_for_contextual)\n\n    ",
      "technique": "Header extraction"
    }
  ]
}