{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1711.09349",
      "https://arxiv.org/abs/1703.07737",
      "https://arxiv.org/abs/1711.09349"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737)\n- [Beyond Part Models: Person Retrieval with Refined Part Pooling](https://arxiv.org/abs/1711.09349)\n- [open-reid](https://github.com/Cysu/open-reid)\n- [Re-ranking Person Re-identification with k-reciprocal Encoding](https://github.com/zhunzhong07/person-re-ranking)\n- [Market1501](http://www.liangzheng.org/Project/project_reid.html)\n- [CUHK03](http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html)\n- [DukeMTMC-reID](https://github.com/layumi/DukeMTMC-reID_evaluation)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhong2017re,\n  title={Re-ranking Person Re-identification with k-reciprocal Encoding},\n  author={Zhong, Zhun and Zheng, Liang and Cao, Donglin and Li, Shaozi},\n  booktitle={CVPR},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "| Duke-S1       | 79.76 | 64.27 | 85.32 | 81.48 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "--steps_per_log 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "--rank_list_size 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "- identities_per_batch = 32, images_per_identity = 4, images_per_batch = 32 x 4 = 128 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huanghoujing/person-reid-triplet-loss-baseline",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-01-27T10:02:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T13:04:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8736543622115642
      ],
      "excerpt": "- ResNet-50, stride = 2 or stride = 1 in last conv block \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9433119905738605
      ],
      "excerpt": "- Only horizontal flipping used for data augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216045500671545
      ],
      "excerpt": "The results are as follows. S1 and S2 means stride = 1 and stride = 2 respectively; R.R. means using re-ranking. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9055459273815957,
        0.8909042813558767,
        0.9031049557310601
      ],
      "excerpt": "We see that stride = 1 (higher spatial resolution before global pooling) has obvious improvement over stride = 2 (original ResNet). I tried this inspired by paper Beyond Part Models: Person Retrieval with Refined Part Pooling. \nOther details of setting can be found in the code. To test my trained models or reproduce these results, see the Examples section. \nThis repository contains following resources \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254851895115763,
        0.8948006896051919
      ],
      "excerpt": "We follow the new training/testing protocol proposed in paper \nDetails of the new protocol can be found here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.805792410273851
      ],
      "excerpt": "The project requires you to configure the dataset paths. In tri_loss/dataset/__init__.py, modify the following snippet according to your saving paths used in preparing datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.892018147402862
      ],
      "excerpt": "    \"Only trainval part of the combined dataset is available now.\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9942275094420175
      ],
      "excerpt": "Datasets used in this project all follow the standard evaluation protocol of Market1501, using CMC and mAP metric. According to open-reid, the setting of CMC is as follows \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8159167554083513
      ],
      "excerpt": ": Compute all kinds of CMC scores \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8459557899083837
      ],
      "excerpt": "You can use a trained model to extract features for a list of images, and then perform whatever you desire with these features. An example is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9817457026987487
      ],
      "excerpt": "For more usage of TensorBoard, see the website and the help: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9400411206848639
      ],
      "excerpt": "Note that the following time consumption is not gauranteed across machines, especially when the system is busy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8469395846507566
      ],
      "excerpt": "- ResNet-50, stride=1 in last block \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Rank-1 89% (Single Query) on Market1501 with raw triplet loss, In Defense of the Triplet Loss for Person Re-Identification, using Pytorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huanghoujing/person-reid-triplet-loss-baseline/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 125,
      "date": "Tue, 28 Dec 2021 21:46:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/huanghoujing/person-reid-triplet-loss-baseline/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "huanghoujing/person-reid-triplet-loss-baseline",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Larger training set tends to benefit deep learning models, so I combine trainval set of three datasets Market1501, CUHK03 and DukeMTMC-reID. After training on the combined trainval set, the model can be tested on three test sets as usual.\n\nTransform three separate datasets as introduced above if you have not done it.\n\nFor the trainval set, you can download what I have transformed from [Google Drive](https://drive.google.com/open?id=1hmZIRkaLvLb_lA1CcC4uGxmA4ppxPinj) or [BaiduYun](https://pan.baidu.com/s/1jIvNYPg). Otherwise, you can run the following script to combine the trainval sets, replacing the paths with yours.\n\n```bash\npython script/dataset/combine_trainval_sets.py \\\n--market1501_im_dir ~/Dataset/market1501/images \\\n--market1501_partition_file ~/Dataset/market1501/partitions.pkl \\\n--cuhk03_im_dir ~/Dataset/cuhk03/detected/images \\\n--cuhk03_partition_file ~/Dataset/cuhk03/detected/partitions.pkl \\\n--duke_im_dir ~/Dataset/duke/images \\\n--duke_partition_file ~/Dataset/duke/partitions.pkl \\\n--save_dir ~/Dataset/market1501_cuhk03_duke\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Inspired by Tong Xiao's [open-reid](https://github.com/Cysu/open-reid) project, dataset directories are refactored to support a unified dataset interface.\n\nTransformed dataset has following features\n- All used images, including training and testing images, are inside the same folder named `images`\n- Images are renamed, with the name mapping from original images to new ones provided in a file named `ori_to_new_im_name.pkl`. The mapping may be needed in some cases.\n- The train/val/test partitions are recorded in a file named `partitions.pkl` which is a dict with the following keys\n  - `'trainval_im_names'`\n  - `'trainval_ids2labels'`\n  - `'train_im_names'`\n  - `'train_ids2labels'`\n  - `'val_im_names'`\n  - `'val_marks'`\n  - `'test_im_names'`\n  - `'test_marks'`\n- Validation set consists of 100 persons (configurable during transforming dataset) unseen in training set, and validation follows the same ranking protocol of testing.\n- Each val or test image is accompanied by a mark denoting whether it is from\n  - query (`mark == 0`), or\n  - gallery (`mark == 1`), or\n  - multi query (`mark == 2`) set\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "It's recommended that you create and enter a python virtual environment, if versions of the packages required here conflict with yours.\n\nI use Python 2.7 and Pytorch 0.3. For installing Pytorch, follow the [official guide](http://pytorch.org/). Other packages are specified in `requirements.txt`.\n\n```bash\npip install -r requirements.txt\n```\n\nThen clone the repository:\n\n```bash\ngit clone https://github.com/huanghoujing/person-reid-triplet-loss-baseline.git\ncd person-reid-triplet-loss-baseline\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9069227768992888
      ],
      "excerpt": "Python version Re-ranking (Originally from re_ranking) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9174008511802688
      ],
      "excerpt": "Download the Market1501 dataset from here. Run the following script to transform the dataset, replacing the paths with yours. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9174008511802688
      ],
      "excerpt": "Download the DukeMTMC-reID dataset from here. Run the following script to transform the dataset, replacing the paths with yours. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044935986319869
      ],
      "excerpt": "if name == 'market1501': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "elif name == 'cuhk03': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "elif name == 'duke': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8403623147943649
      ],
      "excerpt": "To play with different CMC options, you can modify it accordingly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8685639828912847
      ],
      "excerpt": "- a dataset name (one of market1501, cuhk03, duke) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8685639828912847
      ],
      "excerpt": "- a dataset name (one of ['market1501', 'cuhk03', 'duke']) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.829522273338649
      ],
      "excerpt": ": Modify the path for --logdir accordingly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8685639828912847
      ],
      "excerpt": "- a dataset name (one of ['market1501', 'cuhk03', 'duke']) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8867395261905681
      ],
      "excerpt": "For following settings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329310167911435
      ],
      "excerpt": "it occupies ~11000MB GPU memory. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9078842495185558,
        0.8908593873052805
      ],
      "excerpt": "python script/dataset/transform_market1501.py \\ \n--zip_file ~/Dataset/market1501/Market-1501-v15.09.15.zip \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8898153642404131
      ],
      "excerpt": "Download the CUHK03 dataset from here. Then download the training/testing partition file from Google Drive or BaiduYun. This partition file specifies which images are in training, query or gallery set. Finally run the following script to transform the dataset, replacing the paths with yours. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078842495185558,
        0.8414146104581761
      ],
      "excerpt": "python script/dataset/transform_cuhk03.py \\ \n--zip_file ~/Dataset/cuhk03/cuhk03_release.zip \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078842495185558,
        0.8414146104581761
      ],
      "excerpt": "python script/dataset/transform_duke.py \\ \n--zip_file ~/Dataset/duke/DukeMTMC-reID.zip \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649171472671949
      ],
      "excerpt": ": In file tri_loss/dataset/init.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061498537195642
      ],
      "excerpt": ": Specify Directory and Partition File #: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076713752689532
      ],
      "excerpt": "  im_dir = ospeu('~/Dataset/market1501/images') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "elif name == 'cuhk03': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076713752689532
      ],
      "excerpt": "  im_dir = ospeu(ospj('~/Dataset/cuhk03', im_type, 'images')) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179,
        0.8076713752689532
      ],
      "excerpt": "elif name == 'duke': \n  im_dir = ospeu('~/Dataset/duke/images') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649171472671949
      ],
      "excerpt": ": In file tri_loss/dataset/init.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "                  first_match_break=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8903108959062577
      ],
      "excerpt": ": In open-reid's reid/evaluators.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179188390243047,
        0.8594142235991984
      ],
      "excerpt": "  'cuhk03': dict(separate_camera_set=True, \n                 single_gallery_shot=True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "                     first_match_break=True)} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8650477492756585
      ],
      "excerpt": "python script/experiment/infer_images_example.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8394254323735771
      ],
      "excerpt": "- a dataset name (one of market1501, cuhk03, duke) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9131568179548282
      ],
      "excerpt": "python script/experiment/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "--only_test true \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8394254323735771
      ],
      "excerpt": "- a dataset name (one of ['market1501', 'cuhk03', 'duke']) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123015943946607
      ],
      "excerpt": "- an experiment directory for saving training log \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9131568179548282
      ],
      "excerpt": "python script/experiment/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8394254323735771
      ],
      "excerpt": "- a dataset name (one of ['market1501', 'cuhk03', 'duke']) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8650477492756585
      ],
      "excerpt": "python script/experiment/visualize_rank_list.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356542243677364
      ],
      "excerpt": "Taking Market1501 as an example \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/huanghoujing/person-reid-triplet-loss-baseline/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Current Results",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "person-reid-triplet-loss-baseline",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "huanghoujing",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huanghoujing/person-reid-triplet-loss-baseline/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 441,
      "date": "Tue, 28 Dec 2021 21:46:41 GMT"
    },
    "technique": "GitHub API"
  }
}