{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Provided codes were adapted from:\n\n- [facebookresearch/pycls](https://github.com/facebookresearch/pycls)\n- [pytorch/examples](https://github.com/pytorch/examples/)\n- [open-mmlab/mmcv](https://github.com/open-mmlab/mmcv)\n\nI strongly recommend you to choose [pycls](https://github.com/facebookresearch/pycls), a brilliant image classification codebase and adopted by a number of projects at [Facebook AI Research](https://github.com/facebookresearch).\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2101.11605"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@misc{bigballon2021distribuuuu,\n  author = {Wei Li},\n  title = {Distribuuuu: The pure and clear PyTorch Distributed Training Framework},\n  howpublished = {\\url{https://github.com/BIGBALLON/distribuuuu}},\n  year = {2021}\n}\n```\n\nFeel free to contact me if you have any suggestions or questions, issues are welcome,\ncreate a PR if you find any bugs or you want to contribute. :cake:\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{bigballon2021distribuuuu,\n  author = {Wei Li},\n  title = {Distribuuuu: The pure and clear PyTorch Distributed Training Framework},\n  howpublished = {\\url{https://github.com/BIGBALLON/distribuuuu}},\n  year = {2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "|                   resnet18                   |  11.690   | 16384 (25664GPUs) |  12.8   | 68.766 | 88.381 |                                                                                                                                    | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BIGBALLON/distribuuuu",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-18T09:13:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T16:35:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Distribuuuu is a Distributed Classification Training Framework powered by native PyTorch.\n\nPlease check [tutorial](./tutorial/) for detailed **Distributed Training** tutorials:\n\n- Single Node Single GPU Card Training [[snsc.py](./tutorial/snsc.py)]\n- Single Node Multi-GPU Cards Training (with DataParallel) [[snmc_dp.py](./tutorial/snmc_dp.py)]\n- Multiple Nodes Multi-GPU Cards Training (with DistributedDataParallel)\n    - torch.distributed.launch [[mnmc_ddp_launch.py](./tutorial/mnmc_ddp_launch.py)]\n    - torch.multiprocessing [[mnmc_ddp_mp.py](./tutorial/mnmc_ddp_mp.py)]\n    - Slurm Workload Manager [[mnmc_ddp_slurm.py](./tutorial/mnmc_ddp_slurm.py)]\n- ImageNet training example [[imagenet.py](./tutorial/imagenet.py)]\n\nFor the complete training framework, please see [distribuuuu](./distribuuuu/). \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9547007391285199,
        0.8362283929179366,
        0.830550358813181,
        0.8818325926775392
      ],
      "excerpt": "We use a reference learning rate of 0.1 and a weight decay of 5e-5 (1e-5 For EfficientNet). \nThe actual learning rate(Base LR) for each model is computed as (batch-size / 128) * reference-lr. \nOnly standard data augmentation techniques(RandomResizedCrop and RandomHorizontalFlip) are used. \nPS: use other robust tricks(more epochs, efficient data augmentation, etc.) to get better performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The pure and clear PyTorch Distributed Training Framework.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BIGBALLON/distribuuuu/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Before PyTorch1.8, ``torch.distributed.launch`` will leave some zombie processes after using  ``Ctrl`` + ``C``, try to use the following cmd to kill the zombie processes. ([fairseq/issues/487](https://github.com/pytorch/fairseq/issues/487)):\n\n```bash\nkill $(ps aux | grep YOUR_SCRIPT.py | grep -v grep | awk '{print $2}')\n```\n\nPyTorch >= 1.8 is suggested, which fixed the issue about zombie process. ([pytorch/pull/49305](https://github.com/pytorch/pytorch/pull/49305))\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 39,
      "date": "Sun, 26 Dec 2021 16:11:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BIGBALLON/distribuuuu/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "BIGBALLON/distribuuuu",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/BIGBALLON/distribuuuu/master/.dev/pre-commit.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "``` bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "``` bash \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8179912547438893
      ],
      "excerpt": "Download the ImageNet dataset and move validation images to labeled subfolders, using the script valprep.sh.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "|_ train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8811267689826031
      ],
      "excerpt": "|                     Arch                     | Params(M) |    Total batch     | Base LR | Acc@1  | Acc@5  |                                                           model / config                                                           | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BIGBALLON/distribuuuu/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Wei Li\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "introduction)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "distribuuuu",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "BIGBALLON",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BIGBALLON/distribuuuu/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install **PyTorch>= 1.6** (has been tested on **1.6, 1.7.1, 1.8** and **1.8.1**)\n- Install other dependencies: ``pip install -r requirements.txt``\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 139,
      "date": "Sun, 26 Dec 2021 16:11:41 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "distributed",
      "imagenet",
      "training",
      "classification",
      "botnet",
      "transformer"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Single Node with one task\n\n\n``` bash\n#: 1 node, 8 GPUs\npython -m torch.distributed.launch \\\n    --nproc_per_node=8 \\\n    --nnodes=1 \\\n    --node_rank=0 \\\n    --master_addr=localhost \\\n    --master_port=29500 \\\n    train_net.py --cfg config/resnet18.yaml\n```\n\nDistribuuuu use [yacs](https://github.com/rbgirshick/yacs), a elegant and lightweight package to define and manage system configurations.\nYou can setup config via a yaml file, and overwrite by other opts. If the yaml is not provided, the default configuration file will be used, please check [distribuuuu/config.py](./distribuuuu/config.py).\n\n```bash\npython -m torch.distributed.launch \\\n    --nproc_per_node=8 \\\n    --nnodes=1 \\\n    --node_rank=0 \\\n    --master_addr=localhost \\\n    --master_port=29500 \\\n    train_net.py --cfg config/resnet18.yaml \\\n    OUT_DIR /tmp \\\n    MODEL.SYNCBN True \\\n    TRAIN.BATCH_SIZE 256\n\n#: --cfg config/resnet18.yaml parse config from file\n#: OUT_DIR /tmp            overwrite OUT_DIR\n#: MODEL.SYNCBN True       overwrite MODEL.SYNCBN\n#: TRAIN.BATCH_SIZE 256    overwrite TRAIN.BATCH_SIZE\n```\n\n\n<details>\n  <summary>Single Node with two tasks</summary>\n\n\n```bash\n#: 1 node, 2 task, 4 GPUs per task (8GPUs)\n#: task 1:\nCUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch \\\n    --nproc_per_node=4 \\\n    --nnodes=2 \\\n    --node_rank=0 \\\n    --master_addr=localhost \\\n    --master_port=29500 \\\n    train_net.py --cfg config/resnet18.yaml\n\n#: task 2:\nCUDA_VISIBLE_DEVICES=4,5,6,7 python -m torch.distributed.launch \\\n    --nproc_per_node=4 \\\n    --nnodes=2 \\\n    --node_rank=1 \\\n    --master_addr=localhost \\\n    --master_port=29500 \\\n    train_net.py --cfg config/resnet18.yaml\n```\n\n</details>\n\n<details>\n  <summary>Multiple Nodes Training</summary>\n\n```bash\n#: 2 node, 8 GPUs per node (16GPUs)\n#: node 1:\npython -m torch.distributed.launch \\\n    --nproc_per_node=8 \\\n    --nnodes=2 \\\n    --node_rank=0 \\\n    --master_addr=\"10.198.189.10\" \\\n    --master_port=29500 \\\n    train_net.py --cfg config/resnet18.yaml\n\n#: node 2:\npython -m torch.distributed.launch \\\n    --nproc_per_node=8 \\\n    --nnodes=2 \\\n    --node_rank=1 \\\n    --master_addr=\"10.198.189.10\" \\\n    --master_port=29500 \\\n    train_net.py --cfg config/resnet18.yaml\n```\n\n</details>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n#: see srun --help \n#: and https://slurm.schedmd.com/ for details\n\n#: example: 64 GPUs\n#: batch size = 64 * 128 = 8192\n#: itertaion = 128k / 8192 = 156 \n#: lr = 64 * 0.1 = 6.4\n\nsrun --partition=openai-a100 \\\n     -n 64 \\\n     --gres=gpu:8 \\\n     --ntasks-per-node=8 \\\n     --job-name=Distribuuuu \\\n     python -u train_net.py --cfg config/resnet18.yaml \\\n     TRAIN.BATCH_SIZE 128 \\\n     OUT_DIR ./resnet18_8192bs \\\n     OPTIM.BASE_LR 6.4\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}