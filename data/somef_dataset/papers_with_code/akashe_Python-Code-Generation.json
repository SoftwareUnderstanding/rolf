{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/akashe/Python-Code-Generation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-01T19:33:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-07T07:13:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9873740019577791
      ],
      "excerpt": "The code in the repository doesn't create the right code for any possible python question. It is trained on a small data and creates a good program related to the questions present in the data. However, it still doesn't understand indentation properly for very long programs. After a ':' model correctly predicts the amount of indentation for the line but fails to capture which statements to keep in which indent specially for a very long program. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8970658432896395,
        0.8011757234063362,
        0.9370381977972196,
        0.9529651275684664
      ],
      "excerpt": "Removing duplicate question answer pairs: Original data had many duplicate questions and python codes submitted by as same assignment by different team members. After removing duplicate pairs, the total unique question answer pair we about 3100 as compared to 4600+ original pairs. \nTokenization: As said earlier, we used python's own tokenizer. There was a problem with it though. It took strings like the ones present in print('Akash Kumar') as a seperate string token 'Akash Kumar'. This unnecessarily increase vocab size. So tokenized these strings as characters to increase models verstality. \nFormatting with proper indentation: Data had multiple indentation schemes. We identify the indent required and finally replace it with corresponding '\\t' to keep sequence length smaller. \nPrimarily we used Cross Entropy as our loss function. We experimented with an additional penalty for code that fails execution but: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9858325557547575,
        0.8982980960678882
      ],
      "excerpt": "2. Model didn't learn. Since there is no way for the parameters to find gradients wrt to actual execution of the scripts, we multiplied it as a separate constant to the loss value. This changes the gradient value and naturally it didn't work. But we tried to see if we can atleast have some rudimentary learning which we can adjust with the punishment_constant we chose as a hyperparameter. \nWe created python embeddings using CONALA mined dataset. The dataset consists of 590763 snippets of python. We train Decoder only transformer architecture and train it in an autoregressive manner. The task is simple to predict the next word given an input token. We train embeddings for a total of 15018 tokens which we got after using pythons in built tokenizer on the CONALA mined dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894661096215074,
        0.9518503021151407,
        0.9871969219164987
      ],
      "excerpt": "Architecture is same as mentioned in the paper \"Attention is all you need.\". It's an encoder decoder model with the natural language prompt processed by the encoder and code generation by the decoder using multi-headed and self attention. \nWe used Rouge-L metric which matches the longest subsequence. In code, there is a fixed structure with snippets following each other to build on previous snippets. In machine translation, the same words can come in the beginning and at the end to form the same meaning so n-grams based evaluation metric makes sense. \nSince in the code, n-grams presenting anywhere doesn't make sense, we chose ROUGE-L metric. It gives score according to matching of the longest common subsequence in target and output codes. We get a maximum of 15.8 ROUGE-L score on the validation set. Refer this file for code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8982788629305294
      ],
      "excerpt": "    In this file, we do similar things as above we just used a char vocab for the decoder. We realized, that decoder outputs didn't have space between statements like 'def gdc(x,y)' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8862211583404046,
        0.9534821980558519,
        0.8564445052042284,
        0.896352199324433
      ],
      "excerpt": "    In this file, we trained decoder embeddings for python tokens using 590763 instances of mined data in conala-mined dataset. The embeddings along with their corresponding vocab are present in the data folder. \nConala data with original data: \n    Similar to details in 1. Here, we trained our model on more data from conala train and test files from CONALA dataset.     \nConala data with original with python Embeddings: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Using language model with attention mechanisms to write python code from an English prompt.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/akashe/Python-Code-Generation/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": " 2. Replace tabs with \"    \": This helps to keep same indentation scheme in the file. Specially for cases with indentation scheme as 4-3-2 spaces.\n 3. Replacing multiple line declarations of variables: We use python's own [tokenizer](https://docs.python.org/3/library/tokenize.html). It was creating problems with multiline declarations.  \n  \n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 07:14:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/akashe/Python-Code-Generation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "akashe/Python-Code-Generation",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/akashe/Python-Code-Generation/main/Embedding_layer_for_python.ipynb",
      "https://raw.githubusercontent.com/akashe/Python-Code-Generation/main/Notebook_with_model_code_improved_for_jit_script.ipynb",
      "https://raw.githubusercontent.com/akashe/Python-Code-Generation/main/Vanilla_Enocder_Decoder_Architecture_with_character_wise_decoder_vocab.ipynb",
      "https://raw.githubusercontent.com/akashe/Python-Code-Generation/main/Original_data_with_penalty.ipynb",
      "https://raw.githubusercontent.com/akashe/Python-Code-Generation/main/Vanilla_Enocder_Decoder_Architecture.ipynb",
      "https://raw.githubusercontent.com/akashe/Python-Code-Generation/main/Conala_with_original_data.ipynb",
      "https://raw.githubusercontent.com/akashe/Python-Code-Generation/main/Python_Embeddings_on_CoNaLa_mined_data.ipynb",
      "https://raw.githubusercontent.com/akashe/Python-Code-Generation/main/Conala_with_original_data_with_python_embeddings.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8035052572997448
      ],
      "excerpt": "In addition to 3100 examples from the original data we add additional 2800 examples from conala-train and conala-test datasets. The datasets are of same format with a natural language prompt for a python code and the corresponding python snippet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8430428535199911
      ],
      "excerpt": "Training python embeddings: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8448941167946358
      ],
      "excerpt": "    Similar to details in 1. Here, we trained our model on more data from conala train and test files from CONALA dataset.     \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/akashe/Python-Code-Generation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Python Code Generation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Python-Code-Generation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "akashe",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/akashe/Python-Code-Generation/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 21 Dec 2021 07:14:13 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "big-code",
      "source-code-modelling",
      "source-code-generator",
      "python-source-code-generation",
      "program-generation",
      "english-to-python"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Check the [file here for example outputs](https://github.com/akashe/Python-Code-Generation/blob/main/data/example_output.txt) for better formatting. Refer [this file](https://github.com/akashe/Python-Code-Generation/blob/main/Conala_with_original_data_with_python_embeddings.ipynb) for code.\n\n",
      "technique": "Header extraction"
    }
  ]
}