{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.03385"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [A 10 million Image Database for Scene Recognition](http://places2.csail.mit.edu/PAMI_places.pdf) - please cite this paper if you use the VGG16-places365 or VGG16-hybrid1365 model in your work.\n- [Learning Deep Features for Scene Recognition using Places Database](https://arxiv.org/abs/1512.03385)\n\n\nAdditionally, don't forget to cite this repo if you use these models:\n\n    @misc{gkallia2017keras_places365,\n    title={Keras-VGG16-Places365},\n    author={Grigorios Kalliatakis},\n    year={2017},\n    publisher={GitHub},\n    howpublished={\\url{https://github.com/GKalliatakis/Keras-VGG16-places365}},\n    }\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{gkallia2017keras_places365,\ntitle={Keras-VGG16-Places365},\nauthor={Grigorios Kalliatakis},\nyear={2017},\npublisher={GitHub},\nhowpublished={\\url{https://github.com/GKalliatakis/Keras-VGG16-places365}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9945803934995224
      ],
      "excerpt": "Places: A 10 million Image Database for Scene Recognition \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999962795077325,
        0.9990920020794191
      ],
      "excerpt": "IEEE Transactions on Pattern Analysis and Machine Intelligence \nPlease consider citing the above paper if you use the pre-trained CNN models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8592871015078041,
        0.8829021559993814
      ],
      "excerpt": "TEST_IMAGE_URL = 'http://places2.csail.mit.edu/imgs/demo/6.jpg' \nimage = Image.open(urllib2.urlopen(TEST_IMAGE_URL)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8592871015078041,
        0.8829021559993814
      ],
      "excerpt": "TEST_IMAGE_URL = 'http://places2.csail.mit.edu/imgs/demo/6.jpg' \nimage = Image.open(urllib2.urlopen(TEST_IMAGE_URL)) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GKalliatakis/Keras-VGG16-places365",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "On Github Issues and Pull Requests\nFound a bug? Have a new feature to suggest? Want to contribute changes to the codebase? Make sure to read this first.\nBug reporting\nYour code doesn't work, and you have determined that the issue lies with Keras-VGG16-places365? Follow these steps to report a bug.\n\n\nYour bug may already be fixed. Make sure to update to the current Keras master branch (pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps), as well as the latest Theano/TensorFlow/CNTK master branch.\nTo easily update Theano: pip install git+git://github.com/Theano/Theano.git --upgrade\n\n\nSearch for similar issues. It's possible somebody has encountered this bug already. Also remember to check out Keras' FAQ. Still having a problem? Open an issue on Github to let us know.\n\n\nMake sure you provide us with useful information about your configuration: what OS are you using? What Keras backend are you using? Are you running on GPU?\n\n\nProvide us with a script to reproduce the issue. This script should be runnable as-is and should not require external data download (use randomly generated data if you need to run a model on some test data). We recommend that you use Github Gists to post your code.\n\n\nIf possible, take a stab at fixing the bug yourself --if you can!\n\n\nThe more information you provide, the easier it is for us to validate that there is a bug and the faster we'll be able to take action. If you want your issue to be resolved quickly, following the steps above is crucial.\n\nRequesting a Feature\nYou can also use Github issues to request features you would like to see in Keras-VGG16-places365.\n\n\nProvide a clear and detailed explanation of the feature you want and why it's important to add. \n\n\nProvide code snippets demonstrating the API you have in mind and illustrating the use cases of your feature. Of course, you don't need to write any real code at this point!\n\n\nAfter discussing the feature you may choose to attempt a Pull Request. If you're at all able, start writing some code. We always have more work to do than time to do it. If you can write some code then that will speed the process along.\n\n\n\nPull Requests\nWhere should I submit my pull request?\n\nKeras-VGG16-places365 improvements and bugfixes go to the Keras-VGG16-places365 master branch.\n\nHere's a quick guide to submitting your improvements:\n\n\nWrite the code (or get others to write it). This is the hard part!\n\n\nWhen committing, use appropriate, descriptive commit messages.\n\n\nUpdate the documentation. If introducing new functionality, make sure you include code snippets demonstrating the usage of your new feature.\n\n\nSubmit your PR. If your changes have been approved in a previous discussion, your PR is likely to be merged promptly.\n\n\n\nAdding new examples\nEven if you don't contribute to the Keras-VGG16-places365 source code, if you have an application of Keras-VGG16-places365 that is concise and powerful, please consider adding it to our collection of examples.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-10-22T15:32:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T00:18:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9236687258137168,
        0.8346532264052557,
        0.9535087271882975
      ],
      "excerpt": "CNNs trained on Places365 database (latest subset of Places2 Database) could be directly used for scene recognition, while the deep scene features from the higher level layer of CNN could be used as generic features for visual recognition. \nThe Keras models has been obtained by directly converting the Caffe models provived by the authors (all the original Caffe-based resources can be found there). \nMore details about the architecture of the networks can be found in the following paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.966465745146067,
        0.8924212690395703,
        0.8924212690395703
      ],
      "excerpt": "This repository contains code for the following Keras models: \n- VGG16-places365 \n- VGG16-hybrid1365 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895608950120991
      ],
      "excerpt": "with open(file_name) as class_file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8797292755747159
      ],
      "excerpt": "features = model.predict(image) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9425503449256115,
        0.9472267703435439,
        0.934596275306482,
        0.8864767730166131
      ],
      "excerpt": "All code in this repository is under the MIT license as specified by the LICENSE file. \nThe VGG16-places365 and VGG16-hybrid1365 weights were originally ported from the ones released by CSAILVision under the MIT license but resulted in an identical & low confidence predictions issue. \nThe current version of the VGG16-places365 and VGG16-hybrid1365 weights are the ones released by landmark-recognition-challenge under the GNU General Public License v3.0 \nWe are always interested in how these models are being used, so if you found them useful or plan to make a release of code based on or using this package, it would be great to hear from you. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Keras code and weights files for the VGG16-places365 and VGG16-hybrid1365 CNNs for scene classification",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GKalliatakis/Keras-VGG16-places365/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 64,
      "date": "Sun, 26 Dec 2021 08:43:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "GKalliatakis/Keras-VGG16-places365",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8401558704798054,
        0.9133368656218674,
        0.9457175861910134,
        0.8044146895679722,
        0.8801854956928516,
        0.8801854956928516
      ],
      "excerpt": "import os \nimport urllib2 \nimport numpy as np \nfrom PIL import Image \nfrom cv2 import resize \nfrom vgg16_places_365 import VGG16_Places365 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8959822723709694
      ],
      "excerpt": "image = np.array(image, dtype=np.uint8) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468
      ],
      "excerpt": "top_preds = np.argsort(preds)[::-1][0:predictions_to_return] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440486680665517
      ],
      "excerpt": "    print(classes[top_preds[i]]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674,
        0.9457175861910134,
        0.8044146895679722,
        0.8801854956928516,
        0.8801854956928516
      ],
      "excerpt": "import urllib2 \nimport numpy as np \nfrom PIL import Image \nfrom cv2 import resize \nfrom vgg16_hybrid_places_1365 import VGG16_Hybrid_1365 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8959822723709694
      ],
      "excerpt": "image = np.array(image, dtype=np.uint8) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Grigorios Kalliatakis\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Keras | VGG16 Places365 - VGG16 CNN models pre-trained on Places365-Standard for scene classification",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Keras-VGG16-places365",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "GKalliatakis",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GKalliatakis/Keras-VGG16-places365/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "GKalliatakis",
        "body": "Initial [faulty] weights converted from caffe which were resulting in issue #5. \r\n\r\nKept only for reference purposes.",
        "dateCreated": "2018-05-11T08:25:27Z",
        "datePublished": "2018-05-31T13:16:46Z",
        "html_url": "https://github.com/GKalliatakis/Keras-VGG16-places365/releases/tag/0.1",
        "name": "Initial [faulty] VGG16-places365 & VGG16-hybrid1365 weights ",
        "tag_name": "0.1",
        "tarball_url": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/tarball/0.1",
        "url": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/releases/11260643",
        "zipball_url": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/zipball/0.1"
      },
      {
        "authorType": "User",
        "author_name": "GKalliatakis",
        "body": "Add places365_class_index.json",
        "dateCreated": "2017-12-26T13:30:26Z",
        "datePublished": "2018-01-03T13:16:49Z",
        "html_url": "https://github.com/GKalliatakis/Keras-VGG16-places365/releases/tag/0.2",
        "name": "",
        "tag_name": "0.2",
        "tarball_url": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/tarball/0.2",
        "url": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/releases/9092398",
        "zipball_url": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/zipball/0.2"
      },
      {
        "authorType": "User",
        "author_name": "GKalliatakis",
        "body": "Add code and [proper] weights files for VGG16-places365 and VGG16-hybrid1365.",
        "dateCreated": "2018-05-11T08:25:27Z",
        "datePublished": "2017-11-27T09:36:31Z",
        "html_url": "https://github.com/GKalliatakis/Keras-VGG16-places365/releases/tag/v1.0",
        "name": "VGG16-places365, VGG16-hybrid1365",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/tarball/v1.0",
        "url": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/releases/8663925",
        "zipball_url": "https://api.github.com/repos/GKalliatakis/Keras-VGG16-places365/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 156,
      "date": "Sun, 26 Dec 2021 08:43:08 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "keras",
      "python",
      "cnn",
      "scene-recognition",
      "baseline-cnns",
      "deep-learning",
      "embeddings",
      "places365",
      "places2-dataset",
      "vgg16"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All architectures are compatible with both TensorFlow and Theano, and upon instantiation the models will be built according to the image dimension ordering set in your Keras configuration file at ~/.keras/keras.json. For instance, if you have set image_dim_ordering=tf, then any model loaded from this repository will get built according to the TensorFlow dimension ordering convention, \"Width-Height-Depth\".\n\nPre-trained weights can be automatically loaded upon instantiation (`weights='places'` argument in model constructor for all image models). Weights are automatically downloaded.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "More info on downloading, converting, and submitting other models can be found on the main [Keras | Application Zoo repository](https://github.com/GKalliatakis/Keras-Application-Zoo).\n\n",
      "technique": "Header extraction"
    }
  ]
}