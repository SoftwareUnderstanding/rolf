{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qqwweee/keras-yolo3",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-04-03T03:36:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T16:49:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A Keras implementation of YOLOv3 (Tensorflow backend) inspired by [allanzelener/YAD2K](https://github.com/allanzelener/YAD2K).\n\n\n---\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8981505110475856,
        0.8229794430205244
      ],
      "excerpt": "The inference result is not totally the same as Darknet but the difference is small. \nThe speed is slower than Darknet. Replacing PIL with opencv may help a little. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Keras implementation of YOLOv3 (Tensorflow backend)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qqwweee/keras-yolo3/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3473,
      "date": "Wed, 29 Dec 2021 22:03:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/qqwweee/keras-yolo3/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "qqwweee/keras-yolo3",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8177852163484425
      ],
      "excerpt": "    One row for one image; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8638225223402811,
        0.8837680365796365
      ],
      "excerpt": "The test environment is \nPython 3.5.2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9007971662933216
      ],
      "excerpt": "Default anchors are used. If you use your own anchors, probably some changes are needed. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8758920678415644
      ],
      "excerpt": "Generate your own annotation file and class names file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8672746139921343,
        0.8211967361428026,
        0.8476539125579702,
        0.8043630205773916
      ],
      "excerpt": "    For VOC dataset, try python voc_annotation.py \n    Here is an example: \n    path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 \n    path/to/img2.jpg 120,300,250,600,2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8635146701739964
      ],
      "excerpt": "Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321594326057143,
        0.9503189345333785,
        0.8548847127929242
      ],
      "excerpt": "Modify train.py and start training. \npython train.py \n    Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8866642960922292,
        0.8768656641713756
      ],
      "excerpt": "    3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 \n    4. use model_data/darknet53_weights.h5 in train.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/qqwweee/keras-yolo3/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 qqwweee\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "keras-yolo3",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "keras-yolo3",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "qqwweee",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/qqwweee/keras-yolo3/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6931,
      "date": "Wed, 29 Dec 2021 22:03:21 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Download YOLOv3 weights from [YOLO website](http://pjreddie.com/darknet/yolo/).\n2. Convert the Darknet YOLO model to a Keras model.\n3. Run YOLO detection.\n\n```\nwget https://pjreddie.com/media/files/yolov3.weights\npython convert.py yolov3.cfg yolov3.weights model_data/yolo.h5\npython yolo_video.py [OPTIONS...] --image, for image detection mode, OR\npython yolo_video.py [video_path] [output_path (optional)]\n```\n\nFor Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with `--model model_file` and `--anchors anchor_file`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Use --help to see usage of yolo_video.py:\n```\nusage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]\n                     [--classes CLASSES] [--gpu_num GPU_NUM] [--image]\n                     [--input] [--output]\n\npositional arguments:\n  --input        Video input path\n  --output       Video output path\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --model MODEL      path to model weight file, default model_data/yolo.h5\n  --anchors ANCHORS  path to anchor definitions, default\n                     model_data/yolo_anchors.txt\n  --classes CLASSES  path to class definitions, default\n                     model_data/coco_classes.txt\n  --gpu_num GPU_NUM  Number of GPU to use, default 1\n  --image            Image detection mode, will ignore all positional arguments\n```\n---\n\n4. MultiGPU usage: use `--gpu_num N` to use N GPUs. It is passed to the [Keras multi_gpu_model()](https://keras.io/utils/#multi_gpu_model).\n\n",
      "technique": "Header extraction"
    }
  ]
}