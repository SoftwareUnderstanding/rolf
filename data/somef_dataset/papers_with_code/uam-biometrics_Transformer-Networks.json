{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1506.02025 \n\n![Header](images/STN.PNG",
      "https://arxiv.org/abs/1807.03247 (2018). \nhttps://arxiv.org/pdf/1807.03247.pdf\n\n-------------------------------------------------------------------------------------------------------------------------------\n\n## How to use the code:\n\n-- Assuming you have an environment with all software dependencies solved:\n\n1) Download or clone the repository to a local folder:\n\n       git clone \"https://github.com/uam-biometrics/Spatial-Transformer-Networks.git\"\n      \n2) Files and descriptions:\n\n- models.py: definition of the backbone CNN\u2019s architecture.\n- coord_conv.py: definition of the CoordConv layers.\n- utils.py: definition of various functions used during training and testing.\n- spatial_transformer_tutorial.py: script for training the network with STNs and Conv2D layers. Based on the tutorial in https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html. \n- spatial_transformer_nets_with_coord_convs.py: script for training the network with STNs and CoordConv layers.\n- evaluate_models.py: script for testing the pretrained models with the MNIST dataset.\n- stn_classic.pt and stn_coordconv.pt: PyTorch trained models.\n\n\n-- Using the models for replicating our results for MNIST classification:\n  \n1) You have to run the evaluate_models.py script : it loads the already trained models and evaluates them on the test partition of the MNIST dataset. \n\n-------------------------------------------------------------------------------------------------------------------------------\n\n## Results for MNIST classification:\n\nResults have been obtained after training the models during 20 epochs.\n\n![Example](images/results.PNG)\n\n![Example](./images/MNIST_example.png)\n\nExamples of image warping for a test batch of the MNIST dataset. CoordConv layers obtain better results in most of the cases. For example, the two upper-left number 1s are more vertical in the CoordConv case than when using only STN with Conv2D layers.\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9999967384690689
      ],
      "excerpt": "Based on the paper: \"Spatial transformer networks\", Max Jaderberg et al., Advances in neural information processing systems, 2015, vol. 28, p. 2017-2025. https://arxiv.org/abs/1506.02025  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.997260906215349
      ],
      "excerpt": "CoordConv layers are presented in the paper:  \"An intriguing failing of convolutional neural networks and the coordconv solution\", Rosanne Liu et al., arXiv preprint arXiv:1807.03247 (2018).  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/uam-biometrics/Transformer-Networks",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-26T11:35:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-30T09:15:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9940551727620356
      ],
      "excerpt": "Repository with the implementation in PyTorch of visual attention mechanisms called Spatial Transformer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9548909676207323,
        0.9962860835739732,
        0.9500787854556328,
        0.9306177760443815
      ],
      "excerpt": "In this repository we provide two different networks in PyTorch: \n- A CNN with Spatial Transformer Networks, designed for making the original network more robust to transformations in input data, e.g., rotations, traslations, etc.  \n- A modification of the first network with the addition of CoordConv layers to the Localization Network (LN) of the STN. This type of layers are meant to provide Conv layers with information about the coordinates of the input images. The LN has the task of obtaining a affine transformation matrix from input images, and the CoordConv layers \nhave shown to improve accuracy in that type of tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9956839111555899,
        0.9858736969155274
      ],
      "excerpt": "Spatial transformer networks are a generalization of differentiable attention to any spatial transformation. STNs allow a neural network to perform spatial manipulation on the input data within the network to enhance the geometric invariance of the model.  \nCNNs are not invariant to rotation and scale and more general affine transformations. In STNs the localization network is a regular CNN which regresses the transformation parameters. The transformation network learns automatically the spatial transformations that enhances the global accuracy on a specific dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.993266206933916,
        0.978245661839467
      ],
      "excerpt": "In this repository also an implementation of a STN with the addition of CoordConv layers is provided.  \nConvolutions present a generic inability to transform spatial representations between two different types: from a dense Cartesian representation to a sparse, pixel-based representation or in the opposite direction. CoordConv layers were designed to solve this limitation modifying the traditional convolutional layers by adding information about the coordinates of the input images to the input tensor. The CoordConv layer is designed to be used a substitute of the regular Conv2D layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Baseline and improved versions (using CoordConv) of a STN for MNIST classification",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/uam-biometrics/Transformer-Networks/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 10:47:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/uam-biometrics/Transformer-Networks/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "uam-biometrics/Transformer-Networks",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/uam-biometrics/Transformer-Networks/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Transformer Networks",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Transformer-Networks",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "uam-biometrics",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/uam-biometrics/Transformer-Networks/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 10:47:26 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "-- Assuming you have an environment with all software dependencies solved:\n\n1) Download or clone the repository to a local folder:\n\n       git clone \"https://github.com/uam-biometrics/Spatial-Transformer-Networks.git\"\n      \n2) Files and descriptions:\n\n- models.py: definition of the backbone CNN\u2019s architecture.\n- coord_conv.py: definition of the CoordConv layers.\n- utils.py: definition of various functions used during training and testing.\n- spatial_transformer_tutorial.py: script for training the network with STNs and Conv2D layers. Based on the tutorial in https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html. \n- spatial_transformer_nets_with_coord_convs.py: script for training the network with STNs and CoordConv layers.\n- evaluate_models.py: script for testing the pretrained models with the MNIST dataset.\n- stn_classic.pt and stn_coordconv.pt: PyTorch trained models.\n\n\n-- Using the models for replicating our results for MNIST classification:\n  \n1) You have to run the evaluate_models.py script : it loads the already trained models and evaluates them on the test partition of the MNIST dataset. \n\n-------------------------------------------------------------------------------------------------------------------------------\n\n",
      "technique": "Header extraction"
    }
  ]
}