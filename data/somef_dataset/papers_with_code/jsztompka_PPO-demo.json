{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347\n\nThe environment used to present the algorithm is Reacher (20 arms"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9554441738822752
      ],
      "excerpt": "* Visual Observations: None. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jsztompka/PPO-demo",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-16T17:50:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-22T21:19:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9251392629608739,
        0.9879619834371589
      ],
      "excerpt": "Repository uses Unity-ML Reacher as environment for Proximal Policy Optimization agent  \nThis project is my sample implementation of Proximal Policy Optimization algorithm described in detail: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8759767815671698,
        0.9047055451320629,
        0.9960501516515539
      ],
      "excerpt": "The environment used to present the algorithm is Reacher (20 arms) from Unity-ML \nYou don\u2019t have to build the environment yourself the prebuilt one included in the project will work fine - please note it\u2019s only compatible with Unity-ML 0.4.0b NOT the current newest version. I don\u2019t have access to the source of the environment as it was prebuilt by Udacity. \nA reward of +0.1 is provided for touching a ball, and a reward of 0 is returned in all other cases. Thus, the goal of the agent is to keep touching the moving ball for as long as possible.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978014865201822,
        0.9436748531480346
      ],
      "excerpt": "* Vector Observation space: 33 variables corresponding to position, rotation, velocity, and angular velocities of the two arm Rigidbodies. \n* Vector Action space: (Continuous) Size of 4, corresponding to torque applicable to two joints. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Repository uses Unity-ML Reacher as environment for Proximal Policy Optimization agent ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jsztompka/PPO-demo/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 02:04:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jsztompka/PPO-demo/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jsztompka/PPO-demo",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please run pip install . in order to ensure you got all dependencies needed\n\nTo start up the project:\npython -m train.py \n\nAll hyper-paramters are in: \nconfig.py \n\nThe config includes PLAY_ONLY argument which decides whether to start Agent with pre-trained weights or spend a few hours and train it from scratch :) \n\nMore details on the project can be found in:  \n[Report](/Report.md)\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9198707510100148
      ],
      "excerpt": "You don\u2019t have to build the environment yourself the prebuilt one included in the project will work fine - please note it\u2019s only compatible with Unity-ML 0.4.0b NOT the current newest version. I don\u2019t have access to the source of the environment as it was prebuilt by Udacity. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jsztompka/PPO-demo/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ASP",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PPO-demo",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PPO-demo",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jsztompka",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jsztompka/PPO-demo/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 02:04:31 GMT"
    },
    "technique": "GitHub API"
  }
}