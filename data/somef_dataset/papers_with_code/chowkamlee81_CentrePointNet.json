{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2006.11275",
      "https://arxiv.org/abs/2006.11275",
      "https://arxiv.org/abs/2006.12671",
      "https://arxiv.org/abs/2006.11275",
      "https://arxiv.org/abs/1908.09492",
      "https://arxiv.org/abs/1904.07850",
      "https://arxiv.org/abs/2004.01177"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhou2020tracking,\n  title={Tracking Objects as Points},\n  author={Zhou, Xingyi and Koltun, Vladlen and Kr{\\\"a}henb{\\\"u}hl, Philipp},\n  journal={arXiv:2004.01177},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhou2019objects,\n  title={Objects as Points},\n  author={Zhou, Xingyi and Wang, Dequan and Kr{\\\"a}henb{\\\"u}hl, Philipp},\n  journal={arXiv:1904.07850},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yan2018second,\n  title={Second: Sparsely embedded convolutional detection},\n  author={Yan, Yan and Mao, Yuxing and Li, Bo},\n  journal={Sensors},\n  year={2018},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhou2018voxelnet,\n   title={VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection},\n   journal={CVPR},\n   author={Zhou, Yin and Tuzel, Oncel},\n   year={2018},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{lang2019pillar,\n   title={PointPillars: Fast Encoders for Object Detection From Point Clouds},\n   journal={CVPR},\n   author={Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},\n   year={2019},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhu2019classbalanced,\n  title={Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection},\n  author={Zhu, Benjin and Jiang, Zhengkai and Zhou, Xiangxin and Li, Zeming and Yu, Gang},\n  journal={arXiv:1908.09492},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yin2020center,\n  title={Center-based 3D Object Detection and Tracking},\n  author={Yin, Tianwei and Zhou, Xingyi and Kr{\\\"a}henb{\\\"u}hl, Philipp},\n  journal={arXiv:2006.11275},\n  year={2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8017660423212514
      ],
      "excerpt": "3D Object Detection and Tracking using center points in the bird-eye view. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9927561676785958
      ],
      "excerpt": "Center-based 3D Object Detection and Tracking,           \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9904682582335301,
        0.9998140645097229,
        0.9933537248551054,
        0.9967495074413848,
        0.9664456561658856
      ],
      "excerpt": "arXiv technical report (arXiv 2006.11275)   \n  title={Center-based 3D Object Detection and Tracking}, \n  author={Yin, Tianwei and Zhou, Xingyi and Kr{\\\"a}henb{\\\"u}hl, Philipp}, \n  journal={arXiv:2006.11275}, \n  year={2020}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| PointPillars-512  |  Val    |  48.3   |  59.1  |  30.3 |  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chowkamlee81/CentrePointNet",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Any questions or discussion are welcome! \n\nTianwei Yin [yintianwei@utexas.edu](mailto:yintianwei@utexas.edu) \nXingyi Zhou [zhouxy@cs.utexas.edu](mailto:zhouxy@cs.utexas.edu)\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-15T03:28:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-20T00:06:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.843384907806306
      ],
      "excerpt": "3D Object Detection and Tracking using center points in the bird-eye view. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8588519025532183,
        0.9969467682726336,
        0.9715071702814654,
        0.9528286963337527,
        0.9248074900822275,
        0.8456988937139246
      ],
      "excerpt": "[2020-08-10] NEW: We now support vehicle detection on Waymo. Please stay tuned for more updates in the fall. \nThree-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection, but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have difficulties enumerating all orientations or fitting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objectsas points. We use a keypoint detector to find centers of objects, and simply regress to other attributes, including 3D size, 3D orientation, and velocity. In our center-based framework, 3D object tracking simplifies to greedy closest-point matching.The resulting detection and tracking algorithm is simple, efficient, and effective. On the nuScenes dataset, our point-based representations performs 3-4mAP higher than the box-based counterparts for 3D detection, and 6 AMOTA higher for 3D tracking. Our real-time model runs end-to-end 3D detection and tracking at 30 FPS with 54.2AMOTA and 48.3mAP while the best single model achieves 60.3mAP for 3D detection, and 63.8AMOTA for 3D tracking. \nSimple: Two sentences method summary: We use standard 3D point cloud encoder with a few convolutional layers in the head to produce a bird-eye-view heatmap and other dense regression outputs including the offset to centers in the previous frame. Detection is a simple local peak extraction, and tracking is a closest-distance matching. \nFast: Our PointPillars model runs at 30 FPS with 48.3 AP and 59.1 AMOTA for simultaneous 3D detection and tracking on the nuScenes dataset.  \nAccurate: Our best single model achieves 60.3 mAP and 67.3 NDS on nuScenes detection testset. \nExtensible: Simple baseline to switch in your backbone and novel algorithms. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9410684244791798
      ],
      "excerpt": "All results are tested on a Titan Xp GPU with batch size 1. More models and details can be found in MODEL_ZOO.md. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9706246823718638,
        0.8431215320338789
      ],
      "excerpt": "Please refer to GETTING_START to prepare the data. Then follow the instruction there to reproduce our detection and tracking results. All detection configurations are included in configs and we provide the scripts for all tracking experiments in tracking_scripts. The pretrained models, log, and each model's prediction files are provided in the MODEL_ZOO.md. \nThis project is not possible without multiple great opensourced codebases. We list some notable examples below.   \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chowkamlee81/CentrePointNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 13:38:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/chowkamlee81/CentrePointNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "chowkamlee81/CentrePointNet",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/chowkamlee81/CentrePointNet/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/det3d/visualization/map_mask.ipynb",
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/det3d/visualization/vis_label_bev.ipynb",
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/det3d/visualization/manipulate%20nuScenes.ipynb",
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/det3d/visualization/lidar_vis_vel.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/setup.sh",
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/tracking_scripts/megvii.sh",
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/tracking_scripts/centerpoint_voxel_1440_dcn_flip_testset.sh",
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/tracking_scripts/centerpoint_voxel_1024_circle_nms.sh",
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/tracking_scripts/centerpoint_pillar_512_circle_nms.sh",
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/tracking_scripts/centerpoint_voxel_1440_dcn_flip_circle_nms.sh",
      "https://raw.githubusercontent.com/chowkamlee81/CentrePointNet/master/tracking_scripts/centerpoint_voxel_1440_dcn_flip.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For more advanced usage, please refer to [INSTALL](docs/INSTALL.md) to set up more libraries needed for distributed training and sparse convolution.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n#: basic python libraries\nconda create --name centerpoint python=3.6\nconda activate centerpoint\nconda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch\ngit clone https://github.com/tianweiy/CenterPoint.git\ncd CenterPoint\npip install -r requirements.txt\n\n#: add CenterPoint to PYTHONPATH by adding the following line to ~/.bashrc (change the path accordingly)\nexport PYTHONPATH=\"${PYTHONPATH}:PATH_TO_CENTERPOINT\"\n```\n\nFirst download the model (By default, [centerpoint_pillar_512](https://drive.google.com/file/d/1ubWKx3Jg1AqF93qqWIZxgGXTycQ77qM3/view?usp=sharing)) from the [Model Zoo](docs/MODEL_ZOO.md) and put it in ```work_dirs/centerpoint_pillar_512_demo```. \n\nWe provide a driving sequence clip from the [nuScenes dataset](https://www.nuscenes.org). Donwload the [folder](https://drive.google.com/file/d/1bK-xeq5UwJzpPfVDhICDJeKiU1QVZwtI/view?usp=sharing) and put in the main directory.     \nThen run a demo by ```python tools/demo.py```. If setup corectly, you will see an output video like (red is gt objects, blue is the prediction): \n\n<p align=\"center\"> <img src='docs/demo.gif' align=\"center\" height=\"350px\"> </p> \n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8811514738762616
      ],
      "excerpt": "| VoxelNet-1440_dcn_flip   |  Test   |  60.3  |  67.3  | 2.2 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "| CenterPoint_voxel_1440_dcn_flip | test | 1ms | 451ms | 63.8 | 0.555 | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/chowkamlee81/CentrePointNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Cuda",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Tianwei Yin and Xingyi Zhou\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Center-based 3D Object Detection and Tracking",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CentrePointNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "chowkamlee81",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chowkamlee81/CentrePointNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Sun, 26 Dec 2021 13:38:02 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide a demo with PointPillars model for 3D object detection on the nuScenes dataset. \n\n",
      "technique": "Header extraction"
    }
  ]
}