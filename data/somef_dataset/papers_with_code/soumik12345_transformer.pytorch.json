{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@misc{\n    1706.03762,\n    Author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    Title = {Attention Is All You Need},\n    Year = {2017},\n    Eprint = {arXiv:1706.03762},\n}\n```",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{\n    1706.03762,\n    Author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    Title = {Attention Is All You Need},\n    Year = {2017},\n    Eprint = {arXiv:1706.03762},\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/soumik12345/transformer.pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-06T17:20:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-14T06:54:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9770023969450463
      ],
      "excerpt": "Pytorch implementation of the Transformer Model as proposed by the paper Attention Is All You Need \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051952194093401
      ],
      "excerpt": "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8140784920491091
      ],
      "excerpt": "the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9340024107575768
      ],
      "excerpt": "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9694147856667054
      ],
      "excerpt": "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8818825313883654,
        0.8003473902097895,
        0.8584729688513395
      ],
      "excerpt": "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections \naround each of the sub-layers, followed by layer normalization. We also modify the self-attention \nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pytorch implementation of Transformer",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/soumik12345/transformer.pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 09:30:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/soumik12345/transformer.pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "soumik12345/transformer.pytorch",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/soumik12345/transformer.pytorch/master/German-English-IWSLT.ipynb",
      "https://raw.githubusercontent.com/soumik12345/transformer.pytorch/master/Noob_Test.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9001986133137966
      ],
      "excerpt": "masking, combined with fact that the output embeddings are offset by one position, ensures that the \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/soumik12345/transformer.pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Transformer.Pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "transformer.pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "soumik12345",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/soumik12345/transformer.pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 09:30:58 GMT"
    },
    "technique": "GitHub API"
  }
}