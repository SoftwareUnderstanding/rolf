{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.05952",
      "https://arxiv.org/abs/1802.09477",
      "https://arxiv.org/abs/1802.09477, 2018.](https://arxiv.org/pdf/1802.09477.pdf)<br/>\nOriginal citation for the PyTorch implementation fo Twin Delayed Deep Deterministic Policy Gradients (TD3), [source code](https://github.com/sfujim/TD3)\n- [ ] Add to Overleaf summaries\n- [x] Upload to shared articles\n4. [Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay.arXiv preprint https://arxiv.org/abs/1511.05952(2015).](https://arxiv.org/abs/1511.05952)<br/>\nPrioritized experience replay- See Overleaf article summary.\n\n## Code References \n1. [TD3 Algorithm Code](https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93) from Towards Data Science implementation of Addressing function approximation error in actor-critic methods.\n2. [OpenAI Gym, Replay Buffer and Priority Replay Buffer](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py) \n3. [ROS Robotics by Example](https://dl.acm.org/citation.cfm?id=3200107) Baxter reference for ROS including: joint angles,... (download the book)[https://drive.google.com/open?id=11UpOH1fZd1qhXr9i8tEyVa1g4NVmL-me]\n4. [TD3 Implementation](https://arxiv.org/abs/1802.09477) Used for TD3 algorithm implementation",
      "https://arxiv.org/abs/1511.05952(2015).](https://arxiv.org/abs/1511.05952)<br/>\nPrioritized experience replay- See Overleaf article summary.\n\n## Code References \n1. [TD3 Algorithm Code](https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93) from Towards Data Science implementation of Addressing function approximation error in actor-critic methods.\n2. [OpenAI Gym, Replay Buffer and Priority Replay Buffer](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py) \n3. [ROS Robotics by Example](https://dl.acm.org/citation.cfm?id=3200107) Baxter reference for ROS including: joint angles,... (download the book)[https://drive.google.com/open?id=11UpOH1fZd1qhXr9i8tEyVa1g4NVmL-me]\n4. [TD3 Implementation](https://arxiv.org/abs/1802.09477) Used for TD3 algorithm implementation"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [TD3 Algorithm Code](https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93) from Towards Data Science implementation of Addressing function approximation error in actor-critic methods.\n2. [OpenAI Gym, Replay Buffer and Priority Replay Buffer](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py) \n3. [ROS Robotics by Example](https://dl.acm.org/citation.cfm?id=3200107) Baxter reference for ROS including: joint angles,... (download the book)[https://drive.google.com/open?id=11UpOH1fZd1qhXr9i8tEyVa1g4NVmL-me]\n4. [TD3 Implementation](https://arxiv.org/abs/1802.09477) Used for TD3 algorithm implementation.",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9354032267556015
      ],
      "excerpt": "Multi-agent deep reinforcement learning research project \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998838606742826
      ],
      "excerpt": "Van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with Double Q-Learning. In Thirtieth AAAI conference on artificial intelligence(2016).<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998957653282557
      ],
      "excerpt": "Fujimoto, S., van Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.<br/> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CharlotteMorrison/Baxter-Research",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-18T14:33:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-04T02:58:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8564543035099689,
        0.9896233974348824,
        0.8566819564100561
      ],
      "excerpt": "Multi-agent deep reinforcement learning research project \nThe TD3 algorithm uses two critic networks and selects the smallest value for the target network.  To prevent overestimation of policies propogating errorthe policy network is updated after a set number of timesteps and the value network is updated after each time step. Variance will be lower in policy network leading to more stable and efficient training and ultimately a better quality policy.  For this implementation, the actor network is updated every 2 timesteps.  The policy is smoothed by adding random noise and averaging over mini-batches to reduce the variance caused by overfitting. <br/> \nVan Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with Double Q-Learning. In Thirtieth AAAI conference on artificial intelligence(2016).<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9719759822760526
      ],
      "excerpt": "In actor-critic networks the policy is updated very slowly making bias a concern.  The older version of Double Q Learning uses clipped double Q learning.  This takes the smaller value of the two critic networks (the better choice).   Even though this promotes underestimation, this is not a concern because the small values will not propogate through the whole algorithm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8720741895353464,
        0.9693233393948762
      ],
      "excerpt": "[ ] Add to Overleaf summaries \n[x] Upload to shared articles \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Multi-agent deep reinforcement learning research project",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CharlotteMorrison/Baxter-Research/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 18:09:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CharlotteMorrison/Baxter-Research/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "CharlotteMorrison/Baxter-Research",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CharlotteMorrison/Baxter-Research/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Baxter-Research",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Baxter-Research",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "CharlotteMorrison",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CharlotteMorrison/Baxter-Research/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 18:09:26 GMT"
    },
    "technique": "GitHub API"
  }
}