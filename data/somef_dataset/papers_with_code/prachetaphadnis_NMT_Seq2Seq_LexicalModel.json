{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1508.04025, 2015.\n- Toan Q Nguyen and David Chiang. Improving lexical choice in neural machine translation. arXiv preprint https://arxiv.org/abs/1710.01329, 2017.\n\n",
      "https://arxiv.org/abs/1710.01329, 2017.\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n- Toan Q Nguyen and David Chiang. Improving lexical choice in neural machine translation. arXiv preprint arXiv:1710.01329, 2017.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prachetaphadnis/NMT_Seq2Seq_LexicalModel",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-09T16:52:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-11T20:07:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9267248023529566,
        0.9017026282921011,
        0.9218729939386949,
        0.980910453747189,
        0.9611929803265511
      ],
      "excerpt": "A full report of the task can be found in NMT_report.pdf \nThis code implements a Sequence to Sequence Neural Machine Translation model based on this paper by Luong et. al. \n- The NMT model used global attention with dropout and input feeding. \n- The model is also extended to implement the lexical model as per this paper \n- The data used for training is Japanese - English parallel corpus with Japanese as source language and English as target - language. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9189953146841664
      ],
      "excerpt": "  - To translate (with default settings): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "  - To calculate BLEU score: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "NMT Lexical Model",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prachetaphadnis/NMT_Seq2Seq_LexicalModel/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 05:04:24 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prachetaphadnis/NMT_Seq2Seq_LexicalModel/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "prachetaphadnis/NMT_Seq2Seq_LexicalModel",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8563697538827689,
        0.8587476985249702,
        0.8473480451372949,
        0.9503189345333785
      ],
      "excerpt": "- The model will train until convergence and last epoch will be saved as checkpoint in the specified directory. The epoch with best validation loss will be saved as best checkpoint in the specified directory. \n- Usage: \n  - To train (with default settings): \n   python train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8095791913675828
      ],
      "excerpt": "   perl multi-bleu.perl -lc raw data/test.en < model translations.txt \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prachetaphadnis/NMT_Seq2Seq_LexicalModel/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Perl"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Denis Emelin\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Neural Machine Translation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "NMT_Seq2Seq_LexicalModel",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "prachetaphadnis",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prachetaphadnis/NMT_Seq2Seq_LexicalModel/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 05:04:24 GMT"
    },
    "technique": "GitHub API"
  }
}