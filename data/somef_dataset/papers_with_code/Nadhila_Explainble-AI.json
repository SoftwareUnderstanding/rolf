{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1903.08808",
      "https://arxiv.org/abs/1602.04938",
      "https://arxiv.org/abs/1911.03644",
      "https://arxiv.org/abs/1903.08808. (https://arxiv.org/abs/1903.08808)\nDing, Z., Xia, R., Yu, J., Li, X., & Yang, J. (2018, August). Densely connected bidirectional lstm with applications to sentence classification. In CCF International Conference on Natural Language Processing and Chinese Computing (pp. 278-287). Springer, Cham. (https://link.springer.com/chapter/10.1007/978-3-319-99501-4_24)\n\nHu, J. (2018). Explainable Deep Learning for Natural Language Processing. (http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1335846&dswid=1510)\n\nLundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model predictions. In Advances in neural information processing systems (pp. 4765-4774). (http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predicti)\n\nMathews, S. M. (2019, July). Explainable Artificial Intelligence Applications in NLP, Biomedical, and Malware Classification: A Literature Review. In Intelligent Computing-Proceedings of the Computing Conference (pp. 1269-1292). Springer, Cham.(https://link.springer.com/chapter/10.1007/978-3-030-22868-2_90)\n\nMolnar, C. (2020). Interpretable machine learning. Lulu. com.(https://christophm.github.io/interpretable-ml-book/)\n\nRibeiro, M. T., Singh, S., & Guestrin, C. (2018, April). Anchors: High-precision model-agnostic explanations. In Thirty-Second AAAI Conference on Artificial Intelligence.(https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16982)\n\nRibeiro, M. T., Singh, S., & Guestrin, C. (2016, August). \" Why should i trust you?\" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144).(https://arxiv.org/abs/1602.04938)\n\nVan Huynh, T., Nguyen, VD, Van Nguyen, K., Nguyen, NLT, & Nguyen, AGT (2019). Hate Speech Detection on Vietnamese Social Media Text using the Bi-GRU-LSTM-CNN Model. arXiv preprint https://arxiv.org/abs/ 1911.03644 . (https://arxiv.org/abs/1911.03644)\n\n(https://github.com/slundberg/shap)",
      "https://arxiv.org/abs/ 1911.03644 . (https://arxiv.org/abs/1911.03644)\n\n(https://github.com/slundberg/shap)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Assawinjaipetch, P., Shirai, K., Sornlertlamvanich, V., & Marukata, S. (2016, December). Recurrent neural network with word embedding for complaint classification. In Proceedings of the Third International Workshop on Worldwide Language Service Infrastructure and Second Workshop on Open Infrastructures and Analysis Frameworks for Human Language Technologies (WLSI/OIAF4HLT2016) (pp. 36-43).(https://www.aclweb.org/anthology/W16-5205/)\n\nCambray, A., & Podsadowski, N. (2019). Bidirectional Recurrent Models for Offensive Tweet Classification. arXiv preprint arXiv:1903.08808. (https://arxiv.org/abs/1903.08808)\nDing, Z., Xia, R., Yu, J., Li, X., & Yang, J. (2018, August). Densely connected bidirectional lstm with applications to sentence classification. In CCF International Conference on Natural Language Processing and Chinese Computing (pp. 278-287). Springer, Cham. (https://link.springer.com/chapter/10.1007/978-3-319-99501-4_24)\n\nHu, J. (2018). Explainable Deep Learning for Natural Language Processing. (http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1335846&dswid=1510)\n\nLundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model predictions. In Advances in neural information processing systems (pp. 4765-4774). (http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predicti)\n\nMathews, S. M. (2019, July). Explainable Artificial Intelligence Applications in NLP, Biomedical, and Malware Classification: A Literature Review. In Intelligent Computing-Proceedings of the Computing Conference (pp. 1269-1292). Springer, Cham.(https://link.springer.com/chapter/10.1007/978-3-030-22868-2_90)\n\nMolnar, C. (2020). Interpretable machine learning. Lulu. com.(https://christophm.github.io/interpretable-ml-book/)\n\nRibeiro, M. T., Singh, S., & Guestrin, C. (2018, April). Anchors: High-precision model-agnostic explanations. In Thirty-Second AAAI Conference on Artificial Intelligence.(https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16982)\n\nRibeiro, M. T., Singh, S., & Guestrin, C. (2016, August). \" Why should i trust you?\" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144).(https://arxiv.org/abs/1602.04938)\n\nVan Huynh, T., Nguyen, VD, Van Nguyen, K., Nguyen, NLT, & Nguyen, AGT (2019). Hate Speech Detection on Vietnamese Social Media Text using the Bi-GRU-LSTM-CNN Model. arXiv preprint arXiv: 1911.03644 . (https://arxiv.org/abs/1911.03644)\n\n(https://github.com/slundberg/shap)\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Nadhila/Explainble-AI",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-13T22:08:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-17T16:23:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8672176799774391
      ],
      "excerpt": "Some plot use javascript since github lock javascript for protection here link to Google Colab for better visualization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9916619678549862
      ],
      "excerpt": "In this repository, we use 3 different methods of XAI on Machine Learning Model for Text Classification (Sentiment classification using Bi-LSTM and BI-GRU+LSTM+CNN).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9958299065724023,
        0.9873050645667343,
        0.9945655062719269,
        0.8606889340434866,
        0.9238745841540642
      ],
      "excerpt": "LIME explanation method for text:Local interpretable model-agnostic explanations (LIME) are a concrete implementation of local surrogate models. Surrogate models are trained to approximate the predictions of the underlying black box model. Instead of training a global surrogate model, LIME focuses on training local surrogate models to explain individual predictions. LIME for text has variations of the data which are generated differently: Starting from the original text, new texts are created by randomly removing words from the original text. The dataset is represented with binary features for each word. A feature is 1 if the corresponding word is included and 0 if it has been removed. \nSHAP (kernel Explanation) method for text: SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model.SHAP has the goal to explain the prediction of an instance x by computing the contribution of each feature to the prediction \nAnchor explanation method: The anchor algorithm is based on the Anchors: High-Precision Model-Agnostic Explanations paper by Ribeiro et al. For text classification, an interpretable anchor consists of the words that need to be present to ensure a prediction, regardless of the other words in the input. \nTo compare the methods we use the commons datasets : \nReview of Indonesian Application in google playstore for classification dataset  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.808315696867024,
        0.856812966838686,
        0.9812171595173064,
        0.985186690491773,
        0.9834565329551406,
        0.991381809760438,
        0.9713673709050463,
        0.9020917026962261,
        0.9092651792312683
      ],
      "excerpt": "Lowering case :  We decide to reduce the noise by normalizing each word to be lowercase \nPunctuation removing : We removed punctuation such as question marks, commas, colons and periods. \nurl removing (tweet data) : All instances of real URLs were removed to reduce the noise of the data. \nMention removing  and hashtag (tweet Data) : Tweet content data consisted some metion to other user and hashtag. These user mentions and hashtag which would be of little value to the model, and thus all such mentions were removed. \nPadding is a special form of masking were the masked steps are at the start or at the beginning of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences. \nWhen processing sequence data, it is very common for individual samples to have different lengths.A requirement for our LSTM models was that the input sentence length has to be \ufb01xed for all training dataset. We use sequence_pads () function from keras with maxlen= input_length which is 100. \nWe trained our own word embeddings during the execution of the whole model, with a randomly initialized embedding layer. \nWe used two different deep recurrent network architectures, they are : \nThe model are inspired by the paper Bidirectional Recurrent Models for Offensive Tweet Classi\ufb01cation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235168845716327
      ],
      "excerpt": "-   1 spatialDropout1D layer Decreasing the number of features that we train on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.870283277733521,
        0.9485497540457251
      ],
      "excerpt": "-   1 dropout layer with 50% which located between the dense layer  \nThe details architecture of the model bi-directional (picture) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235168845716327
      ],
      "excerpt": "-   1 spatialDropout1D layer Decreasing the number of features that we train on \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Nadhila/Explainble-AI/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 23:25:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Nadhila/Explainble-AI/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Nadhila/Explainble-AI",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Nadhila/Explainble-AI/master/explainable_ai%28blibli%29%28updated%29.ipynb",
      "https://raw.githubusercontent.com/Nadhila/Explainble-AI/master/explainable_ai%28usAir%29%28Updated%29.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Nadhila/Explainble-AI/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Review 3 Different Methods for Explainability Artificial Intelligence (XAI)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Explainble-AI",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Nadhila",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Nadhila/Explainble-AI/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 22 Dec 2021 23:25:10 GMT"
    },
    "technique": "GitHub API"
  }
}