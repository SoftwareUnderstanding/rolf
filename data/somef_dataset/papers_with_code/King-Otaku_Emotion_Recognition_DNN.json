{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.1556, 2015\n\n[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. \u201cImageNet classification with deep convolutional neural networks,\u201dCommunications of the ACM, vol. 60, no. 6, pp. 84-90, May 2017.\n\n[21] ] M. D. Zeiler and R. Fergus, \u201cVisualizing and Understanding Convolutional Networks,\u201d in Computer Vision-ECCV 2014, Springer pp. 818-833, 2014.\n\n[22] C. Szegedy et al.,\u201cGoing deeper with convolutions,\u201d in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR",
      "https://arxiv.org/abs/1605.07146, 2016\n\n[29] 1000x Faster Data Augmentation, https://bair.berkeley.edu/blog/2019/06/07/data_aug/, 2019\n",
      "https://arxiv.org/abs/1801.07883, 2018. \n\n[16] OpenCV, https://ko.wikipedia.org/wiki/OpenCV, 2021.\n\n[17] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, 9(8), 1997.\n\n[18] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D.Minnen,J. Shor, and M. Convell, \u201cFull resolution image compression with recurrent neural networks,\u201d arXiv preprint https://arxiv.org/abs/1608.05148, 2016.\n\n[19] Very Deep Convolutional Networks for Large-Scale Image Recognition, https://arxiv.org/abs/1409.1556, 2015\n\n[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. \u201cImageNet classification with deep convolutional neural networks,\u201dCommunications of the ACM, vol. 60, no. 6, pp. 84-90, May 2017.\n\n[21] ] M. D. Zeiler and R. Fergus, \u201cVisualizing and Understanding Convolutional Networks,\u201d in Computer Vision-ECCV 2014, Springer pp. 818-833, 2014.\n\n[22] C. Szegedy et al.,\u201cGoing deeper with convolutions,\u201d in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\n[23] He, K., Zhang, X., Ren, S., & Sun, J. \u201cDeep residual learning for image recognition\u201d, In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n[24] Automatic Hierarchical Classification of Kelps Using Deep Residual Features, https://www.researchgate.net/figure/ResNet-50-architecture-26-shown-with-the-residual-units-the-size-of-the-filters-and_fig1_338603223, 2015\n\n[25] G. Huang, Z. Liu, \u201cDensely connected convolutional networks,\u201d Proc. of the IEEE conference on computer vision and pattern recognition, Vol. 1, No. 2, pp. 3, 2017.\n\n[26] SqueezeNet: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND <0.5MB MODEL SIZE,  https://arxiv.org/pdf/1602.07360v4.pdf, 2016\n\n[27] M. Tan, Q. V. Le. \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\" arXiv preprint https://arxiv.org/abs/1905.11946, 2019.\n\n[28] Wide Residual Networks, https://arxiv.org/abs/1605.07146, 2016\n\n[29] 1000x Faster Data Augmentation, https://bair.berkeley.edu/blog/2019/06/07/data_aug/, 2019\n",
      "https://arxiv.org/abs/1608.05148, 2016.\n\n[19] Very Deep Convolutional Networks for Large-Scale Image Recognition, https://arxiv.org/abs/1409.1556, 2015\n\n[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. \u201cImageNet classification with deep convolutional neural networks,\u201dCommunications of the ACM, vol. 60, no. 6, pp. 84-90, May 2017.\n\n[21] ] M. D. Zeiler and R. Fergus, \u201cVisualizing and Understanding Convolutional Networks,\u201d in Computer Vision-ECCV 2014, Springer pp. 818-833, 2014.\n\n[22] C. Szegedy et al.,\u201cGoing deeper with convolutions,\u201d in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\n[23] He, K., Zhang, X., Ren, S., & Sun, J. \u201cDeep residual learning for image recognition\u201d, In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n[24] Automatic Hierarchical Classification of Kelps Using Deep Residual Features, https://www.researchgate.net/figure/ResNet-50-architecture-26-shown-with-the-residual-units-the-size-of-the-filters-and_fig1_338603223, 2015\n\n[25] G. Huang, Z. Liu, \u201cDensely connected convolutional networks,\u201d Proc. of the IEEE conference on computer vision and pattern recognition, Vol. 1, No. 2, pp. 3, 2017.\n\n[26] SqueezeNet: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND <0.5MB MODEL SIZE,  https://arxiv.org/pdf/1602.07360v4.pdf, 2016\n\n[27] M. Tan, Q. V. Le. \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\" arXiv preprint https://arxiv.org/abs/1905.11946, 2019.\n\n[28] Wide Residual Networks, https://arxiv.org/abs/1605.07146, 2016\n\n[29] 1000x Faster Data Augmentation, https://bair.berkeley.edu/blog/2019/06/07/data_aug/, 2019\n",
      "https://arxiv.org/abs/1905.11946, 2019.\n\n[28] Wide Residual Networks, https://arxiv.org/abs/1605.07146, 2016\n\n[29] 1000x Faster Data Augmentation, https://bair.berkeley.edu/blog/2019/06/07/data_aug/, 2019\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Seung Hyeog Moon, \u201cAnalysis of AI-Applied Industry and Development Direction\u201d, The Journal of the Convergence on Culture Technology(JCCT), 5:1, 77-82, 2019.\n\n[2] M. H. Jang, \u201cUnplugged Education Program for Artificial Intelligence Education in Elementary Schools-Focus on \u2018Constraint satisfaction problem\u2019-\u201d, Master D. thesis, Gyeongin National University of Education, Incheon, 2020.\n\n[3] Y. Adini, Y. Moses & S. Ullman, \u201cFace Recognition: The problem of compensating for changes in illumination direction, IEEE Trans\u201d. on Pattern Analysis and Machine Intelligence, 19(7), 721-732, 1997.\n\n[4] S. H. Kim, H. H. Kim & H. S. Lee. \u201cAn Improved Face Recognition Method Using SIFT-Grid\u201d. Journal of Digital Convergence, 11(2), 299-307, 2013.\n\n[5] Liu, B., Sentiment, \u201cAnalysis : mining opinions, senti- ments, and emotions\u201d, Cambridge University Press, 2015.\n\n[6] Suh, S. and Kim, J., \u201cResearch Trend of Deep Learningbased Sentiment Analysis\u201d, Journal of Korea Multimedia Society, Vol. 20, No. 3, pp. 8-22, 2016.\n\n[7] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K., \u201cBERT : Pre-training of Deep Bidirectional Transformers for Language Understanding\u201d, NAACL-HLT, 2019. Yoo, S. and Jeong, O., An Intelligent Chatbot Utilizing BERT Model and Knowledge Graph, Journal of Society for e-Business Studies, Vol. 24, No. 3, pp. 87-98, 2019\n\n[8] TTA, \u201cSentiment Ontology for Social Web\u201d, Telecommunications Technology Association Report TTAK.KO-10.0639/R1, 2013.\n\n[9] B. Liu, \u201cWeb Data Mining,\u201d Springer, 2007.\n\n[10] D. Hussein, \u201cA survey on sentiment analysis challenges,\u201d Journal of King Saud University \u2013 Engineering Sciences, Vol.30, No.4, pp.330-338, 2018. \n\n[11] K.-J. Lee, J.-H. Kim, H.-W. Seo, and K.-S. Ryoo, \u201cFeature weighting for opinion classification of comments on news articles,\u201d Journal of the Korean Society of Marine Engineering, Vol.34, No.6, pp.871-879, 2010.\n\n[12] P. Turney, \u201cThumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews,\u201d Proceedings of the Association for Computational Linguistics, pp.417\u2013424, 2002.\n\n[13] E. Gilbert and C. J. Hutto, \u201cVADER : A parsimonious rulebased model for sentiment analysis of social media text,\u201d Proceedings of the 8th International Conference on Weblogs and Social Media, pp.216-225, 2014. \n\n[14] M. Taboada and J. Brooke, \u201cLexicon-based methods for sentiment analysis,\u201d Computational Linguistics, Vol.37, No.2, pp.272\u2013274, 2011. \n\n[15] L. Zhang, S. Wang, and B. Liu, \u201cDeep learning for sentiment analysis: A survey,\u201d arXiv:1801.07883, 2018. \n\n[16] OpenCV, https://ko.wikipedia.org/wiki/OpenCV, 2021.\n\n[17] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, 9(8), 1997.\n\n[18] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D.Minnen,J. Shor, and M. Convell, \u201cFull resolution image compression with recurrent neural networks,\u201d arXiv preprint arXiv:1608.05148, 2016.\n\n[19] Very Deep Convolutional Networks for Large-Scale Image Recognition, https://arxiv.org/abs/1409.1556, 2015\n\n[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. \u201cImageNet classification with deep convolutional neural networks,\u201dCommunications of the ACM, vol. 60, no. 6, pp. 84-90, May 2017.\n\n[21] ] M. D. Zeiler and R. Fergus, \u201cVisualizing and Understanding Convolutional Networks,\u201d in Computer Vision-ECCV 2014, Springer pp. 818-833, 2014.\n\n[22] C. Szegedy et al.,\u201cGoing deeper with convolutions,\u201d in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\n[23] He, K., Zhang, X., Ren, S., & Sun, J. \u201cDeep residual learning for image recognition\u201d, In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n[24] Automatic Hierarchical Classification of Kelps Using Deep Residual Features, https://www.researchgate.net/figure/ResNet-50-architecture-26-shown-with-the-residual-units-the-size-of-the-filters-and_fig1_338603223, 2015\n\n[25] G. Huang, Z. Liu, \u201cDensely connected convolutional networks,\u201d Proc. of the IEEE conference on computer vision and pattern recognition, Vol. 1, No. 2, pp. 3, 2017.\n\n[26] SqueezeNet: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND <0.5MB MODEL SIZE,  https://arxiv.org/pdf/1602.07360v4.pdf, 2016\n\n[27] M. Tan, Q. V. Le. \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\" arXiv preprint arXiv:1905.11946, 2019.\n\n[28] Wide Residual Networks, https://arxiv.org/abs/1605.07146, 2016\n\n[29] 1000x Faster Data Augmentation, https://bair.berkeley.edu/blog/2019/06/07/data_aug/, 2019\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/King-Otaku/Emotion_Recognition_DNN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-26T04:12:07Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-06T10:28:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8222352155259031
      ],
      "excerpt": "Tensorflow(VGG-16, Densenet-121, Densenet-201, Resnet-50, Resnet-101), Pytorch(Resnet-18, Resnet-50, WideResnet-101, Densenet-201, EfficientNet-B5, SqueezeNet1.1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Tensorflow(VGG-16, Densenet-121, Densenet-201), Pytorch(Densenet-121, Densenet-201, EfficientNet-B5, SqueezeNet1.1)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/King-Otaku/Emotion_Recognition_DNN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 17:29:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/King-Otaku/Emotion_Recognition_DNN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "King-Otaku/Emotion_Recognition_DNN",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Model_Use_capstone.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/ConvertImageToCSV.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/CalcStdMean.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/ConvertH5toPB.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/cuda%20test.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_lr0.001_adam_batch16_epoch30.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_lr0.001_adam_batch16_epoch10_imsize48_CalcMeanStd_emotion6ver.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_keras_emotion6ver_imsize48__lr0.001_epoch100.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/VGG16_Complete.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_keras_emotion6ver_imsize48_epoch50_likeVGG.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Resnet5_50_adam_batch64_learninglr0.001_nnlinear128.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet121-revised2.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Resnet5_50_adam_batch128_learninglr0001_nnlinear48.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/WideResnet50_adam_batch64_learninglr0.0001_nnlinear128_epoch100.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/EfficientNet2.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_lr0.001_adam_batch16_epoch20_emotion4ver_imsize256_AfPic.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/VGG16_lr0.001.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Resnet.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/EfficientNet_6emotion.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet121_dr0.2_lr.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/EfficientNet_keras_emotion6ver_imsize48__lr0.01_epoch100.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/EfficientNet_keras_emotion6ver_imsize48__lr0.01_epoch100_nonprocess.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/VGG16_lr0.000001_dr0.2.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Squeezenet1.1_lr0.001_batch64_epoch100.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/WideResnet50_adam_batch128_learninglr00005_nnlinear48_epoch30.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_lr0.0001_adam_batch16_epoch30_pretrainFalse_emotion6_imsize256_AfPic.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/VGG16_new.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_keras_emotion6ver_imsize48__lr0.01_epoch100_likeVGG.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_handmade.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Resnet5_50_adam_batch64_learninglr00001_nnlinear48.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet121-revised3.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet121-revised1.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet121_emotion6ver_imsize48_epoch30.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/WideResnet101_adam_batch8_learninglr0.001_nnlinear48_epoch30.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet121-revised4.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Resnet5_50_adam_batch64.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/EfficientNet3-1%20-%20adam.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Squeezenet1.1_lr0.001_batch64_epoch50_imsize48_CalcMeanStd_emotion6ver.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Resnet3_adam_batch128.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/EfficientNet_keras_emotion6ver_imsize48__lr0.01_epoch100_dr0.5-Copy1.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/EfficientNet_keras_emotion6ver_imsize48__lr0.01_epoch100_dr0.5.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_lr0.00001_adam_batch16_epoch20.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_lr0.0001_adam_batch16_epoch20_pretrainFalse.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_lr0.0001_adam_batch16_epoch30_pretrainFalse_emotion6_imsize256.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_emotion6ver_imsize48_epoch100.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/EfficientNet_keras_emotion6ver_imsize48__lr0.0001_epoch100_dr0.25_comple.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Resnet2.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet121-emotion6ver.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Resnet4_adam_batch64.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/WideResnet101_pytorch_adam_batch8_learninglr0.001_nnlinear48_epoch30.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_lr0.0001_adam_batch16_epoch10.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_lr0.00001_adam_batch16_epoch20_emotion6ver_imsize256_AfPic.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_keras_emotion6ver_imsize48__lr0.1_epoch50.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet121-original.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/Densenet201_lr0.0001_adam_batch16_epoch30_emotion6ver_imsize48.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Experiment%20Model/EfficientNet1.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Final%20Model/Resnet18_pytorch_Final.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Final%20Model/Squeezenet1.1_pytorch_Final.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Final%20Model/Densenet201_pytorch_Final.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Final%20Model/Resnet101_keras_Final.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Final%20Model/VGG16_keras_Final.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Final%20Model/Densenet121_keras_Final.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Final%20Model/Resnet50_pytorch_Final.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Final%20Model/EfficientNetB0_pytorch_Final.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Final%20Model/WideResnet101_pytorch_adam_batch8_learninglr0.001_nnlinear48_epoch30%20%282%29.ipynb",
      "https://raw.githubusercontent.com/King-Otaku/Emotion_Recognition_DNN/main/Final%20Model/Densenet201_keras_Final.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![\ub370\uc774\ud130\uc14b](https://user-images.githubusercontent.com/53389350/122148987-4e5bfe00-ce96-11eb-8774-a56174f3a982.png)\n\nI used commonly used datasets for the development of sentiment analyzers. The FER-2013 Faces Database. They are approximately 29000 image data, 48x48 size and classified into seven emotions. But One emotion, disgust, has not enough images so I decided to exclude it and train AI. This is because I heard that imbalance in datasets affects accuracy.\nHowever, We go through the pre-process of resizing and arranging data for easy learning. Furthermore, image augmentation (zoom_range = 0.2, horizontal_flip=True, shear_range=0.2) is applied so that arbitrary data can be clearly classified.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/King-Otaku/Emotion_Recognition_DNN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Emotion_Recognition_DNN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Emotion_Recognition_DNN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "King-Otaku",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/King-Otaku/Emotion_Recognition_DNN/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sun, 26 Dec 2021 17:29:10 GMT"
    },
    "technique": "GitHub API"
  }
}