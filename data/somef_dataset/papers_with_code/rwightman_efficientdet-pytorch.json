{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1911.09070",
      "https://arxiv.org/abs/2102.06171"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9648007483015567
      ],
      "excerpt": "* paper by Mingxing Tan, Ruoming Pang, Quoc V. Le EfficientDet: Scalable and Efficient Object Detection  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8125854950540023
      ],
      "excerpt": "Add AGC (Adaptive Gradient Clipping support via timm). Idea from (High-Performance Large-Scale Image Recognition Without Normalization - https://arxiv.org/abs/2102.06171) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| tf_efficientdet_d0_ap | 34.8 | TBD | 35.2 | 35.3 | 3.88 | 512 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "| tf_efficientdet_lite2 | 35.9 | TBD | 35.1 | N/A | 5.25 | 448 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| tf_efficientdet_d2 | 43.4 | TBD | 42.5 | 43 | 8.10 | 768 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| tf_efficientdet_d2_ap | 44.2 | TBD | 44.3 | 44.3 | 8.10 | 768 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "| tf_efficientdet_d3 | 47.1 | TBD | 47.2 | 47.5 | 12.0 | 896 | \n| tf_efficientdet_d3_ap | 47.7 | TBD | 48.0 | 47.7 | 12.0 | 896 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8028046190715653
      ],
      "excerpt": "wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8884268718049406
      ],
      "excerpt": "annotations/challenge-2019/&lt;csv anno for challenge2019&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8513058295046586
      ],
      "excerpt": "[x] Basic Training (object detection) reimplementation \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rwightman/efficientdet-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-26T23:09:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T18:25:06Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9598235882618746,
        0.991642613679062,
        0.9726355948662915
      ],
      "excerpt": "A PyTorch implementation of EfficientDet. \nIt is based on the \n* official Tensorflow implementation by Mingxing Tan and the Google Brain team \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9299117134683325,
        0.9837388253920664,
        0.8871075414379098
      ],
      "excerpt": "There are other PyTorch implementations. Either their approach didn't fit my aim to correctly reproduce the Tensorflow models (but with a PyTorch feel and flexibility) or they cannot come close to replicating MS COCO training from scratch. \nAside from the default model configs, there is a lot of flexibility to facilitate experiments and rapid improvements here -- some options based on the official Tensorflow impl, some of my own: \n* BiFPN connections and combination mode are fully configurable and not baked into the model code \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9283277776386557,
        0.8403300338394327
      ],
      "excerpt": "* Any backbone in my timm model collection that supports feature extraction (features_only arg) can be used as a bacbkone. \nUpdate efficientnetv2_dt weights to a new set, 46.1 mAP @ 768x768, 47.0 mAP @ 896x896 using AGC clipping. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8754639558601397,
        0.808041202168133,
        0.9861872758998543
      ],
      "excerpt": "Add EfficientNetV2 backbone experiment efficientnetv2_dt based on timm's efficientnetv2_rw_t (tiny) model. 45.8 mAP @ 768x768. \nUpdated TF EfficientDet-Lite model defs incl weights ported from official impl (https://github.com/google/automl) \nFor Lite models, updated feature resizing code in FPN to be based on feat size instead of reduction ratios, needed to support image size that aren't divisible by 128. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761611446772142,
        0.8099203777125265
      ],
      "excerpt": "Add some new model weights with bilinear interpolation for upsample and downsample in FPN. \n40.9 mAP - efficientdet_q1  (replace prev model at 40.6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9726529246099502
      ],
      "excerpt": "Add weights for alternate FPN layouts. QuadFPN experiments (efficientdet_q0/q1/q2) and CSPResDeXt + PAN (cspresdext50pan). See updated table below. Special thanks to Artus for providing resources for training the Q2 model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8245053278034621
      ],
      "excerpt": "set model config to read-only after creation to reduce likelyhood of misuse \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8709284159709829,
        0.8907684041104869,
        0.9505796728460416
      ],
      "excerpt": "Merged a few months of accumulated fixes and additions. \n* Proper fine-tuning compatible model init (w/ changeable \nThe table below contains models with pretrained weights. There are quite a number of other models that I have defined in model configurations that use various timm backbones. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9561429436102821,
        0.8599121080296623,
        0.9823513244639626
      ],
      "excerpt": "See model configurations for model checkpoint urls and differences. \nNOTE: Official scores for all modules now using soft-nms, but still using normal NMS here. \nNOTE: In training some experimental models, I've noticed some potential issues with the combination of synchronized BatchNorm (--sync-bn) and model EMA weight everaging (--model-ema) during distributed training. The result is either a model that fails to converge, or appears to converge (training loss) but the eval loss (running BN stats) is garbage. I haven't observed this with EfficientNets, but have with some backbones like CspResNeXt, VoVNet, etc. Disabling either EMA or sync bn seems to eliminate the problem and result in good models. I have not fully characterized this issue. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8985318598731464
      ],
      "excerpt": "* Training script currently defaults to a model that does NOT have redundant conv + BN bias layers like the official models, set correct flag when validating. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9487019186028449,
        0.8762986279078417
      ],
      "excerpt": "* The official training code uses EMA weight averaging by default, it's not clear there is a point in doing this with the cosine LR schedule, I find the non-EMA weights end up better than EMA in the last 10-20% of training epochs  \n* The default h-params is a very close to unstable (exploding loss), don't try using Nesterov momentum. Try to keep the batch size up, use sync-bn. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9643322145455526
      ],
      "excerpt": "Setting up OpenImages dataset is a commitment. I've tried to make it a bit easier wrt to the annotations, but grabbing the dataset is still going to take some time. It will take approx 560GB of storage space. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9118794651520725
      ],
      "excerpt": "Once everything is downloaded and extracted the root of your openimages data folder should contain: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179236216711109,
        0.9073750002923894,
        0.8696445302371543,
        0.9621089502895652,
        0.988883103687679
      ],
      "excerpt": "These hparams above resulted in a good model, a few points: \n* the mAP peaked very early (epoch 200 of 300) and then appeared to overfit, so likely still room for improvement \n* I enabled my experimental LR noise which tends to work well with EMA enabled \n* the effective LR is a bit higher than official. Official is .08 for batch 64, this works out to .0872 \n* drop_path (aka survival_prob / drop_connect) rate of 0.1, which is higher than the suggested 0.0 for D0 in official, but lower than the 0.2 for the other models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078086417133998
      ],
      "excerpt": "NOTE: I've only tried submitting D7 to dev server for sanity check so far \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9474411945823907,
        0.8860449520141279
      ],
      "excerpt": "[ ] Addition and cleanup of EfficientNet based U-Net and DeepLab segmentation models that I've used in past projects \n[x] Addition and cleanup of OpenImages dataset/training support from a past project \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A PyTorch impl of EfficientDet faithful to the original Google impl w/ ported weights",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Follow the s3 download directions here: https://github.com/cvdfoundation/open-images-dataset#download-images-with-bounding-boxes-annotations\n\nEach `train_<x>.tar.gz` should be extracted to `train/<x>` folder, where x is a hex digit from 0-F. `validation.tar.gz` can be extracted as flat files into `validation/`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Annotations can be downloaded separately from the OpenImages home page above. For convenience, I've packaged them all together with some additional 'info' csv files that contain ids and stats for all image files. My datasets rely on the `<set>-info.csv` files. Please see https://storage.googleapis.com/openimages/web/factsfigures.html for the License of these annotations. The annotations are licensed by Google LLC under CC BY 4.0 license. The images are listed as having a CC BY 2.0 license.\n```\nwget https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1-anno/openimages-annotations.tar.bz2\nwget https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1-anno/openimages-annotations-challenge-2019.tar.bz2\nfind . -name '*.tar.bz2' -exec tar xf {} \\;\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rwightman/efficientdet-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 229,
      "date": "Wed, 22 Dec 2021 07:04:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rwightman/efficientdet-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rwightman/efficientdet-pytorch",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rwightman/efficientdet-pytorch/master/distributed_train.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Tested in a Python 3.7 - 3.9 conda environment in Linux with:\n* PyTorch 1.6 - 1.10\n* PyTorch Image Models (timm) >= 0.4.12, `pip install timm` or local install from (https://github.com/rwightman/pytorch-image-models)\n* Apex AMP master (as of 2020-08). I recommend using native PyTorch AMP and DDP now.\n\n*NOTE* - There is a conflict/bug with Numpy 1.18+ and pycocotools 2.0, force install numpy <= 1.17.5 or ensure you install pycocotools >= 2.0.2\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8893304669956607
      ],
      "excerpt": "Python notebook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8278930462199948
      ],
      "excerpt": "Heads can have a different activation from FPN via config \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978554787830663
      ],
      "excerpt": "PyTorch >= 1.6 now required \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8092971106547161
      ],
      "excerpt": "NOTE: Official scores for all modules now using soft-nms, but still using normal NMS here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907276221048449
      ],
      "excerpt": "find . -name '*.tar' -exec tar xf {} \\; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9076723806120817
      ],
      "excerpt": "The 500 (Challenge2019) or 601 (V5/V6) class head for OI takes up a LOT more GPU memory vs COCO. You'll likely need to half batch sizes. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.856680098154364
      ],
      "excerpt": "Add training example to README provided by Chris Hughes for training w/ custom dataset & Lightning training code \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.86064739357783
      ],
      "excerpt": "Add EfficientDet AdvProp-AA weights for D0-D5 from TF impl. Model names tf_efficientdet_d?_ap \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597
      ],
      "excerpt": "| tf_efficientdet_lite4 | 44.2 | TBD | 43.2 | N/A | 15.1 | 640 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8162612450116957
      ],
      "excerpt": "| cspdarkdet53m | 45.2 | TBD | N/A | N/A | 35.6 | 768 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082022887640018
      ],
      "excerpt": "unzip annotations_trainval2017.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082022887640018
      ],
      "excerpt": "unzip -q test2017.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082022887640018,
        0.8608557847138604,
        0.9489366974762425
      ],
      "excerpt": "unzip image_info_test2017.zip \nRun validation (val2017 by default) with D2 model: python validate.py /localtion/of/mscoco/ --model tf_efficientdet_d2 \nRun test-dev2017: python validate.py /localtion/of/mscoco/ --model tf_efficientdet_d2 --split testdev \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8115504232218421
      ],
      "excerpt": "There should be a VOC2007 and VOC2012 folder within VOCdevkit, dataset root for cmd line will be VOCdevkit. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9442318440621337
      ],
      "excerpt": "python validate.py /data/VOCdevkit --model efficientdet_d0 --num-gpu 2 --dataset voc2007 --checkpoint mychekpoint.pth --num-classes 20 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8766890273738487
      ],
      "excerpt": "/distributed_train.sh 4 /data/VOCdevkit --model efficientdet_d0 --dataset voc0712 -b 16 --amp --lr .008 --sync-bn --opt fusedmomentum --warmup-epochs 3 --model-ema --model-ema-decay 0.9966 --epochs 150 --num-classes 20 --pretrained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8767080213718437
      ],
      "excerpt": "./distributed_train.sh 4 /data/openimages --model efficientdet_d0 --dataset openimages-challenge2019 -b 7 --amp --lr .042 --sync-bn --opt fusedmomentum --warmup-epochs 1 --lr-noise 0.4 0.9 --model-ema --model-ema-decay 0.999966 --epochs 100 --remode pixel --reprob 0.15 --recount 4 --num-classes 500 --val-skip 2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.356439 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.577 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.360218 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.422998 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.463488 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.505127 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.527791 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.552920 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.560973 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.571787 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.585 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rwightman/efficientdet-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright 2020 Ross Wightman\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "EfficientDet (PyTorch)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "efficientdet-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rwightman",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rwightman/efficientdet-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "",
        "dateCreated": "2021-04-30T23:49:21Z",
        "datePublished": "2021-05-01T00:11:57Z",
        "html_url": "https://github.com/rwightman/efficientdet-pytorch/releases/tag/v0.2.4",
        "name": "AdvProp-AA Models",
        "tag_name": "v0.2.4",
        "tarball_url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/tarball/v0.2.4",
        "url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/releases/42298213",
        "zipball_url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/zipball/v0.2.4"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Some pre-collected dataset annotations where it may be inconvenient to gather manually.\r\n\r\n",
        "dateCreated": "2020-09-18T18:01:16Z",
        "datePublished": "2020-10-22T19:52:48Z",
        "html_url": "https://github.com/rwightman/efficientdet-pytorch/releases/tag/v0.1-anno",
        "name": "Dataset annotations",
        "tag_name": "v0.1-anno",
        "tarball_url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/tarball/v0.1-anno",
        "url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/releases/32936922",
        "zipball_url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/zipball/v0.1-anno"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Latest TF weights, including D7X model. ",
        "dateCreated": "2020-09-18T18:01:16Z",
        "datePublished": "2020-09-18T18:04:54Z",
        "html_url": "https://github.com/rwightman/efficientdet-pytorch/releases/tag/v0.1.6",
        "name": "Latest TF weights",
        "tag_name": "v0.1.6",
        "tarball_url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/tarball/v0.1.6",
        "url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/releases/31538565",
        "zipball_url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/zipball/v0.1.6"
      },
      {
        "authorType": "User",
        "author_name": "rwightman",
        "body": "Models with `tf_` prefix are ported from Tensorflow pretrained weights and models without the prefix are trained with this codebase.",
        "dateCreated": "2020-04-04T20:16:31Z",
        "datePublished": "2020-04-09T09:09:10Z",
        "html_url": "https://github.com/rwightman/efficientdet-pytorch/releases/tag/v0.1",
        "name": "weights",
        "tag_name": "v0.1",
        "tarball_url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/tarball/v0.1",
        "url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/releases/25353380",
        "zipball_url": "https://api.github.com/repos/rwightman/efficientdet-pytorch/zipball/v0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1249,
      "date": "Wed, 22 Dec 2021 07:04:06 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "efficientdet",
      "efficientnet",
      "object-detection",
      "semantic-segmentation",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* A new dataset interface with dataset support (via parser classes) for COCO, VOC 2007/2012, and OpenImages V5/Challenge2019\n* New focal loss def w/ label smoothing available as an option, support for jit of loss fn for (potential) speedup\n* Improved a few hot spots that squeek out a couple % of throughput gains, higher GPU utilization\n* Pascal / OpenImages evaluators based on Tensorflow Models Evaluator framework (usable for other datasets as well)\n* Support for native PyTorch DDP, SyncBN, and AMP in PyTorch >= 1.6. Still defaults to APEX if installed.\n* Non-square input image sizes are allowed for the model (the anchor layout). Specified by image_size tuple in model config. Currently still restricted to `size % 128 = 0` on each dim.\n* Allow anchor target generation to be done in either dataloader process' via collate or in model as in past. Can help balance compute.\n* Filter out unused target cls/box from dataset annotations in fixed size batch tensors before passing to target assigner. Seems to speed convergence.\n* Letterbox aware Random Erasing augmentation added.\n* A (very slow) SoftNMS impl added for inference/validation use. It can be manually enabled right now, can add arg if demand.\n* Tested with PyTorch 1.7\n* Add ResDet50 model weights, 41.6 mAP.\n\nA few things on priority list I haven't tackled yet:\n* Mosaic augmentation\n* bbox IOU loss (tried a bit but so far not a great result, need time to debug/improve)\n\n**NOTE** There are some breaking changes:\n* Predict and Train benches now output XYXY boxes, NOT XYWH as before. This was done to support other datasets as XYWH is COCO's evaluator requirement.\n* The TF Models Evaluator operates on YXYX boxes like the models. Conversion from XYXY is currently done by default. Why don't I just keep everything YXYX? Because PyTorch GPU NMS operates in XYXY.\n* You must update your version of `timm` to the latest (>=0.3), as some APIs for helpers changed a bit.\n\nTraining sanity checks were done on VOC and OI\n  * 80.0 @ 50 mAP finetune on voc0712 with no attempt to tune params (roughly as per command below)\n  * 18.0 mAP @ 50 for OI Challenge2019 after couple days of training (only 6 epochs, eek!). It's much bigger, and takes a LOONG time, many classes are quite challenging.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The models here have been used with custom training routines and datasets with great results. There are lots of details to figure out so please don't file any 'I get crap results on my custom dataset issues'. If you can illustrate a reproducible problem on a public, non-proprietary, downloadable dataset, with public github fork of this repo including working dataset/parser implementations, I MAY have time to take a look.\n\nExamples:\n* Chris Hughes has put together a great example of training w/ `timm` EfficientNetV2 backbones and the latest versions of the EfficientDet models here\n  * [Medium blog post](https://medium.com/data-science-at-microsoft/training-efficientdet-on-custom-data-with-pytorch-lightning-using-an-efficientnetv2-backbone-1cdf3bd7921f)\n  * [Python notebook](https://gist.github.com/Chris-hughes10/73628b1d8d6fc7d359b3dcbbbb8869d7)\n* Alex Shonenkov has a clear and concise Kaggle kernel which illustrates fine-tuning these models for detecting wheat heads: https://www.kaggle.com/shonenkov/training-efficientdet (NOTE: this is out of date wrt to latest versions here, many details have changed)\n\nIf you have a good example script or kernel training these models with a different dataset, feel free to notify me for inclusion here...\n\n",
      "technique": "Header extraction"
    }
  ]
}