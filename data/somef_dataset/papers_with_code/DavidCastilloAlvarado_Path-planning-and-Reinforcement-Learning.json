{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1.] OpenIA    \n\n[2.] https://github.com/MorvanZhou\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.982877065711717
      ],
      "excerpt": "           Ra = 0.4 + (0.2 if s[0]&lt;1 else 0.001) + (0.4 if m &lt;1 else 0.001) or -1 if collides \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DavidCastilloAlvarado/Path-planning-and-Reinforcement-Learning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-12T17:49:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-03T04:25:12Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8088870617297267
      ],
      "excerpt": "We will create a map from the reality and put a diferential robot in there with the aim to use an path planning algorith through reinforecement learning (PPO) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.99189836183608,
        0.9484873869644066,
        0.8627813632864737
      ],
      "excerpt": "The typical framing of a Reinforcement Learning (RL) scenario: an agent takes actions in an environment, which is interpreted into a reward and a representation of the state, which are fed back into the agent. \nReinforcement learning is considered as one of three machine learning paradigms, alongside supervised learning and unsupervised learning. It differs from supervised learning in that correct input/output pairs[clarification needed] need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is on performance[clarification needed], which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). \nA Markov decision process is a 4-tuple {S,A Pa,Ra} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183019110790126,
        0.9160924386679562,
        0.8799393465344022
      ],
      "excerpt": "2. A is a finite set of actions[Steering angle between -6|6 degrees] \n3. Pa is the probability that action a in state s at time \"t\" t will lead to state s' at time t+1 \n4. Ra is the immediate reward (or expected immediate reward) received after transitioning from state s to state s', due to action  a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9522550603911669,
        0.9202774155026905,
        0.9241187512431522
      ],
      "excerpt": "The Policy was optimizer using a method call PPO (2017) a new family of policy gradient methods for reinforcement learning. \nWe use the following paper, about proximal policy optimization, the particular sub-method aplied in this proyect was the CLIP method whit epsilon = 0.2  \nwe choose a value for gamma for the discounter equal to 0.9  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8976185355445576
      ],
      "excerpt": "The NN was improved using batch normalization in from the input of every layer.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "We will create a map from the reality and put a diferential robot in there with the aim to use an path planning algorithm through reinforcement learning (PPO)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DavidCastilloAlvarado/Path-planning-and-Reinforcement-Learning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 23:09:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DavidCastilloAlvarado/Path-planning-and-Reinforcement-Learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DavidCastilloAlvarado/Path-planning-and-Reinforcement-Learning",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/DavidCastilloAlvarado/Path-planning-and-Reinforcement-Learning/master/path_planning_RL_PPO/PP_RL_map%26PPO%20%28copy%29.ipynb",
      "https://raw.githubusercontent.com/DavidCastilloAlvarado/Path-planning-and-Reinforcement-Learning/master/getting%20map/PP_finding_obstacles.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.833782873776028
      ],
      "excerpt": "pandas  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DavidCastilloAlvarado/Path-planning-and-Reinforcement-Learning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 David Castillo Alvarado\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Path planning self driving",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Path-planning-and-Reinforcement-Learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DavidCastilloAlvarado",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DavidCastilloAlvarado/Path-planning-and-Reinforcement-Learning/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Mon, 27 Dec 2021 23:09:14 GMT"
    },
    "technique": "GitHub API"
  }
}