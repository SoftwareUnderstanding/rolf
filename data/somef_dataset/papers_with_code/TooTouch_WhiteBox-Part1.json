{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1807.06521",
      "https://arxiv.org/abs/1512.04150",
      "https://arxiv.org/abs/1704.06904",
      "https://arxiv.org/abs/1807.07320",
      "https://arxiv.org/abs/1311.2901",
      "https://arxiv.org/abs/1412.6806",
      "https://arxiv.org/abs/1610.02391",
      "https://arxiv.org/abs/1706.03825",
      "https://arxiv.org/abs/1806.10758",
      "https://arxiv.org/abs/1412.6806. [[Paper](https://arxiv.org/abs/1412.6806)]\r\n\r\n- [7] Sundararajan, M., Taly, A., & Yan, Q. (2017, August). Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3319-3328). JMLR. org. [[Paper](https://arxiv.org/pdf/1703.01365.pdf)]\r\n\r\n- [8] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision (pp. 618-626). [[Paper](https://arxiv.org/abs/1610.02391)] [[Korean version](https://www.notion.so/tootouch/Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization-504a3f7a58fd4c3eafdc26258befd643)]\r\n\r\n- [9] Smilkov, D., Thorat, N., Kim, B., Vi\u00e9gas, F., & Wattenberg, M. (2017). Smoothgrad: removing noise by adding noise. arXiv preprint https://arxiv.org/abs/1706.03825. [[Paper](\r\nhttps://arxiv.org/abs/1706.03825)] [[Korean version](https://datanetworkanalysis.github.io/2019/10/22/smoothgrad)]\r\n\r\n- [10] Hooker, S., Erhan, D., Kindermans, P. J., & Kim, B. (2018). Evaluating feature importance estimates. arXiv preprint https://arxiv.org/abs/1806.10758. [[Paper](https://arxiv.org/abs/1806.10758)] [[Korean version](https://datanetworkanalysis.github.io/2019/11/13/roar_kar)]\r\n\r",
      "https://arxiv.org/abs/1706.03825. [[Paper](\r\nhttps://arxiv.org/abs/1706.03825)] [[Korean version](https://datanetworkanalysis.github.io/2019/10/22/smoothgrad)]\r\n\r\n- [10] Hooker, S., Erhan, D., Kindermans, P. J., & Kim, B. (2018). Evaluating feature importance estimates. arXiv preprint https://arxiv.org/abs/1806.10758. [[Paper](https://arxiv.org/abs/1806.10758)] [[Korean version](https://datanetworkanalysis.github.io/2019/11/13/roar_kar)]\r\n\r",
      "https://arxiv.org/abs/1806.10758. [[Paper](https://arxiv.org/abs/1806.10758)] [[Korean version](https://datanetworkanalysis.github.io/2019/11/13/roar_kar)]\r\n\r"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [1] Woo, S., Park, J., Lee, J. Y., & So Kweon, I. (2018). Cbam: Convolutional block attention module. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 3-19). [[Paper](https://arxiv.org/abs/1807.06521)]\r\n\r\n- [2] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., & Torralba, A. (2016). Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2921-2929). [[Paper](https://arxiv.org/abs/1512.04150)]\r\n\r\n- [3] Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., ... & Tang, X. (2017). Residual attention network for image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3156-3164). [[Paper](https://arxiv.org/abs/1704.06904)]\r\n\r\n- [4] Rodr\u00edguez, P., Gonfaus, J. M., Cucurull, G., XavierRoca, F., & Gonzalez, J. (2018). Attend and rectify: a gated attention mechanism for fine-grained recovery. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 349-364). [[Paper](https://arxiv.org/abs/1807.07320)]\r\n\r\n- [5] Zeiler, M. D., & Fergus, R. (2014, September). Visualizing and understanding convolutional networks. In European conference on computer vision (pp. 818-833). Springer, Cham. [[Paper](https://arxiv.org/abs/1311.2901)] [[Korean version](https://datanetworkanalysis.github.io/2019/10/27/deconvnet)]\r\n\r\n- [6] Springenberg, J. T., Dosovitskiy, A., Brox, T., & Riedmiller, M. (2014). Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806. [[Paper](https://arxiv.org/abs/1412.6806)]\r\n\r\n- [7] Sundararajan, M., Taly, A., & Yan, Q. (2017, August). Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3319-3328). JMLR. org. [[Paper](https://arxiv.org/pdf/1703.01365.pdf)]\r\n\r\n- [8] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision (pp. 618-626). [[Paper](https://arxiv.org/abs/1610.02391)] [[Korean version](https://www.notion.so/tootouch/Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization-504a3f7a58fd4c3eafdc26258befd643)]\r\n\r\n- [9] Smilkov, D., Thorat, N., Kim, B., Vi\u00e9gas, F., & Wattenberg, M. (2017). Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825. [[Paper](\r\nhttps://arxiv.org/abs/1706.03825)] [[Korean version](https://datanetworkanalysis.github.io/2019/10/22/smoothgrad)]\r\n\r\n- [10] Hooker, S., Erhan, D., Kindermans, P. J., & Kim, B. (2018). Evaluating feature importance estimates. arXiv preprint arXiv:1806.10758. [[Paper](https://arxiv.org/abs/1806.10758)] [[Korean version](https://datanetworkanalysis.github.io/2019/11/13/roar_kar)]\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8164327028374208
      ],
      "excerpt": "- Residual Attention Network (RAN) [3] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "- SmoothGrad-Squared (SG-SQ) [10] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671519928528644,
        0.9392901210467604
      ],
      "excerpt": "Remove and Retrain (ROAR) [10] \nKeep and Retrain (KAR) [10] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TooTouch/WhiteBox-Part1",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-04T15:34:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-18T21:54:45Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9964176309856357,
        0.8371886979614206
      ],
      "excerpt": "The White Box Project is a project that introduces many ways to solve the part of the black box of machine learning. In this part, i've introduced and experimented with ways to interpret and evaluate models in the field of image.  \nI shared Korean versions for each reference to study methodology and English. Please refer to the reference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9626010153630652
      ],
      "excerpt": "Simple CNN Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9673782464260561
      ],
      "excerpt": "More information on the model architectures and learning process can be found on the notebook : [Evaluation] - Model Performance \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.817068314651952,
        0.9234203247011775,
        0.817068314651952
      ],
      "excerpt": "| Simple CNN        | 1284042  | 0.998   | 0.995   | 0.995     | 0.995    | 0.993    | 0.990   | 0.986     | 0.989     | 0.996    | 0.985 | 0.992 | \n| Simple CNN + CAM  | 1285332  | 0.994   | 0.995   | 0.989     | 0.995    | 0.988    | 0.988   | 0.993     | 0.981     | 0.986    | 0.977 | 0.988 | \n| Simple CNN + CBAM | 1288561  | 0.998   | 0.995   | 0.992     | 0.996    | 0.990    | 0.990   | 0.990     | 0.991     | 0.995    | 0.989 | 0.993 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9234203247011775,
        0.9234203247011775,
        0.9234203247011775
      ],
      "excerpt": "| Simple CNN        | 2202122  | 0.872      | 0.905 | 0.692 | 0.731 | 0.843 | 0.660 | 0.904 | 0.864 | 0.860 | 0.916 | 0.825 | \n| Simple CNN + CAM  | 2203412  | 0.760      | 0.896 | 0.585 | 0.477 | 0.752 | 0.804 | 0.769 | 0.711 | 0.837 | 0.862 | 0.745 | \n| Simple CNN + CBAM | 2206641  | 0.858      | 0.945 | 0.749 | 0.685 | 0.790 | 0.761 | 0.826 | 0.798 | 0.873 | 0.896 | 0.818 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503889098484496
      ],
      "excerpt": "  <strong>Saliency maps of RAN by layers : CIFAR10</strong><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616949996025994
      ],
      "excerpt": "Coherence is a qualitative evaluation method that shows the importance of images. Attributions should fall on discriminative features (e.g. the object of interest).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9852984559076087
      ],
      "excerpt": "  <strong>Saliency maps of each attribution methods applied to the Simple CNN : MNIST & CIFAR10</strong><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9825287429279792,
        0.9275026053659222
      ],
      "excerpt": "Selecticity is a method for quantitative evaluation of the attribution methods. The evaluation method is largely divided into two courses. First, the feature map for the image is created and the most influential part is deleted from the image. The second is to create the feature map again with the modified image and repeat the first process.  \nAs a result, IB, GB and GB-GC were the most likely attribution methods to degrade the performance of models for the two datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9964013110250148,
        0.8541567232962902
      ],
      "excerpt": "ROAR/KAR is a method for quantitative evaluation of the attribution methods that how the performance of the classifier changes as features are removed based on the attribution method.  \n- ROAR : replace N% of pixels estimated to be most important [Notebook] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9965101214205334
      ],
      "excerpt": "  <strong>ROAR and KAR graph of saliency maps of each attribution methods applied to the Simple CNN</strong><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9845820421581137
      ],
      "excerpt": "  <strong>ROAR and KAR graph of saliency maps extracted by Grad-CAM for each model</strong><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "In this part, I've introduced and experimented with ways to interpret and evaluate models in the field of image.  (Pytorch)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TooTouch/WhiteBox-Part1/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16,
      "date": "Mon, 27 Dec 2021 14:35:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TooTouch/WhiteBox-Part1/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "TooTouch/WhiteBox-Part1",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttention%5D%20%20CBAM%20-%20ConvOutput%20.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BEvaluation%5D%20-%20Model%20Performance.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttribution%5D%20-%20GradCAM%20%26%20Ensemble.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BEvaluation%5D%20-%20ROAR%26KAR.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttribution%5D%20-%20ConvOutput.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttribution%5D%20-%20Vanilla%20Backpropagation%20%26%20Ensemble.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttribution%5D%20-%20Guided%20Backpropagation%20%26%20Ensemble.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttribution%5D%20-%20Integrated%20Gradients%20%26%20Ensemble.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttention%5D%20%20CAM%20%26%20Ensemble.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttribution%5D%20-%20DeconvNet%20%26%20Ensemble.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BEvaluation%5D%20-%20Selectivity.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BEvaluation%5D%20-%20Sample%20Images.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttribution%5D%20-%20Input%20x%20Backpropagation%20%26%20Ensemble.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttribution%5D%20-%20Guided-GradCAM%20%26%20Ensemble.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BEvaluation%5D%20-%20Coherence.ipynb",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/notebook/%5BAttention%5D%20%20RAN%20-%20ConvOutput%20.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RAN_GC_KAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RANDOM_ROAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CNN_mnist_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CBAM_CO_ROAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CBAM_GC_KAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RANDOM_KAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RANDOM_KAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CO_KAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CO_ROAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CO_KAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RAN_CO_KAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RAN_GC_KAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CAM_KAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CAM_ROAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CO_ROAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RANDOM_ROAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CAM_ROAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CAM_KAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CBAM_CO_KAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CBAM_GC_ROAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CBAM_CO_ROAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RAN_GC_ROAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CBAM_GC_KAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CBAM_CO_KAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RAN_CO_KAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RAN_CO_ROAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/CBAM_GC_ROAR_cifar10.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RAN_CO_ROAR_mnist.sh",
      "https://raw.githubusercontent.com/TooTouch/WhiteBox-Part1/master/script/RAN_GC_ROAR_cifar10.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9032837796845528
      ],
      "excerpt": "Attention Modules \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8009381845811903
      ],
      "excerpt": "- Input x Backpropagation (IB) [Notebook] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8758872290934939
      ],
      "excerpt": "- SmoothGrad-VAR (SG-VAR) [10] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8347382413666812
      ],
      "excerpt": "| CIFAR10           | Number of Parameters | airplane | automobile | bird  | cat   | deer  | dog   | frog  | horse | ship  | truck | Total | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052494420970112
      ],
      "excerpt": "  <img src=\"https://github.com/TooTouch/WhiteBox-Part1/blob/master/images/results/RAN_CO(cifar10).jpg?raw=true\" width=\"700\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TooTouch/WhiteBox-Part1/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\r\\n\\r\\nCopyright (c) 2019 bllfpc\\r\\n\\r\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\r\\nof this software and associated documentation files (the \"Software\"), to deal\\r\\nin the Software without restriction, including without limitation the rights\\r\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\r\\ncopies of the Software, and to permit persons to whom the Software is\\r\\nfurnished to do so, subject to the following conditions:\\r\\n\\r\\nThe above copyright notice and this permission notice shall be included in all\\r\\ncopies or substantial portions of the Software.\\r\\n\\r\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\r\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\r\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\r\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\r\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\r\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\r\\nSOFTWARE.\\r\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "WhiteBox - Part1",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "WhiteBox-Part1",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "TooTouch",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TooTouch/WhiteBox-Part1/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\r\npytorch >= 1.2.0\r\ntorchvision == 0.4.0\r\n```\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Model Train**\r\n```\r\npython main.py --train --target=['mnist','cifar10'] --attention=['CAM','CBAM','RAN','WARN']\r\n```\r\n\r\n**Model Selectivity Evaluation**\r\n```\r\npython main.py --eval=selectivity --target=['mnist','cifar10'] --method=['VGB','IB','DeconvNet','IG','GB','GC','GBGC']\r\n```\r\n\r\n**Model ROAR & KAR Evaluation**  \r\nFor ROAR and KAR, the saliency map of each attribution methods that you want to evaluate must be saved prior to the evaluation.\r\n```\r\npython main.py --eval=['ROAR','KAR'] --target=['mnist','cifar10'] --method=['VGB','IB','DeconvNet','IG','GB','GC','GBGC']\r\n```\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 34,
      "date": "Mon, 27 Dec 2021 14:35:52 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "explainable-ai",
      "deep-learning",
      "saliency-map",
      "interpretable-ai",
      "blackbox",
      "smoothgrad",
      "gradients",
      "backpropagation",
      "evaluate-models",
      "computer-vision",
      "machine-learning",
      "grad-cam",
      "guided-backpropagation",
      "integrated-gradients",
      "mnist",
      "cifar-10"
    ],
    "technique": "GitHub API"
  }
}