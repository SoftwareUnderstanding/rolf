{
  "citation": [
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "                ->10         \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "categories: 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "                ->10            \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "categories: 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "% permuted labels | 0%         | 20%        | 30% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "% permuted labels | 0%         | 20%        | 30% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "SGNHT             | 76.60%     | 73.86%     | 71.37% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "% permuted labels | 0%         | 20%        | 30% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593299683604384
      ],
      "excerpt": "SGNHT             | 90.18%     | 89.10%     | 88.58% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8523512033337022
      ],
      "excerpt": "The reference for SGHMC is: https://arxiv.org/pdf/1402.4102.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8523512033337022
      ],
      "excerpt": "The reference for Adam is: https://arxiv.org/pdf/1412.6980.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8806563644179664
      ],
      "excerpt": "if abs(sampler.model.xi.item()) &lt;= 0.85*sampler.standard_interval and nIter &gt;= num_burn_in: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hsvgbkhgbv/Thermostat-assisted-continuously-tempered-Hamiltonian-Monte-Carlo-for-Bayesian-learning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-05-21T13:29:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-15T20:32:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9918968649411132,
        0.944937712960218,
        0.8372656012149255
      ],
      "excerpt": "This project implements the algorithm of TACTHMC and demonstrates the experiments compared with SGHMC, SGNHT, ADAM and SGD WITH MOMENTUM. \nThe paper of TACTHMC is shown as: http://papers.nips.cc/paper/8266-thermostat-assisted-continuously-tempered-hamiltonian-monte-carlo-for-bayesian-learning. \nAll of algorithms are implemented in Python 3.6, with Pytorch 0.4.0 and Torchvision 0.2.1, so please install the relevant dependencies before running the codes or invoking the functions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868478622525969,
        0.9938872797859397,
        0.9275220549017019
      ],
      "excerpt": "We have done three experiments based on MLP, CNN and RNN. All of tasks are classifications. \nThe task of MLP is on EMNIST. The task of CNN is on CIFAR-10. The task of RNN is on Fashion-MNIST. \nIn experiments, we assign random labels to 0%, 20% and 30% of each batch of training data respectively so as to constitute a noisy training environment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8494713548988322
      ],
      "excerpt": "                ->the output of the last time step \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9561518709058915
      ],
      "excerpt": "For the conventional optimization algorithms such as Adam and SGD, we use the point estimate to evaluate the performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9179473242613859
      ],
      "excerpt": "For the sampling algorithms such as SGHMC, SGNHT and TACTHMC, we use the fully bayesian to evaluate the performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "SGD with Momentum \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8869546870526478
      ],
      "excerpt": "In these experiments, we implement SGNHT and SGHMC, as well as invoke SGD and Adam from Pytorch directly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9044405892063822,
        0.8694925326751606
      ],
      "excerpt": "The reference for SGNHT is: http://people.ee.duke.edu/~lcarin/sgnht-4.pdf \nThe reference for SGD is: http://leon.bottou.org/publications/pdf/compstat-2010.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8829561143703287,
        0.9846706587293279,
        0.8110285346980298
      ],
      "excerpt": "--eta-theta ETA_THETA                                      #: set up the learning rate of parameters, which should be divided by the size of the whole training dataset (float) \n--eta-xi ETA_XI                                            #: set up the learning rate of the tempering variable which is similar to that of parameters (float) \n--c-theta C_THETA                                          #: set up the noise level of parameters (float) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9161786707016495,
        0.8902434411629359,
        0.9653434659677199,
        0.8840650300122673
      ],
      "excerpt": "--gamma-theta GAMMA_THETA                                  #: set up the value of the thermal initia of parameters (float) \n--gamma-xi GAMMA_XI                                        #: set up the value of the thermal initia of the tempering variable (float) \n--prior-precision PRIOR_PRECISION                          #: set up the penalty parameter of L2-Regularizer or the precision of a Gaussian prior from the view of bayesian stats (float) \n--permutation PERMUTATION                                  #: set up the percentage of random assignments on labels (float) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8954486555585615
      ],
      "excerpt": "--tempering-model-type TEMPERING_MODEL_TYPE                #: set up the model type for the tempering variable (1 for Metadynamics/2 for ABF) (int) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207540970535306
      ],
      "excerpt": "--save-tempering-model                                     #: set up whether it is necessary to save the tempering model (bool) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9475425026942382,
        0.9081042398191375
      ],
      "excerpt": "Here the tempering model is to handle the unexpected noise for the tempering variable occuring during the dynamics. \nInitialize an instance of the object TACTHMC such as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8036761444293574
      ],
      "excerpt": "model means the model instance constructed by Pytorch, which should be an instance inherited from nn.Module \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8186121355231429
      ],
      "excerpt": "eta_theta0 means the learning rate of parameters divided by N, which should be a float \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9224085430766872,
        0.9683348753020227
      ],
      "excerpt": "standard_interval means the half range of the interval that the effective system temperature is at unity, which should be a float \ngaussian_decay means the decayed height of the stacked Gaussian (which is only feasible when temper_model='Metadynamics'), which should be a float \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.913110421565698,
        0.8183408739017618
      ],
      "excerpt": "temper_model means which model is selected as the tempering variable model, which can be selectd between 'Metadynamics' and 'ABF' \nInitialize an estimator such as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663073302589583,
        0.8109049702781834
      ],
      "excerpt": "test_loader means the data loader, which should be an instance of torch.utils.data.DataLoader \nnum_labels means the number of labels of dataset, which should be an int \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9578367802467698
      ],
      "excerpt": "Initialize the momenta of parameters such as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.902468277727183
      ],
      "excerpt": "Evaluate with training data and get loss \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8429232659923044
      ],
      "excerpt": "Update the parameters and the tempering variable such as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9701061741807189
      ],
      "excerpt": "Periodically resample the momenta of parameters and evaluate with test data such as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9469555341386856
      ],
      "excerpt": "num_burn_in means the iterations of waiting for convergence of the algorithm, which shoud be an int \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9852439206005631,
        0.973245132262199,
        0.8742817815835875
      ],
      "excerpt": "sampler.get_z_theta()                               #: get the norm of thermostats of parameters \nsampler.get_z_xi()                                  #: get the norm of thermostats of the tempering variable \nsampler.get_fU()                                    #: get the current force of potential w.r.t the tempering variable \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hsvgbkhgbv/TACTHMC/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 18:28:57 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hsvgbkhgbv/Thermostat-assisted-continuously-tempered-Hamiltonian-Monte-Carlo-for-Bayesian-learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hsvgbkhgbv/Thermostat-assisted-continuously-tempered-Hamiltonian-Monte-Carlo-for-Bayesian-learning",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9882740621198297,
        0.9788072839912785,
        0.9699902762191287
      ],
      "excerpt": "The suggested solution is to install Anaconda Python 3.6 version: https://www.anaconda.com/download/. \nThen install the latest version of Pytorch and Torchvision by the command shown as below \npip install torch torchvision \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8044290599680651
      ],
      "excerpt": "batch_size: 128 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "P(\\theta | D_{train}) = \\frac{P(D_{train}, \\theta)}{P(D_{train})} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.905256171169429
      ],
      "excerpt": "eval = \\int_{\\theta} P(D_{test}| \\theta) P(\\theta | D_{train}) \\ d\\theta \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806977296449471
      ],
      "excerpt": "TACTHMC           | 90.84% | 89.61% | 89.01% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.846509515717751,
        0.9314189481622092
      ],
      "excerpt": "python cnn_tacthmc.py --permutation 0.2 --c-theta 0.1 \npython mlp_tacthmc.py --permutation 0.2 --c-theta 0.05 \npython rnn_tacthmc.py --permutation 0.2 --c-theta 0.15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.9246227682586091,
        0.9246227682586091
      ],
      "excerpt": "python cnn_sgd.py --permutation 0.2 \npython mlp_sgd.py --permutation 0.2 \npython rnn_sgd.py --permutation 0.2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.9246227682586091,
        0.9246227682586091
      ],
      "excerpt": "python cnn_adam.py --permutation 0.2 \npython mlp_adam.py --permutation 0.2 \npython rnn_adam.py --permutation 0.2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.9246227682586091,
        0.9246227682586091
      ],
      "excerpt": "python cnn_sghmc.py --permutation 0.2 --c-theta 0.1 \npython mlp_sghmc.py --permutation 0.2 --c-theta 0.1 \npython rnn_sghmc.py --permutation 0.2 --c-theta 0.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.9246227682586091,
        0.9246227682586091
      ],
      "excerpt": "python cnn_sgnht.py --permutation 0.2 --c-theta 0.1 \npython mlp_sgnht.py --permutation 0.2 --c-theta 0.1 \npython rnn_sgnht.py --permutation 0.2 --c-theta 0.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8673763503553918,
        0.8492277286498129
      ],
      "excerpt": "--train-batch-size TRAIN_BATCH_SIZE                        #: set up the training batch size (int) \n--test-batch-size TEST_BATCH_SIZE                          #: set up the test batch size (please set the size of the whole test data) (int) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8487436711186368
      ],
      "excerpt": "--num-epochs NUM_EPOCHS                                    #: set up the total number of epochs for training (int) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8311347778835604
      ],
      "excerpt": "--device-num DEVICE_NUM                                    #: select an appropriate GPU for usage (int) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8121359779892278
      ],
      "excerpt": "--load-tempering-model                                     #: set up whether necessarily load pre-trained tempering model (action=true) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8756096020334445
      ],
      "excerpt": "estimator = FullyBayesian((len(test_loader.dataset), num_labels), model, test_loader, cuda_availability) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8659255039446199
      ],
      "excerpt": "sampler.temper_model.loader(filename, enable_cuda)  #: load the pre-trained tempering model \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hsvgbkhgbv/Thermostat-assisted-continuously-tempered-Hamiltonian-Monte-Carlo-for-Bayesian-learning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning (TACTHMC)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Thermostat-assisted-continuously-tempered-Hamiltonian-Monte-Carlo-for-Bayesian-learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hsvgbkhgbv",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hsvgbkhgbv/Thermostat-assisted-continuously-tempered-Hamiltonian-Monte-Carlo-for-Bayesian-learning/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Sun, 26 Dec 2021 18:28:57 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "sampling-methods",
      "deep-learning",
      "hybrid-monte-carlo",
      "bayesian-nonparametric-models",
      "bayesian-methods",
      "bayesian-neural-networks"
    ],
    "technique": "GitHub API"
  }
}