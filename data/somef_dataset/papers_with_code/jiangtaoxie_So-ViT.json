{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "pytorch: https://github.com/pytorch/pytorch\n\ntimm: https://github.com/rwightman/pytorch-image-models\n\nT2T-ViT: https://github.com/yitu-opensource/T2T-ViT\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2104.10935v2"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please consider cite the paper if it's useful for you. \n\n    @articles{SoT,\n        author = {Jiangtao Xie, Ruiren Zeng, Qilong Wang, Ziqi Zhou, Peihua Li},\n        title = {SoT: Delving Deeper into Classification Head for Transformer},\n        booktitle = {arXiv:2104.10935v2},\n        year = {2021}\n    }\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@articles{SoT,\n    author = {Jiangtao Xie, Ruiren Zeng, Qilong Wang, Ziqi Zhou, Peihua Li},\n    title = {SoT: Delving Deeper into Classification Head for Transformer},\n    booktitle = {arXiv:2104.10935v2},\n    year = {2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.991460940735398,
        0.8714162992508173
      ],
      "excerpt": "- if your backbone without classification token, please useOnlyVisualTokensClassifierto replaceClassifier` \n- key arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| DeiT-B + ours | 82.9 | 29.1 | 94.9 | 18.2 | Coming soon | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920864572608084
      ],
      "excerpt": "<p align=\"center\" style=\"color:rgb(255,0,0);\">&radic;:<font color=\"black\"> correct prediction;</font> &#10007;: <font color=\"black\">incorrect prediction</font></p> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jiangtaoxie/SoT",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**If you have any questions or suggestions, please contact me**\n\n`jiangtaoxie@mail.dlut.edu.cn`; `coke990921@mail.dlut.edu.cn`\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-20T14:58:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T08:42:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository is the official  implementation of \"[SoT: Delving Deeper into Classification Head for Transformer](https://arxiv.org/pdf/2104.10935.pdf)\". It\ncontains the source code under **PyTorch** framework and models for image classification and text classification tasks.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9909772955430589
      ],
      "excerpt": "For classification tasks whether in CV or NLP, the current works based on pure transformer architecture pay little attention to the classification head, applying Classification token (ClassT) solely in the classifier,  however neglecting the Word tokens (WordT) which contains rich information. In our experiments, we show the ClassT and WordT are highly complementary, and the fusion of all tokens can further boost the performance. Therefore, we propose a novel classification paradigm by jointly utilizing ClassT and WordT, where the multiheaded global cross-covariance pooling with singluar value power normalization is proposed for effectively harness the rich information of WordT. We evaluate our proposed classfication scheme on the both CV and NLP tasks, achieving the very competitive performance with the counterparts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9438993221730059
      ],
      "excerpt": "Basic hyper-parameter of our SoT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705859556636731
      ],
      "excerpt": "On validation set of ImageNet-1K: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8086319173283181
      ],
      "excerpt": "    - dim: equal to the embedding dimension \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9588742491312884
      ],
      "excerpt": "Besides, we provide the implementation based on the DeiT and Swin-Transformer in CV tasks and BERT in NLP tasks for reference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248719327580357
      ],
      "excerpt": "    large_output=False, #: When the resulotion of input image is 224, Ture for the 56x56 output, False for 14x14 output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9029240807933836
      ],
      "excerpt": "Accuracy (single crop 224x224, %) on the validation set of ImageNet-1K and ImageNet-A \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "| DeiT-T | 72.2 | 7.3 | 5.7 | 1.3 | model | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8381399994644566
      ],
      "excerpt": "| DeiT-B | 81.8 | 27.4 | 86.6 | 17.6 |model | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "| Swin-T | 81.3 | 21.6 | 28.3 | 4.5 |model | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "| Swin-B | 83.5 | 35.8 | 87.8 | 15.4 | model | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8439989118259665
      ],
      "excerpt": "- +ours means we adopt the proposed classification head and token embedding module upon the other architectures. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9929910855407925,
        0.9807610118465049,
        0.9290770711558353,
        0.9804844541499885
      ],
      "excerpt": "CoLA (The Corpus of Linguistic Acceptability): the task is to judge whether a English sentence is grammatical or not. \nRTE (The Recognizing Textual Entailment datasets): the task is to determine whether the given pair of sentences is entailment or not. \nMNLI (The Multi-Genre Natural Language Inference Corpus): the task is to classify the given pair of sentences from multi-source is entailment, contradiction or neutral. \nQNLI (Qusetion-answering Natural Language Inference Corpus): the task is to decide the question-answer sentence pair is entailment or not. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "| BERT-base | 54.82 | 67.15 | 83.47 | 90.11 | model | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8460005547622456
      ],
      "excerpt": "| BERT-large | 60.63 | 73.65 | 85.90 | 91.82 | model | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8460005547622456
      ],
      "excerpt": "| SpanBERT-large | 64.32 | 78.34 | 87.89 | 94.22 |model| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9653311769162867,
        0.9327903613357913,
        0.8995508414747551,
        0.916990319821735
      ],
      "excerpt": "We make the further analysis by visualizing the models for CV and NLP tasks, where the SoT-Tiny and BERT-base are used as the backbone for each task respectively. We compare three variants base on the SoT-Tiny and BERT-base as follows: \n- ClassT: only classification token is used for classifier \n- WordT: only word tokens are used for classifier \n- ClassT+WordT: both classification token and word tokens are used for classifier based on the sum scheme. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9878169628334538
      ],
      "excerpt": "We can see the ClassT is more suitable for classifying the categories associated with the backgrounds and the whole context. The WordT performs classfication primarily based on some local discriminative regions. Our ClassT+WordT can make fully use of merits of both word tokens and classfication token, which can focus on the most important regions for better classficaiton by exploiting both local and global information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "SoT: Delving Deeper into Classification Head for Transformer",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jiangtaoxie/So-ViT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sun, 26 Dec 2021 02:24:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jiangtaoxie/SoT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jiangtaoxie/SoT",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jiangtaoxie/So-ViT/main/distributed_train.sh",
      "https://raw.githubusercontent.com/jiangtaoxie/So-ViT/main/scripts/train_SoT_Small.sh",
      "https://raw.githubusercontent.com/jiangtaoxie/So-ViT/main/scripts/train_SoT_Base.sh",
      "https://raw.githubusercontent.com/jiangtaoxie/So-ViT/main/scripts/train_SoT_Tiny.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please prepare the dataset as the following file structure:\n```sh\n.\n\u251c\u2500\u2500 train\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 class1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 class1_001.jpg\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 class1_002.jpg\n|   |   \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 class2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 class3\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 classN\n\u2514\u2500\u2500 val\n    \u251c\u2500\u2500 class1\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 class1_001.jpg\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 class1_002.jpg\n    |   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 class2\n    \u251c\u2500\u2500 class3\n    \u251c\u2500\u2500 ...\n    \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 classN\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- clone\n```sh\ngit clone https://github.com/jiangtaoxie/SoT.git\ncd SoT/\n```\n- install dependencies\n```sh\npip install -r requirments.txt\n```\nmain libs: torch(>=1.7.0) | timm(==0.3.4) | apex (alternative)\n- install\n```sh\npython setup.py install \n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9197220183943359
      ],
      "excerpt": "You can train the models of SoT family by using the command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.867277199996773,
        0.8067337688746127,
        0.8587479149996388
      ],
      "excerpt": "sh ./scripts/train_SoT_Tiny.sh #: reproduce SoT-Tiny \nsh ./scripts/train_SoT_Small.sh #: reproduce SoT-Small \nsh ./scripts/train_SoT_Base.sh #: reproduce SoT-Base \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806189353485689
      ],
      "excerpt": "    - regular: you can use dropout regularization to alleviate the overfitting \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8985497472164632
      ],
      "excerpt": "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img src=\"images/overview.jpg\" width=\"100%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.872425427181721
      ],
      "excerpt": "python main.py $DATA_ROOT $MODEL_NAME --b 256 --eval_checkpoint $CHECKPOINT_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.872425427181721
      ],
      "excerpt": "python main.py $DATA_ROOT $MODEL_NAME --b 256 --eval_checkpoint $CHECKPOINT_PATH --IN_A \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934477375241742
      ],
      "excerpt": "from sot_src.model import Classifier, OnlyVisualTokensClassifier \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from sot_src import TokenEmbed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8074395339715204
      ],
      "excerpt": "    large_output=False, #: When the resulotion of input image is 224, Ture for the 56x56 output, False for 14x14 output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123722800733537
      ],
      "excerpt": "| Swin-B | 83.5 | 35.8 | 87.8 | 15.4 | model | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920559551961167
      ],
      "excerpt": "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img src=\"images/vis.png\" width=\"100%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920559551961167
      ],
      "excerpt": "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img src=\"images/nlp_vis.png\" width=\"100%\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jiangtaoxie/SoT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SoT: Delving Deeper into Classification Head for Transformer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SoT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jiangtaoxie",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jiangtaoxie/SoT/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 36,
      "date": "Sun, 26 Dec 2021 02:24:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "deep-learning"
    ],
    "technique": "GitHub API"
  }
}