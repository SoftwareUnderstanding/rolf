{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2108.11250",
      "https://arxiv.org/abs/2108.11250",
      "https://arxiv.org/abs/2108.11250"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find our paper and code useful for your research, please consider giving a star :star:   and citation :pencil: :\n\n```BibTeX\n@misc{2108.11250,\nAuthor = {Dong Wu and Manwen Liao and Weitian Zhang and Xinggang Wang},\nTitle = {YOLOP: You Only Look Once for Panoptic Driving Perception},\nYear = {2021},\nEprint = {arXiv:2108.11250},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{2108.11250,\nAuthor = {Dong Wu and Manwen Liao and Weitian Zhang and Xinggang Wang},\nTitle = {YOLOP: You Only Look Once for Panoptic Driving Perception},\nYear = {2021},\nEprint = {arXiv:2108.11250},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9520170870521434,
        0.998725224435634,
        0.9818894004866677,
        0.9904682582335301
      ],
      "excerpt": "You Only Look at Once for Panoptic driving Perception \nby Dong Wu, Manwen Liao, Weitian Zhang, Xinggang Wang<sup> :email:</sup>     School of EIC, HUST \n(<sup>:email:</sup>) corresponding author. \narXiv technical report (arXiv 2108.11250) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| DLT-Net      | 89.4      | 68.4     | 9.3        | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| DLT-Net     | 71.3    | 9.3        | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hustvl/YOLOP",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-25T07:05:19Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-25T16:25:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9876289512383293,
        0.9817000159811112
      ],
      "excerpt": "We put forward an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection to save computational costs, reduce inference time as well as improve the performance of each task. Our work is the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100Kdataset. \nWe design the ablative experiments to verify the effectiveness of our multi-tasking scheme. It is proved that the three tasks can be learned jointly without tedious alternating optimization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639133800130596
      ],
      "excerpt": "| Faster R-CNN | 77.2      | 55.6     | 5.3        | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "| Model         | mIOU(%) | IOU(%) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8226525502478987
      ],
      "excerpt": "\u2502 \u251c\u2500core     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8313226313912119
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500evaluate.py   #: calculation of metric \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9407289142082519
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500general.py   #:calculation of metric\u3001nms\u3001conversion of data-format\u3001visualization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8994300841972388
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500postprocess.py   #: postprocess(refine da-seg and ll-seg, unrelated to paper) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9029368758445343
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500YOLOP.py    #: Setup and Configuration of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9910516134918242
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500deploy    #: Deployment of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167153484804862,
        0.8531402337945501
      ],
      "excerpt": "You can set the training configuration in the ./lib/config/default.py. (Including:  the loading of preliminary model,  loss,  data augmentation, optimizer, warm-up and cosine annealing, auto-anchor, training epochs, batch_size). \nIf you want try alternating optimization or train model for single task, please modify the corresponding configuration in ./lib/config/default.py to True. (As following, all configurations is False, which means training multiple tasks end to end). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9953299172415586
      ],
      "excerpt": "Our model can reason in real-time on Jetson Tx2, with Zed Camera to capture image. We use TensorRT tool for speeding up. We provide code for deployment and reasoning of model in  ./toolkits/deploy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "You Only Look Once for Panopitic Driving Perception.\uff08https://arxiv.org/abs/2108.11250\uff09",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Download the images from [images](https://bdd-data.berkeley.edu/).\n\n- Download the annotations of detection from [det_annotations](https://drive.google.com/file/d/1Ge-R8NTxG1eqd4zbryFo-1Uonuh0Nxyl/view?usp=sharing). \n- Download the annotations of drivable area segmentation from [da_seg_annotations](https://drive.google.com/file/d/1xy_DhUZRHR8yrZG3OwTQAHhYTnXn7URv/view?usp=sharing). \n- Download the annotations of lane line segmentation from [ll_seg_annotations](https://drive.google.com/file/d/1lDNTPIQj_YLNZVkksKM25CvCHuquJ8AP/view?usp=sharing). \n\nWe recommend the dataset directory structure to be the following:\n\n```\n#: The id represent the correspondence relation\n\u251c\u2500dataset root\n\u2502 \u251c\u2500images\n\u2502 \u2502 \u251c\u2500train\n\u2502 \u2502 \u251c\u2500val\n\u2502 \u251c\u2500det_annotations\n\u2502 \u2502 \u251c\u2500train\n\u2502 \u2502 \u251c\u2500val\n\u2502 \u251c\u2500da_seg_annotations\n\u2502 \u2502 \u251c\u2500train\n\u2502 \u2502 \u251c\u2500val\n\u2502 \u251c\u2500ll_seg_annotations\n\u2502 \u2502 \u251c\u2500train\n\u2502 \u2502 \u251c\u2500val\n```\n\nUpdate the your dataset path in the `./lib/config/default.py`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hustvl/yolop/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 172,
      "date": "Sun, 26 Dec 2021 03:15:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hustvl/YOLOP/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hustvl/YOLOP",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.800062136092593
      ],
      "excerpt": "\u2502 \u251c\u2500output   #: inference result \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8293841025514995
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500activations.py   #: activation function \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095861108490795
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500function.py   #: training and validation of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8813640456191563,
        0.876013646167693
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500AutoDriveDataset.py   #: Superclass dataset\uff0cgeneral function \n\u2502 \u2502 \u251c\u2500bdd.py   #: Subclass dataset\uff0cspecific function \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500convect.py  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924976426181745,
        0.9403770569177873,
        0.8203109519093676
      ],
      "excerpt": "\u2502 \u251c\u2500utils \n\u2502 \u2502 \u251c\u2500augmentations.py    #: data augumentation \n\u2502 \u2502 \u251c\u2500autoanchor.py   #: auto anchor(k-means) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9065702584881985,
        0.8008331685760428
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500utils.py  #: logging\u3001device_select\u3001time_measure\u3001optimizer_select\u3001model_save&amp;initialize \u3001Distributed training \n\u2502 \u251c\u2500run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8847560337905006,
        0.9521174821235511,
        0.950563948951535
      ],
      "excerpt": "\u2502 \u2502 \u251c\u2500demo.py    #: demo(folder\u3001camera) \n\u2502 \u2502 \u251c\u2500test.py     \n\u2502 \u2502 \u251c\u2500train.py     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233780710066119
      ],
      "excerpt": "You can set the training configuration in the ./lib/config/default.py. (Including:  the loading of preliminary model,  loss,  data augmentation, optimizer, warm-up and cosine annealing, auto-anchor, training epochs, batch_size). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8600727252980054
      ],
      "excerpt": "_C.TRAIN.SEG_ONLY = False           #: Only train two segmentation branchs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174751442005451
      ],
      "excerpt": "_C.TRAIN.ENC_SEG_ONLY = False       #: Only train encoder and two segmentation branchs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8563373328498016,
        0.8563373328498016,
        0.8068536020595574
      ],
      "excerpt": "_C.TRAIN.DRIVABLE_ONLY = False      #: Only train da_segmentation task \n_C.TRAIN.LANE_ONLY = False          #: Only train ll_segmentation task \n_C.TRAIN.DET_ONLY = False          #: Only train detection task \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8407651001510851
      ],
      "excerpt": "Start training: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.898584498893152
      ],
      "excerpt": "python tools/train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8295783008041001,
        0.8295783008041001
      ],
      "excerpt": "        <td><img src=pictures/input1.gif /></td> \n        <td><img src=pictures/output1.gif/></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8295783008041001,
        0.8295783008041001
      ],
      "excerpt": "         <td><img src=pictures/input2.gif /></td> \n        <td><img src=pictures/output2.gif/></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python toolkits/datasetpre/gen_bdd_seglabel.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.807324400598758
      ],
      "excerpt": "python toolkits/deploy/gen_wts.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hustvl/YOLOP/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "CMake"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Hust Visual Learning Team\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# You Only :eyes: Once for Panoptic \u200b :car: Perception",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "YOLOP",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hustvl",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hustvl/YOLOP/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This codebase has been developed with python version 3.7, PyTorch 1.7+ and torchvision 0.8+:\n\n```\nconda install pytorch==1.7.0 torchvision==0.8.0 cudatoolkit=10.2 -c pytorch\n```\n\nSee `requirements.txt` for additional dependencies and version requirements.\n\n```setup\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "TensorRT needs an engine file for inference. Building an engine is time-consuming. It is convenient to save an engine file so that you can reuse it every time you run the inference. The process is integrated in `main.cpp`. It can determine whether to build an engine according to the existence of your engine file.\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 811,
      "date": "Sun, 26 Dec 2021 03:15:53 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "| Training_method | Recall(%) | AP(%) | mIoU(%) | Accuracy(%) | IoU(%) |\n| --------------- | --------- | ----- | ------- | ----------- | ------ |\n| `ES-W`          | 87.0      | 75.3  | 90.4    | 66.8        | 26.2   |\n| `ED-W`          | 87.3      | 76.0  | 91.6    | 71.2        | 26.1   |\n| `ES-D-W`        | 87.0      | 75.1  | 91.7    | 68.6        | 27.0   |\n| `ED-S-W`        | 87.5      | 76.1  | 91.6    | 68.0        | 26.8   |\n| `End-to-end`    | 89.2      | 76.5  | 91.5    | 70.5        | 26.2   |\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "| Training_method | Recall(%) | AP(%) | mIoU(%) | Accuracy(%) | IoU(%) | Speed(ms/frame) |\n| --------------- | --------- | ----- | ------- | ----------- | ------ | --------------- |\n| `Det(only)`     | 88.2      | 76.9  | -       | -           | -      | 15.7            |\n| `Da-Seg(only)`  | -         | -     | 92.0    | -           | -      | 14.8            |\n| `Ll-Seg(only)`  | -         | -     | -       | 79.6        | 27.9   | 14.8            |\n| `Multitask`     | 89.2      | 76.5  | 91.5    | 70.5        | 26.2   | 24.4            |\n\n**Notes**: \n\n- The works we has use for reference including `Multinet`  ([paper](https://arxiv.org/pdf/1612.07695.pdf?utm_campaign=affiliate-ir-Optimise%20media%28%20South%20East%20Asia%29%20Pte.%20ltd._156_-99_national_R_all_ACQ_cpa_en&utm_content=&utm_source=%20388939),[code](https://github.com/MarvinTeichmann/MultiNet)\uff09,`DLT-Net`   ([paper](https://ieeexplore.ieee.org/abstract/document/8937825)\uff09,`Faster R-CNN`  ([paper](https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf),[code](https://github.com/ShaoqingRen/faster_rcnn)\uff09,`YOLOv5s`\uff08[code](https://github.com/ultralytics/yolov5))  ,`PSPNet`([paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Pyramid_Scene_Parsing_CVPR_2017_paper.pdf),[code](https://github.com/hszhao/PSPNet)) ,`ENet`([paper](https://arxiv.org/pdf/1606.02147.pdf),[code](https://github.com/osmr/imgclsmob))    `SCNN`([paper](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16802/16322),[code](https://github.com/XingangPan/SCNN))    `SAD-ENet`([paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Learning_Lightweight_Lane_Detection_CNNs_by_Self_Attention_Distillation_ICCV_2019_paper.pdf),[code](https://github.com/cardwing/Codes-for-Lane-Detection)). Thanks for their wonderful works.\n- In table 4, E, D, S and W refer to Encoder, Detect head, two Segment heads and whole network. So the Algorithm (First, we only train Encoder and Detect head. Then we freeze the Encoder and Detect head as well as train two Segmentation heads. Finally, the entire network is trained jointly for all three tasks.) can be marked as ED-S-W, and the same for others.\n\n---\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "multitask-learning",
      "object-detection",
      "lane-detection",
      "drivable-area-segmentation",
      "jetson-tx2",
      "autonomous-driving"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![yolop](pictures/yolop.png)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide two testing method.\n\n",
      "technique": "Header extraction"
    }
  ]
}