{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This work is based on the following paper (`Link\n<https://aaai.org/ojs/index.php/AAAI/article/view/4594>`_.)\n\n+ Di Carlo, V., Bianchi, F., & Palmonari, M. (2019). **Training Temporal Word Embeddings with a Compass**. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01), 6326-6334. https://doi.org/10.1609/aaai.v33i01.33016326\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9469621764797549
      ],
      "excerpt": "&lt;https://aaai.org/ojs/index.php/AAAI/article/view/4594&gt;_ for more details. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/valedica/twec",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-11T18:22:03Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-18T00:46:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9830444886934984,
        0.8178508686091848,
        0.9672426452361688,
        0.9367271509204094,
        0.9772824800067242,
        0.884593769380148
      ],
      "excerpt": "News May-2021: Thanks to wabyking (https://github.com/wabyking) we found that a gensim compilation problem was affecting the installation of our tool. The compass was unstable during the second part of the training. We updated our edited gensim package with the compilation so that this problem does not occur. There might be a small variation in the results you get with the new stable version. Our AAAI results were computed on a compiled version of the software and were not affected by this issue. \nThis package contains Python code to build temporal word embeddings with a compass! \nOne of the problems of temporal word embeddings is that they require alignment between corpora. \nWe propose a method to aligned distributional representation based on word2vec. \nThis method is efficient and it is based on a simple heuristic: we train an atemporal word embedding, the compass \nand we use this embedding to freeze one of the layers of the CBOW architecture. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9363140523640255
      ],
      "excerpt": "temporal embedding on the other matrix. See the paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9680856357004448,
        0.9668264359513639
      ],
      "excerpt": "Temporal word embeddings have been proposed to  support the analysis of word meaning shifts during time and to study \nthe evolution of languages. Different approaches have been proposed to generate vector representations of words that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8901657258453577,
        0.914571989821158,
        0.9667070747710225,
        0.8620007603842658,
        0.9466802312829613,
        0.9751619635485979,
        0.9291192398732897
      ],
      "excerpt": "these approaches may be difficult to apply in resource-scarce domains or by scientists with \nlimited in-depth knowledge of embedding models. In this paper, we propose a new heuristic to train \ntemporal word embeddings based on the Word2vec model. \nThe heuristic consists in using atemporal vectors as a reference, i.e., as a compass, when training the representations specific \nto a given time interval. The use of the compass simplifies the training process and makes it more efficient. \nExperiments conducted using state-of-the-art datasets and methodologies suggest that our approach outperforms or \nequals comparable approaches while being more robust in terms of the required corpus size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Training Temporal Word Embeddings with a Compass",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/valedica/twec/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sat, 25 Dec 2021 04:40:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/valedica/twec/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "valedica/twec",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/valedica/twec/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* **Remember**: when you call the training method of :code:`TWEC` the class creates a \"model/\" folder where it is going to save the trained objects. The compass will be trained as first element and it will be saved in that folder. If you want to overwrite it remember to set the parameter :code:`overwrite=True`, **otherwise** it will reload the already trained compass.\n\n* **What do you need**: temporal slices of text (i.e., text from 1991, text from 1992, etc...) and the concatenation of those text slices (the compass).\n\n* **The compass** should be the concatenation of the slice you want to align. In the next code section you will see that we are going to use arxiv papers text from two different years. The \"compass.txt\" file contains the concatenation of both slices.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Important**: always create a virtual environment because TWEC uses a custom version of the gensim library.\n\n* clone the repository\n* :code:`virtualenv -p python3.6 env`\n* :code:`source env/bin/activate`\n* :code:`pip install cython`\n* :code:`pip install git+https://github.com/valedica/gensim.git`\n* cd in repository\n* :code:`pip install -e .`\n\n**Jupyter**: you can use this in a jupyter-notebook, but remember that you need the virtual environment!\nIn the following the commands you need to use, but for a more detailed description of what we are doing see this `link\n<https://anbasile.github.io/programming/2017/06/25/jupyter-venv/>`_.\n\n* you need to install the virtual environment inside jupyter\n* :code:`source env/bin/activate`\n* :code:`(venv) $ pip install ipykernel`\n* :code:`(venv) $ ipython kernel install --user --name=twec_kernel`\n* you will find the \"twec_kernel\" when you create a new notebook\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/valedica/twec/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019, Federico Bianchi\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "twec",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "valedica",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/valedica/twec/blob/master/README.rst",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 46,
      "date": "Sat, 25 Dec 2021 04:40:48 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Training\n\nSuppose you have two slices of temporal text \"arxiv_14.txt\" and \"arxiv_9.txt\". First of all, create the concatenation\nof these two and create a \"compass.txt\" file. Now you can train the compass.\n\n.. code-block:: python\n\n    from twec.twec import TWEC\n    from gensim.models.word2vec import Word2Vec\n\n    aligner = TWEC(size=30, siter=10, diter=10, workers=4)\n\n    # train the compass: the text should be the concatenation of the text from the slices\n    aligner.train_compass(\"examples/training/compass.txt\", overwrite=False) # keep an eye on the overwrite behaviour\n..\n\nYou can see that the class covers the same parameters the Gensim word2vec library has. \"siter\" refers to the compass\ntraining iterations while \"diter\" refers to the training iteration of the specific temporal slices.\nAfter this first training you can train the slices:\n\n.. code-block:: python\n\n    # now you can train slices and they will be already aligned\n    # these are gensim word2vec objects\n    slice_one = aligner.train_slice(\"examples/training/arxiv_14.txt\", save=True)\n    slice_two = aligner.train_slice(\"examples/training/arxiv_9.txt\", save=True)\n..\n\nThese two slices are now aligned and can be compared!\n\n* Load Data\n\nYou can load data has you do with gensim.\n\n.. code-block:: python\n\n    model1 = Word2Vec.load(\"model/arxiv_14.model\")\n    model2 = Word2Vec.load(\"model/arxiv_9.model\")\n..\n\n",
      "technique": "Header extraction"
    }
  ]
}