{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1610.02915. \n\nThe code is based on Facebook's implementation of ResNet (https://github.com/facebook/fb.resnet.torch"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9998292185141419,
        0.9995991827938789
      ],
      "excerpt": "Dongyoon Han, Jiwhan Kim, and Junmo Kim, \"Deep Pyramidal Residual Networks\", CVPR 2017 (* equal contribution). \nArxiv: https://arxiv.org/abs/1610.02915.  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jhkim89/PyramidNet",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Jiwhan Kim (jhkim89@kaist.ac.kr),\nDongyoon Han (dyhan@kaist.ac.kr),\nJunmo Kim (junmo.kim@kaist.ac.kr)\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-10-10T10:00:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-17T02:23:21Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.982950314784915
      ],
      "excerpt": "This repository contains the code for the paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95339611876106,
        0.997388080126208
      ],
      "excerpt": "The code is based on Facebook's implementation of ResNet (https://github.com/facebook/fb.resnet.torch). \nDeep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolution layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. At the same time, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the capability of high-level attributes. Moreover, this also applies to residual networks and is very closely related to their performance. In this research, instead of using downsampling to achieve a sharp increase at each residual unit, we gradually increase the feature map dimension at all the units to involve as many locations as possible. This is discussed in depth together with our new insights as it has proven to be an effective design to improve the generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR datasets have shown that our network architecture has a superior generalization ability compared to the original residual networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9371466542445872,
        0.8943208013020187
      ],
      "excerpt": "Figure 2: Visual illustrations of (a) additive PyramidNet, (b) multiplicative PyramidNet, and (c) comparison of (a) and (b). \nTop-1 error rates on CIFAR-10 and CIFAR-100 datasets.  \"alpha\" denotes the widening factor; \"add\" and \"mul\" denote the results obtained with additive and multiplicative pyramidal networks, respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414254199797274
      ],
      "excerpt": "Top-1 error rates of our model with the bottleneck architecture on CIFAR-10 and CIFAR-100 datasets.  We use the additive pyramidal networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.954739165786929
      ],
      "excerpt": "Top-1 and Top-5 error rates of single-model, single-crop (224*224) on ImageNet dataset.  We use the additive PyramidNet for our results.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9205051930361378
      ],
      "excerpt": "We are currently testing our code in the ImageNet dataset.  We will upload the result when the training is completed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9675074405104094
      ],
      "excerpt": "Caffe implementation of PyramidNet is released. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9392628047335544
      ],
      "excerpt": "Results of the bottleneck architecture on CIFAR datasets are updated. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Torch implementation of the paper \"Deep Pyramidal Residual Networks\" (https://arxiv.org/abs/1610.02915).",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jhkim89/PyramidNet/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "| ----------------------------------------- | --------------- | ---------------------- | ----------- | ----------- |\n| PreResNet-200                             | 64.5M           |  2048                  | 21.66       | 5.79        |\n| PyramidNet-200, alpha=300                 | 62.1M           |  1456                  | 20.47       | 5.29        |\n| PyramidNet-200, alpha=450, Dropout (0.5)  | 116.4M          |  2056                  | 20.11       | 5.43        |\n\nModel files download: [link](https://1drv.ms/f/s!AmNvwgeB0n4GsiDFDNJWZkEbajJf)\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 38,
      "date": "Tue, 28 Dec 2021 05:09:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jhkim89/PyramidNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jhkim89/PyramidNet",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jhkim89/PyramidNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Lua"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PyramidNet",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PyramidNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jhkim89",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jhkim89/PyramidNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 124,
      "date": "Tue, 28 Dec 2021 05:09:55 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "residual-networks",
      "network-architecture",
      "imagenet",
      "deep-learning",
      "cnn-architecture",
      "resnet",
      "cifar-10",
      "cifar-100",
      "torch7"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install Torch (http://torch.ch) and ResNet (https://github.com/facebook/fb.resnet.torch).\n2. Add the files addpyramidnet.lua and mulpyramidnet.lua to the folder \"models\".\n3. Manually set the parameter \"alpha\" in the files addpyramidnet.lua and mulpyramidnet.lua (Line 28).\n4. Change the learning rate schedule in the file train.lua: \"decay = epoch >= 122 and 2 or epoch >= 81 and 1 or 0\" to \"decay = epoch >= 225 and 2 or epoch >= 150 and 1 or 0\".\n5. Train our PyramidNet, by running main.lua as below:\n\nTo train additive PyramidNet-164 (alpha=48) on CIFAR-10 dataset:\n```bash\nth main.lua -dataset cifar10 -depth 164 -nEpochs 300 -LR 0.1 -netType addpyramidnet -batchSize 128 -shareGradInput true\n```\nTo train additive PyramidNet-164 (alpha=48) with 4 GPUs on CIFAR-100 dataset:\n```bash\nth main.lua -dataset cifar100 -depth 164 -nEpochs 300 -LR 0.5 -nGPU 4 -nThreads 8 -netType addpyramidNet -batchSize 128 -shareGradInput true\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}