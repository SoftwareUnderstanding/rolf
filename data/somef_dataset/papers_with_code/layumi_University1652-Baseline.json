{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.12186",
      "https://arxiv.org/abs/2002.12186"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The following paper uses and reports the result of the baseline model. You may cite it in your paper.\n```bibtex\n@article{zheng2020university,\n  title={University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization},\n  author={Zheng, Zhedong and Wei, Yunchao and Yang, Yi},\n  journal={ACM Multimedia},\n  year={2020}\n}\n```\nInstance loss is defined in \n```bibtex\n@article{zheng2017dual,\n  title={Dual-Path Convolutional Image-Text Embeddings with Instance Loss},\n  author={Zheng, Zhedong and Zheng, Liang and Garrett, Michael and Yang, Yi and Xu, Mingliang and Shen, Yi-Dong},\n  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},\n  doi={10.1145/3383184},\n  volume={16},\n  number={2},\n  pages={1--23},\n  year={2020},\n  publisher={ACM New York, NY, USA}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zheng2017dual,\n  title={Dual-Path Convolutional Image-Text Embeddings with Instance Loss},\n  author={Zheng, Zhedong and Zheng, Liang and Garrett, Michael and Yang, Yi and Xu, Mingliang and Shen, Yi-Dong},\n  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},\n  doi={10.1145/3383184},\n  volume={16},\n  number={2},\n  pages={1--23},\n  year={2020},\n  publisher={ACM New York, NY, USA}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zheng2020university,\n  title={University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization},\n  author={Zheng, Zhedong and Wei, Yunchao and Yang, Yi},\n  journal={ACM Multimedia},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| Query_drone | 37,855 | 701 |  39 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9104388306336967
      ],
      "excerpt": "\u251c\u2500\u2500 University-1652/ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/layumi/University1652-Baseline",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-04T07:27:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T14:44:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8666032096254705
      ],
      "excerpt": "Code Features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9128264750167299,
        0.9737539123905984,
        0.8933586441438572,
        0.8397501200953357
      ],
      "excerpt": "21 January 2021 The GPU-Re-Ranking,  a GNN-based real-time post-processing code, is at Here. \n21 August 2020 The transfer learning code for Oxford and Paris is at Here. \n27 July 2020 The meta data of 1652 buildings, such as latitude and longitude, are now available at Google Driver. (You could use Google Earth Pro to open the kml file or use vim to check the value). \nWe also provide the spiral flight tour file at Google Driver. (You could open the kml file via Google Earth Pro to enable the flight camera).   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195571780090807
      ],
      "excerpt": "12 March 2020 I add the state-of-the-art page for geo-localization and tutorial, which will be updated soon. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055455732502326
      ],
      "excerpt": "- Float16 to save GPU memory based on apex \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "ACM Multimedia2020 University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization :helicopter: annotates 1652 buildings in 72 universities around the world.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/layumi/University1652-Baseline/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 40,
      "date": "Tue, 28 Dec 2021 11:35:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/layumi/University1652-Baseline/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "layumi/University1652-Baseline",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/layumi/University1652-Baseline/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/layumi/University1652-Baseline/master/GPU-Re-Ranking/extension/make.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download [University-1652] upon request. You may use the request [template](https://github.com/layumi/University1652-Baseline/blob/master/Request.md).\n\nOr download [CVUSA](http://cs.uky.edu/~jacobs/datasets/cvusa/) / [CVACT](https://github.com/Liumouliu/OriCNN). \n\nFor CVUSA, I follow the training/test split in (https://github.com/Liumouliu/OriCNN). \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install Pytorch from http://pytorch.org/\n- Install Torchvision from the source\n```\ngit clone https://github.com/pytorch/vision\ncd vision\npython setup.py install\n```\n- [Optinal] You may skip it. Install apex from the source\n```\ngit clone https://github.com/NVIDIA/apex.git\ncd apex\npython setup.py install --cuda_ext --cpp_ext\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.920992687207367
      ],
      "excerpt": "- Float16 to save GPU memory based on apex \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8194842505323139
      ],
      "excerpt": "Dataset Preparation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.810949288049394
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 train/ \n\u2502       \u251c\u2500\u2500 drone/                   /* drone-view training images  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810949288049394
      ],
      "excerpt": "\u2502       \u251c\u2500\u2500 street/                  /* street-view training images  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 test/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8155013436345219,
        0.963577944896241
      ],
      "excerpt": "python train.py --name three_view_long_share_d0.75_256_s1_google  --extra --views 3  --droprate 0.75  --share  --stride 1 --h 256  --w 256 --fp16;  \npython test.py --name three_view_long_share_d0.75_256_s1_google \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8676285822639416,
        0.9447871161116602
      ],
      "excerpt": "python prepare_cvusa.py \npython train_cvusa.py --name usa_vgg_noshare_warm5_lr2 --warm 5 --lr 0.02 --use_vgg16 --h 256 --w 256  --fp16 --batchsize 16; \npython test_cvusa.py  --name usa_vgg_noshare_warm5_lr2 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/layumi/University1652-Baseline/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "MATLAB",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Zhedong Zheng\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Table of contents",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "University1652-Baseline",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "layumi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/layumi/University1652-Baseline/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "layumi",
        "body": "Add supports for seven losses in one repo, including contrast, triplet, lifted, circle, arcface, cosface and sphere.",
        "dateCreated": "2021-12-16T03:44:52Z",
        "datePublished": "2021-12-16T03:51:04Z",
        "html_url": "https://github.com/layumi/University1652-Baseline/releases/tag/v1.1",
        "name": "v1.1",
        "tag_name": "v1.1",
        "tarball_url": "https://api.github.com/repos/layumi/University1652-Baseline/tarball/v1.1",
        "url": "https://api.github.com/repos/layumi/University1652-Baseline/releases/55418916",
        "zipball_url": "https://api.github.com/repos/layumi/University1652-Baseline/zipball/v1.1"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.6\n- GPU Memory >= 8G\n- Numpy > 1.12.1\n- Pytorch 0.3+ \n- [Optional] apex (for float16) \n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 283,
      "date": "Tue, 28 Dec 2021 11:35:01 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npython train_no_street.py --name two_view_long_no_street_share_d0.75_256_s1  --share --views 3  --droprate 0.75  --stride 1 --h 256  --w 256  --fp16; \npython test.py --name two_view_long_no_street_share_d0.75_256_s1\n```\nSet three views but set the weight of loss on street images to zero.\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "drone",
      "image-retrieval",
      "cross-view",
      "pytorch",
      "cvusa",
      "apex",
      "geo-localization",
      "multi-source-benchmark",
      "satellite",
      "awesome-list",
      "dataset",
      "cvact",
      "gem-pooling",
      "remote-sensing"
    ],
    "technique": "GitHub API"
  }
}