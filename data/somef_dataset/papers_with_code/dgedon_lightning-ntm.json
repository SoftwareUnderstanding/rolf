{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1410.5401"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dgedon/lightning-ntm",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-26T09:04:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-10T14:43:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9827240049566404,
        0.9831193327873574,
        0.9101032820598596,
        0.9478996837780389
      ],
      "excerpt": "This is a PyTorch Lightning implementation of the Neural Turing Machine (NTM). \nFor more details on NTM please see the paper. \nPytorch lightning is the lightweight PyTorch wrapper.  \nIt organises your code neatly, abstracts away all the complicated and error prone engineering, is  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9606274606978624,
        0.9845865105987562,
        0.9396887532106665,
        0.8288064714685593,
        0.96415008725243,
        0.9643882449233069,
        0.9814647520088362,
        0.8095558704548378,
        0.8837478213570749,
        0.948633436942741,
        0.9408051003501285,
        0.9168574524644437,
        0.8133770561892228
      ],
      "excerpt": "For more information on PyTorch Lightning, see the documentation. \nThis repository is a PyTorch Lighting conversion of this PyTorch NTM implementation.  \nWe extend the available implementation with the LSTM network as baseline comparison. We can divide the repository in three main parts: \n1. run_train.py is the Lightning trainer which runs the training loop and logs the outputs. \n2. data_copytask.py is the Lightning dataset for the copy task in the original paper. We do not implement the copy-repeat task but this could be done similar to the original PyTorch repository. \n3. model.py is the Lightning model which specifies the training and validation loop. Within this model we call the different models which are: \n- model_ntm.py which is the NTM implementation. The remaining files are in the folder ntm/*. This is a copy of the files from the original repository. Credits go to these authors. \n- 'model_lstm.py' which is the LSTM baseline implementation. \nNote that we are generating training and validation sequences on the fly for each epoch differently. \nIn this part we present some results that we obtained for the copy task.  \nThe goal of the copy task is to test the ability to store and remember arbitrary long sequences.  \nThe input is a sequence random length (between 1 and 20) with a given number of bits (8) followed by a delimiter bit. \nE.g. we may obtain an input sequence of 20 by 8 which we want to store and remember at the output. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch Lightning implementation of Neural Turing Machine (NTM).",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dgedon/lightning-ntm/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 10:41:37 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dgedon/lightning-ntm/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dgedon/lightning-ntm",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dgedon/lightning-ntm/main/multiple_train.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9027484815469652
      ],
      "excerpt": "We run both networks over 10 seeds using the bash command multiple_train.sh. See the options within the scripts for \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dgedon/lightning-ntm/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PyTorch Lightning Neural Turing Machine (NTM)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "lightning-ntm",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dgedon",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dgedon/lightning-ntm/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 10:41:37 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "python",
      "neural-network",
      "pytorch",
      "pytorch-lightning",
      "lstm",
      "ntm",
      "neural-turing-machine"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Setup the environment\n```bash\npip install -r requirements.txt\n```\n\nTo run a model, call\n```bash\npython run_train.py --model MODELNAME\n```\nwith MODELNAME either ntm or lstm.\n\nYou can add any number of Lightning specific options e.g.\n```bash\npython run_train.py --model ntm --gpus 1 --fast_dev_run True\n```\nruns the ntm model on a single GPU but it only does one fast test run to check all parts of the code.\n\n",
      "technique": "Header extraction"
    }
  ]
}