{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.1556\n\n[3]<a id='3'></a> D. Erhan, Y. Bengio, A. Courville and P. Vincent. *Visualizing higher-layer features of a deep network*  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Technical report, University of Montreal, 1341 (2009",
      "https://arxiv.org/abs/1804.11191"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1]<a id='1'></a> https://github.com/vdumoulin/conv_arithmetic  \n\n[2]<a id='2'></a> *Very Deep Convolutional Networks for Large-Scale Image Recognition.* Simonyan, K.,\nZisserman, A. 2015.  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;https://arxiv.org/abs/1409.1556\n\n[3]<a id='3'></a> D. Erhan, Y. Bengio, A. Courville and P. Vincent. *Visualizing higher-layer features of a deep network*  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Technical report, University of Montreal, 1341 (2009), p3.\n\n[4]<a id='4'></a> Z. Qin, F. Yu, C. Liu, and X. Chen, *How convolutional neural network see the world - A survey of   \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;convolutional neural network visualization methods*. 2018. https://arxiv.org/abs/1804.11191 ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9024179703694309
      ],
      "excerpt": "            <td width=\"5%\" align=\"center\"> Layer 10 - Filter 10</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8345534538202741
      ],
      "excerpt": "            <td width=\"5%\" align=\"center\"> Layer 12 - Filter 10</td> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/justinbellucci/cnn-visualizations-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-04T16:38:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-16T10:09:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9985595368875405
      ],
      "excerpt": "This repository is an attempt to visually represent the inner workings of convolutional neural networks. This work is by no means revolutionary, however, the goal is to illustrate various methods for representing how a CNN makes decisions. In this effort I hope to understand the fine details of CNNs. Deep neural networks do not have to be black boxes. It may seem that it is some miracle that a model can identify a cat in an image, but believe me, it's not. It's just really complicated math under the hood. I believe that every ML engineer should understand how their model makes decisions, which ultimatly should answer questions related to bias. I'm new at this so bare with me... \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9923711435631896
      ],
      "excerpt": "Generally speaking, filters in a CNN are used to extract information from an image that is then passed through the network to make predictions. These filters are called kernels. Mathmatically they perform operations on pixels that reduce an image to basic features. Each CNN layer can have hundreds of layers (kernels). These layers make up the depth of a CNN. The following gif<sup>[1]</sup> illustrates how a filter is applied to an an image: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717525938185565,
        0.9783895188502194
      ],
      "excerpt": "In order to visualize the various filters and feature maps of a neural netork we first need to load a pre-trained network from Pytorch. We will use the VGG16<sup>[2]</sup> neural network and extract each corresponding convolutional layer. We will not performing backpropagation. Instead, we will use each layer's weights to help visualize the filters used and the resulting image processing. \nTaking a look at 3 of the 13 convolutional layers in the VGG16 model we see that there is increased depth as we move through the model. The following images illustrate each filter in the respective layers. Note: The filters are displayed in grayscale for readability. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307576597544879
      ],
      "excerpt": "When we pass an image into the pre-trained network we process it at each layer and save the respective image representation. This is essentially what the image looks like after each filter is applied. First we will pass in an adorable picture of a black lab. Yea, I know.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9558790444658999
      ],
      "excerpt": "When we pass the image through the first convolutional layer we will essentially get 64 corresponding activation maps. Let's take a look at when kernel 17 is applied to the image on layer 1. Note: There is some preprocessing that was done which is why the image looks squished.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.967269574544777
      ],
      "excerpt": "After some pre-processing the below block of code takes an image and applies it to each torch.nn.Conv2d layer. The output of one layer is the input to the next.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9927873284158655
      ],
      "excerpt": "The depth of Layer 1 is 64. You can see how each filter extracts different details from the image. Layer 1 feature maps are fairly clear. As we move deeper into the model we can see how the detail in the image starts to degrade. Can you pick out what the feature maps are representing? Sometimes the outline of the image is clear, sometimes dark colors are emphesized, and sometimes it is hard to tell it what the image is originally of.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9484796484322163,
        0.9923995484387554
      ],
      "excerpt": "Activation Maximization was first proposed by Erhan et al.<sup>[3]</sup> in 2009 as a way to communicate CNN behavior. Specifically as a way to intepret or visualize learned feature maps. This learned feature map can be represented by an active state of particular neurons. By looking at the maximimum activation of particular neurons we can visualize what patters are larned in particular filters. \nWe start with a pretrained Vgg16 model and a noisy image as seen below. This image is passed through the network. At a particular layer the gradient with respect to the noisy image is calculated at each neuron.<sup>[4]</sup> This is calculted using backpropagation, while keeping the parameters of the model fixed. The hook_fn in the ActivationMaximizationVis() class captures the calculated gradients. Each pixel in the original noisy image is then iteratively changed to maximize the activation of the neuron. In otherwords, each pixel in the noisy image is iteratively changed to push the gradient to a maximum for that particular value. The pixel values are updated until a desired image is found.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863568594622075
      ],
      "excerpt": "As we move deeper in the network you can see that more complex patters emerge. Some of the activation maps of later layers look like trees, eyes, and feathers. Well, at least that's what it looks like to me. We all may see something different.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Exploration of various methods to visualize layers of deep Convolutional Neural Networks using Pytorch.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/justinbellucci/cnn-visualizations-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 02:19:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/justinbellucci/cnn-visualizations-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "justinbellucci/cnn-visualizations-pytorch",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/justinbellucci/cnn-visualizations-pytorch/master/cnn_filter_vis.ipynb",
      "https://raw.githubusercontent.com/justinbellucci/cnn-visualizations-pytorch/master/max_activations_vis.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If would like to tinker feel free to install locally and make it your own.\n\n1. Install dependencies. I generally use Conda for my environment and package management. \n\n\t>`conda install -c conda-forge jupyterlab`  \n\n\t>`pip install requirments.txt` \n\n2. The following Jupyter notebooks outline various visualization methods:\n    * `cnn_filter_vis.ipynb` Jupyter notebook \n\t* `max_activations_vis.ipynb` Jupyter notebook \n\n<a id='filter_vis'></a>\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8643359819275696
      ],
      "excerpt": "Now if we take a look more layers you can see... \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8895456601305671
      ],
      "excerpt": "<img width=\"250\" src = \"images/padding_strides.gif\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8325643651212097,
        0.8179963753741714,
        0.8536205892285845
      ],
      "excerpt": "            <td width=\"20%\" align=\"center\"> Layer 1: 3x3 Kernel: Depth 64 </td> \n            <td width=\"20%\" align=\"center\"> Layer 5: 3x3 Kernel: Depth 256 </td> \n            <td width=\"20%\" align=\"center\"> Layer 10: 3x3 Kernel: Depth 512 </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8870392772742128,
        0.8870392772742128,
        0.8870392772742128
      ],
      "excerpt": "            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/conv_layer_1_filter.jpg\"> </td> \n            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/conv_layer_5_filter.jpg\"> </td> \n            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/conv_layer_10_filter.jpg\"> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295406139801248
      ],
      "excerpt": "<img width=\"250\" src = \"images/Labrador_retriever_01.jpg\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9197735714648932
      ],
      "excerpt": "<img width=\"500\" src = \"filter_imgs/lab_layer_1.jpg\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8870392772742128,
        0.8870392772742128
      ],
      "excerpt": "            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/conv_layer_1_filter.jpg\"> </td> \n            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/conv_layer_1_output.jpg\"> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8870392772742128,
        0.8870392772742128,
        0.8870392772742128
      ],
      "excerpt": "            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/lab_layer_2.jpg\"> </td> \n            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/lab_layer_2.jpg\"> </td> \n            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/lab_layer_6.jpg\"> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8870392772742128,
        0.8870392772742128,
        0.8870392772742128
      ],
      "excerpt": "            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/lab_layer_8.jpg\"> </td> \n            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/lab_layer_10.jpg\"> </td> \n            <td width=\"20%\" align=\"center\"> <img src=\"filter_imgs/lab_layer_12.jpg\"> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9197735714648932,
        0.9197735714648932
      ],
      "excerpt": "<img width=\"250\" src = \"filter_imgs/01_noisy_image.jpg\"> \n<img width=\"250\" src = \"activ_max_imgs/am_vis_l15_f220_iter51.jpg\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807
      ],
      "excerpt": "            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l1_f1_iter31.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l1_f5_iter21.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l1_f6_iter31.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l1_f55_iter41.jpg\"> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807
      ],
      "excerpt": "            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l3_f1_iter31.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l3_f5_iter41.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l3_f28_iter31.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l3_f38_iter31.jpg\"> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807
      ],
      "excerpt": "            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l10_f5_iter41.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l10_f10_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l10_f65_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l10_f165_iter51.jpg\"> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807
      ],
      "excerpt": "            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l12_f28_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l12_f68_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l12_f168_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l12_f178_iter51.jpg\"> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807
      ],
      "excerpt": "            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l14_f28_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l14_f58_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l14_f158_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l14_f178_iter51.jpg\"> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807
      ],
      "excerpt": "            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l15_f40_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l15_f65_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l15_f165_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l15_f220_iter51.jpg\"> </td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807,
        0.8779133247856807
      ],
      "excerpt": "            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l16_f17_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l16_f128_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l16_f156_iter51.jpg\"> </td> \n            <td width=\"5%\" align=\"center\"> <img src=\"activ_max_imgs/am_vis_l16_f157_iter41.jpg\"> </td> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/justinbellucci/cnn-visualizations-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Convolutional Neural Network Visualizations",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "cnn-visualizations-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "justinbellucci",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/justinbellucci/cnn-visualizations-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Fri, 24 Dec 2021 02:19:14 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "cnn",
      "cnn-visualization",
      "pytorch",
      "filter-visualization",
      "activation-maximization"
    ],
    "technique": "GitHub API"
  }
}