# Deep Q-Learning for Connect Four

This repository provides code for training a neural network to play Connect Four.
Connect Four is a small strategic board game where two players take turns laying a stone of their color into one of seven columns. If a player manages to have four connecting stones in a row, column, or diagonal, it wins.

## Prerequisites

python3 3.8

jax

scipy

## Usage

Run the code with

`python3 main.py`.

## Background

Deep Q-Learning was first introduced by [1], who trained a relatively small neural network to play Atari games. The idea is to train a neural network to return the Q-function for each action given the state of the game.
The Q-function is defined as the expected discounted reward when following a policy.
Choosing the action that maximizes the Q-value as a policy yields a better policy.
Training on the resulting policy optimizes the Q-function again, leading to overall improvement.
Exploring different actions once in a while can help with exploration.
One therefore often does pick the policy that maximizes the Q-function, but rather picks a random action with probability epsilon.

## Implementation

`main.py` holds the training loop, in which many rounds of self-play are carried out. The generated trajectories are stored in a Memory object, of which the implementation is inside `memory.py`. This object then provides preprocessed batches for training the agent.
The agent itself is defined in `agent.py`. It consists of a simple feed forward convolutional neural network. 
The game and win condition are defined in `fwins_env.py`.
The agent can be tested by playing against it after training.
Hereby the power is enhanced by recursively looking ahead using a min-max algorithm.

## Observation

The Q-learner picks up some aspects and tactics, such as seeing the benefit of starting in the middle. 
This happens already when training for only a short amount of time.
However, the bot can still be defeated with a bit of effort, even when its power is enhanced by recursively looking ahead.
The learned Q-function itself still 'oversees' some possibilities to win or loose the game, even if it is possible in the next step.
Potentially the power could be increased by training a larger network using more computation time.

Another potential pitfall might be the outdated approach that is taken here. Normally, Q-learner learn by training on trajectories generated by one previous version of themselves. Here, however, the Q-learner is trained on trajectories generated by itself, and then immediately updated with a sample from the memory buffer. This might lead to undesired feedback loops and makes the algorithm less stable overall.

The algorithm could also be improved by using a more modern actor critic learning algorithm.

[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra and Martin Riedmiller. "Playing Atari with Deep Reinforcement Learning", https://arxiv.org/abs/1312.5602v1
