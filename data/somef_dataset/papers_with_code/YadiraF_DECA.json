{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For functions or scripts that are based on external sources, we acknowledge the origin individually in each file.  \nHere are some great resources we benefit:  \n- [FLAME_PyTorch](https://github.com/soubhiksanyal/FLAME_PyTorch) and [TF_FLAME](https://github.com/TimoBolkart/TF_FLAME) for the FLAME model  \n- [Pytorch3D](https://pytorch3d.org/), [neural_renderer](https://github.com/daniilidis-group/neural_renderer), [SoftRas](https://github.com/ShichenLiu/SoftRas) for rendering  \n- [kornia](https://github.com/kornia/kornia) for image/rotation processing  \n- [face-alignment](https://github.com/1adrianb/face-alignment) for cropping   \n- [FAN](https://github.com/1adrianb/2D-and-3D-face-alignment) for landmark detection\n- [face_segmentation](https://github.com/YuvalNirkin/face_segmentation) for skin mask\n- [VGGFace2-pytorch](https://github.com/cydonia999/VGGFace2-pytorch) for identity loss  \n\nWe would also like to thank other recent public 3D face reconstruction works that allow us to easily perform quantitative and qualitative comparisons :)  \n[RingNet](https://github.com/soubhiksanyal/RingNet), \n[Deep3DFaceReconstruction](https://github.com/microsoft/Deep3DFaceReconstruction/blob/master/renderer/rasterize_triangles.py), \n[Nonlinear_Face_3DMM](https://github.com/tranluan/Nonlinear_Face_3DMM),\n[3DDFA-v2](https://github.com/cleardusk/3DDFA_V2),\n[extreme_3d_faces](https://github.com/anhttran/extreme_3d_faces),\n[facescape](https://github.com/zhuhao-nju/facescape)\n<!-- 3DMMasSTN, DenseReg, 3dmm_cnn, vrn, pix2vertex -->\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2012.04012",
      "https://arxiv.org/abs/2012.04012"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find our work useful to your research, please consider citing:\n```\n@inproceedings{DECA:Siggraph2021,\n  title={Learning an Animatable Detailed {3D} Face Model from In-The-Wild Images},\n  author={Feng, Yao and Feng, Haiwen and Black, Michael J. and Bolkart, Timo},\n  journal = {ACM Transactions on Graphics, (Proc. SIGGRAPH)}, \n  volume = {40}, \n  number = {8}, \n  year = {2021}, \n  url = {https://doi.org/10.1145/3450626.3459936} \n}\n```\n\n<!-- ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{DECA:Siggraph2021,\n  title={Learning an Animatable Detailed {3D} Face Model from In-The-Wild Images},\n  author={Feng, Yao and Feng, Haiwen and Black, Michael J. and Bolkart, Timo},\n  journal = {ACM Transactions on Graphics, (Proc. SIGGRAPH)}, \n  volume = {40}, \n  number = {8}, \n  year = {2021}, \n  url = {https://doi.org/10.1145/3450626.3459936} \n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/YadiraF/DECA",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-08T00:51:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T02:38:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9917059388670153,
        0.879645721993968
      ],
      "excerpt": "This is the official Pytorch implementation of DECA.  \nDECA reconstructs a 3D head model with detailed facial geometry from a single input image. The resulting 3D head model can be easily animated. Please refer to the [arXiv paper](https://arxiv.org/abs/2012.04012) for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8487214575194624,
        0.9298655532950987,
        0.8552803511479157,
        0.9198887663329444,
        0.8983405465997888
      ],
      "excerpt": "* **Animation:** animate the face with realistic wrinkle deformations. \n* **Robustness:** tested on facial images in unconstrained conditions.  Our method is robust to various poses, illuminations and occlusions.  \n* **Accurate:** state-of-the-art 3D face shape reconstruction on the [NoW Challenge](https://ringnet.is.tue.mpg.de/challenge) benchmark dataset. \nDECA (ours) achieves 9% lower mean shape reconstruction error on the NoW Challenge dataset compared to the previous state-of-the-art method. \nThe left figure compares the cumulative error of our approach and other recent methods (RingNet and Deng et al. have nearly identitical performance, so their curves overlap each other). Here we use point-to-surface distance as the error metric, following the NoW Challenge.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9632290613340868
      ],
      "excerpt": "For more details of the evaluation, please check our arXiv paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8427313910263682
      ],
      "excerpt": "face_segmentation to get skin mask   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8087191067941633
      ],
      "excerpt": "    We use the model from VGGFace2-pytorch for calculating identity loss, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9656403739548208
      ],
      "excerpt": "    and put it into ./data   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "DECA: Detailed Expression Capture and Animation (SIGGRAPH 2021)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/YadiraF/DECA/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 159,
      "date": "Wed, 29 Dec 2021 05:01:57 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/YadiraF/DECA/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "YadiraF/DECA",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/YadiraF/DECA/master/install_conda.sh",
      "https://raw.githubusercontent.com/YadiraF/DECA/master/install_pip.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8096422022825122
      ],
      "excerpt": "    download resnet50_ft, \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8699780450325489
      ],
      "excerpt": "<img src=\"TestSamples/teaser/results/teaser.gif\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9001373101481486
      ],
      "excerpt": "<img src=\"Doc/images/DECA_evaluation_github.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810870454768102
      ],
      "excerpt": "    download resnet50_ft, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8407651001510851,
        0.8012239702840642
      ],
      "excerpt": "Start training \nTrain from scratch:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587684566769097,
        0.8587684566769097,
        0.8587684566769097
      ],
      "excerpt": "python main_train.py --cfg configs/release_version/deca_pretrain.yml  \npython main_train.py --cfg configs/release_version/deca_coarse.yml  \npython main_train.py --cfg configs/release_version/deca_detail.yml \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/YadiraF/DECA/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/YadiraF/DECA/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'License\\n\\nSoftware Copyright License for non-commercial scientific research purposes\\nPlease read carefully the following terms and conditions and any accompanying documentation before you download\\nand/or use the DECA model, data and software, (the \"Model & Software\"), including 3D meshes, software, and scripts.\\nBy downloading and/or using the Model & Software (including downloading, cloning, installing, and any other use\\nof this github repository), you acknowledge that you have read these terms and conditions, understand them, and\\nagree to be bound by them. If you do not agree with these terms and conditions, you must not download and/or use\\nthe Model & Software. Any infringement of the terms of this agreement will automatically terminate your rights\\nunder this License\\n\\nOwnership / Licensees\\nThe Model & Software and the associated materials has been developed at the\\nMax Planck Institute for Intelligent Systems (hereinafter \"MPI\").\\n\\nAny copyright or patent right is owned by and proprietary material of the\\nMax-Planck-Gesellschaft zur F\\xc3\\xb6rderung der Wissenschaften e.V. (hereinafter \\xe2\\x80\\x9cMPG\\xe2\\x80\\x9d; MPI and MPG hereinafter\\ncollectively \\xe2\\x80\\x9cMax-Planck\\xe2\\x80\\x9d) hereinafter the \\xe2\\x80\\x9cLicensor\\xe2\\x80\\x9d.\\n\\nLicense Grant\\nLicensor grants you (Licensee) personally a single-user, non-exclusive, non-transferable, free of charge right:\\n\\n\\t\\xe2\\x80\\xa2 To install the Model & Software on computers owned, leased or otherwise controlled by you and/or your organization.\\n\\t\\xe2\\x80\\xa2 To use the Model & Software for the sole purpose of performing peaceful non-commercial scientific research, \\n    non-commercial education, or non-commercial artistic projects.\\n\\nAny other use, in particular any use for commercial, pornographic, military, or surveillance purposes is prohibited. \\nThis includes, without limitation, incorporation in a commercial product, use in a commercial service, \\nor production of other artefacts for commercial purposes. \\n\\nThe Model & Software may not be used to create fake, libelous, misleading, or defamatory content of any kind, excluding \\nanalyses in peer-reviewed scientific research. \\n\\nThe Model & Software may not be reproduced, modified and/or made available in any form to any third party \\nwithout Max-Planck\\xe2\\x80\\x99s prior written permission.\\n\\nThe Model & Software may not be used for pornographic purposes or to generate pornographic material whether\\ncommercial or not. This license also prohibits the use of the Model & Software to train methods/algorithms/neural\\nnetworks/etc. for commercial use of any kind. By downloading the Model & Software, you agree not to reverse engineer it.\\n\\nNo Distribution\\nThe Model & Software and the license herein granted shall not be copied, shared, distributed, re-sold, offered\\nfor re-sale, transferred or sub-licensed in whole or in part except that you may make one copy for archive\\npurposes only.\\n\\nDisclaimer of Representations and Warranties\\nYou expressly acknowledge and agree that the Model & Software results from basic research, is provided \\xe2\\x80\\x9cAS IS\\xe2\\x80\\x9d,\\nmay contain errors, and that any use of the Model & Software is at your sole risk.\\nLICENSOR MAKES NO REPRESENTATIONS\\nOR WARRANTIES OF ANY KIND CONCERNING THE MODEL & SOFTWARE, NEITHER EXPRESS NOR IMPLIED, AND THE ABSENCE OF ANY\\nLEGAL OR ACTUAL DEFECTS, WHETHER DISCOVERABLE OR NOT. Specifically, and not to limit the foregoing, licensor\\nmakes no representations or warranties (i) regarding the merchantability or fitness for a particular purpose of\\nthe Model & Software, (ii) that the use of the Model & Software will not infringe any patents, copyrights or other\\nintellectual property rights of a third party, and (iii) that the use of the Model & Software will not cause any\\ndamage of any kind to you or a third party.\\n\\nLimitation of Liability\\nBecause this Model & Software License Agreement qualifies as a donation, according to Section 521 of the German\\nCivil Code (B\\xc3\\xbcrgerliches Gesetzbuch \\xe2\\x80\\x93 BGB) Licensor as a donor is liable for intent and gross negligence only.\\nIf the Licensor fraudulently conceals a legal or material defect, they are obliged to compensate the Licensee\\nfor the resulting damage.\\n\\nLicensor shall be liable for loss of data only up to the amount of typical recovery costs which would have\\narisen had proper and regular data backup measures been taken. For the avoidance of doubt Licensor shall be\\nliable in accordance with the German Product Liability Act in the event of product liability. The foregoing\\napplies also to Licensor\\xe2\\x80\\x99s legal representatives or assistants in performance. Any further liability shall \\nbe excluded. Patent claims generated through the usage of the Model & Software cannot be directed towards the copyright holders.\\nThe Model & Software is provided in the state of development the licensor defines. If modified or extended by\\nLicensee, the Licensor makes no claims about the fitness of the Model & Software and is not responsible\\nfor any problems such modifications cause.\\n\\nNo Maintenance Services\\nYou understand and agree that Licensor is under no obligation to provide either maintenance services,\\nupdate services, notices of latent defects, or corrections of defects with regard to the Model & Software.\\nLicensor nevertheless reserves the right to update, modify, or discontinue the Model & Software at any time.\\n\\nDefects of the Model & Software must be notified in writing to the Licensor with a comprehensible description\\nof the error symptoms. The notification of the defect should enable the reproduction of the error.\\nThe Licensee is encouraged to communicate any use, results, modification or publication.\\n\\nPublications using the Model & Software\\nYou acknowledge that the Model & Software is a valuable scientific resource and agree to appropriately reference\\nthe following paper in any publication making use of the Model & Software.\\n\\nCommercial licensing opportunities\\nFor commercial uses of the Model & Software, please send email to ps-license@tue.mpg.de\\n\\nThis Agreement shall be governed by the laws of the Federal Republic of Germany except for the UN Sales Convention.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DECA: Detailed Expression Capture and Animation (SIGGRAPH2021)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DECA",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "YadiraF",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/YadiraF/DECA/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3.7 (numpy, skimage, scipy, opencv)  \n* PyTorch >= 1.6 (pytorch3d)  \n* face-alignment (Optional for detecting face)  \n  You can run \n  ```bash\n  pip install -r requirements.txt\n  ```\n  Or use virtual environment by runing \n  ```bash\n  bash install_conda.sh\n  ```\n  For visualization, we use our rasterizer that uses pytorch JIT Compiling Extensions. If there occurs a compiling error, you can install [pytorch3d](https://github.com/facebookresearch/pytorch3d/blob/master/INSTALL.md) instead and set --rasterizer_type=pytorch3d when running the demos.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 849,
      "date": "Wed, 29 Dec 2021 05:01:57 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "3d",
      "face",
      "reconstruction",
      "alignment",
      "depth",
      "model",
      "flame"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone the repo:\n  ```bash\n  git clone https://github.com/YadiraF/DECA\n  cd DECA\n  ```  \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Prepare data   \n    a. download [FLAME model](https://flame.is.tue.mpg.de/download.php), choose **FLAME 2020** and unzip it, copy 'generic_model.pkl' into ./data  \n    b. download [DECA trained model](https://drive.google.com/file/d/1rp8kdyLPvErw2dTmqtjISRVvQLj6Yzje/view?usp=sharing), and put it in ./data (**no unzip required**)  \n    c. (Optional) follow the instructions for the [Albedo model](https://github.com/TimoBolkart/BFM_to_FLAME) to get 'FLAME_albedo_from_BFM.npz', put it into ./data\n\n2. Run demos  \n    a. **reconstruction**  \n    ```bash\n    python demos/demo_reconstruct.py -i TestSamples/examples --saveDepth True --saveObj True\n    ```   \n    to visualize the predicted 2D landmanks, 3D landmarks (red means non-visible points), coarse geometry, detailed geometry, and depth.   \n    <p align=\"center\">   \n    <img src=\"Doc/images/id04657-PPHljWCZ53c-000565_inputs_inputs_vis.jpg\">\n    </p>  \n    <p align=\"center\">   \n    <img src=\"Doc/images/IMG_0392_inputs_vis.jpg\">\n    </p>  \n    You can also generate an obj file (which can be opened with Meshlab) that includes extracted texture from the input image.  \n\n    Please run `python demos/demo_reconstruct.py --help` for more details. \n\n    b. **expression transfer**   \n    ```bash\n    python demos/demo_transfer.py\n    ```   \n    Given an image, you can reconstruct its 3D face, then animate it by tranfering expressions from other images. \n    Using Meshlab to open the detailed mesh obj file, you can see something like that:\n    <p align=\"center\"> \n    <img src=\"Doc/images/soubhik.gif\">\n    </p>  \n    (Thank Soubhik for allowing me to use his face ^_^)   \n    \n    Note that, you need to set '--useTex True' to get full texture.   \n\n    c. for the [teaser gif](https://github.com/YadiraF/DECA/results/teaser.gif) (**reposing** and **animation**)\n    ```bash\n    python demos/demo_teaser.py \n    ``` \n    \n    More demos and training code coming soon.\n\n",
      "technique": "Header extraction"
    }
  ]
}