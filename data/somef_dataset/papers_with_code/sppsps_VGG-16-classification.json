{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.1556 <br />\r\n - Author : Karen Simonyan, Andrew Zisserman <br />\r\n - Published : 10 Apr 2015 <br />\r\n\r\n# Summary\r\n\r\n## Introduction\r\nThis model is implementing the architecture and training techniques used in VGG paper. We used CIFAR10 dataset by keras.<br />The preprocessing :<br />\r\nA"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": " - Title : Very Deep Convolutional Networks for Large-Scale Image Recognition <br />\r\n - Link : https://arxiv.org/abs/1409.1556 <br />\r\n - Author : Karen Simonyan, Andrew Zisserman <br />\r\n - Published : 10 Apr 2015 <br />\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9156566588472104
      ],
      "excerpt": "conv2d (Conv2D)              (None, 32, 32, 64)        1792       \nconv2d_1 (Conv2D)            (None, 32, 32, 64)        36928      \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9076501442428265,
        0.9030859728368266
      ],
      "excerpt": "dense_3 (Dense)              (None, 10)                10010      \nactivation (Activation)      (None, 10)                0          \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sppsps/VGG-classification",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-17T03:48:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-10T09:36:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This model is implementing the architecture and training techniques used in VGG paper. We used CIFAR10 dataset by keras.<br />The preprocessing :<br />\r\nA) Dividing all the pixels of images by 225<br />\r\nB) Converting y_train(labels) into float type.<br />\r\nC) Using to_categorical() on training labels to use categorical loss as the loss function while training.<br />\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9666389040977295,
        0.8020426343371733,
        0.860059181823877
      ],
      "excerpt": "This model is made for 224x224x3 images, but in this model, it is implemented on 32x32x3 images, so the learable parameters changed from 134 million to 43 million. \nThe following are the output values of all the layers used:<br/> \nModel: \"sequential\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense (Dense)                (None, 4096)              2101248    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense_1 (Dense)              (None, 4096)              16781312   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense_2 (Dense)              (None, 1000)              4097000    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024502222522734,
        0.9023441246788243,
        0.9183073128864022
      ],
      "excerpt": "So in the end, we get a softmax layer with 10 units of output which will be used to train with the labels. \nCategorical crossentropy: It is a loss function that is used in multi-class classification tasks. These are tasks where an example can only belong to one out of many possible categories, and the model must decide which one. \nFormally, it is designed to quantify the difference between two probability distributions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Cats and some adorable dogs.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sppsps/VGG-16-classification/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 04:36:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sppsps/VGG-classification/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sppsps/VGG-classification",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.906614234580968
      ],
      "excerpt": "Layer (type)                 Output Shape              Param #: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "  <img src=\"assets/categoricalloss.png\" width=\"500\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "  <img src=\"assets/lossplot.png\" width=\"600\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sppsps/VGG-classification/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "TENSORFLOW IMPLEMENTATION OF VGG CLASSIFICATION",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "VGG-classification",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sppsps",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sppsps/VGG-classification/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 04:36:15 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\r\nUsage: ipykernel_launcher.py [-h] [--learning_rate LEARNING_RATE]\r\n                                [--dropout DROPOUT] [--regularizer REGULARIZER]\r\n                                [--epoch EPOCH] [--batch_size BATCH_SIZE]\r\n                                [--momentum MOMENTUM] [--patience PATIENCE]\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  --learning_rate LEARNING_RATE\r\n                        learning rate for SGD\r\n  --epoch EPOCH     max #: of epoch\r\n  --batch_size BATCH_SIZE\r\n                        #: of batch size\r\n  --momentum MOMENTUM     momentum for SGD\r\n  --dropout DROPOUT\r\n                        Dropout percentage for the layers\r\n  --regularization REGULARIZATION\r\n                        Regularization value for L2 \r\n```                        \r\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\r\npython training.py\r\n```\r\nRunning this script will both train the model, and show the metric and loss plot.\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}