{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "AdaHessian has been developed as part of the following paper. We appreciate it if you would please cite the following paper if you found the library useful for your work:\n\n```text\n@article{yao2020adahessian,\n  title={ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning},\n  author={Yao, Zhewei and Gholami, Amir and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W},\n  journal={AAAI (Accepted)},\n  year={2021}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yao2020adahessian,\n  title={ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning},\n  author={Yao, Zhewei and Gholami, Amir and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W},\n  journal={AAAI (Accepted)},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "Fast.ai Discussion | Link | --  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/amirgholami/adahessian/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amirgholami/adahessian",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nLicense\nBy contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq),\nyou agree that your contributions will be licensed under the LICENSE file in\nthe root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-24T19:23:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-02T16:24:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![Block](imgs/diagonal_illustration.png)\n\nAdaHessian is a second order based optimizer for the neural network training based on PyTorch. The library supports the training of convolutional neural networks ([image_classification](https://github.com/amirgholami/adahessian/tree/master/image_classification)) and transformer-based models ([transformer](https://github.com/amirgholami/adahessian/tree/master/transformer)). Our TensorFlow implementation is [adahessian_tf](https://github.com/amirgholami/adahessian/tree/master/adahessian_tf).\n\nPlease see [this paper](https://arxiv.org/pdf/2006.00719.pdf) for more details on the AdaHessian algorithm.\n\nFor more details please see:\n\n- [Video explanation of AdaHessian](https://www.youtube.com/watch?v=S87ancnZ0MM)\n- [AdaHessian paper](https://arxiv.org/pdf/2006.00719.pdf).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9980432636823942
      ],
      "excerpt": "Below is the convergence of AdaHessian on Rastrigin and Rosenbrock functions, and comparison with SGD and ADAM. Please see pytorch-optimizer repo for comparison with other optimizers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9651988113480291,
        0.9889408283553363,
        0.9084124241846043
      ],
      "excerpt": "We found out it would be helpful to add instruction about how to adopt AdaHessian for your own models and problems. Hence, we add a prototype version of AdaHessian as well as some useful comments in the instruction folder. \nWe are thankful to all the researchers who have extended AdaHessian for different purposes or analyzed it. We include the following links in case you are interested to learn more about AdaHessian. \nDescription | Link | New Features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amirgholami/adahessian/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 29,
      "date": "Tue, 21 Dec 2021 01:18:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amirgholami/adahessian/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "amirgholami/adahessian",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/image_classification/config/resnet32_cifar10/sgd.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/image_classification/config/resnet32_cifar10/adam.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/image_classification/config/resnet32_cifar10/adamw.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/image_classification/config/resnet32_cifar10/adahessian.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/image_classification/config/resnet20_cifar10/sgd.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/image_classification/config/resnet20_cifar10/adam.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/image_classification/config/resnet20_cifar10/adamw.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/image_classification/config/resnet20_cifar10/adahessian.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/transformer/scripts/compound_split_bleu.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/transformer/scripts/sacrebleu_pregen.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/transformer/config/adahessian_pretrained_model.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/transformer/config/adam.sh",
      "https://raw.githubusercontent.com/amirgholami/adahessian/master/transformer/config/adahessian.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you are interested to install the library through pip, then we recommend doing so through pytorch-optimizer package as follows:\n\n```\n$ pip install torch_optimizer\n```\n\n```python\nimport torch_optimizer as optim\n\n#: model = ...\noptimizer = optim.Adahessian(\n    m.parameters(),\n    lr= 1.0,\n    betas= (0.9, 0.999)\n    eps= 1e-4,\n    weight_decay=0.0,\n    hessian_power=1.0,\n)\n      loss_fn(m(input), target).backward(create_graph = True) #: create_graph=True is necessary for Hessian calculation\noptimizer.step()\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Please first clone the AdaHessian library to your local system:\n```\ngit clone https://github.com/amirgholami/adahessian.git\n```\nYou can import the optimizer as follows:\n\n```python\nfrom optim_adahessian import Adahessian\n...\nmodel = YourModel()\noptimizer = Adahessian(model.parameters())\n...\nfor input, output in data:\n  optimizer.zero_grad()\n  loss = loss_function(output, model(input))\n  loss.backward(create_graph=True)  #: You need this line for Hessian backprop\n  optimizer.step()\n...\n```\n\nPlease note that the optim_adahessian is in the image_classification folder. We also have adapted the Adahessian implementation to be compatible with fairseq repo, which can be used for NLP tasks. This is the [link](https://github.com/amirgholami/adahessian/blob/master/transformer/fairseq/optim/adahessian.py) to that version, which can be found in transformer folder.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amirgholami/adahessian/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Cython",
      "Shell",
      "Lua"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Facebook, Inc. and its affiliates.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "adahessian",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "amirgholami",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amirgholami/adahessian/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 150,
      "date": "Tue, 21 Dec 2021 01:18:26 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "second-order-optimization",
      "hessian",
      "hessian-free",
      "optimizer",
      "adahessian"
    ],
    "technique": "GitHub API"
  }
}