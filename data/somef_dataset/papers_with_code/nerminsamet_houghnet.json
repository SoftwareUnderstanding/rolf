{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This work was supported by the AWS Cloud Credits for Research program and by the Scientific and Technological Research Council of Turkey (TUBITAK) through the project titled \"Object Detection in Videos with Deep Neural Networks\" (grant number 117E054). The numerical calculations reported in this paper were partially performed at TUBITAK ULAKBIM,  High Performance and Grid Computing Center (TRUBA resources). We also thank the authors of [CenterNet](https://github.com/xingyizhou/CenterNet) for their clean code and inspiring work.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2007.02355",
      "https://arxiv.org/abs/2007.02355",
      "https://arxiv.org/abs/2104.06773",
      "https://arxiv.org/abs/2104.06773",
      "https://arxiv.org/abs/2001.00309",
      "https://arxiv.org/abs/2104.06773",
      "https://arxiv.org/abs/2104.06773"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find HoughNet useful for your research, please cite our paper as follows.\n\n> N. Samet, S. Hicsonmez, E. Akbas, \"HoughNet: Integrating near and long-range evidence for bottom-up object detection\",\n> In European Conference on Computer Vision (ECCV), 2020.\n\n> N. Samet, S. Hicsonmez, E. Akbas, \"HoughNet: Integrating near and long-range evidence for visual detection\",\n> arXiv, 2021.\n\nBibTeX entry:\n```\n@inproceedings{HoughNet,\n  author = {Nermin Samet and Samet Hicsonmez and Emre Akbas},\n  title = {HoughNet: Integrating near and long-range evidence for bottom-up object detection},\n  booktitle = {European Conference on Computer Vision (ECCV)},\n  year = {2020},\n}\n```\n```\n@misc{HoughNet2021,\n      title={HoughNet: Integrating near and long-range evidence for visual detection}, \n      author={Nermin Samet and Samet Hicsonmez and Emre Akbas},\n      year={2021}, \n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{HoughNet2021,\n      title={HoughNet: Integrating near and long-range evidence for visual detection}, \n      author={Nermin Samet and Samet Hicsonmez and Emre Akbas},\n      year={2021}, \n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{HoughNet,\n  author = {Nermin Samet and Samet Hicsonmez and Emre Akbas},\n  title = {HoughNet: Integrating near and long-range evidence for bottom-up object detection},\n  booktitle = {European Conference on Computer Vision (ECCV)},\n  year = {2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9507374082549614,
        0.9916956837314976
      ],
      "excerpt": "Nermin Samet, Samet Hicsonmez, Emre Akbas,       \nECCV 2020. (arXiv pre-print)     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9691731755746195,
        0.9507374082549614,
        0.9545298468015296,
        0.9796229304612593
      ],
      "excerpt": "HoughNet: Integrating near and long-range evidence for visual detection,           \nNermin Samet, Samet Hicsonmez, Emre Akbas,       \nUnder review at TPAMI. (arXiv pre-print) \n(April, 2021) We extended HoughNet with other visual detection tasks: video object detection, instance segmentation, keypoint detection and 3D object detection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017608798623265
      ],
      "excerpt": "learning model for generic object detection. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nerminsamet/houghnet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-06T11:19:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T14:22:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Object detection methods typically rely on only local evidence. For example, to detect the mouse in the image below,\nonly the features extracted at/around the mouse are used. In contrast, HoughNet is able to utilize long-range (i.e. far away) evidence, too.\nBelow, on the right, the votes that support the detection of the mouse are shown: in addition to the local evidence,\nfar away but semantically relevant objects, the two keyboards, vote for the mouse.\n\n<img src=\"/readme/teaser.png\" width=\"550\">\n\nHoughNet is a one-stage, anchor-free, voting-based, bottom-up object detection method. Inspired by the Generalized Hough Transform,\nHoughNet determines the presence of an object at a certain location by the sum of the\nvotes cast on that location. Votes are collected from both near and long-distance locations\nbased on a log-polar vote field. Thanks to this voting mechanism, HoughNet is able to integrate both near and long-range,\nclass-conditional evidence for visual recognition, thereby generalizing and enhancing current object detection methodology,\nwhich typically relies on only local evidence. On the COCO dataset, HoughNet achieves 46.4 AP (and 65.1 AP<sub>50</sub>),\nperforming on par with the state-of-the-art in bottom-up object detection and outperforming most  major one-stage and two-stage methods.\nWe further validate the effectiveness of HoughNet in another task, namely, \"labels to photo\" image generation by integrating the\nvoting module to two different GAN models and showing that the accuracy is significantly improved in both cases.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9628314740200209
      ],
      "excerpt": "Official PyTorch implementation of HoughNet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8261800253420801
      ],
      "excerpt": "Extended HoughNet with new tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8530248376006473,
        0.8589621454418351,
        0.926498474295575
      ],
      "excerpt": "Extended the voting idea to the temporal domain by developing a new video object detection method. Code is avaliable at HoughNet-VID repo. \nInspired from BlendMask, we extended HoughNet for instance segmentation. More details regarding training and network architecture are in the paper and supplementary material.  \nWe showed the effectivenes of HoughNet for keypoint detection and 3D object detection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.869885758155209
      ],
      "excerpt": "Hough voting idea is applied through a log-polar vote field to utilize short and long-range evidence in a deep \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9381554502587902
      ],
      "excerpt": "HoughNet is effective for small objects (+2.5 AP points over the baseline). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9714109575112955
      ],
      "excerpt": "  reducing the cost of ablation experiments. minitrain is strongly  positively correlated with the performance of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8345597378940992
      ],
      "excerpt": "A step-by-step animation of the voting process is provided here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[ECCV-20] Official PyTorch implementation of HoughNet, a voting-based object detector.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nerminsamet/houghnet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Mon, 27 Dec 2021 21:59:28 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nerminsamet/houghnet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "nerminsamet/houghnet",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/ctdet_coco_resdcn101.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/ctdet_coco_res101.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/ctdet_coco_hg104_scratch.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/ctseg_coco_resdcn101_light.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/ctdet_coco_hg104_cornernet.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/ctseg_coco_resdcn101_baseline.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/ctdet_coco_resdcn101_light.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/multi_pose_hp_coco_dla34_1x.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/ddd_sub.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/multi_pose_hm_coco_dla34_1x.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/multi_pose_hm_hp_coco_dla34_1x.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/experiments/ctdet_coco_hg104_extremenet.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/src/lib/models/networks/DCNv2/make.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/src/tools/get_pascal_voc.sh",
      "https://raw.githubusercontent.com/nerminsamet/houghnet/master/src/tools/get_kitti.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please refer to [INSTALL.md](readme/INSTALL.md) for installation instructions.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nerminsamet/houghnet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "Shell",
      "Cython",
      "C",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Nermin Samet\\nAll rights reserved.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "HoughNet: Integrating near and long-range evidence for bottom-up object detection",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "houghnet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "nerminsamet",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nerminsamet/houghnet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 150,
      "date": "Mon, 27 Dec 2021 21:59:28 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "object-detection",
      "deep-learning",
      "pytorch",
      "voting",
      "voting-classifier",
      "bottom-up-model",
      "hough-transform",
      "hough",
      "hough-transformation",
      "instance-segmentation",
      "video-object-detection",
      "video-object-tracking",
      "3d-object-detection",
      "human-pose-estimation",
      "2d-pose-estimation",
      "pose-estimation",
      "kitti-dataset",
      "coco-dataset",
      "eccv",
      "eccv-2020"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "| Model                    |   AP / AP50        |   Box AP / AP50    |\n|--------------------------|--------------------|--------------------|\n|Baseline | 27.2 / 46.4  | 33.9 / 51.3 |\n|HoughNet | 28.4 / 48.0  | 35.0 / 52.9 |\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}