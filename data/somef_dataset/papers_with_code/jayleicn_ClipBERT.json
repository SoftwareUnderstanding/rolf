{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We thank [Yen-Chun Chen](https://scholar.google.com/citations?user=Gptgy4YAAAAJ&hl=en), \n[Ruotian Luo](https://ttic.uchicago.edu/~rluo/), and other members and interns at \n[Microsoft Multimodal AI](https://multimodalai.azurewebsites.net/people/members) \nfor their helpful discussions.\nWe also thank the anonymous reviewers for their constructive feedback.\n\nThis code used resources from [transformers](https://github.com/huggingface/transformers), \n[UNITER](https://github.com/ChenRocks/UNITER), [HERO](https://github.com/linjieli222/HERO), \n[grid-feats-vqa](https://github.com/facebookresearch/grid-feats-vqa), \n[SlowFast](https://github.com/facebookresearch/SlowFast), \n[Detectron2](https://github.com/facebookresearch/detectron2). \nThe code is implemented using [PyTorch](https://github.com/pytorch/pytorch), \nwith multi-GPU support from [Horovod](https://github.com/horovod/horovod) \nand mixed precision support from [apex](https://github.com/NVIDIA/apex).  We thank the authors for open-sourcing their awesome projects.\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2102.06183"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this code useful for your research, please consider citing:\n```\n@inproceedings{lei2021less,\n  title={Less is More: ClipBERT for Video-and-Language Learningvia Sparse Sampling},\n  author={Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L. and Bansal, Mohit and Liu, Jingjing},\n  booktitle={CVPR},\n  year={2021}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{lei2021less,\n  title={Less is More: ClipBERT for Video-and-Language Learningvia Sparse Sampling},\n  author={Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L. and Bansal, Mohit and Liu, Jingjing},\n  booktitle={CVPR},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9717124805508957,
        0.9384552127166319
      ],
      "excerpt": "CVPR 2021, Oral, Best Student Paper Honorable Mention. \nJie Lei*, Linjie Li*, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "Jingjing Liu \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9803976005988218
      ],
      "excerpt": "end-to-end learning for image-text and video-text tasks.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9588370481837368
      ],
      "excerpt": "to enable efficient end-to-end video-and-language learning. In this repository,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9491086698325211
      ],
      "excerpt": "Video-QA finetuning on TGIF-QA and MSRVTT-QA. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9332767710678362
      ],
      "excerpt": "    mkdir -p $PATH_TO_STORAGE/vis_db  #: image and video  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "    --file_type video  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jayleicn/ClipBERT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-10T20:48:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T07:54:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9354760107083896
      ],
      "excerpt": "Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938381310578386,
        0.9116731033542783
      ],
      "excerpt": "Official PyTorch code for ClipBERT, an efficient framework for  \nend-to-end learning for image-text and video-text tasks.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9419910306633443,
        0.9679578405045155,
        0.9495009893502111
      ],
      "excerpt": "ClipBERT is designed based on 2D CNNs and transformers, and uses a sparse sampling strategy  \nto enable efficient end-to-end video-and-language learning. In this repository,  \nwe support end-to-end pretraining and finetuning for the following tasks: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.837487942774553
      ],
      "excerpt": "Text-to-video retrieval finetuning on MSRVTT, DiDeMo, and ActivityNet Captions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9843069404298873,
        0.9234192753855013
      ],
      "excerpt": "It is also feasible and easy to add other image-text or video-text tasks for pretraining and finetuning. \nCreate a folder that stores pretrained models, all the data, and results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9215204482011689
      ],
      "excerpt": "model weights, which are used as initialization for pretraining.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774252512289115,
        0.9483114904308354,
        0.8809221593613773
      ],
      "excerpt": "    Note that the source code is mounted into the container under /clipbert instead  \n    of built into the image so that user modification will be reflected without \n    re-building the image. (Data folders are mounted into the container separately \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "for MSRVTT retrieval. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8564361141004853
      ],
      "excerpt": "   $OUTPUT_DIR/ckpt/model_step_$STEP.pt for inference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.932300922578975
      ],
      "excerpt": "    for inference, such as 1 or 16. Using more clips will have a large impact  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9099084771158424
      ],
      "excerpt": "for the MSRVTT MC Test task as well, which is essentially a retrieval  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "    for MSRVTT-QA. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8564361141004853
      ],
      "excerpt": "   $OUTPUT_DIR/ckpt/model_step_$STEP.pt for inference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.932300922578975
      ],
      "excerpt": "for inference, such as 1 or 16. Using more clips will have a large impact  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[CVPR 2021 Best Student Paper Honorable Mention, Oral] Official PyTorch code for ClipBERT, an efficient framework for end-to-end learning on image-text and video-text tasks. ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jayleicn/ClipBERT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 55,
      "date": "Wed, 29 Dec 2021 02:45:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jayleicn/ClipBERT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jayleicn/ClipBERT",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jayleicn/ClipBERT/main/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jayleicn/ClipBERT/main/setup.sh",
      "https://raw.githubusercontent.com/jayleicn/ClipBERT/main/launch_container.sh",
      "https://raw.githubusercontent.com/jayleicn/ClipBERT/main/scripts/download_anet.sh",
      "https://raw.githubusercontent.com/jayleicn/ClipBERT/main/scripts/download_msrvtt.sh",
      "https://raw.githubusercontent.com/jayleicn/ClipBERT/main/scripts/download_vqa.sh",
      "https://raw.githubusercontent.com/jayleicn/ClipBERT/main/scripts/download_coco_vg.sh",
      "https://raw.githubusercontent.com/jayleicn/ClipBERT/main/scripts/download_tgif_qa.sh",
      "https://raw.githubusercontent.com/jayleicn/ClipBERT/main/scripts/download_pretrained.sh",
      "https://raw.githubusercontent.com/jayleicn/ClipBERT/main/scripts/download_didemo.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8329798563356866
      ],
      "excerpt": "    mkdir -p $PATH_TO_STORAGE/txt_db  #: annotations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8919958595477101,
        0.818033245669587,
        0.8027759438359542,
        0.9414119019616536
      ],
      "excerpt": "    mkdir -p $PATH_TO_STORAGE/pretrained  #: pretrained models \nDownload pretrained models. \nOur e2e pretrained ClipBERT model (849MB), can be downloaded with the following command. \nbash scripts/download_pretrained.sh $PATH_TO_STORAGE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9654765604684976
      ],
      "excerpt": "    source launch_container.sh $PATH_TO_STORAGE/txt_db $PATH_TO_STORAGE/vis_db \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230982898658457
      ],
      "excerpt": "    Note that the source code is mounted into the container under /clipbert instead  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414119019616536,
        0.8158061032801732
      ],
      "excerpt": "    bash scripts/download_$DSET.sh $PATH_TO_STORAGE \n    $DSET can be one of msrvtt, didemo, anet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    ```bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109626495628542
      ],
      "excerpt": ": for single GPU \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414119019616536
      ],
      "excerpt": "    bash scripts/download_msrvtt.sh $PATH_TO_STORAGE   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414119019616536
      ],
      "excerpt": "    bash scripts/download_tgif_qa.sh $PATH_TO_STORAGE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414119019616536
      ],
      "excerpt": "    bash scripts/download_coco_vg.sh $PATH_TO_STORAGE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414119019616536
      ],
      "excerpt": "    bash scripts/download_vqa.sh $PATH_TO_STORAGE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414119019616536
      ],
      "excerpt": "    bash scripts/download_coco_vg.sh $PATH_TO_STORAGE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8331008273004277
      ],
      "excerpt": "    --lmdb_save_dir /path/to/save/lmdb \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.819338745906253
      ],
      "excerpt": "Create a folder that stores pretrained models, all the data, and results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8262623383225983,
        0.8778534729040356
      ],
      "excerpt": "    mkdir -p $PATH_TO_STORAGE/pretrained  #: pretrained models \nDownload pretrained models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483334307822219
      ],
      "excerpt": "Launch the Docker container for running the experiments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8634339304689512
      ],
      "excerpt": "Download data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589344271683286
      ],
      "excerpt": "    horovodrun -np 4 python src/tasks/run_video_retrieval.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.924710159749085
      ],
      "excerpt": "python src/tasks/run_video_retrieval.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807357760755256
      ],
      "excerpt": "``$CONFIG_PATHshould be set to one of the .json config files available at [src/configs](src/configs)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589344271683286
      ],
      "excerpt": "    horovodrun -np 4 python src/tasks/run_video_retrieval.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589344271683286
      ],
      "excerpt": "horovodrun -np 4 python src/tasks/run_msrvtt_mc.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8634339304689512
      ],
      "excerpt": "Download data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589344271683286
      ],
      "excerpt": "    horovodrun -np 4 python src/tasks/run_video_qa.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633201826919991,
        0.824702717809317
      ],
      "excerpt": "    $CONFIG_PATH should be set to one of the .json config files available at src/configs  \n    contains the substring _qa. For example, you can use src/configs/msrvtt_qa_base_resnet50.json  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589344271683286
      ],
      "excerpt": "    horovodrun -np 4 python src/tasks/run_video_qa.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8634339304689512
      ],
      "excerpt": "Download data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589344271683286,
        0.8881242957353287
      ],
      "excerpt": "    horovodrun -np 4 python src/tasks/run_vqa.py \\ \n        --config src/configs/vqa_base_resnet50.json \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589344271683286
      ],
      "excerpt": "    horovodrun -np 4 python src/tasks/run_vqa.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8634339304689512
      ],
      "excerpt": "Download data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9597200832066214,
        0.8881242957353287
      ],
      "excerpt": "    horovodrun -np 8 python src/pretrain/run_pretrain.py \\ \n        --config src/configs/pretrain_image_text_base_resnet50_mlm_itm.json \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9130626349494089
      ],
      "excerpt": "python src/preprocessing/file2lmdb.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8071891105508943
      ],
      "excerpt": "Text annotation files are reorganized into jsonl files,  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jayleicn/ClipBERT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Jie Lei\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ClipBERT",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ClipBERT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jayleicn",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jayleicn/ClipBERT/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide a Docker image for easier reproduction. Please install the following:\n  - [nvidia driver](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-installation) (418+), \n  - [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/) (19.03+), \n  - [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-docker#quickstart).\n\nOur scripts require the user to have the [docker group membership](https://docs.docker.com/install/linux/linux-postinstall/)\nso that docker commands can be run without sudo.\nWe only support Linux with NVIDIA GPUs. We test on Ubuntu 18.04 and V100 cards.\nWe use mixed-precision training hence GPUs with Tensor Cores are recommended.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 465,
      "date": "Wed, 29 Dec 2021 02:45:44 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "video-retrieval",
      "video-question-answering",
      "vqa",
      "vision-and-language",
      "cvpr2021"
    ],
    "technique": "GitHub API"
  }
}