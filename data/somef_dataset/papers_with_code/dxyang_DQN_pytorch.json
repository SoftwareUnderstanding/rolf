{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.06461",
      "https://arxiv.org/abs/1511.06581",
      "https://arxiv.org/abs/1509.06461",
      "https://arxiv.org/abs/1511.06581"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dxyang/DQN_pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-10-10T00:33:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T11:37:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo is a [PyTorch](https://www.pytorch.org/) implementation of Vanilla DQN, Double DQN, and Dueling DQN based off these papers.\n\n- [Human-level control through deep reinforcement learning](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)\n- [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)\n- [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)\n\nStarter code is used from [Berkeley CS 294 Assignment 3](https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3) and modified for PyTorch with some guidance from [here](https://github.com/transedward/pytorch-dqn). Tensorboard logging has also been added (thanks [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/04-utils/tensorboard) for visualization during training in addition to what the Gym Monitor already does).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9757182332044333
      ],
      "excerpt": "Human-level control through deep reinforcement learning introduced using a experience replay buffer that stores past observations and uses them as training input to reduce correlations between data samples. They also used a separate target network consisting of weights at a past time step for calculating the target Q value. These weights are periodically updated to match the updated, latest set of weights on the main Q network. This reduces the correlation between the target and current Q values. Q target is calculated as below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9837735037642332
      ],
      "excerpt": "Noting that vanilla DQN can overestimate action values, Deep Reinforcement Learning with Double Q-learning proposes an alternative Q target value that takes the argmax of the current Q network when inputted with the next observations. These actions, together with the next observations, are passed into the frozen target network to yield Q values at each update. This new Q target is shown below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9617876336992834
      ],
      "excerpt": "Finally, Dueling Network Architectures for Deep Reinforcement Learning proposes a different architecture for approximating Q functions. After the last convolutional layer, the output is split into two streams that separately estimate the state-value and advantages for each action within the state. These two estimations are then combined together to generate a Q value through the equation below. The architecture is also shown here in contrast to traditional Deep Q-Learning networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Vanilla DQN, Double DQN, and Dueling DQN implemented in PyTorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dxyang/DQN_pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 44,
      "date": "Fri, 24 Dec 2021 16:41:28 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dxyang/DQN_pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dxyang/DQN_pytorch",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.897953614168866
      ],
      "excerpt": "    <img src=\"assets/nature_dqn_model.png\" height=\"300px\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897953614168866
      ],
      "excerpt": "    <img src=\"assets/nature_dqn_target.png\" height=\"100px\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897953614168866
      ],
      "excerpt": "    <img src=\"assets/double_q_target.png\" height=\"70px\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897953614168866,
        0.897953614168866
      ],
      "excerpt": "    <img src=\"assets/dueling_q_target.png\" height=\"150px\"> \n    <img src=\"assets/dueling_q_arch.png\" height=\"300px\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample gameplay \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924574507284144
      ],
      "excerpt": "    <img src=\"assets/spaceinvaders.gif\" height=\"400\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample gameplay \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924574507284144
      ],
      "excerpt": "    <img src=\"assets/pong.gif\" height=\"400\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003751159116907
      ],
      "excerpt": "Sample gameplay \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924574507284144
      ],
      "excerpt": "    <img src=\"assets/breakout.gif\" height=\"400\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dxyang/DQN_pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Vanilla DQN, Double DQN, and Dueling DQN in PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DQN_pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dxyang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dxyang/DQN_pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 2.7\n- [PyTorch 0.2.0](http://pytorch.org/)\n- [NumPy](http://www.numpy.org/)\n- [OpenAI Gym](https://github.com/openai/gym)\n- [OpenCV 3.3.0](https://pypi.python.org/pypi/opencv-python)\n- [Tensorboard](https://github.com/tensorflow/tensorboard)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 198,
      "date": "Fri, 24 Dec 2021 16:41:28 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Execute the following command to train a model on vanilla DQN:\n\n```\n$ python main.py train --task-id $TASK_ID\n```\n\nFrom the Atari40M spec, here are the different environments you can use:\n* `0`: BeamRider\n* `1`: Breakout\n* `2`: Enduro\n* `3`: Pong\n* `4`: Qbert\n* `5`: Seaquest\n* `6`: Spaceinvaders\n\nHere are some options that you can use:\n* `--gpu`: id of the GPU you want to use (if not specified, will train on CPU)\n* `--double-dqn`: 1 to train with double DQN, 0 for vanilla DQN \n* `--dueling-dqn`: 1 to train with dueling DQN, 0 for vanilla DQN\n\n",
      "technique": "Header extraction"
    }
  ]
}