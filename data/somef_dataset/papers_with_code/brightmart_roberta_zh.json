{
  "citation": [
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "\u672c\u9879\u76ee\u4e0e\u4e2d\u6587\u9884\u8bad\u7ec324\u5c42XLNet\u6a21\u578b <a href=\"https://github.com/brightmart/xlnet_zh\">XLNet_zh</a>\u9879\u76ee\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u8bad\u7ec3\u6570\u636e\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9843157504138738
      ],
      "excerpt": "\u6ce8\uff1a\u6570\u636e\u6765\u6e90\u4e8e<a href=\"https://github.com/guoday/CCF-BDCI-Sentiment-Analysis-Baseline/blob/master/README.md\">guoday\u7684\u5f00\u6e90\u9879\u76ee</a>\uff1b\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4ecb\u7ecd\u89c1\uff1a<a href=\"https://www.datafountain.cn/competitions/350/ranking\">CCF\u4e92\u8054\u7f51\u65b0\u95fb\u60c5\u611f\u5206\u6790</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9799883766133161
      ],
      "excerpt": "BERT-wwm-ext\u6765\u81ea\u4e8e<a href=\"https://github.com/ymcui/Chinese-BERT-wwm\">\u8fd9\u91cc</a>\uff1bXLNet\u6765\u81ea\u4e8e<a href=\"https://github.com/ymcui/Chinese-PreTrained-XLNet\">\u8fd9\u91cc</a>; RoBERTa-zh-base\uff0c\u630712\u5c42RoBERTa\u4e2d\u6587\u6a21\u578b \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "\u9605\u8bfb\u7406\u89e3\u6d4b\u8bd5\u5bf9\u6bd4\u6570\u636e\u6765\u6e90<a href=\"https://github.com/ewrfcas/bert_cn_finetune\">bert_cn_finetune</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302556419090275
      ],
      "excerpt": "\u4e0b\u8f7d<a href=\"https://drive.google.com/open?id=1HXYMqsXjmA5uIfu_SFqP7r_vZZG-m_H0\">LCQMC</a>\u6570\u636e\u96c6\uff0c\u5305\u542b\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\uff0c\u8bad\u7ec3\u96c6\u5305\u542b24\u4e07\u53e3\u8bed\u5316\u63cf\u8ff0\u7684\u4e2d\u6587\u53e5\u5b50\u5bf9\uff0c\u6807\u7b7e\u4e3a1\u62160\u30021\u4e3a\u53e5\u5b50\u8bed\u4e49\u76f8\u4f3c\uff0c0\u4e3a\u8bed\u4e49\u4e0d\u76f8\u4f3c\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8656070203791273
      ],
      "excerpt": "PyTorch\u52a0\u8f7d\u65b9\u5f0f\uff0c\u5148\u53c2\u8003<a href=\"https://github.com/brightmart/roberta_zh/issues/9\">issue 9</a>\uff1b\u5c06\u5f88\u5feb\u63d0\u4f9b\u66f4\u5177\u4f53\u65b9\u5f0f\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "\u901a\u7528\u8bed\u6599\u6570\u636e\u89c1<a href=\"https://github.com/brightmart/nlp_chinese_corpus\">nlp_chinese_corpus</a>:\u5305\u542b\u591a\u4e2a\u62e5\u6709\u6570\u5343\u4e07\u53e5\u5b50\u7684\u8bed\u6599\u7684\u6570\u636e\u96c6\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "nohup bash create_pretrain_data.sh 1 10 &amp; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "...          | 384        | 12 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "<a href=\"https://github.com/skyhawk1990\"> skyhawk1990</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977994744046882,
        0.9452861872232974,
        0.9936884742717579
      ],
      "excerpt": "1\u3001<a href=\"https://arxiv.org/pdf/1907.11692.pdf\">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> \n2\u3001<a href=\"https://arxiv.org/pdf/1906.08101.pdf\">Pre-Training with Whole Word Masking for Chinese BERT</a> \n3\u3001<a href=\"https://arxiv.org/pdf/1810.04805.pdf\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/brightmart/roberta_zh",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-02T17:11:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T07:05:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8656499619292638,
        0.9044053032219079,
        0.9886760533775554,
        0.9359752413616758
      ],
      "excerpt": "What is RoBERTa: \nA robustly optimized method for pretraining natural language processing (NLP) systems that improves on Bidirectional Encoder Representations from Transformers, or BERT, the self-supervised method released by Google in 2018. \nRoBERTa, produces state-of-the-art results on the widely used NLP benchmark, General Language Understanding Evaluation (GLUE). The model delivered state-of-the-art performance on the MNLI, QNLI, RTE, STS-B, and RACE tasks and a sizable performance improvement on the GLUE benchmark. With a score of 88.5, RoBERTa reached the top position on the GLUE leaderboard, matching the performance of the previous leader, XLNet-Large. \n(Introduction from Facebook blog) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9340447966860281
      ],
      "excerpt": "You can also send pull request to report you performance on your task or add methods on how to load models for PyTorch and so on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "RoBERTa\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b: RoBERTa for Chinese ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/brightmart/roberta_zh/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 334,
      "date": "Tue, 28 Dec 2021 23:20:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/brightmart/roberta_zh/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "brightmart/roberta_zh",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/brightmart/roberta_zh/master/create_pretrain_data.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8073299048681466
      ],
      "excerpt": "\u53d1\u5e03\u8ba1\u5212 Release Plan\uff1a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717106327039013
      ],
      "excerpt": "RoBERTa\u4e2d\u6587\u7248 Chinese Version \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879863063452118
      ],
      "excerpt": "1\u3001\u590d\u5236\u672c\u9879\u76ee\uff1a git clone https://github.com/brightmart/roberta_zh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8001948021378019
      ],
      "excerpt": "PyTorch\u52a0\u8f7d\u65b9\u5f0f\uff0c\u5148\u53c2\u8003<a href=\"https://github.com/brightmart/roberta_zh/issues/9\">issue 9</a>\uff1b\u5c06\u5f88\u5feb\u63d0\u4f9b\u66f4\u5177\u4f53\u65b9\u5f0f\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117662062985821
      ],
      "excerpt": "nohup bash create_pretrain_data.sh 1 10 &amp; \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8131458369274005
      ],
      "excerpt": "1\u3001\u6570\u636e\u751f\u6210\u65b9\u5f0f\u548c\u4efb\u52a1\u6539\u8fdb\uff1a\u53d6\u6d88\u4e0b\u4e00\u4e2a\u53e5\u5b50\u9884\u6d4b\uff0c\u5e76\u4e14\u6570\u636e\u8fde\u7eed\u4ece\u4e00\u4e2a\u6587\u6863\u4e2d\u83b7\u5f97(\u89c1\uff1aModel Input Format and Next Sentence Prediction\uff0cDOC-SENTENCES) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_classifier.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8594142235991984
      ],
      "excerpt": "  --do_train=true \\ \n  --do_eval=true \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9381336739640681,
        0.8954225961930997
      ],
      "excerpt": "nohup python3 run_pretraining.py --input_file=./tf_records_all/tf*.tfrecord  \\ \n--output_dir=my_new_model_path --do_train=True --do_eval=True --bert_config_file=$BERT_BASE_DIR/bert_config.json \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/brightmart/roberta_zh/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "##### ** \u63a8\u8350 RoBERTa-zh-Large \u901a\u8fc7\u9a8c\u8bc1**",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "roberta_zh",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "brightmart",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/brightmart/roberta_zh/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1966,
      "date": "Tue, 28 Dec 2021 23:20:04 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "roberta",
      "chinese",
      "bert",
      "pre-trained-language-models",
      "pre-trained",
      "gpt2"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\u5f53\u524d\u672c\u9879\u76ee\u662f\u4f7f\u7528sequence length\u4e3a256\u8bad\u7ec3\u7684\uff0c\u6240\u4ee5\u53ef\u80fd\u5bf9\u957f\u5ea6\u5728\u8fd9\u4e2a\u8303\u56f4\u5185\u7684\u6548\u679c\u4e0d\u9519\uff1b\u5982\u679c\u4f60\u7684\u4efb\u52a1\u7684\u8f93\u5165\u6bd4\u8f83\u957f\uff08\u5982\u5e8f\u5217\u957f\u5ea6\u4e3a512\uff09\uff0c\u6216\u8bb8\u6548\u679c\u6709\u5f71\u54cd\u3002\n\n\u6709\u540c\u5b66\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u7684\u5f62\u5f0f\uff0c\u5c06\u5e8f\u5217\u505a\u62c6\u5206\uff0c\u8fd8\u662f\u5f97\u5230\u4e86\u6bd4\u8f83\u597d\u7684\u6548\u679c\uff0c\u89c1<a href=\"https://github.com/brightmart/roberta_zh/issues/16\">#issue-16</a>\n\n",
      "technique": "Header extraction"
    }
  ]
}