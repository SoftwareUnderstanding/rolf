{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/krocki/np-transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-23T23:30:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-10T06:35:55Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Positional encoding is important (it's quite obvious) to make it work\n- It seems to be more robust than other approaches I tried before\n- Surprisingly it works with one head and with only one layer, also without having the residual connection and normalization\n- Vanilla SGD did not work that well\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9637307625042926
      ],
      "excerpt": "This in an implementation of a single layer of the Transformer module as described in the paper 'Attention is all you need' (Vaswani et al.) [arxiv link][1]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8845953418966838,
        0.9640940407145375,
        0.9520969988163203,
        0.8613480900011514
      ],
      "excerpt": "My main objective was to determine how the self-attention mechanism deals with some toy algorithmic problems and develop intuition on which tasks can or cannot be solved easily. \nThis version is NOT optimized for performance. Some features are different and some are TODO. \nSome time ago I was very interested in memory-augmented architectures and their theoretical power when it comes to solving some simple problems requiring manipulating some external storage (Neural Turing Machine). Similarly to a CPU, we could focus on a given memory location based on its address or its content. \nDifferentiable Neural Computer (DNC) was another incarnation of this idea, well described in the Nature paper[2]. Here's my toy numpy code which tests DNC on a character-level prediction task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8591794828361475,
        0.8593147348441036
      ],
      "excerpt": "Those architectures where primarily developed with a hope that they could solve problems in an algorithmic way, just like a computer. \nI am very interested in this concept and always try to understand if a given problem can be solved in a machine-learning way of seeing input-output examples. I often perform mental experiments in order to determine the number of registers, memory locations, addressing, etc to have a better understanding of what can be learned. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9846338441382455,
        0.8189475784854345,
        0.953188920826105
      ],
      "excerpt": "A program then for something like 'copy' would ignore the content of memory cells, but instead move something from one location to another while advancing the pointer [i]. It means that it should be fairly simple to learn such a program, given that the can focus on a location effectively. One major disadvantage of the 1st generation of the algorithm-learning architectures such as NTM or DNC was the fact that the process was sequential in nature and constrained by the recurrent controller. This simple task was actually not that easy since i has to be shifted by some precise disrete amount in every iteration. The process also takes N steps. \nOne concept which actually worked quite well was the content-based addressing mechanism (key-value dot product). It is quite useful if we need to process data based on some content.  \nLet's define a task of filtering an array based on a value (let's say we want only even numbers), then the network needs to learn that the values are important as well as their positions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8290609006326831,
        0.9926382116557747,
        0.9567342124694678,
        0.8596710177597771
      ],
      "excerpt": "We can see that we need to 'query' if the value is odd, which in turn can be learned by a network by forming an odd/even mask and the dot product of that mask and a value will indicate if NULL or the value should be written. \nThe core of the Tranformer module is the dot-product attention which allows us to focus with respect to some value. Here's the crucial part which makes the Transformer different than the previous approaches - We encode the information about position directly into the input stream and therefore we can process the entire input sequence in parallel instead of sequentially. \nLet's go back to the copy example. Since all 'i' memory locations are independent, we can copy the data in a single step, assuming no additional constraints. In a way it's a hack, since we mark every item with a tag and later we effectively use content-based addressing to determine the location. It works well in practice. \nHere are some experiments I tried and they give some insight into how things are learned. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9946306426976059,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "The input to the attention module is xs of size N x M, where M is the input(sequence) and N is the attention module dimension. xs is fixed, but vs, ks and qs which as arrays for V, K and Q respectively are determined by learnable parameters Wxv, Wxk and Wxq. \nvs = np.dot(model['Wxv'].T, xs)                                                                             \nks = np.dot(model['Wxk'].T, xs) \nqs = np.dot(model['Wxq'].T, xs) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9570398593838296,
        0.9849642634016929
      ],
      "excerpt": "What we can see in the images above - input looks the same as the output - that's good, we have copied the sequence. In my implementation, I follow the original paper and compute the attention values (size N x N, att_sm in the pdf). Then attention * vs gives the output of the module zs. The decoder in my case takes zs and produces ys = dot(Wzy.T, zs), so it's as simple as possible. Then it's normalized by softmax and the loss is based on the cross-entropy. \nBy observing the input-key weights, we can see that indeed the attention module focuses on the location and ignores the content (upper part of the input vector is the original content, lower part is the positional encoding). vs weights on the other hand, learn to retrieve the content and ignore the location information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.94217909349393
      ],
      "excerpt": "This task is the same as copy, but write 'invalid' (value 0) when the value is above some threshold. We can see that both the location and the content are relevant. In the images below, the three initial values are greater than the threshold, we see that it has been correctly filtered and zeros appear on the output. The other 2 values are copied without a change. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8442698673995843
      ],
      "excerpt": "We can see the clear pattern in weights which mark a mask for the values which are not desired. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723450095624361
      ],
      "excerpt": "Similar to the copy case, the content is not relevant, which is good from the generalization perpective. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/krocki/np-transformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 23:29:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/krocki/np-transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "krocki/np-transformer",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8731562459058029,
        0.833758656594279
      ],
      "excerpt": "  value = mem[src_base+i]; \n  mem[dst_base+i] = value is odd ? NULL : value; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882747530828131
      ],
      "excerpt": "<img src=./imgs/attention.png width=500/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265467535009607,
        0.8275663643537139,
        0.8275663643537139
      ],
      "excerpt": "vs = np.dot(model['Wxv'].T, xs)                                                                             \nks = np.dot(model['Wxk'].T, xs) \nqs = np.dot(model['Wxq'].T, xs) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8998118325065148
      ],
      "excerpt": "<img src=./imgs/pe.png width=400/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8901855885754546
      ],
      "excerpt": "python3 transformer.py -t copy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.898813341600578
      ],
      "excerpt": "<img src=./imgs/copy.png width=500/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882747530828131
      ],
      "excerpt": "<img src=./imgs/filter0.png width=300/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882747530828131,
        0.9336801098518991
      ],
      "excerpt": "<img src=./imgs/filter1.png width=600/> \npython3 transformer.py -t rotate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882747530828131
      ],
      "excerpt": "<img src=./imgs/rotate0.png width=300/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882747530828131,
        0.9336801098518991
      ],
      "excerpt": "<img src=./imgs/rotate1.png width=600/> \npython3 transformer.py -t reverse \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/krocki/np-transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "np-transformer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "np-transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "krocki",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/krocki/np-transformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Wed, 22 Dec 2021 23:29:29 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n  python3 transformer.py -t {copy, reverse, rotate, filter} [-l saved_model] [-S sequence length]\n```\n\nExample:\n```\n  python3 transformer.py -t reverse\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}