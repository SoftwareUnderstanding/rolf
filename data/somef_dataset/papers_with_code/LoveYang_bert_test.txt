# Bert_task_test
*tensorflow1.12*

本项目目的在于测试bert在中文的使用效果

bert原生的中文分词是按字切分，同时对一些字符组合用sub-token切分（BPE）

原生的优化器不支持多GPU计算，借鉴https://github.com/JayYip/bert-multitask-learning研究了一下多gpu的实现。


这个测试项目包括两块： 分词和情感标签序列生成

## 分词
- bert_token_test.py
在测试集的效果准确度达到98.6%，计算方式是按每个序列独立标注的对错。
会存在和RNN一样明显错误标签序列，如SS，可能加了CRF效果能更好一些。
训练速度上和双层784的lstm速度差不多，略慢一点，可能是由于bert的参数量实在太大了

## 情感标签生成
数据集是ai-challenger里的判断细粒度情感标签的任务的数据集，被我拿来做相应的情感文本生成了。
数据是点评商铺的评论数据，分了20个类，每个类4中感情级别（没提及，消极，积极，中性）
，这里对比借鉴同义词替换的思路《Conditional BERT Contextual Augmentation》

数据预处理，对每一条文本随机mask相应的词作为训练数据

bert-encoder+lstm-decoder,lstm-encoder+lstm-decoder测试对比：
- bert-encoder收敛更快，不过参数远多于lstm，训练跟更慢。
bert训练了5个epoch（3天） lstm训练了15个（4天），在数据集上准确度差不多都在80%。
生成的可替换词基本属于正常词，但是和原词含义相比，逻辑性还是不高（有的比较合适，有的很怪异），算是生产模型的通病。
- 基于bert+lstm模型的文本数据增强对比lstm+lstm：均加入attention结构，增加情感标签输入：
lstm+lstm：2385914 step（4天），训练loss在3-4之间（已经有过拟合情况，loss开始上升），测试效果和无attention差别不明显，情感标签可以改变候选词的输出（可以影响部分候选词概率排序结果）
bert+lstm：700014 step(3天)，训练loss在1-2之间（无过拟合迹象，loss还在下降过程中）。
到了90w step的模型开始过拟合（训练数据loss还在下降而验证集已经开始上升），准确度也在80%
- 不同类别的不同级别情感标签对模型生成有一定影响，但是比较小（部分标签积极和消极的候选词得分排序不同，但消极和积极对比不明显），只有比较少的情况
有明显效果，可能是 
   - 标签类别过多 
   - 同时生成的数据噪声比较大，不能准确的判断标签和词的关系
最后对比了不用bert初始化的效果（120wstep），bert初始的模型已经过拟合且准确度 训练集/测试集 (0.8/0.79) ,而随机初始化的 准确度 (0.496/0.489)且已经趋近稳定但未过拟合
   
## 总结
- bert 初始化有较大提升
- bert 预训练的模型对加速模型训练有效（在同等参数下）
- bert 预训练的模型增加了模型容量
- bert 预训练的模型对最终结果不一定有影响（也可能看数据集）
- transformer的速度确实快，而且可以堆叠多层增加模型容量应对复杂模型和大量数据，这点有明显优势
