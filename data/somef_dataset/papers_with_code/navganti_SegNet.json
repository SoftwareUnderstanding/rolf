{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.02680, 2015.\n\nhttp://arxiv.org/abs/1511.00561\nVijay Badrinarayanan, Alex Kendall and Roberto Cipolla \"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.\" PAMI, 2017.\n\n## License\n\nThis software is released under a creative commons license which allows for personal and research use only. For a commercial license please contact the authors. You can view a license summary here:\nhttp://creativecommons.org/licenses/by-nc/4.0/\n\n\n## Contact\n\nAlex Kendall\n\nagk34@cam.ac.uk\n\nCambridge University"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9507374082549614
      ],
      "excerpt": "Net.__init__(Net, str, str, int) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9800277544866333
      ],
      "excerpt": "Alex Kendall, Vijay Badrinarayanan and Roberto Cipolla \"Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding.\" arXiv preprint arXiv:1511.02680, 2015. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/navganti/SegNet",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Alex Kendall\n\nagk34@cam.ac.uk\n\nCambridge University\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-03-15T20:33:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-22T06:21:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8971965160661097
      ],
      "excerpt": "TXT_PATH: Text file location which contains all the location of all the processed images. This is needed for Caffe. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9056889700189538
      ],
      "excerpt": "CROP_TO_ASPECT_RATIO: Crops input images to the aspect ratio of the wanted image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152313653086799,
        0.8779330371192023
      ],
      "excerpt": "    - data_dirs: locations of the datasets you preprocessed \n    - data_proportions: proportion of data you want to save for testing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8301510317514772,
        0.9508634085934767
      ],
      "excerpt": "- Init_Weights: Initial weights of the model (.caffemodel file) \n- Inference_Weights: Final inference weights of the model once training is completed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9692077694327298,
        0.8446436607261736
      ],
      "excerpt": "You usually need to wrap your strings inputted to Caffe functions with str(). This is only an issue with Python 2 which is explained in more detail here. \nFor more information about the SegNet architecture: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Scripts, models, and custom Caffe installation to run SegNet or Bayesian SegNet for semantic segmentation.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/navganti/SegNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sun, 26 Dec 2021 21:22:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/navganti/SegNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "navganti/SegNet",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8425760510645777
      ],
      "excerpt": "SegNet requires a modified version of Caffe to run. Please see the caffe-segnet-cudnn7 submodule within this repository, and follow the installation instructions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8888865185241291
      ],
      "excerpt": "If you notice an error that looks like the following: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.828533093851302
      ],
      "excerpt": "You can set these parameters directly in the file or using command line arguments. Your training and test set should be processed using this script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8203046462611201
      ],
      "excerpt": "To complete training, you must have a solver.prototxt and a train.prototxt. If you are performing inference with your generated model on the validation set, you will need a test.prototxt as well. Here are some things to look out for when configuring these files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8353844364580288
      ],
      "excerpt": "- The first layer's param dense_image_data_param.source should be the txt file of your validation set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423770616320789,
        0.8052146683670027
      ],
      "excerpt": "- Solvers: Solver file (.prototxt file) \n- Init_Weights: Initial weights of the model (.caffemodel file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8203882971642216
      ],
      "excerpt": "- Test_Models: Test model that is run on snapshots while training is done in parallel (.prototxt file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8773564089223136
      ],
      "excerpt": "Once all your trained models are in the ini file, you can run python train_and_test.py with the following arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8185103884552127
      ],
      "excerpt": "- --run_inference: Flag to test in parallel while training the model. Will only occur for each snapshot created \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/navganti/SegNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "MATLAB"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SegNet and Bayesian SegNet Tutorial",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SegNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "navganti",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/navganti/SegNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 21:22:33 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository contains all the files for you to complete the 'Getting Started with SegNet' and the 'Bayesian SegNet' tutorials here:\nhttp://mi.eng.cam.ac.uk/projects/segnet/tutorial.html\n\nPlease note that if following this instruction set, that the folder names __have been modified__.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To start, you can use the `scripts/inference/segnet_inference.py` script. It is recommended to use this with the `models/inference/SegNet/CityScapes/segnet_cityscapes.prototxt` model, and Timo S\u00e4mann's trained weights, which are available for download [here](http://mi.eng.cam.ac.uk/~agk34/resources/SegNet/segnet_iter_30000_timo.caffemodel).\n\nThe inference script can be used as follows:\n\n```\npython scripts/inference/inference.py models/inference/SegNet/CityScapes/segnet_cityscapes.prototxt \\\n/PATH/TO/segnet_iter_30000_timo.caffemodel data/test_segmentation.avi [--cpu]\n```\n\nIf the `--cpu` flag is set, then inference will be run on your CPU instead of your GPU. This is not recommended, unless you don't have a GPU.\n\nThe script uses OpenCV's [VideoCapture](https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-videocapture) to parse the data. An example video file has been provided for testing, `data/test_segmentation.avi`.\n\nThe easiest way to specify your own segmentation data is via a video file, such as an `.mp4` or `.avi`. Else, you must be sure to specify a folder of images with the format required for VideoCapture.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "A number of example models for indoor and outdoor road scene understanding can be found in the [SegNet Model Zoo](https://github.com/navganti/SegNet/blob/master/inference_models/segnet_model_zoo.md).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Cityscapes is a dataset that can be used to train SegNet/Bayesian SegNet, but a few steps must be done first. You can download the dataset [here](https://www.cityscapes-dataset.com/) and the Cityscape scripts repo [here](https://github.com/mcordts/cityscapesScripts). Once downloaded, follow these steps:\n1. Edit `/cityscapesScripts/cityscapescripts/helpers/labels.py` to contain the classes you want to train on.\n2. Set the `CITYSCAPES_DATASET` environment variable to wherever you downloaded the Cityscapes dataset.\n3. Run `python /cityscapesScripts/cityscapescripts/preparation/createTrainIdLabelImgs.py` to create the labeled images.\n4. Once the script is completed you should have a `${CITYSCAPES_DATASET}/gtFine/*/labelTrainIds.png` images created.\n\n",
      "technique": "Header extraction"
    }
  ]
}