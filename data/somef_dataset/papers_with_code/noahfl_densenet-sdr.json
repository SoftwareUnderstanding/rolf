{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1808.03578",
      "https://arxiv.org/abs/1608.06993",
      "https://arxiv.org/abs/1608.06993"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8356013927728488,
        0.8444342525991423,
        0.9919784769950174
      ],
      "excerpt": "|DenseNet(k = 12)    |40     |2.256(5.160)     |09.36(22.60)      | \n|DenseNet(k = 12)    |100    |1.360(3.820) |05.16(11.06)  | \n|DenseNet-BC(k = 12) |100    |2.520(6.340)     |11.12(25.08)      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "|DenseNet(k = 12)    |100    |8 \\ 13 \\ 25     |28 \\ 60 \\ --     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9725140271112827
      ],
      "excerpt": "|DenseNet-BC(k = 12) |100    |10 \\ 25 \\ --    |-- \\ -- \\ --     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "|DenseNet(k = 12)    |40     |5 \\  8 \\ 15 |27 \\ 48 \\ --               | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/noahfl/densenet-sdr",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-05-14T21:03:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-01T07:48:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9754771855390412
      ],
      "excerpt": "This table shows the results on CIFAR shown in the paper. Parameters are all the same as what are used in the paper, except for a batch size of 100 and an epoch size of 100. SDR's beta value was 0.1 and zeta was 0.01. The augmented datasets were not tested on because dropout was not used on these datasets in the original paper, however they may be added in the future (as will the SVHN results and results with higher layer counts). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9568161652402647
      ],
      "excerpt": "Comparison to original DenseNet implementation with dropout \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8381459300945023
      ],
      "excerpt": "Difference compared to the original implementation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8949570439375694
      ],
      "excerpt": "Model should work as expected with TensorFlow >= 0.10 FOR DROPOUT ONLY. SDR was added using a development environment with TensorFlow 1.7 so it may require 1.0+. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "repo that holds code for improving on dropout using Stochastic Delta Rule",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/noahfl/densenet-sdr/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The below tables show the number of training epochs required to reach a training error of 15, 10, and 5, respectively. For example, the dropout version of DenseNet-40 on CIFAR-10 took 8 epochs to reach a training error of 15, 16 epochs to reach a training error of 10, and 94 epochs to reach a training error of 5. In contrast, the SDR version of DenseNet-40 on CIFAR-10 took 5 epochs to reach a training error of 15, 5 epochs to reach a training error of 10, and 15 epochs to reach a training error of 5. Best results for each value, across both dropout and SDR, are bolded.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15,
      "date": "Sun, 26 Dec 2021 08:43:34 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/noahfl/densenet-sdr/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "noahfl/densenet-sdr",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.9028503186323809
      ],
      "excerpt": "|Model type            |Depth  |C10              |C100              | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9028503186323809
      ],
      "excerpt": "|Model type            |Depth  |C10             |C100             | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8054114879568159
      ],
      "excerpt": "|DenseNet-BC(k = 12) |100    |10 \\ 25 \\ --    |-- \\ -- \\ --     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9028503186323809
      ],
      "excerpt": "|Model type            |Depth  |C10                     |C100                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9028503186323809
      ],
      "excerpt": "|Model type            |Depth  |C10         |C10+       |C100          |C100+       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8147617807814931
      ],
      "excerpt": "|DenseNet-BC(k = 12) |100    |5.54(5.92)  |4.87(4.51) |24.88(24.15)  |22.85(22.27)| \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/noahfl/densenet-sdr/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Illarion\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stochastic Delta Rule implementation using DenseNet in TensorFlow",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "densenet-sdr",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "noahfl",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/noahfl/densenet-sdr/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 145,
      "date": "Sun, 26 Dec 2021 08:43:34 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dropout",
      "neural-network",
      "deep-learning",
      "deep-neural-networks",
      "statistics"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**NOTE** This is repository is based off of [Illarion Khlestov's DenseNet implementation](https://github.com/ikhlestov/vision_networks/ \"ikhlestov/vision_networks/\"). Check out his blog post about implementing DenseNet in TensorFlow [here](https://medium.com/@illarionkhlestov/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#.55qu3tfqm).\n\n\n---------------------------------------------------------------------------------------\n\n\nCheck out @lifeiteng's [results from implementing SDR with WaveNet](https://twitter.com/FeitengLi/status/1029166830844227584).\n\n\n**UPDATE**: Due to a bug found by @basveeling which has now been corrected, the testing errors are being recalculated. Here are the preliminary results, which I will continue to update as the results come out. \"-----\" indicates results that have not yet been redone.\n\n|Model type            |Depth  |C10              |C100              |\n|:---------------------|:------|:----------------|:-----------------|\n|DenseNet(*k* = 12)    |40     |-----(-----)     |-----(-----)      |\n|DenseNet(*k* = 12)    |100    |**-----**(-----) |**-----**(-----)  |\n|DenseNet-BC(*k* = 12) |100    |-----(-----)     |-----(-----)      |\n\n\n\nThis repository holds the code for the paper \n\n'Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning' (submitted to NIPS; on [arXiv](https://arxiv.org/abs/1808.03578))\n\n[Noah Frazier-Logue](https://www.linkedin.com/in/noah-frazier-logue-1524b796/), [Stephen Jose Hanson](http://nwkpsych.rutgers.edu/~jose/)\n\nStochastic Delta Rule (SDR) is a weight update mechanism that assigns to each weight a standard deviation that changes as a function of the gradients every training iteration. At the beginning of each training iteration, the weights are re-initialized using a normal distribution bound by their standard deviations. Over the course of the training iterations and epochs, the standard deviations converge towards zero as the network becomes more sure of what the values of each of the weights should be. For a more detailed description of the method and its properties, have a look at the paper [link here].\n\n\n\nTwo types of [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993) (DenseNets) are available:\n\n- DenseNet - without bottleneck layers\n- DenseNet-BC - with bottleneck layers\n\nEach model can be tested on such datasets:\n\n- CIFAR-10\n- CIFAR-10+ (with data augmentation)\n- CIFAR-100\n- CIFAR-100+ (with data augmentation)\n- SVHN\n\nA number of layers, blocks, growth rate, image normalization and other training params may be changed trough shell or inside the source code.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Example run:\n\n```\n    python run_dense_net.py --depth=40 --train --test --dataset=C10 --sdr\n```\n\nThis run uses SDR instead of dropout. To use dropout, run something like\n\n```\n    python run_dense_net.py --depth=40 --train --test --dataset=C10 --keep_prob=0.8\n```\n\nwhere `keep_prob` is the probability (in this case 80%) that a neuron is *kept* during dropout.\n\n**NOTE:** the `--sdr` argument will override the `--keep_prob` argument. For example:\n\n```\n    python run_dense_net.py --depth=40 --train --test --dataset=C10 --keep_prob=0.8 --sdr\n```\n\nwill use SDR and not dropout.\n\n\nList all available options:\n\n```    \n    python run_dense_net.py --help\n```\n\nThere are also many [other implementations](https://github.com/liuzhuang13/DenseNet) - they may be useful.\n\nCitation:\n\n```     \n     @article{Huang2016Densely,\n            author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q.},\n            title = {Densely Connected Convolutional Networks},\n            journal = {arXiv preprint arXiv:1608.06993},\n            year = {2016}\n     }\n```\n\n**KNOWN ISSUES**\n\n - model will not save due to graph definiton being larger than 2GB\n\nIf you see anything wrong, feel free to open an issue!\n\n\n",
      "technique": "Header extraction"
    }
  ]
}