{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Natural TTS synthesis by conditioning Wavenet on MEL spectogram predictions](https://arxiv.org/pdf/1712.05884.pdf)\n- [Original tacotron paper](https://arxiv.org/pdf/1703.10135.pdf)\n- [Attention-Based Models for Speech Recognition](https://arxiv.org/pdf/1506.07503.pdf)\n- [Wavenet: A generative model for raw audio](https://arxiv.org/pdf/1609.03499.pdf)\n- [Fast Wavenet](https://arxiv.org/pdf/1611.09482.pdf)\n- [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder)\n- [keithito/tacotron](https://github.com/keithito/tacotron)\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rayhane-mamah/Tacotron-2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-12-20T16:08:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T17:58:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9236473297363677,
        0.9626193816739484,
        0.9661729718014122
      ],
      "excerpt": "Tensorflow implementation of DeepMind's Tacotron-2. A deep neural network architecture described in this paper: Natural TTS synthesis by conditioning Wavenet on MEL spectogram predictions \nThis Repository contains additional improvements and attempts over the paper, we thus propose paper_hparams.py file which holds the exact hyperparameters to reproduce the paper results without any additional extras. \nSuggested hparams.py file which is default in use, contains the hyperparameters with extras that proved to provide better results in most cases. Feel free to toy with the parameters as needed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8998397105261863
      ],
      "excerpt": "The previous tree shows the current state of the repository (separate training, one step at a time). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8204704956124925
      ],
      "excerpt": "- Our preprocessing only supports Ljspeech and Ljspeech-like datasets (M-AILABS speech data)! If running on datasets stored differently, you will probably need to make your own preprocessing script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066256054126103
      ],
      "excerpt": "Pre-trained models and audio samples will be added at a later date. You can however check some primary insights of the model performance (at early stages of training) here. THIS IS VERY OUTDATED, I WILL UPDATE THIS SOON \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537354180005452
      ],
      "excerpt": "The model described by the authors can be divided in two parts: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9758105514720279,
        0.9937703239955189,
        0.816386414578732
      ],
      "excerpt": "To have an in-depth exploration of the model architecture, training procedure and preprocessing logic, refer to our wiki \nTo have an overview of our advance on this project, please refer to this discussion \nsince the two parts of the global model are trained separately, we can start by training the feature prediction model to use his predictions later during the wavenet training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.981323590424365
      ],
      "excerpt": "We are also running current tests on the new M-AILABS speech dataset which contains more than 700h of speech (more than 80 Gb of data) for more than 10 languages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9092948537102926
      ],
      "excerpt": "To synthesize audio in an End-to-End (text to audio) manner (both models at work): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8919804556653665,
        0.8542289841147476
      ],
      "excerpt": "For the spectrogram prediction network (separately), there are three types of mel spectrograms synthesis: \nEvaluation (synthesis on custom sentences). This is what we'll usually use after having a full end to end model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9367617248464664
      ],
      "excerpt": "Ground Truth Aligned synthesis (DEFAULT: the model is assisted by true labels in a teacher forcing manner). This synthesis method is used when predicting mel spectrograms used to train the wavenet vocoder. (yields better results as stated in the paper) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "DeepMind's Tacotron-2 Tensorflow implementation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rayhane-mamah/Tacotron-2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 863,
      "date": "Thu, 23 Dec 2021 05:53:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rayhane-mamah/Tacotron-2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rayhane-mamah/Tacotron-2",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Rayhane-mamah/Tacotron-2/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Rayhane-mamah/Tacotron-2/master/griffin_lim_synthesis_tool.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Before proceeding, you must pick the hyperparameters that suit best your needs. While it is possible to change the hyper parameters from command line during preprocessing/training, I still recommend making the changes once and for all on the **hparams.py** file directly.\n\nTo pick optimal fft parameters, I have made a **griffin_lim_synthesis_tool** notebook that you can use to invert real extracted mel/linear spectrograms and choose how good your preprocessing is. All other options are well explained in the **hparams.py** and have meaningful names so that you can try multiple things with them.\n\nAWAIT DOCUMENTATION ON HPARAMS SHORTLY!!\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8368615424282961,
        0.9906248903846466
      ],
      "excerpt": "Before running the following steps, please make sure you are inside Tacotron-2 folder \ncd Tacotron-2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8234439612355421
      ],
      "excerpt": "To train both models sequentially (one after the other): \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8924976426181745
      ],
      "excerpt": "\u2502\u00a0\u00a0 \u2514\u2500\u2500 utils \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8493476293807158
      ],
      "excerpt": "Step (2): Train your Tacotron model. Yields the logs-Tacotron folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8255108530356875
      ],
      "excerpt": "Step (4): Train your Wavenet model. Yield the logs-Wavenet folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python preprocess.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778487586960795,
        0.8255344933059476
      ],
      "excerpt": "Example M-AILABS: \npython preprocess.py --dataset='M-AILABS' --language='en_US' --voice='female' --reader='mary_ann' --merge_books=False --book='northandsouth' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8591070667708376
      ],
      "excerpt": "python preprocess.py --dataset='M-AILABS' --language='en_US' --voice='female' --reader='mary_ann' --merge_books=True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9506851822712116
      ],
      "excerpt": "python train.py --model='Tacotron-2' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9506851822712116
      ],
      "excerpt": "python train.py --model='Tacotron' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9506851822712116
      ],
      "excerpt": "python train.py --model='WaveNet' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8030628381364274
      ],
      "excerpt": "- If model argument is not provided, training will default to Tacotron-2 model training. (both models) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9313930516571822
      ],
      "excerpt": "python synthesize.py --model='Tacotron-2' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9313930516571822
      ],
      "excerpt": "python synthesize.py --model='Tacotron' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9280437445278036
      ],
      "excerpt": "python synthesize.py --model='Tacotron' --mode='synthesis' --GTA=False \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9434932108139206
      ],
      "excerpt": "python synthesize.py --model='Tacotron' --mode='synthesis' --GTA=True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9313930516571822
      ],
      "excerpt": "python synthesize.py --model='WaveNet' \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rayhane-mamah/Tacotron-2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Rayhane Mama\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tacotron-2:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tacotron-2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rayhane-mamah",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rayhane-mamah/Tacotron-2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1970,
      "date": "Thu, 23 Dec 2021 05:53:59 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tacotron",
      "tensorflow",
      "paper",
      "python",
      "speech-synthesis",
      "text-to-speech",
      "wavenet"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- **Machine Setup:**\n\nFirst, you need to have python 3 installed along with [Tensorflow](https://www.tensorflow.org/install/).\n\nNext, you need to install some Linux dependencies to ensure audio libraries work properly:\n\n> apt-get install -y libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg libav-tools\n\nFinally, you can install the requirements. If you are an Anaconda user: (else replace **pip** with **pip3** and **python** with **python3**)\n\n> pip install -r requirements.txt\n\n- **Docker:**\n\nAlternatively, one can build the **docker image** to ensure everything is setup automatically and use the project inside the docker containers.\n**Dockerfile is insider \"docker\" folder**\n\ndocker image can be built with:\n\n> docker build -t tacotron-2_image docker/\n\nThen containers are runnable with:\n\n> docker run -i --name new_container tacotron-2_image\n\nPlease report any issues with the Docker usage with our models, I'll get to it. Thanks!\n\n",
      "technique": "Header extraction"
    }
  ]
}