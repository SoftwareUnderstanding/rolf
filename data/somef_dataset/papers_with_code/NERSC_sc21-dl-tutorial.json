{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2106.12662",
      "https://arxiv.org/abs/1505.04597"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8444342525991423,
        0.8566955015433729
      ],
      "excerpt": "2021-11-09 00:19:14,226 - root - INFO -   Avg val loss=0.040343 \n2021-11-09 00:19:14,227 - root - INFO -   Total validation time: 10.133511781692505 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 00:20:07,986 - root - INFO -   Avg val loss=0.025327 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 00:21:00,246 - root - INFO -   Avg val loss=0.024092 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384,
        0.8043073075947367
      ],
      "excerpt": "sbatch -n1 submit_pm.sh --config=short --num_epochs 10 --enable_benchy \nIf running interactively: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "If running interactively: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "2021-11-09 00:21:52,277 - root - INFO -   Avg val loss=0.025949 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 00:22:20,662 - root - INFO -   Avg val loss=0.024352 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9015781078351486,
        0.9559715772848645
      ],
      "excerpt": "2021-11-09 00:32:48,064 - root - INFO -   Avg train loss=0.073569 \n2021-11-09 00:32:52,265 - root - INFO -   Avg val loss=0.048459 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9835281548744423,
        0.9042750916398984
      ],
      "excerpt": "2021-11-09 00:33:10,462 - root - INFO -   Avg val loss=0.030250 \n2021-11-09 00:33:10,462 - root - INFO -   Total validation time: 2.910416841506958 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 00:33:28,357 - root - INFO -   Avg val loss=0.027871 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 00:21:06,210 - root - INFO -   Avg val loss=0.043009 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 00:21:25,424 - root - INFO -   Avg val loss=0.028309 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 00:21:44,875 - root - INFO -   Avg val loss=0.026001 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "If running interactively: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9015781078351486
      ],
      "excerpt": "2021-11-09 19:45:32,451 - root - INFO -   Avg val loss=0.027525 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "2021-11-09 20:22:29,499 - root - INFO -   Avg val loss=0.046892 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 20:23:02,227 - root - INFO -   Avg val loss=0.029362 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "2021-11-09 20:23:34,895 - root - INFO -   Avg val loss=0.027441 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "If running interactively: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 20:15:52,365 - root - INFO -   Avg val loss=0.044170 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9618188552511033
      ],
      "excerpt": "2021-11-09 20:16:10,394 - root - INFO -   Avg val loss=0.024537 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "If running interactively: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 20:22:28,518 - root - INFO -   Avg val loss=0.045140 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 20:22:38,057 - root - INFO -   Avg val loss=0.028168 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 20:22:46,480 - root - INFO -   Avg val loss=0.025598 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "If running interactively: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 20:22:55,358 - root - INFO -   Avg val loss=0.028591 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "2021-11-09 20:23:03,665 - root - INFO -   Avg val loss=0.025971 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9356322232318476
      ],
      "excerpt": "Question: what do you think would happen if we simply increased our learning rate without increasing batch size? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "sbatch -t 10 -n 8 submit_pm.sh --config=bs512_test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9304587109704758
      ],
      "excerpt": "You should also consider the following questions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9758185342340638,
        0.9618188552511033
      ],
      "excerpt": "2021-11-10 04:03:37,792 - root - INFO -   Avg train loss=0.006371 \n2021-11-10 04:03:41,047 - root - INFO -   Avg val loss=0.006337 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9190281883855547,
        0.913999346918293
      ],
      "excerpt": "2021-11-10 04:04:32,869 - root - INFO -   Avg train loss=0.005793 \n2021-11-10 04:04:36,134 - root - INFO -   Avg val loss=0.005889 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139993469182865,
        0.9896534279187987,
        0.9380142677289868
      ],
      "excerpt": "2021-11-10 04:05:27,672 - root - INFO -   Avg train loss=0.005587 \n2021-11-10 04:05:30,891 - root - INFO -   Avg val loss=0.005936 \n2021-11-10 04:05:30,891 - root - INFO -   Total validation time: 3.2182624340057373 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9758185342340638,
        0.9958868711349751,
        0.9745587028363859
      ],
      "excerpt": "2021-11-10 04:12:07,932 - root - INFO -   Avg train loss=0.006372 \n2021-11-10 04:12:11,173 - root - INFO -   Avg val loss=0.006370 \n2021-11-10 04:12:11,173 - root - INFO -   Total validation time: 3.2399580478668213 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139993469182865,
        0.913999346918293
      ],
      "excerpt": "2021-11-10 04:13:01,406 - root - INFO -   Avg train loss=0.005815 \n2021-11-10 04:13:04,636 - root - INFO -   Avg val loss=0.005902 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139993469182865,
        0.9618188552511033
      ],
      "excerpt": "2021-11-10 04:13:54,473 - root - INFO -   Avg train loss=0.005614 \n2021-11-10 04:13:57,722 - root - INFO -   Avg val loss=0.005941 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139993469182865,
        0.9618188552511033
      ],
      "excerpt": "2021-11-10 04:19:48,472 - root - INFO -   Avg train loss=0.006478 \n2021-11-10 04:19:51,711 - root - INFO -   Avg val loss=0.006588 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139993469182865,
        0.9618188552511033
      ],
      "excerpt": "2021-11-10 04:20:41,475 - root - INFO -   Avg train loss=0.005917 \n2021-11-10 04:20:44,730 - root - INFO -   Avg val loss=0.006044 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.870561293637803,
        0.9896534279187987,
        0.9380142677289868
      ],
      "excerpt": "2021-11-10 04:21:34,517 - root - INFO -   Avg train loss=0.005700 \n2021-11-10 04:21:37,772 - root - INFO -   Avg val loss=0.006073 \n2021-11-10 04:21:37,773 - root - INFO -   Total validation time: 3.2548396587371826 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.813341438260231,
        0.9896534279187987,
        0.9934273497841208,
        0.9598596565459202,
        0.8802874359092572,
        0.9845600173250374,
        0.9835281548744423,
        0.9042750916398984
      ],
      "excerpt": "2021-11-10 02:41:30,680 - root - INFO - Time taken for epoch 2 is 44.45261096954346 sec, avg 1474.2891040731388 samples/sec \n2021-11-10 02:41:30,710 - root - INFO -   Avg train loss=0.007586 \n2021-11-10 02:41:32,457 - root - INFO -   Avg val loss=0.007256 \n2021-11-10 02:41:32,457 - root - INFO -   Total validation time: 1.7458698749542236 sec \n2021-11-10 02:42:08,002 - root - INFO - Time taken for epoch 3 is 35.54009485244751 sec, avg 1844.0018315113414 samples/sec \n2021-11-10 02:42:08,028 - root - INFO -   Avg train loss=0.006422 \n2021-11-10 02:42:09,688 - root - INFO -   Avg val loss=0.006547 \n2021-11-10 02:42:09,688 - root - INFO -   Total validation time: 1.6595783233642578 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139993469182865,
        0.9835281548744423,
        0.9042750916398984
      ],
      "excerpt": "2021-11-10 02:42:45,644 - root - INFO -   Avg train loss=0.006166 \n2021-11-10 02:42:47,310 - root - INFO -   Avg val loss=0.006547 \n2021-11-10 02:42:47,310 - root - INFO -   Total validation time: 1.6650199890136719 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9618188552511033,
        0.9956086121706917,
        0.9728768603164633
      ],
      "excerpt": "2021-11-10 02:41:28,528 - root - INFO -   Avg train loss=0.007528 \n2021-11-10 02:41:30,271 - root - INFO -   Avg val loss=0.007238 \n2021-11-10 02:41:30,272 - root - INFO -   Total validation time: 1.742598056793213 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9618188552511033,
        0.9835281548744423,
        0.9042750916398984
      ],
      "excerpt": "2021-11-10 02:42:05,136 - root - INFO -   Avg train loss=0.006444 \n2021-11-10 02:42:06,803 - root - INFO -   Avg val loss=0.006532 \n2021-11-10 02:42:06,804 - root - INFO -   Total validation time: 1.6663029193878174 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139993469182865,
        0.9835281548744423,
        0.9042750916398984
      ],
      "excerpt": "2021-11-10 02:42:42,123 - root - INFO -   Avg train loss=0.006195 \n2021-11-10 02:42:43,763 - root - INFO -   Avg val loss=0.006568 \n2021-11-10 02:42:43,786 - root - INFO -   Total validation time: 1.6387364864349365 sec \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NERSC/sc21-dl-tutorial",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-26T05:07:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-26T02:33:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To submit a multi-GPU job, use the `submit_pm.sh` with the `-n` option set to the desired number of GPUs. For example, to launch a training with multiple GPUs, you will use commands like:\n```\nsbatch -n NUM_GPU submit_pm.sh [OPTIONS]\n```\nThis script automatically uses the slurm flags `--ntasks-per-node 4`, `--cpus-per-task 32`, `--gpus-per-task 1`, so slurm will allocate one process for each GPU we request, and give each process 1/4th of the CPU resources available on a Perlmutter GPU node. This way, multi-node trainings can easily be launched simply by setting `-n` greater than 4.\n\n*Question: why do you think we run 1 task (cpu process) per GPU, instead of 1 task per node (each running 4 GPUs)?*\n\nPyTorch `DistributedDataParallel`, or DDP for short, is flexible and can initialize process groups with a variety of methods. For this code, we will use the standard approach of initializing via environment variables, which can be easily read from the slurm environment. Take a look at the `export_DDP_vars.sh` helper script, which is used by our job script to expose for PyTorch DDP the global rank and node-local rank of each process, along with the total number of ranks and the address and port to use for network communication. In the [`train.py`](train.py) script, near the bottom in the main script execution, we set up the distributed backend using these environment variables via `torch.distributed.init_proces_group`.\n\nWhen distributing a batch of samples in DDP training, we must make sure each rank gets a properly-sized subset of the full batch. See if you can find where we use the `DistributedSampler` from PyTorch to properly partition the data in [`utils/data_loader.py`](utils/data_loader.py). Note that in this particular example, we are already cropping samples randomly form a large simulation volume, so the partitioning does not ensure each rank gets unique data, but simply shortens the number of steps needed to complete an \"epoch\". For datasets with a fixed number of unique samples, `DistributedSampler` will also ensure each rank sees a unique minibatch.\n\nIn `train.py`, after our U-Net model is constructed,\nwe convert it to a distributed data parallel model by wrapping it as:\n```\nmodel = DistributedDataParallel(model, device_ids=[local_rank])\n```\n\nThe DistributedDataParallel (DDP) model wrapper takes care of broadcasting\ninitial model weights to all workers and performing all-reduce on the gradients\nin the training backward pass to properly synchronize and update the model\nweights in the distributed setting.\n\n*Question: why does DDP broadcast the initial model weights to all workers? What would happen if it didn't?*\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8420146467442114
      ],
      "excerpt": "Access to NERSC's Perlmutter machine is provided for this tutorial via jupyter.nersc.gov.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9243321840532015,
        0.8430858676558484,
        0.987712978762021,
        0.9262452378875251,
        0.9979538016686731,
        0.9213840673652215
      ],
      "excerpt": "For running slurm jobs on Perlmutter, we will use training accounts which are provided under the ntrain4 project. The slurm script submit_pm.sh included in the repository is configured to work automatically as is, but if you submit your own custom jobs via salloc or sbatch you must include the following flags for slurm: \n* -A ntrain4_g is required for training accounts \n* --reservation=sc21_tutorial_01 is required to access the set of GPU nodes we have reserved for the duration of the tutorial. \nThe code can be run using the romerojosh/containers:sc21_tutorial docker container. On Perlmutter, docker containers are run via shifter, and this container is already downloaded and automatically invoked by our job submission scripts. Our container is based on the NVIDIA ngc 21.10 pytorch container, with a few additional packages added. See the dockerfile in docker/Dockerfile for details. \nThe model in this repository is adapted from a cosmological application of deep learning (Harrington et al. 2021), which aims to augment computationally expensive simulations by using a U-Net model to reconstruct physical fields of interest (namely, hydrodynamic quantities associated with diffuse gas in the universe): \nThe U-Net model architecture used in these examples can be found in networks/UNet.py. U-Nets are a popular and capable architecture, as they can extract long-range features through sequential downsampling convolutions, while fine-grained details can be propagated to the upsampling path via skip connections. This particular U-Net is relatively lightweight, to better accommodate our 3D data samples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9960341982510744,
        0.9854232984155621,
        0.9166687590599728,
        0.9690499698425192,
        0.967793799972453
      ],
      "excerpt": "* The RandomCropDataset which accesses the simulation data stored on disk, and randomly crops sub-volumes of the physical fields to serve for training and validation. For this repository, we will be using a crop size of 64^3 \n* The RandomRotator transform, which applies random rotations and reflections to the samples as data augmentations \n* The above components are assembled to feed a PyTorch DataLoader which takes the augmented samples and combines them into a batch for each training step. \nIt is common practice to decay the learning rate according to some schedule as the model trains, so that the optimizer can settle into sharper minima during gradient descent. Here we opt for the cosine learning rate decay schedule, which starts at an intial learning rate and decays continuously throughout training according to a cosine function. This is handled by the lr_schedule routine defined in utils/__init__.py, which also has logic to implement learning rate scaling and warm-up for use in the Distributed GPU training section \nAs we will see in the Single GPU performance profiling and optimization section, the random rotations add considerable overhead to data loading during training, and we can achieve performance gains by doing these preprocessing steps on the GPU instead using NVIDIA's DALI library. Code for this is found in utils/data_loader_dali.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9410067815626052
      ],
      "excerpt": "1.  Set up the data loaders and construct our U-Net model, the Adam optimizer, and our L1 loss function. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95572047333207,
        0.8523755285339476,
        0.8202238340657223,
        0.8024357866062946
      ],
      "excerpt": "    * Looping over data batches from our data loader. \n    * Applying the forward pass of the model and computing the loss function. \n    * Calling backward() on the loss value to backpropagate gradients. Note the use of the grad_scaler will be explained below when enabling mixed precision. \n    * Applying the model to the validation dataset and logging training and validation metrics to visualize in TensorBoard (see if you can find where we construct the TensorBoard SummaryWriter and where our specific metrics are logged via the add_scalar call). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382489126698333,
        0.8727212069988654
      ],
      "excerpt": "script, which implements the same functionality with added capability for using the CUDA Graphs APIs introduced in PyTorch 1.10. This topic will be covered in the Single GPU performance profiling and optimization section. \nMore info on the model and data can be found in the slides. If you are experimenting with this repository after the tutorial date, you can download the data from here: https://portal.nersc.gov/project/dasrepo/pharring/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8322370328363108
      ],
      "excerpt": "On Perlmutter for the tutorial, we will be submitting jobs to the batch queue. To submit this job, use the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8496466099469845
      ],
      "excerpt": "For V100 systems, you will likely need to update the config to reduce the local batch size to 32 due to the reduced memory capacity. Otherwise, instructions are the same. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9034137352400649,
        0.8105835172660663,
        0.9869942807993187
      ],
      "excerpt": "Note we will use the default batch size for the optimization work in the next section \nand will push beyond to larger batch sizes in the distributed training section. \nIn the baseline configuration, the model converges to a loss of about 4.75e-3 on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.845540961677338
      ],
      "excerpt": "We want to compare our training results against the base config baseline, and TensorBoard makes this easy as long as all training runs are stored in the same place.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8711820566125921,
        0.9459456662815269,
        0.8889094946947484
      ],
      "excerpt": "As our training with the short config runs, it should also dump the training metrics to the TensorBoard directory, and TensorBoard will parse the data and display it for you. You can hit the refresh button in the upper-right corner of TensorBoard to update the plots with the latest data. \nThis is the performance of the baseline script for the first three epochs on a 40GB A100 card with batch size 64: \n2021-11-09 00:19:04,091 - root - INFO - Time taken for epoch 1 is 110.217036485672 sec, avg 37.1630387697139 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761968568493606
      ],
      "excerpt": "2021-11-09 00:20:03,014 - root - INFO - Time taken for epoch 2 is 48.785075426101685 sec, avg 83.96010386833387 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9553823460275689
      ],
      "excerpt": "2021-11-09 00:20:55,329 - root - INFO - Time taken for epoch 3 is 47.339499711990356 sec, avg 86.52393930902795 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8922811675518593,
        0.9255974454987569
      ],
      "excerpt": "After the first epoch, we see that the throughput achieved is about 85 samples/s. \nBefore generating a profile with Nsight, we can add NVTX ranges to the script to add context to the produced timeline. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8146957083051415,
        0.9022003321843121
      ],
      "excerpt": "We can also add calls to torch.cuda.profiler.start() and torch.cuda.profiler.stop() to control the duration of the profiling \n(e.g., limit profiling to single epoch). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8502557626641452,
        0.9646854434990367,
        0.996961843586694
      ],
      "excerpt": "From this zoomed out view, we can see a lot idle gaps between iterations. These gaps are due to the data loading, which we will address in the next section. \nBeyond this, we can zoom into a single iteration and get an idea of where compute time is being spent: \nAs an alternative to manually specifying NVTX ranges, we've included the use of a simple profiling tool benchy that overrides the PyTorch dataloader in the script to produce throughput information to the terminal, as well as add NVTX ranges/profiler start and stop calls. This tool also runs a sequence of tests to measure and report the throughput of the dataloader in isolation (IO), the model running with synthetic/cached data (SYNTHETIC), and the throughput of the model running normally with real data (FULL). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8750760891860706,
        0.9807800103406678
      ],
      "excerpt": "benchy uses epoch boundaries to separate the test trials it runs, so in these cases we increase the epoch limit to 10 to ensure the full experiment runs. \nbenchy will report throughput measurements directly to the terminal, including a simple summary of averages at the end of the job. For this case on Perlmutter, the summary output from benchy is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9273581636197291,
        0.9870532147598021,
        0.9305540798705676
      ],
      "excerpt": "From these throughput values, we can see that the SYNTHETIC (i.e. compute) throughput is greater than the IO (i.e. data loading) throughput. \nThe FULL (i.e. real) throughput is bounded by the slower of these two values, which is IO in this case. What these throughput \nvalues indicate is the GPU can achieve much greater training throughput for this model, but is being limited by the data loading \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9463757659108151,
        0.9830551234776232,
        0.8053883586201952,
        0.801451435789502,
        0.906707979655761
      ],
      "excerpt": "pin_memory has the data loader read input data into pinned host memory, which typically yields better host-to-device and device-to-host \nmemcopy bandwidth. persistent_workers allows PyTorch to reuse workers between epochs, instead of the default behavior which is to \nrespawn them. One knob we've left to adjust is the num_workers argument, which we can control via the --num_data_workers command \nline arg to our script. The default in our config is two workers, but we can experiment with this value to see if increasing the number \nof workers improves performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9755519778924845,
        0.9703809212705992
      ],
      "excerpt": "This is the performance of the training script for the first three epochs on a 40GB A100 card with batch size 64 and 4 data workers: \n2021-11-09 00:21:17,371 - root - INFO - Time taken for epoch 1 is 79.13155698776245 sec, avg 51.761903290155644 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761968568493606
      ],
      "excerpt": "2021-11-09 00:21:48,916 - root - INFO - Time taken for epoch 2 is 25.728514432907104 sec, avg 159.20079686999583 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9703809212705992
      ],
      "excerpt": "2021-11-09 00:22:17,380 - root - INFO - Time taken for epoch 3 is 25.10083317756653 sec, avg 163.18183428511588 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9755519778924845,
        0.9761968568493606
      ],
      "excerpt": "This is the performance of the training script for the first three epochs on a 40GB A100 card with batch size 64 and 8 data workers: \n2021-11-09 00:32:48,064 - root - INFO - Time taken for epoch 1 is 62.2959144115448 sec, avg 65.75070032587757 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761968568493606
      ],
      "excerpt": "2021-11-09 00:33:07,551 - root - INFO - Time taken for epoch 2 is 15.283130884170532 sec, avg 268.00791219045453 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9488899790803059
      ],
      "excerpt": "2021-11-09 00:33:25,404 - root - INFO - Time taken for epoch 3 is 14.93994927406311 sec, avg 274.16425081917566 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9654618818356759,
        0.9761968568493606
      ],
      "excerpt": "This is the performance of the training script for the first three epochs on a 40GB A100 card with batch size 64 and 16 data workers: \n2021-11-09 00:21:01,556 - root - INFO - Time taken for epoch 1 is 62.40265655517578 sec, avg 65.63823122463319 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329631397748734
      ],
      "excerpt": "2021-11-09 00:21:22,464 - root - INFO - Time taken for epoch 2 is 16.23646593093872 sec, avg 252.27164688560939 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329631397748734
      ],
      "excerpt": "2021-11-09 00:21:41,607 - root - INFO - Time taken for epoch 3 is 16.159828186035156 sec, avg 253.46804142012112 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.839636728190166
      ],
      "excerpt": "Increasing the number of workers to 8 improves performance to around 270 samples per second, while increasing to 16 workers causes a slight reduction from this. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138084663740083,
        0.9365085547950406,
        0.9609110300416874,
        0.9821608431392318,
        0.8534918417929596,
        0.939541475434171,
        0.9332115655769456
      ],
      "excerpt": "argument and load that profile in Nsight Systems. This is what this profile (8workers.qdrep) looks like: \nand zoomed in: \nWith 8 data workers, the large gaps between steps are mostly alleviated, improving the throughput. Looking at the zoomed in profile, we \nstill see that the H2D copy in of the input data takes some time and could be improved. One option here is to implement a prefetching \nmechanism in PyTorch directly using CUDA streams to concurrently load and copy in the next batch of input during the current batch, however \nthis is left as an exercise outside of this tutorial. A good example of this can be found in here. \nUsing benchy, we can also check how the various throughputs compare using 8 data workers. Running this configuration on Perlmutter \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916461812230552
      ],
      "excerpt": "IO is faster as expected, and the FULL throughput increases correspondingly. However, IO is still lower than SYNTHETIC, meaning we \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823699678210075
      ],
      "excerpt": "While we were able to get more performance out of the PyTorch native DataLoader, there are several overheads we cannot overcome in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8469644634192777,
        0.9102790991702548,
        0.8002850991487639,
        0.8990807513357595,
        0.8698040034589543
      ],
      "excerpt": "2. The PyTorch DataLoader uses multi-processing to spawn data workers, which has performance overheads compared to true threads \nThe NVIDIA DALI library is a data loading library that can address both of these points: \n1. DALI can perform a wide array of data augmentation operations on the GPU, benefitting from acceleration relative to the CPU. \n2. DALI maintains its own worker threads in the C++ backend, enabling much more performant threading and concurrent operation. \nFor this tutorial, we've provided an alternative data loader using DALI to accelerate the data augementations used in this training script (e.g. 3D cropping, rotations, and flips) that can be found in utils/data_loader_dali.py. This data loader is enabled via the command line \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9632316594703139,
        0.9173694312090043
      ],
      "excerpt": "This is the performance of the training script for the first three epochs on a 40GB A100 card with batch size 64 and DALI: \n2021-11-09 19:45:15,642 - root - INFO - Time taken for epoch 1 is 252.39585137367249 sec, avg 16.22847593455831 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663440190265597
      ],
      "excerpt": "2021-11-09 19:45:31,323 - root - INFO - Time taken for epoch 2 is 10.39395022392273 sec, avg 394.0753911417279 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663440190265597
      ],
      "excerpt": "2021-11-09 19:45:42,819 - root - INFO - Time taken for epoch 3 is 10.365204572677612 sec, avg 395.1682739380698 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138084663740083,
        0.9313038037935661
      ],
      "excerpt": "argument and load that profile in Nsight Systems. This is what this profile (dali.qdrep) looks like: \nand zoomed in to a single iteration: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9652854386669781,
        0.8430064163608646
      ],
      "excerpt": "to run data augmentation kernels and any memory movement concurrently with the existing PyTorch compute kernels. Stream 13 in this view, in particular, shows concurrent H2D memory copies of the batch input data, which is an improvement over the native dataloader. \nRunning this case using benchy on Perlmutter results in the following throughput measurements: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9267236871065568,
        0.9827412709765714
      ],
      "excerpt": "One thing we can notice here is that the SYNTHETIC speed is increased from previous cases. This is because the synthetic data sample that \nis cached and reused from the DALI data loader is already resident on the GPU, in contrast to the case using the PyTorch dataloader where \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9508725416574182
      ],
      "excerpt": "In general, we now see that the IO throughput is greater than the SYNTHETIC, meaning the data loader can keep up with the compute \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.909519687755623
      ],
      "excerpt": "Now that the data loading performance is faster than the synthetic compute throughput, we can start looking at improving compute performance. As a first step to improve the compute performance of this training script, we can enable automatic mixed precision (AMP) in PyTorch. AMP provides a simple way for users to convert existing FP32 training scripts to mixed FP32/FP16 precision, unlocking \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9132512217137818
      ],
      "excerpt": "As a quick note, the A100 GPUs we've been using to report results thus far have been able to benefit from Tensor Core compute via the use of TF32 precision operations, enabled by default for CUDNN and CUBLAS in PyTorch. We can measure the benefit of TF32 precision usage on the A100 GPU by temporarily disabling it via setting the environment variable NVIDIA_TF32_OVERRIDE=0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761968568493606
      ],
      "excerpt": "2021-11-09 20:22:23,191 - root - INFO - Time taken for epoch 1 is 265.8437602519989 sec, avg 15.407546132048822 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9489224848829158
      ],
      "excerpt": "2021-11-09 20:23:00,372 - root - INFO - Time taken for epoch 2 is 30.84027910232544 sec, avg 132.81332462685626 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9489224848829158
      ],
      "excerpt": "2021-11-09 20:23:33,090 - root - INFO - Time taken for epoch 3 is 30.859854459762573 sec, avg 132.7290770389302 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874863136041407,
        0.8971919770928616,
        0.9637301353584695
      ],
      "excerpt": "From here, we can see that running in FP32 without TF32 acceleration is much slower and we are already seeing great performance from \nTF32 Tensor Core operations without any code changes to add AMP. With that said, AMP can still be a useful improvement for A100 GPUs, \nas TF32 is a compute type only, leaving all data in full precision FP32. FP16 precision has the compute benefits of Tensor Cores combined with a reduction in storage and memory bandwidth requirements.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9632316594703139,
        0.9761968568493606
      ],
      "excerpt": "This is the performance of the training script for the first three epochs on a 40GB A100 card with batch size 64, DALI, and AMP: \n2021-11-09 20:15:47,161 - root - INFO - Time taken for epoch 1 is 262.75472021102905 sec, avg 15.588682847297035 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329631397748734
      ],
      "excerpt": "2021-11-09 20:16:00,747 - root - INFO - Time taken for epoch 2 is 8.379497528076172 sec, avg 488.8121258197197 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329631397748734
      ],
      "excerpt": "2021-11-09 20:16:09,708 - root - INFO - Time taken for epoch 3 is 7.986395835876465 sec, avg 512.8721496122143 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138084663740083,
        0.9313038037935661,
        0.9963939282708814,
        0.8430064163608646
      ],
      "excerpt": "argument and load that profile in Nsight Systems. This is what this profile (dali_amp.qdrep) looks like: \nand zoomed in to a single iteration: \nWith AMP enabled, we see that the forward (and, correspondingly the backward) time is significatly reduced. As this is a CNN, the forward and backward convolution ops are well-suited to benefit from acceleration with tensor cores and that is where we see the most benefit. \nRunning this case using benchy on Perlmutter results in the following throughput measurements: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9000705767579383
      ],
      "excerpt": "From these results, we can see a big improvement in the SYNTHETIC and FULL throughput from using mixed-precision training over \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8531405795976275
      ],
      "excerpt": "APEX, apex.optimizers.FusedAdam. This fused optimizer uses fewer kernels to perform the weight \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326280377686921
      ],
      "excerpt": "reuse. We can enabled the use of the FusedAdam optimizer in our training script by adding the flag --enable_apex.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9711292158180406,
        0.9761968568493606
      ],
      "excerpt": "This is the performance of the training script for the first three epochs on a 40GB A100 card with batch size 64, DALI, and AMP, and APEX: \n021-11-09 20:22:21,666 - root - INFO - Time taken for epoch 1 is 262.5772747993469 sec, avg 15.599217423251996 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761968568493606
      ],
      "excerpt": "2021-11-09 20:22:36,945 - root - INFO - Time taken for epoch 2 is 8.411178827285767 sec, avg 486.97098041865706 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761968568493606
      ],
      "excerpt": "2021-11-09 20:22:45,729 - root - INFO - Time taken for epoch 3 is 7.669205904006958 sec, avg 534.0839783503462 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9313941914542199
      ],
      "excerpt": "While APEX provides some already fused kernels, for more general fusion of eligible pointwise operations in PyTorch, we can enable \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9711292158180406,
        0.9488899790803059
      ],
      "excerpt": "This is the performance of the training script for the first three epochs on a 40GB A100 card with batch size 64, DALI, and AMP, APEX and JIT: \n2021-11-09 20:22:39,140 - root - INFO - Time taken for epoch 1 is 278.4678325653076 sec, avg 14.709059794327901 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761968568493606
      ],
      "excerpt": "2021-11-09 20:22:54,241 - root - INFO - Time taken for epoch 2 is 8.250621557235718 sec, avg 496.44744599973154 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761968568493606
      ],
      "excerpt": "2021-11-09 20:23:02,919 - root - INFO - Time taken for epoch 3 is 7.557044267654419 sec, avg 542.0108517203818 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9313038037935661,
        0.9229573842430454
      ],
      "excerpt": "and zoomed in to a single iteration: \nRunning this case with APEX and JIT enabled using benchy on Perlmutter results in the following throughput measurements: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9556451349771998
      ],
      "excerpt": "We see a modest gain in the SYNTHETIC throughput, resuling in a slight increase in the FULL throughput. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9204983610005071
      ],
      "excerpt": "PyTorch's new CUDA Graphs functionality to the existing model and training loop. Our tutorial model configuration does not benefit \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.926831397236234
      ],
      "excerpt": "Now you can run the full model training on a single GPU with our optimizations. For convenience, we provide a configuration with the optimizations already enabled. Submit the full training with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.889291152409029,
        0.849911002887239
      ],
      "excerpt": "Now that we have model training code that is optimized for training on a single GPU, \nwe are ready to utilize multiple GPUs and multiple nodes to accelerate the workflow \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9544727918481675,
        0.8853605079037398
      ],
      "excerpt": "wrapper in PyTorch with the NCCL backend for optimized communication operations on \nsystems with NVIDIA GPUs. Refer to the PyTorch documentation for additional details  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879263094779006
      ],
      "excerpt": "with larger learning rates. The base config uses a batchsize of 64 for single-GPU training, so we will set base_batch_size=64 in our configs and then increase the global_batch_size parameter in increments of 64 for every additional GPU we add to the distributed training. Then, we can take the ratio of global_batch_size and base_batch_size to decide how much to scale up the learning rate as the global batch size grows. In this section, we will make use of the square-root scaling rule, which multiplies the base initial learning rate by sqrt(global_batch_size/base_batch_size). Take a look at utils/__init__.py to see how this is implemented. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.966103833632166,
        0.9452360210393107
      ],
      "excerpt": "Looking at the TensorBoard log, we can see that the rate of convergence is increased initially, but the validation loss plateaus quickly and our final accuracy ends up worse than the single-GPU training: \nFrom the plot, we see that with a global batch size of 512 we complete each epoch in a much shorter amount of time, so training concludes rapidly. This affects our learning rate schedule, which depends on the total number of steps as set in train.py: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.830171746932503
      ],
      "excerpt": "If we increase the total number of epochs, we will run longer (thus giving the model more training iterations to update weights) and the learning rate will decay more slowly, giving us more time to converge quickly with a larger learning rate. To try this out, run the bs512_opt config, which runs for 40 epochs rather than the default 10: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9888508316016533,
        0.9714213379268429,
        0.991397510050249,
        0.9887422018262272,
        0.9616686414023266,
        0.8429058296088466,
        0.9421486138618003
      ],
      "excerpt": "With the longer training, we can see that our higher batch size results are slightly better than the baseline configuration. Furthermore, the minimum in the loss is reached sooner, despite running for more epochs: \nBased on our findings, we can strategize to have trainings with larger batch sizes run for half as many total iterations as the baseline, as a rule of thumb. You can see this imlemented in the different configs for various global batchsizes: bs256_opt, bs512_opt, bs2048_opt. However, to really compare how our convergence is improving between these configurations, we must consider the actual time-to-solution. To do this in TensorBoard, select the \"Relative\" option on the left-hand side, which will change the x-axis in each plot to show walltime of the job (in hours), relative to the first data point: \nWith this selected, we can compare results between these different configs as a function of time, and see that all of them improve over the baseline. Furthermore, the rate of convergence improves as we add more GPUs and increase the global batch size: \nBased on our study, we see that scaling up our U-Net can definitely speed up training and reduce time-to-solution. Compared to our un-optimized single-GPU baseline from the first section, which took around 2 hours to train, we can now converge in about 10 minutes, which is a great speedup! We have also seen that there are several considerations to be aware of and several key hyperparameters to tune. We encourage you to now play with some of these settings and observe how they can affect the results. The main parameters in config/UNet.yaml to consider are: \nnum_epochs, to adjust how long it takes for learning rate to decay and for training to conclude. \nlr_schedule, to choose how to scale up learning rate, or change the start and end learning rates. \nglobal_batch_size. We ask that you limit yourself to a maximum of 8 GPUs initially for this section, to ensure everyone gets sufficient access to compute resources. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.926907144053615,
        0.888258654900662
      ],
      "excerpt": "* What are the limitations to scaling up batch size and learning rates? \n* What would happen to the learning curves and runtime if we did \"strong scaling\" instead (hold global batch size fixed as we increase GPUs, and respectively decrease the local batch size)? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978680700752142
      ],
      "excerpt": "the performance of our application as we scale. Then we can go in more details and profile  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8727466644842794,
        0.9639623823907797
      ],
      "excerpt": "The plot shows the throughput as we scale up to 32 nodes. The solid green line shows the real data throughput, while the dotted green line shows the ideal throughput, i.e. if we multiply the single GPU throughput by the number of GPUs used. For example for 32 nodes we get around 78% scaling efficiency. The blue lines show the data throughput by running the data-loader in isolation. The orange lines show the throughput for synthetic data. \nNext we can further breakdown the performance of the applications, by switching off the communication between workers. An example command to generate the points for 8 nodes and adding the noddp flag is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9306222482672034,
        0.9838404226494145
      ],
      "excerpt": "The orange line is with synthetic data, so no I/O overhead, and the orange dotted line is with synthetic data but having the communication between compute switched off. That effectively makes the dotted orange line the compute of the application. By comparing it with the solid orange line we can get the communication overhead. For example in this case for 32 nodes the communication overhead is around 25%. \nOne thing we can do to improve communication is to make sure that we are using the full compute capabilities of our GPU. Because Pytorch is optimizing the overlap between communication and compute, increasing the compute performed between communication will lead to better throughput. In the following plot we increased the local batch size from 64 to 128 and we can see the scaling efficiency increased to around 89% for 32 nodes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9792935723937705
      ],
      "excerpt": "Also to understand better the reason for this improvement we can look at the following plot of the communication overhead. The blue lines are with batch size of 128 and the orange lines with batch size 64. The difference between the solid and dotted lines is smaller for larger batch size as expected. For example for 32 nodes we see an improvement in the communication overhead from 25% for batch size 64, to 12% for batch size 128. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9014414236365477
      ],
      "excerpt": "Using the optimized options for compute and I/O, we profile the communication baseline with  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8613000045334006,
        0.8835527709310815
      ],
      "excerpt": "Considering both the case of strong scaling and large-batch training limitation, the  \nlocal_batch_size, i.e. per GPU batch size, is set to 16 to show the effect of communication. Loading this profile (4gpu_baseline.qdrep) in Nsight Systems will look like this:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9359752686735999
      ],
      "excerpt": "By default, for our model there are 8 NCCL calls per iteration, as shown in zoomed-in view: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8635776017000497
      ],
      "excerpt": "2021-11-10 04:05:27,672 - root - INFO - Time taken for epoch 4 is 51.53450584411621 sec, avg 1271.6916350810875 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "The tuning knobs  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276056502651254
      ],
      "excerpt": "additional communication (syncing buffers) and is enabled by default, which is often not necessary. bucket_cap_mb  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8380086255877267
      ],
      "excerpt": "calls per iteration. The proper bucket size depends on the overlap between communication and computation, and requires  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897534778157955
      ],
      "excerpt": "Since there is no batch norm layer in our model, it's safe to disable the broadcast_buffers with the added knob --disable_broadcast_buffers: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8864952311972439
      ],
      "excerpt": "The per step timing is slightly improved comparing to the baseline.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8635776017000497
      ],
      "excerpt": "2021-11-10 04:12:07,932 - root - INFO - Time taken for epoch 2 is 62.6831419467926 sec, avg 1045.5123652804289 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8989497220591622
      ],
      "excerpt": "2021-11-10 04:13:01,406 - root - INFO - Time taken for epoch 3 is 50.23048114776611 sec, avg 1304.705798202663 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051862520897684
      ],
      "excerpt": "2021-11-10 04:13:54,472 - root - INFO - Time taken for epoch 4 is 49.83389210700989 sec, avg 1315.088933035222 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9735897446958802
      ],
      "excerpt": "To show the effect of the message bucket size, we add another knob to the code, --bucket_cap_mb. The current  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8554257905788453
      ],
      "excerpt": "2021-11-10 04:19:48,472 - root - INFO - Time taken for epoch 2 is 59.066428899765015 sec, avg 1109.5304256706254 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051862520897684
      ],
      "excerpt": "2021-11-10 04:20:41,475 - root - INFO - Time taken for epoch 3 is 49.75986886024475 sec, avg 1317.0452716437817 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051862520897684
      ],
      "excerpt": "2021-11-10 04:21:34,517 - root - INFO - Time taken for epoch 4 is 49.78394103050232 sec, avg 1316.4084370067546 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8121964839650235
      ],
      "excerpt": "Similarly, to understand the cross node performance, we run the baseline and optimized options with 2 nodes on Perlmutter.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833449966539778,
        0.8435816155977078
      ],
      "excerpt": "and the performance of the run:  \n2021-11-10 02:41:30,680 - root - INFO - Time taken for epoch 2 is 44.45261096954346 sec, avg 1474.2891040731388 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8842748318078084
      ],
      "excerpt": "2021-11-10 02:42:08,002 - root - INFO - Time taken for epoch 3 is 35.54009485244751 sec, avg 1844.0018315113414 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8842748318078084
      ],
      "excerpt": "2021-11-10 02:42:45,635 - root - INFO - Time taken for epoch 4 is 35.94469451904297 sec, avg 1823.245429594067 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833449966539778,
        0.8842748318078084
      ],
      "excerpt": "and the performance of the run: \n2021-11-10 02:41:28,509 - root - INFO - Time taken for epoch 2 is 43.84619975090027 sec, avg 1494.67913689953 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8842748318078084
      ],
      "excerpt": "2021-11-10 02:42:05,129 - root - INFO - Time taken for epoch 3 is 34.85356664657593 sec, avg 1880.3240616534827 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135314100215508
      ],
      "excerpt": "2021-11-10 02:42:42,100 - root - INFO - Time taken for epoch 4 is 35.293962717056274 sec, avg 1856.8614843673777 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.964402579900782
      ],
      "excerpt": "Note that the batch size is set to a small value to tune the knobs at smaller scale. To have a better scaliing efficiency, we \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9845574852095925,
        0.9529570822018031,
        0.9052732611592741,
        0.8840254789311635
      ],
      "excerpt": "With all of our multi-GPU settings and optimizations in place, we now leave it to you to take what you've learned and try to achieve the best performance on this problem. Specifically, try to further tune things to either reach the lowest possible validation loss, or converge to the single-GPU validation loss (~4.7e-3) in the shortest amount of time. Some ideas for things to adjust are: \n* Further tune num_epochs to adjust how long it takes for learning rate to decay, and for training to conclude. \n* Play with the learning rate: try out a different scaling rule, such as linear scale-up of learning rate, or come up with your own learning rate schedule. \n* Change other components, such as the optimizer used. Here we have used the standard Adam optimizer, but many practitioners also use the SGD optimizer (with momentum) in distributed training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Material for the SC21 Deep Learning at Scale Tutorial",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NERSC/sc21-dl-tutorial/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Mon, 27 Dec 2021 06:15:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/NERSC/sc21-dl-tutorial/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "NERSC/sc21-dl-tutorial",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/NERSC/sc21-dl-tutorial/main/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/NERSC/sc21-dl-tutorial/main/start_tensorboard.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/NERSC/sc21-dl-tutorial/main/export_DDP_vars.sh",
      "https://raw.githubusercontent.com/NERSC/sc21-dl-tutorial/main/submit_cgpu.sh",
      "https://raw.githubusercontent.com/NERSC/sc21-dl-tutorial/main/submit_pm.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In this tutorial, we will be generating profile files using NVIDIA Nsight Systems on the remote systems. In order to open and view these\nfiles on your local computer, you will need to install the Nsight Systems program, which you can download [here](https://developer.nvidia.com/gameworksdownload#?dn=nsight-systems-2021-4-1-73). Select the download option required for your system (e.g. Mac OS host for MacOS, Window Host for Windows, or Linux Host .rpm/.deb/.run for Linux). You may need to sign up and create a login to NVIDIA's developer program if you do not\nalready have an account to access the download. Proceed to run and install the program using your selected installation method.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9305495786425573
      ],
      "excerpt": "NERSC JupyterHub: https://jupyter.nersc.gov \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8379542744984202
      ],
      "excerpt": "Training account setup instructions will be given during the session. Once you have your provided account credentials, you can log in to Jupyter via the link (leave the OTP field blank when logging into Jupyter). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8463494993625212,
        0.9771278327632522
      ],
      "excerpt": "To begin, start a terminal from JupyterHub and clone this repository with: \ngit clone https://github.com/NERSC/sc21-dl-tutorial.git \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8959986210571929
      ],
      "excerpt": "cd sc21-dl-tutorial \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8804249547989114,
        0.8109392474892433
      ],
      "excerpt": "sbatch -n 1 ./submit_pm.sh --config=short --num_epochs 3 \nsubmit_pm.sh is a batch submission script that defines resources to be requested by SLURM as well as the command to run. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9002477172796869
      ],
      "excerpt": "For interactive jobs, you can run the Python script directly using the following command (NOTE: please don't run training on the Perlmutter login nodes): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8425458266058042
      ],
      "excerpt": "For V100 systems, you will likely need to update the config to reduce the local batch size to 32 due to the reduced memory capacity. Otherwise, instructions are the same. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8330378500606573
      ],
      "excerpt": "To generate a profile using our scripts on Perlmutter, run the following command:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9667670016354323
      ],
      "excerpt": "To run using using benchy on Perlmutter, use the following command:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142607429777735
      ],
      "excerpt": "The PyTorch dataloader has several knobs we can adjust to improve performance. If you look at the DataLoader initialization in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8397551242878919
      ],
      "excerpt": "respawn them. One knob we've left to adjust is the num_workers argument, which we can control via the --num_data_workers command \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8729579101477549
      ],
      "excerpt": "We can run this experiment on Perlmutter by running the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8140197797200897
      ],
      "excerpt": "We can run the 8 worker configuration through profiler using the instructions in the previous section with the added --num_data_workers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8605282362313387
      ],
      "excerpt": "using the tool yields the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "PyTorch alone: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9142770433642953,
        0.8804249547989114
      ],
      "excerpt": "We can run this experiment on Perlmutter using DALI with 4 worker threads by running the following command: \nsbatch -n 1 ./submit_pm.sh --config=short --num_epochs 3 --num_data_workers 4 --data_loader_config=dali-lowmem \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8729579101477549,
        0.8804249547989114
      ],
      "excerpt": "We can run this experiment on Perlmutter by running the following command: \nNVIDIA_TF32_OVERRIDE=0 sbatch -n 1 ./submit_pm.sh --config=short --num_epochs 3 --num_data_workers 4 --data_loader_config=dali-lowmem \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9025403213817215,
        0.8804249547989114
      ],
      "excerpt": "We can run this experiment using AMP on Perlmutter by running the following command: \nsbatch -n 1 ./submit_pm.sh --config=short --num_epochs 3 --num_data_workers 4 --data_loader_config=dali-lowmem --enable_amp \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9025403213817215,
        0.8804249547989114
      ],
      "excerpt": "We can run this experiment using APEX on Perlmutter by running the following command: \nsbatch -n 1 ./submit_pm.sh --config=short --num_epochs 3 --num_data_workers 4 --data_loader_config=dali-lowmem --enable_amp --enable_apex \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9025403213817215,
        0.8804249547989114
      ],
      "excerpt": "We can run this experiment using JIT on Perlmutter by running the following command: \nsbatch -n 1 ./submit_pm.sh --config=short --num_epochs 3 --num_data_workers 4 --data_loader_config=dali-lowmem --enable_amp --enable_apex --enable_jit \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8804249547989114
      ],
      "excerpt": "sbatch -n 1 -t 40 ./submit_pm.sh --config=bs64_opt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9124541833753296
      ],
      "excerpt": "sbatch -t 20 -n 8 submit_pm.sh --config=bs512_opt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8270459217980545
      ],
      "excerpt": "You should also consider the following questions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979549995370801
      ],
      "excerpt": "ENABLE_PROFILING=1 PROFILE_OUTPUT=4gpu_baseline sbatch -n 4 ./submit_pm.sh --config=bs64_opt --num_epochs 4 --num_data_workers 8 --local_batch_size 16 --enable_manual_profiling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979549995370801
      ],
      "excerpt": "ENABLE_PROFILING=1 PROFILE_OUTPUT=4gpu_nobroadcast sbatch -n 4 ./submit_pm.sh --config=bs64_opt --num_epochs 4 --num_data_workers 8 --local_batch_size 16 --enable_manual_profiling --disable_broadcast_buffers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8376127473655874
      ],
      "excerpt": "ENABLE_PROFILING=1 PROFILE_OUTPUT=4gpu_bucket100mb sbatch -n 4 ./submit_pm.sh --config=bs64_opt --num_epochs 4 --num_data_workers 8 --local_batch_size 16 --enable_manual_profiling --disable_broadcast_buffers --bucket_cap_mb 100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979549995370801
      ],
      "excerpt": "ENABLE_PROFILING=1 PROFILE_OUTPUT=8gpu_baseline sbatch -N 2 ./submit_pm.sh --config=bs64_opt --num_epochs 4 --num_data_workers 8 --local_batch_size 16 --enable_manual_profiling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8376127473655874
      ],
      "excerpt": "ENABLE_PROFILING=1 PROFILE_OUTPUT=8gpu_bucket100mb sbatch -N 2 ./submit_pm.sh --config=bs64_opt --num_epochs 4 --num_data_workers 8 --local_batch_size 16 --enable_manual_profiling --disable_broadcast_buffers --bucket_cap_mb 100 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.841989441779451
      ],
      "excerpt": "The basic data loading pipeline is defined in utils/data_loader.py, whose primary components are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8827115547830371,
        0.8799668488570505,
        0.8451825415838722
      ],
      "excerpt": "The script to train the model is train.py, which uses the following arguments to load the desired training setup: \n--yaml_config YAML_CONFIG   path to yaml file containing training configs \n--config CONFIG             name of desired config in yaml file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123113744741721
      ],
      "excerpt": "Besides the train.py script, we have a slightly more complex train_graph.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8291726646438298
      ],
      "excerpt": "Note that any arguments for train.py, such as the desired config (--config), can be added after submit_pm.sh when submitting, and they will be passed to train.py properly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317135315960992
      ],
      "excerpt": "python train.py --config=short --num_epochs 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811538344720883
      ],
      "excerpt": "This will run 3 epochs of training on a single GPU using a default batch size of 64. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8620290104663516
      ],
      "excerpt": "To copy the example TensorBoard log to the scratch directory where our training jobs will output their logs, do \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276014641942563
      ],
      "excerpt": "nsys profile -o baseline --trace=cuda,nvtx -c cudaProfilerApi --kill none -f true python train.py --config=short --num_epochs 2 --enable_manual_profiling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9408327980820074
      ],
      "excerpt": "python train.py --config=short ---num_epochs 10 -enable_benchy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167660398476358
      ],
      "excerpt": "python train.py --config=short --num_epochs 3 --num_data_workers &lt;value of your choice&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009138317445168
      ],
      "excerpt": "2021-11-09 00:21:23,185 - root - INFO -   Total validation time: 5.7792582511901855 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317135315960992
      ],
      "excerpt": "python train.py --config=short --num_epochs 3 --num_data_workers 4 --data_loader_config=dali-lowmem \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8328904896712951,
        0.8317269352542317
      ],
      "excerpt": "2021-11-09 19:45:15,642 - root - INFO - Time taken for epoch 1 is 252.39585137367249 sec, avg 16.22847593455831 samples/sec \n2021-11-09 19:45:15,643 - root - INFO -   Avg train loss=0.067969 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8425342077358209
      ],
      "excerpt": "2021-11-09 19:45:20,927 - root - INFO -   Total validation time: 5.283399343490601 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8096594810726311
      ],
      "excerpt": "2021-11-09 19:45:32,451 - root - INFO -   Total validation time: 1.127028226852417 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8250826144833867
      ],
      "excerpt": "2021-11-09 19:45:43,599 - root - INFO -   Total validation time: 0.7794735431671143 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8125694586952884
      ],
      "excerpt": "2021-11-09 20:22:23,191 - root - INFO - Time taken for epoch 1 is 265.8437602519989 sec, avg 15.407546132048822 samples/sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181459441302915
      ],
      "excerpt": "2021-11-09 20:23:02,228 - root - INFO -   Total validation time: 1.8474531173706055 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8506270497138819
      ],
      "excerpt": "2021-11-09 20:23:34,917 - root - INFO -   Total validation time: 1.8037540912628174 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317135315960992
      ],
      "excerpt": "python train.py --config=short --num_epochs 3 --num_data_workers 4 --data_loader_config=dali-lowmem --enable_amp \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828006500162776
      ],
      "excerpt": "2021-11-09 20:15:52,365 - root - INFO -   Total validation time: 5.2027716636657715 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8636726037987268
      ],
      "excerpt": "2021-11-09 20:16:01,719 - root - INFO -   Total validation time: 0.9710891246795654 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8225477051439468
      ],
      "excerpt": "2021-11-09 20:16:10,394 - root - INFO -   Total validation time: 0.6851544380187988 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317135315960992
      ],
      "excerpt": "python train.py --config=short --num_epochs 3 --num_data_workers 4 --data_loader_config=dali-lowmem --enable_amp --enable_apex \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8065957515626428
      ],
      "excerpt": "2021-11-09 20:22:45,759 - root - INFO -   Avg train loss=0.023003 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317135315960992
      ],
      "excerpt": "python train.py --config=short --num_epochs 3 --num_data_workers 4 --data_loader_config=dali-lowmem --enable_amp --enable_apex --enable_jit \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471990260717436
      ],
      "excerpt": "2021-11-09 20:22:45,988 - root - INFO -   Total validation time: 6.814955234527588 sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060372108626078
      ],
      "excerpt": "params.lr_schedule['tot_steps'] = params.num_epochs*(params.Nsamples//params.global_batch_size) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img src=\"tutorial_images/scale_perfEff.png\" width=\"500\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img src=\"tutorial_images/scale_perfComm.png\" width=\"500\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img src=\"tutorial_images/scale_perfEff_bs128.png\" width=\"500\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img src=\"tutorial_images/scale_perfDiffBS.png\" width=\"500\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8305884151131088
      ],
      "excerpt": "default value in PyTorch is 25 mb. We profile a run with 100 mb bucket size with following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.843109772277641
      ],
      "excerpt": "the total number of NCCL calls per step now reduced to 5.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8015230560947847,
        0.8162135468920809
      ],
      "excerpt": "2021-11-10 02:42:45,635 - root - INFO - Time taken for epoch 4 is 35.94469451904297 sec, avg 1823.245429594067 samples/sec \n2021-11-10 02:42:45,644 - root - INFO -   Avg train loss=0.006166 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033212599347294
      ],
      "excerpt": "2021-11-10 02:42:42,123 - root - INFO -   Avg train loss=0.006195 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8248561816334002
      ],
      "excerpt": " want to increase the per GPU compute intensity by increasing the per GPU batch size. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/NERSC/sc21-dl-tutorial/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Jupyter Notebook",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SC21 Deep Learning at Scale Tutorial",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sc21-dl-tutorial",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "NERSC",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NERSC/sc21-dl-tutorial/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Mon, 27 Dec 2021 06:15:40 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository contains the example code material for the SC21 tutorial:\n*Deep Learning at Scale*.\n\n**Contents**\n* [Links](#links)\n* [Installation](#installation-and-setup)\n* [Model, data, and code overview](#model-data-and-training-code-overview)\n* [Single GPU training](#single-gpu-training)\n* [Single GPU performance](#single-gpu-performance-profiling-and-optimization)\n* [Distributed training](#distributed-gpu-training)\n* [Multi GPU performance](#multi-gpu-performance-profiling-and-optimization)\n* [Putting it all together](#putting-it-all-together)\n\n",
      "technique": "Header extraction"
    }
  ]
}