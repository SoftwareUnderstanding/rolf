{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1803.09820",
      "https://arxiv.org/abs/1901.09891v2",
      "https://arxiv.org/abs/1712.01034v2",
      "https://arxiv.org/abs/1710.09412",
      "https://arxiv.org/abs/1901.09891v2](https://arxiv.org/abs/1901.09891v2) and [https://arxiv.org/abs/1712.01034v2](https://arxiv.org/abs/1712.01034v2) suggests modification in data augmentation and normalization layers, were built on top of ResNet to obtain the best results. \n\n2. **ResNet-152** provides the best accuracy (2-3% increase) over **ResNet-50** in the expense of increased training time ( 2 minutes/epoch increase).\n\n3. Several Transfer Learning steps are used to achieve the best performing model (in order) : \n* Transfer Learning from model trained with **ImageNet images** to **Mixed-Up Stanford Car's dataset**.\n* Transfer Learning from model trained with **Mixed-Up Stanford Car's dataset** to **vanilla Stanford Car's dataset**.\n\n4. Training data are **augmented** with several transformations to improve variety of the dataset. This helps model to generalize better. Details of data augmentation are explained in the *Stanford Car Model Training.ipynb* notebook.\n\n5. Images with **higher resolution** train better model. However that comes with the expense of training time. Due to time constraint I am not able to train images with higher resolution than 299x299.\n\n6. Training with images **squished** to target resolution train better model. Automatic cropping risks deleting important features that are out of the cropping boundary. Padding introduce artefacts that lowers the training accuracy. Squished Image preserve most features, except in the scenario where the model/make of a car is mostly determined by the width:height ratio (aspect ratio) of a car.\n\n7. Instead of using squared Image, I have experimented on resizing the dataset to **rectangular image** with 16:9 and 4:3 aspect ratios. The aim is to preserve features that is determined by the aspect ratio of a car. It shows a slight increase in accuracy (0.3%). However, this is only achievable because of the dataset provided are mostly in landscape. \n\n8. Considering most **Grab** users are **mobile**, images taken are usually in portrait. Resizing a portrait image to landscape will severely distort the features of a car. Therefore, I have decided not to select a \"rectangular\" model as our final model.\n  \n9. Training with images cropped with **bounding box** produces significantly worse results. The model trained was not able to distinguish the noise in the background and the car in the foreground well enough in the test dataset.\n\n10. Augmenting data with **[mixup](https://arxiv.org/abs/1710.09412)** yields over 2-3% increase of accuracy. \n\n## Evaluation with Custom Dataset\n### Prerequisites\n* Linux Based Operating System (fast.ai does not support MacOS in their current build)\n* Use of Virtual Environment such as `conda` or `virtualenv`\n* 10 GB of free disk space (To be safe). Pytorch, Fast.ai, and their dependencies takes up good amount of disk space.\n* (Optional) [Git Large File Storage](https://git-lfs.github.com/). Used for hosting model files (They are huge).\n* (Optional) GPU in machine. This will speed up the prediction by a huge margin if you are running inference on a large dataset.  \n\n### Downloading Model File\n#### With Git LFS\nBefore cloning the repository, run:\n```\ngit lfs install\n```\nin the repository directory to initialize Git LFS. Then, clone repository as usual.\n\n**OR**\n\nIf you cloned the repository before initializing, run:\n```\ngit lfs install\ngit lfs pull\n```\nin the repository directory to download the model file.\n#### Manual download\nDownload the `best-model.pkl` manually from github and replace the file in your local repository.\n\n### Setting up virtual environment\nSetup a `python >= 3.6.0` virtual environement with `conda` or `virtualenv`\n#### Installing dependencies\nwith `pip`\n```\npip install -r requirements.txt\n```\n\n### Running test script\n0. Activate virtual environment\n\n#### Generate a .csv with predictions based on unlabelled images in a folder\n1. Create a fresh directory and place all the test images in the folder. (Make sure there is nothing else other than images in the folder)\n2. Run `python predict.py generate_csv_for_test_data --img_path=<your_test_folder_path> --output_fpath=<output_file_path>` in terminal.\nExample:\n###### See `test_images` folder as sample\n```\npython predict.py generate_csv_for_test_data --img_path=test_images --output_fpath=test.csv\n```\nThis will output a csv file with predictions and probability on each images.\n\n#### Populate an existing .csv with predictions based on labelled images in a folder\n1. Create a fresh directory and place all the test images in the folder. (Make sure there is nothing else other than images in the folder)\n2. Create a csv file with two columns, `fname` for image filenames and `label` for labels of the image.\n\n| fname  | label |\n| ------------- | ------------- |  \n| 00001.jpg | Suzuki Aerio Sedan 2007  |  \n| 00002.jpg | Ferrari 458 Italia Convertible 2012  |  \n| 00003.jpg | Jeep Patriot SUV 2012  | \n| 00004.jpg | Toyota Camry Sedan 2012  | \n| 00005.jpg | Tesla Model S Sedan 2012  | \n\n**IMPORTANT** : `fname` in the csv files should match exact the filename of images in the folder. (Filename only, not path)\n\n3. Run `python predict.py populate_csv_for_labelled_data --csv_path=<your_csv_path> --img_path=<your_test_folder_path> --output_fpath=<output_file_path>` in terminal.\nExample:\n###### See `test_images` folder and `data_with_labels.csv` as sample\n```\npython predict.py populate_csv_for_labelled_data --csv_path=data_with_labels.csv --img_path=test_images --output_fpath=labelled.csv\n```\n\nThis will populate the csv file with predictions and probability for each image. It will also output performance metrics: Accuracy, Recall, Precision, and F1-Score in the terminal.\n",
      "https://arxiv.org/abs/1712.01034v2](https://arxiv.org/abs/1712.01034v2) suggests modification in data augmentation and normalization layers, were built on top of ResNet to obtain the best results. \n\n2. **ResNet-152** provides the best accuracy (2-3% increase) over **ResNet-50** in the expense of increased training time ( 2 minutes/epoch increase).\n\n3. Several Transfer Learning steps are used to achieve the best performing model (in order) : \n* Transfer Learning from model trained with **ImageNet images** to **Mixed-Up Stanford Car's dataset**.\n* Transfer Learning from model trained with **Mixed-Up Stanford Car's dataset** to **vanilla Stanford Car's dataset**.\n\n4. Training data are **augmented** with several transformations to improve variety of the dataset. This helps model to generalize better. Details of data augmentation are explained in the *Stanford Car Model Training.ipynb* notebook.\n\n5. Images with **higher resolution** train better model. However that comes with the expense of training time. Due to time constraint I am not able to train images with higher resolution than 299x299.\n\n6. Training with images **squished** to target resolution train better model. Automatic cropping risks deleting important features that are out of the cropping boundary. Padding introduce artefacts that lowers the training accuracy. Squished Image preserve most features, except in the scenario where the model/make of a car is mostly determined by the width:height ratio (aspect ratio) of a car.\n\n7. Instead of using squared Image, I have experimented on resizing the dataset to **rectangular image** with 16:9 and 4:3 aspect ratios. The aim is to preserve features that is determined by the aspect ratio of a car. It shows a slight increase in accuracy (0.3%). However, this is only achievable because of the dataset provided are mostly in landscape. \n\n8. Considering most **Grab** users are **mobile**, images taken are usually in portrait. Resizing a portrait image to landscape will severely distort the features of a car. Therefore, I have decided not to select a \"rectangular\" model as our final model.\n  \n9. Training with images cropped with **bounding box** produces significantly worse results. The model trained was not able to distinguish the noise in the background and the car in the foreground well enough in the test dataset.\n\n10. Augmenting data with **[mixup](https://arxiv.org/abs/1710.09412)** yields over 2-3% increase of accuracy. \n\n## Evaluation with Custom Dataset\n### Prerequisites\n* Linux Based Operating System (fast.ai does not support MacOS in their current build)\n* Use of Virtual Environment such as `conda` or `virtualenv`\n* 10 GB of free disk space (To be safe). Pytorch, Fast.ai, and their dependencies takes up good amount of disk space.\n* (Optional) [Git Large File Storage](https://git-lfs.github.com/). Used for hosting model files (They are huge).\n* (Optional) GPU in machine. This will speed up the prediction by a huge margin if you are running inference on a large dataset.  \n\n### Downloading Model File\n#### With Git LFS\nBefore cloning the repository, run:\n```\ngit lfs install\n```\nin the repository directory to initialize Git LFS. Then, clone repository as usual.\n\n**OR**\n\nIf you cloned the repository before initializing, run:\n```\ngit lfs install\ngit lfs pull\n```\nin the repository directory to download the model file.\n#### Manual download\nDownload the `best-model.pkl` manually from github and replace the file in your local repository.\n\n### Setting up virtual environment\nSetup a `python >= 3.6.0` virtual environement with `conda` or `virtualenv`\n#### Installing dependencies\nwith `pip`\n```\npip install -r requirements.txt\n```\n\n### Running test script\n0. Activate virtual environment\n\n#### Generate a .csv with predictions based on unlabelled images in a folder\n1. Create a fresh directory and place all the test images in the folder. (Make sure there is nothing else other than images in the folder)\n2. Run `python predict.py generate_csv_for_test_data --img_path=<your_test_folder_path> --output_fpath=<output_file_path>` in terminal.\nExample:\n###### See `test_images` folder as sample\n```\npython predict.py generate_csv_for_test_data --img_path=test_images --output_fpath=test.csv\n```\nThis will output a csv file with predictions and probability on each images.\n\n#### Populate an existing .csv with predictions based on labelled images in a folder\n1. Create a fresh directory and place all the test images in the folder. (Make sure there is nothing else other than images in the folder)\n2. Create a csv file with two columns, `fname` for image filenames and `label` for labels of the image.\n\n| fname  | label |\n| ------------- | ------------- |  \n| 00001.jpg | Suzuki Aerio Sedan 2007  |  \n| 00002.jpg | Ferrari 458 Italia Convertible 2012  |  \n| 00003.jpg | Jeep Patriot SUV 2012  | \n| 00004.jpg | Toyota Camry Sedan 2012  | \n| 00005.jpg | Tesla Model S Sedan 2012  | \n\n**IMPORTANT** : `fname` in the csv files should match exact the filename of images in the folder. (Filename only, not path)\n\n3. Run `python predict.py populate_csv_for_labelled_data --csv_path=<your_csv_path> --img_path=<your_test_folder_path> --output_fpath=<output_file_path>` in terminal.\nExample:\n###### See `test_images` folder and `data_with_labels.csv` as sample\n```\npython predict.py populate_csv_for_labelled_data --csv_path=data_with_labels.csv --img_path=test_images --output_fpath=labelled.csv\n```\n\nThis will populate the csv file with predictions and probability for each image. It will also output performance metrics: Accuracy, Recall, Precision, and F1-Score in the terminal.\n"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jianshen92/stanford-car-grab-challenge",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-13T18:49:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-01T13:39:36Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8416632911848503,
        0.9968029537584643
      ],
      "excerpt": "This repository is an attempt for the Computer Vision Challenge by Grab.  \nTable of Contents: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9860762267155018,
        0.8865476870323988,
        0.8561142289523012
      ],
      "excerpt": "Model is built with fast.ai v1 and PyTorch v1, trained on Google Cloud Platform's Deep Learning VM with 16GB NVIDIA Tesla T4. \nData consist of 8144 Training Images (80:20 Train:Validation Split) and 8041 Test Images. Architecture used is ResNet-152 with squared image (299x299), pretrained with ImageNet. Data is augmented with several affine and perspective transformation. Mixup technique is used. Final Top-1 Accuracy is 92.53% on Test Images. \nStanford Car Model Training.ipynb is the notebook used to perform model training and evaluation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9743087705271459
      ],
      "excerpt": "Stopping Criteria for all models is when no improvement on validation loss across 2 Cycles of training. One cycle of training refers to training with any number of epochs with the One Cycle Policy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9639318402540334
      ],
      "excerpt": "299x299 image size yield better results. This criteria is applied to all further models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734,
        0.8150771532802734,
        0.8150771532802734,
        0.9674312045236604
      ],
      "excerpt": "| Resizing Method - Zero Padding | 86.0  |  -  | -  | \n| Resizing Method - Crop | 86.6  |  -  | -  | \n| Resizing Method - Squishing | 88.0  |  -  | -  | \nSquishing image yield better results. This criteria is applied to all further models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8396283666386003
      ],
      "excerpt": "Using Mix Up on training data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235308252586383,
        0.9485830975850603,
        0.8184627163740422
      ],
      "excerpt": "| With Mix Up | 89.3  |  90.9  | 92.53  | \nTraining done on Google Cloud Platform Deep Learning VM with GPU 16GB NVIDIA Tesla T4, with batch size of 16. \n|  | Resnet 50 |  Resnet 101 | Resnet 152 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9599212207315667,
        0.961402378376888,
        0.9125725925676924
      ],
      "excerpt": "I chose ResNet as the model architecture because it has achieved State-of-the-Art results for many fine-grained image classification problems since 2015. Recent breakthrough in fine-grained image classification such as arXiv:1901.09891v2 and arXiv:1712.01034v2 suggests modification in data augmentation and normalization layers, were built on top of ResNet to obtain the best results.  \nResNet-152 provides the best accuracy (2-3% increase) over ResNet-50 in the expense of increased training time ( 2 minutes/epoch increase). \nSeveral Transfer Learning steps are used to achieve the best performing model (in order) :  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9875763723571488,
        0.8643470594930205,
        0.9893393142225874,
        0.997330455272874,
        0.9720428549490513,
        0.8012264782727022,
        0.9867498565832756
      ],
      "excerpt": "Training data are augmented with several transformations to improve variety of the dataset. This helps model to generalize better. Details of data augmentation are explained in the Stanford Car Model Training.ipynb notebook. \nImages with higher resolution train better model. However that comes with the expense of training time. Due to time constraint I am not able to train images with higher resolution than 299x299. \nTraining with images squished to target resolution train better model. Automatic cropping risks deleting important features that are out of the cropping boundary. Padding introduce artefacts that lowers the training accuracy. Squished Image preserve most features, except in the scenario where the model/make of a car is mostly determined by the width:height ratio (aspect ratio) of a car. \nInstead of using squared Image, I have experimented on resizing the dataset to rectangular image with 16:9 and 4:3 aspect ratios. The aim is to preserve features that is determined by the aspect ratio of a car. It shows a slight increase in accuracy (0.3%). However, this is only achievable because of the dataset provided are mostly in landscape.  \nConsidering most Grab users are mobile, images taken are usually in portrait. Resizing a portrait image to landscape will severely distort the features of a car. Therefore, I have decided not to select a \"rectangular\" model as our final model. \nTraining with images cropped with bounding box produces significantly worse results. The model trained was not able to distinguish the noise in the background and the car in the foreground well enough in the test dataset. \nAugmenting data with mixup yields over 2-3% increase of accuracy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714284472580934
      ],
      "excerpt": "Create a csv file with two columns, fname for image filenames and label for labels of the image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "| 00005.jpg | Tesla Model S Sedan 2012  |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Computer Vision Challenge by Grab. Top 50 Finalist Entry.",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download the `best-model.pkl` manually from github and replace the file in your local repository.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jianshen92/stanford-car-grab-challenge/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sun, 26 Dec 2021 09:47:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jianshen92/stanford-car-grab-challenge/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jianshen92/stanford-car-grab-challenge",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jianshen92/stanford-car-grab-challenge/master/Stanford%20Car%20Model%20Training.ipynb",
      "https://raw.githubusercontent.com/jianshen92/stanford-car-grab-challenge/master/Cropping%20Stanford%20Car%20DS%20according%20to%20bounding%20box.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "with `pip`\n```\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Setup a `python >= 3.6.0` virtual environement with `conda` or `virtualenv`\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Download the `best-model.pkl` manually from github and replace the file in your local repository.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.999004679103593
      ],
      "excerpt": "git lfs install \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999004679103593,
        0.8504291753296438
      ],
      "excerpt": "git lfs install \ngit lfs pull \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8825175609196556
      ],
      "excerpt": "| Training Time per epoch | 3:30 minutes |  4:10 minutes  | 5:40 minutes  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8122401945474502
      ],
      "excerpt": "Create a fresh directory and place all the test images in the folder. (Make sure there is nothing else other than images in the folder) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586621293578539,
        0.8750615243095867,
        0.8122401945474502
      ],
      "excerpt": "python predict.py generate_csv_for_test_data --img_path=test_images --output_fpath=test.csv \nThis will output a csv file with predictions and probability on each images. \nCreate a fresh directory and place all the test images in the folder. (Make sure there is nothing else other than images in the folder) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579,
        0.8392237912831396,
        0.8392237912831396,
        0.8392237912831396,
        0.8129006625214237,
        0.8392968129241083
      ],
      "excerpt": "| 00001.jpg | Suzuki Aerio Sedan 2007  | \n| 00002.jpg | Ferrari 458 Italia Convertible 2012  | \n| 00003.jpg | Jeep Patriot SUV 2012  |  \n| 00004.jpg | Toyota Camry Sedan 2012  |  \n| 00005.jpg | Tesla Model S Sedan 2012  |  \nIMPORTANT : fname in the csv files should match exact the filename of images in the folder. (Filename only, not path) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.929382085335439
      ],
      "excerpt": "python predict.py populate_csv_for_labelled_data --csv_path=data_with_labels.csv --img_path=test_images --output_fpath=labelled.csv \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jianshen92/stanford-car-grab-challenge/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stanford Car Fine Grained Image Classification (Grab Challenge)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "stanford-car-grab-challenge",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jianshen92",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jianshen92/stanford-car-grab-challenge/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Linux Based Operating System (fast.ai does not support MacOS in their current build)\n* Use of Virtual Environment such as `conda` or `virtualenv`\n* 10 GB of free disk space (To be safe). Pytorch, Fast.ai, and their dependencies takes up good amount of disk space.\n* (Optional) [Git Large File Storage](https://git-lfs.github.com/). Used for hosting model files (They are huge).\n* (Optional) GPU in machine. This will speed up the prediction by a huge margin if you are running inference on a large dataset.  \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "with `pip`\n```\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "0. Activate virtual environment\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Sun, 26 Dec 2021 09:47:38 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "image-classification",
      "fastai",
      "python"
    ],
    "technique": "GitHub API"
  }
}