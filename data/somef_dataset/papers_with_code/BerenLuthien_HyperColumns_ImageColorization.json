{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1411.5752 and *much of these codes are based on*  https://tinyclouds.org/colorize/. \n- This project is to  illustrate hypercolumns and its usage in image colorization (which was proposed by Hypercolumns for Object Segmentation and Fine-grained Localization, Bharath Hariharan et al.",
      "https://arxiv.org/abs/1411.5752"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BerenLuthien/HyperColumns_ImageColorization",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-04-01T19:00:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-31T03:09:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In this project, from the pre-trained VGG model \"HyperColumns\" is harvested and is used to colorize gray images.\nThe major target of this project is to explore HyperColumns and how it can be used in such computer vision tasks as image auto-colorizations. The training data is flower data set which is separated into train, validation and test sets. The trained model is also tested on images that are not from the flower data set. The project is done in Tensorflow 1.0 and Python.\n![](pics/head.jpg)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.956568627983044,
        0.950881142057832,
        0.994294218716465,
        0.9707764762468718
      ],
      "excerpt": "This project is a revised version of https://arxiv.org/abs/1411.5752 and much of these codes are based on  https://tinyclouds.org/colorize/.  \n- This project is to  illustrate hypercolumns and its usage in image colorization (which was proposed by Hypercolumns for Object Segmentation and Fine-grained Localization, Bharath Hariharan et al.) \nThe project is different from the above two links in that it made trials and validated that a careful sampling of feature maps (i.e., a subset of feature maps) give comparable performance of using all feature maps, so as to reduce complexity. \nA colorful image can be decomposed into three channels, such as RGB, LAB, HSL and HSV.  LAB is used in this project (https://en.wikipedia.org/wiki/Lab_color_space) where L means \"lightness\". L-channel representing a gray color image is the input of my model,  and the output is the predicted colorful image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897864737672472,
        0.8415109224258991,
        0.8095366418152624,
        0.9235645224619912
      ],
      "excerpt": "The answer to this question leads to the usage of HyperColumns and a pre-trained convolutional neural network (CNN). In this project, pre-trained VGG is adopted and tweaked. VGG was trained on huge amount of images and it contains a lot of information regarding quite many of (if not all) objects in the world. Taking advantage of VGG, we should be able to colorize the gray images. VGG as \"external information\" is the essential reason why this colorization task can be done. \nMaking an analogy. Given three data points, we need to output a curve to fit them. There are tons of various curves that can fit these three data points. However, if somebody tells us (external information !) that the curve is most probably a quardratic curve, then we probably will produce the blue color curve. \nIn order to harvest this external information that VGG has to provide, we need HyperColumns. \nRegarding CNN and VGG, refer to http://cs231n.github.io/convolutional-networks/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944150763921282
      ],
      "excerpt": "The layers of a convolutional network is like as a non-linear counterpart of the image pyramids. The feature maps have different sizes. The topper they are on the VGG model, the smaller their sizes are. However, we need them to be of the same size, e.g., the size of the input grey image. Thus, the feature maps are upscaled by bilinear interpolation and are contatenated together to give us a \"HyperColumn\".  (It looks to me more like HyperMaps rather than HyperColumns though.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9877742778116292,
        0.9932430179626068,
        0.9969546202520593,
        0.965627915953584
      ],
      "excerpt": "This terminology comes from neuroscience, where some part of our brain is called hypercolumn (refer to the paper link at the bottom).  \nHere is a quick illustration from wiki: \"A cortical column, also called hypercolumn, macrocolumn, functional column or sometimes cortical module, is a group of neurons in the cortex of the brain that can be successively penetrated by a probe inserted perpendicularly to the cortical surface, and which have nearly identical receptive fields.\" \nThis analogy is quite similar to the feature maps of CNN. \nSince gray color image contains only one channel, in order for VGG to be able to process it, the first convolutional filter of VGG is replaced with a new filter. This new filter takes into one channel tensor and then output 64-channels tensor which is then fed into the rest part of VGG.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9669075243682355,
        0.8439455822571034,
        0.8881075502766023
      ],
      "excerpt": "Further and most importantly, the HyperColumns layer is added upon it, and then the HyperColumns are \"squeezed\" into a two-channels tensor which corresponds to the prediction of the A & B channels.  \nImagine a chopstick pushes through many slices of bread. It gives us many holes on the breads, and then all the holes comprise one HyperColumn which corresponds to one pixel of the original image. \nThis process is done by 1-by-1 convolution that \"stiches\" the feature maps together. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.87961368399217,
        0.9792810157929793
      ],
      "excerpt": "Instead, we can perform early stopping manually by checking the predicted the image. It is explicitly shown by the analogous figure below: \nWhy we do not use the traditional early stop ?  The major reason is that in here the validation loss is not U shape, but is L shape. More explaination is in the later section \"Only Plausible\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9849566062719126,
        0.9251726307138748,
        0.9901715407055457,
        0.8610552812085568,
        0.959931917265984,
        0.8224511460194378
      ],
      "excerpt": "Applied the model on illustration pictures of old books, and here are some picked up good results (not all predictions are good since the model is not really trained on such kind of images): \nTensorboard allows us to peek into how the network weights (conv filters) change during training. Here shows some of the filters and biases: \nActually all layers of filters have been updated to a considerable extent during the training. This indicates that all of the feature maps are useful and they probably contains different information. We should better incorporate all of the feature maps into our HyperColumns to avoid information loss. \nThat said, what if we only sample a portion of the feature maps ? \nInstead of using all feature maps, simplified model only use a portion of them. \nThis simplified model picks up the output of the pooling of the first four layers of VGG, upscale them, and then concatenated them into a thinner HyperColumn.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9846115066200627,
        0.9400368387195932,
        0.9527205180802006,
        0.9590991467108285
      ],
      "excerpt": "Apparently some information that VGG has to provied is lost, but this thinner model requests less computation and less memory. It is not powerful enough. Here are some samples of its predictions : \nThe predictions are not as good as the full model above, but still not very bad.  \nHere is its training loss, which is larger than the full model above. \nThis simplified model picks up the output of ReLu (which means before pooling) of the first five layers (which means the top conv layer is included) of VGG. As earlier, they are upscaled and then concatenated into a thinner HyperColumns. Surprisingly, its performance is almost as good as the full model above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9455142558346583
      ],
      "excerpt": "Some of its predictions are compared against the full model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9756456285477133
      ],
      "excerpt": "It may be interesting to try different combinations of layers, such as only the outputs of layers 1,2,3, or only the outputs of layeys 3,4,5, and so on. Is it possible that only some specific layers contribute most to the colorization task ? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9819529407223778,
        0.955682676206842,
        0.8998480262169419,
        0.8888988376026558
      ],
      "excerpt": "the ReLu output of the conv layers 3,4,5, and its performance is close to that of the full model. This is a very interesting signal that the higher conv layers are majorly useful for colorization.  \nthe ReLu output of the conv layers 1 to 5, (64+128+256+512+512 feature maps) and it has given as good result as the full model (5440 feature maps). This is not very surprising and details are given later as below. \nthe ReLu output of the conv layers 4 and 5, (512+512 feature maps) and the result is not very good. Probably this is because we have lost too much information by skiping layer 3. \nthe ReLu output of the conv layer 5 only, (512 feature maps) and the result is not very good either. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052289063363351
      ],
      "excerpt": "Though the higher conv layer contains more feature maps (512 maps e.g.), its map size is too small (14-by-14, e.g.). On the other hand, though the lower layer provides larger feature map (224-by-224 e.g.), its number of featuer maps is too small (64 maps, e.g.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894344090287772
      ],
      "excerpt": "Refer to http://cs231n.github.io/understanding-cnn/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9826353749975043
      ],
      "excerpt": "Do not need all feature maps. Sampling is fine as long sampling the output of the layers (before pooling). -- Probably adjacent sub-layers in the same layer contain correlated information, and this is why we can sample the layers but avoid performance degrading.  Here is an analogy: in order to recover a sine wave, how many samples we need, 1000 or 100? The performance (recover error) of 1000 samples is almost the same as the performance of 100 samples because the samples that are very close to each other give similar information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.998924310465893,
        0.9913085235396487,
        0.9565477855961695,
        0.9484861046449877,
        0.9231063803989065
      ],
      "excerpt": "Eventually, it is possible that sampled model (i.e., simplified model) may not perform well on huge amount of dataset, e.g., 1000 categories of 1.2 Million images. Huge amount of data and large number of categories of data may request a powerful model to handle. It is possible that a sampled model performs as well as the full model only because the amount of data and the number of categories of data are relatively small in this project.-- If this is true, the idea of  simplifyng model is still very useful, especially when we only need work on some specific type of data, e.g., predicting color for all the cartoon books from the same author. This means we can make a smart phone App to convert cartoons into colorful, but slightly different models for different authors. Each model runs much faster than the full model so as to give better user experience. \nI come up with some model based on the concept of HyperColumns. It tries to introduce more capacity and non-linearity, but did not give better performance. Anyway, here are what I've tried. Basically the feature maps from the same layer of VGG are concatenated together to give a layer-based-HyperColumns. Each layer-based-HyperColumns is \"squeezed\" by 1by1 conv into a single feature map. At last, these five feature maps go through non linear function ReLu respectively, are concatenated into one HyperColumns, and are used to make prediction.  \nIt is worth to try, because it gives some insights. It looks that the original feature maps of VGG already contains enough information. Redundant functions or capacities are not really requested. \nI actually also tried residual model as proposed by some others, but it only gives as good performance as sampling all conv layers. It looks that a reasonable sampling of the feature maps across all conv layers already contain enough information in order to recover the color. Fancier design is not really needed. \nThe task is actually \"hallucinating a plausible color version of the photograph\". The reconstructed color may not be the same as or even close to the ground truth, but it is plausible. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915925006297509,
        0.9564579999802661
      ],
      "excerpt": "The color of the reconstruction is different from that of groundtruth. However, the reconstruction looks fine (plausible).  This means that the validation or test loss may be larger than expected, even if the model has been trained very well. In the experiments, the model gives L shape validation loss, where the loss is considerable larger than train loss which is reasonable apparently.  Further, the validation loss vibrates (goes up and down randomly) after the model has been trained well, majorly because the model may match ground truth on some images but may not on other images. Thus, traditional early stop does not work. The manual early stop method has been described in the \"Training\" section above. \nActually, there is no way to tell what is the true color of the ground truth. A sunflower may be yellow, but also could be purple. The grass could be green in summer, but yellow in winter. This is why the validation loss could be always large, and L shape.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213014044001049,
        0.8771977577606047,
        0.8405182194417734
      ],
      "excerpt": "We probably give sunflower a yellow color, only because most sunflowers we've seen are yellow. The same with machine. The machine has been trained on mostly yellow sunflowers. Thus, the machine gives it a yellow color. This indicates that the machine may do a good job on the same type of data that it has been trained on.  \nWhat if we apply the model on some image set that it has never seen, such as, cartoons ? \nI did apply the model on cartoons, and it did not give a satisfying result. This is what is expected because the model has never been trained on cartoons. What if we have enough training data set of cartoons ? It will be interesting to see if it can colorize the cartoons from the same author. After all, an author usually presents a consistent style of art of herself/himself. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BerenLuthien/HyperColumns_ImageColorization/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Fri, 24 Dec 2021 13:56:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BerenLuthien/HyperColumns_ImageColorization/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "BerenLuthien/HyperColumns_ImageColorization",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/BerenLuthien/HyperColumns_ImageColorization/master/HyperColumns_Test.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "    HyperColumns = tf.concat([layer_relu1_2, \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9082637427762955,
        0.9082637427762955,
        0.8982766083140744,
        0.8878024139826244
      ],
      "excerpt": "    W0 = utils.weight_variable([3, 3, 1, 64], name=\"W0\") \n    b0 = utils.bias_variable([64], name=\"b0\") \n    conv0 = utils.conv2d_basic(images, W0, b0) \n    hrelu0 = tf.nn.relu(conv0, name=\"relu\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "    HyperColumns = tf.concat([layer_relu1_2, \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9288100766820754,
        0.9288100766820754,
        0.8170396574231907,
        0.8123763140827432,
        0.916303043955523
      ],
      "excerpt": "wc1 = utils.weight_variable([1, 1, 5440, 2], name=\"wc1\") \n    wc1_biase = utils.bias_variable([2], name=\"wc1_biase\") \n    pred_AB_conv = tf.nn.conv2d(HyperColumns, wc1, [1, 1, 1, 1], padding='SAME') \n    pred_AB = tf.nn.bias_add(pred_AB_conv, wc1_biase)         \nreturn tf.concat(values=[images, pred_AB], axis=3,  name=\"pred_image\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9288100766820754
      ],
      "excerpt": "    wc1 = utils.weight_variable([1, 1, 960, 2], name=\"wc1\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8170396574231907,
        0.9288100766820754,
        0.8123763140827432,
        0.916303043955523
      ],
      "excerpt": "    HyperColumns = tf.concat([layer1, layer2,layer3, layer4] ,3)         \n    pred_AB_conv = tf.nn.conv2d(HyperColumns, wc1, [1, 1, 1, 1], padding='SAME') \n    wc1_biase = utils.bias_variable([2], name=\"wc1_biase\") \n    pred_AB = tf.nn.bias_add(pred_AB_conv, wc1_biase)         \nreturn tf.concat(values=[images, pred_AB], axis=3,  name=\"pred_image\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9288100766820754
      ],
      "excerpt": "    wc1 = utils.weight_variable([1, 1, 1472, 2], name=\"wc1\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8170396574231907,
        0.9288100766820754,
        0.8123763140827432,
        0.916303043955523
      ],
      "excerpt": "    HyperColumns = tf.concat([layer1, layer2,layer3, layer4, layer5] ,3)         \n    pred_AB_conv = tf.nn.conv2d(HyperColumns, wc1, [1, 1, 1, 1], padding='SAME') \n    wc1_biase = utils.bias_variable([2], name=\"wc1_biase\") \n    pred_AB = tf.nn.bias_add(pred_AB_conv, wc1_biase)         \nreturn tf.concat(values=[images, pred_AB], axis=3,  name=\"pred_image\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8316795980915235
      ],
      "excerpt": "Here are some examples: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BerenLuthien/HyperColumns_ImageColorization/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2017, Bob Guo\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the {organization} nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "HyperColumns of CNN and Image Colorization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "HyperColumns_ImageColorization",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "BerenLuthien",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BerenLuthien/HyperColumns_ImageColorization/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Fri, 24 Dec 2021 13:56:29 GMT"
    },
    "technique": "GitHub API"
  }
}