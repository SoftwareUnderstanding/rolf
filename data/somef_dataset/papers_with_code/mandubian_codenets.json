{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1909.11942 aimed at providing a much lighter model than Bert with the same performance. The main interesting aspect of Albert consists in introducing an intermediate internal embedding layer `E` with much lower size than hidden size `H`. This allows to reduce the number of parameters from `O(V \u00d7 H"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": ": Query1CodeN: Single Bert query encoder+tokenizer, Single encoder+tokenizer per language \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003055025700121
      ],
      "excerpt": "|1 Query+Code Path|Bert 256/12/12/72|BPE 60K| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "|   |0.151|0.09706|0.2014|0.1036|0.1299|0.1894|0.1846| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8177918124334298
      ],
      "excerpt": "Encode AST graphs for embedding \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mandubian/codenets",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-05T22:18:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-13T21:50:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9837758960604337,
        0.9813752161280486,
        0.9465167609305286,
        0.9244444111736132,
        0.9232777242810492,
        0.9925889594062285
      ],
      "excerpt": "A playground to play with PLP (Programming Language Processing) datasets & Deep Learning \nCode is currently under heavy work so it's not stable but all the principles are there. \nIn this repository, I want to study PLP, programming languages processing (searching, modifying, generating, translating my code...) using AI techniques like Deep Learning. \nAt the same time, I want to use this code base to evaluate advanced programming techniques with Python (yes it is possible to do that), AI Libraries (Pytorch for now but could use others later) using typesafe programming (Mypy) and some functional programming techniques with becoming too mad. \nIf you have GPU(s) to share with me for some time to perform more experiments on those topics, don't hesitate to contact me here on github or on twitter @mandubian. \nThe current code is a ~80% rewrite of the original Github repository open-sourced by Microsoft team with a paper and blog post with a benchmark on W&B and the Github CodeSearchNet dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8603731600952153,
        0.9767120680064012
      ],
      "excerpt": "- to use the dataset independently of Tensorflow as existing code is really focused on it (Pytorch is my current choice for NLP until I find another one ;)). \n- to be able to use the best NLP Deep Learning library for now Huggingface Transformers and more recently their new beta project Huggingface Rust Tokenizers and I must say it's much better used in Pytorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9293368858133134,
        0.8961051202204977
      ],
      "excerpt": "Support of Pytorch, \nSupport of Huggingface transformers pretrained & non-pretrained: Sample of Bert from scratch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8987411739949861
      ],
      "excerpt": "experimental typesafe \"typeclass-like helpers\" to save/load full Training heterogenous contexts (models, optimizers, tokenizers, configuration using different libraries): a sample recordable Pytorch model and a full recordable training context) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9217433341776391
      ],
      "excerpt": "I'm currently experimenting different models & runs are recorded on W&B \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8273936649314533
      ],
      "excerpt": ": QueryCodeSiamese: Single Bert encoder+tokenizer for query and coding languages \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9800502685251751,
        0.8597896656929862
      ],
      "excerpt": "I haven't submitted any model to official leaderboard https://app.wandb.ai/github/codesearchnet/benchmark/leaderboard because you can submit only every 2 weeks and I'm experimenting many different models during such long period so I prefer to keep searching for better models instead of submitting. But you can compute your NDCG metrics on the benchmark website and it is stored in your W&B run so this is what I use for now to evaluate my experiments. \nPlease note that all experiments are currently done on my own 1080TI GPU on a 32GB workstation. So being strongly limited by my resources and time, I have to choose carefully my experiments and can't let them run forever when they do not give satisfying results after a few epochs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8339221849294147,
        0.8662461850884768,
        0.8302063448876537,
        0.8118963803922665
      ],
      "excerpt": "This is the model from original paper https://arxiv.org/pdf/1909.09436.pdf and I haven't pushed such trainings too far as the authors did it. You can check whole results in the paper. \nJust for information here are their MRR and NDCG results for: \n- NBOW a simple NN with a linear token embedding (providing the baseline) \n- Bert-like encoding with self-attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9675266157857425,
        0.9946841011810708
      ],
      "excerpt": "Interestingly, Bert-like models reach much better MRR on training compared to NBOW but not on NDCG. MRR doesn't care about the relevance of the result, just the rank. NDCG cares about relevance and rank. But when you train using default data, there is no relevance, the choice is binary. \nIn terms of languages, NBOW is better in all cases and behaves better on Java/Python/Ruby than Go/JS/PHP. Bert-like models are also better for Python/Java/Ruby but very bad on PHP, Go and worse in Javascript. Python/Java/Ruby are fairly simple, sequentially structured and imperative languages so maybe seq-aware models can rely on that. For Javascript, when you look at code samples, you see that JS is a language with lots of anonymous functions/callbacks so the code is much less sequentially interpretable. PHP is quite messy language to me but I would need a deeper analysis to know why it's harder. For Go, I cannot really give any good interpretation without deeper qualitative study. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9601200372087595
      ],
      "excerpt": "This model is my first target experiment: N encoders is not viable as I am a poor data scientist in my free-time and have only one GPU and 32GB of RAM. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9722389263530622
      ],
      "excerpt": "BPE Vocabulary size is set to 10K for query and code tokenizers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9048483124013917
      ],
      "excerpt": "Query Encoder is a smaller BERT than code encoder to reflect the more complex 5 languages in one single BERT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9533141434517146,
        0.9846266452938957,
        0.9949994307784451,
        0.9366364818655983,
        0.9231985747696061,
        0.920975871025155
      ],
      "excerpt": "We see that MRR is much higher than what was obtained in CodeSearchNet paper (0.95 vs 0.70) but the NDCG is lower (0.10 vs 0.11). \nQuery1Code1 has learnt to find the right exact piece of code from a query and it seems quite good at it (despite we haven't the MRR on benchmark dataset). But in terms of ranking pieces of code between each other with a relevance ranking (which is what NDCG represents), it's not good. We haven't the MRR on benchmark dataset so it's hard to know if our model tends to overfit on codesearchnet dataset (due to an unknown bias) and if it's a generalization issue. \nYet, this also demonstrates that MRR and NDCG are 2 different metrics and evaluating your model on MRR is not really enough when the benchmark metrics is NDCG. NDCG takes into account the relevance of pieces of code between each others and the ranking. In the dataset, we haven't relevance of piece of codes between each other so it's not trivial to train on a ranking+relevance objective. \nHaving several BERT models on my 11GB 1080TI limits a lot the size of each model and batches. Thus, I can't train deeper model able to store more knowledge (recent studies show that bigger NLP models store more \"knowledge\"). Moreover, not being able to increase batch size reduces the speed of training too. \nSo I've decided to try the most extreme case: one single shared BERT and tokenizer for query and tokenizer and see what happens. \nThe model is then: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9534567670783602,
        0.9952695836092801
      ],
      "excerpt": "BPE Vocabulary size is set to 60K which is 6 times bigger than previous experiment to reflect that it must encode both query (which are almost normal sentences) and code from 5 languages (with lots of technical tokens like {}[]..., acronyms and numbers). No further token techniques like snake/camel-case subtokenization have been applied till now. \nIn this 1st experiment on this model, output embedding size is set to smaller 72 (<128) to test capability of model to learn a lot with less encoding space. But then I've fixed the intermediate BERT size also to a smaller 256 size while increasing the number of heads and layers to 12 to give the model more capabilities in terms of diversity and depth. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.992534474004903,
        0.9677024682028024,
        0.9258439476424258,
        0.9694881536659964
      ],
      "excerpt": "So with a single encoder/tokenizer of smaller size and smaller batches, we see that MRR is slightly higher than Query1Code1. The NDCG is also a bit higher (0.13 vs 0.10) but lower than NBOW baseline. The performance on the languages remain consistent (better on java/python/ruby and not on go/js/ruby and very bad in js). \nWith a shared model and tokenizer with smaller embedding and intermediate size but deeper layers and heads, we haven't lost any performance compared to separated branches. But we haven't reached the baseline performance. \nNow let's see what happens with same siamese configuration but an even smaller embedding size and model. \nSame configuration as before but with smaller output embedding size of 64 and smaller BERT model but a bigger batch size of 290 which accelerates the training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9941023957787618,
        0.9903774771157742
      ],
      "excerpt": "We see that compared to Query1Code1, Train MRR is a bit lower (92 vs 96) but Val MRR is much lower (74 vs 85). So the ranking of precise piece of code is not as good with a single branch model which sounds logical. \nYet surprisingly the NDCG is much higher (0.17 vs 0.10) not so far from SOTA (0.19 on leaderboard). So this single encoder/tokenizer with a smaller embedding size and smaller model is much better at scoring pieces of code between each other for a query. But not as good at finding and ranking the exact right piece of code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9915906440528021,
        0.9816782300121091,
        0.8430920903953205
      ],
      "excerpt": "In the current state of my study, I can imagine that bigger BERT models have enough space to overfit the distribution and dispatch all samples (query and code) independently in the embedding space. Then using a much smaller architecture forces the model to gather/clusterize \"similar\" query/code samples in the same sub-space. But does it mean it unravels any semantics between query and code? It's hard to say in the current state of study and will need much deeper analysis... TO BE CONTINUED. \nSame configuration as previous QueryCodeSiamese but replacing Bert by optimized Albert model. Albert is a model described in this paper https://arxiv.org/abs/1909.11942 aimed at providing a much lighter model than Bert with the same performance. The main interesting aspect of Albert consists in introducing an intermediate internal embedding layer E with much lower size than hidden size H. This allows to reduce the number of parameters from O(V \u00d7 H) to O(V \u00d7 E + E \u00d7 H) where V is the BPE Vocabulary size. \nThis Albert internal embedding layer seemed interesting to me in the CodeSearchNet context because it naturally introduces an embedding space compression and decompression that typically corresponds to learning atomic representation first and then extracting higher semantic representations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.883504575581776
      ],
      "excerpt": "MRR on Train dataset is a bit lower that with Bert and about the same on val dataset. But the NDCG is really bad in all languages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9602236715974645,
        0.9874947289241429,
        0.9652021814844512
      ],
      "excerpt": "The CrossEntropy used to train model by default with MRR as evaluation metric doesn't seem to be compliant with NDCG used for benchmark. NDCG takes rank and relevance into account. based on it \nMRR & NDCG metrics aren't continuous so it's hard to build differentiable objective. Yet, there is a loss called LambdaLoss presented in this paper https://research.google/pubs/pub47258/ as a probabilistic framework for ranking metric optimization. It tries to provide a continuous, differentiable and convex rank+relevance-aware objective (in certain conditions). Converge of this loss is ensured by an EM algorithm in the paper. Mini-batch gradient descent can be seen as a kind of Expectation Maximization algorithm so we can suppose it should converge to a local minimum. \nCodesearchnet dataset doesn't provide relevance of query/code samples between each other. We just know which query corresponds to which piece of code. So LambdaLoss in this context reduces to a binary ranking objective looking like MRR. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9380980556859835
      ],
      "excerpt": "I've chosen to use smaller Vocabulary of 30K tokens and the same Bert as in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9907685949769737,
        0.9038358835708662
      ],
      "excerpt": "We see that the model converges but reaches lower performances than classic CrossEntropy Loss. The vocabulary and the model size weren't the same so it's hard to conclude anything robust. I can only say that used in MRR approximation (as we haven't relevance between samples), LambaLoss converges to a local optimum. \nNext step will be to compute some relevances in some way (coming soon) and retry this lambdaloss on same model compared to crossentropy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633659454153468
      ],
      "excerpt": "Let's go even further with smaller embedding size of 32 and very small Bert model and larger batches. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9856569446404592,
        0.9820259736175935,
        0.9977449032772698,
        0.8357299826568839,
        0.9477102512689186,
        0.9922896646878154,
        0.9710752158844776
      ],
      "excerpt": "We see that final MRR is much lower on both train and val datasets which sounds logical. This model is not very good at finding the exact right piece of code for a query. \nBut the NDCG computed on benchmark is even higher than previous best bigger Bert model with bigger embedding of 64. So this model is better at ranking documents between each others. \nSo, the higher compression of embedding space seems to be better for NDCG and worse for MRR. I can imagine this compression forces the model to clusterize samples in the same sub-space. Something I didn't study is also the common vocabulary which also gathers all queries and language in the same tokenization space. \nI'd need to study distribution of embedding space and tokenization to find further clues. \nIn previous experiments, Javascript language have appeared to give the worst results in all configurations. \nIf we check language code distribution in distribution notebook, we see that JS is a language that tends to be more verbose with more tokens than other languages. In previous experiments, we have trained our models with 200 max code tokens because in all languages, 200 represents the 0.9-quantile in all languages except JS. We could try to accept more code tokens and see how the model behaves (specially for JS) \nWe take the same model with small emnedding but accept up to 400 code tokens for all languages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.972624815390705,
        0.9424480637780949,
        0.8520589793173063
      ],
      "excerpt": "the mean NDCG is a bit higher than previous experiment. But if we check each language, we see that Javascript is indeed better with more code tokens (0.129 vs 0.108) but all other languages are a bit lower or the same. \nSo, increasing the max number of code tokens improves a bit JS results but not the others. As previous 200 max code tokens were already in the 0.9-quantile for most languages, it means adding more tokens doesn't bring much more to model training. \nin languages, function and symbols are often written in camelcase like functionName or in snakecase like function_name. Splitting those elements in 2 tokens function and name might help the model to extract more meaninful information from code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9710752158844776
      ],
      "excerpt": "We take the same model with small emnedding but accept up to 400 code tokens for all languages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.989728714874453
      ],
      "excerpt": "NDCG is lower than both previous experiments with same model for all language except java that seems to get light advantage from this tokenizer. This is a bit surprising to me: is it due to the fact that sub-tokenization doesn't really bring anything or that smaller embedding/BERT can't take advantage from it? We need to experiment with a bigger embedding and model to check that. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "My own playground for PLP (Programming Language Processing) using DeepLearning techniques",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mandubian/codenets/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 21 Dec 2021 23:07:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mandubian/codenets/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mandubian/codenets",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mandubian/codenets/master/codesearchnet_distrib.ipynb",
      "https://raw.githubusercontent.com/mandubian/codenets/master/codenets/codesearchnet/notebooks/predictions.ipynb",
      "https://raw.githubusercontent.com/mandubian/codenets/master/codenets/codesearchnet/notebooks/codesearchnet_distrib.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```sh\npoetry install\npoetry shell\n```\n\nNow you should be in a console with your virtualenv environment and all your custom python dependencies. Now you can run python.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```sh\npyenv local 3.7.2\n```\n\n> Please python devs, stop using Python 2.x, this is not possible anymore to use such bloated oldies.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Following instructions to install [Poetry](https://python-poetry.org/docs/).\n\n>Why poetry instead of basic requirements.txt?\n\nBecause its dependency management is more automatic. Poetry has big defaults & bugs but its dependency management is much more production ready than other Python solutions I've tested and it isolates by default your python environment in a virtual env (The other best solution I've found is a hand-made virtualenv in which I install all my dependencies with a requirements.txt).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9030452181964486
      ],
      "excerpt": "If you have GPU(s) to share with me for some time to perform more experiments on those topics, don't hesitate to contact me here on github or on twitter @mandubian. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9352659431905456
      ],
      "excerpt": "Poetry Python dependencies management with isolated virtualenv (Poetry config. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8080885025556722
      ],
      "excerpt": "Take care to give a training.name to your experiment and a unique training.iteration to your current run. You can have several training.iteration for one training.name. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.890063909988214
      ],
      "excerpt": "python codenets/codesearchnet/tokenizer_build.py --config ./conf/MY_CONF_FILE.conf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317135315960992
      ],
      "excerpt": "python codenets/codesearchnet/train.py --config ./conf/MY_CONF_FILE.conf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.858953220640131
      ],
      "excerpt": "python codenets/codesearchnet/eval.py --restore ./checkpoints/YOUR_RUN_DIRECTORY \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8413180484889083
      ],
      "excerpt": "python codenets/codesearchnet/predictions.py --restore ./checkpoints/YOUR_RUN_DIRECTORY \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8377621879540922
      ],
      "excerpt": "| NBOW |0.164|0.130|0.121|0.175| 0.123|0.223|0.212| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8454594548995328
      ],
      "excerpt": "|Training|epochs|lr|loss|batch size|seed|epoch duration| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059252102425262
      ],
      "excerpt": "|Max MRR|Train|Val| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8454594548995328
      ],
      "excerpt": "|Training|epochs|lr|loss|batch size|seed|epoch duration| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059252102425262
      ],
      "excerpt": "|Max MRR|Train|Val| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8454594548995328
      ],
      "excerpt": "|Training|epochs|lr|loss|batch size|seed|epoch duration| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059252102425262
      ],
      "excerpt": "|Max MRR|Train|Val| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8454594548995328
      ],
      "excerpt": "|Training|epochs|lr|loss|batch size|seed|epoch duration| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059252102425262
      ],
      "excerpt": "|Max MRR|Train|Val| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8454594548995328
      ],
      "excerpt": "|Training|epochs|lr|loss|batch size|seed|epoch duration| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059252102425262
      ],
      "excerpt": "|Max MRR|Train|Val| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8454594548995328
      ],
      "excerpt": "|Training|epochs|lr|loss|batch size|seed|epoch duration| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059252102425262
      ],
      "excerpt": "|Max MRR|Train|Val| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8173058653792059
      ],
      "excerpt": "|Training|epochs|lr|loss|batch size|seed|epoch duration|max code tokens| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059252102425262
      ],
      "excerpt": "|Max MRR|Train|Val| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8173058653792059
      ],
      "excerpt": "|Training|epochs|lr|loss|batch size|seed|epoch duration|max code tokens| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059252102425262
      ],
      "excerpt": "|Max MRR|Train|Val| \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mandubian/codenets/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/mandubian/codenets/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright 2020 Pascal Voitot\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CodeNets",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "codenets",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mandubian",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mandubian/codenets/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "https://app.wandb.ai/mandubian/codenets/runs/j12z3vfr/overview?workspace=user-mandubian\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "https://app.wandb.ai/mandubian/codenets/runs/f6ebrliy/overview?workspace=user-mandubian\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "https://app.wandb.ai/mandubian/codenets/runs/ath9asmp/overview?workspace=user-mandubian\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "https://app.wandb.ai/mandubian/codenets/runs/mv433863/overview?workspace=user-mandubian\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "https://app.wandb.ai/mandubian/codenets/runs/4nnj6vgh?workspace=user-mandubian\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "https://app.wandb.ai/mandubian/codenets/runs/wz2uafe7?workspace=user-mandubian\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "https://app.wandb.ai/mandubian/codenets/runs/e42kovab/overview?workspace=user-mandubian\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "https://app.wandb.ai/mandubian/codenets/runs/5jbus5as?workspace=user-mandubian\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Tue, 21 Dec 2021 23:07:38 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Deep Learning bug correction by AST modification\n- Function generation from function signature\n- Type error detection by Deep Learning\n- Type logic & reasoning by Deep Learning\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "nlp",
      "programming-language",
      "huggingface",
      "transformer",
      "bert",
      "tokenizer",
      "ai"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```sh\npyenv local 3.7.2\n```\n\n> Please python devs, stop using Python 2.x, this is not possible anymore to use such bloated oldies.\n\n",
      "technique": "Header extraction"
    }
  ]
}