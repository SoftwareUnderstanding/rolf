{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2009.14794\">Performer</a>, a linear attention-based transformer variant with a **F**ast **A**ttention **V**ia positive **O**rthogonal **R**andom features approach (FAVOR+"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{choromanski2020rethinking,\n    title   = {Rethinking Attention with Performers},\n    author  = {Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},\n    year    = {2020},\n    eprint  = {2009.14794},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{choromanski2020rethinking,\n    title   = {Rethinking Attention with Performers},\n    author  = {Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},\n    year    = {2020},\n    eprint  = {2009.14794},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xl402/performer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-03T22:21:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-28T03:51:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8524034231794068
      ],
      "excerpt": "An implementation of <a href=\"https://arxiv.org/abs/2009.14794\">Performer</a>, a linear attention-based transformer variant with a Fast Attention Via positive Orthogonal Random features approach (FAVOR+). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Tensorflow implementation of a linear attention architecture",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xl402/performer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Thu, 23 Dec 2021 15:13:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xl402/performer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "xl402/performer",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create a Python 3 virtual environment and activate:\n```\nvirtualenv -p python3 env\nsource ./env/bin/activate\n```\nInstall requirements by running:\n```\npip install -r requirements.txt\n```\nThen export project to python path:\n```\nexport PYTHONPATH=$PATH_TO_REPO/performer\n```\nTo test the scripts, run `pytest` in the root directory, you may wish to\ninstall `pytest` separately\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xl402/performer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Tom Lu\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tensorflow Implementation of Performer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "performer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "xl402",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xl402/performer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 24,
      "date": "Thu, 23 Dec 2021 15:13:30 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`Performer` inherites from a lightly modified version of tf-nightly's `MultiHeadAttention` and is made to be fully\ncompatible with the parents' use cases, with added flexibility for performing attention in linear time and space complexity. Currently masked attention is not supported.\n```python\nfrom performer.networks.linear_attention import Performer\n\nlayer = Performer(num_heads=2, #: Number of attention heads\n                  key_dim=2, #: Size of each attention head for query and key\n                  attention_method='linear', #: attention method, 'linear' or 'quadratic'\n\t\t  supports=2, #: only used in 'linear' attention, number of random features\n\t\t  attention_axes=None #: axes over which the attention is applied.\n\t\t  )\nquery = tf.keras.Input(shape=[8, 16])\nkey = tf.keras.Input(shape=[4, 16])\noutput_tensor = layer([query, key])\nprint(output_tensor.shape)\n#: (None, 8, 16)\n```\n\n`Performer` supports attention in any arbituary axis, below is an example of 2D\nself-attention over a 5D input tensor on axes 2 and 3.\n\n```python\nlayer = Performer(num_heads=2, key_dim=2, attention_method='linear',\n                  supports=10, attention_axes=(2, 3))\ninput_tensor = tf.keras.Input(shape=[5, 3, 4, 16])\noutput_tensor = layer([input_tensor, input_tensor])\nprint(output_tensor.shape)\n#: (None, 5, 3, 4, 16)\n```\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}