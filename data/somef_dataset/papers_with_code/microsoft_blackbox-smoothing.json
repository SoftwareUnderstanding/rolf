{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.01908 <br>\nBlog post: https://www.microsoft.com/en-us/research/blog/denoised-smoothing-provably-defending-pretrained-classifiers-against-adversarial-examples/\n\nOur paper presents a method for provably defending any pretrained image classifier against Lp adversarial attacks.\n\n<p>\n<img src=\"github_figures/main_figure.PNG\" width=\"1000\" >\n</p>\n\n<p>\n<img src=\"github_figures/main_tables.PNG\" width=\"1000\" >\n</p>\n \n## Overview of the Repository\n\nOur code is based on the open source codes of [Cohen et al (2019"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9625923568277878,
        0.8356013927728488,
        0.9977994744046882
      ],
      "excerpt": "Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, J. Zico Kolter <br> \nNeurIPS 2020 <br> \nPaper: https://arxiv.org/abs/2003.01908 <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9750296638266478
      ],
      "excerpt": "Our code is based on the open source codes of Cohen et al (2019) and Salman et al. (2019). The major contents of our repo are as follows: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/microsoft/blackbox-smoothing/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/denoised-smoothing",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-01T02:28:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-05T08:36:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9945716789504542,
        0.9394449182630016
      ],
      "excerpt": "This repository contains the code and models necessary to replicate the results of our recent paper: \nDenoised Smoothing: A Provable Defense for Pretrained Classifiers <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457625244672645
      ],
      "excerpt": "Our paper presents a method for provably defending any pretrained image classifier against Lp adversarial attacks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9795816715907066,
        0.9293571701510281
      ],
      "excerpt": "Our code is based on the open source codes of Cohen et al (2019) and Salman et al. (2019). The major contents of our repo are as follows: \nvision_api/ contains the code for our experiments on online Vision APIs. Check out the tutorial! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095188878553238,
        0.9844085590648219
      ],
      "excerpt": "code/ contains the code for our experiments on CIFAR-10 and ImageNet. \nanalysis/ contains the plots and tables that are shown in our paper. Keep reading to see how you can replicate these easily! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9688138391746925
      ],
      "excerpt": "train_denoiser.py: the main code of our paper which is used to train the different denoisers used in our paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9807739290424833
      ],
      "excerpt": "test_denoiser.py: a script to test the performance of the denoiser on reconstruction task, and on image classification under Gaussian noise when a pretrained classifier is attached to the denoiser. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8703806816694382
      ],
      "excerpt": "To train a denoiser with MSE loss to denoise Gaussian noise of stddev of 0.25, run the following \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895763474419747
      ],
      "excerpt": "will load the $denoiser and attach it to the pretrained classifier $pretrained_classifier, smooth it using a noise level &sigma;=0.25, and certify 500 samples of the cifar10 test set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    data/certify/cifar10/mse_obj/MODEL_resnet110_90epochs_DENOISER_cifar10_dncnn_epochs_90/noise_0.25/test_N10000/sigma_0.25. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.990030220089302
      ],
      "excerpt": "To see this more clearly, let's try to certify the pretrained classifier without using a denoiser and compare the certification results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8844261591859259,
        0.9717748987837852
      ],
      "excerpt": "Note how adding a denoiser substantially improves the certified accuracy of the pretraing classifier! \nWe provide code to generate all the tables and results of our paper. Simply run  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.961018252456412
      ],
      "excerpt": "This code reads from the data/ folder (which should appear if you followed the Getting started section correctly) i.e. the logs that were generated when we certifiied our trained models, and automatically generates the tables and figures that we present in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8957234575976235
      ],
      "excerpt": "agree to a Contributor License Agreement (CLA) declaring that you have the right to, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8519881310095426
      ],
      "excerpt": "to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073737513954784,
        0.9747832544093742
      ],
      "excerpt": "This project has adopted the Microsoft Open Source Code of Conduct. \nFor more information see the Code of Conduct FAQ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Provably defending pretrained classifiers including the Azure, Google, AWS, and Clarifai APIs",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**You can download our trained models [here](https://drive.google.com/open?id=1MCXSKlz8qYQOGqMhbqmwN4Y0YPNMxVj6)**. These contain all our trained denoisers and pretrained classfiers that we use in our paper.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/blackbox-smoothing/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Sun, 26 Dec 2021 12:14:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/denoised-smoothing/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft/denoised-smoothing",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/blackbox-smoothing/master/vision_api/tutorial.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/blackbox-smoothing/master/vision_api/run_certify.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8572241453072775
      ],
      "excerpt": "Lazy to train? No worries, we have trained one for you! Just run the following in the command-line, and continue with the example \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img src=\"github_figures/main_figure.PNG\" width=\"1000\" > \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img src=\"github_figures/main_tables.PNG\" width=\"1000\" > \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.829654013903501
      ],
      "excerpt": "majority_class, _, _ = RobustAPI(api_name, denoiser=denoiser, online=True).predict(img, ...) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117382915173286
      ],
      "excerpt": "majority_class, _ = RobustAPI(api_name, online=False).predict(logs, ...) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8693997551849303
      ],
      "excerpt": "train_classifier.py: a generic script for training ImageNet/Cifar-10 classifiers, with Gaussian agumentation option, achieving SOTA. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8199068006308169
      ],
      "excerpt": "train_denoiser_multi_classifier.py: a variant of train_denoiser.py that allows training denoisers using multiple surrogate models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8478188493555946,
        0.831076309408216
      ],
      "excerpt": "python code/train_denoiser.py --dataset cifar10 --arch cifar_dncnn --outdir denoiser_output_dir --noise 0.25 \nLazy to train? No worries, we have trained one for you! Just run the following in the command-line, and continue with the example \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8851856401576541,
        0.8659909719823131,
        0.8786282000285618,
        0.8702189688815748,
        0.8134274359400094
      ],
      "excerpt": "python code/test_denoiser.py --dataset cifar10 --denoiser $denoiser_output_dir/checkpoint.pth.tar --clf $pretrained_classifier --noise 0.25 \nCertify the trained model on CIFAR-10 test set using &sigma;=0.25 \npython code/certify.py --dataset cifar10 --base_classifier $pretrained_classifier --sigma 0.25  \n--outfile certification_output/sigma_0.25 --skip 20 --denoiser $denoiser_output_dir/checkpoint.pth.tar \nwill load the $denoiser and attach it to the pretrained classifier $pretrained_classifier, smooth it using a noise level &sigma;=0.25, and certify 500 samples of the cifar10 test set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8086185361859174
      ],
      "excerpt": "    data/certify/cifar10/mse_obj/MODEL_resnet110_90epochs_DENOISER_cifar10_dncnn_epochs_90/noise_0.25/test_N10000/sigma_0.25. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8786282000285618
      ],
      "excerpt": "python code/certify.py --dataset cifar10 --base_classifier $pretrained_classifier --sigma 0.25  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8171619658877173
      ],
      "excerpt": "Now, run python code/generate_github_result.py (you might need to change the paths to the certification results in this script) to generate the below certification curves from the above certification results, you will get \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8994906389678163
      ],
      "excerpt": "<img src=\"github_figures/readme_example.png\"  width=\"400\" > \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img src=\"github_figures/imagenet_full_access.PNG\"  width=\"1000\" > \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/denoised-smoothing/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'    MIT License\\n\\n    Copyright (c) Microsoft Corporation.\\n\\n    Permission is hereby granted, free of charge, to any person obtaining a copy\\n    of this software and associated documentation files (the \"Software\"), to deal\\n    in the Software without restriction, including without limitation the rights\\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n    copies of the Software, and to permit persons to whom the Software is\\n    furnished to do so, subject to the following conditions:\\n\\n    The above copyright notice and this permission notice shall be included in all\\n    copies or substantial portions of the Software.\\n\\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n    SOFTWARE\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Denoised Smoothing: A Provable Defense for Pretrained Classifiers",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "denoised-smoothing",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/denoised-smoothing/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 74,
      "date": "Sun, 26 Dec 2021 12:14:07 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "machine-learning",
      "adversarial-defense",
      "adversarial-examples",
      "neural-networks",
      "computer-vision",
      "provable-defense",
      "adversarial-robustness",
      "image-classification",
      "aws-rekognition",
      "google-cloud-vision",
      "clarifai",
      "azure-computer-vision"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1.  `git clone https://github.com/microsoft/denoised-smoothing.git`\n\n2.  Install dependencies:\n    ```\n    conda create -n denoised-smoothing python=3.6\n    conda activate denoised-smoothing\n    conda install numpy matplotlib pandas seaborn scipy==1.1.0\n    conda install pytorch torchvision cudatoolkit=10.0 -c pytorch #: for Linux\n    pip install google-cloud-vision boto3 clarifai\n    ```\n\n3. Download and extract our certification logs from [here](https://www.dropbox.com/s/fjmncwhsnfgkmzk/data.tar.gz?dl=0). You can instead simply run the following from within the root directory of this repository \n\n    ```\n    wget -O data.tar.gz https://www.dropbox.com/s/fjmncwhsnfgkmzk/data.tar.gz?dl=0 && tar -xzvf data.tar.gz\n    ```\n\n4. Download our trained models (denoisers and classifiers) from [here](https://drive.google.com/open?id=1MCXSKlz8qYQOGqMhbqmwN4Y0YPNMxVj6). Then move the downloaded `pretrained_models.tar.gz` into the root directory of this repository. Run `tar -xzvf pretrained_models.tar.gz` to extract the models.\n\n5. If you want to run ImageNet experiments, obtain a copy of ImageNet and preprocess the val directory to look like the train directory by running [this script](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh). Finally, set the environment variable `IMAGENET_DIR` to the directory where ImageNet is located.\n\n6. Let us try to certify the robustness of a CIFAR-10 pretrained model with an attached MSE-trained DnCNN denoiser.\n    ```\n    pretrained_classifier=\"pretrained_models/cifar10_classifiers/ResNet110_90epochs/noise_0.00/checkpoint.pth.tar\"\n    denoiser=\"pretrained_models/trained_denoisers/cifar10/mse_obj/dncnn/epochs_90/noise_0.25/checkpoint.pth.tar\"\n    output=\"certification_output/sigma_0.25\"\n    python code/certify.py --dataset cifar10 --base_classifier $pretrained_classifier --sigma 0.25 --outfile $output --skip 20 --denoiser $denoiser\n    ```\nCheck the results in `certification_output/sigma_0.25`. You should get similar to \n    ```\n    data/certify/cifar10/mse_obj/MODEL_resnet110_90epochs_DENOISER_cifar10_dncnn_epochs_90/noise_0.25/test_N10000/sigma_0.25\n    ```\n\n\n**Are they similar? Perfect! You can keep going.**\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Let's now convert a pretrained *non-robust* CIFAR-10 classifier to a provably robust one!\n\nIn what follows, we will show you how you can train a denoiser on CIFAR-10 using the MSE objective, attach it to a pretrained classifier, then certify the robustness of the resultant robust classifier. This is the pretrained model we consider\n```\npretrained_classifier=\"pretrained_models/cifar10_classifiers/ResNet110_90epochs/noise_0.00/checkpoint.pth.tar\"\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}