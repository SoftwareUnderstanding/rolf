{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We thank Jaakko Lehtinen, David Luebke, and Tuomas Kynk&auml;&auml;nniemi for in-depth discussions and helpful comments; Janne Hellsten, Tero Kuosmanen, and Pekka J&auml;nis for compute infrastructure and help with the code release.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.00567",
      "https://arxiv.org/abs/1801.03924",
      "https://arxiv.org/abs/1512.00567",
      "https://arxiv.org/abs/1409.1556",
      "https://arxiv.org/abs/1801.03924",
      "https://arxiv.org/abs/1512.00567"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8334710283794773,
        0.8753150968738145
      ],
      "excerpt": "For business inquiries, please contact researchinquiries@nvidia.com \nFor press and other inquiries, please contact Hector Marinez at hmarinez@nvidia.com \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906174419333412,
        0.9554953249972649
      ],
      "excerpt": "| http://stylegan.xyz/paper | Paper PDF. \n| http://stylegan.xyz/video | Result video. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pbaylies/stylegan-encoder",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-15T16:25:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T13:39:36Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9992043603196542,
        0.8443969433331355
      ],
      "excerpt": "This is my StyleGAN Encoder; there are many like it, but this one is mine. Thanks to @Puzer for the original, of which this is a fork, and to @SimJeg for the initial code that formed the basis of the ResNet model used here, and to @Pender for his fork as well! \nFrom left to right: original image, predicted image from a ResNet trained on generated StyleGAN faces, and the final encoded image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8434568193555387
      ],
      "excerpt": "3) More loss functions for the iterative encoder to improve convergence speed and face quality. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9428120795285447,
        0.9173338186999095
      ],
      "excerpt": " * Added L1 penalty on dlatents - this keeps the representation closer to StyleGAN's concept of faces. \n4) Added support for generating videos of the encoding process! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8798512439133293
      ],
      "excerpt": "9) Added support for stochastic weight averaging of StyleGAN checkpoints \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285903115041672,
        0.9270518359467552
      ],
      "excerpt": "11) Follow @Puzer's instructions below for encoder usage as well, all of that still applies! \nNew Features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": " - ggt and lbfgs optimizers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8765248809566042,
        0.9014397645799872,
        0.8774331873650348,
        0.9216312064590996
      ],
      "excerpt": "  --model_res MODEL_RES                   The dimension of images in the StyleGAN model (default: 1024) \n  --batch_size BATCH_SIZE                 Batch size for generator and perceptual model (default: 1) \n  --image_size IMAGE_SIZE                 Size of images for perceptual model (default: 256) \n  --resnet_image_size RESNET_IMAGE_SIZE   Size of images for the Resnet model (default: 256)                       \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9744585173855465
      ],
      "excerpt": "  --load_resnet LOAD_RESNET               Model to load for Resnet approximation of dlatents \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8517715979687207
      ],
      "excerpt": "  --randomize_noise RANDOMIZE_NOISE       Add noise to dlatents during optimization (default: False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8867860319730773
      ],
      "excerpt": "  --clipping_threshold CLIPPING_THRESHOLD Stochastic clipping of gradient values outside of this threshold (default: 2.0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860359885071516
      ],
      "excerpt": "  --face_mask FACE_MASK Generate a mask for predicting only the face area (default: False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8281787531708814
      ],
      "excerpt": "  --scale_mask SCALE_MASK Look over a wider section of foreground for grabcut (default: 1.5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9076524912789271
      ],
      "excerpt": "  --output_video OUTPUT_VIDEO             Generate videos of the optimization process (default: False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8765248809566042
      ],
      "excerpt": "                        The dimension of images in the StyleGAN model (default: 1024) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8647993007062746
      ],
      "excerpt": "                        Model size - 0 - small, 1 - medium, 2 - large, or 3 - full size. (default: 1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774331873650348
      ],
      "excerpt": "                        Size of images for EfficientNet model (default: 256) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8398781554771957
      ],
      "excerpt": "                        Batch size for training the EfficientNet model (default: 2048) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8802544306060907
      ],
      "excerpt": "                        Batch size for testing the EfficientNet model (default: 512) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8765248809566042
      ],
      "excerpt": "                        The dimension of images in the StyleGAN model (default: 1024) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8094760922680225
      ],
      "excerpt": "                        Model size - 0 - small, 1 - medium, 2 - large. (default: 0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941562285905135
      ],
      "excerpt": "                        Size of images for ResNet model (default: 256) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663237183758161
      ],
      "excerpt": "                        Batch size for training the ResNet model (default: 2048) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8980247087670322
      ],
      "excerpt": "                        Batch size for testing the ResNet model (default: 512) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.952160098330463
      ],
      "excerpt": "                        The dimension of images for input to the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8764386300506086
      ],
      "excerpt": "Short explanation of encoding approach: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9838520084570767,
        0.944877158039604,
        0.9618608456017658,
        0.9326185996812886
      ],
      "excerpt": "1) Pre-trained VGG16 network is used for transforming a reference image and generated image into high-level features space \n2) Loss is calculated as a difference between them in the features space \n3) Optimization is performed only for latent representation which we want to obtain.  \n4) Upon completion of optimization you are able to transform your latent vector as you wish. For example you can find a \"smiling direction\" in your latent space, move your latent vector in this direction and transform it back to image using the generator.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9313201443769926,
        0.8789262697748386,
        0.9854402611856209
      ],
      "excerpt": "Feel free to join the research. There is still much room for improvement: \n1) Better model for perceptual loss \n2) Is it possible to generate latent representations by using other model instead of direct optimization ? (WIP) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9746567879134004
      ],
      "excerpt": "This repository contains (no longer) official TensorFlow implementation of the following paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944976272873034
      ],
      "excerpt": "Abstract: We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9694802929823498
      ],
      "excerpt": "All material related to our paper is available via the following links: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051988754586414
      ],
      "excerpt": "| &boxvr;&nbsp; stylegan-paper.pdf | High-quality version of the paper PDF. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216983964400391
      ],
      "excerpt": "| &ensp;&ensp; &boxur;&nbsp; metrics | Auxiliary networks for the quality and disentanglement metrics. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8919362615671552,
        0.8412464389363853,
        0.8760296784729327
      ],
      "excerpt": "| &ensp;&ensp; &ensp;&ensp; &boxvr;&nbsp; celebahq-classifier-00-male.pkl | Binary classifier trained to detect a single attribute of CelebA-HQ. \n| &ensp;&ensp; &ensp;&ensp; &boxur;&nbsp;&#x22ef; | Please see the file listing for remaining networks. \nA minimal example of using a pre-trained StyleGAN generator is given in pretrained_example.py. When executed, the script downloads a pre-trained StyleGAN generator from Google Drive and uses it to generate an image: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922213228298583
      ],
      "excerpt": "A more advanced example is given in generate_figures.py. The script reproduces the figures from our paper in order to illustrate style mixing, noise inputs, and truncation: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8775776390329579
      ],
      "excerpt": "    #: Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9651401881890899,
        0.8830907854839266
      ],
      "excerpt": "   The first argument is a batch of latent vectors of shape[num, 512]. The second argument is reserved for class labels (not used by StyleGAN). The remaining keyword arguments are optional and can be used to further modify the operation (see below). The output is a batch of images, whose format is dictated by theoutput_transform` argument. \nUse Gs.get_output_for() to incorporate the generator as a part of a larger TensorFlow expression: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589038510961784,
        0.9627019913336683,
        0.8618246469878545
      ],
      "excerpt": "   The above code is from metrics/frechet_inception_distance.py. It generates a batch of random images and feeds them directly to the Inception-v3 network without having to convert the data to numpy arrays in between. \nLook up Gs.components.mapping and Gs.components.synthesis to access individual sub-networks of the generator. Similar to Gs, the sub-networks are represented as independent instances of dnnlib.tflib.Network: \n   src_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in src_seeds) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589643029001007,
        0.9029463808950026,
        0.9612225415270402
      ],
      "excerpt": "   The above code is from generate_figures.py. It first transforms a batch of latent vectors into the intermediate W space using the mapping network and then turns these vectors into a batch of images using the synthesis network. The dlatents array stores a separate copy of the same w vector for each layer of the synthesis network to facilitate style mixing. \nThe exact details of the generator are defined in training/networks_stylegan.py (see G_style, G_mapping, and G_synthesis). The following keyword arguments can be specified to modify the behavior when calling run() and get_output_for(): \ntruncation_psi and truncation_cutoff control the truncation trick that that is performed by default when using Gs (&psi;=0.7, cutoff=8). It can be disabled by setting truncation_psi=1 or is_validation=True, and the image quality can be further improved at the cost of variation by setting e.g. truncation_psi=0.5. Note that truncation is always disabled when using the sub-networks directly. The average w needed to manually perform the truncation trick can be looked up using Gs.get_var('dlatent_avg'). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979508650824699,
        0.9225017619251502
      ],
      "excerpt": "When using the mapping network directly, you can specify dlatent_broadcast=None to disable the automatic duplication of dlatents over the layers of the synthesis network. \nRuntime performance can be fine-tuned via structure='fixed' and dtype='float16'. The former disables support for progressive growing, which is not needed for a fully-trained generator, and the latter performs all computation using half-precision floating point arithmetic. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8086294239246505,
        0.9418454074084751,
        0.8085639336463422
      ],
      "excerpt": "The training may take several days (or weeks) to complete, depending on the configuration. \nBy default, train.py is configured to train the highest-quality StyleGAN (configuration F in Table 1) for the FFHQ dataset at 1024&times;1024 resolution using 8 GPUs. Please note that we have used 8 GPUs in all of our experiments. Training with fewer GPUs may not produce identical results &ndash; if you wish to compare against our technique, we strongly recommend using the same number of GPUs. \nExpected training time for 1024&times;1024 resolution using Tesla V100 GPUs: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806255452618748
      ],
      "excerpt": "The quality and disentanglement metrics used in our paper can be evaluated using run_metrics.py. By default, the script will evaluate the Fr&eacute;chet Inception Distance (fid50k) for the pre-trained FFHQ generator and write the results into a newly created directory under results. The exact behavior can be changed by uncommenting or editing specific lines in run_metrics.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "StyleGAN Encoder - converts real images to latent space",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pbaylies/stylegan-encoder/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 168,
      "date": "Wed, 22 Dec 2021 19:43:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pbaylies/stylegan-encoder/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pbaylies/stylegan-encoder",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pbaylies/stylegan-encoder/master/Play_with_latent_directions.ipynb",
      "https://raw.githubusercontent.com/pbaylies/stylegan-encoder/master/StyleGAN_Encoder_Tutorial.ipynb",
      "https://raw.githubusercontent.com/pbaylies/stylegan-encoder/master/Learn_direction_in_latent_space.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pbaylies/stylegan-encoder/master/robust_loss/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The training and evaluation scripts operate on datasets stored as multi-resolution TFRecords. Each dataset is represented by a directory containing the same image data in several resolutions to enable efficient streaming. There is a separate *.tfrecords file for each resolution, and if the dataset contains labels, they are stored in a separate file as well. By default, the scripts expect to find the datasets at `datasets/<NAME>/<NAME>-<RESOLUTION>.tfrecords`. The directory can be changed by editing [config.py](./config.py):\n\n```\nresult_dir = 'results'\ndata_dir = 'datasets'\ncache_dir = 'cache'\n```\n\nTo obtain the FFHQ dataset (`datasets/ffhq`), please refer to the [Flickr-Faces-HQ repository](http://stylegan.xyz/ffhq).\n\nTo obtain the CelebA-HQ dataset (`datasets/celebahq`), please refer to the [Progressive GAN repository](https://github.com/tkarras/progressive_growing_of_gans).\n\nTo obtain other datasets, including LSUN, please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided [dataset_tool.py](./dataset_tool.py):\n\n```\n> python dataset_tool.py create_lsun datasets/lsun-bedroom-full ~/lsun/bedroom_lmdb --resolution 256\n> python dataset_tool.py create_lsun_wide datasets/lsun-car-512x384 ~/lsun/car_lmdb --width 512 --height 384\n> python dataset_tool.py create_lsun datasets/lsun-cat-full ~/lsun/cat_lmdb --resolution 256\n> python dataset_tool.py create_cifar10 datasets/cifar10 ~/cifar10\n> python dataset_tool.py create_from_images datasets/custom-dataset ~/custom-images\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.843355984723316
      ],
      "excerpt": "2) Drop-in replacement to use an EfficientNet based encoder with train_effnet.py - thanks to @qubvel for his Keras implementation of EfficientNets! Install from source to get the latest version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8089539702491283
      ],
      "excerpt": "  dlatent_dir           Directory for storing dlatent representations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9264361456577099
      ],
      "excerpt": "  --load_last LOAD_LAST                   Start with embeddings from directory (default: ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8337973035193803
      ],
      "excerpt": "More examples you can find in the Jupyter notebook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9645914629050719
      ],
      "excerpt": "3) Then you can play with Jupyter notebook \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9122106277232005
      ],
      "excerpt": "1) The ResNet encoder - train your own with train_resnet.py or download my pre-trained model! Put the model in data/finetuned_resnet.h5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8291043705234369
      ],
      "excerpt": "10) A tutorial notebook! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778487586960795
      ],
      "excerpt": "Example presets: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8729495550295081
      ],
      "excerpt": "python encode_images.py --optimizer=lbfgs --face_mask=True --iterations=6 --use_lpips_loss=0 --use_discriminator_loss=0 aligned_images generated_images latent_representations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8797254017557662
      ],
      "excerpt": "python encode_images.py --optimizer=ggt --face_mask=True --iterations=30 --lr=0.2 --decay_rate=0.95 --decay_steps=6 --use_lpips_loss=0 --use_discriminator_loss=0 aligned_images generated_images latent_representations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8977936188078832
      ],
      "excerpt": "python encode_images.py --optimizer=adam --lr=0.02 --face_mask=True --iterations=100 --use_lpips_loss=0 --use_discriminator_loss=0 aligned_images generated_images latent_representations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9291341996298503,
        0.921516418145452
      ],
      "excerpt": "python encode_images.py --optimizer=adam --lr=0.02 --decay_rate=0.95 --decay_steps=8 --face_mask=True --iterations=100 --batch_size=4 --early_stopping=True --early_stopping_threshold=0.25 --average_best_loss=0.5 --use_lpips_loss=0 --use_discriminator_loss=0 aligned_images generated_images latent_representations \npython encode_images.py --optimizer=adam --lr=0.02 --decay_rate=0.95 --decay_steps=6 --face_mask=True --iterations=100 --use_adaptive_loss=True --early_stopping_threshold=0.5 --average_best_loss=0.5 --use_lpips_loss=0 --use_discriminator_loss=0 --batch_size=4 --output_video=True aligned_images generated_images latent_representations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965933350679125
      ],
      "excerpt": "python encode_images.py --optimizer=adam --lr=0.02 --decay_rate=0.95 --decay_steps=6 --face_mask=True --iterations=400 --early_stopping=True --early_stopping_threshold=0.05 --average_best_loss=0.5 --use_lpips_loss=0 --use_discriminator_loss=0 aligned_images generated_images latent_representations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456292920265406
      ],
      "excerpt": "usage: encode_images.py [-h] [--data_dir DATA_DIR] [--mask_dir MASK_DIR] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8405528510558415,
        0.8184974067912293
      ],
      "excerpt": "  --data_dir DATA_DIR                     Directory for storing optional models (default: data) \n  --load_last LOAD_LAST                   Start with embeddings from directory (default: ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8042505415566975
      ],
      "excerpt": "  --resnet_image_size RESNET_IMAGE_SIZE   Size of images for the Resnet model (default: 256)                       \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8643953164753937
      ],
      "excerpt": "  --face_mask FACE_MASK Generate a mask for predicting only the face area (default: False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456292920265406
      ],
      "excerpt": "usage: train_effnet.py [-h] [--model_url MODEL_URL] [--model_res MODEL_RES] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8509714194509729
      ],
      "excerpt": "Train an EfficientNet to predict latent representations of images in a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.836918431255318
      ],
      "excerpt": "  --data_dir DATA_DIR   Directory for storing the EfficientNet model (default: data) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8688287214928414
      ],
      "excerpt": "                        Save / load / create the EfficientNet model with this file path (default: data/finetuned_effnet.h5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.868979764582265
      ],
      "excerpt": "  --use_fp16 USE_FP16   Use 16-bit floating point (default: False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860817790239783
      ],
      "excerpt": "                        Batch size for training the EfficientNet model (default: 2048) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8497414171216929
      ],
      "excerpt": "                        Batch size for testing the EfficientNet model (default: 512) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059811075638323
      ],
      "excerpt": "                        Generate images using truncation trick (default: 0.7) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8531178982930181
      ],
      "excerpt": "                        Start training with the pre-trained network frozen, then unfreeze (default: False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456292920265406
      ],
      "excerpt": "usage: train_resnet.py [-h] [--model_url MODEL_URL] [--model_res MODEL_RES] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8834538809623521
      ],
      "excerpt": "Train a ResNet to predict latent representations of images in a StyleGAN model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8443167675126109
      ],
      "excerpt": "  --data_dir DATA_DIR   Directory for storing the ResNet model (default: data) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8756870039171221,
        0.868979764582265
      ],
      "excerpt": "                        Save / load / create the ResNet model with this file path (default: data/finetuned_resnet.h5) \n  --use_fp16 USE_FP16   Use 16-bit floating point (default: False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8086711636760296
      ],
      "excerpt": "                        Size of images for ResNet model (default: 256) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8672797079808368
      ],
      "excerpt": "                        Batch size for training the ResNet model (default: 2048) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8578195601576672
      ],
      "excerpt": "                        Batch size for testing the ResNet model (default: 512) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8531178982930181
      ],
      "excerpt": "                        Start training with the pre-trained network frozen, then unfreeze (default: False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456292920265406
      ],
      "excerpt": "usage: swa.py [-h] [--filespec FILESPEC] [--output_model OUTPUT_MODEL] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471921661993329
      ],
      "excerpt": "                        The averaged model to output (default: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456292920265406
      ],
      "excerpt": "usage: align_images.py [-h] [--output_size OUTPUT_SIZE] [--x_scale X_SCALE] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088262392535559
      ],
      "excerpt": "  raw_dir               Directory with raw images for face alignment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python align_images.py raw_images/ aligned_images/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python encode_images.py aligned_images/ generated_images/ latent_representations/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8749076941118147
      ],
      "excerpt": "| StyleGAN | Main folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python pretrained_example.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060372108626078
      ],
      "excerpt": "Gs                              Params    OutputShape          WeightShape \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.853492186136904
      ],
      "excerpt": "Total                           26219627 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python generate_figures.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737288687529231
      ],
      "excerpt": "   rnd = np.random.RandomState(5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179188390243047,
        0.8970261090115176
      ],
      "excerpt": "   fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True) \n   images = Gs.run(latents, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8843981688057236
      ],
      "excerpt": "   images = Gs_clone.get_output_for(latents, None, is_validation=True, randomize_noise=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8927949441196024
      ],
      "excerpt": "   src_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in src_seeds) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9523651689170409
      ],
      "excerpt": "Run the training script with python train.py. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pbaylies/stylegan-encoder/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# StyleGAN &mdash; Encoder for Official TensorFlow Implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "stylegan-encoder",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pbaylies",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pbaylies/stylegan-encoder/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Both Linux and Windows are supported, but we strongly recommend Linux for performance and compatibility reasons.\n* 64-bit Python 3.6 installation. We recommend Anaconda3 with numpy 1.14.3 or newer.\n* TensorFlow 1.10.0 or newer with GPU support.\n* One or more high-end NVIDIA GPUs with at least 11GB of DRAM. We recommend NVIDIA DGX-1 with 8 Tesla V100 GPUs.\n* NVIDIA driver 391.35 or newer, CUDA toolkit 9.0 or newer, cuDNN 7.3.1 or newer.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 649,
      "date": "Wed, 22 Dec 2021 19:43:40 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "generative-adversarial-network",
      "gan",
      "resnet-50",
      "perceptual-losses",
      "loss-functions",
      "neural-networks",
      "deep-neural-networks",
      "deep-learning"
    ],
    "technique": "GitHub API"
  }
}