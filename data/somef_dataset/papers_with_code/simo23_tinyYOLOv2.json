{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1502.03167 "
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simo23/tinyYOLOv2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-08-08T08:37:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-25T15:24:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I've been searching for a Tensorflow implementation of YOLOv2 for a while but the darknet version and derivatives are not really easy to understand. This one is an hopefully easier-to-understand version of Tiny YOLOv2. The weight extraction, weights structure, weight assignment, network, inference and postprocessing are made as simple as possible.\n\nThe output of this implementation on the test image \"dog.jpg\" is the following:\n\n![alt text](https://github.com/simo23/tinyYOLOv2/blob/master/dog_output.jpg \"YOLOv2 output\")\n\nJust to be clear, this implementation is called \"tiny-yolo-voc\" on pjreddie's site and can be found here:\n\n![alt text](https://github.com/simo23/tinyYOLOv2/blob/master/pjsite.png \"YOLOv2 site\")\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8896645891425122
      ],
      "excerpt": "Extract weights from binary file of the original yolo-v2, assign them to a TF network, save ckpt, perform detection on an input image or webcam \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9213312024509731
      ],
      "excerpt": "test_webcam.py: performs detection on the webcam. It is exactly like test.py but some functions are slightly modified to take directly the frames from the webcam as inputs (instead of the image_path). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Tiny YOLOv2 in Tensorflow made simple!",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simo23/tinyYOLOv2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 30,
      "date": "Sun, 26 Dec 2021 22:47:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simo23/tinyYOLOv2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "simo23/tinyYOLOv2",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9230058942501466
      ],
      "excerpt": "The code runs at ~15fps on my laptop which has a 2GB Nvidia GeForce GTX 960M GPU \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simo23/tinyYOLOv2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "TinyYOLOv2 in Tensorflow made easier",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "tinyYOLOv2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "simo23",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simo23/tinyYOLOv2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I've implemented everything with Tensorflow 1.0, Ubuntu 16.04, Numpy 1.13.0, Python 3.4, OpenCV 3.0\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 109,
      "date": "Sun, 26 Dec 2021 22:47:40 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "yolov2",
      "tensorflow",
      "weights-loader",
      "tiny-yolo"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Clone the project and place it where you want\n- Download the binary file (~60MB) from pjreddie's site: https://pjreddie.com/media/files/yolov2-tiny-voc.weights and place it into the folder where the scripts are\n- Launch test.py or test_webcam.py. Change the input_img_path and the weights_path in the main if you want, now the network has \"dog.jpg\" as input_img. The code is now configured to run with weights and input image in the same folder as the script.\n\n```python\npython3 test.py\n```\n\n- If you are launching them for the first time, the weights will be extracted from the binary file and a ckpt will be created. Next time only the ckpt will be used!\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "I've been struggling on understanding how the binary weights file was written. I hope to save you some time by explaining how I imported the weights into a Tensorflow network:\n\n- Download the binary file from pjreddie's site: https://pjreddie.com/media/files/yolov2-tiny-voc.weights \n- Extract the weights from binary to a numpy float32 array with  weight_array = np.fromfile(weights_path, dtype='f')\n- Delete the first 4 numbers because they are not relevant\n- Define a function ( load_conv_layer ) to take a part of the array and assign it to the Tensorflow variables of the net\n- IMPORTANT: the weights order is [ 'biases','gamma','moving_mean','moving_variance','kernel'] \n- IMPORTANT: the 'biases' here refer to the beta value of the Batch Normalization. It does not refer to the biases that must be added after the conv2d because they are set all to zero! ( According to the paper by Ioffe et al. https://arxiv.org/abs/1502.03167 ) \n- IMPORTANT: the kernel weights are written in Caffe style which means they have shape = (out_dim, in_dim, height, width). They must be converted into Tensorflow style which has shape = (height, width, in_dim, out_dim)\n- IMPORTANT: in order to obtain the correct results from the weights they need to be DENORMALIZED according to Batch Normalization. It can be done in two ways: define the network with Batch Normalization and use the weights as they are OR define the net without BN ( this implementation ) and DENORMALIZE the weights. ( details are in weights_loader.py )\n- In order to verify that the weights extraction is succesfull, I check the total number of params with the number of weights into the weight file. They are both 15867885 in my case.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Another key point is how the predictions tensor is made. It is a 13x13x125 tensor. To process it better:\n\n- Convert the tensor to have shape = 13x13x5x25 = grid_cells x n_boxes_in_each_cell x n_predictions_for_each_box\n- The 25 predictions are: 2 coordinates and 2 shape values (x,y,h,w), 1 Objectness score, 20 Class scores\n- Now access to the tensor in an easy way! E.g. predictions[row, col, b, :4] will return the 2 coords and shape of the \"b\" B-Box which is in the [row,col] grid cell\n- They must be postprocessed according to the parametrization of YOLOv2. In my implementation it is made like this: \n\n```python\n\n#: Pre-defined anchors shapes!\n#: They are not coordinates of the boxes, they are height and width of the 5 anchors defined by YOLOv2\nanchors = [1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52]\nimage_height = image_width = 416\nn_grid_cells = 13\nn_b_boxes = 5\n\nfor row in range(n_grid_cells):\n  for col in range(n_grid_cells):\n    for b in range(n_b_boxes):\n\n      tx, ty, tw, th, tc = predictions[row, col, b, :5]\n      \n      #: IMPORTANT: (416) / (13) = 32! The coordinates and shape values are parametrized w.r.t center of the grid cell\n      #: They are parameterized to be in [0,1] so easier for the network to predict and learn\n      #: With the iterations on every grid cell at [row,col] they return to their original positions\n      \n      #: The x,y coordinates are: (pre-defined coordinates of the grid cell [row,col] + parametrized offset)*32 \n      center_x = (float(col) + sigmoid(tx)) * 32.0\n      center_y = (float(row) + sigmoid(ty)) * 32.0\n\n      #: Also the width and height must return to the original value by looking at the shape of the anchors\n      roi_w = np.exp(tw) * anchors[2*b + 0] * 32.0\n      roi_h = np.exp(th) * anchors[2*b + 1] * 32.0\n      \n      #: Compute the final objectness score (confidence that there is an object in the B-Box) \n      final_confidence = sigmoid(tc)\n\n      class_predictions = predictions[row, col, b, 5:]\n      class_predictions = softmax(class_predictions)\n      \n```\n\nYOLOv2 predicts parametrized values that must be converted to full size by multiplying them by 32! You can see other EQUIVALENT ways to do this but this one works fine. I've seen someone who, instead of multiplying by 32, divides by 13 and then multiplies by 416 which at the end equals a single multiplication by 32.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}