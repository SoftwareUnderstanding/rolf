{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "````\n@article{Ref 1,\n\tabstract = {This paper deals with the approximation behaviour of soft computing techniques. First, we give a survey of the results of universal approximation theorems achieved so far in various soft computing areas, mainly in fuzzy control and neural networks. We point out that these techniques have common approximation behaviour in the sense that an arbitrary function of a certain set of functions (usually the set of continuous function, C) can be approximated with arbitrary accuracy \u03b5 on a compact domain. The drawback of these results is that one needs unbounded numbers of ``building blocks'' (i.e. fuzzy sets or hidden neurons) to achieve the prescribed \u03b5 accuracy. If the number of building blocks is restricted, it is proved for some fuzzy systems that the universal approximation property is lost, moreover, the set of controllers with bounded number of rules is nowhere dense in the set of continuous functions. Therefore it is reasonable to make a trade-off between accuracy and the number of the building blocks, by determining the functional relationship between them. We survey this topic by showing the results achieved so far, and its inherent limitations. We point out that approximation rates, or constructive proofs can only be given if some characteristic of smoothness is known about the approximated function.},\n\tauthor = {Domonkos Tikk and L{\\'a}szl{\\'o} T. K{\\'o}czy and Tam{\\'a}s D. Gedeon},\n\tdoi = {https://doi.org/10.1016/S0888-613X(03)00021-5},\n\tissn = {0888-613X},\n\tjournal = {International Journal of Approximate Reasoning},\n\tkeywords = {Universal approximation performed by fuzzy systems and neural networks, Kolmogorov's theorem, Approximation behaviour of soft computing techniques, Course of dimensionality, Nowhere denseness, Approximation rates, Constructive proofs},\n\tnumber = {2},\n\tpages = {185-202},\n\ttitle = {A survey on universal approximation and its limits in soft computing techniques},\n\turl = {https://www.sciencedirect.com/science/article/pii/S0888613X03000215},\n\tvolume = {33},\n\tyear = {2003},\n\tBdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0888613X03000215},\n\tBdsk-Url-2 = {https://doi.org/10.1016/S0888-613X(03)00021-5}}\n\n````\n\n````\n@article{Ref 2,\n\tabstract = {In this paper, we present a review of some recent works on approximation by feedforward neural networks. A particular emphasis is placed on the computational aspects of the problem, i.e. we discuss the possibility of realizing a feedforward neural network which achieves a prescribed degree of accuracy of approximation, and the determination of the number of hidden layer neurons required to achieve this accuracy. Furthermore, a unifying framework is introduced to understand existing approaches to investigate the universal approximation problem using feedforward neural networks. Some new results are also presented. Finally, two training algorithms are introduced which can determine the weights of feedforward neural networks, with sigmoidal activation neurons, to any degree of prescribed accuracy. These training algorithms are designed so that they do not suffer from the problems of local minima which commonly affect neural network learning algorithms.},\n\tauthor = {Franco Scarselli and Ah {Chung Tsoi}},\n\tdoi = {https://doi.org/10.1016/S0893-6080(97)00097-X},\n\tissn = {0893-6080},\n\tjournal = {Neural Networks},\n\tkeywords = {Approximation by neural networks, Approximation of polynomials, Constructive approximation, Feedforward neural networks, Multilayer neural networks, Radial basis functions, Universal approximation},\n\tnumber = {1},\n\tpages = {15-37},\n\ttitle = {Universal Approximation Using Feedforward Neural Networks: A Survey of Some Existing Methods, and Some New Results},\n\turl = {https://www.sciencedirect.com/science/article/pii/S089360809700097X},\n\tvolume = {11},\n\tyear = {1998},\n\tBdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S089360809700097X},\n\tBdsk-Url-2 = {https://doi.org/10.1016/S0893-6080(97)00097-X}}\n````\n\n````\n@ARTICLE{Ref 3,\n  author={Bianchini, Monica and Scarselli, Franco},\n  journal={IEEE Transactions on Neural Networks and Learning Systems}, \n  title={On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures}, \n  year={2014},\n  volume={25},\n  number={8},\n  pages={1553-1565},\n  doi={10.1109/TNNLS.2013.2293637}}\n}\n````\n\n````\n@article{Ref 4,\n      title={DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators}, \n      author={Lu Lu and Pengzhan Jin and George Em Karniadakis},\n      year={2020},\n      eprint={1910.03193},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}}\n````\n\n````\n@article{Ref 5,\n      title={DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators}, \n      author={Lu Lu and Pengzhan Jin and George Em Karniadakis},\n      year={2020},\n      eprint={1910.03193},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}}\n````\n\n````\n\n@inproceedings{Ref 6,\n\tabstract = {Statistical learning algorithms often rely on the Euclidean distance. In practice, non-Euclidean or non-metric dissimilarity measures may arise when contours, spectra or shapes are compared by edit distances or as a consequence of robust object matching [1,2]. It is an open issue whether such measures are advantageous for statistical learning or whether they should be constrained to obey the metric axioms.},\n\taddress = {Berlin, Heidelberg},\n\tauthor = {P{\\k{e}}kalska, El{\\.{z}}bieta and Harol, Artsiom and Duin, Robert P. W. and Spillmann, Barbara and Bunke, Horst},\n\tbooktitle = {Structural, Syntactic, and Statistical Pattern Recognition},\n\teditor = {Yeung, Dit-Yan and Kwok, James T. and Fred, Ana and Roli, Fabio and de Ridder, Dick},\n\tisbn = {978-3-540-37241-7},\n\tpages = {871--880},\n\tpublisher = {Springer Berlin Heidelberg},\n\ttitle = {Non-Euclidean or Non-metric Measures Can Be Informative},\n\tyear = {2006}}\n\n````\n\n````\n@article{Ref 7,\nauthor = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},\ntitle = {A Survey on Bias and Fairness in Machine Learning},\nyear = {2021},\nissue_date = {July 2021},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nvolume = {54},\nnumber = {6},\nissn = {0360-0300},\nurl = {https://doi.org/10.1145/3457607},\ndoi = {10.1145/3457607},\njournal = {ACM Comput. Surv.},\nmonth = jul,\narticleno = {115},\nnumpages = {35},\nkeywords = {machine learning, deep learning, representation learning, natural language processing, Fairness and bias in artificial intelligence}\n}\n````\n\n````\n@article{ref 8,\n  author    = {Sebastian Ruder},\n  title     = {An overview of gradient descent optimization algorithms},\n  journal   = {CoRR},\n  volume    = {abs/1609.04747},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1609.04747},\n  eprinttype = {arXiv},\n  eprint    = {1609.04747},\n  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/Ruder16.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n````\n\n\n```\n@misc{Ref 9,\n      title={On the Convergence of Adam and Beyond}, \n      author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\n      year={2019},\n      eprint={1904.09237},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n```\n@article{Ref 10,\n      title={On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes}, \n      author={Xiaoyu Li and Francesco Orabona},\n      year={2019},\n      eprint={1805.08114},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n}\n```\n\n```\n@InProceedings{Ref11,\n  title = \t {Multi-Task Learning with User Preferences: Gradient Descent with Controlled Ascent in Pareto Optimization},\n  author =       {Mahapatra, Debabrata and Rajan, Vaibhav},\n  booktitle = \t {Proceedings of the 37th International Conference on Machine Learning},\n  pages = \t {6597--6607},\n  year = \t {2020},\n  editor = \t {III, Hal Daum\u00e9 and Singh, Aarti},\n  volume = \t {119},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {13--18 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v119/mahapatra20a/mahapatra20a.pdf},\n  url = \t {https://proceedings.mlr.press/v119/mahapatra20a.html},\n  abstract = \t {Multi-Task Learning (MTL) is a well established paradigm for jointly learning models for multiple correlated tasks. Often the tasks conflict, requiring trade-offs between them during optimization. In such cases, multi-objective optimization based MTL methods can be used to find one or more Pareto optimal solutions. A common requirement in MTL applications, that cannot be addressed by these methods, is to find a solution satisfying userspecified preferences with respect to task-specific losses. We advance the state-of-the-art by developing the first gradient-based multi-objective MTL algorithm to solve this problem. Our unique approach combines multiple gradient descent with carefully controlled ascent to traverse the Pareto front in a principled manner, which also makes it robust to initialization. The scalability of our algorithm enables its use in large-scale deep networks for MTL. Assuming only differentiability of the task-specific loss functions, we provide theoretical guarantees for convergence. Our experiments show that our algorithm outperforms the best competing methods on benchmark datasets.}\n}\n\n```\n\n```\n@misc{Ref 12,\n      title={Deep contextualized word representations}, \n      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},\n      year={2018},\n      eprint={1802.05365},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n```\n\n```\n@misc{Ref 13,\n      title={Neural Machine Translation by Jointly Learning to Align and Translate}, \n      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},\n      year={2016},\n      eprint={1409.0473},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n--------------------------------------\n\n````\n@book{Ref i-Ref ii,\n        title={Deep Learning},\n        author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},\n        publisher={MIT Press},\n        note={\\url{http://www.deeplearningbook.org}},\n        year={2016}\n }\n\n````\n\n````\n@article{Ref iii,\n\tabstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},\n\tauthor = {Cybenko, G. },\n\tda = {1989/12/01},\n\tdate-added = {2021-09-02 13:04:35 -0700},\n\tdate-modified = {2021-09-02 13:04:35 -0700},\n\tdoi = {10.1007/BF02551274},\n\tid = {Cybenko1989},\n\tisbn = {1435-568X},\n\tjournal = {Mathematics of Control, Signals and Systems},\n\tnumber = {4},\n\tpages = {303--314},\n\ttitle = {Approximation by superpositions of a sigmoidal function},\n\tty = {JOUR},\n\turl = {https://doi.org/10.1007/BF02551274},\n\tvolume = {2},\n\tyear = {1989},\n\tBdsk-Url-1 = {https://doi.org/10.1007/BF02551274}}\n````\n\n````\n@book{Ref iv,\n        title={Infinite Dimensional Analysis: A Hitchhiker\u2019s Guide},\n        author={Charalambos D. AliprantisKim C. Border},\n        publisher={Springer},\n        note={\\url{https://link.springer.com/book/10.1007%2F3-540-29587-9}},\n        year={2006}\n }\n````\n\n````\n@ARTICLE{Ref v,\n  author={Tianping Chen and Hong Chen},\n  journal={IEEE Transactions on Neural Networks}, \n  title={Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems}, \n  year={1995},\n  volume={6},\n  number={4},\n  pages={911-917},\n  doi={10.1109/72.392253}}\n````\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "````\n@article{Ref 1,\n\tabstract = {This paper deals with the approximation behaviour of soft computing techniques. First, we give a survey of the results of universal approximation theorems achieved so far in various soft computing areas, mainly in fuzzy control and neural networks. We point out that these techniques have common approximation behaviour in the sense that an arbitrary function of a certain set of functions (usually the set of continuous function, C) can be approximated with arbitrary accuracy \u03b5 on a compact domain. The drawback of these results is that one needs unbounded numbers of ``building blocks'' (i.e. fuzzy sets or hidden neurons) to achieve the prescribed \u03b5 accuracy. If the number of building blocks is restricted, it is proved for some fuzzy systems that the universal approximation property is lost, moreover, the set of controllers with bounded number of rules is nowhere dense in the set of continuous functions. Therefore it is reasonable to make a trade-off between accuracy and the number of the building blocks, by determining the functional relationship between them. We survey this topic by showing the results achieved so far, and its inherent limitations. We point out that approximation rates, or constructive proofs can only be given if some characteristic of smoothness is known about the approximated function.},\n\tauthor = {Domonkos Tikk and L{\\'a}szl{\\'o} T. K{\\'o}czy and Tam{\\'a}s D. Gedeon},\n\tdoi = {https://doi.org/10.1016/S0888-613X(03)00021-5},\n\tissn = {0888-613X},\n\tjournal = {International Journal of Approximate Reasoning},\n\tkeywords = {Universal approximation performed by fuzzy systems and neural networks, Kolmogorov's theorem, Approximation behaviour of soft computing techniques, Course of dimensionality, Nowhere denseness, Approximation rates, Constructive proofs},\n\tnumber = {2},\n\tpages = {185-202},\n\ttitle = {A survey on universal approximation and its limits in soft computing techniques},\n\turl = {https://www.sciencedirect.com/science/article/pii/S0888613X03000215},\n\tvolume = {33},\n\tyear = {2003},\n\tBdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0888613X03000215},\n\tBdsk-Url-2 = {https://doi.org/10.1016/S0888-613X(03)00021-5}}\n\n````\n\n````\n@article{Ref 2,\n\tabstract = {In this paper, we present a review of some recent works on approximation by feedforward neural networks. A particular emphasis is placed on the computational aspects of the problem, i.e. we discuss the possibility of realizing a feedforward neural network which achieves a prescribed degree of accuracy of approximation, and the determination of the number of hidden layer neurons required to achieve this accuracy. Furthermore, a unifying framework is introduced to understand existing approaches to investigate the universal approximation problem using feedforward neural networks. Some new results are also presented. Finally, two training algorithms are introduced which can determine the weights of feedforward neural networks, with sigmoidal activation neurons, to any degree of prescribed accuracy. These training algorithms are designed so that they do not suffer from the problems of local minima which commonly affect neural network learning algorithms.},\n\tauthor = {Franco Scarselli and Ah {Chung Tsoi}},\n\tdoi = {https://doi.org/10.1016/S0893-6080(97)00097-X},\n\tissn = {0893-6080},\n\tjournal = {Neural Networks},\n\tkeywords = {Approximation by neural networks, Approximation of polynomials, Constructive approximation, Feedforward neural networks, Multilayer neural networks, Radial basis functions, Universal approximation},\n\tnumber = {1},\n\tpages = {15-37},\n\ttitle = {Universal Approximation Using Feedforward Neural Networks: A Survey of Some Existing Methods, and Some New Results},\n\turl = {https://www.sciencedirect.com/science/article/pii/S089360809700097X},\n\tvolume = {11},\n\tyear = {1998},\n\tBdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S089360809700097X},\n\tBdsk-Url-2 = {https://doi.org/10.1016/S0893-6080(97)00097-X}}\n````\n\n````\n@ARTICLE{Ref 3,\n  author={Bianchini, Monica and Scarselli, Franco},\n  journal={IEEE Transactions on Neural Networks and Learning Systems}, \n  title={On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures}, \n  year={2014},\n  volume={25},\n  number={8},\n  pages={1553-1565},\n  doi={10.1109/TNNLS.2013.2293637}}\n}\n````\n\n````\n@article{Ref 4,\n      title={DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators}, \n      author={Lu Lu and Pengzhan Jin and George Em Karniadakis},\n      year={2020},\n      eprint={1910.03193},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}}\n````\n\n````\n@article{Ref 5,\n      title={DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators}, \n      author={Lu Lu and Pengzhan Jin and George Em Karniadakis},\n      year={2020},\n      eprint={1910.03193},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}}\n````\n\n````\n\n@inproceedings{Ref 6,\n\tabstract = {Statistical learning algorithms often rely on the Euclidean distance. In practice, non-Euclidean or non-metric dissimilarity measures may arise when contours, spectra or shapes are compared by edit distances or as a consequence of robust object matching [1,2]. It is an open issue whether such measures are advantageous for statistical learning or whether they should be constrained to obey the metric axioms.},\n\taddress = {Berlin, Heidelberg},\n\tauthor = {P{\\k{e}}kalska, El{\\.{z}}bieta and Harol, Artsiom and Duin, Robert P. W. and Spillmann, Barbara and Bunke, Horst},\n\tbooktitle = {Structural, Syntactic, and Statistical Pattern Recognition},\n\teditor = {Yeung, Dit-Yan and Kwok, James T. and Fred, Ana and Roli, Fabio and de Ridder, Dick},\n\tisbn = {978-3-540-37241-7},\n\tpages = {871--880},\n\tpublisher = {Springer Berlin Heidelberg},\n\ttitle = {Non-Euclidean or Non-metric Measures Can Be Informative},\n\tyear = {2006}}\n\n````\n\n````\n@article{Ref 7,\nauthor = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},\ntitle = {A Survey on Bias and Fairness in Machine Learning},\nyear = {2021},\nissue_date = {July 2021},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nvolume = {54},\nnumber = {6},\nissn = {0360-0300},\nurl = {https://doi.org/10.1145/3457607},\ndoi = {10.1145/3457607},\njournal = {ACM Comput. Surv.},\nmonth = jul,\narticleno = {115},\nnumpages = {35},\nkeywords = {machine learning, deep learning, representation learning, natural language processing, Fairness and bias in artificial intelligence}\n}\n````\n\n````\n@article{ref 8,\n  author    = {Sebastian Ruder},\n  title     = {An overview of gradient descent optimization algorithms},\n  journal   = {CoRR},\n  volume    = {abs/1609.04747},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1609.04747},\n  eprinttype = {arXiv},\n  eprint    = {1609.04747},\n  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/Ruder16.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n````\n\n\n```\n@misc{Ref 9,\n      title={On the Convergence of Adam and Beyond}, \n      author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\n      year={2019},\n      eprint={1904.09237},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n```\n@article{Ref 10,\n      title={On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes}, \n      author={Xiaoyu Li and Francesco Orabona},\n      year={2019},\n      eprint={1805.08114},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n}\n```\n\n```\n@InProceedings{Ref11,\n  title = \t {Multi-Task Learning with User Preferences: Gradient Descent with Controlled Ascent in Pareto Optimization},\n  author =       {Mahapatra, Debabrata and Rajan, Vaibhav},\n  booktitle = \t {Proceedings of the 37th International Conference on Machine Learning},\n  pages = \t {6597--6607},\n  year = \t {2020},\n  editor = \t {III, Hal Daum\u00e9 and Singh, Aarti},\n  volume = \t {119},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {13--18 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v119/mahapatra20a/mahapatra20a.pdf},\n  url = \t {https://proceedings.mlr.press/v119/mahapatra20a.html},\n  abstract = \t {Multi-Task Learning (MTL) is a well established paradigm for jointly learning models for multiple correlated tasks. Often the tasks conflict, requiring trade-offs between them during optimization. In such cases, multi-objective optimization based MTL methods can be used to find one or more Pareto optimal solutions. A common requirement in MTL applications, that cannot be addressed by these methods, is to find a solution satisfying userspecified preferences with respect to task-specific losses. We advance the state-of-the-art by developing the first gradient-based multi-objective MTL algorithm to solve this problem. Our unique approach combines multiple gradient descent with carefully controlled ascent to traverse the Pareto front in a principled manner, which also makes it robust to initialization. The scalability of our algorithm enables its use in large-scale deep networks for MTL. Assuming only differentiability of the task-specific loss functions, we provide theoretical guarantees for convergence. Our experiments show that our algorithm outperforms the best competing methods on benchmark datasets.}\n}\n\n```\n\n```\n@misc{Ref 12,\n      title={Deep contextualized word representations}, \n      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},\n      year={2018},\n      eprint={1802.05365},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n```\n\n```\n@misc{Ref 13,\n      title={Neural Machine Translation by Jointly Learning to Align and Translate}, \n      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},\n      year={2016},\n      eprint={1409.0473},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n--------------------------------------\n\n````\n@book{Ref i-Ref ii,\n        title={Deep Learning},\n        author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},\n        publisher={MIT Press},\n        note={\\url{http://www.deeplearningbook.org}},\n        year={2016}\n }\n\n````\n\n````\n@article{Ref iii,\n\tabstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},\n\tauthor = {Cybenko, G. },\n\tda = {1989/12/01},\n\tdate-added = {2021-09-02 13:04:35 -0700},\n\tdate-modified = {2021-09-02 13:04:35 -0700},\n\tdoi = {10.1007/BF02551274},\n\tid = {Cybenko1989},\n\tisbn = {1435-568X},\n\tjournal = {Mathematics of Control, Signals and Systems},\n\tnumber = {4},\n\tpages = {303--314},\n\ttitle = {Approximation by superpositions of a sigmoidal function},\n\tty = {JOUR},\n\turl = {https://doi.org/10.1007/BF02551274},\n\tvolume = {2},\n\tyear = {1989},\n\tBdsk-Url-1 = {https://doi.org/10.1007/BF02551274}}\n````\n\n````\n@book{Ref iv,\n        title={Infinite Dimensional Analysis: A Hitchhiker\u2019s Guide},\n        author={Charalambos D. AliprantisKim C. Border},\n        publisher={Springer},\n        note={\\url{https://link.springer.com/book/10.1007%2F3-540-29587-9}},\n        year={2006}\n }\n````\n\n````\n@ARTICLE{Ref v,\n  author={Tianping Chen and Hong Chen},\n  journal={IEEE Transactions on Neural Networks}, \n  title={Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems}, \n  year={1995},\n  volume={6},\n  number={4},\n  pages={911-917},\n  doi={10.1109/72.392253}}\n````\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{Ref 13,\n      title={Neural Machine Translation by Jointly Learning to Align and Translate}, \n      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},\n      year={2016},\n      eprint={1409.0473},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{Ref 12,\n      title={Deep contextualized word representations}, \n      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},\n      year={2018},\n      eprint={1802.05365},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{Ref11,\n  title =    {Multi-Task Learning with User Preferences: Gradient Descent with Controlled Ascent in Pareto Optimization},\n  author =       {Mahapatra, Debabrata and Rajan, Vaibhav},\n  booktitle =    {Proceedings of the 37th International Conference on Machine Learning},\n  pages =    {6597--6607},\n  year =     {2020},\n  editor =   {III, Hal Daum\u00e9 and Singh, Aarti},\n  volume =   {119},\n  series =   {Proceedings of Machine Learning Research},\n  month =    {13--18 Jul},\n  publisher =    {PMLR},\n  pdf =      {http://proceedings.mlr.press/v119/mahapatra20a/mahapatra20a.pdf},\n  url =      {https://proceedings.mlr.press/v119/mahapatra20a.html},\n  abstract =     {Multi-Task Learning (MTL) is a well established paradigm for jointly learning models for multiple correlated tasks. Often the tasks conflict, requiring trade-offs between them during optimization. In such cases, multi-objective optimization based MTL methods can be used to find one or more Pareto optimal solutions. A common requirement in MTL applications, that cannot be addressed by these methods, is to find a solution satisfying userspecified preferences with respect to task-specific losses. We advance the state-of-the-art by developing the first gradient-based multi-objective MTL algorithm to solve this problem. Our unique approach combines multiple gradient descent with carefully controlled ascent to traverse the Pareto front in a principled manner, which also makes it robust to initialization. The scalability of our algorithm enables its use in large-scale deep networks for MTL. Assuming only differentiability of the task-specific loss functions, we provide theoretical guarantees for convergence. Our experiments show that our algorithm outperforms the best competing methods on benchmark datasets.}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Ref 10,\n      title={On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes}, \n      author={Xiaoyu Li and Francesco Orabona},\n      year={2019},\n      eprint={1805.08114},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{Ref 9,\n      title={On the Convergence of Adam and Beyond}, \n      author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},\n      year={2019},\n      eprint={1904.09237},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{ref 8,\n  author    = {Sebastian Ruder},\n  title     = {An overview of gradient descent optimization algorithms},\n  journal   = {CoRR},\n  volume    = {abs/1609.04747},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1609.04747},\n  eprinttype = {arXiv},\n  eprint    = {1609.04747},\n  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/Ruder16.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Ref 6,\n    abstract = {Statistical learning algorithms often rely on the Euclidean distance. In practice, non-Euclidean or non-metric dissimilarity measures may arise when contours, spectra or shapes are compared by edit distances or as a consequence of robust object matching [1,2]. It is an open issue whether such measures are advantageous for statistical learning or whether they should be constrained to obey the metric axioms.},\n    address = {Berlin, Heidelberg},\n    author = {P{\\k{e}}kalska, El{.{z}}bieta and Harol, Artsiom and Duin, Robert P. W. and Spillmann, Barbara and Bunke, Horst},\n    booktitle = {Structural, Syntactic, and Statistical Pattern Recognition},\n    editor = {Yeung, Dit-Yan and Kwok, James T. and Fred, Ana and Roli, Fabio and de Ridder, Dick},\n    isbn = {978-3-540-37241-7},\n    pages = {871--880},\n    publisher = {Springer Berlin Heidelberg},\n    title = {Non-Euclidean or Non-metric Measures Can Be Informative},\n    year = {2006}}\n````\n@article{Ref 7,\nauthor = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},\ntitle = {A Survey on Bias and Fairness in Machine Learning},\nyear = {2021},\nissue_date = {July 2021},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nvolume = {54},\nnumber = {6},\nissn = {0360-0300},\nurl = {https://doi.org/10.1145/3457607},\ndoi = {10.1145/3457607},\njournal = {ACM Comput. Surv.},\nmonth = jul,\narticleno = {115},\nnumpages = {35},\nkeywords = {machine learning, deep learning, representation learning, natural language processing, Fairness and bias in artificial intelligence}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Ref 5,\n      title={DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators}, \n      author={Lu Lu and Pengzhan Jin and George Em Karniadakis},\n      year={2020},\n      eprint={1910.03193},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Ref 4,\n      title={DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators}, \n      author={Lu Lu and Pengzhan Jin and George Em Karniadakis},\n      year={2020},\n      eprint={1910.03193},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Ref 1,\n    abstract = {This paper deals with the approximation behaviour of soft computing techniques. First, we give a survey of the results of universal approximation theorems achieved so far in various soft computing areas, mainly in fuzzy control and neural networks. We point out that these techniques have common approximation behaviour in the sense that an arbitrary function of a certain set of functions (usually the set of continuous function, C) can be approximated with arbitrary accuracy \u03b5 on a compact domain. The drawback of these results is that one needs unbounded numbers ofbuilding blocks'' (i.e. fuzzy sets or hidden neurons) to achieve the prescribed \u03b5 accuracy. If the number of building blocks is restricted, it is proved for some fuzzy systems that the universal approximation property is lost, moreover, the set of controllers with bounded number of rules is nowhere dense in the set of continuous functions. Therefore it is reasonable to make a trade-off between accuracy and the number of the building blocks, by determining the functional relationship between them. We survey this topic by showing the results achieved so far, and its inherent limitations. We point out that approximation rates, or constructive proofs can only be given if some characteristic of smoothness is known about the approximated function.},\n    author = {Domonkos Tikk and L{\\'a}szl{\\'o} T. K{\\'o}czy and Tam{\\'a}s D. Gedeon},\n    doi = {https://doi.org/10.1016/S0888-613X(03)00021-5},\n    issn = {0888-613X},\n    journal = {International Journal of Approximate Reasoning},\n    keywords = {Universal approximation performed by fuzzy systems and neural networks, Kolmogorov's theorem, Approximation behaviour of soft computing techniques, Course of dimensionality, Nowhere denseness, Approximation rates, Constructive proofs},\n    number = {2},\n    pages = {185-202},\n    title = {A survey on universal approximation and its limits in soft computing techniques},\n    url = {https://www.sciencedirect.com/science/article/pii/S0888613X03000215},\n    volume = {33},\n    year = {2003},\n    Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0888613X03000215},\n    Bdsk-Url-2 = {https://doi.org/10.1016/S0888-613X(03)00021-5}}\n````\n@article{Ref 2,\n    abstract = {In this paper, we present a review of some recent works on approximation by feedforward neural networks. A particular emphasis is placed on the computational aspects of the problem, i.e. we discuss the possibility of realizing a feedforward neural network which achieves a prescribed degree of accuracy of approximation, and the determination of the number of hidden layer neurons required to achieve this accuracy. Furthermore, a unifying framework is introduced to understand existing approaches to investigate the universal approximation problem using feedforward neural networks. Some new results are also presented. Finally, two training algorithms are introduced which can determine the weights of feedforward neural networks, with sigmoidal activation neurons, to any degree of prescribed accuracy. These training algorithms are designed so that they do not suffer from the problems of local minima which commonly affect neural network learning algorithms.},\n    author = {Franco Scarselli and Ah {Chung Tsoi}},\n    doi = {https://doi.org/10.1016/S0893-6080(97)00097-X},\n    issn = {0893-6080},\n    journal = {Neural Networks},\n    keywords = {Approximation by neural networks, Approximation of polynomials, Constructive approximation, Feedforward neural networks, Multilayer neural networks, Radial basis functions, Universal approximation},\n    number = {1},\n    pages = {15-37},\n    title = {Universal Approximation Using Feedforward Neural Networks: A Survey of Some Existing Methods, and Some New Results},\n    url = {https://www.sciencedirect.com/science/article/pii/S089360809700097X},\n    volume = {11},\n    year = {1998},\n    Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S089360809700097X},\n    Bdsk-Url-2 = {https://doi.org/10.1016/S0893-6080(97)00097-X}}\n@ARTICLE{Ref 3,\n  author={Bianchini, Monica and Scarselli, Franco},\n  journal={IEEE Transactions on Neural Networks and Learning Systems}, \n  title={On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures}, \n  year={2014},\n  volume={25},\n  number={8},\n  pages={1553-1565},\n  doi={10.1109/TNNLS.2013.2293637}}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9644973735463637
      ],
      "excerpt": "Deep Learning in Computer Vision \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8933119880257014
      ],
      "excerpt": "| Oct. 29th, 2021       | DL in Natural Language Processing | Ref 12 | Ref xii, Ref xiii|  | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Deep-Learning-at-UC-Merced/DeepLearningGroupReading",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-06T04:05:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-09T03:01:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9756487470968466,
        0.997660961340795
      ],
      "excerpt": "This repo hosts the material and references covered in the DLG learning group in Fall 2021. This README will be updated regularly before each meeting, so please check back for more information and resources. Contributions to this repo will also be greatly appreciated, so please feel free to fork and pull request with any updates you find necessary. Please star \u2b50\ufe0f the repository for staying tuned with the updates \nThe main objective of the learning group is to dive deeper into fundamentals of Machine Learning (Deep Learning in particular) through a mathematical lens. This group will provide a forum for graduate students, postdocs and faculty interested in Deep Learning to learn about the fundamentals and advances in the field while fostering broader discussions and collaborations. This group is organized by Ali Heydari. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9936343894273106,
        0.9841694923133152
      ],
      "excerpt": "The first half hour of the sessions will be dedicated to reading, followed by discussions of the topics. This format is intended to provide a dedicated time for participants to read about the specific topics during each session, so that all can engage in fruitful discussions and contribute to the group\u2019s collective learning. Given the time constraints however, participants are expected to have a working knowledge of machine learning and other relevant prerequisites. \nGiven the broad range of topics and the short amount of time, we would like to tailor each session to address the interest of the audience in the following meta topics:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9421955683415187
      ],
      "excerpt": "Optimization in Deep Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183249581598492,
        0.9652887741931917
      ],
      "excerpt": "Deep Learning in Natural Language Processing \nHere we provide a list of hand-picked resources for each of the meeting sessions, in addition to references for pre-requisites needed before attending the sessions. Per UC Merced's policy, the links will expire in 90 days. However, we have included the citations to each paper in the Acknowledgment section.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8258796753463739
      ],
      "excerpt": "| Sep. 10th, 2021 | Universal Approximation Theorem (Continuation of last week) | Ref iii (Main), Ref 3, Ref 4 | Ref i, Ref ii, Ref iv |   | \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Deep-Learning-at-UC-Merced/DeepLearningGroupReading/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 11:32:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Deep-Learning-at-UC-Merced/DeepLearningGroupReading/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep-Learning-at-UC-Merced/DeepLearningGroupReading",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Deep-Learning-at-UC-Merced/DeepLearningGroupReading/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Ali Heydari\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "UCM Applied Math Deep Learning Group (DLG)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepLearningGroupReading",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep-Learning-at-UC-Merced",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Deep-Learning-at-UC-Merced/DeepLearningGroupReading/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 11:32:52 GMT"
    },
    "technique": "GitHub API"
  }
}