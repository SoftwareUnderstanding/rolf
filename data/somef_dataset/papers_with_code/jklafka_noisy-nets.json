{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jklafka/noisy-nets",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-15T21:41:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-24T08:36:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9635274733969093,
        0.9503257023355203,
        0.8569067371565736,
        0.930917826811876,
        0.9674821759968136
      ],
      "excerpt": "In this project, we use state-of-the-art linguistic encoders such as BERT and GPT-2 to denoise corrupted text within a sequential denoising autoencoder architecture and training objective. What does that mean? For example, in this sentence: \n\"Where is the the teacher?\" \nyou can easily tell that the intended message is \"where is the teacher\". You unconsciously deletes the repeated \"the\" from the sentence with minimal, if any, mental effort. \nTo approach this task, I train an autoencoder: a network that recreates the sentence I give it as input. Since I feed the autoencoder words one by one, it is a sequential autoencoder. Finally, I give the autoencoder noisy input i.e. corrupted text, where a word has been deleted or inserted, or two words have been swapped, and ask it to give me the original, uncorrupted text back. This means that the autoencoder denoises. \nHow do powerful language models such as BERT encode noisy sentences? Can these language models help us denoise sentences at scale? What does this process reveal about the model's knowledge of language in general? \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jklafka/noisy-nets/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 12:58:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jklafka/noisy-nets/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jklafka/noisy-nets",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/Analysis/analysis_updated.ipynb",
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/Analysis/analysis.ipynb",
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/Analysis/Topic/lda.ipynb",
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/Analysis/Dep/dep.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/noisy-nets.sh",
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/noisy-nets_input_output_vary.sh",
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/preprocess.sh",
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/preprocess_input_output_vary.sh",
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/clear-nets.sh",
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/Slurm/noisy-nets.sh",
      "https://raw.githubusercontent.com/jklafka/noisy-nets/master/Slurm/clear-nets.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jklafka/noisy-nets/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "A better sentence-level autocorrect: sequential denoising autoencoders with language models",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "noisy-nets",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jklafka",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jklafka/noisy-nets/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 12:58:13 GMT"
    },
    "technique": "GitHub API"
  }
}