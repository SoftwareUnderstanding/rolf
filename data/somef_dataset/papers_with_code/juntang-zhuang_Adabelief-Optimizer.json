{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<p align=\"center\">\n<img src=\"./imgs/image_recog.png\" width=\"80%\"/> \n</p>\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2010.07468\"> arXiv </a>, <a href=\"https://www.reddit.com/r/MachineLearning/comments/jc1fp2/r_neurips_2020_spotlight_adabelief_optimizer\">Reddit </a>, <a href=\"https://twitter.com/JuntangZhuang/status/1316934184607354891\">Twitter</a>, <a href=\"https://www.bilibili.com/video/BV1uy4y1q7RG\">BiliBili (\u4e2d\u6587"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{zhuang2020adabelief,\n  title={AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients},\n  author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},\n  journal={Conference on Neural Information Processing Systems},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhuang2020adabelief,\n  title={AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients},\n  author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},\n  journal={Conference on Neural Information Processing Systems},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8648619474683512
      ],
      "excerpt": "A quick look at the algorithm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999992651852239,
        0.9105368110547479
      ],
      "excerpt": "<a href=\"https://juntang-zhuang.github.io/adabelief/\"> Project Page</a>, <a href=\"https://arxiv.org/abs/2010.07468\"> arXiv </a>, <a href=\"https://www.reddit.com/r/MachineLearning/comments/jc1fp2/r_neurips_2020_spotlight_adabelief_optimizer\">Reddit </a>, <a href=\"https://twitter.com/JuntangZhuang/status/1316934184607354891\">Twitter</a>, <a href=\"https://www.bilibili.com/video/BV1uy4y1q7RG\">BiliBili (\u4e2d\u6587)</a>, <a href=\"https://www.bilibili.com/video/BV1vi4y1c71S\">BiliBili (Engligh)</a>, <a href=\"https://youtu.be/oGH7dmwvuaY\">Youtube</a> \nSN-GAN https://github.com/juntang-zhuang/SNGAN-AdaBelief <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9449813035477502,
        0.9449813035477502
      ],
      "excerpt": "Reinforcement Learning (Toy) https://github.com/juntang-zhuang/rainbow-adabelief <br> \nReinforcement Learning (HalfCheetah-v2 Walker2d-v2) https://github.com/juntang-zhuang/SAC-Adabelief <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146894306581498
      ],
      "excerpt": "| Object detection (PASCAL) | 1e-4 | 0.9   | 0.999 | 1e-8    | 1e-4         | False           | False      | False   | False   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.8356013927728488
      ],
      "excerpt": "| Reinforcement (Rainbow)| 1e-4 | 0.9 | 0.999 | 1e-10|     0.0           | True=False (decay=0)| True   | False   | False   | \n| Reinforcement (HalfCheetah-v2)| 1e-3 | 0.9 | 0.999 | 1e-12|     0.0           | True=False (decay=0)| True   | False   | False   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8648619474683512
      ],
      "excerpt": "<h2>A quick look at the algorithm</h2> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "   (2) If weight_decouple == True: <br> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/juntang-zhuang/Adabelief-Optimizer",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please contact me at ```j.zhuang@yale.edu``` or open an issue here if you would like to help improve it, especially the tensorflow version, or explore combination with other methods, some discussion on the theory part, or combination with other methods to create a better optimizer. Any thoughts are welcome!\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-10T01:31:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T22:19:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8149963046223481
      ],
      "excerpt": "Table of hyper-parameters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8240108300364541
      ],
      "excerpt": "Reproduce results in the paper  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9747762198172
      ],
      "excerpt": "In the next release of adabelief-pytorch, we will modify the default of several arguments, in order to fit the needs of for general tasks such as GAN and Transformer. Please check if you specify these arguments or use the default when upgrade from version 0.0.5 to higher. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9664378768551926
      ],
      "excerpt": "In adabelief-tf==0.1.0, we modify adabelief-tf to have the same feature as adabelief-pytorch, inlcuding decoupled weight decay and learning rate rectification. Furthermore, we will add support for TensorFlow>=2.0 and Keras. The source code is in pypi_packages/adabelief_tf0.1.0. We tested with a text classification task and a word embedding task.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9782618991597538
      ],
      "excerpt": "Note weight decay varies with tasks, for different tasks the weight decay is untuned from the original repository (only changed the optimizer and other hyper-parameters). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9382595222207507,
        0.9889359130359392
      ],
      "excerpt": "epsilon is used in a different way in Tensorflow (default 1e-7) compared to PyTorch (default 1e-8), so eps in Tensorflow might needs to be larger than in PyTorch (perhaps 100 times larger in Tensorflow, e.g.  eps=1e-16 in PyTorch v.s eps=1e-14 in Tensorflow). But personally I don't have much experience with Tensorflow, it's likely that you need to slightly tune eps. \n\uff08 Results in the paper are all generated using the PyTorch implementation in adabelief-pytorch package, which is the ONLY package that I have extensively tested for now.) <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9236369159543674,
        0.8383693657939414,
        0.8817202120425514
      ],
      "excerpt": "        Adam and AdaBelief are summarized in Algo.1 and Algo.2, where all operations are  \n        element-wise, with differences marked in blue. Note that no extra parameters are introduced in AdaBelief. For simplicity, \n         we omit the bias correction step. Specifically, in Adam, the update  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915845662266376
      ],
      "excerpt": "refer to jupyter notebook for visualization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321661248064526
      ],
      "excerpt": "AdaBelief uses a different denominator from Adam, and is orthogonal to other techniques such as recification, decoupled weight decay, weight averaging et.al. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8681689385753242
      ],
      "excerpt": "epsilon in AdaBelief plays a different role as in Adam, typically when you use epslison=x in Adam, using epsilon=x*x will give similar results in AdaBelief. The default value epsilon=1e-8 is not a good option in many cases, in version >0.1.0 the default eps is set as 1e-16. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757735860849558,
        0.8836036469901526
      ],
      "excerpt": "If decoupled weight decay is very important for your task, which means AdamW is much better than Adam, then you need to set weight_decouple as True to turn on decoupled decay in AdaBelief. Note that many optimizers uses decoupled weight decay without specifying it as an options, e.g. RAdam, but we provide it as an option so users are aware of what technique is actually used. \nDon't use \"gradient threshold\" (clamp each element independently) in AdaBelief, it could result in division by 0 and explosion in update; but \"gradient clip\" (shrink amplitude of the gradient vector but keeps its direction) is fine, though from my limited experience sometimes the clip range needs to be the same or larger than Adam. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9476562452314656
      ],
      "excerpt": "   Currently there are two ways to perform weight decay for adaptive optimizers, directly apply it to the gradient (Adam), or decouple weight decay from gradient descent (AdamW). This is passed to the optimizer by argument weight_decouple (default: False). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9328760256939963,
        0.9676289261904029,
        0.9721825253305169,
        0.9174052046724579,
        0.9885355179976648,
        0.9316276163213766,
        0.9832299442883866,
        0.9832285676222248,
        0.9360167068619899,
        0.9580898365235381,
        0.9633473006977971,
        0.8095890377830701,
        0.898359049561674
      ],
      "excerpt": "        <ul>  If fixed_decay == True, the weight is multiplied by 1 - weight_decay. This is implemented as an option but not used to produce results in the paper. </ul> \nWhat is the acutal weight-decay we are using? <br> \n   This is seldom discussed in the literature, but personally I think it's very important. When we set weight_decay=1e-4 for SGD, the weight is scaled by 1 - lr x weight_decay. Two points need to be emphasized: (1) lr in SGD is typically larger than Adam (0.1 vs 0.001), so the weight decay in Adam needs to be set as a larger number to compensate. (2) lr decays, this means typically we use a larger weight decay in early phases, and use a small weight decay in late phases. \nAdaBelief seems to require a different epsilon from Adam. In CV tasks in this paper, epsilon is set as 1e-8. For GAN training it's set as 1e-16. We recommend try different epsilon values in practice, and sweep through a large region. We recommend use eps=1e-8 when SGD outperforms Adam, such as many CV tasks; recommend eps=1e-16 when Adam outperforms SGD, such as GAN and Transformer. Sometimes you might need to try eps=1e-12, such as in some reinforcement learning tasks. \nWhether to turn on the rectification as in RAdam. The recitification basically uses SGD in early phases for warmup, then switch to Adam. Rectification is implemented as an option, but is never used to produce results in the paper. \nWhether to take the max (over history) of denominator, same as AMSGrad. It's set as False for all experiments. \nResults in the paper are generated using the PyTorch implementation in adabelief-pytorch package. This is the ONLY package that I have extensively tested for now. <br> \nWe also provide a modification of ranger optimizer in ranger-adabelief which combines RAdam + LookAhead + Gradient Centralization + AdaBelief, but this is not used in the paper and is not extensively tested.  \n<del> The adabelief-tf is a naive implementation in Tensorflow. It lacks many features such as decoupled weight decay, and is not extensively tested. Currently I don't have plans to improve it since I seldom use Tensorflow, please contact me if you want to collaborate and improve it.</del> \nThe adabelief-tf==0.1.0 supports the same feature as adabelief-pytorch==0.1.0, including decoupled weight decay and rectification. But personally I don't have the chance to perform extensive tests as with the PyTorch version. \nThe experiments on Cifar is the same as demo in AdaBound, with the only difference is the optimizer. The ImageNet experiment uses a different learning rate schedule, typically is decayed by 1/10 at epoch 30, 60, and ends at 90. For some reasons I have not extensively experimented, AdaBelief performs good when decayed at epoch 70, 80 and ends at 90, using the default lr schedule produces a slightly worse result. If you have any ideas on this please open an issue here or email me. \nI got some feedbacks on RNN on reddit discussion, here are a few tips: \n* The epsilon is suggested to set as a smaller value for RNN (e.g. 1e-12, 1e-16). Please try different epsilon values, it varies from task to task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663942406620961,
        0.8794953714684998,
        0.994329375488713
      ],
      "excerpt": "  (2) By \"gradient clip\" I refer to the operation on a vector or tensor. Suppose X is a tensor, if ||X|| > thres, then X <- X/||X|| * thres. Take X as a vector, \"gradient clip\" shrinks the amplitude but keeps the direction.<br> \n  (3) \"Gradient threshold\" is incompatible with AdaBelief, because if gt is thresholded for a long time, then  |gt-mt|~=0, and the division will explode; however, \"gradient clip\" is fine for Adabelief, yet the clip range still needs tuning (perhaps AdaBelief needs a larger range than Adam).<br> \n<del>Someone (under the wechat group Jiqizhixin) points out that the results on GAN is bad, this might be due to the choice of GAN model (We pick the simplest code example from PyTorch docs without adding more tricks), and we did not perform cherry-picking or worsen the baseline perfomance intentionally. We will update results on new GANs (e.g. SN-GAN) and release code later. </del>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9072568305118702,
        0.8873416209993574
      ],
      "excerpt": "<del> (10/23/2020) Transformer trains fine locally with PyTorch 1.1 CUDA9.0 (BLEU score 35.74 (highest is 35.85) on IWSLT14 DE-En with small transformer), but works much worse on a server with PyTorch 1.4  CUDA 10.0 (BLEU score < 26) using the same code.  \nThe code is to reproduce the error is at: https://github.com/juntang-zhuang/transformer-adabelief </del> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938813443059922
      ],
      "excerpt": "<del> Compare the rectified update, currently the implementation is slightly different from RAdam implementation.</del> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9691552772694221
      ],
      "excerpt": "Updated results on an SN-GAN is in https://github.com/juntang-zhuang/SNGAN-AdaBelief, AdaBelief achieves 12.36 FID (lower is better) on Cifar10, while Adam achieves 13.25 (number taken from the log of official repository PyTorch-studioGAN). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9854726414260628,
        0.9758680684332804
      ],
      "excerpt": "Identify the problem of Transformer with PyTorch 1.4, to be an old version fairseq is incompatible with new version PyTorch, works fine with latest fairseq. <br> Code on Transformer to work with PyTorch 1.6 is at: https://github.com/juntang-zhuang/fairseq-adabelief <br> \n  Code for transformer to work with PyTorch 1.1 and CUDA9.0 is at: https://github.com/juntang-zhuang/transformer-adabelief \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955122521709533,
        0.9304683583230594
      ],
      "excerpt": "Released adabelief-pytorch==0.1.0 and adabelief-tf==0.1.0. The Tensorflow version now supports TF>=2.0 and Keras, with the same features as in the PyTorch version, including decoupled weight decay and rectification. \nReleased adabelief-pytorch==0.2.0. Fix the error with coupled weight decay in adabelief-pytorch==0.1.0, fix the amsgrad update in adabelief-pytorch==0.1.0. Add options to disable the message printing, by specify print_change_log=False when initiating the optimizer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Repository for NeurIPS 2020 Spotlight  \"AdaBelief Optimizer: Adapting stepsizes by the belief in observed gradients\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/juntang-zhuang/Adabelief-Optimizer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 102,
      "date": "Tue, 21 Dec 2021 05:20:28 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/juntang-zhuang/Adabelief-Optimizer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "juntang-zhuang/Adabelief-Optimizer",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/juntang-zhuang/Adabelief-Optimizer/update_0.2.0/PyTorch_Experiments/LSTM/LSTM_test.ipynb",
      "https://raw.githubusercontent.com/juntang-zhuang/Adabelief-Optimizer/update_0.2.0/PyTorch_Experiments/classification_cifar10/visualization.ipynb",
      "https://raw.githubusercontent.com/juntang-zhuang/Adabelief-Optimizer/update_0.2.0/TensorFlow_Experiments/convolutional_network.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/juntang-zhuang/Adabelief-Optimizer/update_0.2.0/PyTorch_Experiments/LSTM/getdata.sh",
      "https://raw.githubusercontent.com/juntang-zhuang/Adabelief-Optimizer/update_0.2.0/PyTorch_Experiments/imagenet/run.sh",
      "https://raw.githubusercontent.com/juntang-zhuang/Adabelief-Optimizer/update_0.2.0/PyTorch_Experiments/classification_cifar10/run.sh",
      "https://raw.githubusercontent.com/juntang-zhuang/Adabelief-Optimizer/update_0.2.0/PyTorch_Experiments/wgan-gp/run.sh",
      "https://raw.githubusercontent.com/juntang-zhuang/Adabelief-Optimizer/update_0.2.0/PyTorch_Experiments/wgan/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please install the latest version from pip, old versions might suffer from bugs. Source code for up-to-date package is available in folder ```pypi_packages```. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* Check if the code is from the latest official implementation (adabelief-pytorch==0.1.0, adabelief-tf==0.1.0)\n      Default hyper-parameters are different from the old version.\n\n* check all hyper-parameters, DO NOT simply use the default,\n\n     >Epsilon in AdaBelief is different from Adam (typically eps_adabelief = eps_adam*eps_adam) <br>\n     >( eps of Adam in Tensorflow is 1e-7, in PyTorch is 1e-8, need to consider this when use AdaBelief in Tensorflow) <br>\n     \n     >> If SGD is better than Adam   ->  Set a large eps (1e-8) in AdaBelief-pytorch (1e-7 in Tensorflow )<br>\n     >> If SGD is worse than Adam   ->  Set a small eps (1e-16) in AdaBelief-pytorch (1e-14 in Tensorflow, rectify=True often helps) <br>\n     \n     >> If AdamW is better than Adam   ->   Turn on \u201cweight_decouple\u201d  in AdaBelief-pytorch (this is on in adabelief-tf==0.1.0 and cannot shut down). <br>\n     >> Note that default weight decay is very different for Adam and AdamW, you might need to consider this when using AdaBelief with and without decoupled weight decay. <br>\n\n* Check ALL hyper-parameters. Refer to our github page for a list of recommended hyper-parameters\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8528307813608896
      ],
      "excerpt": "We have released adabelief-pytorch==0.2.0 and adabelief-tf==0.2.0. Please use the latest version from pip. Source code is available under folder pypi_packages/adabelief_pytorch0.2.0 and pypi_packages/adabelief_tf0.2.0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8270968207608426
      ],
      "excerpt": "Update Plan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406,
        0.9667326416418838,
        0.9667326416418838
      ],
      "excerpt": "SN-GAN https://github.com/juntang-zhuang/SNGAN-AdaBelief <br> \nTransformer (PyTorch 1.1) https://github.com/juntang-zhuang/transformer-adabelief <br> \nTransformer (PyTorch 1.6) https://github.com/juntang-zhuang/fairseq-adabelief <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8819424550639401
      ],
      "excerpt": "Object detection (by yuanwei2019) https://github.com/yuanwei2019/EAdam-optimizer (Note that this version uses adabelief-pytorch==0.0.5, and the default hyper-parameters is different from adabelief-pytorch==0.1.0. Please check your version of adabelief, and whether you specify all hyper-parameters, or does the default is what you want.) <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717106327039013
      ],
      "excerpt": "|   Version| epsilon | weight_decouple | rectify     |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132237905420107,
        0.9717106327039013
      ],
      "excerpt": "The default value is updated, please check if you specify these arguments or use the default when upgrade from version 0.0.1 to higher.: \n|   Version| epsilon | weight_decouple | rectify     |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.820282558777336,
        0.92030981303426,
        0.9995486579735738
      ],
      "excerpt": "\uff08 Results in the paper are all generated using the PyTorch implementation in adabelief-pytorch package, which is the ONLY package that I have extensively tested for now.) <br> \nPlease install latest version (0.2.0), previous version (0.0.5) uses different default arguments. \npip install adabelief-pytorch==0.2.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "pip install ranger-adabelief==0.1.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897411650844645
      ],
      "excerpt": "pip install adabelief-tf==0.2.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8680312351385695
      ],
      "excerpt": "<del> The AMSGrad implmentation might be problematic, see discusssion https://github.com/juntang-zhuang/Adabelief-Optimizer/issues/32#issuecomment-742350592 </del> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9371328906696165,
        0.8102140432896932
      ],
      "excerpt": "Identify the problem of Transformer with PyTorch 1.4, to be an old version fairseq is incompatible with new version PyTorch, works fine with latest fairseq. <br> Code on Transformer to work with PyTorch 1.6 is at: https://github.com/juntang-zhuang/fairseq-adabelief <br> \n  Code for transformer to work with PyTorch 1.1 and CUDA9.0 is at: https://github.com/juntang-zhuang/transformer-adabelief \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.830545430216933
      ],
      "excerpt": "Released adabelief-pytorch==0.2.0. Fix the error with coupled weight decay in adabelief-pytorch==0.1.0, fix the amsgrad update in adabelief-pytorch==0.1.0. Add options to disable the message printing, by specify print_change_log=False when initiating the optimizer. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8019668265389785
      ],
      "excerpt": "| ImageNet | 1e-3 |0.9   | 0.999 | 1e-8    | 1e-2         | True            | False       | False   | False   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8430126040642103,
        0.869169660801936,
        0.8650617545608159,
        0.8436426605825739
      ],
      "excerpt": "| SN-GAN (large)|2e-4 | 0.5   | 0.999 | 1e-16   | 0     | True=False (decay=0)| True      | False   | False   | \n| Transformer| 5e-4| 0.9 | 0.999  | 1e-16   | 1e-4         | True            | True      | False   | False   | \n| Reinforcement (Rainbow)| 1e-4 | 0.9 | 0.999 | 1e-10|     0.0           | True=False (decay=0)| True   | False   | False   | \n| Reinforcement (HalfCheetah-v2)| 1e-3 | 0.9 | 0.999 | 1e-12|     0.0           | True=False (decay=0)| True   | False   | False   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516,
        0.8483127295186341
      ],
      "excerpt": "from adabelief_pytorch import AdaBelief \noptimizer = AdaBelief(model.parameters(), lr=1e-3, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from ranger_adabelief import RangerAdaBelief \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from adabelief_tf import AdaBeliefOptimizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8761971630348885
      ],
      "excerpt": "<img src=\"./imgs/GAN.png\" width=\"80%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8761971630348885
      ],
      "excerpt": "<img src=\"./imgs/sn-gan.png\" width=\"80%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8913652091191709
      ],
      "excerpt": "<img src=\"./imgs/lstm.png\" width=\"80%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882747530828131
      ],
      "excerpt": "<img src=\"./imgs/transformer.png\" width=\"60%\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/juntang-zhuang/Adabelief-Optimizer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 2-Clause \"Simplified\" License",
      "url": "https://api.github.com/licenses/bsd-2-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2017, \\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Release of package",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Adabelief-Optimizer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "juntang-zhuang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/juntang-zhuang/Adabelief-Optimizer/blob/update_0.2.0/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 959,
      "date": "Tue, 21 Dec 2021 05:20:28 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<p align=\"center\">\n<img src=\"./imgs/Beale2.gif\" width=\"80%\"/>\n</p>\n\n",
      "technique": "Header extraction"
    }
  ]
}