{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1907.10529",
      "https://arxiv.org/abs/1907.11692"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{sun2020joint,\n    title={Joint Keyphrase Chunking and Salience Ranking with BERT},\n    author={Si Sun and Chenyan Xiong and Zhenghao Liu and Zhiyuan Liu and Jie Bao},\n    year={2020},\n    eprint={2004.13639},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9904289819775982
      ],
      "excerpt": "Please cite our paper if our experimental results, analysis conclusions or the code are helpful to you ~ :) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UnknownGenie/altered-BERT-KPE",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For any question, please contact **Si Sun** by email s-sun17@mails.tsinghua.edu.cn , we will try our best to solve.\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-21T06:05:02Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-21T06:10:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9947861495849973,
        0.9942624177838875,
        0.8915260504636822
      ],
      "excerpt": "This repository provides the code of the paper Joint Keyphrase Chunking and Salience Ranking with BERT. \nIn this paper, we conduct an empirical study of <u>5 keyphrase extraction models</u> with <u>3 BERT variants</u>, and then propose a multi-task model BERT-JointKPE. Experiments on two KPE benchmarks, OpenKP with Bing web pages and KP20K demonstrate JointKPE\u2019s state-of-the-art and robust effectiveness. Our further analyses also show that JointKPE has advantages in predicting <u>long keyphrases</u> and <u>non-entity keyphrases</u>, which were challenging for previous KPE techniques. \nPlease cite our paper if our experimental results, analysis conclusions or the code are helpful to you ~ :) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276960452502296,
        0.9474353402644582
      ],
      "excerpt": "|1|BERT-JointKPE (Bert2Joint)|A <u>multi-task</u> model is trained jointly on the chunking task and the ranking task, balancing the estimation of keyphrase quality and salience. | \n|2|BERT-RankKPE (Bert2Rank)|Learn the salience phrases in the documents using a <u>ranking</u> network. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9352911463640435,
        0.8237843533120746
      ],
      "excerpt": "|4|BERT-TagKPE (Bert2Tag)|We modified the <u>sequence tagging</u> model to generate enough candidate keyphrases for a document. | \n|5|BERT-SpanKPE (Bert2Span)|We modified the <u>span extraction</u> model to extract multiple keyphrases from a document. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066995144995134
      ],
      "excerpt": "Tensorflow (tested on 1.14.0, only for tensorboardX) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9314617442782533
      ],
      "excerpt": "  --output_path           The dir to save preprocess data; default: ../data/prepro_dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8386744151671134
      ],
      "excerpt": "  PS. Running the training script for the first time will take some time to perform preprocess such as tokenization, and by default, the processed features will be saved under ../data/cached_features, which can be directly loaded next time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9104586854837566
      ],
      "excerpt": "We always keep the following settings in all our experiments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8707563406948582
      ],
      "excerpt": "We recommend using DistributedDataParallel to train models on multiple GPUs (It's faster than DataParallel, but it will take up more memory) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9308305399959483
      ],
      "excerpt": "--eval_checkpoint       The filepath of our provided checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "|Rank|Method|F1 @1,@3,@5|Precision @1,@3,@5|Recall @1,@3,@5| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "|Rank|Method|F1 @1,@3,@5|Precision @1,@3,@5|Recall @1,@3,@5| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "|Rank|Method|F1 @1,@3,@5|Precision @1,@3,@5|Recall @1,@3,@5| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9508197791355082,
        0.9344228545636208,
        0.936430798701692,
        0.8838285136700594,
        0.8938145407771622,
        0.995360413060193
      ],
      "excerpt": "Word-Level Representations :   We encode an input document into a sequence of WordPiece tokens' vectors with a pretrained BERT (or its variants), and then we pick up the first sub-token vector of each word to represent the input in word-level. \nPhrase-Level Representations : We perform a soft-select method to decode phrase from word-level vector instead of hard-select used in the standard sequence tagging task . \nThe word-level representation is feed into an classification layer to obtain the tag probabilities of each word on 5 classes  (O, B, I, E, U) , and then we employ different tag patterns for extracting different n-grams ( 1 \u2264 n \u2264 5 ) over the whole sequence. \nLast there are a collect of n-gram candidates, each word of the n-gram just has one score. \nSoft-select Example : considering all 3-grams (B I E) on the L-length document, we can extract (L-3+1)  3-grams sequentially like sliding window. In each 3-gram, we only keep B score for the first word, I score for the middle word, and E score for the last word, etc. \nO : Non Keyphrase ;  B : Begin word of the keyprase ;  I : Middle word of the keyphrase ;  E : End word of keyprhase ;  U : Uni-word keyphrase \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9926267652355384
      ],
      "excerpt": "Incorporating with term frequency, we employ Min Pooling to get the final score of each n-gram (we called it Buckets Effect: No matter how high a bucket, it depends on the height of the water in which the lowest piece of wood) . Based on the final scores, we extract 5 top ranked keyprhase candidates for each document. \n",
      "technique": "Supervised classification"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- First download and decompress our data folder to this repo, the folder includes benchmark datasets and pre-trained BERT variants.\n\n  - [Data Download Link](https://drive.google.com/open?id=1UugkRsKM8GXPPrrZxWa8HvGe1nyWdd6F)\n\n- We also provide 15 checkpoints (5 KPE models * 3 BERT variants) trained on OpenKP training dataset.\n\n  - [Checkpoint Download Link](https://drive.google.com/open?id=13FvONBTM4NZZCR-I7LVypkFa0xihxWnM)\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UnknownGenie/altered-BERT-KPE/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 04:50:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/UnknownGenie/altered-BERT-KPE/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "UnknownGenie/altered-BERT-KPE",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/UnknownGenie/altered-BERT-KPE/master/scripts/test.sh",
      "https://raw.githubusercontent.com/UnknownGenie/altered-BERT-KPE/master/scripts/train.sh",
      "https://raw.githubusercontent.com/UnknownGenie/altered-BERT-KPE/master/preprocess/preprocess.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8837680365796365,
        0.9322609392449874
      ],
      "excerpt": "python 3.5 \nPytorch 1.3.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8526112798470697,
        0.9654765604684976
      ],
      "excerpt": "To preprocess the source datasets using preprocess.sh in the preprocess folder: \nsource preprocess.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9036881717605214
      ],
      "excerpt": "  --source_dataset_dir    The path to the source dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.968557623089146
      ],
      "excerpt": "source train.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.945956423953397
      ],
      "excerpt": "source test.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8525863574207089,
        0.8152249660077784
      ],
      "excerpt": "  --output_path           The dir to save preprocess data; default: ../data/prepro_dataset \nTo train a new model from scratch using train.sh in the scripts folder: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8168107655428365
      ],
      "excerpt": "  Complete optional arguments can be seen in config.py in the scripts folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8162717594042472
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1 OMP_NUM_THREADS=2 python -m torch.distributed.launch --nproc_per_node=2 --master_port=1234 train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8086314445935857
      ],
      "excerpt": "  --eval_checkpoint       The checkpoint file to be evaluated \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/UnknownGenie/altered-BERT-KPE/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "**BERT for Keyphrase Extraction** (Pytorch)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "altered-BERT-KPE",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "UnknownGenie",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/UnknownGenie/altered-BERT-KPE/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 04:50:15 GMT"
    },
    "technique": "GitHub API"
  }
}