{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Stable Baselines was created in the [robotics lab U2IS](http://u2is.ensta-paristech.fr/index.php?lang=en) ([INRIA Flowers](https://flowers.inria.fr/) team) at [ENSTA ParisTech](http://www.ensta-paristech.fr/en).\n\nLogo credits: [L.M. Tenkes](https://www.instagram.com/lucillehue/)\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To cite this repository in publications:\n\n```\n@misc{stable-baselines,\n  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},\n  title = {Stable Baselines},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/hill-a/stable-baselines}},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{stable-baselines,\n  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},\n  title = {Stable Baselines},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/hill-a/stable-baselines}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9809845813829983
      ],
      "excerpt": "Github repo: https://github.com/araffin/rl-baselines-zoo \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hill-a/stable-baselines",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Stable-Baselines\nIf you are interested in contributing to Stable-Baselines, your contributions will fall\ninto two categories:\n1. You want to propose a new Feature and implement it\n    - Create an issue about your intended feature, and we shall discuss the design and\n    implementation. Once we agree that the plan looks good, go ahead and implement it.\n2. You want to implement a feature or bug-fix for an outstanding issue\n    - Look at the outstanding issues here: https://github.com/hill-a/stable-baselines/issues\n    - Look at the roadmap here: https://github.com/hill-a/stable-baselines/projects/1\n    - Pick an issue or feature and comment on the task that you want to work on this feature.\n    - If you need more context on a particular issue, please ask and we shall provide.\nOnce you finish implementing a feature or bug-fix, please send a Pull Request to\nhttps://github.com/hill-a/stable-baselines/\nIf you are not familiar with creating a Pull Request, here are some guides:\n- http://stackoverflow.com/questions/14680711/how-to-do-a-github-pull-request\n- https://help.github.com/articles/creating-a-pull-request/\nDeveloping Stable-Baselines\nTo develop Stable-Baselines on your machine, here are some tips:\n\nClone a copy of Stable-Baselines from source:\n\nbash\ngit clone https://github.com/hill-a/stable-baselines/\ncd stable-baselines\n\nInstall Stable-Baselines in develop mode, with support for building the docs and running tests:\n\nbash\npip install -e .[docs,tests]\nCodestyle\nWe follow the PEP8 codestyle. Please order the imports as follows:\n\nbuilt-in\npackages\ncurrent module\n\nwith one space between each,  that gives for instance:\n```python\nimport os\nimport warnings\nimport numpy as np\nfrom stable_baselines import PPO2\n```\nIn general, we recommend using pycharm to format everything in an efficient way.\nPlease document each function/method and type them using the following template:\n```python\ndef my_function(arg1: type1, arg2: type2) -> returntype:\n    \"\"\"\n    Short description of the function.\n:param arg1: (type1) describe what is arg1\n:param arg2: (type2) describe what is arg2\n:return: (returntype) describe what is returned\n\"\"\"\n...\nreturn my_variable\n\n```\nPull Request (PR)\nBefore proposing a PR, please open an issue, where the feature will be discussed. This prevent from duplicated PR to be proposed and also ease the code review process.\nEach PR need to be reviewed and accepted by at least one of the maintainers (@hill-a, @araffin, @erniejunior, @AdamGleave or @Miffyli).\nA PR must pass the Continuous Integration tests (travis + codacy) to be merged with the master branch.\nNote: in rare cases, we can create exception for codacy failure.\nTest\nAll new features must add tests in the tests/ folder ensuring that everything works fine.\nWe use pytest.\nAlso, when a bug fix is proposed, tests should be added to avoid regression.\nTo run tests with pytest:\nmake pytest\nType checking with pytype:\nmake type\nBuild the documentation:\nmake doc\nCheck documentation spelling (you need to install sphinxcontrib.spelling package for that):\nmake spelling\nChangelog and Documentation\nPlease do not forget to update the changelog (docs/misc/changelog.rst) and add documentation if needed.\nA README is present in the docs/ folder for instructions on how to build the documentation.\nCredits: this contributing guide is based on the PyTorch one.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-02T14:28:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T11:44:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9850875759907083
      ],
      "excerpt": "Stable Baselines is a set of improved implementations of reinforcement learning algorithms based on OpenAI Baselines. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906992316522041,
        0.9499944502180233,
        0.9895393555954357
      ],
      "excerpt": "These algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details. \nNote: despite its simplicity of use, Stable Baselines (SB) assumes you have some knowledge about Reinforcement Learning (RL). You should not utilize this library without some practice. To that extent, we provide good resources in the documentation to get started with RL. \nThis toolset is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8570123623608925,
        0.8557095226094126
      ],
      "excerpt": "- More tests & more code coverage \n- Additional algorithms: SAC and TD3 (+ HER support for DQN, DDPG, SAC and TD3) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240568874637005
      ],
      "excerpt": "| State of the art RL methods | :heavy_check_mark: <sup>(1)</sup> | :heavy_check_mark:                | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.822749199304693
      ],
      "excerpt": "| Ipython / Notebook friendly | :heavy_check_mark:                | :x:                               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8286351967023563
      ],
      "excerpt": "<sup><sup>(1): Forked from previous version of OpenAI baselines, with now SAC and TD3 in addition</sup></sup><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807845446245967,
        0.9198527438930553,
        0.8470311252652417,
        0.9808772988276458
      ],
      "excerpt": "<sup><sup>(6): Passing a callback function is only available for DQN</sup></sup><br> \nRL Baselines Zoo. is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines. \nIt also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos. \nGoals of this repository: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9020262640531543
      ],
      "excerpt": "<sup><sup>(2): Only implemented for TRPO.</sup></sup><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9754244979175596,
        0.8550831187878903
      ],
      "excerpt": "<sup><sup>(5): TODO, in project scope.</sup></sup> \nNOTE: Soft Actor-Critic (SAC) and Twin Delayed DDPG (TD3) were not part of the original baselines and HER was reimplemented from scratch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8460360420896518
      ],
      "excerpt": " * MultiBinary: A list of possible actions, where each timestep any of the actions can be used in any combination. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9847221675530855
      ],
      "excerpt": "We try to maintain a list of project using stable-baselines in the documentation, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9557807277020227
      ],
      "excerpt": "Stable-Baselines is currently maintained by Ashley Hill (aka @hill-a), Antonin Raffin (aka @araffin), Maximilian Ernestus (aka @ernestum), Adam Gleave (@AdamGleave) and Anssi Kanervisto (@Miffyli). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9782073173060808
      ],
      "excerpt": "To any interested in making the baselines better, there is still some documentation that needs to be done. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A fork of OpenAI Baselines, implementations of reinforcement learning algorithms",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Documentation is available online: [https://stable-baselines.readthedocs.io/](https://stable-baselines.readthedocs.io/)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://mpi4py.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hill-a/stable-baselines/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 655,
      "date": "Mon, 20 Dec 2021 16:40:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hill-a/stable-baselines/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hill-a/stable-baselines",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/hill-a/stable-baselines/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/hill-a/stable-baselines/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/hill-a/stable-baselines/master/scripts/run_tests_travis.sh",
      "https://raw.githubusercontent.com/hill-a/stable-baselines/master/scripts/run_docker_cpu.sh",
      "https://raw.githubusercontent.com/hill-a/stable-baselines/master/scripts/run_docker_gpu.sh",
      "https://raw.githubusercontent.com/hill-a/stable-baselines/master/scripts/build_docker.sh",
      "https://raw.githubusercontent.com/hill-a/stable-baselines/master/scripts/run_tests.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All unit tests in baselines can be run using pytest runner:\n```\npip install pytest pytest-cov\nmake pytest\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Install the Stable Baselines package:\n```\npip install stable-baselines[mpi]\n```\n\nThis includes an optional dependency on MPI, enabling algorithms DDPG, GAIL, PPO1 and TRPO. If you do not need these algorithms, you can install without MPI:\n```\npip install stable-baselines\n```\n\nPlease read the [documentation](https://stable-baselines.readthedocs.io/) for more details and alternatives (from source, using docker).\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Note:** Stable-Baselines supports Tensorflow versions from 1.8.0 to 1.14.0. Support for Tensorflow 2 API is planned.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8443136707471312
      ],
      "excerpt": "<sup><sup>(5): EDIT: you did it OpenAI! :cat:</sup></sup><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8690374565823498
      ],
      "excerpt": "Github repo: https://github.com/araffin/rl-baselines-zoo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9785605051820454,
        0.9835083482395235,
        0.9730041125621232,
        0.8573889943382627
      ],
      "excerpt": "sudo apt-get update &amp;&amp; sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev \nInstallation of system packages on Mac requires Homebrew. With Homebrew installed, run the following: \nbrew install cmake openmpi \nTo install stable-baselines on Windows, please look at the documentation. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hill-a/stable-baselines/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Dockerfile",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License\\n\\nCopyright (c) 2017 OpenAI (http://openai.com)\\nCopyright (c) 2018-2019 Stable-Baselines Team\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stable Baselines",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "stable-baselines",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hill-a",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hill-a/stable-baselines/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n## Breaking Changes:\r\n\r\n- ``render()`` method of ``VecEnvs`` now only accept one argument: ``mode``\r\n\r\n## New Features:\r\n\r\n- Added momentum parameter to A2C for the embedded RMSPropOptimizer (@kantneel)\r\n- ActionNoise is now an abstract base class and implements ``__call__``, ``NormalActionNoise`` and ``OrnsteinUhlenbeckActionNoise`` have return types (@PartiallyTyped)\r\n- HER now passes info dictionary to compute_reward, allowing for the computation of rewards that are independent of the goal (@tirafesi)\r\n\r\n## Bug Fixes:\r\n\r\n- Fixed DDPG sampling empty replay buffer when combined with HER  (@tirafesi)\r\n- Fixed a bug in ``HindsightExperienceReplayWrapper``, where the openai-gym signature for ``compute_reward`` was not matched correctly (@johannes-dornheim)\r\n- Fixed SAC/TD3 checking time to update on learn steps instead of total steps (@PartiallyTyped)\r\n- Added ``**kwarg`` pass through for ``reset`` method in ``atari_wrappers.FrameStack`` (@PartiallyTyped)\r\n- Fix consistency in ``setup_model()`` for SAC, ``target_entropy`` now uses ``self.action_space`` instead of ``self.env.action_space`` (@PartiallyTyped)\r\n- Fix reward threshold in ``test_identity.py``\r\n- Partially fix tensorboard indexing for PPO2 (@enderdead)\r\n- Fixed potential bug in ``DummyVecEnv`` where ``copy()`` was used instead of ``deepcopy()``\r\n- Fixed a bug in ``GAIL`` where the dataloader was not available after saving, causing an error when using ``CheckpointCallback``\r\n- Fixed a bug in ``SAC`` where any convolutional layers were not included in the target network parameters.\r\n- Fixed ``render()`` method for ``VecEnvs``\r\n- Fixed ``seed()``` method for ``SubprocVecEnv``\r\n- Fixed a bug ``callback.locals`` did not have the correct values (@PartiallyTyped)\r\n- Fixed a bug in the ``close()`` method of ``SubprocVecEnv``, causing wrappers further down in the wrapper stack to not be closed. (@NeoExtended)\r\n- Fixed a bug in the ``generate_expert_traj()`` method in ``record_expert.py`` when using a non-image vectorized environment (@jbarsce)\r\n- Fixed a bug in CloudPickleWrapper's (used by VecEnvs) ``__setstate___`` where loading was incorrectly using ``pickle.loads`` (@shwang).\r\n- Fixed a bug in ``SAC`` and ``TD3`` where the log timesteps was not correct(@YangRui2015)\r\n- Fixed a bug where the environment was reset twice when using ``evaluate_policy``\r\n\r\n## Others:\r\n\r\n- Added ``version.txt`` to manage version number in an easier way\r\n- Added ``.readthedocs.yml`` to install requirements with read the docs\r\n- Added a test for seeding ``SubprocVecEnv``` and rendering\r\n\r\n## Documentation:\r\n\r\n- Fix typos (@caburu)\r\n- Fix typos in PPO2 (@kvenkman)\r\n- Removed ``stable_baselines\\deepq\\experiments\\custom_cartpole.py`` (@aakash94)\r\n- Added Google's motion imitation project\r\n- Added documentation page for monitor\r\n- Fixed typos and update ``VecNormalize`` example to show normalization at test-time\r\n- Fixed ``train_mountaincar`` description\r\n- Added imitation baselines project\r\n- Updated install instructions\r\n- Added Slime Volleyball project (@hardmaru)\r\n- Added a table of the variables accessible from the ``on_step`` function of the callbacks for each algorithm (@PartiallyTyped)\r\n- Fix typo in README.md (@ColinLeongUDRI)",
        "dateCreated": "2020-08-05T19:42:50Z",
        "datePublished": "2020-08-05T19:45:11Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.10.1",
        "name": "Bug fixes release",
        "tag_name": "v2.10.1",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.10.1",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/29379871",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.10.1"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "## Breaking Changes\r\n\r\n\r\n- ``evaluate_policy`` now returns the standard deviation of the reward per episode\r\n  as second return value (instead of ``n_steps``)\r\n- ``evaluate_policy`` now returns as second return value a list of the episode lengths\r\n  when ``return_episode_rewards`` is set to ``True`` (instead of ``n_steps``)\r\n- Callback are now called after each ``env.step()`` for consistency (it was called every ``n_steps`` before\r\n  in algorithm like ``A2C`` or ``PPO2``)\r\n- Removed unused code in ``common/a2c/utils.py`` (``calc_entropy_softmax``, ``make_path``)\r\n- **Refactoring, including removed files and moving functions.**\r\n\r\n   - Algorithms no longer import from each other, and ``common`` does not import from algorithms.\r\n   - ``a2c/utils.py`` removed and split into other files:\r\n\r\n      - common/tf_util.py: ``sample``, ``calc_entropy``, ``mse``, ``avg_norm``, ``total_episode_reward_logger``,\r\n        ``q_explained_variance``, ``gradient_add``, ``avg_norm``, ``check_shape``,\r\n        ``seq_to_batch``, ``batch_to_seq``.\r\n      - common/tf_layers.py: ``conv``, ``linear``, ``lstm``, ``_ln``, ``lnlstm``, ``conv_to_fc``, ``ortho_init``.\r\n      - a2c/a2c.py: ``discount_with_dones``.\r\n      - acer/acer_simple.py: ``get_by_index``, ``EpisodeStats``.\r\n      - common/schedules.py: ``constant``, ``linear_schedule``, ``middle_drop``, ``double_linear_con``, ``double_middle_drop``,\r\n        ``SCHEDULES``, ``Scheduler``.\r\n\r\n   - ``trpo_mpi/utils.py`` functions moved (``traj_segment_generator`` moved to ``common/runners.py``, ``flatten_lists`` to ``common/misc_util.py``).\r\n   - ``ppo2/ppo2.py`` functions moved (``safe_mean`` to ``common/math_util.py``, ``constfn`` and ``get_schedule_fn`` to ``common/schedules.py``).\r\n   - ``sac/policies.py`` function ``mlp`` moved to ``common/tf_layers.py``.\r\n   - ``sac/sac.py`` function ``get_vars`` removed (replaced with ``tf.util.get_trainable_vars``).\r\n   - ``deepq/replay_buffer.py`` renamed to ``common/buffers.py``.\r\n\r\n\r\n## New Features:\r\n\r\n- Parallelized updating and sampling from the replay buffer in DQN. (@flodorner)\r\n- Docker build script, `scripts/build_docker.sh`, can push images automatically.\r\n- Added callback collection\r\n- Added ``unwrap_vec_normalize`` and ``sync_envs_normalization`` in the ``vec_env`` module\r\n  to synchronize two VecNormalize environment\r\n- Added a seeding method for vectorized environments. (@NeoExtended)\r\n- Added extend method to store batches of experience in ReplayBuffer. (@solliet)\r\n\r\n\r\n## Bug Fixes:\r\n\r\n- Fixed Docker images via ``scripts/build_docker.sh`` and ``Dockerfile``: GPU image now contains ``tensorflow-gpu``,\r\n  and both images have ``stable_baselines`` installed in developer mode at correct directory for mounting.\r\n- Fixed Docker GPU run script, ``scripts/run_docker_gpu.sh``, to work with new NVidia Container Toolkit.\r\n- Repeated calls to ``RLModel.learn()`` now preserve internal counters for some episode\r\n  logging statistics that used to be zeroed at the start of every call.\r\n- Fix `DummyVecEnv.render` for ``num_envs > 1``. This used to print a warning and then not render at all. (@shwang)\r\n- Fixed a bug in PPO2, ACER, A2C, and ACKTR where repeated calls to ``learn(total_timesteps)`` reset\r\n  the environment on every call, potentially biasing samples toward early episode timesteps.\r\n  (@shwang)\r\n- Fixed by adding lazy property ``ActorCriticRLModel.runner``. Subclasses now use lazily-generated\r\n    ``self.runner`` instead of reinitializing a new Runner every time ``learn()`` is called.\r\n- Fixed a bug in ``check_env`` where it would fail on high dimensional action spaces\r\n- Fixed ``Monitor.close()`` that was not calling the parent method\r\n- Fixed a bug in ``BaseRLModel`` when seeding vectorized environments. (@NeoExtended)\r\n- Fixed ``num_timesteps`` computation to be consistent between algorithms (updated after ``env.step()``)\r\n  Only ``TRPO`` and ``PPO1`` update it differently (after synchronization) because they rely on MPI\r\n- Fixed bug in ``TRPO`` with NaN standardized advantages (@richardwu)\r\n- Fixed partial minibatch computation in ExpertDataset (@richardwu)\r\n- Fixed normalization (with ``VecNormalize``) for off-policy algorithms\r\n- Fixed ``sync_envs_normalization`` to sync the reward normalization too\r\n- Bump minimum Gym version (>=0.11)\r\n\r\n\r\n## Others:\r\n\r\n- Removed redundant return value from ``a2c.utils::total_episode_reward_logger``. (@shwang)\r\n- Cleanup and refactoring in ``common/identity_env.py`` (@shwang)\r\n- Added a Makefile to simplify common development tasks (build the doc, type check, run the tests)\r\n\r\n\r\n## Documentation:\r\n\r\n- Add dedicated page for callbacks\r\n- Fixed example for creating a GIF (@KuKuXia)\r\n- Change Colab links in the README to point to the notebooks repo\r\n- Fix typo in Reinforcement Learning Tips and Tricks page. (@mmcenta)",
        "dateCreated": "2020-03-12T22:04:01Z",
        "datePublished": "2020-03-12T22:26:33Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.10.0",
        "name": "Callback collection, cleanup and bug fixes",
        "tag_name": "v2.10.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.10.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/24482253",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.10.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "## Breaking Changes:\r\n\r\n- The `seed` argument has been moved from `learn()` method to model constructor\r\n  in order to have reproducible results\r\n- `allow_early_resets` of the `Monitor` wrapper now default to `True`\r\n- `make_atari_env` now returns a `DummyVecEnv` by default (instead of a `SubprocVecEnv`)\r\n  this usually improves performance.\r\n- Fix inconsistency of sample type, so that mode/sample function returns tensor of tf.int64 in CategoricalProbabilityDistribution/MultiCategoricalProbabilityDistribution (@seheevic)\r\n\r\n## New Features:\r\n\r\n- Add `n_cpu_tf_sess` to model constructor to choose the number of threads used by Tensorflow\r\n- Environments are automatically wrapped in a `DummyVecEnv` if needed when passing them to the model constructor\r\n- Added `stable_baselines.common.make_vec_env` helper to simplify VecEnv creation\r\n- Added `stable_baselines.common.evaluation.evaluate_policy` helper to simplify model evaluation\r\n- `VecNormalize` changes:\r\n\r\n   - Now supports being pickled and unpickled (@AdamGleave).\r\n   - New methods `.normalize_obs(obs)` and `normalize_reward(rews)` apply normalization\r\n     to arbitrary observation or rewards without updating statistics (@shwang)\r\n   - `.get_original_reward()` returns the unnormalized rewards from the most recent timestep\r\n   - `.reset()` now collects observation statistics (used to only apply normalization)\r\n\r\n- Add parameter `exploration_initial_eps` to DQN. (@jdossgollin)\r\n- Add type checking and PEP 561 compliance.\r\n  Note: most functions are still not annotated, this will be a gradual process.\r\n- DDPG, TD3 and SAC accept non-symmetric action spaces. (@Antymon)\r\n- Add `check_env` util to check if a custom environment follows the gym interface (@araffin and @justinkterry)\r\n\r\n## Bug Fixes:\r\n\r\n- Fix seeding, so it is now possible to have deterministic results on cpu\r\n- Fix a bug in DDPG where `predict` method with `deterministic=False` would fail\r\n- Fix a bug in TRPO: mean_losses was not initialized causing the logger to crash when there was no gradients (@MarvineGothic)\r\n- Fix a bug in `cmd_util` from API change in recent Gym versions\r\n- Fix a bug in DDPG, TD3 and SAC where warmup and random exploration actions would end up scaled in the replay buffer (@Antymon)\r\n\r\n## Deprecations:\r\n\r\n- `nprocs` (ACKTR) and `num_procs` (ACER) are deprecated in favor of `n_cpu_tf_sess` which is now common\r\n  to all algorithms\r\n- `VecNormalize`: `load_running_average` and `save_running_average` are deprecated in favour of using pickle.\r\n\r\n## Others:\r\n\r\n- Add upper bound for Tensorflow version (<2.0.0).\r\n- Refactored test to remove duplicated code\r\n- Add pull request template\r\n- Replaced redundant code in load_results (@jbulow)\r\n- Minor PEP8 fixes in dqn.py (@justinkterry)\r\n- Add a message to the assert in `PPO2`\r\n- Update replay buffer doctring\r\n- Fix `VecEnv` docstrings\r\n\r\n## Documentation:\r\n\r\n- Add plotting to the Monitor example (@rusu24edward)\r\n- Add Snake Game AI project (@pedrohbtp)\r\n- Add note on the support Tensorflow versions.\r\n- Remove unnecessary steps required for Windows installation.\r\n- Remove `DummyVecEnv` creation when not needed\r\n- Added `make_vec_env` to the examples to simplify VecEnv creation\r\n- Add QuaRL project (@srivatsankrishnan)\r\n- Add Pwnagotchi project (@evilsocket)\r\n- Fix multiprocessing example (@rusu24edward)\r\n- Fix `result_plotter` example\r\n- Add JNRR19 tutorial (by @edbeeching, @hill-a and @araffin)\r\n- Updated notebooks link\r\n- Fix typo in algos.rst, \"containes\" to \"contains\" (@SyllogismRXS)\r\n- Fix outdated source documentation for load_results\r\n- Add PPO_CPP project (@Antymon)\r\n- Add section on C++ portability of Tensorflow models (@Antymon)\r\n- Update custom env documentation to reflect new gym API for the `close()` method (@justinkterry)\r\n- Update custom env documentation to clarify what step and reset return (@justinkterry)\r\n- Add RL tips and tricks for doing RL experiments\r\n- Corrected lots of typos\r\n- Add spell check to documentation if available",
        "dateCreated": "2019-12-19T23:11:15Z",
        "datePublished": "2019-12-19T23:18:24Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.9.0",
        "name": "Reproducible results, automatic `VecEnv` wrapping, env checker and more usability improvements",
        "tag_name": "v2.9.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.9.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/22372972",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.9.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "## Breaking Changes:\r\n\r\n  - OpenMPI-dependent algorithms (PPO1, TRPO, GAIL, DDPG) are disabled\r\n    in the default installation of stable\\_baselines. mpi4py is now\r\n    installed as an extra. When mpi4py is not available,\r\n    stable-baselines skips imports of OpenMPI-dependent algorithms. See\r\n    <span data-role=\"ref\">installation notes \\<openmpi\\></span> and\r\n    [Issue \\#430](https://github.com/hill-a/stable-baselines/issues/430).\r\n  - SubprocVecEnv now defaults to a thread-safe start method, forkserver\r\n    when available and otherwise spawn. This may require application\r\n    code be wrapped in if \\_\\_name\\_\\_ == '\\_\\_main\\_\\_'. You can\r\n    restore previous behavior by explicitly setting start\\_method =\r\n    'fork'. See [PR \\#428](https://github.com/hill-a/stable-baselines/pull/428).\r\n  - Updated dependencies: tensorflow v1.8.0 is now required\r\n  - Removed checkpoint\\_path and checkpoint\\_freq argument from DQN that\r\n    were not used\r\n  - Removed bench/benchmark.py that was not used\r\n  - Removed several functions from common/tf\\_util.py that were not used\r\n  - Removed ppo1/run\\_humanoid.py\r\n\r\n## New Features:\r\n\r\n  - **important change** Switch to using zip-archived JSON and Numpy\r\n    savez for storing models for better support across library/Python\r\n    versions. (@Miffyli)\r\n  - ACKTR now supports continuous actions\r\n  - Add double\\_q argument to DQN constructor\r\n\r\n## Bug Fixes:\r\n\r\n  - Skip automatic imports of OpenMPI-dependent algorithms to avoid an\r\n    issue where OpenMPI would cause stable-baselines to hang on Ubuntu\r\n    installs. See <span data-role=\"ref\">installation notes\r\n    \\<openmpi\\></span> and [Issue \\#430](https://github.com/hill-a/stable-baselines/issues/430).\r\n  - Fix a bug when calling logger.configure() with MPI enabled\r\n    (@keshaviyengar)\r\n  - set allow\\_pickle=True for numpy\\>=1.17.0 when loading expert\r\n    dataset\r\n  - Fix a bug when using VecCheckNan with numpy ndarray as state. [Issue \\#489](https://github.com/hill-a/stable-baselines/issues/489). (@ruifeng96150)\r\n\r\n## Deprecations:\r\n\r\n  - Models saved with cloudpickle format (stable-baselines\\<=2.7.0) are\r\n    now deprecated in favor of zip-archive format for better support\r\n    across Python/Tensorflow versions. (@Miffyli)\r\n\r\n## Others:\r\n\r\n  - Implementations of noise classes (AdaptiveParamNoiseSpec,\r\n    NormalActionNoise, OrnsteinUhlenbeckActionNoise) were moved from\r\n    stable\\_baselines.ddpg.noise to stable\\_baselines.common.noise. The\r\n    API remains backward-compatible; for example from\r\n    stable\\_baselines.ddpg.noise import NormalActionNoise is still okay.\r\n    (@shwang)\r\n  - Docker images were updated\r\n  - Cleaned up files in common/ folder and in acktr/ folder that were\r\n    only used by old ACKTR version (e.g. filter.py)\r\n  - Renamed acktr\\_disc.py to acktr.py\r\n\r\n## Documentation:\r\n\r\n  - Add WaveRL project (@jaberkow)\r\n  - Add Fenics-DRL project (@DonsetPG)\r\n  - Fix and rename custom policy names (@eavelardev)\r\n  - Add documentation on exporting models.\r\n  - Update maintainers list (Welcome to @Miffyli)\r\n",
        "dateCreated": "2019-09-29T11:46:43Z",
        "datePublished": "2019-09-29T16:54:57Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.8.0",
        "name": "MPI dependency optional, new save format, ACKTR with continuous actions",
        "tag_name": "v2.8.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.8.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/20335903",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.8.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "## New Features\r\n\r\n  - added Twin Delayed DDPG (TD3) algorithm, with HER support\r\n  - added support for continuous action spaces to <span class=\"title-ref\">action\\_probability</span>, computing the\r\n    PDF of a Gaussian policy in addition to the existing support for categorical stochastic policies.\r\n  - added flag to <span class=\"title-ref\">action\\_probability</span> to return log-probabilities.\r\n  - added support for python lists and numpy arrays in `logger.writekvs`. (@dwiel)\r\n  - the info dict returned by VecEnvs now include a `terminal_observation` key providing access to the last observation in a trajectory. (@qxcv)\r\n\r\n##  Bug Fixes\r\n\r\n  - fixed a bug in `traj_segment_generator` where the `episode_starts` was wrongly recorded, resulting in wrong calculation of Generalized Advantage Estimation (GAE), this affects TRPO, PPO1 and GAIL (thanks to @miguelrass for spotting the bug)\r\n  - added missing property <span class=\"title-ref\">n\\_batch</span> in <span class=\"title-ref\">BasePolicy</span>.\r\n\r\n## Others\r\n\r\n  - renamed some keys in `traj_segment_generator` to be more meaningful\r\n  - retrieve unnormalized reward when using Monitor wrapper with TRPO, PPO1 and GAIL to display them in the logs (mean episode reward)\r\n  - clean up DDPG code (renamed variables)\r\n\r\n## Documentation\r\n\r\n  - doc fix for the hyperparameter tuning command in the rl zoo\r\n  - added an example on how to log additional variable with tensorboard and a callback",
        "dateCreated": "2019-07-31T11:15:52Z",
        "datePublished": "2019-07-31T11:25:41Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.7.0",
        "name": "Twin Delayed DDPG (TD3) and GAE bug fix (TRPO, PPO1, GAIL)",
        "tag_name": "v2.7.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.7.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/18982462",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.7.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "## Breaking Changes:\r\n\r\n  - **breaking change** removed `stable_baselines.ddpg.memory` in favor of `stable_baselines.deepq.replay_buffer` (see fix below)\r\n\r\n**Breaking Change:** DDPG replay buffer was unified with DQN/SAC replay buffer. As a result, when loading a DDPG model trained with stable\\_baselines\\<2.6.0, it throws an import error. You can fix that using:\r\n\r\n``` python\r\nimport sys\r\nimport pkg_resources\r\n\r\nimport stable_baselines\r\n\r\n# Fix for breaking change for DDPG buffer in v2.6.0\r\nif pkg_resources.get_distribution(\"stable_baselines\").version >= \"2.6.0\":\r\n    sys.modules['stable_baselines.ddpg.memory'] = stable_baselines.deepq.replay_buffer\r\n    stable_baselines.deepq.replay_buffer.Memory = stable_baselines.deepq.replay_buffer.ReplayBuffer\r\n```\r\n\r\nWe recommend you to save again the model afterward, so the fix won't be needed the next time the trained agent is loaded.\r\n\r\n## New Features:\r\n\r\n  - **revamped HER implementation**: clean re-implementation from scratch, now supports DQN, SAC and DDPG\r\n  - add `action_noise` param for SAC, it helps exploration for problem with deceptive reward\r\n  - The parameter `filter_size` of the function `conv` in A2C utils now supports passing a list/tuple of two integers (height and width), in order to have non-squared kernel matrix. (@yutingsz)\r\n  - add `random_exploration` parameter for DDPG and SAC, it may be useful when using HER + DDPG/SAC. This hack was present in the original OpenAI Baselines DDPG + HER implementation.\r\n  - added `load_parameters` and `get_parameters` to base RL class. With these methods, users are able to load and get parameters to/from existing model, without touching tensorflow. (@Miffyli)\r\n  - added specific hyperparameter for PPO2 to clip the value function (`cliprange_vf`)\r\n  - added `VecCheckNan` wrapper\r\n\r\n## Bug Fixes:\r\n\r\n  - bugfix for `VecEnvWrapper.__getattr__` which enables access to class attributes inherited from parent classes.\r\n  - fixed path splitting in `TensorboardWriter._get_latest_run_id()` on Windows machines (@PatrickWalter214)\r\n  - fixed a bug where initial learning rate is logged instead of its placeholder in `A2C.setup_model` (@sc420)\r\n  - fixed a bug where number of timesteps is incorrectly updated and logged in `A2C.learn` and `A2C._train_step` (@sc420)\r\n  - fixed `num_timesteps` (total\\_timesteps) variable in PPO2 that was wrongly computed.\r\n  - fixed a bug in DDPG/DQN/SAC, when there were the number of samples in the replay buffer was lesser than the batch size (thanks to @dwiel for spotting the bug)\r\n  - **removed** `a2c.utils.find_trainable_params` please use `common.tf_util.get_trainable_vars` instead. `find_trainable_params` was returning all trainable variables, discarding the scope argument. This bug was causing the model to save duplicated parameters (for DDPG and SAC) but did not affect the performance.\r\n\r\n## Deprecations:\r\n\r\n  - **deprecated** `memory_limit` and `memory_policy` in DDPG, please use `buffer_size` instead. (will be removed in v3.x.x)\r\n\r\n## Others:\r\n\r\n  - **important change** switched to using dictionaries rather than lists when storing parameters, with tensorflow Variable names being the keys. (@Miffyli)\r\n  - removed unused dependencies (tdqm, dill, progressbar2, seaborn, glob2, click)\r\n  - removed `get_available_gpus` function which hadn't been used anywhere (@Pastafarianist)\r\n\r\n## Documentation:\r\n\r\n  - added guide for managing `NaN` and `inf`\r\n  - updated ven\\_env doc\r\n  - misc doc updates",
        "dateCreated": "2019-06-13T09:02:40Z",
        "datePublished": "2019-06-13T10:50:56Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.6.0",
        "name": "Hindsight Experience Replay (HER) - Reloaded | get/load parameters",
        "tag_name": "v2.6.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.6.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/17967441",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.6.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "**Warning: breaking change when using custom policies**\r\n\r\n- doc update (fix example of result plotter + improve doc)\r\n- fixed logger issues when stdout lacks `read` function\r\n- fixed a bug in `common.dataset.Dataset` where shuffling was not disabled properly (it affects only PPO1 with recurrent policies)\r\n- fixed output layer name for DDPG q function, used in pop-art normalization and l2 regularization of the critic\r\n- added support for multi env recording to `generate_expert_traj` (@XMaster96)\r\n- added support for LSTM model recording to `generate_expert_traj` (@XMaster96)\r\n- `GAIL`: remove mandatory matplotlib dependency and refactor as subclass of `TRPO` (@kantneel and @AdamGleave)\r\n- added `get_attr()`, `env_method()` and `set_attr()` methods for all VecEnv.\r\n  Those methods now all accept `indices` keyword to select a subset of envs.\r\n  `set_attr` now returns `None` rather than a list of `None`. (@kantneel)\r\n- `GAIL`: `gail.dataset.ExpertDataset` supports loading from memory rather than file, and\r\n  `gail.dataset.record_expert` supports returning in-memory rather than saving to file.\r\n- added support in `VecEnvWrapper` for accessing attributes of arbitrarily deeply nested\r\n  instances of `VecEnvWrapper` and `VecEnv`. This is allowed as long as the attribute belongs\r\n  to exactly one of the nested instances i.e. it must be unambiguous. (@kantneel)\r\n- fixed bug where result plotter would crash on very short runs (@Pastafarianist)\r\n- added option to not trim output of result plotter by number of timesteps (@Pastafarianist)\r\n- clarified the public interface of `BasePolicy` and `ActorCriticPolicy`. **Breaking change** when using custom policies: `masks_ph` is now called `dones_ph`.\r\n- support for custom stateful policies.\r\n- fixed episode length recording in `trpo_mpi.utils.traj_segment_generator` (@GerardMaggiolino)",
        "dateCreated": "2019-05-04T09:00:16Z",
        "datePublished": "2019-05-04T09:05:37Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.5.1",
        "name": "Bug Fixes and Improvements (VecEnv)",
        "tag_name": "v2.5.1",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.5.1",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/17148028",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.5.1"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "- fixed various bugs in GAIL\r\n- added scripts to generate dataset for gail\r\n- added tests for GAIL + data for Pendulum-v0\r\n- removed unused ``utils`` file in DQN folder\r\n- fixed a bug in A2C where actions were cast to ``int32`` even in the continuous case\r\n- added addional logging to A2C when Monitor wrapper is used\r\n- changed logging for PPO2: do not display NaN when reward info is not present\r\n- change default value of A2C lr schedule\r\n- removed behavior cloning script\r\n- added ``pretrain`` method to base class, in order to use behavior cloning on all models\r\n- fixed ``close()`` method for DummyVecEnv.\r\n- added support for Dict spaces in DummyVecEnv and SubprocVecEnv. (@AdamGleave)\r\n- added support for arbitrary multiprocessing start methods and added a warning about SubprocVecEnv that are not thread-safe by default.  (@AdamGleave)\r\n- added support for Discrete actions for GAIL\r\n- fixed deprecation warning for tf: replaces ``tf.to_float()`` by ``tf.cast()``\r\n- fixed bug in saving and loading ddpg model when using normalization of obs or returns (@tperol)\r\n- changed DDPG default buffer size from 100 to 50000.\r\n- fixed a bug in ``ddpg.py`` in ``combined_stats`` for eval. Computed mean on ``eval_episode_rewards`` and ``eval_qs`` (@keshaviyengar)\r\n- fixed a bug in ``setup.py`` that would error on non-GPU systems without TensorFlow installed\r\n\r\nWelcome to @AdamGleave who joins the maintainer team.",
        "dateCreated": "2019-03-28T13:19:41Z",
        "datePublished": "2019-03-28T13:27:13Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.5.0",
        "name": "Working GAIL, pretrain RL models and hotfix for A2C with continuous actions",
        "tag_name": "v2.5.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.5.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/16409797",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.5.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "  - fixed computation of training metrics in TRPO and PPO1\r\n  - added `reset_num_timesteps` keyword when calling train() to continue\r\n    tensorboard learning curves\r\n  - reduced the size taken by tensorboard logs (added a\r\n    `full_tensorboard_log` to enable full logging, which was the\r\n    previous behavior)\r\n  - fixed image detection for tensorboard logging\r\n  - fixed ACKTR for recurrent policies\r\n  - fixed gym breaking changes\r\n  - fixed custom policy examples in the doc for DQN and DDPG\r\n  - remove gym spaces patch for equality functions\r\n  - fixed tensorflow dependency: cpu version was installed overwritting\r\n    tensorflow-gpu when present.\r\n  - fixed a bug in `traj_segment_generator` (used in ppo1 and trpo)\r\n    where `new` was not updated. (spotted by @junhyeokahn)",
        "dateCreated": "2019-02-11T17:11:48Z",
        "datePublished": "2019-02-11T19:13:15Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.4.1",
        "name": "Bug fixes and improvements",
        "tag_name": "v2.4.1",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.4.1",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/15491325",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.4.1"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "- added Soft Actor-Critic (SAC) model\r\n- fixed a bug in DQN where prioritized_replay_beta_iters param was not used\r\n- fixed DDPG that did not save target network parameters\r\n- fixed bug related to shape of true_reward (@abhiskk)\r\n- fixed example code in documentation of tf_util:Function (@JohannesAck)\r\n- added learning rate schedule for SAC\r\n- fixed action probability for continuous actions with actor-critic models\r\n- added optional parameter to action_probability for likelihood calculation of given action being taken.\r\n- added more flexible custom LSTM policies\r\n- added auto entropy coefficient optimization for SAC\r\n- clip continuous actions at test time too for all algorithms (except SAC/DDPG where it is not needed)\r\n- added a mean to pass kwargs to policy when creating a model (+ save those kwargs)\r\n- fixed DQN examples in DQN folder\r\n- added possibility to pass activation function for DDPG, DQN and SAC\r\n\r\nWe would like to thanks our contributors (in random order): @abhiskk @JohannesAck\r\n@EliasHasle @mrakgr @Bleyddyn\r\nand welcoming a new maintainer: @erniejunior ",
        "dateCreated": "2019-01-17T16:19:40Z",
        "datePublished": "2019-01-17T17:16:25Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.4.0",
        "name": "Soft Actor-Critic (SAC) and policy kwargs",
        "tag_name": "v2.4.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.4.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/15037154",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.4.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "- added support for storing model in file like object. (thanks to @erniejunior)\r\n- fixed wrong image detection when using tensorboard logging with DQN\r\n- fixed bug in ppo2 when passing non callable lr after loading\r\n- fixed tensorboard logging in ppo2 when nminibatches=1\r\n- added early stoppping via callback return value (@erniejunior)\r\n- added more flexible custom mlp policies (@erniejunior)\r\n",
        "dateCreated": "2018-12-05T19:20:50Z",
        "datePublished": "2018-12-05T19:25:01Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.3.0",
        "name": "Flexible Custom MLP Policies + bug fixes",
        "tag_name": "v2.3.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.3.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/14371014",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.3.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "- added VecVideoRecorder to record mp4 videos from environment.\r\n",
        "dateCreated": "2018-11-18T11:42:11Z",
        "datePublished": "2018-11-18T12:33:15Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.2.1",
        "name": "Video Recorder",
        "tag_name": "v2.2.1",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.2.1",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/14064267",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.2.1"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "- Hotfix for ppo2, the wrong placeholder was used for the value function\r\n\r\nNote: this bug was present since v1.0, so we recommend to update to the latest version of stable-baselines",
        "dateCreated": "2018-11-07T19:54:23Z",
        "datePublished": "2018-11-07T19:59:27Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.2.0",
        "name": "Hotfix PPO2",
        "tag_name": "v2.2.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.2.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/13884241",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.2.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "- added `async_eigen_decomp` parameter for ACKTR and set it to `False` by default (remove deprecation warnings)\r\n- added methods for calling env methods/setting attributes inside a VecEnv (thanks to @bjmuld)\r\n- updated gym minimum version\r\n\r\nContributors (since v2.0.0):\r\n\r\nThanks to @bjmuld @iambenzo @iandanforth @r7vme @brendenpetersen @huvar",
        "dateCreated": "2018-11-06T20:05:31Z",
        "datePublished": "2018-11-06T20:45:11Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.1.2",
        "name": "New VecEnv Features",
        "tag_name": "v2.1.2",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.1.2",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/13862636",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.1.2"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "- fixed MpiAdam synchronization issue in PPO1 (thanks to @brendenpetersen) issue #50\r\n- fixed dependency issues (new mujoco-py requires a mujoco licence + gym broke MultiDiscrete space shape)\r\n",
        "dateCreated": "2018-10-20T08:15:01Z",
        "datePublished": "2018-10-20T08:28:17Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.1.1",
        "name": "Clean up dependencies + bug fix",
        "tag_name": "v2.1.1",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.1.1",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/13558363",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.1.1"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "**WARNING: This version contains breaking changes, please read the full details**\r\n\r\n- added patch fix for equal function using gym.spaces.MultiDiscrete and gym.spaces.MultiBinary\r\n- fixes for DQN action_probability\r\n- re-added double DQN + refactored DQN policies **breaking changes**\r\n- replaced async with async_eigen_decomp in ACKTR/KFAC for python 3.7 compatibility\r\n- removed action clipping for prediction of continuous actions (see issue #36)\r\n- fixed NaN issue due to clipping the continuous action in the wrong place (issue #36)\r\n",
        "dateCreated": "2018-10-02T12:33:28Z",
        "datePublished": "2018-10-02T12:35:26Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.1.0",
        "name": "Bug fixes ",
        "tag_name": "v2.1.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.1.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/13200270",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.1.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "**WARNING: \tThis version contains breaking changes, please read the full details**\r\n\r\n- Renamed DeepQ to DQN **breaking changes**\r\n- Renamed DeepQPolicy to DQNPolicy **breaking changes**\r\n- fixed DDPG behavior **breaking changes**\r\n- changed default policies for DDPG, so that DDPG now works correctly **breaking changes**\r\n- added more documentation (some modules from common).\r\n- added doc about using custom env\r\n- added Tensorboard support for A2C, ACER, ACKTR, DDPG, DeepQ, PPO1, PPO2 and TRPO\r\n- added episode reward to Tensorboard\r\n- added documentation for Tensorboard usage\r\n- added Identity for Box action space\r\n- fixed render function ignoring parameters when using wrapped environments\r\n- fixed PPO1 and TRPO done values for recurrent policies\r\n- fixed image normalization not occurring when using images\r\n- updated VecEnv objects for the new Gym version\r\n- added test for DDPG\r\n- refactored DQN policies\r\n- added registry for policies, can be passed as string to the agent\r\n- added documentation for custom policies + policy registration\r\n- fixed numpy warning when using DDPG Memory\r\n- fixed DummyVecEnv not copying the observation array when stepping and resetting\r\n- added pre-built docker images + installation instructions\r\n- added ``deterministic`` argument in the predict function\r\n- added assert in PPO2 for recurrent policies\r\n- fixed predict function to handle both vectorized and unwrapped environment\r\n- added input check to the predict function\r\n- refactored ActorCritic models to reduce code duplication\r\n- refactored Off Policy models (to begin HER and replay_buffer refactoring)\r\n- added tests for auto vectorization detection\r\n- fixed render function, to handle positional arguments",
        "dateCreated": "2018-09-18T09:16:05Z",
        "datePublished": "2018-09-18T09:19:19Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v2.0.0",
        "name": "Tensorboard, refactoring and bug fixes",
        "tag_name": "v2.0.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v2.0.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/12953104",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v2.0.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "- added html documentation using sphinx + integration with read the docs\r\n- cleaned up README + typos\r\n-  fixed normalization for DQN with images\r\n- fixed DQN identity test\r\n",
        "dateCreated": "2018-08-29T11:46:39Z",
        "datePublished": "2018-08-29T11:52:51Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v1.0.7",
        "name": "Bug fixes and documentation",
        "tag_name": "v1.0.7",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v1.0.7",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/12630860",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v1.0.7"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "- refactored A2C, ACER, ACTKR, DDPG, DeepQ, GAIL, TRPO, PPO1 and PPO2 under a single constant class\r\n- added callback to refactored algorithm training\r\n- added saving and loading to refactored algorithms\r\n- refactored ACER, DDPG, GAIL, PPO1 and TRPO to fit with A2C, PPO2 and ACKTR policies\r\n- added new policies for most algorithms (Mlp, MlpLstm, MlpLnLstm, Cnn, CnnLstm and CnnLnLstm)\r\n- added dynamic environment switching (so continual RL learning is now feasible)\r\n- added prediction from observation and action probability from observation for all the algorithms\r\n- fixed graphs issues, so models wont collide in names\r\n- fixed behavior_clone weight loading for GAIL\r\n- fixed Tensorflow using all the GPU VRAM\r\n- fixed models so that they are all compatible with vectorized environments\r\n- fixed ```set_global_seed``` to update ```gym.spaces```'s random seed\r\n- fixed PPO1 and TRPO performance issues when learning identity function\r\n- added new tests for loading, saving, continuous actions and learning the identity function\r\n- fixed DQN wrapping for atari\r\n- added saving and loading for Vecnormalize wrapper\r\n- added automatic detection of action space (for the policy network)\r\n- fixed ACER buffer with constant values assuming n_stack=4\r\n- fixed some RL algorithms not clipping the action to be in the action_space, when using ```gym.spaces.Box```\r\n- refactored algorithms can take either a ```gym.Environment``` or a ```str``` ([if the environment name is registered](https://github.com/openai/gym/wiki/Environments))\r\n- Hoftix in ACER (compared to v1.0.0)\r\n\r\nFuture Work :\r\n-  Finish refactoring HER\r\n- Refactor ACKTR and ACER for continuous implementation",
        "dateCreated": "2018-08-20T14:59:21Z",
        "datePublished": "2018-08-20T15:01:55Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v1.0.1",
        "name": "Refactored Stable Baselines",
        "tag_name": "v1.0.1",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v1.0.1",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/12483902",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v1.0.1"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "Do not use: bug in ACER, fixed in v1.0.1",
        "dateCreated": "2018-08-20T13:21:45Z",
        "datePublished": "2018-08-20T13:23:16Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v1.0.0",
        "name": "",
        "tag_name": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v1.0.0",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/12481836",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v1.0.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "- Fixed ```tf.session().__enter__()``` being used, rather than ```sess = tf.session()``` and passing the session to the objects\r\n- Fixed uneven scoping of TensorFlow Sessions throughout the code\r\n- Fixed rolling vecwrapper to handle observations that are not only grayscale images\r\n- Fixed deepq saving the environment when trying to save itself\r\n- Fixed ```ValueError: Cannot take the length of Shape with unknown rank.``` in ```acktr```, when running ```run_atari.py``` script.\r\n- Fixed calling baselines sequentially no longer creates graph conflicts\r\n- Fixed mean on empty array warning with deepq\r\n- Fixed kfac eigen decomposition not cast to float64, when the parameter use_float64 is set to True\r\n- Fixed Dataset data loader, not correctly resetting id position if shuffling is disabled\r\n- Fixed ```EOFError``` when reading from connection in the ```worker``` in ```subproc_vec_env.py```\r\n- Fixed ```behavior_clone``` weight loading and saving for GAIL\r\n- Avoid taking root square of negative number in `trpo_mpi.py`\r\n- Removed some duplicated code (a2cpolicy, trpo_mpi)\r\n- Removed unused, undocumented and crashing function ```reset_task``` in ```subproc_vec_env.py```\r\n- Reformated code to PEP8 style\r\n- Documented all the codebase\r\n- Added atari tests\r\n- Added logger tests\r\n\r\nMissing: tests for acktr continuous (+ HER, gail but they rely on mujoco...)",
        "dateCreated": "2018-08-14T09:38:33Z",
        "datePublished": "2018-08-14T09:39:50Z",
        "html_url": "https://github.com/hill-a/stable-baselines/releases/tag/v0.1.6",
        "name": "Deobfuscation of the code base + pep8 and fixes",
        "tag_name": "v0.1.6",
        "tarball_url": "https://api.github.com/repos/hill-a/stable-baselines/tarball/v0.1.6",
        "url": "https://api.github.com/repos/hill-a/stable-baselines/releases/12392588",
        "zipball_url": "https://api.github.com/repos/hill-a/stable-baselines/zipball/v0.1.6"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Baselines requires python3 (>=3.5) with the development headers. You'll also need system packages CMake, OpenMPI and zlib. Those can be installed as follows\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3382,
      "date": "Mon, 20 Dec 2021 16:40:19 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning-algorithms",
      "reinforcement-learning",
      "machine-learning",
      "gym",
      "openai",
      "baselines",
      "toolbox",
      "python",
      "data-science"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Most of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.\n\nHere is a quick example of how to train and run PPO2 on a cartpole environment:\n```python\nimport gym\n\nfrom stable_baselines.common.policies import MlpPolicy\nfrom stable_baselines.common.vec_env import DummyVecEnv\nfrom stable_baselines import PPO2\n\nenv = gym.make('CartPole-v1')\n#: Optional: PPO2 requires a vectorized environment to run\n#: the env is now wrapped automatically when passing it to the constructor\n#: env = DummyVecEnv([lambda: env])\n\nmodel = PPO2(MlpPolicy, env, verbose=1)\nmodel.learn(total_timesteps=10000)\n\nobs = env.reset()\nfor i in range(1000):\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = env.step(action)\n    env.render()\n\nenv.close()\n```\n\nOr just train a model with a one liner if [the environment is registered in Gym](https://github.com/openai/gym/wiki/Environments) and if [the policy is registered](https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html):\n\n```python\nfrom stable_baselines import PPO2\n\nmodel = PPO2('MlpPolicy', 'CartPole-v1').learn(10000)\n```\n\nPlease read the [documentation](https://stable-baselines.readthedocs.io/) for more examples.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}