{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find MASS useful in your work, you can cite the paper as below:\n\n    @inproceedings{song2019mass,\n        title={MASS: Masked Sequence to Sequence Pre-training for Language Generation},\n        author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},\n        booktitle={International Conference on Machine Learning},\n        pages={5926--5936},\n        year={2019}\n    }\n    \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{song2019mass,\n    title={MASS: Masked Sequence to Sequence Pre-training for Language Generation},\n    author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},\n    booktitle={International Conference on Machine Learning},\n    pages={5926--5936},\n    year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9947094038368315,
        0.9947094038368315
      ],
      "excerpt": "[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/mass-masked-sequence-to-sequence-pre-training/machine-translation-on-wmt2017-chinese)](https://paperswithcode.com/sota/machine-translation-on-wmt2017-chinese?p=mass-masked-sequence-to-sequence-pre-training)                                                              \n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/mass-masked-sequence-to-sequence-pre-training/machine-translation-on-wmt2016-romanian)](https://paperswithcode.com/sota/machine-translation-on-wmt2016-romanian?p=mass-masked-sequence-to-sequence-pre-training)                        \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511535834563841,
        0.8511535834563841
      ],
      "excerpt": "Unsupervised Neural Machine Translation \nSupervised Neural Machine Translation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.887167692142383
      ],
      "excerpt": "valid_fr-en_mt_bleu -&gt; 10.55 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "  --max_epoch 30                                       \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/MASS",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to,\nand actually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.microsoft.com.\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\ninstructions provided by the bot. You will only need to do this once across all repositories using our CLA.\nThis project has adopted the Microsoft Open Source Code of Conduct.\nFor more information see the Code of Conduct FAQ\nor contact opencode@microsoft.com with any additional questions or comments.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-27T13:02:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T02:46:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8606212732858117,
        0.9686165205753966
      ],
      "excerpt": "MASS: Masked Sequence to Sequence Pre-training for Language Generation, by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu, is a novel pre-training method for sequence to sequence based language generation tasks. It randomly masks a sentence fragment in the encoder, and then predicts it in the decoder. \nMASS can be applied on cross-lingual tasks such as neural machine translation (NMT), and monolingual tasks such as text summarization. The current codebase supports unsupervised NMT (implemented based on XLM), supervised NMT, text summarization and conversational response generation, which are all based on Fairseq. We will release our implementation for other sequence to sequence generation tasks in the future. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9288045220482152,
        0.83886059909893
      ],
      "excerpt": "Unsupervised Neural Machine Translation just uses monolingual data to train the models. During MASS pre-training, the source and target languages are pre-trained in one model, with the corresponding langauge embeddings to differentiate the langauges. During MASS fine-tuning, back-translation is used to train the unsupervised models. Code is under MASS-unsupNMT. We provide pre-trained and fine-tuned models: \n| Languages | Pre-trained Model | Fine-tuned Model | BPE codes | Vocabulary | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513243872860823,
        0.9251401156517021,
        0.860059181823877,
        0.9669773309074705
      ],
      "excerpt": "| EN - FR   | MODEL    |   MODEL   | BPE codes | Vocabulary | \n| EN - DE   | MODEL | MODEL | BPE codes | Vocabulary | \n| En - RO   | MODEL | MODEL | BPE_codes | Vocabulary | \nWe are also preparing larger models on more language pairs, and will release them in the future. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "  --reload_model \"$MODEL,$MODEL\"                       \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9710087041128412
      ],
      "excerpt": "| Model | Ro-En BLEU (with BT) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    --reload_model \"$MODEL,$MODEL\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9603602943509616,
        0.9734440518367233
      ],
      "excerpt": "We also implement MASS on fairseq, in order to support the pre-training and fine-tuning for large scale supervised tasks, such as neural machine translation, text summarization. Unsupervised pre-training usually works better in zero-resource or low-resource downstream tasks. However, in large scale supervised NMT, there are plenty of bilingual data, which brings challenges for conventional unsupervised pre-training. Therefore, we design new pre-training loss to support large scale supervised NMT. The code is under MASS-supNMT. \nWe extend the MASS to supervised setting where the supervised sentence pair (X, Y) is leveraged for pre-training. The sentence X is masked and feed into the encoder, and the decoder predicts the whole sentence Y. Some discret tokens in the decoder input are also masked, to encourage the decoder to extract more informaiton from the encoder side.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151467054009306
      ],
      "excerpt": "We release the pre-trained model and example codes of how to pre-train and fine-tune on WMT Chinese<->English (Zh<->En) translation.: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8251503885678884,
        0.8410630159301589
      ],
      "excerpt": "|Zh - En      | MODEL | CODE | VOCAB | VOCAB \nWe provide a simple demo code to demonstrate how to deploy mass pre-training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9535116917392057,
        0.8039616690723781
      ],
      "excerpt": "We also provide a pre-training script which is used for our released model. \nAfter pre-training stage, we fine-tune the model on bilingual sentence pairs: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    --reload_checkpoint $model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9562733499471942
      ],
      "excerpt": "We also provide a fine-tuning script which is used for our pre-trained model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810655369388614,
        0.8985477887134261
      ],
      "excerpt": "MASS for text summarization is also implemented on fairseq. The code is under MASS-summarization. \nMASS uses default Transformer structure. We denote L, H, A as the number of layers, the hidden size and the number of attention heads.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.824351421433897,
        0.824351421433897
      ],
      "excerpt": "| MASS-base-uncased | 6L-768H-12A | 6L-768H-12A | MODEL |  \n| MASS-middle-uncased | 6L-1024H-16A | 6L-1024H-16A | MODEL | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9627487715650112
      ],
      "excerpt": "| CNN/Daily Mail | 43.05 | 20.02 | 40.08 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8811979089297111
      ],
      "excerpt": "dict.txt is included in mass-base-uncased.tar.gz. A copy of binarized data can be obtained from here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723200785439778
      ],
      "excerpt": "min-len is sensitive for different tasks, lenpen needs to be tuned on the dev set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9385521822717969
      ],
      "excerpt": "MASS-base-uncased uses 32x NVIDIA 32GB V100 GPUs and trains on (Wikipekia + BookCorpus, 16GB) for 20 epochs (float32), batch size is simulated as 4096. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "MASS: Masked Sequence to Sequence Pre-training for Language Generation",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our model is trained on Wikipekia + BookCorpus. Here we use wikitext-103 to demonstrate how to process data.\n```\nwget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\nunzip wikitext-103-raw-v1.zip\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/MASS/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 199,
      "date": "Tue, 21 Dec 2021 19:07:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/MASS/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft/MASS",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/MASS/master/MASS-unsupNMT/get-data-gigaword.sh",
      "https://raw.githubusercontent.com/microsoft/MASS/master/MASS-unsupNMT/install-tools.sh",
      "https://raw.githubusercontent.com/microsoft/MASS/master/MASS-unsupNMT/get-data-bilingual-enro-nmt.sh",
      "https://raw.githubusercontent.com/microsoft/MASS/master/MASS-unsupNMT/get-data-nmt.sh",
      "https://raw.githubusercontent.com/microsoft/MASS/master/MASS-supNMT/run_mass_enzh.sh",
      "https://raw.githubusercontent.com/microsoft/MASS/master/MASS-supNMT/generate_enzh_data.sh",
      "https://raw.githubusercontent.com/microsoft/MASS/master/MASS-supNMT/translate.sh",
      "https://raw.githubusercontent.com/microsoft/MASS/master/MASS-supNMT/ft_mass_enzh.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We first prepare the monolingual and bilingual sentences for Chinese and English respectively. The data directory looks like:\n\n```\n- data/\n  \u251c\u2500 mono/\n  |  \u251c\u2500 train.en\n  |  \u251c\u2500 train.zh\n  |  \u251c\u2500 valid.en\n  |  \u251c\u2500 valid.zh\n  |  \u251c\u2500 dict.en.txt\n  |  \u2514\u2500 dict.zh.txt\n  \u2514\u2500 para/\n     \u251c\u2500 train.en\n     \u251c\u2500 train.zh\n     \u251c\u2500 valid.en\n     \u251c\u2500 valid.zh\n     \u251c\u2500 dict.en.txt\n     \u2514\u2500 dict.zh.txt\n```\nThe files under `mono` are monolingual data, while under `para` are bilingual data. `dict.en(zh).txt` in different directory should be identical. The dictionary for different language can be different. Running the following command can generate the binarized data:\n\n```\n#: Ensure the output directory exists\ndata_dir=data/\nmono_data_dir=$data_dir/mono/\npara_data_dir=$data_dir/para/\nsave_dir=$data_dir/processed/\n\n#: set this relative path of MASS in your server\nuser_dir=mass\n\nmkdir -p $data_dir $save_dir $mono_data_dir $para_data_dir\n\n\n#: Generate Monolingual Data\nfor lg in en zh\ndo\n\n  fairseq-preprocess \\\n  --task cross_lingual_lm \\\n  --srcdict $mono_data_dir/dict.$lg.txt \\\n  --only-source \\\n  --trainpref $mono_data_dir/train --validpref $mono_data_dir/valid \\\n  --destdir $save_dir \\\n  --workers 20 \\\n  --source-lang $lg\n\n  #: Since we only have a source language, the output file has a None for the\n  #: target language. Remove this\n\n  for stage in train valid\n  do\n    mv $save_dir/$stage.$lg-None.$lg.bin $save_dir/$stage.$lg.bin\n    mv $save_dir/$stage.$lg-None.$lg.idx $save_dir/$stage.$lg.idx\n  done\ndone\n\n#: Generate Bilingual Data\nfairseq-preprocess \\\n  --user-dir $mass_dir \\\n  --task xmasked_seq2seq \\\n  --source-lang en --target-lang zh \\\n  --trainpref $para_data_dir/train --validpref $para_data_dir/valid \\\n  --destdir $save_dir \\\n  --srcdict $para_data_dir/dict.en.txt \\\n  --tgtdict $para_data_dir/dict.zh.txt\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We use the same BPE codes and vocabulary with XLM. Here we take English-French as an example.\n\n```\ncd MASS\n\nwget https://dl.fbaipublicfiles.com/XLM/codes_enfr\nwget https://dl.fbaipublicfiles.com/XLM/vocab_enfr\n\n./get-data-nmt.sh --src en --tgt fr --reload_codes codes_enfr --reload_vocab vocab_enfr\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9126433182145228
      ],
      "excerpt": "Download dataset by the below command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "mkdir -p $save_dir \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9043929270298233
      ],
      "excerpt": "    --source-langs en,zh \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "mkdir -p $save_dir \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "mkdir -p mono \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9226917079601906,
        0.8150746518872923
      ],
      "excerpt": "wget -c https://modelrelease.blob.core.windows.net/mass/mass-base-uncased.tar.gz \ntar -zxvf mass-base-uncased.tar.gz \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "Text Summarization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py                                      \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "--gelu_activation true                               \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "--eval_bleu true                                     \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  --gelu_activation true                               \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  --eval_bleu true                                     \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8501534490666007
      ],
      "excerpt": "./get-data-bilingual-enro-nmt.sh --src en --tgt ro --reload_codes codes_enro --reload_vocab vocab_enro \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    --gelu_activation true                               \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    --eval_bleu true                                     \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "fairseq-train $data_dir \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8266883653525674
      ],
      "excerpt": "model=checkpoint/mass/pre-training/checkpoint_last.pt #: The path of pre-trained model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "fairseq-train $data_dir \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "    python encode.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8407450268648645
      ],
      "excerpt": ": Move dict.txt from tar file to the data directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8277402966439862
      ],
      "excerpt": "    --trainpref mono/train.txt --validpref mono/valid.txt --testpref mono/test.txt \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8223758761593589,
        0.8424760504462719
      ],
      "excerpt": "    --sample-break-mode none \\ \n    --tokens-per-sample $TOKENS_PER_SAMPLE \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8336686503214243,
        0.8828665034782968
      ],
      "excerpt": "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\ \n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8635670535437081
      ],
      "excerpt": "    --trainpref cnndm/para/train --validpref cnndm/para/valid --testpref cnndm/para/test \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8493592368006115
      ],
      "excerpt": "    --batch-size 64 --beam 5 --min-len 50 --no-repeat-ngram-size 3 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/MASS/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Perl"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "MASS",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MASS",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/MASS/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Currently we implement MASS for unsupervised NMT based on the codebase of [XLM](https://github.com/facebookresearch/XLM). The depencies are as follows:\n- Python 3\n- NumPy\n- PyTorch (version 0.4 and 1.0)\n- fastBPE (for BPE codes)\n- Moses (for tokenization)\n- Apex (for fp16 training)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "After download the repository, you need to install `fairseq` by `pip`:\n```\npip install fairseq==0.7.1\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npip install torch==1.0.0 \npip install fairseq==0.8.0\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nfairseq-train cnndm/processed/ \\\n    --user-dir mass --task translation_mass --arch transformer_mass_base \\\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n    --lr 0.0005 --min-lr 1e-09 \\\n    --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \\\n    --weight-decay 0.0 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n    --update-freq 8 --max-tokens 4096 \\\n    --ddp-backend=no_c10d --max-epoch 25 \\\n    --max-source-positions 512 --max-target-positions 512 \\\n    --skip-invalid-size-inputs-valid-test \\\n    --load-from-pretrained-model mass-base-uncased.pt \\\n```\n`lr=0.0005` is not the optimal choice for any task. It is tuned on the dev set (among 1e-4, 2e-4, 5e-4). \n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1029,
      "date": "Tue, 21 Dec 2021 19:07:33 GMT"
    },
    "technique": "GitHub API"
  }
}