{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you reference our work, please consider citing the following:\n\n\t@inproceedings{icra_2019_fastdepth,\n\t\tauthor      = {{Wofk, Diana and Ma, Fangchang and Yang, Tien-Ju and Karaman, Sertac and Sze, Vivienne}},\n\t\ttitle       = {{FastDepth: Fast Monocular Depth Estimation on Embedded Systems}},\n\t\tbooktitle   = {{IEEE International Conference on Robotics and Automation (ICRA)}},\n\t\tyear        = {{2019}}\n\t}\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{icra_2019_fastdepth,\n    author      = {{Wofk, Diana and Ma, Fangchang and Yang, Tien-Ju and Karaman, Sertac and Sze, Vivienne}},\n    title       = {{FastDepth: Fast Monocular Depth Estimation on Embedded Systems}},\n    booktitle   = {{IEEE International Conference on Robotics and Automation (ICRA)}},\n    year        = {{2019}}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9941858110574581,
        0.951631562983953,
        0.951631562983953,
        0.9395581263655511,
        0.975216409729457,
        0.9909341088909676,
        0.906685992571851
      ],
      "excerpt": "| Eigen et al. [NIPS 2014]                | 228\u00d7304 | 2.06 | 0.907 | 0.611 | 300 | 23 | \n| Eigen et al. [ICCV 2015] (with AlexNet) | 228\u00d7304 | 8.39 | 0.753 | 0.697 | 1400 | 96 | \n| Eigen et al. [ICCV 2015] (with VGG)     | 228\u00d7304 | 23.4 | 0.641 | 0.769 | 2800 | 195 | \n| Laina et al. [3DV 2016] (with UpConv)   | 228\u00d7304 | 22.9 | 0.604 | 0.789 | 2400 | 237 | \n| Laina et al. [3DV 2016] (with UpProj)   | 228\u00d7304 | 42.7 | 0.573 | 0.811 | 3900 | 319 | \n| Xian et al. [CVPR 2018] (with UpProj)   | 384\u00d7384 | 61.8 | 0.660 | 0.781 | 4400 | 283 | \n| This Work                                   | 224\u00d7224 | 0.37 | 0.604 | 0.771 | 37 | 5.6 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dwofk/fast-depth",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-18T19:19:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T22:41:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9863516960886566
      ],
      "excerpt": "Our final model is mobilenet-nnconv5-skipadd-pruned, i.e. a MobileNet-NNConv5 architecture with depthwise separable layers in the decoder, with additive skip connections between the encoder and decoder, and after network pruning using NetAdapt. The other models are offered to provide insight into our approach. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808635290626508,
        0.9701835601215658
      ],
      "excerpt": "The model file for the pretrained MobileNet used in our model definition can be downloaded from http://datasets.lids.mit.edu/fastdepth/imagenet/. \nThis step requires a valid PyTorch installation and a saved copy of the NYU Depth v2 dataset. It is meant to be performed on a host machine with a CUDA GPU, not on an embedded platform. Deployment on an embedded device is discussed in the next section. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374495429650217
      ],
      "excerpt": "The evaluation code will report model accuracy in terms of the delta1 metric as well as RMSE in millimeters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9846737114590599
      ],
      "excerpt": "We use the TVM compiler stack to compile trained models for deployment on an NVIDIA Jetson TX2. Models are cross-compiled on a host machine and then deployed on the TX2. The tvm-compile/tuning folder in this repo contains the results of our auto-tuning the layers within our models for both the TX2 GPU and CPU. These can be used during the compilation process to achieve low model runtimes on the TX2. Outputs of TVM compilation for our trained models can be found at http://datasets.lids.mit.edu/fastdepth/results/tvm_compiled/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9499804181590406
      ],
      "excerpt": "\"This Work\" refers to MobileNet-NNConv5(depthwise), with additive skip connections, pruned. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "ICRA 2019 \"FastDepth: Fast Monocular Depth Estimation on Embedded Systems\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dwofk/fast-depth/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 161,
      "date": "Thu, 23 Dec 2021 07:28:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dwofk/fast-depth/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dwofk/fast-depth",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Deployment requires building the TVM runtime code on the target embedded device (that will be used solely for running a trained and compiled model). The following instructions are taken from [this TVM tutorial](https://docs.tvm.ai/tutorials/cross_compilation_and_rpc.html#build-tvm-runtime-on-device) and have been tested on a **TX2 with CUDA-8.0 and LLVM-4.0 installed**.\n\nFirst, clone the TVM repo and modify config file:\n```bash\ngit clone --recursive https://github.com/dmlc/tvm\ncd tvm\ngit reset --hard ab4946c8b80da510a5a518dca066d8159473345f\ngit submodule update --init\ncp cmake/config.cmake .\n```\nMake the following edits to the `config.cmake` file:\n```cmake\nset(USE_CUDA OFF) -> set(USE_CUDA [path_to_cuda]) #: e.g. /usr/local/cuda-8.0/\nset(USE_LLVM OFF) -> set(USE_LLVM [path_to_llvm-config]) #: e.g. /usr/lib/llvm-4.0/bin/llvm-config\n```\n\nThen build the runtime:\n```bash\nmake runtime -j2\n```\nFinally, update the `PYTHONPATH` environment variable:\n```bash\nexport PYTHONPATH=$PYTHONPATH:~/tvm/python\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "  bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "  cd .. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.85139247959405
      ],
      "excerpt": "On the TX2, power consumption on the main VDD_IN rail can be measured by running the following command: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8978477365863436
      ],
      "excerpt": "python3 main.py --evaluate [path_to_trained_model] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8162922993238315
      ],
      "excerpt": "On the TX2, download the trained models as explained above in the section Trained Models. The compiled model files should be located in results/tvm_compiled. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807977559667914
      ],
      "excerpt": "| on NYU Depth v2     |  Input Size  |  MACs [G]  | RMSE [m] | delta1 | CPU [ms] | GPU [ms] | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8162089604858594,
        0.8162089604858594
      ],
      "excerpt": "<img src=\"img/acc_fps_gpu.png\" alt=\"photo not available\" width=\"375\"> \n<img src=\"img/acc_fps_cpu.png\" alt=\"photo not available\" width=\"375\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dwofk/fast-depth/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Diana Wofk\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FastDepth",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fast-depth",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dwofk",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dwofk/fast-depth/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install [PyTorch](https://pytorch.org/) on a machine with a CUDA GPU. Our code was developed on a system running PyTorch v0.4.1.\n- Install the [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) format libraries. Files in our pre-processed datasets are in HDF5 format.\n  ```bash\n  sudo apt-get update\n  sudo apt-get install -y libhdf5-serial-dev hdf5-tools\n  pip3 install h5py matplotlib imageio scikit-image opencv-python\n  ```\n- Download the preprocessed [NYU Depth V2](http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html) dataset in HDF5 format and place it under a `data` folder outside the repo directory. The NYU dataset requires 32G of storage space.\n\t```bash\n\tmkdir data; cd data\n\twget http://datasets.lids.mit.edu/fastdepth/data/nyudepthv2.tar.gz\n\ttar -xvf nyudepthv2.tar.gz && rm -f nyudepthv2.tar.gz\n\tcd ..\n\t```\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run a compiled model on the device, navigate to the `deploy` folder and run:\n\n```bash\npython3 tx2_run_tvm.py --input-fp [path_to_input_npy_file] --output-fp [path_to_output_npy_file] --model-dir [path_to_folder_with_tvm_compiled_model_files]\n```\n\nNote that when running a model compiled for the GPU, a `cuda` argument must be specified. For instance:\n\n```bash\npython3 tx2_run_tvm.py --input-fp data/rgb.npy --output-fp data/pred.npy --model-dir ../../results/tvm_compiled/tx2_cpu_mobilenet_nnconv5dw_skipadd_pruned/\npython3 tx2_run_tvm.py --input-fp data/rgb.npy --output-fp data/pred.npy --model-dir ../../results/tvm_compiled/tx2_gpu_mobilenet_nnconv5dw_skipadd_pruned/ --cuda True\n```\n\nExample RGB input, ground truth, and model prediction data (as numpy arrays) is provided in the `data` folder. To convert the `.npy` files into `.png` format, navigate into `data` and run `python3 visualize.py`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 686,
      "date": "Thu, 23 Dec 2021 07:28:22 GMT"
    },
    "technique": "GitHub API"
  }
}