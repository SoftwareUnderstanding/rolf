# Knowledge Base Completion (kbc)
This is a fork of the codebase that reproduces results in [Canonical Tensor Decomposition for Knowledge Base Completion](https://arxiv.org/abs/1806.07297) (ICML 2018), adapted to compute Graph Embedding for ClaimsKG. 

## Installation
Create a conda environment with pytorch cython and scikit-learn :
```
conda create --name kbc_env python=3.7
source activate kbc_env
conda install --file requirements.txt -c pytorch
```

Then install the kbc package to this environment
```
python setup.py install
```

## Datasets

To download the ClaimsKG datasets, go to the kbc/kbc/ folder and run:
```bash
mkdir src_data
cd src_data
wget https://tinyurl.com/y26n3dfz
tar -xjf CKG.tbz
wget https://tinyurl.com/y4hclb69
tar -xjf CKGE.tbz
wget https://tinyurl.com/y2jrbnhd
tar -xjf CKGE-KW.tbz
```



Once the datasets are download, add them to the package data folder by running :

```bash
python -m kbc.process_datasets
```

This will create the files required to compute the filtered metrics in `kbc/data/`

## Training the model
Reproduce the model training with the following command. Please repeat for CKG and CKGE-KW.
```
python -m kbc.learn --dataset CKGE --model CP --rank 50 --optimizer
Adagrad --learning_rate 1e-1 --batch_size 150 --regularizer N3 --reg 5e-3
 --max_epochs 30 --valid 10
```

The training will produce a `.pickle` file for each model in the `kbc/models` directory.

## Pre-trained models 

If you wish, you may download our pre-trained models and place them in `kbc/models`

- CKG: https://tinyurl.com/yxps3ca2
- CKGE: https://tinyurl.com/y42ktou3
- CKGE-KW: https://tinyurl.com/y4xshws3



## Evaluating

To compute the link prediction performance for the models, you may use:

```bash
python -m kbc.evaluate path/to/dataset path/to/model.pickle
```

The datasets generated by process_datasets should be in `kbc/data`


## License
kbc is CC-BY-NC licensed, as found in the LICENSE file. So is this fork. 
