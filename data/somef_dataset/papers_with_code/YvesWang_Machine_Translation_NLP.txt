# Machine_Translation_NLP

For this work, we implement four types of modern NMT models, an RNN-based encoder-decoder model, an attention-based RNN encoder-decoder model, a self-attention encoder with an RNN-based attention decoder model, and the Transformer on two language pairs: Vietnamese to English and Chinese to English. The attention-based RNN encoder-decoder model generalizes best on the prepared test set, with 23.16 BLEU score for Vi-En and 13.38 BLEU score for Zh-En. We have dived into each model and presented an in-depth analysis regarding their pros and cons on our task. For potential future work, we will continue working on optimizing our implementations of the self-attention module and the Transformer. We expect that a thoroughly trained and engineering orientated Transformer should exceed our current best model on this sequence to sequence translation task.
