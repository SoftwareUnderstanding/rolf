{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.10059",
      "https://arxiv.org/abs/1706.10059",
      "https://arxiv.org/abs/1706.10059",
      "https://arxiv.org/abs/1706.10059* (2017).\n\n# Results\n\nI have managed to overfit to the training data with no trading costs but it could not generalise to the test data. So far there have been poor results. I have not yet tried hyperparameter optimisation so it could be that parameter tweaking will allow the model to fit, or I may have subtle bugs.\n\n- VPG model,\n  - training: 190% portfolio growth in 50 days\n  - testing: 100% portfolio growth in 50 days\n\n![](https://raw.githubusercontent.com/wassname/rl-portfolio-management/8c74f136765f621eb45d484553b9f778e9243a84/docs/tensorforce-VPG-test.png)\n\nThis test period is directly after the training period and it looks like the usefullness of the models learned knowledge may decay as it moves away from its training interval.\n\nThere are other experiments stored as notebooks in past commits.\n\n# Installing\n\n- `git clone https://github.com/wassname/rl-portfolio-management.git`\n- `cd rl-portfolio-management`\n- `pip install -r requirements/requirements.txt`\n- `jupyter-notebook`\n    - Then open tensorforce-VPG.ipynb in jupyter\n    - Or try an alternative agent  with tensorforce-PPO.ipynb and train\n\n\n# Using the environment\n\nThese environments are dervied from the OpenAI environment class which you can learn about in their [documentation](https://gym.openai.com/docs/).\n\n\nThese environments come with 47k steps of training data and 8k test steps. Each step represents 30 minutes. Thanks to reddit user [ARRRBEEE](https://www.reddit.com/r/BitcoinMarkets/comments/694q0a/historical_pricing_data_for_poloniex_btceth_pairs) for sharing the data.\n\nThere are three output options which you can use as follows:\n\n```py\nimport gym\nimport rl_portfolio_management.environments  # this registers them\n\nenv = gym.envs.spec('CryptoPortfolioEIIE-v0').make()\nprint(\"CryptoPortfolioEIIE has an history shape suitable for an EIIE model (see https://arxiv.org/abs/1706.10059)\")\nobservation = env.reset()\nprint(\"shape =\", observation[\"history\"].shape)\n# shape = (5, 50, 3)\n\nenv = gym.envs.spec('CryptoPortfolioMLP-v0').make()\nprint(\"CryptoPortfolioMLP history has an flat shape for a dense/multi-layer perceptron model\")\nobservation = env.reset()\nprint(\"shape =\", observation[\"history\"].shape)\n# shape = (750,)\n\nenv = gym.envs.spec('CryptoPortfolioAtari-v0').make()\nprint(\"CryptoPortfolioAtari history has been padded to represent an image so you can reuse models tuned on Atari games\")\nobservation = env.reset()\nprint(\"shape =\", observation[\"history\"].shape)\n# shape = (50, 50, 3)\n```\n\nOr define your own:\n```py\nimport rl_portfolio_management.environments import PortfolioEnv\ndf_test = pd.read_hdf('./data/poloniex_30m.hf', key='test')\nenv_test = PortfolioEnv(\n  df=df_test,\n  steps=256,\n  scale=True,\n  augment=0.00,\n  trading_cost=0.0025,\n  time_cost=0.00,\n  window_length=50,\n  output_mode='mlp'\n)\n```\n\nLets try it with a random agent and plot the results:\n\n\n```py\nimport numpy as np\nimport gym\nimport rl_portfolio_management.environments  # this registers them\n\nenv = gym.envs.spec('CryptoPortfolioMLP-v0').make()\nsteps = 150\nstate = env.reset()\nfor _ in range(steps):\n    # The observation contains price history and portfolio weights\n    old_portfolio_weights = state[\"weights\"]\n\n    # the action is an array with the new portfolio weights\n    # for out action, let's change the weights by around a 20th each step\n    action = old_portfolio_weights + np.random.normal(loc=0, scale=1/20., size=(4,))\n\n    # clip and normalize since the portfolio weights should sum to one\n    action = np.clip(action, 0, 1)\n    action /= action.sum()\n\n    observation, reward, done, info = env.step(action)\n\n    if done:\n        break\n\n# plot\nenv.render('notebook')\n```\n\nUnsuprisingly, a random agent doesn't perform well in portfolio management. If it had chosen to bet on blue then black if could have outperformed any single asset, but hindsight is 20/20.\n\n![](docs/img/price_performance.png)\n![](docs/img/weights.png)\n\n# Plotting\n\nYou can run `env.render('notebook')` or extract a pandas dataframe and plot how you like. To use pandas: `pd.DataFrame(gym.unwrapped.infos)`.\n\n\n# Tests\n\nWe have partial test coverage of the environment, just run:\n\n- `python -m pytest`\n\n\n# Files\n\n- enviroments/portfolio.py - contains an openai environment for porfolio trading\n- tensorforce-PPO-IEET.ipynb - notebook to try a policy gradient agent\n\n# Differences in implementation\n\nThe main differences from Jiang et. al. 2017 are:\n\n- The first step in a deep learning project should be to make sure the model can overfit, this provides a sanity check. So I am first trying to acheive good results with no trading costs.\n- I have not used portfolio vector memory. For ease of implementation I made the information available by using the last weights.\n- Instead of DPG ([deterministic policy gradient](http://jmlr.org/proceedings/papers/v32/silver14.pdf)) I tried and DDPG ([deep deterministic policy gradient]( http://arxiv.org/pdf/1509.02971v2.pdf)) and VPG (vanilla policy gradient) with generalized advantage estimation and PPO.\n- I tried to replicate the best performing CNN model from the paper and haven't attempted the LSTM or RNN models.\n- instead of selecting 12 assets for each window I chose 3 assets that have existed for the longest time\n- ~~My topology had an extra layer [see issue 3](https://github.com/wassname/rl-portfolio-management/issues/3)~~ fixed\n\n# TODO\n\nSee issue [#4](https://github.com/wassname/rl-portfolio-management/issues/4) and [#2](https://github.com/wassname/rl-portfolio-management/issues/2) for ideas on where to go from here"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9818894004866677
      ],
      "excerpt": "Author: wassname \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9990486604811452
      ],
      "excerpt": "[1] Jiang, Zhengyao, Dixing Xu, and Jinjun Liang. \"A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem.\" arXiv preprint arXiv:1706.10059 (2017). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283216015784888,
        0.8028046190715653,
        0.8043073075947367
      ],
      "excerpt": "action /= action.sum() \nobservation, reward, done, info = env.step(action) \nif done: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wassname/rl-portfolio-management",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-07-18T14:42:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-11T08:26:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9889230049601745,
        0.9210643563292907
      ],
      "excerpt": "This paper trains an agent to choose a good portfolio of cryptocurrencies. It's reported that it can give 4-fold returns in 50 days and the paper seems to do all the right things so I wanted to see if I could achieve the same results. \nThis repo includes an environment for portfolio management (with unit tests). Hopefully others will find this usefull as I am not aware of any other implementations (as of 2017-07-17). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9604659333269079,
        0.860059181823877
      ],
      "excerpt": "I have managed to overfit to the training data with no trading costs but it could not generalise to the test data. So far there have been poor results. I have not yet tried hyperparameter optimisation so it could be that parameter tweaking will allow the model to fit, or I may have subtle bugs. \nVPG model, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.879554106382239
      ],
      "excerpt": "These environments come with 47k steps of training data and 8k test steps. Each step represents 30 minutes. Thanks to reddit user ARRRBEEE for sharing the data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440096299923007
      ],
      "excerpt": "    #: The observation contains price history and portfolio weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8989375817776913
      ],
      "excerpt": "#: the action is an array with the new portfolio weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051393313040967
      ],
      "excerpt": "Unsuprisingly, a random agent doesn't perform well in portfolio management. If it had chosen to bet on blue then black if could have outperformed any single asset, but hindsight is 20/20. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9013282888922282,
        0.9271123610269956,
        0.8913333900298477,
        0.9878961148779649,
        0.9699261178617721
      ],
      "excerpt": "The first step in a deep learning project should be to make sure the model can overfit, this provides a sanity check. So I am first trying to acheive good results with no trading costs. \nI have not used portfolio vector memory. For ease of implementation I made the information available by using the last weights. \nInstead of DPG (deterministic policy gradient) I tried and DDPG (deep deterministic policy gradient) and VPG (vanilla policy gradient) with generalized advantage estimation and PPO. \nI tried to replicate the best performing CNN model from the paper and haven't attempted the LSTM or RNN models. \ninstead of selecting 12 assets for each window I chose 3 assets that have existed for the longest time \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Attempting to replicate \"A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem\" https://arxiv.org/abs/1706.10059 (and an openai gym environment)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wassname/rl-portfolio-management/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 172,
      "date": "Sat, 25 Dec 2021 13:29:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wassname/rl-portfolio-management/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "wassname/rl-portfolio-management",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/wassname/rl-portfolio-management/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/wassname/rl-portfolio-management/master/pytorch-deeprl-DDPG-EIIE.ipynb",
      "https://raw.githubusercontent.com/wassname/rl-portfolio-management/master/tensorforce-PPO-IEET.ipynb",
      "https://raw.githubusercontent.com/wassname/rl-portfolio-management/master/data/0.%20load%20poliniex%20data%2030m%20multindex.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- `git clone https://github.com/wassname/rl-portfolio-management.git`\n- `cd rl-portfolio-management`\n- `pip install -r requirements/requirements.txt`\n- `jupyter-notebook`\n    - Then open tensorforce-VPG.ipynb in jupyter\n    - Or try an alternative agent  with tensorforce-PPO.ipynb and train\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8314995642045568
      ],
      "excerpt": "env = gym.envs.spec('CryptoPortfolioEIIE-v0').make() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314995642045568
      ],
      "excerpt": "env = gym.envs.spec('CryptoPortfolioMLP-v0').make() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314995642045568
      ],
      "excerpt": "env = gym.envs.spec('CryptoPortfolioAtari-v0').make() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314995642045568
      ],
      "excerpt": "env = gym.envs.spec('CryptoPortfolioMLP-v0').make() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8565940500943436
      ],
      "excerpt": "You can run env.render('notebook') or extract a pandas dataframe and plot how you like. To use pandas: pd.DataFrame(gym.unwrapped.infos). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9005655364327316
      ],
      "excerpt": "python -m pytest \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9133368656218674,
        0.8147287712898238
      ],
      "excerpt": "import gym \nimport rl_portfolio_management.environments  #: this registers them \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8381475436444512
      ],
      "excerpt": "print(\"shape =\", observation[\"history\"].shape) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8381475436444512
      ],
      "excerpt": "print(\"shape =\", observation[\"history\"].shape) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8381475436444512
      ],
      "excerpt": "print(\"shape =\", observation[\"history\"].shape) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674,
        0.8296543510244946
      ],
      "excerpt": "import rl_portfolio_management.environments import PortfolioEnv \ndf_test = pd.read_hdf('./data/poloniex_30m.hf', key='test') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411430919313182,
        0.9457175861910134,
        0.9133368656218674,
        0.8147287712898238
      ],
      "excerpt": "Lets try it with a random agent and plot the results: \nimport numpy as np \nimport gym \nimport rl_portfolio_management.environments  #: this registers them \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8513834729170773
      ],
      "excerpt": "action = old_portfolio_weights + np.random.normal(loc=0, scale=1/20., size=(4,)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8963349340329686
      ],
      "excerpt": ": plot \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wassname/rl-portfolio-management/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "About",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rl-portfolio-management",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "wassname",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wassname/rl-portfolio-management/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 472,
      "date": "Sat, 25 Dec 2021 13:29:25 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "portfolio-management",
      "deep-reinforcement-learning",
      "deeprl",
      "cryptocurrency",
      "openai-gym-environments",
      "openai-gym"
    ],
    "technique": "GitHub API"
  }
}