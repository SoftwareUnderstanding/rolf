{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1905.02244",
      "https://arxiv.org/abs/1905.11946"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite the following paper if you find this is useful for your work: \n\n- C. Kyrkou and T. Theocharides, \"EmergencyNet: Efficient Aerial Image Classification for Drone-Based Emergency Monitoring Using Atrous Convolutional Feature Fusion,\" in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 13, pp. 1687-1699, 2020.[paper\ud83d\udcdc ](https://ieeexplore.ieee.org/abstract/document/9050881)\n\nFor more please visit: https://www.christoskyrkou.com\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ckyrkou/EmergencyNet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-10T14:37:02Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-02T02:01:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9899025349124722,
        0.9936364364919233,
        0.98671501602878,
        0.814561968619232,
        0.9642039080796052,
        0.9516771624192519
      ],
      "excerpt": "There is a need to design specialized networks that are inherently computationally efficient to enable there use in resource contraint devices such as UAVs. The design space can be explored by focusing on the layer configurations, type and connectivity. An architecture is poropsed that allows for flexible aggregation of the multi-scale contextual information while keeping the same resolution and reduced number of parameters. It is based on the Atrous Convolutional Feature Fusion (ACFF) block. computes multiple such atrous convolutional \nfeatures for the same input map across different dilation rates. Each atrous convolution is factored into depth-wise convolution that performs lightweight filtering by applying a single convolutional kernel per input channel to reduce the computational complexity. An essential part of optimized CNNs is reducing not only the spatial size of feature maps but also the channel dimensions. Hence, prior to the atrous convolutions the input feature map channels are halved. This makes it possible to have multiple branches for atrous convolution without significantly impacting the performance. The depth reduction factor is a hyperparameter that can be further tuned depending on the requirements. The atrous convolutional features at different dilation rates need to be combined together to allow the unit to learn from representations from a large effective receptive field. The fusion mechanism is then followed by 1x1 convolutions and activation that non-linearly combine channel features together and projects them into a higher dimensional space. \nThe ACFF macro block is used as a starting point to build a deep neural network that is characterized by lowcomputational complexity and is suitable for embedded platforms.  \n- Reduced Cost of First Layer: The first layer typically incurs the higher computational cost since it is applied across the whole image. Hence, a relatively small number of filters is selected (16) \n- Early downsampling: Downsampling is performed at all the initial layers. A combination of stride and maxpooling layers are used in an effort to reduce the loss of information by aggressive striding \n- Regularization: Due to the relatively small size of the dataset compared to databases such as ImageNet; additional regularization techniques are also incorporated \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9260866192705483
      ],
      "excerpt": "  - Model definition is in model.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.873868460969054
      ],
      "excerpt": "Outputs are the full model file, model weights, accuracy and loss curves. These are stored within the results folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8605023149704653
      ],
      "excerpt": "- EmegencyNet: The ACFF based convolutional neural network (model\ud83d\udcdc ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A small deep learning model based on atrous convolutional feature fusion for the application of emergency response.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ckyrkou/EmergencyNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Mon, 27 Dec 2021 06:05:46 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ckyrkou/EmergencyNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ckyrkou/EmergencyNet",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8002516044630349
      ],
      "excerpt": "- EfficientNet B0: A convolutional neural network finetuned from the EfficientNet B0 architecture (model\ud83d\udcdc ) | You will need to install efficientnet for keras through this repo \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8965131447161919
      ],
      "excerpt": "<img src=\"./Figure/Emergency_Net_ACFF.png\" width=\"1024\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8884675255692654
      ],
      "excerpt": "  - Run train_ACFFNet.py to train a network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8678914137190042
      ],
      "excerpt": "Download and place your dataset and put it in the data folder. Put all images for each class in one folder. Training and Validation splits are handled through a keras generator. If you have already split the two then you need to change to write custom generators. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579,
        0.8639986685036579
      ],
      "excerpt": "\u2502   \u2502   \u2502    |    collapsed_building (1).jpg \n\u2502   \u2502   \u2502    |    collapsed_building (2).jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579,
        0.8639986685036579
      ],
      "excerpt": "\u2502   \u2502   \u2502    |    fire (1).jpg \n\u2502   \u2502   \u2502    |    fire (2).jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579,
        0.8639986685036579
      ],
      "excerpt": "\u2502   \u2502   \u2502    |    flooded_areas (1).jpg \n\u2502   \u2502   \u2502    |    flooded_areas (2).jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579,
        0.8639986685036579
      ],
      "excerpt": "\u2502   \u2502   \u2502    |    normal (1).jpg \n\u2502   \u2502   \u2502    |    normal (2).jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579,
        0.8639986685036579
      ],
      "excerpt": "\u2502   \u2502   \u2502    |    traffic_incident (1).jpg \n\u2502   \u2502   \u2502    |    traffic_incident (2).jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8120015037588023
      ],
      "excerpt": "-- Load the models using the load_model function of keras and read the images with opencv's imread function (BGR format). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8264846994977295
      ],
      "excerpt": "| Class | Network Output Value | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ckyrkou/EmergencyNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "HTML"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "EmergencyNet",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "EmergencyNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ckyrkou",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ckyrkou/EmergencyNet/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "These models where trained using Keras v2.2 and TF v1.8.0. However, the training pipeline is updated to TF v2. See requirements.txt for more details.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Mon, 27 Dec 2021 06:05:46 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The network can correctly classify some indicative examples from real world events such as\n- The Tesla Model X crash into a roadside barrier (2018) -> *traffic_incident* with probability 0.78\n<img src=\"./Figure/Tesla.jpg\" width=\"512\">\n\n- The Hawaii Volcano Eruption (2018) -> *fire* with probability 0.9116652\n<img src=\"./Figure/Hawaii Vulcano.jpg\" width=\"512\">\n\nThe network can also be ported on andoid and itegraded with UAV applications to process the video feed locally.\n\n<img src=\"./Figure/Android_App.jpg\" height=\"512\">\n\n",
      "technique": "Header extraction"
    }
  ]
}