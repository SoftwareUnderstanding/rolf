{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1710.05941](https://arxiv.org/abs/1710.05941",
      "https://arxiv.org/abs/1710.05941"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9774379244639443,
        0.9420169183111816,
        0.9999835195558455
      ],
      "excerpt": "        - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{blue}f(x)=\\frac{xe^x}{1+e^x}=x\\varsigma(x)\" title=\"Swish\"/> \n    - &lt;img src=\"https://latex.codecogs.com/svg.latex?&amp;space;\\color{red}f'(x)=\\frac{e^x}{1+e^x}+\\frac{xe^x}{1+e^x}\\Big(1-\\frac{e^x}{1+e^x}\\Big)=\\varsigma(x)+f(x)\\Big(1-\\varsigma(x)\\Big)\" title=\"Swish Derivative\"/&gt; \n    [Ref paper - https://arxiv.org/abs/1710.05941](https://arxiv.org/abs/1710.05941) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9685968415274212,
        0.9420169183111816
      ],
      "excerpt": "    - &lt;img src=\"https://latex.codecogs.com/svg.latex?&amp;space;\\color{blue}f(x)=\\frac{x}{\\sqrt{x^2+1}}\" title=\"Algebraic\"/&gt; \n    - &lt;img src=\"https://latex.codecogs.com/svg.latex?&amp;space;\\color{red}f'(x)=\\frac{1}{(x^2+1)^\\frac{3}{2}}\" title=\"Algebraic Derivative\"/&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8940583111730483
      ],
      "excerpt": "Combined L1 and L2 (elastic net) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8820449140371859
      ],
      "excerpt": "Objectives - a set of loss functions for minimizing the error between prediction <img src=\"https://latex.codecogs.com/svg.latex?&space;y\" title=\"truth\"/> and truth <img src=\"https://latex.codecogs.com/svg.latex?&space;\\hat{y}\" title=\"truth\"/> with gradient descent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774379244639443,
        0.9420169183111816
      ],
      "excerpt": "    - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{blue}Loss=\\frac1{N}\\sum_{i=1}^{N}(x_i)\\cdot\\tanh(x_i)\\quad where \\quad x_i=y_i-\\hat{y_i}\" title=\"XTanh Loss\"/> \n- &lt;img src=\"https://latex.codecogs.com/svg.latex?&amp;space;\\color{red}\\frac{\\partial{Error}}{\\partial{y}}=\\tanh(x)+x\\cdot(1-\\tanh^2(x))\\quad where \\quad x=y-\\hat{y}\" title=\"XTanh Error gradient\"/&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774379244639443,
        0.9420169183111816
      ],
      "excerpt": "    - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{blue}Loss=\\frac1{N}\\sum_{i=1}^{N}\\frac{x_i^2}{\\sqrt{x_i^2+1}}\\quad where \\quad x_i=y_i-\\hat{y_i}\" title=\"Algebraic Loss\"/> \n- &lt;img src=\"https://latex.codecogs.com/svg.latex?&amp;space;\\color{red}\\frac{\\partial{error}}{\\partial{y}}=\\frac{x^3+2x}{(x^2+1)^{\\frac3{2}}}\\quad where \\quad x=y-\\hat{y}\" title=\"Algebraic Error gradient\"/&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "            size=dim * 12, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "         1        sgd      link        (2, 32)     66 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "         2                 swish       (*, 32)     32 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "         3        adam     link        (32, 24)    800 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511535834563826
      ],
      "excerpt": "        learning_losses.append(evaluated_metric['learning']['loss']) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511535834563826
      ],
      "excerpt": "        learning_accuracies.append(evaluated_metric['learning']['accuracy']) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9386744757199824
      ],
      "excerpt": "        expected_output_t[ix] = np.array([1 if j == 0 else 0, 1 if j == 1 else 0, 1 if j == 2 else 0]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "source bin/activate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "source bin/activate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "source bin/activate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612
      ],
      "excerpt": "0.1.1 (8/13/2019) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9782759268835692
      ],
      "excerpt": "0.1.0 (1/30/2019) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tuantle/simple_nn_with_numpy",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-14T22:57:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-13T23:06:15Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9667177066270394
      ],
      "excerpt": "This was project was created for educational purposes because the best way to learn and understand the fundamentals of artificial neural network algorithms is to build one up from scratch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9719854072173244
      ],
      "excerpt": "Below is a list of implemented (and soon to be implemented) features: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8539207986509055
      ],
      "excerpt": "Both Xtanh and Algebraic losses are similar to Log-cosh loss in which they approximate MSE loss for small x and MAE loss for large x. --&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8074917550722069
      ],
      "excerpt": "Obtimizations - per layer optimation where it is possible to use a combination of optimazation algrorithms in a network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "SGD with momentum \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082886081407988
      ],
      "excerpt": "2D (to be implemented) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9686760747027822
      ],
      "excerpt": "In this example, the objective is to perform a simple classification task on a dataset. Below is a plot of 3 group of 2d points (pink, blue, and teal), clustered in a concentric ring patterns, that will be used to as a test dataset for this example. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9517861394072583
      ],
      "excerpt": "First, import 2 moduldes from simple_nn_with_numpy, a Sequencer and a FeedForward. The job of the sequencer is to serve as a tool for stacking model layers together sequentially. FeedForward serves as the base class for the model. It handles model creation, training/predicting, and saving/loading. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9903672305265713
      ],
      "excerpt": "For this example, CategoryClassifierModel is an inheritance class of FeedForward. The next step is to define the model's feed forward layers by creating a class method called construct. The purpose of this method is to create and return the model's layers as a sequence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8379646292183569
      ],
      "excerpt": "        self._reports = []  #: monitoring data for every epoch during training phase \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9638765779194236,
        0.9278720159544455
      ],
      "excerpt": "Below is the summary printout of the model's layers sequence created above. Notice that one feature of this library is that different optimizers can be use for each layer. Here, Adam optimizer is use lony for the middle hidden layer with the most parameters and SGD optimizers for the outer layers. \nNote that there is an opportunity here to explore whether using a mixture of opimizers rather than one would improve training time & results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8977156423135352
      ],
      "excerpt": "Below are some helper class methods to create animation of the training progress and make nice plots of loss/accuracy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9326285456996185
      ],
      "excerpt": "        #:  this monitor hook is called at the end of every epoch during training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.942593579766155
      ],
      "excerpt": "    #:  save the prediction output at the end of every training epoch for making cool animated plots later on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051419082246902
      ],
      "excerpt": "    for report in self._reports: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.985198825149613
      ],
      "excerpt": "Finally in the code block below, generate_rings method is used to generate concentric rings dataset for training & prediction. And run_example is where everything is put together. Here the model is created, trained, and save/load. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9158673060932769
      ],
      "excerpt": "    #: create the model with minimizing softmax crossentropy loss objectiveself. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8695250731068549
      ],
      "excerpt": "This is a regression example. MNIST digits are recontructed with simple autoencoder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241416084098961
      ],
      "excerpt": "This is an other regression example. Using denoising autoencoder to attempt and remove noise and reconstruct the original signal. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9270518359467552
      ],
      "excerpt": "New Features: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921580416426135
      ],
      "excerpt": "    - Fixed a bug in Sequencer load_snapshot method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9270518359467552
      ],
      "excerpt": "New Features: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A simple neural network library built with Numpy for educational purpose.",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tuantle/simple_nn_with_numpy/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 22:03:34 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tuantle/simple_nn_with_numpy/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "tuantle/simple_nn_with_numpy",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/tuantle/simple_nn_with_numpy/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If using OSX, first install python3 with homebrew. Go here if you don't have homebrew installed(https://brew.sh).\n\n```\nbrew install python3\n```\n\nUse pip3 to install virtualenv and virtualenvwrapper\n\n```\npip3 install virtualenv virtualenvwrapper\n```\n\nAdd these lines to your .bashrc\n\n```\nexport WORKON_HOME=$HOME/.virtualenvs\nexport VIRTUALENVWRAPPER_PYTHON=/usr/local/bin/python3\nexport VIRTUALENVWRAPPER_VIRTUALENV=/usr/local/bin/virtualenv\nsource /usr/local/bin/virtualenvwrapper.sh\n```\n\nClone simple_nn_with_numpy project folder from github and activate it with virtualenv:\n\n```\ngit clone https://github.com/tuantle/simple_nn_with_numpy.git\nvirtualenv simple_nn_with_numpy\ncd simple_nn_with_numpy\nsource bin/activate\n```\n\nIn the activated simple_nn_with_numpy dir, install the only required numpy package\n```\npip3 install numpy\n```\n\nTo run the examples, install the following helper packages (torch and tourchvision for the MNIST dataset & utilities):\n```\npip3 install pandas, matplotlib, seaborn, torch, torchvision\n```\n\n---\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8172233963137904
      ],
      "excerpt": "from matplotlib.pyplot import cm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8036947905999369
      ],
      "excerpt": "        super().__init__(name=name) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "            name='hidden1' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "            name='hidden2' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8833268802260206
      ],
      "excerpt": "            name='output', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "        seq.name = 'category_classifier' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466,
        0.8545850832688996
      ],
      "excerpt": "cd simple_nn_with_numpy \nsource bin/activate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466,
        0.8545850832688996
      ],
      "excerpt": "cd simple_nn_with_numpy \nsource bin/activate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466,
        0.8545850832688996
      ],
      "excerpt": "cd simple_nn_with_numpy \nsource bin/activate \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8073861031678962
      ],
      "excerpt": "    &lt;img width=\"340\" height=\"200\" src=\"assets/plots/xtanh_loss.png\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073861031678962
      ],
      "excerpt": "    &lt;img width=\"340\" height=\"200\" src=\"assets/plots/algebraic_loss.png\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9109777641770318,
        0.8959761549990021
      ],
      "excerpt": "Save trained model as a JSON file \nLoad/reload trained model from a JSON file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9143234175448514
      ],
      "excerpt": "    <img width=\"340\" height=\"300\" src=\"modules/examples/plots/category_classifier_rings_dataset.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8035341326803854
      ],
      "excerpt": "First, import 2 moduldes from simple_nn_with_numpy, a Sequencer and a FeedForward. The job of the sequencer is to serve as a tool for stacking model layers together sequentially. FeedForward serves as the base class for the model. It handles model creation, training/predicting, and saving/loading. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134,
        0.8934477375241742,
        0.8934477375241742,
        0.9068127677393759
      ],
      "excerpt": "import numpy as np \nfrom model.sequencer import Sequencer \nfrom model.nn.feed_forward import FeedForward \nimport matplotlib.pyplot as plt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8019856613580753,
        0.9012248701992861
      ],
      "excerpt": "from matplotlib.pyplot import cm \nimport seaborn as sns \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8180840002533373,
        0.8180840002533373,
        0.8706600329096855
      ],
      "excerpt": "training_input_t = np.zeros((sample_size * class_size, dim))  #: data matrix (each row = single example) \nexpected_output_t = np.zeros((sample_size * class_size, class_size))  #: data matrix (each row = single example) \nexpected_output_labels = np.zeros(sample_size * class_size, dtype='uint8')  #: class labels \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.852750560889179
      ],
      "excerpt": "            name='input' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "            name='hidden1' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "            name='hidden2' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979340603630218
      ],
      "excerpt": "            name='output', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "        seq.name = 'category_classifier' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8891602681090897
      ],
      "excerpt": "Layer   Index   Optim   Type        Shape      Params \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468,
        0.8997243352845468
      ],
      "excerpt": "    (xx, yy) = np.meshgrid(np.arange(x_min, x_max, 0.005), np.arange(y_min, y_max, 0.005)) \n    testing_input_t = np.c_[xx.ravel(), yy.ravel()] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8355986768525016
      ],
      "excerpt": "    figure1 = plt.figure() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310159060815993,
        0.8539464288592398
      ],
      "excerpt": "    plt.subplot(2, 1, 1) \n    plt.xlabel('Epoch') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8410412841906959,
        0.9008815092097038,
        0.9068886930144022,
        0.9259057526449173,
        0.8310159060815993,
        0.8539464288592398
      ],
      "excerpt": "    plt.plot(learning_losses, color='orangered', linewidth=1, linestyle='solid', label='Learning Loss') \n    plt.plot(testing_losses, color='salmon', linewidth=1, linestyle='dotted', label='Testing Loss') \n    plt.legend(fancybox=True) \n    plt.grid() \n    plt.subplot(2, 1, 2) \n    plt.xlabel('Epoch') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.817691889467742,
        0.8852134085057775,
        0.9068886930144022,
        0.9259057526449173,
        0.8913380120550219,
        0.8355986768525016,
        0.877007585726287
      ],
      "excerpt": "    plt.plot(learning_accuracies, color='deepskyblue', linewidth=1, linestyle='solid', label='Learning Accuracy') \n    plt.plot(testing_accuracies, color='aqua', linewidth=1, linestyle='dotted', label='Testing Accuracy') \n    plt.legend(fancybox=True) \n    plt.grid() \n    #: plotting prediction result per training epoch \n    figure2 = plt.figure() \n    figure2.suptitle('Training Results Per Epoch') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468
      ],
      "excerpt": "    (xx, yy) = np.meshgrid(np.arange(x_min, x_max, 0.005), np.arange(y_min, y_max, 0.005)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021374470029473
      ],
      "excerpt": "        im = plt.contourf(xx, yy, zz, cmap=cmap_spring) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150231697958438,
        0.8150231697958438,
        0.9259057526449173
      ],
      "excerpt": "    plt.xlim(xx.min(), xx.max()) \n    plt.ylim(yy.min(), yy.max()) \n    plt.grid() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8668258580770863
      ],
      "excerpt": "    plt.show() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8521427894707788,
        0.9057698899794786,
        0.8958412311986941
      ],
      "excerpt": "        r = 0.125 * (j + 1) + np.random.randn(sample_size) * 0.025 \n        t = np.linspace(j * 4, (j + 1) * 4, sample_size) + np.random.randn(sample_size) * 0.4  #: theta \n        training_input_t[ix] = np.c_[r * np.sin(2 * np.pi * t) + 0.5, r * np.cos(2 * np.pi * t) + 0.5] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8212872038053463
      ],
      "excerpt": "    print('Simple Category Classifier Rings Example.') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338920376454912
      ],
      "excerpt": "    model = CategoryClassifierModel(name='CategoryClassifier').setup(objective='softmax_crossentropy_loss', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9316937895561977,
        0.8502826597757729
      ],
      "excerpt": "    print(model.summary) \n    #: generate  concentric rings dataset for training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8301457765165754
      ],
      "excerpt": "    #: start the training... \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8533841300292017,
        0.9011250291579453
      ],
      "excerpt": "    model.save_snapshot('modules/examples/models/', save_as='category_classifier_rings') \n    anim = model.plot() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9301218376774802,
        0.9316937895561977
      ],
      "excerpt": "    model.load_snapshot('modules/examples/models/category_classifier_rings.json', overwrite=True) \n    print(model.summary) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468,
        0.8997243352845468
      ],
      "excerpt": "    (xx, yy) = np.meshgrid(np.arange(x_min, x_max, 0.005), np.arange(y_min, y_max, 0.005)) \n    testing_input_t = np.c_[xx.ravel(), yy.ravel()] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8525645461651434
      ],
      "excerpt": "    predicted_output_t = model.predict(testing_input_t) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8014284684534173
      ],
      "excerpt": "    figure = plt.figure() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150231697958438,
        0.8150231697958438,
        0.9259057526449173,
        0.8668258580770863,
        0.8458751354831934
      ],
      "excerpt": "    plt.xlim(xx.min(), xx.max()) \n    plt.ylim(yy.min(), yy.max()) \n    plt.grid() \n    plt.show() \nif name == 'main': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833768072879339
      ],
      "excerpt": "To run this example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9128490277849476
      ],
      "excerpt": "python modules/examples/category_classifier.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8945139764042871,
        0.8775206981688755
      ],
      "excerpt": "    <img src=\"modules/examples/plots/category_classifier_rings_training.png\"> \n    <img src=\"modules/examples/plots/category_classifier_rings.gif\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833768072879339
      ],
      "excerpt": "To run this example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9128490277849476
      ],
      "excerpt": "python modules/examples/mnist_reconstruction.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8945139764042871,
        0.8775206981688755
      ],
      "excerpt": "    <img src=\"modules/examples/plots/mnist_reconstruction_training.png\"> \n    <img src=\"modules/examples/plots/mnist_reconstruction.gif\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8945139764042871
      ],
      "excerpt": "    <img src=\"modules/examples/plots/mnist_0_9_recontructed.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833768072879339
      ],
      "excerpt": "To run this example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9128490277849476
      ],
      "excerpt": "python modules/examples/signal_denoising.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8945139764042871,
        0.8775206981688755
      ],
      "excerpt": "    <img src=\"modules/examples/plots/signal_denoising_training.png\"> \n    <img src=\"modules/examples/plots/signal_denoising.gif\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tuantle/simple_nn_with_numpy/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Tuan Le\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# [Simple NN with Numpy](https://github.com/tuantle/simple_nn_with_numpy)\n\n#### A simple neural network library built with numpy!\n\nThis was project was created for educational purposes because the best way to learn and understand the fundamentals of artificial neural network algorithms is to build one up from scratch.\n\nBecause this is a pure numpy library, many calculations have to be done manually, such as computing the gradient for backpropagation. For later time, numpy could be replaced with [JAX numpy](https://github.com/google/jax) for the autograd feature.\n\nBelow is a list of implemented (and soon to be implemented) features:\n- **Linearity & Nonlinearities** - (activation functions)\n    - Linear\n    - ReLU\n    - Leaky ReLU\n    - ELU\n    - SoftPlus\n    - Sigmoid\n    - Tanh\n    - Swish\n        - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{blue}f(x)=\\frac{xe^x}{1+e^x}=x\\varsigma(x)\" title=\"Swish\"/>\n\n        - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{red}f'(x)=\\frac{e^x}{1+e^x}+\\frac{xe^x}{1+e^x}\\Big(1-\\frac{e^x}{1+e^x}\\Big)=\\varsigma(x)+f(x)\\Big(1-\\varsigma(x)\\Big)\" title=\"Swish Derivative\"/>\n\n        [Ref paper - https://arxiv.org/abs/1710.05941](https://arxiv.org/abs/1710.05941)\n\n    <!-- - Algebraic\n        - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{blue}f(x)=\\frac{x}{\\sqrt{x^2+1}}\" title=\"Algebraic\"/>\n\n        - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{red}f'(x)=\\frac{1}{(x^2+1)^\\frac{3}{2}}\" title=\"Algebraic Derivative\"/>\n\n        This activation function has asymptotes of 1 as <img src=\"https://latex.codecogs.com/svg.latex?&space;x\\to\\infty\" title=\"\"/> and -1 as <img src=\"https://latex.codecogs.com/svg.latex?&space;x\\to-\\infty\" title=\"\"/>, so it is simlilar to Tanh.\n        <p align=\"center\">\n            <img width=\"340\" height=\"340\" src=\"assets/plots/algebraic_nonlinearity.png\">\n        </p> -->\n    [Source](https://github.com/tuantle/simple_nn_with_numpy/blob/master/modules/npcore/layer/gates.py)\n- **Regularizations**\n    - Batch Normalization - normalize input batch for a specified layer to have mean = 0 and variance = 1 distribution\n    - Dropout - dropping out neurons for a specified layer with a given probability\n    - Weight - regularize the weights at a specified layer\n        - L1 (lasso)\n        - L2 (ridge)\n        - Combined L1 and L2 (elastic net)\n    [Source](https://github.com/tuantle/simple_nn_with_numpy/blob/master/modules/npcore/regularizers.py)\n- **Initializations**\n    - Zeros\n    - Ones\n    - Constant\n    - Identity\n    - RandomBinary\n    - RandomOrthonormal\n    - RandomUniform\n    - RandomNormal\n    - GlorotRandomUniform\n    - GlorotRandomNormal\n    [Source](https://github.com/tuantle/simple_nn_with_numpy/blob/master/modules/npcore/initializers.py)\n- **Objectives** - a set of loss functions for minimizing the error between prediction <img src=\"https://latex.codecogs.com/svg.latex?&space;y\" title=\"truth\"/> and truth <img src=\"https://latex.codecogs.com/svg.latex?&space;\\hat{y}\" title=\"truth\"/> with gradient descent.\n    - Regression Metrics\n        - MAE loss (L1 loss)\n        - MSE loss (L2 loss)\n        - Log-cosh Loss\n\n        <!-- - XTanh Loss\n            - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{blue}Loss=\\frac1{N}\\sum_{i=1}^{N}(x_i)\\cdot\\tanh(x_i)\\quad where \\quad x_i=y_i-\\hat{y_i}\" title=\"XTanh Loss\"/>\n\n            - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{red}\\frac{\\partial{Error}}{\\partial{y}}=\\tanh(x)+x\\cdot(1-\\tanh^2(x))\\quad where \\quad x=y-\\hat{y}\" title=\"XTanh Error gradient\"/>\n\n            Below is the plot of XTanh loss (red) overlaying MSE loss (blue) and MAE loss (green)\n            <p align=\"center\">\n                <img width=\"340\" height=\"200\" src=\"assets/plots/xtanh_loss.png\">\n            </p> -->\n\n        <!-- - Algebraic Loss\n            - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{blue}Loss=\\frac1{N}\\sum_{i=1}^{N}\\frac{x_i^2}{\\sqrt{x_i^2+1}}\\quad where \\quad x_i=y_i-\\hat{y_i}\" title=\"Algebraic Loss\"/>\n\n            - <img src=\"https://latex.codecogs.com/svg.latex?&space;\\color{red}\\frac{\\partial{error}}{\\partial{y}}=\\frac{x^3+2x}{(x^2+1)^{\\frac3{2}}}\\quad where \\quad x=y-\\hat{y}\" title=\"Algebraic Error gradient\"/>\n\n            Below is the plot of Algebraic loss (red) overlaying MSE loss (blue) and MAE loss (green)\n            <p align=\"center\">\n                <img width=\"340\" height=\"200\" src=\"assets/plots/algebraic_loss.png\">\n            </p>\n\n            Both Xtanh and Algebraic losses are similar to Log-cosh loss in which they approximate MSE loss for small x and MAE loss for large x. -->\n\n    - Classification Metrics\n        - CatergorialCrossentropy Loss (with softmax)\n        - CatergorialCrossentropy Accuracy, Recall, Precision, F1-score\n        - BinaryCrossentropy Loss (with sigmoid)\n        - BinaryCrossentropy Accuracy, Recall, Precision, F1-score\n    [Source](https://github.com/tuantle/simple_nn_with_numpy/blob/master/modules/npcore/layer/objectives.py)\n\n- **Obtimizations** - per layer optimation where it is possible to use a combination of optimazation algrorithms in a network.\n    - SGD\n    - SGD with momentum\n    - RMSprop\n    - Adam\n    [Source](https://github.com/tuantle/simple_nn_with_numpy/blob/master/modules/npcore/optimizers.py)\n- **Models** -\n    - FeedForward\n    - Conv\n        - 1D (to be implemented)\n        - 2D (to be implemented)\n    - Recurrence (to be implemented)\n    - Save trained model as a JSON file\n    - Load/reload trained model from a JSON file\n    [Source](https://github.com/tuantle/simple_nn_with_numpy/blob/master/modules/model)\n----\n\n- [Examples](#examples)\n- [Installation](#installation)\n- [Documentations](#documentations)\n- [Change Log](#change-log)\n- [License](#license)\n\n----\n\n### Examples\n\n#### Catergorial Classification\n\nIn this example, the objective is to perform a simple classification task on a dataset. Below is a plot of 3 group of 2d points (pink, blue, and teal), clustered in a concentric ring patterns, that will be used to as a test dataset for this example.\n\n<p align=\"center\">\n    <img width=\"340\" height=\"300\" src=\"modules/examples/plots/category_classifier_rings_dataset.png\">\n</p>\n\nFirst, import 2 moduldes from simple_nn_with_numpy, a **Sequencer** and a **FeedForward**. The job of the sequencer is to serve as a tool for stacking model layers together sequentially. **FeedForward** serves as the base class for the model. It handles model creation, training/predicting, and saving/loading.\n\n```python\nimport numpy as np\n\nfrom model.sequencer import Sequencer\nfrom model.nn.feed_forward import FeedForward\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import animation\nfrom matplotlib.pyplot import cm\nimport seaborn as sns\n\n# global variables\nsample_size = 512  # number of 2d datapoints\ndim = 2  # dimensionality\nclass_size = 3  # number of classes (rings)\ntraining_input_t = np.zeros((sample_size * class_size, dim))  # data matrix (each row = single example)\nexpected_output_t = np.zeros((sample_size * class_size, class_size))  # data matrix (each row = single example)\nexpected_output_labels = np.zeros(sample_size * class_size, dtype='uint8')  # class labels\n\n```\nFor this example, **CategoryClassifierModel** is an inheritance class of **FeedForward**. The next step is to define the model's feed forward layers by creating a class method called **construct**. The purpose of this method is to create and return the model's layers as a sequence.\n\n```python\nclass CategoryClassifierModel(FeedForward):\n    def __init__(self, name):\n        super().__init__(name=name)\n        self._reports = []  # monitoring data for every epoch during training phase\n        self.assign_hook(monitor=self.monitor)  # attach monitoring hook\n    def construct(self):\n        seq = Sequencer.create(\n            'swish',\n            size=dim,\n            name='input'\n        )\n        seq = Sequencer.create(\n            'swish',\n            size=dim * 16,\n            name='hidden1'\n        )(seq)\n        seq.reconfig(\n            optim='sgd'\n        )\n        seq = Sequencer.create(\n            'swish',\n            size=dim * 12,\n            name='hidden2'\n        )(seq)\n        seq.reconfig(\n            optim='adam'\n        )\n        seq = Sequencer.create(\n            'linear',\n            size=class_size,\n            name='output',\n        )(seq)\n        seq.reconfig(\n            optim='sgd'\n        ).reconfig_all(\n            weight_init='glorot_random_normal',\n            weight_reg='l1l2_elastic_net',\n            bias_init='zeros'\n        )\n        seq.name = 'category_classifier'\n        return seq\n```\n\nBelow is the summary printout of the model's layers sequence created above. Notice that one feature of this library is that different optimizers can be use for each layer. Here, Adam optimizer is use lony for the middle hidden layer with the most parameters and SGD optimizers for the outer layers.\n\nNote that there is an opportunity here to explore whether using a mixture of opimizers rather than one would improve training time & results.\n\n```\nLayer\tIndex\tOptim\tType\t\tShape\t   Params",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "simple_nn_with_numpy",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "tuantle",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tuantle/simple_nn_with_numpy/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 22:03:34 GMT"
    },
    "technique": "GitHub API"
  }
}