{
  "citation": [
    {
      "confidence": [
        0.9788127900661171
      ],
      "excerpt": "By Ilgar Rasulov (Ilgar.Rasulov@hsrw.org), Md. Rakib Hassan (Md-Rakib.Hassan@hsrw.org) and Prabhat Kumar (Prabhat.Kumar@hsrw.org). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059683956071569
      ],
      "excerpt": "Landing algorithm and code explanation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9829698642002277
      ],
      "excerpt": "Computer Vision \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9677855590392761
      ],
      "excerpt": "Single object detection and landing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179493569047226
      ],
      "excerpt": "1.8) Click on \u201cController: SATA\u201d. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9690176918346006
      ],
      "excerpt": "1.10) click on \u201cAdds hard disk\u201d icon. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9537267708315463
      ],
      "excerpt": "$ wget \"https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png\" -O lenna.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "if ret == True: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "            corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9610293202239182
      ],
      "excerpt": "            if k == 27: #-- ESC Button \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "    if (nPatternFound &gt; 1): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "marker_size     = 10 #:- [cm] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "land_speed_cms      = 30.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if marker_found: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9694196313305269,
        0.8665716475375693
      ],
      "excerpt": "        #:-- If high altitude, use baro rather than visual \n        if uav_location.alt >= 5.0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490817347094297
      ],
      "excerpt": "11) calculate north and east attitudepython      \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8422862053358879
      ],
      "excerpt": "12) calculate marker's latitude and longitudepython      \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "            if check_angle_descend(angle_x, angle_y, angle_descend): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693,
        0.8387611005341266
      ],
      "excerpt": "        if z_cm <= land_alt_cm: \n            if vehicle.mode == \"GUIDED\": \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "*/10 8-20 * * 1-5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "link: https://fireship.io/snippets/crontab-crash-course/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593299683604384
      ],
      "excerpt": "  fig.set_size_inches(18, 10) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "  if(iterator > eighty_limit): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": " if (strcmp(names[class], \"banana\") ==0){ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "                    if check_angle_descend(angle_x, angle_y, angle_descend): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955,
        0.8387611005341266
      ],
      "excerpt": "                If Z_cm &lt;= Land_alt_cm: \n                    If Vehicle.Mode == \"Guided\": \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-27T11:40:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-25T08:54:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "OpenCV library provides methods to create and detect aruco markers. The drawmarker() method is defined for generating markers. Dictionary should be chosen beforehand. Example:\r\n\r\n```c\r\ncv::Mat markerImage;\r\ncv::Ptr<cv::aruco::Dictionary> dictionary cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250);\r\ncv::aruco::drawMarker(dictionary, 23, 200, markerImage, 1);\r\ncv::imwrite(\"marker23.png\", markerImage);`\r\n```\r\n\r\nParameters:\r\n-      Dictionary object, created from getPredefinedDictionary method\r\n-      marker id \u2013 should be in the range, defined for the selected dictionary\r\n-      the size of the marker in pixels\r\n-      output image\r\n-      black border width, expressed by the number of internal bits (optional, default equals 1)\r\n\r\nThere are also online aruco generators, one of which was used in this project. As an example, http://chev.me/arucogen/ website can be mentioned.\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "On the wave of current trends in computer vision and drone applications, we are introducing you to a project that we worked on \u201cLanding of Drone on an object\u201d. In general, this means making a drone land on any object by using a landing algorithm and a deep learning algorithm for the detection of an object. We choose the state-of-the-art YOLO algorithm as the object detection algorithm. In this project, our final goal was to land a drone on an object. Therefore, in the first phase, we researched and found an existing solution where the drone was landed on an Aruco marker. In the second phase, we trained the YOLO algorithm to recognize an object (Banana) and modified the landing algorithm to land on that object. Then we further modified the landing algorithm to hover over an object.\r\n\r\nIn this git laboratory, all the procedures and steps needed to reach the goals of this project were written in detail. During the different phases of our project, we faced a lot of challenges. A special thanks to Tiziano Fiorenzani for whom this project was successful. His YouTube videos and tutorials from GitHub helped us a lot.\r\n\r\nReferences: \r\n\r\nTiziano Fiorenzani - Drone Tutorials\r\n\r\nhttps://www.youtube.com/watch?v=TFDWs_DG2QY&list=PLuteWQUGtU9BcXXr3jCG00uVXFwQJkLRa\r\n\r\nAlexeyAB's Darknet repo:\r\n\r\nhttps://github.com/AlexeyAB/darknet/#how-to-train-tiny-yolo-to-detect-your-custom-objects\r\n\r\nYOLv3 Series by Ivan Goncharov\r\n\r\nhttps://www.youtube.com/watch?v=R0hipZXJjlI&list=PLZBN9cDu0MSk4IFFnTOIDihvhnHWhAa8W \r\n\r\nIvangrov's Darknet repo:\r\n\r\nhttps://github.com/ivangrov/YOLOv3-Series\r\n\r\nColab Like a Pro by Ivan Goncharov\r\n\r\nhttps://www.youtube.com/watch?v=yPSphbibqrI&list=PLZBN9cDu0MSnxbfY8Pb1gRtR0rKW4RKBw\r\n\r\nDroneKit-Python project documentation:\r\n\r\nhttps://dronekit-python.readthedocs.io/en/latest/\r\n\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9178604277314122
      ],
      "excerpt": "Remote Access to the Raspberry Pi \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "Raspberry Pi to Pixhawk connection \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "Data Annotation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8344514850842158
      ],
      "excerpt": "Testing the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9867715057530727,
        0.919111702308138
      ],
      "excerpt": "In the testing phase, the communication between the drone and Raspberry Pi was established with Python codes. Normally, Raspberry Pi requires a monitor and a keyboard to work according to your requirements. But these hardwires are not possible to attach to the drone while flying. Therefore, the connection over a network was decided as the optimal solution. We also wanted to run and observe the output of the python scripts on our laptop, so that we can make changes in codes accordingly. \nTo access the Raspberry Pi over a network, both Raspberry and PC/laptop should be connected on the same internet, and the IP address of Raspberry Pi should be defined. The first approach was to connect both the devices to a common WLAN. Unfortunately, as the university WLAN didn\u2019t give access to connected devices\u2019 IP addresses as well as third-party tools were not reliable enough, this approach became inoperable. Instead, the second approach where both devices were connected to the same WiFi hotspot was preferred. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9265792784764137
      ],
      "excerpt": "Network on Rpi also should be configured. The easy way to do is to use a monitor, keyboard, and mouse only once to connect to a hotspot through the standard menu: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372971529347819
      ],
      "excerpt": "Remember DeviceID, in this case, it is \u201c\\.\\PHYSICALDRIVE1\u201d \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372396012058242
      ],
      "excerpt": "1.8) Click on \u201cController: SATA\u201d. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737293746686927
      ],
      "excerpt": "1.13) Launch the virtual machine and verify that the sd card appears in the list of devices. For that run the \u201cfdisk -l\u201d command. In this example, Linux recognized the sd card as /dev/sdb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810706751001248
      ],
      "excerpt": "to mount the sd card \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8923602071010508
      ],
      "excerpt": "Connected devices IP addresses can be viewed from the table below. Currently, the maximum number of devices is 8. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9448272098332261,
        0.990154973801099,
        0.9894744220611426
      ],
      "excerpt": "In the main menu enter the IP address of the Raspberry Pi, for connection type select SSH and click Open: \n8) Enter login and password for access. The default login is \u201cpi\u201d, and password is \u201craspberry\u201d \nAny camera that you will be using, it needs to be calibrated as well. In case of using a raspberry pi on the drone, it is better to use the raspberry camera, because of its lightweight and easy connection to the Pi  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9229634331700834
      ],
      "excerpt": "The full description of code lines can be found in the here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8727945218767179
      ],
      "excerpt": "Take almost 30 to 35 images of the checkboard and save in Raspberry Pi memory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8942522025912093
      ],
      "excerpt": "2) For each image in folder find chessboard corners with findChessboardCorners method and draw them with drawChessboardCorners method from OpenCV. At the same time, store object points (ids of eacht image) and corners  in an array: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "    for i in xrange(len(objpoints)): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9790900581710673,
        0.9642824637664169
      ],
      "excerpt": "Select the drone as per your requirement, after selecting the type of drone-based on the motors or propellers, weather it is quadcopter or hexacopter it has to be calibrated. \nIt can be calibrated with the help of mission planner software \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8743297404003966,
        0.8413574691511022,
        0.9154689971014689
      ],
      "excerpt": "3.     After connecting from drone pixhawk to your laptop or pc click on CONNECT in mission planner \n4.     Go to INITIAL SETUP click on Frame type (select the frame according to your drone). \n5.     Followed to that in INITIAL SETUP click on mandatory hardware for calibrating: - \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9901384721889946
      ],
      "excerpt": "For a drone to let know all the sides and directions, accelerometer calibration is required, it helps the drone to identify which side is left, right front and back. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9366837468056264,
        0.8797725721032874
      ],
      "excerpt": "2. Go according to the Guidelines of Calibrate Accel of setting sides of your drone. The setup will take around 5 to 10 mins. \n3. In case required full instructions, please visit the website:- \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9902929085998392,
        0.9869140670003821,
        0.9100999010553248
      ],
      "excerpt": "It is for the drone to understand all the directions. \n1. To perform the onboard calibration of the compass on the drone: \n2. Click on Compass in Mandatory hardware \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8977344883652697
      ],
      "excerpt": "4. Hold the drone and rotate it in all the directions like a full rotation from the front, back, left, right, top and bottom and keep rotating until you hear three rising beeps, which means your calibration is successful. If you hear an unhappy failure tone, start the procedure again from step 3. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9885847707257227,
        0.9887253269156514
      ],
      "excerpt": "It is the calibration between controller and drone, to set the proper connection of the transmitter and the receiver in both the devices. \n\u2022 Click on Radio Calibration in mandatory hardware, then click Calibrate Radio on the bottom right, press \u201cOK\u201d, when prompted to check the radio control equipment, is on, the battery is not connected, and propellers are not attached. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8196336210243401
      ],
      "excerpt": "\u2022  Follow the complete instruction in detail given on the website: https://ardupilot.org/copter/docs/common-radio-control-calibration.html \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9798254334294746,
        0.9678321793888258
      ],
      "excerpt": "\u2022 A window will appear with the prompt, \u201cEnsure all your sticks are centered and the throttle is down and click ok to continue\u201d. Move the throttle to zero and press \u201cOK\u201d. \n\u2022 Mission Planner will show a summary of the calibration data. Normal values are around 1100 for minimums and 1900 for maximums. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9694346938849965
      ],
      "excerpt": "Communication is done through the Telem2 telemetry port of Pixhawk. Ground, Rx (receiver), Tx(transmitter) and +5V should be connected to 4 pins of Raspberry Pi. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8636552582906797
      ],
      "excerpt": "sudo apt-get update    #:Update the list of packages in the software center \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051386373557338
      ],
      "excerpt": "Answer \u201cNo\u201d to \u201cWould you like a login shell to be accessible over serial?\u201d and \u201cYes\u201d to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.980009851920552
      ],
      "excerpt": "An ArUco marker is a black square marker with the inner binary representation of the identifier. Having black borders, these markers are easy to detect in a frame. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9873770984083011,
        0.9930843912269539,
        0.9755558853639911,
        0.8038287682153252
      ],
      "excerpt": "For each the specific application a dictionary \u2013 a set of markers \u2013 is defined. Dictionaries have such properties as the dictionary size and the marker size. The size of the dictionary is defined by the number of markers it contains, and the marker size is the number of bits it has in the inner part. \nThe identification code of the marker is not the result of the conversion of a binary image to a decimal base, but the market index in the dictionary. The reason is that for a high number of bits the results may become unmanageable. \nThe goal of marker detection is to return the position and id of each marker found in the image. This process can be divided into two steps: \nPossible candidates\u2019 detection \u2013 return all the square shapes and discard non-convex ones, by analyzing contours of each figure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9391416439045188,
        0.8889989524440469
      ],
      "excerpt": "    \u2022 the number of black and white pixels is counted to determine the color of a cell \n    \u2022 the bits are analyzed to their relevance to the selected dictionary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9638815518566987
      ],
      "excerpt": "    \u2022 image with markers for detections \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577131691431271,
        0.8999531193718416
      ],
      "excerpt": "    \u2022 structure of marker corners \n    \u2022 list of marker ids \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669439203956038
      ],
      "excerpt": "    \u2022 list of rejected parameters  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391219811929146
      ],
      "excerpt": "Camera pose can be obtained from the position of the markers if the camera was calibrated (camera matrix and distortion coefficients are available). The camera pose is the transformation from the marker coordinate system to the camera coordinate system. Rotation and transformation are estimated in the form of vectors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8556671124314975
      ],
      "excerpt": "1) check the aruco class object for new detection. ArucoSingleTracker is a wrapper class for access to aruco API. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9682776014594235,
        0.9682776014594235,
        0.9901470555253858
      ],
      "excerpt": "x_cm \u2013 x coordinate of the marker on the image \ny_cm \u2013 y coordinate of the marker on the image \nz_cm \u2013 z coordinate of the marker on the image, for the images taken from less than 5 meters, z_cm is taken as an altitude of the drone. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9960196336748842
      ],
      "excerpt": "3) A drone can navigate to the location of the marker with or without altitude reduction. The decision is done according to the angle between the drone's vertical axis and the vector to the marker. This angle indicates drones closeness to the marker and commands to move with a landing when it is less than some threshold values. The command  of moving with descending is given if the expression \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.949431331353599,
        0.8973027631019259,
        0.9246532601248899,
        0.9322925096094387
      ],
      "excerpt": "4) Calculate latitude and longitude from the marker coordinates. The algorithm was taken from the gis portal and is relatively accurate over small distances (10m within 1km). First, north and east attitudes should be calculated for the current yaw of a drone. Yaw is a rotation indicator in horizontal space. \nSecond, latitude and longitude are calculated. The drone\u2019s coordinate is taken from GPS, earth radius is taken approximately 6378137 meters. \n5) Check angles, calculated at step 3 : go to the marker location at the same altitude or go with the lowering down. Dronekit's simple_goto function for navigating vehicle to specified latitude and longitude is used. \n6) If the height of a drone is less than some threshold altitude, perform vertical landing by changing the mode of the vehicle to \u201cLAND\u201d value. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939945101664401,
        0.9597262010652897
      ],
      "excerpt": "DroneKit-Python is an open-source project, which provides APIs for communication with a vehicle over MAVlink protocol. The aim of these APIs is reading the vehicle's state and parameters, and also enables control over drone's movements.  \nThe main classes and method used in this project are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8467073089891991
      ],
      "excerpt": "dNorth, dEast - North and East attitude of drone, real value \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8806616256394577,
        0.9364301448239745,
        0.8329081330584505
      ],
      "excerpt": "    The algorithm is relatively accurate over small distances (10m within 1km) except close to the poles. \n    For more information see: \n    http://gis.stackexchange.com/questions/2951/algorithm-for-offsetting-a-latitude-longitude-by-some-amount-of-meters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8165999956829729
      ],
      "excerpt": "    earth_radius=6378137.0 #:Radius of \"spherical\" earth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918558397151951
      ],
      "excerpt": "x,y,z coordinates of detected marker \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918558397151951,
        0.9154347397706972
      ],
      "excerpt": "x,y coordinates of detected marker \nReturns converted x,y coordinates of marker to uav coordinate systempython \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9665114197156233
      ],
      "excerpt": "Returns north and east attitudes of the drone \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9473290267301704
      ],
      "excerpt": "Compares angles x and y with a threshold value of angle_desc and returns \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "4) connecting to drone \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810706751001248
      ],
      "excerpt": ":-- Connect to the vehicle \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9709365469826247
      ],
      "excerpt": "7) aruco tracker object is commanded to track marker. It returns marker_found flag, x,y,z coordinates of the markerpython \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "11) calculate north and east attitudepython      \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082905700243322
      ],
      "excerpt": "13) compare angle to decide about descendingpython     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826976468279056
      ],
      "excerpt": "14) call  *simple_goto* method to navigate to a new locationpython     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132083065378932
      ],
      "excerpt": "(Check if the camera is supported and detected)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338852909209754
      ],
      "excerpt": "In other words: \u201cAt every 10th minute past every hour from 8 through 20 on every day-of-week from Monday through Friday\u201d \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9228477935037026
      ],
      "excerpt": "\u2022Step 3: Finding a Pretrained Model for Transfer Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8751494852874246
      ],
      "excerpt": "\u2022Step 6: Device predicts objects in the image or frame \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456923237252608
      ],
      "excerpt": "-Absence of GPU on Raspberry Pi for image processing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669926965632366
      ],
      "excerpt": "-Using YoloV3 tiny version (with degraded accuracy, but better performance) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8614262111885056,
        0.8401101497139473
      ],
      "excerpt": "As we have no GPU, we decided to train the model on Google Colab. \nData was needed for training the tiny yolo but unfortunately, the images from the internet were unacceptable because the model works better if the inference images are as close as possible to the training images. We took six videos in different light intensity and background as shown in the above picture. The object will be under the drone so the banana was put on the ground. Sometimes there were multiple bananas in the video. Frames were extracted from the video using the following tool in Windows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8750555330495635
      ],
      "excerpt": "We had to manually draw a bounding box for 1800 pictures of Banana like the above pictures. We used the following tool given by Ivangrob to do so: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9747771157854559
      ],
      "excerpt": "One advantage of using Google Colab is that the data can be accessed directly from the google drive. We have to mount the drive on the Colab, then Colab will send the link for authorization code and offer input for this code typing. If drive was already mounted, to force remount of google drive, the following parameter should be added to mount method \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9627286031788167
      ],
      "excerpt": ":for force remount of drive \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8527171279650961
      ],
      "excerpt": "This should be done only once, then it is saved in google drive. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9203393736669393
      ],
      "excerpt": "imgShow() - Will help us to show an image in the remote VM  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8648840117028802
      ],
      "excerpt": ": Execute darknet using YOLOv3 model with pre-trained weights to detect objects on 'person.jpg' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9463723640516722
      ],
      "excerpt": "for fichier in filelist[:]:#: filelist[:] makes a copy of filelist. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9044109937113436
      ],
      "excerpt": "Divide the list of files to training and testing set (80% and 20%) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for item in filelist: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8127642756037959
      ],
      "excerpt": "So, for 1 class there should be filters=18, for 2 classes filters=21. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735934956945948
      ],
      "excerpt": "The output is an array of anchors \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136462483417459
      ],
      "excerpt": "They should be put into anchors field of each [yolo] block of the cfg file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9397016067482147,
        0.8348212592059744,
        0.8263992018319982,
        0.9693233393948762
      ],
      "excerpt": "When training is over, different weight files (for each thousand iteration count) in /darknet/backup folder can be used in the detection and the best one is chosen. Using weights file with maximum iterations is not always the best solution for the Overfitting phenomena when the model is trained too much and can recognize only the objects, which look exactly like the training set images. \nA copy of the file with beforementioned operations was saved from Google Colab as ipynb file in this repository, and can be accessed from here. \nFor more information, visit the following links: \nTutorial Darknet To Colab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8345132925718411
      ],
      "excerpt": "Mean Average Precision (mAP) chart can be calculated while the model is learning. To do this in training command -map parameter should be added: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9835244811258373,
        0.942671389917893
      ],
      "excerpt": "The above picture depicts the mean average precision values for every 1000 iterations of our models. We reached mAP50 96% which is very good. We can take 2000, 3000 or 4000 weights and see which one is performing better. We finally choose the weight for 4000 iterations after lots of testing because the Bananas were recognized best. \nTo learn more about Mean Average Precision(mAP), visit the following link: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8099996829629795
      ],
      "excerpt": "To be able to use a camera, a reboot is required. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9875797465670799
      ],
      "excerpt": "NNPack is used to optimize Darknet for embedded devices with ARM CPUs without using a GPU. It uses transform-based convolution computation which allows 40% faster performance on non-initial frames. Particularly, for repeated inferences, ie. video, NNPack is more beneficial to use. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9053763961288188,
        0.9516744041784417
      ],
      "excerpt": "The next step is to do inference or test the YOLO model. The trained weight of the model was download from Google Colab.  \nFor inference, we first used the code provided by ivangrov. His code is also uploaded to this Github. Here is the link \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213843563452123,
        0.8385893917783847,
        0.9150058782610206
      ],
      "excerpt": "The screenshot below shows the section of the code where the center of the bounding box could be retrieved.  \nFor more understanding, please follow his tutorial . \nAfter a lot of research,, we came across darknet-nnpack which was described in the earlier sections. Here is the link for darknet-nnpack. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8812962138385424,
        0.9578885194054715,
        0.876303434113751,
        0.9642719726127916
      ],
      "excerpt": "We needed to calculate the center of the bounding box for the detected objects. This could be retrieved from the Image.c file. The screenshot below shows the part of the script where the center of the bounding box was generated. We took this center of the bounding box for the \"Landing Algorithm with object detection\" code. \nA copy of our trained model with all the files except the images and CUDA files are uploaded in this Github. Here is the link. \nThe Google Colab ipython notebook describing the main steps for training the model is named as 'Tiny YOLO v3 explained.ipynb'. \nThe part with coordinates\u2019 processing was changed to the needs of the task. Darknet library for object detection is capable to draw boxes over the detected object in the frame but doesn\u2019t return the coordinates of the boxes for further processing. The source code of darknet (image.c file)was changed in the way that before drawing bounding box, coordinates of box are saved into a .txt file. The following lines were added: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.816316928816816
      ],
      "excerpt": "In the landing algorithm, we need to add the center of the bounding boxes. The bounding box coordinates are read from the aforementioned .txt file and integrated to the landing algorithm as shown below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8547452256980289
      ],
      "excerpt": "The z coordinate is taken from the drone\u2019s altitude as darknet does not provide that information: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.935320584316636,
        0.985689730506845
      ],
      "excerpt": "The first step is only changed where the drone will try to read the bounding box center of the detected object. The rest of the steps will remain the same. The code single_banana_detection.py of the modified landing algorithm is commented for better understanding. The code is not tested yet. \nFor cases of 2 and more bananas, the approach was changed to hovering over all detected objects in turn. Image.c file was changed to save to .txt file all the coordinates of the detected objects. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9916650001287476
      ],
      "excerpt": "All the centers of the bounding boxes of the detected objects(multiple bananas) are loaded from the text file and integrated into the landing algorithm. The idea is to hover over all the objects one by one and descend 1 meter. It can be changed with the help of dronekit library. The following codes show this process:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.990240729976162,
        0.9204273563616121,
        0.8254975401106441,
        0.8854132203577819
      ],
      "excerpt": "Hovering approach was chosen for multiple objects which is discussed in the next section of this documentation. The rest of the steps will remain same. The code multiple_bananas_detection.py of the modified landing algorithm is commented for better understanding. The code is not tested yet. \nThis code is for tracking an object. It detects an object and hovers on it. \nIn hovering_approach.py hover_alt_cm variable was added to define the altitude of drone. \nThe part of code in the first step dealing with the new location was changed  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8620299013289446
      ],
      "excerpt": "Part of code with command to land at specified height was also removed \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 22 Dec 2021 19:41:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "HSRW-EMRP-WS1920-Drone/object_detection_and_landing",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/Tiny_yolo_v3_explained.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/Testing%20for%20yolov3%20tiny/darknet-nnpack/scripts/dice_label.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/Testing%20for%20yolov3%20tiny/darknet-nnpack/scripts/gen_tactic.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/Testing%20for%20yolov3%20tiny/darknet-nnpack/scripts/get_coco_dataset.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/Testing%20for%20yolov3%20tiny/darknet-nnpack/scripts/imagenet_label.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/image_yolov2.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/video_v2.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/json_mjpeg_streams.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/image_yolov3.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/build.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/net_cam_v3.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/video_yolov3.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/scripts/dice_label.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/scripts/get_imagenet_train.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/scripts/gen_tactic.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/scripts/get_coco_dataset.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/scripts/imagenet_label.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/scripts/windows/windows_imagenet_train.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/scripts/windows/windows_imagenet_label.sh",
      "https://raw.githubusercontent.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/master/darknet%20Google%20Colab/scripts/windows/otb_get_labels.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Weights file is required for training model in darknet. This enables *continuing* training from the point where algorithm stopped, as darknet saves weights file after every 1000 iterations. But for the first run of command there is not yet any file generated. In this case standard weights file \"yolov3-tiny.weights\" may be used. Another possible solution is to generate an initial weights file by running the command\r\n\r\n`!./darknet partial yolov3-tiny0802.cfg yolov3-tiny.weights weights/yolov3-tiny.conv.15 15`\r\n\r\nwhich will save the file to \"yolov3-tiny.conv.15\" in this case.\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": " \r\nUpload img folder with images and txt files to google drive /darknet/img\r\ncreate obj.data and obj.names files\r\nobj.data (count of classes, paths to train, test, class names and results folders) content:\r\n\r\n\r\n    classes= 1\r\n    train = /content/gdrive/My\\ Drive/darknet/train.txt\r\n    valid = /content/gdrive/My\\ Drive/darknet/test.txt \r\n    names = /content/gdrive/My\\ Drive/darknet/obj.names\r\n    backup = /content/gdrive/My\\ Drive/darknet/backup\r\n\r\n \r\nobj.names (names of classes) content:\r\n```python\r\nBanana\r\n```\r\n \r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nDarknet was cloned from https://github.com/kriyeng/darknet/ repository and compiled.\r\n\r\n\r\n    !git clone https://github.com/kriyeng/darknet/\r\n    %cd darknet    \r\n     \r\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    !cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n    \r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nThe version of cuDNN for linux (archived as tgz) was downloaded from https://developer.nvidia.com/ and uploaded to google drive.\r\n\r\n\r\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9330399853344404
      ],
      "excerpt": "GPU Support \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9481832461083745
      ],
      "excerpt": "1) Insert SD card to PC running Linux. If PS works on Windows, Linux can be installed as a guest OS via virtualization tools. The setup steps for VirtualBox software is given below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8706407300098272
      ],
      "excerpt": "VirtualBox steps are over at this point. Further steps are provided for the Linux environment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8902627162932362
      ],
      "excerpt": "\u201cmkdir /mnt/SD\u201d \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9252471999246258
      ],
      "excerpt": "1.16) Type the command \"cd /mnt/SD\" to access the files on the SD card. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd boot \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9391837128648746
      ],
      "excerpt": "4) Enter the following text with your wifi name and password \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd /boot \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.959206846752924
      ],
      "excerpt": "6) Enable WiFi Hotspot on Windows \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9888476258823505,
        0.8468902673898102,
        0.9625160964234066,
        0.9759733396555043,
        0.9749738741761527,
        0.9792447275420578,
        0.9931626210712194
      ],
      "excerpt": "Follow the codes one by one to install and compile OpenCV 4.1.2 on Raspbian OS \n$ chmod +x *.sh \n$ ./download-opencv.sh \n$ ./install-deps.sh \n$ ./build-opencv.sh \n$ cd ~/opencv/opencv-4.1.2/build \n$ sudo make install \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9007942192412138
      ],
      "excerpt": "After installing OpenCV, do a test run by using below codes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8610624824402715
      ],
      "excerpt": "Command to run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9652429714592387,
        0.8771402993159422,
        0.9465743288369519
      ],
      "excerpt": "To calibrate follow the steps one by one: - \n1. Download and install the mission planner software from here (https://firmware.ardupilot.org/Tools/MissionPlanner/). \n2.     Click on INITIAL SETUP > Install Firmware > (based on your number of motors in drone select the ArduCopter version) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9824960639975979
      ],
      "excerpt": "The required packages should be installed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9301730090243674,
        0.964169884472086,
        0.9913835070321887,
        0.9913835070321887
      ],
      "excerpt": "sudo apt-get install python3-dev python3-opencv python3-wxgtk3.0 libxml2-dev python3-pip python3-matplotlib python3-lxml \nsudo pip3 install future \nsudo pip3 install pymavlink \nsudo pip3 install mavproxy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933538006273495
      ],
      "excerpt": "cv::Mat outputImage = inputImage.clone(); \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8120073529341687
      ],
      "excerpt": "1) Import libraries from dronekit, pymavlink packages for connection with a drone \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "python     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "python     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836940488750035
      ],
      "excerpt": ":--- Get the camera calibration path \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.834464664226565
      ],
      "excerpt": "Sometimes the camera does not get detected so use the following commands to solve the problem: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9575727196795211
      ],
      "excerpt": "You can edit your cron jobs with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9018913341475877
      ],
      "excerpt": "You can cancel it with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.827506319687822
      ],
      "excerpt": "Using OpenLabeling (https://github.com/ivangrov/YOLOv3-Series) tool bananas were marked on images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9894831604068929,
        0.8180159340673421
      ],
      "excerpt": "Before installing cuDNN CUDA version should be checked. \n!/usr/local/cuda/bin/nvcc --version \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8137495241225027
      ],
      "excerpt": "!chmod a+r /usr/local/cuda/include/cudnn.h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8516685309054747
      ],
      "excerpt": "#Compile Darknet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8725649885637233,
        0.8828827306434418,
        0.8057675898656085
      ],
      "excerpt": "download() - Will allow you to get some file from your notebook in case you need to  \nupload() - You can upload files to your current folder on the remote VM \ndef imShow(path): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9031092957667828
      ],
      "excerpt": "def download(path): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.846020436779509
      ],
      "excerpt": "  files.download(path) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9453056052288827
      ],
      "excerpt": "To test that darknet was correctly installed, the following code can be run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179850560064857
      ],
      "excerpt": "A decision was made to use NNPACK - a library for the neural network to run on a multi-core CPU. The following steps should be completed to use NNPack. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9985848843376449,
        0.9987587249549833
      ],
      "excerpt": "Install  package installer for Python pip \nsudo apt-get install python-pip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9769420979004304
      ],
      "excerpt": "NNPack installation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.884812952321687,
        0.9714524924589099,
        0.9979742820108133,
        0.9979742820108133,
        0.999833231880651,
        0.9943696579696117,
        0.9906248903846466,
        0.9442478622630452
      ],
      "excerpt": "Install building tool Ninja \nInstall PeachPy and confu \nsudo pip install --upgrade git+https://github.com/Maratyszcza/PeachPy \nsudo pip install --upgrade git+https://github.com/Maratyszcza/confu \nInstall Ninja \ngit clone https://github.com/ninja-build/ninja.git \ncd ninja \ngit checkout release \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999833231880651,
        0.9879863063452118,
        0.9906248903846466,
        0.9072934417257499
      ],
      "excerpt": "Install NNPack \ngit clone https://github.com/shizukachan/NNPACK \ncd NNPACK \nconfu setup \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9279645640371155
      ],
      "excerpt": "Build NNPack using Ninja \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999833231880651,
        0.9879863063452118,
        0.9906248903846466,
        0.9206633769338113
      ],
      "excerpt": "Install darknet-nnpack \ngit clone -b yolov3 https://github.com/zxzhaixiang/darknet-nnpack \ncd darknet-nnpack \ngit checkout yolov3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "  python \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8074341581685098
      ],
      "excerpt": "Testing the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8063270112066849
      ],
      "excerpt": "1.3) insert an SD card and run the command from 1.2 again. The output will be different: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8489642329726498
      ],
      "excerpt": "1.15) Type the command \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803177068631598
      ],
      "excerpt": "After the first boot, Linux will move this file to /etc/wpa_supplicant/ folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8180010781961637
      ],
      "excerpt": "Create empty file ssh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8883320236164733,
        0.9650262079181304
      ],
      "excerpt": "$ python2 test.py lenna.jpg \n$ python3 test.py lenna.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.904285992551401,
        0.9246227682586091
      ],
      "excerpt": "For taking images, run save_images.py script:  \n$ python save_images.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8189680553317972
      ],
      "excerpt": "The images will be saved in the same folder where the (camera_test.py) python script is located \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8700259426728446,
        0.8395836020776087
      ],
      "excerpt": "folder - path containing images files, default \"./camera_01\" \nimage type - the type of these images, default \"jpg\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8545281381758029
      ],
      "excerpt": "        #-- Read the file and convert in greyscale \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8781579062986529
      ],
      "excerpt": "        print(\"Reading image \", fname) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052665961656552
      ],
      "excerpt": "                print(\"Image Skipped\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8620053153196068
      ],
      "excerpt": "            print(\"Image accepted\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466914366639081
      ],
      "excerpt": "        print(\"Image to undistort: \", imgNotGood) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8219347682623649
      ],
      "excerpt": "        dst = cv2.remap(img,mapx,mapy,cv2.INTER_LINEAR) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8507865069687717,
        0.936606094659785,
        0.8666897621829477,
        0.8665944066572312,
        0.8061603637074384,
        0.936606094659785,
        0.8908416552919326
      ],
      "excerpt": "        dst = dst[y:y+h, x:x+w] \n        print(\"ROI: \", x, y, w, h) \n        cv2.imwrite(workingFolder + \"/calibresult.png\",dst) \n        print(\"Calibrated picture saved as calibresult.png\") \n        print(\"Calibration Matrix: \") \n        print(mtx) \n        print(\"Disortion: \", dist) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550335416452869,
        0.9366742056987271,
        0.8550335416452869,
        0.919801971280925
      ],
      "excerpt": "    filename = workingFolder + \"/cameraMatrix.txt\" \n    np.savetxt(filename, mtx, delimiter=',') \n    filename = workingFolder + \"/cameraDistortion.txt\" \n    np.savetxt(filename, dist, delimiter=',') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8126753621872488
      ],
      "excerpt": "        error = cv2.norm(imgpoints[i],imgpoints2,             cv2.NORM_L2)/len(imgpoints2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9498289382149775
      ],
      "excerpt": "    print(\"total error: \", mean_error/len(objpoints)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9561733974561525
      ],
      "excerpt": "Run mavlink.py file to test the connection with Pixhawk \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8094817608407268
      ],
      "excerpt": "returns true value, where   and   in radians. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from pymavlink import mavutil \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785
      ],
      "excerpt": "print \"dlat, dlon\", dLat, dLon \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029665106384296
      ],
      "excerpt": "boolean true, commanding to descend and false, commanding to move at the same altitude. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8839533744076357,
        0.8839533744076357
      ],
      "excerpt": "camera_matrix       = np.loadtxt(calib_path+'cameraMatrix_raspi.txt', delimiter=',') \ncamera_distortion   = np.loadtxt(calib_path+'cameraDistortion_raspi.txt', delimiter=',')                                     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.809726610547402
      ],
      "excerpt": "while True:               \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785
      ],
      "excerpt": "            print  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785,
        0.936606094659785,
        0.936606094659785
      ],
      "excerpt": "            #: print \"\" \n            print \" \" \n            print \"Altitude = %.0fcm\"%z_cm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "            marker_lat, marker_lon  = get_location_metres(uav_location, north0.01, east0.01) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8236891885803026,
        0.8828665034782968
      ],
      "excerpt": "                print \"Low error: descending\" \n                location_marker         = LocationGlobalRelative(marker_lat, marker_lon, uav_location.alt-(land_speed_cms0.01/freq_send)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481281421191109,
        0.9160799407096906
      ],
      "excerpt": "            print \"UAV Location    Lat = %.7f  Lon = %.7f\"%(uav_location.lat, uav_location.lon) \n            print \"Commanding to   Lat = %.7f  Lon = %.7f\"%(location_marker.lat, location_marker.lon) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216270093103228
      ],
      "excerpt": "For example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674,
        0.9068127677393759
      ],
      "excerpt": "  import cv2 \n  import matplotlib.pyplot as plt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8769509153442827
      ],
      "excerpt": "fig = plt.gcf() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.868422234365396,
        0.8310159060815993,
        0.8668258580770863
      ],
      "excerpt": "  #:plt.rcParams['figure.figsize'] = [10, 5] \n  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)) \n  plt.show() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238375460560197
      ],
      "excerpt": "  from google.colab import files \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8452962700954253,
        0.852354753729585,
        0.8701026194892192,
        0.9280524198687952
      ],
      "excerpt": "  for name, data in uploaded.items(): \n    with open(name, 'wb') as f: \n      f.write(data) \n      print ('saved file', name) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238375460560197,
        0.8137273863466191
      ],
      "excerpt": "  from google.colab import files \n  files.download(path) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8377947450552261
      ],
      "excerpt": "!./darknet detect cfg/yolov3.cfg yolov3.weights data/person.jpg -dont-show \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401558704798054
      ],
      "excerpt": "import os \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8442050447375637,
        0.8461874511003897
      ],
      "excerpt": "train = open('/content/gdrive/My Drive/darknet/train.txt', 'w') \ntest=open('/content/gdrive/My Drive/darknet/test.txt', 'w') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227891941992628
      ],
      "excerpt": "    test.write(\"/content/gdrive/My Drive/darknet/img/%s\\r\\n\" % item) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8218047550497161
      ],
      "excerpt": "    train.write(\"/content/gdrive/My Drive/darknet/img/%s\\r\\n\" % item) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "./configure.py --bootstrap \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8193763070449588
      ],
      "excerpt": "python ./configure.py --backend auto \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "sudo python rpi_video.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8078402863087443
      ],
      "excerpt": "sudo python rpi_record.py \nWeight and configuration file paths can be changed inside beforementioned python files at the line: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8472302176346418
      ],
      "excerpt": "In the OD.py file, the coco.names, config and weight files were renamed. The config file is the config we used to train our model. The weight file is the trained weight which we have to mention inside the OD.py file. Inside the coco.names, we put the name of the banana as banana is the only class.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8122437314536581
      ],
      "excerpt": "With darknet-nnpack, the banana was detected after running the rpi_record.py. Like above, we changed the config and weight file names in the python script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8105788893052202,
        0.8159625784020268
      ],
      "excerpt": "            FILE * fp; // create file link \n            fp = fopen(\"results.txt\",\"w\"); // open / create file \u201cresults.txt\u201d for writing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8684116046780243,
        0.864344793584503
      ],
      "excerpt": "with open(\"results.txt\",\"r\") as file: \n            for line in file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621882743635256
      ],
      "excerpt": "FILE * fp; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8684116046780243
      ],
      "excerpt": "with open(\"results.txt\",\"r\") as file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.882491443924941,
        0.882491443924941
      ],
      "excerpt": "                print(\"x is \",x_cm) \n                print(\"y is \",y_cm) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785,
        0.936606094659785
      ],
      "excerpt": "                print(\" \") \n                print(\"Altitude = %.0fcm\",z_cm) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "                marker_lat, marker_lon  = get_location_metres(uav_location, north*0.01, east*0.01) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481281421191109,
        0.9160799407096906
      ],
      "excerpt": "                print(\"UAV Location    Lat = %.7f  Lon = %.7f\",uav_location.lat, uav_location.lon) \n                print(\"Commanding to   Lat = %.7f  Lon = %.7f\",location_marker.lat, location_marker.lon) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8236891885803026,
        0.8828665034782968
      ],
      "excerpt": "                        print(\"Low error: descending\") \n                        location_marker         = LocationGlobalRelative(marker_lat, marker_lon, uav_location.alt-(land_speed_cms*0.01/freq_send)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8684116046780243
      ],
      "excerpt": "with open(\"results.txt\",\"r\") as file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.882491443924941,
        0.882491443924941
      ],
      "excerpt": "                print(\"x is \",x_cm) \n                print(\"y is \",y_cm) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785,
        0.936606094659785
      ],
      "excerpt": "                print(\" \") \n                print(\"Altitude = %.0fcm\",z_cm) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "                marker_lat, marker_lon  = get_location_metres(uav_location, north*0.01, east*0.01) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481281421191109,
        0.9160799407096906
      ],
      "excerpt": "                print(\"UAV Location    Lat = %.7f  Lon = %.7f\",uav_location.lat, uav_location.lon) \n                print(\"Commanding to   Lat = %.7f  Lon = %.7f\",location_marker.lat, location_marker.lon) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C",
      "Jupyter Notebook",
      "Cuda",
      "Python",
      "C++",
      "CMake",
      "Batchfile",
      "Shell",
      "Makefile",
      "PowerShell",
      "C#"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Object Detection and drone landing",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "object_detection_and_landing",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "HSRW-EMRP-WS1920-Drone",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HSRW-EMRP-WS1920-Drone/object_detection_and_landing/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nYou can run the python script at bootup with Crontab. We were running the python script in the testing phase with remote access. It is better to enable automatic execution of python script at a specific time of the day or when the Raspberry Pi boots up. First, we need to run the code:\r\n\r\nRun crontab with the -e flag to edit the cron table:\r\n\r\n```c\r\ncrontab -e\r\n```\r\n\r\nThen type the following command in the crontab:\r\n\r\n    0 8 * * 1-5 python /home/pi/code1.py\r\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    !cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n    \r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 19:41:49 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n![darknet](images/colab.png)\r\n\r\n\r\nWe choose Google colab over AWS and Floydhub for the following reasons:\r\n\r\n\u25cfProvides free GPU support\r\n\r\n\u25cfAllows developers to use and share Jupyter Notebooks among each other like Google Docs\r\n\r\n\u25cfCan access data from Google Drive\r\n\r\n\u25cfAll major Python libraries, like TensorFlow, Scikit-learn, Matplotlib, etc. are pre-installed\r\n\r\n\u25cfBuilt on top of Jupyter Notebook\r\n\r\n\u25cfRun two files for 12 hours\r\n\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    }
  ]
}