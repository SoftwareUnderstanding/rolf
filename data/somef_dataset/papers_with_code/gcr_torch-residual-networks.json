{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is a Torch implementation of [\"Deep Residual Learning for Image Recognition\",Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun](http://arxiv.org/abs/1512.03385) the winners of the 2015 ILSVRC and COCO challenges.\n\n**What's working:** CIFAR converges, as per the paper.\n\n**What's not working yet:** Imagenet. I also have only implemented Option\n(A) for the residual network bottleneck strategy.\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.9688370202610737
      ],
      "excerpt": "2016-01-12: Release results of CIFAR experiments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9737692239911679
      ],
      "excerpt": "Second, results on CIFAR-10 often contradicts results on ImageNet. I.e., leaky ReLU > ReLU on CIFAR, but worse on ImageNet. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gcr/torch-residual-networks",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2015-12-31T23:29:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-11T00:51:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9100543493931427,
        0.8887919598960596,
        0.8988832722922695
      ],
      "excerpt": "CIFAR: Effect of model size \nCIFAR: Effect of model architecture on shallow networks \n...on deep networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8988832722922695
      ],
      "excerpt": "...on deep networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327137588946802
      ],
      "excerpt": "2016-01-21: Completed the 'alternate solver' experiments on deep networks. These ones take quite a long time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9632697623990528,
        0.9565604749570176,
        0.9808706040204527
      ],
      "excerpt": "New results: Re-ran the 'alternate building block' results on deeper networks. They have more of an effect. \nAdded a table of contents to avoid getting lost. \nAdded experimental artifacts (log of training loss and test error, the saved model, the any patches used on the source code, etc) for two of the more interesting experiments, for curious folks who want to reproduce our results. (These artifacts are hereby released under the zlib license.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8679602198121987
      ],
      "excerpt": "New CIFAR results: I re-ran all the CIFAR experiments and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9704425816954181
      ],
      "excerpt": "  with replacement. These new results are much more stable over time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9294444264672171,
        0.9774177650849752
      ],
      "excerpt": "For this test, our goal is to reproduce Figure 6 from the original paper: \nWe train our model for 200 epochs (this is about 7.8e4 of their \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9518939951991752
      ],
      "excerpt": "learning rate of 0.1 and reduce it to 0.01 at 80 epochs and then to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826526926056183
      ],
      "excerpt": "In all cases except for the 32-layer network, we achieve very slightly \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8569797321671049
      ],
      "excerpt": "\u00b9: For this run, we started from a learning rate of 0.001 until the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9180053315759182
      ],
      "excerpt": "trained as usual. This is consistent with the actual paper's results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8415793602336195,
        0.909726477149209,
        0.9144141170408999,
        0.9027345703186043
      ],
      "excerpt": "on the test set. (This method of reporting effectively introduces an \nextra parameter into the model--which model to use from the \nensemble--and this parameter is fitted to the test set) \nThis experiment explores the effect of different NN architectures that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030622469027243,
        0.9669660394701247
      ],
      "excerpt": "The original paper used a \"Building Block\" similar to the \"Reference\" \nmodel on the left part of the figure below, with the standard \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9289971853912135
      ],
      "excerpt": "of this architecture is that they move the ReLU after the addition. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033839382131517,
        0.8810272659602354,
        0.9680214198484612
      ],
      "excerpt": "  (Middle) The reasoning behind this choice is to test whether \n  normalizing the first term of the addition is desirable. It grew out \n  of the mistaken belief that batch normalization always normalizes to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9642865625349574,
        0.8346004676775673,
        0.8046477113810384
      ],
      "excerpt": "  layers of the network. To avoid this, we could either move the ReLU \n  before the addition or remove it completely. However, it is not \n  correct to move the ReLU before the addition: such an architecture \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8526243442149487,
        0.8750285101729768,
        0.9442286436877683,
        0.8738370983177984
      ],
      "excerpt": "  addition term could never be negative. The other option is to simply \n  remove the ReLU completely, sacrificing the nonlinear property of \n  this layer. It is unclear which approach is better. \nTo test these strategies, we repeat the above protocol using the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.87653981504624
      ],
      "excerpt": "All methods achieve accuracies within about 0.5% of each other. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999902052409947
      ],
      "excerpt": "seems to make a small improvement on CIFAR, but there is too much \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9284292338281305
      ],
      "excerpt": "pronounced when evaluated on very deep networks. We retry the above \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9426859874410056
      ],
      "excerpt": "For deep networks, it's best to put the batch normalization before \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.831415761817418
      ],
      "excerpt": "  This could be problematic because each BN is not idempotent (the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222294067903393
      ],
      "excerpt": "Removing the ReLU layer at the end of each building block appears to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8599160013470599
      ],
      "excerpt": "For ordinary CaffeNet networks, @ducha-aiki found that putting batch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.862473703151547
      ],
      "excerpt": "Can we improve on the basic SGD update rule with Nesterov momentum? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9749143330236025
      ],
      "excerpt": "In our experiments, vanilla SGD with Nesterov momentum and a learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.988229651021485,
        0.8230225897646798,
        0.9365268660032661
      ],
      "excerpt": "for more details on each of these learning strategies. \nDeeper networks are more prone to overfitting. Unlike the earlier \nexperiments, all of these models (except Adagrad with a learning rate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.884236576752009
      ],
      "excerpt": "Once again, using vanilla SGD with Nesterov momentum achieves the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302092136063466,
        0.8846333946454523
      ],
      "excerpt": "For our experiments, we use batch normalization using an exponential \nrunning mean and standard deviation with a momentum of 0.1, meaning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9213572222074315,
        0.9314392696683101
      ],
      "excerpt": "The strictest interpretation of the original batch normalization paper \nis to calculate the mean and standard deviation across the entire \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8300664124310889
      ],
      "excerpt": "We attempt to see whether batch normalization momentum affects \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9710947417075954,
        0.9469474321015792
      ],
      "excerpt": "is the number of batches seen so far (N resets to 0 at every epoch). \nAt the end of training for a certain epoch, this means the batch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8766961126798444
      ],
      "excerpt": "None of these effects appear to make a significant difference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is a Torch implementation of [\"Deep Residual Learning for Image Recognition\",Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun](http://arxiv.org/abs/1512.03385) the winners of the 2015 ILSVRC and COCO challenges.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gcr/torch-residual-networks/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 152,
      "date": "Tue, 21 Dec 2021 10:17:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gcr/torch-residual-networks/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "gcr/torch-residual-networks",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/gcr/torch-residual-networks/master/Residual%20Network%20Experiment%20Results.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8117092341479553
      ],
      "excerpt": "iterations on the above graph). Like their paper, we start at a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8619389470726903
      ],
      "excerpt": "To test these strategies, we repeat the above protocol using the \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8085377873664554
      ],
      "excerpt": "0.01 at 160 epochs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327124213669819
      ],
      "excerpt": "| Model                                 | My Test Error | Reference Test Error from Tab. 6 | Artifacts | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002975329356464
      ],
      "excerpt": "  have zero mean and unit variance. If this were true, building an \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "| BN, momentum = 0.01                |  0.0835 | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gcr/torch-residual-networks/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Lua"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "zlib License",
      "url": "https://api.github.com/licenses/zlib"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b\"Copyright (c) 2016 Michael Wilber\\n\\nThis software is provided 'as-is', without any express or implied\\nwarranty. In no event will the authors be held liable for any damages\\narising from the use of this software.\\n\\nPermission is granted to anyone to use this software for any purpose,\\nincluding commercial applications, and to alter it and redistribute it\\nfreely, subject to the following restrictions:\\n\\n1. The origin of this software must not be misrepresented; you must not\\n   claim that you wrote the original software. If you use this software\\n   in a product, an acknowledgement in the product documentation would be\\n   appreciated but is not required.\\n2. Altered source versions must be plainly marked as such, and must not be\\n   misrepresented as being the original software.\\n3. This notice may not be removed or altered from any source distribution.\\n\"",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Residual Learning for Image Recognition",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "torch-residual-networks",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "gcr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gcr/torch-residual-networks/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 554,
      "date": "Tue, 21 Dec 2021 10:17:13 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- You need at least CUDA 7.0 and CuDNN v4.\n- Install Torch.\n- Install the Torch CUDNN V4 library: `git clone https://github.com/soumith/cudnn.torch; cd cudnn; git co R4; luarocks make` This will give you `cudnn.SpatialBatchNormalization`, which helps save quite a lot of memory.\n- Install nninit: `luarocks install nninit`.\n- Download\n  [CIFAR 10](http://torch7.s3-website-us-east-1.amazonaws.com/data/cifar-10-torch.tar.gz).\n  Use `--dataRoot <cifar>` to specify the location of the extracted CIFAR 10 folder.\n- Run `train-cifar.lua`.\n\n",
      "technique": "Header extraction"
    }
  ]
}