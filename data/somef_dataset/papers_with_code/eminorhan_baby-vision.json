{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We are very grateful to the volunteers who contributed recordings to the SAYCam dataset. We thank Jessica Sullivan for her generous assistance with the dataset. We also thank the team behind the Toybox dataset, as well as the developers of PyTorch and torchvision for making this work possible. This project was partly funded by the NSF Award 1922658 NRT-HDR: FUTURE Foundations, Translation, and Responsibility for Data Science.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2007.16189",
      "https://arxiv.org/abs/2002.05709"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9042581090606773
      ],
      "excerpt": "Orhan AE, Gupta VV, Lake BM (2020) Self-supervised learning through the eyes of a child. Advances in Neural Information Processing Systems 34 (NeurIPS 2020). \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eminorhan/baby-vision",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-03T00:14:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-18T04:37:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [`temporal_classification.py`](https://github.com/eminorhan/baby-vision/blob/master/temporal_classification.py): trains temporal classification models as described in the paper. This file uses code recycled from the PyTorch ImageNet training [example](https://github.com/pytorch/examples/tree/master/imagenet).\n* [`read_saycam.py`](https://github.com/eminorhan/baby-vision/blob/master/read_saycam.py): SAYCam video-to-image reader.\n* [`moco`](https://github.com/eminorhan/baby-vision/tree/master/moco) directory contains helper files for training static and temporal MoCo models. The code here was modified from [Facebook's MoCo repository](https://github.com/facebookresearch/moco).\n* [`moco_img.py`](https://github.com/eminorhan/baby-vision/blob/master/moco_img.py): trains an image-based MoCo model as described in the paper. This code was modified from [Facebook's MoCo repository](https://github.com/facebookresearch/moco).\n* [`moco_temp.py`](https://github.com/eminorhan/baby-vision/blob/master/moco_temp.py): trains a temporal MoCo model as described in the paper. This code was also modified from [Facebook's MoCo repository](https://github.com/facebookresearch/moco).\n* [`moco_utils.py`](https://github.com/eminorhan/baby-vision/blob/master/moco_utils.py): some utility functions for MoCo training.\n* [`linear_decoding.py`](https://github.com/eminorhan/baby-vision/blob/master/linear_decoding.py): evaluates self-supervised models on downstream linear classification tasks.\n* [`linear_combination_maps.py`](https://github.com/eminorhan/baby-vision/blob/master/linear_combination_maps.py): plots spatial attention maps as in Figure 4b and Figure 6 in the paper.\n* [`highly_activating_imgs.py`](https://github.com/eminorhan/baby-vision/blob/master/highly_activating_imgs.py): finds highly activating images for a given feature as in Figure 7b in the paper.\n* [`selectivities.py`](https://github.com/eminorhan/baby-vision/blob/master/selectivities.py): measures the class selecitivity indices of all features in a given layer as in Figure 7a in the paper.\n* [`hog_baseline.py`](https://github.com/eminorhan/baby-vision/blob/master/hog_baseline.py): runs the HOG baseline model as described in the paper.\n* [`imagenet_finetuning.py`](https://github.com/eminorhan/baby-vision/blob/master/imagenet_finetuning.py): ImageNet evaluations.\n* [`feature_animation.py`](https://github.com/eminorhan/baby-vision/blob/master/feature_animation.py) and [`feature_animation_class.py`](https://github.com/eminorhan/baby-vision/blob/master/feature_animation_class.py): Some tools for visualizing the learned features.\n\nFor specific usage examples, please see the slurm scripts provided in the [`scripts`](https://github.com/eminorhan/baby-vision/tree/master/scripts) directory.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.97593193790166
      ],
      "excerpt": "This repository contains code for reproducing the results reported in the following paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302948112509837
      ],
      "excerpt": "This project uses the SAYCam dataset described in the following paper:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.973095071902868,
        0.919545575404823,
        0.9857501106548893,
        0.9343200620393548
      ],
      "excerpt": "The dataset is hosted on the Databrary repository for behavioral science. Unfortunately, we are unable to publicly share the SAYCam dataset here due to the terms of use. However, interested researchers can apply for access to the dataset with approval from their institution's IRB.  \nIn addition, this project also uses the Toybox dataset for evaluation purposes. The Toybox dataset is publicly available at this address. \nSince the publication of the paper, we have found that training larger capacity models for longer with the temporal classification objective significantly improves the evaluation results. Hence, we provide below pre-trained resnext50_32x4d type models that are currently our best models trained with the SAYCam data. We encourage people to use these new models instead of the mobilenet_v2 type models reported in the paper (the pre-trained mobilenet_v2 models reported in the paper are also provided below for the record).  \nFour pre-trained resnext50_32x4d models are provided here: temporal classification models trained on data from the individual children in the SAYCam dataset (TC-S-resnext, TC-A-resnext, TC-Y-resnext) and a temporal classification model trained on data from all three children (TC-SAY-resnext). These models were all trained for 16 epochs (with batch size 256) with the following data augmentation pipeline: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9771091085560109
      ],
      "excerpt": "This data augmentation pipeline is similar to that used in the SimCLR paper with slightly larger random crops and slightly stronger color augmentation. Here are some evaluation results for these resnext50_32x4d models (to download the models, click on the links over the model names): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8019699433842047
      ],
      "excerpt": "| TC-A-resnext    | 86.8 | 50.4 | -- | -- | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9674805097857093,
        0.9063425986289931
      ],
      "excerpt": "Here, ImageNet (linear) refers to the top-1 validation accuracy on ImageNet with only a linear classifier trained on top of the frozen features, and ImageNet (1% ft + linear) is similar but with the entire model first fine-tuned on 1% of the ImageNet training data (~12800 images). Note that these are results from a single run, so you may observe slightly different numbers. \nThese models come with the temporal classification heads attached. To load these models, please do something along the lines of: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8477678735851494
      ],
      "excerpt": "model = models.resnext50_32x4d(pretrained=False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9769597640572742,
        0.9829226956442136
      ],
      "excerpt": "where n_out should be 6269 for TC-SAY-resnext, 2765 for TC-S-resnext, 1786 for TC-A-resnext, and 1718 for TC-Y-resnext. The differences here are due to the different lengths of the datasets.  \nIn addition, please find below the best performing ImageNet models reported above: a model with a linear ImageNet classifier trained on top of the frozen features of TC-SAY-resnext (TC-SAY-resnext-IN-linear) and a model that was first fine-tuned with 1% of the ImageNet training data (TC-SAY-resnext-IN-1pt-linear): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Self-supervised learning through the eyes of a child",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eminorhan/baby-vision/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Fri, 24 Dec 2021 10:30:35 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/eminorhan/baby-vision/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "eminorhan/baby-vision",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/hog_baseline.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/selectivities.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/feature_animation_class.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/read_saycam.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/moco_img.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/imagenet_finetuning.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/linear_combination_maps.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/linear_decoding.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/moco_temp.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/highly_activating_imgs.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/feature_animation.sh",
      "https://raw.githubusercontent.com/eminorhan/baby-vision/master/scripts/temporal_classification.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8762729030460817,
        0.8824631550975379
      ],
      "excerpt": "import torchvision.models as models \nmodel = models.resnext50_32x4d(pretrained=False) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/eminorhan/baby-vision/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Emin Orhan\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Self-supervised learning through the eyes of a child",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "baby-vision",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "eminorhan",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eminorhan/baby-vision/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* pytorch == 1.5.1\n* torchvision == 0.6.1\n\nSlightly older or newer versions will probably work fine as well.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 130,
      "date": "Fri, 24 Dec 2021 10:30:35 GMT"
    },
    "technique": "GitHub API"
  }
}