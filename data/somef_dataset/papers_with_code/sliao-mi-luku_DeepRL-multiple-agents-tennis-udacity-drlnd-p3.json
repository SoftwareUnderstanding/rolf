{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.02275\n2. T. P. Lillicrap et al., 2016. *Continuous control with deep reinforcement learning*\\\nhttps://arxiv.org/abs/1509.02971\n3. Udacity's GitHub repository **ddpg-pendulum**\\\nhttps://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum\n4. Udacity's drlnd jupyter notebook template of **Project: Collaboration and Competition**\n5. Udacity's drlnd MADDPG-Lab (maddpg.py",
      "https://arxiv.org/abs/1509.02971\n3. Udacity's GitHub repository **ddpg-pendulum**\\\nhttps://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum\n4. Udacity's drlnd jupyter notebook template of **Project: Collaboration and Competition**\n5. Udacity's drlnd MADDPG-Lab (maddpg.py"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. R. Lowe et al., 2017. *Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments*\\\nhttps://arxiv.org/abs/1706.02275\n2. T. P. Lillicrap et al., 2016. *Continuous control with deep reinforcement learning*\\\nhttps://arxiv.org/abs/1509.02971\n3. Udacity's GitHub repository **ddpg-pendulum**\\\nhttps://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum\n4. Udacity's drlnd jupyter notebook template of **Project: Collaboration and Competition**\n5. Udacity's drlnd MADDPG-Lab (maddpg.py)\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sliao-mi-luku/AI-Tennis-Players",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-05T23:07:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-25T22:52:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9971285687098973
      ],
      "excerpt": "This repository corresponds to Project #3 of Udacity's Deep Reinforcement Learning Nanodegree (drlnd)\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.989878166229622,
        0.9521040510283131
      ],
      "excerpt": "In this project, I used the multi-agent deep deterministic policy gradient (MADDPG) algorithm to train 2 agents to play tennis with each other. The goal is to hit the ball to the other side as many times as possible, while avoiding hitting the ball to the ground or out of bounds. \nThe environment is originally from Unity Machine Learning Agents (Unity ML-Agents). For more details and other environments, please visit:\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8963187944542506
      ],
      "excerpt": "This project uses the environment provided by Udacity, which is slightly different from the original Unity environment. To run the codes in this repository successfully, Udacity's environment must be used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9380312261418368
      ],
      "excerpt": "In the environment, two agents (with blue and red rackets) are hitting a tennis ball back and forth to each other. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151525781747472
      ],
      "excerpt": "The action has 4 dimensions (2 for each agent). The action space is continuous, representing the horizontal and vertical movement of the player. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9348207990174126
      ],
      "excerpt": "If the agent hits the ball over the net, a reward of +0.1 is provided. If the ball hits the ground or out of bounds, a reward of -0.01 is provided. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9910189987134604
      ],
      "excerpt": "When an episode ends, we use the maximum of the scores of agent 0 and agent 1 as the score of the episode. The environment is considered solved when the average of the scores of 100 consecutive episodes reaches above +0.5. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Unity Machine Learning Agents (ML-Agents) Tennis Environment",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sliao-mi-luku/DeepRL-multiple-agents-tennis-udacity-drlnd-p3/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 00:46:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sliao-mi-luku/AI-Tennis-Players/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sliao-mi-luku/AI-Tennis-Players",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sliao-mi-luku/DeepRL-multiple-agents-tennis-udacity-drlnd-p3/master/SL_Tennis_MADDPG.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For information of the hyperparameters, please refer to `Report.md`\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8034661354415259
      ],
      "excerpt": "(figure) The tennis environment by Unity ML-Agents \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sliao-mi-luku/AI-Tennis-Players/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "AI Tennis Players",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "AI-Tennis-Players",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sliao-mi-luku",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sliao-mi-luku/AI-Tennis-Players/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please follow the steps below to train the agents or to watch pre-trained agents perform the task.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "``` cmd\ncd /path/to/the/p3_collab-compet/folder\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "``` cmd\njupyter notebook\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Paste the path to `Tennis.exe` after **\"file_name = \"**\n\nfor example, `file_name = \"./Tennis_Windows_x86_64/Tennis.exe\"`\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "For information of the hyperparameters, please refer to `Report.md`\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "A figure of the noise simulation will be displayed first, which can be used for tuning the parameters of the noise process. Please note that this simulation is independent of the noise process used in the maddpg traning, i.e., the exact noise values generated during training will be different from the values shown in the figure.\n\nAfter training, the weights and biases of the actor and critic networks will be saved with the file names:\\\n `checkpoint_actor_0.pth` and `checkpoint_critic_0.pth` for Agent 0, and\\\n `checkpoint_actor_1.pth` and `checkpoint_critic_1.pth` for Agent 1\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 00:46:02 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please follow the steps below to download all the necessary files and dependencies.\n\n1. Install Anaconda (with Python 3.x)\\\n    https://www.anaconda.com/products/individual\n    \n2. Create (if you haven't) a new environment with Python 3.6 by typing the following command in the Anaconda Prompt:\\\n    `conda create --name drlnd python=3.6`\n    \n3. Install (need only minimal install) `gym` by following the **Installation** section of the OpenAI Gym GitHub:\n    https://github.com/openai/gym#id5\n    \n4. Clone the repository from Udacity's drlnd GitHub\n    ``` console\n    git clone https://github.com/udacity/deep-reinforcement-learning.git\n    cd deep-reinforcement-learning/python\n    pip install .\n    ```\n    (For Windows) If the error \"Could not find a version that satisfies the requirement torch==0.4.0 (from unityagents==0.4.0)\" occurs, please refer to this thread:\\\n    https://github.com/udacity/deep-reinforcement-learning/issues/13\n  \n5. Download the Tennis Environment (Udacity's modified version)\\\n    Linux: https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip \\\n    Mac OSX: https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip \\\n    Windows (32-bit): https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip \\\n    Windows (64-bit): https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip\n    \n    Extract the .zip file and move the folder `Tennis_Windows_x86_64` (or `Tennis`, `Tennis_Linux`, `Tennis_Windows_x86`, depending on the operating system) into the folder `p3_collab-compet` from Step 4.\n\n6. Download all the files (see the table below) from this repository. Place all files in the folder `p3_collab-compet` from Step 4.\n\n    | File Name | Notes |\n    | ----------- | ----------- |\n    | SL_Tennis_MADDPG.ipynb | main code |\n    | maddpgAgent.py | ddpg agent class |\n    | networkModels.py | architectures of actor and critic |\n    | buffer.py | replay buffer |\n    | noiseModels.py | noise process |\n    | checkpoint_actor_0.pth | saved weights for Agent 0's actor |\n    | checkpoint_critic_0.pth | saved weights for Agent 0's critic |\n    | checkpoint_actor_1.pth | saved weights for Agent 1's actor |\n    | checkpoint_critic_1.pth | saved weights for Agent 1's critic |\n\n7. You're ready to run the code! Please see the next section.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "A figure of the noise simulation will be displayed first, which can be used for tuning the parameters of the noise process. Please note that this simulation is independent of the noise process used in the maddpg traning, i.e., the exact noise values generated during training will be different from the values shown in the figure.\n\nAfter training, the weights and biases of the actor and critic networks will be saved with the file names:\\\n `checkpoint_actor_0.pth` and `checkpoint_critic_0.pth` for Agent 0, and\\\n `checkpoint_actor_1.pth` and `checkpoint_critic_1.pth` for Agent 1\n",
      "technique": "Header extraction"
    }
  ]
}