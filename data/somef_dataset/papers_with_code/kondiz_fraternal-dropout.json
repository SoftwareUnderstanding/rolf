{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1711.00066",
      "https://arxiv.org/abs/1711.00066",
      "https://arxiv.org/abs/1711.00066"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kondiz/fraternal-dropout",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-11-03T15:00:03Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-03T02:19:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9814514805396292
      ],
      "excerpt": "If you want to replicate state-of-the-art results from Fraternal Dropout paper on Penn Treebank dataset (PTB) or WikiText-2 dataset (WT2) you have to apply fraternal dropout on the top of AWD-LSTM 3-layer architercure. It is more time-consuming (approximately one day for PTB and three days for WT2). The code with hyper-parameters used in the paper may be found in the other branches (PTB or WT2). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9896996406078833
      ],
      "excerpt": "to get the full list of all of them. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8495396633809243,
        0.9732164176805206,
        0.8569565688494749
      ],
      "excerpt": "use the same dropout mask for the RNN hidden to hidden matrix in both networks. That gives a little better results i.e. 66.9 / 64.6 (validation / testing). \nWith this example, it should be easy to apply fraternal dropout in any PyTorch model that uses dropout. However, this example incorporates additional options (like using the same dropout mask for a part of neural network or applying expectation-linear dropout model instead of fraternal dropout), and hence simpler example is provided below. \nIf you are interested in applying fraternal dropout without additional options (which are not important to achieve better results, they are implemented just to have a comparison) just a simple modification of your code should be enough. You will have to find and modify the lines of code that calculate the output and loss. In the simplest, typical case you should find something like that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9088678383147151
      ],
      "excerpt": "and replace with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9518602519746595
      ],
      "excerpt": "kappa_output = model(data) \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kondiz/fraternal-dropout/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Mon, 27 Dec 2021 22:29:34 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kondiz/fraternal-dropout/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kondiz/fraternal-dropout",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8327503506665598
      ],
      "excerpt": "These models do not support all options implemended for a single LSTM. You should simply run python main.py to start training AWD-LSTM 3-layer model with fraternal dropout. For fine-tuning just run python finetune.py --save PATH where PATH is the path to the model that should be fine-tuned (the model will be override, so make a copy if needed). The perplexities from the corresponding branches can be expected to be: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8289390932725463
      ],
      "excerpt": "These models do not support all options implemended for a single LSTM. You should simply run python main.py to start training AWD-LSTM 3-layer model with fraternal dropout. For fine-tuning just run python finetune.py --save PATH where PATH is the path to the model that should be fine-tuned (the model will be override, so make a copy if needed). The perplexities from the corresponding branches can be expected to be: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9244428231235091
      ],
      "excerpt": "python main.py --help \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9384952669529307
      ],
      "excerpt": "python main.py --model FD --same_mask_w \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8901065026153029
      ],
      "excerpt": "output = model(data) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8901065026153029,
        0.8248328724624491
      ],
      "excerpt": "output = model(data) \nkappa_output = model(data) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kondiz/fraternal-dropout/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2017, \\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Fraternal Dropout",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fraternal-dropout",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kondiz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kondiz/fraternal-dropout/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Python 3 and PyTorch 0.2.\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The easiest way to train FD model (baseline model enchanced by fraternal dropout with \u03ba=0.15) achiving perplexities of approximately `67.5` / `64.9` (validation / testing) is to run\n\n+ `python main.py --model FD`\n\nFor the comparison you can try\n\n+ `python main.py --model ELD`\nor\n+ `python main.py --model PM`\n\nto train expectation-linear dropout model (\u03ba=0.25) or \u03a0-model (\u03ba=0.15), respectively.\n\nIf you want to override default \u03ba value just use `--kappa`, for example\n\n+ `python main.py --model FD --kappa 0.1`\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 65,
      "date": "Mon, 27 Dec 2021 22:29:34 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository contains the code originally forked from the [AWD-LSTM Language Model](https://github.com/salesforce/awd-lstm-lm) that is simplified and modified to present the performance of [Fraternal Dropout](https://arxiv.org/abs/1711.00066) (FD).\n\nThe architecture used here is a single layer LSTM with [DropConnect (Wan et al. 2013)](https://cs.nyu.edu/~wanli/dropc/dropc.pdf) applied on the RNN hidden to hidden matrix. The same that is used in the ablation studies in the [Fraternal Dropout](https://arxiv.org/abs/1711.00066) paper.\nIncluded below are hyper-parameters to get equivalent results to those in the original paper for a single layer LSTM.\n\n",
      "technique": "Header extraction"
    }
  ]
}