{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1712.05884"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9968499335706166
      ],
      "excerpt": "In April 2017, Google published a paper, Tacotron: Towards End-to-End Speech Synthesis, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8962193280377287
      ],
      "excerpt": "After unpacking, your tree should look like this for LJ Speech: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    12.5 seconds. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keithito/tacotron",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-07-08T17:03:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T10:38:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8995872138656487
      ],
      "excerpt": "An implementation of Tacotron speech synthesis in TensorFlow. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9073311597970475,
        0.803947228464494,
        0.8959619856450499
      ],
      "excerpt": "@begeekmyfriend created a fork that adds location-sensitive attention and the stop token from the Tacotron 2 paper. This can greatly reduce the amount of data required to train a model. \nIn April 2017, Google published a paper, Tacotron: Towards End-to-End Speech Synthesis, \nwhere they present a neural text-to-speech model that learns to synthesize speech directly from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9687675909317199,
        0.8524799046651341
      ],
      "excerpt": "independent attempt to provide an open-source implementation of the model described in their paper. \nThe quality isn't as good as Google's demo yet, but hopefully it will get there someday :-). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815610590696234
      ],
      "excerpt": "The following are supported out of the box: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8983690214891055
      ],
      "excerpt": "or like this for Blizzard 2012: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.951232532257774
      ],
      "excerpt": "Preprocess the data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9758069237552234,
        0.8669707026592877
      ],
      "excerpt": "   The default hyperparameters are recommended for LJ Speech and other English-language data. \n   See TRAINING_DATA.md for other languages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230219055595791
      ],
      "excerpt": "    time to force a particular pronunciation, e.g. Turn left on {HH AW1 S S T AH0 N} Street. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401870412428636
      ],
      "excerpt": "Occasionally, you may see a spike in loss and the model will forget how to attend (the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.959283033879018
      ],
      "excerpt": "    save time to restart at a checkpoint prior to the spike by passing the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238972697809994,
        0.9447900621544552
      ],
      "excerpt": "During eval and training, audio length is limited to max_iters * outputs_per_step * frame_shift_ms \n    milliseconds. With the defaults (max_iters=200, outputs_per_step=5, frame_shift_ms=12.5), this is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9344783666529387
      ],
      "excerpt": "Here is the expected loss curve when training on LJ Speech with the default hyperparameters: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A TensorFlow implementation of Google's Tacotron speech synthesis with pre-trained model (unofficial)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keithito/tacotron/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 924,
      "date": "Sat, 25 Dec 2021 02:11:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/keithito/tacotron/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "keithito/tacotron",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install Python 3.\n\n2. Install the latest version of [TensorFlow](https://www.tensorflow.org/install/) for your platform. For better\n   performance, install with GPU support if it's available. This code works with TensorFlow 1.3 and later.\n\n3. Install requirements:\n   ```\n   pip install -r requirements.txt\n   ```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.89228001933378
      ],
      "excerpt": "Monitor with Tensorboard (optional) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8606408806253599
      ],
      "excerpt": "    can enable it by installing it and setting LD_PRELOAD=/usr/lib/libtcmalloc.so. With TCMalloc, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.88939845009673
      ],
      "excerpt": "You can train with CMUDict by downloading the \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.901103070905114,
        0.8018538835028992
      ],
      "excerpt": "   python3 demo_server.py --checkpoint /tmp/tacotron-20180906/model.ckpt \nPoint your browser at localhost:9000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8447634792014815
      ],
      "excerpt": "Download a speech dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152750849795331
      ],
      "excerpt": "   python3 preprocess.py --dataset ljspeech \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737873112808705,
        0.950563948951535
      ],
      "excerpt": "Train a model \n   python3 train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.873912578201294
      ],
      "excerpt": "   line using the --hparams flag, for example --hparams=\"batch_size=16,outputs_per_step=2\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8219800857611791,
        0.896708016310023
      ],
      "excerpt": "Synthesize from a checkpoint \n   python3 demo_server.py --checkpoint ~/tacotron/logs-tacotron/model.ckpt-185000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9193197715722139,
        0.9048361095016548,
        0.8266552325137427
      ],
      "excerpt": "   run eval.py at the command line: \n   python3 eval.py --checkpoint ~/tacotron/logs-tacotron/model.ckpt-185000 \n   If you set the --hparams flag when training, set the same value here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8337460726883364
      ],
      "excerpt": "    dictionary to ~/tacotron/training and then passing the flag --hparams=\"use_cmudict=True\" to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8417615239762735
      ],
      "excerpt": "    --restore_step=150000 flag to train.py (replacing 150000 with a step number prior to the \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/keithito/tacotron/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2017 Keith Ito\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tacotron",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "tacotron",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "keithito",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keithito/tacotron/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "keithito",
        "body": "- PR https://github.com/keithito/tacotron/pull/41 adds a TensorFlow implementation of Griffin-Lim.\r\n  - Audio can be generated at 7.5x real-time on a GTX 1080Ti.\r\n- PR https://github.com/keithito/tacotron/pull/40 makes the text processing pipeline more configurable\r\n  - This can make it easier to train on non-English data.\r\n  - See [TRAINING_DATA.md](https://github.com/keithito/tacotron/blob/master/TRAINING_DATA.md) for details.\r\n- TensorFlow 1.3 is now required.\r\n",
        "dateCreated": "2017-09-13T03:58:42Z",
        "datePublished": "2017-09-13T04:09:08Z",
        "html_url": "https://github.com/keithito/tacotron/releases/tag/v0.2.0",
        "name": "Faster synthesis, better support for non-English training data",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/keithito/tacotron/tarball/v0.2.0",
        "url": "https://api.github.com/repos/keithito/tacotron/releases/7732416",
        "zipball_url": "https://api.github.com/repos/keithito/tacotron/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "keithito",
        "body": "",
        "dateCreated": "2017-08-22T21:42:26Z",
        "datePublished": "2017-09-05T04:47:32Z",
        "html_url": "https://github.com/keithito/tacotron/releases/tag/v0.1.0",
        "name": "Initial version",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/keithito/tacotron/tarball/v0.1.0",
        "url": "https://api.github.com/repos/keithito/tacotron/releases/7632395",
        "zipball_url": "https://api.github.com/repos/keithito/tacotron/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install Python 3.\n\n2. Install the latest version of [TensorFlow](https://www.tensorflow.org/install/) for your platform. For better\n   performance, install with GPU support if it's available. This code works with TensorFlow 1.3 and later.\n\n3. Install requirements:\n   ```\n   pip install -r requirements.txt\n   ```\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2585,
      "date": "Sat, 25 Dec 2021 02:11:39 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tacotron",
      "tensorflow",
      "speech-synthesis",
      "python",
      "machine-learning",
      "tts"
    ],
    "technique": "GitHub API"
  }
}