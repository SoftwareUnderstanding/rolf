{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762\n    'use_positional_encoding': hp.choice('use_positional_encoding', [False, True]",
      "https://arxiv.org/abs/1808.05578"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For more information, see the paper's page on [arXiv](https://arxiv.org/abs/1808.05578).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The current project contains code derived from those other projects:\n\n- https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n- https://github.com/guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs\n- https://github.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100\n- https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100\n- https://github.com/harvardnlp/annotated-transformer\n\nMore information on which pieces of code comes from where in the headers of each Python files. All of those references are licensed under permissive open-source licenses, such as the MIT License and the Apache 2.0 License.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    'decay_each_N_epoch': hp.quniform('decay_each_N_epoch', 3 - 0.499, 10 + 0.499, 1), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187756947909643
      ],
      "excerpt": "    \"larnn_mode\": \"residual\", \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-05-03T03:46:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-17T09:42:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9707106636047486,
        0.9718696483270306,
        0.9708708080259112,
        0.977386251863634
      ],
      "excerpt": "A fixed-size, go-back-k recurrent attention module on an RNN so as to have linear short-term memory by the means of attention. The LARNN model can be easily used inside a loop on the cell state just like any other RNN. The cell state keeps the k last states for its multi-head attention mechanism. \nThe LARNN is derived from the Long Short-Term Memory (LSTM) cell. The LARNN introduces attention on the state's past values up to a certain range, limited by a time window k to keep the forward processing linear in time in terms sequence length (time steps). \nTherefore, multi-head attention with positional encoding is used on the most recent past values of the inner state cell so as to enable a better mid-term memory, such that at each new time steps, the cell looks back at it's own previous cell state values with an attention query. \nNote that the positional encoding is concatenated rather than added. Also, the ELU activation is used in the cell. There is also batch normalization at many places (not drawn). The Multi-Head Attention Mechanism uses an ELU activation rather than unactivated Linears, for the keys and values and the query. There is here only one query rather than many queries. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8536255793092389,
        0.8550300649961301,
        0.8550300649961301,
        0.9092379678132477
      ],
      "excerpt": "You can visually inspect the effect of every hyperparameter on the accuracy, and their correlated effect, by navigating at: \nResults for round 1 \nResults for round 2 \nYou could also copy and run one of those files on new results by simply changing the results folder in the jupyter-notebook such that your new folder is taken. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8145507995419725
      ],
      "excerpt": "    #: it vary exponentially, in a multiplicative fashion rather than in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8068478648290912
      ],
      "excerpt": "#: The dropout on the hidden unit on top of each LARNN cells \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8937011840990955
      ],
      "excerpt": "#: The number 'h' of attention heads: from 1 to 20 attention heads. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8562877418302668
      ],
      "excerpt": "#: How restricted is the attention back in time steps (across sequence) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9865875192633018
      ],
      "excerpt": "#: How the new attention is placed in the LSTM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9056809091381754
      ],
      "excerpt": "#: Wheter or not to use BN(ELU(.)) in the Linear() layers of the keys and values in the multi-head attention. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9132630537770209
      ],
      "excerpt": "The best results were found with those hyperparameters, for a test accuracy of 91.924%: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9840917539351939,
        0.9805213126094431,
        0.9833224798378436,
        0.9110514271586743,
        0.9909914871418491,
        0.8822671187001517,
        0.9837958784760646,
        0.9554451354880088,
        0.9733221007151069
      ],
      "excerpt": "Although the LARNN cell obtains better results than the LSTM cell as explored here, the LARNN is more complicated and hence the LSTM cell is still very interesting and probably of greater value. \nHowever, the LARNN would still have to be compared to a deeply stacked setup such as done here, where better results are obtained, but by using many more cells which means the current project could still perform better with more cells and proper regularization. \nIt seems that the positional encoding tried here is not helpful for the learning. \nSo overall, despite the LARNN not bringing huge improvements in accuracy, the most interesting thing about this project are:  \n- The code which is reusable and neat for being easily adaptable to automatically hyperoptimize on other datasets and networks.  \n- The discovery that adding an activation on the multi-head self-attention mechanism's keys, queries and values performed well in the context here, better than using no activation.  \n- To my best knowledge, a new neural attention data structure is created by using a queue for an attention mechanism, sliding through time, and this data structure could potentially be very interesting in many other applications where attention is required.  \n- The figures are reusable, published under CC-BY in the subfolder, while the code is published under the MIT License and also reusable.  \nThe current dataset is solveable with good accuracy without any attention mechanism. So the current project was more to code something interesting to than genuinely try to improve the accuracy on a small dataset. I coded this in 1 week so I couldn't use a very complicated dataset and rebuild a complete data pipeline - I had to reuse old code of mine that I already knew. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A recurrent attention module consisting of an LSTM cell which can query its own past cell states by the means of windowed multi-head attention. The formulas are derived from the BN-LSTM and the Transformer Network. The LARNN cell with attention can be easily used inside a loop on the cell state, just like any other RNN. (LARNN)",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\ncd data\npython3 download_dataset.py\ncd ..\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 32,
      "date": "Tue, 28 Dec 2021 00:39:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/master/AnalyzeTestHyperoptResults_round_1.ipynb",
      "https://raw.githubusercontent.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/master/AnnotatedMultiHeadAttention.ipynb",
      "https://raw.githubusercontent.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/master/AnalyzeTestHyperoptResults_round_2.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8244453191770256
      ],
      "excerpt": "    #: Note: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8666894911217105
      ],
      "excerpt": "You can re-train on the best hyperparameters found with this command: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "  <img src=\"inkscape_drawings/png_exported/larnn-cell.png\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801882466611699
      ],
      "excerpt": "This will launch a round of meta-optimisation which will save the results under a new ./results/ folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8120657020667222
      ],
      "excerpt": "    #: Number of loops on the whole train dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549190955787082
      ],
      "excerpt": "    #: Number of examples fed per training step \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8768801849289117
      ],
      "excerpt": "'use_positional_encoding': hp.choice('use_positional_encoding', [False, True]), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8768801849289117
      ],
      "excerpt": "'activation_on_keys_and_values': hp.choice('activation_on_keys_and_values', [False, True]), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8768801849289117
      ],
      "excerpt": "'is_stacked_residual': hp.choice('is_stacked_residual', [False, True]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    \"activation_on_keys_and_values\": true, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    \"is_stacked_residual\": true, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.838182596395936
      ],
      "excerpt": "python3 train.py --dataset UCIHAR --device cuda \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "python3 larnn.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Guillaume Chevalier\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "[LARNN: Linear Attention Recurrent Neural Network](https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Linear-Attention-Recurrent-Neural-Network",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "guillaume-chevalier",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 126,
      "date": "Tue, 28 Dec 2021 00:39:12 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "rnn",
      "lstm",
      "attention-mechanism",
      "attention-model",
      "attention-is-all-you-need",
      "recurrent-neural-networks",
      "pytorch"
    ],
    "technique": "GitHub API"
  }
}