{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1805.09300",
      "https://arxiv.org/abs/1812.01600",
      "https://arxiv.org/abs/1812.01600",
      "https://arxiv.org/abs/1812.01600",
      "https://arxiv.org/abs/1712.01802",
      "https://arxiv.org/abs/1806.06986",
      "https://arxiv.org/abs/1708.03979"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{najibi2019autofocus,\n  title={{AutoFocus}: Efficient Multi-Scale Inference},\n  author={Najibi, Mahyar and Singh, Bharat and Davis, Larry S},\n  journal={ICCV},\n  year={2019}\n}\n@article{sniper2018,\n  title={{SNIPER}: Efficient Multi-Scale Training},\n  author={Singh, Bharat and Najibi, Mahyar and Davis, Larry S},\n  journal={NeurIPS},\n  year={2018}\n}\n@article{analysissnip2017,\n  title={An analysis of scale invariance in object detection-snip},\n  author={Singh, Bharat and Davis, Larry S},\n  journal={CVPR},\n  year={2018}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{analysissnip2017,\n  title={An analysis of scale invariance in object detection-snip},\n  author={Singh, Bharat and Davis, Larry S},\n  journal={CVPR},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{sniper2018,\n  title={{SNIPER}: Efficient Multi-Scale Training},\n  author={Singh, Bharat and Najibi, Mahyar and Davis, Larry S},\n  journal={NeurIPS},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{najibi2019autofocus,\n  title={{AutoFocus}: Efficient Multi-Scale Inference},\n  author={Najibi, Mahyar and Singh, Bharat and Davis, Larry S},\n  journal={ICCV},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8715509475085587
      ],
      "excerpt": "<img src=\"https://mahyarnajibi.github.io/github_readme_data/sniper_object_detector.gif\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.984910345122585
      ],
      "excerpt": "SNIPER is initially described in the following paper published at NeurIPS 2018: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8592915573379081,
        0.9969347196970157,
        0.9772144729505301
      ],
      "excerpt": "<b>SNIPER: Efficient Multi-Scale Training \n<a href=https://github.com/bharatsingh430>Bharat Singh*</a>, <a href=https://github.com/mahyarnajibi>Mahyar Najibi*</a>, and Larry S. Davis (* denotes equal contribution)</b> \nNeurIPS, 2018. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9856185662657159
      ],
      "excerpt": "AutoFocus is initially described in the following paper published at ICCV 2019: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9969347196970157,
        0.9782759268835692
      ],
      "excerpt": "<a href=https://github.com/mahyarnajibi>Mahyar Najibi*</a>, <a href=https://github.com/bharatsingh430>Bharat Singh*</a>, and Larry S. Davis (* denotes equal contribution)</b> \nICCV, 2019. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130052599129608
      ],
      "excerpt": "| <sub>SNIPER</sub> | <sub>MobileNetV2</sub> | <sub>ImageNet</sub> | <sub>test-dev15</sub>| 34.3 |  54.4   | 37.9   | 18.5  | 36.9  | 46.4  | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mahyarnajibi/SNIPER",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-01-13T14:45:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T01:45:45Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9509070913884903
      ],
      "excerpt": "Instead of processing all pixels in an image pyramid, SNIPER selectively processes context regions around the ground-truth objects (a.k.a chips). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9986765200947173
      ],
      "excerpt": "AutoFocus, on the other hand, is an efficient multi-scale inference algorithm for deep-learning based object detectors. Instead of processing an entire image pyramid, AutoFocus adopts a coarse to fine approach and only processes regions that are likely to contain small objects at finer scales. This is achieved by predicting category agnostic segmentation maps for small objects at coarser scales, called FocusPixels. FocusPixels can be predicted with high recall, and in many cases, they only cover a small fraction of the entire image. To make efficient use of FocusPixels, an algorithm is proposed which generates compact rectangular FocusChips which enclose FocusPixels. The detector is while processing finer scales. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8699701130872385
      ],
      "excerpt": "AutoFocus is initially described in the following paper published at ICCV 2019: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9180208286274305
      ],
      "excerpt": "Train with a batch size of 160 images with a ResNet-101 backbone on 8 V100 GPUs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.973034345213272,
        0.8143429301781365,
        0.831202569151314
      ],
      "excerpt": "The R-FCN-3K branch is also powered by SNIPER. Now 21% better than YOLO-9000 on ImageNetDet. This branch also supports on-the-fly training (in seconds) with very few samples (no bounding boxes needed!) \nTrain on OpenImagesV4 (14x bigger than COCO) with ResNet-101 in 3 days on a p3.x16.large AWS instance! \nHere are the COCO results for SNIPER trained using this repository. The models are trained on the trainval set (using only the bounding box annotations) and evaluated on the test-dev set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.867271294038045
      ],
      "excerpt": "Training a model with SNIPER / AutoFocus \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8720868692836683
      ],
      "excerpt": "Other methods and branches in this repo (SSH Face Detector, R-FCN-3K, open-images) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": " data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": " data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9422407936422895
      ],
      "excerpt": "Negative chip mining results in a relative improvement in AP (please refer to the paper for the details). To determine the candidate hard negative regions, SNIPER uses proposals extracted from a proposal network trained for a short training schedule.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8496828607386846
      ],
      "excerpt": "However, it is also possible to extract the required proposals using this repository (e.g. if you plan to train SNIPER on a new dataset). We provided an all-in-one script that performs all the required steps for training SNIPER with Negative Chip Mining. Running the following script trains a proposal network for a short cycle (i.e. 2 epochs), extract the proposals, and train the SNIPER detector with Negative Chip Mining: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9203965990407541
      ],
      "excerpt": "It is also possible to set individual configuration key-values by passing --set as the last argument to the module  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9637773220750034
      ],
      "excerpt": " Also, multi-processing is used to process the data. For smaller amounts of memory, you may need to reduce the number of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9489189636953798
      ],
      "excerpt": "It is possible to modify the AutoFocus default hyper-parameters through the config file to control the speed-accuracy tradeoff. Please see the paper for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8405117421345518
      ],
      "excerpt": "(See the configs folder for examples). It is also possible to set individual configuration key-values by passing --set as the last argument to the module followed by the desired key-values (i.e. --set key1 value1 key2 value2 ...). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317375862007877
      ],
      "excerpt": "Evaluating a model trained with this repository \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9722818573537205
      ],
      "excerpt": "This repo also contains the R-FCN-3k detector.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9602809338040432
      ],
      "excerpt": "Please switch to the R-FCN-3k branch for specific instructions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9227500635485697
      ],
      "excerpt": "Please switch to the openimages2 branch for specific instructions. The detector on OpenImagesV4 was trained with Soft Sampling. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8521470232760635
      ],
      "excerpt": "The SSH face detector would be added to this repository soon. In the meanwhile, you can use the code available at the original SSH repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "SNIPER / AutoFocus is an efficient multi-scale object detection training / inference algorithm",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Running the following script downloads and extracts the pre-trained models into the default path (```data/pretrained_model```):\n```\nbash download_pretrained_models.sh\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MahyarNajibi/SNIPER/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 447,
      "date": "Thu, 23 Dec 2021 12:16:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mahyarnajibi/SNIPER/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mahyarnajibi/SNIPER",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/MahyarNajibi/SNIPER/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/MahyarNajibi/SNIPER/master/scripts/download_sniper_autofocus_detectors.sh",
      "https://raw.githubusercontent.com/MahyarNajibi/SNIPER/master/scripts/train_neg_props_and_sniper.sh",
      "https://raw.githubusercontent.com/MahyarNajibi/SNIPER/master/scripts/download_sniper_neg_props.sh",
      "https://raw.githubusercontent.com/MahyarNajibi/SNIPER/master/scripts/download_pretrained_models.sh",
      "https://raw.githubusercontent.com/MahyarNajibi/SNIPER/master/scripts/compile.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone the repository:\n```\ngit clone --recursive https://github.com/mahyarnajibi/SNIPER.git\n```\n\n2. Compile the provided MXNet fork in the repository. \n\nYou need to install *CUDA*, *CuDNN*, *OpenCV*, and *OpenBLAS*. These libraries are set to be used by default in the provided ```config.mk``` file in the ```SNIPER-mxnet``` repository. You can use the ```make``` command to build the MXNet library:\n```\ncd SNIPER-mxnet\nmake -j [NUM_OF_PROCESS] USE_CUDA_PATH=[PATH_TO_THE_CUDA_FOLDER]\n```\n\nIf you plan to train models on multiple GPUs, it is optional but recommended to install *NCCL* and build MXNet with the *NCCL* support as instructed below:\n```\nmake -j [NUM_OF_PROCESS] USE_CUDA_PATH=[PATH_TO_THE_CUDA_FOLDER] USE_NCCL=1 \n```\nIn this case, you may also need to set the ```USE_NCCL_PATH``` variable in the above command to point to your *NCCL* installation path.\n\nIf you need more information on how to compile MXNet please see [*here*](https://mxnet.incubator.apache.org/install/build_from_source.html).\n\n3. Compile the C++ files in the lib directory. The following script compiles them all:\n```\nbash scripts/compile.sh\n```\n\n4. Install the required python packages:\n```\npip install -r requirements.txt\n```\n\n<a name=\"demo\"> </a>\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9957715891595061
      ],
      "excerpt": "<a name=\"install\"> </a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8564041923489975
      ],
      "excerpt": "Please follow the official COCO dataset website to download the dataset. After downloading the dataset you should have the following directory structure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8727487047449518
      ],
      "excerpt": "   |--coco \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803281197902062
      ],
      "excerpt": "You can train the SNIPER detector with or without negative chip mining as described below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265261021501731,
        0.9465718491881494
      ],
      "excerpt": "For COCO and Pascal VOC datasets, we provide the pre-computed proposals. The following commands download the pre-computed proposals, extracts them into the default path (data/proposals), and trains the SNIPER detector with the default parameters on the COCO dataset: \nbash download_sniper_neg_props.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9271442284050626,
        0.9465718491881494
      ],
      "excerpt": "However, it is also possible to extract the required proposals using this repository (e.g. if you plan to train SNIPER on a new dataset). We provided an all-in-one script that performs all the required steps for training SNIPER with Negative Chip Mining. Running the following script trains a proposal network for a short cycle (i.e. 2 epochs), extract the proposals, and train the SNIPER detector with Negative Chip Mining: \nbash train_neg_props_and_sniper.sh --cfg [PATH_TO_CFG_FILE] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name=\"evaluating\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465718491881494
      ],
      "excerpt": "bash download_sniper_detector.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9672792453808401
      ],
      "excerpt": "To evaluate these models on COCO test-dev with the default configuration, you can run the following script: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8055034869144635
      ],
      "excerpt": "Train with a batch size of 160 images with a ResNet-101 backbone on 8 V100 GPUs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8727306547777878,
        0.8336852773234704
      ],
      "excerpt": "| <sub>SNIPER </sub>| <sub>ResNet-101</sub> | <sub>ImageNet</sub> | <sub>test-dev15</sub> | 46.5 | 67.5    |   52.2  | 30.0  | 49.4  | 58.4 |  \n| <sub>SNIPER</sub> |<sub>ResNet-101</sub>  | <sub>OpenImagesV4</sub> | <sub>test-dev15</sub>| 47.8 |  68.2   | 53.6   | 31.5  | 50.4  | 59.8  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.85281889016886,
        0.8333236702918404
      ],
      "excerpt": "Running the demo \nTraining a model with SNIPER / AutoFocus \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158985299102827
      ],
      "excerpt": "For training SNIPER/AutoFocus, you first need to download the pre-trained models and configure the datasets as described below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172695006605855
      ],
      "excerpt": "Please download the training, validation, and test subsets from the [official Pascal VOC dataset website (http://host.robots.ox.ac.uk/pascal/VOC/). After downloading the dataset you should have the following directory structure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8717315786475134
      ],
      "excerpt": "python main_train.py \nFor training on Pascal VOC with the provided pre-computed proposals, you can run python main_train.py --cfg configs/faster/sniper_res101_e2e_pascal_voc.yml. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9324470098559241,
        0.8077742692768469
      ],
      "excerpt": "python main_train.py --set TRAIN.USE_NEG_CHIPS False \nIn any case, the default training settings can be overwritten by passing a configuration file (see the configs folder for example configuration files). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8295353995986269
      ],
      "excerpt": "(by setting TRAIN.BATCH_IMAGES in the config file or passing --set TRAIN.BATCH_IMAGES [DISIRED_VALUE] as the last argument to the module). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name=\"evaluating\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python main_test.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9388653246905805
      ],
      "excerpt": "python main_test.py --cfg sniper_res101_e2e_mask_autofocus.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8507672862166588,
        0.8487077720883187
      ],
      "excerpt": "As an example, for evaluating the provided PASCAL VOC pre-trained model on the VOC 2007 test-set you can pass the provided PASCAL config file to the script: \npython main_test.py --cfg configs/faster/sniper_res101_e2e_pascal_voc.yml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8852827872319342,
        0.921749148686149,
        0.8421074476017179
      ],
      "excerpt": "The test settings can be set by updating the TEST section of the configuration file (See the configs folder for examples). \npython main_test.py --cfg [PATH TO THE CONFIG FILE USED FOR TRAINING] \n<a name=\"others\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8580415765090782
      ],
      "excerpt": "<img src=\"http://www.cs.umd.edu/~bharat/ss.jpg\" width=\"650px\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mahyarnajibi/SNIPER/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "Cython",
      "C",
      "C++",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/MahyarNajibi/SNIPER/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright 2018 Mahyar Najibi and Bharat Singh\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n   \\n\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n##############################################################################\\nTHIRD-PARTY SOFTWARE LICENSES\\n\\n1) MXNet\\nCopyright (c) 2015-2016 by Contributors\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n   http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n2) Fast R-CNN\\nCopyright (c) Microsoft Corporation\\nAll rights reserved.\\nMIT License\\nPermission is hereby granted, free of charge, to any person obtaining a\\ncopy of this software and associated documentation files (the \"Software\"),\\nto deal in the Software without restriction, including without limitation\\nthe rights to use, copy, modify, merge, publish, distribute, sublicense,\\nand/or sell copies of the Software, and to permit persons to whom the\\nSoftware is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included\\nin all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED AS IS, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\\nTHE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\\nOTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\\nOTHER DEALINGS IN THE SOFTWARE.\\n\\n3)Faster R-CNN\\nThe MIT License (MIT)\\nCopyright (c) 2015 Microsoft Corporation\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n\\n4) MS COCO API\\nCopyright (c) 2014, Piotr Dollar and Tsung-Yi Lin\\nAll rights reserved.\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met: \\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer. \\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution. \\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\nThe views and conclusions contained in the software and documentation are those\\nof the authors and should not be interpreted as representing official policies, \\neither expressed or implied, of the FreeBSD Project.\\n\\n5) Deformable-ConvNets\\nCopyright Microsoft Corporation\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\nApache License, Version 2.0\\nCopyright (c) by respective contributors\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n    http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SNIPER / AutoFocus: Efficient Multi-Scale Training / Inference",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SNIPER",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mahyarnajibi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mahyarnajibi/SNIPER/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<p align=\"center\">\n<img src=\"https://mahyarnajibi.github.io/github_readme_data/sniper_object_detector_detections.jpg\" width=\"700px\"/>\n</p>\n\nFor running the demo, you need to download the provided SNIPER models. The following script downloads SNIPER models and extracts them into the default location:\n```\nbash download_sniper_autofocus_detectors.sh\n```\nAfter downloading the model, the following command would run the SNIPER detector trained on the COCO dataset with the default configs on the provided sample image:\n```\npython demo.py\n```\nIf everything goes well, the sample detections would be saved as ```data/demo/demo_detections.jpg```.\n\nYou can also run the detector on an arbitrary image by providing its path to the script:\n```\npython demo.py --im_path [PATH to the image]\n```\nHowever, if you plan to run the detector on multiple images, please consider using the provided multi-process and multi-batch ```main_test``` module. \n\nYou can also test the provided SNIPER model based on the ```MobileNetV2``` architecture trained on the COCO dataset by passing the provided config file as follows:\n```\npython demo.py --cfg configs/faster/sniper_mobilenetv2_e2e.yml\n```\n\n<a name=\"training\"></a>\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2655,
      "date": "Thu, 23 Dec 2021 12:16:12 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<p align=\"center\">\n<img src=\"https://mahyarnajibi.github.io/github_readme_data/sniper_object_detector_detections.jpg\" width=\"700px\"/>\n</p>\n\nFor running the demo, you need to download the provided SNIPER models. The following script downloads SNIPER models and extracts them into the default location:\n```\nbash download_sniper_autofocus_detectors.sh\n```\nAfter downloading the model, the following command would run the SNIPER detector trained on the COCO dataset with the default configs on the provided sample image:\n```\npython demo.py\n```\nIf everything goes well, the sample detections would be saved as ```data/demo/demo_detections.jpg```.\n\nYou can also run the detector on an arbitrary image by providing its path to the script:\n```\npython demo.py --im_path [PATH to the image]\n```\nHowever, if you plan to run the detector on multiple images, please consider using the provided multi-process and multi-batch ```main_test``` module. \n\nYou can also test the provided SNIPER model based on the ```MobileNetV2``` architecture trained on the COCO dataset by passing the provided config file as follows:\n```\npython demo.py --cfg configs/faster/sniper_mobilenetv2_e2e.yml\n```\n\n<a name=\"training\"></a>\n",
      "technique": "Header extraction"
    }
  ]
}