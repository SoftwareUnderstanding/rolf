{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<a href= https://arxiv.org/pdf/1608.03983.pdf> *SGDR: Stochastic Gradient Descent with Warm Restarts* </a>\n\n\"\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bluesky314/Cyclical_LR_Scheduler_With_Decay_Pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-06T16:27:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-07T10:34:12Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9508775903098011
      ],
      "excerpt": "Reach multiple minimas to create a powerful ensemble or just to find the best one using Cyclical Learning Rates with Decay. Ideally decay milestones should intersect with cyclical milestones for smooth transition as shown below. Can be used with any optimizer such as Adam. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Cyclical_LR_Scheduler_With_Decay_Pytorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bluesky314/Cyclical_LR_Scheduler_With_Decay_Pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Thu, 30 Dec 2021 11:04:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bluesky314/Cyclical_LR_Scheduler_With_Decay_Pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "bluesky314/Cyclical_LR_Scheduler_With_Decay_Pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8610983578639211
      ],
      "excerpt": "Adapted from: https://github.com/Harshvardhan1/cyclic-learning-schedulers-pytorch \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bluesky314/Cyclical_LR_Scheduler_With_Decay_Pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Cyclical Learning Rate Scheduler With Decay in Pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Cyclical_LR_Scheduler_With_Decay_Pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "bluesky314",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bluesky314/Cyclical_LR_Scheduler_With_Decay_Pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- numpy \n- python >= 2.7\n- PyTorch >= 0.4.0\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 31,
      "date": "Thu, 30 Dec 2021 11:04:01 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Sample - (follow similarly for CyclicLinearLR)\nmilestones specifies when learning rate should shoot back up and decay_milestones when learning rate should be decayed.\n```\nfrom cyclicLR import CyclicCosAnnealingLR\nimport torch\n\noptimizer = torch.optim.SGD(model.parameters(),lr=1e-3)\nscheduler = CyclicCosAnnealingLR(optimizer,milestones=[10,25,60,80,120,180,240,320,400,480],decay_milestones=[60, 120, 240, 480, 960],eta_min=1e-6)\nfor epoch in range(500):\n  scheduler.step()\n  train(..)\n  validate(..)\n```\n>Note: scheduler.step() shown is called at every epoch. It can be called even in every batch. Remember to specify milestones in number of batches (and not number of epochs) in such as case. For only cyclical lr with no decay, do not pass a decay list. eta_min is the minimum lr it will go to and continue on that once cyclical shedule is over which is by default 1e-6.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}