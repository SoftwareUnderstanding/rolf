{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Thank you to [Daniel Bolya](https://github.com/dbolya/) for his fork of [YOLACT & YOLACT++](https://github.com/dbolya/yolact), which is an exellent work for real-time instance segmentation.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1911.08287",
      "https://arxiv.org/abs/2005.03572"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@Article{zheng2021ciou,\n  author    = {Zheng, Zhaohui and Wang, Ping and Ren, Dongwei and Liu, Wei and Ye, Rongguang and Hu, Qinghua and Zuo, Wangmeng},\n  title     = {Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation},\n  booktitle = {IEEE Transactions on Cybernetics},\n  year      = {2021},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@Inproceedings{zheng2020diou,\n  author    = {Zheng, Zhaohui and Wang, Ping and Liu, Wei and Li, Jinze and Ye, Rongguang and Ren, Dongwei},\n  title     = {Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression},\n  booktitle = {The AAAI Conference on Artificial Intelligence (AAAI)},\n  year      = {2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8565072005093729
      ],
      "excerpt": " - Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "YOLO v3 https://github.com/Zzh-tju/DIoU-darknet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9422100272984922
      ],
      "excerpt": "Simulation Experiment https://github.com/Zzh-tju/DIoU \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321597499656605,
        0.9720529500681988
      ],
      "excerpt": "| 550  | Resnet101-FPN | SL1  | Fast NMS | 30.6 | 31.5 | 29.1 |SL1.pth  |  \n| 550  | Resnet101-FPN | CIoU | Fast NMS | 30.6 | 32.1 | 29.6 | CIoU.pth |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968156676108885
      ],
      "excerpt": "| 550  | Resnet101-FPN | CIoU  |                 Fast NMS               |30.6|  32.1  |  33.9  |  43.0  |  29.6  |  30.9  |  40.3  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014,
        0.9278824608274014,
        0.9278824608274014
      ],
      "excerpt": "| 550  | Resnet101-FPN | CIoU  |             SPM Cluster-NMS            |  28.6  |  33.1  |  35.2  |  48.8  |30.3|31.7|  43.6  | \n| 550  | Resnet101-FPN | CIoU  |       SPM + Distance Cluster-NMS       |  27.1  |  33.2  |  35.2  |49.2|  30.2  |31.7|43.8| \n| 550  | Resnet101-FPN | CIoU  | SPM + Distance + Weighted Cluster-NMS  |  26.5  |33.4|35.5|  49.1  |30.3|  31.6  |43.8| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702105236373249,
        0.9811934373974328,
        0.9702105236373249,
        0.9278824608274014,
        0.8955886365383559
      ],
      "excerpt": "| 550  | Resnet50-FPN | SL1  |                 Fast NMS               |41.6|  30.2  |  31.9  |  42.0  |  28.0  |  29.1  |  39.4  | \n| 550  | Resnet50-FPN | SL1  |               Original NMS             |  12.8  |  30.7  |  32.0  |  44.1  |  28.1  |  29.2  |  40.7  | \n| 550  | Resnet50-FPN | SL1  |               Cluster-NMS              |  38.2  |  30.7  |  32.0  |  44.1  |  28.1  |  29.2  |  40.7  | \n| 550  | Resnet50-FPN | SL1  |             SPM Cluster-NMS            |  37.7  |  31.3  |  33.2  |  48.0  |28.8|29.9|  42.8  | \n| 550  | Resnet50-FPN | SL1  |       SPM + Distance Cluster-NMS       |  35.2  |  31.3  |  33.3  |  48.2  |  28.7  |29.9|  42.9  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9811934373974328
      ],
      "excerpt": "| 550  | Resnet101-FPN | SL1  |                 Fast NMS               |30.6|  32.5  |  34.6  |  43.9  |  29.8  |  31.3  |  40.8  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8528760958819365,
        0.9702105236373249,
        0.9115465676107721,
        0.8906174419333412
      ],
      "excerpt": "| 550  | Resnet101-FPN | SL1  |               Cluster-NMS              |  29.2  |  32.9  |  34.8  |  45.9  |  29.9  |  31.4  |  42.1  | \n| 550  | Resnet101-FPN | SL1  |             SPM Cluster-NMS            |  28.8  |  33.5  |  35.9  |  49.7  |30.5|32.1|  44.1  | \n| 550  | Resnet101-FPN | SL1  |       SPM + Distance Cluster-NMS       |  27.5  |  33.5  |  35.9  |50.2|  30.4  |  32.0  |44.3| \n| 550  | Resnet101-FPN | SL1  | SPM + Distance + Weighted Cluster-NMS  |  26.7  |34.0|36.6|  49.9  |30.5|  32.0  |44.3| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042750916398984
      ],
      "excerpt": "| 550  | Resnet101-FPN | SL1  |               Original NMS             |  10.9  |  36.4  |  39.1  |  48.0  |  34.7  |  37.1  |  44.1  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "| 550  | Resnet101-FPN | SL1  |             SPM Cluster-NMS            |  23.2  |  36.9  |  40.1  |  52.8  |35.0|  37.5  |46.3| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9799411683948944
      ],
      "excerpt": "| 550  | Resnet101-FPN | SL1  | SPM + Distance + Weighted Cluster-NMS  |  21.7  |37.4|40.6|  52.5  |35.0|37.6|46.3| \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Zzh-tju/CIoU",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-03T18:34:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T09:05:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "An example diagram of our Cluster-NMS, where X denotes IoU matrix which is calculated by `X=jaccard(boxes,boxes).triu_(diagonal=1) > nms_thresh` after sorted by score descending. (Here use 0,1 for visualization.)\n\n<img src=\"cluster-nms01.png\" width=\"1150px\"/>\n<img src=\"cluster-nms02.png\" width=\"1150px\"/>\n\nThe inputs of NMS are `boxes` with size [n,4] and `scores` with size [80,n]. (take coco as example)\n\nThere are two ways for NMS. One is that all classes have the same number of boxes. First, we use top k=200 to select the top 200 detections for every class. Then `boxes` will be [80,m,4], where m<=200. Do Cluster-NMS and keep the boxes with `scores>0.01`. Finally, return top 100 boxes across all classes.\n\nThe other approach is that different classes have different numbers of boxes. First, we use a score threshold (e.g. 0.01) to filter out most low score detection boxes. It results in the number of remaining boxes in different classes may be different. Then put all the boxes together and sorted by score descending. (Note that the same box may appear more than once, because its scores of multiple classes are greater than the threshold 0.01.) Adding offset for all the `boxes` according to their class labels. (use `torch.arange(0,80)`.) For example, since the coordinates (x1,y1,x2,y2) of all the boxes are on interval (0,1). By adding offset, if a box belongs to class 61, its coordinates will on interval (60,61). After that, the IoU of boxes belonging to different classes will be 0. (because they are treated as different clusters.) Do Cluster-NMS and return top 100 boxes across all classes. (For this method, please refer to another our repository https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/detection/detection.py)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.994780294459279
      ],
      "excerpt": "This is the code for our papers: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857899008282228,
        0.808274215371622
      ],
      "excerpt": " - Enhancing Geometric Factors into Model Learning and Inference for Object Detection and Instance Segmentation \nYOLACT (See YOLACT) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9831524629951867,
        0.9333891183249508
      ],
      "excerpt": "DIoU and CIoU losses are incorporated into state-of-the-art detection algorithms, including YOLO v3, SSD and Faster R-CNN.  \nThe details of implementation and comparison can be respectively found in the following links.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361532747829203
      ],
      "excerpt": "Please take a look at ciou function of layers/modules/multibox_loss.py for our CIoU loss implementation in PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9305167065181886,
        0.8765514045588882
      ],
      "excerpt": "Currently, NMS supports fast_nms, cluster_nms, cluster_diounms, spm, spm_dist, spm_dist_weighted.  \nSee layers/functions/detection.py for our Cluster-NMS implementation in PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819449831720002
      ],
      "excerpt": "The training is carried on two GTX 1080 Ti with command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8721444281139398
      ],
      "excerpt": "The following table is evaluated by using their pretrained weight of YOLACT. (yolact_resnet50_54_800000.pth) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8721444281139398
      ],
      "excerpt": "The following table is evaluated by using their pretrained weight of YOLACT. (yolact_base_54_800000.pth) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8721444281139398
      ],
      "excerpt": "The following table is evaluated by using their pretrained weight of YOLACT++. (yolact_plus_base_54_800000.pth) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.975403205709423,
        0.937830765792705,
        0.8687022904932624
      ],
      "excerpt": "Things we did but did not appear in the paper: SPM + Distance + Weighted Cluster-NMS. Here the box coordinate weighted average is only performed in IoU&gt; 0.8. We searched that IoU&gt;0.5 is not good for YOLACT and IoU&gt;0.9 is almost same to SPM + Distance Cluster-NMS. (Refer to CAD for the details of Weighted-NMS.) \nThe Original NMS implemented by YOLACT is faster than ours, because they firstly use a score threshold (0.05) to get the set of candidate boxes, then do NMS will be faster (taking YOLACT ResNet101-FPN as example, 22 ~ 23 FPS with a slight performance drop). In order to get the same result with our Cluster-NMS, we modify the process of Original NMS. \nNote that Torchvision NMS has the fastest speed, that is owing to CUDA implementation and engineering accelerations (like upper triangular IoU matrix only). However, our Cluster-NMS requires less iterations for NMS and can also be further accelerated by adopting engineering tricks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8652955379411984
      ],
      "excerpt": "Torchvision NMS is a function in Torchvision>=0.3, and our Cluster-NMS can be applied to any projects that use low version of Torchvision and other deep learning frameworks as long as it can do matrix operations. No other import, no need to compile, less iteration, fully GPU-accelerated and better performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8053652264294214
      ],
      "excerpt": ": Process an image and save it to another file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811398198421458
      ],
      "excerpt": " - To train, grab an imagenet-pretrained model and put it in ./weights. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9305844754386702
      ],
      "excerpt": ": Trains using the base config with a batch size of 8 (the default). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9619635814644867
      ],
      "excerpt": ": Trains yolact_base_config with a batch_size of 5. For the 550px models, 1 batch takes up around 1.5 gigs of VRAM, so specify accordingly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Complete-IoU (CIoU) Loss and Cluster-NMS for Object Detection and Instance Segmentation (YOLACT)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Zzh-tju/CIoU/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 32,
      "date": "Sat, 25 Dec 2021 06:07:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Zzh-tju/CIoU/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Zzh-tju/CIoU",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Zzh-tju/CIoU/master/scripts/train.sh",
      "https://raw.githubusercontent.com/Zzh-tju/CIoU/master/scripts/eval.sh",
      "https://raw.githubusercontent.com/Zzh-tju/CIoU/master/scripts/resume.sh",
      "https://raw.githubusercontent.com/Zzh-tju/CIoU/master/data/scripts/COCO_test.sh",
      "https://raw.githubusercontent.com/Zzh-tju/CIoU/master/data/scripts/COCO.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to use YOLACT++, make sure you compile the DCNv2 code.\n\n - Clone this repository and enter it:\n   ```Shell\n   git clone https://github.com/Zzh-tju/CIoU.git\n   cd yolact\n   ```\n - Set up the environment using one of the following methods:\n   - Using [Anaconda](https://www.anaconda.com/distribution/)\n     - Run `conda env create -f environment.yml`\n   - Manually with pip\n     - Set up a Python3 environment (e.g., using virtenv).\n     - Install [Pytorch](http://pytorch.org/) 1.0.1 (or higher) and TorchVision.\n     - Install some other packages:\n       ```Shell\n       #: Cython needs to be installed before pycocotools\n       pip install cython\n       pip install opencv-python pillow pycocotools matplotlib \n       ```\n - If you'd like to train YOLACT, download the COCO dataset and the 2014/2017 annotations. Note that this script will take a while and dump 21gb of files into `./data/coco`.\n   ```Shell\n   sh data/scripts/COCO.sh\n   ```\n - If you'd like to evaluate YOLACT on `test-dev`, download `test-dev` with this script.\n   ```Shell\n   sh data/scripts/COCO_test.sh\n   ```\n - If you want to use YOLACT++, compile deformable convolutional layers (from [DCNv2](https://github.com/CharlesShang/DCNv2/tree/pytorch_1.0)).\n   Make sure you have the latest CUDA toolkit installed from [NVidia's Website](https://developer.nvidia.com/cuda-toolkit).\n   ```Shell\n   cd external/DCNv2\n   python setup.py build develop\n   ```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9667326416418838
      ],
      "excerpt": "YOLOv3-pytorch https://github.com/Zzh-tju/ultralytics-YOLOv3-Cluster-NMS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9725002728977397
      ],
      "excerpt": "SSD-pytorch https://github.com/Zzh-tju/DIoU-SSD-pytorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406,
        0.9667326416418838,
        0.8404049025955376
      ],
      "excerpt": "YOLO v3 https://github.com/Zzh-tju/DIoU-darknet \nSSD https://github.com/Zzh-tju/DIoU-SSD-pytorch \nFaster R-CNN https://github.com/Zzh-tju/DIoU-pytorch-detectron \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068505665992845
      ],
      "excerpt": ": To output a coco json file for test-dev, make sure you have test-dev downloaded from above and go \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9690363989351758
      ],
      "excerpt": "By default, we train on COCO. Make sure to download the entire dataset using the commands above. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8009423159042363
      ],
      "excerpt": "Currently, NMS surports two modes: (See eval.py) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399222251557793
      ],
      "excerpt": "Per-class mode. (cross_class_nms=False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9294449210756155
      ],
      "excerpt": "python train.py --config=yolact_base_config --batch_size=8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8412335001788804
      ],
      "excerpt": "To evalute the model, put the corresponding weights file in the ./weights directory and run one of the following commands. The name of each config is everything before the numbers in the file name (e.g., yolact_base for yolact_base_54_800000.pth). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8379980507828566
      ],
      "excerpt": ": Output a COCOEval json to submit to the website or to use the run_coco_eval.py script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924383356538079
      ],
      "excerpt": "python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python run_coco_eval.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908339001275627
      ],
      "excerpt": "python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json --dataset=coco2017_testdev_dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9174520697939065
      ],
      "excerpt": "python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --display \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8785361714933823
      ],
      "excerpt": "python eval.py --trained_model=weights/yolact_base_54_800000.pth --benchmark \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8226895192127409
      ],
      "excerpt": "| 550  | Resnet101-FPN | SL1  |               Cluster-NMS              |  29.2  |  32.9  |  34.8  |  45.9  |  29.9  |  31.4  |  42.1  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158220446078586
      ],
      "excerpt": "| 550  | Resnet101-FPN | SL1  |                 Fast NMS               |25.1|  35.8  |  38.7  |  45.5  |  34.4  |  36.8  |  42.6  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9176396801197213
      ],
      "excerpt": "python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --ima \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.913778027454216
      ],
      "excerpt": "python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=input_image.png:output_image.png \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9505019341436516
      ],
      "excerpt": "python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --images=path/to/input/folder:path/to/output/folder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.886724042592196
      ],
      "excerpt": "python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=my_video.mp4 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.886724042592196
      ],
      "excerpt": "python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.886724042592196
      ],
      "excerpt": "python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=input_video.mp4:output_video.mp4 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8942789510331012
      ],
      "excerpt": "python eval.py --help \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8552792018425563,
        0.8610204993948013,
        0.8610204993948013,
        0.8610204993948013
      ],
      "excerpt": " - To train, grab an imagenet-pretrained model and put it in ./weights. \n   - For Resnet101, download resnet101_reducedfc.pth from here. \n   - For Resnet50, download resnet50-19c8e357.pth from here. \n   - For Darknet53, download darknet53.pth from here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9317135315960992
      ],
      "excerpt": "python train.py --config=yolact_base_config \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9294449210756155,
        0.8738075488522679,
        0.9226275707486514
      ],
      "excerpt": "python train.py --config=yolact_base_config --batch_size=5 \n: Resume training yolact_base with a specific weight file and start from the iteration specified in the weight file's name. \npython train.py --config=yolact_base_config --resume=weights/yolact_base_10_32100.pth --start_iter=-1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9355986061254542
      ],
      "excerpt": "python train.py --help \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Zzh-tju/CIoU/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "JavaScript",
      "Shell",
      "CSS",
      "HTML",
      "Cython"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "GNU General Public License v3.0",
      "url": "https://api.github.com/licenses/gpl-3.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2019, Charles Shang\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its\\n   contributors may be used to endorse or promote products derived from\\n   this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Complete-IoU Loss and Cluster-NMS for Improving Object Detection and Instance Segmentation.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CIoU",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Zzh-tju",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Zzh-tju/CIoU/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 227,
      "date": "Sat, 25 Dec 2021 06:07:59 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "YOLACT now supports multiple GPUs seamlessly during training:\n\n - Before running any of the scripts, run: `export CUDA_VISIBLE_DEVICES=[gpus]`\n   - Where you should replace [gpus] with a comma separated list of the index of each GPU you want to use (e.g., 0,1,2,3).\n   - You should still do this if only using 1 GPU.\n   - You can check the indices of your GPUs with `nvidia-smi`.\n - Then, simply set the batch size to `8*num_gpus` with the training commands above. The training script will automatically scale the hyperparameters to the right values.\n   - If you have memory to spare you can increase the batch size further, but keep it a multiple of the number of GPUs you're using.\n   - If you want to allocate the images per GPU specific for different GPUs, you can use `--batch_alloc=[alloc]` where [alloc] is a comma seprated list containing the number of images on each GPU. This must sum to `batch_size`.\n   \n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "yolact",
      "cluster-nms",
      "ciou",
      "diou",
      "object-detection",
      "instance-segmentation"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our paper is accepted by **IEEE Transactions on Cybernetics (TCYB)**.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "An example diagram of our Cluster-NMS, where X denotes IoU matrix which is calculated by `X=jaccard(boxes,boxes).triu_(diagonal=1) > nms_thresh` after sorted by score descending. (Here use 0,1 for visualization.)\n\n<img src=\"cluster-nms01.png\" width=\"1150px\"/>\n<img src=\"cluster-nms02.png\" width=\"1150px\"/>\n\nThe inputs of NMS are `boxes` with size [n,4] and `scores` with size [80,n]. (take coco as example)\n\nThere are two ways for NMS. One is that all classes have the same number of boxes. First, we use top k=200 to select the top 200 detections for every class. Then `boxes` will be [80,m,4], where m<=200. Do Cluster-NMS and keep the boxes with `scores>0.01`. Finally, return top 100 boxes across all classes.\n\nThe other approach is that different classes have different numbers of boxes. First, we use a score threshold (e.g. 0.01) to filter out most low score detection boxes. It results in the number of remaining boxes in different classes may be different. Then put all the boxes together and sorted by score descending. (Note that the same box may appear more than once, because its scores of multiple classes are greater than the threshold 0.01.) Adding offset for all the `boxes` according to their class labels. (use `torch.arange(0,80)`.) For example, since the coordinates (x1,y1,x2,y2) of all the boxes are on interval (0,1). By adding offset, if a box belongs to class 61, its coordinates will on interval (60,61). After that, the IoU of boxes belonging to different classes will be 0. (because they are treated as different clusters.) Do Cluster-NMS and return top 100 boxes across all classes. (For this method, please refer to another our repository https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/detection/detection.py)\n\n",
      "technique": "Header extraction"
    }
  ]
}