{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The OpenAI baselines [Tensorflow implementation](https://github.com/openai/baselines/tree/master/baselines/ddpg) and Ilya Kostrikov's [Pytorch implementation](https://github.com/ikostrikov/pytorch-ddpg-naf) of DDPG were used as references. After the majority of this codebase was complete, OpenAI released their [code](https://github.com/openai/maddpg) for MADDPG, and I made some tweaks to this repo to reflect some of the details in their implementation (e.g. gradient norm clipping and policy regularization).",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.02275"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9947470274942327
      ],
      "excerpt": "Cooperative-Competitive Environments (Lowe et. al. 2017) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shariqiqbal2810/maddpg-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-01-25T03:01:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T23:42:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9126632322316478
      ],
      "excerpt": "PyTorch Implementation of MADDPG from Multi-Agent Actor-Critic for Mixed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939627955342162
      ],
      "excerpt": "In this task, the two blue agents are rewarded by minimizing the closest of their distances to the green landmark (only one needs to be close to get optimal reward), while maximizing the distance of the red adversary from the green landmark. The red adversary is rewarded by minimizing it's distance to the green landmark; however, on any given trial, it does not know which landmark is green, so it must follow the blue agents. As such, the blue agents should learn to deceive the red agent by covering both landmarks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9660042122512603
      ],
      "excerpt": "This task involves two agents, one that is stationary and one that can move. The stationary agent sees the color of the other agent as its observation, and outputs a one-hot communication vector as its action. The moving agent receives the communication vector, as well as its relative distance to all landmarks on the screen; however, it does not know its own color. The goal of both agents is for the moving agent to reach the landmark that matches its own color. Thus, the agents must learn to communicate such that the moving agent knows where to go on each randomized trial. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9928229588047421,
        0.8526729964526365
      ],
      "excerpt": "This task involves a single prey agent (in green) and a team of three predators (in red). The prey agent is 30% faster than the predators, so the predators must learn how to team up in order to catch the prey. \nIn the trials below, the prey agent uses DDPG as its learning algorithm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch Implementation of MADDPG (Lowe et. al. 2017)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shariqiqbal2810/maddpg-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 77,
      "date": "Fri, 24 Dec 2021 20:46:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/shariqiqbal2810/maddpg-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "shariqiqbal2810/maddpg-pytorch",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.9138013295881398
      ],
      "excerpt": "<img src=\"assets/physical_deception/1.gif?raw=true\" width=\"33%\"> <img src=\"assets/physical_deception/2.gif?raw=true\" width=\"33%\"> <img src=\"assets/physical_deception/3.gif?raw=true\" width=\"33%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9138013295881398
      ],
      "excerpt": "<img src=\"assets/cooperative_communication/1.gif?raw=true\" width=\"33%\"> <img src=\"assets/cooperative_communication/2.gif?raw=true\" width=\"33%\"> <img src=\"assets/cooperative_communication/3.gif?raw=true\" width=\"33%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9138013295881398
      ],
      "excerpt": "<img src=\"assets/predator_prey/1.gif?raw=true\" width=\"33%\"> <img src=\"assets/predator_prey/2.gif?raw=true\" width=\"33%\"> <img src=\"assets/predator_prey/3.gif?raw=true\" width=\"33%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Ensemble Training \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/shariqiqbal2810/maddpg-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Shariq Iqbal\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "MADDPG-PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "maddpg-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "shariqiqbal2810",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shariqiqbal2810/maddpg-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [OpenAI baselines](https://github.com/openai/baselines), commit hash: 98257ef8c9bd23a24a330731ae54ed086d9ce4a7\n* My [fork](https://github.com/shariqiqbal2810/multiagent-particle-envs) of Multi-agent Particle Environments\n* [PyTorch](http://pytorch.org/), version: 0.3.0.post4\n* [OpenAI Gym](https://github.com/openai/gym), version: 0.9.4\n* [Tensorboard](https://github.com/tensorflow/tensorboard), version: 0.4.0rc3 and [Tensorboard-Pytorch](https://github.com/lanpa/tensorboard-pytorch), version: 1.0 (for logging)\n\nThe versions are just what I used and not necessarily strict requirements.\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All training code is contained within `main.py`. To view options simply run:\n\n```\npython main.py --help\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 245,
      "date": "Fri, 24 Dec 2021 20:46:07 GMT"
    },
    "technique": "GitHub API"
  }
}