{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Code is modified by [PyTorch-CycleGAN](https://github.com/aitorzip/PyTorch-CycleGAN). All credit goes to the authors of [CycleGAN](https://arxiv.org/abs/1703.10593), Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.\n=======\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.10593",
      "https://arxiv.org/abs/1703.10593"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9805712299079478
      ],
      "excerpt": "A clean and readable Pytorch implementation of CycleGAN (https://arxiv.org/abs/1703.10593) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "30% (Assistant)  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sweet-rytsar/cvfve_hw1",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-04T08:03:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-04T15:20:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9508539780503613
      ],
      "excerpt": "20% (Compare with other method) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "homework1-color-transfer",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nmkdir datasets\nbash ./download_dataset.sh <dataset_name>\n```\nValid <dataset_name> are: apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, cezanne2photo, ukiyoe2photo, vangogh2photo, maps, cityscapes, facades, iphone2dslr_flower, ae_photos\n\nAlternatively you can build your own dataset by setting up the following directory structure:\n\n    .\n    \u251c\u2500\u2500 datasets                   \n    |   \u251c\u2500\u2500 <dataset_name>         ",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sweet-rytsar/cvfve_hw1/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 18:03:37 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sweet-rytsar/cvfve_hw1/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sweet-rytsar/cvfve_hw1",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sweet-rytsar/cvfve_hw1/master/download_dataset.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9848784820636317
      ],
      "excerpt": "If you don't own a GPU remove the --cuda option, although I advise you to get one! \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8137668354141352
      ],
      "excerpt": "20% (Training cycleGAN) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8337155790885644
      ],
      "excerpt": "This command will start a training session using the images under the dataroot/train directory with the hyperparameters that showed best results according to CycleGAN authors.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021759920731393,
        0.8340494702208944
      ],
      "excerpt": "This command will take the images under the dataroot/testA/ and dataroot/testB/ directory, run them through the generators and save the output under the ./output/&lt;dataset_name&gt;/ directories.  \nExamples of the generated outputs (default params) apple2orange, summer2winter_yosemite, horse2zebra dataset: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sweet-rytsar/cvfve_hw1/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "<<<<<<< HEAD\n# Homework 1 (Color-Transfer and Texture-Transfer)\n\nA clean and readable Pytorch implementation of CycleGAN (https://arxiv.org/abs/1703.10593)\n## Assign\n\n1.  20% (Training cycleGAN)\n2.  10% (Inference cycleGAN in personal image)\n3.  20% (Compare with other method)\n4.  30% (Assistant) \n5.  20% (Mutual evaluation)\n\nreference:\n[Super fast color transfer between images](https://github.com/jrosebr1/color_transfer)\n\n## Getting Started\nPlease first install [Anaconda](https://anaconda.org) and create an Anaconda environment using the environment.yml file.\n\n```\nconda env create -f environment.yml\n```\n\nAfter you create the environment, activate it.\n```\nsource activate hw1\n```\n\nOur current implementation only supports GPU so you need a GPU and need to have CUDA installed on your machine.\n\n## Training\n### 1. Download dataset\n\n```\nmkdir datasets\nbash ./download_dataset.sh <dataset_name>\n```\nValid <dataset_name> are: apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, cezanne2photo, ukiyoe2photo, vangogh2photo, maps, cityscapes, facades, iphone2dslr_flower, ae_photos\n\nAlternatively you can build your own dataset by setting up the following directory structure:\n\n    .\n    \u251c\u2500\u2500 datasets                   \n    |   \u251c\u2500\u2500 <dataset_name>         # i.e. apple2orange\n    |   |   \u251c\u2500\u2500 trainA             # Contains domain A images (i.e. apple)\n    |   |   \u251c\u2500\u2500 trainB             # Contains domain B images (i.e. orange) \n    |   |   \u251c\u2500\u2500 testA              # Testing\n    |   |   \u2514\u2500\u2500 testB              # Testing\n    \n### 2. Train\n```\npython train.py --dataroot datasets/<dataset_name>/ --cuda\n```\nThis command will start a training session using the images under the *dataroot/train* directory with the hyperparameters that showed best results according to CycleGAN authors. \n\nBoth generators and discriminators weights will be saved ```./output/<dataset_name>/``` the output directory.\n\nIf you don't own a GPU remove the --cuda option, although I advise you to get one!\n\n\n\n## Testing\nThe pre-trained file is on [Google drive](https://drive.google.com/open?id=17FREtttCyFpvjRJxd4v3VVlVAu__Y5do). Download the file and save it on  ```./output/<dataset_name>/netG_A2B.pth``` and ```./output/<dataset_name>/netG_B2A.pth```. \n \n```\npython test.py --dataroot datasets/<dataset_name>/ --cuda\n```\nThis command will take the images under the ```dataroot/testA/``` and ```dataroot/testB/``` directory, run them through the generators and save the output under the ```./output/<dataset_name>/``` directories. \n\nExamples of the generated outputs (default params) apple2orange, summer2winter_yosemite, horse2zebra dataset:\n\n![Alt text](./output/imgs/0167.png)\n![Alt text](./output/imgs/0035.png)\n![Alt text](./output/imgs/0111.png)\n\n\n\n## Acknowledgments\nCode is modified by [PyTorch-CycleGAN](https://github.com/aitorzip/PyTorch-CycleGAN). All credit goes to the authors of [CycleGAN](https://arxiv.org/abs/1703.10593), Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "cvfve_hw1",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sweet-rytsar",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sweet-rytsar/cvfve_hw1/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 18:03:37 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please first install [Anaconda](https://anaconda.org) and create an Anaconda environment using the environment.yml file.\n\n```\nconda env create -f environment.yml\n```\n\nAfter you create the environment, activate it.\n```\nsource activate hw1\n```\n\nOur current implementation only supports GPU so you need a GPU and need to have CUDA installed on your machine.\n\n",
      "technique": "Header extraction"
    }
  ]
}