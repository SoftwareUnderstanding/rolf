{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.10135",
      "https://arxiv.org/abs/1712.05884",
      "https://arxiv.org/abs/2005.11129",
      "https://arxiv.org/abs/2008.03802",
      "https://arxiv.org/abs/2003.01950",
      "https://arxiv.org/abs/1905.09263",
      "https://arxiv.org/abs/1710.08969",
      "https://arxiv.org/abs/1907.09006",
      "https://arxiv.org/abs/1910.10288",
      "https://arxiv.org/abs/2108.10447",
      "https://arxiv.org/abs/1710.10467",
      "https://arxiv.org/abs/1910.06711",
      "https://arxiv.org/abs/2005.05106",
      "https://arxiv.org/abs/1910.11480",
      "https://arxiv.org/abs/1909.11646",
      "https://arxiv.org/abs/2009.00713",
      "https://arxiv.org/abs/2010.05646",
      "https://arxiv.org/abs/2106.07889"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8090016440670298,
        0.8656070203791273
      ],
      "excerpt": "| \ud83d\udea8 Bug Reports              | GitHub Issue Tracker                  | \n| \ud83c\udf81 Feature Requests & Ideas | GitHub Issue Tracker                  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8514579470062061
      ],
      "excerpt": "<!-- [Details...](https://github.com/coqui-ai/TTS/wiki/Mean-Opinion-Score-Results) --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "Guided Attention: paper \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/coqui-ai/TTS/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/coqui-ai/TTS",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "{include} ../../CONTRIBUTING.md\n:relative-images:",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-20T15:45:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T23:04:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8980692969008903,
        0.8922884354589178
      ],
      "excerpt": "\ud83d\udc38TTS is a library for advanced Text-to-Speech generation. It's built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality. \n\ud83d\udc38TTS comes with pretrained models, tools for measuring dataset quality and already used in 20+ languages for products and research projects. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865,
        0.8413312631448417
      ],
      "excerpt": "\ud83d\udce2 English Voice Samples and SoundCloud playlist \n\ud83d\udcc4 Text-to-Speech paper collection \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9170819819777659
      ],
      "excerpt": "Please use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8573455802227357
      ],
      "excerpt": "Underlined \"TTS\" and \"Judy\" are \ud83d\udc38TTS models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9233262493220061
      ],
      "excerpt": "High-performance Deep Learning models for Text2Speech tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9742497168468732
      ],
      "excerpt": "Ability to convert PyTorch models to Tensorflow 2.0 and TFLite for inference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723677095288299
      ],
      "excerpt": "Tools to curate Text2Speech datasets underdataset_analysis. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9292966095106179
      ],
      "excerpt": "Modular (but not too much) code base enabling easy implementation of new ideas. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8964291686728832
      ],
      "excerpt": "    |- tts/             (text to speech models) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8477678735851494,
        0.8824411505570533
      ],
      "excerpt": "        |- models/          (model definitions) \n        |- tf/              (Tensorflow 2 utilities and model implementations) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "\ud83d\udc38\ud83d\udcac - a deep learning toolkit for Text-to-Speech, battle-tested in research and production",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://tts.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/coqui-ai/TTS/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 249,
      "date": "Fri, 24 Dec 2021 07:58:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/coqui-ai/TTS/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "coqui-ai/TTS",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/coqui-ai/TTS/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/notebooks/PlotUmapLibriTTS.ipynb",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/notebooks/Tutorial_Converting_PyTorch_to_TF_to_TFlite.ipynb",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/notebooks/TestAttention.ipynb",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/notebooks/ExtractTTSpectrogram.ipynb",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/notebooks/dataset_analysis/CheckDatasetSNR.ipynb",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/notebooks/dataset_analysis/PhonemeCoverage.ipynb",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/notebooks/dataset_analysis/CheckSpectrograms.ipynb",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/notebooks/dataset_analysis/CheckPitch.ipynb",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/notebooks/dataset_analysis/AnalyzeDataset.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/run_bash_tests.sh",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/tests/bash_tests/test_resample.sh",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/tests/bash_tests/test_demo_server.sh",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/tests/bash_tests/test_compute_statistics.sh",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/recipes/ljspeech/download_ljspeech.sh",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/recipes/vctk/download_vctk.sh",
      "https://raw.githubusercontent.com/coqui-ai/TTS/main/recipes/kokoro/tacotron2-DDC/run.sh"
    ],
    "technique": "File Exploration"
  },
  "identifier": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://zenodo.org/badge/latestdoi/265612440",
      "technique": "Regular expression"
    }
  ],
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\ud83d\udc38TTS is tested on Ubuntu 18.04 with **python >= 3.6, < 3.9**.\n\nIf you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released \ud83d\udc38TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nBy default, this only installs the requirements for PyTorch. To install the tensorflow dependencies as well, use the `tf` extra.\n\n```bash\npip install TTS[tf]\n```\n\nIf you plan to code or train models, clone \ud83d\udc38TTS and install it locally.\n\n```bash\ngit clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks,tf]  #: Select the relevant extras\n```\n\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\n\n```bash\n$ make system-deps  #: intended to be used on Ubuntu (Debian). Let us know if you have a diffent OS.\n$ make install\n```\n\nIf you are on Windows, \ud83d\udc51@GuyPaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8424418097815196
      ],
      "excerpt": "<img src=\"https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8711444198174407
      ],
      "excerpt": "| \ud83d\udcbe Installation               | TTS/README.md| \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9058405005910268,
        0.8923390729687973
      ],
      "excerpt": "      |- train*.py                  (train your target model.) \n      |- distribute.py              (train your TTS model using Multiple GPUs.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8465696616018751
      ],
      "excerpt": "      |- convert*.py                (convert target torch model to TF.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481215394023953
      ],
      "excerpt": "    |- tts/             (text to speech models) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/coqui-ai/TTS/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "HTML",
      "Shell",
      "Makefile",
      "Cython"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Mozilla Public License 2.0",
      "url": "https://api.github.com/licenses/mpl-2.0"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "<img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png\" height=\"56\"/>",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "TTS",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "coqui-ai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/coqui-ai/TTS/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "## What's Changed\r\n* Model zoo tests by @erogol in https://github.com/coqui-ai/TTS/pull/900\r\n* v0.4.2 by @erogol in https://github.com/coqui-ai/TTS/pull/901\r\n* Optional silence trimming during inference and find_endpoint() fix by @george-roussos in https://github.com/coqui-ai/TTS/pull/898\r\n* Update gruut to version 2.0 by @synesthesiam in https://github.com/coqui-ai/TTS/pull/882\r\n* Documentation corrections for finetuning and data preparation by @gullabi in https://github.com/coqui-ai/TTS/pull/931\r\n* server: fix compatibility with tts_models/en/ljspeech/fast_pitch by @Mic92 in https://github.com/coqui-ai/TTS/pull/893\r\n* v0.4.2 by @erogol in https://github.com/coqui-ai/TTS/pull/914\r\n\r\n## New Contributors\r\n* @george-roussos made their first contribution in https://github.com/coqui-ai/TTS/pull/898\r\n* @gullabi made their first contribution in https://github.com/coqui-ai/TTS/pull/931\r\n\r\n**Full Changelog**: https://github.com/coqui-ai/TTS/compare/v0.4.1...v0.4.2",
        "dateCreated": "2021-12-08T15:41:44Z",
        "datePublished": "2021-12-08T15:42:51Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.4.2",
        "name": "v0.4.2",
        "tag_name": "v0.4.2",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.4.2",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/54883822",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.4.2"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "## What's Changed\r\n* v0.4.1  by @erogol in https://github.com/coqui-ai/TTS/pull/891\r\n\r\n\r\n**Full Changelog**: https://github.com/coqui-ai/TTS/compare/v0.4.0...v0.4.1",
        "dateCreated": "2021-10-26T17:54:00Z",
        "datePublished": "2021-10-26T17:55:11Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.4.1",
        "name": "v0.4.1",
        "tag_name": "v0.4.1",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.4.1",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/52079501",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.4.1"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# \ud83d\udc38 v0.4.0\r\n\r\n- Update multi-speaker training API.\r\n- VCTK recipes for all the TTS models.\r\n- Documentation for multi-speaker training.\r\n- Pre-trained Ukrainian GlowTTS model from \ud83d\udc51 https://github.com/robinhad/ukrainian-tts\r\n- Pre-trained FastPitch VCTK model\r\n- Dataset downloaders for LJSpeech and VCTK under `TTS.utils.downloaders`\r\n- Documentation reformatting.\r\n- Trainer V2  and compact. updates in model implementations.\r\n       \r\n    This update makes the Trainer V2  responsible for only the training of a model. The rest is excluded from the trainer and they need to be done either in the model or before calling the trainer. \r\n\r\n## Try out new models\r\n\r\n- Pre-trained FastPitch VCTK model\r\n\r\n```bash \r\ntts --model_name tts_models/en/vctk/fast_pitch --text \"This is my sample text to voice.\" --speaker_idx VCTK_p229\r\n```\r\n\r\n- Pre-trained Ukrainian GlowTTS model from \ud83d\udc51 https://github.com/robinhad/ukrainian-tts\r\n\r\n```bash\r\ntts --model_name tts_models/uk/mai/glow-tts --text \"\u0426\u0435 \u0437\u0440\u0430\u0437\u043e\u043a \u0442\u0435\u043a\u0441\u0442\u0443, \u0449\u043e\u0431 \u0441\u043f\u0440\u043e\u0431\u0443\u0432\u0430\u0442\u0438 \u043d\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c.\"  \r\n``` \r\n",
        "dateCreated": "2021-10-26T16:32:46Z",
        "datePublished": "2021-10-26T16:34:16Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.4.0",
        "name": "v0.4.0",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.4.0",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/51849527",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "Mostly fixes for GlowTTS training. ",
        "dateCreated": "2021-09-17T23:53:22Z",
        "datePublished": "2021-09-17T23:55:13Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.3.1",
        "name": "v0.3.1",
        "tag_name": "v0.3.1",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.3.1",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/49782465",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.3.1"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# \ud83d\udc38 v0.3.0\r\n\r\n## New `ForwardTTS` implementation.\r\nThis version implements a new `ForwardTTS` interface that can be configured as any feed-forward TTS model that uses a duration predictor at inference time. Currently, we provide 3 pre-configured models and plan to implement one more. \r\n\r\n1. SpeedySpeech\r\n2. FastSpeech\r\n3. FastPitch\r\n4. FastSpeech 2 (TODO) \r\n\r\nThrough this API, any model can be trained in two ways. Either using pre-computed durations from a pre-trained Tacotron model or using an alignment network to learn durations from the dataset. The alignment network is only used at training and discarded at inference.  You can set which mode you want to use by just setting the `use_aligner` field in the configuration. \r\n\r\nThis new API will help us to design more efficient inference run-time for all these models using ONNX like run-time optimizers. \r\n\r\nOld `FastPitch` and `SpeedySpeech` implementations are deprecated for the sake of this new implementation. \r\n\r\n## Fine-Tuning Documentation\r\nThis version introduces documentation for model fine-tunning. You can see it under https://tts.readthedocs.io/ when this is merged. \r\n\r\n## New Model Releases\r\n\r\n- English Speedy Speech model on LJSpeech \r\n\r\nTry out:\r\n```bash\r\ntts --text \"This is a sample text for my model to speak.\" --model_name tts_models/en/ljspeech/speedy-speech\r\n```\r\n\r\n\r\n- Fine-tuned UnivNet Vocoder\r\n\r\nTry out:\r\n```bash\r\ntts --text \"This is how it is.\" --model_name tts_models/en/ljspeech/tacotron2-DDC_ph \r\n```\r\n\r\n\r\n",
        "dateCreated": "2021-09-13T12:13:33Z",
        "datePublished": "2021-09-13T12:13:49Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.3.0",
        "name": "v0.3.0",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.3.0",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/49438082",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# \ud83d\udc38 v0.2.2\r\n\r\nFastPitch model with an Aligner Network is implemented with other changes accompanying it. \r\n\r\n- Alignment Network: https://arxiv.org/abs/2108.10447\r\n- Fast Pitch Model: https://arxiv.org/abs/2006.06873\r\n\r\nThanks to \ud83d\udc51  @kaiidams for his Japanese g2p update. \r\n\r\nTry FastPitch model:\r\n```bash\r\ntts --model_name tts_models/en/ljspeech/fast_pitch --text \"This is my sample text to voice.\"\r\n```\r\n",
        "dateCreated": "2021-09-06T17:25:34Z",
        "datePublished": "2021-09-06T17:25:58Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.2.2",
        "name": "v0.2.2",
        "tag_name": "v0.2.2",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.2.2",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/49084979",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.2.2"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# \ud83d\udc38 v0.2.1\r\n\r\n### \ud83d\udc1eBug Fixes \r\n- Fix distributed training and solve compact issues with the Trainer API.\r\n- Fix bugs in the VITS model implementation that caused training instabilities. \r\n- Fix some Abstract Class usage issues in WaveRNN and WaveGrad models.\r\n\r\n### \ud83d\udcbe Code updates\r\n- Use a single gradient scaler for all the optimizers in TrainerAPI. Previously, we used one scaler per optimizer. \r\n\r\n### \ud83c\udfc3\u200d\u2640\ufe0fOperational Updates\r\n- Update to Pylint 2.10.2\r\n\r\nThanks to \ud83d\udc51  @fijipants for his fixes \ud83d\udee0\ufe0f\r\nThanks to \ud83d\udc51  @agrinh for his flag and discussion in DDP issues \r\n",
        "dateCreated": "2021-08-31T10:38:28Z",
        "datePublished": "2021-08-31T10:39:04Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.2.1",
        "name": "v0.2.1",
        "tag_name": "v0.2.1",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.2.1",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/48733918",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# \ud83d\udc38 v0.2.0\r\n\r\n### \ud83d\udc1eBug Fixes \r\n\r\n- Fix phoneme pre-compute issue.\r\n- Fix multi-speaker setup in Tacotron models.\r\n- Fix small issues in the Trainer regarding multi-optimizer training.\r\n\r\n### \ud83d\udcbe Code updates\r\n\r\n- W&B integration for model logging and experiment tracking, (\ud83d\udc51 @AyushExel)\r\n      Code uses the Tensorboard by default. For W&B, you need to set `log_dashboard` option in the config and define `project_name` and `wandb_entity`. \r\n- Use ffsspec for model saving/loading (\ud83d\udc51 @agrinh)\r\n- Allow models to define their own symbol list with in-class `make_symbols()`\r\n- Allow choosing after epoch or after step LR scheduler update with `scheduler_after_epoch`.\r\n- Make converting spectrogram from amplitude to DB optional with `do_amp_to_db_linear` and `do_amp_to_db_linear` options.\r\n\r\n### \ud83d\uddd2\ufe0f  Docs updates\r\n\r\n- Add GlowTTS and VITS docs.\r\n\r\n### \ud83e\udd16 Model implementations\r\n\r\n- VITS implementation with pre-trained models (https://arxiv.org/abs/2106.06103)\r\n\r\n### \ud83d\ude80 Model releases\r\n\r\n- vocoder_models--ja--kokoro--hifigan_v1 (\ud83d\udc51 @kaiidams)\r\n\r\n    HiFiGAN model trained on Kokoro dataset to complement the existing Japanese model. \r\n\r\n    ### Try it out:\r\n    ```bash\r\n    tts --model_name tts_models/ja/kokoro/tacotron2-DDC --text \"\u3053\u3093\u306b\u3061\u306f\u3001\u4eca\u65e5\u306f\u3044\u3044\u5929\u6c17\u3067\u3059\u304b\uff1f\"\r\n    ``` \r\n \r\n- tts_models--en--ljspeech--tacotronDDC_ph\r\n\r\n    TacotronDDC with phonemes trained on LJSpeech. It is to fix the pronunciation errors caused by the raw text\r\n    in the released TacotronDDC model.\r\n\r\n    ### Try it out:\r\n    ```bash\r\n    tts --model_name tts_models/en/ljspeech/tacotronDDC_ph --text \"hello, how are you today?\"\r\n    ``` \r\n\r\n- tts_models--en--ljspeech--vits\r\n\r\n    VITS model trained on LJSpeech.\r\n\r\n    ### Try it out:\r\n    ```bash\r\n    tts --model_name tts_models/en/ljspeech/vits --text \"hello, how are you today?\"\r\n    ```\r\n\r\n- tts_models--en--vctk--vits\r\n\r\n    VITS model trained on VCTK with multi-speaker support.\r\n\r\n    ### Try it out:\r\n    ```bash\r\n    tts-server --model_name tts_models/en/vctk/vits     \r\n    ```\r\n\r\n- vocoder_models--en--ljspeech--univnet\r\n\r\n    UnivNet model trained on LJSpeech to complement the TacotronDDC model above.\r\n\r\n    ### Try it out:\r\n    ```bash\r\n    tts --model_name tts_models/en/ljspeech/tacotronDDC_ph --text \"hello, how are you today?\"\r\n    ``` \r\n\r\n",
        "dateCreated": "2021-08-11T08:30:48Z",
        "datePublished": "2021-08-11T08:43:19Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.2.0",
        "name": "v0.2.0",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.2.0",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/47208495",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# \ud83d\udc38 v0.1.3\r\n\r\n### \ud83d\udc1eBug Fixes \r\n- Fix Tacotron stopnet training \r\n\r\n    Models trained after v0.1 had the problem that the stopnet was not trained. It caused models not to generate audio\r\n    at evaluation and inference time.\r\n\r\n- Fix `test_run` at training. (\ud83d\udc51 @WeberJulian)\r\n\r\n    In training :frog: TTS would skip the `test_run` and not generate test audio samples. Now it is fixed :). \r\n\r\n- Fix `server.py` for multi-speaker models.\r\n\r\n### \ud83d\udcbe Code updates\r\n- Refactoring in `compute_embeddings.py` for efficiency and compatibility with the latest speaker encoder. (\ud83d\udc51 @Edresson)\r\n\r\n### \ud83d\ude80 Model releases\r\n\r\n- New Fullband-MelGAN model for Thorsten German dataset. (\ud83d\udc51 @thorstenMueller)\r\n\r\n#### Try it:\r\n```python\r\ntts --model_name tts_models/de/thorsten/tacotron2-DCA --text \"Was geschehen ist geschehen, es ist geschichte.\" \r\n```\r\n",
        "dateCreated": "2021-07-26T19:37:02Z",
        "datePublished": "2021-07-26T16:37:32Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.1.3",
        "name": "v0.1.3",
        "tag_name": "v0.1.3",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.1.3",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/46762460",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.1.3"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "",
        "dateCreated": "2021-07-06T12:44:59Z",
        "datePublished": "2021-07-06T13:26:09Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.1.2",
        "name": "v0.1.2",
        "tag_name": "v0.1.2",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.1.2",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/45771142",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.1.2"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "- Fix #607 and #608",
        "dateCreated": "2021-07-04T10:59:27Z",
        "datePublished": "2021-07-06T08:32:01Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.1.1",
        "name": "v0.1.1",
        "tag_name": "v0.1.1",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.1.1",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/45754761",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.1.1"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# \ud83d\udc38 v0.1.0\r\n\r\nIn a nutshell, there are a ton of updates in this release. I don't know if we can cover them all here but let's try.\r\n\r\nAfter this release, \ud83d\udc38 TTS stands on the following architecture. \r\n\r\n- `Trainer API` for training.\r\n- `Synthesizer API` for inference.\r\n- `ModelManager API` for managing \ud83d\udc38TTS model zoo.\r\n- `SpeakerManager API` for managing speakers in a multi-speaker setting.\r\n- (TBI) `Exporter API` for exporting models to ONNX, TorchScript, etc.\r\n- (TBI) `Data Processing API` for making a dataset ready for training.\r\n- `Model API` for implementing models, compatible with all the other components above.\r\n\r\n## Updates\r\n\r\n<!-- ### \ud83d\udc1eBug Fixes -->\r\n\r\n### \ud83d\udcbe Code updates\r\n- Brand new `Trainer API`\r\n\r\n    We unified all the training code in a lightweight but feature complete `Trainer API`. From now on all the \ud83d\udc38TTS\r\n    models will use this new API for training.\r\n\r\n    It provides mixed precision (with Nvidia's APEX of `torch.amp`) and multi-gpu training for all the models.\r\n\r\n- Brand new `Model API`\r\n\r\n    Abstract `BaseModel` and its `BaseTTS`, `BaseVocoder` child classes are used as the basis of the \ud83d\udc38TTS models now.\r\n    Any model that implements one of these classes, works seamlessly with the `Trainer` and `Synthesizer`.\r\n\r\n- Brand new \ud83d\udc38TTS `recipes`.\r\n\r\n    We decided to merge the recipes to the main project. Now we host recipes for the LJspeech dataset, covering all the implemented models.\r\n    So you can pick the model you want,  change the parameters, and train your own model easily.\r\n\r\n    Thanks to the new `Trainer API` and \ud83d\udc69\u200d\u2708\ufe0fCoqpit integration, we could implement these recipes with pure python.\r\n\r\n- Updates `SpeakerManager API`\r\n\r\n    `TTS.utilsSpeakerManager` is now the core unit to manage speakers in a multi-speaker model and interface a `SpeakerEncoder` model with the `tts` and `vocoder` models.\r\n\r\n- Updated model training mechanics.\r\n\r\n    You can now use pure Python to define your model and run the training. It is useful to train models on a Jupyter\r\n    Notebook or the other python environments.\r\n\r\n    We also keep the old mechanics by using `TTS/bin/train_tts.py` or ``TTS/bin/train_vocoder.py`. You just need to\r\n    change the previous training script name with one of these two based on your model.\r\n\r\n    ```bash\r\n    python TTS/bin/train_tacotron.py --config_path config.json\r\n    ```\r\n\r\n    becomes\r\n\r\n    ```bash\r\n    python TTS/bin/train_tts.py --config_path config.json\r\n    ```\r\n\r\n- Use \ud83d\udc69\u200d\u2708\ufe0fCoqpit for managing model class arguments.\r\n\r\n    Now all the model arguments are defined in a `coqpit` class and imported by the model config.\r\n\r\n- `gruut` based character to phoneme conversion.  (\ud83d\udc51 @synesthesiam)\r\n\r\n    As a drop-in replacement for the previous solution that is compatible with the released models. So now all these\r\n    models are functional again without version nitpicking.\r\n\r\n- Set `test_sentences` in the config rather than providing a txt file.\r\n- Set the maximum number of decoder steps of `Tacotron1-2` models in the config.\r\n\r\n### \ud83c\udfc3\u200d\u2640\ufe0f Operational Updates\r\n- FINALLY DOCUMENTATION!! https://tts.readthedocs.io\r\n- Enable support for Python 3.9\r\n- Changes for PyTorch 1.9.0\r\n\r\n### \ud83c\udfc5 Model implementations\r\n- Univnet GAN Vocoder: https://arxiv.org/pdf/2106.07889.pdf (\ud83d\udc51 @rishikksh20)\r\n\r\n### \ud83d\ude80 Model releases\r\n\r\nWe solved the compat issues and re-release some of the models. You can see them in the released binaries section.\r\n\r\nYou don't need to change anything. If you use v0.1.0, by default, it uses these new models.\r\n\r\n\r\n",
        "dateCreated": "2021-07-03T11:55:27Z",
        "datePublished": "2021-07-03T12:18:49Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.1.0",
        "name": "v0.1.0",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.1.0",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/45558007",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.1.0"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "- Fix manifest file to include `VERSION`  for pypi distribution.  ",
        "dateCreated": "2021-06-08T07:21:01Z",
        "datePublished": "2021-06-08T08:57:06Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.0.15.1",
        "name": "v0.0.15.1 ",
        "tag_name": "v0.0.15.1",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.0.15.1",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/44261819",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.0.15.1"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# \ud83d\udc38 v0.0.15\r\n## \ud83d\udc1eBug Fixes\r\n- [x] Fix tb_logger init for rank > 0 processes in distributed training.\r\n\r\n## \ud83d\udcbe Code updates\r\n- [x] Refactoring and optimization in the speaker encoder module.  (:crown: @Edresson )\r\n- [x] Replacing `unidecode` with `anyascii`\r\n- [x] Japanese text to phoneme conversion. (:crown: @kaiidams)\r\n- [x] Japanese `tts` recipe to train Tacotron2-DDC on [Kokoro](https://www.kaggle.com/kaiida/kokoro-speech-dataset-v11-tiny) dataset (:crown: @kaiidams)\r\n\r\n## :walking_woman:  Operational Updates\r\n- [x] Start using `pylint == 2.8.3`\r\n- [x] Reorg `tests` files.\r\n- [x] Upload to pypi automatically on release. \r\n- [x] Move `VERSION` file under `TTS` folder.\r\n\r\n## \ud83c\udfc5 Model implementations\r\n- [x] New Speaker Encoder implementation based on https://arxiv.org/abs/2009.14153  (:crown: @Edresson )\r\n\r\n## \ud83d\ude80 New Pre-Trained Model Releases\r\n- [x] Japanese Tacotron model (:crown: @kaiidams)\r\n\r\n:bulb:  All the models below are available by ```tts``` or ```tts-server``` endpoints on CLI as explained [here](https://github.com/coqui-ai/TTS#example-synthesizing-speech-on-terminal-using-the-released-models).\r\n",
        "dateCreated": "2021-06-04T12:02:53Z",
        "datePublished": "2021-06-04T12:06:00Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.0.15",
        "name": "v0.0.15",
        "tag_name": "v0.0.15",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.0.15",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/43971690",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.0.15"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# :frog: v0.0.14\r\n## \ud83d\udc1eBug Fixes\r\n- [x] Remove breaking line from Tacotron models. (\ud83d\udc51 @a-froghyra)\r\n\r\n## \ud83d\udcbe Code updates\r\n- [x] **BREAKING:** Coqpit integration for config management and the first \ud83d\udc38TTS recipe, for LJSpeech Check #476.\r\n\r\nEvery model now tied to a Python class that defines the configuration scheme. It provides a better interface and lets the user know better what are the default values, expected value types, and mandatory fields.\r\n\r\nSpecific model configs are defined under `TTS/tts/configs` and `TTS/vocoder/configs`. `TTS/config/shared_configs.py` hosts configs that are shared by all the :frog: TTS models.  Configs shared by `tts` models are hosted under `TTS/tts/configs/shared_configs.py` and shared by `vocoder` models are under `TTS/vocoder/configs/shared_config.py`.\r\n\r\nFor example `TacotronConfig` follows `BaseTrainingConfig -> BaseTTSConfig -> TacotronConfig`.\r\n- [x] **BREAKING:** Remove `phonemizer` support due to License conflict. \r\n\r\nThis essentially deprecates the support for all the models using phonemes as input. Feel free to suggest in-place options if you are affected by this change. \r\n\r\n- [x] Start hosting :woman_cook: **recipes** under :frog: TTS. The first recipe is for Tacotron2-DDC with LJspeech dataset under `TTS/recipes/`. \r\n\r\nPlease check [here](https://github.com/coqui-ai/TTS/tree/main/recipes) for more details.\r\n- [x] Add `extract_tts_spectrograms.py` that supports GlowTTS and Tacotron1-2. (\ud83d\udc51 @Edresson)\r\n- [x] Add `version.py` (\ud83d\udc51 @chmodsss)",
        "dateCreated": "2021-05-19T12:03:26Z",
        "datePublished": "2021-05-19T12:09:30Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.0.14",
        "name": "v0.0.14",
        "tag_name": "v0.0.14",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.0.14",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/43210530",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.0.14"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# :frog: v0.0.13\r\n## \ud83d\udc1eBug Fixes\r\n\r\n## \ud83d\udcbe Code updates\r\n- `SpeakerManager` class for handling multi-speaker model management and interfacing `speaker.json` file. \r\n- Enabling multi-speaker models with `tts` and `tts-server` endpoints. (:crown: @kirianguiller )\r\n- Allow choosing a different `noise scale` for GlowTTS at inference. \r\n- Glow-TTS updates to import SC-Glow Models.\r\n- Fixing windows support (:crown: @WeberJulian )\r\n\r\n## :walking_woman:  Operational Updates\r\n- Refactoring :frog: TTS installation and allow selecting different scopes (`all, tf, notebooks`)for installation depending on the specific needs. \r\n\r\n## \ud83c\udfc5 Model implementations\r\n\r\n## \ud83d\ude80 New Pre-Trained Model Releases\r\n- SC-GlowTTS multi-speaker English model from our work https://arxiv.org/abs/2104.05557 (:crown: @Edresson )\r\n- HiFiGAN vocoder finetuned for the above model. \r\n- Tacotron DDC Non-Binary English model using Accenture's Sam dataset. \r\n- HiFiGAN vocoder trained for the models above.\r\n\r\n# Released Models\r\n:bulb:  All the models below are available by ```tts``` or ```tts-server``` endpoints on CLI as explained [here](https://github.com/coqui-ai/TTS#example-synthesizing-speech-on-terminal-using-the-released-models).\r\n\r\nModels with \u2728\ufe0f below are new with this release.\r\n\r\n - SC-GlowTTS model is from our [latest paper](https://arxiv.org/abs/2104.05557) in a collaboration with @Edresson and @mueller91.\r\n - The new non-binary TTS model is trained using the SAM dataset from Accenture Labs. Check out their [blog post](https://www.accenture.com/us-en/blogs/technology-innovation/danielescu-building-tech-that-reflects-our-diversity)\r\n\r\n| Language | Dataset | Model Name | Model Type | TTS version | Download|\r\n| -- | -- | -- | -- | -- | :-: |\r\n:sparkles: English  (non-binary)       | sam (acccenture) |  Tacotron2-DDC  | tts  | :smile: v0.0.13 | [:floppy_disk: ](https://github.com/coqui-ai/TTS/releases/download/v0.0.13/tts_models--en--sam--tacotron_DDC.zip)  \r\n:sparkles: English  (multi-speaker)   | VCTK        | SC-GlowTTS        | tts       | :smile: v0.0.13| [:floppy_disk: ](https://github.com/coqui-ai/TTS/releases/download/v0.0.12/tts_models--en--vctk--sc-glowtts-transformer.zip)| \r\nEnglish                     |  LJSpeech | Tacotron-DDC          | tts       | v0.0.12| [:floppy_disk: ](https://github.com/coqui-ai/TTS/releases/download/v0.0.12/tts_models--en--ljspeech--tacotron2-DDC.zip)\r\nGerman                    | Thorsten-DE   | Tacotron-DCA     | tts       | v0.0.11   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.11/tts_models--de--thorsten--tacotron2-DCA.zip)\r\nGerman                    | Thorsten-DE   | Wavegrad     | vocoder      |v0.0.11   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.11/vocoder_models--de--thorsten--wavegrad.zip)\r\nEnglish                    | LJSpeech   | SpeedySpeech      | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--en--ljspeech--speedy-speech-wn.zip)\r\nEnglish                    | EK1        | Tacotron2         | tts       |v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--en--ek1--tacotron2.zip)\r\nDutch                      | MAI        | TacotronDDC       | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--nl--mai--tacotron2-DDC.zip)\r\nChinese                    | Baker      | TacotronDDC-GST   | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--zh-CN--baker--tacotron2-DDC-GST.zip)\r\nEnglish                    | LJSpeech   | TacotronDCA       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--tacotron2-DCA.zip)\r\nEnglish                    | LJSpeech   | Glow-TTS          | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--glow-tts.zip)\r\nSpanish                    | M-AILabs   | TacotronDDC       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--es--mai--tacotron2-DDC.zip)\r\nFrench                     | M_AILabs   | TacotronDDC       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--fr--mai--tacotron2-DDC.zip)\r\nDutch                      | MAI        | TacotronDDC       | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--nl--mai--tacotron2-DDC.zip)\r\n:sparkles: English   | sam (accenture)   | HiFiGAN          | vocoder   | :smile: v0.0.13| [:floppy_disk: ](https://github.com/coqui-ai/TTS/releases/download/v0.0.13/vocoder_model--en--sam--hifigan_v2.zip) \r\n:sparkles: English   | VCTK       | HiFiGAN          | vocoder   | :smile: v0.0.13| [:floppy_disk: ](https://github.com/coqui-ai/TTS/releases/download/v0.0.12/vocoder_model--en--vctk--hifigan_v2.zip) \r\nEnglish   | LJSpeech | HiFiGAN          | vocoder   | v0.0.12| [:floppy_disk: ](https://github.com/coqui-ai/TTS/releases/download/v0.0.12/vocoder_model--en--ljspeech-hifigan_v2.zip)\r\nEnglish                    | EK1        | WaveGrad          | vocoder   | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/vocoder_models--en--ek1--wavegrad.zip)\r\nDutch                      | MAI        | ParallelWaveGAN   | vocoder   | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/vocoder_models--nl--mai--parallel-wavegan.zip)\r\nEnglish                    | LJSpeech   | MB-MelGAN         | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--en--ljspeech--mulitband-melgan.zip)\r\n:earth_africa:  Multi-Lang | LibriTTS   | FullBand-MelGAN   | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--fullband-melgan.zip)\r\n:earth_africa:  Multi-Lang | LibriTTS   | WaveGrad          | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--wavegrad.zip)\r\n\r\n**Update Jun 7 2021:** Ruslan (Russian) model has been removed due to the license conflict. ",
        "dateCreated": "2021-04-29T07:32:36Z",
        "datePublished": "2021-04-29T07:17:24Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.0.13",
        "name": "v0.0.13",
        "tag_name": "v0.0.13",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.0.13",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/42122899",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.0.13"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# :frog: v0.0.12\r\n## \ud83d\udc1eBug Fixes\r\n- [x] fix #419 (This is a crucial bug fix).\r\n- [x] fix #408\r\n\r\n## \ud83d\udcbe Code updates\r\n- [x] Enable logging model config.json on Tensorboard. #418 \r\n- [x] Update code style standards and use a  ```Makefile``` to ease regular tasks.  #423 \r\n- [x] Enable using ```Tacotron.prenet.dropout``` at inference time. This leads to a better quality with some models.\r\n- [x] Update default ```tts``` model to LJspeech TacotronDDC. \r\n- [x] Show the real waveform on Tensorboard in GAN vocoder training.\r\n\r\n## :walking_woman:  Operational Updates\r\n\r\n## \ud83c\udfc5 Model implementations\r\n- [x] initial HiFiGAN implementation (:crown: @rishikksh20 @erogol) #422 \r\n\r\n## \ud83d\ude80 New Pre-Trained Model Releases\r\n- [ ] ~~Universal HifiGAN model~~(postponed to the next version for :crown: @Edresson's updated model.)\r\n- [x] LJSpeech, Tacotron2 Double Decoder Consistency v2 model.\r\n       Check our [blog post](https://coqui.ai/blog/tts/solving-attention-problems-of-tts-models-with-double-decoder-consistency) to learn more about Double Decoder Consistency.\r\n- [x] LJSpeech HifiGAN model. \r\n\r\n# Released Models\r\n:bulb:  All the models below are available by ```tts``` end point as explained [here](https://github.com/coqui-ai/TTS#example-synthesizing-speech-on-terminal-using-the-released-models).\r\n\r\n| Language | Dataset | Model Name | Model Type | TTS version | Download|\r\n| -- | -- | -- | -- | -- | :-: |\r\n:sparkles:  English    |  LJSpeech | Tacotron-DDC          | tts       |:smiley: v0.0.12| [:floppy_disk: ](https://github.com/coqui-ai/TTS/releases/download/v0.0.12/tts_models--en--ljspeech--tacotron2-DDC.zip)\r\nGerman                    | Thorsten-DE   | Tacotron-DCA     | tts       | v0.0.11   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.11/tts_models--de--thorsten--tacotron2-DCA.zip)\r\nGerman                    | Thorsten-DE   | Wavegrad     | vocoder      |v0.0.11   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.11/vocoder_models--de--thorsten--wavegrad.zip)\r\nEnglish                    | LJSpeech   | SpeedySpeech      | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--en--ljspeech--speedy-speech-wn.zip)\r\nEnglish                    | EK1        | Tacotron2         | tts       |v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--en--ek1--tacotron2.zip)\r\nDutch                      | MAI        | TacotronDDC       | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--nl--mai--tacotron2-DDC.zip)\r\nChinese                    | Baker      | TacotronDDC-GST   | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--zh-CN--baker--tacotron2-DDC-GST.zip)\r\nEnglish                    | LJSpeech   | TacotronDCA       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--tacotron2-DCA.zip)\r\nEnglish                    | LJSpeech   | Glow-TTS          | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--glow-tts.zip)\r\nSpanish                    | M-AILabs   | TacotronDDC       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--es--mai--tacotron2-DDC.zip)\r\nFrench                     | M_AILabs   | TacotronDDC       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--fr--mai--tacotron2-DDC.zip)\r\nDutch                      | MAI        | TacotronDDC       | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--nl--mai--tacotron2-DDC.zip)\r\n:sparkles:  English   | LJSpeech | HiFiGAN          | vocoder   |  :smiley:  v0.0.12| [:floppy_disk: ](https://github.com/coqui-ai/TTS/releases/download/v0.0.12/vocoder_model--en--ljspeech-hifigan_v2.zip)\r\nEnglish                    | EK1        | WaveGrad          | vocoder   | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/vocoder_models--en--ek1--wavegrad.zip)\r\nDutch                      | MAI        | ParallelWaveGAN   | vocoder   | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/vocoder_models--nl--mai--parallel-wavegan.zip)\r\nEnglish                    | LJSpeech   | MB-MelGAN         | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--en--ljspeech--mulitband-melgan.zip)\r\n:earth_africa:  Multi-Lang | LibriTTS   | FullBand-MelGAN   | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--fullband-melgan.zip)\r\n:earth_africa:  Multi-Lang | LibriTTS   | WaveGrad          | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--wavegrad.zip)",
        "dateCreated": "2021-04-15T15:06:38Z",
        "datePublished": "2021-04-15T15:02:29Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.0.12",
        "name": "v0.0.12",
        "tag_name": "v0.0.12",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.0.12",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/41483548",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.0.12"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# :frog: v0.0.11\r\n## \ud83d\udc1eBug Fixes\r\n- [x] Fixed #374. (Thx for reporting @a-froghyar )\r\n\r\n## \ud83d\udcbe Code updates\r\n- [x] ```/bin/resample.py``` to resample wavefiles (:crown: @WeberJulian)\r\n- [x] Some updates for Windows compat. (:crown: @GuyPaddock)\r\n- [x] Fixing ```CheckSpectrogram``` notebook. (:crown: @GuyPaddock) \r\n- [x] Fix #392  \r\n\r\n## :walking_woman:  Operational Updates\r\n\r\n## \ud83c\udfc5 Model implementations\r\n- [x] initial AlignTTS implementation. (https://github.com/coqui-ai/TTS/pull/398)\r\n- [ ] initial HiFiGAN implementation (:crown: @rishikksh20) (postponed to the next release)\r\n\r\n## \ud83d\ude80 New Pre-Trained Model Releases\r\n- [x] German - Tacotron2-DCA  trained with [thorsten_dataset](https://github.com/thorstenMueller/deep-learning-german-tts). (:crown: @thorstenMueller )\r\n- [x] German - Wavegrad vocoder with [thorsten_dataset. ](https://github.com/thorstenMueller/deep-learning-german-tts)(:crown:  @thorstenMueller)\r\n\r\n# Released Models\r\n:bulb:  All the models below are available by ```tts``` end point as explained [here](https://github.com/coqui-ai/TTS#example-synthesizing-speech-on-terminal-using-the-released-models).\r\n\r\n| Language | Dataset | Model Name | Model Type | TTS version | Download|\r\n| -- | -- | -- | -- | -- | :-: |\r\n:sparkles:   German                    | Thorsten-DE   | Tacotron-DCA     | tts       |:smiley: v0.0.11   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.11/tts_models--de--thorsten--tacotron2-DCA.zip)\r\n:sparkles:  German                    | Thorsten-DE   | Wavegrad     | vocoder      |:smiley: v0.0.11   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.11/vocoder_models--de--thorsten--wavegrad.zip)\r\nEnglish                    | LJSpeech   | SpeedySpeech      | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--en--ljspeech--speedy-speech-wn.zip)\r\nEnglish                    | EK1        | Tacotron2         | tts       |v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--en--ek1--tacotron2.zip)\r\nDutch                      | MAI        | TacotronDDC       | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--nl--mai--tacotron2-DDC.zip)\r\nChinese                    | Baker      | TacotronDDC-GST   | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--zh-CN--baker--tacotron2-DDC-GST.zip)\r\nEnglish                    | LJSpeech   | TacotronDCA       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--tacotron2-DCA.zip)\r\nEnglish                    | LJSpeech   | Glow-TTS          | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--glow-tts.zip)\r\nSpanish                    | M-AILabs   | TacotronDDC       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--es--mai--tacotron2-DDC.zip)\r\nFrench                     | M_AILabs   | TacotronDDC       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--fr--mai--tacotron2-DDC.zip)\r\nDutch                      | MAI        | TacotronDDC       | tts       | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--nl--mai--tacotron2-DDC.zip)\r\nEnglish                    | EK1        | WaveGrad          | vocoder   | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/vocoder_models--en--ek1--wavegrad.zip)\r\nDutch                      | MAI        | ParallelWaveGAN   | vocoder   | v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/vocoder_models--nl--mai--parallel-wavegan.zip)\r\nEnglish                    | LJSpeech   | MB-MelGAN         | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--en--ljspeech--mulitband-melgan.zip)\r\n:earth_africa:  Multi-Lang | LibriTTS   | FullBand-MelGAN   | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--fullband-melgan.zip)\r\n:earth_africa:  Multi-Lang | LibriTTS   | WaveGrad          | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--wavegrad.zip)",
        "dateCreated": "2021-04-02T14:42:37Z",
        "datePublished": "2021-04-02T14:43:22Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.0.11",
        "name": "v0.0.11",
        "tag_name": "v0.0.11",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.0.11",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/40534497",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.0.11"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# :frog: v0.0.10\r\n## \ud83d\udc1eBug Fixes\r\n- [x] Make ```synthesizer.py``` saving the output audio with the vocoder sampling rate. It is necessary if there is sampling rates of the tts and the vocoder models are different and interpolation is applied to the tts model output before running the vocoder. Practically, it fixes generated Spanish and French voices by ```tts``` or ```tts-server``` on the terminal. \r\n- [x] Handling utf-8 on Windows. (by @adonispujols)\r\n- [x] Fix Loading the last model when ```--continue_training```.  It was loading the best_model regardless. \r\n\r\n## \ud83d\udcbe Code updates\r\n- [x] **Breaking Change:** Update default set of characters in ```symbols.py```. This might require you to set your character set in ```config.json ``` if you like to use this version with your models trained with the previous version. \r\n- [x] **Chinese** backend for text processing (#654  by @kirianguiller)\r\n- [x] Enable torch.hub integration for the released models.\r\n- [x] First github release. \r\n- [x] dep. version fixes. Using numpy > 1.17.5 breaks some tests. \r\n- [x] WaveRNN fix (by @gerazov )\r\n- [x] Big refactoring for the training scripts to share the init part of the code. (by @gerazov)\r\n- [x] Enable ModelManager to download models from Github releases.\r\n- [x] Add a test for ```compute_statistics.py```\r\n- [x] light-touch updates in ```tts``` and ```tts-server``` entry points. (thanks @thorstenMueller )\r\n- [x] Define default vocoder models for each tts model in ```.models.json```.  ```tts``` and ```tts-server``` entry points use the default vocoder if the user does not specify. \r\n- [x] ```find_unique_chars.py``` to find all the unique characters in a dataset.\r\n- [x] A better way to handling best models through training. (thx @gerazov )\r\n- [x] pass used characters to the model config.json at the beginning of the training. This prevents any code update later to affect the trained models. \r\n- [x] Migration to Github Actions for CI.\r\n- [x] Deprecate wheel based use of tts-server for the sake of the new design.\r\n- [x] :frog:\r\n\r\n## :walking_woman:  Operational Updates\r\n- [x] Move released models to Github Releases and deprecate GDrive being the first option.  \r\n\r\n## \ud83c\udfc5 Model implementations\r\n - No updates \ud83d\ude13\r\n\r\n## \ud83d\ude80 New Pre-Trained Model Releases\r\n- [x] **English** ek1 - Tacotron2 model and WaveGrad vocoder under ```.models.json```.  (huge THX!! to @nmstoker) \r\n- [x] **Russian** Ruslan - Tacotron2-DDC model.\r\n- [x] Dutch model.  (huge THX!! to @r-dh )\r\n- [x] **Chinese** Tacotron2 model. (huge THX!! to @kirianguiller)\r\n- [x] **English** LJSpeech - SpeechSpeech with WaveNet decoder. \r\n\r\n\r\n# Released Models\r\n:bulb:  All the models below are available by ```tts``` end point as explained [here](https://github.com/coqui-ai/TTS#example-synthesizing-speech-on-terminal-using-the-released-models).\r\n\r\n\r\n| Language | Dataset | Model Name | Model Type | TTS version | Download|\r\n| -- | -- | -- | -- | -- | :-: |\r\nEnglish                    | LJSpeech   | SpeedySpeech      | tts       |:smiley: v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--en--ljspeech--speedy-speech-wn.zip)\r\nEnglish                    | EK1        | Tacotron2         | tts       |:smiley: v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--en--ek1--tacotron2.zip)\r\nDutch                      | MAI        | TacotronDDC       | tts       |:smiley: v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--nl--mai--tacotron2-DDC.zip)\r\nChinese                    | Baker      | TacotronDDC-GST   | tts       |:smiley: v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--zh-CN--baker--tacotron2-DDC-GST.zip)\r\nEnglish                    | LJSpeech   | TacotronDCA       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--tacotron2-DCA.zip)\r\nEnglish                    | LJSpeech   | Glow-TTS          | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--glow-tts.zip)\r\nSpanish                    | M-AILabs   | TacotronDDC       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--es--mai--tacotron2-DDC.zip)\r\nFrench                     | M_AILabs   | TacotronDDC       | tts       |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--fr--mai--tacotron2-DDC.zip)\r\nDutch                      | MAI        | TacotronDDC       | tts       |:smiley: v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/tts_models--nl--mai--tacotron2-DDC.zip)\r\nEnglish                    | EK1        | WaveGrad          | vocoder   |:smiley: v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/vocoder_models--en--ek1--wavegrad.zip)\r\nDutch                      | MAI        | ParallelWaveGAN   | vocoder   |:smiley: v0.0.10   |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.10/vocoder_models--nl--mai--parallel-wavegan.zip)\r\nEnglish                    | LJSpeech   | MB-MelGAN         | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--en--ljspeech--mulitband-melgan.zip)\r\n:earth_africa:  Multi-Lang | LibriTTS   | FullBand-MelGAN   | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--fullband-melgan.zip)\r\n:earth_africa:  Multi-Lang | LibriTTS   | WaveGrad          | vocoder   |v0.0.9             |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--wavegrad.zip)",
        "dateCreated": "2021-03-10T17:33:42Z",
        "datePublished": "2021-03-10T17:11:11Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.0.10",
        "name": "v0.0.10",
        "tag_name": "v0.0.10",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.0.10",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/39596734",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.0.10"
      },
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "# :frog: TTS v0.0.9 - the first release :tada: \r\n\r\nThis is the first and v0.0.9 release of :frog:TTS.\r\n:frog:TTS is still an evolving project and any upcoming release might be significantly different and not backward compatible.\r\n\r\nIn this release, we provide the following models.\r\n\r\nLanguage | Dataset | Model Name | Model Type | Download\r\n-- | -- | -- | -- | --\r\nEnglish | LJSpeech | TacotronDCA | tts |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--tacotron2-DCA.zip)\r\nEnglish | LJSpeech | Glow-TTS | tts |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--glow-tts.zip)\r\nSpanish | M-AILabs | TacotronDDC | tts |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--es--mai--tacotron2-DDC.zip)\r\nFrench | M_AILabs | TacotronDDC | tts |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/tts_models--fr--mai--tacotron2-DDC.zip)\r\nEnglish | LJSpeech | MB-MelGAN | vocoder |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--en--ljspeech--mulitband-melgan.zip)\r\n:earth_africa:  Multi-Lang | LibriTTS | FullBand-MelGAN | vocoder|[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--fullband-melgan.zip)\r\n:earth_africa:  Multi-Lang | LibriTTS | WaveGrad | vocoder |[:floppy_disk:](https://github.com/coqui-ai/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--wavegrad.zip)\r\n\r\n# Notes\r\n- Multi-Lang vocoder models are intended for non-English models.\r\n- Vocoder models are independently trained from the tts models with possibly different sampling rates. Therefore, the performance is not optimal.\r\n- All models are trained with phonemes generated by espeak back-end (not espeak-ng).\r\n- This release has been tested under Python 3.6, 3.7, and 3.8. It is strongly suggested to use ```conda``` to install the dependencies and set up the  environment.\r\n\r\n# Edit:\r\n(22.03.2021) - Fullband Universal Vocoder is corrected with the right model files. Previously, we released the wrong model with that name. ",
        "dateCreated": "2021-03-09T15:39:17Z",
        "datePublished": "2021-03-09T15:51:32Z",
        "html_url": "https://github.com/coqui-ai/TTS/releases/tag/v0.0.9",
        "name": "v0.0.9 ",
        "tag_name": "v0.0.9",
        "tarball_url": "https://api.github.com/repos/coqui-ai/TTS/tarball/v0.0.9",
        "url": "https://api.github.com/repos/coqui-ai/TTS/releases/39520482",
        "zipball_url": "https://api.github.com/repos/coqui-ai/TTS/zipball/v0.0.9"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2947,
      "date": "Fri, 24 Dec 2021 07:58:19 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "python",
      "text-to-speech",
      "deep-learning",
      "speech",
      "pytorch",
      "tts",
      "vocoder",
      "tacotron",
      "glow-tts",
      "melgan",
      "speaker-encoder",
      "tensorflow2",
      "align-tts",
      "hifigan",
      "melgan-stft",
      "speaker-encodings",
      "multi-speaker-tts",
      "tts-model"
    ],
    "technique": "GitHub API"
  }
}