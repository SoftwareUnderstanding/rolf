{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Pre-trained Transformer network for satellite image time series classification\nThis code is supporting by a paper published in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing.\nYou can find the paper at: https://ieeexplore.ieee.org/document/9252123.\n**NOTE: If you use our work, please also cite the paper:\n\n```\n@ARTICLE{9252123,\n  author={Y. {Yuan} and L. {Lin}},\n  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, \n  title={Self-Supervised Pre-Training of Transformers for Satellite Image Time Series Classification}, \n  year={2020},\n  volume={},\n  number={},\n  pages={1-14},\n  doi={10.1109/JSTARS.2020.3036602}}\n\n```\n\nCitation: Y. Yuan and L. Lin, \"Self-Supervised Pre-Training of Transformers for Satellite Image Time Series Classification,\" \nin IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, doi: 10.1109/JSTARS.2020.3036602.\n\nIf you would like to get in touch, please contact yuanyuan@njupt.edu.cn, linlei1214@163.com.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --num_features 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --warmup_epochs 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --num_features 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --num_features 10 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/linlei1214/SITS-BERT",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Pre-trained Transformer network for satellite image time series classification\nThis code is supporting by a paper published in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing.\nYou can find the paper at: https://ieeexplore.ieee.org/document/9252123.\n**NOTE: If you use our work, please also cite the paper:\n\n```\n@ARTICLE{9252123,\n  author={Y. {Yuan} and L. {Lin}},\n  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, \n  title={Self-Supervised Pre-Training of Transformers for Satellite Image Time Series Classification}, \n  year={2020},\n  volume={},\n  number={},\n  pages={1-14},\n  doi={10.1109/JSTARS.2020.3036602}}\n\n```\n\nCitation: Y. Yuan and L. Lin, \"Self-Supervised Pre-Training of Transformers for Satellite Image Time Series Classification,\" \nin IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, doi: 10.1109/JSTARS.2020.3036602.\n\nIf you would like to get in touch, please contact yuanyuan@njupt.edu.cn, linlei1214@163.com.\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-15T07:38:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-16T22:02:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9864857960357934
      ],
      "excerpt": "This repository provides a PyTorch implementation of the pre-training scheme presented in our paper  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.82452456425627,
        0.9657502792307384,
        0.870802657620756,
        0.9542177413299703
      ],
      "excerpt": "Satellite image time series (SITS) classification is a major research topic in remote sensing \nand is relevant for a wide range of applications. Deep learning approaches have been commonly \nemployed for SITS classification and have provided state-of-the-art performance. However, deep \nlearning methods suffer from overfitting when labeled data is scarce. To address this problem, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9749991635300338,
        0.9531885135629724,
        0.9740224309334855,
        0.8239337698420649
      ],
      "excerpt": "by utilizing large-scale unlabeled data. In detail, the model is asked to predict randomly  \ncontaminated observations given an entire time series of a pixel. The main idea of our proposal \nis to leverage the inherent temporal structure of satellite time series to learn general-purpose \nspectral-temporal representations related to land cover semantics. Once pre-training is completed, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9632812571971004
      ],
      "excerpt": "all the model parameters on small-scale task-related labeled data. In this way, the general knowledge \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9636164522082441
      ],
      "excerpt": "generalization performance of the model as well as reducing the risk of overfitting. Comprehensive \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550861386592058
      ],
      "excerpt": "results demonstrate the effectiveness of the proposed pre-training scheme, leading to substantial  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828347732102113
      ],
      "excerpt": "(CNN-1D), and Bidirectional Long Short-Term Memory (Bi-LSTM) network. The code and the pre-trained  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8320759084521284
      ],
      "excerpt": "can automatically split the data into training/validation sets according to the given valid_rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8253554474518527
      ],
      "excerpt": "We have provided the pre-trained model parameters for SITS-BERT, which are available in  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8505527801095067,
        0.892262191021058
      ],
      "excerpt": "The file finetuning.py is the main code for fine-tuning a pre-trained SITS-BERT model and using  \nthe fine-tuned model for satellite time series classification. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    --file_path '../data/California-Labeled/' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    --file_path '../data/California-Labeled/' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9773992690729741,
        0.8449130008689172,
        0.9312887985999561
      ],
      "excerpt": "This implementation is based on the repository https://github.com/codertimo/BERT-pytorch, which is a Pytorch \nimplementation of Google AI's 2018 BERT: \n[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]  \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/linlei1214/SITS-BERT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Wed, 22 Dec 2021 11:06:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/linlei1214/SITS-BERT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "linlei1214/SITS-BERT",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/linlei1214/SITS-BERT/master/code/run_finetuning_unpretrain.sh",
      "https://raw.githubusercontent.com/linlei1214/SITS-BERT/master/code/run_pretraining.sh",
      "https://raw.githubusercontent.com/linlei1214/SITS-BERT/master/code/run_finetuning.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All the Sentinel-2 images (Level-1C) we used were downloaded from the United States Geological Survey\n(USGS) EarthExplorer website and preprocessed to Bottom-Of-Atmosphere (BOA) reflectance Level-2A using\nthe Sen2Cor plugin v2.8 and the Sentinel Application Platform (SNAP 7.0). The Multispectral Instrument\n(MSI) sensor provides 13 spectral bands, i.e., four bands at 10-m (Blue, Green, Red, NIR), six bands at\n20-m (Vegetation Red Edge 1-3, Narrow NIR, SWIR 1-2), and three atmospheric bands at 60-m spatial resolution.\nWith the exception of the atmospheric bands, all 10 bands (i.e., Sentinel band-2,3,4,5,6,7,8,8A,11,12) \nwere used in this study. Bands at 20-m resolution were resampled to 10-m via nearest sampling. A scene \nclassification map was generated for each image along with the Level-2A processing, which assigned pixels\nto clouds, cloud shadows, vegetation, soils/deserts, water, snow, etc. According to the scene classification\nmap, low-quality observations belonging to clouds (including cirrus), cloud-shadows, and snow were discarded\nwhen extracting the annual time series of each pixel.\n\nTime series data is organized in *CSV* format. Each row of a csv file corresponds to an annual satellite observation \ntime series of a pixel. For each row, the storage order is [[observation data], [DOY data], [class label(optional)]],\n where [observation data] is stored in the order of bands:\n\nA pre-training time series is recorded like this:\n\n<div align=\"center\">\n  <img src=\"fig/pretraining-data-organization.png\" width=\"400\"><br><br>\n</div>\n\nA fine-tuning time series is recorded like this:\n\n<div align=\"center\">\n  <img src=\"fig/finetuning-data-organization.png\" width=\"400\"><br><br>\n</div>\n\nNote: [DOY data] record the acquisition Julian dates of the observations in a time series. \n[class label] is not included in the pre-training data.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8668046336460936
      ],
      "excerpt": "You can run the following Linux command for pre-training a SITS-BERT model on your own data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.97727274704482
      ],
      "excerpt": "You can run the following Linux command to run the experiment: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9116428653133316
      ],
      "excerpt": "You can also run the following command to train a SITS-BERT model from scratch for comparison: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8205050868242326
      ],
      "excerpt": "This implementation is based on the repository https://github.com/codertimo/BERT-pytorch, which is a Pytorch \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8877006939393441,
        0.8313391876513724
      ],
      "excerpt": "The file pretraining.py is the main code for pre-training a SITS-BERT model. \nAll unlabeled time series are stored in a single csv file (e.g., Pre-Training-Data.csv), and the program  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411624709648624,
        0.846095565235764
      ],
      "excerpt": "python pretraining.py \\ \n    --dataset_path '../data/California-Unlabeled/Pre-Training-Data.csv' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8386436006895361
      ],
      "excerpt": "The file finetuning.py is the main code for fine-tuning a pre-trained SITS-BERT model and using  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8522737708010661
      ],
      "excerpt": "Download the sample dataset from Google Driver or BaiduYunPan, password: kpud \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python finetuning.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044290599680651
      ],
      "excerpt": "    --batch_size 128 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python finetuning.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044290599680651
      ],
      "excerpt": "    --batch_size 128 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/linlei1214/SITS-BERT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SITS-BERT",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SITS-BERT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "linlei1214",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/linlei1214/SITS-BERT/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "+ tqdm = 4.48.2\n+ numpy = 1.19.1\n+ python = 3.6.10\n+ pytorch = 1.6.0\n+ tensorboard = 2.3.0\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Wed, 22 Dec 2021 11:06:26 GMT"
    },
    "technique": "GitHub API"
  }
}