{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.03385",
      "https://arxiv.org/abs/1605.07146"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{pleiss2021limitations,\n  title={The Limitations of Large Width in Neural Networks: A Deep {G}aussian Process Perspective},\n  author={Pleiss, Geoff and Cunningham, John P},\n  booktitle={Neural Information Processing Systems},\n  year={2021}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gpleiss/limits_of_large_width",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-16T18:26:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-08T23:25:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9750562595416441,
        0.952821757893431,
        0.8910455036503893
      ],
      "excerpt": "This repo contains the code to reproduce the experiments in Section 6 of \"The Limitations of Large Width in Neural Networks: A Deep Gaussian Process Perspective\" (NeurIPS, 2021). To cite: \nThe purpose of these experiments is \nFor all the experiments in Section 6.1, we perform a 2 (or 3) step pipeline. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183191943828515
      ],
      "excerpt": "and then we perform NUTS sampling of the Deep GP and NN posteriors at various widths. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.876674417279697
      ],
      "excerpt": "We will use these models to get the hyperparameters for the deeper models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9677232736878852
      ],
      "excerpt": "(The argument to initialize should be VI_SAVE_NAME for Deep GP models, and LIM_GP_SAVE_NAME for NN.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9594718891630619
      ],
      "excerpt": "It is up to the user to find the model that produces the best hyperparameters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9942443389069039,
        0.8495197590241769,
        0.8974423439515684
      ],
      "excerpt": "std corresponds to the standard deviation of the Gaussian prior on the parameters. It is inversely proportional to the L2 regularization coefficient. \nFor ResNet models, we use the standard hyperparameters defined in the original paper. \nThese hyperparameters have been shown to work well on both narrow and wide models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code to reproduce the experiments from our NeurIPS 2021 paper \" The Limitations of Large Width in Neural Networks: A Deep Gaussian Process Perspective\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gpleiss/limits_of_large_width/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 20:50:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gpleiss/limits_of_large_width/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "gpleiss/limits_of_large_width",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8563787196752052
      ],
      "excerpt": "We will use these models to get the hyperparameters for the deeper models. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8830510505135104
      ],
      "excerpt": "- train - test - kernel_fit - done \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8830510505135104
      ],
      "excerpt": "- train - test - kernel_fit - done \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8390394868641832
      ],
      "excerpt": "python multi_runner_mnist.py --n 50000 --width ${WIDTH} --std ${STD} --seed ${SEED} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8830510505135104
      ],
      "excerpt": "- train - test - done \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gpleiss/limits_of_large_width/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "The Limitations of Large Width in Neural Networks: A Deep Gaussian Process Perspective",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "limits_of_large_width",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "gpleiss",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gpleiss/limits_of_large_width/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- pytorch (>= 1.8.1)\n- gpytorch (>= 1.5)\n- tqdm\n- pandas\n- scipy\n- fire\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In this intermediate step, we fit a Deep GP to the data using SVI.\nWe will use this approximate posterior as initialization for the NUTS sampler.\n\n```sh\npython runner.py new --save <VI_SAVE_NAME> --data <PATH_TO_DATA_DIR> --dataset <DATASET> --model deep_gp --n 1000 \\\n- --width <WIDTH> \\\n- initialize <LIM_GP_SAVE_NAME> \\\n- train - test - kernel_fit - done\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Fri, 24 Dec 2021 20:50:19 GMT"
    },
    "technique": "GitHub API"
  }
}