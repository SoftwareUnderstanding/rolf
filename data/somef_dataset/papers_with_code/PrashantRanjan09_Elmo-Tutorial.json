{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.05365.\nThis tutorial can help in using:\n\n* **Pre Trained Elmo Model**  - refer _Elmo_tutorial.ipynb_ <br>\n* **Training an Elmo Model on your new data from scratch** <br>\n\nTo train and evaluate a biLM, you need to provide:\n   * a vocabulary file \n   * a set of training files \n   * a set of heldout files \n\nThe vocabulary file is a text file with one token per line. It must also include the special tokens , and\nThe vocabulary file should be sorted in descending order by token count in your training data. The first three entries/lines should be the special tokens : <br>\n`<S>` , <br>\n`</S>`  and <br>\n`<UNK>`.<br>\n\nThe training data should be randomly split into many training files, each containing one slice of the data. Each file contains pre-tokenized and white space separated text, one sentence per line. \n\n**Don't include the `<S>` or `</S>` tokens in your training data.**\n\nOnce done, git clone **https://github.com/allenai/bilm-tf.git**\nand run:\n\n    python bin/train_elmo.py --train_prefix= <path to training folder> --vocab_file <path to vocab file> --save_dir <path where models will be checkpointed>\n\nTo get the weights file, \nrun:\n\n    python bin/dump_weights.py --save_dir /output_path/to/checkpoint --outfile/output_path/to/weights.hdf5\n\nIn the save dir, one options.json will be dumped and above command will give you a weights file required to create an Elmo model (options file and the weights file"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "if __name__ == '__main__': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.9030859728368266
      ],
      "excerpt": " 'all_clip_norm_val': 10.0, \n 'n_epochs': 10, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrashantRanjan09/Elmo-Tutorial",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-06T08:51:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T06:22:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9438872695379521
      ],
      "excerpt": "This is a short tutorial on using Deep contextualized word representations (ELMo) which is discussed in the paper https://arxiv.org/abs/1802.05365. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9291078733062821
      ],
      "excerpt": "Training an Elmo Model on your new data from scratch <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8606470911263242
      ],
      "excerpt": "To incrementally train an existing model with new data <br>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9475621353371637
      ],
      "excerpt": "The code reads the checkpointed file and reads all the current variables in the graph and excludes the layers mentioned in the exclude variable, restores rest of the variables along with the associated weights. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9508530841742504
      ],
      "excerpt": "Visualization of the word vectors using Elmo: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A short tutorial on Elmo training (Pre trained, Training on new data, Incremental training)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrashantRanjan09/Elmo-Tutorial/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 37,
      "date": "Tue, 28 Dec 2021 20:24:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PrashantRanjan09/Elmo-Tutorial/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "PrashantRanjan09/Elmo-Tutorial",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/PrashantRanjan09/Elmo-Tutorial/master/Elmo_tutorial.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9742953936471677
      ],
      "excerpt": "Once done, git clone https://github.com/allenai/bilm-tf.git \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9835569693521744
      ],
      "excerpt": "git clone https://github.com/allenai/bilm-tf.git \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8621882743635256
      ],
      "excerpt": "   * a vocabulary file  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.864596614586109,
        0.8315708857448637
      ],
      "excerpt": "The vocabulary file is a text file with one token per line. It must also include the special tokens , and \nThe vocabulary file should be sorted in descending order by token count in your training data. The first three entries/lines should be the special tokens : <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910729993255943
      ],
      "excerpt": "The training data should be randomly split into many training files, each containing one slice of the data. Each file contains pre-tokenized and white space separated text, one sentence per line.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.869397989825391,
        0.8632084284323602
      ],
      "excerpt": "python bin/dump_weights.py --save_dir /output_path/to/checkpoint --outfile/output_path/to/weights.hdf5 \nIn the save dir, one options.json will be dumped and above command will give you a weights file required to create an Elmo model (options file and the weights file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9049351240313146
      ],
      "excerpt": "train(options, data, n_gpus, tf_save_dir, tf_log_dir,restart_ckpt_file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.836395170934972
      ],
      "excerpt": "parser.add_argument('--vocab_file', help='Vocabulary file') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144754925200841
      ],
      "excerpt": "parser.add_argument('--restart_ckpt_file', help='latest checkpoint file to start with') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8644677555981636,
        0.8484386800340059
      ],
      "excerpt": "replace training.py within allenai/bilm-tf/bilm/ with training_updated.py provided at home. \nAlso, make sure to put your embedding layer name in line 758 in training_updated.py : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8691521349895033
      ],
      "excerpt": "        reader = tf.train.NewCheckpointReader(your_checkpoint_file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8936954105699045,
        0.8936954105699045
      ],
      "excerpt": "        variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude) \n        loader = tf.train.Saver(variables_to_restore) \n        #loader = tf.train.Saver() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8379958252064681,
        0.8832706637002766
      ],
      "excerpt": "        with open(os.path.join(tf_save_dir, 'options.json'), 'w') as fout: \n            fout.write(json.dumps(options)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8393538756525978
      ],
      "excerpt": "For training run:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044290599680651
      ],
      "excerpt": "batch_size = 128 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": " 'bidirectional': True, \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PrashantRanjan09/Elmo-Tutorial/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Elmo-Tutorial",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Elmo-Tutorial",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "PrashantRanjan09",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrashantRanjan09/Elmo-Tutorial/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 148,
      "date": "Tue, 28 Dec 2021 20:24:01 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "elmo",
      "word-embeddings",
      "word-vectors",
      "allennlp",
      "allen",
      "tutorial",
      "elmo-tutorial"
    ],
    "technique": "GitHub API"
  }
}