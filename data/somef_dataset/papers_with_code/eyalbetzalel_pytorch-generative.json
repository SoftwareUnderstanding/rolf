{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1606.05328\n1. http://www.scottreed.info/files/iclr2017.pdf\n1. https://arxiv.org/abs/1502.03509 \n1. http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf\n\n### Neural Style Transfer\nBlog: https://towardsdatascience.com/how-to-get-beautiful-results-with-neural-style-transfer-75d0c05d6489 <br>\nNotebook: https://github.com/EugenHotaj/pytorch-generative/blob/master/notebooks/style_transfer.ipynb <br>\nPaper: https://arxiv.org/pdf/1508.06576.pdf\n\n### Compositional Pattern Producing Networks\nNotebook: https://github.com/EugenHotaj/pytorch-generative/blob/master/notebooks/cppn.ipynb <br>\nBackground: https://en.wikipedia.org/wiki/Compositional_pattern-producing_network",
      "https://arxiv.org/abs/1502.03509 \n1. http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf\n\n### Neural Style Transfer\nBlog: https://towardsdatascience.com/how-to-get-beautiful-results-with-neural-style-transfer-75d0c05d6489 <br>\nNotebook: https://github.com/EugenHotaj/pytorch-generative/blob/master/notebooks/style_transfer.ipynb <br>\nPaper: https://arxiv.org/pdf/1508.06576.pdf\n\n### Compositional Pattern Producing Networks\nNotebook: https://github.com/EugenHotaj/pytorch-generative/blob/master/notebooks/cppn.ipynb <br>\nBackground: https://en.wikipedia.org/wiki/Compositional_pattern-producing_network"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. https://arxiv.org/pdf/1601.06759.pdf \n1. https://arxiv.org/abs/1606.05328\n1. http://www.scottreed.info/files/iclr2017.pdf\n1. https://arxiv.org/abs/1502.03509 \n1. http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "Paper: https://arxiv.org/pdf/1508.06576.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eyalbetzalel/pytorch-generative",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-19T17:39:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-19T20:19:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9638127304037866,
        0.9449296890364416,
        0.9725252947899514,
        0.9292017784390072,
        0.9085647610524826,
        0.8087088689817594
      ],
      "excerpt": "pytorch-generative is a nascent project that aims to provide a simple, easy to use library for generative modeling in PyTorch.  \nThe library makes generative model implementation and experimentation easier by abstracting common building blocks such as MaskedConv2d and MaskedAttention. \nIt also provides clean, high quality reference implementations of recent State of the Art papers that are easy to read, understand, and extend.  \nFinally, it provides utilities for training, debugging, and working with Google Colab. \nSo far, the library has primarily focues on Autoregressive modeling. The future goal is to also expand into VAEs, GANS, Flows, etc. \npytorch-generative supports the following algorithms.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811266144932007,
        0.8638454796813093
      ],
      "excerpt": "Binary MNIST (NLL):  \n| Algorithm | Our Results | Best Other Results | Links | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9929228658699591,
        0.8534103359519767
      ],
      "excerpt": "Note: Our reported binary MNIST results may be optimistic. Instead of using a fixed dataset, we resample a new binary MNIST dataset on every epoch. We can think of this as using data augmentation which helps our models learn better. \nBlog: https://towardsdatascience.com/how-to-get-beautiful-results-with-neural-style-transfer-75d0c05d6489 <br> \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eyalbetzalel/pytorch-generative/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 23:39:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/eyalbetzalel/pytorch-generative/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "eyalbetzalel/pytorch-generative",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/eyalbetzalel/pytorch-generative/master/notebooks/MADE.ipynb",
      "https://raw.githubusercontent.com/eyalbetzalel/pytorch-generative/master/notebooks/cppn.ipynb",
      "https://raw.githubusercontent.com/eyalbetzalel/pytorch-generative/master/notebooks/pixel_snail.ipynb",
      "https://raw.githubusercontent.com/eyalbetzalel/pytorch-generative/master/notebooks/pixel_cnn.ipynb",
      "https://raw.githubusercontent.com/eyalbetzalel/pytorch-generative/master/notebooks/style_transfer.ipynb",
      "https://raw.githubusercontent.com/eyalbetzalel/pytorch-generative/master/notebooks/NADE.ipynb",
      "https://raw.githubusercontent.com/eyalbetzalel/pytorch-generative/master/notebooks/gated_pixel_cnn.ipynb",
      "https://raw.githubusercontent.com/eyalbetzalel/pytorch-generative/master/notebooks/image_gpt.ipynb",
      "https://raw.githubusercontent.com/eyalbetzalel/pytorch-generative/master/notebooks/bag_of_logistics.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8107238751719249
      ],
      "excerpt": "pytorch-generative supports the following algorithms.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9216475586766066
      ],
      "excerpt": "Notebook: https://github.com/EugenHotaj/pytorch-generative/blob/master/notebooks/style_transfer.ipynb <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9216475586766066
      ],
      "excerpt": "Notebook: https://github.com/EugenHotaj/pytorch-generative/blob/master/notebooks/cppn.ipynb <br> \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8345687630277501
      ],
      "excerpt": "| PixelCNN | 81.45 | 81.30 [1] | Code, Notebook | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/eyalbetzalel/pytorch-generative/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Eugen Hotaj\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-generative",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-generative",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "eyalbetzalel",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eyalbetzalel/pytorch-generative/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 23:39:22 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Supported models are implemented as PyTorch Modules and are easy to use:\n\n```python\nfrom pytorch_generative import models\n\nmodel = models.ImageGPT(in_channels=1, in_size=28)\n...\nmodel(data)\n```\n\nAlternatively, lower level building blocks in [pytorch_generative.nn](https://github.com/EugenHotaj/pytorch-generative/blob/master/pytorch_generative/nn.py) can be used to write models from scratch. For example, we implement a convolutional [ImageGPT](https://openai.com/blog/image-gpt/)-like model below:\n\n```python\n\nfrom torch import nn\n\nfrom pytorch_generative import nn as pg_nn\n\n\nclass TransformerBlock(nn.Module):\n  \"\"\"An ImageGPT Transformer block.\"\"\"\n\n  def __init__(self, \n               n_channels, \n               n_attention_heads):\n    \"\"\"Initializes a new TransformerBlock instance.\n    \n    Args:\n      n_channels: The number of input and output channels.\n      n_attention_heads: The number of attention heads to use.\n    \"\"\"\n    super().__init__()\n    self._attn = pg_nn.MaskedAttention(\n        in_channels=n_channels,\n        embed_channels=n_channels,\n        out_channels=n_channels,\n        n_heads=n_attention_heads,\n        is_causal=False)\n    self._out = nn.Sequential(\n        nn.Conv2d(\n            in_channels=n_channels, \n            out_channels=4*n_channels, \n            kernel_size=1),\n        nn.GELU(),\n        nn.Conv2d(\n            in_channels=4*n_channels, \n            out_channels=n_channels, \n            kernel_size=1))\n\n  def forward(self, x):\n    x = x + self._attn(x)\n    return x + self._out(x)\n\n\nclass ImageGPT(nn.Module):\n  \"\"\"The ImageGPT Model.\n  \n  Note that we don't use LayerNorm because it would break the model's \n  autoregressive property.\n  \"\"\"\n  \n  def __init__(self,       \n               in_channels,\n               in_size,\n               n_transformer_blocks=8,\n               n_attention_heads=4,\n               n_embedding_channels=16):\n    \"\"\"Initializes a new ImageGPT instance.\n    \n    Args:\n      in_channels: The number of input channels.\n      in_size: Size of the input images. Used to create positional encodings.\n      n_transformer_blocks: Number of TransformerBlocks to use.\n      n_attention_heads: Number of attention heads to use.\n      n_embedding_channels: Number of attention embedding channels to use.\n    \"\"\"\n    super().__init__()\n    self._pos = nn.Parameter(torch.zeros(1, in_channels, in_size, in_size))\n    self._input = pg_nn.MaskedConv2d(\n        is_causal=True,\n        in_channels=in_channels,\n        out_channels=n_embedding_channels,\n        kernel_size=3,\n        padding=1)\n    self._transformer = nn.Sequential(\n        *[TransformerBlock(n_channels=n_embedding_channels,\n                         n_attention_heads=n_attention_heads)\n          for _ in range(n_transformer_blocks)])\n    self._out = nn.Conv2d(in_channels=n_embedding_channels,\n                          out_channels=in_channels,\n                          kernel_size=1)\n\n  def forward(self, x):\n    x = self._input(x + self._pos)\n    x = self._transformer(x)\n    return self._out(x)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}