{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{liu2021swin,\n      title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, \n      author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},\n      year={2021},\n      eprint={2103.14030},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Some part of the code is adapted from the PyTorch - VisionTransformer repository [https://github.com/lucidrains/vit-pytorch](https://github.com/lucidrains/vit-pytorch) ,\nwhich provides a very clean VisionTransformer implementation to start with.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{liu2021swin,\n      title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, \n      author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},\n      year={2021},\n      eprint={2103.14030},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999776133895221
      ],
      "excerpt": "All credits go to the authors Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin and Baining Guo. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/berniwal/swin-transformer-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-27T18:33:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T13:16:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.99411758590752,
        0.9837612869496969
      ],
      "excerpt": "Implementation of the Swin Transformer architecture. This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. \nThis is NOT the official repository of the Swin Transformer. At the moment in time the official code of the authors is not available yet but can be found later at: https://github.com/microsoft/Swin-Transformer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "heads: 4-tuple of ints  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9296709628432834
      ],
      "excerpt": "Number of channels of the input.     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "downscaling_factors: 4-tuple of ints. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of the Swin Transformer in PyTorch.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/berniwal/swin-transformer-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 74,
      "date": "Sun, 26 Dec 2021 13:26:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/berniwal/swin-transformer-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "berniwal/swin-transformer-pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install swin-transformer-pytorch\n```\n\nor (if you clone the repository)\n\n```bash\n$ pip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8047467390994424
      ],
      "excerpt": "Num classes the output should have.     \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/berniwal/swin-transformer-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Bernhard Walser\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Swin Transformer - PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "swin-transformer-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "berniwal",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/berniwal/swin-transformer-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "berniwal",
        "body": "Added Relative Positional Bias",
        "dateCreated": "2021-03-29T17:55:13Z",
        "datePublished": "2021-03-29T17:56:01Z",
        "html_url": "https://github.com/berniwal/swin-transformer-pytorch/releases/tag/0.4",
        "name": "Added Relative Positional Bias",
        "tag_name": "0.4",
        "tarball_url": "https://api.github.com/repos/berniwal/swin-transformer-pytorch/tarball/0.4",
        "url": "https://api.github.com/repos/berniwal/swin-transformer-pytorch/releases/40650447",
        "zipball_url": "https://api.github.com/repos/berniwal/swin-transformer-pytorch/zipball/0.4"
      },
      {
        "authorType": "User",
        "author_name": "berniwal",
        "body": "Initial Release",
        "dateCreated": "2021-03-28T11:42:27Z",
        "datePublished": "2021-03-28T11:47:04Z",
        "html_url": "https://github.com/berniwal/swin-transformer-pytorch/releases/tag/0.2",
        "name": "Initial Release",
        "tag_name": "0.2",
        "tarball_url": "https://api.github.com/repos/berniwal/swin-transformer-pytorch/tarball/0.2",
        "url": "https://api.github.com/repos/berniwal/swin-transformer-pytorch/releases/40596291",
        "zipball_url": "https://api.github.com/repos/berniwal/swin-transformer-pytorch/zipball/0.2"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 471,
      "date": "Sun, 26 Dec 2021 13:26:33 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "machine-learning",
      "pytorch",
      "artificial-intelligence",
      "transformer-pytorch",
      "transformer-architecture",
      "attention-model"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport torch\nfrom swin_transformer_pytorch import SwinTransformer\n\nnet = SwinTransformer(\n    hidden_dim=96,\n    layers=(2, 2, 6, 2),\n    heads=(3, 6, 12, 24),\n    channels=3,\n    num_classes=3,\n    head_dim=32,\n    window_size=7,\n    downscaling_factors=(4, 2, 2, 2),\n    relative_pos_embedding=True\n)\ndummy_x = torch.randn(1, 3, 224, 224)\nlogits = net(dummy_x)  #: (1,3)\nprint(net)\nprint(logits)\n```\n",
      "technique": "Header extraction"
    }
  ]
}