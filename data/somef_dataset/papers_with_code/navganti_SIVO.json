{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.02680, 2015. [PDF](http://arxiv.org/abs/1511.02680).\n\nVijay Badrinarayanan, Alex Kendall and Roberto Cipolla __\"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.\"__ PAMI, 2017. [PDF](http://arxiv.org/abs/1511.00561).\n\n## ORB_SLAM2\n\nSIVO's localization functionality builds upon ORB_SLAM2. ORB_SLAM2 is a real-time SLAM library for **Monocular**, **Stereo** and **RGB-D** cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the [KITTI dataset](http://www.cvlibs.net/datasets/kitti/eval_odometry.php) as stereo or monocular, in the [TUM dataset](http://vision.in.tum.de/data/datasets/rgbd-dataset) as RGB-D or monocular, and in the [EuRoC dataset](http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets) as stereo or monocular. ORB_SLAM2 also contains a ROS node to process live monocular, stereo or RGB-D streams. **The library can be compiled without ROS**. ORB-SLAM2 provides a GUI to change between a *SLAM Mode* and *Localization Mode*, see [here](#slam-and-localization-modes).\n\n### Example Videos\n<a href=\"https://www.youtube.com/embed/ufvPS5wJAx0\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/ufvPS5wJAx0/0.jpg\"\nalt=\"ORB-SLAM2\" width=\"240\" height=\"180\" border=\"10\" /></a>\n<a href=\"https://www.youtube.com/embed/T-9PYCKhDLM\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/T-9PYCKhDLM/0.jpg\"\nalt=\"ORB-SLAM2\" width=\"240\" height=\"180\" border=\"10\" /></a>\n<a href=\"https://www.youtube.com/embed/kPwy8yA4CKM\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/kPwy8yA4CKM/0.jpg\"\nalt=\"ORB-SLAM2\" width=\"240\" height=\"180\" border=\"10\" /></a>\n\n### Publications:\n\n[Monocular] Ra\u00fal Mur-Artal, J. M. M. Montiel and Juan D. Tard\u00f3s. **ORB-SLAM: A Versatile and Accurate Monocular SLAM System**. *IEEE Transactions on Robotics,* vol. 31, no. 5, pp. 1147-1163, 2015. (**2015 IEEE Transactions on Robotics Best Paper Award**). **[PDF](http://webdiis.unizar.es/~raulmur/MurMontielTardosTRO15.pdf)**.\n\n[Stereo and RGB-D] Ra\u00fal Mur-Artal and Juan D. Tard\u00f3s. **ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras**. *IEEE Transactions on Robotics,* vol. 33, no. 5, pp. 1255-1262, 2017. **[PDF](https://128.84.21.199/pdf/1610.06475.pdf)**.\n\n[DBoW2 Place Recognizer] Dorian G\u00e1lvez-L\u00f3pez and Juan D. Tard\u00f3s. **Bags of Binary Words for Fast Place Recognition in Image Sequences**. *IEEE Transactions on Robotics,* vol. 28, no. 5, pp.  1188-1197, 2012. **[PDF](http://doriangalvez.com/php/dl.php?dlp=GalvezTRO12.pdf)**\n\nIf you use ORB_SLAM2 (Monocular) in an academic work, please cite:\n\n    @article{murTRO2015,\n      title={{ORB-SLAM"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@mastersthesis{ganti2018SIVO,\nauthor={{Ganti, Pranav}},\ntitle={SIVO: Semantically Informed Visual Odometry and Mapping},\nyear={2018},\npublisher=\"UWSpace\",\nurl={http://hdl.handle.net/10012/14111}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{ganti2019network,\n  title={Network Uncertainty Informed Semantic Feature Selection for Visual SLAM},\n  author={Ganti, Pranav and Waslander, Steven},\n  booktitle={16th Conference on Computer and Robot Vision (CRV)},\n  pages={121--128},\n  year={2019},\n  organization={IEEE}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9736653811213918,
        0.9685775816097435
      ],
      "excerpt": "The paper can be found here. If you find this code useful, please cite the paper: \nIf you'd like to deep dive further into the theory, background, or methodology, please refer to my thesis. If you use refer to this document in your work, please cite it: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9818894004866677,
        0.9971447987282395
      ],
      "excerpt": "author={{Ganti, Pranav}}, \ntitle={SIVO: Semantically Informed Visual Odometry and Mapping}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9800277544866333
      ],
      "excerpt": "Alex Kendall, Vijay Badrinarayanan and Roberto Cipolla \"Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding.\" arXiv preprint arXiv:1511.02680, 2015. PDF. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999895615321573,
        0.9997301328784802,
        0.9997529647984453,
        0.9693283607360661,
        0.9778510933505252,
        0.9956306229412123,
        0.9977505718007624
      ],
      "excerpt": "[Monocular] Ra\u00fal Mur-Artal, J. M. M. Montiel and Juan D. Tard\u00f3s. ORB-SLAM: A Versatile and Accurate Monocular SLAM System. IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147-1163, 2015. (2015 IEEE Transactions on Robotics Best Paper Award). PDF. \n[Stereo and RGB-D] Ra\u00fal Mur-Artal and Juan D. Tard\u00f3s. ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras. IEEE Transactions on Robotics, vol. 33, no. 5, pp. 1255-1262, 2017. PDF. \n[DBoW2 Place Recognizer] Dorian G\u00e1lvez-L\u00f3pez and Juan D. Tard\u00f3s. Bags of Binary Words for Fast Place Recognition in Image Sequences. IEEE Transactions on Robotics, vol. 28, no. 5, pp.  1188-1197, 2012. PDF \nIf you use ORB_SLAM2 (Monocular) in an academic work, please cite: \n  title={{ORB-SLAM}: a Versatile and Accurate Monocular {SLAM} System}, \n  author={Mur-Artal, Ra\\'ul, Montiel, J. M. M. and Tard\\'os, Juan D.}, \n  journal={IEEE Transactions on Robotics}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923,
        0.9980335556020778,
        0.9864698047705643
      ],
      "excerpt": "  pages={1147--1163}, \n  doi = {10.1109/TRO.2015.2463671}, \n  year={2015} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.913398484315037,
        0.9742998729344013,
        0.9956306229412123,
        0.9977505718007624
      ],
      "excerpt": "if you use ORB_SLAM2 (Stereo or RGB-D) in an academic work, please cite: \n  title={{ORB-SLAM2}: an Open-Source {SLAM} System for Monocular, Stereo and {RGB-D} Cameras}, \n  author={Mur-Artal, Ra\\'ul and Tard\\'os, Juan D.}, \n  journal={IEEE Transactions on Robotics}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923,
        0.9994373684694372,
        0.9960965048981569
      ],
      "excerpt": "  pages={1255--1262}, \n  doi = {10.1109/TRO.2017.2705103}, \n  year={2017} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/navganti/SIVO",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-20T23:47:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T09:34:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9602397884510887,
        0.9943784362563437
      ],
      "excerpt": "SIVO is a novel feature selection method for visual SLAM which facilitates long-term localization. This algorithm enhances traditional feature detectors with deep learning based scene understanding using a Bayesian neural network, which provides context for visual SLAM while accounting for neural network uncertainty. \nOur method selects features which provide the highest reduction in Shannon entropy between the entropy of the current state and the joint entropy of the state, given the addition of the new feature with the classification entropy of the feature from a Bayesian neural network. This strategy generates a sparse map suitable for long-term localization, as each selected feature significantly reduces the uncertainty of the vehicle state and has been detected to be a static object (building, traffic sign, etc.) repeatedly with a high confidence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9455476221843399
      ],
      "excerpt": "This method builds on the work of Bayesian SegNet and ORB_SLAM2. Detailed background information can be found below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9028120394107837,
        0.9385309968668725
      ],
      "excerpt": "A powerful CPU (e.g. Intel i7), and a powerful GPU (e.g. NVIDIA TitanX) are required to provide more stable and accurate results. Due to the technique of approximating a Bayesian Neural Network by passing an image through the network several times, this network does not quite run in real time. \nThe thread and chrono functionalities of C++11 are required \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9169822427487686
      ],
      "excerpt": "Change PATH_TO_PROTOTXT to the full path to the desired Bayesian SegNet .prototxt file (e.g. basic/kitti/bayesian_segnet_basic_kitti.prototxt). Modify the input_dim value to be a batch size which fits on your GPU. Keep the image size in mind - for KITTI, we are resizing the images to be 352 x 1024 such that they work with the network dimensions. This resizing happens within the src/orbslam/System.cc file, and the resizing dimensions are taken from the .prototxt file.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481348381555034,
        0.9196752434217426,
        0.8219832704430858,
        0.8091667023122107,
        0.8681948822105795,
        0.8446436607261736
      ],
      "excerpt": "You can change between the SLAM and Localization modes using the GUI of the map viewer. \nSLAM Mode: This is the default mode. The system runs three threads in parallel : Tracking, Local Mapping and Loop Closing. The system localizes the camera, builds new map and tries to close loops. \nLocalization Mode: This mode can be used when you have a good map of your working area. In this mode the Local Mapping and Loop Closing are deactivated. The system localizes the camera in the map (which is no longer updated), using relocalization if needed. \nThis work uses the Bayesian SegNet architecture for semantic segmentation, created by Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla. \nBayesian SegNet is an extension of SegNet, a deep convolutional encoder-decoder architecture for image segmentation. This network extends SegNet by incorporating model uncertainty, implemented through dropout layers. \nFor more information about the SegNet architecture: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.98099988256351
      ],
      "excerpt": "SIVO's localization functionality builds upon ORB_SLAM2. ORB_SLAM2 is a real-time SLAM library for Monocular, Stereo and RGB-D cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset as stereo or monocular, in the TUM dataset as RGB-D or monocular, and in the EuRoC dataset as stereo or monocular. ORB_SLAM2 also contains a ROS node to process live monocular, stereo or RGB-D streams. The library can be compiled without ROS. ORB-SLAM2 provides a GUI to change between a SLAM Mode and Localization Mode, see here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8509914776310118
      ],
      "excerpt": "[DBoW2 Place Recognizer] Dorian G\u00e1lvez-L\u00f3pez and Juan D. Tard\u00f3s. Bags of Binary Words for Fast Place Recognition in Image Sequences. IEEE Transactions on Robotics, vol. 28, no. 5, pp.  1188-1197, 2012. PDF \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "SIVO - Semantically Informed Visual Odometry and Mapping. Integrated Bayesian semantic segmentation with ORBSLAM_2 to select better features for Visual SLAM.",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The network weights (`*.caffemodel`) are stored using Git LFS. If you already have this installed, the `git clone` command above should download all necessary files. If not, perform the following.\n\n1. Install Git LFS. The steps can be found [here](https://help.github.com/articles/installing-git-large-file-storage/).\n2. Run the command: `git lfs install`\n3. Navigate to the location of the SIVO home folder.\n4. Run the command: `git lfs pull`. This should download the weights and place them in the appropriate subfolders within `config/bayesian_segnet/`.\n\nThere are 2 separate weights files. There are files for both _Standard_ and _Basic_ Bayesian SegNet trained on the [KITTI Semantic Dataset](http://www.cvlibs.net/datasets/kitti/eval_semantics.php); these weights were first trained using the [Cityscapes Dataset](https://www.cityscapes-dataset.com), and were then fine tuned. All weights have the batch normalization layer merged with the preceding convolutional layer in order to speed up inference.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/navganti/SIVO/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 69,
      "date": "Sun, 26 Dec 2021 10:46:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/navganti/SIVO/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "navganti/SIVO",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/navganti/SIVO/master/build.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone the repository:\n\n`git clone --recursive https://github.com/navganti/SIVO.git`\n\nor\n\n`git clone --recursive git@github.com:navganti/SIVO.git`\n\nEnsure you use the recursive flag to initialize the submodule.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8394649314714749
      ],
      "excerpt": "This implementation has been tested with Ubuntu 16.04. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586216227281582,
        0.9002786560321396,
        0.923172803508418,
        0.8561374353406057
      ],
      "excerpt": "OpenCV is used to manipulate images and features. Download and install instructions can be found here. Required version > OpenCV 3.2. \nEigen3 is used for linear algebra, specifically matrix and tensor manipulation. Required by g2o and Bayesian SegNet. Download and install instructions can be found at: http://eigen.tuxfamily.org. Required version > 3.2.0 (for Tensors). \nBuild caffe-segnet-cudnn7. Navigate to dependencies/caffe-segnet-cudnn7, and follow the instructions listed in the README, specifically the CMake installation steps. This process is a little involved - please follow the instructions carefully. \nEnsure all other prerequisites are installed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9386568594778147
      ],
      "excerpt": "chmod +x build.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842048069819826
      ],
      "excerpt": "To use SIVO with the KITTI dataset, perform the following \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8791447595029418
      ],
      "excerpt": "Download the dataset (colour images) from here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052420848133074,
        0.8086300341597529
      ],
      "excerpt": "Change PATH_TO_CAFFEMODEL to the full path to the desired Bayesian SegNet .caffemodel file (e.g. basic/kitti/bayesian_segnet_basic_kitti.caffemodel). \nChange PATH_TO_DATASET_FOLDER to the full path to the downloaded dataset folder. Change SEQUENCE_NUMBER to 00, 01, 02,.., 10. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/navganti/SIVO/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "CMake",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "GNU General Public License v3.0",
      "url": "https://api.github.com/licenses/gpl-3.0"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SIVO: Semantically Informed Visual Odometry and Mapping",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SIVO",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "navganti",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/navganti/SIVO/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A modified version of Caffe is required to use Bayesian SegNet. Please see the [`caffe-segnet-cudnn7`](https://github.com/navganti/caffe-segnet-cudnn7/tree/b37d681223c15cb7a65181ad675fca54f7b02e9d) submodule within this repository, and follow the installation instructions.\n\nIf you wish to test or train weights for the Bayesian SegNet architecture, please see our modified [SegNet](https://www.github.com/navganti/SegNet) repository for information and a tutorial.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We use modified versions of the [DBoW2](https://github.com/dorian3d/DBoW2) library to perform place recognition.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We use a modified version of the [g2o](https://github.com/navganti/g2o) library to perform non-linear optimization for the SLAM backend. The original repository can be found [here](https://github.com/rainerkuemmerle/g2o).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 251,
      "date": "Sun, 26 Dec 2021 10:46:36 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The program can be run with the following:\n\n```bash\n./bin/SIVO config/Vocabulary/ORBvoc.txt config/CONFIGURATION_FILE config/bayesian_segnet/PATH_TO_PROTOTXT config/bayesian_segnet/PATH_TO_CAFFEMODEL PATH_TO_DATASET_FOLDER/dataset/sequences/SEQUENCE_NUMBER\n```\nThe parameters `CONFIGURATION_FILE`, `PATH_TO_PROTOTXT`, `PATH_TO_CAFFEMODEL`, and `PATH_TO_DATASET_FOLDER` must be modified.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "<a href=\"https://www.youtube.com/embed/ufvPS5wJAx0\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/ufvPS5wJAx0/0.jpg\"\nalt=\"ORB-SLAM2\" width=\"240\" height=\"180\" border=\"10\" /></a>\n<a href=\"https://www.youtube.com/embed/T-9PYCKhDLM\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/T-9PYCKhDLM/0.jpg\"\nalt=\"ORB-SLAM2\" width=\"240\" height=\"180\" border=\"10\" /></a>\n<a href=\"https://www.youtube.com/embed/kPwy8yA4CKM\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/kPwy8yA4CKM/0.jpg\"\nalt=\"ORB-SLAM2\" width=\"240\" height=\"180\" border=\"10\" /></a>\n\n",
      "technique": "Header extraction"
    }
  ]
}