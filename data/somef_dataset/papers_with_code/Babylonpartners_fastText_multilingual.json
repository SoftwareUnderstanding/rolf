{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1702.03859",
      "https://arxiv.org/abs/1607.04606",
      "https://arxiv.org/abs/1702.03859",
      "https://arxiv.org/abs/1309.4168",
      "https://arxiv.org/abs/1412.6568"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nThere are a number of great papers on this topic. We've listed a few of them below:\r\n\r\n1. [Enriching word vectors with subword information](https://arxiv.org/abs/1607.04606)  \r\nBojanowski et al., 2016\r\n2. [Offline bilingual word vectors, orthogonal transformations and the inverted softmax](https://arxiv.org/abs/1702.03859)  \r\nSmith et al., ICLR 2017\r\n3. [Exploiting similarities between languages for machine translation](https://arxiv.org/abs/1309.4168)  \r\nMikolov et al., 2013\r\n4. [Improving vector space word representations using multilingual correlation](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1031&context=lti)  \r\nFaruqui and Dyer, EACL 2014\r\n5. [Improving zero-shot learning by mitigating the hubness problem](https://arxiv.org/abs/1412.6568)  \r\nDinu et al., 2014\r\n6. [Learning principled bilingual mappings of word embeddings while preserving monolingual invariance](https://pdfs.semanticscholar.org/9a2e/ed5f8175275af0d55d4aed39afc8e2b2acf2.pdf?_ga=1.8571637.130713154.1492676520)  \r\nArtetxe et al., EMNLP 2016\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9973912106088191
      ],
      "excerpt": "If you use this repository, please cite: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9963125660771343
      ],
      "excerpt": "ICLR 2017 (conference track) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.946498592459726
      ],
      "excerpt": "| Target language | Precision @1 | Precision @5 | Precision @10 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187756947909643
      ],
      "excerpt": "| et              | 0.53         | 0.73         | 0.78          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| ko              | 0.37         | 0.58         | 0.66          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "| he              | 0.33         | 0.45         | 0.48          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| bn              | 0.30         | 0.49         | 0.56          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| mr              | 0.20         | 0.37         | 0.44          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614,
        0.9030859728368266
      ],
      "excerpt": "| km              | 0.12         | 0.26         | 0.30          | \n| my              | 0.10         | 0.19         | 0.23          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283216015784888
      ],
      "excerpt": "| Language 1 | Language 2 | Inter-pair precision @1 | English-pair precision @1  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "|     kk     |     ky     |           0.30          |            0.28           | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/babylonhealth/fastText_multilingual",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-04-21T12:15:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T18:46:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9658601179104125,
        0.9849162008406324,
        0.9946400757853098,
        0.937912928197079
      ],
      "excerpt": "Facebook recently open-sourced word vectors in 89 languages. However these vectors are monolingual; meaning that while similar words within a language share similar vectors, translation words from different languages do not have similar vectors. In a recent paper at ICLR 2017, we showed how the SVD can be used to learn a linear transformation (a matrix), which aligns monolingual vectors from two languages in a single vector space. In this repository we provide 78 matrices, which can be used to align the majority of the fastText languages in a single space. \nThis readme explains how the matrices should be used. We also present a simple evaluation task, where we show we are able to successfully predict the translations of words in multiple languages. Our procedure relies on collecting bilingual training dictionaries of word pairs in two languages, but remarkably we are able to successfully predict the translations of words between language pairs for which we had no training dictionary! \nWord embeddings define the similarity between two words by the normalised inner product of their vectors. The matrices in this repository place languages in a single space, without changing any of these monolingual similarity relationships. When you use the resulting multilingual vectors for monolingual tasks, they will perform exactly the same as the original vectors. To learn more about word embeddings, check out Colah's blog or Sam's introduction to vector representations.  \nNote that since we released this repository Facebook have released an additional 204 languages; however the word vectors of the original 90 languages have not changed, and the transformations provided in this repository will still work. If you would like to learn your own alignment matrices, we provide an example in align_your_own.ipynb. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8498667372848012
      ],
      "excerpt": "Clone a local copy of this repository, and download the fastText vectors you need from here. I'm going to assume you've downloaded the vectors for French and Russian in the text format. Let's say we want to compare the similarity of \"chat\" and \"\u043a\u043e\u0442\". We load the word vectors: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9784996339194507
      ],
      "excerpt": "The cosine similarity runs between -1 and 1. It seems that \"chat\" and \"\u043a\u043e\u0442\" are neither similar nor dissimilar. But now we apply the transformations to align the two dictionaries in a single space:python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.893915112972435,
        0.9052488733681985,
        0.9929247843661949,
        0.9952030729455403,
        0.9538314051164439,
        0.869318036506448
      ],
      "excerpt": "Turns out \"chat\" and \"\u043a\u043e\u0442\" are pretty similar after all. This is good, since they both mean \"cat\". \nOf the 89 languages provided by Facebook, 78 are supported by the Google Translate API. We first obtained the 10,000 most common words in the English fastText vocabulary, and then use the API to translate these words into the 78 languages available. We split this vocabulary in two, assigning the first 5000 words to the training dictionary, and the second 5000 to the test dictionary. \nWe described the alignment procedure in this blog. It takes two sets of word vectors and a small bilingual dictionary of translation pairs in two languages; and generates a matrix which aligns the source language with the target. Sometimes Google translates an English word to a non-English phrase, in these cases we average the word vectors contained in the phrase. \nTo place all 78 languages in a single space, we align every language to the English vectors (the English matrix is the identity). \nTo prove that the procedure works, we can predict the translations of words not seen in the training dictionary. For simplicity we predict translations by nearest neighbours. So for example, if we wanted to translate \"dog\" into Swedish, we would simply find the Swedish word vector whose cosine similarity to the \"dog\" word vector is highest. \nFirst things first, let's test the translation performance from English into every other language. For each language pair, we extract a set of 2500 word pairs from the test dictionary. The precision @n denotes the probability that, of the 2500 target words in this set, the true translation was one of the top n nearest neighbours of the source word. If the alignment was completely random, we would expect the precision @1 to be around 0.0004. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891785691729262
      ],
      "excerpt": "| is              | 0.29         | 0.51         | 0.59          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.972153465677877,
        0.9891571466054196,
        0.9680967616134972,
        0.9560237336732706,
        0.9844564653302396
      ],
      "excerpt": "As you can see, the alignment is consistently much better than random! In general, the procedure works best for other European languages like French, Portuguese and Spanish. We use 2500 word pairs, because of the 5000 words in the test dictionary, not all the words found by the Google Translate API are actually present in the fastText vocabulary. \nNow let's do something much more exciting, let's evaluate the translation performance between all possible language pairs. We exhibit this translation performance on the heatmap below, where the colour of an element denotes the precision @1 when translating from the language of the row into the language of the column. \nWe should emphasize that all of the languages were aligned to English only. We did not provide training dictionaries between non-English language pairs. Yet we are still able to succesfully predict translations between pairs of non-English languages remarkably accurately.  \nWe expect the diagonal elements of the matrix above to be 1, since a language should translate perfectly to itself. However in practice this does not always occur, because we constructed the training and test dictionaries by translating common English words into the other languages. Sometimes multiple English words translate to the same non-English word, and so the same non-English word may appear multiple times in the test set. We haven't properly accounted for this, which reduces the translation performance. \nIntriquingly, even though we only directly aligned the languages to English, sometimes a language translates better to another non-English language than it does to English! We can calculate the inter-pair precision of two languages; the average precision from language 1 to language 2 and vice versa. We can also calculate the English-pair precision; the average of the precision from English to language 1 and from English to language 2. Below we list all the language pairs for which the inter-pair precision exceeds the English-pair precision: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Multilingual word vectors in 78 languages",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Babylonpartners/fastText_multilingual/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 121,
      "date": "Tue, 28 Dec 2021 19:06:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/babylonhealth/fastText_multilingual/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "babylonhealth/fastText_multilingual",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Babylonpartners/fastText_multilingual/master/align_your_own.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9023697225149864
      ],
      "excerpt": "| sh              | 0.56         | 0.77         | 0.81          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023697225149864
      ],
      "excerpt": "|     bs     |     sh     |           0.88          |            0.52           | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023697225149864
      ],
      "excerpt": "|     hr     |     sh     |           0.78          |            0.56           | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023697225149864
      ],
      "excerpt": "|     sr     |     sh     |           0.73          |            0.54           | \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from fasttext import FastVector \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785
      ],
      "excerpt": "print(FastVector.cosine_similarity(fr_vector, ru_vector)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785
      ],
      "excerpt": "print(FastVector.cosine_similarity(fr_dictionary[\"chat\"], ru_dictionary[\"\u043a\u043e\u0442\"])) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537648785978966
      ],
      "excerpt": "| fa              | 0.45         | 0.68         | 0.75          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8139453978820247
      ],
      "excerpt": "|     bs     |     sr     |           0.67          |            0.50           | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/babylonhealth/fastText_multilingual/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2017, babylon health\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Aligning the fastText vectors of 78 languages",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fastText_multilingual",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "babylonhealth",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/babylonhealth/fastText_multilingual/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1106,
      "date": "Tue, 28 Dec 2021 19:06:54 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "word-vectors",
      "machine-learning",
      "machine-translation",
      "natural-language-processing",
      "nlp",
      "distributed-representations"
    ],
    "technique": "GitHub API"
  }
}