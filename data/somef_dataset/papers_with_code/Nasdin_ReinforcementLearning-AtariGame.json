{
  "citation": [
    {
      "confidence": [
        0.9422100272984922
      ],
      "excerpt": "<a href= \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" >Understanding LSTM Post </a> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Nasdin/ReinforcementLearning-AtariGame",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-09-27T02:31:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T08:51:42Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The A3C algorithm was released by Google\u2019s DeepMind group earlier this year, and it made a splash by essentially obsoleting DQN. It was faster, simpler, more robust, and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this, it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces\n\n\n    \n<a href= \"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2\" >Medium Article explaining A3c reinforcement learning </a>\n<br>\n![A3C LSTM playing MsPacman-v0](https://github.com/Nasdin/ReinforcementLearning-AtariGame/blob/master/img/MsPacman.gif)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.911788934475809
      ],
      "excerpt": "Using OpenAi Gym and Universe.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9688080265398389,
        0.8310097132657844,
        0.942985138985861
      ],
      "excerpt": "Implementation of Google Deepmind's Asynchronous Advantage Actor-Critic (A3C) \nInputs are changed in the Jupyter Notebook \nAn algorithm from Google Deep Mind's paper \"Asynchronous Methods for Deep Reinforcement Learning.\"<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8191516023634737
      ],
      "excerpt": "Implemented using Pytorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302958729978186,
        0.9410868506538157,
        0.863965616972508,
        0.892143625085877,
        0.9034180789161474,
        0.9009927872670039,
        0.9855206577427054,
        0.9766965222082309,
        0.9298448058782193
      ],
      "excerpt": "Trained models are generated when you run through a full training episode for the sim. Continous running will update the model with new training. The L(Load) parameter is set to false in the demo, When you have trained data where it can pick up from, then set it to true. \nIn gym atari the agents randomly repeat the previous action with probability 0.25 and there is time/step limit that limits performance. You can adjust these parameters. \nRMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class. \nRMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta  \nRMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \u03b3 \nto be set to 0.9, while a good default value for the learning rate \u03b7 is 0.001. \nis another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum: \nAdam (short for Adaptive Moment Estimation) is an update to the RMSProp optimizer. In this optimization algorithm, running averages of both the gradients and the second moments of the gradients are used. \nIt is important to limit number of worker threads to number of cpu cores available in your com. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pytorch LSTM RNN for reinforcement learning to play Atari games from OpenAI Universe. We also use Google Deep Mind's Asynchronous Advantage Actor-Critic (A3C) Algorithm. This is much superior and efficient than DQN and obsoletes it. Can play on many games",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Nasdin/ReinforcementLearning-AtariGame/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 35,
      "date": "Sun, 26 Dec 2021 22:14:18 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Nasdin/ReinforcementLearning-AtariGame/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Nasdin/ReinforcementLearning-AtariGame",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Nasdin/ReinforcementLearning-AtariGame/master/OpenAI%20Reinforcement%20Learning%20Agent%20Plays%20Atari%20Games.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8319704241261763
      ],
      "excerpt": "Implemented using Pytorch \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8314823419435533,
        0.8314823419435533
      ],
      "excerpt": "<img src = \"img/A3CStructure.png\"> \n<img src = \"img/A3CProcessFlow.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8683095007105495
      ],
      "excerpt": "Trained models are generated when you run through a full training episode for the sim. Continous running will update the model with new training. The L(Load) parameter is set to false in the demo, When you have trained data where it can pick up from, then set it to true. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Nasdin/ReinforcementLearning-AtariGame/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2017, Nasrudin Bin Salim\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reinforcement Learning implementation of LSTM with Asynchronous Advantage Actor Critic Algorithm",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ReinforcementLearning-AtariGame",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Nasdin",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Nasdin/ReinforcementLearning-AtariGame/blob/master/README.MD",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 136,
      "date": "Sun, 26 Dec 2021 22:14:18 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "a3c",
      "a3c-lstm",
      "pytorch",
      "openai-gym",
      "reinforcement-learning",
      "universe",
      "lstm",
      "adam",
      "rmsprop",
      "reinforcement-learning-algorithms",
      "reinforcement-agents",
      "python",
      "deep-reinforcement-learning",
      "actor-critic",
      "asynchronous-advantage-actor-critic"
    ],
    "technique": "GitHub API"
  }
}