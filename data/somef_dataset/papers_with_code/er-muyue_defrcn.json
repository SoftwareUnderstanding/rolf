{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo is developed based on [TFA](https://github.com/Megvii-BaseDetection/cvpods) and [Detectron2](https://github.com/facebookresearch/detectron2). Please check them for more details and features.\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2108.09017",
      "https://arxiv.org/abs/2108.09017",
      "https://arxiv.org/abs/1506.01497",
      "https://arxiv.org/abs/2007.09384",
      "https://arxiv.org/abs/2108.09017",
      "https://arxiv.org/abs/2108.09017",
      "https://arxiv.org/abs/1506.01497",
      "https://arxiv.org/abs/2108.09017",
      "https://arxiv.org/abs/2108.09017",
      "https://arxiv.org/abs/1506.01497",
      "https://arxiv.org/abs/2007.09384",
      "https://arxiv.org/abs/2108.09017",
      "https://arxiv.org/abs/2108.09017",
      "https://arxiv.org/abs/1506.01497",
      "https://arxiv.org/abs/2108.09017",
      "https://arxiv.org/abs/2108.09017"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this work in your research or wish to refer to the baseline results published here, please use the following BibTeX entries:\r\n```\r\n@inproceedings{qiao2021defrcn,\r\n  title={DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection},\r\n  author={Qiao, Limeng and Zhao, Yuxuan and Li, Zhiyuan and Qiu, Xi and Wu, Jianan and Zhang, Chi},\r\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\r\n  pages={8681--8690},\r\n  year={2021}\r\n}\r\n```\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{qiao2021defrcn,\n  title={DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection},\n  author={Qiao, Limeng and Zhao, Yuxuan and Li, Zhiyuan and Qiu, Xi and Wu, Jianan and Zhang, Chi},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={8681--8690},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8715509475085587
      ],
      "excerpt": "Few-shot Object Detection \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9725140271112827
      ],
      "excerpt": "  | Shot |  1  |  2  |  3  |  5  |  10 |  30 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "  |TFA|4.4|5.4|6.0|7.7|10.0|13.7| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9572487334023723,
        0.880054048434319
      ],
      "excerpt": "  |FSDetView|4.5|6.6|7.2|10.7|12.5|14.7| \n  |DeFRCN (Our Paper)|9.3|12.9|14.8|16.1|18.5|22.6| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9725140271112827
      ],
      "excerpt": "  | Shot |  1  |  2  |  3  |  5  |  10 |  30 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.9030859728368266,
        0.8935828248868415
      ],
      "excerpt": "  |TFA|1.9|3.9|5.1|7|9.1|12.1| \n  |FSDetView|3.2|4.9|6.7|8.1|10.7|15.9| \n  |DeFRCN (Our Paper)|4.8|8.5|10.7|13.6|16.8|21.2| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8715509475085587
      ],
      "excerpt": "Few-shot Object Detection \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977903201658708,
        0.9956679394393552
      ],
      "excerpt": "  |Shot|1|2|3|5|10|1|2|3|5|10|1|2|3|5|10| \n  |YOLO-ft|6.6|10.7|12.5|24.8|38.6|12.5|4.2|11.6|16.1|33.9|13.0|15.9|15.0|32.2|38.4| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702105236373249
      ],
      "excerpt": "  |MetaDet|18.9|20.6|30.2|36.8|49.6|21.8|23.1|27.8|31.7|43.0|20.6|23.9|29.4|43.9|44.1| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293185
      ],
      "excerpt": "  |TFA|39.8|36.1|44.7|55.7|56.0|23.5|26.9|34.1|35.1|39.1|30.8|34.8|42.8|49.5|49.8| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130052599129608
      ],
      "excerpt": "  |DeFRCN (Our Paper)|53.6|57.5|61.5|64.1|60.8|30.1|38.1|47.0|53.3|47.9|48.4|50.9|52.3|54.9|57.4| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977903201658708
      ],
      "excerpt": "  |Shot|1|2|3|5|10|1|2|3|5|10|1|2|3|5|10| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9949774332681249
      ],
      "excerpt": "  |FSRW|14.2|23.6|29.8|36.5|35.6|12.3|19.6|25.1|31.4|29.8|12.5|21.3|26.8|33.8|31.0| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9690798918356124,
        0.948241993447799
      ],
      "excerpt": "  |FSDetView|24.2|35.3|42.2|49.1|57.4|21.6|24.6|31.9|37.0|45.7|21.2|30.0|37.2|43.8|49.6| \n  |DeFRCN (Our Paper)|40.2|53.6|58.2|63.6|66.5|29.5|39.7|43.4|48.1|52.8|35.0|38.3|52.9|57.7|60.8| \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/er-muyue/DeFRCN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-05T06:12:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T15:16:43Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nThis repo contains the official PyTorch implementation of our ICCV paper\r\n[DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection](https://arxiv.org/abs/2108.09017).\r\n\r\n<div align=\"center\"><img src=\"assets/arch.png\" width=\"800\"></div>\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8108731065808569,
        0.9611474620455094
      ],
      "excerpt": "\u30102021/10/10\u3011 We release the official PyTorch implementation of DeFRCN. \n\u30102021/08/20\u3011 We have uploaded our paper (long version with supplementary material) on arxiv, review it for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9627487715650112
      ],
      "excerpt": "  |MetaR-CNN|-|-|-|-|8.7|12.4| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8766626358032775,
        0.8986776404475127,
        0.9662231776421819
      ],
      "excerpt": "* indicates that the results are reproduced by us with their source code. \nIt's normal to observe -0.3~+0.3AP noise between your results and this repo.  \nThe results of mAP<sup>base</sup> and mAP<sup>all</sup> for G-FSOD are list here GoogleDrive, BaiduYun.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "|Method| | |Split-1| | | | |Split-2| | | | |Split-3| | | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8104547731086869
      ],
      "excerpt": "  |DeFRCN (Our Paper)|53.6|57.5|61.5|64.1|60.8|30.1|38.1|47.0|53.3|47.9|48.4|50.9|52.3|54.9|57.4| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "|Method| | |Split-1| | | | |Split-2| | | | |Split-3| | | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8729704157946452
      ],
      "excerpt": "  |DeFRCN (Our Paper)|40.2|53.6|58.2|63.6|66.5|29.5|39.7|43.4|48.1|52.8|35.0|38.3|52.9|57.7|60.8| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9851436175107281,
        0.9662231776421819
      ],
      "excerpt": "Note that we change the \u03bb<sup>GDL-RCNN</sup> for VOC to 0.001 (0.01 in paper) and get better performance, check the configs for more details. \nThe results of mAP<sup>base</sup> and mAP<sup>all</sup> for G-FSOD are list here GoogleDrive, BaiduYun. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/er-muyue/defrcn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Sat, 25 Dec 2021 11:19:51 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/er-muyue/DeFRCN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "er-muyue/DeFRCN",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/er-muyue/defrcn/main/run_coco.sh",
      "https://raw.githubusercontent.com/er-muyue/defrcn/main/run_voc.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8537137908049488
      ],
      "excerpt": "  |FSDetView|3.2|4.9|6.7|8.1|10.7|15.9| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8378870916581522,
        0.814569354956707,
        0.8317163336119839
      ],
      "excerpt": "  |FSRW|14.8|15.5|26.7|33.9|47.2|15.7|15.2|22.7|30.1|40.5|21.3|25.6|28.4|42.8|45.9| \n  |MetaDet|18.9|20.6|30.2|36.8|49.6|21.8|23.1|27.8|31.7|43.0|20.6|23.9|29.4|43.9|44.1| \n  |MetaR-CNN|19.9|25.5|35.0|45.7|51.5|10.4|19.4|29.6|34.8|45.4|14.3|18.2|27.5|41.2|48.1| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8395432109389886
      ],
      "excerpt": "  |TFA|25.3|36.4|42.1|47.9|52.8|18.3|27.5|30.9|34.1|39.5|17.9|27.2|34.3|40.8|45.6| \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/er-muyue/DeFRCN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 er-muyue\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeFRCN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "er-muyue",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/er-muyue/DeFRCN/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 58,
      "date": "Sat, 25 Dec 2021 11:19:51 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n**1. Check Requirements**\r\n* Linux with Python >= 3.6\r\n* [PyTorch](https://pytorch.org/get-started/locally/) >= 1.6 & [torchvision](https://github.com/pytorch/vision/) that matches the PyTorch version.\r\n* CUDA 10.1, 10.2\r\n* GCC >= 4.9\r\n\r\n**2. Build DeFRCN**\r\n* Clone Code\r\n  ```angular2html\r\n  git clone https://github.com/er-muyue/DeFRCN.git\r\n  cd DeFRCN\r\n  ```\r\n* Create a virtual environment (optional)\r\n  ```angular2html\r\n  virtualenv defrcn\r\n  cd /path/to/venv/defrcn\r\n  source ./bin/activate\r\n  ```\r\n* Install PyTorch 1.6.0 with CUDA 10.1 \r\n  ```shell\r\n  pip3 install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\r\n  ```\r\n* Install Detectron2\r\n  ```angular2html\r\n  python3 -m pip install detectron2==0.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html\r\n  ```\r\n  - If you use other version of PyTorch/CUDA, check the latest version of Detectron2 in this page: [Detectron2](https://github.com/facebookresearch/detectron2/releases). \r\n  - Sorry for that I don\u2019t have enough time to test on more versions, if you run into problems with other versions, please let me know.\r\n* Install other requirements. \r\n  ```angular2html\r\n  python3 -m pip install -r requirements.txt\r\n  ```\r\n\r\n**3. Prepare Data and Weights**\r\n* Data Preparation\r\n  - We evaluate our models on two datasets for both FSOD and G-FSOD settings:\r\n\r\n    | Dataset | Size | GoogleDrive | BaiduYun | Note |\r\n    |:---:|:---:|:---:|:---:|:---:|\r\n    |VOC2007| 0.8G |[download](https://drive.google.com/file/d/1BcuJ9j9Mtymp56qGSOfYxlXN4uEVyxFm/view?usp=sharing)|[download](https://pan.baidu.com/s/1kjAmHY5JKDoG0L65T3dK9g)| - |\r\n    |VOC2012| 3.5G |[download](https://drive.google.com/file/d/1NjztPltqm-Z-pG94a6PiPVP4BgD8Sz1H/view?usp=sharing)|[download](https://pan.baidu.com/s/1DUJT85AG_fqP9NRPhnwU2Q)| - |\r\n    |vocsplit| <1M |[download](https://drive.google.com/file/d/1BpDDqJ0p-fQAFN_pthn2gqiK5nWGJ-1a/view?usp=sharing)|[download](https://pan.baidu.com/s/1518_egXZoJNhqH4KRDQvfw)| refer from [TFA](https://github.com/ucbdrive/few-shot-object-detection#models) |\r\n    |COCO| ~19G | - | - | download from [offical](https://cocodataset.org/#download)|\r\n    |cocosplit| 174M |[download](https://drive.google.com/file/d/1T_cYLxNqYlbnFNJt8IVvT7ZkWb5c0esj/view?usp=sharing)|[download](https://pan.baidu.com/s/1NELvshrbkpRS8BiuBIr5gA)| refer from [TFA](https://github.com/ucbdrive/few-shot-object-detection#models) |\r\n  - Unzip the downloaded data-source to `datasets` and put it into your project directory:\r\n    ```angular2html\r\n      ...\r\n      datasets\r\n        | -- coco (trainval2014/*.jpg, val2014/*.jpg, annotations/*.json)\r\n        | -- cocosplit\r\n        | -- VOC2007\r\n        | -- VOC2012\r\n        | -- vocsplit\r\n      defrcn\r\n      tools\r\n      ...\r\n    ```\r\n* Weights Preparation\r\n  - We use the imagenet pretrain weights to initialize our model. Download the same models from here: [GoogleDrive](https://drive.google.com/file/d/1rsE20_fSkYeIhFaNU04rBfEDkMENLibj/view?usp=sharing) [BaiduYun](https://pan.baidu.com/s/1IfxFq15LVUI3iIMGFT8slw)\r\n  - The extract code for all BaiduYun link is **0000**\r\n\r\n**4. Training and Evaluation**\r\n\r\nFor ease of training and evaluation over multiple runs, we integrate the whole pipeline of few-shot object detection into one script `run_*.sh`, including base pre-training and novel-finetuning (both FSOD and G-FSOD).\r\n* To reproduce the results on VOC, `EXP_NAME` can be any string (e.g defrcn, or something) and `SPLIT_ID` must be `1 or 2 or 3` (we consider 3 random splits like other papers).\r\n  ```angular2html\r\n  bash run_voc.sh EXP_NAME SPLIT_ID (1, 2 or 3)\r\n  ```\r\n* To reproduce the results on COCO, `EXP_NAME` can be any string (e.g defrcn, or something) \r\n  ```angular2html\r\n  bash run_coco.sh EXP_NAME\r\n  ```\r\n* Please read the details of few-shot object detection pipeline in `run_*.sh`, you need change `IMAGENET_PRETRAIN*` to your path.\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}