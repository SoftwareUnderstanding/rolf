{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2006.04558v1",
      "https://arxiv.org/abs/2006.04558v1",
      "https://arxiv.org/abs/2006.04558",
      "https://arxiv.org/abs/2006.04558"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@INPROCEEDINGS{chien2021investigating,\n  author={Chien, Chung-Ming and Lin, Jheng-Hao and Huang, Chien-yu and Hsu, Po-chun and Lee, Hung-yi},\n  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, \n  title={Investigating on Incorporating Pretrained and Learnable Speaker Representations for Multi-Speaker Multi-Style Text-to-Speech}, \n  year={2021},\n  volume={},\n  number={},\n  pages={8588-8592},\n  doi={10.1109/ICASSP39728.2021.9413880}}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- [FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://arxiv.org/abs/2006.04558), Y. Ren, *et al*.\n- [xcmyz's FastSpeech implementation](https://github.com/xcmyz/FastSpeech)\n- [TensorSpeech's FastSpeech 2 implementation](https://github.com/TensorSpeech/TensorflowTTS)\n- [rishikksh20's FastSpeech 2 implementation](https://github.com/rishikksh20/FastSpeech2)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "./montreal-forced-aligner/bin/mfa_align raw_data/LJSpeech/ lexicon/librispeech-lexicon.txt english preprocessed_data/LJSpeech \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "./montreal-forced-aligner/bin/mfa_train_and_align raw_data/LJSpeech/ lexicon/librispeech-lexicon.txt preprocessed_data/LJSpeech \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ming024/FastSpeech2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-25T13:57:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T14:23:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9740643969580681,
        0.9963580097184209,
        0.8225212713931427,
        0.9827061578470314,
        0.9639710763705942,
        0.8670091663879794
      ],
      "excerpt": "This is a PyTorch implementation of Microsoft's text-to-speech system FastSpeech 2: Fast and High-Quality End-to-End Text to Speech.  \nThis project is based on xcmyz's implementation of FastSpeech. Feel free to use/modify the code. \nThere are several versions of FastSpeech 2. \nThis implementation is more similar to version 1, which uses F0 values as the pitch features. \nOn the other hand, pitch spectrograms extracted by continuous wavelet transform are used as the pitch features in the later versions. \n2021/7/8: Release the checkpoint and audio samples of a multi-speaker English TTS model trained on LibriTTS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9922656268500749,
        0.9243717878618372
      ],
      "excerpt": "Here is an example of synthesized mel-spectrogram of the sentence \"Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\", with the English single-speaker TTS model. \nBatch inference is also supported, try \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8605590358331859
      ],
      "excerpt": "The supported datasets are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8868845803163208,
        0.9810459684292248,
        0.8475391244333449
      ],
      "excerpt": "for some preparations. \nAs described in the paper, Montreal Forced Aligner (MFA) is used to obtain the alignments between the utterances and the phoneme sequences. \nAlignments of the supported datasets are provided here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9668898513485841
      ],
      "excerpt": "The model takes less than 10k steps (less than 1 hour on my GTX1080Ti GPU) of training to generate audio samples with acceptable quality, which is much more efficient than the autoregressive models such as Tacotron2. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8969194243581906,
        0.8098339447139975,
        0.9868811112412567
      ],
      "excerpt": "Following xcmyz's implementation, I use an additional Tacotron-2-styled Post-Net after the decoder, which is not used in the original FastSpeech 2. \nGradient clipping is used in the training. \nIn my experience, using phoneme-level pitch and energy prediction instead of frame-level prediction results in much better prosody, and normalizing the pitch and energy features also helps. Please refer to config/README.md for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An implementation of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ming024/FastSpeech2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 222,
      "date": "Mon, 27 Dec 2021 19:50:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ming024/FastSpeech2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ming024/FastSpeech2",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8028670661788904
      ],
      "excerpt": "You have to download the pretrained models and put them in output/ckpt/LJSpeech/,  output/ckpt/AISHELL3, or output/ckpt/LibriTTS/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9039733694903622
      ],
      "excerpt": "Alternately, you can align the corpus by yourself.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8594882474169214
      ],
      "excerpt": "You have to download the pretrained models and put them in output/ckpt/LJSpeech/,  output/ckpt/AISHELL3, or output/ckpt/LibriTTS/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9077393228626486
      ],
      "excerpt": "python3 synthesize.py --text \"YOUR_DESIRED_TEXT\" --restore_step 900000 --mode single -p config/LJSpeech/preprocess.yaml -m config/LJSpeech/model.yaml -t config/LJSpeech/train.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9077393228626486
      ],
      "excerpt": "python3 synthesize.py --text \"\u5927\u5bb6\u597d\" --speaker_id SPEAKER_ID --restore_step 600000 --mode single -p config/AISHELL3/preprocess.yaml -m config/AISHELL3/model.yaml -t config/AISHELL3/train.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9077393228626486,
        0.8511167213741988
      ],
      "excerpt": "python3 synthesize.py --text \"YOUR_DESIRED_TEXT\"  --speaker_id SPEAKER_ID --restore_step 800000 --mode single -p config/LibriTTS/preprocess.yaml -m config/LibriTTS/model.yaml -t config/LibriTTS/train.yaml \nThe generated utterances will be put in output/result/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9008340221033896
      ],
      "excerpt": "python3 synthesize.py --source preprocessed_data/LJSpeech/val.txt --restore_step 900000 --mode batch -p config/LJSpeech/preprocess.yaml -m config/LJSpeech/model.yaml -t config/LJSpeech/train.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9077393228626486
      ],
      "excerpt": "python3 synthesize.py --text \"YOUR_DESIRED_TEXT\" --restore_step 900000 --mode single -p config/LJSpeech/preprocess.yaml -m config/LJSpeech/model.yaml -t config/LJSpeech/train.yaml --duration_control 0.8 --energy_control 0.8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136055429559771
      ],
      "excerpt": "AISHELL-3: a Mandarin TTS dataset with 218 male and female speakers, roughly 85 hours in total. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8898963314049357,
        0.8925035593013155
      ],
      "excerpt": "First, run  \npython3 prepare_align.py config/LJSpeech/preprocess.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8925035593013155
      ],
      "excerpt": "python3 preprocess.py config/LJSpeech/preprocess.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8925035593013155,
        0.8287974015137436,
        0.8924227954990515
      ],
      "excerpt": "python3 preprocess.py config/LJSpeech/preprocess.yaml \nTrain your model with \npython3 train.py -p config/LJSpeech/preprocess.yaml -m config/LJSpeech/model.yaml -t config/LJSpeech/train.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.825361263358686
      ],
      "excerpt": "tensorboard --logdir output/log/LJSpeech \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ming024/FastSpeech2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "HTML"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Jungil Kong\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FastSpeech 2 - PyTorch Implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "FastSpeech2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ming024",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ming024/FastSpeech2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can install the Python dependencies with\n```\npip3 install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 658,
      "date": "Mon, 27 Dec 2021 19:50:43 GMT"
    },
    "technique": "GitHub API"
  }
}