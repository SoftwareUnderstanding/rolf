# Variational-Auto-Encoder
PyTorch implementation of Variational Auto-Encoder as described in [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) from ICLR 2014.


<div align='center'>
  <img src='img/sample.png' height="280px">
  <span> <p> Randomly Sampled Images for 2D Latent Space </p></span>
</div>

### Latent Space
This model was trained to encode 784 dimensional MNIST images to just 2 dimensions and to then reconstruct it. The image below is a grid of outputs generated by walking through the 2D latent space Z.
<div align='center'>
  <img src='img/fig.jpg' width="512.jpg" >
</div>


### Implementation Details
- The encoder and decoder are symmetrical MLPs with 256 neurons in each's hidden layer. 
- This implementation is inspired by [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html).
- All the code was written and ran on Google Colab.

### Requirements
``` bash
torch
torchvision
numpy
matplotlib
```

### References
1. **Diederik P. Kingma, et al.** *Auto-Encoding Variational Bayes*  ICLR 2014[[arxiv](https://arxiv.org/abs/1312.6114)]
