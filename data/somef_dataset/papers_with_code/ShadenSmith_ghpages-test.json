{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1910.02054",
      "https://arxiv.org/abs/2010.13369",
      "https://arxiv.org/abs/2101.06840",
      "https://arxiv.org/abs/2102.02888",
      "https://arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)](https://dl.acm.org/doi/10.5555/3433701.3433727).\n2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).\n3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [https://arxiv.org/abs/2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [https://arxiv.org/abs/2101.06840](https://arxiv.org/abs/2101.06840).\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [https://arxiv.org/abs/2102.02888](https://arxiv.org/abs/2102.02888).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)",
      "https://arxiv.org/abs/2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [https://arxiv.org/abs/2101.06840](https://arxiv.org/abs/2101.06840).\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [https://arxiv.org/abs/2102.02888](https://arxiv.org/abs/2102.02888).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)",
      "https://arxiv.org/abs/2101.06840](https://arxiv.org/abs/2101.06840).\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [https://arxiv.org/abs/2102.02888](https://arxiv.org/abs/2102.02888).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)",
      "https://arxiv.org/abs/2102.02888](https://arxiv.org/abs/2102.02888).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9137948391978392
      ],
      "excerpt": "[2020/11/10] Efficient and robust compressed training through progressive layer dropping \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612
      ],
      "excerpt": "| Article                                                                                        | Description                                  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8687179211148074
      ],
      "excerpt": "| CIFAR-10 Tutorial                               |  Getting started with CIFAR-10 and DeepSpeed | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8706498213570278
      ],
      "excerpt": "DeepSpeed welcomes your contributions! Please see our \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999622207033674
      ],
      "excerpt": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. arXiv:1910.02054 and In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9787417321952059,
        0.9998209888974385,
        0.9999417208520345
      ],
      "excerpt": "Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. arXiv:2010.13369 and NeurIPS 2020. \nJie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. arXiv:2101.06840. \nHanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. arXiv:2102.02888. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9398184441320769
      ],
      "excerpt": "DeepSpeed hands on deep dive: part 1, part 2, part 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8793854886769767
      ],
      "excerpt": "Microsoft Research Webinar \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ShadenSmith/ghpages-test",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "title: \"Contributing\"\npermalink: /contributing/\n\nDeepSpeed welcomes your contributions!\nPrerequisites\nDeepSpeed uses pre-commit to ensure that formatting is\nconsistent across DeepSpeed. First, ensure that pre-commit is installed from either\ninstalling DeepSpeed or pip install pre-commit. Next, the pre-commit hooks must be\ninstalled once before commits can be made:\nbash\npre-commit install\nAfterwards, our suite of formatting tests run automatically before each git commit. You\ncan also run these manually:\nbash\npre-commit run --all-files\nIf a formatting test fails, it will fix the modified code in place and abort\nthe git commit. After looking over the changes, you can git add &lt;modified files&gt;\nand then repeat the previous git commit command.\nTesting\nDeepSpeed tracks two types of tests: unit tests and more costly model convergence tests.\nThe model convergence tests train\nDeepSpeedExamples and measure\nend-to-end convergence and related metrics. Unit tests are found in tests/unit/ and\nthe model convergence tests are found in tests/model/.\nUnit Tests\nPyTest is used to execute tests. PyTest can be\ninstalled from PyPI via pip install pytest. Simply invoke pytest --forked to run the\nunit tests:\nbash\npytest --forked tests/unit/\nYou can also provide the -v flag to pytest to see additional information about the\ntests. Note that pytest-forked and the\n--forked flag are required to test CUDA functionality in distributed tests.\nModel Tests\nModel tests require four GPUs and training data downloaded for\nDeepSpeedExamples.\nTo execute model tests, first install DeepSpeed. The\nDeepSpeedExamples repository is cloned\nas part of this process. Next, execute the model test driver:\nbash\ncd tests/model/\npytest run_sanity_check.py\nNote that the --forked flag is not necessary for the model tests.\nContributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\nCode of Conduct\nThis project has adopted the Microsoft Open Source Code of\nConduct. For more information see the\nCode of Conduct FAQ or contact\nopencode@microsoft.com with any additional questions or\ncomments.",
    "technique": "File Exploration"
  },
  "contributor": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-14T01:17:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-23T21:00:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8526218993920512
      ],
      "excerpt": "10x bigger model training on a single GPU with ZeRO-Offload \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8457457155819266
      ],
      "excerpt": "| Features                   |  Feature list and overview                  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "| Contributing           |  Instructions for contributing              | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333769817186591,
        0.9175423955855755
      ],
      "excerpt": "Training advanced deep learning models is challenging. Beyond model design, \nmodel scientists also need to set up the state-of-the-art training techniques \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8982750200382874,
        0.9949901874433842,
        0.8699759846391634
      ],
      "excerpt": "performance and convergence rate. Large model sizes are even more challenging: \na large model easily runs out of memory with pure data parallelism and it is \ndifficult to use model parallelism. DeepSpeed addresses these challenges to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.878151521971628
      ],
      "excerpt": "Below we provide a brief feature list, see our detailed feature \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.9103754479321569
      ],
      "excerpt": "Model Parallelism \nSupport for Custom Model Parallelism \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196241412598118,
        0.8490037945672047
      ],
      "excerpt": "3D Parallelism \nThe Zero Redundancy Optimizer (ZeRO) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8104426079870416,
        0.9214975586515987
      ],
      "excerpt": "Performance Analysis and Debugging \nAll DeepSpeed documentation can be found on our website: deepspeed.ai \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8930901044020226
      ],
      "excerpt": "| DeepSpeed Features                                       |  DeepSpeed features                          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8272945003240391,
        0.8807518949129635,
        0.9234539619149897
      ],
      "excerpt": "| 1Cycle Tutorial                                  |  SOTA learning schedule in DeepSpeed         | \nDeepSpeed welcomes your contributions! Please see our \ncontributing guide for more details on formatting, testing, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399748571878369,
        0.9177762152211659,
        0.8373310029600464
      ],
      "excerpt": "This project has adopted the Microsoft Open Source Code of \nConduct. For more information see the \nCode of Conduct FAQ or contact \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9351750086128282
      ],
      "excerpt": "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897298524178459
      ],
      "excerpt": "DeepSpeed hands on deep dive: part 1, part 2, part 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Playing around with GitHub pages.",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://deepspeed.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ShadenSmith/ghpages-test/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 20:27:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ShadenSmith/ghpages-test/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ShadenSmith/ghpages-test",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/ShadenSmith/ghpages-test/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/install.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/azure/create_vms.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/azure/attach.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/azure/azure_ssh.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/azure/build_docker_image.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/azure/setup_vms.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/azure/setup_docker.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/azure/start_container.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/azure/shutdown_vms.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/tests/model/BingBertSquad/run_BingBertSquad_sanity.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/tests/model/BingBertSquad/run_BingBertSquad.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/tests/model/BingBertSquad/run_tests.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/tests/model/Megatron_GPT2/ds_gpt2_test.sh",
      "https://raw.githubusercontent.com/ShadenSmith/ghpages-test/master/docs/code-docs/build-api-docs.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The quickest way to get started with DeepSpeed is via pip, this will install\nthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDA\nversions. DeepSpeed includes several C++/CUDA extensions that we commonly refer\nto as our 'ops'.  By default, all of these extensions/ops will be built\njust-in-time (JIT) using [torch's JIT C++ extension loader that relies on\nninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and\ndynamically link them at runtime.\n\n**Note:** [PyTorch](https://pytorch.org/) must be installed _before_ installing\nDeepSpeed.\n\n```bash\npip install deepspeed\n```\n\nAfter installation, you can validate your install and see which extensions/ops\nyour machine is compatible with via the DeepSpeed environment report.\n\n```bash\nds_report\n```\n\nIf you would like to pre-install any of the DeepSpeed extensions/ops (instead\nof JIT compiling) or install pre-compiled ops via PyPI please see our [advanced\ninstallation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9405465232564941
      ],
      "excerpt": "[2020/11/12] Simplified install, JIT compiled ops, PyPI releases, and reduced dependencies \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9786124855018795
      ],
      "excerpt": "| Install                |  Installation details                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.858204411280099
      ],
      "excerpt": "| Contributing           |  Instructions for contributing              | \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8138858718376174
      ],
      "excerpt": "[2020/09/10] DeepSpeed v0.3: Extreme-scale model training for everyone \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8405210537621255
      ],
      "excerpt": "Sparse attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training Optimizers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training Agnostic Checkpointing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8334721411866115
      ],
      "excerpt": "| BERT Pre-training Tutorial             |  Pre-train BERT with DeepSpeed | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810094876824159
      ],
      "excerpt": "17B T-NLG demo \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ShadenSmith/ghpages-test/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell",
      "Dockerfile",
      "C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'    MIT License\\r\\n\\r\\n    Copyright (c) Microsoft Corporation.\\r\\n\\r\\n    Permission is hereby granted, free of charge, to any person obtaining a copy\\r\\n    of this software and associated documentation files (the \"Software\"), to deal\\r\\n    in the Software without restriction, including without limitation the rights\\r\\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\r\\n    copies of the Software, and to permit persons to whom the Software is\\r\\n    furnished to do so, subject to the following conditions:\\r\\n\\r\\n    The above copyright notice and this permission notice shall be included in all\\r\\n    copies or substantial portions of the Software.\\r\\n\\r\\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\r\\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\r\\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\r\\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\r\\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\r\\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\r\\n    SOFTWARE\\r\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "News",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ghpages-test",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ShadenSmith",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ShadenSmith/ghpages-test/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 20:27:49 GMT"
    },
    "technique": "GitHub API"
  }
}