{
  "citation": [
    {
      "confidence": [
        0.9593299683604384,
        0.919614608585817,
        0.9593299683604384
      ],
      "excerpt": "2012-10-30T08:56:21 Initialized mrlin table. \n2012-10-30T08:56:31 Importing RDF/NTriples from URL http://dbpedia.org/data/Galway.ntriples into graph http://dbpedia.org/ \n2012-10-30T08:56:31 == STATUS == \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593299683604384
      ],
      "excerpt": "2012-10-30T08:56:31 == STATUS == \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252756551763226,
        0.9593299683604384
      ],
      "excerpt": "2012-10-30T08:56:31  Import speed: 1506.61 triples per sec \n2012-10-30T08:56:31 == STATUS == \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9704838993743424,
        0.9593299683604384,
        0.9372151656983196
      ],
      "excerpt": "2012-10-30T08:56:31  Import speed: 4059.10 triples per sec \n2012-10-30T08:56:31 ========== \n2012-10-30T08:56:31 Imported 233 triples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.932284124062775,
        0.9593299683604384,
        0.9372151656983196
      ],
      "excerpt": "2012-10-30T04:01:22 Key: http://dbpedia.org/resource/Galway - Value: {'O:148': 'u\\'\"City of the Tribes\"\\'', 'O:66': 'u\\'\"City of the Tribes\"\\'',  ...} \n2012-10-30T04:01:22 ============ \n2012-10-30T04:01:22 Query took me 0.01 seconds. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mhausenblas/mrlin",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2012-10-27T22:57:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-16T21:39:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9298412680219628
      ],
      "excerpt": "The basic idea of mrlin is to enable Map Reduce processing of Linked Data - hence the name. In the following I'm going to show you first to how to use HBase to store Linked Data with RDF, and then how to use Hadoop to run MapReduce jobs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9033390628072481
      ],
      "excerpt": "Dig into RESTful interactions with HBase, in mrlin. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8069294642195112
      ],
      "excerpt": "2012-10-30T04:01:22 Key: http://dbpedia.org/resource/Galway - Value: {'O:148': 'u\\'\"City of the Tribes\"\\'', 'O:66': 'u\\'\"City of the Tribes\"\\'',  ...} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "mrlin is 'MapReduce processing of Linked Data' \u2026 because it's magic",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mhausenblas/mrlin/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "*TBD*\n\n* setup in virtual env: `source hb/bin/activate` then `pip install mrjob`\n* `cp .mrjob.conf ~` before launch\n* `source hb/bin/activate`\n* run `python mrlin_mr.py README.md` for standalone\n* set up [Hadoop 1.0.4](http://ftp.heanet.ie/mirrors/www.apache.org/dist/hadoop/common/hadoop-1.0.4/hadoop-1.0.4.tar.gz) - if unsure follow a [single-node setup](http://orzota.com/blog/single-node-hadoop-setup-2/)  tutorial\n* `cp .mrjob.conf ~` before launch if you change settings (!)\n* note all changes that were necessary in ` conf/core-site.xml`, `conf/mapred-site.xml`, `conf/hdfs-site.xml`, and `hadoop-env.sh` (provide examples)\n* run `python mrlin_mr.py -r hadoop README.md` for local Hadoop \n\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Mon, 20 Dec 2021 21:31:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mhausenblas/mrlin/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mhausenblas/mrlin",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I assume you have HBase installed in some directory `HBASE_HOME` and mrlin in some other directory `MRLIN_HOME`. First let's make sure that Happybase is installed correctly - we will use a [virtualenv](http://pypi.python.org/pypi/virtualenv \"virtualenv 1.8.2 : Python Package Index\"). You only need to do this once: go to `MRLIN_HOME` and type:\n\n\t$ virtualenv hb\n\nTime to launch HBase and the Thrift server: in the `HBASE_HOME` directory, type the following:\n\n\t$ ./bin/start-hbase.sh \n\t$ ./bin/hbase thrift start -p 9191\n\nOK, now we're ready to launch mrlin - change to the directory `MRLIN_HOME` and first activate the virtualenv we created earlier:\n\n\t$ source hb/bin/activate\n\nYou should see a change in the prompt to something like `(hb)michau@~/Documents/dev/mrlin$` ... and this means we're good to go!\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8242789422373622
      ],
      "excerpt": "To import  RDF NTriples documents, use the mrlin import script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8573177258802974
      ],
      "excerpt": "2012-10-30T08:56:31  Import speed: 1506.61 triples per sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8904551800924276
      ],
      "excerpt": "2012-10-30T08:56:31  Import speed: 4059.10 triples per sec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8115852458859433,
        0.8846690803805363,
        0.8824267851962554
      ],
      "excerpt": "2012-10-30T04:01:22 Key: http://dbpedia.org/resource/Galway - Value: {'O:148': 'u\\'\"City of the Tribes\"\\'', 'O:66': 'u\\'\"City of the Tribes\"\\'',  ...} \n2012-10-30T04:01:22 ============ \n2012-10-30T04:01:22 Query took me 0.01 seconds. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mhausenblas/mrlin/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "mrlin - MapReduce processing of Linked Data",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "mrlin",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mhausenblas",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mhausenblas/mrlin/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* You'll need [Apache HBase](http://hbase.apache.org/) first. I downloaded [`hbase-0.94.2.tar.gz`](http://ftp.heanet.ie/mirrors/www.apache.org/dist/hbase/stable/hbase-0.94.2.tar.gz) and followed the [quickstart](http://hbase.apache.org/book/quickstart.html) up to section 1.2.3. to set it up.\n* The mrlin Python scripts depend on:\n * [Happybase](https://github.com/wbolster/happybase) to manage HBase; see also the [docs](http://happybase.readthedocs.org/en/latest/index.html) for further details.\n * [mrjob](https://github.com/Yelp/mrjob) to run MapReduce jobs; see also the [docs](http://packages.python.org/mrjob/) for further details.\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "*TBD*\n\n* setup in virtual env: `source hb/bin/activate` then `pip install mrjob`\n* `cp .mrjob.conf ~` before launch\n* `source hb/bin/activate`\n* run `python mrlin_mr.py README.md` for standalone\n* set up [Hadoop 1.0.4](http://ftp.heanet.ie/mirrors/www.apache.org/dist/hadoop/common/hadoop-1.0.4/hadoop-1.0.4.tar.gz) - if unsure follow a [single-node setup](http://orzota.com/blog/single-node-hadoop-setup-2/)  tutorial\n* `cp .mrjob.conf ~` before launch if you change settings (!)\n* note all changes that were necessary in ` conf/core-site.xml`, `conf/mapred-site.xml`, `conf/hdfs-site.xml`, and `hadoop-env.sh` (provide examples)\n* run `python mrlin_mr.py -r hadoop README.md` for local Hadoop \n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Mon, 20 Dec 2021 21:31:45 GMT"
    },
    "technique": "GitHub API"
  }
}