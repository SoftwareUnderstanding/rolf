{
  "citation": [
    {
      "confidence": [
        0.929687186305763
      ],
      "excerpt": "@http://lobid.org/resource/HT002189125 http://purl.org/dc/elements/1.1/creator http://d-nb.info/gnd/118580604 .@ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920594315055738
      ],
      "excerpt": "@http://d-nb.info/gnd/118580604 : http://lobid.org/resource/HT002189125@ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8854398367006624
      ],
      "excerpt": "@http://d-nb.info/gnd/118580604 http://d-nb.info/standards/elementset/gnd#preferredNameForThePerson \"Melville, Herman\" .@ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lobid/lodmill",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2012-11-05T11:36:04Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-26T00:28:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8025717258041685
      ],
      "excerpt": "This software implements the backend of \"http://lobid.org\":http://lobid.org, the LOD service offered by \"hbz\":http://www.hbz-nrw.de. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8436334399115106
      ],
      "excerpt": "See the @.travis.yml@ file for details on the CI config used by Travis. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8627168912455806
      ],
      "excerpt": "Raw data is transformed with \"Metafacture\":https://github.com/culturegraph/metafacture-core/wiki and \"Hadoop\":http://hadoop.apache.org/, indexed in \"Elasticsearch\":http://www.elasticsearch.org/, and exposed via an HTTP API implemented with the \"Play framework\":http://www.playframework.com/: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.995795928798799
      ],
      "excerpt": "The @lodmill-rd@ folder contains code for working with raw data and its transformation to LOD, based on the Culturegraph Metafacture toolkit. The @lodmill-ld@ folder contains code for processing RDF with Hadoop and for indexing it in Elasticsearch. The @hbz/lobid@ repo contains a web app based on the Play framework that interacts with the resulting Elasticsearch index. It provides an HTTP API to the index and a UI for documentation and basic sample usage. See below for details on the data workflow. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8103577874870261,
        0.8692425356752701
      ],
      "excerpt": "Prerequisites: Java 8 and Maven 3 (check @mvn -version@ to make sure Maven is using Java 8) \nTo set up a local build of the lodmill components follow these steps: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723434172034557
      ],
      "excerpt": "For information on how we have set up our Hadoop and Elasticsearch backend clusters see @README.textile@ in @lodmill-ld@. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9385331549653876
      ],
      "excerpt": "The complete data workflow transforms the original raw data to Linked Data served via an HTTP API: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9163232457464039,
        0.9866098558909372,
        0.9823421214671846
      ],
      "excerpt": "h2. Raw Data to Linked Data Triples \nThe raw data is transformed to linked data triples in the N-Triples RDF serialization. Being a line based format, N-Triples are well suited for batch processing with Hadoop. Every triple, both those generated from the raw data, and the enrichment data from external sources (like GND and Dewey), are stored in the Hadoop distributed file system (HDFS) and made available to the two Hadoop jobs that process and convert the data. \nh2. Linked Data Triples to Records \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8991139928595674
      ],
      "excerpt": "The creator of the resource is identified via its GND ID. To allow searching by the actual name of the creator, we want to resolve the name literals, so we declare that the @creator@ property needs to be resolved in the @resolve.properties@ file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8451918140976973
      ],
      "excerpt": "Having the property declared to need resolution, when the first Hadoop job encounters the triple above, it will map the GND ID to the resource ID: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9899125579390996
      ],
      "excerpt": "The second Hadoop job (implemented in @NTriplesToJsonLd.java@) collects all triples with the same subject (i.e. all statements about a resource), and converts them to a JSON-LD record. In addition to the selected triples, we also need details about the entities defined as needing resolution in the first job. For instance, we want the @preferredNameForThePerson@ of @creator@ in our records, so we declare it as a resolution property in @resolve.properties@: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9382597359939873
      ],
      "excerpt": "This will cause the second Hadoop job to perform a lookup on the subject of a triple containing that property, e.g.: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9976239901868347,
        0.8521113514639665
      ],
      "excerpt": "Since the subject is mapped to the @http://lobid.org/resource/HT002189125@ resource ID, we add that triple to the triples of that resource, which yields a record for @http://lobid.org/resource/HT002189125@ that contains not only the triples with that subject, but also the enrichment triples defined in the @resolve.properties@ file. That way, we effectively define our records to be subgraphs of the complete triple set. \nThe same mechanism is used to resolve information modeled using blank nodes. See our current \"resolve.properties file\":https://github.com/lobid/lodmill/blob/master/lodmill-ld/src/main/resources/resolve.properties for details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8030880424407117
      ],
      "excerpt": "The second Hadoop job writes the records as expanded JSON-LD in the \"Elasticsearch bulk import format\":http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-bulk.html, which is then indexed in Elasticsearch using the Elasticsearch bulk Java API. We use \"expanded JSON-LD\":http://www.w3.org/TR/json-ld-api/#expansion-algorithms in the index to have consistent types for each field. In compact JSON-LD, if a field has just a single value, the type of that field will simply be the type of the value. If the same field has multiple values, the type will be an array, etc. Elasticsearch learns the index schema from the data, so we need to use consistent types for a given field. The expanded JSON-LD serialization does exactly this. For instance, it always uses arrays, even if there is only a single value. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9686818116986767
      ],
      "excerpt": "Finally our Play frontend accesses the Elasticsearch index and serves the records as JSON-LD. There are multiple options for queries and different supported results formats, see documentation at \"http://lobid.org/api\":http://lobid.org/api (implemented in the @app/views/index.scala.html@ template in the @hbz/lobid@ repo). Since the expanded JSON-LD described above is cumbersome, the API serves \"compact JSON-LD\":http://www.w3.org/TR/json-ld-api/#compaction-algorithms. It also uses an external JSON-LD context document to allow shorter keys instead of full URI properties, and to encapsulate the actual properties used in the expanded form. That way, we can change the properties, without requiring API clients to change how they process the JSON responses. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repo is replaced by i.a. https://github.com/hbz/lobid-resources/",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lobid/lodmill/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Tue, 21 Dec 2021 00:01:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lobid/lodmill/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lobid/lodmill",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/copyHbz01Testset.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/install-dependencies.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/gnd/concatGndNtInFsAndMove2hdfs.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/gnd/startXmlSplitterRdfWriter.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/obvsg/startObvsg2lobidRdf.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/organisations/moveToHadoop.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/organisations/prerequisites.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/organisations/startLobidOrganisationsUpdateOaiPmh.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/hbz01/startHbz012lobidUpdateMysql.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/hbz01/wikidataNrwSettlements.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/hbz01/startHbz01ToLobidResources.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/hbz01/createTestSet.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/hbz01/mysql_bash_singleHT.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/doc/scripts/hbz01/stats/stats.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-rd/transformations/zvdd/statistic/harvestOAI_hbz_zvdd.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/scripts/processResources.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/scripts/processTest.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/scripts/processOrganisation.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/scripts/process-etherpad.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/scripts/indexEtherpad.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/scripts/convert.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/scripts/mysqlDumpTable.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/scripts/process.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/scripts/preprocessDewey.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/scripts/copyNwbibVocabsIntoHdfs.sh",
      "https://raw.githubusercontent.com/lobid/lodmill/master/lodmill-ld/doc/hadoop/sample/lobid-sample.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9096104964140866
      ],
      "excerpt": "h1. Build \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9072934417257499,
        0.8595400615156014
      ],
      "excerpt": "h1. Setup \nPrerequisites: Java 8 and Maven 3 (check @mvn -version@ to make sure Maven is using Java 8) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9348805199217399,
        0.9958795943885314,
        0.9096104964140866,
        0.9906248903846466
      ],
      "excerpt": "h6. Clone lodmill and run the Maven build \n@git clone https://github.com/lobid/lodmill.git; cd lodmill; echo \"required for MRUnit tests:\"; umask 0022; mvn clean install -DdescriptorId=jar-with-dependencies -DskipIntegrationTests --settings settings.xml@ \nh6. Build Hadoop Jar \n@cd .. ; cd lodmill-ld@ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lobid/lodmill/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Java",
      "Shell",
      "FLUX",
      "Puppet",
      "Pascal"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "preferredNameForThePerson; \\ [...]@",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "lodmill",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lobid",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lobid/lodmill/blob/master/README.textile",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 19,
      "date": "Tue, 21 Dec 2021 00:01:33 GMT"
    },
    "technique": "GitHub API"
  }
}