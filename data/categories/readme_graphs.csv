Label;Text
Graphs;google research repository contains code released google datasets repository released cc 40 international license found source file repository released apache 20 license text found license file repo large recommend download subdirectory interest subdirfoo svn export youd like submit pull request youll need clone repository recommend making shallow clone without history git clone gitgithubcomgoogleresearchgoogleresearchgit depth1 disclaimer official google product
Graphs;largescale learnable graph convolutional networkslgcn created hongyang zhengyang shuiwang texas university accepted kdd18 introduction largescale learnable graph convolutional network provide efficient way lgcl lgcn learnable graph convolution detailed information lgcl lgcn provided method work propose learnable graph convolution layer lgcl based lgcl propose learnable graph convolutional network learnable graph convolution layer lgcldoclayerpng learnable graph convolutional network lgcndocmodelpng batch training batchdocbatchpng citation using code please cite paper inproceedingsgao2018large titlelargescale learnable graph convolutional network authorgao hongyang wang zhengyang ji shuiwang booktitleproceedings 24th acm sigkdd international conference knowledge discovery data mining pages14161424 year2018 organizationacm start training configure network start train run python mainpy training result cora dataset displayed result model cora citeseer pubmed deepwalk 672 432 653 planetoid 757 647 772 chebyshev 812 698 744 gcn 815 703 790 lgcn 833 ± 05 730 ± 06 795 ± 02
Graphs;graph convolutional network relational graph kerasbased implementation relational graph convolutional network semisupervised node classification directed relational graph reproduction entity classification result paper modeling relational data graph convolutional 2017 1 see instruction code link prediction task 1 found following repository installation python setuppy install dependency important kera 20 higher supported break theano sparse matrix api tested kera 121 theano 090 version might work well theano 090 kera 121 panda rdflib note possible use tensorflow backend kera well note kera 121 us tensorflow 011 api using tensorflow backend limit maximum allowed size sparse matrix therefore experiment might throw error usage important switch kera backend theano disable gpu execution gpu memory limited experiment gpu speedup sparse operation essential running model cpu still quite fast replicate experiment paper 1 first run aifb python preparedatasetpy aifb afterwards train model python trainpy aifb base 0 hidden 16 l2norm 0 testing note theano performs expensive compilation step first time computational graph executed take several minute complete mutag dataset run python preparedatasetpy mutag python trainpy mutag base 30 hidden 16 l2norm 5e4 testing bgs run python preparedatasetpy bgs python trainpy bgs base 40 hidden 16 l2norm 5e4 testing run python preparedatasetpy python trainpy base 40 hidden 10 l2norm 5e4 testing note result depend random seed vary rerun setting kera backend theano create file keraskerasjson content imagedimordering tf epsilon 1e07 floatx float32 backend theano enforcing cpu execution enforce execution cpu hiding gpu resource cudavisibledevices python trainpy aifb base 0 hidden 16 l2norm 0 testing reference 1 schlichtkrull n kipf p bloem r van den berg titov welling modeling relational data graph convolutional 2017 cite please cite paper use code work articleschlichtkrull2017modeling titlemodeling relational data graph convolutional network authorschlichtkrull michael kipf thomas n bloem peter berg rianne van den titov ivan welling max journalarxiv preprint arxiv170306103 year2017
Graphs;ggnnreasoning pytorch implementantion gated graph neural li yujia et al gated graph sequence neural network arxiv preprint arxiv151105493 2015 implementation follows framework main difference implemantation suitable graph datasets tremendous edge type knowledge graph memoryefficient note implementation find designed datasets several edge type babi though scenario using ggnn approximate abox consistency checking problem owl2 el abox sample deemed small directed graph thus consistency checking modeled graphlevel binary classification problem implementation quite generic requirement python 36 br pytorch 04 br usage input json data formatbr sample format followsbr target label sample br graph edge graph edge represented triple sourceid edgeid targetid br nodefeatures taskspecific innitial annotation node graph br br id start 1 run code please use command python mainpy run gpu please use command python mainpy cuda br general use care file without suffix plus file specific use abox reasoning model specifically ggnnplus need specify initial annotation node annotation node stored embedding layer also learnable training process experiment demonstrate ggnnplus outperforms ggnn abox reasoning term efficiency effectiveness
Graphs;graph convolutional network relational link prediction repository contains tensorflow implementation relational graph convolutional network rgcn well experiment relational link prediction description model result found paper modeling relational data graph convolutional michael schlichtkrull thomas n kipf peter bloem rianne van den berg ivan titov max welling arxiv 2017 requirement tensorflow 10 running demo provide bash script run demo code folder setting collection configuration file found block diagonal model used paper represented configuration file settingsgcnblockexp run given experiment execute bash script follows bash runtrainsh configuration advise training take several hour require significant amount memory citation please cite paper use code work articleschlichtkrull2017modeling titlemodeling relational data graph convolutional network authorschlichtkrull michael kipf thomas n bloem peter berg rianne van den titov ivan welling max journalarxiv preprint arxiv170306103 year2017
Graphs;multihop graph relation network emnlp 2020 img width10 license repo emnlp20 scalable multihop relational reasoning knowledgeaware question answering yanlin feng xinyue chen bill yuchen lin peifeng wang jun yan xiang ren emnlp 2020 equal contritbution repository also implement graph encoding model question answering including vanilla lm finetuning relationnet rgcn kagnet gconattn kvmem mhgrn multigrn model support following text encoders lstm gpt bert xlnet roberta resource provide preprocessed conceptnet pretrained entity embeddings usage resource independent source code note following reousrces download conceptnet 560 description downloads note entity vocab one entity per line space replaced relation vocab one relation per line merged conceptnet csv format english tuples extracted full conceptnet merged relation conceptnet networkx format networkx pickled format pruned filtering stop word entity embeddings node feature entity embeddings packed matrix shape ent dim stored numpy format use npload read file may need download vocabulary file first embedding model dimensionality description downloads transe 100 obtained using openke optimsgd lr1e3 epoch1000 numberbatch 300 bertbased 1024 provided zhengwei dependency 36 110 200 031 gpu version 23 run following command create conda environment assume cuda10 bash conda create n krqa python36 numpy matplotlib ipython source activate krqa conda install pytorch110 torchvision cudatoolkit100 c pytorch pip install dglcu100031 pip install transformers200 tqdm networkx23 nltk spacy216 python spacy download en usage 1 download data first need download necessary data order train model bash git clone cd mhgrn bash scriptsdownloadsh script download dataset download download pretrained transe embeddings 2 preprocess preprocess data run bash python preprocesspy default available cpu core used multiprocessing order speed process alternatively use p specify number process use bash python preprocesspy p 20 script convert original datasets jsonl file stored datacsqastatement extract english relation conceptnet merge original 42 relation type 17 type identify mentioned concept question answer extract subgraphs qa pair preprocessing procedure take approximately 3 hour 40core cpu server intermediate file jsonl pk format stored various folder resulting file structure look like plain ├── readmemd └── data ├── cpnet prerocessed conceptnet ├── glove pretrained glove embeddings ├── transe pretrained transe embeddings └── csqa ├── trainrandsplitjsonl ├── devrandsplitjsonl ├── testrandsplitnoanswersjsonl ├── statement converted statement ├── grounded grounded entity ├── path unprunedpruned path ├── graph extracted subgraphs ├── 3 hyperparameter search optional search parameter robertalarge commonsenseqa bash bash scriptsparamsearchlmsh csqa robertalarge search parameter bertrelationnet commonsenseqa bash bash scriptsparamsearchrnsh csqa bertlargeuncased 4 training graph encoding model implemented single script graph encoder script description none lmpy wo knowledge graph relation rnpy rgcnpy use gnnlayernum numbasis specify layer basis kagnetpy adapted still tuning gconattn gconattnpy kvmemory kvmempy mhgrn grnpy important command line argument listed follows run python lmrnrgcnpy h complete list arg value description note mode train eval training evaluation defaulttrain enc encoder lstm openaigpt bertlargeunased robertalarge text encoer model name except lstm one used defaultbertlargeuncased optim adam adamw radam optimizer defaultradam d dataset csqa obqa dataset defaultcsqa ih inhouse 0 1 run inhouse split default1 applicable csqa entemb transe numberbatch tzw entity embeddings defaulttzw bertbased node feature sl maxseqlen 32 64 128 256 maximum sequence length use 128 256 datasets contain long sentence default64 elr encoderlr 1e5 2e5 3e5 6e5 1e4 text encoder lr dataset specific text encoder specific default value utilsparserutilspy dlr decoderlr 1e4 3e4 1e3 3e3 graph encoder lr dataset specific model specific default value modelpy lrschedule fixed warmuplinear warmupconstant learning rate schedule defaultfixed maxepochsbeforestop 2 4 6 early stopping patience default2 unfreezeepoch 0 3 freeze text encoder n epoch model specific b batchsize 16 32 64 batch size default32 savedir str checkpoint directory model specific seed 0 1 2 3 random seed default0 example run following command train robertalarge model commonsenseqa bash python lmpy encoder robertalarge dataset csqa train relationnet bertlargeuncased encoder bash python rnpy encoder bertlargeuncased reproduce reported result multigrn commonsenseqa official set bash scriptsrungrncsqash 5 evaluation evaluate trained model need specify savedir checkpoint stored default directory bash python lmrnrgcnpy mode eval savedir pathtodirectory use dataset convert dataset traindevteststatementjsonl jsonl format see datacsqastatementtrainstatementjsonl create directory datayourdataset store jsonl file modify preprocesspy perform subgraph extraction data modify utilsparserutilspy support dataset tune encoderlrdecoderlr important hyperparameters modify utilsparserutilspy modelpy record tuned hyperparameters
Graphs;pytorchgat pytorch implementation graph attention networkbr paper addressgraph attention network veličković et al iclr 2018 implementation based official code graph attention network get performance original code deepen understanding tensorflow pytorch want better performance refer tobr official implementation another pytorch implementation kera learn convert tensorflow code pytorch code br introduction utilspy read data data processingbr layerpy attention layerbr modelpy graph attention model networkbr mainpy training validation testingbr run through： python python mainpy result refer another implementation pytorch order make easier compare code tensorflow version code constructed according tensorflow structurebr following result running official code python dataset cora opt hyperparams lr 0005 l2coef 00005 archi hyperparams nb layer 1 nb unit per layer 8 nb attention head 8 1 residual false nonlinearity function elu 0x7f1b7507af28 model class modelsgatgat 2708 2708 2708 1433 epoch 1 training loss 194574 acc 014286 val loss 193655 acc 013600 epoch 2 training loss 194598 acc 015714 val loss 193377 acc 014800 epoch 3 training loss 194945 acc 014286 val loss 193257 acc 019600 epoch 4 training loss 193438 acc 024286 val loss 193172 acc 022800 epoch 5 training loss 193199 acc 017143 val loss 193013 acc 036400 。。。。。。 epoch 674 training loss 123833 acc 049286 val loss 101357 acc 081200 early stop min loss 1010906457901001 max accuracy 08219999074935913 early stop model validation loss 13742048740386963 accuracy 08219999074935913 test loss 13630210161209106 test accuracy 08219999074935913 following result running code python 2708 2708 2708 1433 训练节点个数： 140 验证节点个数： 500 测试节点个数： 1000 epoch001trainloss79040trainacc00000valloss79040valacc00000 epoch002trainloss79040trainacc00000valloss79039valacc01920 epoch003trainloss79039trainacc00714valloss79039valacc01600 epoch004trainloss79038trainacc01000valloss79039valacc01020 。。。。。。 epoch2396trainloss70191trainacc08929valloss74967valacc07440 epoch2397trainloss70400trainacc08786valloss74969valacc07580 epoch2398trainloss70188trainacc08929valloss74974valacc07580 epoch2399trainloss70045trainacc09071valloss74983valacc07620 epoch2400trainloss70402trainacc08714valloss74994valacc07620 testloss74805testacc07700 following result br loss change epoch br acc change epoch br dimensionality reduction visualization test result br
Graphs;based article semisupervised classification graph convolutional network thomas n kipf max welling link code tensorflow implementation gcn try handle graphclassification task two similar way 1 global node approach graph global node added global node classified 2 global mean pooling approacah added meanpooling layer last layer softmax gcn approach start sparse blockdiagonal version adjacency matrix gather graph dataset see better explanation figure result protein dataset setting code obtain average classification accuracy 788 27 standard deviation two approach yield close result accuracy average 100 result test conducted using 1000 graph training set 100 test set random split
Graphs;lightgcnpytorch pytorch implementation sigir 2020 paper sigir 2020 xiangnan kuan deng xiang wang yan li yongdong zhang meng wang2020 lightgcn simplifying powering graph convolution network recommendation paper author prof xiangnan staffustceducnhexn also see tensorflow introduction work aim simplify design gcn make concise appropriate recommendation propose new model named lightgcnincluding essential component gcn—neighborhood aggregation—for collaborative filtering enviroment requirement pip install r requirementstxt dataset provide three processed datasets gowalla yelp2018 amazonbook one small dataset lastfm see dataloaderpy example run 3layer lightgcn run lightgcn gowalla dataset command cd code python mainpy decay1e4 lr0001 layer3 seed2020 datasetgowalla topks20 recdim64 log output shell epoch51000 bprsample time1621584042 savedbpraver loss1128e01 03043mtest0m precision array003315359 recall array010711388 ndcg array008940792 total time 359975962638855 epoch1161000 bprsample time1691660045 savedbpraver loss2056e02 total time 3099874997138977 note 1 even though offer code split useritem matrix matrix multiplication strongly suggest dont enable since extremely slow training speed 2 feel test process slow try increase testbatch enable multicorewindows system may encounter problem multicore option enabled 3 use tensorboard option good 4 since fix seedseed2020 numpy torch beginning run command exact output log despite running time check output epoch 5 epoch 116 extend want run lightgcn dataset go dataloaderpy implement dataloader inherited basicdataset register registerpy want run model datasets offer go modelpy implement model inherited basicmodel register registerpy want run sampling method datasets model offer go procedurepy implement function modify corresponding code mainpy result metric top20 tensorflow version result pytorch version result stop 1000 epoch seed2020 gowalla recall ndcg precision layer1 01687 01417 005106 layer2 01786 01524 005456 layer3 01824 01547 005589 layer4 01825 01537 005576 note layers4 use seed1000 attain better performance yelp2018 recall ndcg precision layer1 005604 04557 002519 layer2 005988 004956 00271 layer3 006347 005238 00285 layer4 006515 005325 002917 want welltrained model please email gusye mailustceducn
Graphs;hyperbolic graph convolutional network pytorch 1 overview repository graph representation learning library containing implementation hyperbolic graph convolution pytorch well multiple embedding approach including shallow method shallow shallow euclidean shallow hyperbolic shallow euclidean feature see shallow hyperbolic feature see neural network nn method multilayer perceptron mlp hyperbolic neural network hnn graph neural network gnn method graph convolutional neural network gcn graph attention network gat hyperbolic graph convolution hgcn model trained link prediction lp node classification nc 2 setup 21 installation conda dont conda installed please install following instruction git clone cd hgcn conda env create f environmentyml 22 installation pip alternatively prefer install dependency pip please follow instruction virtualenv p path python37 binary hgcn source hgcnbinactivate pip install r requirementstxt 23 datasets data folder contains source file cora pubmed disease airport run code new datasets please add corresponding data processing loading loaddatanc loaddatalp function utilsdatautilspy 3 usage 31 setenvsh training run source setenvsh create environment variable used code 32 trainpy script train model link prediction node classification task metric printed end training saved directory adding command line argument save1 optional argument h help show help message exit lr lr learning rate dropout dropout dropout probability cuda cuda cuda device use 1 cpu training epoch epoch maximum number epoch train weightdecay weightdecay l2 regularization strength optimizer optimizer optimizer use adam riemannianadam momentum momentum momentum optimizer patience patience patience early stopping seed seed seed training logfreq logfreq often compute print trainval metric epoch evalfreq evalfreq often compute val metric epoch save save 1 save model log 0 otherwise savedir savedir path save training log model weight default logstaskdaterun sweepc sweepc lrreducefreq lrreducefreq reduce lr every lrreducefreq none keep lr constant gamma gamma gamma lr scheduler printepoch printepoch gradclip gradclip max norm gradient clipping none gradient clipping minepochs minepochs early stop minepochs task task task train lp nc model model encoder use shallow mlp hnn gcn gat hgcn dim dim embedding dimension manifold manifold manifold use euclidean hyperboloid poincareball c c hyperbolic radius set none trainable curvature r r fermidirac decoder parameter lp fermidirac decoder parameter lp pretrainedembeddings pretrainedembeddings path pretrained embeddings npy file shallow node classification posweight posweight whether upweight positive class node classification task numlayers numlayers number hidden layer encoder bias bias whether use bias 1 0 act act activation function use none activation nheads nheads number attention head graph attention network must divisor dim alpha alpha alpha leakyrelu graph attention network useatt useatt whether use hyperbolic attention hgcn model doubleprecision doubleprecision whether use double precision dataset dataset dataset use valprop valprop proportion validation edge link prediction testprop testprop proportion test edge link prediction usefeats usefeats whether use node feature normalizefeats normalizefeats whether normalize input node feature normalizeadj normalizeadj whether rownormalize adjacency matrix splitseed splitseed seed data split traintestval 4 example provide example training command used train hgcn graph embedding model link prediction node classification example used fixed random seed set 1234 reproducibility purpose note result might slightly vary based machine used reproduce result paper run commad 10 random seed average result 41 training hgcn link prediction cora test rocauc9379 python trainpy task lp dataset cora model hgcn lr 001 dim 16 numlayers 2 act relu bias 1 dropout 05 weightdecay 0001 manifold poincareball logfreq 5 cuda 0 c none pubmed test rocauc 9517 python trainpy task lp dataset pubmed model hgcn lr 001 dim 16 numlayers 2 act relu bias 1 dropout 04 weightdecay 00001 manifold poincareball logfreq 5 cuda 0 disease test rocauc 8714 python trainpy task lp dataset diseaselp model hgcn lr 001 dim 16 numlayers 2 numlayers 2 act relu bias 1 dropout 0 weightdecay 0 manifold poincareball normalizefeats 0 logfreq 5 airport test rocauc9743 python trainpy task lp dataset airport model hgcn lr 001 dim 16 numlayers 2 act relu bias 1 dropout 00 weightdecay 0 manifold poincareball logfreq 5 cuda 0 c none node classification cora pubmed train train hgcn node classification model cora pubmed datasets pretrain embeddings link prediction decribed previous section train mlp classifier using pretrained embeddings embeddingsnpy file saved savedir directory instance pubmed dataset python trainpy task nc dataset pubmed model shallow lr 001 dim 16 numlayers 2 act relu bias 1 dropout 02 weightdecay 00005 manifold euclidean logfreq 5 cuda 0 usefeats 0 pretrainedembeddings pathtoembeddings disease test accuracy 7677 python trainpy task nc dataset diseasenc model hgcn dim 16 lr 001 dim 16 numlayers 2 act relu bias 1 dropout 0 weightdecay 0 manifold poincareball logfreq 5 cuda 0 42 train graph embedding model link prediction cora dataset shallow euclidean test rocauc8640 python trainpy task lp dataset cora model shallow manifold euclidean lr 001 weightdecay 00005 dim 16 numlayers 0 usefeats 0 dropout 02 act none bias 0 optimizer adam cuda 0 shallow hyperbolic test rocauc8597 python trainpy task lp dataset cora model shallow manifold poincareball lr 001 weightdecay 00005 dim 16 numlayers 0 usefeats 0 dropout 02 act none bias 0 optimizer riemannianadam cuda 0 gcn test rocauc8922 python trainpy task lp dataset cora model gcn lr 001 dim 16 numlayers 2 act relu bias 1 dropout 02 weightdecay 0 manifold euclidean logfreq 5 cuda 0 hnn test rocauc9079 python trainpy task lp dataset cora model hnn lr 001 dim 16 numlayers 2 act none bias 1 dropout 02 weightdecay 0001 manifold poincareball logfreq 5 cuda 0 c 1 node classification pubmed dataset hnn test accuracy6820 python trainpy task nc dataset pubmed model hnn lr 001 dim 16 numlayers 2 act none bias 1 dropout 05 weightdecay 0 manifold poincareball logfreq 5 cuda 0 mlp test accuracy7300 python trainpy task nc dataset pubmed model mlp lr 001 dim 16 numlayers 2 act none bias 0 dropout 02 weightdecay 0001 manifold euclidean logfreq 5 cuda 0 gcn test accuracy7830 python trainpy task nc dataset pubmed model gcn lr 001 dim 16 numlayers 2 act relu bias 1 dropout 07 weightdecay 00005 manifold euclidean logfreq 5 cuda 0 gat test accuracy7850 python trainpy task nc dataset pubmed model gat lr 001 dim 16 numlayers 2 act elu bias 1 dropout 05 weightdecay 00005 alpha 02 nheads 4 manifold euclidean logfreq 5 cuda 0 citation find code useful please cite following paper inproceedingschami2019hyperbolic titlehyperbolic graph convolutional neural network authorchami ines ying zhitao christopher leskovec jure booktitleadvances neural information processing system pages48694880 year2019 code forked following repository reference 1 chami ying r ré c leskovec j hyperbolic graph convolutional neural network nip 2 nickel kiela poincaré embeddings learning hierarchical representation nip 3 ganea bécigneul g hofmann hyperbolic neural network nip 4 kipf tn welling semisupervised classification graph convolutional network iclr 5 veličković p cucurull g casanova romero lio p bengio graph attention network iclr
Graphs;documentation python build pypi github coverage pykg2vec python library kge method pykg2vec library learning representation entity relation knowledge graph built top pytorch 15 tf2 version available branch well attempted bring stateoftheart knowledge graph embedding kge algorithm necessary building block pipeline knowledge graph embedding task single library hope pykg2vec practical educational people want explore related field feature support stateoftheart kge model implementation benchmark datasets also support custom datasets support automatic discovery hyperparameters tool inspecting learned embeddings support exporting learned embeddings tsv pandassupported format interactive result inspector tsnebased kpi summary visualization mean rank hit ratio various format csvs figure latex table welcome form contribution please refer detail get started using pykg2vec recommend user following library installed python 37 recommended pytorch 15 quick guide anaconda user setup virtual environment encourage use anaconda work pykg2vec bash base conda create name pykg2vec python37 base conda activate pykg2vec setup pytorch encourage use pytorch gpu support good training performance however cpu version also run following sample command setting pytorch bash gpu cuda 101 installed pykg2vec conda install pytorch torchvision cudatoolkit101 c pytorch cpuonly pykg2vec conda install pytorch torchvision cpuonly c pytorch setup pykg2vec bash pykg2vec git clone pykg2vec cd pykg2vec pykg2vec python setuppy install beginner paper review relational machine learning knowledge knowledge graph embedding survey approach overview embedding model entity relationship knowledge base good starting point user documentation documentation usage example pykg2vec commandline interface 1 run single algorithm various model datasets customized dataset also supported check tunnable parameter pykg2vec pykg2vectrain h train transe fb15k benchmark dataset pykg2vec pykg2vectrain mn transe train using different kge method pykg2vec pykg2vectrain mn transetransdtranshtransgtransmtransrcomplexcomplexn3 cprotateanalogydistmultkg2ekg2eelntnrescalslmsmesmeblhole conveconvkbprojepointwisemurpquateoctonioneinteractehyper kge using projectionbased loss function use process batch generation pykg2vec pykg2vectrain mn conveconvkbprojepointwise npg number process 4 6 train transe model using different benchmark datasets pykg2vec pykg2vectrain mn transe d fb15kwn18wn18rryago310fb15k237ksnationsumlsdl50anell955 train transe model using hyperparameters pykg2vec pykg2vectrain exp true mn transe d fb15k hpf examplescustomhpyaml use dataset pykg2vec pykg2vectrain mn transe d name dsp path custom dataset 2 tune single algorithm tune transe using benchmark dataset pykg2vec pykg2vectune mn transe d dataset name tune transe search space pykg2vec pykg2vectune exp true mn transe d fb15k ssf examplescustomssyaml 3 perform inference task advanced train model perform inference task pykg2vec pykg2vecinfer mn transe perform inference task pretrained model pykg2vec pykg2vecinfer mn transe ld path pretrained model nb window use pykg2vectrainexe pykg2vectuneexe pykg2vecinferexe instead usage pykg2vec apis please check programming citation please kindly consider citing paper find pykg2vec useful research articleyu2019pykg2vec titlepykg2vec python library knowledge graph embedding authoryu shih yuan rokka chhetri sujit canedo arquimedes goyal palash faruque mohammad abdullah al journalarxiv preprint arxiv190604239 year2019
Graphs;graph autoencoders tensorflow implementation variational graph autoencoder model described paper n kipf welling variational graph nip workshop bayesian deep learning 2016 graph autoencoders gaes endtoend trainable neural network model unsupervised learning clustering link prediction graph variational graph autoencoderfigurepng gaes successfully used link prediction largescale relational data schlichtkrull n kipf et al modeling relational data graph convolutional 2017 matrix completion recommendation side information r berg et al graph convolutional matrix 2017 gaes based graph convolutional network gcns recent class model endtoend semisupervised learning graph n kipf welling semisupervised classification graph convolutional iclr 2017 highlevel introduction given blog post thomas kipf graph convolutional 2016 installation bash python setuppy install requirement tensorflow 10 later python 27 networkx scikitlearn scipy run demo bash python trainpy data order use data provide n n adjacency matrix n number node n feature matrix number feature per node optional look loaddata function inputdatapy example example load citation network data cora citeseer pubmed original datasets found different format specify dataset follows bash python trainpy dataset citeseer editing trainpy model choose following model gcnae graph autoencoder gcn encoder gcnvae variational graph autoencoder gcn encoder cite please cite paper use code work articlekipf2016variational titlevariational graph autoencoders authorkipf thomas n welling max journalnips workshop bayesian deep learning year2016
Graphs;pytorch graph attention network pytorch implementation graph attention network gat model presented veličković et al 2017 repo forked initially simplifying class spgraphattentionlayer using pytorch sparse operator please cite following article velickovic2018graph titlegraph attention network authorvelivckovic petar cucurull guillem casanova arantxa romero adriana lio pietro bengio yoshua journalinternational conference learning representation year2018 noteaccepted poster branch master contains implementation using sparse operator
Graphs;repo build coverage p aligncenter img width90 p karate club unsupervised machine learning extension library please look relevant promo external karate club consists stateoftheart method unsupervised learning graph structured data put simply swiss army knife smallscale graph mining research first provides network embedding technique node graph level second includes variety overlapping nonoverlapping community detection method implemented method cover wide range network science data mining artificial intelligence machine learning conference workshop piece prominent journal newly introduced graph classification datasets available tud graph kernel citing find karate club new datasets useful research please consider citing following paper bibtex inproceedingskarateclub title karate club api oriented opensource python framework unsupervised learning graph author benedek rozemberczki oliver kiss rik sarkar year 2020 page 3125–3132 booktitle proceeding 29th acm international conference information knowledge management cikm 20 organization acm simple example karate club make use modern community detection technique quite easy see accompanying tutorial example take use wattsstrogatz graph python import networkx nx karateclub import egonetsplitter g nxnewmanwattsstrogatzgraph1000 20 005 splitter egonetsplitter10 splitterfitg printsplittergetmemberships model included detail following community detection embedding method implemented overlapping community detection ye et al deep autoencoderlike nonnegative matrix factorization community cikm 2018 wang et al community preserving network aaai 2017 epasto et al egosplitting framework nonoverlapping overlapping kdd 2017 sun et al nonnegative symmetric encoderdecoder approach community cikm 2017 yang leskovec overlapping community detection scale nonnegative matrix factorization wsdm 2013 kuang et al symmetric nonnegative matrix factorization graph sdm 2012 nonoverlapping community detection rozemberczki et al gemsec graph embedding self asonam 2019 li et al edmot edge enhancement approach motifaware community kdd 2019 pratperez et al high quality scalable parallel community detectionfor large real www 2014 label raghavan et al near linear time algorithm detect community structure largescale physic review e 2007 proximity preserving node embedding cao et al grarep learning graph representation global structural cikm 2015 perozzi et al deepwalk online learning social kdd 2014 grover et al node2vec scalable feature learning kdd 2016 tang et al relational learning via latent social dimensionsttpwwwpublicasueduhuanliupaperskdd09pdf kdd 2009 torres et al glee geometric laplacian eigenmap journal complex network 2020 li et al multilevel network embedding boosted lowrank matrix asonam 2019 yang et al nodesketch highlyefficient graph embeddings via recursive kdd 2019 rozemberczki sarkar fast sequence based embedding diffusion complenet 2018 qiu et al network embedding matrix factorization unifying deepwalk line pte wsdm 2018 zhang et al billionscale network embedding iterative random icdm 2018 perozzi et al dont walk skip online learning multiscale network asonam 2017 ou et al asymmetric transitivity preserving graph kdd 2016 sun févotte alternating direction method multiplier nonnegative matrix factorization icassp 2014 laplacian belkin niyogi laplacian eigenmaps spectral technique embedding nip 2001 structural node level embedding donnat et al learning structural node embeddings via diffusion kdd 2018 ahmed et al learning rolebased graph ijcai starai 2018 attributed node level embedding rozemberczki et al characteristic function graph bird feather statistical descriptor parametric cikm 2020 yang et al network representation learning rich text ijcai 2015 rozemberczki et al multiscale attributed node arxiv 2019 rozemberczki et al multiscale attributed node arxiv 2019 bandyopadhyay et al fusing structure content via nonnegative matrix factorization embedding information arxiv 2018 zhang et al sine scalable incomplete network icdm 2018 yang et al binarized attributed network icdm 2018 yang et al enhanced network embedding text icpr 2018 liao et al attributed social network tkde 2018 meta node embedding yang et al fast network embedding enhancement via high order proximity ijcai 2017 graph level embedding rozemberczki et al characteristic function graph bird feather statistical descriptor parametric cikm 2020 narayanan et al graph2vec learning distributed representation mlgworkshop 2017 tsitsulin et al netlsd hearing shape kdd 2018 wang et al graph embedding via diffusionwaveletsbased node feature distribution cikm 2021 galland et al invariant embedding graph icml 2019 lrgsd workshop cai et al simple yet effective baseline nonattributed graph iclr 2019 gao et al geometric scattering graph data icml 2019 chen koga gl2vec graph embedding enriched line graph edge iconip 2019 de lara pineau simple baseline algorithm graph neurips rrl workshop 2018 verma zhang hunt unique stable sparse fast feature learning neurips 2017 head find installation data handling full list implemented method datasets quick start check notice anything unexpected please open let u know missing specific method feel free open feature motivated constantly make karate club even better installation karate club installed following pip command sh pip install karateclub create new release frequently upgrading package casually might beneficial sh pip install karateclub upgrade running example part documentation provide number use case show clustering embeddings utilized downstream learning accessed detailed linebyline explanation besides case study provide synthetic example model tried running example script order run one example graph2vec snippet sh cd exampleswholegraphembedding python graph2vecexamplepy running test sh python setuppy test license gnu general public license
Graphs;benchmark dataset graph classification repository contains datasets quickly test graph classification algorithm graph kernel graph neural network purpose dataset make feature node adjacency matrix completely uninformative considered alone therefore algorithm relies node feature graph structure fail achieve good classification result dataset detail dataset consists graph belonging 3 different class number node graph variable feature vector node onehot vector size 5 encodes color node class determined relative position color graph 4 version dataset smalleasy 100 graph per class number node varying 40 80 highly connected graph easy 600 graph per class number node varying 100 200 highly connected graph smallhard 100 graph per class number node varying 40 80 sparse graph hard 600 graph per class number node varying 100 200 sparse graph hard dataset necessary consider higher order neighborhood understand correct class graph might disconnected dataset class graph tr size val size test size avg node avg edge node attr dim easysmall 3 300 239 30 31 5825 3588 5 hardsmall 3 300 245 29 26 5864 22494 5 easy 3 1800 1475 162 163 14782 92266 5 hard 3 1800 1451 159 190 14832 57232 5 format dataset already split training validation classification set set contains list adjacency matrix csrmatrix format list node feature numpy array class label contained numpy array following code snippet show load data python import numpy np loaded nploaddatasetshardnpz allowpickletrue xtrain loadedtrfeat node feature atrain listloadedtradj list adjacency matrix ytrain loadedtrclass class label xval loadedvalfeat node feature aval listloadedvaladj list adjacency matrix yval loadedvalclass class label xtest loadedtefeat node feature atest listloadedteadj list adjacency matrix ytest loadedteclass class label optional convert networkx format import networkx nx gtrain x zipatrain xtrain g nxfromscipysparsematrixa xtuple tuplemaptuple x nxsetnodeattributesg dictenumeratextuple feature gtrainappendg gval x zipaval xval g nxfromscipysparsematrixa xtuple tuplemaptuple x nxsetnodeattributesg dictenumeratextuple feature gvalappendg gtest x zipatest xtest g nxfromscipysparsematrixa xtuple tuplemaptuple x nxsetnodeattributesg dictenumeratextuple feature gtestappendg result classification result obtained using graph kernel technique reported feel free send pull request result youd like share graph kernel graph kernel computed library kernel computed svm us precomputed kernel graph kernel trained evaluated test data svm implementation module used code used generate result found repository dependecies run notebook scikitlearn pip install sklearn networkx pip install networkx grakel pip install grakeldev easysmall hardsmall shortest path accuracy 100 time 2067 accuracy 6923 time 785 graphlet sampling accuracy 4194 time 28135 accuracy 3846 time 3784 pyramid match accuracy 5161 time 291 accuracy 2308 time 286 svm theta accuracy 3226 time 334 accuracy 2308 time 291 neighborhood hash accuracy 9032 time 273 accuracy 6923 time 271 subtree wl accuracy 2903 time 001 accuracy 1538 time 003 odd sth accuracy 7742 time 5875 accuracy 4231 time 2448 propagation accuracy 871 time 335 accuracy 5385 time 261 vertex histogram accuracy 2903 time 002 accuracy 1538 time 001 weisfeiler lehman accuracy 100 time 15181 accuracy 7308 time 5892 core framework accuracy 100 time 6218 accuracy 6923 time 1862 graph neural network result obtained following gnn architecture mp32poolmp32poolmp32globalpooldensesoftmax mp messagepassing architecture chebyshev convolutional layer 1 k1 32 hidden unit used result refer different graph pooling layer graclus 2 node decimation pooling ndp 3 diffpool 4 topk pooling 5 sagpool 6 mincutpool 7 easy hard graclus accuracy 975 ± 05 accuracy 690 ± 15 ndp accuracy 979 ± 05 accuracy 726 ± 09 diffpool accuracy 986 ± 04 accuracy 699 ± 19 topk accuracy 824 ± 89 accuracy 427 ± 152 sagpool accuracy 842 ± 23 accuracy 377 ± 145 mincutpool accuracy 990 ± 00 accuracy 738 ± 19 embedding simplicial complex esc technique proposed 8 easysmall hardsmall esc rbfsvm accuracy 7419 ± 684 time 068 accuracy 4846 ± 843 time 048 esc l1svm accuracy 9419 ± 270 time 068 accuracy 7077 ± 583 time 048 esc l2svm accuracy 9226 ± 289 time 068 accuracy 6923 ± 544 time 048 easy hard esc rbfsvm accuracy 8037 ± 704 time 1094 accuracy 6253 ± 458 time 1665 esc l1svm accuracy 9607 ± 093 time 1094 accuracy 7221 ± 101 time 1665 esc l2svm accuracy 9337 ± 196 time 1094 accuracy 6926 ± 185 time 1665 hypergraph kernel technique proposed 9 easysmall hardsmall hist kernel accuracy 094 ± 002 time 072 accuracy 077 ± 002 time 046 jaccard kernel accuracy 094 ± 00 time 086 accuracy 068 ± 002 time 054 edit kernel accuracy 091 ± 001 time 997 accuracy 06 ± 002 time 770 stratedit kernel accuracy 094 ± 00 time 514 accuracy 058 ± 002 time 479 easy hard hist kernel accuracy 094 ± 001 time 1039 accuracy 072 ± 001 time 693 jaccard kernel accuracy 094 ± 001 time 1415 accuracy 063 ± 00 time 811 edit kernel accuracy 093 ± 00 time 278447 accuracy 06 ± 00 time 218341 stratedit kernel accuracy 093 ± 00 time 93296 accuracy 06 ± 001 time 95487 reference 1 defferrard bresson x vandergheynst p 2016 convolutional neural network graph fast localized spectral filtering advance neural information processing system 2 dhillon guan kulis b 2007 weighted graph cut without eigenvectors multilevel approach ieee transaction pattern analysis machine intelligence 3 bianchi f grattarola livi l alippi c 2019 hierarchical representation learning graph neural network node decimation pooling 4 ying z j morris c ren x hamilton w leskovec j 2018 hierarchical graph representation learning differentiable pooling advance neural information processing system 5 gao h ji graph unets icml 2019 6 lee j lee kang j selfattention graph pooling icml 2019 7 f bianchi grattarola c alippi spectral clustering graph neural network graph pooling icml 2020 8 martino giuliani rizzi hyper graph embedding classification via simplicial complex algorithm 2019 nov 1211223 9 martino rizzi hypergraph kernel simplicial complex 2020 pattern recognition license dataset code released mit license see attached license file
Graphs;gated graph sequence neural network code iclr16 paper yujia li daniel tarlow marc brockschmidt richard zemel gated graph sequence neural international conference learning representation 2016 please cite paper use code code released mit licenselicense testing run th testlua test module ggnn rnn library reproducing babi task graph algorithm experiment result run babi experiment experiment two extra sequence task 1 go babidata run bash get10folddatash get 10 fold babi data 5 task 4 15 16 18 19 preprocessing 2 go babidataextraseqtasks run bash generate10folddatash get 10 fold data two extra sequence task 3 go back babi use runexperimentspy run ggnnggsnn experiment eg python runexperimentspy babi18 run ggnn babi task 18 10 fold data 4 use runrnnbaselinespy run rnnlstm baseline experiment eg python runrnnbaselinespy babi18 lstm run lstm babi task 18 10 fold data note make sure lua initlua lua path example export luapathluainitlualuapath experiment result may differ slightly reported paper datasets randomly generated different run run
Graphs;prediction treewidth using graph neural network code predicts treewidth using graph neural network want information code please read article graph neural network sorry written japanese preferred network research code also implement general framework gnn described 1 detail code implement max average sum aggregation readout described paper compatible gpu also provide trained model random graph datasets software developed part pfn summer internship main developer yuta outline prediction flow code follows prediction flow described 1 training gnn classification task regression task 2 predicting treewidth given graph using trained gnn model detail written abovementioned article dependency code tested chainer 630 python 374 havent installed chainer please install chainer following instruction official want run gpu please install cupy following instruction official dependency matplotlib networkx numpy scikitlearn scipy seaborn timeoutdecorator install dependency requirementstxt please run script pip install r requirementstxt example training gnn skip note provide pretrained model use model skip step first unzip dataset file running script unpacksh approach 1 run script python gnnpy tasktype task1 default parameter best performing hyperparameters may want tune hyperparameters check hyperparameters specified please type python gnnpy help approach 2 run script python gnnpy tasktype regression predicting treewidth approach 1 run script python mainpy tasktype approach1 approach 2 run script python mainpy tasktype approach2 license mit license provide warranty support implementation use risk please see licenselicense file detail reference 1 keyulu xu weihua hu jure leskovec stefanie jegelka powerful graph neural network iclr 2019 2 pacechallengetreewidth list treewidth solver instance tool
Graphs;gat graph attention network pytorch computer graph mega heart repo contains pytorch implementation original gat paper link veličković et br aimed making easy start playing learning gat gnns general br table content graph neural network gatwhataregnns visualization cora ppi attention tsne embeddings entropy histogramscoravisualized setupsetup usageusage training gattraininggat tip understanding codetipforunderstandingthecode profiling gatprofilinggat visualization toolsvisualizationtools hardware requirementshardwarerequirements learning materiallearningmaterial gnns graph neural network family neural network dealing signal defined graph graph model many interesting natural phenomenon youll see used everywhere computational biology predicting potent antibiotic like computational pharmacology predicting drug side traffic forecasting eg used google recommendation system used etc way particle large hedron collider fake news list go gat representative spatial convolutional gnns since cnns tremendous success field computer vision researcher decided generalize graph nerdface schematic gat structure p aligncenter img srcdatareadmepicsgatschematicpng width600 p cora visualized cant start talking gnns without mentioning single famous graph dataset cora node cora represent research paper link guessed citation paper ive added utility visualizing cora basic network analysis cora look like p aligncenter img srcdatareadmepicscoragraphjupyterpng p node size corresponds degree ie number inoutgoing edge edge thickness roughly corresponds popular connected edge edge betweennesses nerdy term check plot showing degree distribution cora p aligncenter img srcdatareadmepicscoradegreestatisticspng width850 p degree plot since dealing undirected graph bottom plot degree distribution see interesting peak happening 2 4 range mean majority node small number edge 1 node 169 edge big green node attention visualized fullytrained gat model visualize attention certain node learned br node use attention decide aggregate neighborhood enough talk let see p aligncenter img srcdatareadmepicsattention1jpg width600 p one cora node edge citation color represent node class clearly see 2 thing plot graph meaning similar node node class tend cluster together edge thickness chart function attention since thickness gat basically learned something similar similar rule hold smaller neighborhood also notice self edge p alignleft img srcdatareadmepicsattention2jpg width400 img srcdatareadmepicsattention4jpg width400 p hand ppi learning much interesting attention pattern p alignleft img srcdatareadmepicsneighborhoodattentionppi3jpg width400 img srcdatareadmepicsneighborhoodattentionppi2jpg width400 p left see 6 neighbor receiving nonnegligible amount attention right see attention focused onto single neighbor finally 2 interesting pattern strong self edge left right see single neighbor receiving bulk attention whereas rest equally distributed across rest neighborhood p alignleft img srcdatareadmepicsneighborhoodattentionppi4jpg width400 img srcdatareadmepicsneighborhoodattentionppi1jpg width400 p important note ppi visualization possible first gat layer reason attention coefficient second third layer almost 0 even though achievedtraininggat published result entropy histogram another way understand gat isnt learning interesting attention pattern cora ie learning const attention treating node neighborhood attention weight probability distribution calculating entropy accumulating info across every node neighborhood wed love gat attention distribution skewed see orange histogram look like ideal uniform distribution see light blue learned distribution exactly p aligncenter img srcdatareadmepicsentropyhistogramslayer0head0jpg width400 img srcdatareadmepicsentropyhistogramslayer1head0jpg width400 p ive plotted single attention head first layer 8 theyre hand ppi learning much interesting attention pattern p alignleft img srcdatareadmepicsentropyhistogramsppilayer0head0jpg width400 expected uniform distribution entropy histogram lie right orange since uniform distribution highest entropy analyzing cora embedding space tsne ok weve seen attention else visualize well let visualize learned embeddings gat last layer output gat tensor shape 2708 7 2708 number node cora 7 number class project 7dim vector 2d using get p aligncenter img srcdatareadmepicstsnepng width600 p see node labelclass roughly clustered together representation easy train simple classifier top tell u class node belongs note ive tried umap well didnt get nicer result lot dependency want use plot util setup talked gnns among thing br let get thing running follow next step 1 git clone 2 open anaconda console navigate project directory cd pathtorepo 3 run conda env create project directory create brand new conda environment 4 run activate pytorchgat running script console setup interpreter ide thats work outofthebox executing environmentyml file deal dependency br pytorch pip package come bundled version cudacudnn highly recommended install systemwide cuda beforehand mostly gpu driver also recommend using miniconda installer way get conda system follow point 1 2 use uptodate version miniconda cudacudnn system usage option 1 jupyter notebook run jupyter notebook anaconda console open session default browser br open annotated gatipynb youre ready play note get dll load failed importing win32api specified module could found br pip uninstall pywin32 either pip install pywin32 conda install pywin32 fix option 2 use ide choice need link python environment created setupsetup section training gat fyi gat implementation achieves published result cora get 8283 accuracy test node ppi achieved 0973 microf1 score actually even higher everything needed train gat cora already setup run console call br python trainingscriptcorapy could also potentially add shouldvisualize visualize graph data add shouldtest evaluate gat test portion data add enabletensorboard start saving metric accuracy loss code well commented hopefully understand training work br script dump checkpoint pth model modelscheckpoints dump final pth model modelsbinaries save metric run run tensorboard logdirruns anaconda visualize periodically write training metadata console go training ppi run python trainingscriptppipy ppi much gpuhungry dont strong gpu least 8 gb youll need add forcecpu flag train gat cpu alternatively try reducing batch size 1 making model slimmer visualize metric training calling tensorboard logdirruns console pasting url browser p aligncenter img srcdatareadmepicsvallosspng height290 img srcdatareadmepicsvalaccpng height290 p note cora train split seems much harder validation test split looking loss accuracy metric said fun actually lie playgroundpy script tip understanding code ive added 3 gat implementation conceptually easier understand efficient interesting hardest one understand implementation 3 implementation 1 implementation 2 differ subtle detail basically thing advice approach code understand implementation 2 first check difference compared implementation 1 finally tackle implementation 3 profiling gat want profile 3 implementation set playgroundfn variable playgroundprofilegat playgroundpy 2 params may care storecache set true wish save memorytime profiling result youve run skipifprofilinginfocached set true want pull profiling info cache result get stored data memorydict timingdict dictionary pickle note implementation 3 far optimized one see detail code ive also added profilesparsematrixformats want get familiarity different matrix sparse format like coo csr csc lil etc visualization tool want visualize tsne embeddings attention embeddings set playgroundfn variable playgroundvisualizegat set visualizationtype visualizationtypeattention wish visualize attention across node neighborhood visualizationtypeembedding wish visualize embeddings via tsne visualizationtypeentropy wish visualize entropy histogram youll get crazy visualization like one visualizationtypeattention option p aligncenter img srcdatareadmepicsattention3jpg width410 img srcdatareadmepicskklayoutjpg width410 p left see node highest degree whole cora dataset youre wondering look like circle ive used layoutreingoldtilfordcircular layout particularly well suited tree like graph since visualizing node neighbor subgraph effectively mary tree also use different drawing algorithm like kamada kawai right etc feel free go code play plotting attention different gat layer plotting different node neighborhood attention head also easily change number layer gat although shallow tend perform best homophilic graph datasets want visualize corappi set playgroundfn playgroundvisualizedataset youll get result readmecoravisualized hardware requirement hw requirement highly dependent graph data youll use want play cora youre good go 2 gb gpu take cora citation network 10 second train gat rtx 2080 gpu 15 gb vram memory reserved pytorchs caching overhead far le allocated actual tensor model 365 kb compare hardware needed even smallest hand ppi dataset much gpuhungry youll need gpu 8 gb vram reduce batch size 1 make model slimmer thus try reduce vram consumption future todos figure attention coefficient equal 0 ppi dataset second third layer potentially add implementation leveraging pytorchs sparse api idea implement gat using pytorchs sparse api please feel free submit pr personally difficulty api beta questionable whether possible make implementation efficient implementation 3 using secondly im still sure gat achieving reported result ppi obvious numeric problem deeper layer manifested attention coefficient equal 0 learning material youre difficulty understanding gat indepth overview paper p alignleft targetblankimg altthe gat paper explained width480 height360 border10 p also made walkthrough repo focusing potential pain point blog getting started graph general heart video could help understand gnns overview gcn overview graphsage overview pinsage overview temporal graph network acknowledgement found repos useful developing one official pytorch citation find code useful please cite following miscgordić2020pytorchgat author gordić aleksa title pytorchgat year 2020 publisher github journal github repository howpublished licence license
Graphs;link prediction task repository provides experiment link prediction task using ogblddi dataset homogeneous unweighted undirected graph representing drugdrug interaction network using featureless approach onehot encoding representation node try test capability graph neural network gnns model reconstruct edge original graph dataset ogblddi graph includes 4267 node 2135822 edge edge splitted follows training edge 1067911 valid edge 133489 test edge 133489 good practice select model achieves best performance validation dataset training process however considering amount available time experiment skip step test link prediction mechanism directly test edge using model resulting last epoch training process evaluation metric evaluation metric experiment mean reciprocal rank mrr nutshell count many correct link positive example ranked topn position bunch synthetic negative example specific experiment drug interaction set approximately 100000 randomly sampled negative example created compute count ratio positive edge ranked kplace hitsk clarify aspect imagine test set includes two ground truth positive example source node1 target node2 source node4 target node5 create five negative example right edge compute trained model link prediction score first edge result following score rank node1 node3 0790 1 node1 node2 0789 2 node4 node1 0695 3 node2 node5 0456 4 node4 node6 0234 5 first edge result following score rank node4 node5 0779 1 node1 node3 0743 2 node4 node1 0695 3 node2 node5 0456 4 node4 node6 0234 5 want compute hits1 hits3 count many positive occur top1 top3 position divide number edge test set example includes two edge hits3 22 10 hits1 12 05 installation python3 venv venv source venvbinactivate pip3 install r requirementstxt tested python 385 approach experiment detail experiment performed using hardware following feature ram size 251 gib gpu nvidia geforce rtx 3090 shared process go test two different model gcn first try make preliminary assumption avoid performing entire training process model save time preliminary test running real experiment need make test training process ensure process concludes unexpected interruption helpful avoid discovering bug end training step start speedy training process run following command python embeddingspy nepochs 1 nsamples 10 hiddendim 8 hypothesis training time step estimate total training time reason run 5 epoch using original configuration test 5 epoch considering dimension training graph directly used training process reason apply sampling process selects fixed number neighbor node want test epoch duration considering nsample 500 hyperparameter number sampled edge epoch around 30000 1067911 python embeddingspy nepochs 5 training 5 epoch took around 6 minute gcn gat time predictable two model close number parameter gcn 68609 gat 68849 additional parameter gat related attention coefficient observing result training 5 epoch took around 6 minute consequence one hour train around 50 epoch testing model low number parameter sample decided perform 1hour training model using low number parameter edge sample result achieved model term mrr gcn test hits10 00007 hits50 00060 hits100 00217 train hits10 00000 hits50 00062 hits100 00179 gat test hits10 00000 hits50 00000 hits100 00000 train hits10 00002 hits50 00004 hits100 00004 gcn model seems promising one likely reason edge sampling process allow attention coefficient learned correctly indeed attention coefficient try grasp relevance neighbor central node unfortunately due sampling process local graph structure around node change training step first experiment suggestion improve performance experiment decided increase value hyperparameters check impact term mrr value applied following change 2 gcn layer 1000 sample adding another gcn layer allows aggregating feature 2hop neighbor consider new sampling value number sampled edge epoch around 70000 1067911 training duration 37 hour resulting value term mrrs test hits10 00210 hits50 00479 hits100 00674 train hits10 00254 hits50 00392 hits100 00441 performance increased significantly still considerable room improvement case goal develop approach hour detect right direction selecting correct model set hyperparameters half day moreover wanted use almost real data ogbddi includes 2 million edge avoid toy data zacharys karate club 78 edge base version improve performance proceed following direction extend number sample best situation would using sample order process entire graph dataset epoch alternatively increasing number epoch good trade increase th number parameter compared inproduction model include million parameter current model overcome number 70000
Graphs;principal neighbourhood aggregation implementation principal neighbourhood aggregation graph net pytorch dgl pytorch geometric symbolmultitaskbenchmarkimagessymbolpng overview provide implementation principal neighbourhood aggregation pna pytorch dgl pytorch geometric framework along script generate run multitask benchmark script running realworld benchmark flexible pytorch gnn framework implementation model used comparison repository organised follows model contains pytorch contains various gnn model implemented pytorch implementation aggregator scaler pna layer pna flexible gnn framework used type graph convolution gnnframeworkpy implementation gnn model used comparison paper namely gcn gat gin mpnn dgl contains pna model implemented via dgl aggregator scaler layer pytorchgeometric contains pna model implemented via pytorch geometric aggregator scaler layer layerspy contains general nn layer used various model multitask contains various script recreate multitask benchmark along file used train various model multitaskreadmemd detail instruction generation training hyperparameters tuned realworld contains various script benchmarking download realworld benchmark train pna realworldreadmemd provide instruction generation training hyperparameters tuned resultsmultitaskbenchmarkimagesresultspng reference inproceedingscorso2020pna title principal neighbourhood aggregation graph net author corso gabriele cavalleri luca beaini dominique lio pietro velivckovic petar booktitle advance neural information processing system year 2020 license mit acknowledgement author would like thank saro passaro running test presented repository giorgos bouritsas fabrizio frasca leonardo cotta zhanghao wu zhanqiu zhang pointing issue code
Graphs;deep graph library dgl pypi latest conda latest build benchmark documentation dgl model discussion dgl easytouse high performance scalable python package deep learning graph dgl framework agnostic meaning deep graph model component endtoend application rest logic implemented major framework pytorch apache mxnet tensorflow p aligncenter img altdgl v04 architecture width600 br bfigureb dgl overall architecture p img width30dgl news 03312020 new v043 release includes official tensorflow support 15 popular gnn module dglke dgllifesci two package knowledge graph embedding chemi bioinformatics respectively graduated standalone package installed pip conda new release provides full support graph sampling heterogeneous graph multigpu acceleration see new feature release 03022020 check cool paper benchmarking graph neural includes dglbased benchmark framework novel mediumscale graph datasets covering mathematical modeling computer vision chemistry combinatorial problem see repo using dgl data scientist may want apply pretrained model data right away use dgls application package formally model application package developed domain application case soon add model zoo knowledge graph embedding learning recommender system here use pretrained model python dgllifedata import tox21 dgllifemodel import loadpretrained dgllifeutils import smilestobigraph canonicalatomfeaturizer dataset tox21smilestobigraph canonicalatomfeaturizer model loadpretrainedgcntox21 pretrained model loaded modeleval smile g label mask dataset0 feat gndatapoph labelpred modelg feat printsmiles ccoc1ccc2ncsnoosc2c1 printlabelpred mask 0 mask nonexisting label tensor 14190 01820 12974 14416 06914 20957 05919 07715 17273 02070 reading dgl released managed service aws sagemaker see medium post easy trip dgl researcher start growing list model implemented developing new model mean start scratch instead reuse many prebuilt get standard twolayer graph convolutional model prebuilt graphconv module python dglnnpytorch import graphconv import torchnnfunctional f build twolayer gcn relu activation class gcnnnmodule def initself infeats hfeats numclasses supergcn selfinit selfgcnlayer1 graphconvinfeats hfeats selfgcnlayer2 graphconvhfeats numclasses def forwardself graph input h selfgcnlayer1graph input h freluh h selfgcnlayer2graph h return h next level may want innovate module dgl offer succinct messagepassing interface see tutorial graph attention network gat implemented complete course also find gat module python import torchnn nn import torchnnfunctional f define gat layer class gatlayernnmodule def initself infeats outfeats supergatlayer selfinit selflinearfunc nnlinearinfeats outfeats biasfalse selfattentionfunc nnlinear2 outfeats 1 biasfalse def edgeattentionself edge concatz torchcatedgessrcz edgesdstz dim1 srce selfattentionfuncconcatz srce fleakyrelusrce return e srce def messagefuncself edge return z edgessrcz eedgesdatae def reducefuncself node fsoftmaxnodesmailboxe dim1 h torchsuma nodesmailboxz dim1 return h h def forwardself graph h z selflinearfunch graphndataz z graphapplyedgesselfedgeattention graphupdateallselfmessagefunc selfreducefunc return graphndatapoph performance scalability microbenchmark speed memory usage leaving tensor autograd function backend framework eg pytorch mxnet tensorflow dgl aggressively optimizes storage computation kernel here comparison another popular package pytorch geometric pyg short story raw speed similar dgl much better memory management dataset model accuracy time br pyg emspemsp dgl memory br pyg emspemsp dgl cora gcn br gat 8131 plusmn 088 br 8398 plusmn 052 b0478b emspemsp 0666 br 1608 emspemsp b1399b 11 emspemsp 11 br 12 emspemsp b11b citeseer gcn br gat 7098 plusmn 068 br 6996 plusmn 053 b0490b emspemsp 0674 br 1606 emspemsp b1399b 11 emspemsp 11 br 13 emspemsp b11b pubmed gcn br gat 7900 plusmn 041 br 7765 plusmn 032 b0491b emspemsp 0690 br 1946 emspemsp b1393b 11 emspemsp 11 br 16 emspemsp b11b reddit gcn 9346 plusmn 006 oomemspemsp b286b oom emspemsp b117b reddits gcn na 2912 emspemsp b944b 157 emspemsp b36b table training timein second 200 epoch memory consumptiongb another comparison dgl tensorflow backend tfbased gnn tool training time second one epoch dateset model dgl graphnet tfgeometric core gcn 00148 00152 00192 reddit gcn 01095 oom oom pubmed gcn 00156 00553 00185 ppi gcn 009 016 021 cora gat 00442 na 0058 ppi gat 0398 na 0752 high memory utilization allows dgl push limit singlegpu performance seen image img width400 img width400 scalability dgl fully leveraged multiple gpus one machine cluster increasing training speed better performance alternative seen image p aligncenter img width600 p img img reading detailed comparison dgl graph alternative found dgl model application dgl research overall 30 model implemented using dgl dgl domain application previously dglchem dglrecsyscoming soon dgl nlpcv problem capsule currently beta stage feature improvement coming installation dgl work linux distribution earlier ubuntu 1604 macos x window 10 dgl requires python 35 later right dgl work 120 151 210 using anaconda conda install c dglteam dgl cpu version conda install c dglteam dglcuda90 cuda 90 conda install c dglteam dglcuda92 cuda 92 conda install c dglteam dglcuda100 cuda 100 conda install c dglteam dglcuda101 cuda 101 using pip latest nightly build version stable version cpu pip install pre dgl pip install dgl cuda 90 pip install pre dglcu90 pip install dglcu90 cuda 92 pip install pre dglcu92 pip install dglcu92 cuda 100 pip install pre dglcu100 pip install dglcu100 cuda 101 pip install pre dglcu101 pip install dglcu101 built source code refer guide dgl major release release date feature v043 03312020 tensorflow support br dglke br dgllifesci br heterograph sampling apis experimental v042 01242020 heterograph support br tensorflow support experimental br mxnet gnn module br v031 08232019 apis gnn module br model zoo dglchem br new installation v02 03092019 graph sampling apis br speed improvement v01 12072018 basic dgl apis br pytorch mxnet support br gnn model example tutorial new deep learning graph deep learning check open source book dive deep new graph neural network please see basic audience looking advanced realistic endtoend example please see model contributing please let u know encounter bug suggestion filing welcome contribution bug fix new feature extension expect contribution discussed issue tracker going pr please refer contribution cite use dgl scientific publication would appreciate citation following paper articlewang2019dgl titledeep graph library towards efficient scalable deep learning graph authorwang minjie yu lingfan zheng da gan quan gai yu ye zihao li mufei zhou jinjing huang qi chao huang ziyue guo qipeng zhang hao lin haibin zhao junbo li jinyang smola alexander j zhang zheng journaliclr workshop representation learning graph manifold year2019 team dgl developed maintained nyu nyu shanghai aws shanghai ai lab aws mxnet science license dgl us apache license 20
Graphs;spectral clustering graph neural network graph pooling img srcfigsmincutpoolpng width400 height200 code reproduces experimental result obtained mincutpool layer presented icml 2020 paper spectral clustering graph neural network graph f bianchi grattarola c alippi official implementation mincutpool layer found implementation mincutpool pytorch also available pytorch setup code based python 35 tensorflow 115 spektral 012 required library listed requirementstxt installed bash pip install r requirementstxt image segmentation img srcfigsoversegandragpng width700 height150 run perform hypersegmentation generate region adjacency graph resulting segment cluster node rag graph mincutpool layer clustering img srcfigsclusteringstatspng width600 height250 run cluster node citation network datasets cora citeseer pubmed selected result provided term homogeneity score completeness score normalized mutual information vscore autoencoder img srcfigsaeringpng width400 height200 img srcfigsaegridpng width400 height200 run train autoencoder bottleneck compute reconstructed graph possible switch ring grid graph also point library supported result provided term mean squared error graph classification run train graph classifier additional classification datasets available drop dataclassification drop data result provided term classification accuracy averaged 10 run citation please cite original paper using mincutpool research inproceedingsbianchi2020mincutpool titlespectral clustering graph neural network graph pooling authorbianchi filippo maria grattarola daniele alippi cesare booktitleproceedings 37th international conference machine learning pages27292738 year2020 organizationacm license code released mit license see attached license file
Graphs;fastgcn method theory repository contains matlab implementation fastgcn chen et al 2018chen2018a well companion code optimization theory paper chen lu 2018chen2018b matlab code fastgcn observed substantially faster implementation tensorflow pytorch theory paper explains stochastic gradient descent biased consistent gradient estimator driver behind fastgcn original fastgcn code published paper implemented tensorflow see fastgcn see directory fastgcn start testfastgcnm namechen2018aareference jie chen tengfei cao xiao fastgcn fast learning graph convolutional network via importance iclr 2018 sgd biased consistent gradient estimator see directory sgdpaper start test1layerm test2layerm namechen2018bareference jie chen ronny lu stochastic gradient descent biased consistent gradient preprint arxiv180711880 2018
Graphs;ce7490ass1gnn team lizhiming wangjian xuxiufeng graphneuralnetwork based project predict demanding vehicle amount inputing timestamp repository focus preprocssing traning evaluating baseline however another frontend nodejavascript based project show visualization demo interest visualization project object large amount yellow cab data new york city available public trip record contains latitude longitude pickup dropoff location date time pickup dropoff additional information construct grid nyc create undirected graph edge represents trip two point grid goal feed graph graph constituting one trip starting end gnn network order predict next demand hotpot preprocess train step1 cluster gridbased region see deepminds br center deepmindsmallgif center step2 build graph dataset train evaluate step3 prediction hotpot demand br centerimg width500 srcdemandnycpngimgcenter additional also use animation show individual route lane demand area predicting br centerezgifcomgifmakergif center network archtecture graphnode embedding paper deep walk diffusion convolutional recurrent neural network dcrnn ccrnn coupling layerwise convolutional network ccrnn benchmark according make fair comparison different proposed build benchmark platform evaluate score respectively put practise test model paper benchmarking graph neural dataset nyc taxi limousine commission make record yellow cab taxi trip across city publicly available year specific attribute recorded trip vary individual year trip arranged csv file month ogb contains graph datasets managed data loader loader handle downloading preprocessing datasets additionally ogb standardized evaluator leaderboards keep track stateoftheart result didi chengdu city competition built didi chuxing gaia open data initiative available data set derived trajectory data didi express didi premier driver chengdu city october – november 2016 driver trip order information encrypted anonymized registering get access data online prepare dataset see preprocessreadmemdpreprocessreadmemd install see installmdinstallmd train cat runsh evaluate cat onetimeinferencesh reference thirdparty library pytorch geometric documentation pytorch data loading pytorch lstm keplergl powerful open source geospatial analysis tool largescale data set keplerglkeplergl ntu graph lab gnn acknowledge project training code model archtecture extended whats contribute cannot reproduce arxivorgabs201208080 checkout commit 467aba7704570fa62b944f96c2f3e99d4f9637e6 modify project required import new insolated dataset didi gaia chengdu compare nyc combined dataset worry whatif bike training mislead taxi inference whatif correlated import baseline project import comprehensive ablation study address suspect influenced different layer gate number dose datasets size crucial point performance detail please check reportpdf
Graphs;modeling heterogeneous hierarchy relationspecific hyperbolic cone official codebase paper modeling heterogeneous hierarchy relationspecific hyperbolic overview present cone cone embedding knowledge graph first knowledge graph kg embedding method capture transitive closure property heterogeneous hierarchical relation well nonhierarchical property figure show illustration model figuremodelpng pytorch implementation proposed cone model based code framework provided knowledge graph embedding implemented feature datasets x wn18rr x ddb14 x go21 x fb15k237 notice propose go21 hierarchical biological knowledge graph containing gene protein drug disease entity please cite paper use dataset model x rotate x protate x transe x complex x distmult x rotc x cone notice rotc degenerate version cone us empty relationspecific subspace rotc utilized initialize embedding cone stabilize training procedure task evaluation metric knowledge graph completion x mrr mr hits1 hits3 hits10 filtered ancestordescendant prediction x map auroc lca prediction x hits1 hits3 hits10 loss function x uniform negative sampling x selfadversarial negative sampling x cone angle loss usage knowledge graph data entitiesdict dictionary mapping entity unique id relationsdict dictionary mapping relation unique id traintxt kge model trained fit data set validtxt create blank file validation data available testtxt kge model evaluated data set relationcategorytxt dictionary mapping relation type 11 indicates nonhierarchical 1m indicates hyponym m1 indicates hypernym required cone model classtestxtxt test data ancestordescendant prediction task xeasy 0 inferred descendant pair xmedium 50 inferred descendant pair xhard 100 inferred descendant pair lcatestxtxt lca prediction xhop evaluated data set training evaluation cone train runsh script provides easy way search hyperparameters batch size learning rate etc training example command train cone model wn18rr dataset gpu 0 bash runsh train cone wn18rr 0 1 1024 50 500 10 05 0001 40000 4 de tailbatchonly dovalid validsteps 20000 savecheckpoint 40000 trainwithrelationcategory uniweight lrdecayepoch 30000 dotestrelationcategory conepenalty fixatt 100 w 05 pretrained modelsrotcwn18rr1checkpointckpt39999 check argparse configuration codesrunpy argument detail moreover provide example training script cone folder example reproduce result test trained model automatically saved folder model evaluation conducted using following command bash runsh category cone wn18rr 0 1 1024 50 500 01 05 0001 20000 4 de init modelsconewn18rr1 ckpt ckpt39999 fixatt 100 dotestrelationcategory doclassification dolca 1 pretrained modelsrotcwn18rr1checkpointckpt39999 model evaluated three task kg completion task ancestordescendant prediction task lca prediction task extend datasets hierarchical kg datasets training cone requires pretrained rotc model initialize embedding refer exampleswn18rrsh example nonhierarchical kg datasets rotc model comparable many strong baseline model refer examplesfb15k237sh example citation please cite paper use method dataset work bibtex bibtex inproceedingsbai2021cone titlemodeling heterogeneous hierarchy relationspecific hyperbolic cone authorbai yushi ying rex ren hongyu leskovec jure booktitleadvances neural information processing system neurips year2021
Graphs;bilateral neural network crosslanguage algorithm classification project implementation work bilateral neural network introduced 2 paper saner19 bilateral dependency neural network crosslanguage algorithm classification nghi q bui yijun yu lingxiao jiang 26th edition ieee international conference software analysis evolution reengineering research track zhejiang university hangzhou february 2427 2019 nl4seaaai18 crosslanguage learning program classification using bilateral treebased convolutional neural network nghi q bui lingxiao jiang yijun yu proceeding 32nd aaai conference artificial intelligence aaai workshop nlp software engineering new orleans lousiana usa 2018 find paper here proposed neural network structure short structure variance bilateral neural network snn class neural network used finding similarity relationship two comparable thing also known name siamese neural network sub network case treebased convolutional neural network tbcnn gated graph neural network implementation ggnn task found dependency tree convolutional neural network dtbcnn installation workflowdocmodelpng prepared fully automated workflow figure 1 classify program known algorithm name workflowdocworkflowpng figure 1 workflow use tool need first install docker command sudo aptget install dockerce enter following script command line build run system respectively r clean modify input insider input subfolder find following file input ├── algorithmname ├── allalgo ├── alllang ├── configjson ├── languagename └── srcmlnodemaptsv want see work prepared two algorithm follows algorithmname name algorithm bubblesort mergesort note add algorithm name nl4se paper used 6 algorithm included allalgo file replace algorithmname allalgo languagename name programming language java java cpp cpp note add programming language nl4se paper used c java programming language included alllang file replace languagename alllang configjson configuration github api please subsitute username access token githubusername githubaccesstoken srcmlnodemaptsv syntax node type selected programming language unitkind 0 decl 1 declstmt 2 init 3 expr 4 exprstmt 5 comment 6 call 7 control 8 incr 9 strong 383 ompomp 384 specialchars 385 note need make sure file consistent underlying parser docker run fasttoolfast fast v fast v007 commit id 3e368dd1e56f5bb8f02673b1c7441f567eab67ee local change id e1b7ca5bf36050ce774cb2650446115bb49bf91ac5d889a9dbb4911ed8130225 built 640 nov 14 2017 200004 different version fast prepared might require regenerated input file ast2vecast2vecfastpb2py change language grammar parameter tensorflow framework stored following two file corresponds tensorflow run ast2vecast2vecparameterspy bitbcnnbitbcnnparameterspy reference inproceedingsdblpconfaaaibuijy18 author nghi q bui lingxiao jiang yijun yu title crosslanguage learning program classification using bilateral treebased convolutional neural network booktitle workshop thirtysecond aaai conference artificial intelligence new orleans louisiana usa february 27 2018 page 758761 year 2018 crossref dblpconfaaai2018w url timestamp thu 19 jul 2018 133855 0200 biburl bibsource dblp computer science bibliography inproceedings8667995 authorb nghi q yu l jiang booktitle2019 ieee 26th international conference software analysis evolution reengineering saner titlebilateral dependency neural network crosslanguage algorithm classification year2019 volume number pages422433 keywordsneural networksprediction algorithmsclassification algorithmssyntacticssemanticsmachine learning algorithmstask analysiscrosslanguage mappingprogram classificationalgorithm classificationcode embeddingcode dependencyneural networkbilateral neural network doi101109saner20198667995 issn15345351 monthfeb
Graphs;recommended learning ressources provide first nonexhaustive list frequently recommended learning ressources reading data science machinelearning rausell lab newcomer populating next month regular update may follow u twitter news ml programming python think like computerscientist learning python 3 scikitlearn machinelearning python pytorch pythonbased scientific computing package tensorflow ggplot grammar graphic python plotnine tensorsensor library visualize python code indicating shape tensor variable work tensorflow pytorch numpy kera fastai spark o’reilly’s new learning spark 2nd edition mllib apache spark scalable machine learning library databricks hail pythonbased data analysis tool additional data type method working genomic data variantspark glow opensource toolkit natively built apache spark working genomic data biobankscale hadoop hadoop definitive guide 4th edition jupyter notebook r r statistical learning david lapiaz data analysis prediction algorithm r rafael irizarry r interface tensorflow managing conda environment installing anaconda managing environment server multiple user database management system big data mongodb mongodb 44 manual graphing data mongodb connect mongodb instance jupyter notebook make plot python mongodb spark connector hbase apache hbase hadoop hdfs spark sql postgresql prototyping sql query dbfiddle pgadmin developper tool opensource library graph neural network graph net deepminds library building graph network tensorflow sonnet pytorch geometric geometric deep learning extension library pytorch deep graph library dgl python package deep learning graph machine learning graph representation learning graph method application graph representation learning william l hamilton thomas kipfs phd thesis deep learning graphstructured representation machine learning graph sequential data prof günnemann tum graph neural network model modeling relational data graph convolutional network semisupervised classification graph convolutional network aaai tutorial forum 2019 tutorial graph representation learning william l hamilton jian tang aaai 2020 tutorial graph neural network model application tutorial graph representation directional graph network basic ml deep learning concept basic deep learning course marc lelarge lecture uc berkeley c 182 deep learning calculating gradient descent manually moocs basic bioinformatics concept mooc bioinformatique pour la génétique médicale en français bioinformatics genome algorithm singlecell data analysis tutorial online course current best practice singlecell rnaseq analysis tutorial orchestrating singlecell analysis bioconductor complete course singlecell rnaseq data analysis univ cambridge 2018 bioinformatics training channel youtube roscoff singlecell transcriptomics epigenomics workshop 2019 slide script french working group singlecell data analysis stepbystep workflow lowlevel analysis singlecell rnaseq data “allinone” environment seurat r toolkit singlecell genomics “allinone” environment ii scanpy scanpy – singlecell analysis python singlecell workshop 2014 rnaseq harvard
Graphs;gatpytorch pytorch inplementation graph attention network petar veličković et algraph attention usage python mainpy explanation mainpy python gat import gat spgat import spgat train import run data import loaddata namemain load data according input data loaddatacora create gat model use sparse version gat reduces computational time memory consumption model spgatdata also use dense version gat model gatdata run model niter time rundata model lr0005 weightdecay5e4 niter10
Graphs;semisupervised classification graph convolutional network installation pip install numpy scipy torch networkx matplotlib searborn train python trainpy result resultpngresultpng
Graphs;ecodqn generic mit implementation ecodqn reported exploratory combinatorial optimization reinforcement requirement beyond standard package found python distribution eg numpy matplotlib etc project requires pytorch networkx numba panda alternatively included environmentymlenvironmentyml file produce working environment called spinsolver git clone recursive cd ecodqn conda env create f environmentyml optional source active spinsolver optional running experiment script reproduce agent trained paper found experimentsexperiments folder straightforward modify use different trainingtesting data parameter reproducing ecodqn agent test train agent two different type graph called erdosrenyi er barabasialbert ba graph type train agent 20 40 60 100 200 500 vertex graph note code typically refers vertex spin training agent reproduced running corresponding script replace er20spin appropriate folder different agent cd ecodqn python experimentser20spintraintraineco creates directory er20spineconetwork store following network parameter various point training fully trained agent test score training pkl file plotted loss training pkl file plotted agent typically tested graph type greater size test agent presuming training script already run simply use cd ecodqn python experimentser20spintesttesteco edit script test subset possible graph size desired creates directory er20spinecodata store following every graph size tested averaged result summarized panda dataframe saved resultsxxxpkl raw result graph summarized panda dataframe saved resultsxxxrawpkl full history every test episode saved resultsxxxhistoriespkl alternativley agent trained tested single script cd ecodqn python experimentser20spintrainandtesteco testing pretrained agent script test pretrained agent different graph set provided experimentspretrainedagenttestecopyexperimentspretrainedagenttestecopy simply edit networksaveloc graphsaveloc parameter point desired agent testgraphs moreover pretrained agent produced paper found experimentspretrainedagentnetworksecoexperimentspretrainedagentnetworkseco reproducing s2vdqn agent test s2vdqn framework origionally proposed khalil et reimplemented use network architecure ecodqn every training test script equivalent s2vdqn script ran replacing eco s2v please note implementation s2vdqn however original repository provided hanjun dai repository content graphsgraphs contains graph instance used paper split three catagories benchmarksgraphsbenchmarks known benchmark tested paper specifically g1g10 800 vertex g22g32 2000 vertex gset along physic 125 vertex dataset testinggraphstesting set 50 graph graph type size graph agent tested training validationgraphsvalidation set 100 graph graph type size graph performance trained agent tested within subfolders opts folder contains optimum solutionsvalues best known benchmarking graph best found optimization method described supplemental material paper testing validation graph graph set pkl file unpickle list graph ultimately code want list numpy array however loadgraphset function experimentsutilspyexperimentsutilspy also convert list either networkx graph scipy sparse matrix correct form memory efficient way storing large graph wish point code custom set graph match one format appropriately stored pkl file experimentsexperiments described srcsrc contains source code ecodqn consists three directory high level agentssrcagents contains dqnsrcagentsdqn agent trained along class solve graph using either trained agent greedy heuristic envssrcenvs contains environment tacking combinatorial optimisation graph rlframework networkssrcnetworks contains qnetwork used ecodqn form message passing neural network mpnn first introduced gilmer et reference find work associated paper useful cited articlebarrett2019exploratory titleexploratory combinatorial optimization reinforcement learning authorbarrett thomas clements william r foerster jakob n lvovsky ai journalarxiv preprint arxiv190904063 year2019
Graphs;pathinfomax repository hold code used ijcai21 paper unsupervised path representation learning curriculum negative sampling overview give pytorch implementation pim repository organized follows data includes training data sample use data follow format model contains implementation pim pipeline pimpy layer contains implementation bilinear discriminator discriminatorpy utils contains necessary processing tool processpy better understand code recommend could read code dgipetar advance besides could optimize code based need dataset use two datasets paper aalborg harbin download dataset train pim preprocessing needed requirement ubuntu o 1804 pytorch 171 numpy 1162 pickle please refer source code install required package python usage python trainpy cite please cite paper make advantage pim research inproceedings ijcai21 titleunsupervised path representation learning curriculum negative sampling authorsean bin yang jilin hu chenjuan guo jian tang bin yang booktitleproceedings 30th international joint conference artificial intelligence year2021
Graphs;404 found
Graphs;powerful graph neural network repository official pytorch implementation experiment following paper keyulu xu weihua hu jure leskovec stefanie jegelka powerful graph neural network iclr 2019 make use codeexperiment gin algorithm work please cite paper bibtex inproceedings xu2018how titlehow powerful graph neural network authorkeyulu xu weihua hu jure leskovec stefanie jegelka booktitleinternational conference learning representation year2019 installation install pytorch following instuctions official website code tested pytorch 041 100 version install dependency pip install r requirementstxt test run unzip dataset file unzip datasetzip run python mainpy default parameter best performinghyperparameters used reproduce result paper hyperparameters need specified commandline argument please refer paper detail set hyperparameters instance collab imdb datasets need add degreeastag node degree used input node feature learn hyperparameters specified please type python mainpy help crossvalidation strategy paper crossvalidation paper us training validation set test set due small dataset size specifically obtaining 10 validation curve corresponding 10 fold first took average validation curve across 10 fold thus obtain averaged validation curve selected single epoch achieved maximum averaged validation accuracy finally standard devision 10 fold computed selected epoch
Graphs;relational pooling graph representation overview code associated paper relational pooling graph accepted icml 2019 first task evaluates rpgin powerful model propose make graph isomorphism network gin xu et al powerful corresponding wl1 test second set task us molecule datasets evaluate different instantiation rp model described plain english appendix paper feel free contact u question see requirement python 3 first set task need scipy scikitlearn docopt schema parsing argument command line molecular task need associated dependency example run example call synthetic task follows trained model cpu please see docstring detail python runginexperimentpy cvfold 0 modeltype rpgin outweightdir somepath outlogdir anotherpath onehotiddim 10 show example molecular task tox 21 dataset smaller demonstrate molecular kary task python duvenaudkarypy tox21 20 molecular rpduvenaud task python rpduvenaudpy tox21 uniquelocal 0 molecular rnn task python rnndfspy tox21 data datasets first set task available syntheticdata directory datasets molecular task available deepchem package question contact please feel free reach ryan murphy murph213purdueedu question citation use code please consider citing paper bibtex entry inproceedingsmurphy19a title relational pooling graph representation author murphy ryan srinivasan balasubramaniam rao vinayak ribeiro bruno booktitle proceeding 36th international conference machine learning page 46634673 year 2019 editor chaudhuri kamalika salakhutdinov ruslan volume 97 series proceeding machine learning research address long beach california usa month 0915 jun publisher pmlr pdf url
Graphs;graphembedding method model paper note deepwalk kdd 2014deepwalk online learning social 【graph line www 2015line largescale information network 【graph node2vec kdd 2016node2vec scalable feature learning 【graph sdne kdd 2016structural deep network 【graph struc2vec kdd 2017struc2vec learning node representation structural 【graph run example 1 clone repo make sure installed tensorflow tensorflowgpu local machine 2 run following command bash python setuppy install cd example python deepwalkwikipy disscussiongroup related project html table stylemarginleft 20px marginright auto tr td 公众号：b浅梦的学习笔记bbrbr img aligncenter srcpicscodepng td td 微信：bdeepctrbotbbrbr img aligncenter srcpicsdeepctrbotpng td td ul lia lia lia lia ul td tr table html usage design implementation follows simple principlesgraph inembedding much possible input format use networkxto create graphsthe input networkx graph follows node1 node2 edgeweight picsedgelistpng deepwalk python g nxreadedgelistdatawikiwikiedgelisttxtcreateusingnxdigraphnodetypenonedataweightint read graph model deepwalkgwalklength10numwalks80workers1init model modeltrainwindowsize5iter3 train model embeddings modelgetembeddings get embedding vector line python g nxreadedgelistdatawikiwikiedgelisttxtcreateusingnxdigraphnodetypenonedataweightintread graph model linegembeddingsize128ordersecond init modelorder firstsecondall modeltrainbatchsize1024epochs50verbose2 train model embeddings modelgetembeddings get embedding vector node2vec python gnxreadedgelistdatawikiwikiedgelisttxt createusing nxdigraph nodetype none data weight intread graph model node2vecg walklength 10 numwalks 80p 025 q 4 worker 1init model modeltrainwindowsize 5 iter 3 train model embeddings modelgetembeddings get embedding vector sdne python g nxreadedgelistdatawikiwikiedgelisttxtcreateusingnxdigraphnodetypenonedataweightintread graph model sdneghiddensize256128 init model modeltrainbatchsize3000epochs40verbose2 train model embeddings modelgetembeddings get embedding vector struc2vec python g nxreadedgelistdataflightbrazilairportsedgelistcreateusingnxdigraphnodetypenonedataweightintread graph model model struc2vecg 10 80 workers4 verbose40 init model modeltrainwindowsize 5 iter 3 train model embeddings modelgetembeddings get embedding vector
Graphs;sdne repository provides reference implementation sdne described paperbr structural deep network embeddingbr daixin wang peng cui wenwu zhubr knowledge discovery data mining 2016br insert paper link sdne algorithm learns representation node graph please check detail basic usage python mainpy c configxxini noted checkout modify config file mainpy get want input input graph data txt file mat file graphdata folder file format txt file edgelist first line n number vertex e number edge mat file adjacent matrix save adjacent matrix using code import scipyio sio siosavematxxxmat graphsparseyouradjacentmatrix recommended use mat file save adjacent matrix sparse form txt file sample 5242 14496 0 1 0 2 4 9 4525 4526 noted nodeid start 0br noted graph undirected graph j exist input file j citing find sdne useful research ask cite following paper inproceedingswang2016sdn29396722939753 author wang daixin cui peng zhu wenwu title structural deep network embedding booktitle proceeding 22nd acm sigkdd international conference knowledge discovery data mining series kdd 16 year 2016 isbn 9781450342322 location san francisco california usa page 12251234 numpages 10 url doi 10114529396722939753 acmid 2939753 publisher acm address new york ny usa keywords deep learning network analysis network embedding
Graphs;tgn temporal graph network blog dynamic graph tgn figuresdynamicgraphpng figurestgnpng introduction despite plethora different model deep learning graph approach proposed thus far dealing graph present sort dynamic nature eg evolving feature connectivity time paper present temporal graph network tgns generic efficient framework deep learning dynamic graph represented sequence timed event thanks novel combination memory module graphbased operator tgns able significantly outperform previous approach time computationally efficient furthermore show several previous model learning dynamic graph cast specific instance framework perform detailed ablation study different component framework devise best configuration achieves stateoftheart performance several transductive inductive prediction task dynamic graph paper link temporal graph network deep learning dynamic running experiment requirement dependency python 37 bash pandas110 torch160 scikitlearn0231 dataset preprocessing download public data download sample datasets eg wikipedia reddit store csv file folder named data preprocess data use dense npy format save feature binary format edge feature node feature absent replaced vector zero bash python utilspreprocessdatapy data wikipedia bipartite python utilspreprocessdatapy data reddit bipartite model training selfsupervised learning using link prediction task bash tgnattn supervised learning wikipedia dataset python trainselfsupervisedpy usememory prefix tgnattn nruns 10 tgnattnreddit supervised learning reddit dataset python trainselfsupervisedpy reddit usememory prefix tgnattnreddit nruns 10 supervised learning dynamic node classification requires trained model selfsupervised task eg running command bash tgnattn selfsupervised learning wikipedia dataset python trainsupervisedpy usememory prefix tgnattn nruns 10 tgnattnreddit selfsupervised learning reddit dataset python trainsupervisedpy reddit usememory prefix tgnattnreddit nruns 10 baseline bash wikipedia selfsupervised jodie python trainselfsupervisedpy usememory memoryupdater rnn embeddingmodule time prefix jodiernn nruns 10 dyrep python trainselfsupervisedpy usememory memoryupdater rnn dyrep usedestinationembeddinginmessage prefix dyreprnn nruns 10 reddit selfsupervised jodie python trainselfsupervisedpy reddit usememory memoryupdater rnn embeddingmodule time prefix jodiernnreddit nruns 10 dyrep python trainselfsupervisedpy reddit usememory memoryupdater rnn dyrep usedestinationembeddinginmessage prefix dyreprnnreddit nruns 10 wikipedia supervised jodie python trainsupervisedpy usememory memoryupdater rnn embeddingmodule time prefix jodiernn nruns 10 dyrep python trainsupervisedpy usememory memoryupdater rnn dyrep usedestinationembeddinginmessage prefix dyreprnn nruns 10 reddit supervised jodie python trainsupervisedpy reddit usememory memoryupdater rnn embeddingmodule time prefix jodiernnreddit nruns 10 dyrep python trainsupervisedpy reddit usememory memoryupdater rnn dyrep usedestinationembeddinginmessage prefix dyreprnnreddit nruns 10 ablation study command replicate result ablation study different module bash tgn2l python trainselfsupervisedpy usememory nlayer 2 prefix tgn2l nruns 10 tgnnomem python trainselfsupervisedpy prefix tgnnomem nruns 10 tgntime python trainselfsupervisedpy usememory embeddingmodule time prefix tgntime nruns 10 tgnid python trainselfsupervisedpy usememory embeddingmodule identity prefix tgnid nruns 10 tgnsum python trainselfsupervisedpy usememory embeddingmodule graphsum prefix tgnsum nruns 10 tgnmean python trainselfsupervisedpy usememory aggregator mean prefix tgnmean nruns 10 general flag txt optional argument data data data data source use wikipedia reddit b b batch size prefix prefix prefix name checkpoint result ndegree ndegree number neighbor sample layer nhead nhead number head used attention layer nepoch nepoch number epoch nlayer nlayer number graph attention layer lr lr learning rate patience patience early stopping strategy nruns number run compute mean std result dropout dropout dropout probability gpu gpu idx gpu use nodedim nodedim dimension node embedding timedim timedim dimension time embedding usememory whether use memory node embeddingmodule type embedding module messagefunction type message function memoryupdater type memory updater aggregator type message aggregator memoryupdateattheend whether update memory end start batch messagedim dimension message memorydim dimension memory backpropevery number batch process performing backpropagation differentnewnodes whether use different unseen node validation testing uniform whether sample temporal neighbor uniformly instead take recent one randomizefeatures whether randomize node feature dyrep whether run model dyrep todos make code memory efficient sake simplicity memory module tgn model implemented parameter stored loaded together model however need case efficient implementation treat model tensor way input feature would amenable large graph cite u bibtex inproceedingstgnicmlgrl2020 titletemporal graph network deep learning dynamic graph authoremanuele rossi ben chamberlain fabrizio frasca davide eynard federico monti michael bronstein booktitleicml 2020 workshop graph representation learning year2020
Graphs;stellargraph machine learning library p aligncenter altdocsimg altdiscourse forumimg altpypiimg altlicenseimg p p aligncenter altcontributions welcomeimg altbuild status masterimg altbuild status developimg altpypi downloadsimg altpypi downloads p stellargraph machine learning library stellargraph python library machine learning graph table content introductionintroduction getting startedgettingstarted getting helpgettinghelp example gcnexamplegcn algorithmsalgorithms installationinstallation install stellargraph using pypiinstallstellargraphusingpypi install stellargraph anaconda pythoninstallstellargraphinanacondapython install stellargraph github sourceinstallstellargraphfromgithubsource citingciting referencesreferences introduction stellargraph library offer stateoftheart algorithm graph machine making easy discover pattern answer question graphstructured data solve many machine learning task representation learning node edge used visualisation various downstream machine learning task classification attribute inference edge classification whole graph link prediction interpretation node 8 graphstructured data represent entity node vertex relationship edge link include data associated either attribute example graph contain people node friendship link data like person age date friendship established stellargraph support analysis many kind graph homogeneous node link one type heterogeneous one type node andor link knowledge graph extreme heterogeneous graph thousand type edge graph without data associated node graph edge weight stellargraph built tensorflow kera highlevel well thus userfriendly modular extensible interoperates smoothly code build standard kera layer easy augment core graph machine learning algorithm provided stellargraph thus also easy install pip anacondainstallation getting started numerous detailed narrated examplesdemos good way get started stellargraph likely one similar data problem let u knowgettinghelp demo start working example immediately google colab binder clicking badge within jupyter notebook alternatively run download local copy demo run using jupyter demo downloaded cloning master branch repository using curl command bash curl l tar xz strip1 stellargraphmasterdemos dependency required run demo notebook locally installed using one following using pip pip install stellargraphdemos using conda conda install c stellargraph stellargraph see installationinstallation section detail option getting help get stuck problem there many way make progress get help support read consult examplesdemos contact u ask question discus problem stellargraph discourse file send u email stellaradmincsiroaumailtostellaradmincsiroausubjectquestion20about20the20stellargraph20library example gcn one earliest deep machine learning algorithm graph graph convolution network gcn 6 following example us node classification predicting class node come show easy apply using stellargraph show stellargraph integrates smoothly panda tensorflow library built data preparation data stellargraph prepared using common library like panda scikitlearn python import panda pd sklearn import modelselection def loadmydata code load data panda dataframes eg csv file database node edge target loadmydata use scikitlearn compute training test set traintargets testtargets modelselectiontraintestsplittargets trainsize05 graph machine learning model part specific stellargraph machine learning model consists graph convolution layer followed layer compute actual prediction tensorflow tensor stellargraph make easy construct layer via gcn model class also make easy get input data right format via stellargraph graph data type data generator python import stellargraph sg import tensorflow tf convert raw data stellargraphs graph format faster operation graph sgstellargraphnodes edge generator sgmapperfullbatchnodegeneratorgraph methodgcn two layer gcn hidden dimension 16 gcn sglayergcnlayersizes16 16 generatorgenerator xinp xout gcninouttensors create input output tensorflow tensor use tensorflow kera add layer compute onehot prediction prediction tfkeraslayersdenseunitslengroundtruthtargetscolumns activationsoftmaxxout use input output tensor create tensorflow kera model model tfkerasmodelinputsxinp outputspredictions training evaluation model conventional tensorflow kera model task training evaluation use function offered kera stellargraphs data generator make simple construct required kera sequence input data python prepare model training adam optimiser appropriate loss function modelcompileadam losscategoricalcrossentropy metricsaccuracy train model train set modelfitgeneratorflowtraintargetsindex traintargets epochs5 check model generalisation test set loss accuracy modelevaluategeneratorflowtesttargetsindex testtargets printftest set loss loss accuracy accuracy algorithm spelled detail extended narrated notebookgcndemo provide many algorithm detailed exampledemos gcndemo algorithm stellargraph library currently includes following algorithm graph machine learning algorithm description graphsage 1 support supervised well unsupervised representation learning node classificationregression link prediction homogeneous network current implementation support multiple aggregation method including mean maxpool meanpool attentional aggregator hinsage extension graphsage algorithm heterogeneous network support representation learning node classificationregression link predictionregression heterogeneous graph current implementation support mean aggregation neighbour node taking account type type link attri2vec 4 support node representation learning node classification outofsample node link prediction homogeneous graph node attribute graph attention network gat 5 gat algorithm support representation learning node classification homogeneous graph version graph attention layer support sparse dense adjacency matrix graph convolutional network gcn 6 gcn algorithm support representation learning node classification homogeneous graph version graph convolutional layer support sparse dense adjacency matrix cluster graph convolutional network clustergcn 10 extension gcn algorithm supporting representation learning node classification homogeneous graph clustergcn scale larger graph used train deeper gcn model using stochastic gradient descent simplified graph convolutional network sgc 7 sgc network algorithm support representation learning node classification homogeneous graph extension gcn algorithm smooth graph bring distant neighbour node without using multiple layer approximate personalized propagation neural prediction ppnpappnp 9 appnp algorithm support fast scalable representation learning node classification attributed homogeneous graph semisupervised setting first multilayer neural network trained using node attribute input prediction latter network diffused across graph using method based personalized pagerank node2vec 2 node2vec deepwalk algorithm perform unsupervised representation learning homogeneous network taking account network structure ignoring node attribute node2vec algorithm implemented combining stellargraphs random walk generator word2vec algorithm learned node representation used downstream machine learning model implemented using python machine learning library metapath2vec 3 metapath2vec algorithm performs unsupervised metapathguided representation learning heterogeneous network taking account network structure ignoring node attribute implementation combine stellargraphs metapathguided random walk generator word2vec algorithm node2vec learned node representation node embeddings used downstream machine learning model solve task node classification link prediction etc heterogeneous network relational graph convolutional network 11 rgcn algorithm performs semisupervised learning node representation node classification knowledge graph rgcn extends gcn directed graph multiple edge type work sparse dense adjacency matrix complex12 complex algorithm computes embeddings node entity edge type relation knowledge graph use link prediction graphwave 13 graphwave calculates unsupervised structural embeddings via wavelet diffusion graph supervised graph classification model supervised graph classification based gcn 6 layer mean pooling readout watch step 14 watch step algorithm computes node embeddings using adjacency power simulate expected random walk deep graph infomax 15 deep graph infomax train unsupervised gnns maximize shared information node level graph level feature continuoustime dynamic network embeddings ctdne 16 support timerespecting random walk used similar way node2vec unsupervised representation learning distmult 17 distmult algorithm computes embeddings node entity edge type relation knowledge graph use link prediction dgcnn 18 deep graph convolutional neural network dgcnn algorithm supervised graph classification tgcn 19 gcnlstm model stellargraph follows temporal graph convolutional network architecture proposed tgcn paper enhancement layer architecture installation stellargraph python 3 library recommend using python version 36 required python version downloaded installed alternatively use anaconda python environment available stellargraph library installed pypi anaconda cloud directly github described install stellargraph using pypi install stellargraph library using pip execute following command pip install stellargraph examplesdemos require installing additional dependency well stellargraph install dependency well stellargraph using pip execute following command pip install stellargraphdemos community detection demo requires pythonigraph available platform install addition demo requirement pip install stellargraphdemosigraph install stellargraph anaconda python stellargraph library available anaconda installed anaconda using command line conda tool execute following command conda install c stellargraph stellargraph install stellargraph github source first clone stellargraph repository using git git clone cd stellargraph folder install library executing following command cd stellargraph pip install example demo directory require installing additional dependency well stellargraph install dependency well stellargraph using pip execute following command pip install demo citing stellargraph designed developed supported csiros use part library research please cite using following bibtex entry latex miscstellargraph author csiros data61 title stellargraph machine learning library year 2018 publisher github journal github repository howpublished reference 1 inductive representation learning large graph wl hamilton r ying j leskovec neural information processing system nip 2017 2 node2vec scalable feature learning network grover j leskovec acm sigkdd international conference knowledge discovery data mining kdd 2016 3 metapath2vec scalable representation learning heterogeneous network yuxiao dong nitesh v chawla ananthram swami acm sigkdd international conference knowledge discovery data mining kdd 135–144 2017 4 attributed network embedding via subspace discovery zhang jie x zhu c zhang data mining knowledge discovery 2019 5 graph attention network p veličković et al international conference learning representation iclr 2018 6 graph convolutional network gcn semisupervised classification graph convolutional network thomas n kipf max welling international conference learning representation iclr 2017 7 simplifying graph convolutional network f wu zhang h de souza c fifty yu k q weinberger international conference machine learning icml 2019 8 adversarial example graph data deep insight attack defense h wu c wang tyshetskiy docherty k lu l zhu ijcai 2019 9 predict propagate graph neural network meet personalized pagerank j klicpera bojchevski günnemann iclr 2019 10 clustergcn efficient algorithm training deep large graph convolutional network w chiang x liu si li bengio c hsiej kdd 2019 11 modeling relational data graph convolutional network schlichtkrull n kipf p bloem r van den berg titov welling european semantic web conference 2018 arxiv160902907 12 complex embeddings simple link prediction trouillon j welbl riedel é gaussier g bouchard icml 2016 13 learning structural node embeddings via diffusion wavelet c donnat zitnik hallac j leskovec sigkdd 2018 arxiv171010321 14 watch step learning node embeddings via graph attention abuelhaija b perozzi r alrfou alemi nip 2018 arxiv171009599 15 deep graph infomax p veličković w fedus w l hamilton p lio bengio r hjelm iclr 2019 arxiv180910341 16 continuoustime dynamic network embeddings giang hoang nguyen john boaz lee ryan rossi nesreen k ahmed eunyee koh sungchul kim proceeding 3rd international workshop learning representation big network www bignet 2018 17 embedding entity relation learning inference knowledge base bishan yang wentau yih xiaodong jianfeng gao li deng iclr 2015 arxiv14126575 18 endtoend deep learning architecture graph classification muhan zhang zhicheng cui marion neumann yixin chen aaai 2018 19 tgcn temporal graph convolutional network traffic prediction ling zhao yujiao song chao zhang yu liu pu wang tao lin min deng haifeng liieee transaction intelligent transportation system 2019
Graphs;attention walk codebeat repo pytorch implementation watch step learning node embeddings via graph attention nip 2018 div styletextaligncenterimg src attentionwalkjpg width720div abstract p alignjustify graph embedding method represent node continuous vector space preserving different type relational information graph many hyperparameters method eg length random walk manually tuned every graph paper replace previously fixed hyperparameters trainable one automatically learn via backpropagation particular propose novel attention model power series transition matrix guide random walk optimize upstream objective unlike previous approach attention model method propose utilizes attention parameter exclusively data eg random walk used model inference experiment link prediction task aim produce embeddings bestpreserve graph structure generalizing unseen information improve stateoftheart result comprehensive suite realworld graph datasets including social collaboration biological network observe graph attention model reduce error 2040 show automaticallylearned attention parameter vary significantly per graph correspond optimal choice hyperparameter manually tune existing methodsp repository provides implementation attention walk described paper watch step learning node embeddings via graph attention sami abuelhaija bryan perozzi ramus alrfou alexander alemi nip 2018 original tensorflow implementation available requirement codebase implemented python 352 package version used development networkx 24 tqdm 4281 numpy 1154 panda 0234 texttable 150 scipy 110 argparse 110 torch 110 torchvision 030 datasets p alignjustify code take input graph csv file every row indicates edge two node separated comma first row header node indexed starting 0 sample graph twitch brasilians wikipedia chameleon included input directory p option learning embedding handled srcmainpy script provides following command line argument input output option edgepath str input graph path default inputchameleonedgescsv embeddingpath str embedding path default outputchameleonawembeddingcsv attentionpath str attention path default outputchameleonawattentioncsv model option dimension int number embeding dimension default 128 epoch int number training epoch default 200 windowsize int skipgram window size default 5 learningrate float learning rate value default 001 beta float attention regularization parameter default 05 gamma float embedding regularization parameter default 05 numofwalks int number walk per source node default 80 example p alignjustify following command learn graph embedding write embedding disk node representation ordered id p p alignjustify creating attention walk embedding default dataset standard hyperparameter setting saving embedding default path p python srcmainpy p aligncenter img stylefloat center srcattentionwalkrunexamplejpg p creating attention walk embedding default dataset 256 dimension python srcmainpy dimension 256 creating attention walk embedding default dataset higher window size python srcmainpy windowsize 20 creating embedding another dataset twitch brasilians saving output custom file name python srcmainpy edgepath inputptbredgescsv embeddingpath outputptbrawembeddingcsv attentionpath outputptbrawattentioncsv license gnu
Graphs;discovering symbolic model deep learning inductive official implementation discovering symbolic model deep learning inductive mile cranmer alvaro sanchezgonzalez peter battaglia rui xu kyle cranmer david spergel shirley ho check interactive requirement model pytorch numpy symbolic regression new opensource eureqa alternative simulation simple nbody simulation dark matter data optional tqdm matplotlib training train example model paper try full model definition given modelspy data generated simulatepy result train simulation produced following equation imagessimulationequationspng giving u time series imagessimulationspng recorded performance model imagestestpredictionpng also measured well model message correlated linear combination force imagesequaltoforcespng finally trained dark matter simulation extracted following equation message function imagesdarkmatterpng
Graphs;node2vec repository provides reference implementation node2vec described paperbr node2vec scalable feature learning networksbr aditya grover jure leskovecbr knowledge discovery data mining 2016br insert paper link node2vec algorithm learns continuous representation node undirected unweighted graph please check project detail basic usage example run node2vec zacharys karate club network execute following command project home directorybr python srcmainpy input graphkarateedgelist output embkarateemd option check option available use node2vec usingbr python srcmainpy help input supported input format edgelist node1idint node2idint weightfloat optional graph assumed undirected unweighted default option changed setting appropriate flag output output file n1 line graph n vertex first line following format numofnodes dimofrepresentation next n line follows nodeid dim1 dim2 dimd dim1 dimd ddimensional representation learned node2vec citing find node2vec useful research please consider citing following paper inproceedingsnode2veckdd2016 author grover aditya leskovec jure title node2vec scalable feature learning network booktitle proceeding 22nd acm sigkdd international conference knowledge discovery data mining year 2016 miscellaneous please send question might code andor algorithm adityagcsstanfordedu note reference implementation node2vec algorithm could benefit several performance enhancement scheme discussed paper
Graphs;graph convolutional network pytorch pytorch implementation graph convolutional network gcns semisupervised classification 1 highlevel introduction gcns see thomas kipf graph convolutional 2016 graph convolutional networksfigurepng note subtle difference tensorflow implementation pytorch reimplementation reimplementation serf proof concept intended reproduction result reported 1 implementation make use cora dataset 2 installation python setuppy install requirement pytorch 04 05 python 27 36 usage python trainpy reference 1 kipf welling semisupervised classification graph convolutional network 2 sen et al collective classification network data ai magazine cite please cite paper use code work articlekipf2016semi titlesemisupervised classification graph convolutional network authorkipf thomas n welling max journalarxiv preprint arxiv160902907 year2016
Graphs;p aligncenter img height200 p latest conda latest build benchmark blitz introduction documentation official examplesexamplesreadmemd discussion slack dgl easytouse high performance scalable python package deep learning graph dgl framework agnostic meaning deep graph model component endtoend application rest logic implemented major framework pytorch apache mxnet tensorflow p aligncenter img altdgl v04 architecture width600 br bfigureb dgl overall architecture p highlighted feature gpuready graph library dgl provides powerful graph object reside either cpu gpu bundle structural data well feature better control provide variety function computing graph object including efficient customizable message passing primitive graph neural network model module benchmark gnn researcher field graph deep learning still rapidly evolving many research idea emerge standing shoulder giant ease process dgl collect rich set example popular gnn model wide range topic researcher related model innovate new idea use baseline experiment moreover dgl provides many stateoftheart gnn layer user build new model architecture dgl one preferred platform many standard graph deep learning benchmark including easy learn use dgl provides plenty learning material kind user ml researcher domain expert blitz introduction 120minute tour basic graph machine learning user explains detail concept graph well training methodology include code snippet dgl runnable ready plugged one’s pipeline scalable efficient convenient train model using dgl largescale graph across multiple gpus multiple machine dgl extensively optimizes whole stack reduce overhead communication memory consumption synchronization result dgl easily scale billionsized graph see system performance comparison tool distdgl parmetis implementation also provides support hetero graph adding back dropped edge back partitioned graph thus handling parmetis hetero graph usecase get started user install dgl pip advanced user follow install source absolute beginner start blitz introduction cover basic concept common graph machine learning task stepbystep building graph neural network gnns solve acquainted user wish learn learn dgl example popular gnn model read user explains concept usage dgl much detail go tutorial advanced feature like stochastic training training study classical graph machine learning alongside dgl search usage specific api api reference organizes dgl apis namespace learning material available documentation new deep learning general check open source book dive deep community get connected provide multiple channel connect community dgl developer user general gnn academic researcher slack channel click discussion forum zhihu blog monthly gnn user group online seminar event past take survey leave feedback make dgl better fit need thanks dglpowered project dgllifesci dglbased package various application life science graph neural network dglke high performance easytouse scalable package learning largescale knowledge graph embeddings benchmarking gnn ogb collection realistic largescale diverse benchmark datasets machine learning graph graph4nlp easytouse library rd intersection deep learning graph natural language processing gnnrecsys amazon neptune ml new capability neptune us graph neural network gnns machine learning technique purposebuilt graph make easy fast accurate prediction using graph data gnnlens2 visualization tool graph neural network awesome paper using dgl 1 benchmarking graph neural vijay prakash dwivedi chaitanya k joshi thomas laurent yoshua bengio xavier bresson 1 open graph benchmark datasets machine learning neurips20 weihua hu matthias fey marinka zitnik yuxiao dong hongyu ren bowen liu michele catasta jure leskovec 1 dropedge towards deep graph convolutional network node iclr20 yu rong wenbing huang tingyang xu junzhou huan 1 discourseaware neural extractive text acl20 jiacheng xu zhe gan yu cheng jingjing liu 1 gcc graph contrastive coding graph neural network kdd20 jiezhong qiu qibin chen yuxiao dong jing zhang hongxia yang ming ding kuansan wang jie tang 1 dglke training knowledge graph embeddings sigir20 da zheng xiang song chao zeyuan tan zihao ye jin dong hao xiong zheng zhang george karypis 1 improving graph neural network expressivity via subgraph isomorphism giorgos bouritsas fabrizio frasca stefanos zafeiriou michael bronstein 1 int inequality benchmark evaluating generalization theorem yuhuai wu albert q jiang jimmy ba roger grosse 1 finding patient zero learning contagion source graph neural chintan shah nima dehmamy nicola perra matteo chinazzi albertlászló barabási alessandro vespignani rose yu 1 featgraph flexible efficient backend graph neural network sc20 yuwei hu zihao ye minjie wang jiali yu da zheng mu li zheng zhang zhiru zhang yida wang detailssummarymoresummary 11 bptransformer modelling longrange context via binary zihao ye qipeng guo quan gan xipeng qiu zheng zhang 12 optimol optimization binding affinity chemical space drug jacques boitreaudvincent mallet carlos oliver jérôme waldispühl 1 jaket joint pretraining knowledge graph language donghan yu chenguang zhu yiming yang michael zeng 1 architectural implication graph neural zhihui zhang jingwen leng lingxiao youshan miao chao li minyi guo 1 combining reinforcement learning constraint programming combinatorial quentin cappart thierry moisan louismartin rousseau1 isabeau prémontschwarz andre cire 1 therapeutic data common machine learning datasets task code kexin huang tianfan fu wenhao gao yue zhao yusuf roohani jure leskovec connor w coley cao xiao jimeng sun marinka zitnik 1 sparse graph attention yang ye shihao ji 1 selfdistilling graph neural yuzhao chen yatao bian xi xiao yu rong tingyang xu junzhou huang 1 learning robust node representation xu chen ya zhang ivor tsang yuangang pan 1 recurrent event network autoregressive structure inference temporal knowledge woojeong jin meng qu xisen jin xiang ren 1 graph neural ordinary differential michael poli stefano massaroli junyoung park atsushi yamashita hajime asama jinkyoo park 1 fusedmm unified sddmmspmm kernel graph embedding graph neural md khaledur rahman majedul haque sujon ariful azad 1 efficient neighborhoodbased interaction model recommendation heterogeneous kdd20 jiarui jin jiarui qin yuchen fang kounianhua du weinan zhang yong yu zheng zhang alexander j smola 1 learning interaction model structured neighborhood heterogeneous information jiarui jin kounianhua du weinan zhang jiarui qin yuchen fang yong yu zheng zhang alexander j smola 1 graphein python library geometric deep learning network analysis protein arian r jamasb pietro lió tom l blundell 1 graph policy gradient large scale robot arbaaz khan ekaterina tolstaya alejandro ribeiro vijay kumar 1 heterogeneous molecular graph neural network predicting molecule zeren shui george karypis 1 could graph neural network learn better molecular representation drug discovery comparison study descriptorbased graphbased dejun jiang zhenxing wu changyu hsieh guangyong chen ben liao zhe wang chao shen dongsheng cao jian wu tingjun hou 1 principal neighbourhood aggregation graph gabriele corso luca cavalleri dominique beaini pietro liò petar veličković 1 collective multitype entity alignment knowledge qi zhu hao wei bunyamin sisman da zheng christos faloutsos xin luna dong jiawei han 1 graph representation forecasting patient medical condition towards digital pietro barbiero ramon viñas torné pietro lió 1 relational graph learning visual kinematics embeddings accurate gesture recognition robotic yonghao long jieying wu bo lu yueming jin mathias unberath yunhui liu phengann heng qi dou 1 dark reciprocalrank boosting graphconvolutional selflocalization network via teachertostudent knowledge takeda koji tanaka kanji 1 graph infoclust leveraging clusterlevel node information unsupervised graph representation costa mavromatis george karypis 1 graphseam supervised graph learning framework semantic uv fatemeh teimury bruno roy juan sebastian casallas david macdonald mark coates 1 comprehensive study molecular supervised learning graph neural doyeong hwang soojung yang yongchan kwon kyung hoon lee grace lee hanseok jo seyeol yoon seongok ryu 1 graph autoencoder model mirnadisease association zhengwei li jiashu li ru nie zhuhong wenzheng bao 1 graph convolutional regression cardiac depolarization sparse endocardial stacom 2020 workshop felix meister tiziano passerini chloé audigier èric lluch viorel mihalef hiroshi ashikaga andreas maier henry halperin tommaso mansi 1 attnio knowledge graph exploration inandout attention flow knowledgegrounded emnlp20 jaehun jung bokyung son sungwon lyu 1 learning nonbinary constituency tree via tensor coling20 daniele castellana davide bacciu 1 inducing alignment structure gated graph attention network sentence peng cui le hu yuanchao liu 1 enhancing extractive text summarization topicaware graph neural coling20 peng cui le hu yuanchao liu 1 double graph based reasoning documentlevel relation emnlp20 shuang zeng runxin xu baobao chang lei li 1 systematic generalization gscan language conditioned aaclijcnlp20 tong gao qi huang raymond j mooney 1 automatic selection clustering algorithm using supervised graph noy cohenshapira lior rokach 1 improving learning branch via reinforcement haoran sun wenbo chen hui li le song 1 practical guide graph neural isaac ronald ward jack joyner casey lickfold stash rowe yulan guo mohammed bennamoun 1 apan asynchronous propagation attention network realtime temporal graph sigmod21 xuhong wang ding lyu mengjian li yang xia qi yang xinwen wang xinguang wang ping cui yupu yang bowen sun zhenyu guo junkui li 1 uncertaintymatching graph neural network defend poisoning uday shankar shanthamallu jayaraman j thiagarajan andreas spanias 1 computing graph neural network survey algorithm sergi abadal akshay jain robert guirado jorge lópezalonso eduard alarcón 1 nhkstrl wnut2020 task 2 gat syntactic dependency edge ctcbased loss text yuki yasuda taichi ishiwatari taro miyazaki jun goto 1 relationaware graph attention network relational position encoding emotion recognition taichi ishiwatari yuki yasuda taro miyazaki jun goto 1 pgmexplainer probabilistic graphical model explanation graph neural minh n vu thai 1 generalization transformer network vijay prakash dwivedi xavier bresson 1 discourseaware neural extractive text acl20 jiacheng xu zhe gan yu cheng jingjing liu 1 learning robust node representation xu chen ya zhang ivor tsang yuangang pan 1 adaptive graph diffusion network hopwise chuxiong sun guoshi wu 1 photoswitch dataset molecular machine learning benchmark advancement synthetic aditya r thawani ryanrhys griffith arian jamasb anthony bourached penelope jones william mccorkindale alexander aldrick alpha lee 1 communitypowered search machine learning strategy space find nmr property prediction lars bratholm gerrard brandon anderson shaojie bai sunghwan choi lam dang pavel hanchar addison howard guillaume huard sanghoon kim zico kolter risi kondor mordechai kornbluth youhan lee youngsoo lee jonathan p mailoa thanh tu nguyen milo popovic goran rakocevic walter reade wonho song luka stojanovic erik h thiede nebojsa tijanic andres torrubia devin willmott craig p butt david r glowacki kaggle participant 1 adaptive layout decomposition graph embedding neural wei li jialu xia yuzhe jialu li yibo lin bei yu dac20 1 transfer learning graph neural network optoelectronic property conjugated j chem phys 154 cheekong lee chengqiang lu yue yu qiming sun changyu hsieh shengyu zhang qi liu liang shi 1 jet tagging lund plane graph journal high energy physic 2021 frédéric dreyer huilin qu 1 global attention improves graph network omri puny heli benhamu yaron lipman 1 learning family set hypergraph representation learning higher order sdm 2021 balasubramaniam srinivasan da zheng george karypis 1 ssfg stochastically scaling feature gradient regularizing graph convolution haimin zhang min xu 1 application evaluation knowledge graph embeddings biomedical peerj computer science 7e341 mona alshahrani​ maha thafar magbubah essack 1 motse interpretable task similarity estimator small molecular property prediction biorxiv 20210113426608 han li xinyi zhao shuya li fangping wan dan zhao jianyang zeng 1 reinforcement learning data poisoning graph neural jacob dineen ahsanul haque matthew bielskas 1 generalising recursive neural model tensor ijcnn20 daniele castellana davide bacciu 1 tensor decomposition recursive neural network treestructured esann20 daniele castellana davide bacciu 1 combining selforganizing graph neural network modeling deformable object robotic frotiers robotics ai valencia angel j pierre payeur 1 joint stroke classification text line grouping online handwritten document edge pooling attention pattern recognition junyu ye yanming zhang qing yang chenglin liu 1 toward accurate prediction atomic property via quantum mechanic descriptor augmented graph convolutional neural network application novel approach nmr chemical shift journal physical chemistry letter peng gao jie zhang yuzhu sun jianguo yu 1 graph neural network model user comfort robot pilar bachiller daniel rodriguezcriado ronit r jorvekar pablo bustos diego r faria luis j manso 1 medical entity disambiguation using graph neural alina vretinaris chuan lei vasilis efthymiou xiao qin fatma özcan 1 chemistryinformed macromolecule graph representation similarity computation supervised somesh mohapatra joyce rafael gómezbombarelli 1 characterizing forecasting user engagement inapp action graph case study yozen liu xiaolin shi lucas pierce xiang ren 1 gipa general information propagation algorithm graph qinkai zheng houyi li peng zhang zhixiong yang guowei zhang xintan zeng yongchao liu 1 graph ensemble learning multiple dependency tree aspectlevel sentiment naacl21 xiaochen hou peng qi guangtao wang rex ying jing huang xiaodong bowen zhou 1 enhancing scientific paper summarization citation aaai21 chenxin ming zhong yiran chen danqing wang xipeng qiu xuanjing huang 1 improving graph representation learning contrastive kaili haochen yang han yang tatiana jin pengfei chen yongqiang chen barakeel fanseu kamhoua james cheng 1 extract knowledge graph neural network go beyond effective knowledge distillation www21 cheng yang jiawei liu chuan shi 1 viking adversarial attack network embeddings via supervised network pakdd21 viresh gupta tanmoy chakraborty 1 knowledge graph embedding using graph convolutional network relationaware nasrullah sheikh xiao qin berthold reinwald christoph miksovic thomas gschwind paolo scotton 1 slap selfsupervision improves structure learning graph neural bahare fatemi layla el asri seyed mehran kazemi 1 finding needle heterogeneous aaai21 bijaya adhikari liangyue li nikhil rao karthik subbian 1 retcl selectionbased approach retrosynthesis via contrastive ijcai 2021 hankook lee sungsoo ahn seungwoo seo young song eunho yang sungju hwang jinwoo shin 1 accurate prediction free solvation energy organic molecule via graph attention network message passing neural network pairwise atomistic ramin ansari amirata ghorbani 1 dipsplus enhanced database interacting protein structure interface alex morehead chen chen ada sedova jianlin cheng 1 coreferenceaware dialogue sigdial21 zhengyuan liu ke shi nancy f chen 1 document structure aware relational graph convolutional network ontology arxiv abhay shalghar ayush kumar balaji ganesan aswin kannan shobha g 1 covid19 detection chest xray patient metadata using graph convolutional neural thosini bamunu mudiyanselage nipuna senanayake chunyan ji yi pan yanqing zhang 1 rossmanntoolbox deep learningbased protocol prediction design cofactor specificity rossmann fold briefing bioinformatics kamil kaminski jan ludwiczak maciej jasinski adriana bukala rafal madaj krzysztof szczepaniak stanislaw duninhorkawicz 1 lgesql line graph enhanced texttosql model mixed local nonlocal acl21 ruisheng cao lu chen zhi chen yanbin zhao su zhu kai yu 1 enhancing graph neural network via auxiliary training semisupervised node knowledgebased system21 yao wu yu song hong huang fanghua ye xing xie hai jin 1 modeling graph node correlation neighbor mixture linfeng liu michael c hughes liping liu 1 combining physic machine learning network flow iclr21 arlei silva furkan kocayusufoglu saber jafarpour francesco bullo ananthram swami ambuj singh 1 classification method academic resource based graph attention future internet21 jie yu yaliu li chenle pan junwei wang 1 large graph convolutional network training gpuoriented data communication seung min kun wu sitao huang mert hidayetoğlu jinjun xiong eiman ebrahimi deming chen wenmei hwu 1 graph attention multilayer wentao zhang ziqi yin zeang sheng wen ouyang xiaosen li yangyu tao zhi yang bin cui 1 gnnlens visual analytics approach prediction error diagnosis graph neural zhihua jin yong wang qianwen wang yao ming tengfei huamin qu 1 attentive graph attention shaked brody uri alon eran yahav detail contributing please let u know encounter bug suggestion filing welcome contribution bug fix new feature extension expect contribution discussed issue tracker going pr please refer contribution cite use dgl scientific publication would appreciate citation following paper articlewang2019dgl titledeep graph library graphcentric highlyperformant package graph neural network authorminjie wang da zheng zihao ye quan gan mufei li xiang song jinjing zhou chao lingfan yu yu gai tianjun xiao tong george karypis jinyang li zheng zhang year2019 journalarxiv preprint arxiv190901315 team dgl developed maintained nyu nyu shanghai aws shanghai ai lab aws mxnet science license dgl us apache license 20
Graphs;cayleynets present tensorflow implementation graph convolutional neural network illustrated cayleynets graph convolutional neural network complex rational spectral filtersbr ieee transaction signal processing 2018br ron levie federico monti xavier bresson michael bronstein repository contains sparse implementation nn used solving mnist digit classification problem described paper rational spectral filter approximated jacobi method provide efficient solution shall use cayleynet cayleynet graph cnn spectral zoom property able effectively operate signal defined graph thanks particular spectral property cayleynet well suited dealing variety different domain eg citation network community graph useritem similarity graph variation architecture implemented achieved stateoftheart performance vertex classification community detection matrix completion task useful link img srcpichome100jpg width20 height20 stylemaxwidth100 img srcpicwebpng width20 height20 stylemaxwidth100
Graphs;event detection dynamic graph pytorch implementation dyged dynamic graph event detection proposed event detection architecture paper mert kosan etal event detection dynamic 2021 get started bash pip install r requirementstxt todo twitter weather dataset trained model test data run model instruction cite use codeimplementation please cite u work articlekosan2021event titleevent detection dynamic graph authorkosan mert silva arlei medya sourav uzzi brian singh ambuj journalarxiv preprint arxiv211012148 year2021
Graphs;stochastic training graph convolutional network code paper stochastic training graph convolutional implementation based thomas kipfs implementation graph convolutional requirement tensorflow 012 networkx 111 loading graphsage datasets installation make sure valid c11 cuda compiler bash pip install upgrade pip pip install numpy scipy networkx111 tensorflowgpu scikitlearn python setuppy install cd gcn buildsh data support data format data format download citeseer cora pubmed ppi reddit dataset mkdir data cd data git clone gitgithubcomtkipfgcngit mv gcngcndata wget unzip ppizip mv ppi wget unzip redditzip mv reddit run demo provide recipe citeseer cora pubmed nell ppi reddit dataset config folder please refer paper algorithm typically cvdpp similar accuracy exact faster bash exact algorithm configcoraconfig nspp algorithm configcoraconfig degree1 testdegree1 cvpp algorithm configcoraconfig cv testcv degree1 testdegree1 cvdpp algorithm configcoraconfig cv cvd testcv degree1 testdegree1 cite please cite paper use code work inproceedingschen2018stochastic titlestochastic training graph convolutional network variance reduction authorchen jianfei zhu jun song le booktitleinternational conference machine learning pages941949 year2018
Graphs;convolutional neural network graph fast localized spectral filtering code repository implement efficient generalization popular convolutional neural network cnns arbitrary graph presented paper michaël defferrard xavier bresson pierre vandergheynst convolutional neural network graph fast localized spectral filteringarxiv neural information processing system nip 2016 additional material nips2016 spotlight videovideo 20161122 deep learning graphsslidesntds lecture epfls master course network tour data sciencentds 20161221 deep learning graphsslidesdlid invited talk deep learning irregular domainsdlid workshop bmvc 20170917 video slidesntds ntds slidesdlid dlid also implementation filter used joan bruna wojciech zaremba arthur szlam yann lecun spectral network locally connected network graphsbruna international conference learning representation iclr 2014 mikael henaff joan bruna yann lecun deep convolutional network graphstructured datahenaff arxiv 2015 arxiv bruna henaff installation 1 clone repository sh git clone cd cnngraph 2 install dependency code run tensorflow 10 newer sh pip install r requirementstxt make install 3 play jupyter notebook sh jupyter notebook reproducing result run notebook reproduce experiment mnistnips2016mnistipynb 20newsnips201620newsipynb presented paper sh cd nips2016 make using model use graph convnet data need 1 data matrix row sample column feature 2 target vector 3 optionally adjacency matrix encodes structure graph see usage notebookusage simple example fabricated data please get touch unsure applying model different setting usage license co code repository released term mit licenselicensetxt please cite paperarxiv use inproceedingscnngraph title convolutional neural network graph fast localized spectral filtering author defferrard michael bresson xavier vandergheynst pierre booktitle advance neural information processing system year 2016 url
Graphs;graphsaint graph usaumpling based uinuductive learning meutuhod hanqing hongkuan ajitesh rajgopal kannan viktor contact hanqing zeng zenghuscedu hongkuan zhou hongkuazuscedu feel free report bug tell u suggestion overview graphsaint general flexible framework training gnns large graph graphsaint highlight novel minibatch method specifically optimized data complex relationship ie graph traditional way training gnn 1 construct gnn full training graph 2 minibatch pick node output layer root node backtrack interlayer connection root node reaching input layer 3 forward backward propagation based loss root way graphsaint train gnn 1 minibatch sample small subgraph full training graph 2 construct complete gnn small subgraph sampling performed within gnn layer 3 forward backward propagation based loss subgraph node graphsaint training algorithmoverviewdiagrampng graphsaint performs graph sampling based training whereas others perform layer sampling based training matter change perspective sampling graphsaint achieves following accuracy perform simple yet effective normalization eliminate bias introduced graph sampling addition since sampling process incurs information loss due dropped neighbor propose lightweight graph sampler preserve important neighbor based topological characteristic fact graph sampling also understood data augmentation training regularization eg may see edge sampling minibatch version efficiency neighbor explosion headache many layer sampling based method graphsaint provides clean solution thanks graph sampling philosophy gnn layer complete unsampled number neighbor keep constant matter deep go computation cost per minibatch reduces exponential linear wrt gnn depth flexibility layer propagation minibatch subgraph graphsaint almost identical full graph therefore gnn architecture designed full graph seamlessly trained graphsaint hand layer sampling algorithm support limited number gnn architecture take jknet example jumping knowledge connection requires node sample shallower layer superset node sampler deeper layer minibatches fastgcn asgcn satisfy condition scalability graphsaint achieves scalability wrt 1 graph size subgraph size need grow proportionally training graph size even dealing millionnode graph subgraphs still easily fit gpu memory 2 model size resolving neighbor explosion training cost scale linearly gnn width depth 3 amount parallel resource graph sampling highly scalable trivial task parallelism addition resolving neighbor explosion also implies dramatic reduction communication overhead critical distributed setting see ieeeipdps 19 hardware accelerator news check new work generalizes subgraph sampling training inference neurips21 repo repo contains source code two paper iclr 20 ieeeipdps 19 see citationcitationacknowledgement section graphsaint directory contains python implementation minibatch training algorithm iclr 20 provide two implementation one tensorflow pytorch two version follow algorithm note experiment paper based tensorflow implementation new experiment open graph benchmark based pytorch version ipdps19cpp directory contains c implementation parallel training technique described ieeeipdps 19 see ipdps19cppreadmemd rest repository graphsaint iclr 20 gnn architecture supported repo gnn arch tensorflow pytorch c graphsage heavycheckmark heavycheckmark heavycheckmark gat heavycheckmark heavycheckmark jknet heavycheckmark gaan heavycheckmark mixhop heavycheckmark heavycheckmark graph sampler supported repo sampler tensorflow pytorch c node heavycheckmark heavycheckmark edge heavycheckmark heavycheckmark rw heavycheckmark heavycheckmark mrw heavycheckmark heavycheckmark heavycheckmark heavycheckmark full graph heavycheckmark heavycheckmark rw random walk sampler mrw multidimensional random walk sampler full graph always return full training graph meant baseline real sampling going add sampler gnn layer easily see customizationcustomization section result new testing graphsaint open graph currently result ogbnproducts graph note ogbnproducts accuracy leaderboard trained method mostly transductive setting result inductive learning harder result iclr 20 reproduced running config trainconfig example trainconfigtable2yml store config table 2 paper trainconfigexploreyml store config deeper gnns various gnn architecture gat jk etc addition result related ogb trained config trainconfigopengraphbenchmarkyml test set f1mic score summarized sampler depth gnn ppi ppi large flickr reddit yelp amazon ogbnproducts node 2 sage 0960 0507 0962 0641 0782 edge 2 sage 0981 0510 0966 0653 0807 rw 2 sage 0981 0941 0511 0966 0653 0815 mrw 2 sage 0980 0510 0964 0652 0809 rw 5 sage 0995 edge 4 jk 0970 rw 2 gat 0510 0967 0652 0815 rw 2 gaan 0508 0968 0651 rw 2 mixhop 0967 edge 3 gat 08027 dependency python 368 tensorflow 1120 pytorch 110 cython 0292 numpy 1143 scipy 110 scikitlearn 0191 pyyaml 312 g 540 openmp 40 datasets datasets used paper available download ppi ppilarge larger version ppi reddit flickr yelp amazon ogbnproducts added available google drive alternatively baiduyun link code rename folder data root directory directory structure graphsaint │ readmemd │ rungraphsaintsh │ │ └───graphsaint │ │ globalspy │ │ cythonsamplerpyx │ │ │ │ │ └───tensorflowversion │ │ │ trainpy │ │ │ modelpy │ │ │ │ │ │ └───pytorchversion │ │ trainpy │ │ modelpy │ │ │ └───data │ └───ppi │ │ │ adjtrainnpz │ │ │ adjfullnpz │ │ │ │ │ │ └───reddit │ │ │ │ │ │ └─── │ also script convert datasets format graphsage format run script python convertpy dataset name example python convertpy ppi convert dataset ppi save new data graphsage format dataignoreppi new data conversion ogb format graphsaint format please use script dataopengraphbenchmarkogbnconverterpy currently script handle ogbnproducts ogbnarxiv cython implemented parallel graph sampler cython module need compilation training start compile module running following root directory python graphsaintsetuppy buildext inplace training configuration hyperparameters needed training set via configuration file trainconfignameyml configuration file reproduce table 2 result packed trainconfigtable2 detailed description configuration file format please see trainconfigreadmemd run training first please compile cython sampler see suggest looking available command line argument defined graphsaintglobalspy shared tensorflow pytorch version properly setting flag maximize cpu utilization sampling step telling number available core select directory place log file turn logger tensorboard timeline etc note method compared paper graphsaint gcn graphsage fastgcn sgcn asgcn clustergcn sampling clustering performed training obtain validation test set accuracy run full batch gnn full graph training validation test node calculate f1 score validation test node see also issue 11 simplicity implementation validation test set evaluation perform layer propagation using full graph adjacency matrix amazon yelp may cause memory issue gpus outofmemory error occurs please use cpueval flag force val test set evaluation take place cpu minibatch training still performed gpu see flag run code cpu python graphsainttensorflowpytorchversiontrain dataprefix datadatasetname trainconfig path trainconfig yml gpu 1 run code gpu python graphsainttensorflowpytorchversiontrain dataprefix datadatasetname trainconfig path trainconfig yml gpu gpu number example gpu 0 run first gpu also use gpu gpu number cpueval make gpu perform minibatch training cpu perform validation test evaluation also implemented dualgpu training speedup runtime simply add flag dualgpu assign two gpus using gpu flag currently work gpus supporting memory pooling connected nvlink new prepared specific script train ogb graph see graphsaintopengraphbenchmark script instruction customization describe customize code base research product prepare dataset suppose full graph contains n node node c class lengthf initial attribute vector trainvaltest split abc ie abc1 adjfullnpz sparse matrix csr format stored scipysparsecsrmatrix shape n n nonzeros matrix correspond edge full graph doesnt matter two node connected edge training validation test node unweighted graph nonzeros 1 adjtrainnpz sparse matrix csr format stored scipysparsecsrmatrix shape also n n however nonzeros matrix correspond edge connecting two training node graph sampler pick nodesedges adjtrain adjfull therefore neither attribute information structural information revealed training also note row col adjtrain contains nonzeros see also issue 11 unweighted graph nonzeros 1 rolejson dictionary three key key tr corresponds list training node index key va corresponds list validation node index key te corresponds list test node index note raw data node may stringtype id would need reassign numerical id 0 n1 node index matrix adj feature class label classmapjson dictionary length n key node index value either length c binary list multiclass classification integer scalar 0 c1 singleclass classification featsnpy numpy array shape n f row corresponds attribute vector node add sampler sampler implemented subclass graphsampler graphsaintgraphsamplerspy two way implement sampler subclass 1 implement pure python overwrite parsample function superclass provide basic example nodesamplingvanillapython class graphsaintgraphsamplerspy pro easy implement con may slow execution speed nontrivial parallelize pure python function 2 implement cython need add subclass sampler graphsaintcythonsamplerpyx subclass need overwrite cinit sample function sample function defines sequential behavior sampler automatically perform tasklevel parallelism launching multiple sampler time pro fit parallelexecution framework c level execution speed con hard code support gnn layer add layer graphsainttensorflow pytorchversionlayerspy would also need minor update init function graphsaint class graphsainttensorflow pytorchversionmodelspy model know lookup correct class based keyword yml config citation acknowledgement supported darpa fa875017c0086 nsf ccf1919289 oac1911229 thank matthias fey providing reference pytorch geometric library thank ogb using graphsaint large scale experiment iclr 2020 inproceedingsgraphsainticlr20 titlegraphsaint graph sampling based inductive learning method authorhanqing zeng hongkuan zhou ajitesh srivastava rajgopal kannan viktor prasanna booktitleinternational conference learning representation year2020 ieeeipdps 2019 inproceedingsgraphsaintipdps19 authorhanqing zeng hongkuan zhou ajitesh srivastava rajgopal kannan viktor prasanna booktitle2019 ieee international parallel distributed processing symposium ipdps titleaccurate efficient scalable graph embedding year2019 monthmay
Graphs;graphembeddingmaster method model paper note deepwalk kdd 2014deepwalk online learning social 【graph line www 2015line largescale information network 【graph node2vec kdd 2016node2vec scalable feature learning 【graph sdne kdd 2016structural deep network 【graph struc2vec kdd 2017struc2vec learning node representation structural 【graph run example 1 clone repo make sure installed tensorflow tensorflowgpu local machine 2 run following command bash python setuppy install cd example python deepwalkwikipy deepwalk python g nxreadedgelistdatawikiwikiedgelisttxtcreateusingnxdigraphnodetypenonedataweightint read graph model deepwalkgwalklength10numwalks80workers1init model modeltrainwindowsize5iter3 train model embeddings modelgetembeddings get embedding vector line python g nxreadedgelistdatawikiwikiedgelisttxtcreateusingnxdigraphnodetypenonedataweightintread graph model linegembeddingsize128ordersecond init modelorder firstsecondall modeltrainbatchsize1024epochs50verbose2 train model embeddings modelgetembeddings get embedding vector node2vec python gnxreadedgelistdatawikiwikiedgelisttxt createusing nxdigraph nodetype none data weight intread graph model node2vecg walklength 10 numwalks 80p 025 q 4 worker 1init model modeltrainwindowsize 5 iter 3 train model embeddings modelgetembeddings get embedding vector sdne python g nxreadedgelistdatawikiwikiedgelisttxtcreateusingnxdigraphnodetypenonedataweightintread graph model sdneghiddensize256128 init model modeltrainbatchsize3000epochs40verbose2 train model embeddings modelgetembeddings get embedding vector struc2vec python g nxreadedgelistdataflightbrazilairportsedgelistcreateusingnxdigraphnodetypenonedataweightintread graph model model struc2vecg 10 80 workers4 verbose40 init model modeltrainwindowsize 5 iter 3 train model embeddings modelgetembeddings get embedding vector seedne tensorflow implementation selfpaced network embedding introduction tensorflow reimplementation selfpaced network embeddinguse random walk get positive pair requirement python 36 tensorflow 1120 usage run code use python seednenewpy result dataset coraf1micro078f1macro077 see seednenewthumay161022562019logtxt resultpng harp code aaai 2018 paper harp hierarchical representation learning network harp metastrategy improve several stateoftheart network embedding algorithm deepwalk line node2vec read preprint paper code run python 2 installation following python package required install harp library processing graph data install run following command git clone cd magicgraph python setuppy install install harp requirement git clone cd harp pip install r requirementstxt usage run harp citeseer dataset using line underlying network embedding model run following command python srcharppy input examplegraphsciteseerciteseermat model line output citeseernpy sfdppath binsfdplinux parameter available input inputfilename 1 format mat matlab mat file containing adjacency matrix default variable name adjacency matrix network also specify matfilevariablename 2 format adjlist adjacency list eg 1 2 3 4 5 6 7 8 9 11 12 13 14 18 20 22 32 2 1 3 4 8 14 18 20 22 31 3 1 2 4 8 9 10 14 28 29 33 3 format edgelist edge list eg 1 2 1 3 1 4 2 5 output outputfilename output representation numpy npy format note assume node input file indexed 0 n 1 model modelname underlying network embeddings model use could deepwalk line node2vec note node2vec us default parameter p10 q10 sfdppath sfdppath path binary file sfdp module used graph coarsening set sfdplinux sfdposx sfdpwindowsexe depending operating system option full list command line option available python srcharppy help evaluation evaluate embeddings multilabel classification task run following command python srcscoringpy e citeseernpy examplegraphsciteseerciteseermat 1 2 3 4 5 6 7 8 9 e specifies embeddings file specifies mat file containing node label specifies list training example ratio use note sfdp library multilevel graph drawing part use sfdp graph coarsening implementation note sfdp included binary file bin please choose proper binary file according operation system currently binary file osx linux window citation find harp useful research please cite paper inproceedingsharp titleharp hierarchical representation learning network authorchen haochen perozzi bryan hu yifan skiena steven booktitleproceedings thirtysecond aaai conference artificial intelligence year2018 organizationaaai press graphgan repository implementation graphgan graph representation learning generative adversarial net hongwei wang jia wang jialin wang miao zhao weinan zhang fuzheng zhang xing xie minyi guo 32nd aaai conference artificial intelligence 2018 graphgan unifies two school graph representation learning methodology generative method discriminative method via adversarial training minimax game generator guided signal discriminator improves generating performance discriminator pushed generator better distinguish ground truth generated sample file folder data training test data pretrain pretrained node embeddings note dimension pretrained node embeddings equal nemb srcgraphganconfigpy result evaluation result learned embeddings generator discriminator src source code requirement code tested running python 365 following package installed along dependency tensorflow 180 tqdm 4234 displaying progress bar numpy 1143 sklearn 0191 input format input data undirected graph node id start 0 n1 n number node graph line contains two node id indicating edge graph txt file sample 0 1 3 2 basic usage mkdir cache cd srcgraphgan python graphganpy
Graphs;graphattentionnetworks framework implementation graph attention network robot img1imgsgatjpg used extracting graph attention embeddings provides framework tensorflow graph attention layer used knowledge graph node base semantic task determines pair wise embedding matrix higher order node representation concatenates attention weight pass leakyrelu activation importance sampling damp negative effect nodeit applies softmax layer normalization attention result determines final output scoresthe graphattentionbasepy script implement tensorflowkeras layer gat used graphmultiheadattentionpy used extract gat embeddings tensorflow 2 implementation graph attention network generating node embeddings knowledge graph well implementing kera layer multihead graph attention paper graph attention network veličković et al iclr dependency usability installation carried using pip command follows python pip install graphattentionnetworks01 library built tensorflow img step generating graph attention embeddings requires import script example shown create function read input csv file input contain atleast 2 column source targetlabels text format include textual extract corresponding label graph created multidigraph networkx target source column input csv file generating embeddings extract label also considered used determine label closest provided sourceinput text example testgatembeddings method show dataset chosen demonstration google quest dataset source label columntextual content used generate embeddings method requires getgatembeddings methodthis method take parameter hiddenunits denotes hidden embedding dimension neural network numheadsnumber attention head epoch number training iterationsnumlayersnumber layer networkmodedefaults averaging mode attention concatenation see graphattentionbasepy dataframe along source target label model output embedding matrix entry hidden dims corresponding graphthe dimension internally reduced suit output gat embeddings python def testgatembeddings printtesting vanillagcn embeddings source target label traindfpdreadcsvetraingraphtraincsv sourcelabelquestionbody targetlabelcategory printinput parameter hidden unit number layerssubset value entry considered embeddingsepochs hiddenunits32 numlayers4 subset34 epochs40 numheads8 modeconcat gatembgatgraphgatgetgatembeddingshiddenunitstraindfsourcelabeltargetlabelepochsnumlayersnumheadsmodesubset printgatembshape return gatembgatgraph theory img neural gcn multiplication order obtain sufficient expressive power transform input feature higher level feature atleast one learnable linear transformation required end initial step shared linear transformation parametrized weight matrix w∈rf×f applied every node selfattention pointwise compute pairwise unnormalized attention score two neighbor first concatenates z embeddings two node denotes concatenation take dot product learnable weight vector applies leakyrelu end form attention usually called additive attention contrast dotproduct attention used transformer model perform selfattention node shared attentional mechanism rf×rf→r compute attention coefficient softmax aggregation case applying softmax kernel attention score normalized multiplying feature map aggregation map concatenation avergaingthis case multihead attention perform multihead attention final prediction layer network concatenation longer sensible instead averaging employed delay applying final nonlinearity usually softmax logistic sigmoid classification problem implement core gat multihead algorithm concatenation aggregation variation returned output dimension batch size number node label gcn embeddings please refer test log testing gat embeddings source target label input parameter hidden unit number layerssubset value entry considered embeddingsepochs adj 39 39 shape target 39 5 model model50 layer type output shape param connected second inputlayer none 39 0 embedding69 embedding none 39 39 1521 second00 first inputlayer none 39 0 multiheadattention31 multihe none 5 4720 embedding6900 first00 total params 6241 trainable params 4720 nontrainable params 1521 none fitting model hiddenunits unit epoch 1200 warningtensorflowgradients exist variable multiheadattention31graphattention269kernelw0 multiheadattention31graphattention269bias0 multiheadattention31graphattention270kernelw0 multiheadattention31graphattention270bias0 multiheadattention31graphattention271kernelw0 multiheadattention31graphattention271bias0 multiheadattention31graphattention272kernelw0 multiheadattention31graphattention272bias0 multiheadattention31graphattention273kernelw0 multiheadattention31graphattention273bias0 multiheadattention31graphattention274kernelw0 multiheadattention31graphattention274bias0 multiheadattention31graphattention275kernelw0 multiheadattention31graphattention275bias0 multiheadattention31graphattention276kernelw0 multiheadattention31graphattention276bias0 minimizing loss warningtensorflowgradients exist variable multiheadattention31graphattention269kernelw0 multiheadattention31graphattention269bias0 multiheadattention31graphattention270kernelw0 multiheadattention31graphattention270bias0 multiheadattention31graphattention271kernelw0 multiheadattention31graphattention271bias0 multiheadattention31graphattention272kernelw0 multiheadattention31graphattention272bias0 multiheadattention31graphattention273kernelw0 multiheadattention31graphattention273bias0 multiheadattention31graphattention274kernelw0 multiheadattention31graphattention274bias0 multiheadattention31graphattention275kernelw0 multiheadattention31graphattention275bias0 multiheadattention31graphattention276kernelw0 multiheadattention31graphattention276bias0 minimizing loss warningtensorflowgradients exist variable multiheadattention31graphattention269kernelw0 multiheadattention31graphattention269bias0 multiheadattention31graphattention270kernelw0 multiheadattention31graphattention270bias0 multiheadattention31graphattention271kernelw0 multiheadattention31graphattention271bias0 multiheadattention31graphattention272kernelw0 multiheadattention31graphattention272bias0 multiheadattention31graphattention273kernelw0 multiheadattention31graphattention273bias0 multiheadattention31graphattention274kernelw0 multiheadattention31graphattention274bias0 multiheadattention31graphattention275kernelw0 multiheadattention31graphattention275bias0 multiheadattention31graphattention276kernelw0 multiheadattention31graphattention276bias0 minimizing loss warningtensorflowgradients exist variable multiheadattention31graphattention269kernelw0 multiheadattention31graphattention269bias0 multiheadattention31graphattention270kernelw0 multiheadattention31graphattention270bias0 multiheadattention31graphattention271kernelw0 multiheadattention31graphattention271bias0 multiheadattention31graphattention272kernelw0 multiheadattention31graphattention272bias0 multiheadattention31graphattention273kernelw0 multiheadattention31graphattention273bias0 multiheadattention31graphattention274kernelw0 multiheadattention31graphattention274bias0 multiheadattention31graphattention275kernelw0 multiheadattention31graphattention275bias0 multiheadattention31graphattention276kernelw0 multiheadattention31graphattention276bias0 minimizing loss 22 0 2msstep loss 16039 acc 02308 epoch 2200 22 0 2msstep loss 16078 acc 02821 epoch 3200 22 0 2msstep loss 16069 acc 02308 epoch 4200 22 0 2msstep loss 16084 acc 02821 epoch 5200 22 0 2msstep loss 16045 acc 02051 epoch 6200 22 0 0sstep loss 16063 acc 02051 epoch 7200 22 0 0sstep loss 16048 acc 02564 epoch 8200 22 0 2msstep loss 16092 acc 02564 epoch 9200 22 0 2msstep loss 16084 acc 02821 epoch 10200 22 0 0sstep loss 16071 acc 02821 epoch 11200 22 0 0sstep loss 16067 acc 02821 epoch 12200 22 0 0sstep loss 16045 acc 02564 epoch 13200 22 0 2msstep loss 16067 acc 02821 epoch 14200 22 0 2msstep loss 16073 acc 03333 epoch 15200 22 0 2msstep loss 16076 acc 02821 epoch 16200 22 0 2msstep loss 16050 acc 02564 epoch 17200 22 0 2msstep loss 16056 acc 03077 epoch 18200 22 0 2msstep loss 16060 acc 02821 epoch 19200 22 0 0sstep loss 16058 acc 02308 epoch 20200 22 0 2msstep loss 16052 acc 02821 epoch 21200 22 0 2msstep loss 16039 acc 03077 epoch 22200 22 0 2msstep loss 16086 acc 02821 epoch 23200 22 0 2msstep loss 16049 acc 02821 epoch 24200 22 0 2msstep loss 16068 acc 02821 epoch 25200 22 0 2msstep loss 16050 acc 02821 epoch 26200 22 0 2msstep loss 16051 acc 03077 epoch 27200 22 0 2msstep loss 16069 acc 03077 epoch 28200 22 0 2msstep loss 16060 acc 03077 epoch 29200 22 0 0sstep loss 16046 acc 03077 epoch 30200 22 0 0sstep loss 16050 acc 03077 epoch 31200 22 0 2msstep loss 16034 acc 02821 epoch 32200 22 0 0sstep loss 16075 acc 03077 epoch 33200 22 0 2msstep loss 16055 acc 03077 epoch 34200 22 0 2msstep loss 16047 acc 03077 epoch 35200 22 0 2msstep loss 16007 acc 03333 epoch 36200 22 0 2msstep loss 16066 acc 02821 epoch 37200 22 0 2msstep loss 16048 acc 03077 epoch 38200 22 0 0sstep loss 16048 acc 02821 epoch 39200 22 0 0sstep loss 16044 acc 02821 epoch 40200 22 0 2msstep loss 16061 acc 03333 epoch 41200 22 0 2msstep loss 16036 acc 03333 epoch 42200 22 0 0sstep loss 16054 acc 03333 epoch 43200 22 0 2msstep loss 16074 acc 02821 epoch 44200 22 0 2msstep loss 16042 acc 03333 epoch 45200 22 0 0sstep loss 16033 acc 03077 epoch 46200 22 0 0sstep loss 16082 acc 02821 epoch 47200 22 0 2msstep loss 16079 acc 03077 epoch 48200 22 0 2msstep loss 16045 acc 02821 epoch 49200 22 0 0sstep loss 16041 acc 03077 epoch 50200 22 0 2msstep loss 16059 acc 03077 epoch 51200 22 0 2msstep loss 16038 acc 03333 epoch 52200 22 0 2msstep loss 16004 acc 03333 epoch 53200 22 0 2msstep loss 16056 acc 03077 epoch 54200 22 0 0sstep loss 16040 acc 02821 epoch 55200 22 0 0sstep loss 16050 acc 03333 epoch 56200 22 0 2msstep loss 16020 acc 03846 epoch 57200 22 0 2msstep loss 16013 acc 03333 epoch 58200 22 0 0sstep loss 16039 acc 03590 epoch 59200 22 0 0sstep loss 16019 acc 03590 epoch 60200 22 0 2msstep loss 15997 acc 03333 epoch 61200 22 0 2msstep loss 16026 acc 03846 epoch 62200 22 0 0sstep loss 16022 acc 03590 epoch 63200 22 0 0sstep loss 16020 acc 03590 epoch 64200 22 0 2msstep loss 16011 acc 03333 epoch 65200 22 0 0sstep loss 15982 acc 03846 epoch 66200 22 0 2msstep loss 16003 acc 03590 epoch 67200 22 0 2msstep loss 16015 acc 04103 epoch 68200 22 0 0sstep loss 16045 acc 03846 epoch 69200 22 0 2msstep loss 15975 acc 04103 epoch 70200 22 0 0sstep loss 15987 acc 03846 epoch 71200 22 0 2msstep loss 15999 acc 04103 epoch 72200 22 0 2msstep loss 16009 acc 04103 epoch 73200 22 0 2msstep loss 15986 acc 04103 epoch 74200 22 0 2msstep loss 16013 acc 04359 epoch 75200 22 0 0sstep loss 16026 acc 04103 epoch 76200 22 0 2msstep loss 15995 acc 04615 epoch 77200 22 0 2msstep loss 16012 acc 04359 epoch 78200 22 0 0sstep loss 16024 acc 04103 epoch 79200 22 0 2msstep loss 16027 acc 03846 epoch 80200 22 0 2msstep loss 16023 acc 04615 epoch 81200 22 0 0sstep loss 16014 acc 04615 epoch 82200 22 0 0sstep loss 15994 acc 04872 epoch 83200 22 0 0sstep loss 16013 acc 04872 epoch 84200 22 0 2msstep loss 16020 acc 04359 epoch 85200 22 0 2msstep loss 15979 acc 04103 epoch 86200 22 0 2msstep loss 16035 acc 04872 epoch 87200 22 0 0sstep loss 16046 acc 05128 epoch 88200 22 0 0sstep loss 16021 acc 04103 epoch 89200 22 0 2msstep loss 16028 acc 04615 epoch 90200 22 0 2msstep loss 16002 acc 04872 epoch 91200 22 0 2msstep loss 16001 acc 04615 epoch 92200 22 0 2msstep loss 16028 acc 04615 epoch 93200 22 0 2msstep loss 16005 acc 05385 epoch 94200 22 0 2msstep loss 15985 acc 05128 epoch 95200 22 0 2msstep loss 16020 acc 04872 epoch 96200 22 0 2msstep loss 15957 acc 04872 epoch 97200 22 0 2msstep loss 16019 acc 04872 epoch 98200 22 0 0sstep loss 16014 acc 05128 epoch 99200 22 0 0sstep loss 15995 acc 05128 epoch 100200 22 0 0sstep loss 16011 acc 05128 epoch 101200 22 0 2msstep loss 16000 acc 04872 epoch 102200 22 0 0sstep loss 16007 acc 05385 epoch 103200 22 0 2msstep loss 15975 acc 05385 epoch 104200 22 0 0sstep loss 15974 acc 05385 epoch 105200 22 0 0sstep loss 16020 acc 05128 epoch 106200 22 0 0sstep loss 15975 acc 05385 epoch 107200 22 0 0sstep loss 15993 acc 05128 epoch 108200 22 0 0sstep loss 15992 acc 05641 epoch 109200 22 0 2msstep loss 15991 acc 05385 epoch 110200 22 0 2msstep loss 15996 acc 05128 epoch 111200 22 0 2msstep loss 16018 acc 05641 epoch 112200 22 0 2msstep loss 16007 acc 05385 epoch 113200 22 0 2msstep loss 16006 acc 04872 epoch 114200 22 0 0sstep loss 15966 acc 05385 epoch 115200 22 0 0sstep loss 16022 acc 05385 epoch 116200 22 0 2msstep loss 15987 acc 05128 epoch 117200 22 0 2msstep loss 15985 acc 05128 epoch 118200 22 0 2msstep loss 15979 acc 05641 epoch 119200 22 0 2msstep loss 15981 acc 05128 epoch 120200 22 0 2msstep loss 15987 acc 05385 epoch 121200 22 0 0sstep loss 16007 acc 05385 epoch 122200 22 0 0sstep loss 15975 acc 05128 epoch 123200 22 0 0sstep loss 15985 acc 05128 epoch 124200 22 0 0sstep loss 15992 acc 05128 epoch 125200 22 0 2msstep loss 15964 acc 05385 epoch 126200 22 0 2msstep loss 16006 acc 05385 epoch 127200 22 0 0sstep loss 16001 acc 05641 epoch 128200 22 0 0sstep loss 15963 acc 05641 epoch 129200 22 0 0sstep loss 15989 acc 05385 epoch 130200 22 0 2msstep loss 15962 acc 05641 epoch 131200 22 0 2msstep loss 15981 acc 05385 epoch 132200 22 0 2msstep loss 15953 acc 05385 epoch 133200 22 0 2msstep loss 15950 acc 05385 epoch 134200 22 0 2msstep loss 15981 acc 05641 epoch 135200 22 0 2msstep loss 15975 acc 05897 epoch 136200 22 0 2msstep loss 15927 acc 05641 epoch 137200 22 0 2msstep loss 15965 acc 05385 epoch 138200 22 0 0sstep loss 15960 acc 05385 epoch 139200 22 0 2msstep loss 15975 acc 05641 epoch 140200 22 0 2msstep loss 15975 acc 05897 epoch 141200 22 0 0sstep loss 15974 acc 05641 epoch 142200 22 0 2msstep loss 15989 acc 05897 epoch 143200 22 0 2msstep loss 15968 acc 05897 epoch 144200 22 0 0sstep loss 15980 acc 05641 epoch 145200 22 0 2msstep loss 15980 acc 06410 epoch 146200 22 0 2msstep loss 15969 acc 05897 epoch 147200 22 0 0sstep loss 15969 acc 06154 epoch 148200 22 0 2msstep loss 15932 acc 05897 epoch 149200 22 0 2msstep loss 15948 acc 05641 epoch 150200 22 0 4msstep loss 16012 acc 05641 epoch 151200 22 0 0sstep loss 15964 acc 05897 epoch 152200 22 0 0sstep loss 15949 acc 05897 epoch 153200 22 0 0sstep loss 15972 acc 06154 epoch 154200 22 0 2msstep loss 15945 acc 06410 epoch 155200 22 0 2msstep loss 15951 acc 05641 epoch 156200 22 0 2msstep loss 15981 acc 06410 epoch 157200 22 0 0sstep loss 15950 acc 06154 epoch 158200 22 0 2msstep loss 15949 acc 06154 epoch 159200 22 0 2msstep loss 15954 acc 05641 epoch 160200 22 0 2msstep loss 15978 acc 05385 epoch 161200 22 0 0sstep loss 15940 acc 05897 epoch 162200 22 0 0sstep loss 15975 acc 06154 epoch 163200 22 0 0sstep loss 15949 acc 06154 epoch 164200 22 0 2msstep loss 15939 acc 05897 epoch 165200 22 0 2msstep loss 15966 acc 06667 epoch 166200 22 0 2msstep loss 15935 acc 06410 epoch 167200 22 0 2msstep loss 15921 acc 06154 epoch 168200 22 0 2msstep loss 15950 acc 06667 epoch 169200 22 0 0sstep loss 15964 acc 05897 epoch 170200 22 0 0sstep loss 15950 acc 06667 epoch 171200 22 0 0sstep loss 15891 acc 06667 epoch 172200 22 0 0sstep loss 15924 acc 06410 epoch 173200 22 0 0sstep loss 15906 acc 06923 epoch 174200 22 0 0sstep loss 15937 acc 06667 epoch 175200 22 0 0sstep loss 15901 acc 06410 epoch 176200 22 0 0sstep loss 15975 acc 06667 epoch 177200 22 0 0sstep loss 15887 acc 06410 epoch 178200 22 0 2msstep loss 15905 acc 06667 epoch 179200 22 0 2msstep loss 15928 acc 06410 epoch 180200 22 0 2msstep loss 15903 acc 06410 epoch 181200 22 0 0sstep loss 15949 acc 06410 epoch 182200 22 0 0sstep loss 15896 acc 06923 epoch 183200 22 0 0sstep loss 15926 acc 06667 epoch 184200 22 0 2msstep loss 15933 acc 07179 epoch 185200 22 0 0sstep loss 15917 acc 06667 epoch 186200 22 0 0sstep loss 15891 acc 06667 epoch 187200 22 0 0sstep loss 15903 acc 06667 epoch 188200 22 0 2msstep loss 15922 acc 06667 epoch 189200 22 0 2msstep loss 15956 acc 06667 epoch 190200 22 0 2msstep loss 15955 acc 06667 epoch 191200 22 0 0sstep loss 15943 acc 06667 epoch 192200 22 0 0sstep loss 15904 acc 06410 epoch 193200 22 0 0sstep loss 15880 acc 06667 epoch 194200 22 0 2msstep loss 15994 acc 06410 epoch 195200 22 0 0sstep loss 15935 acc 06667 epoch 196200 22 0 2msstep loss 15899 acc 06923 epoch 197200 22 0 2msstep loss 15879 acc 06923 epoch 198200 22 0 2msstep loss 15942 acc 06410 epoch 199200 22 0 0sstep loss 15971 acc 06923 epoch 200200 22 0 0sstep loss 15896 acc 06923 dimension embeddings 39 5 02012526 020259565 0198657 019916654 01983282 019945449 019912691 019899039 020362976 019879843 019992991 019903784 020286745 020007017 019809467 020213112 0199936 019900537 020027836 01986492 019776767 020560838 020104136 019821526 01973673 020053644 01997569 019718741 020560017 019691905 020018028 019936264 01996499 019978425 020102301 019945793 019914173 020245528 020021063 019873443 019884491 020046438 019927275 020058933 020082861 019917706 020128284 019944142 019948685 020061186 019849762 019817543 019879887 020052965 020399839 019931518 02013433 0199615 019973907 019998741 019847561 020165245 019825539 01994863 020213024 019950281 020215113 019978707 020000161 019855739 019828378 01995312 019828445 019930516 02045954 020060964 020382763 019792375 020018485 019745411 019908005 019907176 020212533 019942604 02002968 02061225 01978762 01984831 01989557 019856244 019875005 019904377 019953431 019955306 020311877 019788706 019793089 019901524 019893484 020623192 020016678 019998111 019964387 020063668 019957155 019831264 019981474 020413318 019969617 019804329 020088041 019825016 019835961 02013865 020112331 020049077 020007299 019954118 01995979 020029715 020174277 019962949 020002913 019979118 01988074 020026413 019844735 020144895 020046745 019937217 019872567 019828579 020021453 019848432 020428972 019901909 02013395 020159248 019940722 019864176 020733383 019861382 019859011 019829814 019716412 019899431 020001376 019994445 019993514 020111232 01980907 019873632 020662917 019852386 019801992 020076685 019984347 019948737 020049627 01994061 019691971 019762805 020314892 020342152 019888185 02024308 019443996 020389585 01996117 019962174 019936486 019557709 020349811 020444478 019711518 019996345 022908325 01896215 018164895 019968286 019765937 019925259 020080289 019597957 02063056 02029202 019314696 020042692 019949025 020401572 01998764 019227904 019804887 021609515 01937005 39 5 contributing pull request welcome major change please open issue first discus would like change license mit
Graphs;graph transformer architecture source code paper generalization transformer network vijay prakash xavier aaai21 workshop deep learning graph method application dlgaaai21 propose generalization transformer neural network architecture arbitrary graph graph transformer brcompared standard highlight presented architecture attention mechanism function neighborhood connectivity node graph position encoding represented laplacian eigenvectors naturally generalize sinusoidal positional encoding often used nlp layer normalization replaced batch normalization layer architecture extended edge representation critical task rich information edge pairwise interaction bond type molecule relationship type kg etc br p aligncenter img srcdocsgraphtransformerpng altgraph transformer architecture width800 br bfigureb block diagram graph transformer architecture p 1 repo installation project based repository follow instructionsdocs01benchmarkinstallationmd install benchmark setup environment br 2 download datasets proceed followsdocs02downloaddatasetsmd download datasets used evaluate graph transformer br 3 reproducibility use pagedocs03runcodesmd run code reproduce published result br 4 reference pagewithcurl paper pencil blog towards data moviecamera video articledwivedi2021generalization titlea generalization transformer network graph authordwivedi vijay prakash bresson xavier journalaaai workshop deep learning graph method application year2021 brbrbr
Graphs;molecular scalar coupling constant prediction using schnet final project csgy9223 deep learning course nyu tandon implement schnet based paper schütt et prediction molecular scalar coupling constant 1 introduction drug discovery process one challenging expensive endeavor biomedicine img atom solar system img chemical compound druglike feature made since unfeasible chemist synthesize evaluate every molecule they’ve grown rely virtual screening narrow promising candidate however challenge searching almost infinite space potential molecule perfect substrate deep learning technique improve drug discovery process even growing number large datasets molecule already enabled creation several useful model application deep learning drug discovery still infancy useful prediction could expedite drug discovery include toxicity ability bind given protein quantum property researcher commonly use nuclear magnetic resonance nmr gain insight molecule’s structure dynamic nmr’s functionality largely depends ability accurately predict scalar coupling strength magnetic interaction pair atom given molecule possible compute scalar coupling inputted 3d molecular structure using advanced quantum mechanical simulation method density functional theory dft approximate schrödinger’s equation however method limited high computational cost therefore reserved use small system le approximate method adopted instead goal project develop fast reliable cheaper method perform task use graph convolutional neural network gcn particular focused implementing optimizing schnet a novel gcn shown achieve stateoftheart performance quantum chemical property benchmark byproduct hoped learn gcns could used chemical application 2 literature survey gcn motivation basic many deep learning model designed aid drug discovery show improvement traditional machine learning method limited due two main reason first rely handcrafted feature prevents structural information learned directly raw input second existing architecture conducive use structured data molecule extraction relevant feature image already proven highly successful using convolutional neural network cnns molecule represented fully connected graph atom bond represented node edge respectively graph irregularly shaped thereby making cnns rely convolution regular gridlike structure unsuitable feature extraction 1 effort made generalize convolution operation graph resulting development graph convolutional neural network gcns kipf welling describe seminal paper 2 idea behind graph convolutional neural network gcns shown fig 1 perform convolution graph aggregating sum average etc node’s neighborhood feature vector new aggregated vector passed neural network layer output new vector representation node additional neural network layer repeat process except input updated vector first layer p aligncenter img srcimagesfigure 1png altdrawing width600 p b quantum mechanical property prediction 2017 gilmer et al 3 released paper focusing specific use neural network predicting quantum property molecule noted symmetry atomic system require graph neural network invariant graph isomorphism therefore reformulated existing model fall category including kipf welling’s gcn common framework called message passing neural network mpnns “message passing” refers aggregation neighborhood vector feature described earlier mpnn gilmer et al developed called enns2s managed achieve stateoftheart performance important molecular property benchmark using qm9 dataset consisting 130k molecule 13 property molecule approximated dft neighborhood message generated used bond type interatomic distance followed set2set model vinyals et al 4 later schutt et al pointed enns2s limited fact atomic position discretized therefore filter learned also discrete rendered incapable capturing gradual positional change atom 5 order remedy schutt et al proposed different method graph convolution continuous filter mapped atomic position corresponding filter value advantageous doesnt require atomic position data lie grid thereby resulting smooth rather discrete energy prediction p aligncenter img srcimagesfigure 2png altdrawing width500 p schnet demonstrated superior performance enns2s predicting molecular energy atomic force three different datasets fig 3 provides overview schnet architecture molecule input model uniquely represented certain set nuclear charge img z1 zn atomic position img rm r1 rm rn n number atom layer atom given molecule represented tuple feature img rm xl1 rm xnl img xil mathbb rf img img number layer feature map respectively representation analogous pixel image embedding layer representation atom initialized random using embedding dependent atom type img optimized training img rm azi img atom type embedding atomwise layer recurring building block architecture dense layer applied representation img xil atom layer responsible recombination feature map shared weight across atom allows architecture scaled respect size molecule interaction atom modeled three interaction block shown sequence atomwise interatomic continuousfilter convolution cfconv two atomwise layer separated softplus nonlinearity produce img vil cfconv layer us radial basis function act continuous filter generator additionally residual connection img xl1 rm xnl img vl1 rm vnl allows incorporation interaction atom previously computed feature map p aligncenter img srcimagesfigure 3png altdrawing width600 p 3 chainer chemistry implementation dataset used champ scalar coupling dataset provided kaggle competition similar objective 6 consists following ul typedisc li traincsv — training set contains four column 1 name molecule coupling constant originates 2 3 atom index atompair create coupling 4 scalar coupling type 5 scalar coupling constant want predictli li scalarcouplingcontributionscsv — scalar coupling constant traincsv sum four term fermi contact spindipolar paramagnetic spinorbit diamagnetic spinorbit contribution contained file organized following column 1 molecule name 2 3 atom index atompair 4 type coupling 5 6 7 8 four aforementioned termsli li structurescsv — contains x z cartesian coordinate atom molecule organized following column 1 molecule name 2 atom index 3 atom name 4 5 6 x z cartesian coordinate respectivelyli ul b preprocessing aimed ultimately train schnet model predicts scalar coupling contribution instead scalar coupling constant general sort multitask learning help combat overfitting therefore merged traincsv scalarcouplingcontributionscsv one panda dataframe performed 801010 6800985018502 molecule split train validation test set structurescsv contains cartesian coordinate additional preprocessing required order create graph representation molecule order created graph class whose object store distance atom adjacency matrix fully processed train validation test datasets input schnet dictionary consisting graph object molecule associated scalar coupling contribution atom pair within molecule example molecule represented graphically fig 4 display first molecule dataset ch4 methane p aligncenter img srcimagesfigure 4png altdrawing width325 p c schnet model constructed trained evaluated optimized schnet model using chainer chemistry library us architecture described schutt et al addition however added batch normalization using chainer chemistry’s graphbatchnormalization loss function used log mean absolute error log mae img loss frac1t sumt1t rm log big frac1nt sumi1nt yi hat yi big img number scalar coupling type img number observation type img img actual scalar coupling constant observation img yi predicted scalar coupling constant observation calculated scalar coupling type averaged across tip 1 decrease error one type provides improvement score decrease another type tested variety hyperparameter configuration order optimize model used adam optimization batch size 4 course 25 epoch particularly focused discovering optimal radial basis function hyperparameters within cfconv layer radial basis function expressed img ri bf rj rm expgamma dij muk 2 located center 0å ≤ µk ≤ 30å every 01å γ 10å 300 interatomic distance used input filter network translates hyperparameter value numrbf300 radiusresolution01 gamma100 chainer chemistry implementation cfconv general value chosen distance occurring datasets covered cfconv filter choosing fewer center corresponds reducing resolution filter restricting range center corresponds filter size usual convolutional layer using default cfconv hyperparameter value ran schnet using adam learning rate img 103 img 103 img 102 25epoch training took approximately 455 hour shown table model performed best achieved lowest log mae learning rate img 103 learning rate log mae 1x103 122 5x103 132 1x102 117 using optimal learning rate ran model using different value radius resolution found lowest validation log mae achieved radius resolution 010 radius resolution log mae 005 121 0075 125 010 132 0125 118 015 127 sample 1400457 predicted scalar coupling constant predicted test dataset using optimal learning rate radius resolution value atom id predicted scalar coupling constant actual scalar coupling constant 236 7978 7909 237 189 145 238 094 093 239 1234 1209 240 1136 1128 241 169 143 242 7986 7964 243 280 307 243 845 826 4 conclusion thus using data provided champ scalar coupling dataset able successfully train schnet model order predict scalar coupling constant pair atom molecule provided order optimize model manually tested variety learning rate radius resolution value achieved optimal performance using learning rate 5x103 radius resolution 010 455 hour training time produced result low log mae drastic improvement day week would take produce similar result using quantum mechanical simulation method density functional theory dft future would like perform even thorough hyperparameter grid search run schnet using combination different value numrbf radiusresolution gamma 5 reference ol typedisc lisun zhao gilvary c elemento zhou j wang f 2019 graph convolutional network computational drug development discovery briefing bioinformatics 213 919935 doi101093bibbbz042li likipf thomas n welling max semisupervised classification graph convolutional network international conference learning representation iclr arxiv60902907v4 2017li lijustin gilmer samuel schoenholz patrick f riley oriol vinyals george e dahl neural message passing quantum chemistry arxiv preprint arxiv170401212 2017li livinyals oriol bengio samy kudlur manjunath order matter sequence sequence set arxiv preprint arxiv151106391 2015li likristof schütt pieterjan kindermans huziel enoc sauceda felix stefan chmiela alexandre tkatchenko klausrober müller schnet continuousfilter convolutional neural network modeling quantum interaction guyon u v luxburg bengio h wallach r fergus vishwanathan r garnett editor advance neural information processing system nip 30 page 992–1002 curran associate inc arxiv 170608566v5 2017li lipredicting molecular property ol
Graphs;convolutional complex knowledge graph embeddings opensource project contains pytorch implementation approach conex training evaluation script link prediction result provide brief overview link prediction result result sorted descending order size respective dataset yago310 mrr hits10 hits3 hits1 340 540 380 240 360 550 400 260 400 620 490 350 530 680 580 460 500 670 550 400 conex 553 696 601 477 fb15k237 freebase denotes newly reported link prediction result mrr hits10 hits3 hits1 distmult 241 419 263 155 complex 247 428 275 158 conve 335 501 356 237 rescal 357 541 393 263 distmult 343 531 378 250 complex 348 536 384 253 conve 339 521 369 248 hyper 341 520 376 252 nkge 330 510 365 241 rotate 338 533 375 241 tucker 358 544 394 266 quate 366 556 401 271 conex 366 555 403 271 ensembleconex 376 570 415 279 wn18rr wordnet denotes newly reported link prediction result mrr hits10 hits3 hits1 distmult 430 490 440 390 complex 440 510 460 410 conve 430 520 440 400 rescal 467 517 480 439 distmult 452 530 466 413 complex 475 547 490 438 conve 442 504 451 411 hyper 465 522 477 436 nkge 450 526 465 421 rotate 476 571 492 428 tucker 470 526 482 443 quate 482 572 499 436 conex 481 550 493 448 ensembleconex 485 559 495 450 wn18rr dataset spot flaw wn18rr fb15k237 yago310 specifically validation test split dataset contain entity occur training split refer outofvocabulary entity link detail mrr hits10 hits3 hits1 distmultcomplex 475 579 497 426 distmulttucker 476 569 492 433 conexdistmult 484 580 501 439 conexcomplex 501 589 518 456 conextucker 514 583 526 479 ensembleconex 517 594 526 479 visualisation embeddings 2d pca projection relation embeddings fb15k237 dataset figure show inverse relation cluster distant region note applied standard data augmentation technique generate inverse relation relation renamed adding suffix inverse done incitebalavzevic2019tucker alt textutilrelembfb15k2371png installation first clone repository git clone obtain required library conda env create f environmentyml source activate conex code compatible python 364 usage runscriptpy used train conex desired dataset gridsearchpy used rerun experiment pretrained model please contact caglardemirupbde wish obtain conex embeddings specific dataset forte hepatitis lymphography mammographic animal yago310 fb15k237 fb15k wn18rr wn18 reproduce link prediction result please follow next step reproduce reported result unzip datasets unzip kgszip create folder pretrained model mkdir pretrainedmodels download pretrained model via pretrainedmodels python reproducelppy reproduces link prediction result fb15k237 fb15k wn18 wn18rr yago310 benchmark datasets python reproducebaselinespy reproduces link prediction result distmult complex tucker fb15k237 wn18rr yago310 benchmark datasets settingsjson file store hyperparameter setting model python reproduceensemblepy report link prediction result ensembled model python reproducelpnewpy report link prediction result wn18rr fb15k237 yago310 python reproduceablationpypy report link prediction result ablation study link prediction result provide brief overview link prediction result yago310 mrr hits10 hits3 hits1 340 540 380 240 360 550 400 260 440 620 490 350 530 678 580 455 495 670 550 400 distmult 543 683 590 466 complex 547 690 594 468 tucker 427 609 476 331 conex 553696601474 fb15k237 mrr hits10 hits3 hits1 241 419 263 155 247 428 275 158 335 501 356 237 343 531 378 250 348 536 384 253 339 521 369 248 338 533 375 241 341 520 376 252 distmult 353 539 390 260 complex 332 509 366 244 tucker 363 553 400 268 conex 366 555 403 271 ensemble conex 376570415279 acknowledgement based implementation open source implementation would like thank readable codebase cite inproceedingsdemir2021convolutional titleconvolutional complex knowledge graph embeddings authorcaglar demir axelcyrille ngonga ngomo booktitleeighteenth extended semantic web conference research track year2021 question suggestion please contact caglardemirupbde
Graphs;edgeaugmented graph transformer introduction official implementation edgeaugmented graph transformer egt described augments transformer architecture residual edge channel resultant architecture directly process graphstructured data acheives good result supervised graphlearning task presented dwivedi et also achieves good performance largescale 01263 mae val dataset egt beat convolutionalmessagepassing graph neural network wide range supervised task thus demonstrates convolutional aggregation essential inductive bias graph requirement python 37 tensorflow 210 h5py 280 numpy 1184 scikitlearn 0221 download datasets experiment converted datasets hdf5 format convenience using without specific library h5py library required datasets downloaded mediumscale datasets gnn benchmarking datasets dwivedi et al largescale dataset pcqm4m hu et al simply run provided bash script downloadmediumscaledatasetssh downloadlargescaledatasetssh default location datasets datasets directory run training evaluation must create json config file containing configuration model training evaluation configs configuration config file used training evaluation run training python runtrainingpy configfilejson end training prematurely python endtrainingpy configfilejson perform evaluation python doevaluationspy configfilejson config file main result presented paper contained configsmain directory whereas configuration ablation study contained configsablation directory path name file selfexplanatory training evaluation training started model folder created model directory specified dataset name folder contain copy input config file convenience resuming trainingevaluation also contain configjson contain configs including unspecified default value used training training checkpointed per epoch case interruption resume training running runtrainingpy configjson file case wish finalize training midway stop training run endtrainingpy script configjson file save model weight training run doevaluationspy script config file perform evaluation alongside printed stdout result saved prediction directory model directory config file config file contain many different configuration however required configuration scheme specifies training scheme configuration specified default value assumed commonly used configuration scheme used specify training scheme format datasetnamepositionalencoding example cifar10svd zinceig encoding used something like pcqm4mmat full list explore libtrainingschemes directory datasetpath datasets contained default location datasets directory config need specified otherwise point towards datasetnameh5 file modelname serf identifier model also specifies default path model directory weight file etc savepath training process create model directory containing log checkpoint configs model summary predictionsevaluations default creates folder modelsdatasetnamemodelname changed via config cachedir first time trainingevaluation data cached tensorflow cache format default path datacachedatasetnamepositionalencoding changed via config distributed multigpu setting set true distributed training batchsize batch size numepochs maximum number epoch initiallr initial learning rate case warmup maximum learning rate rlrfactor reduce lr plateau factor setting value 10 turn reduce lr rlrpatience reduce lr patience ie number epoch lr reduced validation loss doesnt improve minlrfactor factor minimum lr smaller initial lr default 001 modelheight number layer l modelwidth dimensionality node channel dh edgewidth dimensionality edge channel de numheads number attention head default 8 ffnmultiplier ffn multiplier channel default 20 virtualnodes number virtual node 0 default would result global average pooling used instead virtual node uptohop clipping value input distance matrix value 1 default would result adjacency matrix used input structural matrix mlplayers dimensionality final mlp layer specified list factor respect dh default 05 025 gateattention set false get ungated egt variant egtu dropout dropout rate channel default 0 edgedropout specified applies different dropout rate edge channel edgechanneltype used create ablated variant egt value residual default implies purefull egt constrained implies egtconstrained bias implies egtsimple warmupsteps specified performs linear learning rate warmup specified number gradient update step totalsteps specified performs cosine annealing warmup model trained specified number step svdbased encoding usesvd turning false would result positional encoding used selsvdfeatures rank svd encoding r randomneg augment svd encoding random negation eigenvectors encoding useeig turning false would result positional encoding used seleigfeatures number eigen vector distance prediction objective distancetarget predict distance specified hop nu distanceloss factor multiply distance prediction loss kappa creation hdf5 datasets scratch included two jupyter notebook demonstrate hdf5 datasets created medium scale datasets view createhdfbenchmarkingdatasetsipynb need pytorch ogb111 dgl042 library run notebook notebook also runnable google colaboratory large scale pcqm4m dataset view createhdfpcqm4mipynb need pytorch ogb130 rdkit2019031 run notebook python environment anaconda environment experiment conducted specified environmentyml file citation please cite following paper find code useful articlehussain2021edge titleedgeaugmented graph transformer global selfattention enough graph authorhussain md shamim zaki mohammed j subramanian dharmashankar journalarxiv preprint arxiv210803348 year2021
Graphs;pytorch fast gat implementation fast gatdiagramfastgatpng implementation old paper graph attention however instead standard impementation one introduces several technique speed process found installation pip install fastgat alternatively git clone cd fastgat make repo faster great paper besides state art performance number benchmark could applied graph regardless structure however algorithm runtime depends number edge graph dense mean run nodes2 time sparsifying technique graph rely somehow decreasing number edge however try different method reducing number node interior representation done similarly decrease memory requirement internal matrix adding parameterized matrix input transforms challenge since graph node connect node plan explore technique reduce size graph node pas gat upscale back original size seeing sparse attention shown perfom well traditional attention could graph try experiment see indeed case yet implemented note idea tested know performance real life application may may provide accurate result code example right exist two different version gat one sparse graph one dense graph idea end use dense version since sparse version run slower currently possible use dense version large graph since creates matrix size nn quickly drain system memory example use sparse version python import torch fastgat import graphattentionnetwork node torchtensor01 02 03 04 05 06 07 08 09 10 11 12 dtype torchfloat edge 0 12 1 023 2 01 3 1 depth 3 head 3 inputdim 3 innerdim 2 net graphattentionnetworkdepth head inputdim innerdim output netnodes edge printoutput point interest one may notice module assume graph directed edge already processed node zero indexed downsampling method first downsampling method came take advantage disjoint set data structure order achieve downsampling onαn time work follows graph desired number node take edge uar graph use global nnlinear run two node get output one node replace starting node combination method disjoint set data structure allows preserve edge node j connected path length k original graph g point downsampling graph g node j whatever merged still connected path length k k information intermediate connection stored single nnlinear layer keeping efficient time le n2 fact method may likely parallelizable best case olognαn worst case still onαn choosing n2 edge max step node would max one edge considered running ologn step downsampling test method downsampling method return edge merged order make upsampling easy run reverse nnlinear upsamples 1 2 node whats nice method requires assumption graph structure work need done test real life graph citation miscveličković2018graph titlegraph attention network authorpetar veličković guillem cucurull arantxa casanova adriana romero pietro liò yoshua bengio year2018 eprint171010903 archiveprefixarxiv primaryclassstatml
Graphs;dcnntensorflow implementation diffusionconvolutional neural 1 tensorflow installation git clone gitgithubcomricardozitsengdcnntensorflowgit cd dcnntensorflow usage python mainpy performance training accuracy 090 validation accuray 081 testing accuray 085 reference 1 atwood james towsley diffusionconvolutional neural network advance neural information processing system 2 original dcnn implementation theano
Graphs;crystal graph neural network repository contains original implementation cgnn architecture described paper crystal graph neural network data mining material p aligncenterimg srcfigssio2png altlogo width200p gilmer et algilmer2017 investigated various graph neural network predicting molecular property proposed neural message passing framework unifies xie et alxie2018 studied graph neural network predict bulk property crystalline material used multigraph named crystal graph schütt et alscheutt2018 proposed deep learning architecture implicit graph neural network predict material property also perform molecular dynamic simulation study use bond distance feature machine learning contrast cgnn architecture use bond distance predict bulk property equilibrium state crystalline material 0 k 0 pa formation energy unit cell volume band gap total magnetization note crystal graph represents repeating unit periodic graph crystal crystallography requirement python 37 pytorch 10 panda matplotlib necessary plotting script installation git clone cgnnhomepwdcgnn usage user guide github page provides complete explanation cgnn architecture description program option usage example contained directory cgnnexamples dataset file cgnn code need following file targetscsv consists target value graphdatanpz consists node neighbor list graph configjson defines node vector splitjson defines data splitting trainvaltest target value targetscsv must header row consisting name target name formationenergyperatom volumedeviation bandgap magnetizationperatom name column must store identifier like id number string unique example dataset target column must store numerical value excluding nan none crystal graph create graph data file graphdatanpz follows python graph dict name structure dataset node speciesindex list neighbor list neighbor list graphsname node neighbor npsavezcompressedgraphdatanpz graphdictgraphs name identifier targetscsv example toolsmpgraphpy creates graph data structure given material project structure format tool used oqmd dataset compiled node vector create configuration file configjson using onehot encoding follows python nspecies number node specie config dict confignodevectors npeyenspeciesnspeciestolist openconfigjson w f jsondumpconfig f data splitting create datasplitting file splitjson follows python split dict splittrain index list training set splitval index list validation set splittest index list testing set opensplitjson w f jsondumpsplit f index must nonnegative integer row label data frame csv file targetscsv read training training script example shell nodefeatures size node vector datasetcgnnhomeyourdataset python cgnnhomesrccgnnpy numepochs 100 batchsize 512 lr 0001 nnodefeat nodefeatures nhiddenfeat 64 ngraphfeat 128 nconv 3 nfc 2 datasetpath dataset splitfile datasetsplitjson targetname formationenergyperatom milestone 80 gamma 01 see training history using toolsplothistorypy plot root mean squared error rmses mean absolute error maes training validation set value loss mean squared error mse mae written historycsv every epoch shell python cgnnhometoolsplothistorypy end training prediction testing set written testpredictionscsv see prediction compared target value using toolsplottestpy shell python cgnnhometoolsplottestpy prediction prediction new data conducted using testingonly mode program first prepare new dataset testing set including example predicted prediction configuration must parameter training configuration except total number epoch must zero testing addition must specify model loaded using loadmodel yourmodel shell datasetcgnnhomenewdataset python cgnnhomesrccgnnpy numepochs 0 batchsize 512 lr 0001 nnodefeat nodefeatures nhiddenfeat 64 ngraphfeat 128 nconv 3 nfc 2 datasetpath dataset splitfile datasetsplitjson targetname formationenergyperatom milestone 80 gamma 01 loadmodel model open quantum material database oqmd v12 contains 563k entry available oqmd detail setup database described readme directory cgnnoqmd citation mention work please cite cgnn techreportyamamoto2019cgnn author takenori yamamoto title crystal graph neural network data mining material science address yokohama japan institution research institute mathematical computational science llc year 2019 note reference 1 namegilmer2017justin gilmera et al neural message passing quantum chemistry proceeding 34th international conference machine learning 2017 2 namexie2018tian xiea et al crystal graph convolutional neural network accurate interpretable prediction material property phys rev lett 120 145301 2018 3 namescheutt2018kristof schütta et al schnet deep learning architecture molecule material j chem phys 148 241722 2018 license apache license 20 c 2019 takenori yamamoto
Graphs;graph convolutional network pytorch pytorch implementation graph convolutional network gcns semisupervised classification 1 highlevel introduction gcns see thomas kipf graph convolutional 2016 graph convolutional networksfigurepng note subtle difference tensorflow implementation pytorch reimplementation reimplementation serf proof concept intended reproduction result reported 1 implementation make use cora dataset 2 installation python setuppy install requirement pytorch 04 05 python 27 36 usage python trainpy reference 1 kipf welling semisupervised classification graph convolutional network 2 sen et al collective classification network data ai magazine cite please cite paper use code work articlekipf2016semi titlesemisupervised classification graph convolutional network authorkipf thomas n welling max journalarxiv preprint arxiv160902907 year2016
Graphs;ecodqn generic mit implementation ecodqn reported exploratory combinatorial optimization reinforcement requirement beyond standard package found python distribution eg numpy matplotlib etc project requires pytorch networkx numba panda alternatively included environmentymlenvironmentyml file produce working environment called spinsolver git clone recursive cd ecodqn conda env create f environmentyml optional source active spinsolver optional running experiment script reproduce agent trained paper found experimentsexperiments folder straightforward modify use different trainingtesting data parameter reproducing ecodqn agent test train agent two different type graph called erdosrenyi er barabasialbert ba graph type train agent 20 40 60 100 200 500 vertex graph note code typically refers vertex spin training agent reproduced running corresponding script replace er20spin appropriate folder different agent cd ecodqn python experimentser20spintraintraineco creates directory er20spineconetwork store following network parameter various point training fully trained agent test score training pkl file plotted loss training pkl file plotted agent typically tested graph type greater size test agent presuming training script already run simply use cd ecodqn python experimentser20spintesttesteco edit script test subset possible graph size desired creates directory er20spinecodata store following every graph size tested averaged result summarized panda dataframe saved resultsxxxpkl raw result graph summarized panda dataframe saved resultsxxxrawpkl full history every test episode saved resultsxxxhistoriespkl alternativley agent trained tested single script cd ecodqn python experimentser20spintrainandtesteco testing pretrained agent script test pretrained agent different graph set provided experimentspretrainedagenttestecopyexperimentspretrainedagenttestecopy simply edit networksaveloc graphsaveloc parameter point desired agent testgraphs moreover pretrained agent produced paper found experimentspretrainedagentnetworksecoexperimentspretrainedagentnetworkseco reproducing s2vdqn agent test s2vdqn framework origionally proposed khalil et reimplemented use network architecure ecodqn every training test script equivalent s2vdqn script ran replacing eco s2v please note implementation s2vdqn however original repository provided hanjun dai repository content graphsgraphs contains graph instance used paper split three catagories benchmarksgraphsbenchmarks known benchmark tested paper specifically g1g10 800 vertex g22g32 2000 vertex gset along physic 125 vertex dataset testinggraphstesting set 50 graph graph type size graph agent tested training validationgraphsvalidation set 100 graph graph type size graph performance trained agent tested within subfolders opts folder contains optimum solutionsvalues best known benchmarking graph best found optimization method described supplemental material paper testing validation graph graph set pkl file unpickle list graph ultimately code want list numpy array however loadgraphset function experimentsutilspyexperimentsutilspy also convert list either networkx graph scipy sparse matrix correct form memory efficient way storing large graph wish point code custom set graph match one format appropriately stored pkl file experimentsexperiments described srcsrc contains source code ecodqn consists three directory high level agentssrcagents contains dqnsrcagentsdqn agent trained along class solve graph using either trained agent greedy heuristic envssrcenvs contains environment tacking combinatorial optimisation graph rlframework networkssrcnetworks contains qnetwork used ecodqn form message passing neural network mpnn first introduced gilmer et reference find work associated paper useful cited articlebarrett2019exploratory titleexploratory combinatorial optimization reinforcement learning authorbarrett thomas clements william r foerster jakob n lvovsky ai journalarxiv preprint arxiv190904063 year2019
Graphs;lightgcn tensorflow implementation sigir 2020 paper xiangnan kuan deng xiang wang yan li yongdong zhang meng wang2020 lightgcn simplifying powering graph convolution network recommendation paper contributor dr xiangnan staffustceducnhexn kuan deng yingxin wu also provide pytorch implementation lightgcn contributor jianbai ye introduction work aim simplify design gcn make concise appropriate recommendation propose new model named lightgcn including essential component gcn—neighborhood aggregation—for collaborative filtering environment requirement code tested running python 365 required package follows tensorflow 1110 numpy 1143 scipy 110 sklearn 0191 cython 02915 c evaluator implemented c code output metric training much efficient python evaluator need compiled first using following command python setuppy buildext inplace compilation c code run default instead python code example run 3layer lightgcn instruction command clearly stated code see parser function lightgcnutilityparserpy gowalla dataset command python lightgcnpy dataset gowalla regs 1e4 embedsize 64 layersize 646464 lr 0001 batchsize 2048 epoch 1000 output log evalscorematrixfoldout cpp nusers29858 nitems40981 ninteractions1027370 ntrain810128 ntest217242 sparsity000084 epoch 1 303s train046925046911 000014 epoch 2 271s train021866021817 000048 epoch 879 816s 313s test013271012645 000626 000000 recall018201 precision005601 ndcg015555 early stopping trigger step 5 log018201370537281036 best iter38328296 recall018236 precision005607 ndcg015539 yelp2018 dataset command python lightgcnpy dataset yelp2018 regs 1e4 embedsize 64 layersize 646464 lr 0001 batchsize 2048 epoch 1000 output log evalscorematrixfoldout cpp nusers31668 nitems38048 ninteractions1561406 ntrain1237259 ntest324147 sparsity000130 epoch 1 565s train033843033815 000028 epoch 2 531s train016253016192 000061 epoch 679 1046s 129s test017217016289 000929 000000 recall006359 precision002874 ndcg005240 early stopping trigger step 5 log006359195709228516 best iter28428150 recall006367 precision002868 ndcg005236 amazonbook dataset command python lightgcnpy dataset amazonbook regs 1e4 embedsize 64 layersize 646464 lr 0001 batchsize 8192 epoch 1000 output log evalscorematrixfoldout cpp nusers52643 nitems91599 ninteractions2984108 ntrain2380730 ntest603378 sparsity000062 epoch 1 532s train057471057463 000008 epoch 2 473s train031518031478 000040 epoch 779 1817s 790s test020300019434 000866 000000 recall004120 precision001703 ndcg003186 early stopping trigger step 5 log004119725897908211 best iter33498754 recall004123 precision001710 ndcg003189 note duration training testing depends running environment dataset provide three processed datasets gowalla yelp2018 amazonbook traintxt train file line user herhis positive interaction item useridt list itemidn testtxt test file positive instance line user herhis positive interaction item useridt list itemidn note treat unobserved interaction negative instance reporting performance userlisttxt user file line triplet orgid remapid one user orgid remapid represent id user original datasets respectively itemlisttxt item file line triplet orgid remapid one item orgid remapid represent id item original datasets respectively efficiency improvement parallelized sampling cpu c evaluation topk recommendation
Graphs;simple deep graph convolutional network repository contains pytorch implementation simple deep graph convolutional dependency cuda 101 python 369 pytorch 131 networkx 21 scikitlearn datasets data folder contains three benchmark datasetscora citeseer pubmed newdata folder contains four datasetschameleon cornell texas wisconsin use semisupervised setting fullsupervised setting geomgcn ppi downloaded result testing accuracy summarized dataset depth metric dataset depth metric cora 64 855 cham 8 6248 cite 32 734 corn 16 7649 pubm 16 803 texa 32 7784 corafull 64 8849 wisc 16 8157 citefull 64 7713 ppi 9 9956 pubmfull 64 9030 obgnarxiv 16 7274 usage replicate semisupervised result run following script sh sh semish replicate fullsupervised result run following script sh sh fullsh replicate inductive result ppi run following script sh sh ppish reference implementation pyg folder includes simple pytorch geometric implementation gcnii requirement torchgeometric 150 ogb 120 running example python corapy python arxivpy citation articlechenwhdl2020gcnii title simple deep graph convolutional network author ming chen zhewei wei zengfeng huang bolin ding yaliang li year 2020 booktitle proceeding 37th international conference machine learning
Graphs;clustergcn codebeat repo pytorch implementation clustergcn efficient algorithm training deep large graph convolutional network kdd 2019 p aligncenter img width600 srcclustergcnjpg p abstract p alignjustify graph convolutional network gcn successfully applied many graphbased application however training largescale gcn remains challenging current sgdbased algorithm suffer either high computational cost exponentially grows number gcn layer large space requirement keeping entire graph embedding node memory paper propose clustergcn novel gcn algorithm suitable sgdbased training exploiting graph clustering structure clustergcn work following step sample block node associate dense subgraph identified graph clustering algorithm restricts neighborhood search within subgraph simple effective strategy lead significantly improved memory computational efficiency able achieve comparable test accuracy previous algorithm test scalability algorithm create new amazon2m data 2 million node 61 million edge 5 time larger previous largest publicly available dataset reddit training 3layer gcn data clustergcn faster previous stateoftheart vrgcn 1523 second v 1961 second using much le memory 22gb v 112gb furthermore training 4 layer gcn data algorithm finish around 36 minute existing gcn training algorithm fail train due outofmemory issue furthermore clustergcn allows u train much deeper gcn without much time memory overhead lead improved prediction accuracy using 5layer clustergcn achieve stateoftheart test f1 score 9936 ppi dataset previous best result 9871p repository provides pytorch implementation clustergcn described paper clustergcn efficient algorithm training deep large graph convolutional network weilin chiang xuanqing liu si si yang li samy bengio chojui hsieh kdd 2019 requirement codebase implemented python 352 package version used development networkx 111 tqdm 4281 numpy 1154 panda 0234 texttable 150 scipy 110 argparse 110 torch 041 torchgeometric 031 metis 02a4 scikitlearn 020 torchsplineconv 104 torchsparse 022 torchscatter 104 torchcluster 115 installing metis ubuntu sudo aptget install libmetisdev datasets p alignjustify code take edge list graph csv file every row indicates edge two node separated comma first row header node indexed starting 0 sample graph pubmed included input directory addition edgelist csv file sparse feature another one target variablep p alignjustify feature matrix sparse one stored csv feature indexed 0 consecutively feature matrix csv structured asp node idfeature idvalue 0 3 02 0 7 05 1 17 08 1 4 54 1 38 13 n 3 09 p alignjustify target vector csv two column header first contains node identifier second target csv sorted node identifier target column contains class meberships indexed zerop node idtarget 0 3 1 1 2 0 3 1 n 3 option p alignjustify training clustergcn model handled srcmainpy script provides following command line argumentsp input output option edgepath str edge list csv default inputedgescsv featurespath str feature csv default inputfeaturescsv targetpath str target class csv default inputtargetcsv model option clusteringmethod str clustering method default metis clusternumber int number cluster default 10 seed int random seed default 42 epoch int number training epoch default 200 testratio float training set ratio default 09 learningrate float adam learning rate default 001 dropout float dropout rate value default 05 layer lst layer size default 16 16 16 example p alignjustify following command learn neural network score test set training model default datasetp sh python srcmainpy p aligncenter img stylefloat center srcclustergcngif p training clustergcn model 100 epoch sh python srcmainpy epoch 100 increasing learning rate dropout sh python srcmainpy learningrate 01 dropout 09 training model different layer structure sh python srcmainpy layer 64 64 training random clustered model sh python srcmainpy clusteringmethod random license
Graphs;gcnn update please check gnnbased fake news detection model along two realworld datasets collection gnnbased fake news detector fake news propagation pytorchgeometric implementation gcnn model fake news detection original model proposed following paper paper fake news detection social medium using geometric deep p aligncenter br img width900 br p run model need least python 36 install following package bash pytorch 160 pytorch geometric 161 tqdm sklearn
Graphs;inductive graph neural network spatiotemporal kriging ignnk code corresponding experiment conducted aaai 2021 paper inductive graph neural network spatiotemporal yuankai wu dingyi zhuang aurélie labbe lijun sun motivation many application placing sensor fully spatial coverage may impractical installation maintenance cost device also limit number sensor deployed network better kriging model achieve higher estimation accuracyreliability le number sensor thus reducing operation maintenance cost sensor network kriging result produce finegrained highresolution realization spatiotemporal data used enhance realworld application travel time estimation disaster evaluation limitation traditional method essentially transductive new sensorsnodes introduced network cannot directly apply previously trained model instead retrain full model even minor change conversely develop inductive graph neural network kriging ignnk model work task img width800 img width800 goal spatiotemporal kriging perform signal interpolation unsampled location given observed signal sampled location period first randomly select subset node available sensor create corresponding subgraph mask missing train gnn reconstruct full signal node including observed masked node subgraph datasets datasets manipulated code downloaded following location metrla traffic data nrel solar energy ushcn weather condition sedata traffic data pems traffic data dependency numpy pytorch matplotlib panda scipy scikitlearn geopandas file utilspy file preprocess datasets basicstructurepy file pytorch implementation basic graph neural network structure ignnkdmetrlaipynb file training example metrla dataset ignnkucentralmissingipynb file present kriging central u precipitation ushcn weather condition basic gnns implementation basicstructurepy graph convolutional network kgcn basicstructurepy kipf thomas n max welling semisupervised classification graph convolutional iclr 2016 chebynet cgcn micha ̈el defferrard xavier bresson pierre vandergheynst convolutional neural network ongraphs fast localized spectral nip 2016 diffusion convolutional network dgcn li yu r shahabi c liu diffusion convolutional recurrent neural network datadriven traffic iclr 2017 ignnk structure based diffusion convolutional network one always build structure using basic building block continue implementing gnn structure suitable kriging task graph attention network gat veličković p cucurull g casanova romero lio p bengio graph attention nip 2017 training metrla datasets simply train ignnk metrla command line python ignnktrainpy metr 150 h 24 nm 50 nu 50 maxiter 750 datasets nrel python ignnktrainpy nrel 100 h 24 nm 30 nu 30 maxiter 750 ushcn python ignnktrainpy ushcn 900 h 6 nm 300 nu 300 maxiter 750 z 350 sedata python ignnktrainpy sedata 240 h 24 nm 80 nu 80 maxiter 750
Graphs;graph convolutional network tensorflow implementation graph convolutional network task semisupervised classification node graph described paper thomas n kipf max welling semisupervised classification graph convolutional iclr 2017 highlevel explanation look blog post thomas kipf graph convolutional 2016 installation bash python setuppy install requirement tensorflow 012 networkx run demo bash cd gcn python trainpy data order use data provide n n adjacency matrix n number node n feature matrix number feature per node n e binary label matrix e number class look loaddata function utilspy example example load citation network data cora citeseer pubmed original datasets found version see data folder use dataset split provided zhilin yang william w cohen ruslan salakhutdinov revisiting semisupervised learning graph icml 2016 specify dataset follows bash python trainpy dataset citeseer editing trainpy model choose following model gcn graph convolutional network thomas n kipf max welling semisupervised classification graph convolutional 2016 gcncheby chebyshev polynomial version graph convolutional network described michaël defferrard xavier bresson pierre vandergheynst convolutional neural network graph fast localized spectral nip 2016 dense basic multilayer perceptron support sparse input graph classification framework also support batchwise classification multiple graph instance potentially different size adjacency matrix best concatenate respective feature matrix build sparse blockdiagonal matrix block corresponds adjacency matrix one graph instance pooling case graphlevel output opposed nodelevel output best specify simple pooling matrix collect feature respective graph instance illustrated cite please cite paper use code work inproceedingskipf2017semi titlesemisupervised classification graph convolutional network authorkipf thomas n welling max booktitleinternational conference learning representation iclr year2017
Graphs;pyorca orca orbit counting python wrapper wrapper c orca algorithm published combinatorial approach graphlet counting bioinformatics 2014 use feature different orca original purpose main use case library use orbit count feature machine learning graphstructured data graph neural network gnns achieve stateoftheart many graph learning domain recommender among many others example architecture include pytorch geometric dgl useful library gnns addition python version c orca interface repo also provide python interface easily create feature vector node based orbit count used utility supply additional input feature node gnn
Graphs;dynaml research toolkit understanding misinformation flow social network using dynamic system simulation graph neural network inspiration understanding spread information particularly disinformation vital today world data spread competing narrative bot agent difficult gather scale extended period provide simulation tool kit enables researcher understand problem model interaction bot agent competing spread different narrative broader community simulating social network dynamic system created data science tooling system enables researcher understand social network better initial step simulation create randomized graph similar edge distribution realworld social network node graph assign agent type nonbot neutral bot promoting idea bot promoting idea b initialize spread information graph beginning bot one trying spread respective idea time step use edge graph identity agent current held belief agent determine percent chance information spread new agent keep track history information network create simulated data set visualize complex system interaction created animated assistant show information graph node identity denoted inner darker color current idea node spreading shown outer lighter color active connection graph colored edge installing via pypi pip install library via following pip command pip install dynamllib import library python project import dynamllib access experiment class dynamllibexperiment import experiment analyzing social network via graph neural network gnns example research using network simulation toolkit dynaml take stance social relationship user necessary component understanding spread information dynamic social network model relationship build detailed graph representation described simulation question becomes make meaningful inference data learning problem example researcher could network simulator toolkit took problem predicting node bot based information propagated node local community learning problem input social network graph simulated using toolkit output label bot 1 bot 0 node user network model based model seminal literature space gnns specifically focused graphsage model read check training code github initial result able develop model could train simulated data toolkit find actual loss one experiment graph neural network may heard fullyconnected nns convolutional nns even recurrent nns none model designed learn graphical data graphical data everywhere social network protein folding essentially gnns designed take advantage graphical structure make better prediction graphsage graph neural network model explores node neighbor across connection aggregate communal information understand content relationship contained graph see graphic check paper hamilton zhitao ying jure leskovec inductive representation learning large graph advance neural information processing system 2017 pytorch geometric pytorch great deep learning framework yet committed handling graphical data get around used library provides additional functionality pytorch process graph data learning use framework load data train gnn rewarding insightful read paper fey matthias jan eric lenssen fast graph representation learning pytorch geometric arxiv preprint arxiv190302428 2019 challenge ran one challenge ran visualizing graph scale created several network thousand node unable effectively visualize graph due large number node edge even trained graph neural network large graph unable visualize result another issue removing huge file git accidentally committed oops accomplishment proud successfully created dynamic simulation competing idea spread social network filled nonbots bot successfully created dynamic visualization spread competing information network involved showing original status different node ie nonbot bot promoting point bot promoting point b current state node destination node trying spread information visualized nonbots start spreading misinformation network influenced bot used pytorch implement graph neural network predict likelihood user social network bot created python library code allow researcher easily use tool conducting research spread misinformation social network help stop problem learned dynamically simulate interaction node graphnetwork train test graph neural network used accurately predict attribute node graph dynamically visualizeanimate graph using javascript develop functional web application using flask future development dynaml want increase customizability network simulation tool kit used researcher multiple area interest disease spreading computer network attack infrastructure grid want investigate combining time series analysis model statespace state art graph neural network predicting information spread multiple time step want show method developed using simulation generalizable realworld data set want able visualize dynamic graph scale hundred thousand node edge
Graphs;fairwalk python3 implementation fairwalk algorithm tahleen rahman bartlomiej surma michael backes yang zhang rahman tahleen et al fairwalk towards fair graph embedding proceeding 2019 international joint conference artifical intelligence ijcai ijcai installation python setuppy install usage python import networkx nx fairwalk import fairwalk create graph graph nxfastgnprandomgraphn100 p05 n lengraphnodes node2group node group node group zipgraphnodes 5nprandomrandomnastypeint nxsetnodeattributesgraph node2group group precompute probability generate walk window work workers1 model fairwalkgraph dimensions64 walklength30 numwalks200 workers4 use tempfolder big graph embed node model modelfitwindow10 mincount1 batchwords4 keywords acceptable gensimword2vec passed diemnsions worker automatically passed fairwalk constructor look similar node modelwvmostsimilar2 output node name always string save embeddings later use modelwvsaveword2vecformatembeddingfilename save model later use modelsaveembeddingmodelfilename embed edge using hadamard method fairwalkedges import hadamardembedder edgesembs hadamardembedderkeyedvectorsmodelwv look embeddings fly pas normal tuples edgesembs1 2 output array 575068220e03 110937878e02 376693785e01 269105062e02 dtypefloat32 get edge separate keyedvectors instance use caution could huge big network edgeskv edgesembsaskeyedvectors look similar edge time tuples must sorted str edgeskvmostsimilarstr1 2 save embeddings later use edgeskvsaveword2vecformatedgesembeddingfilename parameter fairwalkfairwalk fairwalk constructor 1 graph first positional argument networkx graph node name must integer string output model always string 2 dimension embedding dimension default 128 3 walklength number node walk default 80 4 numwalks number walk per node default 10 5 p return hyper parameter default 1 6 q inout parameter default 1 7 weightkey weighted graph key weight attribute default weight 8 worker number worker parallel execution default 1 9 samplingstrategy node specific sampling strategy support setting node specific q p numwalks walklength use key exactly set use global one passed object initialization 10 quiet boolean controlling verbosity default false 11 tempfolder string path pointing folder save shared memory copy graph supply working graph big fit memory algorithm execution fairwalkfit method accepts key word argument acceptable gensimword2vec fairwalkedgeembedder edgeembedder abstract class concrete edge embeddings class inherit class averageembedder hadamardembedder weightedl1embedder weightedl2embedder practical definition could found table 1 notice edge embeddings defined pair node connected even node constructor 1 keyedvectors gensimmodelskeyedvectors instance containing node embeddings 2 quiet boolean controlling verbosity default false edgeembeddergetitemitem method better known edgeembedderitem 1 item tuple consisting 2 node keyedvectors passed constructor return embedding edge edgeembedderaskeyedvectors method return gensimmodelskeyedvectors instance possible node pair sorted manner string example node 1 2 3 key 1 1 1 2 1 3 2 2 2 3 3 3 caveat node name input graph must string ints parallel execution working window joblib known issue run nonparallel window pas workers1 fairwalks constructor
Graphs;graph convolutional network pytorch version gcn implemented paper semisupervised classification graph convolutional network paper author code 1 kera 2 pytorch 3 tensorflow v1 requirement install requirement console pip3 install r requirementstxt
Graphs;dirichlet graph autoencoders tensorflow implementation dirchlet graph variational autoencoder model nip 2020 one pytorch version dgvae endtoend trainable neural network model unsupervised learning generation clustering graph code related graph generation described paper dgvae based variational graph autoencoder vgae n kipf welling variational graph nip workshop bayesian deep learning 2016 installation bash python setuppy install requirement tensorflow 1100 python 364 networkx scikitlearn scipy run demo bash python dgvaetraingeneratepy model option model default ourvaedgvae others including ouraedgaegcnvaegcnaegraphitevaegraphiteae dataset default erdosrenyi others including egoregulargeometricpowerlawbarabasialbert
Graphs;gosh embedding big graph small hardware gosh gpubased graph embedding tool take graph produce ddimensional vector every node graph embeddings used multitude machine learning task including node classification link prediction graph visualization anomaly detection embedding pipelineembpng gosh employ novel coarsening algorithm multiedgecollapse compress graph smaller graph embeds smaller graph produce accurate embeddings quickly besides us special scheduling algorithm embed graph using single gpu even memory requirement graph exceeds gpu getting started requirement compiled program nvcc using cuda 101and ran ubuntu 440159 compiling build executables project single command simply clone repo navigate call make command git clone cd gosh make clean make produce two executables execsgoshout executable take input flag option execsgoshnargparseout executable take parameter sequence string none parameter optional executing basic execution gosh done follows execsgoshout inputgraph string outputembedding string directed number epoch number inputgraph string edge list file vertex id zeroindexed integer ie file form j k l b j k l b vertex id graph please note vertex get embeddings even without edge outputembedding string file embeddings printed embeddings either printed ascii formatted text file binary file triggered binaryoutput flag output format ascii text follows numvertices dimension 0 e1 e2 e3 edimension1 1 e1 e2 e3 edimension1 numvertices1 e1 e2 e3 edimension1 numvertices dimension number vertex embedding dimensionality respectively every line afterward corresponds vertex graph line start id vertex followed embeddings element within line spaceseparated epoch number number epoch run entirety graph running epoch graph gi coarsening time corresponds running vi positive sample graph note strategy epoch distributed across different level coarsening tuned using option epochstrategy smoothingratio discussed directed number whether graph directed 1 undirected 0 passed binary compressed sparse row format 2 optional parameter many optional parameter used finetune embedding global parameter dimension number integer dimensionality embedding negativesamples number number negative sample used every positive update negativeweight float scaling factor used negative sample scale gradient used updating embeddings negative update deviceid nummber id gpu device used embedding binaryoutput whether output printed binary format compactness ease processing memory format binary output follows number vertex signed integer embedding dimension signed integer embeddings vertex printed sequentially single precision float c sampling parameter samplingalgorithm number method used create positive sample training currently two sampling strategy implemented samplingalgorithm 0 1hop neighbor sampling ppr sampling described depending value alpha alpha 0 positive sample node sampled direct neighbor alpha 0 alpha 1 positive sample node node reached performing personalized page rank random walk alpha damping factor defined samplingalgorithm 1 randomwalk based sampling method random walk generated cpu sample extracted sent gpu controlled three parameter walklength number length random walk augmentationdistance number within walk sequence number pair node used poisitive sample samplepoolsize number sample added pool cpu copied gpu alpha number value positive sampling strategy used model based alpha 0 use adjacency similarity positive sampling approach 0 alpha 100 use ppr alpha100 damping factor learning rate parameter l learningrate float global learning rate model learningratedecaystrategy num strategy used decay learning rate level different level four strategy 0123 difference shown 0 1 every level learning rate decay linearly initial learning rate starting first epoch last epoch based following equation currentlearningrate max1currentepochtotalepochs 1e4initiallearningrate currentepoch totalepochs current total epoch current coarsening level 2 3 learning rate end level beginning decay 1 3 initial learning rate every coarsening level differs based following heuristic lri lr vi lrrevnv lri lrsqrtvi lrrevnv otherwise lr input learning rate global lri initial learning rate coarsening level lrrevnv tunable hyperparameter srcmaincu 0 2 initial learning rate level original learning rate given input coarsening parameter nocoarsening apply coarsening run embedding directly original graph coarseningstoppingthreshold num number vertex stop coarsening ie graph gi generated vi num added coarsened set coarsening continue coarseningstoppingprecision float accpetable shrinkage graph coarsening ie graph gi coarsened graph gi1 vi1 vi float graph gi1 added coarsened set coarsening continue coarseningmatchingthresholdratio num control total number match allowed per vertex given graph gi coarsened time vertex gi allowed match num vi vertex coarseningminverticesingraph num minimum number vertex acceptable graph added coarsened set ie graph gi coarsened graph gi1 vi1 num graph gi1 added coarsened set epoch distribution parameter epochstrategy choose strategy use distribute epoch across level multiple strategy available fast smallest graph given half total epoch next level given half sfast coarsesmoothingratio totalepochs epoch distributed equally across level remainder distributed based fast rule normal equal distribution epoch across level accurate opposite fast biggest graph given half total epoch smaller level given half saccurate coarsesmoothingratio totalepochs epoch distributed equally across level remainder distributed based accurate rule note aforementioned rule level allocated minimum 1 epoch smoothingratio smoothing ratio used distributing epoch smooth strategy ie sfast saccurate large graph parameter epochbatchsize num number epoch run per large graph execution round single round consists full rotation embedding part pair numparts num number embedding part store concurrently gpu numpools num number sample pool store concurrently gpu samplingthreads num number thread work sampling parallel concurrentsamplers num number sample pool sampled concurrently single pool sampled maximum samplingthreads concurrentsamplers thread taskqueuethreads num number thread execute task task queue numsamplepoolsets num number sample pool set author amro alabsi taha atahan kamer acknowldegments adapted positive sampling embedding update paper written anton davide panagiotis emmanuel used argparse header c written hilton process cli input citation find code useful research please cite u article9623416 author alabsi aljundi amro akyildiz taha atahan kaya kamer journal ieee transaction parallel distributed system title boosting graph embedding single gpu year 2021 volume number page 11 doi 101109tpds20213129617 inproceedings9377898 authorakyildiz taha atahan alabsi aljundi amro kaya kamer booktitle2020 ieee international conference big data big data titleunderstanding coarsening embedding largescale graph year2020 volume number pages29372946 doi101109bigdata5002220209377898 inproceedings10114534043973404456 author akyildiz taha atahan aljundi amro alabsi kaya kamer title gosh embedding big graph small hardware year 2020 isbn 9781450388160 publisher association computing machinery address new york ny usa url doi 10114534043973404456 booktitle 49th international conference parallel processing icpp articleno 4 numpages 11 keywords gpu parallel graph algorithm link prediction graph embedding graph coarsening location edmonton ab canada series icpp 20 note nominated bestpaper award
Graphs;netmf python implementation netmf task network embedding learning described paper network embedding matrix factorization unifying deepwalk line pte data set blogcatalog proteinprotein interaction wikipedia cite please cite paper use code work inproceedingsqiu2018network titlenetwork embedding matrix factorization unifying deepwalk line pte node2vec authorqiu jiezhong dong yuxiao hao li jian wang kuansan tang jie booktitleproceedings eleventh acm international conference web search data mining pages459467 year2018 organizationacm
Graphs;schnetpack deep neural network atomistic system build code style schnetpack aim provide accessible atomistic neural network trained applied outofthebox still extensible custom atomistic architecture currently provided model schnet endtoend continuousfilter cnn molecule material 13 wacsf weighted atomcentered symmetry function 45 note last version major update next version plan adopt hydra pytorch lightning switch indexing instead masking make network compatible torchscript therefore breaking change requirement ase numpy pytorch 18 h5py optional tensorboardx note recommend using gpu training neural network installation install pip pip install schnetpack install source clone repository git clone cd schnetpack install requirement pip install r requirementstxt install schnetpack pip install youre ready go getting started best place start training schnetpack model common benchmark dataset example script provided schnetpack inserted path installation qm9 example qm9 example script allows train evaluate schnet wacsf neural network training started using spkrunpy train schnetwacsf qm9 dbpath modeldir split numtrain numval cuda numtrain numval need replaced number training validation datapoints respectively choose schnet wacsf network provide path database file path directory used store model database path exist data downloaded stored please note database path must include file extension db cuda flag activate gpu training default hyperparameters work fine however change commandline argument please refer help spkrunpy train schnetwacsf help training progress logged modeldirlog either csv default tensorboard event file latter tensorboard need installed view event file done installing version included tensorflow pip install tensorflow standalone evaluate trained model best validation error call spkrunpy eval datapath modeldir split test cuda run specified split write result file evaluationtxt model directory documentation full api reference visit using schnetpack research please cite kt schütt p kessel gastegger k nicoli tkatchenko kr müller schnetpack deep learning toolbox atomistic system j chem theory comput 2018 reference 1 kt schütt f arbabzadah chmiela kr müller tkatchenko quantumchemical insight deep tensor neural network nature communication 8 13890 2017 2 kt schütt pj kindermans h e sauceda chmiela tkatchenko kr müller schnet continuousfilter convolutional neural network modeling quantum interaction advance neural information processing system 30 pp 9921002 2017 3 kt schütt pj kindermans h e sauceda chmiela tkatchenko kr müller schnet deep learning architecture molecule material journal chemical physic 14824 241722 2018 4 gastegger l schwiedrzik bittermann f berzsenyi p marquetand wacsf—weighted atomcentered symmetry function descriptor machine learning potential journal chemical physic 14824 241709 2018 5 j behler parrinello generalized neuralnetwork representation highdimensional potentialenergy surface physical review letter 9814 146401 2007
Graphs;rotate knowledge graph embedding relational rotation complex space introduction pytorch implementation model knowledge graph embedding kge provide toolkit give stateoftheart performance several popular kge model toolkit quite efficient able train large kge model within hour single gpu faster multigpu implementation rotate kge model available implemented feature model x rotate x protate x transe x complex x distmult evaluation metric x mrr mr hits1 hits3 hits10 filtered x aucpr country data set loss function x uniform negative sampling x selfadversarial negative sampling usage knowledge graph data entitiesdict dictionary map entity unique id relationsdict dictionary map relation unique id traintxt kge model trained fit data set validtxt create blank file validation data available testtxt kge model evaluated data set train example command train rotate model fb15k dataset gpu 0 cudavisibledevices0 python u codesrunpy dotrain cuda dovalid dotest datapath datafb15k model rotate n 256 b 1024 1000 g 240 10 adv lr 00001 maxsteps 150000 save modelsrotatefb15k0 testbatchsize 16 de check argparse configuration codesrunpy argument detail test cudavisibledevicesgpudevice python u codepathrunpy dotest cuda init save reproducing best result reprocude result iclr 2019 paper rotate knowledge graph embedding relational rotation complex run bash command bestconfigsh get best performance rotate transe complex five widely used datasets fb15k fb15k237 wn18 wn18rr country runsh script provides easy way search hyperparameters bash runsh train rotate fb15k 0 0 1024 256 1000 240 10 00001 200000 16 de speed kge model usually take half hour run 10000 step single geforce gtx 1080 ti gpu default configuration model need different maxsteps converge different data set dataset fb15k fb15k237 wn18 wn18rr country maxsteps 150000 100000 80000 80000 40000 time 9 h 6 h 4 h 4 h 2 h result rotate model dataset fb15k fb15k237 wn18 wn18rr mrr 797 ± 001 337 ± 001 949 ± 000 477 ± 001 mr 40 177 309 3340 hits1 746 241 944 428 hits3 830 375 952 492 hits10 884 533 959 571 using library python libarary organized around 3 object traindataset dataloaderpy prepare data stream training testdataset dataloaderpy prepare data stream evluation kgemodel modelpy calculate triple score provide traintest api runpy file contains main function par argument read data initilize model provides training loop add model modelpy like def transeself head relation tail mode mode headbatch score head relation tail else score head relation tail score selfgammaitem torchnormscore p1 dim2 return score citation use code please cite following inproceedings sun2018rotate titlerotate knowledge graph embedding relational rotation complex space authorzhiqing sun zhihong deng jianyun nie jian tang booktitleinternational conference learning representation year2019
Graphs;lightgcn tensorflow implementation paper xiangnan kuan deng xiang wang yan li yongdong zhang meng wang2020 lightgcn simplifying powering graph convolution network recommendation paper author dr xiangnan staffustceducnhexn introduction work aim simplify design gcn make concise appropriate recommendation propose new model named lightgcnincluding essential component gcn—neighborhood aggregation—for collaborative filtering environment requirement code tested running python 365 required package follows tensorflow 1110 numpy 1143 scipy 110 sklearn 0191 example run code instruction command clearly stated code see parser function lightgcnutilityparserpy gowalla dataset python lightgcnpy dataset gowalla regs 1e4 embedsize 64 layersize 64646464 lr 0001 batchsize 2048 epoch 1000 yelp2018 dataset python lightgcnpy dataset yelp2018 regs 1e4 embedsize 64 layersize 64646464 lr 0001 batchsize 2048 epoch 1000 amazonbook dataset python lightgcnpy dataset amazonbook regs 1e4 embedsize 64 layersize 64646464 lr 0001 batchsize 1024 epoch 200 dataset provide three processed datasets gowalla yelp2018 amazonbook traintxt train file line user herhis positive interaction item useridt list itemidn testtxt test file positive instance line user herhis positive interaction item useridt list itemidn note treat unobserved interaction negative instance reporting performance userlisttxt user file line triplet orgid remapid one user orgid remapid represent id user original datasets respectively itemlisttxt item file line triplet orgid remapid one item orgid remapid represent id item original datasets respectively improvement parallelize sampling cpu
Graphs;graph convolutional network relational link prediction repository contains tensorflow implementation relational graph convolutional network rgcn well experiment relational link prediction description model result found paper modeling relational data graph convolutional michael schlichtkrull thomas n kipf peter bloem rianne van den berg ivan titov max welling arxiv 2017 requirement tensorflow 14 running demo provide bash script run demo code folder setting collection configuration file found block diagonal model used paper represented configuration file settingsgcnblockexp run given experiment execute bash script follows bash runtrainsh configuration advise training take several hour require significant amount memory citation please cite paper use code work articleschlichtkrull2017modeling titlemodeling relational data graph convolutional network authorschlichtkrull michael kipf thomas n bloem peter berg rianne van den titov ivan welling max journalarxiv preprint arxiv170306103 year2017
Graphs;image1 imagesacclossfromscratchadampng image2 imagestsneprojectionpng image3 imagescctembedding2png image4 imagesroccurveinfectedpng image5 imagesroccurveexposedpng image6 imagesroccurvesusceptiblepng image7 imagesconfusionmatrixsusceptiblepng motivation goal repo demonstrate use stellargraph implementation graphsage algorithm graph node inference develop graph embedding learned covid19 contact network predict risk classification novel contact based prior knowledge contact vulnerability measure resulting lowdimensional embedding grapsage output layer stack reveal useful contact proximity information readily available based solely contact list vulnerability data alone introduction deeptrace graphsagebased machinelearning pipeline contact tracing conventional method exploit knowledge individual person contact taken set individual contact set essentially graph node representing people edge connecting contact people proposed method allows u utilize information stored graph contact well node feature develop method classify individual contact set either susceptible exposed infected particular case use covid vulnerability index assign feature vector node able learn contact network based graph node edgelist specification also vulnerability feature mapping thus create threedimensional node embedding new contact show assessment likelihood one three exposure category – infected exposed susceptible low dimensional embedding allows contact tracing personnel prioritize individual contact test situation pandemic evolving quickly limited personnel test resource correspond everyone contact set thu one quickly identify prioritize person contact isolate br figure 1 show tsne projection data onto three dimension simulated case study 27 infected 519 susceptible 419 exposed individual alt textimage2 2d projection tsne embedding alt textimage3 training accuracy loss plot training dataset initial scratch training alt textimage1 dataset dataset consists fictional contact using cora dataset link data covid19 vulnerability example feature data found performance analysis following roc curve show performance test data infected exposed susceptible test class respectively alt textimage4 alt textimage5 alt textimage6 confusion matrix susceptible class alt textimage7 critical dependency stellargraph networkx sklearn python3 tensorflow 20 kera 23 panda using anaconda conda env create f deeptraceyml note requirementstxt contains many extraneous package used project wont need reference stellargraph graphsage paper graph node embeddings compartmental modeling br
Graphs;deepmind research repository contains implementation illustrative code accompany deepmind publication along publishing paper accompany research conducted deepmind release opensource data enable broader research community engage work build upon ultimate goal accelerating scientific progress benefit society example build implementation deep differential neural experiment environment use research deepmind starcraft enjoy building tool environment software library infrastructure kind listed view open position work related area career full list publication please see project pushing frontier density functionals solving fractional electron problemdensityfunctionalapproximationdm21 science 2021 mind gap assessing temporal generalization neural language modelspitfallsstaticlanguagemodels neurips 2021 difficulty passive learning deep reinforcement learningtandemdqn neurips 2021 skilful precipitation nowcasting using deep generative model radarnowcasting nature 2021 computeaided design languagecadl encoders ensemble continual learningcontinuallearning towards mental time travel hierarchical memory reinforcement learning agentshierarchicaltransformermemory perceiver io general architecture structured input outputsperceiver solving mixed integer program using neural networksneuralmipsolving realistic simulation framework learning label noisenoisylabel rapid tasksolving novel environmentsrapidtasksolving iclr 2021 wikigraphs wikipedia knowledge graph paired datasetwikigraphs textgraphs 2021 behavior prior efficient reinforcement learningboxarrangement learning meshbased simulation graph networksmeshgraphnets iclr 2021 open graph benchmark largescale challenge ogblscogblsc synthetic return longterm credit assignmentsyntheticreturns deep learning approach characterizing major galaxy mergersgalaxymergers better faster fermionic neural networkskfacferminetalpha kfac implementation objectbased attention spatiotemporal reasoningobjectattentionforreasoning effective gene expression prediction sequence integrating longrange interactionsenformer satore firstorder logic saturation atom rewritingsatore characterizing signal propagation close performance gap unnormalized resnetsnfnets iclr 2021 uncovering limit adversarial training normbounded adversarial examplesadversarialrobustness learning rich touch representation crossmodal selfsupervisioncmtouch corl 2020 functional regularisation continual learningfunctionalregularisationforcontinuallearning iclr 2020 selfsupervised multimodal versatile networksmmv neurips 2020 odegan training gans solving ordinary differential equationsodegan neurips 2020 algorithm causal reasoning probability treescausalreasoning gated linear networksgatedlinearnetworks neurips 2020 valuedriven hindsight modellinghimo neurips 2020 targeted free energy estimation via learned mappingslearnedfreeenergyestimation journal chemical physic 2020 learning simulate complex physic graph networkslearningtosimulate icml 2020 physically embedded planning problemsphysicsplanninggames polygen polygen autoregressive generative model 3d meshespolygen icml 2020 bootstrap latentbyol catch carry reusable neural controller visionguided wholebody taskscatchcarry siggraph 2020 memo deep network flexible combination episodic memoriesmemo iclr 2020 rl unplugged benchmark offline reinforcement learningrlunplugged disentangling subspace diffusion geomancergeomancer neurips 2020 theory affordances reinforcement learningaffordancestheory icml 2020 scaling datadriven robotics reward sketching batch reinforcement learningsketchy r 2020 pathspecific counterfactual fairnesscounterfactualfairness aaai 2019 option keyboard combining skill reinforcement learningoptionkeyboard neurips 2019 visr fast task inference variational intrinsic successor featuresvisr iclr 2020 unveiling predictive power static structure glassy systemsglassydynamics nature physic 2020 multiobject representation learning iterative variational inference iodineiodine alphafold casp13alphafoldcasp13 nature 2020 unrestricted adversarial challengeunrestrictedadvx hierarchical probabilistic unet hpunethierarchicalprobabilisticunet training language gans scratchscratchgan neurips 2019 temporal value transporttvt nature communication 2019 continual unsupervised representation learning curlcurl neurips 2019 unsupervised learning object keypoints transportertransporter neurips 2019 bigbiganbigbigan neurips 2019 deep compressed sensingcsgan icml 2019 side effect penaltiessideeffectspenalties predinet architecture relation game datasetspredinet unsupervised adversarial trainingunsupervisedadversarialtraining neurips 2019 graph matching network learning similarity graph structured objectsgraphmatchingnetworks icml 2019 regal transfer learning fast optimization computation graphsregal deep ensemble loss landscape perspectiveensemblelosslandscape powerpropagationpowerpropagation physic inspired modelsphysicsinspiredmodels disclaimer official google product
Graphs;spectralembeddings spectral embedding library made graph convolution network autoencoders robot img1previewsgraphpreviewpng embedding generator library used creating graph convolution network graph autoencoder embeddings knowledge graph allows projection higher order network dependency creating node embeddings respect neighborhood 2 different approach graph autoencoder approach model first higher order similarity measure graph node neighborhood first second order similarity measure created autoencoder circuit preserve proximity loss similarity reconstruction loss img img1previewsgraphaepreviewpng graph convolution network variant include vanillagcnchebgcn splinegcn kernel provide spectral embeddings knowledge graph vanillagcn step produce include creating adjacency matrix representation along node feature input label one hot encoded maintain dimension input model input form node featuresadjacency matrix representation output one hot encoded node label matrix processed additional layer embedding layerlstm added perform node classification extract penultimate layer getting embeddings case img img2previewsvanillagcnpreview1png splinegcn spline gcn involve computing smooth spectral filter get localized spatial filter connection smoothness frequency domain localization space based parseval’s identity also heisenberg uncertainty principle smaller derivative spectral filter smoother function smaller variance spatial filter localization case wrap vanilla gcn additional spline functionality decomposing laplacian diagonal 1spline represents eigenvectors added independently instead taking entire laplacian one time rest code segment remains img img3previewssplinegcnpreviewpng chebgcn one important part spectral gcn chebyshev polynomial used instead laplacian chebnets gcns used arbitrary graph domain limitation isotropic standard convnets produce anisotropic filter euclidean grid direction spectral gcns compute isotropic filter since graph notion direction left right img img4previewschebyshevgcnpreviewpng dependency usability installation carried using pip command follows python pip install spectralembeddings01 library built tensorflow img spectral python used generate embeddings knowledge graph help deep graph convolution kernel autoencoder network library used generate 2 different kind embeddings graph autoencoder embeddings model first higher order similarity measure graph node neighborhood first second order similarity measure created autoencoder circuit preserve proximity loss similarity reconstruction loss model implemented along line embeddings cover first order dependency also used capture second order dependency node neighbor output autoencoder network dimension number input entriesdimension embedding space provided graph autoencoder also produce full embedding subspace entry provided hidden dimension found example provided herea preview generated embeddings shown img architecture graph autoencoder represented help unsupervised local structure component first order supervised global structure component second order linked node graph img using library graph autoencoder embeddings following step install library pip python pip install spectralembeddings02 create function read input csv file input contain atleast 2 column source targetlabels text format include textual extract corresponding label graph created multidigraph networkx target source column input csv file generating embeddings extract label also considered used determine label closest provided sourceinput text example testgraphae method show dataset chosen demonstration google quest dataset source label columntextual content used generate embeddings main function creating graph autoencoder embedding getsdneembeddings method method take parameter hiddendims denotes hidden embedding dimension neural network alpha beta empirical constant finetuning embeddings epoch number training iteration dataframe along source target label model output embedding matrix entry hidden dims corresponding graph graph used plotting chebyshev similarity node rest community neighborhood following preview show code generating graph autoencoder embeddings python def testgraphae sourcelabelquestionbody targetlabelcategory printinput parameter hidden dimension alphabetaepochs hiddendims3216 alpha1e4 beta1e5 epochs20 gembgraphaeplgraphaegetsdneembeddingstraindfsourcelabeltargetlabelhiddendimsalphabetaepochs printgemb return gembgraphaepl plottingwith plotly node embedding particular node represented number plotaeembed method used take parameter subgraph containing input node rest node input node number embedding matrix embedding weight represented python def plotaeembedgraphnodenumemblabel nodedistancesquestionsgraphaenodelevelembeddinggraphnodenumemb vgdfpddataframe vgdfpremisenodelendistances vgdfhypothesisquestions vgdfchebyshevdistancedistances vggnxfrompandasedgelistvgdfsourcehypothesistargetpremiseedgeattrchebyshevdistance plottervgglabel return vgg alternately pyvisplotter method also used us pyvis thus requirement creating autoencoder based node representation dataframe containing source target column textual format graph convolution kernel embeddings embeddings based spectral graph convolution kernel capture node representation laplacian norm matrix part based graph convolution network gcns based deep neural network operate node feature normalized laplacian adjacency matrix input graph gcns mainly used nodesubgraph classification task interested capturing embeddings penultimate layer network create embedding based tensorflow node feature define node don’t predecessor layer 0 embeddings node feature calculate embeddings layer k weight average embeddings layer k1 put activation function kernel 3 variation vanilla gcn chebyshev gcn spline gcn embeddings img vanillagcn kernel vanilla gcngnn utilizes graph laplacian normalized laplacian along spectral filter recursively augments weight next layer based previous layer spectral filter weight initialized using kerastf rest part involves multiplying laplacian tuple nodefeaturesadjacency matrix spectral filter kernel applying activation result generally softmax activation applied classifying output according labelssince classifying node extract final node weight penultimate layer allows projection embedding subspace vgcn kernel step using thisor variant gcn kernel follows install library pip python pip install spectralembeddings02 create function similar testvanillagcn method important submethod taken spectralembeddingsvanillagcn python script getgcnembeddings method importance used create final embeddings passing kernel input parameter hidden unitsnumber hidden neuron intermediate gcn layer number layer signifies number hidden layer subsetthis includes part entire dataframe considered instance 2000 entry would like get node embeddings 25 entry subset becomes 25 epoch dataframe input sourcetarget label string method return embedding matrix graph embedding matrix dimension size subset entry number label instance subset size 20 set label 6 final embedding dimension 206 also since gcn us classification kernel embeddings projected basis number unique label inputall graph kernel follow rule projection python def testvanillagcn printtesting vanillagcn embeddings source target label traindfpdreadcsvetraingraphtraincsv sourcelabelquestionbody targetlabelcategory printinput parameter hidden unit number layerssubset value entry considered embeddingsepochs hiddenunits32 numlayers4 subset34 epochs10 vembvgraphvgcngetgcnembeddingshiddenunitstraindfsourcelabeltargetlabelepochsnumlayerssubset printvembshape return vembvgraph preview generated embeddings dataset 34 node entry represented img plotting embeddings nodesimilar graph autoencoder embeddings use plotvgcnembed method follows python def plotvgcnembedgraphnodenumemblabel nodedistancesquestionsvgcnnodelevelembeddinggraphnodenumemb vgdfpddataframe vgdfpremisenodelendistances vgdfhypothesisquestions vgdfchebyshevdistancedistances vggnxfrompandasedgelistvgdfsourcehypothesistargetpremiseedgeattrchebyshevdistance plottervgglabel return vgg chebyshevgcn kernel chebyshev gcngnn used arbitrary graph domain limitation isotropic standard convnets produce anisotropic filter euclidean grid direction spectral gcns compute isotropic filter since graph notion direction left right cgcn based chebyshev polynomialsthe kernel used spectral convolution made chebyshev polynomial diagonal matrix laplacian eigenvalue chebyshev polynomial type orthogonal polynomial property make good task like approximating function step using thisor variant gcn kernel follows install library pip python pip install spectralembeddings02 create function similar testchebgcn method important submethod taken spectralembeddingschebgcn python script getchebgcnembeddings method importance used create final embeddings passing kernel input parameter hidden unitsnumber hidden neuron intermediate gcn layer number layer signifies number hidden layer subsetthis includes part entire dataframe considered instance 2000 entry would like get node embeddings 25 entry subset becomes 25 epochsk order chebyshev polynomial generate dataframe input sourcetarget label string method return embedding matrix graph embedding matrix dimension size subset entry number label instance subset size 20 set label 6 final embedding dimension 206 also since gcn us classification kernel embeddings projected basis number unique label inputall graph kernel follow rule projection python def testchebgcn printtesting chebgcn embeddings source target label traindfpdreadcsvetraingraphtraincsv sourcelabelquestionbody targetlabelcategory printinput parameter hidden unit number layerssubset value entry considered embeddingsepochs k cheby polynomial hiddenunits32 numlayers4 subset34 epochs10 k4 cembcgraphcgcngetchebgcnembeddingshiddenunitstraindfsourcelabeltargetlabelepochsnumlayerssubsetk printcembshape return cembcgraph preview generated embeddings dataset 34 node entry represented img plotting embeddings nodesimilar graph autoencoder embeddings use plotcgcnembed method follows python def plotcgcnembedgraphnodenumemblabel nodedistancesquestionscgcnnodelevelembeddinggraphnodenumemb vgdfpddataframe vgdfpremisenodelendistances vgdfhypothesisquestions vgdfchebyshevdistancedistances vggnxfrompandasedgelistvgdfsourcehypothesistargetpremiseedgeattrchebyshevdistance plottervgglabel return vgg splinegcn kernel spline gcngnn involve computing smooth spectral filter get localized spatial filter connection smoothness frequency domain localization space based parseval’s identity also heisenberg uncertainty principle smaller derivative spectral filter smoother function smaller variance spatial filter localization case wrap vanilla gcn additional spline functionality decomposing laplacian diagonal 1spline represents eigenvectors added independently instead taking entire laplacian one time step using thisor variant gcn kernel follows install library pip python pip install spectralembeddings02 create function similar testsgcn method important submethod taken spectralembeddingssplinegcn python script getsplinegcnembeddings method importance used create final embeddings passing kernel input parameter hidden unitsnumber hidden neuron intermediate gcn layer number layer signifies number hidden layer subsetthis includes part entire dataframe considered instance 2000 entry would like get node embeddings 25 entry subset becomes 25 epoch dataframe input sourcetarget label string method return embedding matrix graph embedding matrix dimension size subset entry number label instance subset size 20 set label 6 final embedding dimension 206 also since gcn us classification kernel embeddings projected basis number unique label inputall graph kernel follow rule projection python def testsgcn printtesting splinegcn embeddings source target label traindfpdreadcsvetraingraphtraincsv sourcelabelquestionbody targetlabelcategory printinput parameter hidden unit number layerssubset value entry considered embeddingsepochs k cheby polynomial hiddenunits32 numlayers4 subset34 epochs10 sembsgraphsgcngetsplinegcnembeddingshiddenunitstraindfsourcelabeltargetlabelepochsnumlayerssubset printsembshape return sembsgraph preview generated embeddings dataset 34 node entry represented img plotting embeddings nodesimilar graph autoencoder embeddings use plotsgcnembed method follows python def plotsgcnembedgraphnodenumemblabel nodedistancesquestionssgcnnodelevelembeddinggraphnodenumemb vgdfpddataframe vgdfpremisenodelendistances vgdfhypothesisquestions vgdfchebyshevdistancedistances vggnxfrompandasedgelistvgdfsourcehypothesistargetpremiseedgeattrchebyshevdistance plottervgglabel return vgg alternately 3 sub gcn embeddings also plotted using pyvis library also importing gcn graph autoencoder script following written start script python import spectralembeddingsvanillagcn vgcn import spectralembeddingschebgcn cgcn import spectralembeddingssplinegcn sgcn import spectralembeddingsgraphautoencoder graphae img sample analogous work similar architecture employed create graphattentionnetworks embeddings provides gat layer extracting kg embeddings well fully trainable tensorflow layer added neural module please refer repository library found contributing pull request welcome major change please open issue first discus would like change license mit support like work would like support please consider buying cup smile
Graphs;pytorch gnns package contains easytouse pytorch implementation graph attention easily imported used like using logistic regression sklearn two version supervised gnns provided one implemented pytorch implemented dgl pytorch note unsupervised version built upon implementation dgl version built upon example given author code package tong tzhao2ndedu tianwen tjiang2ndedu important dependency python368 pytorch101post2 dgl042 usage parameter gnnsunsupervised adjmatrix scipysparsecsrmatrix adjacency matrix graph nonzero entry indicates edge number nonzero entry indicates number edge two node feature numpyndarray optional 2dimension np array store given raw feature node ith row raw feature vector node raw feature given onehot degree feature used label list 1d numpyndarray optional class label node used supervised learning supervised bool optional default false whether use supervised learning model gat graphsage default gat gnn model used graphsage graphsage gat graph attention network nlayer int optional default 2 number layer gnn embsize int optional default 128 size node embeddings learnt randomstate int optional default 1234 random seed device cpu cuda auto default auto device use epoch int optional default 5 number epoch training batchsize int optional default 20 number node per batch training lr float optional default 07 learning rate unsuplosstype margin normal default margin loss function used unsupervised learning margin hinge loss margin 3 normal unsupervised loss function described paper graphsage printprogress bool optional default true whether print training progress example usage detailed example usage unsupervised gnns different setting cora dataset found exampleusagepy run unsupervised graphsage cuda python gnnsunsupervised import gnn gnn gnnadjmatrix featuresrawfeatures supervisedfalse modelgraphsage devicecuda train model gnnfit get node embeddings trained model embs gnngenerateembeddings todo doc example supervised gnns added soon
Graphs;404 found
Graphs;img srcdocssourcestaticlogopng altthe logo paddle graph learning pgl width320 pypi latest quick 中文readmezhmd breaking news pgl v12 20201120 pgl team proposed new unified message passing model unimp achieved state art three task ogb leaderboards find code hereogbexamplesnodeproppredunimp pgl team proposed twostage recall ranking model based erniesage first place competition coorganized coling pgl team worked hard develop open course graph neural network gnn help getting started graph neural network seven day detail found pgl v11 2020429 find erniesage novel model modeling text graph structure introduction hereexampleserniesage pgl open graph example find hereogbexamples add newly graph level operator like graphpooling graph level prediction relase pglke toolkit hereexamplespglke including classical knowledge graph embedding algorithm like transe transr rotate paddle graph learning pgl efficient flexible graph learning framework based img srcdocssourcestaticframeworkofpglenpng altthe framework paddle graph learning pgl width800 newly released pgl support heterogeneous graph learning walk based paradigm messagepassing based paradigm providing metapath sampling message passing mechanism heterogeneous graph furthermor newly released pgl also support distributed graph storage distributed training algorithm distributed deep walk distributed graphsage combined paddlepaddle deep learning framework able support graph representation learning model graph neural network thus framework wide range graphbased application highlight efficiency support scattergather lodtensor message passing one important benefit graph neural network compared model ability use nodetonode connectivity information coding communication node cumbersome pgl adopt message passing paradigm similar help build customize graph neural network easily user need write send recv function easily implement simple gcn shown following figure first step send function defined edge graph user customize send function send message source target node second step recv function responsible aggregating message together different source img srcdocssourcestaticmessagepassingparadigmpng altthe basic idea message passing paradigm width800 shown left following figure adapt general userdefined message aggregate function dgl us degree bucketing method combine node degree batch apply aggregate function batch serially pgl udf aggregate function organize message taking message variable length sequence utilize feature lodtensor paddle obtain fast parallel aggregation img srcdocssourcestaticparalleldegreebucketingpng altthe parallel degree bucketing pgl width800 user need call sequenceops function provided paddle easily implement efficient message aggregation example using sequencepool sum neighbor message python import paddlefluid fluid def recvmsg return fluidlayerssequencepoolmsg sum although dgl kernel fusion optimization general sum max aggregate function scattergather complex userdefined function degree bucketing algorithm serial execution degree bucket cannot take full advantage performance improvement provided gpu however operation pgl lodtensorbased message performed parallel fully utilize gpu parallel optimization experiment pgl reach 13 time speed dgl complex userdefined function even without scattergather optimization pgl still excellent performance course still provide buildin scatteroptimized message aggregation function performance test following gnn algorithm tesla v100sxm216g running 200 epoch get average speed report accuracy test dataset without early stoppping dataset model pgl accuracy pgl speed epoch time dgl 030 speed epoch time cora gcn 8175 00047s 00045s cora gat 835 00119s 00141s pubmed gcn 792 00049s 00051s pubmed gat 77 00193s00144s citeseer gcn 702 00045 00046s citeseer gat 688 00124s 00139s use complex userdefined aggregation like aggregate neighbor feature lstm ignoring order recieved message optimized messagepassing dgl forced degenerate degree bucketing scheme speed performance much slower one implemented pgl performance may various different scale graph experiment pgl reach 13 time speed dgl dataset pgl speed epoch time dgl 030 speed epoch time speed cora 00186s 01638s 880x pubmed 00388s 05275s 1359x citeseer 00150s 01278s 852x highlight flexibility natively support heterogeneous graph learning graph conveniently represent relation thing real world category thing relation thing various therefore heterogeneous graph need distinguish node type edge type graph network pgl model heterogeneous graph contain multiple node type multiple edge type describe complex connection different type support meta path walk sampling heterogeneous graph img srcdocssourcestaticmetapathsamplingpng altthe metapath sampling heterogeneous graph width800 left side figure describes shopping social network node two category user good relation user user user good good good right figure simple sampling process metapath input metapath upu userproductuser find following result img srcdocssourcestaticmetapathresultpng altthe metapath result width320 basis introducing word2vec method support learning metapath2vec algorithm heterogeneous graph representation support message passing mechanism heterogeneous graph img srcdocssourcestatichimmessagepassingpng altthe message passing mechanism heterogeneous graph width800 different node type heterogeneous graph message delivery also different shown left five neighbor belonging two different node type shown right figure node belonging different type need aggregated separately message delivery merged final message update target node basis pgl support heterogeneous graph algorithm based message passing gatne algorithm largescale support distributed graph storage distributed training algorithm case largescale graph learning need distributed graph storage distributed training support shown following figure pgl provided general solution largescale training adopted distributed parameter server support large scale distributed embeddings lightweighted distributed storage engine easily set large scale distributed training algorithm mpi cluster img srcdocssourcestaticdistributedframepng altthe distributed frame pgl width800 model zoo following graph learning model implemented framework find examplesexamples model feature erniesageexampleserniesage ernie sample aggregate text graph gcn graph convolutional neural network gat graph attention network graphsage largescale graph convolution network based neighborhood sampling unsupgraphsage unsupervised graphsage line representation learning based firstorder secondorder neighbor deepwalk representation learning dfs random walk metapath2vec representation learning based metapath node2vec representation learning combined dfs bfs struct2vec representation learning based structural similarity sgc simplified graph convolution neural network ge graph represents learning method node feature dgi unsupervised representation learning based graph convolution network gatne representation learning heterogeneous graph based messagepassing model consists three part namely graph representation learning graph neural network heterogeneous graph learning also divided graph representation learning graph neural network system requirement pgl requires paddle 16 cython pgl support python 2 3 installation simply install via pip sh pip install pgl team pgl developed maintained nlp paddle team baidu email nlpgnnatbaiducom license pgl us apache license 20
Graphs;dcnn implementation diffusionconvolutional neural 1 theano lasagne installation git clone cd dcnn usage node classification cora python clientrun modelnodeclassification datacora graph classification nci1 python clientrun modelgraphclassification datanci1 code structure client client code running command line parserpy par command line args configuration parameter runpy run experiment data example datasets python dcnn library datapy dataset parser layerspy lasagne internals dcnn layer modelspy userfacing endtoend model provide scikitlearnlike interface paramspy simple container configuration parameter utilpy misc utility function reference 1 atwood james towsley diffusionconvolutional neural network advance neural information processing system
Graphs;node2vec python3 implementation node2vec algorithm aditya grover jure leskovec vid kocijan node2vec scalable feature learning network grover j leskovec acm sigkdd international conference knowledge discovery data mining kdd installation pip install node2vec usage python import networkx nx node2vec import node2vec create graph graph nxfastgnprandomgraphn100 p05 precompute probability generate walk window work workers1 node2vec node2vecgraph dimensions64 walklength30 numwalks200 workers4 use tempfolder big graph embed node model node2vecfitwindow10 mincount1 batchwords4 keywords acceptable gensimword2vec passed dimension worker automatically passed node2vec constructor look similar node modelwvmostsimilar2 output node name always string save embeddings later use modelwvsaveword2vecformatembeddingfilename save model later use modelsaveembeddingmodelfilename embed edge using hadamard method node2vecedges import hadamardembedder edgesembs hadamardembedderkeyedvectorsmodelwv look embeddings fly pas normal tuples edgesembs1 2 output array 575068220e03 110937878e02 376693785e01 269105062e02 dtypefloat32 get edge separate keyedvectors instance use caution could huge big network edgeskv edgesembsaskeyedvectors look similar edge time tuples must sorted str edgeskvmostsimilarstr1 2 save embeddings later use edgeskvsaveword2vecformatedgesembeddingfilename parameter node2vecnode2vec node2vec constructor 1 graph first positional argument networkx graph node name must integer string output model always string 2 dimension embedding dimension default 128 3 walklength number node walk default 80 4 numwalks number walk per node default 10 5 p return hyper parameter default 1 6 q inout parameter default 1 7 weightkey weighted graph key weight attribute default weight 8 worker number worker parallel execution default 1 9 samplingstrategy node specific sampling strategy support setting node specific q p numwalks walklength use key exactly set use global one passed object initialization 10 quiet boolean controlling verbosity default false 11 tempfolder string path pointing folder save shared memory copy graph supply working graph big fit memory algorithm execution 12 seed seed random number generator default none deterministic result obtained seed set workers1 node2vecfit method accepts key word argument acceptable gensimword2vec node2vecedgeembedder edgeembedder abstract class concrete edge embeddings class inherit class averageembedder hadamardembedder weightedl1embedder weightedl2embedder practical definition could found table 1 notice edge embeddings defined pair node connected even node constructor 1 keyedvectors gensimmodelskeyedvectors instance containing node embeddings 2 quiet boolean controlling verbosity default false edgeembeddergetitemitem method better known edgeembedderitem 1 item tuple consisting 2 node keyedvectors passed constructor return embedding edge edgeembedderaskeyedvectors method return gensimmodelskeyedvectors instance possible node pair sorted manner string example node 1 2 3 key 1 1 1 2 1 3 2 2 2 3 3 3 caveat node name input graph must string ints parallel execution working window joblib known issue run nonparallel window pas workers1 node2vecs constructor todo x parallel implementation walk generation parallel implementation probability precomputation
Graphs;ampligraphdocsimgampligraphlogotransparent300png documentation join conversation docsimgslacklogopng open source library based tensorflow predicts link concept knowledge graph ampligraph suite neural machine learning model relational learning branch machine learning deal supervised learning knowledge graph use ampligraph need discover new knowledge existing knowledge graph complete large knowledge graph missing statement generate standalone knowledge graph embeddings develop evaluate new relational model ampligraphs machine learning model generate knowledge graph embeddings vector representation concept metric space docsimgkglpstep1png combine embeddings modelspecific scoring function predict unseen novel link docsimgkglpstep2png key feature intuitive apis ampligraph apis designed reduce code amount required learn model predict link knowledge graph gpuready ampligraph based tensorflow designed run seamlessly cpu gpu device speedup training extensible roll knowledge graph embeddings model extending ampligraph base estimator module ampligraph includes following submodules datasets helper function load datasets knowledge graph model knowledge graph embedding model ampligraph contains transe distmult complex hole conve convkb come evaluation metric evaluation protocol ass predictive power model discovery highlevel convenience apis knowledge discovery discover new fact cluster entity predict near duplicate installation prerequisite linux macos window python 37 provision virtual environment create activate virtual environment conda conda create name ampligraph python37 source activate ampligraph install tensorflow ampligraph built tensorflow 1x install pip conda cpuonly pip install tensorflow115220 conda install tensorflow1152200 gpu support pip install tensorflowgpu115220 conda install tensorflowgpu1152200 install ampligraph install latest stable release pip pip install ampligraph instead want recent development version clone repository install source local working copy latest commit develop branch code snippet install library editable mode e git clone cd ampligraph pip install e sanity check python import ampligraph ampligraphversion 140 predictive power evaluation mrr filtered ampligraph includes implementation transe distmult complex hole conve convkb predictive power reported compared stateoftheart result literature detail available fb15k237 wn18rr yago310 fb15k wn18 literature best 035 048 049 084 095 transe ampligraph 031 022 051 063 066 distmult ampligraph 031 047 050 078 082 complex ampligraph 032 051 049 080 094 hole ampligraph 031 047 050 080 094 conve ampligraph 026 045 030 050 093 conve 1n ampligraph 032 048 040 080 095 convkb ampligraph 023 039 030 065 080 sub timothee lacroix nicolas usunier guillaume obozinski canonical tensor decomposition knowledge base completion international conference machine learning 2869–2878 2018 br kadlec rudolf ondrej bajgar jan kleindienst knowledge base completion baseline strike back arxiv preprint arxiv170510744 2017 sub sub result computed assigning worst rank positive case tie although conservative approach published literature may adopt evaluation protocol assigns best rank instead sub documentation documentation available project documentation built local working copy cd doc make clean autogen html contribute see ampligraph documentation cite like ampligraph use project starring project github github instead use ampligraph academic publication cite miscampligraph author luca costabello sumit pai chan le van rory mcgrath nicholas mccarthy pedro tabacof title ampligraph library representation learning knowledge graph month mar year 2019 doi 105281zenodo2595043 url license ampligraph licensed apache 20 license
Graphs;grm graph relevance miner img height35 repository contains implementation grm stierle et al cite work please use articlestierle2020grm title technique determining relevance score process activity using graphbased neural network journal decision support system page 113511 volume 144 year 2021 issn 01679236 doi url author matthias stierle sven weinzierl maximilian harl martin matzner implementation model based ggnn implementation li et al find grm extract relevance score process activity given performance measure event log use grm first preprocess event log included preprocessing function use input data grm structure repository grm initpy init file grm ggnnpy base implementation ggnn ggnnsparsepy subclass ggnn enable use sparsity matrix grmpy subclass ggnnsparse implement graph relevance miner grm preprocessingpy script containing preprocessing function event log later use grm utilpy utility function used grm preprocessing eval baseline dir contains baseline used evaluation predictivequality dir application baseline model bilstm rf xgboost event log casestudy dir contains script used case study data dir data used grm predictivequality dir script apply grm certain event log mostleast dir file data mostleast relevant activity removed result dir result evaluation run significance testing including r script used util dir utility get information data characteristic artefact metric gitignore gitignore file licence licence readmemd readme file environmentyml environment file installation install anaconda conda env create f environmentyml conda env update file environmentyml prune conda activate grm track result experiment used want execute script eval directoryeval configure mlflow first pip install mlflow databricks configure usage main function implemented class grm grmpygrmgrmpy use grm event log need preprocess data train model making prediction example usage found hereevalsimpletestpy load event log use event log grm preprocess data convert trace log process instance graph method importdatagrmpreprocessingpy provided load x csv file build model build model create instance grmgrmgrmpy receives especially training data list activity contained log obtained getactivitiesgrmutilpy needed subsample event log may contain distinct activity event log need known model training restore model course also restore model already trained simply done providing path model file creation grm instance optional argument restoremodel evaluate model extensive evaluation comparison grm bilstms xgboost randomforest important note bilstm model implemented tensorflow 220 grm tensorflow 130 data used implementation found evaldata utilitiy script used get metric log characteristic evalutil used data collected company event log evaluation use 10 fold cross validation evaluated metric aucroc specificity sensitivity
Graphs;gmi graphical mutual information graph representation learning via graphical mutual information maximization peng z huang w luo et al www 2020 overview note propose two variant gmi paper one gmimean gmiadaptive since gmimean often outperforms gmiadaptive see experiment paper give pytorch implementation gmimean make gmi practical provide alternative solution compute fmi solution still ensures effectiveness gmi improves efficiency greatly repository organized follows data includes three benchmark datasets model contains implementation gmi pipeline gmipy logistic regression classifier logregpy layer contains implementation standard gcn layer gcnpy bilinear discriminator discriminatorpy meanpooling operator avgneighborpy utils contains necessary processing tool processpy better understand code recommend could read code dgipetar advance besides could optimize code based need display easytoread form requirement pytorch 120 python 36 usage python executepy cite please cite paper make advantage gmi research inproceedings peng2020graph titlegraph representation learning via graphical mutual information maximization authorpeng zhen huang wenbing luo minnan zheng qinghua rong yu xu tingyang huang junzhou booktitleproceedings web conference year2020
Graphs;reproducible generative learning quantify reproducibility graph neural network using generative learning please contact mohammedamineghgmailcom inquiry thanks reproducible generative learning pipelinemainfigurepng introduction work accepted prime workshop miccai 2021 investigating quantifying reproducibility graph neural network predictive medicine mohammed amine gharsallaoui furkan tornaci islem rekik basira lab faculty computer informatics istanbul technical university istanbul turkey abstract graph neural network gnns gained unprecedented attention many domain including dysconnectivity disorder diagnosis thanks high performance tackling graph classification task despite large stream gnns developed recently prior effort invariably focus boosting classification accuracy ignoring model reproducibility interpretability vital pinning disorderspecific biomarkers although le investigated discriminativeness original input feature biomarkers reflected learnt weight using gnn give informative insight reliability intuitively reliability given biomarker emphasized belongs set top discriminative region interest roi using different model therefore define first axis work emphreproducibility across model evaluates commonality set top discriminative biomarkers pool gnns task mainly answer question emphhow likely two model congruent term respective set top discriminative biomarkers second axis research work investigate emphreproducibility generated connectomic datasets addressed answering question emphhow likely would set top discriminative biomarkers trained model groundtruth dataset consistent predicted dataset generative learning paper propose reproducibility assessment framework method quantifying commonality gnnspecific learnt feature map across model complement explanatory approach gnns provide new way ass predictive medicine via biomarkers reliability evaluated framework using four multiview connectomic datasets healthy neurologically disordered subject five gnn architecture two different learning mindset conventional training sample resourceful b fewshot training random sample frugal code code implemented using python 38 anaconda window 10 installation anaconda installattion go download version system used python 38 64bit window 10 install platform create conda environment typing conda create –n envreproducibility pip python38 dependency installattion copy paste following command install package cpu version sh conda activate envreproducibility conda install pytorch140 torchvision050 cpuonly c pytorch pip install scikitlearn pip install matplotlib pip install torchscatterlatestcpu f pip install torchsparselatestcpu f pip install torchclusterlatestcpu f pip install torchsplineconvlatestcpu f pip install torchgeometric pip install annoy pip install fbpca instruction cpu installation want gpu installation please visit optional pytorchgeometric’s web page description installing gpu version code check version dependency availability gpu everything configured correctly utilize gpu automatically data format case want use framework input dataset list numpy array numpy array size nr nr nv nr nv number region view respectively provided within code python file handledatasimulatedatapy simulate data specify number subject number view number region simulating dataset predict data via generative learning using file topoganmaintopoganpy input data vectorized using generative learning vectorize data use handledatavectorizepy output generative learning vectorized format restore matrix format generated data use file handledatacollectgeneratedpy predicting dataset using generative learning put repository real generated path reproducibilitydata run reproducibility framework obtaining real generated datasets run gnn model running file reproducibilitydemopy open terminal reproducibility directory type sh conda activate envreproducibility python demopy gnn model gnn model included model paper diffpool sagpool gat gunets gcn main component code component content handledata includes file required simulate vectorize reshape data reproducibility contains gnn code reproducibility framework implementation topogan contains code file generative learning technique example result reproducibility scoresresultsfigurepng figure demonstrates example output population 80 subject subject 2 view represented 35 35 matrix computed reproducibility score 5 gnn model using two training setting crossvalidation fewshot view display score using real generated datasets relevant reference alaa bessadok mohamed ali mahjoub islem rekik brain multigraph prediction using topologyaware adversarial graph neural network medical image analysis 72 2021 nicolas george islem mhiri islem rekik identifying best datadriven feature selection method boosting reproducibility classification task pattern recognition 101 2020 youtube video paper please cite following paper using framework latex inproceedingsgharsallaoui2021 titleinvestigating quantifying reproducibility graph neural network predictive medicine authorgharsallaoui mohammed amine tornaci furkan rekik islem booktitleinternational workshop predictive intelligence medicine pages104116 year2021 organizationspringer
Graphs;graph convolutional network gcn pytorch implementation paper code folder contains gcn implementation based dgl three training script available run cora citeseer pubmed result run following bash python traincorapy cora 0823 paper 0815 citeseer 0705 paper 0703 pubmed 0800 paper 0790
Graphs;ipo prediction graph convolutional neural network repository demo show process forming data science project around data stored tigergraph graph database us giraffle gradle install gsql query onto database instance call rest endpoint pytigergraph python wrapper written make accessing tigergraph database easy feed data recieve database graph convolutional neural network due various simplification computational constraint gcn doesnt perform great classifying company ipo serve good demo done datasets setup tigergraph cloud instance setup version server head create free account walk creating cloud database instance make sure choose crunchbase knowledge graph template get started getting gradle installed setup follow direction get gradle installed machine done need get ssl certification cloud instance run bash openssl sclient connect hostnameitgcloudio14240 devnull 2 devnull openssl x509 text certtxt installing python package needed need install quite python package good news easily done via pip pip3 install pytigergraph pip3 install torch pip3 install dgl pip3 install networkx ready try notebook find pyscripts credit pimg altpicture parker erickson height150px alignright hspace20px vspace20pxp demotutorial written parker erickson student university minnesota pursuing b computer science interest include graph database machine learning travelling playing saxophone watching minnesota twin baseball feel free reach find linkedin github medium email parkererickson30gmailcomparkererickson30gmailcom gcn resource dgl documentation gcn paper kipf welling rgcn paper notebook adapted
Graphs;cabgraphembeddings cab graph lower dimensional space team brett hagan ben smith matt lane problem definition large amount yellow cab data new york city available public trip record contains latitude longitude pickup dropoff location date time pickup dropoff additional information construct grid nyc create undirected graph edge represents trip two point grid ultimate goal feed graph graph constituting day worth trip lstm network order predict next days’ trip test selection different node embedding method graph order feed information lstm different node embedding method make use deep walk v2v vector embedding line large scale information network embedding motivation interesting effect different embedding method upon prediction task lstm network interest ml community large need employ deep learning machine learning technique graph structured data continues increase effectiveness various node embedding method representing data accurately manner fed directly existing deep learning model increasing importance ii think going used ie application area addition general relevance deep learning graph particular problem would interest dispatcher yellow cab rival rideshare industry ability accurately predict flow traffic appropriately place cab location higher number ride requested could lead significant financial benefit literature review reading examine provide context background please put citation articleblog post full citation graphnode embedding paper deep walk perozzi b alrfou r skiena 2014 deepwalk proceeding 20th acm sigkdd international conference knowledge discovery data mining doi10114526233302623732 line largescale information network embedding tang j qu wang zhang yan j mei q 2015 line proceeding 24th international conference world wide web doi10114527362772741093 random walk restart stanford network analysis project embeddings lecture v2v vector embeddings graph application nguyen tirthapura 2018 v2v vector embedding graph application 2018 ieee international parallel distributed processing symposium workshop ipdpsw doi101109ipdpsw201800182 network information pytorch geometric documentation pytorch data loading pytorch lstm dataset data use collecting new data nyc taxi limousine commission make record yellow cab taxi trip across city publicly available year specific attribute recorded trip vary individual year trip arranged csv file month
Graphs;physion evaluating physical prediction vision human machine daniel bear elia wang damian mrowca felix j binder hsiaoyu fish tung rt pramod cameron holdaway sirui tao kevin smith fanyun sun li feifei nancy kanwisher joshua b tenenbaum daniel lk yamins judith e fan official implementation particlebased model gns dpinet physion dataset code built based original implementation dpinet contact sfish0101gmailcom fish tung paper gns dpinet learning simulate complex physic graph network alvaro sanchezgonzalez jonathan godwin tobias pfaff rex ying jure leskovec peter w battaglia learning particle dynamic manipulating rigid body deformable object fluid yunzhu li jiajun wu rus tedrake joshua b tenenbaum antonio torralba demo rollout learned model left ground truth right prediction domino imgsdominoesgif roll imgsrollingslidinggif contain imgscontainmentgif drape imgsclothsagginggif installation clone repo git clone cd dpinetp git submodule update init recursive install dependency using conda conda user provide installation script bash scriptscondadepssh pip install pyyaml use tensorboard training visualization pip install tensorboardx pip install tensorboard install binvox use binvox transform object mesh particle use binvox please download binvox put bin include path export pathpathpwdbin might need chmod 777 binvox order execute file setup data path open pathsyaml write path set different path different machine different user name preprocessing physion dataset 1 need convert mesh scene particle scene line generate separate folder dpidatadir specified pathsyaml hold data particlebased model bash runpreprocessingtdwcheapsh scenarioname mode eg bash runpreprocessingtdwcheapsh domino train scenarioname one following domino collide support link contain roll drop drape mode either train test visualize original video generated particle scene python preprocessingtdwcheappy scenario dominones mode train visualization 1 video generated folder vispy 2 try generate traintxt validtxt file indicates trial want use training validaiton python createtrainvalidpy also design specific split put trial name one txt file 3 evalution redhitsyellow prediciton get binary redhitsyellow label txt file test dataset bash rungetlabeltxtsh scenarioname test generate folder called label outputfolder dpidatadir folder scenario corresponding label file called scenarionametxt training ok ready start training modelsyou use following command train scratch train gns bash scriptstraingnssh scenarioname gpuid scenarioname one following domino collide support link contain roll drop drape train dpi bash scriptstraindpish scenarioname gpuid implementation different original dpi paper 2 way 1 model take input relative position opposed absolute position 2 model trained injected noise two feature suggested gns paper found critcial model generalize well unseen scene train multiple scenario also train one scenario adding different scenario argument dataf python trainpy env tdwdominoes modelname gns logperiter 1000 trainingfpt 3 ckpperiter 5000 floorcheat 1 dataf domino collide support link roll drop contain drape outf allgns visualize training progress model model log saved outdirdumpdumptdwdominoes visualize training progress using tensorboard tensorboard logdir modelnamelog evaluation evaluate gns bash scriptsevalgnssh trainscenarioname epoch iter test scenarioname gpuid get prediction txt file evalevaltdwdominoesmodelname eg testdrapetxt contains result testing model drape scenario visualize result additional argument vi 1 evaluate gnsransac bash scriptsevalgnsransacsh trainscenarioname epoch iter test scenarioname gpuid evaluate dpi bash scriptsevaldpish trainscenarioname epoch iter test scenarioname gpuid evaluate model trained multiple scenario provide example evaluating arbitray model trained scenario bash evalallgnssh epoch iter test scenarioname gpuid bash evalalldpish epoch iter test scenarioname gpuid bash evalallgnsransacsh epoch iter test scenarioname gpuid visualize trained model provide example visualizing rollout result trained arbitray model bash visgnssh epoch iter test scenarioname gpuid find visualization evalevaltdwdominoesmodelnametestscenario see gif original rgb video another gif sidebyside comparison gt particle scene predicted particle scene citing physion find codebase useful research please consider citing inproceedingsbear2021physion titlephysion evaluating physical prediction vision human machine author daniel bear elia wang damian mrowca felix j binder hsiaoyu fish tung r pramod cameron holdaway sirui tao kevin smith fanyun sun li feifei nancy kanwisher joshua b tenenbaum daniel l k yamins judith e fan url archiveprefix arxiv eprint 210608261 year 2021
Graphs;propositional satisfiability problem sat go neural deep wed like use graph neural network graph transformer solve sat general constraint satisfaction problem repo pytorch implementation paper transformer transformerbased machine learning fast sat solver logic general question convert csp ksat sat n binary variable constraint constraint associated ktuple distinct variable k depending constraint along subset img k allowed assignment ktuple variable example graph coloring rather whether given graph g xcolored viewed csp n vertex identify vertex set img n group img img lceil log2 x big rceil variable img 1dots n constraint group img stating assignment img binary encoding number range img 1 two connected vertex img j constraint group img xj stating img neq xj csp satisfiable iff g xcolorable order convert binary csp sat instance replace constraint corresponding cnf continuing example img img constraint img neq xj realize img vee neg xj wedge neg xi vee xj resulting cnf satisfiable iff csp use standard reduction convert cnf 3cnf wish also handle csps binary ie variable binary rather finite domain idea similar described coloring csp left reader extension satisfiability modulo smt construction reference transformer graph neural network pytorch geometric implementation graph attention learning local search heuristic boolean satisfiability pdp framework neural constraint satisfaction solving
Graphs;gat modified graph attention network veličković et al iclr 2018 imgtensorboardpng overview provide implementation graph attention network gat layer tensorflow along minimal execution example cora dataset repository organised follows data contains necessary dataset file cora model contains implementation gat network gatpy pretrained store model checkpoint utils contains implementation attention head along experimental sparse version layerspy preprocessing subroutine processpy sparse version experimental sparse version also available working batch size equal 1 sparse model may found modelsspgatpy may execute full training run sparse model cora executepy command line argument optional argument h help show help message exit sparse use sparse operation reduce memory consumption epoch epoch number epoch lr lr learning rate patience patience early stopping l2coef l2coef l2 regularization coefficient hidunits hidunits hidunits number hidden unit per attention head layer nheads nheads nheads number attention head residual use residual connection attentiondrop attentiondrop dropout probability attention layer edgeattrdirectory edgeattrdirectory directory storing edge attribute npz file store sparse adjacency matrix nodefeaturespath nodefeaturespath csv file path node feature labelpath labelpath csv file path ground truth label logdirectory logdirectory directory logging tensorboard trainratio trainratio ratio data used training rest used testing data preparation edgeattrdirectory directory contains multiple npz file npz file store scipy sparse matrix n n adjacency matrix edge attribute reference nodefeaturespath csv file containing node attribute first row contains feature name node ordering edge attribute adjacency matrix labelpath csv file containing node label first row must id label id corresponds node id zero indexing label string example train bash git clone cd data curl ethtargz md5 62aef8b070d7be703152419f16e830d1 tar zxvf ethtargz cd python executepy sparse epoch 100000 lr 0008 patience 50 l2coef 0005 hidunits 5 nheads 2 1 residual attentiondrop 00 edgeattr dataethedges nodefeaturespath dataethnodefeaturescsv logdirectory tmptensorboard labelpath dataethlabelcsv tensorboard logdirtmptensorboard run tensorboard tensorboard running navigate web browser localhost6006 view tensorboard example load model bash curl pretrainedtargz md5 041de9eb6e7dcd4ca74267c30a58ad70 tar zxvf pretrainedtargz python loadmodelpy sparse hidunits 5 nheads 2 1 residual edgeattrdataethedges nodefeaturespath dataethnodefeaturescsv labelpath dataethlabelcsv trainratio 08 modelpath pretrainedmodtestckpt print test loss 0579380989074707 test accuracy 086021488904953 dependency script tested running python 352 following package installed along dependency numpy1141 scipy100 networkx21 tensorflowgpu160 pandas0234 addition cuda 90 cudnn 7 used reference make advantage gat model research please cite following manuscript article velickovic2018graph titlegraph attention network authorvelivckovic petar cucurull guillem casanova arantxa romero adriana lio pietro bengio yoshua journalinternational conference learning representation year2018 noteaccepted poster license mit
Graphs;multi source random walk graph traversal repository contains code paper multisourcenode2vec algorithm please refer detailed description algorithm code description work progress
Graphs;graph neural network method model paper note gcn iclr 2017semisupervised classification graph convolutional 【graph neural graphsage nip 2017inductive representation learning large 【graph neural gat iclr 2018graph attention run example 1 clone repo use pip install r requirementstxt setup operating environment 2 run following command bash cd gnn python rungcncorapy python rungraphsagecorapy python rungatcorapy disscussiongroup 公众号：浅梦的学习笔记 wechat id deepctrbot
Graphs;knowledge base completion kbc fork codebase reproduces result canonical tensor decomposition knowledge base icml 2018 adapted compute graph embedding claimskg installation create conda environment pytorch cython scikitlearn conda create name kbcenv python37 source activate kbcenv conda install file requirementstxt c pytorch install kbc package environment python setuppy install datasets download claimskg datasets go kbckbc folder run bash mkdir srcdata cd srcdata wget tar xjf ckgtbz wget tar xjf ckgetbz wget tar xjf ckgekwtbz datasets download add package data folder running bash python kbcprocessdatasets create file required compute filtered metric kbcdata training model reproduce model training following command please repeat ckg ckgekw python kbclearn dataset ckge model cp rank 50 optimizer adagrad learningrate 1e1 batchsize 150 regularizer n3 reg 5e3 maxepochs 30 valid 10 training produce pickle file model kbcmodels directory pretrained model wish may download pretrained model place kbcmodels ckg ckge ckgekw evaluating compute link prediction performance model may use bash python kbcevaluate pathtodataset pathtomodelpickle datasets generated processdatasets kbcdata license kbc ccbync licensed found license file fork
Graphs;recommenders documentation whats new january 13 2022 new release recommenders codebase migrated tensorflow version 26 27 spark version 3 addition change dependency extra installed pip see guiderecommendersreadmemdoptionaldependencies also made improvement code ci cd pipeline starting release 060 recommenders available pypi installed using pip find pypi page find package documentation introduction repository contains example best practice building recommendation system provided jupyter notebook example detail learning five key task prepare dataexamples01preparedata preparing loading data recommender algorithm modelexamples00quickstart building model using various classical deep learning recommender algorithm alternating least square extreme deep factorization machine evaluateexamples03evaluate evaluating algorithm offline metric model select optimizeexamples04modelselectandoptimize tuning optimizing hyperparameters recommender model operationalizeexamples05operationalize operationalizing model production environment azure several utility provided recommendersrecommenders support common task loading datasets format expected different algorithm evaluating model output splitting trainingtest data implementation several stateoftheart algorithm included selfstudy customization application see recommenders detailed overview repository please see document wiki getting started please see setup guidesetupmd detail setting machine locally data science virtual machine azure databrickssetupmdsetupguideforazuredatabricks installation recommenders package tested python version 36 37 currently support version 38 recommended install package dependency inside clean environment set local machine install core utility cpubased algorithm dependency 1 ensure software required compilation python library installed linux supported adding bash sudo aptget install buildessential libpythonversion version 36 37 appropriate window need microsoft c build 2 create conda virtual environment see setup guidesetupmd detail 3 within created environment install package bash pip install upgrade pip pip install upgrade setuptools pip install recommendersexamples 4 register conda virtual environment jupyter bash python ipykernel install user name myenvironmentname displayname python reco 5 start jupyter notebook server bash jupyter notebook 6 run sar python cpu movielensexamples00quickstartsarmovielensipynb notebook 00quickstart folder make sure change kernel python reco additional option install package support gpu spark etc see guiderecommendersreadmemd note alternating least square alsexamples00quickstartalsmovielensipynb notebook require pyspark environment run please follow step setup guidesetupmddependenciessetup run notebook pyspark environment deep learning algorithm recommended use gpu machine follow step setup guidesetupmddependenciessetup set nvidia library note dsvm user please follow step dependency setup set pyspark environment variable linux macossetupmddependenciessetup troubleshooting dsvmsetupmdtroubleshootingforthedsvm section encounter issue docker another easy way try recommenders repository get started quickly build docker imagestoolsdockerreadmemd suitable different environment algorithm table list recommender algorithm currently available repository notebook linked example column quick start showcasing easy run example algorithm deep dive explaining detail math implementation algorithm algorithm type description example alternating least square al collaborative filtering matrix factorization algorithm explicit implicit feedback large datasets optimized scalability distributed computing capability work pyspark environment quick startexamples00quickstartalsmovielensipynb deep diveexamples02modelcollaborativefilteringalsdeepdiveipynb attentive asynchronous singular value decomposition a2svdsupsup collaborative filtering sequentialbased algorithm aim capture long shortterm user preference using attention mechanism work cpugpu environment quick startexamples00quickstartsequentialrecsysamazondatasetipynb cornacbayesian personalized ranking bpr collaborative filtering matrix factorization algorithm predicting item ranking implicit feedback work cpu environment deep diveexamples02modelcollaborativefilteringcornacbprdeepdiveipynb cornacbilateral variational autoencoder bivae collaborative filtering generative model dyadic data eg useritem interaction work cpugpu enviroment deep diveexamples02modelcollaborativefilteringcornacbivaedeepdiveipynb convolutional sequence embedding recommendation caser collaborative filtering algorithm based convolution aim capture user’s general preference sequential pattern work cpugpu enviroment quick startexamples00quickstartsequentialrecsysamazondatasetipynb deep knowledgeaware network dknsupsup contentbased filtering deep learning algorithm incorporating knowledge graph article embeddings providing news article recommendation work cpugpu enviroment quick startexamples00quickstartdknmindipynb deep diveexamples02modelcontentbasedfilteringdkndeepdiveipynb extreme deep factorization machine xdeepfmsupsup hybrid deep learning based algorithm implicit explicit feedback useritem feature work cpugpu environment quick startexamples00quickstartxdeepfmcriteoipynb fastai embedding dot bias fast collaborative filtering general purpose algorithm embeddings bias user item work cpugpu environment quick startexamples00quickstartfastaimovielensipynb lightfmhybrid matrix factorization hybrid hybrid matrix factorization algorithm implicit explicit feedback work cpu environment quick startexamples02modelhybridlightfmdeepdiveipynb lightgbmgradient boosting treesupsup contentbased filtering gradient boosting tree algorithm fast training low memory usage contentbased problem work cpugpupyspark environment quick start cpuexamples00quickstartlightgbmtinycriteoipynb deep dive pysparkexamples02modelcontentbasedfilteringmmlsparklightgbmcriteoipynb lightgcn collaborative filtering deep learning algorithm simplifies design gcn predicting implicit feedback work cpugpu enviroment deep diveexamples02modelcollaborativefilteringlightgcndeepdiveipynb geoimcsupsup hybrid matrix completion algorithm account user item feature using riemannian conjugate gradient optimization following geometric approach work cpu enviroment quick startexamples00quickstartgeoimcmovielensipynb gru4rec collaborative filtering sequentialbased algorithm aim capture long shortterm user preference using recurrent neural network work cpugpu enviroment quick startexamples00quickstartsequentialrecsysamazondatasetipynb multinomial vae collaborative filtering generative model predicting useritem interaction work cpugpu enviroment deep diveexamples02modelcollaborativefilteringmultivaedeepdiveipynb neural recommendation long shortterm user representation lstursupsup contentbased filtering neural recommendation algorithm recommending news article long shortterm user interest modeling work cpugpu enviroment quick startexamples00quickstartlsturmindipynb neural recommendation attentive multiview learning namlsupsup contentbased filtering neural recommendation algorithm recommending news article attentive multiview learning work cpugpu enviroment quick startexamples00quickstartnamlmindipynb neural collaborative filtering ncf collaborative filtering deep learning algorithm enhanced performance useritem implicit feedback work cpugpu enviroment quick startexamples00quickstartncfmovielensipynb neural recommendation personalized attention npasupsup contentbased filtering neural recommendation algorithm recommending news article personalized attention network work cpugpu enviroment quick startexamples00quickstartnpamindipynb neural recommendation multihead selfattention nrmssupsup contentbased filtering neural recommendation algorithm recommending news article multihead selfattention work cpugpu enviroment quick startexamples00quickstartnrmsmindipynb next item recommendation nextitnet collaborative filtering algorithm based dilated convolution residual network aim capture sequential pattern considers useritem interaction feature work cpugpu enviroment quick startexamples00quickstartsequentialrecsysamazondatasetipynb restricted boltzmann machine rbm collaborative filtering neural network based algorithm learning underlying probability distribution explicit implicit useritem feedback work cpugpu enviroment quick startexamples00quickstartrbmmovielensipynb deep diveexamples02modelcollaborativefilteringrbmdeepdiveipynb riemannian lowrank matrix completion rlrmcsupsup collaborative filtering matrix factorization algorithm using riemannian conjugate gradient optimization small memory consumption predice useritem interaction work cpu enviroment quick startexamples00quickstartrlrmcmovielensipynb simple algorithm recommendation sarsupsup collaborative filtering similaritybased algorithm implicit useritem feedback work cpu environment quick startexamples00quickstartsarmovielensipynb deep diveexamples02modelcollaborativefilteringsardeepdiveipynb shortterm longterm preference integrated recommender slirecsupsup collaborative filtering sequentialbased algorithm aim capture long shortterm user preference using attention mechanism timeaware controller contentaware controller work cpugpu environment quick startexamples00quickstartsequentialrecsysamazondatasetipynb multiinterestaware sequential user modeling sumsupsup collaborative filtering enhanced memory networkbased sequential user model aim capture user multiple interest work cpugpu environment quick startexamples00quickstartsequentialrecsysamazondatasetipynb standard vae collaborative filtering generative model predicting useritem interaction work cpugpu environment deep diveexamples02modelcollaborativefilteringstandardvaedeepdiveipynb surprisesingular value decomposition svd collaborative filtering matrix factorization algorithm predicting explicit rating feedback small datasets work cpugpu environment deep diveexamples02modelcollaborativefilteringsurprisesvddeepdiveipynb term frequency inverse document frequency tfidf contentbased filtering simple similaritybased algorithm contentbased recommendation text datasets work cpu environment quick staertexamples00quickstarttfidfcovidipynb vowpal wabbit vwsupsup contentbased filtering fast online learning algorithm great scenario user feature context constantly changing us cpu online learning deep diveexamples02modelcontentbasedfilteringvowpalwabbitdeepdiveipynb wide deep hybrid deep learning algorithm memorize feature interaction generalize user feature work cpugpu environment quick startexamples00quickstartwidedeepmovielensipynb xlearnfactorization machine fm fieldaware fm ffm hybrid quick memory efficient algorithm predict label useritem feature work cpugpu environment deep diveexamples02modelhybridfmdeepdiveipynb note supsup indicates algorithm inventedcontributed microsoft independent incubating algorithm utility candidate contribcontrib folder house contribution may easily fit core repository need time refactor mature code add necessary test algorithm type description example sarplus supsup collaborative filtering optimized implementation sar spark quick startcontribsarplusreadmemd algorithm comparison provide benchmark notebookexamples06benchmarksmovielensipynb illustrate different algorithm could evaluated compared notebook movielens dataset split trainingtest set 7525 ratio using stratified split recommendation model trained using collaborative filtering algorithm utilize empirical parameter value reported literature ranking metric use k10 top 10 recommended item run comparison standard nc6sv2 azure 6 vcpus 112 gb memory 1 p100 gpu spark al run local standalone mode table show result movielens 100k running algorithm 15 epoch algo map ndcgk precisionk recallk rmse mae rsup2sup explained variance alsexamples00quickstartalsmovielensipynb 0004732 0044239 0048462 0017796 0965038 0753001 0255647 0251648 bivaeexamples02modelcollaborativefilteringcornacbivaedeepdiveipynb 0146126 0475077 0411771 0219145 na na na na bprexamples02modelcollaborativefilteringcornacbprdeepdiveipynb 0132478 0441997 0388229 0212522 na na na na fastaiexamples00quickstartfastaimovielensipynb 0025503 0147866 0130329 0053824 0943084 0744337 0285308 0287671 lightgcnexamples02modelcollaborativefilteringlightgcndeepdiveipynb 0088526 0419846 0379626 0144336 na na na na ncfexamples02modelhybridncfdeepdiveipynb 0107720 0396118 0347296 0180775 na na na na sarexamples00quickstartsarmovielensipynb 0110591 0382461 0330753 0176385 1253805 1048484 0569363 0030474 svdexamples02modelcollaborativefilteringsurprisesvddeepdiveipynb 0012873 0095930 0091198 0032783 0938681 0742690 0291967 0291971 code conduct project adheres microsofts open source code conductcodeofconductmd order foster welcoming inspiring communtity contributing project welcome contribution suggestion contributing please see contribution guidelinescontributingmd build status test nightly build compute smoke integration test main principal branch staging development branch use pytest testing python utility recommendersrecommenders papermill notebooksexamples information testing pipeline please see test documentationtestsreadmemd dsvm build status following test run linux dsvm daily build type branch status branch status linux cpu main build staging build linux gpu main build staging build linux spark main build staging build window cpu main build staging build window gpu main build staging build window spark main build staging build related project microsoft ai find best practice project azure ai design pattern central repository nlp best best practice example nlp computer vision best best practice example computer vision forecasting best best practice example time series forecasting reference paper argyriou gonzálezfierro l zhang microsoft recommenders best practice productionready recommendation system www 2020 international world wide web conference taipei 2020 available online l zhang wu x xie argyriou gonzálezfierro j lian building productionready recommendation system scale acm sigkdd conference knowledge discovery data mining 2019 kdd 2019 2019 graham jk min wu microsoft recommenders tool accelerate developing recommender system recsys 19 proceeding 13th acm conference recommender system 2019 available online
Graphs;convolutional neural network graph fast localized spectral filtering michaël xavier pierre conference neural information processing system nip 2016 work interested generalizing convolutional neural network cnns lowdimensional regular grid image video speech represented highdimensional irregular domain social network brain connectomes word embedding represented graph present formulation cnns context spectral graph theory provides necessary mathematical background efficient numerical scheme design fast localized convolutional filter graph importantly proposed technique offer linear computational complexity constant learning complexity classical cnns universal graph structure experiment mnist 20news demonstrate ability novel deep learning system learn local stationary compositional feature graph inproceedingscnngraph title convolutional neural network graph fast localized spectral filtering author defferrard michael bresson xavier vandergheynst pierre booktitle advance neural information processing system nip year 2016 archiveprefix arxiv eprint 160609375 url resource pdf available arxiv nip epfl related poster slide video code arxiv nip epfl poster slide video code compilation compile latex source pdf make run make clean remove temporary file make arxivzip prepare archive uploaded arxiv figure figure figuresfigures folder pdfs generated make figure peerreview paper got metareviewreviewmetareviewhtm based six reviewsreviewreviewshtm rebuttalreviewrebuttaltxt based review also nip
Graphs;graph convolutional network pytorch pytorch implementation graph convolutional network gcns semisupervised classification 1 highlevel introduction gcns see thomas kipf graph convolutional 2016 graph convolutional networksfigurepng note subtle difference tensorflow implementation pytorch reimplementation reimplementation serf proof concept intended reproduction result reported 1 implementation make use cora dataset 2 installation python setuppy install requirement pytorch 04 05 python 27 36 usage python trainpy reference 1 kipf welling semisupervised classification graph convolutional network 2 sen et al collective classification network data ai magazine cite please cite paper use code work articlekipf2016semi titlesemisupervised classification graph convolutional network authorkipf thomas n welling max journalarxiv preprint arxiv160902907 year2016
Graphs;404 found
Graphs;knowledge base completion kbc code reproduces result canonical tensor decomposition knowledge base icml 2018 installation create conda environment pytorch cython scikitlearn conda create name kbcenv python37 source activate kbcenv conda install file requirementstxt c pytorch install kbc package environment python setuppy install datasets download datasets go kbcscripts folder run chmod x downloaddatash downloaddatash datasets download add package data folder running python kbcprocessdatasetspy create file required compute filtered metric running code reproduce result following command python kbclearnpy dataset fb15k model complex rank 500 optimizer adagrad learningrate 1e1 batchsize 1000 regularizer n3 reg 1e2 maxepochs 100 valid 5 result addition result performance complex regularized weighted n3 several datasets several dimension use init scale 1e3 learning rate 01 batch size 1000 100 max epoch unless specified otherwise use adagrad optimizer fb15k rank 2000 learning rate 1e2 batchsize 100 max epoch 200 rank 525501005002000 mrr 036061078083084086 h1 027052073079080083 h3 041067081085087087 h10 055077086089091091 reg 1e51e51e575e41e225e3 params 163k815k1630m3259m1630m65184m wn18 max epoch 20 rank 581625501005002000 mrr 019045092094095095095095 h1 014037091094094094094094 h3 020050093094095095095095 h10 029060094095095095096096 reg 1e35e45e41e35e35e25e25e2 params 410k656k1311m2049m4098m8196m40979m163916m fb15k237 batch size 100 1000 rank 1000 rank 5255010050010002000 mrr 028033034035036037037 h1 020024025026027027027 h3 031036037039040040040 h10 044051052054056056056 reg 5e45e25e25e25e25e25e2 params 150k751k1502m3003m15015m30030m60060m wn18rr batch size 100 1000 rank 8 rank 581625501005002000 mrr 026036042044046047049049 h1 020038039041043043044044 h3 029038042045047049050050 h10 036041046049052056058058 reg 5e45e45e21e11e11e11e11e1 params 410k655k1311m2048m4097m8193m40975m163860m yago310 rank 51625501005001000 mrr 015034046054056057058 h1 010026038047049050050 h3 016037050058060062062 h10 025050060067069071071 reg 1e31e45e35e35e35e35e3 params 1233m3944m6163m12326m24652m123262m246524m license kbc ccbync licensed found license file
Graphs;graph convolutional network pytorch pytorch implementation graph convolutional network gcns semisupervised classification 1 highlevel introduction gcns see thomas kipf graph convolutional 2016 graph convolutional networksfigurepng note subtle difference tensorflow implementation pytorch reimplementation reimplementation serf proof concept intended reproduction result reported 1 implementation make use cora dataset 2 installation python setuppy install requirement pytorch 04 05 python 27 36 usage python trainpy reference 1 kipf welling semisupervised classification graph convolutional network 2 sen et al collective classification network data ai magazine cite please cite paper use code work articlekipf2016semi titlesemisupervised classification graph convolutional network authorkipf thomas n welling max journalarxiv preprint arxiv160902907 year2016
Graphs;kera graph attention network deprecated implementation gat longer actively maintained may work modern version tensorflow kera check gat tensorflowkeras implementation gat kera implementation graph attention network gat model veličković et al 2017 arxiv acknowledgement affiliation author paper implementing code noncommercial reason author published reference tensorflow implementation check something guaranteed work intended implementation slightly different mine may something keep mind cite paper use code research article velickovic2018graph titlegraph attention network authorvelivckovic petar cucurull guillem casanova arantxa romero adriana lio pietro bengio yoshua journalinternational conference learning representation year2018 noteaccepted poster would like give credit feel free link github also copied code utilspy almost verbatim repo thomas thank sincerely sharing work gcns gaes giving pointer split data traintestval set thanks helping bug performance improvement running experiment disclaimer right datasets distributed code publicly available following link cora pubmed citeseer installation install module git clone cd kerasgat pip install python kerasgat import graphattention copy paste graphattentionlayerpy project replicating experiment replicate experimental result paper simply run sh python examplesgatpy
Graphs;implemented deepwalk algorithm overview repo simple deepwalk algorithm implementation input networkx get random walk path use gensim build word2vec skipgram model installation pip install r requirement use 1 using networkx create network architecture 2 using deepwalk module call randomwalk build random walk record 3 using deepwalk module call buildword2vec build word2vec model reference deepwalk online learning social representation
Graphs;graphsage representation learning large graph author william l wleifstanfordedu rex rexyingstanfordedu project alternative reference pytorch overview directory contains code necessary run graphsage algorithm graphsage viewed stochastic generalization graph convolution especially useful massive dynamic graph contain rich feature information see detail algorithm note graphsage also better support training smaller static graph graph dont node feature original algorithm paper focused task inductive generalization ie generating embeddings node present training many benchmarkstasks use simple static graph necessarily feature support use case graphsage includes optional identity feature used without node attribute including identity feature increase runtime also potentially increase performance usual risk overfitting see section running code note graphsage intended use large graph 100000 node overhead subsampling start outweigh benefit smaller graph exampledata subdirectory contains small example proteinprotein interaction data includes 3 training graph one validation graph one test graph full reddit ppi datasets described paper available project make use code graphsage algorithm work please cite following paper inproceedingshamilton2017inductive author hamilton william l ying rex leskovec jure title inductive representation learning large graph booktitle nip year 2017 requirement recent version tensorflow numpy scipy sklearn networkx required networkx must 111 install required package using following command pip install r requirementstxt guarantee right package version use easily set virtual environment see docker subsection info docker installed need click preceding link installation pretty painless run graphsage inside image cloning project build run image following docker build graphsage docker run graphsage bash start jupyter notebook instead bash docker run p 88888888 graphsage also run gpu image using docker build graphsagegpu f dockerfilegpu nvidiadocker run graphsagegpu bash running code exampleunsupervisedsh examplesupervisedsh file contain example usage code use unsupervised supervised variant graphsage respectively benchmarktask require generalizing unseen data recommend try setting identitydim flag value range 64256 flag make model embed unique node id attribute increase runtime number parameter also potentially increase performance note set flag try pas dense onehot vector feature due sparsity dimension identity feature specifies many parameter per node sparse identityfeature lookup table note exampleunsupervisedsh set small max iteration number increased improve performance generally found performance continued improve even loss near convergence ie even loss decreasing slow rate note ppi data multiouput dataset allows individual node belong multiple class necessary set sigmoid flag supervised training default model assumes dataset onehot categorical setting input format input minimum code requires trainprefix option specified specifies following data file trainprefixgjson networkxspecified json file describing input graph node val test attribute specifying part validation test set respectively trainprefixidmapjson jsonstored dictionary mapping graph node id consecutive integer trainprefixclassmapjson jsonstored dictionary mapping graph node id class trainprefixfeatsnpy optional numpystored array node feature ordering given idmapjson omitted identity feature used trainprefixwalkstxt optional text file specifying random walk cooccurrences one pair per line unsupervised version graphsage run model new dataset need make data file format described run random walk unsupervised model generate prefixwalkstxt file use runwalks function graphsageutils model variant user must also specify model variant described detail paper graphsagemean graphsage meanbased aggregator graphsageseq graphsage lstmbased aggregator graphsagemaxpool graphsage maxpooling aggregator described nip 2017 paper graphsagemeanpool graphsage meanpooling aggregator variant pooling aggregator elementwie mean replaces elementwise max gcn graphsage gcnbased aggregator n2v implementation called n2v short code logging directory finally baselogdir specified default current directory output model log file stored subdirectory baselogdir path logged data form supunsupdataprefixgraphsagemodeldescription supervised model output f1 score unsupervised model train embeddings store unsupervised embeddings stored numpy formated file named valnpy valtxt specifying order embeddings perline list node id note full log output stored embeddings 510gb size full data running unsupervised variant using output unsupervised model unsupervised variant graphsage output embeddings logging directory described embeddings used downstream machine learning application evalscripts directory contains example feeding embeddings simple logistic classifier acknowledgement original version code base originally forked owe many thanks thomas kipf making code available also thank yuanfang li xin li contributed course project based work please see funding detail additional noncode related acknowledgement
Graphs;advancing graphsage datadriven node sampling author jihun oj9040gmailcom jihun2331ohsamsungcom kyunghyun kyunghyunchonyuedu joan brunacimsnyuedu presented iclr 2019 workshop representation learning graph manifold overview efficient scalable graph neural network graphsage enabled inductive capability inferring unseen node graph aggregating subsampled local neighborhood learning minibatch gradient descent fashion neighborhood sampling used graphsage effective order improve computing memory efficiency inferring batch target node diverse degree parallel despite advantage default uniform sampling suffers high variance training inference leading suboptimum accuracy propose new datadriven sampling approach reason realvalued importance neighborhood nonlinear regressor use value criterion subsampling neighborhood regressor learned using valuebased reinforcement learning implied importance combination vertex neighborhood inductively extracted negative classification loss output graphsage result inductive node classification benchmark using three datasets method enhanced baseline using uniform sampling outperforming recent variant graph neural network accuracy requirement recent version tensorflow numpy scipy sklearn networkx required networkx must 111 install required package using following command pip install r requirementstxt guarantee right package version use easily set virtual environment see docker subsection info docker installed need click preceding link installation pretty painless run graphsage inside image cloning project build run image following docker build graphsage docker run graphsage bash start jupyter notebook instead bash docker run p 88888888 graphsage also run gpu image using docker build graphsagegpu f dockerfilegpu nvidiadocker run graphsagegpu bash running code exampleunsupervisedsh file contains example usage three dataset ppi reddit pubmed code supervised classification task benchmarktask require generalizing unseen data recommend try setting identitydim flag value range 64256 flag make model embed unique node id attribute increase runtime number parameter also potentially increase performance note set flag try pas dense onehot vector feature due sparsity dimension identity feature specifies many parameter per node sparse identityfeature lookup table note ppi data multiouput dataset allows individual node belong multiple class necessary set sigmoid flag supervised training default model assumes dataset onehot categorical setting input format input minimum code requires trainprefix option specified specifies following data file trainprefixgjson networkxspecified json file describing input graph node val test attribute specifying part validation test set respectively trainprefixidmapjson jsonstored dictionary mapping graph node id consecutive integer trainprefixclassmapjson jsonstored dictionary mapping graph node id class trainprefixfeatsnpy optional numpystored array node feature ordering given idmapjson omitted identity feature used trainprefixwalkstxt optional text file specifying random walk cooccurrences one pair per line unsupervised version graphsage run model new dataset need make data file format described run random walk unsupervised model generate prefixwalkstxt file use runwalks function graphsageutils dataset download dataset included github due big size downloaded link ppi proteinprotein interaction wget reddit wget pubmed included datapubmed folder dataset different format convert using creategraphforgraphsagepy model variant user must also specify model variant described detail paper meanconcat graphsage meanconcat based aggregator meanadd graphsage meanadd based aggregator default graphsageseq graphsage lstmbased aggregator graphsagemaxpool graphsage maxpooling aggregator described nip 2017 paper graphsagemeanpool graphsage meanpooling aggregator variant pooling aggregator elementwie mean replaces elementwise max gcn graphsage gcnbased aggregator n2v implementation called n2v short code logging directory finally baselogdir specified default current directory output model log file stored subdirectory baselogdir path logged data form supunsupdataprefixgraphsagemodeldescription supervised model output f1 score unsupervised model train embeddings store unsupervised embeddings stored numpy formated file named valnpy valtxt specifying order embeddings perline list node id note full log output stored embeddings 510gb size full data running unsupervised variant bibliography articleoh2019advancing titleadvancing graphsage datadriven node sampling authoroh jihun cho kyunghyun bruna joan journalarxiv preprint arxiv190412935 year2019
Graphs;multiple object tracking using structuraltemporal graph attention project built based idea paper dyglip dynamic graph model link prediction accurate multicamera multiple object bnoteb project still construction example short example prw dataset short demo prw datasetassetsdemoprwgif todo list add evaluation metric retrain model mar dataset test model campus dataset reference graph attention dyglip dynamic graph model link prediction accurate multicamera multiple object inductive representation learning temporal graph attention network
Graphs;graph convolutional network pytorch reimplementation work described semisupervised classification graph convolutional implementation contains two different propagation model one original gcn described paper chebyshev filter based one convolutional neural network graph fast localized spectral installation usage quickly check open bash git clone cd gcnpytorch requirement dependent whether want use gpu bash pip install r requirementsgputxt bash pip install r requirementscputxt simple evaluation model cora dataset python import torch gcnmodel import twolayergcn gcntrainer import trainer runconfig gcnutils import dataset loaddata feature label trainlabels vallabels testlabels adjacencymatrix laplacianmatrix numclasses loaddatadatasetcora device torchdevicecuda torchcudaisavailable else cpu training parameter runconfig runconfiglearningrate01 numepochs200 weightdecay5e4 outputdirgcn constructing gcn model model twolayergcn inputsizefeaturessize1 hiddensize16 outputsizenumclasses dropout05 training trainer trainermodel trainertrainfeatures trainlabels vallabels adjacencymatrix device runconfig logfalse evaluating celoss accuracy trainerevaluatefeatures testlabels adjacencymatrix device check notebooksgcntestingipynb contains code reproducing result run notebook google colab follow link open result test set accuracy implementation comparison original paper result based public split analyzed datasets result report standard deviation accuracy based 100 repetition table thead tr thdatasetth thcorath thciteseerth thpubmedth tr thead tbody tr td colspan4original papertd tr tr tdgcntd td815td td703td td790td tr tr tdcheb k2td td812td td696td td738td tr tr tdcheb k3td td795td td698td td744td tr tr td colspan4this implementationtd tr tr tdgcntd td822 ± 05td td710 ± 06td td791 ± 05td tr tr tdcheb k2td td813 ± 07td td711 ± 09td td779 ± 09td tr tr tdcheb k3td td825 ± 07td td712 ± 08td td790 ± 07td tr tbody table result experiment model depth residual connection shown original paper whole dataset used mean accuracy 5fold cross validation plotted p floatleft img srccorapng width600 img srcciteseerpng width600 img srcpubmedpng width600 p reference citation official gcn tensorflow spectral graph convnets chebnets bibtex articlekipf2016semi titlesemisupervised classification graph convolutional network authorkipf thomas n welling max journalarxiv preprint arxiv160902907 year2016 bibtex inproceedingsdefferrard2016convolutional titleconvolutional neural network graph fast localized spectral filtering authordefferrard michael bresson xavier vandergheynst pierre booktitleadvances neural information processing system pages38443852 year2016
Graphs;rgselect rgselect gnn reproducibility assessment datasets multigraphs coded python mohammed amine gharsallaoui please contact mohammedamineghgmailcom inquiry thanks repository provides official pytorch implementation following paper p aligncenter img srcfig1png p investigating reproducibility graph neuralnetworks using multigraph data mohammed amine islem sup1supbasira lab faculty computer informatics istanbul technical university istanbul turkey sup2supschool science engineering computing university dundee uk abstract graph neural network gnns witnessed unprecedented proliferation tackling several problem computer vision computeraided diagnosis related field prior study focused boosting model accuracy quantifying reproducibility discriminative feature identified gnns still intact problem yield concern reliability clinical application particular specifically reproducibility biological marker across clinical datasets distribution shift across class eg healthy disordered brain paramount importance revealing underpinning mechanism disease well propelling development personalized treatment motivated issue propose first time reproducibilitybased gnn selection rgselect framework gnn reproducibility assessment via quantification discriminative feature ie biomarkers shared different model ascertain soundness framework reproducibility assessment embrace variation different factor training strategy data perturbation despite challenge framework successfully yielded replicable conclusion across different training strategy various clinical datasets finding could thus pave way development biomarker trustworthiness reliability assessment method computeraided diagnosis prognosis task rgselect code available github work published neural network based graph topology rgselect framework evaluates reproducibility graph neural network gnns framework investigates reproducibility using learnt gnn weight studying commonality discriminative biomarkers across gnn rgselect assigns reproducibility score model incorporating variation many factor training strategy edge eg brain connectivity measure number top biomarkers selected evaluated model using crossvalidation fewshot approach small largescale datasets repository release code reproducibility evaluation simulated vector representing learnt weight respective different gnns installation code tested python 3 pytorch 171 window gpu required run code also need dependency eg numpy panda matplotlib seaborn installed via sh pip install numpy pip install torch pip install panda pip install scikitlearn pip install matplotlib pip install seaborn generating simulated data provide demo code usage rgselect gnn reproducibility assessment datasets multigraphs simulatedatapy generate datasets mutigraphs hyperparameters multigraphs eg number node number view varied user addition user vary distribution property generated population graph generate simulated data following command bash python simulatedatapy simulated data saved pickle format within directory code file simulated data order use reproducibility framework need provide number view number node standard deviation population two class mean population two class parameter generate dataset graph graph represented 3d numpy matrix output file label list containing label graph adjs list contining graph represented adjacency matrix gnn model gnn model included diffpool sagpool gat gunets gcn running gnn model simulating dataset run gnn model following command bash python demopy file run gnn model different training setting tune general parameter dataset number view model included also tune specific hyperparameters model running file maingatpy maingcnpy maingunetpy maindiffpoolpy mainsagpy file contain method training model hyperparameters number layer learning rate dimension specific layer extracting reproducibility score running gnns file extractresultspy extract weight learnt gnns computes overlap score different combination setting view training strategy top biomarkers considered output file panda dataframe containing reproducibility matrix gnns used dataframe saved pickle file plotting heatmaps reproducibility matrix saving dataframes extractresultspy run file plotheatmapspy visualize heatmaps reproducibility matrix note displayed plot contain heatmaps 3 matrix reproducibility matrix using view average method reproducibility matrix using rank correlation method overall reproducibility matrix p aligncenter img srcfig2png p related reference identifying best datadriven feature selection method boosting reproducibility classification task george n mhiri rekik alzheimer’s disease neuroimaging initiative identifying best datadriven feature selection method boosting reproducibility classification task 2020 citation code useful work please cite paper latex uploaded soon rgselect arxiv license code released mit license see license file detail
Graphs;knowbert knowbert general method embed multiple knowledge base bert repository contains pretrained model evaluation training script knowbert wikipedia wordnet citation inproceedingspeters2019knowledgeec authormatthew e peter mark neumann robert l logan roy schwartz vidur joshi sameer singh noah smith titleknowledge enhanced contextual word representation booktitleemnlp year2019 getting started git clone gitgithubcomallenaikbgit cd kb conda create n knowbert python367 source activate knowbert pip install torch120 pip install r requirementstxt python c import nltk nltkdownloadwordnet python spacy download encorewebsm pip install editable make sure test pas pytest v test pretrained model embed sentence sentence pair programmatically python kbincludeall import modelarchivefromparams kbknowbertutils import knowbertbatchifier allennlpcommon import params import torch pretrained model eg wordnetwikipedia archivefile load model batcher params paramsarchivefile archivefile model modelarchivefromparamsfromparamsparamsparams batcher knowbertbatchifierarchivefile sentence paris located france knowbert knowledge enhanced bert batcher take raw untokenized sentence yield batch tensor needed run knowbert batch batcheriterbatchessentences verbosetrue modeloutputcontextualembeddings batchsize seqlen embeddim tensor top layer activation modeloutput modelbatch run intrinisic evaluation first download one pretrained model previous section heldout perplexity table 1 download heldout run modelarchivelocation model heldoutfilewikipediabookscorpusknowbertheldouttxt python binevaluateperplexitypy modelarchive e heldoutfile heldout perplexity key explmlosswgt wikidata kg probe table 1 run modelarchivelocation model mkdir p kgprobe cd kgprobe curl kgprobezip unzip kgprobezip cd python binevaluatemrrpy modelarchive modelarchive datadir kgprobe cudadevice 0 result key mrr wordsense disambiguation evaluate internal wordnet linker task evaluation raganato et al 2017 follow step table 2 first download java evaluation run command generate prediction knowbert evaluationfilesemeval2007semeval2013semeval2015senseval2senseval3alljson knowbertpredictionsknowbertwordnetpredictedtxt modelarchivelocation model python binevaluatewsdofficialpy evaluationfile evaluationfile outputfile knowbertpredictions modelarchive modelarchive cudadevice 0 evaluate prediction decompress java scorer navigate directory wsdevaluationframeworkevaluationdatasets run java scorer allallgoldkeytxt knowbertpredictions aida entity linking reproduce result table 3 knowbertww aidatesttxt evaluationfileaidadevtxt modelarchivelocation model curl evaluationfile python binevaluatewikilinkingpy modelarchive modelarchive evaluationfile evaluationfile wikiandwordnet result key wikielf1 fine tuning knowbert downstream task fine tuning knowbert similar fine tuning bert downstream task provide configuration model file following task relation extraction tacred semeval 2010 task 8 entity typing choi et al 2018 binary sentence classification wordsincontext reproduce result following task find appropriate config file trainingconfigdownstream edit location training dev data file run example provided tacred allennlp train filefriendlylogging includepackage kbincludeall trainingconfigdownstreamtacredjsonnet outputdirectory similar bert task performance vary significantly hyperparameter choice random seed used script binrunhyperparameterseedssh perform small grid search learning rate number epoch random seed choosing best model based validation set evaluating fine tuned model finetuned knowbertwikiwordnet model available semeval2010 task entity evaluate model first download model archive run allennlp evaluate includepackage kbincludeall cudadevice 0 modelarchivehere devortestfilenamehere tacred evaluate model official scorer run python binwritetacredforofficialscorerpy modelarchive modelarchivehere evaluationfile tacreddevortestjson outputfile knowbertpredictionstacreddevortesttxt python bintacredscorerpy tacreddevortestgold knowbertpredictionstacreddevortesttxt semeval 2010 task 8 evaluate model official scorer first download testing gold run curl semeval2010task8testjson python binwritesemeval2010task8forofficialevalpy modelarchive modelarchivehere evaluationfile semeval2010task8testjson outputfile knowbertpredictionssemeval2010task8testtxt perl w binsemeval2010task8scorerv12pl knowbertpredictionssemeval2010task8testtxt semeval2010task8testingkeystxt wic use binwritewicforcodalabpy write file submission codalab evaluation server pretrain knowbert roughly speaking process fine tune bert knowbert 1 prepare corpus 2 prepare knowledge base necessary using wikipedia wordnet already prepared 3 knowledge base 1 pretrain entity linker freezing everything else 2 fine tune parameter except entity embeddings prepare corpus 1 sentence tokenize training corpus using spacy prepare input file nextsentenceprediction sampling file contains one sentence per line consecutive sentence subsequent line blank line separating document 2 run bincreatepretrainingdataforbertpy group sentence length nsp sampling write file training 3 reserve one training file heldout evaluation prepare input knowledge base 1 already prepared knowledge base wikipedia wordnet necessary file automatically downloaded needed running evaluation fine tuning knowbert 2 would like add additional knowledge source knowbert roughly step follow 1 compute entity embeddings entity knowledge base 2 write candidate generator entity linkers use existing wordnet wikipedia generator template 3 wikipedia candidate dictionary list embeddings extracted endtoend neural entity linking kolitsas et al via manual process 4 wordnet candidate generator rule based see code embeddings computed via multistep process combine embeddings prepared file contain everything needed run knowbert include 1 entitiesjsonl metadata wordnet synset 2 wordnetsynsetsmasknullvocabtxt wordnetsynsetsmasknullvocabembeddingstuckergensenhdf5 vocabulary file embedding file wordnet synset 3 semcorandwordnetexamplesjson annotated training data combining semcor wordnet example supervising wordnet linker 5 would like generate file scratch follow step 1 extract wordnet metadata relationship graph python binextractwordnetpy extractgraph entityfile workdirentitiesjsonl relationshipfile workdirrelationstxt 2 download wordsincontext exclude extracted wordnet example usage workdir cd workdir wget unzip wicdatasetzip 2 download word sense diambiguation cd workdir wget unzip wsdevaluationframeworkzip 2 convert wsd data xml jsonl concatenate evaluation file easy evaluation mkdir workdirwsdjsonl python binpreprocesswsdpy wsdframeworkroot workdirwsdevaluationframework outdir workdirwsdjsonl cat workdirwsdjsonlsemeval workdirwsdjsonlsenseval workdirsemeval2007semeval2013semeval2015senseval2senseval3json 2 extract synset example usage wordnet removing sentence wic heldout set python binextractwordnetpy extractexampleswordnet entityfile workdirentitiesjsonl wicrootdir workdir wordnetexamplefile workdirwordnetexamplesremovewicdevtestjson 2 combine wordnet example definition semcor training knowbert cat workdirwordnetexamplesremovewicdevtestjson workdirwsdjsonlsemcorjson workdirsemcorandwordnetexamplesjson 3 create training test split relationship graph python binextractwordnetpy splitwordnet relationshipfile workdirrelationstxt relationshiptrainfile workdirrelationstrain99txt relationshipdevfile workdirrelationsdev01txt 4 train tucker embeddings extracted graph configuration file us relationship graph file s3 although substitute file generated previous step modifying configuration file allennlp train workdirwordnettucker includepackage kbkgembedding filefriendlylogging trainingconfigwordnettuckerjson 5 generate vocabulary file useful wordnet synset special token python bincombinewordnetembeddingspy generatewordnetsynsetvocab entityfile workdirentitiesjsonl vocabfile workdirwordnetsynsetsmasknullvocabtxt 6 get embeddings synset definition first install code link run python bincombinewordnetembeddingspy generategensenembeddings entityfile workdirentitiesjsonl vocabfile workdirwordnetsynsetsmasknullvocabtxt gensenfile workdirgensensynsetshdf5 7 extract tucker embeddings synset trained model python bincombinewordnetembeddingspy extracttucker tuckerarchivefile workdirwordnettuckermodeltargz vocabfile workdirwordnetsynsetsmasknullvocabtxt tuckerhdf5file workdirtuckerembeddingshdf5 8 finally combine tucker gensen embeddings one file python bincombinewordnetembeddingspy combinetuckergensen tuckerhdf5file workdirtuckerembeddingshdf5 gensenfile workdirgensensynsetshdf5 allembeddingsfile workdirwordnetsynsetsmasknullvocabembeddingstuckergensenhdf5 pretraining entity linkers step pretrains entity linker freezing rest network using supervised data config file trainingconfigpretrainingknowbertwikilinkerjsonnet trainingconfigpretrainingknowbertwordnetlinkerjsonnet train wikipedia linker knowbertwiki run allennlp train outputdirectory filefriendlylogging includepackage kbincludeall trainingconfigpretrainingknowbertwikilinkerjsonnet command similar wordnet fine tuning bert pretraining entity linkers step fine tune bert pretrained model paper trained single gpu 24gb ram multiple gpu training change cudadevice list device id config file trainingconfigpretrainingknowbertwikijsonnet trainingconfigpretrainingknowbertwordnetjsonnet training modify following key config file use override flag allennlp train languagemodeling modelarchive point modeltargz previous linker pretraining step knowbert wordnet wiki first train knowbertwiki pretrain wordnet linker finally fine tune entire network config file pretrain wordnet linker knowbertwiki trainingconfigpretrainingknowbertwordnetwikilinkerjsonnet config train knowbertww trainingconfigpretrainingknowbertwordnetwikijsonnet
Graphs;relation prediction auxiliary training objective knowledge graph completion repo contains code accompanying paper “relation prediction auxiliary training objective improving multirelational graph found incorporating relation prediction 1vsall objective yield new selfsupervised training objective knowledge base completion kbc result significant performance improvement 99 hits1 adding little 3–10 line code unleash true power kbc model relation prediction objective codebase also come sota result several kbc datasets echoing previous find traditional factorisationbased model eg complex distmult rescal outperform recently proposed model trained appropraitely case find 1vsall relation prediction objective effective require le tweaking sophisticated architecture docimgsslrprepopng table content zap link prediction jigsaw pretrained compass use use repo ogb use repo conventional kbc smilingfacewiththreehearts pagewithcurl whitecheckmark news 16122021 pretrained embeddings fb15k237wn18rrcodexmcodexs released check 01122021 hyperparameters codex ogblbiokg ogblwikikgv2 released zap link prediction result attempt include many result possible recent knowledge graph completion datasets release foster easy reproduction feel free create issue want suggest additional datasets u include currently result ogb link property prediction dataset training done single 16gb gpu except ogblwikikg2 run 32gb gpu ogblwikikg2 model params using rp mrr hits1 hits3 hits10 complex 50dim3 250m 03804 complex 250dim3 1b 04027 complex 25dim 125m 05161 04576 05310 06324 complex 25dim 125m yes 05192 04540 05394 06483 complex 50dim 250m 06193 05503 06468 07589 complex 50dim 250m yes 06392 05684 06686 07822 complex 100dim 500m 06458 05750 06761 07896 complex 100dim 500m yes 06509 05814 06800 07923 note training 50100 dim take 3 day additional training time likely lead better result currently use one 32gb gpu acceleration multiple gpus considered future ogblbiokg model params using rp mrr hits1 hits3 hits10 complex 2 188m 08095 complex 188m 08482 07887 08913 09536 complex 188m yes 08494 07915 08902 09540 codex model using rp mrr hits1 hits3 hits10 complex 1 0465 0372 0504 0646 complex 1000dim 0472 0378 0508 0658 complex 1000dim yes 0473 0375 0514 0663 codexm model using rp mrr hits1 hits3 hits10 complex 1 0337 0262 0370 0476 complex 1000dim 0351 0276 0385 0492 complex 1000dim yes 0352 0277 0386 0490 codexl model using rp mrr hits1 hits3 hits10 complex 1 0294 0237 0318 0400 complex 1000dim 0342 0275 0374 0470 complex 1000dim yes 0345 0277 0377 0473 wn18rr model using rp mrr hits1 hits3 hits10 complex 0487 0441 0501 0580 complex yes 0488 0443 0505 0578 fb15k237 model using rp mrr hits1 hits3 hits10 complex 0366 0271 0401 0557 complex yes 0388 0298 0425 0568 aristov4 model using rp mrr hits1 hits3 hits10 complex 0301 0232 0324 0438 complex yes 0311 0240 0336 0447 1 result taken awesome codex 2 result taken ogb link property prediction leaderboard 3 result taken ogb link property prediction leaderboard pretrained embeddings dataset pred including reciprocal predicate ent model hyperparameters download link params file size fb15k237 474 14541 complex1000dim 30m 115mb wn18rr 22 40943 complex1000dim 82m 313m codexm 102 17050 complex1000dim 34m 131m codex 84 2034 complex1000dim 4m 17m note also learn embeddings reciprocal predicate reported helpful dettmers et al lacroix et al use repo use repo ogb datasets edit preprocessdatasetspy specify dataset want run either datasets ogblwikikg2 datasets ogblbiokg run preprocessdatasetspy follows mkdir data python preprocessdatasetspy preprocessing complete model trained running mainpy example train complex ogblbiokg use following command python mainpy dataset ogblbiokg model complex scorerel true rank 1000 learningrate 1e1 batchsize 500 optimizer adagrad regularizer n3 lmbda 001 wrel 025 valid 1 train complex ogblwikikg2 use following command gpu 32gb memory python mainpy dataset ogblwikikg2 model complex scorerel true rank 50 learningrate 1e1 batchsize 250 optimizer adagrad regularizer n3 lmbda 01 wrel 0125 valid 1 obtain training curve similar figure ogblbiokg ogblwikikg2 docimgogblbiokgpng docimgogblwikikg2png use repo conventional kbc datasets customized datasets prepare datasets download datasets place srcdata name file containing training triplet train validation triplet valid test triplet test folder look like srcdatafb15k237train tab separated file row like head relation tail srcdatafb15k237valid tab separated file row like head relation tail srcdatafb15k237test tab separated file row like head relation tail downloading datasets preprocessing quick completed within minute first edit preprocessdatasetspy specify dataset want run eg datasets customgraph run mkdir data python preprocessdatasetspy download together umls nation kinship fb15k237 wn18rr aristov4 also download datasets separately train model use option scorerel enable auxiliary relation prediction objective use option wrel set weight relation prediction objective example following command train complex model auxiliary relation prediction objective fb15k237 python mainpy dataset fb15k237 scorerel true model complex rank 1000 learningrate 01 batchsize 1000 lmbda 005 wrel 4 maxepochs 100 following command train complex model without auxiliary relation prediction objective fb15k237 python mainpy dataset fb15k237 scorerel false model complex rank 1000 learningrate 01 batchsize 1000 lmbda 005 wrel 4 maxepochs 100 dependency pytorch wandb acknowledgement repo based repo provides efficient implementation 1vsall complex cp repo also includes implementation model transe rescal tucker citation find repo useful please cite u inproceedings chen2021relation titlerelation prediction auxiliary training objective improving multirelational graph representation authoryihong chen pasquale minervini sebastian riedel pontus stenetorp booktitle3rd conference automated knowledge base construction year2021 license repo ccbync licensed found license file
Graphs;graph convolutional network pytorch pytorch implementation graph convolutional network gcns semisupervised classification 1 highlevel introduction gcns see thomas kipf graph convolutional 2016 graph convolutional networksfigurepng note subtle difference tensorflow implementation pytorch reimplementation reimplementation serf proof concept intended reproduction result reported 1 implementation make use cora dataset 2 installation python setuppy install requirement pytorch 04 05 python 27 36 usage python trainpy reference 1 kipf welling semisupervised classification graph convolutional network 2 sen et al collective classification network data ai magazine cite please cite paper use code work articlekipf2016semi titlesemisupervised classification graph convolutional network authorkipf thomas n welling max journalarxiv preprint arxiv160902907 year2016
Graphs;representationlearning introduction repository build work n thomas max “variational graph autoencoders” arxivorg 21 nov 2016 arxivorgabs161107308 learn representation scientific citation data latent space latent space representation underlying information dataset investigating latent space correlation feature begin understand feature interact complex network scientific citation network dataset 1 citeseer 1 web science preliminary result github logoplotsciteseerplotpng
Graphs;gcn course csce 636 neural network project cora dataset used semisupervised classification node performed dataset consists machine learning paper paper classified one following seven class casebased geneticalgorithms neuralnetworks probabilisticmethods reinforcementlearning rulelearning theory pygcnmasterdatacora readme detail data available pygcnmasterpygcn layerspy modelspy trainpy found change dropout hidden layer made youtube link gui demo according paper baseline accuracy cora dataset 815
Graphs;tutorial learn basic deep learning graph schedule time title 30mn introduction ml graph gnns 15mn handson part 1 predicting molecular property 30mn handson part 2 link prediction knowledge graph 15mn discussion reference closing remark requirement tutorial designed everyone would like learn basic deep learning graphstructured data graph neural network gnn tutorial provide introduction key concept recent development field machine learning graph guide participant design first deep learning pipeline operate graphstructured data setup strongly suggest setup machine prior tutorial ideally day following instruction incur issue please hesitate contact guillaumemailtogjazurichibmcom installation conda setup conda machine already done create conda environment console conda env create f environmentyml activate console conda activate graphmlenv relevant paper powerful graph neural network theoretical analysis graph neural network great starting point understand gnns cannot since many paper proposed deeper analysis expressive power gnns modeling relational data graph convolutional network first gnn designed operate knowledge graph able beat stateoftheart better distmult complex rotate etc benchmarking graph neural network uptodate sept 2020 paper properly benchmarking popular graph neural network architecture variety datasets participant would like graphrelated reference refer uptodate list found heregraphbaseddeeplearningliterature organiser tutor guillaume ibm research zürich
Graphs;simple deep graph convolutional network repository contains pytorch implementation simple deep graph convolutional dependency cuda 101 python 369 pytorch 131 networkx 21 scikitlearn datasets data folder contains three benchmark datasetscora citeseer pubmed newdata folder contains four datasetschameleon cornell texas wisconsin use semisupervised setting fullsupervised setting geomgcn ppi downloaded result testing accuracy summarized dataset depth metric dataset depth metric cora 64 855 cham 8 6248 cite 32 734 corn 16 7649 pubm 16 803 texa 32 7784 corafull 64 8849 wisc 16 8157 citefull 64 7713 ppi 9 9956 pubmfull 64 9030 obgnarxiv 16 7274 usage replicate semisupervised result run following script sh sh semish replicate fullsupervised result run following script sh sh fullsh replicate inductive result ppi run following script sh sh ppish reference implementation pyg folder includes simple pytorch geometric implementation gcnii requirement torchgeometric 150 ogb 120 running example python corapy python arxivpy citation articlechenwhdl2020gcnii title simple deep graph convolutional network author ming chen zhewei wei zengfeng huang bolin ding yaliang li year 2020 booktitle proceeding 37th international conference machine learning
Graphs;fastwalk multithread implementation node2vec random walk introduction repository provides multithread implementation node2vec random walk alias table based lru cache process limited memory usage walking giant graph single machine possible tested graph contains 23 thousand node 23 million edge parameter walklength80 numwalks10 workers20 maxnodes50000 maxedges100000 p10 q001 11gb memory used finished walking within 2 hour visit prerequisite g 48 usage prepare input data format node1 node2 edgeweight node2 node3 edgeweight edgeweight 10 default compile make run fastwalk edgelist pathtoedgelist output pathtooutput delimiter space p 10 q 001 maxnodes 50000 maxedges 50000 worker 10 wanna walk faster add worker wanna run le memory consumption decrease maxnodes maxedges checkout information fastwalk help reference node2vec scalable feature learning networksbr aditya grover jure leskovecbr knowledge discovery data mining 2016br acknowledgement would like thank yuanyuan zhu discussion performance node2vec thanks weichen shen great
Graphs;building powerful equivariant graph neural network structural messagepassing paper contains code paper building powerful equivariant graph neural network structural messagepassing neurips 2020 clément andreas pascal link abstract messagepassing proved effective way design graph neural network able leverage permutation equivariance inductive bias towards learning local structure order achieve good generalization however current messagepassing architecture limited representation power fail learn basic topological property graph address problem propose powerful equivariant messagepassing framework based two idea first propagate onehot encoding node addition feature order learn local context matrix around node matrix contains rich local information feature topology eventually pooled build node representation second propose method parametrization message update function ensure permutation equivariance representation independent specific choice onehot encoding permit inductive reasoning lead better generalization property experi mentally model predict various graph topological property synthetic data accurately previous method achieves stateoftheart result molecular graph regression zinc dataset code overview folder contains source code used structural message passing three task cycle detection multitask regression graph property presented constrained solubility regression zinc source code second task adapted dependency geometric v161 used please follow instruction website simple installation via pip work particular version pytorch used must match one torchgeometric install dependency pip install r requirementstxt dataset generation cycle detection first download data unzip datadatasetskcyclensamples10000 run python3 datasetsgenerationbuildcyclespy multitask regression simply run python datasetsgenerationmultitaskdataset zinc use pytorchgeometric downloader nothing hand folder structure task launched running corresponding main file cyclesmain zincmain multitaskmain model parameter changed associated configyaml file training parameter modified command line argument model used task located model folder modelcycles modelmultitask modelzinc use smp layer parametrized smplayers file smp layer use set base function modelsutilslayerspy function map tensor one order tensor another order using predefined set equivariant transformation train cycle detection order train smp specify cycle length size graph used potentially proportion training data kept example python3 cyclemainpy k 4 n 12 proportion 10 gpu 0 train 4cycle graph average 12 node 10 100 100 training data order run another model modify modelsconfigyaml run mpnn architecture smp set usextrue file mpnn gin transforms specified order add onehot encoding node degree onehot identifier available option seen using python3 cyclesmainpy help multitask regression specify configuration file configmultitaskyaml available option using python3 multitaskmainpy help use default parameter simply run python3 multitaskmainpy gpu 0 zinc zinc dataset downloaded pytorch geometric destination folder specified beginning zincmainpy model parameter changed configzincyaml use default parameter simply run python3 zincmainpy gpu 0 use smp new data code currently available library need copypaste file adapt data code reused may need adapt model problem advise look different model file modelcycles modelmultitask modelzinc see built follow design local context first created using function modelsutilsmisc node feature wish use smp use mapxtou include local context one three smp layer smp fastsmp simplifiedfastsmp used layer update local context either nodelevel feature graphlevel feature extracted purpose use nodeextractor graphextractor class modelsutilslayerspy extracted feature processed standard neural network use multilayer perceptron complex structure gated recurrent network take input feature extracted layer sum need copy following file folder modelssmplayerspy modelsutilslayerspy modelsutilsmiscpy adapt following file problem main file eg zincmainpy config file eg configzincyaml model file eg modelsmodelzincpy advise use weight bias library well found convenient store result license mit cite paper inproceedingsneurips2020a32d7eea author vignac clement loukas andreas frossard pascal booktitle advance neural information processing system editor h larochelle ranzato r hadsell f balcan h lin page 1414314155 publisher curran associate inc title building powerful equivariant graph neural network structural messagepassing url volume 33 year 2020
Graphs;gravityinspired graph autoencoders directed link prediction repository provides python code reproduce experiment article gravityinspired graph autoencoders directed link published proceeding 28th acm international conference information knowledge management cikm 2019 release tensorflow implementation following four directed graph embedding model paper gravityinspired graph autoencoders gravityinspired graph variational autoencoders sourcetarget graph autoencoders sourcetarget graph variational autoencoders together standard graph autoencoders ae graph variational autoencoders vae model kipf welling evaluate six model three directed link prediction task introduced section 41 paper general directed link prediction biased negative sample directed link prediction bidirectionality prediction code build upon thomas kipfs original tensorflow standard graph aevae br p aligncenter img height550 srcgraphvisucorapng p installation bash python setuppy install requirement tensorflow 1x networkx numpy scikitlearn scipy run experiment bash cd gravitygae python trainpy modelgcnvae datasetcora tasktask1 python trainpy modelgravitygcnvae datasetcora tasktask1 command train graph vae line 2 gravityinspired graph vae line 3 cora dataset evaluate node embdeddings task 1 general directed link prediction parameter set default value complete list parameter parameter type description default value model string name model amongbr gcnae graph ae kipf welling 2016 2layerbr gcn encoder inner product decoderbr gcnvae graph vae kipf welling 2016 gaussian br distribution 2layer gcn encoders inner product decoderbr sourcetargetgcnae sourcetarget graph ae introduced br section 26 paper 2layer gcn encoder asymmetric inner product decoder br sourcetargetgcnvae sourcetarget graph vae introduced br section 26 gaussian distribution 2layer gcn encoders asymmetric inner productbr gravitygcnae gravityinspired graph ae introduced br section 33 paper 2layer gcn encoder gravityinspired asymmetric decoder br gravitygcnvae gravityinspired graph vae introduced br section 34 paper gaussian distribution 2layer gcn encoders gravityinspired decoder gcnae dataset string name dataset amongbr cora scientific publication citation network br citeseer scientific publication citation network br google hyperlink network web page br br note specify additional graph dataset edgelist formatbr editing inputdatapy cora task string name link prediction evaluation task among br task1 general directed link prediction br task2 biased negative sample directed link prediction br task3 bidirectionality prediction task1 dropout float dropout rate 0 epoch int number epoch model training 200 feature boolean include node feature gcn encoder false lamb float lambda parameter gravity aevae model introduced br section 35 paper balance mass proximity term 1 learningrate float initial learning rate adam optimizer 01 hidden int number unit gcn encoder hidden layer 64 dimension int dimension gcn output br equal embedding dimension standard aevae br sourcetarget aevae model br equal embedding dimension 1 gravityinspired aevae br model last dimension capture mass parameter br br dimension must even sourcetarget aevae model 32 normalize boolean gravity model whether normalize embedding vector false epsilon float gravity model add epsilon l2 distance computation numerical stability 001 nbrun integer number model run test 1 propval float proportion edge validation set task 1 5 proptest float proportion edge test set task 1 2 10 validation boolean whether report validation result epoch task 1 false verbose boolean whether print full comment detail true model paper cora task 1 bash python trainpy datasetcora modelgcnvae tasktask1 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelgcnae tasktask1 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelsourcetargetgcnvae tasktask1 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelsourcetargetgcnae tasktask1 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelgravitygcnvae tasktask1 epochs200 learningrate01 hidden64 dimension33 lamb10 nbrun5 python trainpy datasetcora modelgravitygcnae tasktask1 epochs200 learningrate01 hidden64 dimension33 lamb10 nbrun5 cora task 2 bash python trainpy datasetcora modelgcnvae tasktask2 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelgcnae tasktask2 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelsourcetargetgcnvae tasktask2 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelsourcetargetgcnae tasktask2 epochs200 learningrate01 hidden64 dimension64 nbrun5 python trainpy datasetcora modelgravitygcnvae tasktask2 epochs200 learningrate01 hidden64 dimension33 lamb005 nbrun5 python trainpy datasetcora modelgravitygcnae tasktask2 epochs200 learningrate01 hidden64 dimension33 lamb005 normalizetrue nbrun5 cora task 3 bash python trainpy datasetcora modelgcnvae tasktask3 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelgcnae tasktask3 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelsourcetargetgcnvae tasktask3 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelsourcetargetgcnae tasktask3 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetcora modelgravitygcnvae tasktask3 epochs200 learningrate01 hidden64 dimension33 lamb10 nbrun5 python trainpy datasetcora modelgravitygcnae tasktask3 epochs200 learningrate01 hidden64 dimension33 lamb10 nbrun5 citeseer task 1 bash python trainpy datasetciteseer modelgcnvae tasktask1 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelgcnae tasktask1 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelsourcetargetgcnvae tasktask1 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelsourcetargetgcnae tasktask1 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelgravitygcnvae tasktask1 epochs200 learningrate01 hidden64 dimension33 lamb10 nbrun5 python trainpy datasetciteseer modelgravitygcnae tasktask1 epochs200 learningrate01 hidden64 dimension33 lamb10 nbrun5 citeseer task 2 bash python trainpy datasetciteseer modelgcnvae tasktask2 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelgcnae tasktask2 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelsourcetargetgcnvae tasktask2 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelsourcetargetgcnae tasktask2 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelgravitygcnvae tasktask2 epochs200 learningrate01 hidden64 dimension33 lamb005 nbrun5 python trainpy datasetciteseer modelgravitygcnae tasktask2 epochs200 learningrate01 hidden64 dimension33 lamb005 normalizetrue nbrun5 citeseer task 3 bash python trainpy datasetciteseer modelgcnvae tasktask3 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelgcnae tasktask3 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelsourcetargetgcnvae tasktask3 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelsourcetargetgcnae tasktask3 epochs200 learningrate01 hidden64 dimension32 nbrun5 python trainpy datasetciteseer modelgravitygcnvae tasktask3 epochs200 learningrate01 hidden64 dimension33 lamb10 nbrun5 python trainpy datasetciteseer modelgravitygcnae tasktask3 epochs200 learningrate01 hidden64 dimension33 lamb10 nbrun5 google task 1 bash python trainpy datasetgoogle modelgcnvae tasktask1 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelgcnae tasktask1 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelsourcetargetgcnvae tasktask1 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelsourcetargetgcnae tasktask1 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelgravitygcnvae tasktask1 epochs200 learningrate02 hidden64 dimension33 lamb100 nbrun5 python trainpy datasetgoogle modelgravitygcnae tasktask1 epochs200 learningrate02 hidden64 dimension33 lamb100 nbrun5 google task 2 bash python trainpy datasetgoogle modelgcnvae tasktask2 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelgcnae tasktask2 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelsourcetargetgcnvae tasktask2 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelsourcetargetgcnae tasktask2 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelgravitygcnvae tasktask2 epochs200 learningrate02 hidden64 dimension33 lamb005 nbrun5 python trainpy datasetgoogle modelgravitygcnae tasktask2 epochs200 learningrate02 hidden64 dimension33 lamb005 normalizetrue epsilon10 nbrun5 google task 3 bash python trainpy datasetgoogle modelgcnvae tasktask3 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelgcnae tasktask3 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelsourcetargetgcnvae tasktask3 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelsourcetargetgcnae tasktask3 epochs200 learningrate02 hidden64 dimension32 nbrun5 python trainpy datasetgoogle modelgravitygcnvae tasktask3 epochs200 learningrate02 hidden64 dimension33 lamb100 nbrun5 python trainpy datasetgoogle modelgravitygcnae tasktask3 epochs200 learningrate02 hidden64 dimension33 lamb100 nbrun5 note set nbrun100 report mean auc ap along standard error 100 run paper recommend gpu usage faster learning cite please cite paper use code work bibtex inproceedingssalha2019gravity titlegravityinspired graph autoencoders directed link prediction authorsalha guillaume limnios stratis hennequin romain tran viet anh vazirgiannis michalis booktitleacm international conference information knowledge management cikm year2019
Graphs;cs6490 network security final project node embeddings botnet detection paper deepwalk graphsage node2vev datasets ctu datasets br internet thing refer
Graphs;graph convnets pytorch october 15 2017 br br img srcpicgraphconvnetjpg alignright width200 xavier bresson img srcpichome100jpg width15 height15 img srcpicgithub100jpg width15 height15 img srcpictwitter100jpg width15 height15 br br description prototype implementation pytorch nips16 paperbr convolutional neural network graph fast localized spectral filteringbr defferrard x bresson p vandergheynstbr advance neural information processing system 38443852 2016br arxiv preprint br br code objective code provides simple example graph convnets mnist classification taskbr graph 8nearest neighbor graph 2d gridbr signal graph mnist image vectorized 282 time 1 vectorsbr br installation sh git clone cd graphconvnetspytorch pip install r requirementstxt installation python 362 python checkinstallpy jupyter notebook run 2 notebook br result gpu quadro m4000br standard convnets 01standardconvnetlenet5mnistpytorchipynb accuracy 9931 speed 69 secepoch br graph convnets 02graphconvnetlenet5mnistpytorchipynb accuracy 9919 speed 1008 secepoch br br note pytorch yet implemented function torchmmsparse dense variable certainly implemented meantime defined new autograd function sparse variable called mysparsemm subclassing torchautogradfunction implementing forward backward pass python class mysparsemmtorchautogradfunction implementation new autograd function sparse variable called mysparsemm subclassing torchautogradfunction implementing forward backward pass def forwardself w x w sparse selfsaveforbackwardw x torchmmw x return def backwardself gradoutput w x selfsavedtensors gradinput gradoutputclone gradinputdldw torchmmgradinput xt gradinputdldx torchmmwt gradinput return gradinputdldw gradinputdldx br use algorithm problem cast analyzing set signal fixed graph want use convnets analysisbr br br br
Graphs;linear graph autoencoders repository provides python tensorflow code reproduce experiment article keep simple graph autoencoders without graph convolutional presented neurips 2019 workshop graph representation learning update extended conference version article available simple effective graph autoencoders onehop linear accepted ecmlpkdd 2020 update 2 prefer pytorch implementation linear graph ae vae available project see example introduction release tensorflow implementation following two graph embedding model paper linear graph autoencoders linear graph variational autoencoders together standard graph autoencoders ae graph variational autoencoders vae model 2layer 3layer graph convolutional network encoders kipf welling evaluate model link prediction node clustering task introduced paper provide cora citeseer pubmed datasets data folder refer section 4 paper direct link additional datasets used experiment code build upon thomas kipfs original tensorflow standard graph aevae linear ae vaefigureslinearsummarypng scalingup graph ae vae standard graph ae vae model suffer scalability issue order scale large graph million node egdes also provide implementation framework article degeneracy framework scalable graph ijcai 2019 paper propose train graph aevae dense subset node namely kcore subgraph propagate embedding representation remaining node using faster heuristic update provide implementation fastgae new effective method group scale graph ae vae degeneracy frameworkfiguresijcaisummarypng installation bash python setuppy install requirement tensorflow 1x networkx numpy scikitlearn scipy run experiment bash cd lineargae python trainpy modelgcnvae datasetcora tasklinkprediction python trainpy modellinearvae datasetcora tasklinkprediction command train standard graph vae 2layer gcn encoders line 2 linear graph vae line 3 cora dataset evaluate embeddings link prediction task parameter set default value bash python trainpy modelgcnvae datasetcora tasklinkprediction kcoretrue k2 python trainpy modelgcnvae datasetcora tasklinkprediction kcoretrue k3 python trainpy modelgcnvae datasetcora tasklinkprediction kcoretrue k4 adding kcoretrue model trained kcore subgraph instead using entire graph k parameter 0 maximal core number graph specify using k flag complete list parameter parameter type description default value model string name model amongbr gcnae graph ae kipf welling 2016 2layer gcn encoder inner product decoderbr gcnvae graph vae kipf welling 2016 gaussian distribution 2layer gcn encoders mu sigma inner product decoder br linearae linear graph ae introduced section 3 neurips workshop paper linear encoder inner product decoder br linearvae linear graph vae introduced section 3 neurips workshop paper gaussian distribution linear encoders mu sigma inner product decoder br deepgcnae deeper version graph ae 3layer gcn encoder inner product decoder br deepgcnvae deeper version graph vae gaussian distribution 3layer gcn encoders mu sigma inner product decoder gcnae dataset string name dataset amongbr cora scientific publication citation network br citeseer scientific publication citation network br pubmed scientific publication citation network br br provide preprocessed version coming repository please check website raw data br br specify additional graph dataset edgelist formatbr editing inputdatapy cora task string name machine learning evaluation task among br linkprediction link prediction br nodeclustering node clustering br br see section 4 supplementary material neurips 2019 workshop paper detail task linkprediction dropout float dropout rate 0 epoch int number epoch model training 200 feature boolean whether include node feature encoder false learningrate float initial learning rate adam optimizer 001 hidden int number unit gcn encoder hidden layer 32 dimension int dimension encoder output ie embedding dimension 16 kcore boolean whether run kcore decomposition use degeneracy framework ijcai paper false aevae trained entire graph false k int kcore use higher k smaller graph faster maybe le accurate training 2 nbrun integer number model run test 1 propval float proportion edge validation set link prediction 5 proptest float proportion edge test set link prediction 10 validation boolean whether report validation result epoch link prediction false verbose boolean whether print full comment detail true model paper cora bash python trainpy datasetcora modellinearae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetcora modellinearvae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetcora modelgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetcora modelgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetcora modeldeepgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetcora modeldeepgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 cora feature bash python trainpy datasetcora featurestrue modellinearae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetcora featurestrue modellinearvae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetcora featurestrue modelgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetcora featurestrue modelgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetcora featurestrue modeldeepgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetcora featurestrue modeldeepgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 citeseer bash python trainpy datasetciteseer modellinearae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetciteseer modellinearvae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetciteseer modelgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetciteseer modelgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetciteseer modeldeepgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetciteseer modeldeepgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 citeseer feature bash python trainpy datasetciteseer featurestrue modellinearae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetciteseer featurestrue modellinearvae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetciteseer featurestrue modelgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetciteseer featurestrue modelgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetciteseer featurestrue modeldeepgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetciteseer featurestrue modeldeepgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 pubmed bash python trainpy datasetpubmed modellinearae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetpubmed modellinearvae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetpubmed modelgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetpubmed modelgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetpubmed modeldeepgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetpubmed modeldeepgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 pubmed feature bash python trainpy datasetpubmed featurestrue modellinearae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetpubmed featurestrue modellinearvae tasklinkprediction epochs200 learningrate001 dimension16 nbrun5 python trainpy datasetpubmed featurestrue modelgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetpubmed featurestrue modelgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetpubmed featurestrue modeldeepgcnae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 python trainpy datasetpubmed featurestrue modeldeepgcnvae tasklinkprediction epochs200 learningrate001 hidden32 dimension16 nbrun5 note set tasknodeclustering hyperparameters evaluate model node clustering table 4 instead link prediction set nbrun100 report mean auc ap along standard error 100 run paper recommend gpu usage faster learning cite 1 please cite following paper use linear graph aevae code work neurips 2019 workshop version bibtex miscsalha2019keep titlekeep simple graph autoencoders without graph convolutional network authorsalha guillaume hennequin romain vazirgiannis michalis howpublishedworkshop graph representation learning 33rd conference neural information processing system neurips year2019 andor extended conference version bibtex inproceedingssalha2020simple titlesimple effective graph autoencoders onehop linear model authorsalha guillaume hennequin romain vazirgiannis michalis booktitleeuropean conference machine learning principle practice knowledge discovery database ecmlpkdd year2020 2 please cite following paper use kcore framework scalability work bibtex inproceedingssalha2019degeneracy titlea degeneracy framework scalable graph autoencoders authorsalha guillaume hennequin romain tran viet anh vazirgiannis michalis booktitle28th international joint conference artificial intelligence ijcai year2019
Graphs;hypernetworks pytorch repository implement 2layer gnns hypernetworks model pubmed citation network semisupervised learning setting nmpedge hypernetowrks extension qm9 depicted twolayer gnns hypernetworks model flow twolayer gnns hypernetworks flowmasterfigure2layergnnhypernetworkspng main file trainpy train 2layer gnns model either gcn gat trainnmppy train nmpedge hypernetowrks extension qm9 testnmppy test pretrained nmpedge hypernetowrks extension model qm9 main directory model folder model implemented gcn gat 2layer gnns hypernetworks gatgcn nmpedge nmpedge hypernetwork extension input input file consist parameter fed trainpy implementation require pytorch geometric pyg library requirement pytorch geomtric installation least pytorch 140 least cuda 100130 pytorch geometry documentation
Graphs;introduction pytorch implementation reproducing result cikm 2021 yifei shen yongji wu yao zhang caihua jun zhang khaled b letaief dongsheng li2021 powerful graph convolution recommendation paper code heavily built lightgcns code sigir 2020 xiangnan kuan deng xiang wang yan li yongdong zhang meng wang2020 lightgcn simplifying powering graph convolution network recommendation paper see pytorch also adopt exactly dataset traintest splitting code optimized speed rather simplicity enviroment requirement pip install r requirementstxt reproduce fig1 run untrained lightgcn gowalla dataset change base directory cd fig1 change rootpath codeworldpy command runsh log output reproduce table 34 run untrained lgcnide gfcf lightgcn amazonbook dataset change base directory cd table34 change rootpath codeworldpy reproduce result lightgcn table 34 python3 mainpy decay1e4 lr0001 layer3 seed2020 datasetamazonbook topks20 recdim256 reproduce result lgcnide table 34 python3 mainpy datasetamazonbook topks20 simplemodel lgnide reproduce result gfcf table 34 python3 mainpy datasetamazonbook topks20 simplemodel gfcf log output
Graphs;opengnn opengnn machine learning library learning graphstructured data built generality mind support task graph regression graphtosequence mapping support various graph encoders including ggnns gcns sequencegnns variation neural graph message library design usage pattern inspired us recent apis installation opengnn requires python 35 tensorflow 110 20 install library aswell commandline entry point run pip install e getting started experiment library use one datasets provided datadata folder example experiment chemical dataset first install rdkit library obtained running conda install c rdkit rdkit datachemdatachem folder run python getdatapy download dataset getting data generate node edge vocabulary using bash ognnbuildvocab fieldname nodelabels savevocab nodevocab moleculesgraphstrainjsonl ognnbuildvocab nopadtoken fieldname edge stringindex 0 savevocab edgevocab moleculesgraphstrainjsonl command line main entry point library ognnmain command bash ognnmain runtype modeltype model config configfileyml currently two run type trainandeval infer example train model previously extracted chemical data inside datachemdatachem using predefined model catalog bash ognnmain trainandeval modeltype chemmodel config configyml also define model custom python script model function example train using custom model modelpy using bash ognnmain trainandeval model modelpy config configyml training script doesnt log training standard output monitor training using tensorboard model directory defined datachemconfigymldatachemconfigyml training perform inference valid file running ognnmain infer modeltype chemmodel config configyml featuresfile moleculesgraphsvalidjsonl predictionfile moleculespredictedvalidjsonl example config file found datadata folder library library also easily integrated code following example show create ggnn encoder encode batch random graph python import tensorflow tf import opengnn ognn tfenableeagerexecution build batch graph random initial feature edge tfsparsetensor index 0 0 0 1 0 0 1 2 1 0 0 0 2 0 1 0 2 0 2 1 2 0 3 2 2 0 4 3 values1 1 1 1 1 1 1 denseshape3 1 5 5 nodefeatures tfrandomuniform3 5 256 graphsizes 3 1 5 encoder ognnencodersggnnencoder1 256 output state encoder edge nodefeatures graphsizes printoutputs graph represented sparse adjency matrix dimensionality numedgetypes x numnodes x numnodes initial distributed representation node similarly sequence batching need pad graph maximum number node graph acknowledgment design library implementation based gated graph neural since code adapted opennmttf spread across multiple file license library located base folderopennmtlicense rather header file reference use library research please cite inproceedings pfernandes2018structsumm titlestructured neural summarization authorpatrick fernandes miltiadis allamanis marc brockschmidt booktitleproceedings 7th international conference learning representation iclr year2019
Graphs;gatedgcnpattern experiment reporsitory contains simplified version code created dwivedi et al available contains code single experiment running gatedgcn pattern dataset node classification task appropriate instruction run code available colab please feel free create copy play used weight bias report feature visualize store result would plug api key able use br reference arxivs articledwivedi2020benchmarkgnns titlebenchmarking graph neural network authordwivedi vijay prakash joshi chaitanya k laurent thomas bengio yoshua bresson xavier journalarxiv preprint arxiv200300982 year2020 brbrbr
Graphs;404 found
Graphs;contextual graph markov model cgmm summary cgmm generative approach learning context graph combine information diffusion local computation use deep architecture stationarity assumption model preprocess graph fixed structure learning instead work graph size shape retaining scalability experiment show model work well compared expensive kernel method extensively analyse entire input structure order extract relevant feature contrast cgmm extract abstract feature architecture built incrementally hope exploitation proposed framework extended many direction contribute extensive use generative discriminative approach adaptive processing structured data repo library includes data script reproduce treegraph classification experiment reported paper describing method research software provided happen use modify code please remember cite foundation paper bacciu davide errica federico micheli alessio probabilistic learning graph via contextual architecture journal machine learning research 211341−39 2020 bacciu davide errica federico micheli alessio contextual graph markov model deep generative approach graph processing proceeding 35th international conference machine learning 80294303 2018 27th july 2020 paper accepted jmlr please see reference 3rd march 2020 update thanks amazing work daniele dramatically increased performance bigram computation c4 continuous posterior matrix operation place nested loop able get speedup 900x yes 900x nci1 single core bravo daniele 5th november 2019 update refactored whole repository allow easy experimentation incremental architecture new efficiency improvement coming soon stay tuned 24th may 2019 update provide extended refactored version cgmm implemented pytorch additional experimental routine try common graph classification task please refer paper version release tag original code paper create data set first need create data set let try parse nci1 python preparedatasetspy data datasetname nci1 config file specify nodetype discrete feature represented atom type social datasets imdbmulti python preparedatasetspy data datasetname imdbbinary usedegree config file specify nodetype continuous degree treated continuous value replicate experiment replicate experiment graph classification first modify configcgmmyml file accordingly use cgmm model execute python launchexperimentspy configfile configcgmmyml innerfolds none outerfolds 10 innerprocesses process use internal cross validation outerprocesses process use external cross validation dataset dataset string default datasets created implement external 10fold cv model assessment ie random split train test internal holdout split training set 10 validation set model selection change number data split modify innerfolds outerfolds argument accordingly note holdout technique associate innerouterfolds none reproducibility hampered different random split case node classification ppi use cgmmppi config file instead cgmm refactored case preprocess ppi running multiprocessing appending debug argument first time try train ppi cgmm
Graphs;mnmf codebeat repo abstract p alignjustify network embedding aiming learn lowdimensional representation node network paramount importance many real application one basic requirement network embedding preserve structure inherent property network previous network embedding method primarily preserve microscopic structure first secondorder proximity node mesoscopic community structure one prominent feature network largely ignored paper propose novel modularized nonnegative matrix factorization mnmf model incorporate community structure network embedding exploit consensus relationship representation node community structure jointly optimize nmf based representation learning model modularity based community detection model unified framework enables learned representation node preserve microscopic community structure also provide efficient updating rule infer parameter model together correctness convergence guarantee extensive experimental result variety realworld network show superior performance proposed method stateofthearts p p aligncenter img width720 srccommunityjpeg p model also available package karate repository provides tensorflow implementation mnmf described community preserving network embedding xiao wang peng cui jing wang jain pei wenwu zhu shiqiang yang proceeding thirstyfirst aaai conference artificial intelligence aaai17 reference matlab implementation available requirement codebase implemented python 352 package version used development networkx 24 tqdm 4195 numpy 1133 panda 0203 tensorflowgpu 1120 jsonschema 260 texttable 121 pythonlouvain 011 datasets p alignjustify code take input graph csv file every row indicates edge two node separated comma first row header node indexed starting 0 sample graph facebook politician dataset included data directory p logging p alignjustify model defined way parameter setting cluster quality logged every single epoch specifically log following p 1 hyperparameter setting save hyperparameter used experiment 2 cluster quality measured modularity calculate every epoch 3 runtime measure time needed optimization measured second option learning embedding handled srcmainpy script provides following command line argument input output option input str input graph path default datafoodedgescsv embeddingoutput str embeddings path default outputembeddingsfoodembeddingcsv clustermeanoutput str cluster center path default outputclustermeansfoodmeanscsv logoutput str log path default outputlogsfoodlog assignmentoutput str nodecluster assignment dictionary path default outputassignmentsfoodjson dumpmatrices bool whether trained model saved default true model option dimension int number dimension default 16 cluster int number cluster default 20 lambd float kkt penalty default 02 alpha float clustering penalty default 005 beta float modularity regularization penalty default 005 eta float similarity mixing parameter default 50 lowercontrol float floating point overflow control default 1015 iterationnumber int number power iteration default 200 earlystopping int early stopping round number based modularity default 3 example p alignjustify following command learn graph embedding cluster center writes disk node representation ordered id p p alignjustify creating mnmf embedding default dataset default hyperparameter setting saving embedding cluster centre log file default path p sh python srcmainpy turning model saving sh python srcmainpy dumpmatrices false creating embedding dataset facebook company saving output log custom place sh python srcmainpy input datacompanyedgescsv embeddingoutput outputembeddingscompanyembeddingcsv clustermeanoutput outputclustermeanscompanymeanscsv creating clustered embedding default dataset 128 dimension 10 cluster center sh python srcmainpy dimension 128 cluster 10 license
Graphs;rethinking pooling graph neural network image infoheadlinepng repo contains code rethinking pooling graph neural network neurips work build upon representative gnns introduce variant challenge need localitypreserving representation either using randomization clustering complement graph result show using variant result decrease performance author diego diegomesquitaaaltofi corresponding author amauri h sami requirement built implementation using pytorch v140 pytorch geometric v143 ogb v121 usage folder main file model diffpool gmn graclus mincut example code run graclus nonlocal variant complement nci1 python maingracluspy dataset nci1 reproduce poolingtype graclus python maingracluspy dataset nci1 reproduce poolingtype complement main file includes optional argument cite inproceedingsrethinkpooling2020 titlerethinking pooling graph neural network authord mesquita h souza kaski booktitleadvances neural information processing system neurips year2020 acknowledgement avoid implementation bias closely followed available code either reused modified code
Graphs;img width 400 knowledge graph kg data structure store information different entity node relation edge common approach using kg various machine learning task compute knowledge graph embeddings dglke high performance easytouse scalable package learning largescale knowledge graph embeddings package implemented top deep graph library developer run dglke cpu machine gpu machine well cluster set popular model including p aligncenter img altdglke architecture width600 br bfigureb dglke overall architecture p currently dglke support three task training train kg embeddings using dglketrainsingle machine dglkedisttraindistributed environment evaluation read pretrained embeddings evaluates embeddings link prediction task test set using dglkeeval inference read pretrained embeddings entitiesrelations linkage predicting inference task using dglkepredict embedding similarity inference task using dglkeembsim quick start install latest version dglke run sudo pip3 install dgl sudo pip3 install dglke train transe model fb15k dataset running following command dglbackendpytorch dglketrain modelname transel2 dataset fb15k batchsize 1000 negsamplesize 200 hiddendim 400 gamma 199 lr 025 maxstep 500 loginterval 100 batchsizeeval 16 adv regularizationcoef 100e09 test numthread 1 numproc 8 command download fb15k dataset train transe model save trained embeddings file performance scalability dglke designed learning scale introduces various novel optimization accelerate training knowledge graph million node billion edge benchmark knowledge graph consisting 86m node 338m edge show dglke compute embeddings 100 minute ec2 instance 8 gpus 30 minute ec2 cluster 4 machine 48 coresmachine result represent 2×∼5× speedup best competing approach p aligncenter img altvsgvfb15k width750 br bfigureb dglke v graphvite fb15k p p aligncenter img altvspbgfb width750 br bfigureb dglke v pytorchbiggraph freebase p learn detail interested optimization dglke please check detail cite use dglke scientific publication would appreciate citation following paper bibtex inproceedingsdglke author zheng da song xiang chao tan zeyuan ye zihao dong jin xiong hao zhang zheng karypis george title dglke training knowledge graph embeddings scale year 2020 publisher association computing machinery address new york ny usa booktitle proceeding 43rd international acm sigir conference research development information retrieval page 739–748 numpages 10 series sigir 20 license project licensed apache20 license
Graphs;gnnbased fake news detection open code installationinstallation datasetsdatasets modelsmodels pyg dgl intro contributehowtocontribute repo includes pytorchgeometric implementation series graph neural network gnn based fake news detection model gnn modelsuserguide implemented evaluated user preferenceaware fake news detection framework fake news detection problem instantiated graph classification task upfd framework make reproducible run without manual configuration upfd dataset example usage also available pytorchgeometric official repo welcome contribution result existing model sota result new model based dataset check hosted paperwithcode sota model performance use code project please cite following paper sigir21 bibtex inproceedingsdou2021user titleuser preferenceaware fake news detection authordou yingtong shu kai xia congying yu philip sun lichao booktitleproceedings 44th international acm sigir conference research development information retrieval year2021 installation install via pyg integrated official pytorchgeometric library please follow installation instruction install latest version pyg check code dataset usage install via dgl integrated official deep graph please follow installation instruction install latest version dgl check dataset class dataset usage manually install run code repo need python36 pytorch16 pytorchgeometric161 please follow installation instruction install pyg dependency installed using following command bash git clone cd gnnfakenews pip install r requirementstxt datasets installed latest version pyg dgl use builtin dataloaders download load upfd dataset install project manually download dataset 12gb via link unzip data data directory google drive baidu disk password fj43 dataset includes fakereal news propagation network twitter built according factcheck information news retweet graph originally extracted crawled near 20 million historical tweet user participated fake news propagation fakenewsnet generate node feature dataset statistic dataset shown data graph fake news total node total edge avg node per graph politifact 314 157 41054 40740 131 gossipcop 5464 2732 314262 308798 58 due twitter policy could release crawled user historical tweet publicly get corresponding twitter user information refer news list nodeidtwitterid mapping data two pkl file include dictionary key nodeids datasets value represent corresponding twitter userids upfd project use twitter developer get user information incorporate four node feature type dataset 768dimensional bert 300dimensional spacy feature encoded using pretrained word2vec respectively 10dimensional profile feature obtained twitter account profile refer profile feature extraction 310dimensional content feature composed 300dimensional user comment word2vec spacy embedding plus 10dimensional profile feature graph hierarchical treestructured graph root node represents news leaf node twitter user retweeted root news user node edge news node heshe retweeted news tweet two user node edge one user retweeted news tweet user following figure show upfd framework including dataset construction detail refer detail dataset p aligncenter br img width1000 br p model gnnbased fake news detection model gnnmodel directory finetune model according argument specified argparser model implemented model follows han yi shanika karunasekera christopher leckie graph neural network continual learning fake news detection social medium arxiv preprint arxiv200703316 2020 monti federico fabrizio frasca davide eynard damon mannion michael bronstein fake news detection social medium using geometric deep learning arxiv preprint arxiv190206673 2019 bian tian xi xiao tingyang xu peilin zhao wenbing huang yu rong junzhou huang rumor detection social medium bidirectional graph convolutional network proceeding aaai conference artificial intelligence vol 34 01 pp 549556 2020 kipf thomas n max welling semisupervised classification graph convolutional network arxiv preprint arxiv160902907 2016 veličković petar guillem cucurull arantxa casanova adriana romero pietro lio yoshua bengio graph attention network arxiv preprint arxiv171010903 2017 hamilton william l rex ying jure leskovec inductive representation learning large graph arxiv preprint arxiv170602216 2017 since upfd framework built upon easily try graph classification model like dataset contribute welcomed submit model code hyperparameters result repo via create pull request verifying result model added repo result updated please email ytongdougmailcommailtoytongdougmailcom inquiry
Graphs;404 found
Graphs;covid gat covid prediction pytorch graph attention network repo pytorch implementation graph attention network prediction covid case based adjacency matrix geographical node edge indicating commuting node edge feature including population proportion population 60 year old previous covid death case original pytorch implementation created forked repo pytorch implementation graph attention network gat model presented veličković et al 2017 repo forked initially official repository gat tensorflow available therefore make advantage pygat model research please cite following article velickovic2018graph titlegraph attention network authorvelivckovic petar cucurull guillem casanova arantxa romero adriana lio pietro bengio yoshua journalinternational conference learning representation year2018 noteaccepted poster
Graphs;pytorch implementation recurrent event network renet paper recurrent event network autoregressive structure inference temporal knowledge tldr propose autoregressive model infer graph structure unobserved time temporal knowledge graph extrapolation problem repository contains implementation renet architecture described paper p aligncenterimg srcfigsrenetpng width500p knowledge graph reasoning critical task natural language processing task becomes challenging temporal knowledge graph fact associated timestamp existing method focus reasoning past timestamps able predict fact happening future paper proposes recurrent event network renet novel autoregressive architecture predicting future interaction occurrence fact event modeled probability distribution conditioned temporal sequence past knowledge graph speciﬁcally renet employ recurrent event encoder encode past fact us neighborhood aggregator model connection fact timestamp future fact inferred sequential manner based two module evaluate proposed method via link prediction future time ﬁve public datasets extensive experiment demonstrate strength renet especially multistep inference future time stamp achieve stateoftheart performance five datasets make use code renet algorithm work please cite following paper bibtex inproceedingsjin2020renet titlerecurrent event network autoregressive structure inference temporal knowledge graph authorjin woojeong qu meng jin xisen ren xiang booktitleemnlp year2020 quick link installationinstallation train testtrainandtest related workrelatedwork datasetsdatasets baselinesbaselines predictive performancespredictiveperformances installation run following command create conda environment assume cuda101 bash conda create n renet python36 numpy conda activate renet pip install torch160cu101 torchvision070cu101 f conda install c dglteam dglcuda101 train test code renet rgcn aggregator included running user preprocess datasets attentive mean pooling aggregator model 012 bash cd datadataname python3 gethistorypy rgcn aggregator model 3 bash cd datadataname python3 gethistorygraphpy first pretrain global model bash python3 pretrainpy dataname gpu 0 dropout 05 nhidden 200 lr 1e3 maxepochs 20 batchsize 1024 train model bash python3 trainpy dataname gpu 0 dropout 05 nhidden 200 lr 1e3 maxepochs 20 batchsize 1024 ready test bash python3 testpy dataname gpu 0 nhidden 200 default hyperparameters give best performance model variant user must specify model variant described detail paper attentive aggregator model 0 mean aggregator model 1 pooling aggregator model 2 rgcn aggregator model 3 related work work extrapolation problem work problem many study temporal knowledge graph focused intrapolation problem organized list related temporal knowledge graph reasoning dynamic graph embedding knowledge graph embedding static graph embedding related literature temporal knowledge graph embedding dynamic graph embedding knowledge graph embedding static graph embedding etc organized list related datasets four datasets knowevolve datasets extrapolation problem time test set larger time train valid set time valid set also larger time train set data folder stattxt traintxt validtxt testtxtand gethistorygraphpy gethistorygraphpy getting history graph model stattxt first value number entity second value number relation traintxt validtxt testtxt first column subject entity second column relation third column object entity fourth column time fifth column knowevolves data format ignored renet relation name gdelt please refer gdlet baseline use following public code baseline hyperparameters validated embedding size among presented value baseline code embedding size batch size transe bordes et al 100 200 1024 distmult yang et al 100 200 1024 complex trouillon et al 50 100 200 100 rgcn schlichtkrull et al 200 default conve dettmers et al 200 128 knowevolve trivedi et al default default hyte dasgupta et al 128 default implemented tatranse tadistmult ttranse user run baseline following command bash cd baseline cudavisibledevices0 python3 tatransepy f 1 icews18 l 1 b 1024 n 1000 user find implementation baseline folder predictive performance icews18 dataset result filtered metric method mrr hits1 hits3 hits10 transe 1756 248 2695 4387 conve 3767 2991 4080 5169 tadistmult 2853 2030 3157 4496 renet mean 4238 3580 4499 5490 renet attn 4146 3467 4419 5444 renet pool 4135 3453 4405 5435 renet rgcn 4320 3663 4558 5591 gdelt dataset result filtered metric method mrr hits1 hits3 hits10 transe 1605 000 2610 4229 conve 3699 2805 4032 5044 tadistmult 2935 2211 3156 4139 renet mean 3915 3084 4307 5348 renet attn 3807 2944 4226 5293 renet pool 3799 3005 4140 5218 renet rgcn 4021 3254 4353 5383 wiki dataset result filtered metric method mrr hits1 hits3 hits10 transe 1930 1401 2401 2582 conve 4957 4823 5107 5253 tadistmult 4809 4601 4951 5170 renet mean 4830 4586 4936 5303 renet attn 5172 5060 5212 5372 renet pool 4515 4141 4698 5257 renet rgcn 5047 4980 5203 5316 yago dataset result filtered metric method mrr hits1 hits3 hits10 transe 3478 2027 4723 5335 conve 6432 6379 6597 6760 tadistmult 6272 5930 6494 6819 renet mean 6551 6385 6606 6803 renet attn 6579 6450 6600 6782 renet pool 6365 6125 6476 6745 renet rgcn 6569 6483 6632 6848
Graphs;graph convolutional network pytorch pytorch implementation graph convolutional network gcns semisupervised classification 1 highlevel introduction gcns see thomas kipf graph convolutional 2016 graph convolutional networksfigurepng note subtle difference tensorflow implementation pytorch reimplementation reimplementation serf proof concept intended reproduction result reported 1 implementation make use cora dataset 2 installation python setuppy install requirement pytorch 04 05 python 27 36 usage python trainpy reference 1 kipf welling semisupervised classification graph convolutional network 2 sen et al collective classification network data ai magazine cite please cite paper use code work articlekipf2016semi titlesemisupervised classification graph convolutional network authorkipf thomas n welling max journalarxiv preprint arxiv160902907 year2016
Graphs;neural message passing quantum chemistry implementation different model neural network graph explained article proposed gilmer et al 1 installation pip install r requirementstxt python mainpy installation rdkit running experiment using qm9 dataset need installing package done following instruction available data data used project downloaded bibliography 1 gilmer et al neural message passing quantum arxiv 2017 2 duvenaud et al convolutional network graph learning molecular nip 2015 3 li et al gated graph sequence neural iclr 2016 4 battaglia et al interaction network learning nip 2016 5 kipf et al semisupervised classification graph convolutional iclr 2017 6 defferrard et al convolutional neural network graph fast localized spectral nip 2016 7 kearnes et al molecular graph convolution moving beyond jcamd 2016 8 bruna et al spectral network locally connected network iclr 2014 cite articlegilmer2017 author justin gilmer samuel schoenholz patrick f riley oriol vinyals george e dahl title neural message passing quantum chemistry journal corr year 2017 author pau riba priba anjan dutta anjandutta
Graphs;schnet deep learning architecture quantum chemistry important package developed supported please consider switching new pytorchbased package schnet deep learning architecture allows spatially chemically resolved insight quantummechanical observables atomistic system requirement python 34 ase numpy tensorflow 10 see script folder training evaluation schnet model predicting total energy u0 gdb9 data set install python3 setuppy install example qm9 download convert qm9 data set python3 loadqm9py qm9destination train qm9 energy u0 prediction python3 trainenergyforcepy qm9destinationqm9db modeldir split50knpz ntrain 50000 nval 10000 fitenergy atomref qm9destinationatomrefnpz potential energy surface predict force energy fullerene c20 configuration python scriptsexamplemdpredictorpy modelsc20 modelsc20c20xyz relax geometry python scriptsexamplemdpredictorpy modelsc20 modelsc20c20xyz relax reference use schnet research please cite kt schütt pj kindermans h e sauceda chmiela tkatchenko kr müller schnet continuousfilter convolutional neural network modeling quantum interaction advance neural information processing system 30 pp 9921002 2017 kt schütt f arbabzadah chmiela kr müller tkatchenko quantumchemical insight deep tensor neural network nature communication 8 13890 2017 doi
Graphs;introduction propose deep learning based model wellorganized dataset context aware paper citation recommendation model comprises document encoder context encoder us graph convolutional network layer bidirectional encoder representation transformer pretrained model textual data modifying related peerreadaan dataset propose new dataset called fulltextpeerread fulltextann containing context sentence cited reference paper metadata code based paper contextaware citation recommendation model bert graph convolutional main result data full context created processing full context ann created processing arxiv disclosed due copyright two type data aan peerread column identical header description strongtargetidstrong citing paper id strongsourceidstrong cited paper id strongleftcitatedtextstrong text left citation tag citing strongrightcitatedtextstrong text right citation tag citing strongtargetyearstrong release target paper year strongsourceyearstrong release source paper year runclassifierpy main script train bert bertgcn model python python runclassifierpy option general parameter model required mode run runclassifierpy script possible value bert bertgcn dataset required dataset run runclassifierpy script possible value aan peerread frequency required parse datasets frequently maxseqlength length cited text use gpu gpu run code bert parameter refer dotrain dopredict datadir vocabfile bertconfigfile initcheckpoint gcnpretrainpy want use bertgcn run python python gcnpretrainpy option   gcn parameter refer gcnmodel gcnlr gcnepochs gcnhidden1 gcnhidden2
Graphs;lightgcn tensorflow implementation sigir 2020 paper xiangnan kuan deng xiang wang yan li yongdong zhang meng wang2020 lightgcn simplifying powering graph convolution network recommendation paper contributor dr xiangnan staffustceducnhexn kuan deng yingxin wu also provide pytorch implementation lightgcn contributor jianbai ye introduction work aim simplify design gcn make concise appropriate recommendation propose new model named lightgcn including essential component gcn—neighborhood aggregation—for collaborative filtering environment requirement code tested running python 365 required package follows tensorflow 1110 numpy 1143 scipy 110 sklearn 0191 cython 02915 c evaluator implemented c code output metric training much efficient python evaluator need compiled first using following command python setuppy buildext inplace compilation c code run default instead python code example run 3layer lightgcn instruction command clearly stated code see parser function lightgcnutilityparserpy gowalla dataset command python lightgcnpy dataset gowalla regs 1e4 embedsize 64 layersize 646464 lr 0001 batchsize 2048 epoch 1000 output log evalscorematrixfoldout cpp nusers29858 nitems40981 ninteractions1027370 ntrain810128 ntest217242 sparsity000084 epoch 1 303s train046925046911 000014 epoch 2 271s train021866021817 000048 epoch 879 816s 313s test013271012645 000626 000000 recall018201 precision005601 ndcg015555 early stopping trigger step 5 log018201370537281036 best iter38328296 recall018236 precision005607 ndcg015539 yelp2018 dataset command python lightgcnpy dataset yelp2018 regs 1e4 embedsize 64 layersize 646464 lr 0001 batchsize 2048 epoch 1000 output log evalscorematrixfoldout cpp nusers31668 nitems38048 ninteractions1561406 ntrain1237259 ntest324147 sparsity000130 epoch 1 565s train033843033815 000028 epoch 2 531s train016253016192 000061 epoch 679 1046s 129s test017217016289 000929 000000 recall006359 precision002874 ndcg005240 early stopping trigger step 5 log006359195709228516 best iter28428150 recall006367 precision002868 ndcg005236 amazonbook dataset command python lightgcnpy dataset amazonbook regs 1e4 embedsize 64 layersize 646464 lr 0001 batchsize 8192 epoch 1000 output log evalscorematrixfoldout cpp nusers52643 nitems91599 ninteractions2984108 ntrain2380730 ntest603378 sparsity000062 epoch 1 532s train057471057463 000008 epoch 2 473s train031518031478 000040 epoch 779 1817s 790s test020300019434 000866 000000 recall004120 precision001703 ndcg003186 early stopping trigger step 5 log004119725897908211 best iter33498754 recall004123 precision001710 ndcg003189 note duration training testing depends running environment dataset provide three processed datasets gowalla yelp2018 amazonbook traintxt train file line user herhis positive interaction item useridt list itemidn testtxt test file positive instance line user herhis positive interaction item useridt list itemidn note treat unobserved interaction negative instance reporting performance userlisttxt user file line triplet orgid remapid one user orgid remapid represent id user original datasets respectively itemlisttxt item file line triplet orgid remapid one item orgid remapid represent id item original datasets respectively efficiency improvement parallelized sampling cpu c evaluation topk recommendation
Graphs;deep graph convolutional neural network dgcnn update 3192018 pytorch implementation dgcnn prefer using python powerful deep neural network toolbox graph classification named deepgraphcnn dgcnn dgcnn feature propagationbased graph convolution layer extract vertex feature well novel sortpooling layer sort vertex representation instead summing sorting enables learning global graph topology retains much node information summing sortpooling layer support backpropagation sort vertex without using preprocessing software nauty enables elegant endtoend training framework information please refer zhang z cui neumann chen endtoend deep learning architecture graph classification proc aaai conference artificial intelligence aaai18 dgcnn written torch matlab required want compare dgcnn graph kernel run th mainlua try dgcnn run dgcnn deep learning environment yet install cuda torch follows suppose gpu install cuda following install torch following link install necessary torch library cutorch cunn order luarocks install local libraryname able run torchbased neural network folder data contains dat graph datasets read dgcnn already mutagdat included demo graph datasets need generated manually th utilsgeneratetorchgraphslua transforms mat graph datasets datarawdata dat format torchmatio required installed 1 osx brew install homebrewsciencelibmatio 2 ubuntu sudo aptget install libmatio2 luarocks install local matio case cannot install torchmatio also provide converted dat downloading directly torch graph run dgcnn dataset dd whose maximum node label 89 learning rate 1e5 maximum epoch number 100 type th mainlua dataname dd maxnodelabel 89 learningrate 1e5 maxepoch 100 append batch batchsize 16 run minibatch optimization repeatedly run dgcnn dataset using different shuffle order use provided runallsh script time runallsh xxx run dgcnn dataset xxx ith gpu machine 100 time using shuffle order stored datashuffle matio required main program dgcnn mainlua please refer advanced function play use datasets dgcnn read graph dataset format dataset instancei 1 ai 2 xi labeli labeli check format type dataset torchloaddatamutagdat torch command line th also provide generatetorchgraphslua utils convert standard wl kernel toolbox graph format dgcnn format option testnumber 200 mainlua allows specifying last 200 example dataset testing data convenient put testing data training data constructing dataset compare graph kernel kernel matrix compared graph kernel put datakernels run computekernelsm compute kernel matrix wl gk rw run computepkkernelsm compute kernel matrix pk since pk software us different graph format also provide precomputed kernel matrix downloading case dont want compute take really long time kernel run graph kernel run comparem matlab accuracy result graph kernel reported time fixed data shuffling order generated saved datashuffle dataset thus trainvaltest split fixed shuffle order already included toolbox change randstart inside comparem generate shuffle order required library torch library needed path torch nn cunn cutorch optim install using luarocks install local libraryname main folder contains required module sortpooling graphconv graphrelu graphtanh graphselecttable graphconcattable compare dgcnn graph kernel libsvm required installed softwarelibsvm322 zip libsvm included software already unzip compile mex file according documentation compare graph kernel toolbox graphkernels propagationkernelsmaster included software already come following paper n shervashidze p schweitzer e j van leeuwen k mehlhorn k borgwardt weisfeilerlehman graph kernel journal machine learning research 1225392561 2011 marion neumann roman garnett christian bauckhage kristian kersting propagation kernel efficient graph kernel propagated information 2015 machine learning 102 2 pp 209245 reference find code useful please cite paper inproceedingszhang2018end titlean endtoend deep learning architecture graph classification authorzhang muhan cui zhicheng neumann marion chen yixin booktitleaaai pages44384445 year2018 muhan zhang muhanwustledu 1222017
Graphs;cnngraphpytorch repository unofficial implement paper convolutional neural network graph fast localized spectral filtering nip 2016 pytorch support train evaluate network 20news mnist requirement python 36 pytorch 11 train evaluation following script train evaluate graphbased cnn corresponding network architecture mnist dataset bash python trainpy data mnist filter fourier gclayers 1 python trainpy data mnist filter fourier gclayers 2 python trainpy data mnist filter chebyshev gclayers 1 python trainpy data mnist filter chebyshev gclayers 2 gclayers1 corresponds network architecture gc32 gclayers2 gc32p4gc64p4fc512 train evaluate 20news dataset need run following script order preprocess document generate required intermediate data python scripts20newspreprocesspy train network different negtwork architecture 20news dataset bash python trainpy data 20news filter fourier gclayers 1 python trainpy data 20news filter chebyshev gclayers 1 note code folder lib completely borrowed original codebase redundant function removed part responsible graph building coarsening performance mnist filter gclayer1 gclayer2 fourier 09747 09788 chebyshev 09816 09818 20news filter gclayer1 gclayer2 fourier 05504 chebyshev 05554 acknowledgement thanks official implemented tensorflow awesome pytorch team
Graphs;grarep codebeat repo scipy implementation grarep learning graph representation global structural information www 2015 p aligncenter img width800 srcgrareppng p abstract p alignjustify paper present grarep novel model learning vertex representation weighted graph model learns low dimensional vector represent vertex appearing graph unlike existing work integrates global structural information graph learning process also formally analyze connection work several previous research effort including deepwalk model perozzi et al well skipgram model negative sampling mikolov et al conduct experiment language network social network well citation network show learned global representation effectively used feature task clustering classification visualization empirical result demonstrate representation significantly outperforms stateoftheart method tasksp model also available package karate repository provides scipy implementation grarep described paper grarep learning graph representation global structural information shaosheng cao wei lu qiongkai xu www 2015 matlab julia implementation available respectively requirement codebase implemented python 352 package version used development networkx 24 tqdm 4281 numpy 1154 panda 0234 texttable 150 scipy 110 argparse 110 scikitlearn 0200 datasets p alignjustify code take edge list graph csv file every row indicates edge two node separated comma first row header node indexed starting 0 sample graph cora included inputedges directory p output p alignjustify embedding saved output directory embedding header column node identifier finally embedding sorted identifier columnp option training model handled srcmainpy script provides following command line argument edgepath str edge list csv default inputedgescoracsv outputpath str output embedding csv default outputcoragrarepcsv dimension int number dimension per embedding default 16 order int number adjacency matrix power default 5 iteration int svd iteration default 20 seed int random seed default 42 example following command learn model save embedding training model default dataset sh python srcmainpy p aligncenter img width500 srcgrarepgif p training grarep model higher dimension size sh python srcmainpy dimension 32 changing batch size sh python srcmainpy order 3 license
Graphs;networked multiagent rl nmarl repo implement stateoftheart marl algorithm networked system control observability communication agent limited neighborhood fair comparison algorithm applied a2c agent classified two group ia2c contains noncommunicative policy utilize neighborhood information whereas ma2c contains communicative policy certain communication protocol available ia2c algorithm policyinferring lowe ryan et al multiagent actorcritic mixed cooperativecompetitive environment advance neural information processing system fingerprint foerster jakob et al stabilising experience replay deep multiagent reinforcement learning arxiv preprint arxiv170208887 consensusupdate zhang kaiqing et al fully decentralized multiagent reinforcement learning networked agent arxiv preprint arxiv180208757 available ma2c algorithm dial foerster jakob et al learning communicate deep multiagent reinforcement learning advance neural information processing system commnet sukhbaatar sainbayar et al learning multiagent communication backpropagation advance neural information processing system neurcomm inspired gilmer justin et al neural message passing quantum chemistry arxiv preprint arxiv170401212 available nmarl scenario atsc grid adaptive traffic signal control synthetic traffic grid atsc monaco adaptive traffic signal control realworld traffic network monaco city cacc catchup cooperative adaptive cruise control catching leadinig vehicle cacc slowdown cooperative adaptive cruise control following leading vehicle slow requirement python3 35 1120 110 usage first define hyperparameters including algorithm dnn structure config file configdir examplesconfig create base directory experiement basedir atsc grid please call buildfilepyenvslargegriddata generate sumo network file training 1 train new agent run python3 mainpy basedir basedir train configdir configdir training configdata trained model output basedirdata basedirmodel respectively 2 access tensorboard training run tensorboard logdirbasedirlog 3 evaluate trained agent run python3 mainpy basedir basedir evaluate evaluationseeds seed evaluation data output basedirevadata make sure evaluation seed different used training 4 visualize agent behavior atsc scenario run python3 mainpy basedir basedir evaluate evaluationseeds seed demo recommended use one evaluation seed demo run launch sumo gui viewxmlenvslargegriddata applied visualize queue length intersectin delay edge color thickness reproducibility paper result based outofdate sumo version 0320 rerun atsc experiment sumo 120 using master code provided following training plot reference paper conclusion remain grid monaco figsgridtrainpng figsnettrainpng pytorch impelmention also avaliable branch citation implementation detail underlying reasoning please check paper multiagent reinforcement learning networked system inproceedings chu2020multiagent titlemultiagent reinforcement learning networked system control authortianshu chu sandeep chinchali sachin katti booktitleinternational conference learning representation year2020
Graphs;benchmarking graph neural network br update nov 2 2020 project based dgl 042 see relevant dependency defined environment yml file cpuenvironmentcpuyml gpuenvironmentgpuyml numerical experiment report faster training time dgl 042 compared dgl 052 version project compatible dgl 052 relevant dependency please use added zincfulldatascriptdownloadmoleculessh dataset 249k molecular graph scriptsscriptszincfull jun 11 2020 second release project major update added experimental pipeline weisfeilerlehmangnns operating dense rank2 tensor added leaderboard datasets updated pattern dataset fixed bug pattern cluster accuracy moved first release new arxivs version mar 3 2020 first release project br img srcdocsgnnsjpg alignright width350 1 benchmark installation follow instructionsdocs01benchmarkinstallationmd install benchmark setup environment br 2 download datasets proceed followsdocs02downloaddatasetsmd download benchmark datasets br 3 reproducibility use pagedocs03runcodesmd run code reproduce published result br 4 adding new dataset instructionsdocs04adddatasetmd add dataset benchmark br 5 adding messagepassing gcn stepbystep directionsdocs05addmpgcnmd add mpgcn benchmark br 6 adding weisfeilerlehman gnn stepbystep directionsdocs06addwlgnnmd add wlgnn benchmark br 7 leaderboards leaderboardsdocs07leaderboardsmd gnn model dataset instructionsdocs07contributeleaderboardsmd contribute leaderboards br 8 reference arxivs articledwivedi2020benchmarkgnns titlebenchmarking graph neural network authordwivedi vijay prakash joshi chaitanya k laurent thomas bengio yoshua bresson xavier journalarxiv preprint arxiv200300982 year2020 brbrbr
Graphs;geniepathpytorch pytorch implementation geniepath model geniepath graph neural network adaptive receptive geniepath scalable approach learning adap tive receptive field neural network defined permuta tion invariant graph data geniepath propose adap tive path layer consists two complementary function de signed breadth depth exploration respectively former learns importance different sized neighbor hood latter extract filter signal aggregated neighbor different hop away method work transductive inductive setting extensive ex periments compared competitive method show approach yield stateoftheart result large graph model img src img src requirement usage install package bash pip install r requirementstxt pip install r requirementstxt choose model ppigeniepathpy bash modelgeniepath import geniepath net modelgeniepath import geniepathlazy net run bash python ppigeniepathpy todo x finish rough implementation f1score 09709 geniepath 09762 geniepathlazy dim 256 lstmhidden 256 x tune model reference contribute
Graphs;physion evaluating physical prediction vision human machine animation 8 scenariosfiguresscenarioanimationgif repo contains code data reproduce result neurips 2021 paper physion evaluating physical prediction vision human brief overview please check project website please see detail download physion dataset replicate modeling human experiment statistical analysis reproduce result 1 downloading physion datasetdownloadingthephysiondataset 2 dataset generationdatasetgeneration 3 modeling experimentsmodelingexperiments 4 human experimentshumanexperiments 5 comparing model humanscomparingmodelsandhumans downloading physion dataset downloading physion test set aka stimulus video physion test set manually evaluated ensure behavior simulated physic feature glitch unexpected behavior small number stimulus contain potential physic glitch identified stimulus name seen hereanalysismanualstimevaluationbuggystimstxt downloaded following link download url physiontestcore 270 mb physiontestcore need evaluate human model exactly test stimulus used paper contains eight directory one scenario type eg collide contain domino drape drop link roll support directory contains three subdirectory map contains png segmentation map test stimulus indicating location agent object red patient object yellow mp4s contains mp4 video file presented human participant agent patient object appear random color mp4sredyellow contains mp4 video file passed model agent patient object consistently appear red yellow respectively download url physiontestcomplete 380 gb physiontestcomplete want need detailed metadata test stimulus stimulus encoded hdf5 file containing comprehensive information regarding depth surface normal optical flow segmentation map associated frame trial well information physical state object time step download url also download testing data individual scenario table next section downloading physion training set downloading physiontraindynamics physiontraindynamics contains full dataset used train dynamic module model benchmarked paper consists approximately 2k stimulus per scenario type download url 770 mb downloading physiontrainreadout physiontrainreadout contains separate dataset used training objectcontact prediction ocp module model pretrained physiontraindynamics dataset consists 1k stimulus per scenario type agent patient object readout stimulus consistently appear red yellow respectively mp4sredyellow example physiontestcore nb code using readout set benchmark pretrained model model trained physion training set released prior publication download url complete physiontraindynamics physiontrainreadout scenario dynamic training set readout training set test set domino support collide contain drop roll link drape dataset generation repo depends output specifically used generate dataset physical scenario aka stimulus including training datasets used train physicalprediction model well test datasets used measure prediction accuracy physicalprediction model human participant instruction using threedworld simulator regenerate datasets used work found link downloading physion testing training readout fitting datasets found modeling experiment todo add topic sentence state highlevel inputoutput relationship physopt current repo modeling component repo depends repo repo implement interface wide variety physic prediction model literature neural network otherwise adapted accept input provided training testing datasets produce output comparison human measurement also contains code model training evaluation specifically implement three traintest procols protocol candidate physic model architecture trained using model native loss function specified model author separately scenario listed eg domino support c produce eight separatelytrained model per candidate architecture one scenario separate model tested comparison human testing data scenario protocol candidate physic architecture trained mixed data scenario simultaneously using model native loss function single model tested compared human separately scenario allbutone protocol candidate physic architecture trained mixed data drawn one scenario separately possible choice heldout scenario produce eight separatelytrained model per candidate architecture one heldout scenario separate model tested comparison human testing data scenario result three protocol separately compared human described section comparison human model modelhuman comparison carried using representationlearning paradigm model trained native loss function encoded original author model trained model evaluated specific physion redobjectcontactsyellowzone prediction task evaluation carried training readout implemented linear logistic regression readout always trained perscenario fashion currently physopt implement following specific physic prediction model model name code link original paper description svg denton fergus imagelike latent op3 veerapaneni et al cswm kipf et al rpin qi et al pvggmlp pvgglstm pdeitmlp touvron et al pdeitlstm gns sanchezgonzalez et al gnsr dpi li et al human experiment repo contains code conduct human behavioral experiment reported paper well analyze resulting data human modeling experiment detail experimental design analysis plan documented study contained within repository format preregistration adapted template provided open science framework study put type version control rest codebase project main directory repo contains experiment directory contains code run online human behavioral experiment reported paper detailed documentation code found file nested within experiment analysis aka notebook directory contains analysis jupyterrmd notebook repo assumes also imported model evaluation result physopt result directory contains intermediate result modelinghuman experiment contains three subdirectory csv plot summary resultscsv contains csv file containing tidy dataframes raw data resultsplots contains pdfpng plot selection polished formatted inclusion paper using adobe illustrator important pushing csv file containing human behavioral data public code repository triple check data properly anonymized mean bare amt worker id prolific participant id stimulus directory contains downloadpreprocessing script data aka stimulus input human behavioral experiment repo assumes generated stimulus using tdwphysics repo us code directory upload stimulus aws s3 generate metadata control timeline stimulus presentation human behavioral experiment utils directory meant contain file containing general helper function comparing model human result reported paper reproduced running jupyter notebook contained analysis directory 1 downloading result download raw human model prediction behavior please navigate analysis directory execute following command command line python downloadresultspy script fetch several csv file download subdirectory within resultscsv work please download zipped folder csv move result directory 2 reproducing analysis reproduce key analysis reported paper please run following notebook sequence summarizehumanmodelbehavioripynb purpose notebook apply preprocessing human behavioral data visualize distribution compute summary statistic human physical judgment visualize distribution compute summary statistic model physical judgment conduct humanmodel comparison output summary csvs used statistical modeling create publicationquality visualization inferencehumanmodelbehavioripynb purpose notebook visualize human model prediction accuracy proportion correct visualize averagehuman model agreement rmse visualize humanhuman modelhuman agreement cohens kappa compare performance model paperplotsipynb purpose notebook create publicationquality figure inclusion paper
Graphs;torchrgcn torchrgcn pytorch implementation rgcn originally proposed schlichtkrull et al modeling relational data graph convolutional reproduce link prediction node classification experiment original paper using reproduction explain rgcn furthermore present two new configuration rgcn getting started requirement conda 48 python 37 following 1 download datasets bash getdatash 2 install dependency inside new virtual environment bash setupdependenciessh 3 activate virtual environment conda activate torchrgcnvenv 4 install torchrgcn module pip install e usage configuration file hyperparameters different experiment found file configsconfigs naming convention file follows configsmodelexperimentdatasetyaml model rgcn standard rgcn model crgcn compression rgcn model ergcn embedding rgcn model experiment lp link prediction nc node classification datasets link prediction wn18 fbtoy node classification aifb mutag bgs part 1 reproduction link prediction link prediction modelimageslinkpredictionpng original link prediction implementation run link prediction experiment using rgcn model using python experimentspredictlinkspy configsrgcnlpdatasetyaml make sure replace dataset one following dataset name fbtoy wn18 node classification node classification modelimagesnodeclassificationpng original node classification implementation run node classification experiment using rgcn model using python experimentsclassifynodespy configsrgcnncdatasetyaml make sure replace dataset one following dataset name aifb mutag bgs part 2 new rgcn configuration node classification node embeddings run node classification experiment use python experimentsclassifynodespy configsergcnncdatasetyaml make sure replace dataset one following dataset name aifb mutag bgs link prediction compressed node embeddings crgcn link prediction modelimageslinkpredictioncompressionpng run link prediction experiment use python experimentspredictlinkspy configscrgcnlpdatasetyaml make sure replace dataset one following dataset name fbtoy wn18 dataset reference node classification aifb stephan bloehdorn york sure kernel method mining instance data semantic web 6th international semantic web conference 2007 mutag k debnath r l lopez de compadre g debnath jshusterman c hansch structureactivity relationship mutagenic aromatic heteroaromatic nitrocompounds correlation molecular orbital energy j med chem34786–797 1991 bgs de vries gkd fast approximation weisfeilerlehman graph kernel rdf european conference machine learning principle practice knowledge discovery database 2013 de boer v wielemaker j van gent j hildebrand isaac van ossenbruggen j schreiber g supporting linked data production cultural heritageinstitutes amsterdam museum case semantic web research application 2012 link prediction wn18 antoine bordes nicolas usunier alberto garciaduran jason weston oksana yakhnenko translating embeddings modeling multirelational advance neural information processing system 2013 fbtoy daniel ruffinelli samuel broscheit rainer gemulla teach old dog new trick training knowledge graph international conference learning representation 2019
Graphs;gatpyg graph attention networkspytorchgeometric version run bash python trainpy need know set parameter bash python trainpy help parameter name command line argument one listed original guide experimental replication experimental result described original dataset could reproduced setting following dataset nclass weightdecay cora 7 5e4 citeseer 6 5e4 pubmed 3 1e3
Graphs;logo img width400 pysr parallel symbolic regression built julia interfaced python us regularized evolution simulated annealing gradientfree optimization doc pip documentation pronounced like py python sur surface cite test status linux window macos coverage check purejulia backend package symbolic regression interpretable machine learning algorithm lowdimensional problem tool search equation space find algebraic relation approximate dataset one also extend approach higherdimensional space using neural network proxy explained apply nbody problem one essentially us symbolic regression convert neural net analytic equation thus tool simultaneously present explicit powerful way interpret deep model backstory previously used efficient userfriendly tool however eureqa guionly doesnt allow userdefined operator distributed capability become proprietary recently merged online service thus goal package opensource symbolic regression tool efficient eureqa also exposing configurable python interface installation pysr us julia python need installed install julia see instruction dont use condaforge version doesnt seem work properly install pysr bash pip3 install pysr python3 c import pysr pysrinstall second line install update required julia package including pycalljl common issue stage solved tweaking julia package use uptodate package docker also test pysr docker without installing locally running following command root directory repo bash docker build pull rm f dockerfile pysr build image called pysr run bash docker run rm v pwddata pysr ipython link current directory container data directory launch ipython quickstart demo code also found examplepy python import numpy np pysr import pysr best dataset x 2 nprandomrandn100 5 2 npcosx 3 x 0 2 2 learn equation equation pysr x niterations5 binaryoperators unaryoperators co exp sin predefined library operator see doc invx 1x define operator julia syntax use ctlc exit early printbestequations give python x02 2000016cosx3 19999845 second additional call pysr significantly faster startup time since first call julia compile cache function symbolic regression backend one also use besttex get latex form bestcallable get function call us score balance complexity error however one see full list equation python printequations panda table additional column mse mean square error formula score metric akin occam razor use help select true equation sympyformat sympy equation lambdaformat lambda function equation pas value
Graphs;pypiimage pypiurl testingimage testingurl lintingimage lintingurl coverageimage coverageurl pytorch cluster pypi versionpypiimagepypiurl testing statustestingimagetestingurl linting statuslintingimagelintingurl code coveragecoverageimagecoverageurl package consists small extension library highly optimized graph cluster algorithm use package consists following clustering algorithm graclusgraclus dhillon et al weighted graph cut without eigenvectors multilevel pami 2007 voxel grid poolingvoxelgrid eg simonovsky komodakis dynamic edgeconditioned filter convolutional neural network cvpr 2017 iterative farthest point samplingfarthestpointsampling eg qi et al pointnet deep hierarchical feature learning point set metric nip 2017 knnknngraph radiusradiusgraph graph generation clustering based nearestnearest point random walk samplingrandomwalksampling eg grover leskovec node2vec scalable feature learning kdd 2016 included operation work varying data type implemented cpu gpu installation anaconda update install pytorchcluster via major ospytorchcuda combination 🤗 given pytorch 180 simply run conda install pytorchcluster c pyg binary alternatively provide pip wheel major ospytorchcuda combination see pytorch 1100 install binary pytorch 1100 simply run pip install torchcluster f cuda replaced either cpu cu102 cu113 depending pytorch installation cpu cu102 cu113 linux ✅ ✅ ✅ window ✅ ✅ ✅ macos ✅ pytorch 190191 install binary pytorch 190 191 simply run pip install torchcluster f cuda replaced either cpu cu102 cu111 depending pytorch installation cpu cu102 cu111 linux ✅ ✅ ✅ window ✅ ✅ ✅ macos ✅ note binary older version also provided pytorch 140 pytorch 150 pytorch 160 pytorch 170171 pytorch 180181 following procedure source ensure least pytorch 140 installed verify cudabin cudainclude path cpath respectively eg python c import torch printtorchversion 140 python c import torch printtorchversion 110 echo path usrlocalcudabin echo cpath usrlocalcudainclude run pip install torchcluster running docker container without nvidia driver pytorch need evaluate compute capability may fail case ensure compute capability set via torchcudaarchlist eg export torchcudaarchlist 60 61 72ptx 75ptx function graclus greedy clustering algorithm picking unmarked vertex matching one unmarked neighbor maximizes edge weight gpu algorithm adapted fagginger auer bisseling gpu algorithm greedy graph lncs 2012 python import torch torchcluster import gracluscluster row torchtensor0 1 1 2 col torchtensor1 0 2 1 weight torchtensor1 1 1 1 optional edge weight cluster graclusclusterrow col weight printcluster tensor0 0 1 voxelgrid clustering algorithm overlay regular grid userdefined size point cloud cluster point within voxel python import torch torchcluster import gridcluster po torchtensor0 0 11 9 2 8 2 2 8 3 size torchtensor5 5 cluster gridclusterpos size printcluster tensor0 5 3 0 1 farthestpointsampling sampling algorithm iteratively sample distant point regard rest point python import torch torchcluster import fps x torchtensor1 1 1 1 1 1 1 1 batch torchtensor0 0 0 0 index fpsx batch ratio05 randomstartfalse printindex tensor0 3 knngraph computes graph edge nearest k point args x tensor node feature matrix shape n f k int number neighbor batch longtensor optional batch vector shape n assigns node specific example batch need sorted default none loop bool optional true graph contain selfloops default false flow string optional flow direction using combination message passing sourcetotarget targettosource default sourcetotarget cosine boolean optional true use cosine distance instead euclidean distance find nearest neighbor default false numworkers int number worker use computation effect case batch none input lie gpu default 1 python import torch torchcluster import knngraph x torchtensor1 1 1 1 1 1 1 1 batch torchtensor0 0 0 0 edgeindex knngraphx k2 batchbatch loopfalse printedgeindex tensor1 2 0 3 0 3 1 2 0 0 1 1 2 2 3 3 radiusgraph computes graph edge point within given distance args x tensor node feature matrix shape n f r float radius batch longtensor optional batch vector shape n assigns node specific example batch need sorted default none loop bool optional true graph contain selfloops default false maxnumneighbors int optional maximum number neighbor return element number actual neighbor greater maxnumneighbors returned neighbor picked randomly default 32 flow string optional flow direction using combination message passing sourcetotarget targettosource default sourcetotarget numworkers int number worker use computation effect case batch none input lie gpu default 1 python import torch torchcluster import radiusgraph x torchtensor1 1 1 1 1 1 1 1 batch torchtensor0 0 0 0 edgeindex radiusgraphx r25 batchbatch loopfalse printedgeindex tensor1 2 0 3 0 3 1 2 0 0 1 1 2 2 3 3 nearest cluster point x together nearest given query point batchxy vector need sorted python import torch torchcluster import nearest x torchtensor1 1 1 1 1 1 1 1 batchx torchtensor0 0 0 0 torchtensor1 0 1 0 batchy torchtensor0 0 cluster nearestx batchx batchy printcluster tensor0 0 1 1 randomwalksampling sample random walk length walklength node index start graph given row col python import torch torchcluster import randomwalk row torchtensor0 1 1 1 2 2 3 3 4 4 col torchtensor1 0 2 3 1 4 1 4 2 3 start torchtensor0 1 2 3 4 walk randomwalkrow col start walklength3 printwalk tensor0 1 2 4 1 3 4 2 2 4 2 1 3 4 2 4 4 3 1 0 running test python setuppy test c api torchcluster also offer c api contains c equivalent python model mkdir build cd build add dwithcudaon support cuda needed cmake make make install
Graphs;graph isomorphism network tensorflow 20 repository contains tensorflow 20 implementation experiment following paper keyulu xu weihua hu jure leskovec stefanie jegelka powerful graph neural network iclr 2019 make use codeexperiment gin algorithm work please cite paper bibtex inproceedings xu2018how titlehow powerful graph neural network authorkeyulu xu weihua hu jure leskovec stefanie jegelka booktitleinternational conference learning representation year2019 created tensorflow 20 implementation educational purpose knowledge domain previewing run colab sheet choose run different datasets modify training property graph isomorphism network installation install tensorflow 20 install dependency pip install r requirementstxt test run unzip dataset file unzip datasetzip run python mainpy note default parameter best performinghyperparameters please refer paper detail researcher set hyperparameters
Graphs;simplifiedgcnmodel 我们提供了简化的gcn和gat模型，原模型取自于iclr的会议文章 source gcn paper gcn pytorch version gat paper gat pytorch version gcn 对于gcn，我们将模型参数的初始化交给了kaiminguniform，定义线性层torchnnlinearinc outc来替代定义变量与变化初始化，并将模型并行化b n f，引入batch的维度 code gcn import gcnmodel 导入模型 model gcnmodelinchannels outchannels hidc16 初始化模型 model modelcuda idx image adj label enumeratedataloader image n f1 adj n n label n one hot n f3 image adj label imagecuda adjcuda labelcuda loss fcrossentropylossoutput label output modelimage adj 将数据喂入模型 gat 对于gat，我们将attention concat的操作简化为简单的矩阵相加，并将模型并行化，引入batch的维度 简化过程如下 h wx eij hihj a1t a2t eij a1t a2t hihj a1t hi a2t hj h1 a1 h h2 a2 h e h1repeat1n h2repeatn1t img width600 height780 altimagebr gat import gatmodel model gatmodelinchannels outchannels hidc16 head8 model modelcuda idx image adj label enumeratedataload image adj label imagecuda adjcuda labelcuda output modelimage adj loss fcrossentropylossoutput label
Graphs;kdd19 note data code jies talk please go cogdl cogdl graph representation learning toolkit allows researcher developer easily train compare baseline custom model node classification link prediction task graph provides implementation many popular model including nongnn baseline like deepwalk line netmf gnn baseline like gcn gat graphsage cogdl feature sparsification fast network embedding largescale network ten million node arbitrary dealing different graph strucutures attributed multiplex heterogeneous network parallel parallel training different seed different model multiple gpus automatically reporting result table extensible easily register new datasets model criterion task getting started requirement installation pytorch version 100 python version 36 please follow instruction install pytorch dependency usage use python trainpy task exampletask dataset exampledataset model examplemethod run examplemethod exampledata evaluate via exampletask general parameter task downsteam task evaluate representation like nodeclassification unsupervisednodeclassification linkprediction dataset dataset name run list datasets space like cora citeseer ppi supported datasets including cora citeseer pumbed ppi wikipedia blogcatalog dblp flickr model model name run list model like deepwalk line prone supported datasets including gcn gat graphsage deepwalk node2vec hope grarep netmf netsmf prone example want run gcn cora node classification task use following operation bash python trainpy task nodeclassification dataset cora model gcn epoch 099 train 09857 val 07900 100██████████████████████████ 100100 00000000 14242its test accuracy 0813 want run parallel experiment server multiple gpus like multiple model gcn gat multiple datasets cora citeseer node classification task use following operation python trainpy task nodeclassification dataset cora citeseer model gcn gat deviceid 0 1 2 3 specific parameter deepwalk node2vec walknum number random walk start node default 10 walklength length walk start node default 50 worker number parallel worker default 10 windowsize window size skipgram model default 10 iteration number iteration default 10 q parameter node2vec default 10 p parameter node2vec default 10 line order order proximity line default 3 12 alpha initial earning rate sgd default 0025 batchsize batch size sgd training process default 100 negative number negative node sampling default 5 hope beta parameter katz hope default 001 grarep step number matrix step grarep prone default 5 netmf windowsize window size deepwalk matrix default 10 islarge large small netmf negative number negative node sampling default 5 rank number eigenpairs netmf default 256 netsmf windowsize window size approximate matrix default 10 negative number negative node sampling default 5 round number round netsmf default 100 worker number parallel worker default 10 prone step number item chebyshev expansion default 5 theta parameter prone default 05 mu parameter prone default 02 gcn drgcn hiddensize size hidden layer default16 numlayers number gcn layer default2 dropout dropout probability default05 gat drgat hiddensize size hidden layer default8 numheads number head attention mechanism default8 dropout dropout probability default06 graphsage hiddensize size hidden layer default8 numlayers number graphsage default2 samplesize list number neighbor sample node graphsage default10 10 dropout dropout probability default05 difficulty get thing working step feel free open issue expect reply within 24 hour customization submit stateoftheart wellperform algorithm willing public submit implementation via opening join slack evaluating originality creativity efficiency add method performance leaderboard add dataset unique interesting willing public submit dataset via opening issue repository commenting slack group run suitable method dataset update leaderboard implement model wellperform algorithm willing implement toolkit help people create pull request detail information found citing find cogdl useful research please consider citing following paper inproceedingsfeylenssen2019 titlefast graph representation learning pytorch geometric authorfey matthias lenssen jan e booktitleiclr workshop representation learning graph manifold year2019 inproceedingsperozzi2014deepwalk titledeepwalk online learning social representation authorperozzi bryan alrfou ramus skiena steven booktitleproceedings 20th acm sigkdd international conference knowledge discovery data mining pages701710 year2014 organizationacm inproceedingstang2015line titleline largescale information network embedding authortang jian qu meng wang mingzhe zhang ming yan jun mei qiaozhu booktitleproceedings 24th international conference world wide web pages10671077 year2015 organizationacm inproceedingsgrover2016node2vec titlenode2vec scalable feature learning network authorgrover aditya leskovec jure booktitleproceedings 22nd acm sigkdd international conference knowledge discovery data mining pages855864 year2016 organizationacm inproceedingscao2015grarep titlegrarep learning graph representation global structural information authorcao shaosheng lu wei xu qiongkai booktitleproceedings 24th acm international conference information knowledge management pages891900 year2015 organizationacm inproceedingsou2016asymmetric titleasymmetric transitivity preserving graph embedding authorou mingdong cui peng pei jian zhang ziwei zhu wenwu booktitlethe acm sigkdd international conference pages11051114 year2016 inproceedingsqiu2018network titlenetwork embedding matrix factorization unifying deepwalk line pte node2vec authorqiu jiezhong dong yuxiao hao li jian wang kuansan tang jie booktitleproceedings eleventh acm international conference web search data mining pages459467 year2018 organizationacm inproceedingsqiu2019netsmf titlenetsmf largescale network embedding sparse matrix factorization authorqiu jiezhong dong yuxiao hao li jian wang chi wang kuansan tang jie booktitlethe world wide web conference pages15091520 year2019 organizationacm articlezhang2018spectral titlespectral network embedding fast scalable method via sparsity authorzhang jie wang yan tang jie journalarxiv preprint arxiv180602623 year2018 articlekipf2016semi titlesemisupervised classification graph convolutional network authorkipf thomas n welling max journalarxiv160902907 year2016 articlevelickovic17gat author petar velickovic guillem cucurull arantxa casanova adriana romero pietro lio yoshua bengio title graph attention network journal arxiv171010903 year 2017 inproceedingshamilton2017inductive titleinductive representation learning large graph authorhamilton ying zhitao leskovec jure booktitlenips17 pages10251035 year2017 articlechen2018fastgcn titlefastgcn fast learning graph convolutional network via importance sampling authorchen jie tengfei xiao cao journalarxiv180110247 year2018 articlezou2019dimensional titledimensional reweighting graph convolutional network authorzou xu jia qiuye zhang jianwei zhou chang yang hongxia tang jie journalarxiv preprint arxiv190702237 year2019 overview nongnn baseline perozzi et al 2014 deepwalk online learning social tang et al 2015 line largescale informa tion network grover leskovec 2016 node2vec scalable feature learning cao et al 2015grarep learning graph representation global structural information ou et al 2016 asymmetric transitivity preserving graph em qiu et al 2017 network embedding matrix factorization unifying deepwalk line pte qiu et al 2019 netsmf largescale network embedding sparse matrix zhang et al 2019 spectral network embedding fast scalable method via gnn baseline kipf welling 2016 semisupervised classification graph convolutional hamilton et al 2017 inductive representation learning large veličković et al 2017 graph attention new cikm 2018 ding et al 2018 semisupervised learning graph generative adversarial new han et al 2019 grouprep unsupervised structural representation learning group new zhang et al 2019 revisiting graph convolutional network neighborhood aggregation network new zhang et al 2019 cotraining graph convolutional network network sparse new www 2019 qiu et al 2019 netsmf largescale network embedding sparse matrix new ijcai 2019 zhang et al 2019 prone fast scalable network representation qa new acl 2019 ding et al 2019 cognitive graph multihop reading comprehension heterogeneous new kdd 2019 cen et al 2019 representation learning attributed multiplex heterogeneous dynamic new ijcai 2019 zhao et al 2019 large scale evolving graph burst
Graphs;update 202009 change print format epoch add cpp extension codesources negative sampling use extension please install pybind11 cppimport environment lightgcnpytorch pytorch implementation sigir 2020 paper sigir 2020 xiangnan kuan deng xiang wang yan li yongdong zhang meng wang2020 lightgcn simplifying powering graph convolution network recommendation paper author prof xiangnan staffustceducnhexn also see tensorflow introduction work aim simplify design gcn make concise appropriate recommendation propose new model named lightgcnincluding essential component gcn—neighborhood aggregation—for collaborative filtering enviroment requirement pip install r requirementstxt dataset provide three processed datasets gowalla yelp2018 amazonbook one small dataset lastfm see dataloaderpy example run 3layer lightgcn run lightgcn gowalla dataset change base directory change rootpath codeworldpy command cd code python mainpy decay1e4 lr0001 layer3 seed2020 datasetgowalla topks20 recdim64 log output shell epoch51000 bprsample time1621584042 savedbpraver loss1128e01 03043mtest0m precision array003315359 recall array010711388 ndcg array008940792 total time 359975962638855 epoch1161000 bprsample time1691660045 savedbpraver loss2056e02 total time 3099874997138977 note 1 even though offer code split useritem matrix matrix multiplication strongly suggest dont enable since extremely slow training speed 2 feel test process slow try increase testbatch enable multicorewindows system may encounter problem multicore option enabled 3 use tensorboard option good 4 since fix seedseed2020 numpy torch beginning run command exact output log despite running time check output epoch 5 epoch 116 extend want run lightgcn dataset go dataloaderpy implement dataloader inherited basicdataset register registerpy want run model datasets offer go modelpy implement model inherited basicmodel register registerpy want run sampling method datasets model offer go procedurepy implement function modify corresponding code mainpy result metric top20 pytorch version result stop 1000 epoch seed2020 gowalla recall ndcg precision layer1 01687 01417 005106 layer2 01786 01524 005456 layer3 01824 01547 005589 layer4 01825 01537 005576 yelp2018 recall ndcg precision layer1 005604 004557 002519 layer2 005988 004956 00271 layer3 006347 005238 00285 layer4 006515 005325 002917
Graphs;gaan mxnet implementation gaan gated attention network learning large spatiotemporal graph uai 2018 support python3 installation compile mxnet operator following guide segopscudasegopscuda install graph sampler following guide graphsamplergraphsampler bash python setuppy develop download datasets download datasets via downloaddatapy script usage like bash python downloaddatapy dataset ppi dataset hyperparameter cora ppi reddit run experiment script experimentsstaticgraphsuptrainsamplepy citation inproceedingszhang18 author jiani zhang xingjian shi junyuan xie hao irwin king dityan yeung title gaan gated attention network learning large spatiotemporal graph booktitle proceeding thirtyfourth conference uncertainty artificial intelligence page 339349 year 2018
Graphs;line largescale information network embedding note repository longer maintained node embedding method please use graph embedding system graphvite introduction line toolkit developed embedding largescale information network suitable variety network including directed undirected binary weighted edge line model quite efficient able embed network million vertex billion edge single machine within hour contact jian tang tangjianpkugmailcom project page work done author working microsoft research usage provide window linux version compile souce code external package required used generate random number edgesampling algorithm line model window version boost package used downloaded linux gsl package used downloaded network input input network consists edge network line input file represents directed edge network specified format sourcenode targetnode weight either separated blank tab undirected edge user must use two directed edge represent input example word cooccurrence network good 3 good 3 good bad 1 bad good 1 bad 4 bad 4 run line train networkfile output embeddingfile binary 1 size 200 order 2 negative 5 sample 100 rho 0025 thread 20 train input file network output output file embedding binary whether saving output file binary mode default 0 size dimension embedding default 100 order order proximity used 1 first order 2 second order default 2 negative number negative sample used negative sampling deault 5 sample total number training sample million rho starting value learning rate default 0025 thread total number thread used default 1 file folder linecpp souce code line reconstructcpp code used reconstructing sparse network dense one described section 43 normalizecpp code normalizing embeddings l2 normalization concatenatecpp code concatenating embeddings 1storder 2ndorder example provide example running script youtube data set available script first run line learn network embeddings evaluate learned embeddings node classification task run script user first need compile evaluation code running makesh folder evaluate afterwards run trainyoutubebat trainyoutubesh run whole pipeline citation inproceedingstang2015line titleline largescale information network embedding authortang jian qu meng wang mingzhe zhang ming yan jun mei qiaozhu booktitlewww year2015 organizationacm
Graphs;graph attention network jax repository implement graph attention network gat jax code contains model definition main gat model two graph attention layer following model used paper graph attention usage run python trainpy train model cora dataset good know repository implementents graph attention network doesnt fully replicate paper example doesnt include early stopping model saving training loop cite use code research please cite paper article velickovic2018graph titlegraph attention network authorvelivckovic petar cucurull guillem casanova arantxa romero adriana lio pietro bengio yoshua journalinternational conference learning representation year2018 noteaccepted poster
