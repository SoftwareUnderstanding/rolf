{
    "syntactic": [
        "weight information",
        "gpu",
        "gpus",
        "inference",
        "convolutional neural networks",
        "utility functions",
        "cpu",
        "detection rates"
    ],
    "semantic": [
        "weight information",
        "gpu",
        "gpus",
        "inference",
        "utility functions",
        "probabilistic inference",
        "cpu"
    ],
    "union": [
        "weight information",
        "gpu",
        "gpus",
        "inference",
        "convolutional neural networks",
        "utility functions",
        "probabilistic inference",
        "cpu",
        "detection rates"
    ],
    "enhanced": [
        "attribute weight",
        "program processors",
        "cuda",
        "inference engines",
        "neural networks",
        "game theory",
        "bayesian methods",
        "probability distributions",
        "intrusion detection"
    ],
    "explanation": {
        "detection rates": [
            "detectron accuracy"
        ],
        "utility functions": [
            "utility function"
        ],
        "weight information": [
            "weight information"
        ],
        "convolutional neural networks": [
            "cnn"
        ],
        "gpu": [
            "gpu cpu",
            "gpu",
            "cpu",
            "gpus"
        ],
        "inference": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "inference batch",
            "inference perform",
            "cpu inference",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "detector inference",
            "perform inference",
            "inference cpu inference",
            "batch inference",
            "inference batch inference",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "cpu": [
            "cpu"
        ],
        "gpus": [
            "gpu",
            "gpus"
        ],
        "probabilistic inference": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "inference batch",
            "inference perform",
            "cpu inference",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "detector inference",
            "perform inference",
            "inference cpu inference",
            "batch inference",
            "inference batch inference",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "attribute weight": [
            "weight information"
        ],
        "program processors": [
            "gpu cpu",
            "cpu",
            "gpu",
            "gpus"
        ],
        "cuda": [
            "gpu",
            "gpus"
        ],
        "inference engines": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "inference batch",
            "inference perform",
            "cpu inference",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "detector inference",
            "perform inference",
            "inference cpu inference",
            "batch inference",
            "inference batch inference",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "neural networks": [
            "cnn"
        ],
        "game theory": [
            "utility function"
        ],
        "bayesian methods": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "inference batch",
            "inference perform",
            "cpu inference",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "detector inference",
            "perform inference",
            "inference cpu inference",
            "batch inference",
            "inference batch inference",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "probability distributions": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "inference batch",
            "inference perform",
            "cpu inference",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "detector inference",
            "perform inference",
            "inference cpu inference",
            "batch inference",
            "inference batch inference",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "intrusion detection": [
            "detectron accuracy"
        ]
    }
}