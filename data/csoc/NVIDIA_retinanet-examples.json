{
    "syntactic": [
        "gpu",
        "object detection",
        "jitter",
        "trade",
        "gpus",
        "inference",
        "video streams",
        "optimization",
        "cache",
        "engine"
    ],
    "semantic": [
        "gpu",
        "object detection",
        "jitter",
        "trade",
        "gpus",
        "inference",
        "video streams",
        "optimization",
        "cache",
        "probabilistic inference",
        "engine"
    ],
    "union": [
        "gpu",
        "object detection",
        "trade",
        "jitter",
        "gpus",
        "inference",
        "video streams",
        "optimization",
        "cache",
        "probabilistic inference",
        "engine"
    ],
    "enhanced": [
        "program processors",
        "object recognition",
        "commerce",
        "bandwidth",
        "cuda",
        "inference engines",
        "video streaming",
        "mathematics",
        "operating systems",
        "buffer storage",
        "cache memory",
        "storage allocation (computer)",
        "bayesian methods",
        "probability distributions",
        "engineering"
    ],
    "explanation": {
        "object detection": [
            "object detection"
        ],
        "video streams": [
            "time video",
            "video stream",
            "video"
        ],
        "gpu": [
            "gpu",
            "nvidia",
            "gpus"
        ],
        "optimization": [
            "optimization",
            "optimize"
        ],
        "trade": [
            "trade"
        ],
        "inference": [
            "performance inference",
            "inference latency",
            "inference nvidia",
            "int8 inference",
            "inference datasets",
            "inference inference documentation",
            "coco inference",
            "inference",
            "detection inference",
            "size inference",
            "schedule inference",
            "inference inference",
            "inference documentation",
            "time inference",
            "usage inference",
            "detection inference inference",
            "inference measure",
            "range inference",
            "optimize inference",
            "inference export",
            "inference int8",
            "inference evaluation",
            "fast inference",
            "accuracy inference"
        ],
        "engine": [
            "engine"
        ],
        "jitter": [
            "jitter"
        ],
        "cache": [
            "cache"
        ],
        "gpus": [
            "gpu",
            "gpus"
        ],
        "probabilistic inference": [
            "performance inference",
            "inference latency",
            "inference nvidia",
            "int8 inference",
            "inference datasets",
            "inference inference documentation",
            "coco inference",
            "detection inference",
            "inference",
            "size inference",
            "schedule inference",
            "inference inference",
            "inference documentation",
            "time inference",
            "usage inference",
            "detection inference inference",
            "inference measure",
            "range inference",
            "optimize inference",
            "inference export",
            "inference int8",
            "inference evaluation",
            "fast inference",
            "accuracy inference"
        ],
        "program processors": [
            "gpu",
            "nvidia",
            "gpus"
        ],
        "object recognition": [
            "object detection"
        ],
        "commerce": [
            "trade"
        ],
        "bandwidth": [
            "jitter"
        ],
        "cuda": [
            "gpu",
            "gpus"
        ],
        "inference engines": [
            "int8 inference",
            "inference datasets",
            "inference",
            "detection inference",
            "size inference",
            "schedule inference",
            "detection inference inference",
            "inference int8",
            "accuracy inference",
            "performance inference",
            "inference latency",
            "inference nvidia",
            "inference inference documentation",
            "coco inference",
            "inference inference",
            "inference documentation",
            "time inference",
            "usage inference",
            "inference measure",
            "range inference",
            "optimize inference",
            "inference export",
            "inference evaluation",
            "fast inference"
        ],
        "video streaming": [
            "time video",
            "video stream",
            "video"
        ],
        "mathematics": [
            "optimization",
            "optimize"
        ],
        "operating systems": [
            "cache"
        ],
        "buffer storage": [
            "cache"
        ],
        "cache memory": [
            "cache"
        ],
        "storage allocation (computer)": [
            "cache"
        ],
        "bayesian methods": [
            "performance inference",
            "inference latency",
            "inference nvidia",
            "int8 inference",
            "inference datasets",
            "inference inference documentation",
            "coco inference",
            "detection inference",
            "inference",
            "size inference",
            "schedule inference",
            "inference inference",
            "inference documentation",
            "time inference",
            "usage inference",
            "detection inference inference",
            "inference measure",
            "range inference",
            "optimize inference",
            "inference export",
            "inference int8",
            "inference evaluation",
            "fast inference",
            "accuracy inference"
        ],
        "probability distributions": [
            "performance inference",
            "inference latency",
            "inference nvidia",
            "int8 inference",
            "inference datasets",
            "inference inference documentation",
            "coco inference",
            "detection inference",
            "inference",
            "size inference",
            "schedule inference",
            "inference inference",
            "inference documentation",
            "time inference",
            "usage inference",
            "detection inference inference",
            "inference measure",
            "range inference",
            "optimize inference",
            "inference export",
            "inference int8",
            "inference evaluation",
            "fast inference",
            "accuracy inference"
        ],
        "engineering": [
            "engine"
        ]
    }
}