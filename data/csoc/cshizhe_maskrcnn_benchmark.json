{
    "syntactic": [
        "weight information",
        "gpu",
        "object detection",
        "gpus",
        "inference",
        "convolutional neural networks",
        "utility functions",
        "cpu",
        "detection rates"
    ],
    "semantic": [
        "weight information",
        "gpu",
        "object detection",
        "gpus",
        "inference",
        "utility functions",
        "probabilistic inference",
        "cpu"
    ],
    "union": [
        "weight information",
        "gpu",
        "object detection",
        "gpus",
        "inference",
        "convolutional neural networks",
        "utility functions",
        "probabilistic inference",
        "cpu",
        "detection rates"
    ],
    "enhanced": [
        "attribute weight",
        "program processors",
        "object recognition",
        "cuda",
        "inference engines",
        "neural networks",
        "game theory",
        "bayesian methods",
        "probability distributions",
        "intrusion detection"
    ],
    "explanation": {
        "object detection": [
            "object detection"
        ],
        "detection rates": [
            "detectron accuracy"
        ],
        "utility functions": [
            "utility function"
        ],
        "weight information": [
            "weight information"
        ],
        "convolutional neural networks": [
            "cnn"
        ],
        "inference": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "google inference",
            "cpu inference",
            "inference image_bbox_extraction",
            "inference perform",
            "inference inference acknowledgement",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "inference inference",
            "format inference",
            "inference acknowledgement",
            "perform inference",
            "maskrcnn_benchmark inference",
            "inference cpu inference",
            "batch inference",
            "format inference inference",
            "inference mix",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "gpu": [
            "gpu",
            "gpu cpu",
            "cpu",
            "nvidia",
            "gpus"
        ],
        "cpu": [
            "cpu"
        ],
        "gpus": [
            "gpu",
            "gpus"
        ],
        "probabilistic inference": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "google inference",
            "cpu inference",
            "inference image_bbox_extraction",
            "inference perform",
            "inference inference acknowledgement",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "inference inference",
            "format inference",
            "inference acknowledgement",
            "perform inference",
            "maskrcnn_benchmark inference",
            "inference cpu inference",
            "batch inference",
            "format inference inference",
            "inference mix",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "attribute weight": [
            "weight information"
        ],
        "program processors": [
            "gpu",
            "gpu cpu",
            "cpu",
            "nvidia",
            "gpus"
        ],
        "object recognition": [
            "object detection"
        ],
        "cuda": [
            "gpu",
            "gpus"
        ],
        "inference engines": [
            "inference perform inference",
            "inference line",
            "google inference",
            "cpu inference",
            "inference image_bbox_extraction",
            "inference inference acknowledgement",
            "inference",
            "inference cpu",
            "inference notebook",
            "format inference",
            "inference acknowledgement",
            "inference cpu inference",
            "batch inference",
            "format inference inference",
            "write inference",
            "inference pipeline",
            "inference perform",
            "support inference",
            "inference inference",
            "perform inference",
            "maskrcnn_benchmark inference",
            "inference mix",
            "inference time",
            "gpu inference"
        ],
        "neural networks": [
            "cnn"
        ],
        "game theory": [
            "utility function"
        ],
        "bayesian methods": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "google inference",
            "cpu inference",
            "inference image_bbox_extraction",
            "inference perform",
            "inference inference acknowledgement",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "inference inference",
            "format inference",
            "inference acknowledgement",
            "perform inference",
            "maskrcnn_benchmark inference",
            "inference cpu inference",
            "batch inference",
            "format inference inference",
            "inference mix",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "probability distributions": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "google inference",
            "cpu inference",
            "inference image_bbox_extraction",
            "inference perform",
            "inference inference acknowledgement",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "inference inference",
            "format inference",
            "inference acknowledgement",
            "perform inference",
            "maskrcnn_benchmark inference",
            "inference cpu inference",
            "batch inference",
            "format inference inference",
            "inference mix",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "intrusion detection": [
            "detectron accuracy"
        ]
    }
}