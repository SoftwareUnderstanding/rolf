{
    "syntactic": [
        "weight information",
        "gpu",
        "gpus",
        "inference",
        "convolutional neural networks",
        "utility functions",
        "cpu",
        "detection rates"
    ],
    "semantic": [
        "detection algorithm",
        "weight information",
        "cuda",
        "gpu",
        "gpus",
        "inference",
        "utility functions",
        "probabilistic inference",
        "cpu",
        "random access memory"
    ],
    "union": [
        "detection algorithm",
        "weight information",
        "cuda",
        "gpu",
        "detection rates",
        "gpus",
        "inference",
        "convolutional neural networks",
        "utility functions",
        "probabilistic inference",
        "cpu",
        "random access memory"
    ],
    "enhanced": [
        "signal detection",
        "attribute weight",
        "parallel programming",
        "graphics processing unit",
        "program processors",
        "intrusion detection",
        "inference engines",
        "neural networks",
        "game theory",
        "bayesian methods",
        "probability distributions",
        "random access storage"
    ],
    "explanation": {
        "detection rates": [
            "detectron accuracy"
        ],
        "utility functions": [
            "utility function"
        ],
        "weight information": [
            "weight information"
        ],
        "convolutional neural networks": [
            "cnn"
        ],
        "gpu": [
            "gpu",
            "gpu cpu",
            "cpu",
            "nvidia",
            "gpus"
        ],
        "inference": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "inference perform",
            "cpu inference",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "perform inference",
            "maskrcnn_benchmark inference",
            "inference cpu inference",
            "batch inference",
            "inference mix",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "cpu": [
            "cpu"
        ],
        "gpus": [
            "gpu",
            "gpus"
        ],
        "probabilistic inference": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "inference perform",
            "cpu inference",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "perform inference",
            "maskrcnn_benchmark inference",
            "inference cpu inference",
            "batch inference",
            "inference mix",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "random access memory": [
            "memory reason",
            "gpu memory",
            "memory error",
            "memory",
            "memory nvidia",
            "lot memory",
            "detail memory",
            "much memory",
            "experience memory",
            "memory mmdetection"
        ],
        "cuda": [
            "gpu",
            "nvidia",
            "gpus"
        ],
        "detection algorithm": [
            "detection tag",
            "shoot detection",
            "detection segmentation",
            "detection cheng",
            "lesion detection",
            "detection"
        ],
        "signal detection": [
            "detection tag",
            "shoot detection",
            "detection segmentation",
            "detection cheng",
            "lesion detection",
            "detection"
        ],
        "attribute weight": [
            "weight information"
        ],
        "parallel programming": [
            "gpu",
            "nvidia",
            "gpus"
        ],
        "graphics processing unit": [
            "gpu",
            "nvidia",
            "gpus"
        ],
        "program processors": [
            "gpu",
            "gpu cpu",
            "cpu",
            "nvidia",
            "gpus"
        ],
        "intrusion detection": [
            "detectron accuracy"
        ],
        "inference engines": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "inference perform",
            "cpu inference",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "perform inference",
            "maskrcnn_benchmark inference",
            "inference cpu inference",
            "batch inference",
            "inference mix",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "neural networks": [
            "cnn"
        ],
        "game theory": [
            "utility function"
        ],
        "bayesian methods": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "inference perform",
            "cpu inference",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "perform inference",
            "maskrcnn_benchmark inference",
            "inference cpu inference",
            "batch inference",
            "inference mix",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "probability distributions": [
            "inference perform inference",
            "inference pipeline",
            "inference line",
            "inference perform",
            "cpu inference",
            "support inference",
            "inference",
            "inference cpu",
            "inference notebook",
            "perform inference",
            "maskrcnn_benchmark inference",
            "inference cpu inference",
            "batch inference",
            "inference mix",
            "write inference",
            "inference time",
            "gpu inference"
        ],
        "random access storage": [
            "memory reason",
            "gpu memory",
            "memory error",
            "memory",
            "memory nvidia",
            "lot memory",
            "detail memory",
            "much memory",
            "experience memory",
            "memory mmdetection"
        ]
    }
}