Label;Repo;Text
Semantic web;https://github.com/albertmeronyo/SPARQL2Git;"""# SPARQL2Git"""
Semantic web;https://github.com/oeg-upm/gtfs-bench;"""# The GTFS-Madrid-Bench        We present GTFS-Madrid-Bench, **a benchmark to evaluate declarative KG construction engines** that can be used for the provision of access mechanisms to (virtual) knowledge graphs. Our proposal introduces several scenarios that aim at measuring performance and scalability as well as the query capabilities of all this kind of engines, considering their heterogeneity. The data sources used in our benchmark are derived from the [GTFS](https://developers.google.com/transit/gtfs) data files of the subway network of Madrid. They can be transformed into several formats (CSV, JSON, SQL and XML) and scaled up. The query set aims at addressing a representative number of SPARQL 1.1 features while covering usual queries that data consumers may be interested in.    <p align=""center"">    <img src=""misc/logo.png"" />  </p>    ### Main Publication:  David Chaves-Fraga, Freddy Priyatna, Andrea Cimmino, Jhon Toledo, Edna Ruckhaus, & Oscar Corcho (2020). GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain. Journal of Web Semantics, 65. [Online](https://doi.org/10.1016/j.websem.2020.100596)        **Citing GTFS-Madrid-Bench**: If you used GTFS-Madrid-Bench in your work, please cite as:    ```bib  @article{chaves2020gtfs,    title={GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain},    author={Chaves-Fraga, David and Priyatna, Freddy and Cimmino, Andrea and Toledo, Jhon and Ruckhaus, Edna and Corcho, Oscar},    journal={Journal of Web Semantics},    volume={65},    pages={100596},    year={2020},    doi={https://doi.org/10.1016/j.websem.2020.100596},    publisher={Elsevier}    }  ```    ### Results  - Virtual KGC results can be reproduced through the resources provided in [this branch](https://github.com/oeg-upm/gtfs-bench/tree/evaluation-jows2020)  - Materialized KGC results can be reproduced through the resources provided in [this repo](https://github.com/oeg-upm/kgc-eval)    ## Requirements for the use:    To have locally installed [docker](https://docs.docker.com/engine/install/).    Decide the distributions to be used for your testing. They can be:  - Standard distributions: data sources are represented in one format (e.g., GTFS-CSV, GTFS-JSON or GTFS-SQL).  - Custom distributions: each data source is represented in the format selected by the user (e.g., SHAPES in JSON, CALENDAR in CSV, etc.)      ## Using Madrid-GTFS-Bench:    1. Download and run the docker image (run it always to ensure you are using the last version of the docker image).  * Docker v20.10 or later: `docker run --pull always -itv ""$(pwd)"":/output oegdataintegration/gtfs-bench`   * Previous versions: `docker pull oegdataintegration/gtfs-bench` and then `docker run -itv ""$(pwd)"":/output oegdataintegration/gtfs-bench`  2. Choose data scales and formats to obtain the distributions you want to test. You have to provide: first the data scales (in one line, separated by a comma), then, select the standard distributions (from none to all) and if is needed, the configuration for one custom distribution. If you want to generate several custom distributions, you will have to run the generator several times. Example:    ![Demo GIF](misc/gtfs-demo.gif)    3. Result will be available as `result.zip` in the current working directory. The folders structure are: one folder for datasets and other for the queries (for virtual KG). Inside the datasets folder will be one folder for each distribution (e.g., csv, sql, custom), and in each distribution folder we provide the required sizes (each size in one folder), the corresponding mapping associated to the distribution, and the SQL schemes if they are needed. **Consider that for not repeating resources at scale level, the mappings and SQL paths to the data are define at distribution level (e.g, ""data/AGENCY.csv"") and their management for performing a correct evaluation has to be done by the user (with an script, for example)**. You can visit the [utils](https://github.com/oeg-upm/gtfs-bench/tree/master/utils) folder where we provide some ideas on how to manage it. See the following example:    ```  .  ├── datasets  │   ├── csv  │   │   ├── 1  │   │   │   ├── AGENCY.csv  │   │   │   ├── CALENDAR.csv  │   │   │   ├── CALENDAR_DATES.csv  │   │   │   ├── FEED_INFO.csv  │   │   │   ├── FREQUENCIES.csv  │   │   │   ├── ROUTES.csv  │   │   │   ├── SHAPES.csv  │   │   │   ├── STOPS.csv  │   │   │   ├── STOP_TIMES.csv  │   │   │   └── TRIPS.csv  │   │   ├── 2  │   │   │   ├── AGENCY.csv  │   │   │   ├── CALENDAR.csv  │   │   │   ├── CALENDAR_DATES.csv  │   │   │   ├── FEED_INFO.csv  │   │   │   ├── FREQUENCIES.csv  │   │   │   ├── ROUTES.csv  │   │   │   ├── SHAPES.csv  │   │   │   ├── STOPS.csv  │   │   │   ├── STOP_TIMES.csv  │   │   │   └── TRIPS.csv  │   │   ├── 3  │   │   │   ├── AGENCY.csv  │   │   │   ├── CALENDAR.csv  │   │   │   ├── CALENDAR_DATES.csv  │   │   │   ├── FEED_INFO.csv  │   │   │   ├── FREQUENCIES.csv  │   │   │   ├── ROUTES.csv  │   │   │   ├── SHAPES.csv  │   │   │   ├── STOPS.csv  │   │   │   ├── STOP_TIMES.csv  │   │   │   └── TRIPS.csv  │   │   └── mapping.csv.nt  │   ├── json  │   │   ├── 1  │   │   │   ├── AGENCY.json  │   │   │   ├── CALENDAR_DATES.json  │   │   │   ├── CALENDAR.json  │   │   │   ├── FEED_INFO.json  │   │   │   ├── FREQUENCIES.json  │   │   │   ├── ROUTES.json  │   │   │   ├── SHAPES.json  │   │   │   ├── STOPS.json  │   │   │   ├── STOP_TIMES.json  │   │   │   └── TRIPS.json  │   │   ├── 2  │   │   │   ├── AGENCY.json  │   │   │   ├── CALENDAR_DATES.json  │   │   │   ├── CALENDAR.json  │   │   │   ├── FEED_INFO.json  │   │   │   ├── FREQUENCIES.json  │   │   │   ├── ROUTES.json  │   │   │   ├── SHAPES.json  │   │   │   ├── STOPS.json  │   │   │   ├── STOP_TIMES.json  │   │   │   └── TRIPS.json  │   │   ├── 3  │   │   │   ├── AGENCY.json  │   │   │   ├── CALENDAR_DATES.json  │   │   │   ├── CALENDAR.json  │   │   │   ├── FEED_INFO.json  │   │   │   ├── FREQUENCIES.json  │   │   │   ├── ROUTES.json  │   │   │   ├── SHAPES.json  │   │   │   ├── STOPS.json  │   │   │   ├── STOP_TIMES.json  │   │   │   └── TRIPS.json  │   │   └── mapping.json.nt  │   └── sql  │       ├── 1  │       │   ├── AGENCY.csv  │       │   ├── CALENDAR.csv  │       │   ├── CALENDAR_DATES.csv  │       │   ├── FEED_INFO.csv  │       │   ├── FREQUENCIES.csv  │       │   ├── ROUTES.csv  │       │   ├── SHAPES.csv  │       │   ├── STOPS.csv  │       │   ├── STOP_TIMES.csv  │       │   └── TRIPS.csv  │       ├── 2  │       │   ├── AGENCY.csv  │       │   ├── CALENDAR.csv  │       │   ├── CALENDAR_DATES.csv  │       │   ├── FEED_INFO.csv  │       │   ├── FREQUENCIES.csv  │       │   ├── ROUTES.csv  │       │   ├── SHAPES.csv  │       │   ├── STOPS.csv  │       │   ├── STOP_TIMES.csv  │       │   └── TRIPS.csv  │       ├── 3  │       │   ├── AGENCY.csv  │       │   ├── CALENDAR.csv  │       │   ├── CALENDAR_DATES.csv  │       │   ├── FEED_INFO.csv  │       │   ├── FREQUENCIES.csv  │       │   ├── ROUTES.csv  │       │   ├── SHAPES.csv  │       │   ├── STOPS.csv  │       │   ├── STOP_TIMES.csv  │       │   └── TRIPS.csv  │       └── mapping.sql.nt  │       └── schema.sql  └── queries      ├── q10.rq      ├── q11.rq      ├── q12.rq      ├── q13.rq      ├── q14.rq      ├── q15.rq      ├── q16.rq      ├── q17.rq      ├── q18.rq      ├── q1.rq      ├── q2.rq      ├── q3.rq      ├── q4.rq      ├── q5.rq      ├── q6.rq      ├── q7.rq      ├── q8.rq      └── q9.rq  ```      ## Resources    Additionally to the generator engine, that provides the data at desirable scales and distributions, together with corresponding mappings and queries, there are also common resources openly available to be modified or used by any practicioner or developer:    - Folder [mappings](https://github.com/oeg-upm/gtfs-bench/tree/master/mappings) contains RML mappings for CSV, XML, JSON and RDB distributions of the input GTFS dataset, R2RML mapping for RDB and xR2RML mapping for MongoDB. It also includes CSVW annotations for the CSV distributions.  - Folder [queries](https://github.com/oeg-upm/gtfs-bench/tree/master/queries) includes 18 queries with different levels of complexity including a representative set of SPARQL 1.1. operators. Additionally, the folder contains [11 simple queries](https://github.com/oeg-upm/gtfs-bench/tree/master/queries/simple) that will help to test the basic capabilities of virtual KG construction engines (i.e., to understand if the engine is able to translate correctly the SPARQL operators over different GTFS distributions before starting to test performance and scalability).    ## Utils    Our experiences testing (virtual) knowledge graph engines have revealed the difficulties for setting up an infrastructure where many variables and resources are involved: databases, raw data, mappings, queries, data paths, mapping paths, databases connections, etc. For that reason, and in order to facilitate the use of the benchmark to any developer or practitioner, we provide a set of [utils](https://github.com/oeg-upm/gtfs-bench/tree/master/utils) such as docker-compose templates or evaluation bash scripts that, in our opinion, can reduce the time for preparing the testing set up.    ## Desirable Metrics:    We highly recommend that (virutalizers or materializers) KG construction engines tested with this benchmark provide (at least) the following metris:  - Total execution time  - Number of answers	  - Memory consumption  - Initial delay	  - Dief@k (only for continuous/streaming behavior)*	  - Dief@t (only for continuous/streaming behavior)*    For virtual knowledge graphs systems, we also encourage developers and tester to provide:  - Loading time	  - Mapping translation time (if applies)  - Number of requests  - Source selection time	  - Query generation (or disitribution) time  - Query rewritting time  - Query translation time  - Query exececution time  - Results aggregation time    *R Package available at: https://github.com/dachafra/dief (extension from https://github.com/maribelacosta/dief)    ## Data License  All the datasets generated by this benchmark have to follow the license of the Consorcio Regional de Transporte de Madrid: https://www.crtm.es/licencia-de-uso?lang=en    ## Contribute  We know that there are variables and dimensions that we did not take into account in the current version of the benchmark (e.g., transformation function defined in the mapping rules). If you are interested in collaborate with us in a new version of the benchmark, send us an email or open a new [discussion](https://github.com/oeg-upm/gtfs-bench/discussions)!      ## Authors    - David Chaves-Fraga - [dchaves@fi.upm.es](mailto:dchaves@fi.upm.es)  - Freddy Priyatna  - Jhon Toledo  - Daniel Doña  - Edna Ruckhaus  - Andrea Cimmino  - Oscar Corcho    Ontology Engineering Group, October 2019 - Present """
Semantic web;https://github.com/dvcama/LodLive;"""LodLive  =======    browse the web of data - a SPARQL navigator    LodLive is an experimental project that was set-up to spread and promote the linked-open-data philosophy and to create a tool that can be used for connecting RDF browser capabilities with the effectiveness of data graph representation.     LodLive is the first navigator to use RDF resources based solely on SPARQL endpoints.     LodLive aims to demonstrate how resources published by W3C standards for the semantic web can be made easily accessible and legible with a few viable tools.     LodLive is capable of connecting the resources existing in its configured endpoints, allowing the user to pass from one endpoint to another by making use of LOD interconnection capacities.    ## About us  LodLive is ideated and maintained by Diego Valerio Camarda and Alessandro Antonuccio with the support of Silvia Mazzini.    Diego is the RDF guy behind the technology (see https://www.linkedin.com/in/dvcama)     Alessandro is the designer responsible for the interface and the UX (see http://hstudio.it)   """
Semantic web;https://github.com/Remixman/Vedas;"""# VEDAS    **VEDAS** is a RDF store engine that be able to query with SPARQL and run on single GPU.     ## Dependencies  - ModernGPU  - Thrust  - [Raptor RDF Syntax Library](http://librdf.org/raptor/INSTALL.html)  - [Rasqal RDF Query Library](http://librdf.org/rasqal/INSTALL.html)    ## Build  ```bash  make  ```    ## Build the VEDAS database  First, you should prepare the RDF data in N-triple format or .nt extension. **vdBuild** is used for load the triple data into VEDAS internal format  ```bash  ./vdBuild <database_name> <path_to_nt_file>  ```  For example  ```bash  ./vdBuild watdiv500M /home/username/data/watdiv/watdiv.500M.nt  ```  The internal database file <database_name>.vdd and <database_name>.vds will be generated.      ## Query RDF data  VEDAS support query only from file. The **vdQuery** is the query engine that load the RDF data and wait for the input file.  ```bash  ./vdQuery <database_name>  ```  The prompt will shown after finish loaded data. To submit the query, use command *sparql <path_to_sparql_query_file>* and *exit* to terminate the program.    You can use *-sparql-path* option to speccify the sparql file path.  ```bash  ./vdQuery <database_name> -sparql-path=<path_to_sparql_query_file>  ```    ## Visualize the RDF Graph  After load the database with **vdBuild**, it will construct the graph vertex and edge files, named *tools/nodes.txt* and *edges/nodes.txt*. You can generate the GraphML file with the following command  ```bash  cd tools  pip install -r requirements.txt  python graphml.py  ```  The output file *triple-data.graphml* can opened with any supported software e.g. Graphia, Gephi etc. """
Semantic web;https://github.com/GeoKnow/TripleGeo;"""<html>  <HEAD>  </head>  <body>    <div id=""readme"" class=""clearfix announce instapaper_body md"">  <article class=""markdown-body entry-content"" itemprop=""mainContentOfPage"">    <h1><a name=""welcome-to-triplegeo"" class=""anchor"" href=""#welcome-to-triplegeo""><span class=""octicon octicon-link""></span></a>TripleGeo: An open-source tool for extracting geospatial features into RDF triples</h1>    <p>TripleGeo is a utility developed by the <a href=""http://www.ipsyp.gr/"">Institute for the Management of Information Systems</a> at <a href=""http://www.athena-innovation.gr/en.html"">Athena Research Center</a> under the EU/FP7 project <a href=""http://geoknow.eu"">GeoKnow: Making the Web an Exploratory for Geospatial Knowledge</a>. This generic purpose, open-source tool can be used for integrating features from geospatial databases into RDF triples.</p>    <p>TripleGeo is based on open-source utility <a href=""https://github.com/boricles/geometry2rdf/tree/master/Geometry2RDF"">geometry2rdf</a>. TripleGeo is written in Java and is still under development; more enhancements will be included in future releases. However, all supported features have been tested and work smoothly in both MS Windows and Linux platforms.</p>    <p>The web site for <a href=""https://web.imis.athena-innovation.gr/redmine/projects/geoknow_public/wiki/TripleGeo"">TripleGeo</a> provides more details about the project, its architecture, usage tips, and foreseen extensions.</p>    <h2>  <a name=""quick-start"" class=""anchor"" href=""#Quick start""><span class=""octicon octicon-link""></span></a>Quick start</h2>    How to use TripleGeo:    You have 2 options: either build from source (using Apache Ant) or use the prepackaged binaries (JARs) shipped with this code.    <h4>1.a Build from source</h4>    <ul>  <li>Build (with ant):<br/>  <code>mkdir build</code><br/>  <code>ant compile</code>  </li>  <li>Package as a jar (with ant):<br/>  <code>ant package</code><br/>  If build finishes successfully, generated JARs will be placed under <code>build/jars</code>.  </li>  </ul>    <h4>1.b Use prepackaged JARs</h4>    In order to use TripleGeo for extracting triples from a spatial dataset, the user should follow these steps (in a Windows platform, but these are similar in Linux as well):  <ul>    <li>  Download the current software bundle from https://github.com/GeoKnow/TripleGeo/archive/master.zip</li>  <li>  Extract the downloaded .zip file into a separate folder, e.g., <code>c:\temp</code>.</li>  <li>  Open a terminal window (in DOS or in Linux) and navigate to the directory where TripleGeo has been extracted, e.g.,  <code>cd c:\temp\TripleGeo-master</code>. This directory must be the one that holds the LICENSE file. For convenience, this is where you can place your configuration file (e.g., options.conf), although you can specify another path for your configuration if you like.</li>  <li>Normally, under this same folder there must be a lib/ subdirectory with the required libraries. Make sure that the actual TripleGeo.jar is under the bin/ subdirectory.</li>  <li>Verify that Java JRE (or SDK) ver 1.7 or later is installed. Currently installed version of Java can be checked using <code>java –version</code> from the command line.</li>  <li>Next, specify all properties in the required configuration file, e.g., options.conf. You must specify correct paths to files (i.e., in[parameters inputFile, outputFile, and tmpDir), which are RELATIVE to the executable.</li>  <li>In case that triples will be extracted from ESRI shapefiles, give the following command (in one line):<br/>      <code>java -cp lib/*;bin/TripleGeo.jar eu.geoknow.athenarc.triplegeo.ShpToRdf options.conf</code><br/>  Make sure that the specified paths to .jar files are correct. You must modify these paths to the libraries and/or the configuration file, if you run this command from a path other than the one containing the LICENSE file, as specified in step (3).</li>  <li>While conversion is running, it periodically issues notifications about its progress. Note that for large datasets (i.e., hundreds of thousands of records), conversion may take several minutes. As soon as processing is finished and all triples are written into a file, the user is notified about the total amount of extracted triples and the overall execution time.</li>    </ul>    <h4>2. Usage and examples</h4>    <p>The current distribution comes with a dummy configuration file <code>options.conf</code>. This file contains indicative values for the most important properties when accessing data from ESRI shapefiles or a spatial DBMS. Self-contained brief instructions can guide you into the extraction process.</p>  <p>Run the jar file from the command line in several alternative modes, depending on the input data source (of course, you should change the directory separator to the one your OS understands, e.g. "":"" in the case of *nix systems):</p>    <p>In case that triples will be extracted from ESRI shapefiles, and assuming that binaries are bundled together in <code>triplegeo.jar</code>, give a command like this:</br>  <code>java -cp ""./lib/*;./build/jars/triplegeo.jar"" eu.geoknow.athenarc.triplegeo.ShpToRdf options.conf</code></p>  <p>Alternatively, if triples will be extracted from a geospatially-enabled DBMS (e.g., Oracle Spatial), give a command like this:</br>  <code>java -cp ""./lib/*;./build/jars/triplegeo.jar"" eu.geoknow.athenarc.triplegeo.wkt.RdbToRdf options.conf</code></p>    <p>Wait until the process gets finished, and verify that the resulting output file is according to your specifications.</p>    The current distribution also offers transformations from other geographical formats, and it also supports GML datasets aligned to EU INSPIRE Directive. More specifically, TripleGeo can transform into RDF triples geometries available in GML (Geography Markup Language) and KML (Keyhole Markup Language). It can also handle INSPIRE-aligned GML data for seven Data Themes (Annex I). Assuming that binaries are bundled together in <code>triplegeo.jar</code>, you may transform such datasets as follows:  <ul>  <li>In case that triples will be extracted from a GML file, give a command like this:</br>  <code>java -cp ""./lib/*;./build/jars/triplegeo.jar"" eu.geoknow.athenarc.triplegeo.GmlToRdf <input.gml> <output.rdf> </code></li>  <li>In case that triples will be extracted from a KML file, give a command like this:</br>  <code>java -cp ""./lib/*;./build/jars/triplegeo.jar"" eu.geoknow.athenarc.triplegeo.KmlToRdf <input.kml> <output.rdf> </code></li>  <li>In case that triples will be extracted from an INSPIRE-aligned GML file, you must first configure XSL stylesheet <code>Inspire_main.xsl</code> with specific parameters and then give a command like this:</br>  <code>java -cp ""./lib/*;./build/jars/triplegeo.jar"" eu.geoknow.athenarc.triplegeo.InspireToRdf <input.gml> <output.rdf> </code></li>  </ul>    An alternative way to run the TripleGeo utility (the jar file) is provided via ant targets:<br/>  in the case of a shapefile input:<br/>  <code>ant run-on-shp -Dconfig=options.conf</code><br/>  in the case of the relational database:<br/>  <code>ant run-on-rdb -Dconfig=options.conf</code><br/>  in the case of a GML input:<br/>  <code>ant run-on-gml -Dinput=sample.gml -Doutput=sample.rdf</code><br/>  in the case of a KML input:<br/>  <code>ant run-on-kml -Dinput=sample.kml -Doutput=sample.rdf</code><br/>  in the case of an INSPIRE-aligned XML input:<br/>  <code>ant run-on-inspire -Dinput=sample.xml -Doutput=sample.rdf</code><br/>    <p>Indicative configuration files for several cases are available <a href=""https://github.com/GeoKnow/TripleGeo/tree/master/test/conf/"">here</a> in order to assist you when preparing your own.    <h3>  <a name=""input"" class=""anchor"" href=""#Input""><span class=""octicon octicon-link""></span></a>Input</h3>    <p>The current version of TripleGeo utility can access geometries from:</p>  <ul>  <li>ESRI shapefiles, a widely used file-based format for storing geospatial features.</li>  <li>Geographical data stored in GML (Geography Markup Language) and KML (Keyhole Markup Language).</li>  <li>INSPIRE-aligned datasets for seven Data Themes (Annex I) in GML format: Addresses, Administrative Units, Cadastral Parcels, GeographicalNames, Hydrography, Protected Sites, and Transport Networks (Roads).</li>  <li>Several geospatially-enabled DBMSs, including: Oracle Spatial, PostGIS, MySQL, and IBM DB2 with Spatial extender.</li>  </ul>  </ul>    <p>Sample geographic <a href=""https://github.com/GeoKnow/TripleGeo/tree/master/test/data/"">datasets</a> for testing are available in ESRI shapefile format.</p>    <h3>  <a name=""output"" class=""anchor"" href=""#Output""><span class=""octicon octicon-link""></span></a>Output</h3>    <p>In terms of <i>output serializations</i>, triples can be obtained in one of the following formats: RDF/XML (<i>default</i>), RDF/XML-ABBREV, N-TRIPLES, N3, TURTLE (TTL).</p>  <p>Concerning <i>geospatial representations</i>, triples can be exported according to:</p>  <ul>  <li>the <a href=""https://portal.opengeospatial.org/files/?artifact_id=47664"">GeoSPARQL standard</a> for several geometric types (including points, linestrings, and polygons)</li>  <li>the <a href=""http://www.w3.org/2003/01/geo/"">WGS84 RDF Geoposition vocabulary</a> for point features</li>  <li>the <a href=""http://docs.openlinksw.com/virtuoso/rdfsparqlgeospat.html"">Virtuoso RDF vocabulary</a> for point features.</li>  </ul>    <p>Resulting triples are written into a local file, so that they can be readily imported into a triple store.</p>      <h2>  <a name=""license"" class=""anchor"" href=""#license""><span class=""octicon octicon-link""></span></a>License</h2>    <p>The contents of this project are licensed under the <a href=""https://github.com/GeoKnow/TripleGeo/blob/master/LICENSE"">GPL v3 License</a>.</p></article>    </body>  </html> """
Semantic web;https://github.com/joachimvh/SPARQLAlgebra.js;"""# SPARQL to SPARQL Algebra converter    [![npm version](https://badge.fury.io/js/sparqlalgebrajs.svg)](https://www.npmjs.com/package/sparqlalgebrajs)  [![Build status](https://github.com/joachimvh/SPARQLAlgebra.js/workflows/CI/badge.svg)](https://github.com/joachimvh/SPARQLAlgebra.js/actions?query=workflow%3ACI)    2 components get exposed: the **translate** function and the **Algebra** object,  which contains all the output types that can occur.    Note that this is still a work in progress so naming conventions could change.  There is also support for 'non-algebra' entities such as ASK, FROM, etc.  to make sure the output contains all relevant information from the query.    ## Translate    Input for the translate function should either be a SPARQL string  or a result from calling [SPARQL.js](https://github.com/RubenVerborgh/SPARQL.js).    ```javascript  const { translate } = require('sparqlalgebrajs');  translate('SELECT * WHERE { ?x ?y ?z }');  ```  Returns:  ```json  {     ""type"": ""project"",    ""input"": {      ""type"": ""bgp"",        ""patterns"": [{          ""type"": ""pattern"",          ""termType"": ""Quad"",          ""subject"": { ""termType"": ""Variable"", ""value"": ""x"" },          ""predicate"": { ""termType"": ""Variable"", ""value"": ""y"" },          ""object"": { ""termType"": ""Variable"", ""value"": ""z"" },          ""graph"": { ""termType"": ""DefaultGraph"", ""value"": """" }        }]    },    ""variables"": [      { ""termType"": ""Variable"", ""value"": ""x"" },      { ""termType"": ""Variable"", ""value"": ""y"" },      { ""termType"": ""Variable"", ""value"": ""z"" }    ]  }    ```    Translating back to SPARQL can be done with the `toSparql` (or `toSparqlJs`) function.    ## Algebra object  The algebra object contains a `types` object,  which contains all possible values for the `type` field in the output results.  Besides that it also contains all the TypeScript interfaces of the possible output results.  The output of the `translate` function will always be an `Algebra.Operation` instance.    The best way to see what output would be generated is to look in the `test` folder,  where we have many SPARQL queries and their corresponding algebra output.    ## Deviations from the spec  This implementation tries to stay as close to the SPARQL 1.1  [specification](https://www.w3.org/TR/sparql11-query/#sparqlDefinition),  but some changes were made for ease of use.  These are mostly based on the Jena ARQ [implementation](https://jena.apache.org/documentation/query/).  What follows is a non-exhaustive list of deviations:    #### Named parameters  This is the biggest visual change.  The functions no longer take an ordered list of parameters but a named list instead.  The reason for this is to prevent having to memorize the order of parameters and also  due to seeing some differences between the spec and the Jena ARQ SSE output when ordering parameters.    #### Multiset/List conversion  The functions `toMultiset` and `toList` have been removed for brevity.  Conversions between the two are implied by the operations used.    #### Quads  The `translate` function has an optional second parameter  indicating whether patterns should be translated to triple or quad patterns.  In the case of quads the `graph` operation will be removed  and embedded into the patterns it contained.  The default value for this parameter is `false`.  ```  PREFIX : <http://www.example.org/>    SELECT ?x WHERE {      GRAPH ?g {?x ?y ?z}  }  ```    Default result:  ```json  {    ""type"": ""project"",      ""input"": {      ""type"": ""graph"",        ""input"": {        ""type"": ""bgp"",          ""patterns"": [{            ""type"": ""pattern"",            ""termType"": ""Quad"",            ""subject"": { ""termType"": ""Variable"", ""value"": ""x"" },            ""predicate"": { ""termType"": ""Variable"", ""value"": ""y"" },            ""object"": { ""termType"": ""Variable"", ""value"": ""z"" },            ""graph"": { ""termType"": ""DefaultGraph"", ""value"": """" }          }]      },      ""name"": { ""termType"": ""Variable"", ""value"": ""g"" }    },    ""variables"": [{ ""termType"": ""Variable"", ""value"": ""x"" }]  }    ```    With quads:  ```json  {    ""type"": ""project"",      ""input"": {      ""type"": ""bgp"",        ""patterns"": [{          ""type"": ""pattern"",          ""termType"": ""Quad"",          ""subject"": { ""termType"": ""Variable"", ""value"": ""x"" },          ""predicate"": { ""termType"": ""Variable"", ""value"": ""y"" },          ""object"": { ""termType"": ""Variable"", ""value"": ""z"" },          ""graph"": { ""termType"": ""Variable"", ""value"": ""g"" }        }]    },    ""variables"": [{ ""termType"": ""Variable"", ""value"": ""x"" }]  }    ```    ### Flattened operators  Several binary operators that can be nested,   such as the path operators,  can take an array of input entries to simply this notation.  For example, the following SPARQL:  ```sparql  SELECT * WHERE { ?x <a:a>|<b:b>|<c:c> ?z }  ```  outputs the following algebra:  ```json  {    ""type"": ""project"",    ""input"": {      ""type"": ""path"",      ""subject"": { ""termType"": ""Variable"", ""value"": ""x"" },      ""predicate"": {        ""type"": ""alt"",        ""input"": [          { ""type"": ""link"", ""iri"": { ""termType"": ""NamedNode"", ""value"": ""a:a"" }},          { ""type"": ""link"", ""iri"": { ""termType"": ""NamedNode"", ""value"": ""b:b"" }},          { ""type"": ""link"", ""iri"": { ""termType"": ""NamedNode"", ""value"": ""c:c"" }}        ]      },      ""object"": { ""termType"": ""Variable"", ""value"": ""z"" },      ""graph"": { ""termType"": ""DefaultGraph"", ""value"": """" }    },    ""variables"": [      { ""termType"": ""Variable"", ""value"": ""x"" },      { ""termType"": ""Variable"", ""value"": ""z"" }    ]  }    ```    #### SPARQL*    [SPARQL*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/) queries can be parsed by setting `sparqlStar` to true in the `translate` options.    #### VALUES  For the VALUES block we return the following output:  ```  PREFIX dc:   <http://purl.org/dc/elements/1.1/>   PREFIX :     <http://example.org/book/>   PREFIX ns:   <http://example.org/ns#>     SELECT ?book ?title ?price  {     VALUES ?book { :book1 :book3 }     ?book dc:title ?title ;           ns:price ?price .  }  ```  ```json  {    ""type"": ""project"",    ""input"": {      ""type"": ""join"",      ""input"": [        {          ""type"": ""values"",          ""variables"": [{ ""termType"": ""Variable"", ""value"": ""book"" }],          ""bindings"": [            { ""?book"": { ""termType"": ""NamedNode"", ""value"": ""http://example.org/book/book1"" }},            { ""?book"": { ""termType"": ""NamedNode"", ""value"": ""http://example.org/book/book3"" }}          ]        },        {          ""type"": ""bgp"",          ""patterns"": [            {              ""type"": ""pattern"",              ""termType"": ""Quad"",              ""subject"": { ""termType"": ""Variable"", ""value"": ""book"" },              ""predicate"": { ""termType"": ""NamedNode"", ""value"": ""http://purl.org/dc/elements/1.1/title"" },              ""object"": { ""termType"": ""Variable"", ""value"": ""title"" },              ""graph"": { ""termType"": ""DefaultGraph"", ""value"": """" }            },            {              ""type"": ""pattern"",              ""termType"": ""Quad"",              ""subject"": { ""termType"": ""Variable"", ""value"": ""book"" },              ""predicate"": { ""termType"": ""NamedNode"", ""value"": ""http://example.org/ns#price"" },              ""object"": { ""termType"": ""Variable"", ""value"": ""price"" },              ""graph"": { ""termType"": ""DefaultGraph"", ""value"": """" }            }          ]        }      ]    },    ""variables"": [      { ""termType"": ""Variable"", ""value"": ""book"" },      { ""termType"": ""Variable"", ""value"": ""title"" },      { ""termType"": ""Variable"", ""value"": ""price"" }    ]  }    ```    #### Differences from Jena ARQ  Some differences from Jena (again, non-exhaustive):  no prefixes are used (all uris get expanded)  and the project operation always gets used (even in the case of `SELECT *`).    ## A note on tests  Every test consists of a sparql file and a corresponding json file containing the algebra result.  Tests ending with `(quads)` in their name are tested/generated with `quads: true` in the options.    If you need to regenerate the parsed JSON files in bulk, you can invoke `node test/generate-json.js`. """
Semantic web;https://github.com/theodi/csv2rdf;"""[![License](http://img.shields.io/:license-mit-blue.svg)](http://theodi.mit-license.org)    # CSV 2 RDF    A ruby gem to convert CSV to RDF, following the CSV on the Web specification.      ## Installation      $ gem install specific_install      $ sudo gem specific_install -l https://github.com/theodi/csv2rdf        ## Usage        $ csv2rdf myfile.csv   """
Semantic web;https://github.com/srdc/ontmalizer;"""<!--  Copyright (C) 2013 SRDC Yazilim Arastirma ve Gelistirme ve Danismanlik Tic. Ltd. Sti.    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at       http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.  -->    Ontmalizer [![License Info](http://img.shields.io/badge/license-Apache%202.0-brightgreen.svg)](https://github.com/srdc/ontmalizer/blob/master/LICENSE.txt)  ===    Ontmalizer performs comprehensive transformations of XML Schemas (XSD) and XML data to RDF/OWL automatically. Through this tool, it is possible to create RDF/OWL representation of XML Schemas, and XML instances that comply with such XML Schemas.    The state of the art open source and/or free tools for RDFizing XSD and XML are not able to handle complex schemas and XML instances such as HL7 Clinical Document Architecture (CDA) R2. Only a few commercial tools such as TopBraid Composer are successfully able to do so. However, we do not want to use commercial tools in our SALUS Project: http://www.srdc.com.tr/projects/salus/. As a result, we implemented our own solution. We make use of Sun's XSOM library for processing XML Schemas, Apache Xerces for processing XML data and Apache Jena for managing RDF data.    Further information and technical details can be found in our blog post accessible at http://www.srdc.com.tr/projects/salus/blog/?p=189.    ## Installation    Apache Maven is required to build the Ontmalizer. Please visit http://maven.apache.org/ in order to install Maven on your system.    Under the root directory of the Ontmalizer project run the following:    	$ ontmalizer> mvn install    In order to make a clean install run the following:    	$ ontmalizer> mvn clean install    These will build the Ontmalizer and also run a number of test cases, which will transform some XML Schemas (e.g. HL7 CDA R2, SALUS Common Information Model) and corresponding XML instances to RDF/OWL.     ## Transforming XSD to RDF/OWL    XSD2OWLMapper is the main class to transform XML Schemas to RDF/OWL. The constructor of this class gets the root XSD file to be transformed. Configuration of the transformation operation is quite simple: the caller can set the prefixes for the object property and datatype property names to be created. Then, the call to the convertXSD2OWL() method performs the transformation.     XSD2OWLMapper is able to print the output ontology in one of these formats: RDF/XML, RDF/XML-ABBREV, N-TRIPLE and N3. An example transformation routine is provided below for the HL7 CDA R2 XML Schema:    ```java      // This part converts XML schema to OWL ontology.      XSD2OWLMapper mapping = new XSD2OWLMapper(new File(""src/test/resources/CDA/CDA.xsd""));      mapping.setObjectPropPrefix("""");      mapping.setDataTypePropPrefix("""");      mapping.convertXSD2OWL();        // This part prints the ontology to the specified file.      FileOutputStream ont;      try {          File f = new File(""src/test/resources/output/cda-ontology.n3"");          f.getParentFile().mkdirs();          ont = new FileOutputStream(f);          mapping.writeOntology(ont, ""N3"");          ont.close();      } catch (Exception e) {          e.printStackTrace();      }  ```    ## Transforming XML to RDF/OWL    XML2OWLMapper is the main class to transform XML data to RDF/OWL by creating instances of the necessary OWL classes, RDFS datatypes, OWL datatype and object properties. The constructor of this class gets the XML file to be transformed together with an instance of XSD2OWLMapper that is already initialized with the corresponding XML Schema of the XML data. No other configuration is necessary for the transformation operation; the prefixes for the object property and datatype property names to be created are gathered from the XSD2OWLMapper configuration. Then, the call to the convertXML2OWL() method performs the transformation.    Similar to XSD2OWLMapper, XML2OWLMapper is able to print the output ontology instance in one of these formats: RDF/XML, RDF/XML-ABBREV, N-TRIPLE and N3. An example transformation routine is provided below for a complete HL7 CDA R2 instance, which is compliant with the HL7/ASTM Continuity of Care Document (CCD) and IHE Patient Care Coordination (PCC) templates:    ```java      // This part converts XML schema to OWL ontology.      XSD2OWLMapper mapping = new XSD2OWLMapper(new File(""src/test/resources/CDA/CDA.xsd""));      mapping.setObjectPropPrefix("""");      mapping.setDataTypePropPrefix("""");      mapping.convertXSD2OWL();        // This part converts XML instance to RDF data model.      XML2OWLMapper generator = new XML2OWLMapper(          new File(""src/test/resources/CDA/SALUS-sample-full-CDA-instance.xml""), mapping);      generator.convertXML2OWL();            // This part prints the RDF data model to the specified file.      try{          File f = new File(""src/test/resources/output/salus-cda-instance.n3"");          f.getParentFile().mkdirs();          FileOutputStream fout = new FileOutputStream(f);          generator.writeModel(fout, ""N3"");          fout.close();        } catch (Exception e){          e.printStackTrace();      }  ```    Please refer to our blog post (http://www.srdc.com.tr/projects/salus/blog/?p=189) for further details. """
Semantic web;https://github.com/edmcouncil/rdf-toolkit;"""<img src=""https://spec.edmcouncil.org/fibo/htmlpages/master/latest/img/logo.66a988fe.png"" width=""150"" align=""right""/>    # rdf-toolkit    The `rdf-toolkit` is a command-line 'swiss army knife' tool for reading and writing RDF and OWL files in whatever format.    The primary reason for creating this tool was to have a reference implementation of the toolkit/formatter that   creates the FIBO ontologies as they are stored in the [Github FIBO repository](https://github.com/edmcouncil/fibo)   (which is at this point in time still a private repository). However, this tool is not in any way specific to FIBO,   it can be used with any set of ontologies or for that matter even ""normal"" RDF files.    It currently uses OWLAPI and RDF4J to do the hard work, see [this page](docs/dependencies.md) for more info about those products.    This will be used in a commit-hook to make sure that all RDF files in the repo are stored in the same way.    See for more information about developing rdf-toolkit [this](docs/develop.md) page or [this page](docs/dependencies.md) for information about dependencies.    # Recommended Output Format    The recommended Output Format at this time is RDF/XML because that is the format that the OMG requires for submissions.   The EDM Council develops the FIBO Ontologies and submits them as RDF/XML, serialized by the `rdf-toolkit` to the OMG.   So that is why we also use RDF/XML in Github itself. There are some issues with that and we're working on resolving that,   by either ""fixing"" the RDF/XML output generated by OWLAPI or RDF4J, or by eventually migrating to some other format.   For use in Git we need a format that:    ## Requirements for Git-based Ontology Serialization    - As few 'diff-lines' as possible per 'pull request'  - Relative URIs    - so that Git branch or tag name can become part of the final Ontology Version IRI    - so that dereferencing from tools like Protege, straight to the GitHub repo would work  - Readable (RDF/XML is only readable by the very few)    # Issues    The FIBO JIRA server has a separate project for the rdf-toolkit: https://jira.edmcouncil.org/browse/RDFSER    Please add your issues, bugs, feature requests, requirements or questions as issues on the JIRA site.    # Download    Download the RDF Toolkit [here](https://jenkins.edmcouncil.org/view/rdf-toolkit/job/rdf-toolkit-build/lastSuccessfulBuild/artifact/target/rdf-toolkit.jar)    # Usage    Download the `rdf-toolkit.jar` file mentioned in the Download section above to your computer.    ## Linux or Mac OS X    On Linux or Mac OS X you can execute the rdf-toolkit as follows:    1. Open a Terminal  2. Type the name of the `rdf-toolkit.jar` file on the command prompt and supply the `--help` option:  ```  $ java -jar rdf-toolkit.jar --help  ```    ## Windows    1. Open a Command Shell by going to the Start menu and type `cmd.exe`.  2. Ensure that Java is installed by typing `java -version` on the command line. The Java version should be at least 1.7 (i.e. Java 7).  3. Then launch the rdf-toolkit's help function as follows:  ```  java -jar rdf-toolkit.jar --help  ```    # --help    The current `--help` option gives the following information:    ```  usage: RdfFormatter (rdf-toolkit version 1.11.0)   -bi,--base-iri <arg>                    set IRI to use as base URI   -dtd,--use-dtd-subset                   for XML, use a DTD subset in order to allow prefix-based                                           IRI shortening   -h,--help                               print out details of the command-line arguments for the                                           program   -i,--indent <arg>                       sets the indent string.  Default is a single tab character   -ibi,--infer-base-iri                   use the OWL ontology IRI as the base URI.  Ignored if an                                           explicit base IRI has been set   -ibn,--inline-blank-nodes               use inline representation for blank nodes.  NOTE: this will                                           fail if there are any recursive relationships involving                                           blank nodes.  Usually OWL has no such recursion involving                                           blank nodes.  It also will fail if any blank nodes are a                                           triple subject but not a triple object.   -ip,--iri-pattern <arg>                 set a pattern to replace in all IRIs (used together with                                           --iri-replacement)   -ir,--iri-replacement <arg>             set replacement text used to replace a matching pattern in                                           all IRIs (used together with --iri-pattern)   -lc,--leading-comment <arg>             sets the text of the leading comment in the ontology.  Can                                           be repeated for a multi-line comment   -osl,--override-string-language <arg>   sets an override language that is applied to all strings   -s,--source <arg>                       source (input) RDF file to format   -sd,--source-directory <arg>            source (input) directory of RDF files to format.  This is a                                           directory processing option   -sdp,--source-directory-pattern <arg>   relative file path pattern (regular expression) used to                                           select files to format in the source directory.  This is a                                           directory processing option   -sdt,--string-data-typing <arg>         sets whether string data values have explicit data types,                                           or not; one of: explicit, implicit [default]   -sfmt,--source-format <arg>             source (input) RDF format; one of: auto (select by                                           filename) [default], binary, json-ld (JSON-LD), n3, n-quads                                           (N-quads), n-triples (N-triples), rdf-a (RDF/A), rdf-json                                           (RDF/JSON), rdf-xml (RDF/XML), trig (TriG), trix (TriX),                                           turtle (Turtle)   -sip,--short-iri-priority <arg>         set what takes priority when shortening IRIs: prefix                                           [default], base-iri   -t,--target <arg>                       target (output) RDF file   -tc,--trailing-comment <arg>            sets the text of the trailing comment in the ontology.  Can                                           be repeated for a multi-line comment   -td,--target-directory <arg>            target (output) directory for formatted RDF files.  This is                                           a directory processing option   -tdp,--target-directory-pattern <arg>   relative file path pattern (regular expression) used to                                           construct file paths within the target directory.  This is                                           a directory processing option   -tfmt,--target-format <arg>             target (output) RDF format: one of: json-ld (JSON-LD),                                           rdf-xml (RDF/XML), turtle (Turtle) [default]   -v,--version                            print out version details  ```    * [Sesame serializer documentation](docs/RdfFormatter.md) """
Semantic web;https://github.com/AKSW/rocker;"""## ROCKER: A Refinement Operator for Key Discovery ##    [![Build Status](http://ci.aksw.org/jenkins/buildStatus/icon?job=Rocker)](http://ci.aksw.org/jenkins/view/All/job/Rocker/)    ### Demo ###    A demo of ROCKER for Data Quality is running at http://rocker.aksw.org. It offers a web interface with accessible APIs. For computations on large datasets, please follow the guide below.    ### Run from terminal ###    First, download the [full jar package](https://github.com/AKSW/rocker/releases/download/v1.2.1/rocker-1.2.1-full.jar), which also contains all required dependencies. Datasets are available here:    OAEI Benchmark 2011 (artificial data)    * [OAEI_2011_Restaurant_1.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/OAEI_2011_Restaurant_1.nt.gz)  * [OAEI_2011_Restaurant_2.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/OAEI_2011_Restaurant_2.nt.gz)    DBpedia 3.9 (real data)    * [album.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/album.nt.gz)  * [animal.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/animal.nt.gz)  * [architecturalStruture.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/architecturalStruture.nt.gz)  * [artist.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/artist.nt.gz)  * [careerstation.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/careerstation.nt.gz)  * [musicalWork.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/musicalWork.nt.gz)  * [organisationMember.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/organisationMember.nt.gz)  * [personFunction.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/personFunction.nt.gz)  * [soccerplayer.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/soccerplayer.nt.gz)  * [village.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/village.nt.gz)    To run ROCKER:    ```  java -Xmx8g -jar rocker-1.2.1-full.jar <dataset name> <dataset path with protocol> <class name> <find one key> <fast search> <alpha threshold>  ```    Example:    ```  java -Xmx8g -jar rocker-1.2.1-full.jar ""restaurant_1"" ""file:///home/rocker/OAEI_2011_Restaurant_1.nt"" ""http://www.okkam.org/ontology_restaurant1.owl#Restaurant"" false true 1.0  ```    We recommend to run your experiments on a machine with at least 8 GB of RAM.    ### Maven    ```xml  <repository>      <id>maven.aksw.internal</id>      <name>University Leipzig, AKSW Maven2 Repository</name>      <url>http://maven.aksw.org/archiva/repository/internal</url>  </repository>  ...  <dependency>      <groupId>org.aksw.rocker</groupId>      <artifactId>rocker</artifactId>      <version>1.3.1</version>  </dependency>  ```    ### Java library ###    You may also download the [Java library](https://github.com/AKSW/rocker/releases/download/v1.2.1/rocker-1.2.1.jar) without dependencies.    ### Basic usage ###    ```java  Rocker r = null;  r = new Rocker(""restaurant_1"", ""file:///home/rocker/OAEI_2011_Restaurant_1.nt"",          ""http://www.okkam.org/ontology_restaurant1.owl#Restaurant"", false, true, 1.0);  r.run();  Set<CandidateNode> results = r.getKeys();  ```    ### Citing ROCKER ###    Please refer to the paper *T. Soru, E. Marx, A.-C. Ngonga Ngomo, ""ROCKER: A Refinement Operator for Key Discovery""*, in proceedings of the 24th International Conference on World Wide Web, WWW 2015. [[PDF](http://svn.aksw.org/papers/2015/WWW_Rocker/public.pdf)] [[ACM](http://dl.acm.org/citation.cfm?id=2741642)]    ```  @inproceedings{Soru:2015:RRO:2736277.2741642,   author = {Soru, Tommaso and Marx, Edgard and {Ngonga Ngomo}, Axel-Cyrille},   title = {ROCKER: A Refinement Operator for Key Discovery},   booktitle = {Proceedings of the 24th International Conference on World Wide Web},   series = {WWW '15},   year = {2015},   isbn = {978-1-4503-3469-3},   location = {Florence, Italy},   pages = {1025--1033},   numpages = {9},   url = {http://doi.acm.org/10.1145/2736277.2741642},   doi = {10.1145/2736277.2741642},   acmid = {2741642},   publisher = {ACM},   address = {New York, NY, USA},   keywords = {key discovery, link discovery, linked data, refinement operators, semantic web},  }  ``` """
Semantic web;https://github.com/pebbie/triplify;"""triplify  ========    Continuing the triplify.org PHP project by AKSW, and merging with triplify-python by rgeorgy     """
Semantic web;https://github.com/AKSW/QuitStore;"""  <img alt=""The QuitStore Logo: A glass of quinch jam (German: Quittenmarmelade) with the Git logo on the lid. 'Graph jam in a git glass'"" src=""https://raw.githubusercontent.com/AKSW/QuitStore/master/assets/quitstore.png"" width=""512"" />    # Quit Store    Build status of `master` branch:    [![Build Status](https://travis-ci.org/AKSW/QuitStore.svg?branch=master)](https://travis-ci.org/AKSW/QuitStore)  [![Coverage Status](https://coveralls.io/repos/github/AKSW/QuitStore/badge.svg?branch=master)](https://coveralls.io/github/AKSW/QuitStore)    The *Quit Store* (stands for <em>Qu</em>ads in G<em>it</em>) provides a workspace for distributed collaborative Linked Data knowledge engineering.  You are able to read and write [RDF Datasets](https://www.w3.org/TR/rdf11-concepts/#section-dataset) (aka. multiple [Named Graphs](https://en.wikipedia.org/wiki/Named_graph)) through a standard SPARQL 1.1 [Query](https://www.w3.org/TR/sparql11-query/) and [Update](https://www.w3.org/TR/sparql11-update/) interface.  To collaborate you can create multiple branches of the Dataset and share your repository with your collaborators as you know it from Git.    If you want to read more about the Quit Store we can recommend our paper:    [*Decentralized Collaborative Knowledge Management using Git*](https://natanael.arndt.xyz/bib/arndt-n-2018--jws)  by Natanael Arndt, Patrick Naumann, Norman Radtke, Michael Martin, and Edgard Marx in Journal of Web Semantics, 2018  [[@sciencedirect](https://www.sciencedirect.com/science/article/pii/S1570826818300416)] [[@arXiv](https://arxiv.org/abs/1805.03721)]    ## Getting Started    To get the Quit Store you have three options:    - Download a binary from https://github.com/AKSW/QuitStore/releases (Currently works for amd64 Linux)  - Clone it with Git from our repository: https://github.com/AKSW/QuitStore  - Use Docker and see the section [Docker](#docker) in the README    ### Installation from Source    Install [poetry](https://python-poetry.org/).    Get the Quit Store source code:  ```  $ git clone https://github.com/AKSW/QuitStore.git  $ cd QuitStore  ```  If you are using virtualenvwrapper:  ```  $ poetry install  $ poetry run quitstore --help  ```    ### Git configuration    Configure your name and email for Git.  This information will be stored in each commit you are creating with Git and the Quit Store on your system.  It is relevant so people know which contribution is coming from whom. Execute the following command if you haven't done that before.    ```  $ git config --global user.name ""Your Name""  $ git config --global user.email ""you@e-mail-provider.org""  ```    ### Start with Existing Data (Optional)    If you already have data which you want to use in the quit store follow these steps:    1. Create a repository which will contain your RDF data.    ```  $ git init /path/to/repo  ```    2. Put your RDF data formatted as [N-Triples](https://www.w3.org/TR/n-triples/) and sorted (e.g. using `cat data-in.nt | LC_ALL=C sort -u > data-out.nt`) into files like `<graph>.nt` into this directory.  3. For each `<graph>.nt` file create a corresponding `<graph>.nt.graph` file which must contain the IRI for the respective graph. (These `.graph` files are also used by the [Virtuoso bulk loading process](https://virtuoso.openlinksw.com/dataspace/doc/dav/wiki/Main/VirtBulkRDFLoader#Bulk%20loading%20process)).  4. Add the data to the repository and create a commit.    ```  $ git add …  $ git commit -m ""init repository""  ```    To ingest further versions of your data into the Quit Store you can add further commits by going through steps 2.-4..  Alternatively you are also able to execute SPARQL 1.1. Update operations to create new versions on the Quit Store.    ### Start the Quit Store    If you are using the binary:  ```  $ chmod +x quit #  $ ./quit -t /path/to/repo  ```    If you have it installed from the sources:  ```  $ poetry run quitstore -t /path/to/repo  ```    Open your browser and go to [`http://localhost:5000/`](http://localhost:5000/).    Have a lot of fun!    For more command line options check out the section [Command Line Options](#command-line-options) in the README.        ## Command Line Options    `-b`, `--basepath`    Specify a base path/application root. This will work with WSGI and docker only.    `-t`, `--targetdir`    Specify a target directory where the repository can be found or will be cloned (if remote is given) to.    `-r`, `-repourl`    Specify a link/URL to a remote repository.    `-c`, `--configfile`    Specify a path to a configuration file. (Defaults to ./config.ttl)    `-nv`, `--disableversioning`    Run Quit-Store without versioning activated    `-f`, `--features`    This option enables additional features of the store:    - `provenance` - Enable browsing interfaces for provenance information.  - `persistance` - Store all internal data as RDF graph.  - `garbagecollection` - Enable garbage collection. With this feature enabled, git will check for garbage collection after each commit. This may slow down response time but will keep the repository size small.    `-v`, `--verbose` and `-vv`, `--verboseverbose`    Set the log level for the standard output to verbose (INFO) respective extra verbose (DEBUG).    `-l`, `--logfile`    Write the log output to the given path.  The path is interpreted relative to the current working directory.  The log level for the logfile is always extra verbose (DEBUG).    ## Configuration File    *deprecated* (we plan to remove the configuration file feature)    If you want to work with configuration files you can create a `config.ttl` file.  This configuration file consists of two parts, the store configuration and the graph configuration.  The store configuration manages everything related to initializing the software, the graph configuration maps graph files to their graph IRIs.  The graph configuration in the `config.ttl` is an alternative to using `<graph>.nt.graph` files next to the graphs.  Make sure you put the correct path to your git repository (`""../store""`) and the IRI of your graph (`<http://example.org/>`) and name of the file holding this graph (`""example.nt""`).    ```  conf:store a <YourQuitStore> ;      <pathOfGitRepo> ""../store"" ; # Set the path to the repository that contains the files .      <origin> ""git:github.com/your/repository.git"" . # Optional a git repo that will be cloned into dir given in line above on startup.      conf:example a <Graph> ; # Define a Graph resource for a named graph      <graphUri> <http://example.org/> ; # Set the IRI of named graph      <isVersioned> 1 ; # Defaults to True, future work      <graphFile> ""example.nt"" . # Set the filename  ```    ## API    The Quit-Store comes with three kinds of interfaces, a SPARQL update and query interface, a provenance interface, and a Git management interface.    ### SPARQL Update and Query Interface  The SPARQL interface support update and select queries and is meant to adhere to the [SPARQL 1.1 Protocol](https://www.w3.org/TR/sparql11-protocol/).  You can find the interface to query the current `HEAD` of your repository under `http://your-quit-host/sparql`.  To access any branch or commit on the repository you can query the endpoints under `http://your-quit-host/sparql/<branchname>` resp. `http://your-quit-host/sparql/<commitid>`.  Since the software is still under development there might be some missing features or strange behavior.  If you are sure that the store does not follow the W3C recommendation please [file an issue](https://github.com/AKSW/QuitStore/issues/new).    #### Examples    Execute a select query with curl  ```  curl -d ""select ?s ?p ?o ?g where { graph ?g { ?s ?p ?o} }"" -H ""Content-Type: application/sparql-query"" http://your-quit-host/sparql  curl -d ""select ?s ?p ?o ?g where { graph ?g { ?s ?p ?o} }"" -H ""Content-Type: application/sparql-query"" http://your-quit-host/sparql/develop  ```  If you are interested in a specific result mime type you can use the content negotiation feature of the interface:  ```  curl -d ""select ?s ?p ?o ?g where { graph ?g { ?s ?p ?o} }"" -H ""Content-Type: application/sparql-query"" -H ""Accept: application/sparql-results+json"" http://your-quit-host/sparql  ```    Execute an update query with curl    ```  curl -d ""insert data { graph <http://example.org/> { <urn:a> <urn:b> <urn:c> } }"" -H ""Content-Type: application/sparql-update""  http://your-quit-host/sparql  ```    ### Provenance Interface  To use the provenance browsing feature you have to enable it with the argument `--feature=provenance`.  The provenance browsing feature extracts provenance meta data for the revisions and makes it available through a SPARQL endpoint and the blame interface.  The provenance interface is available under the following two URLs:    - `http://your-quit-host/provenance` which is a SPARQL query interface (see above) to query the provenance graph  - `http://your-quit-host/blame` to get a `git blame` like output per statement in the store    ### Git Management Interface    The git management interface allows access to some operations of quit in conjunction with the underlying git repository.  You can access them with your browser at the following paths.    - `/commits`: See commits, messages, committer, and date of commits.  - `/branch`, `/merge`: allows to manage branches and merge branches with different strategies.  - `/pull`, `/fetch`, `/push` work similar to the respective git commands. (These operations will only works if you have configured remotes on the repository.)    ## Docker    We provide a Docker image for the Quit Store on the [public docker hub](https://hub.docker.com/r/aksw/quitstore/) as well as on the [github docker registry](https://github.com/AKSW/QuitStore/pkgs/container/quitstore).  The image exposes port 8080 by default.  The default user within the image is the user `quit` with the user id `1000`.  For this user a git configuration with `user.name QuitStore` and `user.email quitstore@example.org` is preset.  Without any further configuration, a git repository is initialized within the container in the `/data` directory (owned by the default user `quit`).    To store the data on the host a local directory or volume is required to store the git repository.  An host directory or volume can be linked to the directory `/data`.  Make sure the quit process running with the user id `1000` within the docker container has write access to this directory.    Alternatively the user id within the container can be set using the [`docker run --user $UID …` option](https://docs.docker.com/engine/reference/commandline/run/).  In this case you have to make sure a `user.name` a `user.email` is configure using `git config` within the repository (`.git/config`) or a git config file is mounted to `/.gitconfig` (to `/usr/src/app/.gitconfig` if you are running it with user id `1000`).    Example setup with the default user:    ```  mkdir /store/repo  sudo chown 1000 /store/repo  sudo chmod u+w /store/repo  ```    To run the image execute the following command (maybe you have to replace `docker` with `sudo docker`):    ```  docker run -it --name containername -p 8080:8080 -v /store/repo:/data aksw/quitstore  ```    The following example will start the quit store in the background in the detached mode.    ```  docker run -d --name containername -p 8080:8080 -v /store/repo:/data aksw/quitstore  ```    Now you should be able to access the quit web interface under `http://localhost:8080` and the SPARQL 1.1 interface under `http://localhost:8080/sparql`.    The default configuration is located in `/etc/quit/config.ttl`, which can also be overwritten using a respective volume or by setting the `QUIT_CONFIGFILE` environment variable.    Further options which can be set are:    * `QUIT_TARGETDIR` - the target repository directory on which quit should run  * `QUIT_CONFIGFILE` - the path to the config.ttl (default: `/etc/quit/config.ttl`)  * `QUIT_LOGFILE` - the path where quit should create its logfile  * `QUIT_BASEPATH` - the HTTP base path where quit will be served  * `QUIT_OAUTH_CLIENT_ID` - the GitHub OAuth client id (for OAuth see also the [github docu](https://developer.github.com/apps/building-oauth-apps/authorization-options-for-oauth-apps/))  * `QUIT_OAUTH_SECRET` - the GitHub OAuth secret    ## Run the Tests    You need to have the quitstore installed from source, see section [Installation from Source](#installation-from-source).    ```  poetry run pytest  ```    ## Troubleshooting    ### Use on Windows with restricted permissions    On Windows you might not be able to download the `.exe` file directly.  If so, use the `curl` command in the power shell.    When you start the QuitStore (e.g. with `quit.exe -t .`) it will try to open a port that is available from outside, which will require permission by the administrator user.  To open the port only locally you should start the QuitStore with:    ```  quit.exe -t . -h localhost  ```    The default port is `5000` (`http://localhost:5000/`).    ## Migrate from old Versions    ### Update to 2018-11-20 from 2018-10-29 and older    If you are migrating from an NQuads based repository, as used in older versions of the QuitStore (release 2018-10-29 and older), to an NTriples based repository (release 2018-11-20 and newer) you can use teh following commands to migrate the graphs.  You should know that it is possible to have multiple graphs in one NQuads file, which is not possible for NTriples files.  Thus, you should make sure to have only one graph per file.  You may execute the steps for each NQuads file and replace `graphfile.nq` according to your filenames.    ```  sed ""s/<[^<>]*> .$/./g"" graphfile.nq | LC_ALL=C sort -u > graphfile.nt  mv graphfile.nq.graph graphfile.nt.graph  git rm graphfile.nq  git add graphfile.nq.graph graphfile.nt graphfile.nt.graph  git commit -m ""Migrate from nq to nt""  ```    ## License    Copyright (C) 2017-2022 Norman Radtke <http://aksw.org/NormanRadtke>, Natanael Arndt <http://aksw.org/NatanaelArndt>, and contributors    This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.    This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.    You should have received a copy of the GNU General Public License along with this program; if not, see <http://www.gnu.org/licenses>.  Please see [LICENSE](LICENSE) for further information. """
Semantic web;https://github.com/Swirrl/matcha;"""# Matcha    [![Clojars Project](https://img.shields.io/clojars/v/grafter/matcha.alpha.svg)](https://clojars.org/grafter/matcha.alpha) [![cljdoc badge](https://cljdoc.org/badge/grafter/matcha.alpha)](https://cljdoc.org/d/grafter/matcha.alpha)    *WARNING: Alpha Software Subject to Change*    A Clojure DSL to query in memory triple models with a SPARQL like  language.  Matcha provides simple BGP (Basic Graph Pattern) style  queries on in memory graphs of linked data triples.    ![Matcha](https://raw.githubusercontent.com/Swirrl/matcha/master/doc/matcha.jpg ""Matcha"")    Whilst Matcha is intended to query RDF models it can also be used to  query arbitrary clojure data, so long as it consists of Clojure values  stored in 3/tuple vectors, each entity of the triple is assumed to  follow Clojure value equality semantics.    The primary use cases for Matcha are to make handling graphs of RDF  data easy by querying data with SPARQL-like queries.  A typical  workflow is to `CONSTRUCT` data from a backend SPARQL query, and then  use Matcha to query this graph locally.    ## Features    - SPARQL-like BGP queries across multiple triple patterns.  - Parameterised queries using just clojure `let`.  - Ability to index your database, with `index-triples`.  In order to    be queried Matcha needs to have indexed the data; if your data is    unindexed it will index it before running the query, and then    dispose of the index.  This can lead to poor performance when you    want to query the same set of data multiple times.  - Construct graph query results directly into clojure datastructures.  - Support for `VALUES` clauses (unlike in SPARQL we do not yet support    binding arbitrary tuples/tables).  So we only support the    `VALUES ?x { ... }` form.  - Support for `OPTIONAL`s with SPARQL-like semantics.    ## Limitations    The initial implementation is macro heavy.  This means use cases where  you want to dynamically create in memory queries may be more awkward.    Currently there is no support for the following SPARQL-like features:    1. Reasoning on in memory vocabularies with RDFS (maybe OWL)  2. Clojurescript support (planned)    ## Usage    Matcha defines some primary query functions `select`, `select-1`,  `build`, `build-1`, `construct`, `construct-1` and `ask`.    First lets define an in memory database of triples, in reality this  could come from a SPARQL query `CONSTRUCT`, but here we'll just define  some RDF-like data inline.    Triples can be vectors of clojure values or any datastructure that  supports positional destructuring via `clojure.lang.Indexed`, this  allows Matcha to work `grafter.rdf.protocols.Statement` records.  Matcha works with any clojure values in the triples, be they java  URI's, or clojure keywords.    ```clojure  (def friends-db [[:rick :rdfs/label ""Rick""]                   [:martin :rdfs/label ""Martin""]                   [:katie :rdfs/label ""Katie""]                   [:julie :rdfs/label ""Julie""]                     [:rick :foaf/knows :martin]                   [:rick :foaf/knows :katie]                   [:katie :foaf/knows :julie]                     [:rick :a :foaf/Person]                   [:katie :a :foaf/Person]                   [:martin :a :foaf/Person]])  ```    Now we can build our query functions:    ### General Query Semantics    There are two main concepts to Matcha queries.  They typically define:    1. a projection, which states what variables to return to your Clojure  program, and the datastructure they should be returned in.  2. a Basic Graph Pattern (BGP), that defines the pattern of the graph     traversal.    BGPs have some semantics you need to be aware of:    - Clojure symbols beginning with a `?` are treated specially as query    variables.  - Other symbols are resolved to their values.    ### `build`    `build` always groups returned solutions into a sequence of clojure  maps, where the subjects are grouped into maps, and the maps are  grouped by their properties. If a property has multiple values they  will be rolled up into a set, otherwise they will be a scalar value.    Each map returned by `build` typically represents a resource in the  built graph, which is projected into a sequence of maps, with  potentially multi-valued keys.    It takes a binding for `?subject` of the map, a map form specifying  the projection of other property/value bindings a `bgp` and a  database.    ``` clojure  (build ?person         {:foaf/knows ?friends}         [[?person :foaf/knows ?friends]]         friends-db)    ;; => ({:grafter.rdf/uri :rick, :foaf/knows #{:martin :katie}}  ;;     {:grafter.rdf/uri :katie, :foaf/knows :julie}  ```    NOTE: `:foaf/knows` is projected into a set of values for `:rick`, but  a single scalar value for `:katie`.    The `?subject` is by default associated with the key  `:grafter.rdf/uri`. If you wish to specify this key yourself you can  by providing a key/value pair as the subject: e.g. substituting  ?person for `[:id ?person]` changes the return values like so:    ``` clojure  (build [:id ?person]         {:foaf/knows ?friends}         [[?person :foaf/knows ?friends]]           friends-db)  ;; => ({:id :rick, :foaf/knows #{:martin :katie}}  ;;     {:id :katie, :foaf/knows :julie}  ```    Because `build` knows it is always returning a sequence of maps, it  will remove any keys corresponding to unbound variables introduced  through optionals.  This is unlike `construct`.    ### `select`    `select` compiles a query from your arguments, that returns results as a  sequence of tuples. It is directly analagous to SPARQL's `SELECT` query.    The `bgp` argument is analagous to a SPARQL `WHERE` clause and should be  a BGP.    When called with one argument, `select` projects all `?qvar`s used in the  query.  This is analagous to `SELECT *` in SPARQL:    ```clojure  (def rick-knows    (select      [[:rick :foaf/knows ?p2]      [?p2 :rdfs/label ?name]]))    (rick-knows friends-db)  ;; => [""Martin"" ""Katie""]  ```    When called with two arguments `select` expects the first argument to be a  vector of variables to project into the solution sequence.    ```clojure  (def rick-knows (select [?name]                    [[:rick :foaf/knows ?p2]                     [?p2 :rdfs/label ?name]]))    (rick-knows friends-db)  ;; => [""Martin"" ""Katie""]  ```    There is also `select-1` which is just like `select` but returns just  the first solution.    ### `construct`    NOTE: if you're using you `construct` to return maps, you should first  consider using `build` which fixes some issues present in common  `construct` usage.    `CONSTRUCT`s allow you to construct arbitrary clojure data structures  directly from your query results, and position the projected query  variables where ever you want within the projected datastructure  template.    Args:   * `construct-pattern`: an arbitrary clojure data structure. Results     will be projected into the `?qvar` ""holes"".   * `bgps`: this argument is analagous to a SPARQL `WHERE` clause and should be     a BGPs.   * `db-or-idx`: A matcha ""database"".    When called with two arguments `construct` returns a query function  that accepts a `db-or-idx` as its only argument. When called, the  function returns a sequence of matching tuples in the form of the  `construct-pattern`.    ```clojure  (construct {:grafter.rdf/uri :rick              :foaf/knows {:grafter.rdf/uri ?p                           :rdfs/label ?name}}    [[:rick :foaf/knows ?p]     [?p :rdfs/label ?name]])    ;; => (fn [db-or-idx] ...)  ```    When called with 3 arguments, queries the `db-or-idx` directly, returning a  sequence of results in the form of the `construct-pattern`.    ```clojure  (construct {:grafter.rdf/uri :rick              :foaf/knows {:grafter.rdf/uri ?p                           :rdfs/label ?name}}    [[:rick :foaf/knows ?p]     [?p :rdfs/label ?name]]    friends-db)    ;; => {:grafter.rdf/uri :rick  ;;     :foaf/knows #{{:grafter.rdf/uri :martin, :rdfs/label ""Martin""}  ;;                   {:grafter.rdf/uri :katie, :rdfs/label ""Katie""}}}  ```    Maps in a projection that contain the special key of  `:grafter.rdf/uri` trigger extra behaviour, and cause the query  engine to group solutions by subject, and merge values into clojure  sets.  For example in the above query you'll notice that `foaf:knows`  groups its solutions.  If you don't want these maps to be grouped,  don't include the magic key `:grafter.rdf/uri` in the top level  projection.    There is also `construct-1` which is just like `construct` but returns  only the first solution.    See the [unit  tests](https://github.com/Swirrl/matcha/blob/ae2449483d5a7849ac60a3e5b6a29e459d74ad8e/test/grafter/matcha/alpha_test.clj#L113)  for more examples, including examples that use Matcha with Grafter  Statements and vocabularies.    ### `ask`    `ask` is the only query that doesn't specify an explicit projection.  It accepts a BGP, like the other query types and returns a boolean  result if there were any matches found.    ```clojure  (def any-triples? (ask [[?s ?p ?o]])    (any-triples? friends-db) ;; => true  ```    ### Parameterising queries    You can parameterise Matcha queries simply by adding a lexical binding or wrapping a function call over your Matcha query.  For example    ```clojure  (defn lookup-friends [person-id database]    (->> database         (construct {:grafter.rdf/uri ?friend                     :name ?name}                     [[person-id :foaf/knows ?friend]                      [?friend :rdfs/label ?name]]))    (lookup-friends :rick friends-db)    ;; => [{:grafter.rdf/uri :martin, :name ""Martin""}  ;;     {:grafter.rdf/uri :katie, :name ""Katie""}]  ```    ### OPTIONALs    We support SPARQL-like `OPTIONAL`s in all query types with the following syntax:    ```clojure  (defn lookup-name [person-id database]    (select [?name]      [[person-id :a :foaf/Person]       (optional [[person :rdfs/label ?name]])       (optional [[person :foaf/name ?name]])]))  ```    ### VALUEs    We support dynamic VALUEs clauses in all query types like so:    ```clojure  (defn lookup-names [person-ids database]    (select [?name]      [(values ?person-id person-ids)       [?person-id :rdfs/label ?name]]))    (lookup-names [:rick :katie] friends-db) ;; => [""Rick"", ""Katie""]  ```    You can also hardcode the values into the query:    ```clojure  (defn lookup-names [person-ids database]    (select [?name]      [(values ?person-id [:rick :katie])       [?person-id :rdfs/label ?name]]))  ```    Any ""flat collection"" (i.e. a `sequential?` or a `set?`) is valid  on the right hand side of a `values` binding.    ## Performance    Matcha is intended to be used on modest sizes of data, typically  thousands of triples, and usually no more than a few hundred thousand  triples.  Proper benchmarking hasn't yet been done but finding all  solutions on a database of a million triples can be done on a laptop  in less than 10 seconds.  Query time scaling seems to be roughly  linear with the database size.    ## Avoiding clj-kondo lint errors with matcha macros    Matcha exports some clj-kondo configuration which prevents clj-kondo  warning about unbound variables when using the matcha query macros.    You can import these configs into your project with the following  command:    ```  $ clj-kondo --copy-configs --dependencies --lint ""$(clojure -Spath)""  Imported config to .clj-kondo/grafter/matcha.alpha. To activate, add ""grafter/matcha.alpha"" to :config-paths in .clj-kondo/config.edn.  ```    Then simply add the following to `.clj-kondo/config.edn`:    ```  {:config-paths [""grafter/matcha.alpha""]}  ```    ## Developing Matcha    Matcha uses [`tools.build`](https://clojure.org/guides/tools_build) and  [`tools.deps`](https://clojure.org/guides/deps_and_cli) for builds,  development and testing.    The command:    ```  $ clojure -T:build test  ```    Will run the tests, whilst    ```  $ clojure -T:build build  $ clojure -T:build install  ```    can be used to build and install a jar into your local mvn repository.    However for consuming local Matcha changes in local projects you are  usually better using `tools.deps` `:classpath-overrides`, or creating  a branch and consuming via a `:git/url`.    ## Deploying to Clojars    For [deployments CircleCI is setup](https://github.com/Swirrl/matcha/blob/fafe7478ae605c4cb2a0253714c3bd286e1ca185/.circleci/config.yml#L46-L55)  to automatically deploy tags of the form `vX.Y.Z` where `X.Y.Z` are  `major.minor.patch` numbers.  If you have permissions (i.e. you are  a Swirrl developer) the recommended workflow is to create a new  release of the `main` branch in github with a tag that bumps the  version number appropriately.    _NOTE_: For this step to work you will need appropriate deployment  privileges on clojars.org.    ## License    Copyright © Swirrl IT Ltd 2018    Distributed under the Eclipse Public License either version 1.0 or (at  your option) any later version. """
Semantic web;https://github.com/AKSW/xodx;"""xodx  ====    This is an implementation of the basic functionalities of a DSSN Provider:  * [Semantic Pingback](http://aksw.org/Projects/SemanticPingback) for Friending  * [Pubsubhubbub](http://code.google.com/p/pubsubhubbub/) (PuSH) for notification along the edges    It is written in PHP and utilizes the Zend Framework and the [Erfurt Framework](http://erfurt-framework.org/)    Installation  ------------  You need a webserver (tested with Apache, but I hope it also runs with nginx and lighttd) and a database backend which is supported by Erfurt (MySQL and Virtuoso).    ### Erfurt  Run `git submodules init` and `git submodules update` to clone Erfurt.    Take one of the prepared `config.ini-*` files in `xodx/libraries/Erfurt/library/Erfurt`, copy it to `config.ini` and configure it according to your system setup.    ### Zend  You have to place a copy of the Zend framework library into `libraries/Zend/` you can do this by doing the following things (replace `${ZENDVERSION}` e.g. with `1.12.0`):        wget http://packages.zendframework.com/releases/ZendFramework-${ZENDVERSION}/ZendFramework-${ZENDVERSION}-minimal.tar.gz      tar xzf ZendFramework-${ZENDVERSION}-minimal.tar.gz      mv ZendFramework-${ZENDVERSION}-minimal/library/Zend libraries      rm -rf ZendFramework-${ZENDVERSION}-minimal.tar.gz ZendFramework-${ZENDVERSION}-minimal    ### JavaScript  You have to add [twitter bootstrap](http://twitter.github.com/bootstrap/) and [jquery](http://jquery.com/) to the `resources` directory.    Code Conventions  ----------------  Currently, this project is developed using [OntoWiki's coding standard](http://code.google.com/p/ontowiki/wiki/CodingStandard). """
Semantic web;https://github.com/nicholashauschild/kotlin-rdf;"""# kotlin-rdf  [![Build Status](https://img.shields.io/travis/nicholashauschild/kotlin-rdf/master.svg?style=flat-square)](https://travis-ci.org/nicholashauschild/kotlin-rdf)    > RDF DSL's written in Kotlin    ## What is it?  A series of DSL's to support creating and querying RDF Graphs.  This specific library is backed by Apache Jena.    ## Usage    ### Add dependency  Release dependencies: Not yet released.  Still experimental.    Snapshot dependencies:  ```  repositories {      maven {          url uri('https://oss.jfrog.org/artifactory/libs-snapshot')      }  }  dependencies {      compile ""com.github.nicholashauschild:kotlin-rdf:0.1.0-SNAPSHOT""  }  ```    ### DSL's    #### propertySchema  The `propertySchema` DSL is used to setup a property or predicate 'namespace'.    Here is an example that showcases the complete set of options available for using the propertySchema DSL:  ```  propertySchema(""http://example/schema/{{property}}"") {      ""price"" {          uri = ""http://example/schema/price""      }  }  ```    ...and here is a breakdown, mostly line-by-line, of what is happening...    `propertySchema(""http://example/schema/{{property}}"") {`    This line is doing two things.  1. It is establishing the start of the propertySchema DSL construct  2. It is providing a value for the propertySchema's namespace.    The namespace is useful for providing a default uri template, which will  allow us to remove some superfluous configuration.    ***    `    ""price"" {`    This line is doing two things.  1. It is providing a common name for a new property.  2. It is establishing the start of the definition for the new property.    ***    `        uri = ""http://example/schema/price""`    This line is defining the URI for the enclosing property.    ***    The last two lines are closing their respective constructs.    ##### Property Configuration  Here is a table with the configuration options available for a property:    | Field | Required | Description           | Default Value |  | ----- | -------- | --------------------- | ------------- |  | uri   | false    | URI for this property | 'common name' merged into namespace template    ##### Aliasing properties  If a property name is too long, or you would like to be allowed to refer  to it with additional names, then you can utilize  the `alias` keyword to create aliases for properties.    Example:  ```  propertySchema(""http://example/schema/{{property}}"") {      ""color"" { uri = ""http://example/schema/color"" } alias ""pigment""  }  ```    In the above example, 'color' and 'pigment' are two different names that refer to  the same property.    ##### propertySchema return type  The `propertySchema` DSL returns an object of type `PropertySchema`.  This object  has a function with signature `operator fun get(name: String): Property` which can be used to access the  underlying property objects, which are implementations of the Property interface of the Apache Jena API.    ```  val schema =      propertySchema(""http://example/schema/{{property}}"") {          ""height"" { uri = ""http://example/schema/height"" }      }        val aProperty: org.apache.jena.rdf.model.Property = schema[""height""]  assertEquals(""http://example/schema/height"", aProperty.getURI())  ```    ##### Reducing ceremonious syntax  The propertySchema definition shown at the beginning of this section can be written up  a bit more succinctly.    In general, the idea behind the namespace template is to be able to use a common 'base' URI  and derive the actual URI for each property from this template based on its name.  This is the  default behavior of the property definition.  With this information, we can rewrite our initial  propertySchema DSL definition like this, and we would get an equivalent result.    ```  propertySchema(""http://example/schema/{{property}}"") {      ""price"" {} // uri is the value of the merged namespace template and property name  }  ```    Pairing this with the `alias` keyword, and you utilize whatever 'common name' for a property  while still keeping the definition concise.  For example    ```  propertySchema(""http://example/schema/{{property}}"") {      ""some_silly_uri_prefix#price"" {} alias ""friendlyName""  }  ```    Going even further, you can use the unary plus operator to add a property that will provide no  configuration outside of default values.    ```  propertySchema(""http://example/schema/{{property}}"") {      +""price""  }  ```    *It is worth noting that these two variations may look the same now, but future versions of  this library will likely utilize further customization of a property.  The unary plus operator  will be creating a property with NO CUSTOMIZATION whatsoever, where the former syntax will  allow for a pick/choose type of customization*      #### rdfGraph  The `rdfGraph` DSL is meant to create an RDF graph or model  that can then be queried against.    Here is an example that showcases the complete set of options available for using the rdfGraph DSL:    *Note* this example uses the `propertySchema` DSL     ```  val props =            pSchema(""http://example/props/{{property}}"") {              +""enemies_with""              +""hair_color""              +""leg_count""          }    val model =            rdfGraph {              resources {                  ""dog""(""http://example/dog"")                  ""cat""(""http://example/cat"")                  ""parrot""(""http://example/parrot"")              }              statements {                  ""dog"" {                      props[""enemies_with""] of !""cat""                      props[""hair_color""] of ""golden""                      props[""leg_count""] of 4                  }                  ""cat"" {                      props[""enemies_with""] of !""parrot""                      props[""hair_color""] of ""black""                      props[""leg_count""] of 4                  }                  ""parrot"" {                      props[""leg_count""] of 2                  }              }          }  ```    ...and here is a breakdown, mostly line-by-line, of what is happening...    `rdfGraph {`    This line is establishing the start of the rdfGraph DSL construct    ***    `resources {`    This line is establishing the start of resource definitions.  Resources  defined within this construct are utilized later in statement creation.    ***    ```  ""dog""(""http://example/dog"")  ""cat""(""http://example/cat"")  ""parrot""(""http://example/parrot"")  ```                     These lines are creating resources, which can be referred to by the leading String,  using a short-hand syntax `!""name""`.  This syntax, when used in the `statements` dsl   construct will refer to the actual Resource objects themselves.    ***    `statements {`    This line is establishig the start of statement definitions.  Statements  are created with a Subject-Predicate-Object triple setup, with the Object  portion able to be literals or other Resources.    ***    `""dog"" {` or `""cat"" {` or `""parrot"" {`    These lines are the 'Subject' part of the triple.  A block is started as a  shorthand means of defining multiple predicate/object pairs for each subject.    ***    ```  props[""enemies_with""] of !""cat""  props[""hair_color""] of ""golden""  props[""leg_count""] of 4  ```    These lines show three different predicate/object pairs that will be assigned with  the enclosing subject to create triples.    The first maps the Predicate of name 'enemies_with' from the PropertySchema  to the 'cat' Resource (remember the !""cat"" syntax)    The second maps the Predicate of name 'hair_color' from the PropertySchema to  the string literal 'golden'.    The third maps the Predicate of name 'leg_count' from the PropertySchema to  the integer literal 4.    ##### rdfGraph return type  The `rdfGraph` DSL will return an object of type `org.apache.jena.rdf.model.Model` of the Apache Jena API.    ```  val model =      rdfGraph {          // ...      }        val numStatements = model.size()  ```    ##### Reducing ceremonious syntax  It is possible to simply references to PropertySchema properties.  It requires  providing the PropertySchemas to your `rdfGraph` DSL setup.    ```  val props1: PropertySchema = //...  val props2: PropertySchema = //...    val model =            rdfGraph(props1, props2) {              resources {                  ""dog""(""http://example/dog"")              }              statements {                  ""dog"" {                      ""someProp"" of 4                  }              }          }  ```    When providing more than one PropertySchema, they will be checked for a property  in the order provided, returning the first matched property.  If none are found,  an exception will be thrown.    It is worth pointing out that the String itself is NOT being considered a Property  on its own (unlike how !""name"" is used to refer to a Resource).  This works as is  only when used in the form of `""string"" of [literal/resource]`    ## Questions  1. Why are you doing this? To learn how to make a DSL in Kotlin and to learn more about RDF.  2. Why doesn't this support feature x/y/z?  I am new to RDF, and so my understanding of it is limited.  If you have any requests, please let me know via email, or via the github issue system.  Please note that a feature request is NOT a guarantee that I will implement something."""
Semantic web;https://github.com/linkeddata/rdflib.js;"""# rdflib.js  [![NPM Version](https://img.shields.io/npm/v/rdflib.svg?style=flat)](https://npm.im/rdflib)  [![Join the chat at https://gitter.im/linkeddata/rdflib.js](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/linkeddata/rdflib.js?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    Javascript RDF library for browsers and Node.js.    - Reads and writes RDF/XML, Turtle and N3; Reads RDFa and JSON-LD  - Read/Write Linked Data client, using WebDav or SPARQL/Update  - Real-Time Collaborative editing with web sockets and PATCHes  - Local API for querying a store  - Compatible with [RDFJS task force spec](https://github.com/rdfjs/representation-task-force/blob/master/interface-spec.md)  - SPARQL queries (not full SPARQL - just graph match and optional)  - Smushing of nodes from `owl:sameAs`, and `owl:{f,inverseF}unctionProperty`  - Tracks provenance of triples keeps metadata (in RDF) from HTTP accesses    ## Documentation    See:    * The [API documentation](https://linkeddata.github.io/rdflib.js/doc/) is partial but useful  * [Tutorial: Using rdflib in a Solid web app](https://linkeddata.github.io/rdflib.js/Documentation/webapp-intro.html)  * [Tutorial: Using rdflib.js](https://github.com/solid/solid-tutorial-rdflib.js)  * [Tutorial: Using Turtle](https://linkeddata.github.io/rdflib.js/Documentation/turtle-intro.html)  * [Using authenticated & alternate fetch methods](https://linkeddata.github.io/rdflib.js/Documentation/alternate-fetches.md)  * [Block diagram: rdflib modules](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/rdflib-block-diagram.svg)  * [Block diagram: The Fetcher](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/fetcher-block-diagram.svg)  * [Block diagram: The Fetcher - handling retries](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/fetcher-block-diagram-2.svg)  * [Block diagram: The Update Manager](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/update-manager-diagram.svg)      * [The Solid developer portal at Inrupt](https://solid.inrupt.com/)    for more information.    ## Install    #### Browser (using a bundler like Webpack)    ```bash  npm install rdflib  ```    #### Browser (generating a `<script>` file to include)    ```bash  git clone git@github.com:linkeddata/rdflib.js.git;  cd rdflib.js;  npm install;  ```    Generate the dist directory    ```bash  npm run build:browser  ```    #### Node.js    Make sure you have Node.js and Node Package Manager ([npm](https://npmjs.org/))  installed.    ```bash  npm install --save rdflib  ```    ## Contribute    #### Subdirectories    - `dist`: Where the bundled libraries are built. Run `npm run build` to generate them.  - `test`: Tests are here.  - `lib`: Transpiled, non-bundled library is built here when the library is    published to npm.    #### Dependencies        - XMLHTTPRequest (Node.js version)    ## Thanks    Thanks to the many contributors who have been involved along the way.  LinkedData team & TimBL    ## LICENSE  MIT """
Semantic web;https://github.com/ontola/ontologies;"""# Ontologies  Never manage a namespace object map again, scrap typo's for well-known ontologies. Like DefinitelyTyped, but for ontologies.    ## Usage    ### @ontologies/core  When working with RDF (linked data) ontologies are very important, but being able to quickly create  and work with the fundamental building blocks of RDF is equally important. `@ontologies/core` exports  an object (_data factory_) which aims to to just that.    Assuming the following import in the examples:    ```javascript   import rdf from ""@ontologies/core""  ```    #### Create literals  ```javascript  // Strings  rdf.literal(""Hello world!"")   // { termType: ""Literal"", value: ""Hello World!"", datatype: { termType: ""NamedNode"", value: ""http://www.w3.org/2001/XMLSchema#string"" } }  ```    ```javascript  // Numbers  rdf.literal(9001)   // { termType: ""Literal"", value: ""9001"", datatype: { termType: ""NamedNode"", value: ""http://www.w3.org/2001/XMLSchema#integer"" } }  ```    Most JS literals will be mapped to their RDF (xsd) counterparts, passing the datatype explicitly is  also possible. Please note that data types in RDF must be IRIs.    ```javascript  rdf.literal(""(5,2)"", rdf.namedNode(""http://example.com/types/myCoordinate""))   // { termType: ""Literal"", value: ""(5,2)"", datatype: { termType: ""NamedNode"", value: ""http://example.com/types/myCoordinate"" } }  ```    #### Create links  Use `namedNode` to create links to other resources which have been _named_, meaning an authority has  given the resource a fixed identifier on their domain. This can be a resource on the web (e.g.   `http:`, `https:`) but also in the internet (e.g. `ftp:` or `magnet:`) or elsewhere (e.g. `urn:isbn:`  or `doi:`). Note that choosing schemes which are widely deployed (e.g. `https:`) will allow others   easier access to find, access, and share your data.     ```javascript  rdf.namedNode(""https://schema.org/Thing"")   // { termType: ""NamedNode"", value: ""https://schema.org/Thing"" }    rdf.namedNode(""https://example.com/myDocument#paragraph"")   // { termType: ""NamedNode"", value: ""https://example.com/myDocument#paragraph"" }    rdf.namedNode(""urn:isbn:978-0-201-61622-4"")   // { termType: ""NamedNode"", value: ""urn:isbn:978-0-201-61622-4"" }  ```    [Cool iris don't change](https://www.w3.org/Provider/Style/URI), but designing systems is difficult,  so there will be resources which don't have their own name or are too expensive to assign a name.  This is where blank nodes can be used, these are resources as well, but with _""an identifier yet to  be assigned""_. This is a bit of a tricky way of saying ""it thing has a name, but I don't know it yet"".    ```javascript  // Create blank nodes (links which have no place in the web yet)  rdf.blankNode()   // { termType: ""BlankNode"", value: ""b0"" }    rdf.blankNode(""fixed"")   // { termType: ""BlankNode"", value: ""fixed"" }  ```    Note that most of the time blank nodes can be replaced with fragment (`#`) IRIs without much trouble.    ```json5  {    ""@id"": ""http://example.com/myCollection"",    ""members"": {      ""@id"": ""http://example.com/myCollection#members"", // Append `#members` to the base rather than use a blank node      ""@type"": ""http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq"",      ""http://www.w3.org/1999/02/22-rdf-syntax-ns#_0"": ""First item"",      ""http://www.w3.org/1999/02/22-rdf-syntax-ns#_1"": ""Second item"",      ""http://www.w3.org/1999/02/22-rdf-syntax-ns#_2"": ""Third item"",    }  }  ```     #### Create statements  ```javascript  // Create quads (Statements about some thing in the world)  rdf.quad(s, p, o)   // { subject: <s>, predicate: <p>, object: <o>, graph: <defaultGraph> }  ```    ```javascript  // Compare rdf objects  import { Thing } from ""@ontologies/schema"";  console.log(rdf.equals(rdf.namedNode(""https://schema.org/Thing""), Thing))   // true      // Serialize to n-triples/quads  console.log(rdf.toNQ())  ```    #### Other exports  Overview of the exports of @ontologies/core;    * A default export which is a proxy to the currently assigned global [Data Factory](http://rdf.js.org/data-model-spec/#datafactory-interface).  * A named export `globalSymbol` which is a symbol to identify the Data Factory  used by the other @ontologies/ packages to create rdf objects with.  * A named export `setup` which binds the PlainFactory to the global scope under the globalSymbol      identifier if it was previously undefined.  * A named export `globalFactory` which should be a reference to the last .  * A named export `PlainFactory` which implements the Data Factory interface (with slight adjustments)     in a functional way (e.g. no instance methods, but provides an `equals` function on the factory itself).   * A named export `createNS` which you can use to create namespaces with which ease NamedNode      creation using the global factory.  * A small set of types useful for working with RDF.    ### @ontologies/*  The other packages are generated from their respective ontologies, providing client applications with  importable symbols and a named export `ns` with which custom additional terms can be created within  the given namespace.    ```javascript  import { name } from '@ontologies/schema'    console.log(name) // http://schema.org/name  ```    All terms    ```javascript  import * as schema from '@ontologies/schema'    console.log(schema.name) // http://schema.org/name  ```    Custom terms    ```javascript  import { ns } from '@ontologies/schema'    console.log(ns('extension')) // http://schema.org/extension  ```    Use `.value` for the string representation    ```javascript  import { name } from '@ontologies/schema'    console.log(name.value) // ""http://schema.org/name""  ```    ### Overriding the default factory  The default factory used is the `PlainFactory` from this package. This is a factory which should  suffice most needs, but certain JS RDF libraries expect more methods to be available on the factory.  It is possible to override the factory with a custom one.    Initialize a custom factory by calling `setup()` from `@ontologies/core`    ```javascript  import { setup } from ""@ontologies/core"";  import myFactory from ""./myFactory"";    setup(new myFactory);  ```    Library authors who want to provide an alternate default than the PlainFactory but who don't want to  override the end-users setting can soft-override the factory;    ```javascript  import { setup } from ""@ontologies/core"";  import LibFactory from ""./LibFactory"";    setup(new LibFactory(), false); // Passing false will override the default but not a user-set factory.  ```     ### Help, my factory isn't loaded!  Chances are you have called `setup` too late in the module initialization cycle. Be sure to;  1. Move the setup call in a _separate_ file.  2. Don't import _any_ package using the default export from `@ontologies/core` in that file.  3. **Import that file before any other import** which uses default export from `@ontologies/core` in that file.    ### Non-js symbols  Dashes in term names are replaced with underscores. The default export contains both the verbatim  and the underscored values.    ```javascript  import dcterms, { ISO639_2 } from '@ontologies'    console.log(ISO639_2) // NamedNode(http://purl.org/dc/terms/ISO639-2)  console.log(dcterms.ISO639_2) // NamedNode(http://purl.org/dc/terms/ISO639-2)  console.log(dcterms['ISO639-2']) // NamedNode(http://purl.org/dc/terms/ISO639-2)  ```    ### Collisions with ES reserved keywords  If a term collides with an [ES 5/6 reserved keyword](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Lexical_grammar#Keywords)  or certain built-in classes, the term is prepended with the symbol of the ontology:    ```javascript  import { name, schemayield } from '@ontologies/schema'    // 'name' is not a JS reserved keyword  console.log(name.value) // ""http://schema.org/name""  // 'yield' is a reserved keyword, so the package name is prepended to the js identifier.  console.log(schemayield.value) // ""http://schema.org/yield""  ``` """
Semantic web;https://github.com/nick-manasys/zeppelin-sparql;"""# zeppelin-sparql  Zeppelin sparql interpreter """
Semantic web;https://github.com/uzh/triplerush;"""TripleRush: A Distributed In-Memory Graph Store  ===============================================    TripleRush is a distributed in-memory graph store that supports SPARQL select queries. Its [architecture](http://www.zora.uzh.ch/111243/1/TR_WWW.pdf) is designed to take full advantage of cluster resources by distributing and parallelizing the query processing.    How to develop in Eclipse  -------------------------  Install the [Typesafe IDE for Scala 2.11](http://scala-ide.org/download/sdk.html).    Ensure that Eclipse uses a Java 8 library and JVM: Preferences → Java → Installed JREs → JRE/JDK 8 should be installed and selected.    Import the project into Eclipse: File → Import... → Maven → Existing Maven Projects → select ""triplerush"" folder    Thanks a lot to  ---------------  * [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Hasler Foundation](http://www.haslerstiftung.ch/en/home) have generously funded the research on graph processing.  * GitHub helps us by hosting our [code repositories](https://github.com/uzh/triplerush).  * Travis.CI offers us very convenient [continuous integration](https://travis-ci.org/uzh/triplerush).  * Codacy gives us automated [code reviews](https://www.codacy.com/public/uzh/triplerush). """
Semantic web;https://github.com/streamreasoning/CSPARQL-engine;"""CSPARQL-engine  ==============    Reasoning over RDF stream made easy.  The project contains parent pom in the root and a number of module that inherit from parent pom.  To install the csparql-core jar, run mvn install on parent pom. """
Semantic web;https://github.com/dkmfbk/rdfpro;"""RDFpro: an extensible tool for building stream-oriented RDF processing pipelines  ================================================================================    RDFpro (RDF Processor) is a public domain (Creative Commons CC0) Java command line tool and embeddable library that offers a suite of stream-oriented, highly optimized processors for common tasks such as data filtering, RDFS inference, smushing and statistics extraction.  RDFpro processors are extensible by users and can be freely composed to form complex pipelines to efficiently process RDF data in one or more passes.  RDFpro model and multi-threaded design allow processing billions of triples in few hours in typical Linked Open Data integration scenarios.    [RDFpro Web site](http://rdfpro.fbk.eu/)    Building RDFpro  ---------------  To build RDFpro, you need to have [`Maven`](https://maven.apache.org/) installed on your machine.   In order to build RDFpro, you can run the following commands:        $ git clone https://github.com/dkmfbk/rdfpro.git  (1)      $ cd rdfpro                                       (2)      $ git checkout BRANCH_NAME                        (3)      $ mvn package -DskipTests -Prelease               (4)    Step (3) is optional, if you want to build a specific branch, otherwise the version on top of the `master` branch will be built.      The `-DskipTests` flag in step (4) disable unit testing to speed up the building process: if you want to run the tests, just omit the flag. The `-Prelease` flag activates a Maven profile called ""release"" that enables the generation of the same `tar.gz` archive including everything that we distribute as RDFpro binaries on the website. This `tar.gz` is located under:        rdfpro-dist/target/rdfpro-dist-VERSION-bin.tar.gz      You may copy it wherever you want, extract it and run rdfpro via the included `rdfpro` script."""
Semantic web;https://github.com/protegeproject/snap-sparql-query;"""# snap-sparql-api  An API for parsing SPARQL queries """
Semantic web;https://github.com/awslabs/amazon-neptune-tools;"""## Amazon Neptune Tools    Utilities to enable loading data and building graph applications with Amazon Neptune.    ### Examples    You may also be interested in the [Neptune Samples github repository](https://github.com/aws-samples/amazon-neptune-samples), which includes samples and example code.    ### GraphML 2 CSV  This is a [utility](graphml2csv/README.md) to convert graphml files into the Neptune CSV format.    ### Neptune Export  Exports Amazon Neptune data to CSV for Property Graph or Turtle for RDF graphs.    You can use [neptune-export](neptune-export/) to export an Amazon Neptune database to the bulk load CSV format used by the Amazon Neptune bulk loader for Property Graph or Turtle for RDF graphs. Alternatively, you can supply your own queries to neptune-export and unload the results to CSV or Turtle.    ### Export Neptune to Elasticsearch  Backfills Elasticsearch with data from an existing Amazon Neptune database.    The [Neptune Full-text Search](https://docs.aws.amazon.com/neptune/latest/userguide/full-text-search-cfn-create.html) CloudFormation templates provide a mechanism for indexing all _new_ data that is added to an Amazon Neptune database in Elasticsearch. However, there are situations in which you may want to index _existing_ data in a Neptune database prior to enabling the full-text search integration.    You can use this [export Neptune to Elasticsearch solution](export-neptune-to-elasticsearch/) to index existing data in an Amazon Neptune database in Elasticsearch.    ### Neo4j to Neptune  A [command-line utility](neo4j-to-neptune/readme.md) for migrating data to Neptune from Neo4j.    ### Glue Neptune    [glue-neptune](glue-neptune/) is a Python library for AWS Glue that helps writing data to Amazon Neptune from Glue jobs. With glue-neptune you can:  * Get Neptune connection information from the Glue Data Catalog  * Create label and node and edge ID columns in DynamicFrames, named in accordance with the Neptune CSV bulk load format for property graphs  * Write from DynamicFrames directly to Neptune    ### Neptune CSV to RDF    If you're interested in converting Neptune's CSV format to RDF, see [amazon-neptune-csv-to-rdf-converter](https://github.com/aws/amazon-neptune-csv-to-rdf-converter).    ### Neptune CSV to Gremlin    [csv-gremlin](csv-gremlin/README.md) is a tool that can turn Amazon Neptune format CSV files into Gremlin steps allowing them to be loaded into different Apache TinkerPop compliant stores (including Amazon Neptune) using Gremlin queries. The tool also tries to validate that the CSV files do not contain errors and can be use to inspect CSV files prior to starting a bulk load.    ### CSV to Neptune Bulk Format CSV    [csv-to-neptune-bulk-format](csv-to-neptune-bulk-format/README.md) is a utility to identify nodes and edges in the source CSV data file(s) and generate the Amazon Neptune gremlin load data format files. A configuration file (JSON) defines the source and target files, nodes/edges definition, and selection logic. The script interprets one or more configuration files and generates Amazon Neptune gremlin load data format files. The generated files can be loaded into the Neptune database.    ## License    This library is licensed under the Apache 2.0 License.  """
Semantic web;https://github.com/baskaufs/guid-o-matic;"""[jump to a detailed explanation](use.md)    [jump straight to a simple try-it-yourself-example](getting-started.md)    [jump to the Darwin Core translator page](dwca-converter.md)    # Guid-O-Matic  Software to convert fielded text (CSV) files to RDF serialized as XML, Turtle, or JSON-LD    ![](https://raw.githubusercontent.com/baskaufs/guid-o-matic/master/squid.bmp)  ![](https://raw.githubusercontent.com/baskaufs/guid-o-matic/master/images/translation.png)    ""You can write better software than this...""    **Note: 2018-08-29. There is a problem with JSON-LD generated by Guid-O-Matic.  If a property is repeated, Guid-O-Matic creates two triples - repeating the property with the value each time.  This is valide JSON-LD, but consuming applications only recognize the last instance of the property for that subject.  The appropriate syntax is to include the multiple values as an array of a single property.  So this is only a problem when a subject had duplicate predicates.**    ## What is the purpose of Guid-O-Matic ?  Best Practices in the biodiversity informatics community, as embodied in the [TDWG GUID Applicability Statement](https://github.com/tdwg/guid-as) dictate that globally unique identifiers (GUIDs, rhymes with ""squids"") should be resolvable (i.e. dereferenceable, Recommendation R7) and that the default metadata response format should be RDF serialized as XML (Recommendation R10).  In practice, machine-readable metadata is rarely provided when the requested content-type is some flavor of RDF. I think the reason is because people think it is ""too hard"" to generate the necessary RDF.      The purpose of Guid-O-Matic is mostly to show that it is not really that hard to create RDF.  Anybody who can create a spreadsheet or a [Darwin Core Archive (DwCa)](http://www.gbif.org/resource/80636) can generate RDF with little additional effort.  In production, providers would probably not use spreadsheets as a data source, but the point of Guid-O-Matic is to demonstrate a general strategy and allow users to experiment with different graph structures and play with the generated serializations.    ## Why is it called ""Guid-O-Matic"" and not something like ""RDF-Generator-O-Matic""?  Because I already had the cute squid picture and ""RDF Generator"" doesn't rhyme with ""squid"".    ## Why did you write this script in XQuery and not something like Python or PHP?  I am not a very good Python programmer and I don't know PHP.  Once you understand what Guid-O-Matic does, you can write your own (better) code to do the same thing.    I used XQuery because I'm in a working group that includes a lot of Digital Humanists, and they love XML.  Also, the awesome XQuery processor, BaseX, is free and easily downloaded and installed.  So anybody can easily run the Guid-O-Matic scripts.  In addition, BaseX can run as a web server, so in theory, one could call the RDF-generating functions in response to a HTTP request and actually use the scripts to provide RDF online.    ## What did Guid-O-Matic 1.1 do?  I wrote Guid-O-Matic 1 in about 2010.  Version 1.1 had a very limited scope:  - it only generated RDF/XML  - it assumed that the focal resource was a specimen  - it assumed that the specimen was depicted by one image  - it was hard-coded to use a specific version of the [Darwin Core](http://rs.tdwg.org/dwc/terms/) and [Darwin-SW](https://github.com/darwin-sw/dsw) vocabularies  - other stuff that I can't remember    Version 1.1 also was written in an old version of Visual Basic, which had the advantage that it could run as an executable, but had the disadvantage that you couldn't hack it unless you had a copy of Visual Basic and knew how to use it.  Even I don't have a functioning copy of that version of Visual Basic any more, so I can't even look at the source code now.  But it doesn't really matter because I don't advise that anyone try to mess with version 1.1 anyway.  I'm only posting it here for historical reasons (and so that you can try running it to see the great squid graphic on the UI!).    ## What does Guid-O-Matic 2 do?  Version 2 is intended to be as general as is practical considering that the source data are being pulled from a CSV file.  It:  - provides RDF output in XML, Turtle, or JSON-LD  - allows the focal resource to be of any class  - allows the use of any RDF vocabularies  - accepts input from any ""flat"" delimited text file (i.e. a CSV file)  - allows the user to set up all of the defaults and CSV-to-RDF mappings via CSV files that can be edited in Excel or any other typical spreadsheet application.  - allows linking of any number of classes whose instances have a one-to-one relationship with the focal class.  - allows linking of any number of classes whose instances have a many-to-one relationship with the focal class (i.e. a ""star schema""). This includes [Darwin Core Archive (DwCa)](http://www.gbif.org/resource/80636) files.  - output can be onscreen or to a file.  - output can be a single record or a dump of the entire database.  - the main script is included in an XQuery module so that it could potentially be called from the [BaseX Web Application](http://docs.basex.org/wiki/Web_Application) and therefore be used to actually dereference IRIs.  (In that case, the code would probably be hacked to pull the data from an XML database rather than from the CSV files.)    Version 2 is written in [XQuery (a W3C Recommendation)](https://www.w3.org/TR/xquery/).  It can be run using [BaseX](http://basex.org/), a free XQuery processor.  Instructions for setting everything up are elsewhere.    In addition to the main script that generates the RDF, there is an additional script that processes a Darwin Core Archive so that it can be used as source data.  It pulls information from the meta.xml file to generate hackable mappings from the CSV files to the RDF.      ## Can I try it?  Yes, please do!  If all you want to do is see what happens, do the following:  - fork the Guid-O-Matic GitHub repo (https://github.com/baskaufs/guid-o-matic) to your local drive.  Where you clone it on your hard drive has implications for the software finding the necessary files.  Read below before you actually do the cloning.  - install [BaseX](http://basex.org/products/download/all-downloads/) (if you haven't already)  - use the Open (file folder) button in BaseX to navigate to the downloaded folder for the repo and load the query test-serialize.xq into BaseX.  - if you downloaded the repo to the default location and are using a Mac (and probably any Linux system), you shouldn't have to do anything for the script to find the necessary files.  If you are running a PC, or if you are using a non-PC with the files located at some non-default location, you need to set the path to the guid-o-matic repo directory as the fourth parameter of the function.  The default repo-cloning location on PCs is in some horrible place inside the default user directory. However, when cloning the repo, you can specify some simpler location for the repo.  I use c:\github\, which is the default given in the function as it is downloaded.  - Click the ""Run Query"" (""play"" triangle) button.  The example data are Chinese religious sites and buildings.*  You can see the [graph model here](graph-model.md).    You can play around with changing the identifier for the focal resource (the first parameter of the function) to generate RDF for other temple sites, and the serialization (the second parameter).  Suggested values are given in the comments above the function.       If you want to try more complicated things like changing the properties or graph model, or if you want to set up mappings for your own data, you will need to read [more detailed instructions](use.md).  To take it a step further and try using a Darwin Core archive as input also requires [reading more instructions](dwca-converter.md).    \* Tang-Song temple data provided by [Dr. Tracy Miller](http://as.vanderbilt.edu/historyart/people/miller.php) of the Vanderbilt University Department of History of Art, who graciously let us use her data as a guinea pig in our Semantic Web working group.  Please contact her for more information about the data. """
Semantic web;https://github.com/mdesalvo/RDFSharp.Semantics;"""This project is now part of <a href=""https://github.com/mdesalvo/RDFSharp"">RDFSharp</a>! """
Semantic web;https://github.com/innoq/iqvoc;"""# iQvoc    [![Gem Version](https://badge.fury.io/rb/iqvoc.png)](http://badge.fury.io/rb/iqvoc)  ![CI](https://github.com/innoq/iqvoc/workflows/CI/badge.svg?branch=master)  [![Code Climate](https://codeclimate.com/github/innoq/iqvoc.png)](https://codeclimate.com/github/innoq/iqvoc)    iQvoc is a vocabulary management tool that combines easy-to-use human interfaces  with Semantic Web interoperability.    iQvoc supports vocabularies that are common to many knowledge organization  systems, such as:    * Thesauri  * Taxonomies  * Classification schemes  * Subject heading systems    iQvoc provides comprehensive functionality for all aspects of managing such  vocabularies:    * import of existing vocabularies from a SKOS representation  * multilingual display and navigation in any Web browser  * editorial features for registered users  * publishing the vocabulary in the Semantic Web    iQvoc is built with state-of-the-art technology and can be easily customized according to user's needs.    ## Demo    You can try out iQvoc right now! In our [sandbox](http://try.iqvoc.net/) you can play around with the public views.  If you like to test the collaborative functions simply [request](mailto:iqvoc@innoq.com) your personal demo account.    ## Setup    ### Heroku    You can easily setup your iQvoc instance in under 5 minutes, we wanted to make  this process really easy. In order to deploy to heroku you need to have an  account and [heroku toolbelt](https://toolbelt.heroku.com) installed.    ```  $ bundle install  $ heroku create  $ heroku config:add HEROKU=true RAILS_ENV=heroku RACK_ENV=heroku SECRET_KEY_BASE=$(bundle exec rake secret)  $ git push heroku master  $ heroku run rake db:migrate  $ heroku run rake db:seed  $ heroku restart  ```    `heroku open` opens your app in the browser.    Remember to visit the Users section and change the default passwords!    ### Docker    If you want to try iQvoc using Docker just clone this repository and run:    ```  $ docker-compose up  ```    This Setup uses Postgres as a database. Please make sure that your Docker daemon is running and that you have docker-compose installed. User credentials can be found in https://github.com/innoq/iqvoc/blob/master/db/seeds.rb.    ### Custom    We recommend running [iQvoc as a Rails engine](https://github.com/innoq/iqvoc/wiki/iQvoc-as-a-Rails-Engine).  Running the cloned source code is possible but any modifications would require a  fork.    1. Configure your database via `config/database.template.yml`.     Don't forget to rename it to `database.yml`  2. Run `bundle install`  3. Run `bundle exec rake db:create` to create the database  4. Create the necessary tables by running `rake db:migrate`  5. Load some base data by running `rake db:seed`  6. Make sure you have got `config/secrets.yml` in place  7. Install nodejs dependencies for compiling assets: `npm install` (make sure nodejs is installed)  8. Compile assets using: `npm run compile` (or `npm run watch` to compile and listen for changes in development)  8. Boot up the app using `bundle exec rails s` (or `passenger start` if you use passenger)  9. Log in with ""admin@iqvoc"" / ""admin123"" or ""demo@iqvoc"" / ""cooluri123"" (cf. step #5)  10. Visit the Users section and change the default passwords    ## Background Jobs    Note that some features like ""Import"" and ""Export"" exposed in the Web UI store  their workload as jobs. You can either issue a job worker that runs continuously  and watches for new jobs via    ```  $ rake jobs:work  ```    or process jobs in a one-off way (in development or via cron):    ```  $ rake jobs:workoff  ```    ## Compatibility    iQvoc is fully compatible with Ruby 2.6.    ## Customization    There are many hooks providing support for your own classes and configuration.  The core app also works as a Rails Engine. The config residing in `lib/iqvoc.rb`  provides a basic overview of the possibilities.    ## Documentation    Documentation resources can be found in the [wiki](https://github.com/innoq/iqvoc/wiki).    iQvoc provides an (inline) API documentation which can be found on `APP_URI/apidoc`. Check out our sandbox to see it in action: http://try.iqvoc.net/apidoc/    ## Related projects    We provide several extensions to add additional features to iQvoc:    * [iqvoc_skosxl](https://github.com/innoq/iqvoc_skosxl): SKOS-XL extension for iQvoc  * [iqvoc_compound_forms](https://github.com/innoq/iqvoc_compound_forms): Compound labels for iQvoc  * [iqvoc_inflectionals](https://github.com/innoq/iqvoc_inflectionals): Inflectionals for iQvoc  * [iqvoc_similar_terms](https://github.com/innoq/iqvoc_similar_terms):  iQvoc engine for similar terms    ## Versioning    Releases will follow a semantic versioning format:        <major>.<minor>.<patch>    For more information on SemVer, visit http://semver.org/.    ## Contributing    If you want to help out there are several options:    - Found a bug? Just create an issue on the    [GitHub Issue tracker](https://github.com/innoq/iqvoc/issues) and/or submit a    patch by initiating a pull request  - You're welcome to fix bugs listed under    [Issues](https://github.com/innoq/iqvoc/issues)  - Proposal, discussion and implementation of new features on our mailing list    [iqvoc@lists.innoq.com] or on the issue tracker    If you make changes to existing code please make sure that the test suite stays  green. Please include tests to your additional contributions.    Tests can be run via `bundle exec rake test`. We're using Cuprite for  integration tests with JavaScript support.    ## Maintainer & Contributors    iQvoc was originally created and is being maintained by [innoQ Deutschland GmbH](http://innoq.com).    * Robert Glaser ([mrreynolds](http://github.com/mrreynolds))  * Till Schulte-Coerne ([tillsc](http://github.com/tillsc))  * Frederik Dohr ([FND](http://github.com/FND))  * Marc Jansing ([mjansing](http://github.com/mjansing))    ## License    Copyright 2022 [innoQ Deutschland GmbH](https://www.innoq.com).    Licensed under the Apache License, Version 2.0. """
Semantic web;https://github.com/AtomGraph/CSV2RDF;"""# CSV2RDF  Streaming, transforming CSV to RDF converter    Reads CSV/TSV data as generic CSV/RDF, transforms each row using SPARQL `CONSTRUCT` or `DESCRIBE`, and streams the output triples.  The generic CSV/RDF format is based on the minimal mode of [Generating RDF from Tabular Data on the Web](https://www.w3.org/TR/2015/REC-csv2rdf-20151217/#dfn-minimal-mode).    Such transformation-based approach enables:  * building resource URIs on the fly  * fixing/remapping datatypes  * mapping different groups of values to different RDF structures    CSV2RDF differs from [tarql](https://tarql.github.io) in the way how mapping queries use graph patterns in the `WHERE` clause. tarql queries operate on a table of bindings  (provided as an implicit `VALUES` block) in which CSV column names become variable names. CSV2RDF generates an intermediary RDF graph for each CSV row (using column names as relative-URI properties)  that the `WHERE` patterns explicitly match against.    Build  -----        mvn clean install    That should produce an executable JAR file `target/csv2rdf-2.0.0-jar-with-dependencies.jar` in which dependency libraries will be included.    Usage  -----    The CSV data is read from `stdin`, the resulting RDF data is written to `stdout`.    CSV2RDF is available as a `.jar` as well as a Docker image [atomgraph/csv2rdf](https://hub.docker.com/r/atomgraph/csv2rdf) (recommended).    Parameters:  * `query-file` - a text file with SPARQL 1.1 [`CONSTRUCT`](https://www.w3.org/TR/sparql11-query/#construct) query string  * `base` - the base URI for the data (also becomes the `BASE` URI of the SPARQL query). Property namespace is constructed by adding `#` to the base URI.    Options:  * `-d`, `--delimiter` - value delimiter character, by default `,`.  * `--max-chars-per-column` - max characters per column value, by default 4096  * `--input-charset` - CSV input encoding, by default UTF-8  * `--output-charset` - RDF output encoding, by default UTF-8    _Note that delimiters might have a [special meaning](https://www.tldp.org/LDP/abs/html/special-chars.html) in shell._ Therefore, always enclose them in single quotes, e.g. `';'` when executing CSV2RDF from shell.    If you want to retrieve the raw CSV/RDF output, use the [identity transform](https://en.wikipedia.org/wiki/Identity_transform) query `CONSTRUCT WHERE { ?s ?p ?o }`.    Example  -------    CSV data in `parking-facilities.csv`:            postDistrict,roadCode,houseNumber,name,FID,long,lat,address,postcode,parkingSpace,owner,parkingType,information      1304 København K,24,5,Adelgade 5 p_hus.0,p_hus.0,12.58228733,55.68268042,Adelgade 5,1304,92,Privat,P-Kælder,""Adelgade 5-7, Q-park.""    `CONSTRUCT` query in `parking-facilities.rq`:    ```sparql  PREFIX schema:     <https://schema.org/>   PREFIX geo:        <http://www.w3.org/2003/01/geo/wgs84_pos#>   PREFIX xsd:        <http://www.w3.org/2001/XMLSchema#>   PREFIX rdf:        <http://www.w3.org/1999/02/22-rdf-syntax-ns#>    CONSTRUCT  {      ?parking a schema:ParkingFacility ;          geo:lat ?lat ;          geo:long ?long ;          schema:name ?name ;          schema:streetAddress ?address ;          schema:postalCode ?postcode ;          schema:maximumAttendeeCapacity ?spaces ;          schema:additionalProperty ?parkingType ;          schema:comment ?information ;          schema:identifier ?id .  }  WHERE  {      ?parkingRow <#FID> ?id ;          <#name> ?name ;          <#address> ?address ;          <#lat> ?lat_string ;          <#postcode> ?postcode ;          <#parkingSpace> ?spaces_string ;          <#parkingType> ?parkingType ;          <#information> ?information ;          <#long> ?long_string .         BIND(URI(CONCAT(STR(<>), ?id)) AS ?parking) # building URI from base URI and ID      BIND(xsd:integer(?spaces_string) AS ?spaces)      BIND(xsd:float(?lat_string) AS ?lat)      BIND(xsd:float(?long_string) AS ?long)  }  ```  Java execution from shell:        cat parking-facilities.csv | java -jar csv2rdf-2.0.0-jar-with-dependencies.jar parking-facilities.rq https://localhost/ > parking-facilities.ttl    Alternatively, Docker execution from shell:        cat parking-facilities.csv | docker run -i -a stdin -a stdout -a stderr -v ""$(pwd)/parking-facilities.rq"":/tmp/parking-facilities.rq atomgraph/csv2rdf /tmp/parking-facilities.rq https://localhost/ > parking-facilities.ttl    Note that using Docker you need to:  * [bind](https://docs.docker.com/engine/reference/commandline/run/#attach-to-stdinstdoutstderr--a) `stdin`/`stdout`/`stderr` streams  * [mount](https://docs.docker.com/storage/volumes/) the query file to the container, and use the filepath from _within the container_ as `query-file`    Output in `parking-facilities.ttl`:        <https://localhost/p_hus.0> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://schema.org/ParkingFacility> .      <https://localhost/p_hus.0> <http://www.w3.org/2003/01/geo/wgs84_pos#long> ""12.58228733""^^<http://www.w3.org/2001/XMLSchema#float> .      <https://localhost/p_hus.0> <https://schema.org/identifier> ""p_hus.0"" .      <https://localhost/p_hus.0> <https://schema.org/additionalProperty> ""P-Kælder"" .      <https://localhost/p_hus.0> <https://schema.org/comment> ""Adelgade 5-7, Q-park."" .      <https://localhost/p_hus.0> <https://schema.org/postalCode> ""1304"" .      <https://localhost/p_hus.0> <http://www.w3.org/2003/01/geo/wgs84_pos#lat> ""55.68268042""^^<http://www.w3.org/2001/XMLSchema#float> .      <https://localhost/p_hus.0> <https://schema.org/streetAddress> ""Adelgade 5"" .      <https://localhost/p_hus.0> <https://schema.org/name> ""Adelgade 5 p_hus.0"" .      <https://localhost/p_hus.0> <https://schema.org/maximumAttendeeCapacity> ""92""^^<http://www.w3.org/2001/XMLSchema#integer> .    Query examples  --------------    More mapping query examples can be found under [LinkedDataHub](https://github.com/AtomGraph/LinkedDataHub)'s [`city-graph`](https://github.com/AtomGraph/LinkedDataHub-Apps/tree/master/demo/city-graph/queries) demo app.    Performance  -----------    Largest dataset tested so far: 2.8 GB / 3709725 rows of CSV to 21.7 GB / 151348939 triples in under 27 minutes. Hardware: x64 Windows 10 PC with Intel Core i5-7200U 2.5 GHz CPU and 16 GB RAM.    Dependencies  ------------    * [Apache Jena](https://jena.apache.org/)  * [uniVocity-parsers](https://www.univocity.com/pages/univocity_parsers_tutorial)  * [picocli](https://picocli.info)"""
Semantic web;https://github.com/SDM-TIB/SDM-RDFizer;"""# SDM-RDFizer  [![License](https://img.shields.io/pypi/l/rdfizer.svg)](https://github.com/SDM-TIB/SDM-RDFizer/blob/master/LICENSE)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6225573.svg)](https://doi.org/10.5281/zenodo.6225573)  [![Latest PyPI version](https://img.shields.io/pypi/v/rdfizer?style=flat)](https://pypi.org/project/rdfizer/)  [![Python Version](https://img.shields.io/pypi/pyversions/rdfizer.svg)](https://pypi.org/project/rdfizer/)  [![PyPI status](https://img.shields.io:/pypi/status/rdfizer?)](https://pypi.org/project/rdfizer/)    This project presents the SDM-RDFizer, an interpreter of mapping rules that allows the transformation of (un)structured data into RDF knowledge graphs. The current version of the SDM-RDFizer assumes mapping rules are defined in the [RDF Mapping Language (RML) by Dimou et al](https://rml.io/specs/rml/). The SDM-RDFizer implements optimized data structures and relational algebra operators that enable an efficient execution of RML triple maps even in the presence of Big data. SDM-RDFizer is able to process data from heterogeneous data sources (CSV, JSON, RDB, XML) processing each set of RML rules (TriplesMap) in a multi-thread safe procedure.    ![SDM-RDFizer workflow](https://raw.githubusercontent.com/SDM-TIB/SDM-RDFizer/beta/images/architecture.png ""SDM-RDFizer workflow"")    # The new features presented by SDM-RDFizer version4.0    In version 4.0 of SDM-RDFizer, we have addressed the problem of efficiency in KG creation in terms of memory storage. SDM-RDFizer version4.0 includes a new module called ""TriplesMap Planning"" a.k.a. TMP which defines an optimized evaluation plan for the execution of triples maps. Additionally, version4.0 extends the previously included module (i.e. TriplesMap Execution a.k.a. TME) by introducing a new operator for compressing data stored in the data structures. These new features can be configured using two new parameters added to the configuration file, named ""large_file"" and ""ordered"".     We have performed extensive empirical evaluation on SDM-RDFizer version4.0 in terms of execution time and memory usage. The experiments are set up to empirically compare the impact of data duplicate rates, data size, and the complexity and the execution order of the triples maps on two versions of SDM-RDFizer (i.e. version4.0 and version3.6) and other exisiting engines icluding [RMLMapper v4.7](https://github.com/RMLio/rmlmapper-java) and [RocketRML](https://github.com/semantifyit/RocketRML) ), in terms of execution time and memory usage. The experiments are performed on two different benchmarks:   - From [SDM-Genomic-datasets](https://figshare.com/articles/dataset/SDM-Genomic-Datasets/14838342/1), datasets including 10k, 100k, and 1M records with 25% and 75% duplicates rates, over six mapping rules with different complexities (1/4 simple object map, 2/5 object reference maps, 2/5 object join maps)  - From [GTFS-Madrid](https://github.com/oeg-upm/gtfs-bench), datasets with scale values of 1-csv, 5-csv, 10-csv, and 50-csv, over two different mapping rules (72 simple object maps and 11 object join maps).     The results of explained experiments can be summarized as the following:  ![Overview of Results (Execution Time Comparison)](https://raw.githubusercontent.com/SDM-TIB/SDM-RDFizer/beta/images/time.png ""Execution Time Comparison"")  As observed in the figures above, both versions of SDM-RDFizer completed all the testbeds successfully while the other two engines have cases of timeout. SDM-RDFizer version3.6 and RocketRML version 1.7.0 are competitve in simple testbeds, however, SDM-RDFizer version4.0 shows the best performance in all the testbeds.   ![Overview of Results (Memory Consumption Comparison)](https://raw.githubusercontent.com/SDM-TIB/SDM-RDFizer/beta/images/memory.png ""Memory Consumption Comparison"")  As illustrated in the figures above, SDM-RDFizer version4.0 has the smallest peak in memory usage compared to the previous version of SDM-RDFizer.        The results of the execution of SDM-RDFizer has been described in the following research reports:    - Enrique Iglesias, Samaneh Jozashoori, David Chaves-Fraga, Diego Collarana, and Maria-Esther Vidal. 2020. SDM-RDFizer: An RML Interpreter for the Efficient Creation of RDF Knowledge Graphs. The 29th ACM International Conference on Information and Knowledge Management (CIKM ’20).    - Samaneh Jozashoori, David Chaves-Fraga, Enrique Iglesias, Oscar Corcho, and Maria-Esther Vidal. 2020. FunMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation. The 19th International Semantic Web Conference - Research Track (ISWC 2020).    - Samaneh Jozashoori and Maria-Esther Vidal. MapSDI: A Scaled-up Semantic Data Integrationframework for Knowledge Graph Creation. The 27th International Conference on Cooperative Information Systems (CoopIS 2019).     - David Chaves-Fraga, Kemele M. Endris, Enrique Iglesias, Oscar Corcho, and Maria-Esther Vidal. What are the Parameters that Affect the Construction of a Knowledge Graph?. The 18th International Conference on Ontologies, DataBases, and Applications of Semantics (ODBASE 2019).    - David Chaves-Fraga, Antón Adolfo, Jhon Toledo, and Oscar Corcho. ONETT: Systematic Knowledge Graph Generation for National Access Points. The 1st International Workshop on Semantics for Transport co-located with SEMANTiCS 2019.    - David Chaves-Fraga, Freddy Priyatna, Andrea Cimmino, Jhon Toledo, Edna Ruckhaus, and Oscar Corcho. GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain. Journal of Web Semantics, 2020.    Additional References:    - Dimou et al. 2014. Dimou, A., Sande, M.V., Colpaert, P., Verborgh, R., Mannens, E., de Walle, R.V.:RML: A generic language for integrated RDF mappings of heterogeneous data. In:Proceedings of the Workshop on Linked Data on the Web co-located with the 23rdInternational World Wide Web Conference (WWW 2014)     # Projects where the SDM-RDFizer has been used    The SDM-RDFizer is used in the creation of the knowledge graphs of EU H2020 projects and national projects where the Scientific Data Management group participates. These projects include:    - iASiS (http://project-iasis.eu/): big data for precision medicine, based on patient data insights. The iASiS RDF knowledge graph comprises more than 1.2B RDF triples collected from more than 40 heterogeneous sources using over 1300 RML triple maps.   - BigMedilytics (https://www.bigmedilytics.eu/): lung cancer pilot. 800 RML triple maps are used to create the lung cancer knowledge graph from around 25 data sources with 500M RDF triples.  - CLARIFY (https://www.clarify2020.eu/): predict poor health status after specific oncological treatments  - P4-LUCAT (https://www.tib.eu/de/forschung-entwicklung/projektuebersicht/projektsteckbrief/p4-lucat)  - ImProVIT (https://www.tib.eu/de/forschung-entwicklung/projektuebersicht/projektsteckbrief/improvit)  - PLATOON (https://platoon-project.eu/)   - EUvsVirus Hackathon (April 2020) (https://blogs.tib.eu/wp/tib/2020/05/06/how-do-knowledge-graphs-contribute-to-understanding-covid-19-related-treatments/). SDM-RDFizer created the Knowledge4COVID-19 knowledge graph during the participation of the team of the Scientific Data Management group. By June 7th, 2020, this KG is comprised of 28M RDF triples describing at a fine-grained level 63527 COVID-19 scientific publications and COVID-19 related concepts (e.g., 5802 substances, 1.2M drug-drug interactions, and 103 molecular disfunctions).     The SDM-RDFizer is also used in EU H2020, EIT-Digital and Spanish national projects where the Ontology Engineering Group (Technical University of Madrid) participates. These projects, mainly focused on the transportation and smart cities domain, include:    - H2020 - SPRINT (http://sprint-transport.eu/): performance and scalability to test a semantic architecture for the Interoperability Framework on Transport across Europe.  - EIT-SNAP (https://www.snap-project.eu/): innovation project on the application of semantic technologies for national access points.  - Open Cities (https://ciudades-abiertas.es/): national project on creating common and shared vocabularies for Spanish Cities  - Drugs4Covid (https://drugs4covid.oeg.fi.upm.es/): NLP annotations and metadata from more than 60,000 scientific papers about COVID viruses are integrated in a KG with almost 44M of facts (triples). SDM-RDFizer was used for creating this KG.    Other projects were the SDM-RDFizer is also used:  -  Virtual Platform for the H2020 European Joint Programme on Rare Disease (https://www.ejprarediseases.org)      # Installing and Running the SDM-RDFizer   From PyPI (https://pypi.org/project/rdfizer/):  ```  python3 -m pip install rdfizer  python3 -m rdfizer -c /path/to/config/file  ```    From Github/Docker:  Visit the [wiki](https://github.com/SDM-TIB/SDM-RDFizer/wiki) of the repository to learn how to install and run the SDM-RDFizer. You can also take a look to our demo at: https://www.youtube.com/watch?v=DpH_57M1uOE  - Install and run the SDM-RDFizer: https://github.com/SDM-TIB/SDM-RDFizer/wiki/Install&Run  - Parameters to configure SDM-RDFizer: https://github.com/SDM-TIB/SDM-RDFizer/wiki/The-Parameters-of-the-Configuration-file  - FAQ: https://github.com/SDM-TIB/SDM-RDFizer/wiki/FAQ    ## Configurations  You can easily customize your own configurations from the set of features that SDM-RDFzier offers by changing the values of the parameters in the config file. The descriptions of each parameter and the possible values are provided [here](https://github.com/SDM-TIB/SDM-RDFizer/wiki/The-Parameters-of-the-Configuration-file); ""ordered"" and ""large_file"" are the new features provided by SDM-RDFizer version4.0.        ## Version   ```  4.2  ```    ## RML-Test Cases  See the results of the SDM-RDFizer over the RML test-cases at the [RML Implementation Report](http://rml.io/implementation-report/). SDM-RDFizer version4.0 is tested over the latest published [test cases](https://rml.io/test-cases/) before the release.    ## Experimental Evaluations  See the results of the experimental evaluations of SDM-RDFizer version 3.* at [SDM-RDFizer-Experiments repository](https://github.com/SDM-TIB/SDM-RDFizer-Experiments)      ## License  This work is licensed under Apache 2.0    # Authors  The SDM-RDFizer has been developed by members of the Scientific Data Management Group at TIB, as an ongoing research effort. The development is coordinated and supervised by Maria-Esther Vidal (maria.vidal@tib.eu). We strongly encourage you to please report any issues you have with the SDM-RDFizer. You can do that over our contact email or creating a new issue here on Github. The SDM-RDFizer has been implemented by Enrique Iglesias (current version, iglesias@l3s.de) and Guillermo Betancourt (version 0.1, guillermojbetancourt@gmail.com) under the supervision of Samaneh Jozashoori (samaneh.jozashoori@tib.eu), David Chaves-Fraga (dchaves@fi.upm.es), and Kemele Endris (kemele.endris@tib.eu)   """
Semantic web;https://github.com/hsolbrig/PyShEx;"""# Python implementation of ShEx 2.0  [![Pyversions](https://img.shields.io/pypi/pyversions/PyShEx.svg)](https://pypi.python.org/pypi/PyShEx)    [![PyPi](https://img.shields.io/pypi/v/PyShEx.svg)](https://pypi.python.org/pypi/PyShEx)      [![DOI](https://zenodo.org/badge/116042298.svg)](https://zenodo.org/badge/latestdoi/116042298)    https://mybinder.org/v2/gh/hsolbrig/pyshex/master      This package is a reasonably literal implementation of the [Shape Expressions Language 2.0](http://shex.io/shex-semantics/).  It can parse and ""execute"" ShExC and ShExJ source.    ## Revisions  * 0.2.dev3 -- added SchemaEvaluator and other tweaks.  There are still some unit tests that fail -- beware  * 0.3.0 -- Fix several issues.  Still does not pass all unit tests -- see `test_manifest.py` for details  * 0.4.0 -- Added sparql_slurper capabilities.   * 0.4.1 -- Resolves several issues with reactome and disease test cases  * 0.4.2 -- Fix issues #13 (missing start) and #14 (Inconsistent shape causes loop)  * 0.4.3 -- Fix issues #16 and #15 and some refactoring  * 0.5.0 -- First cut at returning fail reasons... some work still needed  * 0.5.1 -- Update shexc parser to include multi-line comments and bug fixes  * 0.5.2 -- Issue with installer - missed the parse_tree package  * 0.5.3 -- make sparql_slurper a dependency  * 0.5.4 -- Fixed long recursion issue with blood pressure example  * 0.5.5 -- Fixed zero cardinality issue (#20)  * 0.5.6 -- Added CLI entry point and cleaned up error reporting  * 0.5.7 -- Throw an error on an invalid focus node (#23)  * 0.5.9 -- Candidate for ShEx 2.1  * 0.5.10 -- Fixed evaluator to load files, strings, etc. as ShEx  * 0.5.11 -- Added Collections Flattening graph option to evaluator.  * 0.5.12 -- Added -A option, catch missing start node early  * 0.6.0 -- Added the -ut and -sp options to allow start nodes to be specified by rdf:type or an arbitrary predicate  * 0.6.1 -- Added the ability to supply a SPARQL Query (-sq option)   * 0.7.0 -- Fixes for issues 28, 29 and 30   * 0.7.1 -- Fix issue 26  * 0.7.2 -- Upgrade error reporting  * 0.7.3 -- Report using namespaces, enhance PrefixLib to inject into a module  * 0.7.4 -- Added '-ps', '-pr', '-gn', '-pb' options to CLI  * 0.7.5 -- Fix CLOSED issue in evaluate call (issue 41)  * 0.7.6 -- bump version due to build error    ## Installation  ```bash  pip install PyShEx  ```  Note: If you need to escape single quotes in RDF literals, you will need to install the bleeding edge  of rdflib:  ```bash  pip uninstall rdflib  pip install git+https://github.com/rdflib/rdflib  ```  Unfortunately, however, `rdflib-jsonld` is NOT compatible with the bleeding edge rdflib, so you can't use a json-ld parser in this situation.    ## shexeval CLI  ```bash  > shexeval -h  usage: shexeval [-h] [-f FORMAT] [-s START] [-ut] [-sp STARTPREDICATE]                  [-fn FOCUS] [-A] [-d] [-ss] [-cf] [-sq SPARQL] [-se]                  [--stopafter STOPAFTER] [-ps] [-pr] [-gn GRAPHNAME] [-pb]                  rdf shex    positional arguments:    rdf                   Input RDF file or SPARQL endpoint if slurper or sparql                          options    shex                  ShEx specification    optional arguments:    -h, --help            show this help message and exit    -f FORMAT, --format FORMAT                          Input RDF Format    -s START, --start START                          Start shape. If absent use ShEx start node.    -ut, --usetype        Start shape is rdf:type of focus    -sp STARTPREDICATE, --startpredicate STARTPREDICATE                          Start shape is object of this predicate    -fn FOCUS, --focus FOCUS                          RDF focus node    -A, --allsubjects     Evaluate all non-bnode subjects in the graph    -d, --debug           Add debug output    -ss, --slurper        Use SPARQL slurper graph    -cf, --flattener      Use RDF Collections flattener graph    -sq SPARQL, --sparql SPARQL                          SPARQL query to generate focus nodes    -se, --stoponerror    Stop on an error    --stopafter STOPAFTER                          Stop after N nodes    -ps, --printsparql    Print SPARQL queries as they are executed    -pr, --printsparqlresults                          Print SPARQL query and results    -gn GRAPHNAME, --graphname GRAPHNAME                          Specific SPARQL graph to query - use '' for any named                          graph    -pb, --persistbnodes  Treat BNodes as persistent in SPARQL endpoint  ```    ## Documentation  See: [examples](notebooks) Jupyter notebooks for sample uses      ## General Layout  The root `pyshex` package is subdivided into:    * [shape_expressions_language](pyshex/shape_expressions_language) - implementation of the various sections in  [Shape Expressions Language 2.0](http://shex.io/shex-semantics/).  As an example, [3. Terminology](http://shex.io/shex-semantics/#terminology) is implemented in [p3_terminology.py](pyshex/shape_expressions_language/p3_terminology.py), [5.2 Validation Definition](http://shex.io/shex-semantics/#validation) in [p5_2_validation_definition.py](pyshex/shape_expressions_language/p5_2_validation_definition.py), etc.  * [shapemap_structure_and_language](pyshex/shapemap_structure_and_language) - implementation of [ShapeMap Structure and Language](http://shex.io/shape-map/) (as well as we can understand it)  * [sparql11_query](pyshex/sparql11_query) - required sections from [SPARQL 1.1 Query Language section 17.2](https://www.w3.org/TR/sparql11-query/#operandDataTypes)  * [utils](pyshex/utils) - supporting utilities    The ShEx schema definitions for this package come from [ShExJSG](https://github.com/hsolbrig/ShExJSG)    We are trying to keep the python as close as possible to the (semi-)formal specification.  As an example, the statement:  ```text  Se is a ShapeAnd and for every shape expression se2 in shapeExprs, satisfies(n, se2, G, m)  ```  is implemented in Python as:  ```python          ...  if isinstance(se, ShExJ.ShapeAnd):      return satisfiesShapeAnd(cntxt, n, se)          ...  def satisfiesShapeAnd(cntxt: Context, n: nodeSelector, se: ShExJ.ShapeAnd) -> bool:      return all(satisfies(cntxt, n, se2) for se2 in se.shapeExprs)  ```    ## Dependencies  This package is built using:  * [ShExJSG](https://github.com/hsolbrig/ShExJSG) -- an object representation of the ShEx AST as defined by [ShEx.jsg](https://github.com/shexSpec/shexTest/blob/master/doc/ShExJ.jsg) and compiled through the [PyJSG](https://github.com/hsolbrig/pyjsg) compiler.  * The python [ShExC](https://github.com/shexSpec/grammar/tree/master/parsers/python) compiler -- which transforms the [Shape Expressions Language](http://shex.io/shex-semantics/index.html) into ShExJSG images.  * [rdflib](https://rdflib.readthedocs.io/en/stable/)       ## Conformance    This implementation passes all of the tests in the master branch of [validation/manifest.ttl](https://raw.githubusercontent.com/shexSpec/shexTest/master/validation/manifest.ttl) with the following exceptions:    At the moment, there are 1088 tests, of which:    * 1007 pass  * 81 are skipped - reasons:  1) (52) sht:LexicalBNode, sht:ToldBNode and sht:BNodeShapeLabel test non-blank blank nodes (`rdflib` does not preserve bnode ""identity"")  2) (18) sht:Import Uses ShEx 2.1 IMPORT feature -- not yet implemented (three aren't tagged)  3) (3) Uses manifest shapemap feature -- not yet implemented  4) (2) sht:relativeIRI -- this isn't a real problem, but we havent taken time to deal with this in the test harness  5) (6) `rdflib` has a parsing error when escaping single quotes. (Issue submitted, awaiting release)    As mentioned above, at the moment this is as literal an implementation of the specification as was sensible.  This means, in particular, that we are less than clever when it comes to partition management.    ## Docker    ### Build    ```shell  docker build -t pyshex docker  ```    ### Run    ```shell  docker run --rm -it pyshex -gn '' -ss -ut -pr -sq 'select distinct ?item where{?item a <http://w3id.org/biolink/vocab/Gene>} LIMIT 1' http://graphdb.dumontierlab.com/repositories/ncats-red-kg https://github.com/biolink/biolink-model/raw/master/shex/biolink-modelnc.shex  ```   """
Semantic web;https://github.com/uzh/katts;"""KATTS  =====    KATTS is a collection of storm operators (bolts) and a configuration facility that enables  a user to write queries in an XML syntax to query a stream of RDF triples.      Information  ===========  - The scripts used for the SSWS/ISWC 2013 publications can be found in src/eval/.  - Information about version numbers can be found in VERSIONS.md                                                                                                                                                                                                           """
Semantic web;https://github.com/ncarboni/awesome-GLAM-semweb;"""## awesome GLAM semweb [![Awesome](https://awesome.re/badge.svg)](https://github.com/ncarboni/Awesome-GLAM-semweb)    A curated list of various semantic web and linked data resources for heritage, humanities and art history practitioners.      The list is an extension of [the semantic web awesome list](https://github.com/semantalytics/awesome-semantic-web) specifically targeted for GLAM (Galleries, Libraries, Archive, Museum). The [the semantic web awesome list](https://github.com/semantalytics/awesome-semantic-web) is the reference for general SM solutions, while this list is specifically target to domain resources which do not belong to the general list (e.g ontologies, specific software widely used within the community, documentation targeting DH practitioners and point of contacts/exchanges). For the purpose of providing to the reader a complete and stand-alone resource, few elements of the [the semantic web awesome list](https://github.com/semantalytics/awesome-semantic-web) will be reported also here.      The list is public and contributions are welcome.      <!-- MarkdownTOC levels=""2,3,4,5"" -->    - [Semantic Web Standards & Recommendation](#semantic-web-standards--recommendation)  	- [RDF](#rdf)  	- [RDFS](#rdfs)  	- [OWL](#owl)  	- [Data Shape](#data-shape)  	- [SPARQL](#sparql)  	- [RDFa](#rdfa)  	- [Linked Data Fragments \(LDF\)](#linked-data-fragments-ldf)  	- [Linked Data Notifications](#linked-data-notifications)  	- [Linked Data Platform](#linked-data-platform)  - [Serialization](#serialization)  - [Ontologies](#ontologies)  	- [CIDOC-CRM](#cidoc-crm)  		- [CIDOC-CRM Official extensions](#cidoc-crm-official-extensions)  		- [CIDOC-CRM Unofficial extensions](#cidoc-crm-unofficial-extensions)  		- [CIDOC-CRM/FRBRoo Tutorials](#cidoc-crmfrbroo-tutorials)  		- [CIDOC-CRM Modelling examples and documentation](#cidoc-crm-modelling-examples-and-documentation)  	- [Gemeinsame Normdatei \(GND\)](#gemeinsame-normdatei-gnd)  	- [Europeana Data Model](#europeana-data-model)  	- [Dublin Core](#dublin-core)  	- [Open Archives Initiative Object Reuse and Exchange \(OAI-ORE\)](#open-archives-initiative-object-reuse-and-exchange-oai-ore)  	- [Encoded Archival Context for Corporate Bodies, Persons, and Families \(EAC-CPF\)](#encoded-archival-context-for-corporate-bodies-persons-and-families-eac-cpf)  	- [ICA Expert Group on Archival Description \(EGAD\)](#ica-expert-group-on-archival-description-egad)  	- [Metadata Object Description Schema \(MADS\)](#metadata-object-description-schema-mads)  	- [BIBFRAME \(Bibliographic Framework Initiative\)](#bibframe-bibliographic-framework-initiative)  	- [BIBO \(Bibliographic Ontology Specification\)](#bibo-bibliographic-ontology-specification)  	- [Resource Description Access Ontology](#resource-description-access-ontology)  	- [PREMIS](#premis)  	- [World Wide Web Consortium \(W3C\)](#world-wide-web-consortium-w3c)  	- [Others](#others)  	- [Where to find ontologies](#where-to-find-ontologies)  - [Mapping tools](#mapping-tools)  	- [X3ML](#x3ml)  	- [Karma](#karma)  	- [Ontop](#ontop)  - [Vocabularies and KOS](#vocabularies-and-kos)  	- [General](#general)  	- [France](#france)  	- [Italy](#italy)  	- [Where to find controlled vocabularies/thesauri](#where-to-find-controlled-vocabulariesthesauri)  - [Vocabulary / KOS Management](#vocabulary--kos-management)  	- [Vocabulary validation & conversion tools](#vocabulary-validation--conversion-tools)  - [Exchange and discussions](#exchange-and-discussions)  	- [Conferences](#conferences)  	- [Conference not specifically on Semantic Web, but with strong ties to the community](#conference-not-specifically-on-semantic-web-but-with-strong-ties-to-the-community)  	- [Discussion groups](#discussion-groups)  	- [Academic Journals](#academic-journals)  - [Knowledge Graph Management](#knowledge-graph-management)  	- [Linked Data Platform \(LDP\)](#linked-data-platform-ldp)  - [Books](#books)  - [Editors](#editors)  	- [TextMate](#textmate)  	- [Sublime Text](#sublime-text)  	- [BBedit](#bbedit)  	- [VIM](#vim)  	- [Emacs](#emacs)  	- [IntelliJ](#intellij)  - [Data Management](#data-management)  	- [OpenRefine Reconciliation services](#openrefine-reconciliation-services)  - [Data Validation](#data-validation)  - [IIIF](#iiif)  - [Misc](#misc)  	- [Prefix](#prefix)  	- [Ontology](#ontology)  		- [Documentation](#documentation)  		- [Management](#management)  	- [Alignment](#alignment)  	- [Conversion](#conversion)  	- [Visualisation](#visualisation)  	- [Images](#images)    <!-- /MarkdownTOC -->      ## Semantic Web Standards & Recommendation    ### RDF    - [RDF 1.1 Primer](https://www.w3.org/TR/rdf11-primer/)  - [RDF 1.1 Semantics](https://www.w3.org/TR/rdf11-mt/)  - [RDF 1.1 Concepts and Abstract Syntax](https://www.w3.org/TR/rdf11-concepts/)  - [RDF 1.1: On Semantics of RDF Datasets](https://www.w3.org/TR/rdf11-datasets/)  - [XSD Datatypes](https://www.w3.org/2011/rdf-wg/wiki/XSD_Datatypes)    ### RDFS    - [RDF Schema 1.1](https://www.w3.org/TR/rdf-schema/)    ### OWL    - [OWL 2 Web Ontology Language Document Overview](https://www.w3.org/TR/owl-overview/)  - [OWL 2 Web Ontology Language Primer](https://www.w3.org/TR/owl-primer/)    ### Data Shape    - [SHACL Core](https://www.w3.org/TR/shacl/)  - [SHACL Advanced](https://www.w3.org/TR/shacl-af/)  - [SHex](http://shex.io)    ### SPARQL    - [SPARQL 1.1 Overview](https://www.w3.org/TR/sparql11-overview/)  - [SPARQL 1.1 Query Language](https://www.w3.org/TR/sparql11-query/)  - [SPARQL 1.1 Update](https://www.w3.org/TR/sparql11-update/)  - [SPARQL 1.1 Service Description](https://www.w3.org/TR/sparql11-service-description/)  - [SPARQL 1.1 Federated Query](https://www.w3.org/TR/sparql11-federated-query/)  - [SPARQL 1.1 Query Results JSON Format](https://www.w3.org/TR/sparql11-results-json/)  - [SPARQL 1.1 Query Results CSV and TSV Formats](https://www.w3.org/TR/sparql11-results-csv-tsv/)  - [SPARQL 1.1 Query Results XML Format (Second Edition)](https://www.w3.org/TR/rdf-sparql-XMLres/)  - [SPARQL 1.1 Entailment Regimes](https://www.w3.org/TR/sparql11-entailment/)  - [SPARQL 1.1 Protocol](https://www.w3.org/TR/sparql11-protocol/)  - [SPARQL 1.1 Graph Store HTTP Protocol](https://www.w3.org/TR/sparql11-http-rdf-update/)    ### RDFa    - [XHTML+RDFa 1.1 - Third Edition](https://www.w3.org/TR/xhtml-rdfa/)  - [RDFa Lite 1.1 - Second Edition](https://www.w3.org/TR/rdfa-lite/)  - [HTML+RDFa 1.1 - Second Edition](https://www.w3.org/TR/html-rdfa/)    ### Linked Data Fragments (LDF)    - [Linked Data Fragments](http://linkeddatafragments.org)    ### Linked Data Notifications    - [Linked Data Notifications](https://www.w3.org/TR/ldn/)    ### Linked Data Platform    - [Linked Data Platform 1.0 Primer](https://www.w3.org/TR/ldp-primer/)  - [Linked Data Platform Best Practices and Guidelines](https://www.w3.org/TR/ldp-bp/)  - [Linked Data Platform 1.0](https://www.w3.org/TR/ldp/)  - [Linked Data Platform 1.0 Test Cases](https://dvcs.w3.org/hg/ldpwg/raw-file/tip/tests/ldp-testsuite.html)      ## Serialization    | Format  | Description | Mime-type |  | :--- | :--- | :---: |  | [Turtle](https://www.w3.org/TR/turtle/) | Terse RDF Triple Language. | `text/turtle`, `application/x-turtle` |  | [TriG](https://www.w3.org/TR/trig/) | Plain text format for serializing named graphs and RDF Datasets. | `application/trig`, `application/x-trig` |  | [JSON-LD](https://json-ld.org/) | JSON-based Serialization for Linked Data. | `application/ld+json` |  | [RDF/JSON](https://www.w3.org/TR/rdf-json/) | RDF 1.1 JSON Alternate Serialization. | `application/rdf+json` |  | [N-Triples](https://www.w3.org/TR/n-triples/) | Line-based syntax for RDF datasets. |  `application/n-triples` |  | [N-Quads](https://www.w3.org/TR/n-quads/) | Line-based syntax for RDF datasets. | `application/n-quads`, `text/x-nquads`, `text/nquads` |  | [Notation3](https://www.w3.org/TeamSubmission/n3/) | Notation3 (N3): A readable RDF syntax. | `text/n3`, `text/rdf+n3` |  | [RDF/XML](https://www.w3.org/TR/REC-rdf-syntax/) | RDF/XML Syntax Specification. | `application/rdf+xml`, `application/xml` |  | [TriX](http://www.hpl.hp.com/techreports/2004/HPL-2004-56.html) | RDF Triples in XML. | `application/trix` |  | [HDT](https://www.w3.org/Submission/2011/03/) | Binary RDF Representation for Publication and Exchange. | `application/x-binary-rdf` |  | [aREF](https://gbv.github.io/aREF/aREF.html) | Another RDF Encoding Form. | |       ## Ontologies    ### CIDOC-CRM    - [Documentation](http://www.cidoc-crm.org/versions-of-the-cidoc-crm): Official website of the CIDOC-CRM  - [RDFS](http://www.cidoc-crm.org/versions-of-the-cidoc-crm) Official version of CIDOC-CRM available in RDF. *No direct link, you can use the latest version available in the page*  - [OWL](http://www.cidoc-crm.org/versions-of-the-cidoc-crm) OWL version of CIDOC-CRM. *No direct link, use the latest version available in the page*.  - [CIDOC-CRM Periodic Table](https://remogrillo.github.io/cidoc-crm_periodic_table/)  Visualize and search the CRM in a user-friendly interface.     #### CIDOC-CRM Official extensions    - [CRMdig](http://www.cidoc-crm.org/crmdig): Model for provenance metadata  - [CRMsci](http://www.cidoc-crm.org/crmsci): Scientific observation model  - [CRMinf](http://www.cidoc-crm.org/crminf/): Argumentation model  - [FRBRoo](http://www.cidoc-crm.org/frbroo): Functional Requirement for Bibliographic Records  - [PRESSoo](http://www.cidoc-crm.org/pressoo/): Modelling of bibliographical information  - [CRMpc](http://www.cidoc-crm.org/versions-of-the-cidoc-crm): Modelling .1 properties in CRM as n-ary relationship. *no direct link download the CRM-PC file from the latest CRM version*  - [CRMgeo](http://www.cidoc-crm.org/crmgeo/): Spatiotemporal model  - [CRMba](http://www.cidoc-crm.org/crmba): Model for archaeological buildings  - [CRMtex](http://www.cidoc-crm.org/crmtex): Model for the study of ancient text  - [CRMarcheo](http://www.cidoc-crm.org/crmarchaeo/): Excavation model    #### CIDOC-CRM Unofficial extensions    - [VIR](http://w3id.org/vir): Model for visual and iconographical representations  - [DOREMUS](http://data.doremus.org/ontology/): Model for describing musical performances and recordings    #### CIDOC-CRM/FRBRoo Tutorials    - [Long Video Tutorial by Stephen Staad](http://old.cidoc-crm.org/cidoc_tutorial/index.html) - Require Flash  - [Short Video Tutorial by George Bruseker](https://youtu.be/lVQFciW7V4I)  - [FRBRoo Tutorial](http://83.212.168.219/FRBR_Tutorial/)    #### CIDOC-CRM Modelling examples and documentation    - [Official Website modelling](http://www.cidoc-crm.org/functional-units) and [Best Practices](http://www.cidoc-crm.org/best_practices)  - [Reference Data Models](https://docs.swissartresearch.net/instruction/) - Ready-Made data patterns for describing Person, Artwork, Documents, Events and more.  - [Consortium for Open Research Data in the Humanities](https://docs.cordh.net) - Basic shared pattern for interoperable CRM  - [Linked Art](https://linked.art) - Art Museum Application Profile for CRM in JSON-LD.  - [DOPHEDA](https://chin-rcip.github.io/collections-model/) - Project of the Canadian Heritage Information Network to foster the development of LOD in heritage institutions, including a Data Model based on CIDOC CRM      ### Gemeinsame Normdatei (GND)    - [GND Ontology](https://d-nb.info/standards/elementset/gnd) for authority files    ### Europeana Data Model    - [EDM](https://pro.europeana.eu/resources/standardization-tools/edm-documentation)    ### Dublin Core    - [DCMI Metadata Terms](http://dublincore.org/documents/dcmi-terms/)    ### Open Archives Initiative Object Reuse and Exchange (OAI-ORE)    - [Vocabulary](http://www.openarchives.org/ore/1.0/vocabulary)    ### Encoded Archival Context for Corporate Bodies, Persons, and Families (EAC-CPF)    - [EAC-CPF Description Ontology for Linked Archival Data](https://labs.regesta.com/progettoReload/wp-content/uploads/2013/10/eac-cpf.html)    ### ICA Expert Group on Archival Description (EGAD)    - [Records in Context - Conceptual Model (RiC-CM)](https://www.ica.org/en/egad-ric-conceptual-model)  - [Records in Context - Ontology](#)    ### Metadata Object Description Schema (MADS)    - [Vocabulary](http://www.loc.gov/standards/mads/rdf/v1.html)    ### BIBFRAME (Bibliographic Framework Initiative)    - [Model Overview](https://www.loc.gov/bibframe/docs/bibframe2-model.html)  - [Vocabulary](http://id.loc.gov/ontologies/bibframe.html)    ### BIBO (Bibliographic Ontology Specification)    - [Vocabulary](http://bibliontology.com)    ### Resource Description Access Ontology    - [RDA Registry](http://www.rdaregistry.info/rgAbout/rdaont/)    ### PREMIS    - [Vocabulary](http://id.loc.gov/ontologies/premis-3-0-0.html) of digital preservation metadata    ### World Wide Web Consortium (W3C)    - [Web Annotation Vocabulary](https://www.w3.org/TR/annotation-vocab/)  - [WGS84](https://www.w3.org/2003/01/geo/) - Basic Geo (WGS84 lat/long) Vocabulary.  - [skos](http://www.w3.org/2004/02/skos/core.html) - SKOS Simple Knowledge Organization System.  - [skos-xl](http://www.w3.org/TR/skos-reference/skos-xl.html) - SKOS Simple Knowledge Organization System eXtension for Labels.  - [vcard](https://www.w3.org/TR/vcard-rdf/) - vCard Ontology - for describing People and Organizations.  - [void](https://www.w3.org/TR/void/) - Describing Linked Datasets with the VoID Vocabulary.  - [time](https://w3c.github.io/sdw/time/) - Time Ontology in OWL.  - [org](https://www.w3.org/TR/vocab-org/) - The Organization Ontology.  - [dqv](http://www.w3.org/ns/dqv#) - Vocabulary for describing quality metadata.  - [PROV-O](https://www.w3.org/TR/prov-o/) - Represent provenance information.    ### Others    - [foaf](http://www.foaf-project.org/) - Friend of a Friend (FOAF) ontology.    - [obo-relations](http://obofoundry.org/ontology/ro.html) - Relation Ontology. Relationship types shared across multiple ontologies.    - [RELATIONSHIP](http://vocab.org/relationship/) - Vocabulary for describing relationships between people.  - [BIO](http://vocab.org/bio/) - Vocabulary for describing biographical information.    - [schema.org](https://schema.org/docs/datamodel.html) - Structured data on the Internet (Google, Microsoft, Yahoo and Yandex).  - [SPAR](http://www.sparontologies.net) - Semantic Publishing and Referencing Ontologies.    - GeoSPARQL ([DOCS](https://www.opengeospatial.org/standards/geosparql)|[RDF](www.opengis.net/ont/geosparql))    - [Creative Commons Rights Expression](https://creativecommons.org/ns)    - [QUDT](http://www.qudt.org) Quantities, Units, Dimensions and Types Ontologies and Vocabularies    - [Ontology of units of measure](http://www.ontology-of-units-of-measure.org) Dimensions and measurements ontology    ### Where to find ontologies     - [Linked Open Vocabularies](https://lov.linkeddata.es/)        ## Mapping tools    Mapping tools for transforming your data (CSV, XML) into RDF    ### X3ML    X3ML is a transformation engine developed by FORTH. It is perfected to work with CIDOC-CRM, however it does work greatly with other ontologies as well. It is available as web application (3M) and a stand alone app (X3ML). In both cases the input file has to be in XML (for transforming a CSV file to XML see [Mr Data Converter](https://shancarter.github.io/mr-data-converter/)).     - [3M](http://139.91.183.3/3M/)  - [X3ML](https://github.com/isl/x3ml)    In order to transform the data it is necessary to create a X3ML declaration and a URI Mapping. Examples of both, together with the necessary commands are available at this addresses:    - [Consortium for Open Research Data in the Humanities](https://docs.cordh.net/tool/mapping/)    ### Karma    [Karma](http://usc-isi-i2.github.io/karma/) is an information integration tool for aggregating, harmonising and transforming diverse data sources (CSV, XML, JSON, KML, Web APIs). The process is driven by an ontology and results in a transformation of the original data in RDF. A graphical user interface help the user map the data and, moreover, it is build to recognize the mapping of data to ontology classes and then uses the ontology to propose a model that ties together these classes. Karma does not only help the user transform the data but it can be used to normalise them too.    ### Ontop        [Ontop](https://ontop.inf.unibz.it) is an application developed by the University of Bolzano for creating a virtual RDF Graph on top of your current data source. Mappings can be easily created using [Protege](http://protege.stanford.edu/) and results are queryable using SPARQL 1.0. Moreover, it support reasoning.              ## Vocabularies and KOS    ### General    - [Getty Art and Architecture Thesaurus](http://vocab.getty.edu/aat/)   - [Getty Union List of Artist Names](http://vocab.getty.edu/ulan/)  - [Thesaurus of Geographic Names](http://vocab.getty.edu/tgn/)  - Iconclass [keyword search](http://www.iconclass.org/rkd/9/) + [help LOD](http://www.iconclass.org/help/lod)  - [CERL Thesaurus](https://data.cerl.org/thesaurus/) for book heritage  - [Library Congress Subject Headings](http://id.loc.gov/authorities/subjects.html)  - [Thesaurus Graphical Materials](http://id.loc.gov/vocabulary/graphicMaterials.html)  - [Nomenclature for Museum Cataloging](https://www.nomenclature.info) + [help LOD](https://www.nomenclature.info/integration.app)    ### France    - [Thésaurus de la désignation des objets mobiliers](http://data.culture.fr/thesaurus/page/ark:/67717/T69)  - [Liste d'autorité Actions pour l'indexation des archives locales](http://data.culture.fr/thesaurus/resource/ark:/67717/T2)  - [Liste d'autorité Contexte historique pour l'indexation des archives locales](http://data.culture.fr/thesaurus/resource/ark:/67717/T4)  - [Liste d'autorité Typologie documentaire pour l'indexation des archives locales](http://data.culture.fr/thesaurus/resource/ark:/67717/T3)  - [Nomenclatures HADOC](http://data.culture.fr/thesaurus/resource/ark:/67717/0404efce-2024-4694-bf80-eba4fc2b336c)  - [Techniques photographiques](http://data.culture.fr/thesaurus/resource/ark:/67717/T61)  - [Thésaurus de la désignation des œuvres architecturales et des espaces aménagés](http://data.culture.fr/thesaurus/resource/ark:/67717/T96)  - [Thésaurus-matières pour l'indexation des archives locales](http://data.culture.fr/thesaurus/resource/ark:/67717/Matiere)  - [Vocabulaire des activités des entités productrices d'archives](http://data.culture.fr/thesaurus/resource/ark:/67717/51232822-adac-4a33-aa14-29e2c701a5ee)  - [Vocabulaire des domaines d'action ou objets des entités productrices d'archives](http://data.culture.fr/thesaurus/resource/ark:/67717/f14e8183-5885-46d6-8fc9-17ebd8f3c27e)  - [Vocabulaire pour les techniques photographiques](http://data.culture.fr/thesaurus/resource/ark:/67717/2012b973-ddb2-4540-a775-9157c3c1d7fd)      ### Italy    - [Thesaurus Portale della Cultural Italiana (PICO)](http://www.culturaitalia.it/pico/thesaurus/4.3/thesaurus_4.3.0.skos.xml#http://culturaitalia.it/pico/thesaurus/4.1%23beni_materiali_della_tradizione_e_del_folklore)  - [Soggettario Biblioteca Nazionale Centrale Firenze](http://thes.bncf.firenze.sbn.it/ricerca.php)    ### China    - [Chinese Iconography Thesaurus (CIT)](https://chineseiconography.org/)    ### Where to find controlled vocabularies/thesauri     - [Bartoc](https://bartoc.org)    ## Vocabulary / KOS Management    - [Skosmos](http://skosmos.org) Access SKOS vocabularies with SPARQL or API  - [VocBench](http://vocbench.uniroma2.it) Web-based, multilingual, collaborative platform for managing OWL, SKOS(/XL) and generic RDF datasets.  - [Ginco](https://github.com/culturecommunication/ginco) Collaborative management and alignment of vocabularies.  - [Opentheso](https://github.com/miledrousset/opentheso) Multilingual collaborative management of KOS  - [iqvoc](https://github.com/innoq/iqvoc) SKOS(-XL) Vocabulary Management System for the Semantic Web.  - [TemaTres](https://www.vocabularyserver.com) Manage, share, publish, and re-use SKOS vocabularies.       ### Vocabulary validation & conversion tools    - [Skosify](https://github.com/NatLibFi/Skosify) Validate, convert and improve SKOS vocabularies  - [qSKOS](https://github.com/cmader/qSKOS) Find quality issues in SKOS vocabularies.  - [SKOS Play](http://labs.sparna.fr/skos-play/) Render and visualise thesaurus, taxonomies or controlled vocabularies. Furthermore, convert Excel spreadsheets into SKOS files.      ## Exchange and discussions     ### Conferences     - [International Semantic Web Conference (ISWC 2019)](http://iswc2020.semanticweb.org)  - [European Semantic Web Conference (ESWC 2019)](https://2020.eswc-conferences.org)  - [CIDOC - ICOM International Committee for Documentation](http://network.icom.museum/cidoc/)  - [Workshop on Humanities in the Semantic Web (WHiSe)](http://whise.kmi.open.ac.uk)  - [Semantic Web in Libraries](http://swib.org)  - [LODLAM Summit](https://lod-lam.net/)    ### Conference not specifically on Semantic Web, but with strong ties to the community    - [International Conference on Theory and Practice of Digital Libraries (TPDL)](http://www.tpdl.eu)  - [International Conference on Metadata and Semantics Research](http://www.mtsr-conf.org/home)  - [Code4Lib](https://code4lib.org/conference/)  - [European Library Automation Group](https://elag.org/)  - [Digital Heritage](#)  - [Europeana](#)      ### Discussion groups    - [CIDOC-CRM SIG Mailing List](http://lists.ics.forth.gr/mailman/listinfo/crm-sig)  - [LODLAM - Linked Open Data in Libraries, Archives & Museum Community Group](https://groups.google.com/forum/#!forum/lod-lam)  - [w3c semantic web Mailing List](https://lists.w3.org/Archives/Public/semantic-web/)  - [w3c Linked Open Data Mailing List](https://lists.w3.org/Archives/Public/public-lod/)  - [GLAM–Wiki initiative](https://en.wikipedia.org/wiki/Wikipedia:GLAM)  - [Wikidata GLAM](https://www.wikidata.org/wiki/Wikidata:GLAM)  - [GLAM Wiki Facebook Group](https://www.facebook.com/groups/GLAMWikiGlobal/)    ### Academic Journals    - [Semantic Web Journal](http://www.semantic-web-journal.net/)  - [Journal of Web Semantics](https://www.journals.elsevier.com/journal-of-web-semantics)  - [International Journal of Web and Semantic Technology](http://www.airccse.org/journal/ijwest/ijwest.html)    ## Knowledge Graph Management    $ - Proprietary    OS - OpenSource    *f* - Free Version      - [Researchspace](https://www.researchspace.org) - (OS) platform for managing, interacting and building entry points (template, graph authoring) for RDF Stores. Specifically targeting GLAM researchers and institutions.  - [Metaphacts](https://metaphacts.com) - (OS)($) platform for managing, interacting and building entry points (template, graph authoring) for RDF Stores.  - [WissKI](http://wiss-ki.eu) - (OS) Drupal-based platform to interact and build entry point for RDF Stores.  - [LinkedDataHub](https://atomgraph.com/products/linkeddatahub/) - (OS) collaborative data and information management for RDF data.   - [GraphDB by Ontotext](https://www.ontotext.com/products/graphdb/) - ($)(*f*) RDF Database for Knowledge Graphs.       ### Linked Data Platform (LDP)    - [fedora](https://duraspace.org/fedora/) - Repository platform with native linked data support.  - [warp](https://github.com/linkeddata/warp) - Warp an LDP file manager.  - [Marmotta](https://github.com/apache/marmotta) - Apache linked data platform implementation.  - [Elda](https://github.com/epimorphics/elda) - Linked data platform from Epimorphics.  - [LDP4j](https://github.com/ldp4j/ldp4j)  - [gold](https://github.com/linkeddata/gold) - Linked Data server for Go.  - [CarbonLDP](https://github.com/CarbonLDP)  - [trellis](https://github.com/trellis-ldp/trellis)    ## Books    - [Linked Data](https://www.manning.com/books/linked-data)  - [Explorer's Guide to the Semantic Web](https://www.manning.com/books/explorers-guide-to-the-semantic-web)  - [Semantic Web Programming](https://www.wiley.com/en-us/Semantic+Web+Programming-p-9781118080603)  - [Semantic Web for the Working Ontologist](http://workingontologist.org/)  - [Programming the Semantic Web](http://shop.oreilly.com/product/9780596153823.do)  - [Building Ontologies with Basic Formal Ontology](https://mitpress.mit.edu/books/building-ontologies-basic-formal-ontology)  - [Structures for Organizing Knowledge: Exploring Taxonomies, Ontologies, and Other Schema](https://www.amazon.com/Structures-Organizing-Knowledge-Taxonomies-Ontologies/dp/1555706991)  - [Validating RDF Data](http://book.validatingrdf.com/)  - [Demystifying OWL for the Enterprise](https://doi.org/10.2200/S00824ED1V01Y201801WBE017)  - [Learning SPARQL](http://learningsparql.com)  - [Knowledge Representation](http://www.jfsowa.com/krbook/)            ## Editors    ### TextMate    - [sparql/turtle extension](https://github.com/peta/turtle.tmbundle)    ### Sublime Text    - [Turtle and SPARQL syntax highlighter](https://github.com/abcoates/sublime-text-turtle-sparql)  - [SPARQL 1.1, Turtle, TriG, N-Triples, N-Quads and Notation3 syntax highlighter](https://github.com/blake-regalia/linked-data.syntaxes)    ### BBedit    - [Turtle syntax highlighter](https://github.com/njh/bbedit-turtle)    ### VIM    - [sparql.vim](https://github.com/vim-scripts/sparql.vim) - SPARQL syntax highlighting.  - [vim-sparql](https://github.com/Omer/vim-sparql)  - [semweb.vim](https://github.com/seebi/semweb.vim)    ### Emacs    - [sparql-mode](https://github.com/ljos/sparql-mode)    ### IntelliJ    - [sparql4idea](https://github.com/mattnathan/sparql4idea) - SPARQL language plugin for IntelliJ IDEA.    ## Data Management    - [Timbuctoo](https://timbuctoo.huygens.knaw.nl) Data management, enrichment and sharing  - [Openrefine](http://openrefine.org) Data cleaning and normalisation    ### OpenRefine Reconciliation services    - [VIAF](https://viaf.org) & [ORCID](https://orcid.org) — OpenRefine reconciliation services for VIAF, ORCID, and Open Library available in [Github](https://github.com/codeforkjeff/conciliator). To make it work, it is necessary the launch a jar file for its use. After that the endpoint is available at: [http://localhost:8080/reconcile/viaf](http://localhost:8080/reconcile/viaf).    - [Geonames](http://www.geonames.org) —  The OpenRefine reconciliation services is available in [GitHub](https://github.com/cmharlow/geonames-reconcile). To make it work, it is necessary to launch a python script. After that the endpoint is available at: [http://0.0.0.0:5000/reconcile](http://0.0.0.0:5000/reconcile).    - [GND](https://lobid.org/gnd) — Reconciliation service offered by [lob-id](https://lobid.org). Endpoint for OpenRefine: [https://lobid.org/gnd/reconcile](http://services.getty.edu/vocab/reconcile/). Possible to ""Add columns from reconciled values"".    - [SNAC](http://snaccooperative.org)— Social Networks and Archival Context. Endpoint for OpenRefine: [http://openrefine.snaccooperative.org](http://services.getty.edu/vocab/reconcile/)- [Nomisma](http://nomisma.org) — Nomisma provide stable digital representations of numismatic concepts. Endpoint for OpenRefine: [http://nomisma.org/apis/reconcile](http://nomisma.org/apis/reconcile).     - [OpenCorporate](https://opencorporates.com/) — Open database of companies. Endpoint for OpenRefine: [https://opencorporates.com/reconcile](https://opencorporates.com/reconcile).    - [Getty Research Institute](https://www.getty.edu/research/tools/vocabularies/obtain/openrefine.html) - OpenRefine reconciliation services for the Getty Vocabularies (ULAN, TGN, AAT).  - [Nomisma](http://nomisma.org) - OpenRefine reconciliation service for Nomisma. Endpoint for OpenRefine: [http://nomisma.org/apis/reconcile](http://nomisma.org/apis/reconcile). Documentation on their [website](https://numishare.blogspot.com/2017/10/nomisma-launches-openrefine.html)  - [Perio.do](https://test.perio.do) - The OpenRefine reconciliation services is available in [GitHub](https://github.com/periodo/periodo-reconciler).  - [Pleiades]() - OpenRefine reconciliation service for Pleiades. Endpoint for OpenRefine: [https://geocollider-sinatra.herokuapp.com/reconcile](https://geocollider-sinatra.herokuapp.com/reconcile). More information [here](http://geocollider-sinatra.herokuapp.com).  - [GODOT](https://godot.date/home) - OpenRefine reconciliation service for GODOT. Endpoint for OpenRefine: [https://godot.date/api/openrefine/reconcile](https://godot.date/api/openrefine/reconcile).    ## Data Validation     - [pySHACL](https://github.com/RDFLib/pySHACL) - a Python validator for SHACL.  - [SHaclEX](https://github.com/weso/shaclex) - Scala implementation of SHEX and SHACL. Possible to use a demo version from a web interface.  - [RDFUnit](http://rdfunit.aksw.org/) - RDF testing suite. Include but not limited to SHACL.  - [dotNetRDF SHACL](http://langsamu.net/shacl) - SHACL procecssor that can check conformance and validate data graphs against shapes graphs.  - [YASHE](http://www.weso.es/YASHE/) -  ShEx editor with examples  - [Shex validator](http://shex.io/webapps/shex.js/doc/shex-simple.html) - Simple Online Validator for ShEx    ## IIIF    - [International Image Interoperability Framework](https://iiif.io/)  - [Awesome IIIF-related resources](https://github.com/IIIF/awesome-iiif)    ## Misc      ### Prefix     - [prefix.cc](https://github.com/cygri/prefix.cc) - Source code to the prefix.cc website.    ### Ontology    #### Documentation    - [LODE](http://www.essepuntato.it/lode) ontology documentation environment.  - [Widoco](https://github.com/dgarijo/Widoco) Ontology documentation (include LODE).    #### Management    - [OntoME](http://ontologies.dataforhistory.org) Ontology Management Environment  - [Grafo](https://gra.fo/) Collaborative and graphical ontology design    ### Alignment    - [SILK](http://silkframework.org) Linked Data Integration Framework.  - [OnAGUI](https://github.com/lmazuel/onagui) Ontology alignment GUI.  - [Alignment API](http://alignapi.gforge.inria.fr) Tool for Expressing, generating and sharing ontology alignments    ### Conversion    - [RDFConvert](https://sourceforge.net/projects/rdfconvert/) - RDFConvert is a simple command-line tool for converting RDF file betweeen different syntax formats.  - [RDF2RDF](http://www.l3s.de/~minack/rdf2rdf/) Java tool to converts RDF files from any format to any format.  - [marc2rdf](https://github.com/DOREMUS-ANR/marc2rdf) Takes as input INTERMARC-XML and UNIMARC-XML files and generates as output RDF.  - [ntcat](https://github.com/cgutteridge/ntcat) Command line tool for concatenating NTriples documents.  - [How to diff RDF](https://www.w3.org/2001/sw/wiki/How_to_diff_RDF)  - [grlc](https://github.com/CLARIAH/grlc) - Web APIs from SPARQL queries.      ### Visualisation    - [Ontology Visualisation](https://github.com/usc-isi-i2/ontology-visualization) Python tool for visualising RDF. Convert rdf to .dot and use Graphviz for constructing a visual representation.    ### Images    - [ImageSnippets](http://www.imagesnippets.com/glam/) - Platform to links RDF descriptions to images   """
Semantic web;https://github.com/isl/XML2RDF-DataTransformation-MappingTool;"""XML2RDF-DataTransformation-MappingTool  ======================================       XML2RDF Data Transformation Tool (Mapping Tool): This generic data transformation tool maps XML data    files to RDF files, given a schema matching definition, based on this Mapping Language Schema.    This distribution contains:  The XML2RDF-DataTransformation-Mapping Tool version 3.3 bundle   	includes: source code (NetBeans project), binary, required libraries and javadocs.     COPYRIGHT (c) 2010-2012 by Institute of Computer Science,   Foundation for Research and Technology - Hellas  Contact:       POBox 1385, Heraklio Crete, GR-700 13 GREECE      Tel:+30-2810-391632      Fax: +30-2810-391638      E-mail: isl@ics.forth.gr      http://www.ics.forth.gr/isl     Authors :  Maria Koutraki, Dimitra Zografistou, Evangelia Daskalaki, Elias Zabetakis     This file is part of XML2RDF Data Transformation Tool (Mapping Tool).        XML2RDF Data Transformation Tool (Mapping Tool) is free software: you can redistribute it and/or modify      it under the terms of the GNU Lesser General Public License as published by      the Free Software Foundation, either version 3 of the License, or      (at your option) any later version.     XML2RDF Data Transformation Tool (Mapping Tool) is distributed in the hope that it will be useful,   but WITHOUT ANY WARRANTY; without even the implied warranty of   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the   GNU Lesser General Public License for more details.     You should have received a copy of the GNU Lesser General Public License   along with XML2RDF Data Transformation Tool (Mapping Tool).  If not, see <http://www.gnu.org/licenses/>.     """
Semantic web;https://github.com/AKSW/HiBISCuS;"""# HiBISCuS  HiBISCuS: Hypergraph-Based Source Selection for SPARQL Endpoint Federation    ## Source Code  The HiBISCuS source code along with all of the 3 extensions (SPLENDID, FedX, DARQ) can be checkout from project website https://github.com/AKSW/HiBISCuS/    ## FedBench  FedBench queries can be downloaded from project website https://code.google.com/p/fbench/    ## Datasets Availability  All the datasets and corresponding virtuoso SPARQL endpoints can be downloaded from the project old website https://code.google.com/p/hibiscusfederation/.       ## Usage Information  In the following we explain how one can setup the BigRDFBench evaluation framework and measure the performance of the federation engine.    ## SPARQL Endpoints Setup  The first step is to download the SPARQL endpoints (portable Virtuoso SAPRQL endpoints from second table above) on different machines, i.e., computers. Best would be one SPARQL endpoint per machine. Therefore, you need a total of 13 machines. However, you can start more than one SPARQL endpoints per machine.  The next step is to start the SPARQL endpoint from bin/start.bat (for windows) or bin/start_virtuoso.sh (for Linux). Make a list of the all SPARQL endpoints URL's ( required as input for index-free SPARQL query federation engines, i.e., FedX). It is important to note that index-assisted federation engines (e.g., SPLENDID, DARQ, ANAPSID) usually stores the endpoint URL's in its index. The local SPARQL endpoints URL's are given above in second table.  Running SPARQL Queries  Provides the list of SPARQL endpoints URL's, and a FedBench? query to the underlying federation engine. The query evaluation start-up files for the selected systems (which you can checkout from project website https://github.com/AKSW/HiBISCuS/) are given below.    ----------FedX-original-----------------------    package : package org.aksw.simba.start;    File:QueryEvaluation?.java    ----------FedX-HiBISCuS-----------------------    package : package org.aksw.simba.fedsum.startup;    File:QueryEvaluation?.java    ----------SPLENDID-original-----------------------    package : package de.uni_koblenz.west.evaluation;    File:QueryProcessingEval?.java    ----------SPLENDID-HiBISCuS-----------------------    package : package de.uni_koblenz.west.evaluation;    File:QueryProcessingEval?.java    ----------ANAPSID-----------------------    Follow the instructions given at https://github.com/anapsid/anapsid to configure the system and then use anapsid/ivan-scripts/runQuery.sh to run a query. """
Semantic web;https://github.com/kasei/SPARQLKit;"""SPARQLKit  =========    An implementation of the SPARQL 1.1 Query and Update language in Objective-C.  ---------------    This code implements a full SPARQL 1.1 Query and Update engine in Objective-C.  The design is based on trait/role-based programming, where possible  allowing for natural extensibility and component selection/replacement  (e.g. using the Raptor RDF parser and a triple-store backed by the OS X Address Book).    The code depends on the [GTWSWBase framework](https://github.com/kasei/GTWSWBase).    Plugins  -------    The system provides an extensible plugin architecture for data sources and RDF parsers.  Plugins are automatically loaded from the `Library/Application Support/SPARQLKit/PlugIns` directory.    Some example plugins include:    * [GTWSPARQLProtocolStore](https://github.com/kasei/GTWSPARQLProtocolStore) provides triplestore access to remote remote data using the [SPARQL Protocol](http://www.w3.org/TR/sparql11-protocol/)  * [GTWRedland](https://github.com/kasei/GTWRedland) provides both a [librdf](http://librdf.org) in-memory triplestore and a [Raptor](http://librdf.org/raptor/) RDF parser  * [GTWAddressBookTripleStore](https://github.com/kasei/GTWAddressBookTripleStore) provides access to a users address book contacts  * [GTWApertureTripleStore](https://github.com/kasei/GTWApertureTripleStore) provides access to photo metadata (including geographic and depiction data) from [Aperture](http://www.apple.com/aperture/) libraries  * [GTWAOF](https://github.com/kasei/GTWAOF) provides a persistent, append-only quad store      Example  -------    The `gtwsparql` tool available in this package provides a command line interface to a  full SPARQL 1.1 Query and Update environment.    ### Loading Data    ```  % gtwsparql  sparql> LOAD <http://dbpedia.org/data/Objective-C.ttl> ;  OK  sparql> SELECT (COUNT(*) AS ?count) WHERE { ?s ?p ?o }  -------------  | # | count |   -------------  | 1 | 372   |   -------------  sparql> LOAD <http://dbpedia.org/data/SPARQL.ttl> INTO GRAPH <http://example.org/SPARQL> ;  OK  sparql> SELECT * WHERE { GRAPH ?g {} }  -----------------------------------  | # | g                           |   -----------------------------------  | 1 | <http://example.org/SPARQL> |   -----------------------------------  sparql> SELECT DISTINCT ?subject WHERE { GRAPH <http://example.org/SPARQL> { ?subject ?p ?o } } ORDER BY ?subject  -----------------------------------------------------------------------------  |  # | subject                                                              |   -----------------------------------------------------------------------------  |  1 | <http://dbpedia.org/resource/SPARQL>                                 |   |  2 | <http://fa.dbpedia.org/resource/اسپارکل>                             |   |  3 | <http://zh.dbpedia.org/resource/SPARQL>                              |   |  4 | <http://de.dbpedia.org/resource/SPARQL>                              |   |  5 | <http://dbpedia.org/resource/SPARQL_Protocol_and_RDF_Query_Language> |   |  6 | <http://ar.dbpedia.org/resource/سباركل>                              |   |  7 | <http://ru.dbpedia.org/resource/SPARQL>                              |   |  8 | <http://dbpedia.org/resource/Sparq>                                  |   |  9 | <http://lv.dbpedia.org/resource/SPARQL>                              |   | 10 | <http://vi.dbpedia.org/resource/SPARQL>                              |   | 11 | <http://nl.dbpedia.org/resource/SPARQL>                              |   | 12 | <http://uk.dbpedia.org/resource/SPARQL>                              |   | 13 | <http://ja.dbpedia.org/resource/SPARQL>                              |   | 14 | <http://dbpedia.org/resource/Sparql>                                 |   | 15 | <http://it.dbpedia.org/resource/SPARQL>                              |   | 16 | <http://hu.dbpedia.org/resource/Sparql>                              |   | 17 | <http://wikidata.dbpedia.org/resource/Q54871>                        |   | 18 | <http://en.wikipedia.org/wiki/SPARQL>                                |   | 19 | <http://pl.dbpedia.org/resource/SPARQL>                              |   | 20 | <http://fr.dbpedia.org/resource/SPARQL>                              |   | 21 | <http://es.dbpedia.org/resource/SPARQL>                              |   -----------------------------------------------------------------------------  ```    ### Namespace Completion    The tool can auto-complete prefix declarations (sourced from [prefix.cc](http://prefix.cc/).  By hitting TAB immediately after a prefix name (including colon), the full prefix IRI  is added to the query string:    `sparql> PREFIX foaf:`**&lt;TAB>**    `sparql> PREFIX foaf: <http://xmlns.com/foaf/0.1/> `    ### Configuring the Data Source    The `gtwsparql` tool can take a string argument to specify the data source configuration.  In its simplest form, this is just the name of a triple- or quad-store plugin.  For example, we can query over Aperture photo metadata loaded into the default graph:    ```  % gtwsparql -s GTWApertureTripleStore  sparql> PREFIX dcterms: <http://purl.org/dc/terms/> PREFIX foaf: <http://xmlns.com/foaf/0.1/> SELECT ?place (SAMPLE(?i) AS ?image) WHERE { ?i dcterms:spatial [ foaf:name ?place ] FILTER(REGEX(?place, ""Airport"")) } GROUP BY ?place ORDER BY ?place  -----------------------------------------------------------------------------------------------------------------------------------------------------  | # | place                              | image                                                                                                    |   -----------------------------------------------------------------------------------------------------------------------------------------------------  | 1 | ""Anchorage Airport""                | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/03/27/20130327-000128/P1040654.RW2> |   | 2 | ""Cape Town Airport""                | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/07/04/20130704-222928/IMG_1931.JPG> |   | 3 | ""Genoa Cristoforo Colombo Airport"" | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/07/04/20130704-222928/IMG_1796.JPG> |   | 4 | ""Indira Gandhi Airport""            | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/05/22/20130522-235054/IMG_1752.JPG> |   | 5 | ""Or Tambo Airport""                 | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/07/04/20130704-222928/IMG_1924.JPG> |   | 6 | ""Vadodara Airport""                 | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/05/22/20130522-235054/IMG_1762.JPG> |   | 7 | ""Wellington Airport""               | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/04/10/20130410-213038/P1070371.RW2> |   -----------------------------------------------------------------------------------------------------------------------------------------------------  ```    The `SPKTripleModel` can be used to construct a dataset with multiple triplestores, each available in a separate graph.  We can query over both Aperture photo metadata and address book contacts:    ```  % gtwsparql -s '{ ""storetype"": ""SPKTripleModel"", ""graphs"": { ""tag:addressbook"": { ""storetype"": ""GTWAddressBookTripleStore"" }, ""tag:aperture"": { ""storetype"": ""GTWApertureTripleStore"" } } }'  sparql> SELECT * WHERE { GRAPH ?g {} }  -------------------------  | # | g                 |   -------------------------  | 1 | <tag:addressbook> |   | 2 | <tag:aperture>    |   -------------------------  ```    This allows us to find the number of photos depicting members of the same family by combining depiction data from Aperture with family name data from the address book (by constructing a query dataset using the `FROM` keyword):    ```  sparql> PREFIX foaf: <http://xmlns.com/foaf/0.1/> SELECT ?family (COUNT(*) AS ?count) FROM <tag:addressbook> FROM <tag:aperture> WHERE { ?image a foaf:Image ; foaf:depicts [ foaf:familyName ?family ] } GROUP BY ?family ORDER BY ?count  ----------------------------------  | # | count | family             |   ----------------------------------  | 1 | 1     | ""Kjernsmo""         |   | 2 | 6     | ""Heath""            |   | 3 | 14    | ""Brickley""         |   | 4 | 25    | ""Aastrand Grimnes"" |   | 5 | 104   | ""Acton""            |   | 6 | 116   | ""Gillis""           |   | 7 | 504   | ""Crawford""         |   | 8 | 2029  | ""Williams""         |   ----------------------------------  ```    ### Starting an Endpoint    A SPARQL endpoint can easily be started:    ```  sparql> endpoint 8080  Endpoint started on port 8080  ```    At this point, `http://localhost:8080/sparql` is a [SPARQL Protocol](http://www.w3.org/TR/sparql11-protocol/) endpoint URL that will respond to queries. """
Semantic web;https://github.com/UMKC-BigDataLab/RIQ;"""# `RIQ`: RDF Indexing on Quadruples    ## Summary    `RIQ` is a new approach for fast processing of SPARQL queries on large  datasets containing RDF quadruples (or quads).   (These queries are also called named graph queries.)  `RIQ` employs a *decrease-and-conquer*  strategy: Rather than indexing the entire RDF dataset, `RIQ` identifies  groups of similar RDF graphs and indexes each group separately. During  query processing, `RIQ` uses novel filtering index to first identify  candidate groups that may contain matches for the query. On these  candidates, it executes optimized queries using a conventional SPARQL  processor (e.g., Jena TDB) to produce the final results.    ## Publications    * Anas Katib, Praveen Rao, Vasil Slavov. ``[A Tool for Efficiently Processing SPARQL Queries on RDF Quads](http://ceur-ws.org/Vol-1963/paper472.pdf)."" In the 16th International Semantic Web Conference (ISWC 2017), 4 pages, Austria, Vienna, October 2017. (demo)    * Anas Katib, Vasil Slavov, Praveen Rao. ``[RIQ: Fast Processing of SPARQL Queries on RDF Quadruples](http://dx.doi.org/10.1016/j.websem.2016.03.005)."" In the Journal of Web Semantics (JWS), Vol. 37, pages 90-111, March 2016. (Elsevier)     * Vasil Slavov, Anas Katib, Praveen Rao, Vinutha Nuchimaniyanda. ""Fast Processing of SPARQL Queries on RDF Quadruples."" The 8th IEEE Symposium on Computational Intelligence for Security and Defense Applications (CISDA 2015), Verona, NY, May 2015. (poster)    * Vasil Slavov, Anas Katib, Praveen Rao, Srivenu Paturi, Dinesh Barenkala. ``[Fast Processing of SPARQL Queries on RDF Quadruples](http://arxiv.org/pdf/1506.01333v1.pdf)."" [*Proceedings of the 17th International Workshop on the Web and Databases*](http://webdb2014.eecs.umich.edu/) (**WebDB 2014**), Snowbird, UT, June 2014.    ## Contributors    ***Faculty:*** Praveen Rao (PI)    ***PhD Students:*** Vasil Slavov, Anas Katib    ***MS Students:*** Srivenu Paturi, Dinesh Barenkala, Vinutha Nuchimaniyanda    ## Acknowledgments    This work was supported by the National Science Foundation under Grant Nos. 1115871 and 1620023. """
Semantic web;https://github.com/AKSW/QuitDiff;"""# Quit Diff    ## Requirements    For using QuitDiff you need to have python version 3 installed.    To install the required packages use pip:        pip install -r requirements.txt    ## Use as `git-difftool`    Add one of the following sections to you `~/.gitconfig` (in your home directory) or `.git/config` (in your git working directory).        [difftool ""quitdiff""]          cmd = quitdiff.py --local=\""$LOCAL\"" --remote=\""$REMOTE\"" --merged=\""$MERGES\"" --base=\""$BASE\""          [difftool ""quitdiff-sparql""]          cmd = quitdiff.py --diffFormat sparql --local=\""$LOCAL\"" --remote=\""$REMOTE\"" --merged=\""$MERGES\"" --base=\""$BASE\""        [difftool ""quitdiff-eccrev""]          cmd = quitdiff.py --diffFormat eccrev --local=\""$LOCAL\"" --remote=\""$REMOTE\"" --merged=\""$MERGES\"" --base=\""$BASE\""    The git diff tool can then called with one of the following commands        $ git difftool -t quitdiff      $ git difftool -t quitdiff HEAD~0..HEAD~2    ## Use as `git-diff`    Add the following sections to you `~/.gitconfig` (in your home directory) or `.git/config` (in your git working directory).        [diff ""quitdiff""]          command = quitdiff.py    and the following to `.gitattributes`  (in your git working directory).        *.nq diff=quitdiff      *.trig diff=quitdiff      *.nt diff=quitdiff      *.ttl diff=quitdiff      *.rdf diff=quitdiff    git diff can then called with one of the following commands        $ git diff      $ git diff HEAD~0..HEAD~2      # Command line parameters  This tool can be used for git-diff or as git-difftool    ## git-diff:    if using as git-diff, the parameters are: `path old-file old-hex old-mode new-file new-hex new-mode`    ## git-difftool:  https://git-scm.com/docs/git-difftool  * $LOCAL is set to the name of the temporary file containing the contents of the diff pre-image and  * $REMOTE is set to the name of the temporary file containing the contents of the diff post-image.  * $MERGED is the name of the file which is being compared.  * $BASE is provided for compatibility with custom merge tool commands and has the same value as $MERGED.    * local is the old version  * remote is the new version    # License    Copyright (C) 2017 Natanael Arndt <http://aksw.org/NatanaelArndt> and Norman Radtke <http://aksw.org/NormanRadtke>    This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.    This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.    You should have received a copy of the GNU General Public License along with this program; if not, see <http://www.gnu.org/licenses>.  Please see [LICENSE](LICENSE.txt) for further information. """
Semantic web;https://github.com/lorenae/qb4olap-tools;"""QB4OLAP tools    [QB4OLAP] (http://lorenae.github.io/qb4olap/) is a RDFS Vocabulary for Business Intelligence over Linked Data.    In this project we include a set of prototypes that conform a showcase of what can be done using this vocabulary, focusing on exploring and querying.    For querying we propose a high level OLAP language, called QL, which consists on a set of well-known operators: `rollup, drilldown, slice,` and `dice`. Using the cube metadata, also written using QB4OLAP, we automatically generate SPARQL queries to implement sequences of QL operations.      INSTALLATION    1) download [zip] (https://github.com/lorenae/qb4olap-tools/archive/master.zip) file and extract it     2) install [nodejs] (https://nodejs.org/en/)    3) open console, go to qb4olap-tools directory (obtained in step 1)    4) install npm packages needed by the application  (`npm install` command, which install the packages liste in packages.json)    5) run the application  (`node qb4olap.js`)!!   """
Semantic web;https://github.com/factsmission/psps;"""# Personal Structured Publishing Space    Create a linked data site serving RDF data from files in a GitHub repository. For example the [FactsMission Website](https://factsmission.com/) is generated by PSPS from the data in the repository at https://github.com/factsmission/website. All RDF data from your GitHub repository will also be accessible via SPARQL.    ## How to use it?    - Add a BASEURI file to the root of your repo with the base URI of your data (see below)  - Start an instance of  PSPS  - Add a webhook in Github notifying `http(s)://<your-host>/webhook` with the set webhook secret (see below)  - Add RDF data to your repository  - To customize the (client-side) rendering of the resource add a `renderes.ttl`file to the root of your repository. See the [RDF2h-Documentation](https://rdf2h.github.io/rdf2h-documentation/) to learn how the rendering works    ### Specifying base URI    The file BASEURI in the root of the repository can either directly contain the base URI for the branch containg the file or a JSON object with branch names as keys and base URIs as values.  Having such a JSON is handy as it allows to have staging branches with proposed modifications differing from the main branch only in the proposed change and not also in the BASEURI file.      ## Building        docker-compose build    ## Starting    You need to get a GitHub personal access token. You can generate one under [ Account Settings / Developer settings / Personal Access tokens](https://github.com/settings/tokens)    On Unix         GITHUB_TOKEN=""YOUR TOKEN HERE""; WEBHOOK_SECRET=""THE WEBHOOK SECRET""; docker-compose up    On windows         $env:GITHUB_TOKEN = ""YOUR TOKEN HERE""       $env:WEBHOOK_SECRET=""THE WEBHOOK SECRET""       docker-compose up    On [Rancher](https://rancher.com/)     * Add Stack for psps   * Configure using the file [docker-compose-no-build.yml](docker-compose-no-build.yml), set GITHUB_TOKEN to you GitHub Personal Access Token and WEBHOOK_SECRET to the desired    webhook secret.    ## Setting up the webhook    PSPS will download the data from any GitHub repository that send a requests to the webhook. This means that everybody that knows your webhook secret can publish to your PSPS instance!    Add a Webhook under *Project Settings / Webhooks*, the Payload URL is `http(s)://<your-host>/webhook`, as Content type choose application/json, PSPS only needs to be notified on `push` events.    ## What's powering PSPS?    PSPS puts together different pieces of software to provide its functionality.    ### Apache Jena Fuseki    [Apache Jena Fuseki](https://jena.apache.org/documentation/fuseki2/) is a SPARQL Server providing SPARQL 1.1 protocols. By default PSPS doesn't fully expose the Fuseki interface. However SPARQL queries sent to `http(s)://<your-host>/sparql` are forwarded to Fuseki.    ### TLDS / SLDS    The linked data site is provided by [SLDS](https://github.com/linked-solutions/slds) respectively its ""templating"" extension [TLDS](https://github.com/linked-solutions/tlds).    ### Apache Clerezza    [Apache Clerezza](http://clerezza.apache.org/) provides the RDF API and Toolkit used in SLDS, TLDS as well as PSPS itsef.    ### RDF2h / LD2h    The templating mechanism introduced by TLDS bases on [RDF2h](https://github.com/rdf2h/rdf2h) which allows defining renderers in RDF. It uses [LD2h](https://github.com/rdf2h/ld2h) to integrate RDF2h in HTML. LD2h also allows including remote resources alongside the resources originating from the data in the repository."""
Semantic web;https://github.com/alangrafu/lodspeakr;"""LODSPeaKr  =========    author: Alvaro Graves (alvaro@graves.cl)    version: 20130612      [LODSPeaKr](http://lodspeakr.org) is a framework for creating Linked Data applications in a simple and easy way. You can see [several applications](http://alangrafu.github.com/lodspeakr/applications.html) created using LODSPeaKr.    Simplest Installation  ---------------------    Simply go to your web server root directory (e.g., /var/www/) and run       bash < <(curl -sL http://lodspeakr.org/install)    You will be required to answer 3 questions:    * What is the location of lodspeakr? If you are running the script in `/var/www/visualizations` it is highly likely it will be `http://localhost/visualizations` or `http://yourdomainname/visualizations`  * What is the domain of the data you want to query? For now, you can leave it as the default (i.e., press Enter)  * What is the URL of your SPARQL endpoint? Where should Visualbox look to execute SPARQL queries.    Finally, give write permissions to the web server in `lodspeakr/meta`, `lodspeakr/cache`, `lodspeakr/settings.inc.php`  and `lodspeakr/components`. This can be done in several ways:    * `sudo chown WEBSERVERUSER lodspeakr/meta lodspeakr/cache lodspeakr/settings.inc.php lodspeakr/components`      * **Note** You can find the name of your web server user by running `ps aux|egrep ""apache|httpd|www"" |egrep -v ""grep|root""|awk '{print $1}'|uniq`  * Alternatively you can run `chdmod -R 777 lodspeakr/meta lodspeakr/cache lodspeakr/settings.inc.php lodspeakr/components` but this is highly discouraged    More documentation on installation of LODSPeaKr is available at the [LODSPeaKr wiki](https://github.com/alangrafu/lodspeakr/wiki) """
Semantic web;https://github.com/agazzarini/SolRDF;"""<p><img src=""https://cloud.githubusercontent.com/assets/7569632/7524584/5971e1ba-f503-11e4-940e-72e808677c48.png"" width=""100"" height=""100""/>  <img src=""https://cloud.githubusercontent.com/assets/7569632/7532363/51104a30-f566-11e4-8481-229f64064905.png"">  </p>  <br/>  SolRDF (i.e. Solr + RDF) is a set of Solr extensions for managing (index and search) RDF data. Join us at solrdf-user-list@googlegroups.com    [![Continuous Integration status](https://travis-ci.org/agazzarini/SolRDF.svg?branch=master)](https://travis-ci.org/agazzarini/SolRDF)    # Get me up and running  This section provides instructions for running SolRDF. We divided the section in two different parts because the different architecture introduced with Solr 5. Prior to that (i.e. Solr 4.x) Solr was distributed as a JEE web application and therefore, being SolRDF a Maven project, you could use Maven for starting up a live instance without downloading Solr (Maven would do that for you, behind the scenes).     Solr 5.x is now delivered as a standalone jar and therefore the SolRDF installation is different; it requires some manual steps in order to deploy configuration files and libraries within an external Solr (which needs to be downloaded separately).        ### SolRDF 1.1 (Solr 5.x)  First, you need Java 8, Apache Maven and Apache Solr installed on your machine.  Open a new shell and type the following:         ```  # cd /tmp  # git clone https://github.com/agazzarini/SolRDF.git solrdf-download  ```    #### Build and run SolrRDF        ```  # cd solrdf-download/solrdf  # mvn clean install    ```  At the end of the build, after seeing    ```  [INFO] --------------------------------------------------------  [INFO] Reactor Summary:  [INFO]   [INFO] Solr RDF plugin .................... SUCCESS [  3.151 s]  [INFO] solrdf-core ........................ SUCCESS [ 10.191 s]  [INFO] solrdf-client ...................... SUCCESS [  3.554 s]  [INFO] solrdf-integration-tests ........... SUCCESS [ 14.910 s]  [INFO] --------------------------------------------------------  [INFO] BUILD SUCCESS  [INFO] --------------------------------------------------------    [INFO] Total time: 32.065 s  [INFO] Finished at: 2015-10-20T14:42:09+01:00  [INFO] Final Memory: 43M/360M  ```    you can find the solr-home directory, with everything required for running SolRDF, under the     ```  /tmp/solr/solrdf-download/solrdf/solrdf-integration-tests/target/solrdf-integration-tests-1.1-dev/solrdf  ```  We refer to this directory as $SOLR_HOME.   At this point, open a shell under the _bin_ folder of your Solr and type:    ```  > ./solr -p 8080 -s $SOLR_HOME -a ""-Dsolr.data.dir=/work/data/solrdf""    Waiting to see Solr listening on port 8080 [/]    Started Solr server on port 8080 (pid=10934). Happy searching!    ```    ### SolRDF 1.0 (Solr 4.x)  If you're using Solr 4.x, you can point to the solrdf-1.0 branch and use the automatic procedure described below for downloading, installing and run it. There's no need to download Solr, as Maven will do that for you.    #### Checkout the project      Open a new shell and type the following:       ```  # cd /tmp  # git clone https://github.com/agazzarini/SolRDF.git solrdf-download  ```      #### Build and run SolrRDF        ```  # cd solrdf-download/solrdf  # mvn clean install  # cd solrdf-integration-tests  # mvn clean package cargo:run  ```  The very first time you run this command a lot of things will be downloaded, Solr included.  At the end you should see something like this:  ```  [INFO] Jetty 7.6.15.v20140411 Embedded started on port [8080]  [INFO] Press Ctrl-C to stop the container...  ```   [SolRDF](http://127.0.0.1:8080/solr/#/store) is up and running!     # Add data     Now let's add some data. You can do that in one of the following ways:     ## Browser  Open your favourite browser and type the follwing URL (line has been split for readability):  ```  http://localhost:8080/solr/store/update/bulk?commit=true  &update.contentType=application/n-triples  &stream.file=/tmp/solrdf-download/solrdf/solrdf-integration-tests/src/test/resources/sample-data/bsbm-generated-dataset.nt  ```  This is an example with the bundled sample data. If you have a file somehere (i.e. remotely) you can use the _stream.url_ parameter to indicate the file URL. For example:      ```  http://localhost:8080/solr/store/update/bulk?commit=true  &update.contentType=application/rdf%2Bxml  &stream.url=http://ec.europa.eu/eurostat/ramon/rdfdata/countries.rdf  ```  ## Command line  Open a shell and type the following  ```  # curl -v http://localhost:8080/solr/store/update/bulk?commit=true \     -H ""Content-Type: application/n-triples"" \    --data-binary @/tmp/solrdf-download/solrdf/solrdf-integration-tests/src/test/resources/sample_data/bsbm-generated-dataset.nt  ```  Ok, you just added (about) [5000 triples](http://127.0.0.1:8080/solr/#/store).     # SPARQL 1.1. endpoint      SolRDF is a fully compliant SPARQL 1.1. endpoint. In order to issue a query just run a query like this:  ```  # curl ""http://127.0.0.1:8080/solr/store/sparql"" \    --data-urlencode ""q=SELECT * WHERE { ?s ?p ?o } LIMIT 10"" \    -H ""Accept: application/sparql-results+json""      Or        # curl ""http://127.0.0.1:8080/solr/store/sparql"" \    --data-urlencode ""**q=SELECT * WHERE { ?s ?p ?o } LIMIT 10**"" \    -H ""Accept: application/sparql-results+xml""    ```    -----------------------------------    _The SolRDF logo was kindly provided by [Umberto Basili](https://it.linkedin.com/in/umberto-basili-14a6a8b1)_  """
Semantic web;https://github.com/rdf-ext/rdf-ext;"""# RDF-Ext    [![Build Status](https://img.shields.io/github/workflow/status/rdf-ext/rdf-ext/CI)](https://github.com/rdf-ext/rdf-ext/actions/workflows/ci.yaml)    [![npm version](https://img.shields.io/npm/v/rdf-ext.svg)](https://www.npmjs.com/package/rdf-ext)    RDF-Ext is a JavaScript library that extends the [RDF/JS](#rdf-js) specs to handle RDF data in a developer-friendly way.    For more details, please check [rdf-ext.org](https://rdf-ext.org/) """
Semantic web;https://github.com/cygri/prefix.cc;"""# prefix.cc Source Code    This is the source code to the [prefix.cc](http://prefix.cc/) web site  operated by Richard Cyganiak ([richard@cyganiak.de](mailto:richard@cyganiak.de), [@cygri](http://twitter.com/cygri)).      ## Requirements    * Apache with `mod_rewrite` enabled  * PHP (recommended version: 7.2)  * MySQL      ## Setting up a test site    1. Clone the repository into a local directory    2. Set up a virtual host `prefixcc.local` with that directory as document root    3. Create a new MySQL database:            echo CREATE DATABASE prefixcc | mysql -u root     4. Set up database tables:            mysql -u root prefixcc < db_schema/schema.sql    5. Make a copy of the default configuration:            cp default.config.php config.php    6. Edit database credentials in `config.php` if necessary    7. Import prefixes from the public prefix.cc site:            php tools/csv-import.php http://prefix.cc/popular/all.file.csv | mysql -u root prefixcc    8. Go to [http://prefixcc.local/](http://prefixcc.local/) and you should have a functional site! """
Semantic web;https://github.com/AtomGraph/Web-Client;"""AtomGraph Web-Client is a Linked Data web client. If you have a triplestore with RDF data that you want to publish  and/or build an end-user application on it, or would like to explore Linked Open Data, Web-Client provides the components you need.    Web-Client renders (X)HTML user interface by transforming [""plain"" RDF/XML](https://jena.apache.org/documentation/io/rdf-output.html#rdfxml) (without nested resource descriptions)  using [XSLT 2.0](https://www.w3.org/TR/xslt20/) stylesheets.    ![AtomGraph Web-Client screenshot](https://raw.github.com/AtomGraph/Web-Client/master/screenshot.png)    Features  ========    What AWC provides for users as out-of-the-box generic features:  * loading RDF data from remote Linked Data sources  * multilingual, responsive user interface built with Twitter Bootstrap (currently [2.3.2](https://getbootstrap.com/2.3.2/))  * multiple RDF rendering modes (currently item/list/table/map)  * RDF editing mode based on [RDF/POST](http://www.lsrn.org/semweb/rdfpost.html) encoding  * SPARQL endpoint with interactive results    Getting started  ===============    * [what is Linked Data](https://github.com/AtomGraph/Web-Client/wiki/What-is-Linked-Data)  * [installing Web-Client](https://github.com/AtomGraph/Web-Client/wiki/Installation)  * [extending Web-Client](https://github.com/AtomGraph/Web-Client/wiki/Extending-Web-Client)    For full documentation, see the [wiki index](https://github.com/AtomGraph/Web-Client/wiki).    Usage  =====    Docker  ------    Processor is available from Docker Hub as [`atomgraph/web-client`](https://hub.docker.com/r/atomgraph/web-client/) image.  It accepts the following environment variables (that become webapp context parameters):    <dl>      <dt><code>STYLESHEET</code></dt>      <dd>Custom XSLT stylesheet</dd>      <dd>URI, optional</dd>      <dt><code>RESOLVING_UNCACHED</code></dt>      <dd>If <code>true</code>, the stylesheet will attempt to resolve (dereference) URI resources in the rendered RDF data to improve the UX</dd>      <dd><code>true</code>/<code>false</code>, optional</dd>  </dl>    Run Web-Client with the [default XSLT stylesheet](https://github.com/AtomGraph/Web-Client/blob/master/src/main/webapp/static/com/atomgraph/client/xsl/bootstrap/2.3.2/layout.xsl) like this:        docker run -p 8080:8080 atomgraph/web-client    Web-Client will be available on [http://localhost:8080](http://localhost:8080).    Maven  -----    Web-Client is released on Maven central as [`com.atomgraph:client`](https://search.maven.org/artifact/com.atomgraph/client/).    Support  =======    Please [report issues](https://github.com/AtomGraph/Web-Client/issues) if you've encountered a bug or have a feature request.    Commercial AtomGraph consulting, development, and support are available from [AtomGraph](https://atomgraph.com).    Community  =========    Please join the W3C [Declarative Linked Data Apps Community Group](http://www.w3.org/community/declarative-apps/) to discuss  and develop AtomGraph and declarative Linked Data architecture in general. """
Semantic web;https://github.com/knakk/rdf2rdf;"""## rdf2rdf  CLI tool to convert between different RDF serialization formats.    Primarly made to test and showcase the capabilites of the [rdf package](https://github.com/knakk/rdf).    ## Status    Currently supported input formats: RDF/XML, N-Triples, N-Quads, Turtle.    Currently supported output formats: N-Triples, Turtle.    More formats are coming soon.    ## Installation  Install as you would any other Go package:        go get -u github.com/knakk/rdf2rdf    Provided that `GOPATH/bin` is on your `PATH`, you're good to go.    When the tool has proven stable and complete, I can provide binaries for the most common OS'es for download.    ## Usage  <pre>  rdf2rdf  -------  Convert between different RDF serialization formats.    Usage:  	rdf2rdf -in=input.xml -out=output.ttl    Options:    -h --help      Show this message.    -in            Input file.    -out           Output file.    -stream=true   Streaming mode.    -v=false       Verbose mode (shows progress indicator)    By default the converter is streaming both input and output, emitting  converted triples/quads as soon as they are available. This ensures you can  convert huge files with minimum memory footprint. However, if you have  small datasets you can choose to load all data into memory before conversion.  This makes it possible to sort the data, remove duplicate triples, and  potentially generate more compact Turtle serializations, maximizing predicate  and object lists. Do this by setting the flag stream=false.    Conversion from a quad-format to a triple-format will disregard the triple's  context (graph). Conversion from a triple-format to a quad-format is not  supported.    Input and ouput formats are determined by file extensions, according to  the following table:      Format    | File extension    ----------|-------------------    N-Triples | .nt    N-Quads   | .nq    RDF/XML   | .rdf .rdfxml .xml    Turtle    | .ttl    </pre>"""
Semantic web;https://github.com/AKSW/OntoWiki;"""# OntoWiki    [![Build Status](http://owdev.ontowiki.net/job/OntoWiki/badge/icon)](http://owdev.ontowiki.net/job/OntoWiki/)  [API Documentation](http://api.ontowiki.net/)    ![](https://raw.github.com/wiki/AKSW/OntoWiki/images/owHeader.png)    ## Introduction    is a tool providing support for agile, distributed knowledge engineering scenarios.  OntoWiki facilitates the visual presentation of a knowledge base as an information map, with different views on instance data.  It enables intuitive authoring of semantic content.  It fosters social collaboration aspects by keeping track of changes, allowing to comment and discuss every single part of a knowledge base.    Other remarkable features are:    * OntoWiki is a Linked Data Server for you data as well as a Linked Data client to fetch additional data from the web  * OntoWiki is a Semantic Pingback Client in order to receive and send back-linking request as known from the blogosphere.  * OntoWiki is backend independent, which means you can save your data on a MySQL database as well as on a Virtuoso Triple Store.  * OntoWiki is easily extendible by you, since it features a sophisticated Extension System.    ## Installation/Update    If you are updating OntoWiki, please don't forget to run `make deploy`.  If `make deploy` fails, you might also have to run `make getcomposer` once before run `make deploy` again.    For further installation instructions please have a look at our [wiki](https://github.com/AKSW/OntoWiki/wiki/GetOntowikiUsers) (might be outdated in some parts).    ## Screenshot / Webinar  Below is a screenshot showing OntoWiki in editing mode.    For a longer visual presentation you can watch our [webinar@youtube](http://www.youtube.com/watch?v=vP1UDKeZsQk)  (thanks to Phil and the Semantic Web company).    ![Screenshot](http://lh4.ggpht.com/-kXpKMqBBCIU/Tpx45SUaItI/AAAAAAAAA9w/aPYaNQjcpvo/s800/ontowiki.png)    ## License    OntoWiki is licensed under the [GNU General Public License Version 2, June 1991](http://www.gnu.org/licenses/gpl-2.0.txt) (license document is in the application subfolder). """
Semantic web;https://github.com/Data-Liberation-Front/csvlint.rb;"""[![Build Status](http://img.shields.io/travis/theodi/csvlint.rb.svg)](https://travis-ci.org/theodi/csvlint.rb)  [![Dependency Status](http://img.shields.io/gemnasium/theodi/csvlint.rb.svg)](https://gemnasium.com/theodi/csvlint.rb)  [![Coverage Status](http://img.shields.io/coveralls/theodi/csvlint.rb.svg)](https://coveralls.io/r/theodi/csvlint.rb)  [![License](http://img.shields.io/:license-mit-blue.svg)](http://theodi.mit-license.org)  [![Badges](http://img.shields.io/:badges-5/5-ff6799.svg)](https://github.com/pikesley/badger)    # CSV Lint    A ruby gem to support validating CSV files to check their syntax and contents. You can either use this gem within your own Ruby code, or as a standalone command line application    ## Summary of features    * Validation that checks the structural formatting of a CSV file    * Validation of a delimiter-separated values (dsv) file accesible via URL, File, or an IO-style object (e.g. StringIO)  * Validation against [CSV dialects](http://dataprotocols.org/csv-dialect/)    * Validation against multiple schema standards; [JSON Table Schema](https://github.com/theodi/csvlint.rb/blob/master/README.md#json-table-schema-support) and [CSV on the Web](https://github.com/theodi/csvlint.rb/blob/master/README.md#csv-on-the-web-validation-support)     ## Development    `ruby version 2.1.4`    ### Tests    The codebase includes both rspec and cucumber tests, which can be run together using:        $ rake    or separately:        $ rake spec      $ rake features    When the cucumber tests are first run, a script will create tests based on the latest version of the [CSV on the Web test suite](http://w3c.github.io/csvw/tests/), including creating a local cache of the test files. This requires an internet connection and some patience. Following that download, the tests will run locally; there's also a batch script:        $ bin/run-csvw-tests    which will run the tests from the command line.    If you need to refresh the CSV on the Web tests:        $ rm bin/run-csvw-tests      $ rm features/csvw_validation_tests.feature      $ rm -r features/fixtures/csvw    and then run the cucumber tests again or:        $ ruby features/support/load_tests.rb      ## Installation    Add this line to your application's Gemfile:        gem 'csvlint'    And then execute:        $ bundle    Or install it yourself as:        $ gem install csvlint    ## Usage    You can either use this gem within your own Ruby code, or as a standalone command line application    ## On the command line    After installing the gem, you can validate a CSV on the command line like so:    	csvlint myfile.csv    You may need to add the gem exectuable directory to your path, by adding '/usr/local/lib/ruby/gems/2.6.0/bin'   or whatever your version is, to your .bash_profile PATH entry. [like so](https://stackoverflow.com/questions/2392293/ruby-gems-returns-command-not-found)    You will then see the validation result, together with any warnings or errors e.g.    ```  myfile.csv is INVALID  1. blank_rows. Row: 3  1. title_row.  2. inconsistent_values. Column: 14  ```    You can also optionally pass a schema file like so:    	csvlint myfile.csv --schema=schema.json    ## In your own Ruby code    Currently the gem supports retrieving a CSV accessible from a URL, File, or an IO-style object (e.g. StringIO)    	require 'csvlint'    	validator = Csvlint::Validator.new( ""http://example.org/data.csv"" )  	validator = Csvlint::Validator.new( File.new(""/path/to/my/data.csv"" ))  	validator = Csvlint::Validator.new( StringIO.new( my_data_in_a_string ) )    When validating from a URL the range of errors and warnings is wider as the library will also check HTTP headers for  best practices    	#invoke the validation  	validator.validate    	#check validation status  	validator.valid?    	#access array of errors, each is an Csvlint::ErrorMessage object  	validator.errors    	#access array of warnings  	validator.warnings    	#access array of information messages  	validator.info_messages    	#get some information about the CSV file that was validated  	validator.encoding  	validator.content_type  	validator.extension  	validator.row_count    	#retrieve HTTP headers from request  	validator.headers    ## Controlling CSV Parsing    The validator supports configuration of the [CSV Dialect](http://dataprotocols.org/csv-dialect/) used in a data file. This is specified by  passing a dialect hash to the constructor:        dialect = {      	""header"" => true,      	""delimiter"" => "",""      }  	validator = Csvlint::Validator.new( ""http://example.org/data.csv"", dialect )    The options should be a Hash that conforms to the [CSV Dialect](http://dataprotocols.org/csv-dialect/) JSON structure.    While these options configure the parser to correctly process the file, the validator will still raise errors or warnings for CSV  structure that it considers to be invalid, e.g. a missing header or different delimiters.    Note that the parser will also check for a `header` parameter on the `Content-Type` header returned when fetching a remote CSV file. As  specified in [RFC 4180](http://www.ietf.org/rfc/rfc4180.txt) the values for this can be `present` and `absent`, e.g:    	Content-Type: text/csv; header=present    ## Error Reporting    The validator provides feedback on a validation result using instances of `Csvlint::ErrorMessage`. Errors are divided into errors, warnings and information  messages. A validation attempt is successful if there are no errors.    Messages provide context including:    * `category` has a symbol that indicates the category or error/warning: `:structure` (well-formedness issues), `:schema` (schema validation), `:context` (publishing metadata, e.g. content type)  * `type` has a symbol that indicates the type of error or warning being reported  * `row` holds the line number of the problem  * `column` holds the column number of the issue  * `content` holds the contents of the row that generated the error or warning    ## Errors    The following types of error can be reported:    * `:wrong_content_type` -- content type is not `text/csv`  * `:ragged_rows` -- row has a different number of columns (than the first row in the file)  * `:blank_rows` -- completely empty row, e.g. blank line or a line where all column values are empty  * `:invalid_encoding` -- encoding error when parsing row, e.g. because of invalid characters  * `:not_found` -- HTTP 404 error when retrieving the data  * `:stray_quote` -- missing or stray quote  * `:unclosed_quote` -- unclosed quoted field  * `:whitespace` -- a quoted column has leading or trailing whitespace  * `:line_breaks` -- line breaks were inconsistent or incorrectly specified    ## Warnings    The following types of warning can be reported:    * `:no_encoding` -- the `Content-Type` header returned in the HTTP request does not have a `charset` parameter  * `:encoding` -- the character set is not UTF-8  * `:no_content_type` -- file is being served without a `Content-Type` header  * `:excel` -- no `Content-Type` header and the file extension is `.xls`  * `:check_options` -- CSV file appears to contain only a single column  * `:inconsistent_values` -- inconsistent values in the same column. Reported if <90% of values seem to have same data type (either numeric or alphanumeric including punctuation)  * `:empty_column_name` -- a column in the CSV header has an empty name  * `:duplicate_column_name` -- a column in the CSV header has a duplicate name  * `:title_row` -- if there appears to be a title field in the first row of the CSV    ## Information Messages    There are also information messages available:    * `:nonrfc_line_breaks` -- uses non-CRLF line breaks, so doesn't conform to RFC4180.  * `:assumed_header` -- the validator has assumed that a header is present    ## Schema Validation    The library supports validating data against a schema. A schema configuration can be provided as a Hash or parsed from JSON. The structure currently  follows JSON Table Schema with some extensions and rudinmentary [CSV on the Web Metadata](http://www.w3.org/TR/tabular-metadata/).    An example JSON Table Schema schema file is:    	{  		""fields"": [  			{  				""name"": ""id"",  				""constraints"": {  					""required"": true,  					""type"": ""http://www.w3.org/TR/xmlschema-2/#integer""  				}  			},  			{  				""name"": ""price"",  				""constraints"": {  					""required"": true,  					""minLength"": 1   				}  			},  			{  				""name"": ""postcode"",  				""constraints"": {  					""required"": true,  					""pattern"": ""[A-Z]{1,2}[0-9][0-9A-Z]? ?[0-9][A-Z]{2}""  				}  			}  		]  	}    An equivalent CSV on the Web Metadata file is:    	{  		""@context"": ""http://www.w3.org/ns/csvw"",  		""url"": ""http://example.com/example1.csv"",  		""tableSchema"": {  			""columns"": [  				{  					""name"": ""id"",  					""required"": true,  					""datatype"": { ""base"": ""integer"" }  				},  				{  					""name"": ""price"",  					""required"": true,  					""datatype"": { ""base"": ""string"", ""minLength"": 1 }  				},  				{  					""name"": ""postcode"",  					""required"": true  				}  			]  		}  	}    Parsing and validating with a schema (of either kind):    	schema = Csvlint::Schema.load_from_json(uri)  	validator = Csvlint::Validator.new( ""http://example.org/data.csv"", nil, schema )    ### CSV on the Web Validation Support    This gem passes all the validation tests in the [official CSV on the Web test suite](http://w3c.github.io/csvw/tests/) (though there might still be errors or parts of the [CSV on the Web standard](http://www.w3.org/TR/tabular-metadata/) that aren't tested by that test suite).    ### JSON Table Schema Support    Supported constraints:    * `required` -- there must be a value for this field in every row  * `unique` -- the values in every row should be unique  * `minLength` -- minimum number of characters in the value  * `maxLength` -- maximum number of characters in the value  * `pattern` -- values must match the provided regular expression  * `type` -- specifies an XML Schema data type. Values of the column must be a valid value for that type  * `minimum` -- specify a minimum range for values, the value will be parsed as specified by `type`  * `maximum` -- specify a maximum range for values, the value will be parsed as specified by `type`  * `datePattern` -- specify a `strftime` compatible date pattern to be used when parsing date values and min/max constraints    Supported data types (this is still a work in progress):    * String -- `http://www.w3.org/2001/XMLSchema#string` (effectively a no-op)  * Integer -- `http://www.w3.org/2001/XMLSchema#integer` or `http://www.w3.org/2001/XMLSchema#int`  * Float -- `http://www.w3.org/2001/XMLSchema#float`  * Double -- `http://www.w3.org/2001/XMLSchema#double`  * URI -- `http://www.w3.org/2001/XMLSchema#anyURI`  * Boolean -- `http://www.w3.org/2001/XMLSchema#boolean`  * Non Positive Integer -- `http://www.w3.org/2001/XMLSchema#nonPositiveInteger`  * Positive Integer -- `http://www.w3.org/2001/XMLSchema#positiveInteger`  * Non Negative Integer -- `http://www.w3.org/2001/XMLSchema#nonNegativeInteger`  * Negative Integer -- `http://www.w3.org/2001/XMLSchema#negativeInteger`  * Date -- `http://www.w3.org/2001/XMLSchema#date`  * Date Time -- `http://www.w3.org/2001/XMLSchema#dateTime`  * Year -- `http://www.w3.org/2001/XMLSchema#gYear`  * Year Month -- `http://www.w3.org/2001/XMLSchema#gYearMonth`  * Time -- `http://www.w3.org/2001/XMLSchema#time`    Use of an unknown data type will result in the column failing to validate.    Schema validation provides some additional types of error and warning messages:    * `:missing_value` (error) -- a column marked as `required` in the schema has no value  * `:min_length` (error) -- a column with a `minLength` constraint has a value that is too short  * `:max_length` (error) -- a column with a `maxLength` constraint has a value that is too long  * `:pattern` (error) --  a column with a `pattern` constraint has a value that doesn't match the regular expression  * `:malformed_header` (warning) -- the header in the CSV doesn't match the schema  * `:missing_column` (warning) -- a row in the CSV file has a missing column, that is specified in the schema. This is a warning only, as it may be legitimate  * `:extra_column` (warning) -- a row in the CSV file has extra column.  * `:unique` (error) -- a column with a `unique` constraint contains non-unique values  * `:below_minimum` (error) -- a column with a `minimum` constraint contains a value that is below the minimum  * `:above_maximum` (error) -- a column with a `maximum` constraint contains a value that is above the maximum    ### Other validation options    You can also provide an optional options hash as the fourth argument to Validator#new. Supported options are:    * :limit_lines -- only check this number of lines of the CSV file. Good for a quick check on huge files.    ```  options = {    limit_lines: 100  }  validator = Csvlint::Validator.new( ""http://example.org/data.csv"", nil, nil, options )  ```    * :lambda -- Pass a block of code to be called when each line is validated, this will give you access to the `Validator` object. For example, this will return the current line number for every line validated:    ```      options = {        lambda: ->(validator) { puts validator.current_line }      }      validator = Csvlint::Validator.new( ""http://example.org/data.csv"", nil, nil, options )      => 1      2      3      4      .....  ``` """
Semantic web;https://github.com/nichtich/wdq;"""# NAME    wdq - command line access to Wikidata Query Service    # STATUS    [![Build Status](https://travis-ci.org/nichtich/wdq.png)](https://travis-ci.org/nichtich/wdq)  [![Coverage Status](https://coveralls.io/repos/nichtich/App-wdq/badge.png)](https://coveralls.io/r/nichtich/App-wdq)  [![Kwalitee Score](http://cpants.cpanauthors.org/dist/App-wdq.png)](http://cpants.cpanauthors.org/dist/App-wdq)    # SYNOPSIS    Access [Wikidata Query Service](https://query.wikidata.org/) via command line  to perform SPARQL queries (`query` mode), lookup entities (`lookup`), or  search items and properties (`search` or `psearch`):        wdq -g en solar system        # search 'solar system' in English      wdq psearch -g es parte       # search property 'parte' in Spanish      wdq P361 Q544                 # lookup properties and items      wdq '?c wdt:P361 wd:Q544'     # query parts of the solar system    See the manual for details or get help via `wdq help`:        wdq help options              # list and explain command line options      wdq help modes                # list and explain request modes      wdq help output               # explain output control      wdq help formats              # list and explain output formats      wdq help ontology             # show Wikidata ontology in a nutshell      wdq help prefixes             # list RDF prefixes allowed in queries      wdq help version              # show version of wdq    # DESCRIPTION    The command line script `wdq`, included in CPAN module [App::wdq](https://metacpan.org/pod/App::wdq), provides a  tool to access [Wikidata Query Service](https://query.wikidata.org/). It  supports formulation and execution of [SPARQL SELECT  queries](http://www.w3.org/TR/sparql11-query/#select) to extract selected  information from Wikidata or other Wikibase instances.    # INSTALLATION    Perl should already installed at most operating systems. Otherwise  [get Perl!](https://www.perl.org/get.html)    ## FROM CPAN    Install sources from CPAN including all dependencies:        cpanm App::wdq    First [install cpanm](https://github.com/miyagawa/cpanminus/#installation) if  missing. If installation of `App::wdq` fails try cpanm option `--notest` or  install dependencies as packages as described below.    ## PREBUILD PACKAGES    Install dependencies as prebuild packages for your operating system:        # Debian based systems e.g. Ubuntu (>= 14.04)      sudo apt-get install libhttp-tiny-perl librdf-query-perl        # Windows/ActiveState      ppm install HTTP-Tiny      ppm install RDF-Query    Then install `wdq` from CPAN as described above or copy the script to some  place in your `$PATH`:        wget https://raw.githubusercontent.com/nichtich/wdq/main/script/wdq      chmod +x wdq    The latter method will not install this documentation.    # MODES    Request mode `query` (default), `lookup`, `serch`, or `psearch` can  explicitly be set via first argument or it's guessed from arguments.    ## query    Read SPARQL query from STDIN, option `--query|-q`, or argument. Namespace  definitions and `SELECT` clause are added if missing.        wdq '?c wdt:P361 wd:Q544'      wdq '{ ?c wdt:P361 wd:Q544 }'                 # equivalent      wdq 'SELECT * WHERE { ?c wdt:P361 wd:Q544 }'  # equivalent      wdq < queryfile    ## lookup    Read Wikidata entity ids, URLs, or Wikimedia project URLs from STDIN or  arguments. Result fields are `label`, `description`, and `id`:        wdq Q1      wdq lookup Q1                                 # equivalent      echo Q1 | wdq lookup                          # equivalent      wdq http://de.wikipedia.org/wiki/Universum    # same result    ## search / psearch    Search for items or properties. Result fields are `label`, `id`,  `description`, and possibly matched `alias`. Search and result language is  read from environment or option `--language`/`-g`:        wdq search -g sv Pippi Långstrump    Default output format in search mode is `text`.    # OPTIONS    - --query|-q QUERY        Query or query file (`-` for STDIN as default)    - --format|-f FORMAT|TEMPLATE        Output format or string template. Call `wdq help formats` for details.    - --export EXPORTER        Use a [Catmandu](https://metacpan.org/pod/Catmandu) exporter as output format.    - --no-header|-H        Exclude header in CSV output or other exporter.    - --enumerate|-e        Enumerate results by adding a counter variable `n`    - --limit INTEGER        Add or override a LIMIT clause to limitate the number of results. Single-digit      options such as `-1` can also be used to also set a limit.    - --ids|-i        Abbreviate Wikidata identifier URIs as strings.    - --language|-g        Language to query labels and descriptions in. Set to the locale by default.      This option is currentl only used on lookup mode.    - --count|-c VARS        Prepend SPARQL QUERY to count distinct values    - --label|-l VARS  - --description|-d VARS  - --text|-t VARS        Add label, description, or both. Adds `label`/`description` for variable `id`      or `xLabel`/`xDescription` for any `x`.    - --ignore        Ignore empty results instead of issuing warning and exit code.    - --color|-C        By default output is colored if writing to a terminal. Disable this with      `--no-color`, `--monochrome`, or `-M`. Force color with `--color` or `-C`.    - --api URL        SPARQL endpoint. Default value:      `https://query.wikidata.org/bigdata/namespace/wdq/sparql`    - --no-mediawiki|-m        Don't query MediaWiki API to map URLs to Wikidata items.    - --no-execute|-n        Don't execute SPARQL queries but show them in expanded form. Useful to      validate and pretty-print queries. MediaWiki API requests may be    - -N        Don't execute any queries. Same as `--no-mediawiki --no-execute`.    - --help|-h|-?        Show usage help    - --ontology        Show information about the Wikidata Ontology    - --no-default-prefixes        Don't add default namespace prefixes to the SPARQL query    - --man        Show detailled manual    - --version|-V        Show version if this script    # OUTPUT    Output can be controlled with options `--format`/`-f`, `--export`,  `--header`/`--no-header`/`-H`, and `--color`/`--no-color`/`-C`.    ## Formats    Option `--format`/`-f` sets an output format or string template:    - `simple` (default in query and lookup mode)        Flat JSON without language tags    - `text` (default in search mode)        Print `label`, `alias`, `id` and `description` or `count` when counting.      Also sets option `--ids`.    - `ldjson`        Line delimited Flat JSON    - `csv`        SPARQL Query Results CSV Format. Suppress header with option      `--no-header`/`-H`.  Use Catmandu CSV exporter for more options    - `tsv`        SPARQL Query Results TSV Format    - `xml`        SPARQL Query Results XML Format    - `json`        SPARQL Query Results JSON Format    - `...`        String template.  Call `wdq help pretty` for details    ## Pretty    Option `--format` can be set to a string template with bracket expressions  with optional template parameters (for instance `{id|pre= (|post=)}`).    - style        Highlight `n` name, `v` value, `i` identifier, `t` title, or `e` error    - length        Abbreviate long values    - align        Use `left` or `right` to align short values to a given `length`    - pre/post        Add string before/after value    ## Export    Option `--export` sets a [Catmandu](https://metacpan.org/pod/Catmandu) exporter to create output with.  Given  the corresponding exporter modules installed, one can write results as `YAML`,  Excel (`XLS`), and Markdown table (`Table`) among other formats:        wdq --export YAML                               # short form      wdq --format ldjson | catmandu convert to YAML  # equivalent    Use Catmandu config file (`catmandu.yml`) to further configure export.  See  also tools such as [jq](http://stedolan.github.io/jq/) and  [miller](http://johnkerl.org/miller/) for processing results.    # EXAMPLES        # search ""solar system"" in English (=> Q544)      wdq -g en solar system        # search part-of property (=> P361)      wdq psearch -g en part        # get all parts of the solar system      wdq '?c wdt:P361 wd:Q544'        # look up label and description      wdq Q42 P9        # look up German Wikipedia article and get label description in French      wdq -g fr http://de.wikipedia.org/wiki/Argon        # get all references used at an item      wdq 'wd:Q1 ?prop [ prov:wasDerivedFrom ?ref ]'        # get doctoral advisor graph (academic genealogy) as CSV      wdq '?student wdt:P184 ?advisor' --ids --format csv        # print expanded SPARQL query      wdq -n '?c wdt:P361 wd:Q544'        # execute query and return first 10 tab-separated values      wdq -f tsv --limit 10 < query        # print result as Markdown Table (requires Catmandu::Exporter::Table)      wdq --export Table < query        # count instances (P31) of books (Q571)      wdq --count x '?x wdt:P31 wd:Q571'        # list types (P279) of Exoplanets (Q44559) with label and description      wdq '?id wdt:P279 wd:Q44559:' --text id --format text    # WIKIDATA ONTOLOGY        Entity (item/property)       wd:Q* <-- owl:sameAs --> wd:Q*             --> rdfs:label, skos:altLabel, schema:description ""*""@*             --> schema:dateModified, schema:version             --> wdt:P* ""*"", URI, _:blank             --> p:P* Statement        Item       wd:Q* <-- schema:about <http://*.wikipedia.org/wiki/*>                                --> schema:inLanguage, wikibase:badge        Property       wd:P* --> wikibase:propertyType PropertyType             --> wkibase:directClaim        wdt:P*             --> wikibase:claim             p:P*             --> wikibase:statementProperty ps:P*             --> wikibase:statementValue    psv:P*             --> wikibase:qualifier         pq:P*             --> wikibase:qualifierValue    pqv:P*             --> wikibase:reference         pr:P*             --> wikibase:referenceValue    prv:P*             --> wikibase:novalue           wdno:P*        PropertyType       wikibase: String, Url, WikibaseItem, WikibaseProperty, CommonsMedia, Math,                 Monolingualtext, GlobeCoordinate, Quantity, Time, ExternalId          Statement       wds:* --> wikibase:rank Rank             --> a wdno:P*             --> ps:P* ""*"", URI, _:blank             --> psv:P* Value             --> pq:P* ""*"", URI, _:blank             --> pqv:P* Value             --> prov:wasDerivedFrom Reference        Reference       wdref:* --> pr:P* ""*"", URI               --> prv:P* Value        Rank       wikibase: NormalRank, PreferredRank, DeprecatedRank, BestRank        Value (GlobecoordinateValue/QuantityValue/TimeValue)       wdv:* --> wikibase: geoLatitude, geoLongitude, geoPrecision, geoGlobe URI             --> wikibase: timeValue, timePrecision, timeTimezone, timeCalendarModel             --> wikibase: quantityAmount, quantityUpperBound, quantityLowerBound,                           quantityUnit URI    # COPYRIGHT AND LICENSE    Copyright by Jakob Voss `voss@gbv.de`    Based on a PHP script by Marius Hoch `hoo@online.de`  at [https://github.com/mariushoch/asparagus](https://github.com/mariushoch/asparagus).    Licensed under GPL 2.0+ """
Semantic web;https://github.com/Callidon/pyHDT;"""# pyHDT    [![Build Status](https://travis-ci.org/Callidon/pyHDT.svg?branch=master)](https://travis-ci.org/Callidon/pyHDT) [![Documentation Status](https://readthedocs.org/projects/pyhdt/badge/?version=latest)](https://callidon.github.io/pyHDT) [![PyPI version](https://badge.fury.io/py/hdt.svg)](https://badge.fury.io/py/hdt)    **pyHDT is joining the RDFlib family as part of the rdflib 6.0 release! The development continues at [rdflib-hdt](https://github.com/RDFLib/rdflib-hdt), and this repository is going into archive.**    Read and query HDT document with ease in Python    [Online Documentation](https://callidon.github.io/pyHDT)    # Requirements    * Python *version 3.6.4 or higher*  * [pip](https://pip.pypa.io/en/stable/)  * **gcc/clang** with **c++11 support**  * **Python Development headers**  > You should have the `Python.h` header available on your system.     > For example, for Python 3.6, install the `python3.6-dev` package on Debian/Ubuntu systems.    Then, install the [pybind11 library](http://pybind11.readthedocs.io/en/stable/)  ```  pip install pybind11  ```    # Installation    Installation in a [virtualenv](https://virtualenv.pypa.io/en/stable/) is **strongly advised!**    ## Pip install (recommended)    ```  pip install hdt  ```    ## Manual installation    ```  git clone https://github.com/Callidon/pyHDT  cd pyHDT/  ./install.sh  ```    # Getting started    ```python  from hdt import HDTDocument     # Load an HDT file.   # Missing indexes are generated automatically, add False as the second argument to disable them  document = HDTDocument(""test.hdt"")    # Display some metadata about the HDT document itself  print(""nb triples: %i"" % document.total_triples)  print(""nb subjects: %i"" % document.nb_subjects)  print(""nb predicates: %i"" % document.nb_predicates)  print(""nb objects: %i"" % document.nb_objects)  print(""nb shared subject-object: %i"" % document.nb_shared)    # Fetch all triples that matches { ?s ?p ?o }  # Use empty strings ("""") to indicates variables  triples, cardinality = document.search_triples("""", """", """")    print(""cardinality of { ?s ?p ?o }: %i"" % cardinality)  for triple in triples:    print(triple)    # Search also support limit and offset  triples, cardinality = document.search_triples("""", """", """", limit=10, offset=100)  # etc ...  ```    # Handling non UTF-8 strings in python    If the HDT document has been encoded with a non UTF-8 encoding the previous code won't work correctly and will result in a `UnicodeDecodeError`.  More details on how to convert string to str from c++ to python [here](https://pybind11.readthedocs.io/en/stable/advanced/cast/strings.html)    To handle this we doubled the API of the HDT document by adding:  - `search_triples_bytes(...)` return an iterator of triples as `(py::bytes, py::bytes, py::bytes)`  - `search_join_bytes(...)` return an iterator of sets of solutions mapping as `py::set(py::bytes, py::bytes)`  - `convert_tripleid_bytes(...)` return a triple as: `(py::bytes, py::bytes, py::bytes)`  - `convert_id_bytes(...)` return a `py::bytes`    **Parameters and documentation are the same as the standard version**    ```python  from hdt import HDTDocument     # Load an HDT file.   # Missing indexes are generated automatically, add False as the second argument to disable them  document = HDTDocument(""test.hdt"")  it = document.search_triple_bytes("""", """", """")    for s, p, o in it:    print(s, p, o) # print b'...', b'...', b'...'    # now decode it, or handle any error    try:      s, p, o = s.decode('UTF-8'), p.decode('UTF-8'), o.decode('UTF-8')    except UnicodeDecodeError as err:      # try another other codecs      pass  ``` """
Semantic web;https://github.com/lukostaz/prissma-studio;"""PRISSMA Studio  ===========  ### A Prism Designer for PRISSMA    PRISSMA Studio is an in-browser web application that creates [PRISSMA](http://wimmics.inria.fr/projects/prissma) Prisms, using the [PRISSMA vocabulary](ns.inria.fr/prissma/v2/prissma_v2.html).    PRISSMA Studio supports designers in the creation of [Fresnel](http://www.w3.org/2005/04/fresnel-info/manual/) Lenses and Formats, and eases the definition of the associated [PRISSMA context](http://ns.inria.fr/prissma/v2/prissma_v2.html#Context).    Once created, Prisms can be stored on the file system of PRISSMA-equipped mobile devices, where they will be processed by the [PRISSMA selection algorithm](http://2014.eswc-conferences.org/sites/default/files/papers/paper_81.pdf) .        ## Installation    Clone the repository:    	$ git clone https://github.com/lukostaz/prissma-studio.git    Install dependencies with [Bower](http://bower.io/):    	$ bower install      ## Demo    [Check out PRISSMA Studio demo here](http://luca.costabello.info/prissma-studio/).      ## Licence  	      Copyright (C) 2014-2015 Luca Costabello, v1.0.0        This program is free software; you can redistribute it and/or modify it      under the terms of the GNU General Public License as published by the      Free Software Foundation; either version 2 of the License, or (at your      option) any later version.        This program is distributed in the hope that it will be useful, but      WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY      or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License      for more details.        You should have received a copy of the GNU General Public License along      with this program; if not, see <http://www.gnu.org/licenses/>.    ## Contacts  Further details on the [PRISSMA Project Page](http://wimmics.inria.fr/projects/prissma/), or contact [Luca Costabello](http://luca.costabello.info).     """
Semantic web;https://github.com/rsgoncalves/module-extractor;"""*owl-me*  ====    #### a Java-based module extractor for OWL ontologies ####    Built using the [OWL API](http://owlapi.sourceforge.net/).       summary  --------------------  *owl-me* is a standalone tool designed to extract different types of [Locality-based modules](http://owl.cs.manchester.ac.uk/research/modularity/) from OWL ontologies.    The tool takes as inputs an ontology and a text file. The latter is the so-called *signature file*, which contains entity (class and object/data property) IRIs. The tool extracts a module for the specified set of IRIs (i.e. signature) onto a chosen location.      usage  --------------------  Build using the Ant script and run the **owl-me.jar** file. For large ontologies you may have to increase the heap space and entity expansion limit (esp. for ontologies in RDF/XML), e.g., for 4GB heap:<br><br>  `java -jar -Xmx4G -DentityExpansionLimit=100000000 owl-me.jar`      signatures for module extraction  --------------------  Signature files should contain entity IRIs as they appear in the original ontology. IRIs can be separated by any of the following delimiters:    * Comma (e.g. CSV files)    * White space    * Vertical bar ""|""    * Tab    * New line    The file may also contain headers or comments, so long as the line or part thereof is preceded with '%'. All text following '%' is ignored. Check the *example signature file contents* below:    % My header<br>  Class_IRI_1, Class_IRI_2 Class_IRI_3<br>  Property_IRI_2 | Property_IRI_3    % Main properties<br>  <br>  % Some comment<br>  Class_IRI_4<br>      SNOMED CT  --------------------  The module extractor accepts signature files for the SNOMED CT ontology in the *UMLS Core Subset format*. Any manually constructed signature files **should have the concept ID's delimited by vertical bars ""|""**, in a similar way as the UMLS Core Subset files.      deployment  --------------------  The module extractor is compatible with **Java 1.6 and above**. It was tested with Java 1.7 and 1.8., and relies mainly on the following project:     * [OWL API](http://owlapi.sourceforge.net/) (v4.0.1)      contact  --------------------  Consider checking the [OWL@Manchester](http://owl.cs.manchester.ac.uk) website (and linked publications) for more information regarding _Locality-based_ modules, before submitting queries.    If you come across any bugs please use the ""Issues"" tab to describe the problem, along with sufficient data to reproduce it (i.e. the ontology and signature used)."""
Semantic web;https://github.com/comunica/comunica;"""<p align=""center"">    <a href=""https://comunica.dev/"">      <img alt=""Comunica"" src=""https://comunica.dev/img/comunica_red.svg"" width=""200"">    </a>  </p>    <p align=""center"">    <strong>A knowledge graph querying framework for JavaScript</strong>    <br />    <i>Flexible SPARQL and GraphQL over decentralized RDF on the Web.</i>  </p>    <p align=""center"">  <a href=""https://github.com/comunica/comunica/actions?query=workflow%3ACI""><img src=""https://github.com/comunica/comunica/workflows/CI/badge.svg"" alt=""Build Status""></a>  <a href=""https://coveralls.io/github/comunica/comunica?branch=master""><img src=""https://coveralls.io/repos/github/comunica/comunica/badge.svg?branch=master"" alt=""Coverage Status""></a>  <a href=""https://zenodo.org/badge/latestdoi/107345960""><img src=""https://zenodo.org/badge/107345960.svg"" alt=""DOI""></a>  <a href=""https://gitter.im/comunica/Lobby""><img src=""https://badges.gitter.im/comunica.png"" alt=""Gitter chat""></a>  </p>    **[Learn more about Comunica on our website](https://comunica.dev/).**    Comunica is an open-source project that is used by [many other projects](https://github.com/comunica/comunica/network/dependents),  and is being maintained by a [group of volunteers](https://github.com/comunica/comunica/graphs/contributors).  If you would like to support this project, you may consider:    * Contributing directly by [writing code or documentation](https://comunica.dev/contribute/); or  * Contributing indirectly by funding this project via [Open Collective](https://opencollective.com/comunica-association).    ## Supported by    Comunica is a community-driven project, sustained by the [Comunica Association](https://comunica.dev/association/).  If you are using Comunica, [becoming a sponsor or member](https://opencollective.com/comunica-association) is a way to make Comunica sustainable in the long-term.    Our top sponsors are shown below!    <a href=""https://opencollective.com/comunica-association/sponsor/0/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/0/avatar.svg""></a>  <a href=""https://opencollective.com/comunica-association/sponsor/1/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/1/avatar.svg""></a>  <a href=""https://opencollective.com/comunica-association/sponsor/2/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/2/avatar.svg""></a>  <a href=""https://opencollective.com/comunica-association/sponsor/3/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/3/avatar.svg""></a>    ## Query with Comunica    Read one of our [guides to **get started** with querying](https://comunica.dev/docs/query/getting_started/):    * [Querying from the command line](https://comunica.dev/docs/query/getting_started/query_cli/)  * [Updating from the command line](https://comunica.dev/docs/query/getting_started/update_cli/)  * [Querying local files from the command line](https://comunica.dev/docs/query/getting_started/query_cli_file/)  * [Querying in a JavaScript app](https://comunica.dev/docs/query/getting_started/query_app/)  * [Updating in a JavaScript app](https://comunica.dev/docs/query/getting_started/update_app/)  * [Querying in a JavaScript browser app](https://comunica.dev/docs/query/getting_started/query_browser_app/)  * [Setting up a SPARQL endpoint](https://comunica.dev/docs/query/getting_started/setup_endpoint/)  * [Querying from a Docker container](https://comunica.dev/docs/query/getting_started/query_docker/)  * [Setting up a Web client](https://comunica.dev/docs/query/getting_started/setup_web_client/)  * [Query using the latest development version](https://comunica.dev/docs/query/getting_started/query_dev_version/)    Or jump right into one of the available query engines:  * [Comunica SPARQL](https://github.com/comunica/comunica/tree/master/engines/query-sparql#readme): SPARQL/GraphQL querying from JavaScript applications or the CLI ([Browser-ready via a CDN](https://github.com/rdfjs/comunica-browser))  * [Comunica SPARQL File](https://github.com/comunica/comunica/tree/master/engines/query-sparql-file#readme): Engine to query over local RDF files  * [Comunica SPARQL RDFJS](https://github.com/comunica/comunica/tree/master/engines/query-sparql-rdfjs#readme): Engine to query over in-memory [RDFJS-compliant sources](https://rdf.js.org/stream-spec/#source-interface).  * [Comunica SPARQL HDT](https://github.com/comunica/comunica-actor-init-sparql-hdt#readme): Library to query over local [HDT](https://www.rdfhdt.org/) files  * [Comunica SPARQL Solid](https://github.com/comunica/comunica-feature-solid/tree/master/engines/query-sparql-solid#readme): Engine to query over files behind [Solid access control](https://solidproject.org/).  * [Comunica SPARQL Link Traversal](https://github.com/comunica/comunica-feature-link-traversal/tree/master/engines/query-sparql-link-traversal#readme): Engine to query over multiple files by following links between them.  * [Comunica SPARQL Link Traversal Solid](https://github.com/comunica/comunica-feature-link-traversal/tree/master/engines/query-sparql-link-traversal-solid#readme): Engine to query within [Solid data vaults](https://solidproject.org/) by following links between documents.    ## Modify or Extending Comunica    [Read one of our guides to **get started** with modifying Comunica](https://comunica.dev/docs/modify/),  or have a look at some [examples](https://github.com/comunica/examples):    * [Querying with a custom configuration from the command line](https://comunica.dev/docs/modify/getting_started/custom_config_cli/)  * [Querying with a custom configuration in a JavaScript app](https://comunica.dev/docs/modify/getting_started/custom_config_app/)  * [Exposing your custom config as an npm package](https://comunica.dev/docs/modify/getting_started/custom_init/)  * [Exposing your custom config in a Web client](https://comunica.dev/docs/modify/getting_started/custom_web_client/)  * [Contributing a new query operation actor to the Comunica repository](https://comunica.dev/docs/modify/getting_started/contribute_actor/)  * [Adding a config parameter to an actor](https://comunica.dev/docs/modify/getting_started/actor_parameter/)    ## Contribute    Interested in contributing? Have a look at our [contribution guide](https://comunica.dev/contribute/).    ## Development Setup    _(JSDoc: https://comunica.github.io/comunica/)_    This repository should be used by Comunica module **developers** as it contains multiple Comunica modules that can be composed.  This repository is managed as a [monorepo](https://github.com/babel/babel/blob/master/doc/design/monorepo.md)  using [Lerna](https://lernajs.io/).    If you want to develop new features  or use the (potentially unstable) in-development version,  you can set up a development environment for Comunica.    Comunica requires [Node.JS](http://nodejs.org/) 8.0 or higher and the [Yarn](https://yarnpkg.com/en/) package manager.  Comunica is tested on OSX, Linux and Windows.    This project can be setup by cloning and installing it as follows:    ```bash  $ git clone https://github.com/comunica/comunica.git  $ cd comunica  $ yarn install  ```    **Note: `npm install` is not supported at the moment, as this project makes use of Yarn's [workspaces](https://yarnpkg.com/lang/en/docs/workspaces/) functionality**    This will install the dependencies of all modules, and bootstrap the Lerna monorepo.  After that, all [Comunica packages](https://github.com/comunica/comunica/tree/master/packages) are available in the `packages/` folder  and can be used in a development environment, such as querying with [Comunica SPARQL (`@comunica/query-sparql`)](https://github.com/comunica/comunica/tree/master/engines/query-sparql).    Furthermore, this will add [pre-commit hooks](https://www.npmjs.com/package/pre-commit)  to build, lint and test.  These hooks can temporarily be disabled at your own risk by adding the `-n` flag to the commit command.    ## Benchmarking    If you want to do benchmarking with Comunica in Node.js,  make sure to run Node.js in production mode as follows:    ```bash  > NODE_ENV=production node packages/some-package/bin/some-bin.js  ```    The reason for this is that Comunica extensively generates  internal `Error` objects.  In non-production mode, these also produce long stacktraces,  which may in some cases impact performance.    ## Cite    If you are using or extending Comunica as part of a scientific publication,  we would appreciate a citation of our [article](https://comunica.github.io/Article-ISWC2018-Resource/).    ```bibtex  @inproceedings{taelman_iswc_resources_comunica_2018,    author    = {Taelman, Ruben and Van Herwegen, Joachim and Vander Sande, Miel and Verborgh, Ruben},    title     = {Comunica: a Modular SPARQL Query Engine for the Web},    booktitle = {Proceedings of the 17th International Semantic Web Conference},    year      = {2018},    month     = oct,    url       = {https://comunica.github.io/Article-ISWC2018-Resource/}  }  ```    ## License  This code is copyrighted by [the Comunica Association](https://comunica.dev/association/) and [Ghent University – imec](http://idlab.ugent.be/)  and released under the [MIT license](http://opensource.org/licenses/MIT). """
Semantic web;https://github.com/RMLio/yarrrml-parser;"""# YARRRML Parser    This library allows to convert [YARRRML](https://w3id.org/yarrrml) rules to [RML](http://rml.io) or [R2RML](https://www.w3.org/TR/r2rml/) rules.    ## Install    - `npm i -g @rmlio/yarrrml-parser`    ## Usage    ### CLI    There are two CLI functions, `yarrrml-parser` and `yarrrml-generator`.  Using the `--help` flag will show all possible commands.    #### yarrrml-parser    If you want to generate RML rules from a YARRRML document,  you do the following: `yarrrml-parser -i rules.yml`.    The rules will be written to standard output.  If you want to write them to a file, you can add the `-o` option.    By default, the parser generates RML rules.  If you want to generate R2RML rules add `-f R2RML`.    If you want to use `rr:class` instead of Predicate Object Maps, use the `-c` flag.    You can use multiple input files too: `yarrrml-parser -i rules-1.yml -i rules-2.yml`.  They are converted to a single RML document.  Note that the keys in `prefixes`, `sources`, and `mappings` have to be unique across all files.  `base` can only be set once.  You find an example at [`test/multiple-input-files`](test/multiple-input-files).    You can overwrite external references via the `-e`.  An external reference starts with `_`.  For example, `-e name=John` will replace all occurrences of `$(_name)` with `John`.  Repeat `-e` for multiple references.  When you do not provide a value for an external reference,  the reference will not be replaced.  You find an example in [`test/template-escape`](test/template-escape).  If you want to use for example `$(_name)` as both an external reference and a normal reference,  then you add a `\` for the latter resulting in `$(\_name)` for the latter.    If you want the outputted RML to be pretty, please provide the `-p` or `--pretty` parameter.    #### yarrrml-generator    If you want to generate YARRRML rules from an RML document, you do the following: `yarrrml-generator -i rules.rml.ttl`.  The rules will be written to standard output.  If you want to write them to a file, you can add the `-o` option.    ### Library    `npm i --save @rmlio/yarrrml-parser`    ```javascript  let yarrrml = require('@rmlio/yarrrml-parser/lib/rml-generator');    const yaml = ""[yarrrml string]"";  const y2r = new yarrrml();  const triples = y2r.convert(yaml);    if ( y2r.getLogger().has('error') ) {     const logs = y2r.getLogger().getAll();     ...  }  ```    ## Development    - Clone this repo.  - Install the dependencies via `npm i`  - Update code, if needed.  - Run the tests via `npm test`    - If you make a new test, make sure the (RML) Turtle is 'pretty'. If you're not sure it's pretty, run `./test/prettify_ttl.js`  - Make the [CLI](#cli) (based on the code in the cloned repo)  available system-wide via `npm link` (optional).    ## Docker    Run (from [DockerHub](https://hub.docker.com/repository/docker/rmlio/yarrrml-parser)):    ```bash  docker run --rm -it -v $(pwd)/resources:/data rmlio/yarrrml-parser:latest -i /data/test.yarrr.yml  ```    Build from source:    ```bash  docker build -t yarrrml-parser .  ```    ## License    This code is copyrighted by [Ghent University – imec](http://idlab.ugent.be/) and released under the [MIT license](http://opensource.org/licenses/MIT). """
Semantic web;https://github.com/modelfabric/reactive-sparql;"""reactive-sparql  ===============    *""A Reactive SPARQL Client for Scala and Akka""*    This client uses [akka-streams](http://doc.akka.io/docs/akka/2.4/scala.html) to do as much as possible asynchronously, with back pressure  support around the HTTP connection towards the triple store. There are no blocking calls crossing process boundaries.    The older Spray HTTP client no longer supported, however it is still available as  release [v0.1.3](https://github.com/agnos-ai/reactive-sparql/tree/v0.1.3).    The akka-streams APIs currently supports 3 flavours of flows:    * [Flavour #1](#flavour-1-run-sparql): Execute SPARQL  * [Flavour #2](#flavour-2-construct-models): Construct Models  * [Flavour #3](#flavour-3-manipulate-graphs): Manipulate Graphs    ### Flavour #1: Run SPARQL    Use the `SparqlQuery(stmt: String)` or `SparqlUpdate(stmt: String)` case class and embed it in a `SparqlRequest()` to be passed to the flow. On the other end a  `SparqlResponse()` pops out. Support for custom mappings is available, where the resulting values get marshaled to a custom domain object.  This is however not mandatory, there is a default result mapper available that will return a [standard  result set model](src/main/scala/ai/agnos/sparql/api/SparqlResult.scala#L25) based on the `application/sparql-results+json` content type.    It is possible to use a single wrapper flow of [`Flow[SparqlRequest, SparqlResponse, _]`](src/main/scala/ai/agnos/sparql/stream/client/SparqlRequestFlowBuilder.scala)  to run both `SparqlUpdate()` and `SparqlQuery()` statements. There is an option to use specialised [query](src/main/scala/ai/agnos/sparql/stream/client/SparqlQueryFlowBuilder.scala)  and [update](src/main/scala/ai/agnos/sparql/stream/client/SparqlUpdateFlowBuilder.scala) flows as well.    The underlying implementation communicates with the triple store via the HTTP endpoints, as documented here  for [queries](https://www.w3.org/TR/2013/REC-sparql11-query-20130321/)  and [updates](https://www.w3.org/TR/2013/REC-sparql11-update-20130321/).    #### Example #1: Run a simple Sparql query    ```scala  /* Define domain case class and mappings */  object Person extends ResultMapper[Person] {    override def map(qs: QuerySolution): Person = {      Person(qs.uri(""g"").get, qs.string(""c"").get)    }  }  case class Person(id: URI, name: String) extends SparqlResult    /* Create a bespoke SparqlQuery with a mapping to a Person */  val mappingQuery2Get = SparqlQuery( """"""    |SELECT ?g ?b ?c    |FROM NAMED <urn:test:agnos:data>    |WHERE {    |  GRAPH ?g {    |   <urn:test:whatever> ?b ?c    |  }    |}"""""", mapping = Person, reasoningEnabled = true)    /* Create the Flow and Probes */  val sparqlRequestFlowUnderTest = SparqlRequestFlowBuilder.sparqlRequestFlow(testServerEndpoint)  val (source, sink) = TestSource.probe[SparqlRequest]    .via(sparqlRequestFlowUnderTest)    .toMat(TestSink.probe[SparqlResponse])(Keep.both)    .run()    /* Send the request to the stream and expect the result */  sink.request(1)  source.sendNext(SparqlRequest(mappingQuery2Get))  sink.expectNext(receiveTimeout) match {    case SparqlResponse(_, true, results, None) =>      val persons: Seq[Person] = results //the  mapped collection is returned      assert(persons.contains(...)    case r@_ =>      fail(r)  }  ```    ### Flavour #2: Construct Models    Working with Sparql query solutions (rows of result bindings as returned by a SELECT statement) is not always suitable. This is because the result  is not plain RDF.    Use of [SPARQL CONSTRUCT](https://www.w3.org/TR/sparql11-query/#construct)s is suitable in cases where we are only interested in triples (i.e. not  quads, where the graph IRI is missing)    At the moment there is no way to write the following statement, so that the resulting RDF is returned in ""quads"" format (N-QUADS or JSON-LD)  ```sparql  CONSTRUCT {    GRAPH ?g {      ?s ?p ?o .    }  } WHERE {  ...  }  ```    This flow has been created to circumvent the problem. It is an extension of the API used in [Flavour #1](#flavour-1-run-sparql).    Instead of a `SparqlQuery()` this flow works with a `SparqlConstruct()` inside the `SparqlRequest()`  ```scala  object SparqlConstruct {    def apply(resourceIRIs: Seq[URI] = Nil,              propertyIRIs: Seq[URI] = Nil,              graphIRIs: Seq[URI] = Nil,              reasoningEnabled: Boolean = false)(      implicit _paging: PagingParams = NoPaging    ): SparqlConstruct = {      ...    }  }  ```  By specifying a set of matching resource, property and/or graph IRIs, we limit the number of results that are returned.  Internally this flow will generate a reified SELECT statement that allows us to capture all 4 properties of the RDF Model, including the graph IRI.    The flow responds with a `SparqlModelResult(model: Model)` within a `SparqlResult()` which contains the RDF4J Model (Graph) instance.  ```scala  case class SparqlModelResult(model: Model) extends SparqlResult  ```    Refer to [`Flow[SparqlRequest, SparqlResponse, _]`](src/main/scala/ai/agnos/sparql/stream/client/SparqlConstructToModelFlowBuilder.scala)  for more detail.    ### Flavour #3: Manipulate Graphs    This flow allows for basic graph manipulation, as defined by the [graph-store protocol](https://www.w3.org/TR/2013/REC-sparql11-http-rdf-update-20130321/).  Not all aspects of the protocol are supported, however it is possible to:    #### Retrieve Graphs    Retrieve an entire graph using `GetGraph(graphUri: Option[URI])` wrapped in a `GraphStoreRequest()`    If no graphIri is specified the query returns the contents of the DEFAULT graph.    #### Drop Graphs    Drop an entire graph using `DropGraph(graphUri: Option[URI])` wrapped in a `GraphStoreRequest()`    If no graphIri is specified the request drops the DEFAULT graph, so be careful with that if you don't use named graphs in your triple store.    #### Insert Models into Graphs    Insert the contents of an RDF Model into the specified graph. There are 3 variants:    * `InsertGraphFromModel(graphModel: Model, graphUri: Option[URI])`: inserts an in-memory RDF Model;  * `InsertGraphFromPath(filePath: Path, graphUri: Option[URI], format: RDFFormat)`: inserts the contents of the specified file in the given RDF format;  * `InsertGraphFromURL(url: URL, format: RDFFormat, graphUri: Option[URI])`: inserts the contents of the file behind the specified HTTP URL in the given RDF format.    All the operations above return a `GraphStoreResponse` which contains the success status of the operation and a optional model (for `GetGraph()` queries only)  ```scala  case class GraphStoreResponse  (    request: GraphStoreRequest,    success: Boolean,    statusCode: Int,    statusText: String,    model: Option[Model] = None  )  ```    There is a `mergeGraphs: Boolean` parameter for all insert messages, that allows us to control how the resulting graph will deal with  the newly inserted triples.    * `mergeGraphs = true` will perform a HTTP PUT operation, which merges the content of the graph being sent with the graph that    is already in the triple store;  * `mergeGraphs = false` is the DEFAULT option and will perform a HTTP POST operation, which replaces the content of the graph with    the one being sent over.    If no graph is specified, the insert will use the DEFAULT graph in the triple store.    Refer to [`Flow[GraphStoreRequest, GraphStoreResponse, _]`](src/main/scala/ai/agnos/sparql/stream/client/GraphStoreRequestFlowBuilder.scala)  for more detail. """
Semantic web;https://github.com/RENCI-NRIG/gleen;"""GLEEN - A regular path library for ARQ SparQL    The GLEEN library is a property function library for the Jena ARQ SparQL query engine.    GLEEN was developed by:    > Todd Detwiler    > Structural Informatics Group    > University of Washington    GLEEN is currently licensed under the Apache License, version 2.0    ------    This version of GLEEN is forked from the last released version: 0.6.1    The fork was created by Victor J. Orlikowski (Duke University) in support of  the ORCA project (https://github.com/RENCI-NRIG/orca5), which makes use of GLEEN.    This new revision ports GLEEN forward to currently supported versions of JENA  (http://jena.apache.org/). There are two branches in the code - master branch tied to Jena 2.11.0 and producing Gleen artifact with version 0.6.3-jena-2.11.0-SNAPSHOT. The other branch is called JENA_3_3_0 and produces an artifact with version 0.6.3-jena-3.3.0-SNAPSHOT.     ------    ### Building    For this revision of GLEEN, simply check out the source, make sure you have a  recent version of maven, and type:    mvn clean compile """
Semantic web;https://github.com/cygri/pubby;"""This is a Linked Data server that adds an HTML interface and  dereferenceable URLs on top of RDF data that sits in a SPARQL  store.    See [the Pubby website](http://www4.wiwiss.fu-berlin.de/pubby/)  for details and instructions. """
Semantic web;https://github.com/mro/librdf.sqlite;"""  [![Build Status](https://travis-ci.org/mro/librdf.sqlite.svg)](https://travis-ci.org/mro/librdf.sqlite)    Improved [SQLite](http://sqlite.org) RDF triple [storage module](http://librdf.org/docs/api/redland-storage-modules.html)  for [librdf](http://librdf.org/).    Cross platform, plain C source file. Comes with a [![Version](https://img.shields.io/cocoapods/v/librdf.sqlite.svg)](https://github.com/CocoaPods/Specs/tree/master/Specs/librdf.sqlite/) for those targeting iOS.    Inspired by the [official sqlite store](https://github.com/dajobe/librdf/blob/master/src/rdf_storage_sqlite.c).    ## Usage    ```c  #include ""rdf_storage_sqlite_mro.h""  ....  librdf_world *world = librdf_new_world();  librdf_init_storage_sqlite_mro(world);  // register storage factory  ....  const char* options = ""new='yes', contexts='no'"";  librdf_storage *newStorage = librdf_new_storage(world, LIBRDF_STORAGE_SQLITE_MRO, file_path, options);  ```    See e.g. in (my) <http://purl.mro.name/ios/librdf.objc>.    ## License    - `test/minunit.h`, Copyright (C) 2002 [John Brewer](http://jera.com), NO WARRANTY,  - *all others*, Copyright (C) 2014-2015 [Marcus Rohrmoser mobile Software](http://mro.name/~me), [Human Rights License](LICENSE)    ## Design Goals    | Quality         | very good | good | normal | irrelevant |  |-----------------|:---------:|:----:|:------:|:----------:|  | Functionality   |           |      |    ×   |            |  | Reliability     |           |  ×   |        |            |  | Usability       |           |      |        |     ×      |  | Efficiency      |     ×     |      |        |            |  | Changeability   |           |  ×   |        |            |  | Portability     |           |      |    ×   |            |    Currently 50% code and 99% runtime saving (for 100k triples).    - intense use of [SQLite prepared statements](https://www.sqlite.org/c3ref/stmt.html) and    [bound values](https://www.sqlite.org/c3ref/bind_blob.html):    - no stringbuffers    - no strcpy/memcpy,    - no SQL escaping,  - re-use compiled statements where possible (at the cost of thread safety),  - as few SQL statements as possible (at the cost of some non-trivial ones),  - SQLite indexes (at the cost of larger DB files). """
Semantic web;https://github.com/linkeddata/warp;"""warp  ====    Warp - the linked data file manager """
Semantic web;https://github.com/BorderCloud/TFT;"""# TFT    TFT (Tester for Triplestore) is a script PHP to pass tests through a sparql endpoint.    # install JMeter for protocol tests  ```  wget http://mirrors.standaloneinstaller.com/apache//jmeter/binaries/apache-jmeter-5.4.1.tgz  tar xvzf apache-jmeter-5.4.1.tgz  mv  apache-jmeter-5.4.1 jmeter  rm apache-jmeter-5.4.1.tgz  ```    ## How to use it ?    You can read the doc here: https://bordercloud.github.io/tft-reports/    ## Usage with Travis Ci    Example of project with Travis Ci and TFT :    * [OpenLink Virtuoso version community 7/stable](https://github.com/BorderCloud/tft-virtuoso7-stable)  * [Blazegraph 2.1.5](https://github.com/BorderCloud/tft-blazegraph)  * [Jena-Fuseki 4.0.0](https://github.com/BorderCloud/tft-jena-fuseki)    ## Usage with Jenkins    Jenkins will be read the reports Junit/XML with this line :    ```  TFT/junit/*junit.xml  ```    ## Usage sparql-auth of Virtuoso  ```  git clone --recursive https://github.com/BorderCloud/TFT.git  cd TFT    #copie tests in a RDF database  ./tft-testsuite -a \                  -t virtuoso \                  -q 'http://database/sparql-auth/' \                  -u 'http://database/sparql-auth/' \                  -l LOGIN -p 'PASS'    #tests Virtuoso  ./tft  \        -t virtuoso \        -q 'http://database/sparql-auth/' \        -u 'http://database/sparql-auth/' \        -tt virtuoso \        -tq http://databasetotest/sparql/ \        -tu http://databasetotest/sparql/ \        -o ./junit \        -r https://marketplace.stratuslab.eu/marketplace/metadata/MvJPyzt00KDfRS-vM5gUEfhlr-R \        --softwareName=""Virtuoso Open-Source Edition""  --softwareDescribeTag=v7.1.1  --softwareDescribe=7.1.1-dev.3211-pthreads \        -l LOGIN -p 'PASSWORD'    #Calculate the score  ./tft-score \        -t virtuoso \        -q 'http://database/sparql-auth/' \        -u 'http://database/sparql-auth/' \        -r https://marketplace.stratuslab.eu/marketplace/metadata/MvJPyzt00KDfRS-vM5gUEfhlr-R \        -l LOGIN -p 'PASSWORD'  ```    ## Read the last score with SPARQL    Example :  ```  SELECT *  WHERE  {  	GRAPH ?graph {         ?service a sd:Service ;                 sd:server ?server ;                 sd:testedBy ?tester ;                 sd:testedDate ?LastDate.         ?server git:name ?serverName ;                 git:describeTag ?serverVersion ;                 git:describe ?serverVersionBuild .         ?tester  git:name ?testerName ;                 git:describeTag ?testerVersion  .  			   ?service sq:scoreTest ?score .  			   ?service sq:totalTest ?total .         }  FILTER(STR(xsd:date(?LastDate)) = STR(xsd:date(NOW())))  }  ```    ## License    TFT (c)2021 by Karima Rafes, BORDERCLOUD    TFT is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.    You should have received a copy of the license along with this work. If not, see http://creativecommons.org/licenses/by-sa/4.0/. """
Semantic web;https://github.com/blazegraph/blazegraph-samples;"""#Welcome to the Blazegraph Samples Project#  The Blazegraph™ Database is our ultra high-performance graph database supporting Blueprints and RDF/SPARQL APIs. It supports up to 50 Billion edges on a single machine and has available enterprise features such as High Availability and Scale-out. It is in production use for Fortune 500 customers such as EMC, Autodesk, and many others.  It powers the Wikimedia Foundation's Wiki Data Query Service.  See the latest [Feature Matrix](http://www.blazegraph.com/product/).    [Sign up](http://eepurl.com/VLpUj) to get the latest news on Blazegraph.      Please also visit us at our: [website](http://www.blazegraph.com), [wiki](https://wiki.blazegraph.com), and [blog](https://wiki.blazegraph.com/).    Find an issue?   Need help?  See [JIRA](https://jira.blazegraph.com) or purchase [Support](https://www.blazegraph.com/buy).    ![image](http://blog.blazegraph.com/wp-content/uploads/2015/07/blazegraph_by_systap_favicon.png)    # blazegraph-samples  Samples for using Blazegraph™    Link to the [Javadoc](https://blazegraph.github.io/blazegraph-samples/apidocs/index.html)    To build everything run:    ```  mvn package  ```    Simple applications demonstrating using Blazegraph for loading/querying data in different modes:    To build a sample, cd in the directory and run:    ```  cd sample-sesame-first  mvn package  ```    1. sample-sesame-first - Sesame API in emmbedded mode    https://wiki.blazegraph.com/wiki/index.php/First_Application_Tutorial    2. 'sample-sesame-embedded' - Sesame API in emmbedded mode    https://wiki.blazegraph.com/wiki/index.php/Sesame_API_embedded_mode    3. 'sample-sesame-remote' - Sesame API in remote mode    https://wiki.blazegraph.com/wiki/index.php/Sesame_API_remote_mode    4. 'sample-blueprints-embedded' - Blueprints API in embedded mode    https://wiki.blazegraph.com/wiki/index.php/Blueprints_API_embedded_mode    5. 'sample-blueprints-remote' - Blueprints API in remote mode    https://wiki.blazegraph.com/wiki/index.php/Blueprints_API_remote_mode    6. 'sample-rdr' - using RDF* and SPARQL* with Blazegraph™    https://wiki.blazegraph.com/wiki/index.php/RDR    7.  'sample-customFunction-embedded'- Custom Embedded Function      https://wiki.blazegraph.com/wiki/index.php/Custom_Function_embedded_mode    8.  'sample-test' - Sample Unit Tests <br>     technical project created for CI system               """
Semantic web;github.com/semantalytics/awesome-ontologies;"""# awesome-ontologies    - xsd  - [rdf](http://www.w3.org/1999/02/22-rdf-syntax-ns) http://www.w3.org/1999/02/22-rdf-syntax-ns#   - [rdfs](http://www.w3.org/2000/01/rdf-schema) http://www.w3.org/2000/01/rdf-schema#  - [owl](http://www.w3.org/2002/07/owl) http://www.w3.org/2002/07/owl#  - [skos](https://www.w3.org/2004/02/skos/core.rdf) http://www.w3.org/2004/02/skos/core#  - [skosxl]() http://www.w3.org/2008/05/skos-xl#  - [dcat]() http://http://www.w3.org/ns/org#www.w3.org/ns/dcat#  - [org]() http://www.w3.org/ns/org#  - [vcard]() http://www.w3.org/2006/vcard/ns#  - [geonames]() http://www.geonames.org/ontology#  - [gr]() http://purl.org/goodrelations/v1#  - [prov]() http://www.w3.org/ns/prov#  - [dc]()  http://purl.org/dc/elements/1.1/  - [dcterm]() http://purl.org/dc/terms/  - [sioc]() http://rdfs.org/sioc/ns#  - [mo]() http://musicontology.com/  - [MarineTLO]()  http://ics.forth.gr/Ontology/MarineTLO/icore#  http://ics.forth.gr/Ontology/MarineTLO/imarine#  - [schema]() http://schema.org/  - [Spar](http://www.sparontologies.net/ontologies)  - yago  - dbpedia  - http://aber-owl.net/ontology/#/    - [ontology list](http://info.slis.indiana.edu/~dingying/Teaching/S604/OntologyList.html)  - [ontology design patterns](http://ontologydesignpatterns.org/wiki/Ontology:Main)  - https://dase.cs.wright.edu/content/modl-modular-ontology-design-library  - http://www.cropontology.org    ## Cyber  - [UCO](https://github.com/ucoProject/UCO)    ## SKOS    - [AGROVOC](http://aims.fao.org/vest-registry/vocabularies/agrovoc)  - [STW Thesaurus](http://zbw.eu/stw/)  - [UNESCO Thesaurus](http://skos.um.es/unescothes/)  - https://www.w3.org/2001/sw/wiki/SKOS/Datasets """
Semantic web;https://github.com/d2rq/d2rq;"""# D2RQ – A Database to RDF Mapper    D2RQ exposes the contents of relational databases as RDF. It consists of:    * The **D2RQ Mapping Language**. Use it to write mappings between database tables and RDF vocabularies or OWL ontologies.  * The **D2RQ Engine**, a SPARQL-to-SQL rewriter that can evaluate SPARQL queries over your mapped database. It extends ARQ, the query engine that is part of Apache Jena.  * **D2R Server**, a web application that provides access to the database via the SPARQL Protocol, as Linked Data, and via a simple HTML interface.    ## Homepage and Documentation    Learn more about D2RQ at its homepage: http://d2rq.org/    ## License    Apache License, Version 2.0    http://www.apache.org/licenses/LICENSE-2.0.html    ## Contact, feedback, discussion    Please use the issue tracker here on GitHub for feature/bug discussion and support requests.    ## Building from source    ### Prerequisites    You need some tools in order to be able to build D2RQ. Depending on your operating system, they may or may not be already installed.    * [git](http://git-scm.com/), for forking the source code repository from GitHub. Run `git` on the command line to see if it's there.  * [Java JDK](http://www.oracle.com/technetwork/java/javase/downloads/index.html) v5 or later, for compiling Java sources. Run `java -version` and `javac` on the command line to see if it's there.  * [Apache Ant](http://ant.apache.org/), for building D2RQ. Run `ant` on the command line to see if it's there.    ### Getting the source    Get the code by forking the GitHub repository and cloning your fork, or directly clone the main repository:    ```git clone git@github.com:d2rq/d2rq.git```    ### Doing Ant builds    D2RQ uses Apache Ant as its build system. You can run `ant -p` from the project's main directory to get an overview of available targets:    To run the D2RQ tools, you need to do at least `ant jar`.    <table>  <tr><td>ant all</td><td>Generate distribution files in zip and tar.gz formats</td></tr>  <tr><td>ant clean</td><td>Deletes all generated artefacts</td></tr>  <tr><td>ant compile</td><td>Compile project classes</td></tr>  <tr><td>ant compile.tests</td><td>Compile test classes</td></tr>  <tr><td>ant jar</td><td>Generate project jar file</td></tr>  <tr><td>ant javadoc</td><td>Generate Javadoc API documentation</td></tr>  <tr><td>ant tar</td><td>Generate distribution file in tar.gz format</td></tr>  <tr><td>ant test</td><td>Run tests</td></tr>  <tr><td>ant vocab.config</td><td>Regenerate Config vocabulary files from Turtle source</td></tr>  <tr><td>ant vocab.d2rq</td><td>Regenerate D2RQ vocabulary files from Turtle source</td></tr>  <tr><td>ant war</td><td>Generate war archive for deployment in servlet container</td></tr>  <tr><td>ant zip</td><td>Generate distribution file in zip format</td></tr>  </table>    ## Running D2RQ    After building with `ant jar`, you can test-run the various components. Let's assume you have a MySQL database called `mydb` on your machine.    ### Generating a default mapping file    ```./generate-mapping -u root -o mydb.ttl jdbc:mysql:///mydb```    This generates a mapping file `mydb.ttl` for your database.    ### Dumping the database    ```./dump-rdf -m mydb.ttl -o dump.nt```    This creates `dump.nt`, a dump containing the mapped RDF in N-Triples format.    ### Running D2R Server    ```./d2r-server mydb.ttl```    This starts up a server at http://localhost:2020/    ### Deploying D2R Server into a servlet container    Edit `/webapp/WEB-INF/web.xml` to point the `configFile` parameter to the location of your mapping file.    Build a war file with `ant war`.    Deploy the war file, e.g., by copying it into the servlet container's `webapps` directory.    ### Running the unit tests    The unit tests can be executed with `ant test`.    Some unit tests rely on MySQL being present, and require that two databases are created:    1. A database called `iswc` that contains the data from `/doc/example/iswc-mysql.sql`:        echo ""CREATE DATABASE iswc"" | mysql -u root      mysql -u root iswc < doc/example/iswc-mysql.sql    2. An empty database called `D2RQ_TEST`. """
Semantic web;https://github.com/jeeger/ttl-mode;"""This is an Emacs mode for editing Turtle (RDF) files.    I've changed the indenting for ttl-mode somewhat to support graphs  (`{}`) better.    Original readme:    It is based on an excellent start made by Hugo Haas.  I've extended it to support indentation, some electric punctuation,  and hungry delete.    To use, download the file `ttl-mode.el` from [Bitbucket](https://bitbucket.org/nxg/ttl-mode)  (or clone the project), put the file in the emacs load path (look at the variable  load-path to find where emacs currently searches), and add something  like the following to your `.emacs` file.        (autoload 'ttl-mode ""ttl-mode"" ""Major mode for OWL or Turtle files"" t)      (add-hook 'ttl-mode-hook    ; Turn on font lock when in ttl mode                'turn-on-font-lock)      (setq auto-mode-alist            (append             (list              '(""\\.n3"" . ttl-mode)              '(""\\.ttl"" . ttl-mode))             auto-mode-alist))    Comments and contributions most welcome.      * Copyright 2003-2007, [Hugo Haas](http://www.hugoh.net)    * Copyright 2011-2012, [Norman Gray](https://nxg.me.uk)    * Copyright 2013, [Daniel Gerber](https://danielgerber.net)    * Copyright 2016, [Peter Vasil](http://petervasil.net)    `ttl-mode.el` is released under the terms of the  [two-clause BSD licence](https://opensource.org/licenses/bsd-license.php)  (see the `ttl-model.el` header for the licence text).    Norman Gray    https://nxg.me.uk """
Semantic web;https://github.com/CMU-Q/DREAM;"""#DREAM   **Distributed RDF Engine with Adaptive Query Planner and Minimal Communication**    RDF and SPARQL query language are gaining wide popularity and acceptance. **DREAM** is a hybrid RDF system, which combines the advantages and averts the disadvantages of the centralized and distributed RDF schemes. In particular, DREAM avoids partitioning RDF datasets and reversely partitions SPARQL queries. By not partitioning datasets, DREAM offers a general paradigm for different types of pattern matching queries and entirely precludes intermediate data shuffling (only auxiliary data are shuffled). By partitioning only queries, DREAM suggests an adaptive scheme, which runs queries on different numbers of machines depending on their complexities. DREAM achieves these goals and significantly outperforms related systems via employing a novel graph-based, rule-oriented query planner and a new cost model.    DREAM is implemented in C and C++, and available as open-source under the MIT License.    Download DREAM  ----------------------    You can download DREAM directly from the Github Repository. Github also offers a zip download of the repository if you do not have git.    The git command line for cloning the repository is:  ```  git clone https://github.com/az-hasan/DREAM.git  cd DREAM  ```      Building  ------------------  The current version of DREAM was tested on Ubuntu Linux 64-bit 14.04. It requires a 64-bit operating system.         Dependencies  ------------------    DREAM has the following dependencies.    1. [g++ (>= 4.8)](https://gcc.gnu.org/gcc-4.8/)  2. [MPICH (>= 3.1)](https://www.mpich.org/downloads/)  3. [Boost](http://www.boost.org/)  4. [TBB](https://www.threadingbuildingblocks.org/)     We use the [rdf3x-0.3.8](https://code.google.com/p/rdf3x/downloads/detail?name=rdf3x-0.3.8.zip&can=2&q=) engine as part of DREAM. We use the unpacked binaries [id2name and rdf3xquery](https://github.com/az-hasan/DREAM/wiki/Running-DREAM#rdf3x-binaries).    Usage   ----------------  The [Wiki entry](https://github.com/az-hasan/DREAM/wiki) provides a guide to install and run DREAM on a cluster.      Contributing  -------------------  1. Fork it ( https://github.com/[my-github-username]/DREAM/fork )  2. Create your feature branch (`git checkout -b my-new-feature`)  3. Commit your changes (`git commit -am 'Add some feature'`)  4. Push to the branch (`git push origin my-new-feature`)  5. Create a new Pull Request """
Semantic web;https://github.com/RMLio/rmlmapper-java;"""# RMLMapper <!-- omit in toc -->    [![Maven Central](https://img.shields.io/maven-central/v/be.ugent.rml/rmlmapper.svg?label=Maven%20Central)](https://search.maven.org/search?q=g:%22be.ugent.rml%22%20AND%20a:%22rmlmapper%22)    The RMLMapper execute RML rules to generate Linked Data.  It is a Java library, which is available via the command line ([API docs online](https://javadoc.io/doc/be.ugent.rml/rmlmapper)).  The RMLMapper loads all data in memory, so be aware when working with big datasets.    Want to get started quickly? Check out [Releases](#releases) on where to find the latest CLI build as a jar,  and see [Usage](#cli) on how to use the commandline interface!    ## Table of contents <!-- omit in toc -->    - [Features](#features)    - [Supported](#supported)    - [Future](#future)  - [Releases](#releases)  - [Build](#build)  - [Usage](#usage)    - [CLI](#cli)    - [Library](#library)    - [Docker](#docker)    - [Including functions](#including-functions)    - [Generating metadata](#generating-metadata)  - [Testing](#testing)    - [RDBs](#rdbs)  - [Dependencies](#dependencies)  - [Commercial Support](#commercial-support)  - [Remarks](#remarks)    - [Typed spreadsheet files](#typed-spreadsheet-files)    - [XML file parsing performance](#xml-file-parsing-performance)    - [Language tag support](#language-tag-support)    - [Duplicate removal and serialization format](#duplicate-removal-and-serialization-format)    - [I have a question! Where can I get help?](#i-have-a-question-where-can-i-get-help)  - [Documentation](#documentation)    - [UML Diagrams](#uml-diagrams)    ## Features    ### Supported  - local data sources:    - Excel (.xlsx)    - LibreOffice (.ods)    - CSV files (including CSVW)    - JSON files (JSONPath)    - XML files (XPath)  - remote data sources:    - relational databases (MySQL, PostgreSQL, Oracle, and SQLServer)    - Web APIs with W3C Web of Things    - SPARQL endpoints    - files via HTTP urls (via GET)      - CSV files      - JSON files (JSONPath (`@` can be used to select the current object.))      - XML files (XPath)  - functions (most cases)    - For examples on how to use functions within RML mapping documents, you can have a look at the [RML+FnO test cases](https://github.com/RMLio/rml-fno-test-cases)  - configuration file  - metadata generation  - output formats: nquads (default), turtle, trig, trix, jsonld, hdt  - join conditions  - targets:    - local file    - VoID dataset    - SPARQL endpoint with SPARQL UPDATE    ### Future  - functions (all cases)  - conditions (all cases)  - data sources:    - NoSQL databases    - TPF servers    ## Releases    The standalone jar file (that has a [commandline interface](#cli)) for every release can be found on the release's page on GitHub.  You can find the latest release [here](https://github.com/RMLio/rmlmapper-java/releases/latest).  This is the recommended way to get started with RMLMapper.  Do you want to build from source yourself? Check [Build](#build).    ## Build  The RMLMapper is build using Maven.  As it is also tested against Oracle (check [here](#accessing-oracle-database) for details),  it needs a specific set-up to run all tests.  That's why we recommend to build without testing: `mvn install -DskipTests=true`.  If you want, you can install with tests, and just skip the Oracle tests: `mvn test -Dtest=!Mapper_OracleDB_Test`.    A standalone jar can be found in `/target`.    Two jars are found in `/target`: a slim jar without bundled dependencies, and a standalone jar (suffixed with `-all.jar`) with all dependencies bundled.    ## Usage    ### CLI  The following options are most common.    - `-m, --mapping <arg>`: one or more mapping file paths and/or strings (multiple values are concatenated).  - `-o, --output <arg>`: path to output file  - `-s,--serialization <arg>`: serialization format (nquads (default), trig, trix, jsonld, hdt)    All options can be found when executing `java -jar rmlmapper.jar --help`,  that output is found below.    ```  usage: java -jar mapper.jar <options>  options:   -c,--configfile <arg>               path to configuration file   -d,--duplicates                     remove duplicates in the output   -dsn,--r2rml-jdbcDSN <arg>          DSN of the database when using R2RML                                       rules   -e,--metadatafile <arg>             path to output metadata file   -f,--functionfile <arg>             one or more function file paths (dynamic                                       functions with relative paths are found                                       relative to the cwd)   -h,--help                           show help info   -l,--metadataDetailLevel <arg>      generate metadata on given detail level                                       (dataset - triple - term)   -m,--mappingfile <arg>              one or more mapping file paths and/or                                       strings (multiple values are                                       concatenated). r2rml is converted to rml                                       if needed using the r2rml arguments.   -psd,--privatesecuritydata <arg>    one or more private security files                                        containing all private security                                        information such as usernames, passwords,                                        certificates, etc.   -o,--outputfile <arg>               path to output file (default: stdout)   -p,--r2rml-password <arg>           password of the database when using                                       R2RML rules   -s,--serialization <arg>            serialization format (nquads (default),                                       turtle, trig, trix, jsonld, hdt)   -t,--triplesmaps <arg>              IRIs of the triplesmaps that should be                                       executed in order, split by ',' (default                                       is all triplesmaps)   -u,--r2rml-username <arg>           username of the database when using                                       R2RML rules   -v,--verbose                        show more details in debugging output   --strict                            Enable strict mode. In strict mode, the                                        mapper will fail on invalid IRIs instead                                        of skipping them.   -b --base-IRI <arg>                 base IRI used to expand relative IRIs in                                        mapped terms. If not set and not in --strict                                        mode, will default to the @base directive                                        inside the provided mapping file.                                                                     ```    #### Accessing Web APIs with authentication    The [W3C Web of Things Security Ontology](https://www.w3.org/2019/wot/security)  is used to describe how Web APIs authentication should be performed   but does not include the necessary credentials to access the Web API.  These credentials can be supplied using the `-psd <PATH>` CLI argument.  The `PATH` argument must point to one or more private security files  which contain the necessary credentials to access the Web API.    An example can be found in the test cases   [src/test/resources/web-of-things](src/test/resources/web-of-things).    #### Accessing Oracle Database    You need to add the Oracle JDBC driver manually to the class path  if you want to access an Oracle Database.  The required driver is `ojdbc8`.    - Download `ojdbc8.jar` from [Oracle](https://www.oracle.com/database/technologies/jdbc-ucp-122-downloads.html).  - Execute the RMLMapper via     ```  java -cp 'rmlmapper.jar:ojdbc8-12.2.0.1.jar' be.ugent.rml.cli.Main -m rules.rml.ttl  ```    The options do the following:    - `-cp 'rmlmapper.jar:ojdbc8-12.2.0.1.jar'`: Put the jar of the RMLMapper and JDBC driver in the classpath.  - `be.ugent.rml.cli.Main`: `be.ugent.rml.cli.Main` is the entry point of the RMLMapper.  - `-m rules.rml.ttl`: Use the RML rules in the file `rules.rml`.ttl.  The exact same options as the ones mentioned earlier are supported.    ### Library    An example of how you can use the RMLMapper as an external library can be found  at [./src/test/java/be/ugent/rml/readme/ReadmeTest.java](https://github.com/RMLio/rmlmapper-java/blob/master/src/test/java/be/ugent/rml/readme/ReadmeTest.java)    ### Docker    #### Dockerhub    We publish our Docker images automatically on Dockerhub for every release.  You can find our images here: [rmlio/rmlmapper-java](https://hub.docker.com/r/rmlio/rmlmapper-java).    #### Build image    You can use Docker to run the RMLMapper by following these steps:    - Build the Docker image: `docker build -t rmlmapper .`.  - Run a Docker container: `docker run --rm -v $(pwd):/data rmlmapper -m mapping.ttl`.    The same parameters are available as via the CLI.  The RMLMapper is executed in the `/data` folder in the Docker container.    ### Including functions    There are two ways to include (new) functions within the RML Mapper    * dynamic loading: you add links to java files or jar files, and those files are loaded dynamically at runtime    * preloading: you register functionality via code, and you need to rebuild the mapper to use that functionality    Registration of functions is done using a Turtle file, which you can find in `src/main/resources/functions.ttl`    The snippet below for example links an fno:function to a library, provided by a jar-file (`GrelFunctions.jar`).    ```turtle  @prefix dcterms: <http://purl.org/dc/terms/> .  @prefix doap:    <http://usefulinc.com/ns/doap#> .  @prefix fno:     <https://w3id.org/function/ontology#> .  @prefix fnoi:    <https://w3id.org/function/vocabulary/implementation#> .  @prefix fnom:    <https://w3id.org/function/vocabulary/mapping#> .  @prefix grel:    <http://users.ugent.be/~bjdmeest/function/grel.ttl#> .  @prefix grelm:   <http://fno.io/grel/rmlmapping#> .  @prefix rdfs:    <http://www.w3.org/2000/01/rdf-schema#> .    grel:toUpperCase a fno:Function ;    fno:name ""to Uppercase"" ;    rdfs:label ""to Uppercase"" ;    dcterms:description ""Returns the input with all letters in upper case."" ;    fno:expects ( grel:valueParam ) ;    fno:returns ( grel:stringOut ) .    grelm:javaString      a                  fnoi:JavaClass ;      doap:download-page ""GrelFunctions.jar"" ;      fnoi:class-name    ""io.fno.grel.StringFunctions"" .    grelm:uppercaseMapping      a                    fnoi:Mapping ;      fno:function         grel:toUpperCase ;      fno:implementation   grelm:javaString ;      fno:methodMapping    [ a                fnom:StringMethodMapping ;                             fnom:method-name ""toUppercase"" ] .  ```    #### Dynamic loading    Just put the java or jar-file in the resources folder,  at the root folder of the jar-location,  or the parent folder of the jar-location,  it will be found dynamically.    > Note: the java or jar-files are found relative to the cwd.  You can change the functions.ttl path (or use multiple functions.ttl paths) using a commandline-option (`-f`).    #### Preloading    This overrides the dynamic loading.  An example of how you can use Preload a custom function can be found  at [./src/test/java/be/ugent/rml/readme/ReadmeFunctionTest.java](https://github.com/RMLio/rmlmapper-java/blob/master/src/test/java/be/ugent/rml/readme/ReadmeFunctionTest.java)    ### Generating metadata    Conform to how it is described in the scientific paper [1],  the RMLMapper allows to automatically generate [PROV-O](https://www.w3.org/TR/prov-o/) metadata.  Specifically, you need the CLI arguments below.  You can specify in which output file the metadata should be stored,  and up to which level metadata should be stored (dataset, triple, or term level metadata).    ```   -e,--metadatafile <arg>          path to output metadata file   -l,--metadataDetailLevel <arg>   generate metadata on given detail level                                    (dataset - triple - term)  ```    ## Testing    Run the tests via `test.sh`.    #### Derived tests  Some tests (Excel, ODS) are derived from other tests (CSV) using a script (`./generate_spreadsheet_test_cases.sh`)    ### RDBs  Make sure you have [Docker](https://www.docker.com) running.    #### Problems  * A problem with Docker (can't start the container) causes the SQLServer tests to fail locally. These tests will always succeed locally.  * A problem with Docker (can't start the container) causes the PostgreSQL tests to fail locally on Windows 7 machines.    ## Dependencies    | Dependency                              | License                                                            |  |:---------------------------------------:|--------------------------------------------------------------------|  | ch.qos.logback logback-classic          | Eclipse Public License 1.0 & GNU Lesser General Public License 2.1 |  | commons-cli commons-lang                | Apache License 2.0                                                 |  | com.opencsv opencsv                     | Apache License 2.0                                                 |  | commons-cli commons-cli                 | Apache License 2.0                                                 |  | org.eclipse.rdf4j rdf4j-runtime         | Eclipse Public License 1.0                                         |  | junit junit                             | Eclipse Public License 1.0                                         |  | com.jayway.jsonpath json-path           | Apache License 2.0                                                 |  | javax.xml.parsers jaxp-api              | Apache License 2.0                                                 |  | org.jsoup                               | MIT                                                                |  | mysql mysql-connector-java              | GNU General Public License v2.0                                    |  | ch.vorbuger.mariaDB4j mariaDB4j         | Apache License 2.0                                                 |  | postgresql postgresql                   | BSD                                                                |  | com.microsoft.sqlserver mssql-jdbc      | MIT                                                                |  | com.spotify docker-client               | Apache License 2.0                                                 |  | com.fasterxml.jackson.core jackson-core | Apache License 2.0                                                 |  | org.eclipse.jetty jetty-server          | Eclipse Public License 1.0 & Apache License 2.0                    |  | org.eclipse.jetty jetty-security        | Eclipse Public License 1.0 & Apache License 2.0                    |  | org.apache.jena apache-jena-libs        | Apache License 2.0                                                 |  | org.apache.jena jena-fuseki-embedded    | Apache License 2.0                                                 |  | com.github.bjdmeest hdt-java            | GNU Lesser General Public License v3.0                             |  | commons-validator commons-validator     | Apache License 2.0                                                 |  | com.github.fnoio grel-functions-java    | MIT                                                                |    ## Commercial Support    Do you need...    -   training?  -   specific features?  -   different integrations?  -   bugfixes, on _your_ timeline?  -   custom code, built by experts?  -   commercial support and licensing?    You're welcome to [contact us](mailto:info@rml.io) regarding  on-premise, enterprise, and internal installations, integrations, and deployments.    We have commercial support available.    We also offer consulting for all-things-RML.    ## Remarks    ### Typed spreadsheet files  All spreadsheet files are as of yet regarded as plain CSV files. No type information like Currency, Date... is used.    ### XML file parsing performance    The RMLMapper's XML parsing implementation (`javax.xml.parsers`) has been chosen to support full XPath.  This implementation causes a large memory consumption (up to ten times larger than the original XML file size).  However, the RMLMapper can be easily adapted to use a different XML parsing implementation that might be better suited for a specific use case.    ### Language tag support    The processor checks whether correct language tags are not, using a regular expression.  The regex has no support for languages of length 5-8, but this currently only applies to 'qaa..qtz'.    ### Duplicate removal and serialization format    Performance depends on the serialization format (`--serialization <format>`)  and if duplicate removal is enabled (`--duplicates`).  Experimenting with various configurations may lead to better performance for   your use case.    ### I have a question! Where can I get help?    Do you have any question related to writing RML mapping rules,   the RML specification, etc., feel free to ask them   here: https://github.com/kg-construct/rml-questions !  If you have found a bug or need a feature for the RMLMapper itself,   you can make an issue in this repository.    ## Documentation  Generate static files at /docs/apidocs with:  ```  mvn javadoc:javadoc  ```    ### UML Diagrams    #### Architecture UML Diagram  ##### How to generate with IntelliJ IDEA  (Requires Ultimate edition)    * Right click on package: ""be.ugent.rml""  * Diagrams > Show Diagram > Java Class Diagrams  * Choose what properties of the classes you want to show in the upper left corner  * Export to file > .png  | Save diagram > .uml    #### Sequence Diagram  ##### Edit on [draw.io](https://www.draw.io)  * Go to [draw.io](https://www.draw.io)  * Click on 'Open Existing Diagram' and choose the .html file    [1]: A. Dimou, T. De Nies, R. Verborgh, E. Mannens, P. Mechant, and R. Van de Walle, “Automated metadata generation for linked data generation and publishing workflows,” in Proceedings of the 9th Workshop on Linked Data on the Web, Montreal, Canada, 2016, pp. 1–10.  [PDF](http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_04.pdf) """
Semantic web;https://github.com/kasei/kineo;"""# Kineo    ## A persistent RDF quadstore and SPARQL engine    ### Build    `swift build -c release`    ### Swift Package Manager    You can use the [Swift Package Manager](https://swift.org/package-manager/) to add Kineo to a Swift project by adding it as a dependency in `Package.swift`:    ```swift  .package(name: ""Kineo"", url: ""https://github.com/kasei/kineo.git"", .upToNextMinor(from: ""0.0.91"")),  ```    ### Load data    Create a database file (`geo.db`) and load one or more N-Triples or Turtle files:    ```  % ./.build/release/kineo -q geo.db -d examples/geo-data/geo.ttl load  ```    Specifying `-d FILENAME` will load data from `FILENAME` into the default graph.  Alternatively, data can be loaded into a specific named graph (similarly, a  custom graph name can be used for the query default graph):    ```  % ./.build/release/kineo -q geo.db -g http://example.org/dbpedia examples/geo-data/geo.ttl load  ```    ### Query    Querying of the data can be done using SPARQL:    ```  % cat examples/geo-data/geo.rq  PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>  SELECT  ?s  WHERE {  	?s geo:lat ?lat ;  	   geo:long ?long ;  	FILTER(?long < -120)  	FILTER(?lat >= 34.0)  	FILTER(?lat <= 35.0)  }  ORDER BY ?s    % ./.build/release/kineo -q geo.db query examples/geo-data/geo.rq  Using default graph <file://examples/geo-data/geo.ttl>  1	Result[s: <http://dbpedia.org/resource/Buellton,_California>]  2	Result[s: <http://dbpedia.org/resource/Lompoc,_California>]  3	Result[s: <http://dbpedia.org/resource/Los_Alamos,_California>]  4	Result[s: <http://dbpedia.org/resource/Mission_Hills,_California>]  5	Result[s: <http://dbpedia.org/resource/Orcutt,_California>]  6	Result[s: <http://dbpedia.org/resource/Santa_Barbara_County,_California>]  7	Result[s: <http://dbpedia.org/resource/Santa_Maria,_California>]  8	Result[s: <http://dbpedia.org/resource/Santa_Ynez,_California>]  9	Result[s: <http://dbpedia.org/resource/Solvang,_California>]  10	Result[s: <http://dbpedia.org/resource/Vandenberg_Air_Force_Base>]  ```    ### Kineo API    The Kineo API can be used to create an in-memory or persistent quadstore,  load RDF data into it, and evaluate SPARQL queries over the data:    ```swift  import Foundation  import SPARQLSyntax  import Kineo    let graph = Term(iri: ""http://example.org/default-graph"")  let store = MemoryQuadStore()    let url = URL(string: ""http://kasei.us/about/foaf.ttl"")!  try store.load(url: url, defaultGraph: graph)    let sparql = ""PREFIX foaf: <http://xmlns.com/foaf/0.1/> SELECT * WHERE { ?person a foaf:Person ; foaf:name ?name }""  let q = try SPARQLParser.parse(query: sparql)  let results = try store.query(q, defaultGraph: graph)  for (i, result) in results.bindings.enumerated() {      print(""\(i+1)\t\(result)"")  }  ```    There is also an API that exposes the RDF data in terms of graph vertices and edge traversals:    ```swift  import Foundation  import SPARQLSyntax  import Kineo    let graph = Term(iri: ""http://example.org/default-graph"")  let store = MemoryQuadStore()    let url = URL(string: ""http://kasei.us/about/foaf.ttl"")!  try store.load(url: url, defaultGraph: graph)    let graphView = store.graph(graph)  let greg = graphView.vertex(Term(iri: ""http://kasei.us/about/#greg""))    let knows = Term(iri: ""http://xmlns.com/foaf/0.1/knows"")  let name = Term(iri: ""http://xmlns.com/foaf/0.1/name"")  for v in try greg.outgoing(knows) {      let names = try v.outgoing(name)      if let nameVertex = names.first {          let name = nameVertex.term          print(""Greg know \(name)"")      }  }  ```    ### SPARQL Endpoint    Finally, using the companion [kineo-endpoint](https://github.com/kasei/kineo-endpoint) package,  a SPARQL endpoint can be run allowing SPARQL Protocol clients to access the data:    ```  % kineo-endpoint -q geo.db &  % curl -H ""Accept: application/sparql-results+json"" -H ""Content-Type: application/sparql-query"" --data 'PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> SELECT ?s ?lat ?long WHERE { ?s geo:lat ?lat ; geo:long ?long } LIMIT 3' 'http://localhost:8080/sparql'  {    ""head"": {      ""vars"": [ ""s"", ""lat"", ""long"" ]    },    ""results"": {      ""bindings"": [        {          ""s"": { ""type"": ""uri"", ""value"": ""http://dbpedia.org/resource/'s-Gravendeel"" },          ""lat"": { ""type"": ""literal"", ""value"": ""5.17833333333333E1"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" },          ""long"": { ""type"": ""literal"", ""value"": ""4.61666666666667E0"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" }        },        {          ""s"": { ""type"": ""uri"", ""value"": ""http://dbpedia.org/resource/'s-Hertogenbosch"" },          ""lat"": { ""type"": ""literal"", ""value"": ""5.17833333333333E1"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" },          ""s"": { ""type"": ""uri"", ""value"": ""http://dbpedia.org/resource/Groesbeek"" },          ""long"": { ""type"": ""literal"", ""value"": ""5.93333333333333E0"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" }        },        {          ""s"": { ""type"": ""uri"", ""value"": ""http://dbpedia.org/resource/'s-Hertogenbosch"" },          ""lat"": { ""type"": ""literal"", ""value"": ""5.1729918E1"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" },          ""long"": { ""type"": ""literal"", ""value"": ""5.306938E0"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" }        }      ]    }  }  ``` """
Semantic web;https://github.com/kasei/URITemplate;"""URITemplate  ===========    Swift implementation of URI Template ([RFC6570](https://tools.ietf.org/html/rfc6570)).    ## Installation    [CocoaPods](http://cocoapods.org/) is the recommended installation method.    ```ruby  pod 'URITemplate'  ```    ## Example    ### Expanding a URI Template    ```swift  let template = URITemplate(template: ""https://api.github.com/repos/{owner}/{repo}/"")  let url = template.expand([""owner"": ""kylef"", ""repo"": ""URITemplate.swift""])  => ""https://api.github.com/repos/kylef/URITemplate.swift/""  ```    ### Determine which variables are in a template    ```swift  let variables = template.variables  => [""owner"", ""repo""]  ```    ### Extract the variables used in a given URL    ```swift  let variables = template.extract(""https://api.github.com/repos/kylef/PathKit/"")  => [""owner"":""kylef"", ""repo"":""PathKit""]  ```    ## [RFC6570](https://tools.ietf.org/html/rfc6570)    The URITemplate library follows the [test suite](https://github.com/uri-templates/uritemplate-test).    We have full support for level 4 of RFC6570 when expanding a template and retrieving the variables in a template.    For extraction of variables from an already expanded template, level 3 is supported.    ## License    URITemplate is licensed under the MIT license. See [LICENSE](LICENSE) for more  info. """
Semantic web;https://github.com/tenforce/SPARQL-parser;"""# SPARQL parser  The SPARQL parser is a library that helps to parse and investigate a SPARQL query and to build up and generate SPARQL queries.    ## Usage  To parse a query you can run:  ```  SPARQLQuery parsedQuery = new SPARQLQuery(""SELECT * FROM <http://graph1> WHERE { ?s ?p ?o . }"");  ```  The parsedQuery object will then contain a java object representation of that query. It will have a hashMap with prefix objects, a type , a list of IStatements, a set of unknowns, possibly a graph and the original query.    ### Prefix objects  Those are quiet simple, they map a name on a URL.    ### Type  The following types are supported  * SPARQLQuery.Type.ASK  * SPARQLQuery.Type.DESCRIBE  * SPARQLQuery.Type.SELECT  * SPARQLQuery.Type.CONSTRUCT  * SPARQLQuery.Type.UPDATE     ### IStatement  The IStatement interface is an abstraction of SPARQL statements. It includes 'select blocks', 'where blocks', 'update blocks', 'parentheses blocks' and 'simple statements'. These blocks support methods to extract the unknowns, inner blocks (for instance a select block contains a parentheses block and that again contains multiple simple statements), the graph on which it operates as well as some functional methods (to change the graph for instance).    ### Unknown  This is just a java String that holds the name of the variable in the query without the '?' (ie. ?mu becomes ""mu"")    ### Graph  This is also just a java String.    ## Installation  Adding to the pom:  ```  <dependency>    <groupId>com.tenforce.semtech</groupId>    <artifactId>SPARQL-parser</artifactId>    <version>0.0.3</version>  </dependency>  ``` """
Semantic web;https://github.com/SemWebCentral/parliament;"""# News    April 17, 2019:  Released Parliament™ version 2.7.12.  Changes of note:    * Parliament was recently moved to this GitHub project from its long-time home on [SemWebCentral](http://parliament.semwebcentral.org/)    * Added a SWRL rules engine    * Added a script to install Parliament as a service/daemon on systemd-based Linux distributions, including CentOS and Ubuntu    * Long-running queries now time out    * Added linear growth settings for Parliament's resource and statement tables    * The inference engine now recognizes classes to be subclasses of themselves    * Decreased the likelihood that an ungraceful shutdown will corrupt the data files    * Fixed numerous bugs        # Parliament™ Introduction    Parliament™ is a high-performance triple store and reasoner designed for the [Semantic Web](http://www.w3.org/2001/sw/).  Parliament's initial development was funded by DARPA through the DAML program under the name [DAML DB](http://www.daml.org/2001/09/damldb/) and was extended by Raytheon BBN Technologies (BBN) for internal use in its R&D programs.  BBN released Parliament as an open source project under the [BSD license](http://opensource.org/licenses/bsd-license.php) on [SemWebCentral](http://parliament.semwebcentral.org/) in 2009.  In 2018, BBN migrated the Parliament open source project to [GitHub](https://github.com/SemWebCentral/parliament) under the same license.    Parliament™ is a trademark of Raytheon BBN Technologies.  It is so named because a group of owls is properly called a _parliament_ of owls. """
Semantic web;https://github.com/joshsh/ripple;"""<!-- This README can be viewed at https://github.com/joshsh/ripple/wiki -->    ![Ripple logo|width=420px|height=100px](https://github.com/joshsh/ripple/wiki/graphics/ripple-logo-text-medium.png)    Welcome to the Ripple wiki!  Ripple is a functional, stack-based query language for Linked Data and other RDF data sources.  Ripple programs resemble path expressions as in [XPath](http://www.w3.org/TR/xpath/)  and postfix-style procedures as in  [Forth](http://en.wikipedia.org/wiki/Forth_&#40;programming_language\&#41;).  Every program has an [RDF](http://www.w3.org/RDF/) representation,  so you can embed programs in the Web of Data as well as querying against it.  The implementation is written in Java and includes an interactive command-line interpreter as well as a query API which interoperates with [Sesame 4.1](http://rdf4j.org/).    ## Contents    * [Running Ripple](https://github.com/joshsh/ripple/wiki/Running-Ripple): getting the software, using the command-line interpreter, and embedding Ripple in Java programs  * Examples      * Ripple on [Linked Data](https://github.com/joshsh/ripple/wiki/ripple-on-linked-data)      * Ripple on [JSON](https://github.com/joshsh/ripple/wiki/ripple-on-json)      * [The Web of Programs](https://github.com/joshsh/ripple/wiki/The-Web-of-Programs): publishing Ripple programs as Linked Data  * Language reference      * [Syntax](https://github.com/joshsh/ripple/wiki/Syntax): Ripple's RDF-oriented syntax for commands and queries      * [Commands](https://github.com/joshsh/ripple/wiki/Commands): how to define programs and inspect the scripting environment  * Libraries and primitives      * Core libraries          * [control library](https://github.com/joshsh/ripple/wiki/control-library): mappings and program flow, regular expressions, looping and branching          * [data library](https://github.com/joshsh/ripple/wiki/data-library): atomic values and datatypes, comparison, type conversion          * [graph library](https://github.com/joshsh/ripple/wiki/graph-library): reading and writing RDF statements, SPARQL support, key/value objects and JSON          * [logic library](https://github.com/joshsh/ripple/wiki/logic-library): boolean algebra          * [math library](https://github.com/joshsh/ripple/wiki/math-library): arithmetic, roots and exponentials, trigonometry           * [stack library](https://github.com/joshsh/ripple/wiki/stack-library): list- and stack-oriented primitives inherited from [Joy](http://en.wikipedia.org/wiki/Joy_(programming_language))          * [stream library](https://github.com/joshsh/ripple/wiki/stream-library): stream splitting and intersection, filters for deduplication and pruning, closed-world operations          * [string library](https://github.com/joshsh/ripple/wiki/string-library): string manipulation          * [system library](https://github.com/joshsh/ripple/wiki/system-library): system calls and network operations, other scripting languages      * Extensions          * [media library](https://github.com/joshsh/ripple/wiki/media-library): primitives for playing audio, showing images, and speaking text          * [blueprints library](https://github.com/joshsh/ripple/wiki/blueprints-library): graph traversal on the [Blueprints](https://github.com/tinkerpop/blueprints/wiki/) API  * Miscellaneous      * [Ripple configuration properties](https://github.com/joshsh/ripple/wiki/Ripple-configuration-properties)      * [LinkedDataSail](https://github.com/joshsh/ripple/wiki/LinkedDataSail): Ripple's dynamic view of the Web of Data      * [Naming conventions](https://github.com/joshsh/ripple/wiki/Naming-conventions) for Ripple programs      * [creating a Ripple library](https://github.com/joshsh/ripple/wiki/creating-a-Ripple-library)  * External links      * Ripple [JavaDocs](http://fortytwo.net/projects/ripple/api/latest)      * [Functional programs as Linked Data](http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-248/paper10.pdf) (the original paper on Ripple)      * The [demo screencast](http://ripple.googlecode.com/svn/trunk/docs/screencast/index.html) from the [SFSW 2007 Scripting Challenge](http://web.archive.org/web/20120326083323/http://www.semanticscripting.org/SFSW2007/)   """
Semantic web;https://github.com/IBCNServices/StreamingMASSIF;"""# StreamingMASSIF    This is the implementation of the *StreamingMASSIF* platform, a streaming extension of the MASSIF platform.    StreamingMASSIF allows to perform cascading reasoning by combining various components. In its standard configuration it allows to filter meaningful events from a datastream through RDF Stream Processing, abstract the selection through DL reasoning and perform Complex Event Processing ontop of these abstraction.    Check the [wikipage](https://github.com/IBCNServices/StreamingMASSIF/wiki) for a more in depth explanation on how to use Streaming MASSIF!    How to cite [Streaming MASSIF](https://www.mdpi.com/1424-8220/18/11/3832):  ```  @article{bonte2018streaming,    title={Streaming MASSIF: Cascading Reasoning for Efficient Processing of IoT Data Streams},    author={Bonte, Pieter and Tommasini, Riccardo and Della Valle, Emanuele and De Turck, Filip and Ongenae, Femke},    journal={Sensors},    volume={18},    number={11},    pages={3832},    year={2018},    publisher={Multidisciplinary Digital Publishing Institute}  }  ```    How to cite [MASSIF](https://link.springer.com/article/10.1007/s10115-016-0969-1):  ```  @article{bonte2017massif,    title={The MASSIF platform: a modular and semantic platform for the development of flexible IoT services},    author={Bonte, Pieter and Ongenae, Femke and De Backere, Femke and Schaballie, Jeroen and Arndt, D{\""o}rthe and Verstichel, Stijn and Mannens, Erik and Van de Walle, Rik and De Turck, Filip},    journal={Knowledge and Information Systems},    volume={51},    number={1},    pages={89--126},    year={2017},    publisher={Springer}  }  ```    ## Building and running MASSIF    ### Requirements:    - Java 9+  - Maven2    ### Build  To build the MASSIF app, call `mvn` and get the compiled `.jar`.  ```shell  mvn clean compile assembly:single  mv target/massif-jar-with-dependencies.jar .  ```    To build the MASSIF classes (e.g. for usage in higher level apps), call `mvn` to compile and install the project in the local repository, then add the package to the higher level app dependencies (see `pom.xml` snippet below):    ```shell  mvn install -Dmaven.test.skip=true   ```  ```xml  <dependency>      <groupId>be.ugent.idlab</groupId>      <artifactId>massif</artifactId>      <version>0.0.1</version>  </dependency>  ```    ### Run  To run MASSIF, call the compiled `.jar` from the command line as follows:  ```shell  java -jar -Dlog4j.configurationFile=webfiles/log4j2.xml massif-jar-with-dependencies.jar  ```  Calling this command will return something like this on the CLI:  ```shell  21:27:32.313 [main] INFO  idlab.massif.run.Run - MASSIF STARTING  21:27:32.403 [main] INFO  idlab.massif.run.Run - MASSIF Listening on port 9000  21:27:32.403 [main] INFO  idlab.massif.run.Run - Access the MASSIF GUI on  localhost:9000 or register a configuration on localhost:9000/register  21:27:32.414 [main] INFO  idlab.massif.run.Run - MASSIF is ONLINE  ```  Run options:  * `-p` : TCP port on which massif listens, default: `9000`    GUI:  * The MASSIF GUI is available on http://localhost:9000    ### REST API  The MASSIF allow for direct management through GET/POST calls.  Here are the most important path:  * `<massif_url>/register`: register a configuration  * `<massif_url>/stop` : stop a certain query  * `<massif_url>/configs` : get all registered configs    ### REST API call examples with the [cURL](https://linuxize.com/post/curl-post-request/) tool  List of active configs:  ```  curl -X GET --verbose \    --url ""http://127.0.0.1:9000/configs""  ```  * Note: returns `{}` if there are no active config.    Stop a config (pre-requisite:  config ID, see previous command):  ```  curl -X POST --verbose \    -H ""Content-Type: application/json"" \    -d '19' \    --url ""http://127.0.0.1:9000/stop""  ```  * Note: returns `ok`    Send a config:  ```  curl -X POST --verbose \    -H ""Content-Type: application/json"" \    -d '{""configuration"":{""0"":[1],""1"":[]},""components"":{""0"":{""type"":""Source"",""impl"":""kafkaSource"",""kafkaServer"":""127.0.0.1:9092"",""kafkaTopic"":""backblaze_smart""},""1"":{""type"":""Sink"",""impl"":""httpGetSinkCombined"",""path"":""1"",""config"":""""}}}' \    --url ""http://127.0.0.1:9000/register""  ```  * Note: returns the config ID    Get a component running metrics (pre-requisite: component ID, see previous commands):  ```  curl -X GET --verbose \    --url ""http://127.0.0.1:9000/monitor/1""  ```     """
Semantic web;https://github.com/oeg-upm/OME;"""  ![OME](static/logo-min.png)    [![Build Status](https://ahmad88me.semaphoreci.com/badges/Morph-OME/branches/master.svg)](https://ahmad88me.semaphoreci.com/projects/Morph-OME)  [![codecov](https://codecov.io/gh/oeg-upm/Morph-OME/branch/master/graph/badge.svg?token=TsSWMQGuoO)](https://codecov.io/gh/oeg-upm/Morph-OME)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3764202.svg)](https://doi.org/10.5281/zenodo.3764202)      An Online Mapping Editor to generate R2RML, RML, and YARRRML without writing a single line.  It also supports automatic suggestions of the subject and property columns using   the APIs of [tada_web](https://github.com/oeg-upm/tada-web).      <!--  # Run with Docker  1. `sh run_docker.sh`  2. In the browser visit `http://127.0.0.1:5000`      # How to install it locally  1. Create virtual environment [here](https://docs.python-guide.org/dev/virtualenvs/) (recommended by not required) e.g. ```virtualenv -p /usr/bin/python2.7 .venv```  1. Access the virtual environment using `source .venv/bin/activate`  1. Install pip [here](https://pip.pypa.io/en/stable/installing/)  1. Install requirements ``` pip install -r requirements.txt ```  1. Set `TADA_HOST` to the url of the pytada_hdt_entity host. For example (`export TADA_HOST=""http://127.0.0.1:5001/`)  1. Run the application ``` python app.py ```  1. Open the browser to the url [http://127.0.0.1:5000/](http://127.0.0.1:5000/)    -->    # Automatic Suggestions  It uses the APIs of[tada_web](https://github.com/oeg-upm/tada-web). To use it, you need to export an environment variable `TADA_HOST` with the   URL of the `tada-web` host url.  For example, you can set it like that  `export TADA_HOST=""http://127.0.0.1:5001/""`          # Environment Variables  * `SECRET_KEY`:      * A random text  * `TADA_HOST`:      * (Optional)      * The URL of TADA APIs. If it is missing, the class and properties won't be annotated automatically      * Default: """"  * `UPLOAD_ONTOLOGY`:       * (Optional)      * To show/hide an ontology upload page (in the main page) for the autocomplete functionality      * Default: True  * `github_secret`:      * Github app secret    * `github_appid`:      * Github app ID  * `MORPH_PATH`:      * The local path to morph-rdb jar to generate ttl  * `RMLMAPPER_PATH`:      * The local path to rmlmapper jar to generate the ttl      ## To activate_this.py   You can add these environment variables to `activate_this.py` in the virtualenv bin directory.  ```  os.environ['SECRET_KEY']=""""  os.environ['github_appid']=""""  os.environ['github_secret']=""""  os.environ['UPLOAD_ONTOLOGY']=""false""  os.environ['RMLMAPPER_PATH']=""""  os.environ['TADA_HOST']=""""  ```    ## To a shell  ```  export SECRET_KEY=""""  export github_appid=""""  export github_secret=""""  export UPLOAD_ONTOLOGY=""false""  export RMLMAPPER_PATH=""""  export TADA_HOST=""""  ```      <!--  # Screenshot  ![screenshot](https://github.com/oeg-upm/OME/raw/master/screenshot.png)  -->    # Remarks  * To run the application on a specific port (e.g. say port 5001) ``` python app.py 5001```.  * To run the application on a specific port (e.g. say port 5001, and any given host 0.0.0.0) ``` python app.py 0.0.0.0 5001```.    # To cite  ```  @software{alobaid_ahmad_2020_3764202,    author       = {Alobaid, Ahmad and                    Corcho, Oscar},    title        = {OME},    month        = apr,    year         = 2020,    publisher    = {Zenodo},    doi          = {10.5281/zenodo.3764202},    url          = {https://doi.org/10.5281/zenodo.3764202}  }  ```   """
Semantic web;https://github.com/cosminbasca/rdftools;"""rdftools  ========    rdftools is a python wrapper over a number of RDF related tools  * rdf parsers / serializers  * void utilities  * lubm generator  * etc    Important Notes  ---------------  This software is the product of research carried out at the [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and comes with no warranty whatsoever. Have fun!    TODO's  ------  * The project is not documented (yet)    How to Compile/Install the Project  ----------------------------------  Ensure that *libraptor2* v2.0.13+ and *cityhash* are installed on your system (either using the package manager of the OS or compiled from source).    To install **rdftools** you have two options: 1) manual installation (install requirements first) or 2) automatic with **pip**    **Manual** installation:  ```sh  $ git clone https://github.com/cosminbasca/rdftools  $ cd rdftools  $ python setup.py install  ```    Install the project with **pip**:  ```sh  $ pip install https://github.com/cosminbasca/rdftools  ```    Also have a look at the build.sh, clean.sh, test.sh scripts included in the codebase     To include the latest JVM RDF tools update to the latest of [jvmrdftools](https://github.com/cosminbasca/jvmrdftools) and create an assembly:    ```sh  $ sbt compile assembly  ```    copy the resulting jar from the target folder to the *lib* folder inside the *rdftools.tools.jvmrdftools* module and reinstall the python package.      The tools  ---------    To find out what a tool does, simply supply the *--help* comand line argument to any of the tools  Available tools:    * rdfconvert, convert RDF files from source format to a destination format using the *libraptor2* C RDF parser    ```sh  usage: rdfconvert [-h] [--clear] [--dst_format DST_FORMAT]                    [--buffer_size BUFFER_SIZE] [--version]                    SOURCE    rdftools v0.9.2, rdf converter, based on libraptor2    positional arguments:    SOURCE                the source file or location (of files) to be converted    optional arguments:    -h, --help            show this help message and exit    --clear               clear the original files (delete) - this action is                          permanent, use with caution!    --dst_format DST_FORMAT                          the destination format to convert to. Supported                          parsers: ['rdfxml', 'ntriples', 'turtle', 'trig',                          'guess', 'rss-tag-soup', 'rdfa', 'nquads', 'grddl'].                          Supported serializers ['rdfxml', 'rdfxml-abbrev',                          'turtle', 'ntriples', 'rss-1.0', 'dot', 'html',                          'json', 'atom', 'nquads'].    --buffer_size BUFFER_SIZE                          the buffer size in Mb of the input buffer (the parser                          will only parse XX Mb at a time)    --version             the current version  ```    * rdfconvert2 convert RDF files from source format to a destination format using the *rdf2rdf* java RDF parser    ```sh  usage: rdfconvert2 [-h] [--clear] [--dst_format DST_FORMAT]                     [--workers WORKERS] [--version]                     SOURCE    rdftools v0.9.2, rdf converter (2), makes use of rdf2rdf bundled - requires  java    positional arguments:    SOURCE                the source file or location (of files) to be converted    optional arguments:    -h, --help            show this help message and exit    --clear               clear the original files (delete) - this action is                          permanent, use with caution!    --dst_format DST_FORMAT                          the destination format to convert to    --workers WORKERS     the number of workers (default -1 : all cpus)    --version             the current version  ```    * rdfencode, endode an ntriples file to a binary format (each S, P, O string is hashed with *cityhash* 64 bit)    ```sh  usage: rdfencode [-h] [--version] SOURCE    rdftools v0.9.2, encode the RDF file(s)    positional arguments:    SOURCE      the source file or location (of files) to be encoded    optional arguments:    -h, --help  show this help message and exit    --version   the current version  ```    * genlubm, generate a **LUBM** dataset (in parallel)    ```sh  usage: genlubm [-h] [--univ UNIV] [--index INDEX] [--seed SEED]                 [--ontology ONTOLOGY] [--workers WORKERS] [--version]                 OUTPUT    rdftools v0.9.2, lubm dataset generator wrapper (bundled) - requires java    positional arguments:    OUTPUT               the location in which to save the generated                         distributions    optional arguments:    -h, --help           show this help message and exit    --univ UNIV          number of universities to generate    --index INDEX        start university    --seed SEED          the seed    --ontology ONTOLOGY  the lubm ontology    --workers WORKERS    the number of workers (default -1 : all cpus)    --version            the current version  ```    * genlubmdistro generate a **LUBM** dataset (in parallel) and mix the universities to *N* sites with the specified distribution    ```sh  usage: genlubmdistro [-h] [--distro DISTRO] [--univ UNIV] [--index INDEX]                       [--seed SEED] [--ontology ONTOLOGY] [--pdist PDIST]                       [--sites SITES] [--clean] [--workers WORKERS] [--version]                       OUTPUT    rdftools v0.9.4, lubm dataset generator wrapper (bundled) - requires java    positional arguments:    OUTPUT               the location in which to save the generated                         distributions    optional arguments:    -h, --help           show this help message and exit    --distro DISTRO      the distibution to use, valid values are ['seedprop',                         'uni2many', 'horizontal', 'uni2one']    --univ UNIV          number of universities to generate    --index INDEX        start university    --seed SEED          the seed    --ontology ONTOLOGY  the lubm ontology    --pdist PDIST        the probabilities used for the uni2many distribution,                         valid choices are ['3S', '7S', '5S'] or file with                         probabilities split by line    --sites SITES        the number of sites    --clean              delete the generated universities    --workers WORKERS    the number of workers (default -1 : all cpus)    --version            the current version  ```  * genvoid, generate [VoID](http://www.w3.org/TR/void/) statistics from the source file    ```sh  usage: genvoid [-h] [--version] SOURCE    rdftools v0.9.2, generate void statistics for RDF source file    positional arguments:    SOURCE      the source file to be analized    optional arguments:    -h, --help  show this help message and exit    --version   the current version  ```    * genvoid2, generate [VoID](http://www.w3.org/TR/void/) statistics from the RDF source file, using the *nxparser VoID* exporter    ```sh  usage: genvoid2 [-h] [--dataset_id DATASET_ID] [--use_nx] [--version] SOURCE    rdftools v0.9.2, generate a VoiD descriptor using the nxparser java package    positional arguments:    SOURCE                the source file to be analized    optional arguments:    -h, --help            show this help message and exit    --dataset_id DATASET_ID                          dataset id    --use_nx              if true (default false) use the nx parser builtin void                          generator    --version             the current version  ```    * ntround, round all numeric literals (typed or untyped) in an ntriples files with the given precision    ```sh  usage: ntround [-h] [--prefix PREFIX] [--precision PRECISION] [--version] PATH    rdftools v0.9.2, rounds ntriple files in a folder, (rounds the floating point literals)    positional arguments:    PATH                  location of the indexes    optional arguments:    -h, --help            show this help message and exit    --prefix PREFIX       the prefix used for files that are transformed, cannot                          be the enpty string!    --precision PRECISION                          the precision to round to, if 0, floating point                          numbers are rounded to long    --version             the current version  ```    Thanks a lot to  ---------------  * [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Swiss National Science Foundation](http://www.snf.ch/en/Pages/default.aspx) for generously funding the research that led to this software. """
Semantic web;https://github.com/AKSW/GeoLift;"""GeoLift  =======    Framework to enrich geographic content in the Semantic Web"""
Semantic web;https://github.com/knakk/fenster;"""## Fenster  Fenster is a fronted for RDF quad-stores.    It is inspired by, and similar to [Pubby](http://wifo5-03.informatik.uni-mannheim.de/pubby/), but differs in that it shows triples from all public graphs, not just the default graph.    Example of how a resolvable URI, [http://data.deichman.no/resource/tnr_1140686](http://data.deichman.no/resource/tnr_1140686), is presented in Fenster:  ![screenshot](https://dl.dropboxusercontent.com/u/27551242/azur.png)    ### Status  Fenster is stable and has been in production since November 2013 as a frontend for the [RDF-catalogue](http://data.deichman.no) of [Oslo public library](http://www.deichman.no). Currently it's only been tested against Virtuoso, but presumably any compliant SPARQL endpoint should work. Please let us know if you run into any issues.    ### Deployment  Fenster is written in Go, so you'll need the [Go toolchain](http://golang.org/doc/install) in order to build. It compiles to a statically linked binary, so deployment couldn't be simpler:    1. `git clone https://github.com/knakk/fenster`  2. `cd fenster`  3. `make package`*  4. copy `fenster.tar.gz` to your server  5. unpack, adjust settings in `config.ini` and run `fenster`    *You have to do the build step manually if your target platform is of a different architecture than your compilation platform.  [See this guide](http://dave.cheney.net/2012/09/08/an-introduction-to-cross-compilation-with-go) if you don't know where to start.    If you're on Ubuntu, you might want to deploy Fenster as an Upstart service. Example config:  ```upstart  description ""Fenster""    start on filesystem or runlevel [2345]  stop on runlevel [!2345]    respawn    chdir /path/to/fenster  exec ./fenster  ```    #### Apache routing  If Fenster is running on same server as the RDF-store, you'll have to proxy the requests to the SPARQL endpoint.    Here is an example Apache config, given Fenster running on localhost:8080 and SPARQL endpoint running on localhost:8890/sparql:    ```apache  <VirtualHost *:80>        ServerAdmin serveradmin@example.no      DocumentRoot /var/www/example.com      ServerName example.com        ProxyRequests off      ProxyPreserveHost on      ProxyTimeout        300      # Proxy ACL      <Proxy *>          Order allow,deny          Allow from all      </Proxy>      RewriteEngine on      RewriteRule ^/sparql(.*)$ http://localhost:8890/sparql$1 [P]        # default proxy if not handled above      ProxyPass / http://example.com:8080/ timeout=300      ProxyPassReverse / http://example.com:8080/    </VirtualHost>  ```      ### License  GPLv3 """
Semantic web;https://github.com/kbss-cvut/jopa;"""# JOPA - Java OWL Persistence API    [![Build Status](https://kbss.felk.cvut.cz/jenkins/buildStatus/icon?job=jopa-stable)](https://kbss.felk.cvut.cz/jenkins/job/jopa-stable)    JOPA is a Java OWL persistence framework aimed at efficient programmatic access to OWL2 ontologies and RDF graphs in Java. The system is based  on integrity constraints [1] in OWL that JOPA uses to establish the contract between a JOPA-enabled Java application and  an OWL ontology. The system architecture and API is similar to JPA 2.1, see [2].    Notable changes:    * **0.17.0** - Support for SPARQL-based entity attributes and Criteria API. See the [wiki](https://github.com/kbss-cvut/jopa/wiki) for more details.  * **0.15.0** - Support for multilingual String attributes. See the [wiki](https://github.com/kbss-cvut/jopa/wiki/Multilingual-String-Attributes) for more info  * **0.14.0** - Support for the Semantic Object Query Language (SOQL). See the [wiki](https://github.com/kbss-cvut/jopa/wiki/Semantic-Object-Query-Language) for more info  * **0.13.0** - Support for RDF simple literals (language-less strings), access to lexical form of literals, updated named graph handling  * **0.12.0** - Support for `EntityManager.getReference`  * **0.11.0** - Support for Java 8 Streams in Query API and RDF4J repository configuration file  * **0.10.0** - Jena OntoDriver implementation  * **0.9.0** - Single class inheritance support  * **0.8.0** - Mapped superclass support    ### Main Features    * Object-ontological mapping (OOM) based on integrity constraints,  * Explicit access to inferred knowledge,  * Access to unmapped properties and individual's types,  * Transactions,  * Separate storage access layer - Jena, OWLAPI, RDF4J (Sesame) drivers are available.    #### Object-ontological mapping based on integrity constraints    Similarly to ORM, OOM enables to map ontological constructs to constructs of an object-oriented programming language and vice versa.    More specifically, OOM in JOPA maps (using the JLS [3] terminology):    | Ontology   | OO Language    |  | ---------- | -------------- |  | OWL Class  | Reference type |  | Object property | Reference type member |  | Data property | Primitive type member (+ String, Date) |  | Annotation property | Reference or primitive type member |  | Class assertions | Reference type instance or @Types record |    All this means that individuals belonging to an OWL class can be retrieved as instances of a (Java) class.    #### Inheritance    * Support for mapped superclasses was added in version __0.8.0__.  * Support for single inheritance was added in version __0.9.0__.    #### Explicit access to inferred knowledge    A member annotated with the `@Inferred` annotation represents a field whose values are retrieved using a reasoner. As such,  they can for example contain values of a inverse object property (like in [our Jedi example](https://github.com/kbss-cvut/jopa-examples/blob/master/example02-jopa-owlapi/src/main/java/cz/cvut/kbss/jopa/example02/Example.java)).    There are limitations to this: JOPA requires explicit class assertion to be able to load individual as instance of a class.  And, values of inferred members are read-only. These restrictions have pragmatic reasons - if the knowledge is inferred, it  cannot be directly modified/removed. Therefore, it would make no sense to remove object property value, if it was inferred.    #### Access to unmapped properties and individual's types    OOM is not meant to completely capture the ontological model. It would not even make much sense. One of the main features  of JOPA is its ability to work with knowledge which is not part of the object model. This is done using members annotated  with `@Types` and `@Properties`. `@Types` field contains all OWL classes whose instance the particular individual represented by  an object is. `@Properties` field contains values of properties not mapped by object model. This way, the application gets (although limited)  access to unmapped property values (e.g. values of newly added properties), without the need to adjust the object model and recompile.    #### Transactions    JOPA supports object-level transactions. In addition, it makes transactional change visible to the transaction that made them.  This means that when you add an instance of some class during a transaction and then list all instances of that class (during the same  transaction), you'll see the newly added instance as well. This is a feature not usually seen even in large triple stores.    There are some limitations to this approach. Currently, pending changes are not taken into account when doing inference.  Also, the current version of Sesame OntoDriver is not able to include pending changes into results of SPARQL queries.    #### Separate storage access layer    Similarly to JPA and JDBC driver, JOPA sits on top of an OntoDriver instance, which provides access to the underlying storage.  There are two main reasons for such split - first, it decouples storage-specific API usage from the more generic OOM core.  Second, it enables the application to switch the underlying storage with as little as 2-3 lines of configuration code. Nothing else  needs to be modified.    Supported storages:    * Jena (since **0.10.0**)  * OWLAPI  * Sesame/RDF4J    ### Not Supported, yet    JOPA currently does not support two important features - multiple inheritance and referential integrity.    Single inheritance (and mapped superclasses) have been supported since version 0.9.0 (0.8.0 resp.), multiple inheritance is currently in planning.    As for referential integrity, this for example means that removing an instance that is referenced by another instance should  not be possible. Such feature is vital for object-oriented application, but not compatible with the open-world nature of ontologies.  Design possibilities and their implications are currently being studied.    Other missing/planned stuff can be found in [TODO.md](TODO.md) and in the GitHub issue tracker.    ## Modules    The whole framework consists of several modules:    * _JOPA API_ - definition of the JOPA API, similar to JPA,  * _OntoDriver API_ - API of the storage access layer,  * _JOPA Implementation_ - persistence provider implementation,  * _OntoDriver Sesame_ - OntoDriver implementation for RDF4J (Sesame)-accessed storages,  * _OntoDriver OWLAPI_ - OntoDriver implementation for OWLAPI-accessed files,  * _Ontodriver Jena_ - OntoDriver implementation for Jena-based storages,  * _OWL2Java_ - generates JOPA entities based on integrity constraints in input ontology (see [Example01](https://github.com/kbss-cvut/jopa-examples/tree/master/example01-jopa-sesame-owl2java)),  * _JOPA Maven plugin_ - Maven plugin for object model generation (using OWL2Java).    Other modules represent integration tests and various utilities.    ## Documentation    Check out the [Wiki](https://github.com/kbss-cvut/jopa/wiki) for general information about JOPA, explanation of its features and their usage.  We will be gradually building up its content.    Javadoc of the latest stable version is available at [https://kbss.felk.cvut.cz/jenkins/job/jopa-stable/javadoc/index.html?overview-summary.html](https://kbss.felk.cvut.cz/jenkins/job/jopa-stable/javadoc/index.html?overview-summary.html).    For more practical examples of JOPA features, see the JOPA examples repository at [https://github.com/kbss-cvut/jopa-examples](https://github.com/kbss-cvut/jopa-examples).      ## Usage    JOPA examples can be found in a separate repository at [https://github.com/kbss-cvut/jopa-examples](https://github.com/kbss-cvut/jopa-examples).    A simplistic demo of JOPA ([https://github.com/kbss-cvut/jopa-examples/tree/master/eswc2016](https://github.com/kbss-cvut/jopa-examples/tree/master/eswc2016)) is running at [http://onto.fel.cvut.cz/eswc2016](http://onto.fel.cvut.cz/eswc2016).    A more mature project using JOPA as its persistence provider can be found at [https://github.com/kbss-cvut/reporting-tool](https://github.com/kbss-cvut/reporting-tool).  It is a safety occurrence reporting tool developed for the aviation industry as part of the INBAS project ([https://www.inbas.cz](https://www.inbas.cz)).  A live demo of it is running at [https://www.inbas.cz/reporting-tool-public](https://www.inbas.cz/reporting-tool-public).    JOPA is also used in the [BINBAS project](https://www.inbas.cz/web/binbas) [6] and, more recently, in [TermIt](https://github.com/kbss-cvut/termit) - a SKOS vocabulary manager.    Note that JOPA requires Java 8 or later.    ## Getting JOPA    There are two ways of getting JOPA for your project:    * Clone repository/download zip and build it with Maven,  * Use a Maven dependency from the [Maven central repository](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22cz.cvut.kbss.jopa%22).    Basically, the _jopa-impl_ module and one of the OntoDriver implementations is all that is needed:    ```xml  <dependencies>      <dependency>          <groupId>cz.cvut.kbss.jopa</groupId>          <artifactId>jopa-impl</artifactId>      </dependency>      <dependency>          <groupId>cz.cvut.kbss.jopa</groupId>          <artifactId>ontodriver-jena</artifactId>          <!-- OR <artifactId>ontodriver-owlapi</artifactId> -->          <!-- OR <artifactId>ontodriver-sesame</artifactId> -->      </dependency>  </dependencies>  ```    ## More Info    More information about JOPA can be found for example in articles [4], [5] and at [https://kbss.felk.cvut.cz/web/kbss/jopa](https://kbss.felk.cvut.cz/web/kbss/jopa).    JOPA build status and code metrics can be found at:    * KBSS Jenkins [https://kbss.felk.cvut.cz/jenkins](https://kbss.felk.cvut.cz/jenkins),  * KBSS SonarQube [https://kbss.felk.cvut.cz/sonarqube](https://kbss.felk.cvut.cz/sonarqube).    ### Performance    A performance comparison of JOPA and other object-triple mapping libraries can be found at [https://kbss.felk.cvut.cz/web/kbss/otm-benchmark](https://kbss.felk.cvut.cz/web/kbss/otm-benchmark).    A comprehensive comparison - feature and performance - of object-triple mapping libraries is presented in [8].    ## Related    Some related libraries:    * [JB4JSON-LD](https://github.com/kbss-cvut/jb4jsonld) - Serialization and deserialization of POJOs into JSON-LD.  * [JOPA-Spring-transaction](https://github.com/ledsoft/jopa-spring-transaction) - Declarative Spring transactions (using the `@Transactional` annotation) with JOPA.  * [Reporting Tool](https://github.com/kbss-cvut/reporting-tool) - Real-life use case of JOPA.  * [TermIt](https://github.com/kbss-cvut/termit) - A more complex and up-to-date use case of JOPA.    ## References      * [1] J. Tao and E. Sirin, J. Bao, D. L. McGuinness, Integrity Constraints in OWL, The Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence, 2010, available online at [http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1931/2229](http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1931/2229)  * [2] JSR 338 [http://jcp.org/en/jsr/detail?id=338](http://jcp.org/en/jsr/detail?id=338)  * [3] The Java Language Specification, Java SE 8 Edition [https://docs.oracle.com/javase/specs/jls/se8/html/index.html](https://docs.oracle.com/javase/specs/jls/se8/html/index.html)  * [4] P. Křemen and Z. Kouba: Ontology-Driven Information System Design. IEEE Transactions on Systems, Man, and Cybernetics: Part C, 42(3):334–344, May 2012 [https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6011704](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6011704)  * [5] M. Ledvinka and P. Křemen: JOPA: Accessing Ontologies in an Object-oriented Way. In Proceedings of the 17th International Conference on Enterprise Information Systems. Porto: SciTePress - Science and Technology Publications, 2015, p. 212-222. ISBN 978-989-758-096-0. [http://www.scitepress.org/DigitalLibrary/PublicationsDetail.aspx?ID=p/CdcFwtlFM=&t=1](http://www.scitepress.org/DigitalLibrary/PublicationsDetail.aspx?ID=p/CdcFwtlFM=&t=1)  * [6] Ledvinka, M.; Křemen, P.; Kostov, B. JOPA: Efficient Ontology-based Information System Design In: The Semantic Web: ESWC 2016 Satellite Events. Cham: Springer International Publishing AG, 2016. pp. 156-160. 9989. ISSN 0302-9743. ISBN 978-3-319-47601-8. [ESWC 2016 Demo](http://2016.eswc-conferences.org/sites/default/files/papers/Accepted%20Posters%20and%20Demos/ESWC2016_DEMO_JOPA.pdf)  * [7] Ledvinka, M.; Křemen, P.; Kostov, B.; Blaško, M. SISel: Aviation Safety Powered by Semantic Technologies In: Data a znalosti 2017. Plzeň: Západočeská univerzita v Plzni, 2017. pp. 77-82. ISBN 978-80-261-0720-0. [https://daz2017.kiv.zcu.cz/data/DaZ2017-Sbornik-final.pdf](https://daz2017.kiv.zcu.cz/data/DaZ2017-Sbornik-final.pdf)  * [8] M. Ledvinka and  P. Křemen: A comparison of object-triple mapping libraries Semantic Web, 2019, doi: [10.3233/SW-190345](http://dx.doi.org/10.3233/SW-190345)    ## License    LGPLv3 """
Semantic web;https://github.com/SANSA-Stack/SANSA-RDF;"""# SANSA-Stack  <!-- [![Maven Central](https://maven-badges.herokuapp.com/maven-central/net.sansa-stack/sansa-parent/badge.svg)](https://maven-badges.herokuapp.com/maven-central/net.sansa-stack/sansa-parent) -->  [![Build Status](https://github.com/SANSA-Stack/SANSA-Stack/workflows/CI/badge.svg)](https://github.com/SANSA-Stack/SANSA-Stack/actions?query=workflow%3ACI)  [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)  [![Twitter](https://img.shields.io/twitter/follow/SANSA_Stack.svg?style=social)](https://twitter.com/SANSA_Stack)    This project comprises the whole Semantic Analytics Stack (SANSA). At a glance, it features the following functionality:    * Ingesting RDF and OWL data in various formats into RDDs  * Operators for working with RDDs and data frames of RDF data at various levels (triples, bindings, graphs, etc)  * *Transformation* of RDDs to data frames and *partitioning* of RDDs into R2RML-mapped data frames  * Distributed SPARQL querying over R2RML-mapped data frame partitions using RDB2RDF engines (Sparqlify & Ontop)  * Enrichment of RDDs with inferences  * Application of machine learning algorithms    For a detailed description of SANSA, please visit http://sansa-stack.net.     ## Layers  The SANSA project is structured in the following five layers developed in their respective sub-folders:    * [RDF](sansa-rdf)  * [OWL](sansa-owl)  * [Query](sansa-query)  * [Inference](sansa-inference)  * [ML](sansa-ml)    ## Release Cycle  A SANSA stack release is done every six months and consists of the latest stable versions of each layer at this point. This repository is used for organising those joint releases.    ## Usage    ### Spark    #### Requirements    We currently require a Spark 3.x.x with Scala 2.12 setup. A Spark 2.x version can be built from source based on the [spark2](https://github.com/SANSA-Stack/SANSA-Stack/tree/spark2) branch.    #### Release Version  Some of our dependencies are not in Maven central (yet), so you need to add following Maven repository to your project POM file `repositories` section:  ```xml  <repository>     <id>maven.aksw.internal</id>     <name>AKSW Release Repository</name>     <url>http://maven.aksw.org/archiva/repository/internal</url>     <releases>        <enabled>true</enabled>     </releases>     <snapshots>        <enabled>false</enabled>     </snapshots>  </repository>  ```    If you want to import the full SANSA Stack, please add the following Maven dependency to your project POM file:  ```xml  <!-- SANSA Stack -->  <dependency>     <groupId>net.sansa-stack</groupId>     <artifactId>sansa-stack-spark_2.12</artifactId>     <version>$LATEST_RELEASE_VERSION$</version>  </dependency>  ```  If you only want to use particular layers, just replace `$LAYER_NAME$` with the corresponding name of the layer  ```xml  <!-- SANSA $LAYER_NAME$ layer -->  <dependency>     <groupId>net.sansa-stack</groupId>     <artifactId>sansa-$LAYER_NAME$-spark_2.12</artifactId>     <version>$LATEST_RELEASE_VERSION$</version>  </dependency>  ```    #### SNAPSHOT Version  While the release versions are available on Maven Central, latest SNAPSHOT versions have to be installed from source code:  ```bash  git clone https://github.com/SANSA-Stack/SANSA-Stack.git  cd SANSA-Stack  ```  Then to build and install the full SANSA Spark stack you can do  ```bash  ./dev/mvn_install_stack_spark.sh   ```  or for a single layer `$LAYER_NAME$` you can do  ```bash  mvn -am -DskipTests -pl :sansa-$LAYER_NAME$-spark_2.12 clean install   ```    Alternatively, you can use the following Maven repository and add it to your project POM file `repositories` section:  ```xml  <repository>     <id>maven.aksw.snapshots</id>     <name>AKSW Snapshot Repository</name>     <url>http://maven.aksw.org/archiva/repository/snapshots</url>     <releases>        <enabled>false</enabled>     </releases>     <snapshots>        <enabled>true</enabled>     </snapshots>  </repository>  ```  Then do the same as for the release version and add the dependency:  ```xml  <!-- SANSA Stack -->  <dependency>     <groupId>net.sansa-stack</groupId>     <artifactId>sansa-stack-spark_2.12</artifactId>     <version>$LATEST_SNAPSHOT_VERSION$</version>  </dependency>  ```    ## How to Contribute  We always welcome new contributors to the project! Please see [our contribution guide](http://sansa-stack.net/contributing-to-sansa/) for more details on how to get started contributing to SANSA. """
Semantic web;https://github.com/ropensci/rdflib;"""  # rdflib <img src=""man/figures/logo.svg"" align=""right"" alt="""" width=""120"" />    [![Project Status: Active – The project has reached a stable, usable  state and is being actively  developed.](http://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/)  [![Travis-CI Build  Status](https://travis-ci.org/ropensci/rdflib.svg?branch=master)](https://travis-ci.org/ropensci/rdflib)  [![Build  status](https://ci.appveyor.com/api/projects/status/n81e9wsh5bh0xrm6?svg=true)](https://ci.appveyor.com/project/cboettig/rdflib)  [![Coverage  Status](https://img.shields.io/codecov/c/github/ropensci/rdflib/master.svg)](https://codecov.io/github/ropensci/rdflib?branch=master)  [![CircleCI](https://app.circleci.com/pipelines/github/ropensci/rdflib.svg?style=svg)](https://app.circleci.com/pipelines/github/ropensci/rdflib ""Docker tests"")  [![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/rdflib)](https://cran.r-project.org/package=rdflib)  [![](http://badges.ropensci.org/169_status.svg)](https://github.com/ropensci/software-review/issues/169)  [![CRAN RStudio mirror  downloads](http://cranlogs.r-pkg.org/badges/rdflib)](https://CRAN.R-project.org/package=rdflib)  [![DOI](https://zenodo.org/badge/100521776.svg)](https://zenodo.org/badge/latestdoi/100521776)    <!-- README.md is generated from README.Rmd. Please edit that file -->    A friendly and consise user interface for performing common tasks on rdf  data, such as parsing and converting between formats including rdfxml,  turtle, nquads, ntriples, and trig, creating rdf graphs, and performing  SPARQL queries. This package wraps the redland R package which provides  direct bindings to the redland C library. Additionally, the package  supports parsing and serialization of rdf into json-ld through the  json-ld package, which binds the official json-ld javascript API. The  package interface takes inspiration from the Python rdflib library.    ## Installation    You can install rdflib from GitHub with:    ``` r  # install.packages(""devtools"")  devtools::install_github(""ropensci/rdflib"")  ```    ## Basic use    While not required, `rdflib` is designed to play nicely with `%>%`  pipes, so we will load the `magrittr` package as well:    ``` r  library(magrittr)  library(rdflib)  ```    Parse a file and serialize into a different format:    ``` r  system.file(""extdata/dc.rdf"", package=""redland"") %>%    rdf_parse() %>%    rdf_serialize(""test.nquads"", ""nquads"")  ```    Perform SPARQL queries:    ``` r  sparql <-   'PREFIX dc: <http://purl.org/dc/elements/1.1/>    SELECT ?a ?c    WHERE { ?a dc:creator ?c . }'    system.file(""extdata/dc.rdf"", package=""redland"") %>%  rdf_parse() %>%  rdf_query(sparql)  #> Rows: 1 Columns: 2  #> ── Column specification ────────────────────────────────────────────────────────  #> Delimiter: "",""  #> chr (2): a, c  #>   #> ℹ Use `spec()` to retrieve the full column specification for this data.  #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.  #> # A tibble: 1 × 2  #>   a                      c             #>   <chr>                  <chr>         #> 1 http://www.dajobe.org/ Dave Beckett  ```    Initialize graph a new object or add triples statements to an existing  graph:    ``` r  x <- rdf()  x <- rdf_add(x,       subject=""http://www.dajobe.org/"",      predicate=""http://purl.org/dc/elements/1.1/language"",      object=""en"")  x  #> Total of 1 triples, stored in hashes  #> -------------------------------  #> <http://www.dajobe.org/> <http://purl.org/dc/elements/1.1/language> ""en"" .  ```    Change the default display format (`nquads`) for graph objects:    ``` r  options(rdf_print_format = ""jsonld"")  x  #> Total of 1 triples, stored in hashes  #> -------------------------------  #> {  #>   ""@id"": ""http://www.dajobe.org/"",  #>   ""http://purl.org/dc/elements/1.1/language"": ""en""  #> }  ```    ## JSON-LD    We can also work with the JSON-LD format through additional functions  provided in the R package, `jsonld`.    ``` r  out <- tempfile()  rdf_serialize(x, out, ""jsonld"")  rdf_parse(out, format = ""jsonld"")  #> Total of 1 triples, stored in hashes  #> -------------------------------  #> {  #>   ""@id"": ""http://www.dajobe.org/"",  #>   ""http://purl.org/dc/elements/1.1/language"": ""en""  #> }  ```    For more information on the JSON-LD RDF API, see  <https://json-ld.org/spec/latest/json-ld-rdf/>.    ## Advanced Use    See [articles](https://docs.ropensci.org/rdflib/articles/) from the  documentation for advanced use including applications to large  triplestores, example SPARQL queries, and information about additional  database backends.    ------------------------------------------------------------------------    ## Citing rdflib    Please also cite the underlying `redland` library when citing `rdflib`    Carl Boettiger. (2018). rdflib: A high level wrapper around the redland  package for common rdf applications (Version 0.1.0). Zenodo.  <https://doi.org/10.5281/zenodo.1098478>    Jones M, Slaughter P, Ooms J, Boettiger C, Chamberlain S (2021).  *redland: RDF Library Bindings in R*. doi: 10.5063/F1VM496B (URL:  <https://doi.org/10.5063/F1VM496B>), R package version 1.0.17-15, \<URL:  <https://github.com/ropensci/redland-bindings/tree/master/R/redland>\>.    [![rofooter](https://ropensci.org//public_images/github_footer.png)](https://ropensci.org/) """
Semantic web;https://github.com/ontop/ontop;"""[![Maven Central](https://img.shields.io/maven-central/v/it.unibz.inf.ontop/ontop.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22it.unibz.inf.ontop%22)  [![GitHub license](https://img.shields.io/badge/license-Apache%20License%202.0-blue.svg?style=flat)](http://www.apache.org/licenses/LICENSE-2.0)  [![SourceForge](https://img.shields.io/sourceforge/dm/ontop4obda.svg)](http://sourceforge.net/projects/ontop4obda/files/)  [![Twitter](https://img.shields.io/twitter/follow/ontop4obda.svg?style=social)](https://twitter.com/ontop4obda)    | Branch    | build status  |  |-----------|---------------|  | [master](https://github.com/ontop/ontop/tree/master)  |[![Build Status](https://travis-ci.org/ontop/ontop.svg?branch=master)](https://travis-ci.org/ontop/ontop)|  | [version4](https://github.com/ontop/ontop/tree/version4) |[![Build Status](https://travis-ci.org/ontop/ontop.svg?branch=version4)](https://travis-ci.org/ontop/ontop)|      Ontop  =====    Ontop is a Virtual Knowledge Graph system.  It exposes the content of arbitrary relational databases as knowledge graphs. These graphs are virtual, which means that data remains in the data sources instead of being moved to another database.    Ontop translates [SPARQL queries](https://www.w3.org/TR/sparql11-query/) expressed over the knowledge graphs into SQL queries executed by the relational data sources. It relies on [R2RML mappings](https://www.w3.org/TR/r2rml/) and can take advantage of lightweight ontologies.    Compiling, packing, testing, etc.  --------------------    The project is a [Maven](http://maven.apache.org/) project. Compiling,  running the unit tests, building the release binaries all can be done  using maven.  Currently, we use Maven 3 and Java 8 to build the  project.      Links  --------------------    - [Official Website and Documentation](https://ontop-vkg.org)  - [SourceForge Download](http://sourceforge.net/projects/ontop4obda/files/)  - [Docker Hub](https://hub.docker.com/r/ontop/ontop-endpoint)  - [GitHub](https://github.com/ontop/ontop/)  - [GitHub Issues](https://github.com/ontop/ontop/issues)  - [Google Group](https://groups.google.com/forum/#!forum/ontop4obda)  - [Facebook](https://www.facebook.com/obdaontop/)  - [Twitter](https://twitter.com/ontop4obda)  - [Travis CI](https://travis-ci.org/ontop/ontop)    License  -------    The Ontop framework is available under the Apache License, Version 2.0    ```    Copyright (C) 2009 - 2021 Free University of Bozen-Bolzano      Licensed under the Apache License, Version 2.0 (the ""License"");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at          http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an ""AS IS"" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.  ```    All documentation is licensed under the  [Creative Commons](http://creativecommons.org/licenses/by/4.0/)  (attribute)  license. """
Semantic web;https://github.com/mhausenblas/mrlin;"""# mrlin - MapReduce processing of Linked Data    > ...because it's magic    The basic idea of **mrlin** is to enable **M**ap **R**educe processing of **Lin**ked Data - hence the name. In the following I'm going to show you first to how to use HBase to store Linked Data with RDF, and then how to use Hadoop to run MapReduce jobs.    ## Background    ### Dependencies    * You'll need [Apache HBase](http://hbase.apache.org/) first. I downloaded [`hbase-0.94.2.tar.gz`](http://ftp.heanet.ie/mirrors/www.apache.org/dist/hbase/stable/hbase-0.94.2.tar.gz) and followed the [quickstart](http://hbase.apache.org/book/quickstart.html) up to section 1.2.3. to set it up.  * The mrlin Python scripts depend on:   * [Happybase](https://github.com/wbolster/happybase) to manage HBase; see also the [docs](http://happybase.readthedocs.org/en/latest/index.html) for further details.   * [mrjob](https://github.com/Yelp/mrjob) to run MapReduce jobs; see also the [docs](http://packages.python.org/mrjob/) for further details.    ### Representing RDF triples in HBase  Learn about how mrlin represents [RDF triples in HBase](https://github.com/mhausenblas/mrlin/wiki/RDF-in-HBase).    ### RESTful Interaction with HBase  Dig into [RESTful interactions](https://github.com/mhausenblas/mrlin/wiki/RESTful-interaction) with HBase, in mrlin.    ## Usage    ### Setup  I assume you have HBase installed in some directory `HBASE_HOME` and mrlin in some other directory `MRLIN_HOME`. First let's make sure that Happybase is installed correctly - we will use a [virtualenv](http://pypi.python.org/pypi/virtualenv ""virtualenv 1.8.2 : Python Package Index""). You only need to do this once: go to `MRLIN_HOME` and type:    	$ virtualenv hb    Time to launch HBase and the Thrift server: in the `HBASE_HOME` directory, type the following:    	$ ./bin/start-hbase.sh   	$ ./bin/hbase thrift start -p 9191    OK, now we're ready to launch mrlin - change to the directory `MRLIN_HOME` and first activate the virtualenv we created earlier:    	$ source hb/bin/activate    You should see a change in the prompt to something like `(hb)michau@~/Documents/dev/mrlin$` ... and this means we're good to go!    ### Import RDF/NTriples  To import  [RDF NTriples](http://www.w3.org/TR/rdf-testcases/#ntriples) documents, use the [`mrlin import`](https://raw.github.com/mhausenblas/mrlin/master/mrlin_import.py) script.    First, try to import **a file** from the local filesystem. Note the second parameter (`http://example.org/`), which specifies the target graph URI to import into:    	$ (hb)michau@~/Documents/dev/mrlin$ python mrlin_import.py data/test_0.ntriples http://example.org/    If this works, try to import directly from **a URL** `http://dbpedia.org/data/Galway.ntriples`:    	(hb)michau@~/Documents/dev/mrlin$ python mrlin_import.py http://dbpedia.org/data/Galway.ntriples http://dbpedia.org/  	2012-10-30T08:56:21 Initialized mrlin table.  	2012-10-30T08:56:31 Importing RDF/NTriples from URL http://dbpedia.org/data/Galway.ntriples into graph http://dbpedia.org/  	2012-10-30T08:56:31 == STATUS ==  	2012-10-30T08:56:31  Time to retrieve source: 9.83 sec  	2012-10-30T08:56:31 == STATUS ==  	2012-10-30T08:56:31  Time elapsed since last checkpoint:  0.07 sec  	2012-10-30T08:56:31  Import speed: 1506.61 triples per sec  	2012-10-30T08:56:31 == STATUS ==  	2012-10-30T08:56:31  Time elapsed since last checkpoint:  0.02 sec  	2012-10-30T08:56:31  Import speed: 4059.10 triples per sec  	2012-10-30T08:56:31 ==========  	2012-10-30T08:56:31 Imported 233 triples.    Note that you can also import **an entire directory** (mrlin will look for `.nt` and `.ntriples` files):  	  	(hb)michau@~/Documents/dev/mrlin$ python mrlin_import.py data/ http://example.org/  	2012-10-30T03:55:18 Importing RDF/NTriples from directory /Users/michau/Documents/dev/mrlin/data into graph http://example.org/  	...  	  To reset the HBase table (and remove all triples from it), use the [`mrlin utils`](https://raw.github.com/mhausenblas/mrlin/master/mrlin_utils.py) script like so:    	(hb)michau@~/Documents/dev/mrlin$ python mrlin_utils.py clear    ### Query  In order to query the mrlin datastore in HBase, use the [`mrlin query`](https://raw.github.com/mhausenblas/mrlin/master/mrlin_query.py) script:    	(hb)michau@~/Documents/dev/mrlin$ python mrlin_query.py Tribes  	2012-10-30T04:01:22 Scanning table rdf with filter ValueFilter(=,'substring:Tribes')  	2012-10-30T04:01:22 Key: http://dbpedia.org/resource/Galway - Value: {'O:148': 'u\'""City of the Tribes""\'', 'O:66': 'u\'""City of the Tribes""\'',  ...}  	2012-10-30T04:01:22 ============  	2012-10-30T04:01:22 Query took me 0.01 seconds.    ### Running MapReduce jobs    *TBD*    * setup in virtual env: `source hb/bin/activate` then `pip install mrjob`  * `cp .mrjob.conf ~` before launch  * `source hb/bin/activate`  * run `python mrlin_mr.py README.md` for standalone  * set up [Hadoop 1.0.4](http://ftp.heanet.ie/mirrors/www.apache.org/dist/hadoop/common/hadoop-1.0.4/hadoop-1.0.4.tar.gz) - if unsure follow a [single-node setup](http://orzota.com/blog/single-node-hadoop-setup-2/)  tutorial  * `cp .mrjob.conf ~` before launch if you change settings (!)  * note all changes that were necessary in ` conf/core-site.xml`, `conf/mapred-site.xml`, `conf/hdfs-site.xml`, and `hadoop-env.sh` (provide examples)  * run `python mrlin_mr.py -r hadoop README.md` for local Hadoop       #### Debug    * `tail -f hadoop-michau-namenode-Michael-Hausenblas-iMac.local.log`        ## License    All artifacts in this repository are licensed under [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0.html) Software License."""
Semantic web;https://github.com/lorenae/qb4olap;"""  QB4OLAP is a Vocabulary for Business Intelligence over Linked Data.    It is an extension of the [DataCube](http://www.w3.org/TR/vocab-data-cube/) vocabulary that allows to represent OLAP cubes in RDF,   and to implement OLAP operators (such as Roll-up, Slice, and Dice) as SPARQL queries directly on this RDF representation.    #The vocabulary     You can find the current version of the vocabulary in Turtle format in this repository.   The latest version (v1.3) is available [here](https://github.com/lorenae/qb4olap/blob/master/rdf/qb4olap.ttl). Previous stable version of the vocabulary (v1.2) is available [here](https://github.com/lorenae/qb4olap/tree/master/rdf).    The main improvement in v1.3 is the ability to represent custom rollup relationships.    We have defined the following purls to refere to each version:    * Version 1.2: http://purl.org/qb4olap/cubes_v1.2  * Version 1.3: http://purl.org/qb4olap/cubes    We suggest to use dcterms:conformsTo property to indicate which version of QB4OLAP is used in a dataset.    #Documentation and related resources  Our [wiki](https://github.com/lorenae/qb4olap/wiki) contains a detailed description of the elements of the vocabulary, examples of QB4OLAP in-use, and related publications.    Check our [QB4OLAP-tools](https://github.com/lorenae/qb4olap-tools) project to see some examples of what can be done with this vocabulary.    #Examples    The [examples](https://github.com/lorenae/qb4olap/tree/master/examples) folder contains ttl files that represent the schema and instances of different cubes in QB4OLAP """
Semantic web;https://github.com/EIS-Bonn/iRap;"""iRap  =======    Interest-based RDF update propagation framework    iRap is an RDF update propagation framework that propagates only interesting parts of an update from the source to the target dataset.   iRap filters interesting parts of changesets from the source dataset based on graph-pattern-based interest expressions registered by a target dataset user.  This repository provides source code and evaluation material for iRap framework.    Configuration  =========    Interest expressions should be specified in RDF. Basic components of interest expression are:    - **Subscriber:** is an object that identifies a target dataset and preferences associated with it.    - **Interest:** is an object that identifies an interest expression of a `Subscriber` dataset.    iRap interest expression ontology: `@prefix irap: <http://eis.iai.uni-bonn.de/irap/ontology/>`    ## Subscriber    `Subscriber` instance contains the following setting:    * **irap:targetType :** type of dataset as a target to the changes. Valid type are: *TDB, SPARQL_ENDPOINT, and VIRTUOSO_JDBC*    * **irap:targetEndpoint :** path to target dataset. If target type is TDB, then endpoint value is path to TDB folder. If target type is SPARQL_ENDPOINT then irap:targetEndpoint is URI to sparql endpoint (This can be public endpoint for querying only or update enabled endpoint).  VIRTUOSO_JDBC type not supported for this  release.    * **irap:targetUpdateURI :** same as irap:targetEndpoint if target type is TDB and update enabled SPARQL_ENDPOINT. If irap:targetType is query only SPARQL_ENDPOINT, then irap:targetUpdateURI should be a URI to SPARQL Update endpoint.    * **irap:piTrackingMethod :** potentially interesting triples tracking method. Valid methods supported are: LOCAL and LIVE_ON_SOURCE    * **irap:piStorageType :** type of dataset for potentially interesting dataset, if irap:piTrackingMethod is LOCAL    * **irap:piStoreBaseURI :** Path (Endpoint URI) to potentially interesting dataset, if irap:piTrackingMethod is LOCAL and irap:piStorageType is TDB (SPARQL_ENDPOINT, respectively)    ## Interest    `Interest` instance contains the following setting:    * **irap:subscriber :** URI of a subscriber for this interest    * **irap:sourceEndpoint :** endpoint to the source dataset (SPARQL_ENDPOINT)    * **irap:changesetPublicationType :** location of changeset publication. Valid values are: REMOTE and LOCAL    * **irap:changesetBaseURI :** URI to changeset publication location    * **irap:lastPublishedFilename :** last publication file name to compare with last downloaded changesets by iRap    * **irap:bgp :** interest basic graph pattern (BGP) expression    * **irap:ogp :** optional graph pattern (OGP) expression              Executing iRap  =========  In order to execute from source, download the code from the repo  `git clone https://github.com/EIS-Bonn/iRap.git`    - Prepare your interest expression (see `Example interest expression` below).  - If your interest expression contains remote changeset publications (such as DBpedia changesets), edit `lastDownloadDate.dat` and adapt the date according to your target dataset  - Running iRap:  				  		$ git clone https://github.com/EIS-Bonn/iRap.git  		$ cd iRap/irap-core  		$ mvn clean install  		$ mvn exec:java -Dexec.args=""<interest-exp>,<run-mode>""      - `interest-exp` specifies an interest expression RDF file      - `run-mode` specifies how long the changeset manager should run.        * -1 - endless, i.e., run 'forever'       * 0  - one-time, i.e., run until all changesets available are evaluated (DO NOT wait new updates)    		    Example (DBpedia replica)  =========  The following example shows an interest expression for DBpedia remote changesets.    ```  # interest.ttl    @prefix : <http://eis.iai.uni-bonn.de/irap/ontology/> .  @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .  @base <http://eis.iai.uni-bonn.de/irap/ontology/> .    ###  http://eis.iai.uni-bonn.de/irap/resource/Soccer  <http://eis.iai.uni-bonn.de/irap/resource/Soccer> rdf:type :Interest ;                                                                                                    :sourceEndpoint ""http://live.dbpedia.org/sparql"" ;                                                    :lastPublishedFilename ""lastPublishedFile.txt"" ;                                                    :bgp ""?a  a <http://dbpedia.org/ontology/Athlete> .  ?a <http://dbpedia.org/property/goals>  ?goals ."" ;                                                    :ogp ""?a <http://xmlns.com/foaf/0.1/homepage>  ?page ."" ;                                                    :changesetBaseURI ""http://live.dbpedia.org/changesets/"" ;                                                    :changesetPublicationType ""REMOTE"" ;                                                    :subscriber <http://eis.iai.uni-bonn.de/irap/resource/Sport.org> .    ###  http://eis.iai.uni-bonn.de/irap/resource/Sport.org  <http://eis.iai.uni-bonn.de/irap/resource/Sport.org> rdf:type :Subscriber;                                                       :piStoreBaseURI ""sports-pi-tdb"" ;                                                       :piStorageType ""TDB"" ;                                                       :targetType ""TDB"" ;                                                       :targetEndpoint ""sports-tdb"" ;                                                       :piTrackingMethod ""LOCAL"" ;                                                       :targetUpdateURI ""sports-tdb"" .    ```    Dependencies  =========    1. Java 7    Contact  =======  [iRap mailing list](https://groups.google.com/forum/#!forum/irap-ld)    ## License    The source code is under the terms of the [GNU General Public License, version 2](http://www.gnu.org/licenses/gpl-2.0.html). """
Semantic web;https://github.com/Claudenw/JenaSecurity;"""JenaSecurity  ============    JenaSecurity is now part of the Apache Jena project.  ====================================================    Please make any contributions at the Apache Jena website: http://jena.apache.org    package name: jena-security (Jena version < 3)  package name: jena-permissions (Jena version >= 3)    See Apache Jena website for documentation, source and binary packages.    ------    Security (Permissions) wrapper around Jena RDF implementation.    By implementing a SecurityEvaluator users may apply access restricitons to graphs and optionally  to triples within the graphs.    Two implementations are available based on Jena 2.7.4 and another on the upcoming 2.10.0    Documentation is most complete for the 2.10.0 version, however most of that documentation will  also apply to 2.7.4.    Maven Info:  <pre>  Group Id: org.apache.jena  Artifact Id: jena-security  </pre>   """
Semantic web;https://github.com/cyberborean/rdfbeans;"""# RDFBeans framework    RDFBeans is an object-RDF mapping framework for Java. It provides ORM-like databinding functionality for RDF databases (""triplestores"") with two basic techniques:      * Classic object persistence: storing and retrieving the state of POJO objects      to/from a RDF model      * Dynamic proxy mechanism to access RDF data with Java interfaces mapped directly       to the underlying model    RDFBeans is built upon [Eclipse RDF4J](http://rdf4j.org/) (Sesame) API  to provide object persistence for a number of state-of-the-art   RDF triplestore implementations.    RDFBeans is based on Java Annotations mechanism.   No special interfaces and superclasses is required, that guarantees minimum   modifications of existing codebase and compatibility with other POJO-oriented   frameworks.         ##Features      * Cascade databinding to reduce development time and ensure referential integrity of complex object models      * Inheritance of RDFBeans annotations from superclasses and/or interfaces          * No external specifications (RDF-schemas) are required: everything is declared by the annotations      * Extensible mechanism of mapping Java types to RDF literals: you can define your own algorithms to represent your data structures with RDF      * Support of basic Java Collections, optionally represented with RDF containers        * Transactions support (triplestore-specific)          * Support of indexed JavaBean properties      * Support of RDF namespaces     """
Semantic web;https://github.com/OnToology/OnToology;"""# ![alt text](https://raw.githubusercontent.com/OnToology/OnToology/master/media/icons/logoprop1_readme.png ""OnToology"")  <!--[![Build Status](https://semaphoreci.com/api/v1/ahmad88me/ontoology/branches/master/badge.svg)](https://semaphoreci.com/ahmad88me/ontoology)  -->  <!--  [![Build Status](https://ahmad88me.semaphoreci.com/badges/OnToology.svg)](https://ahmad88me.semaphoreci.com/projects/OnToology)   [![codecov](https://codecov.io/gh/OnToology/OnToology/branch/master/graph/badge.svg)](https://codecov.io/gh/OnToology/OnToology)  -->  [![Build Status](https://ahmad88me.semaphoreci.com/badges/OnToology/branches/master.svg)](https://ahmad88me.semaphoreci.com/projects/OnToology)   [![codecov](https://codecov.io/gh/OnToology/OnToology/branch/master/graph/badge.svg?token=PJgHWaaa9l)](https://codecov.io/gh/OnToology/OnToology)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1317786.svg)](https://doi.org/10.5281/zenodo.1317786)  [![Twitter](https://img.shields.io/twitter/follow/OnToology.svg?style=social&label=@OnToology)](https://twitter.com/OnToology)    A system for collaborative ontology development process. Given a repository with an owl file, **OnToology** will survey it and produce diagrams, a complete documentation and validation based on common pitfalls.    You can find a live version of OnToology online: http://ontoology.linkeddata.es.    Team: Ahmad Alobaid, Daniel Garijo, Maria Poveda, Idafen Santa, Alba Fernandez Izquierdo, Oscar Corcho    License: Apache License v2 (http://www.apache.org/licenses/LICENSE-2.0)    If you want to cite Ontoology in a scientific paper or technical report, you can use the following [Bibtex citation](/media/references/ontoology.bib) or directly this text: Alobaid A, Garijo D, Poveda-Villalón M, Santana-Pérez I, Fernández-Izquierdo A, Corcho O (2019) Automating ontology engineering support activities with OnToology. Journal of Web Semantics 57:100472, https://doi.org/10.1016/j.websem.2018.09.003    # Funding  The development of OnToology has been supported by the Spanish national project Datos 4.0 (TIN2016-78011-C4-4-R)    # Tools  Here is a list of tools being used by OnToology.  * [owl2jsonld](https://github.com/stain/owl2jsonld) ( [zenodo](http://dx.doi.org/10.5281/zenodo.10565) )  * [Widoco](https://github.com/dgarijo/Widoco) ( [zenodo](https://zenodo.org/badge/latestdoi/11427075) )  * [OOPS!](http://oops.linkeddata.es)  * [AR2DTool](https://github.com/idafensp/ar2dtool)  * [oops-report](https://github.com/OnToology/oops-report)  * [Themis](https://github.com/oeg-upm/Themis)      # Documentation for users  If you are an ontology engineering willing to use Ontoology, you can check our [step by step documentation](http://ontoology.linkeddata.es/stepbystep). Please check also our list of [Frequently Asked Questions](http://ontoology.linkeddata.es/faqs)      # Documentation for developers  Next we provide some documentation for developers who want to contribute to the further development Ontoology or for those who are interested in deploying Ontoology locally or in their servers. Feel free to contact us if you are interested in contributing of fixing some functionality      ## To run automated tests  1. You should have [docker](https://docs.docker.com/) and [docker-compose](https://docs.docker.com/compose/) installed  2. You need to have a GitHub user to act as ""OnToologyUser"" (you can choose any username you like).  3. Add the details as in the *secret setup* section below.  4. Run the automated tests script `sh scripts/run_tests.sh`       ## Run Locally  ### via script  1. `sh scripts/run_web.sh`  ### manual  1. `cp -Rf ~/.ssh/ ssh` (assuming you have a *nix and that you already have an ssh key)  1. `mkdir -p .git`  1. `docker-compose build --no-cache`  1. `docker-compose run -p 8000:8000 web .venv/bin/python manage.py runserver 0.0.0.0:8000`  1. ~~Run the RabbitMQ server (consumers).~~      - ~~Locally: `python OnToology/rabbit.py`~~      - ~~For a linux server: `nohup .venv/bin/python OnToology/rabbit.py &`~~  1. ~~(Optional) you can run it with multiple threads `nohup .venv/bin/python OnToology/rabbit.py 3 &`~~  1. Now, this is run automatically. But, make sure that the environment variable `rabbit_processes` is set to a value > 0      ## Development  For development, you can run the db `sh scripts/run_db.sh`. And then locally,   you can access that db. Or you can install mongo db locally on your machine.      ## To access the command line  `sh scripts/run_docker.sh`      ### Secret setup  This file should be added in `scripts/secret_setup.sh`  ```  #!/bin/sh  export github_password=""""  export github_email=""""  export client_id_login=""""  export client_id_public=""""  export client_id_private=""""  export client_secret_login=""""  export client_secret_public=""""  export client_secret_private=""""  export test_user_token=""""  export test_user_email=""""  export rabbit_host=""""  ```    ### Environment variables  Here we describe some of the main ones  * `rabbit_processes` : The number of rabbit processes to automatically run (0 means do not run it automatically).      ### How to contribute  There are two workflows:      ##### Case 1: If you are a contributor:  1. Create a new branch from the current live one (now it is `master`). Make sure to give it a presentive name. In case it is for a specific issue, include the issue number in the branch name, e.g. change-spinner-123.  2. Once you push your changes on the new branch, **create a pull request** and one of the admins will check your code base and will merge if it is ok.      ##### Case 2: If you are not added as a contributor yet (or you are a contributor who prefers this workflow):  1. Fork from the current live branch (now it is `master`).  2. Create a pull request, we will review it and merge if it is ok.      ### Dependency notice  * To run the tests, we use the `mock` option for github api. It was rejected by the `PyGithub` maintainers, so make sure to use  the version in `ahmad88me/PyGithub`.  (see below)      ## Local Setup  ### On Linux  (tested on ubuntu, debian, mint and fedora)  #### To install the tools  1. Open the terminal and `cd` to the location of choice.  2. `export PLAYGROUND=$PWD`.  3. Copy and paste the commands of choice to the terminal from `scripts/setup_docker_base.sh`      ### Install Pygithub (not the upstream version)  #### either directly from github  `pip install git+https://github.com/ahmad88me/PyGithub.git`  #### or locally  1. `git clone https://github.com/ahmad88me/PyGithub.git`  1. `cd OnToology` (assuming both are on the same level/directory)  1. `pip install -e ../Pygithub` (change this to any directory you want)   """
Semantic web;https://github.com/AKSW/ORE;"""ORE - Ontology Repair and Enrichment  ===    The ORE (Ontology Repair and Enrichment) tool allows for knowledge engineers to improve an OWL ontology by fixing inconsistencies and making suggestions for adding further axioms to it.    Ontology Debugging: ORE uses OWL reasoning to detect inconsistencies and satisfiable classes. State-of-the-art methods are then used to detect the most likely sources for the problems. In a simple process, the user can create a repair plan to resolve a problem, while maintaining full control over desired and undesired inferences.    Ontology Enrichment: ORE uses the DL-Learner framework to suggest definitions and super classes for existing classes in the knowledge base. This works if instance data is available and can be used to detect potential problems and harmonise schema and data in the knowledge base. """
Semantic web;https://github.com/pkumod/gStore;"""# gStore    gStore is a graph database engine for managing large graph-structured data, which is open-source and targets at Linux. This project is written in C++, with the help of some libraries such as readline, antlr, and so on. Only source tarballs are provided currently, which means you have to compile the source code if you want to use gStore.    **The formal help document is in [English(EN)](docs/help/gStore_help.pdf) and [中文(ZH)](docs/help/gStore_help_ZH.pdf).**    **The formal experiment result is in [Experiment](docs/test/formal_experiment.pdf).**    **We have built an IRC channel named #gStore on freenode, and you can visit [the homepage of gStore](http://gstore-pku.com).**    <!--**You can write your information in [survey](http://59.108.48.38/survey) if you like.**-->    ## Getting Started  ### Compile from Source  This system is really user-friendly and you can pick it up in several minutes. Remember to check your platform where you want to run this system by viewing [System Requirements](docs/DEMAND.md). After all are verified, please get this project's source code. There are several ways to do this:    - (suggested)type `git clone https://github.com/pkumod/gStore.git` in your terminal or use git GUI to acquire it    - download the zip from this repository and extract it    - fork this repository in your github account    Then you need to compile the project, for the first time you need to type `make pre` to prepare the `ANTLR` library and some Lexer/Parser programs.  Later you do not need to type this command again, just use the `make` command in the home directory of gStore, then all executables will be generated.  (For faster compiling speed, use `make -j4` instead, using how many threads is up to your machine)  To check the correctness of the program, please type `make test` command.    The first strategy is suggested to get the source code because you can easily acquire the updates of the code by typing `git pull` in the home directory of gStore repository.   In addition, you can directly check the version of the code by typing `git log` to see the commit logs.  If you want to use code from other branches instead of master branch, like 'dev' branch, then:    - clone the master branch and type `git checkout dev` in your terminal    - clone the dev branch directly by typing `git clone -b dev`    ### Deploy via Docker  You can easily deploy gStore via Docker. We provide both of Dockerfile and docker image. Please see our [Docker Deployment Doc(EN)](docs/DOCKER_DEPLOY_EN.md) or [Docker部署文档(中文)](docs/DOCKER_DEPLOY_CN.md) for details.    ### Run  To run gStore, please type `bin/gbuild database_name dataset_path` to build a database named by yourself. And you can use `bin/gquery database_name` command to query an existing database. What is more, `bin/ghttp` is a wonderful tool designed for you, as a database server which can be accessed via HTTP protocol. Notice that all commands should be typed in the root directory of gStore, and your database name should not end with "".db"".    - - -    ## Advanced Help    If you want to understand the details of the gStore system, or you want to try some advanced operations(for example, using the API, server/client), please see the chapters below.    - [Basic Introduction](docs/INTRO.md): introduce the theory and features of gStore    - [Install Guide](docs/INSTALL.md): instructions on how to install this system    - [How To Use](docs/USAGE.md): detailed information about using the gStore system    - [API Explanation](docs/API.md): guide you to develop applications based on our API    - [Project Structure](docs/STRUCT.md): show the whole structure and process of this project    - [Related Essays](docs/ESSAY.md): contain essays and publications related with gStore    - [Update Logs](docs/CHANGELOG.md): keep the logs of the system updates    - [Test Results](docs/TEST.md): present the test results of a series of experiments    - - -    ## Other Business    Bugs are recorded in [BUG REPORT](docs/BUGS.md).  You are welcomed to submit the bugs you discover if they do not exist in this file.    We have written a series of short essays addressing recurring challenges in using gStore to realize applications, which are placed in [Recipe Book](docs/TIPS.md).    You are welcome to report any advice or errors in the github Issues part of this repository, if not requiring in-time reply. However, if you want to urgent on us to deal with your reports, please email to <gjsjdbgroup@pku.edu.cn> to submit your suggestions and report bugs. A full list of our whole team is in [Mailing List](docs/MAIL.md).    There are some restrictions when you use the current gStore project, you can see them on [Limit Description](docs/LIMIT.md).    Sometimes you may find some strange phenomena(but not wrong case), or something hard to understand/solve(don't know how to do next), then do not hesitate to visit the [Frequently Asked Questions](docs/FAQ.md) page.    Graph database engine is a new area and we are still trying to go further. Things we plan to do next is in [Future Plan](docs/PLAN.md) chapter, and we hope more and more people will support or even join us. You can support in many ways:    - watch/star our project    - fork this repository and submit pull requests to us    - download and use this system, report bugs or suggestions    - ...    People who inspire us or contribute to this project will be listed in the [Thanks List](docs/THANK.md) chapter.    <!--This whole document is divided into different pieces, and each them is stored in a markdown file. You can see/download the combined markdown file in [help_markdown](docs/gStore_help.md), and for html file, please go to [help_html](docs/gStore_help.html). What is more, we also provide help file in pdf format, and you can visit it in [help_pdf](docs/latex/gStore_help.pdf).-->   """
Semantic web;https://github.com/jpcik/morph-starter;"""morph-starter  =============    Getting started with morph. this project is a simple Java (and Scala) demo of how to use morph.    Currently it shows how to generate RDF data from relational databases, using an [R2RML](http://www.w3.org/TR/r2rml/) mapping.    **Requirements**  * Java7  * Sbt 0.13 (or maven)    **Running**    To run the example, download the code and run Sbt:    ```  >sbt run  ```    It will run the main method that has a configured small HSQLDB memory database, and uses predefined mappings.  The RDF is output to the console.   You can check the `DemoQueryJava` file to tweak and change whaterver you want.    The script to create the test DB (.sql) and the mappings (.r2rml) are is in `src/main/resources/data`.   The database jdbc config is in `src/main/resources/application.conf`    **Eclipse**    If you want to use Eclipse, you can type the following to generate the .project files:  ```  sbt eclipse  ```    This will generate the necessary Eclipse project files, classpath dependencies, etc. Then you can import the project in your Eclipse installation.  If you plan to use Scala we recommend installing the Scala IDE plugin.    **Maven**    If you prefer to use maven instead of sbt, there is a pom.xml file available. Otherwise you can just ignore its existence.  You can compile the code as usual: `mvn compile`, import it to Eclipse using the m2e plugin, etc. """
Semantic web;https://github.com/gbv/dso;"""This repository contains the **Document Service Ontology (DSO)**    The URI of this ontology is going to be <http://purl.org/ontology/dso> and it's  URI namespace is going to be <http://purl.org/ontology/dso#> (not registered  yet).    The current version of this specification can be found at <http://gbv.github.io/dso/>  and a public git repository at <https://github.com/gbv/dso>.  [Feedback](https://github.com/gbv/dso/issues) is welcome!    The following diagram illustrates the classes and properties defined in this ontology.    ~~~      +---------------------+      |  dso:ServiceEvent   |      | +-----------------+ |  hasDocument    +-----------------------+      | | DocumentService |------------------>| ...any document class |      | |                 |<------------------|                       |      | |  Loan           | |  hasService     +-----------------------+      | |  Presentation   | |      | |  Interloan      | |      | |  OpenAccess     | |      | |  Digitization   | |      | |  Identification | |      | |  ...            | |      | +-----------------+ |      +---------------------+  ~~~     """
Semantic web;https://github.com/stoewer/fluent-sparql;"""[![Build Status](https://travis-ci.org/stoewer/fluent-sparql.png?branch=master)](https://travis-ci.org/stoewer/fluent-sparql)  [![Coverage Status](https://coveralls.io/repos/stoewer/fluent-sparql/badge.png?branch=master)](https://coveralls.io/r/stoewer/fluent-sparql?branch=master)    ## About Fluent SPARQL    This project aims to provide an interface for creating and executing SPARQL queries using a DSL-like fluent API that  allows to write code that resembles the SPARQL syntax, such as:    ```java  Query q = sparql.select(""name"")                      .add(""?a"", RDF.type, FOAF.Person)                      .add(""?a"", FOAF.givenname, ""Frodo"")                      .add(""?a"", FOAF.family_name, ""?name"")                      .filter(""?name"").regexp(""^Baggins$"")                  .orderByAsc(""name"")                  .limit(1)                  .query();  ```    ## Status    This project is still work in progress and at this time there is no stable release! """
Semantic web;https://github.com/specgen/specgen;"""SpecGen v6  ==========    About  -----    This is an experimental new codebase for [specgen](http://forge.morfeo-project.org/wiki_en/index.php/SpecGen) tools based on danbri's [specgen5 version](http://svn.foaf-project.org/foaftown/specgen/),   which was heavily updated by [Bo Ferri](http://github.com/zazi/) in summer 2010.    <b>Features (incl. modifications + extensions)</b>:    * multiple property and class types  * muttiple restrictions modelling  * rdfs:label, rdfs:comment  * classes and properties from other namespaces  * inverse properties (explicit and anonymous)  * sub properties  * union ranges and domains (appear only in the property descriptions, not on the class descriptions)  * equivalent properties  * simple individuals as optional feature    Dependencies  ------------  		  It depends utterly upon     * [rdflib](http://rdflib.net/)  * [rdfextras](http://code.google.com/p/rdfextras/) (`easy_install rdfextras`)  * [pyparsing](http://pyparsing.wikispaces.com/) (`easy_install pyparsing`)  * [igraph](http://igraph.org/python/) (`easy_install python-igraph`)  	  (at least I had to install these packages additionally ;) )    If you're lucky, typing this is enough:    	easy_install rdflib python-igraph    and if you have problems there, update easy_install etc with:    	easy_install -U setuptools    Ubuntu, you can install the dependencies with pip after installing the relevant libraries  ```  sudo apt-get install python-dev build-essential libxml2-dev libxslt python-igraph  sudo pip install -r requirements.txt  ```  	  Purpose  -------  	  <b>Inputs</b>: [RDF](http://www.w3.org/TR/rdf-primer/), HTML and [OWL](http://www.w3.org/TR/owl-semantics/) description(s) of an RDF vocabulary<br/>  <b>Output</b>: an [XHTML+RDFa](http://www.w3.org/TR/rdfa-syntax/) specification designed for human users    Example  -------    	specgen6.py --indir=onto/olo/ --ns=http://purl.org/ontology/olo/core#  --prefix=olo --ontofile=orderedlistontology.owl --outdir=spec/olo/ --templatedir=onto/olo/ --outfile=orderedlistontology.html    * the template of this example can also be found in the folder: onto/olo  * the output of this example can also be found in the folder: spec/olo    See [libvocab.py](https://github.com/specgen/specgen/blob/master/libvocab.py) and [specgen6.py](https://github.com/specgen/specgen/blob/master/specgen6.py) for details.    Status  ------    * we load up and interpret the core RDFS/OWL   * we populate Vocab, Term (Class, Property or Individual) instances  * able to generate a XHTML/RDFa ontology specification with common concepts and properties from OWL, [RDFS](http://www.w3.org/TR/rdf-schema/), RDF    Known Forks  -----------    * [WebID fork](http://dvcs.w3.org/hg/WebID/file/029f115c08a5/ontologies/specgen) (note this link is only a reference to a specific revision of that fork; to ensure that you'll utilise the most recent one, go to summary and walk that path to the specgen directory again from the most recent revision ;) )    TODO  ----    * enable more OWL features, especially an automated construction of owl:Ontology (currently this must be done manually in the template)  * enable more support for other namespaces (super classes and super properties from other namespaces already possible)  * restructure the code !!!  * write a cool parser for the ""\n""'s and ""\t""'s etc. in the parsed comments (e.g. ""\n"" to \<br\/\> ...)    Known Issues  ------------    <ol>  	<li>librdf doesn't seem to like abbreviations in FILTER clauses.    <ul>  	<li>this worked:  <pre><code>q= 'SELECT ?x ?l ?c ?type WHERE { ?x rdfs:label ?l . ?x rdfs:comment ?c . ?x a ?type .  FILTER (?type = &lt;http://www.w3.org/2002/07/owl#ObjectProperty\&gt;)  } '</code></pre></li>  <li>while this failed:  <pre><code>q= 'PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?x ?l ?c ?type WHERE { ?x rdfs:label ?l . ?x rdfs:comment ?c . ?x a ?type .  FILTER (?type = owl:ObjectProperty)  } '</pre></code>  (even when passing in bindings)</li>  <li>This forces us to be verbose, ie.  <pre><code>q= 'SELECT distinct ?x ?l ?c WHERE { ?x rdfs:label ?l . ?x rdfs:comment ?c . ?x a ?type . FILTER (?type = &lt;http://www.w3.org/2002/07/owl#ObjectProperty&gt; || ?type = &lt;http://www.w3.org/2002/07/owl#DatatypeProperty&gt; || ?type = &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#Property&gt; || ?type = &lt;http://www.w3.org/2002/07/owl#FunctionalProperty&gt; || ?type = &lt;http://www.w3.org/2002/07/owl#InverseFunctionalProperty&gt;) } '</pre></code></li>  </ul></li>	  <li>TODO: work out how to do "".encode('UTF-8')"" everywhere</li>  <li>Be more explicit and careful re defaulting to English, and more robust when multilingual labels are found.</li></ol>    PS  --    The [old project repository location at SourceForge](http://smiy.svn.sourceforge.net/viewvc/smiy/specgen/) is now deprecated. All new developments will be pushed to this repository location here at GitHub. """
Semantic web;https://github.com/schlegel/balloon;"""![](http://schlegel.github.io/balloon/images/balloon_logo_big.png)  =======  A tool-suite for Linked Data consumption. **balloon** aims in offering public services and tools to take advantage of the semantic web with less effort.  The basic motivation is to establish a foundation for Linked Data as a Service (**LDaaS**).     **!!! The project is currently under development and will be available soon !!!**    More detailed information available online:  [http://schlegel.github.io/balloon](http://schlegel.github.io/balloon)    #Features  - **balloon Overflight**  	-	SPARQL based LOD crawling  	-	Index of basic structural relationships  	- Bird's-eye view on Linked Data  - **balloon Fusion**   	- SPARQL query federation service based on equivalence resources  	- Automatic endpoint discovery  - **balloon Synopsis** ([moved to a seperate github repository](https://github.com/schlegel/balloon-synopsis))  	- HTML/JavaScript RDF Viewer & Browser  	- jQuery Plugin   	-	Automatic information enhancement  	-	Configurable Templating  -	**balloon Commonalities**  	-	Finding common (relevant) super types of entities  	-	Finding instances for given type  	-	Finding related entities based on types and predicates    #Overview  Today’s vision of a common Web of Data is mostly achieved and coined by the Linked Open Data movement. The first wave of this movement transformed silo-based portions of data into a plethora of open accessible and interlinked data sets. The community itself provided guidelines (e.g., 5 ★ Open Data ) as well as open source tools to foster interactions with the Web of data. Harmonization between those data sets has been established at the modeling level with unified description schemes characterizing a formal syntax and common data semantic. Without doubt, Linked Open Data is the de-facto standard to publish and interlink distributed data sets in the Web commonly exposed in SPARQL endpoints. However, a convenient request considering the globally described data set is only possible with strong limitations. **balloon** wants to overcome this issue by providing a large choice of services to simplify utilizing the web of data for your projects. We think that Linked Data tools should be available as a service (**LDaaS**), to avoid high setup and integration costs and data duplication. Therefore all balloon services will be public available and don't need to be installed locally.    ---------------------------------------  This software has been brought to you by the [Chair of Distributed Information Systems (DIMIS)](dimis.fim.uni-passau.de) and [Media Computer Science (MiCS)](http://mics.fim.uni-passau.de/) at the University Passau. The project was developed within the CODE project funded by the EU Seventh Framework Programme, grant agreement number 296150. """
Semantic web;https://github.com/netage/carml-cli;"""# carml-cli  Interface for CARML library. At this moment works only with xml files.    #How to build  Build with maven  ```  mvn clean package  ```    #How to use  - Possible to convert a single file:  ```  java -jar cli-0.0.1-SNAPSHOT-jar-with-dependencies.jar -i inputfile.xml -m rml.mapping.ttl -o output  ```    - Possible to convert a folder (After the conversion of each file the output is streamed to the rdf output file):  ```  java -jar cli-0.0.1-SNAPSHOT-jar-with-dependencies.jar -f /folder -m /rml.mapping.ttl -o /output.ttl  ```    - Adding output format:  ```  java -jar cli-0.0.1-SNAPSHOT-jar-with-dependencies.jar -f /folder -m /rml.mapping.ttl -o /output.nt -of nt  ```    - Adding mapping format:  ```  java -jar cli-0.0.1-SNAPSHOT-jar-with-dependencies.jar -f /folder -m /rml.mapping.ttl -o /output.nt -mf nt  ```   """
Semantic web;https://github.com/rollxx/antlr-sparql-grammar;"""SPARQL grammars  ================    This project includes parser and lexer grammars in ANTLR-specific syntax for the following SPARQL specifications:    - SPARQL 1.1 update and query   - SPARQL 1.0   - OpenLink's Virtuoso-specific SPARQL Parser grammar """
Semantic web;https://github.com/yasarkhangithub/SAFE;"""# SAFE: SPARQL Federation over RDF Data Cubes with Access Control    SAFE, a SPARQL query federation engine that enables decentralised, access to clinical information represented as RDF data cubes with controlled access.    ## Experimental Setup  The experimental setup (i.e. code, datasets, setting, queries) for evaluation of SAFE is described here.    ### Code  The SAFE source code can be checkedout from [SAFE GitHub Page](https://github.com/yasarkhangithub/SAFE/).     ### Datasets  We use two groups of datasets exploring two different use cases, i.e. INTERNAL and EXTERNAL.    The first group of datasets **(INTERNAL)** are collected from the three clinical partners involved in our primary use case. These datasets contain aggregated clinical data represented as RDF data cubes and are privately owned/restricted.    The second group of datasets **(EXTERNAL)** are collected from legacy Linked Data containing sociopolitical and economical statistics (in the form of RDF data cubes) from the **World Bank**, **IMF (International Monetary Fund)**, **Eurostat**, **FAO (Food and Agriculture Organization of the United Nations)** and **TI (Transparency International)**.    External datasets used in evaluation experiments of SAFE can be downloaded from [SAFE External Datasets](https://goo.gl/6s4juv).    ### Settings    Each dataset was loaded into a different SPARQL endpoint on separate physical machines. All experiments are carried out on a local network, so that network cost remains negligible. The machines used for experiments have a 2.60 GHz Core i7 processor, 8 GB of RAM and 500 GB hard disk running a 64-bit Windows 7 OS. Each dataset is hosted as a Virtuoso Open Source SPARQL endpoint hosted physically on separate machines. The details of the parameters used to configure Virtuoso are listed in Table 1 below. Virtuoso version 7.2.4.2 has been used in experiments.    **Table 1:** SPARQL Endpoints Configuration    | SPARQL Endpoint       | Port           | URL  | Virtuoso Config Parameters  |  | ------------- |-------------| -----| -----|  | IMF      | 8890 | {System-IP}:8890/sparql | NoB=680000, MDF=500000, MQM=8G |  | World Bank      | 8891      |   {System-IP}:8891/sparql | NoB=680000, MDF=500000, MQM=8G |  | TI | 8892      |    {System-IP}:8892/sparql | NoB=680000, MDF=500000, MQM=8G |  | Eurostat | 8893      |    {System-IP}:8893/sparql | NoB=680000, MDF=500000, MQM=8G |  | FAO | 8895      |    {System-IP}:8895/sparql | NoB=680000, MDF=500000, MQM=8G |    - *NoB = NumberOfBuffers*  - *MDF = MaxDirtyBuffers*  - *MQM = MaxQueryMem*    ### Queries    A total of 15 queries are designed to evaluate and compare the query federation performance of SAFE against FedX, HiBISCuS and SPLENDID based on the metrics defined. We define ten queries for the federation of EXTERNAL datasets and five for the federation of INTERNAL datasets. Only ten queries (EXTERNAL dataset queries) are made available due to owner restrictions.    Queries used in evaluation experiments of SAFE can be downloaded from [SAFE Queries](https://goo.gl/WCCnx3).     ### Metrics    For each query type we measured (1) the number of sources selected; (2) the average source selection time; (3) the average query execution time; and (4) the number of ASK requests issued to sources.    ## Team    [Yasar Khan](https://www.insight-centre.org/users/yasar-khan)    [Muhammad Saleem](http://aksw.org/MuhammadSaleem.html)    [Aidan Hogan](http://aidanhogan.com/)    [Muntazir Mehdi](https://www.insight-centre.org/users/muntazir-mehdi)    [Qaiser Mehmood](https://www.insight-centre.org/users/qaiser-mehmood)    [Dietrich Rebholz-Schuhmann](https://www.insight-centre.org/users/dietrich-rebholz-schuhmann)    [Ratnesh Sahay](https://www.insight-centre.org/users/ratnesh-sahay) """
Semantic web;https://github.com/arc-lasalle/Map-On;"""# Map-On Ontology Mapping environment  Map-On is a web-based editor for visual ontology mapping developed at the Architecture, Representation and Computation research group of La Salle, Ramon Llull University. The Map-On editor provides a graphical environment for the ontology mapping creation using an interactive graph layout. A point-and-click interface simplifies the mapping creation process. The editor automatically generates a R2RML document based on user inputs, particularly producing IRI patterns and SQL queries. It has been used in real scenarios alleviating the effort of coding R2RML statements which is one of the main barriers for adopting R2RML in research and in the industry communities.    The Map-On features:  -	Multiuser web environment for manual creation of relational-to-ontology mappings.  -	Mapping spaces for distribution of the mapping creation process.  -	Top-down visual representation of relational source schema, ontology structure, and mappings based on a graph layout which can be customised by users.  -	Visual representation of an ontology using [WebVOWL](https://github.com/VisualDataWeb/WebVOWL) and a relational source based on Entity-Relationship diagrams.  -	Input relational sources can be a SQL database or a tabular source such as comma separated values (CSV) file.  -	Support of R2RML recommendation.   -	R2RML documents generated by [AutoMap4OBDA](https://github.com/arc-lasalle/AutoMap4OBDA) can be imported in Map-On.   -	Automated generation of IRI patterns and SQL queries based on mappings defined by users.  -	Dialog window in input boxes with suggestions of elements to be used in the mappings based on the text introduced by users.  -	Point-and-click interface for reducing the effort required for mapping activities .  -	Ontology-driven mapping approach, where the mapping process starts from the ontology instead of working with the database.  -	Contextual menus to help users in mapping creation.  -	Log of the activities carried out by users.  -	Pop-ups with tips as an integrated help.    http://semanco-tools.eu/map-on    http://arc.salleurl.edu/    Map-On tool has been developed in PHP using the framework Code Igniter. The graphical ontology representation has been implement using the VivaGraphJS library and ARC to parse RDF files. The mapping file generated is written in the R2RML Mapping Language.    - CodeIgniter, Open source PHP web application framework – http://codeigniter.com  - Graph drawing library for JavaScript – https://github.com/anvaka/VivaGraphJS  - Text editor implemented in JavaScript with turtle syntax style – http://codemirror.net/  - ARC, Appmosphere RDF classes – https://github.com/semsol/arc2/wiki  - R2RML: RDB to RDF Mapping Language – http://www.w3.org/TR/r2rml/    For the Map-On installation please go to the [Installation guide](./docs/installation.md)    Please cite Map-On with the following reference:    >Sicilia, Álvaro; Nemirovski, German; Nolle, Andreas. [Map-on: A web-based editor for visual ontology mapping](http://www.semantic-web-journal.net/system/files/swj1266.pdf). Semantic Web, vol. 8, no. 6, pp. 969-980, 2017.    Copyright (C) 2019 ARC Engineering and Architecture La Salle, Ramon Llull University.     for comments please contact Álvaro Sicilia (alvaro.sicilia@salle.url.edu) """
Semantic web;https://github.com/anqit/spanqit;"""# spanqit    ***  ## Exciting update!  Spanqit has been merged into the Eclipse Foundation project [rdf4j](http://rdf4j.org/)! As such, further development will continue there first, and perhaps backported here. Check out links to [docs](https://rdf4j.org/documentation/tutorials/sparqlbuilder/) and the [source code](https://github.com/eclipse/rdf4j/tree/main/core/sparqlbuilder)  ***      A Java-based SPARQL query generator    Spanqit is an open source, fluent Java library to programatically create SPARQL queries. See the [wiki](https://github.com/anqit/spanqit/wiki) and javadocs (coming soon) for more info.      ### New with version 1.1:  - Support for all of the [Update Queries](https://www.w3.org/TR/sparql11-update/)!!  - No more need for adapters, all RDF elements can be created from with Spanqit, including IRI's and blank nodes  - support for more SPARQL syntax constructs like predicate-object lists in triple patterns and blank nodes  - more convenience methods  - some bug fixes and javadocs  Check out the updated [examples project](https://github.com/anqit/spanqit-examples) for detailed usage """
Semantic web;https://github.com/ont-app/igraph;"""# <img src=""http://ericdscott.com/NaturalLexiconLogo.png"" alt=""NaturalLexicon logo"" :width=100 height=100/> ont-app/igraph    IGraph defines a protocol which aims to provide a general interface to  a variety of graph-based representations (RDF, datascript, datomic,  loom, ...)    It also defines a `Graph` datatype which implements `IGraph`.    There is a [15-minute video introduction here](https://www.youtube.com/watch?v=BlH__4iNHZE&amp;feature=youtu.be).    ## Contents  - [Dependencies](#h2-dependencies)  - [Motivation](#h2-motivation)  - [The IGraph protocol](#h2-igraph-protocol)    - [Methods summary](#h3-methods-summary)    - [Member access](#Member_access)      - [Normal form](#Normal_form)      - [Tractability](#h4-tractability)      - [`subjects`](#subjects_method)      - [`get-p-o`](#get-p-o_method)      - [`get-o`](#get-o_method)      - [`ask`](#ask_method)      - [`query`](#query_method)      - [`invoke` for arities 0-3](#invoke_method)    - [Content Manipulation](#Content_Manipulation)      - [`mutability`](#mutability_method)      - [The `add-to-graph` multimethod](#add-to-graph)      - [The `remove-from-graph` multimethod](#remove-from-graph)  - [The IGraphImmutable protocol](#IGraphImmutable)      - [`add`](#add_method)      - [`subtract`](#subtract_method)  - [The IGraphMutable protocol](#IGraphMutable)      - [`add!`](#add!_method)      - [`subtract!`](#subtract!_method)  - [The IGraphAccumulateOnly protocol](#IGraphAccumulateOnly)      - [`claim`](#claim_method)      - [`retract`](#retract_method)  - [The IGraphSet protocol](#h2-igraphset-protocol)    - [Methods summary](#h3-igraphset-methods-summary)    - [`union`](#union_method)    - [`intersection`](#intersection_method)    - [`difference`](#difference_method)  - [Traversal](#Traversal)    - [The `traverse` function](#traverse_function)    - [Traversal functions](#Traversal_functions)      - [Context](#h4-context)      - [The queue](#queue)    - [Traversal utilities](#Traversal_utilities)      - [`transitive-closure`](#h4-transitive-closure)      - [`traverse-link`](#h4-traverse-link)      - [`maybe-traverse-link`](#h4-maybe-traverse-link)      - [`traverse-or`](#h4-traverse-or)    - [Traversal composition with `t-comp`](#Traversal_composition)      - [short form](#h4-t-comp-short)      - [long form](#h4-t-comp-long)    - [Using traversal functions as a `p` argument to `invoke`](#traversal-fn-as-p)  - [Cardinality-1 utilites](#cardinality-1_utilities)    - [`unique`](#h3-unique)    - [`flatten-description`](#h3-flatten-description)    - [`normalize-flat-description`](#h3-normalize-flat-description)    - [`assert-unique`](#h3-assert-unique)  - [I/O](#i-o)    - [`write-to-file`](#h3-write-to-file)    - [`read-from-file`](#h3-read-from-file)  - [Other utilities](#Other_utilities)    - [`reduce-spo`](#h3-reduce-spo)  - [Implementations](#h2-implementations)    - [`ont-app.igraph.graph/Graph`](#Graph)      - [Graph creation](#h4-graph-creation)      - [Querying](#h4-querying)    - [sparql-client](#h3-sparql-client)    - [datascript-graph](#h3-datascript-graph)    - [datomic-client](#h3-datomic-client)  - [Acknowledgements](#h2-acknowledgements)  - [Future Work](#h2-future-work)  - [License](#h2-license)  ---    <a name=""h2-dependencies""></a>  ## Dependencies    This is deployed to [clojars](https://clojars.org/ont-app/igraph):    [![Clojars  Project](https://img.shields.io/clojars/v/ont-app/igraph.svg)](https://clojars.org/ont-app/igraph)    ```  [ont-app/igraph ""0.1.7""]  ```    Require thus:  ```  (:require     [ont-app.igraph.core :as igraph] ;; for the IGraph protocol and related stuff    [some.igraph.implementation ...] ;; implements IGraph    )               ```    <a name=""h2-motivation""></a>  ## Motivation    One of the defining characteristics of Clojure is that it revolves  around a minimal set of basic data structures.    I think it can be argued that the collection primitives in Clojure can  be approximately ordered by their degree of expressiveness:    - seq - first/rest  - < set - membership  - < vector - indexable members  - < map - a collection of associations    The conceit of IGraph is that there is room for a new collection  primitive with one higher level of expressiveness:    - < graph - a collection of named relationships between named entities    This is informed to a large degree by the  [RDF](https://www.wikidata.org/wiki/Q54872) model, and aims to align  with [linked data](https://www.wikidata.org/wiki/Q515701) encoded in  RDF, while keeping direct dependencies to a minimum.    <a name=""h2-igraph-protocol""></a>  ## The IGraph protocol    This protocol defines the basic operations over a graph conceived of  as a set of triples S-P-O, where subject `S` and object `O` typically  name entities, and property `P` is a named relation that applies  between those entities.    This is directly inspired by the RDF model, but the requirement that  these identifiers adhere strictly to RDF specifications for URIs, and  that literal values be restricted to a small set of scalars is relaxed  quite a bit.    <a name=""h3-methods-summary""></a>  ### Methods summary    The `IGraph` protocol specifies the following methods:    #### Member access  - `(normal-form g)` -> `{s {p #{o...}...}...}`  - `(subjects g)` -> `(s ...)`, a lazy sequence  - `(get-p-o g s)` -> `{p #{o...} ...}`  - `(get-o g s p)` -> `#{o ...}`  - `(ask g s p o)` -> truthy  - `(query g q)` -> implementation-dependent query results    #### Content manipulation  - `(mutability g)` -> One of `#{::read-only ::immutable ::mutable ::accumulate-only}`    #### `invoke` to support `IFn`  - `(g)` = `(normal-form g)`  - `(g s)` -> {p #{o...} ...}  - `(g s p)` -> #{o ...}  - `(g s p o)` -> truthy    <a name=""Member_access""></a>  ### Member access    <a name=""Normal_form""></a>  #### Normal form    Any implemetation of this protocol, regardless of its _native  representation_ must be expressable in IGraph's `Normal Form`.    As an example, let's start with a graph called 'eg' with four triples:    ```  > (igraph/normal-form eg)  {:john     {:isa #{:person},      :likes #{:beef}},   :mary    {:isa #{:person},     :likes #{:chicken}}}  >  ```    These are facts about two subjects, :john and :mary with two facts  each.    John is a person who likes beef.    Mary is also a person, and likes chicken.      The normal form has three tiers. The ""s-level"" is a map from each  subject to a ""p-level"" `description` of that subject.  The normal form  for descriptions is a map from a property identifier to an ""o-level""  set of objects for said subject and property.    What I'm aiming for here is a form that's  - extremely regular and simple  - lends itself to expressing and thinking about basic set operations  on graphs.    ##### A note on the keyword identifiers used in these examples    To keep things simple and readable, none of the keywords used in these  examples are [namespaced](https://blog.jeaye.com/2017/10/31/clojure-keywords/#namespaced-keywords).     In practice you will probably want to used namespaced keywords, and  some implementations of IGraph, e.g. those that interact directly with  RDF-based representations, will expect them.      <a name=""h4-tractability""></a>  #### Tractability    It is expected that while many implementations of IGraph will be  in-memory data structures of modest size, others might be huge  knowledge bases provided on a server somewhere  ([Wikidata](https://www.wikidata.org/wiki/Q2013), for example). In the  latter case it is always acceptable throw an `::igraph/Intractable`  for any method that warrants it:    ```  (throw (ex-info ""Normal form for Wikidata is intractable""     {:type ::igraph/Intractable}))    ```      <a name=""subjects_method""></a>  #### `subjects`    The `subjects` method must return a lazy sequence of complete set of  subjects in the graph (modulo tractability):    ```  > (igraph/subjects eg)  `(:john :mary)  > (type (igraph/subjects eg))  clojure.lang.LazySeq  >  ```    <a name=""get-p-o_method""></a>  #### `get-p-o`    We must be able to get the p-level description of any subject with  `get-p-o`:    ```  > (igraph/get-p-o eg :john)  {:isa #{:person}, :likes #{:beef}}  >  ```    <a name=""get-o_method""></a>  #### `get-o`    We must be able to get the o-level set of objects for any subject and  predicate with `get-o`:    ```  > (igraph/get-o eg :john :isa)  #{:person}  >  ```     <a name=""ask_method""></a>  #### `ask`    We must be able to test for whether any particular triple is in the  graph with `ask` (any truthy response will do).    ```   > (igraph/ask eg :john :likes :beef)   :beef   > (igraph/ask eg :john :likes :chicken)   nil  >  ```    <a name=""query_method""></a>  #### `query`    We must be able to query the graph using a format appropriate to the  native representation. This example uses the format expected by  `ont-app.igraph.graph/Graph`, described [below](#h4-querying):    ```  > (igraph/query eg [[:?person :isa :person]])  #{{:?person :mary} {:?person :john}}  >  ```    In this case, the result is a set of `binding maps`, mapping  :?variables to values, similar to the result set of a  [SPARQL](https://www.wikidata.org/wiki/Q54871) query.    For comparison, here is a sketch of an equivalent SPARQL query, which  would be appropriate if our IGraph protocol was targeted to a SPARQL  endpoint which we might call `sparql-eg`:    ```  > (query sparql-eg     ""PREFIX : <http://path/to/my/ns#>      SELECT * WHERE      {       ?person a :person     }"")  [{:person :mary} {:person :john}]  >   ```    <a name=""invoke_method""></a>  #### `invoke` for arities 0-3    An instance of IGraph must provide `invoke` implementations as  follows:    Without arguments, it must return Normal Form (or throw an ::igraph/Intractable):    ```  > (eg)  {:john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:chicken}}}  >    ```    With a single ""s"" argument, it must treat the argument as the subject  of get-p-o:    ```  > (eg :john)  {:isa #{:person}, :likes #{:beef}},  >  ```    With two arguments ""s"" and ""p"", a set of objects must be returned:    ```  > (eg :mary :likes)  #{:chicken}  >  ```    This will often be the value of `get-o`, but it may also accept as the  ""p"" argument a _traversal function_, described  [below](#traversal-fn-as-p).    With three arguments ""s"" ""p"" and ""o"", the response must be truthy:    ```  > (eg :mary :likes :chicken)  :chicken  >  >  > (eg :mary :likes :beef)  nil  >  ```    This will often be equivalent to `ask`, but again, the ""p"" argument  can be a traversal function, described [below](#traversal-fn-as-p).    <a name=""Content_Manipulation""></a>  ### Content Manipulation    There a several factors to take into account when adding or removing  content from a graph.    Some graphs, (such as a public SPARQL endpoint to which one does not  have UPDATE permissions) may not be subject to modification. Other  native representations (such as a SPARQL endpoint with UPDATE  permissions) might best be treated as mutable graphs.    Naturally, other things being equal, the preferred solution is to use  immutable graphs when it is possible to do so. The examples in this  README will all be applied to immutable graphs.    <a name=""mutability_method""></a>  #### `mutability`    The `mutability` method returns one of the following values  - `::igraph/read-only` - there is no means for altering the contents of    the graph  - `::igraph/immutable` - the graph implements    [IGraphImmutable](#IGraphImmutable)  - `::igraph/mutable` - the graph implements    [IGraphMutable](#IGraphMutable)  - `::igraph/accumulate-only` - the graph implements [IGraphAccumulateOnly](#IGraphAccumulateOnly), the approach used in Datomic    <a name=""add-to-graph""></a>  #### The `add-to-graph` multimethod    IGraph defines a multimethod `add-to-graph`, dispatched on the type of  graph, and a function `triples-format`. This multimethod can inform  mutable, immutable and accumulate-only graphs.    Naturally Normal Form is one possible format:    ```  > (igraph/triples-format {:john {:likes# #{:beef}}})  :normal-form  >  ```    Another possible value is `:vector`, with a subject and at least one  P-O pair:    ```  > (igraph/triples-format [:john :likes :beef])  :vector  > (igraph/triples-format [:john :isa :person :likes :beef])  :vector  >  ```     Finally, we have `:vector-of-vectors`:    ```  > (igraph/triples-format  [[:john :isa :person] [:mary :isa :person]])  :vector-of-vectors  >  ```    Any implementation of IGraph should support adding to the graph in all  of these formats.    <a name=""remove-from-graph""></a>  #### The `remove-from-graph` multimethod    IGraph also defines multimethod `remove-from-graph`, dispatched on the  graph types and a function `triples-removal-format`. This multimethod  can inform both mutable and immutable graphs.    The `triples-removal-format` function returns the same keywords as  `triples-format`, but adds one more: `:underspecified-triple`, a  vector with fewer than 3 elements:    ```  > (igraph/triples-removal-format [:john])  :underspecified-triple  > (igraph/triples-removal-format [:john :likes])  :underspecified-triple  >  ```    `triples-removal-format` assigns the :vector-of-vectors flag to a  vector of either :vector or :underspecified-vector. All  implementations of IGraph should support each of these flags.    This allows us to subtract any format that could also be added, plus  all `[s * *]` or all `[s p *]`.        <a name=""IGraphImmutable""></a>  ## The IGraphImmutable protocol    An add or subtract operation to an immutable graph returns a cheap  copy of the original graph modified per the argument provided.    <a name=""add_method""></a>  ### `add`    Calling `(add g to-add)` must return an immutable graph such that the  graph now contains `to-add`. Any triples in `to-add` which are already  in the graph should be skipped.    See the notes above about the [add-to-graph](#add-to-graph)  multimethod.    Typically adding to a graph in code is most easily expressed using a  vector or a vector of vectors:    ```  > (igraph/normal-form       (igraph/add         eg         [[:chicken :subClassOf :meat]         [:beef :subClassOf :meat]         ]))  {:john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:chicken}},   :chicken {:subClassOf #{:meat}},   :beef {:subClassOf #{:meat}}}  >  ```    We can use the Normal Form of one graph to add it to another:    ```  > (meats)  {:chicken {:subClassOf #{meat}}   :beef {:subClassOf #{meat}}}  >  > (igraph/normal-form (add eg (meats)))  {:john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:chicken}},   :chicken {:subClassOf #{:beef}},   :beef {:subClassOf #{:beef}}}  >   ```    <a name=""subtract_method""></a>  #### `subtract`    The multimethod `remove-from-graph` supports the `subtract` operation,  dispatched on the type of the graph and `triples-removal-format`,  described [above](#remove-from-graph):    ```  > (igraph/normal-form (igraph/subtract eg [:john]))  {:mary {:isa #{:person}, :likes #{:chicken}}}  >  > (igraph/normal-form (igraph/subtract eg [:john :likes]))  {:john {:isa #{:person}},    :mary {:isa #{:person}, :likes #{:chicken}}}  >  ```    <a name=""IGraphMutable""></a>  ## The IGraphMutable protocol    Some graphs' native representations are implemented as mutable  repositories. To support this, the IGraphMutable protocol provides  methods `add!` and `subtract!`.    The [add-to-graph](#add-to-graph) and  [remove-from-graph](#remove-from-graph) multimethods should still  inform the logic here, and the behavior should be essentially the  same, with the exception that the graph returned is the same object,  mutated as specified.    <a name=""add!_method""></a>  ### `add!`    `(add! g to-add)` -> g, where g is both the argument and return value.    An error should be thrown if `(mutablility g)` != ::igraph/mutable.    <a name=""subtract!_method""></a>  ### `subtract!`    `(subtract! g to-subtract)` -> g, where g is both the argument and  return value.    An error should be thrown if `(mutablility g)` != ::igraph/mutable.    <a name=""IGraphAccumulateOnly""></a>  ## The IGraphAccumulateOnly protocol    A graph whose native representation is based on  [Datomic](https://www.datomic.com/) implements what Datomic calls an  ""Accumulate-only"" approach to adding and removing from a graph. To  support this, the IGraphAccumulateOnly protocol provides methods  `claim` (corresponding to the datomic 'add' operation), and  `retract`. In this scheme the state of the graph can be rolled back to  any point in its history. See the [Datomic  documentation](https://docs.datomic.com/) for details.    The [add-to-graph](#add-to-graph) and  [remove-from-graph](#remove-from-graph) multimethods should still  inform the logic here, and the behavior should be essentially the  same, with the exception that the graph returned now points to the  most recent state of the graph after making the modification. Any  given instantiation of the graph will remain immutable.    <a name=""claim_method""></a>  ### `claim`    `(claim g to-add)` -> g', where g is an append-only graph, and  g' now points to the most recent state of g's  [transactor](https://docs.datomic.com/on-prem/transactor.html).    An error should be thrown if `(mutablility g)` != ::igraph/accumulate-only.    <a name=""retract_method""></a>  ### `retract`    `(retract g to-retract)` -> g', where g is an append-only graph, and  g' now points to the most recent state of g's  [transactor](https://docs.datomic.com/on-prem/transactor.html).    An error should be thrown if `(mutablility g)` != ::igraph/accumulate-only.      <a name=""h2-igraphset-protocol""></a>  ## The IGraphSet protocol    It will make sense for many implementations of IGraph also to  implement the basic set operations, defined in IGraphSet. Set  operations may not be suitable between very large graphs.    For purposes of demonstration, let's assume a second graph `other-eg`:    ```  > (igraph/normal-form other-eg)  {:mary {:isa #{:person}, :likes #{:pork}},   :waldo {:isa #{:person}, :likes #{:beer}}}  >  ```    I think examples of each operation should serve to describe them.    <a name=""h3-igraphset-methods-summary""></a>  ### Methods summary  - `(union g1 g2)` -> A new graph with all triples from both graphs  - `(difference g1 g2)` -> A new graph with triples in g1 not also in    g2  - `(intersection g1 g2)` -> A new graph with only triples shared in    both graphs    <a name=""union_method""></a>  ### `union`  ```  > (igraph/normal-form (igraph/union eg other-eg))  {:john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:pork :chicken}},   :waldo {:isa #{:person}, :likes #{:beer}}}  >    ```     <a name=""intersection_method""></a>  ### `intersection`    ```  > (igraph/normal-form (igraph/intersection eg other-eg))   {:mary {:isa #{:person}}  >  ```    <a name=""difference_method""></a>  ### `difference`  ```  > (igraph/normal-form (igraph/difference eg other-eg))  {:john {:isa #{:person}, :likes #{:beef}},    :mary {:likes #{:chicken}}}  >  > (igraph/normal-form (igraph/difference other-eg eg))  {:mary {:likes #{:pork}}, :waldo {:isa #{:person}, :likes #{:beer}}}  >  ```    <a name=""Traversal""></a>  ## Traversal    Clojure and other functional programming languages have a  [reduce](https://clojuredocs.org/clojure.core/reduce) idiom, which  allows the user to create aggregations over a sequence by providing a  ""reducer function"" expressing the relationship between each member of  that sequence and the resulting aggregation.    IGraph defines a `traverse` function to allow the user to create  aggregations over the contents of a graph by providing a `traversal  function`, which is analogous to a reducer function, but is  nessesarily a bit more involved.      - `(traverse g traversal context acc queue)` -> `acc'`  - `(traverse g traversal acc queue)` -> `acc'` ;; default context = {}        ... traversing `g` per the `traversal` function, starting with the      first element of `queue`, possibly informed by `context`.    This function will repeatedly call the `traversal` function until  `queue` is empty, returning the final value for `acc`. Each call to  the traversal function returns modified versions of `context`, `acc`  and `queue`.    To illustrate traversal, let's expand on our `eg` graph by adding some  type structure:    Assume we have a graph called 'eg-with-types':    ```  > (def eg-with-types       (add eg        [[:person :subClassOf :thing]         [:beef :subClassOf :meat]         [:chicken :subClassOf :meat]         [:meat :subClassOf :food]         [:beer :subClassOf :beverage]         [:beverage :subClassOf :consumable]         [:food :subClassOf :consumable]         [:consumable :subClassOf :thing]]))  eg-with-types  > (eg-with-types)  {:consumable {:subClassOf #{:thing}},   :beef {:subClassOf #{:meat}},   :person {:subClassOf #{:thing}},   :beer {:subClassOf #{:beverage}},   :meat {:subClassOf #{:food}},   :food {:subClassOf #{:consumable}},   :beverage {:subClassOf #{:consumable}},   :pork {:subClassOf #{:meat}},   :john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:chicken}},   :chicken {:subClassOf #{:meat}}}  ```    Our `eg-with-types` now provides a bit more context for what's going  on with our heroes John and Mary. Note that `:isa` and `:subClassOf`  differ in their _domain_. `:isa` relates an instance to its class,  while `:subClassOf` relates a class to its parent.    <a name=""traverse_function""></a>  ### The `traverse` function    Here's an example of how the `traverse` function works, starting with  a traversal function we'll call `subClassOf*`, which follows and  accumulates all :subClassOf links, starting with an initial queue of  say, `[:meat :beer]`:    ```  > (igraph/traverse eg-with-types subClassOf* {} #{} [:meat :beer])  #{:consumable :beer :meat :food :beverage :thing}  >  ```    The arguments for `traverse` are  - `g` - an invariant graph  - `traversal-fn` - A function `[g c acc q]` -> `[c' acc' q']`,    defining the logic of each step in the traversal  - `context` - (optional) a map holding the traversal history plus    whatever `traversal-fn` may want to track. Default is {}  - `acc` - (optional) accumulates the resulting value of the traversal. Default is `[]`.  - `queue` - the starting queue    <a name=""Traversal_functions""></a>  #### Traversal functions    The traversal function takes 4 arguments and returns a vector of  length 3.    ```  > (subClassOf* eg-with-types {} #{} [:meat])  [{} #{:meat} (:food)]  >  > (subClassOf* eg-with-types {} #{:meat} '(:food))  [{} #{:meat :food} (:consumable)]  >  ```    The first argument is the invariant graph itself.    The second argument (and first element returned) is the context, which  subClassOf* leaves unchanged.  Context is used by `traverse` to avoid  cycles, and will be explained in detail [below](#h4-context). More  sophisticated traversal functions may use the context as a kind of  blackboard.    The third argument (and second element returned) is the value to be  accumulated, identical to its counterpart in the _reduce_ idiom.    The fourth argument (and third element returned) is the traversal  queue. It must be sequential, and may be ordered in any way that makes  sense. An empty queue signals and end of the traversal, at which point  `traverse` will return the value of the accumulator.    Here's a possible definition of subClassOf*:    ```  (defn subClassOf* [g c acc q]    ""Traversal function to accumulate super-classes.""    (let [elt (first q)]      [c                      ;; context is unchanged       (conj acc elt)         ;; updating the accumulator       (reduce conj          (rest q)          (g elt :subClassOf)) ;; adding super-classes to the queue         ]))  ```    <a name=""h4-context""></a>  #### Context    The `context` argument to `traverse` and its traversal function is a  map containing key-values which may inform the course of the  traversal, but are not part of the accumulated value. This will  include:  - `:history` set by `traverse`, and updated to hold all elements    encountered in the course of the traversal. In order to avoid    cycles, any element in the history will be skipped should it ever    re-appear at the head of the queue.      The `traverse` function also supports these optional keys in the  context:    - `:skip?` (optional), a function (fn[x] ...) -> truthy, applicable to    the head of the queue, which will override `:history`.      - `:seek` (optional), a function `(fn [context acc]...)` -> `acc'`. If    specified, this function will be called at the beginning of each    traversal, and if truthy and non-empty, the traversal will end    immediately with that value.    In addition, the traversal function may use the context as a  blackboard to communicate between iterations of the traversal. For  example, you may want to prune and re-order your queue based on a set  of heuristics, details of which are stored in the context.    <a name=""queue""></a>  #### The queue    The `queue` argument must be sequential, but is otherwise  unrestricted. An empty queue signals the end of the traversal, at  which point `traverse` will return the accumulated value.    Note that conj-ing to a vector in the traversal function suggests a  breadth-first traversal, while conj-ing to a seq suggests a  depth-first tranversal.    More sophisticated traversal functions may use the context to inform  logic to prune and re-order the queue to optimize the traversal.    <a name=""Traversal_utilities""></a>  ### Traversal utilities    IGraph provides utilities to express several common types of traversal  functions.    <a name=""h4-transitive-closure""></a>  #### `transitive-closure`    - `(trasitive-closure p)` -> `(fn [g context acc to-visit] ...) ->    [context' acc' queue']`,        This returns a traversal function which will accumulate all _o_    s.t. any _s_ in the queue is associated with _o_ through zero or    more _p_ links.    So in the example above, the `subClassOf*` function could be defined  thus:    ```  (def subClassOf* (igraph/transitive-closure :subClassOf))  ```    <a name=""h4-traverse-link""></a>  ### `traverse-link`    - `(traverse-link p)` -> (fn [g context acc queue] ...) -> [context    acc' []],    The function returned here will accumulate all _o_ s.t. for all _s_ in  _queue_, (g s p o) is truthy:    ```  > (igraph/traverse       eg-with-types       (igraph/traverse-link :isa)       #{}       [:john :mary])  #{:person}  >  ```    <a name=""h4-maybe-traverse-link""></a>  #### `maybe-traverse-link`    - `(maybe-traverse-link p)` -> (fn [g context acc queue] ...) ->    [context acc' []]    Matches 0 or 1 occurrences of _p_:    ```  > (igraph/traverse eg-with-types       (igraph/maybe-traverse-link :isa)       #{}       [:john :mary])  #{:person :john :mary}  >  ```    <a name=""h4-traverse-or""></a>  #### `traverse-or`    - `(traverse-or & ps)` -> (fn [g context acc queue] ...) -> [context    acc' []],    Where _ps_ is one or more traversal functions, merging all of their outputs.    Keyword arguments are interpreted as an implicit `traverse-link`.    ```  > (def subsumed-by (igraph/traverse-or :isa :subClassOf))  subsumed-by  > (igraph/traverse eg-with-types subsumed-by #{} [:john])  #{:person}  >  > (igraph/traverse eg-with-types subsumed-by #{} [:meat])  #{:food}  >  ```    <a name=""Traversal_composition""></a>  ### Traversal composition with `t-comp`  Composition functions are composable with a 'short form' and a 'long  form'.    <a name=""h4-t-comp-short""></a>  #### short form    Short-form composition can be used when the traversal function meets  the following criteria:  - None of the component functions manipulate the traversal context  - Each component function accumulates a sequential value suitable to    serve as the initial queue of the component function that follows    it.    Such functions can be called as a simple vector:    ```  > (def instance-of       (igraph/t-comp [:isa (igraph/transitive-closure :subClassOf)]))  >  > (igraph/traverse eg-with-types instance-of #{} [:john])  #{:person :thing}  >  ```    <a name=""h4-t-comp-long""></a>  #### long form    In cases where you want to compose a traversal function that cannot  meet the criteria above, then instead of passing to `traversal-comp` a  vector of traversal functions, you pass in a map with the following  keys:    ```  { :path  [:<traversal-stage-1> :<traversal-stage-2> ...]     :<traversal-stage-1> {:fn <traversal-fn>                           :doc <docstring> (optional)                           :into <initial accumulator> (default [])                           :local-context-fn <context> (default nil)                           :update-global-context (default nil)                        }     :<traversal-stage-2> ...     ...   }   ```     A call to `(t-comp [:a :b :c])` is equivalent to calling `(t-comp  {:path [:a :b :c]})`.    These parameters should allow you as much control as you need over the  flow of contexts between each stage of traversal, and over the flow of  outputs from any one stage into the input queue of its next stage.    However, most of the time, the short form is sufficient, and at this point,  the long form has not been tested heavily.    ##### the :path parameter    This is a vector of traversal function specifications. Each traversal  function specification must be either:  - A traversal function  - A keyword with an entry in the long-form map  - A keyword eligible as an implicit [traverse-link](#h4-traverse-link)    If the traversal function specification is itself a function, it will  be applied directly.    If the traversal function specification is a keyword, and the t-comp  map has a matching entry for that keyword, it will look for and  interpret a map with the parameters described in the next section.    If the spec is a keyword without an entry in the long-form map, it is  assumed to be a candidate for an implicit traverse-link, i.e. a graph  element in 'p' position in _g_.    ##### traversal specification parameters    - :fn - a traversal function  - :doc (optional) - a docstring  - :into (optional) - a container to which the output should be coerced    (default [])  - :local-context-fn (optional) - a function [global-context] -> `local    context` producing the context for this stage of the traversal.  - :update-local-context (optional) - a function [global-context    local-context] -> `global-context'`, capturing whatever aspects of    the current stage of traversal may be of interest to subsequent    stages.    <a name=""traversal-fn-as-p""></a>  ### Using traversal functions as a `p` argument to `invoke`    Recall that implementations of IGraph should provide `invoke`  functions with 0-3 arguments.    Two of these functions involve specification of a _p_ parameter:    ```  (g s p) -> {<o>...}    (g s p o) -> truthy.  ```    This is informed by a multimethod dispatched on whether _p_ is a  function.    - `(match-or-traverse g s p)` -> #{<o>...}    - `(match-or-traverse g s p o)` -> truthy    A typical declaration for an IGraph implementation will contain  these two method declarations:    ```    #?(:clj clojure.lang.IFn       :cljs cljs.core/IFn)    ...    (invoke [g s p] (igraph/match-or-traverse g s p))    (invoke [g s p o] (igraph/match-or-traverse g s p o))    ...  ```    If the _p_ argument is a function, then _p_ will be expected to match  the signature of a traversal function, and the output of the method  will be the value of its traversal, starting with queue [_s_].    If _p_ is not a function it will be matched directly against elements  of the graph.    So given the traversal functions in the examples above:    ```  > (eg-with-types :beef subClassOf*)  #{:consumable :beef :meat :food :thing}  >  > (eg-with-types :beef subClassOf* :food)  :food  >  > (eg-with-types :john (igraph/t-comp [:likes subClassOf*]))  #{:consumable :beef :meat :food :thing}  >  ```        <a name=""cardinality-1_utilities""></a>  ## cardinality-1 utilites    Requiring normal form to provide a set as its 3rd-tier representation  has the advantage of ensuring that the normal form is as simple and  regular as possible, and makes it easy to think about set operations  over graphs. However, it can be a bit unwieldy when dealing with the  many cases where the descriptive map's keys reliably map to a single  scalar value.    The following utilities are provided to help with this:    - `(unique [x]) -> x` - translates a singleton sequence to its only    value  - `(flatten-description (g s))` Automatically translates the p-o    description into a simple k-v mappings wherever only a single _v_    exists.  - `(normalize-flat-description m)` is the inverse of    `flatten-description`.  - `(assert-unique g s p o) - replaces one singleton object with    another.    <a name=""h3-unique""></a>  ### `unique`    The `unique` function takes a sequence and an optional `on-ambiguity`  argument. Default on-ambiguity throws ex-info of type  `::igraph/Non-unique`.    ```  > (eg-with-types :john :isa)  {:person}  >  > (igraph/unique (eg-with-types :john :isa))  :person  >  > (igraph/unique (eg-with-types :beef subClassOf*))  Execution error (ExceptionInfo) at ont-app.igraph.core/unique$fn (core.cljc:640).  Unique called on non-unique collection  >  > (igraph/unique (eg-with-types :beef subClassOf*)                   first) ;; arbitrary disambiguation  :consumable  ```    Sometimes defining `the` as an alias for `unique` reads better, and is  easier to type:    ```  > (def the igraph/unique)  > (the (eg-with-types :john :isa))  :person  >  ```    <a name=""h3-flatten-description""></a>  ### `flatten-description`    ```  (igraph/flatten-description (eg-with-types :john))  {:isa :person, :likes :beef}  >  > (let [g (igraph/add                eg               [:john :likes :beer :has-vector [1 2 3]])          ]      (igraph/flatten-description (g :john)))  {:isa :person, :likes #{:beef :beer}, :has-vector [1 2 3]}  >  ```    <a name=""h3-normalize-flat-description""></a>  ### `normalize-flat-description`    This is the inverse of `flatten-description`:    ```  > (igraph/normalize-flat-description       {:isa :person, :likes #{:beef :beer}, :has-vector [1 2 3]})  {:isa #{:person}, :likes #{:beef :beer}, :has-vector #{[1 2 3]}}  >  > (let [g (igraph/add                eg                {:john (igraph/normalize-flat-description {:likes :beer})})          ]      (g :john))  {:isa #{:person}, :likes #{:beef :beer}}  >  ```    <a name=""h3-assert-unique""></a>  ### `assert-unique`    We can replace one singleton value with another using `(assert-unique  g s p o) -> g'`:    ```  > (let [g (igraph/assert-unique eg :john :isa :man)]      (g :john))  {:likes #{:beef}, :isa #{:man}}  >  ```    <a name=""i-o""></a>  ## I/O    In general writing the normal form of a graph to a stream and applying  the reader to it on the other end should be fairly  straightforward. Any predicates bearing reader-choking objects will of  course need to be filtered out.    At this point, only the :clj platform is directly supported with a  pair of functions to read/write to the file system.    <a name=""h3-write-to-file""></a>  ### `write-to-file`  `(write-to-file [path g] ...) -> path`    Will write an edn file with the normal form contents of _g_.    <a name=""h3-read-from-file""></a>  ### `read-from-file`    `(read-from-file [g path] ...) -> g'`    Will read the normal form contents of _path_ into _g_.    <a name=""Other_utilities""></a>  ## Other utilities    <a name=""h3-reduce-spo""></a>  ### `reduce-spo`  - `(reduce-spo f acc g)` -> `acc'`, such that _f_ is called on each  triple in _g_.  Where _f_ := `(fn [acc s p o]...) ->  acc'`. Cf. [reduce-kv](https://clojuredocs.org/clojure.core/reduce-kv).    ```  > (defn tally-triples [tally s p o]      (inc tally))  > (igraph/reduce-spo tally-triples 0 eg)  4  ```  <a name=""h2-implementations""></a>  ## Implementations    The `ont-app.igraph.graph` module makes one implementation of IGraph  available without any additional dependencies, and so far there are  three other libraries in the ont-app project which implement this  protocol.    Other implementations are planned, and I'd be interested to learn of  any implementations published by other parties.    <a name=""Graph""></a>  ### `ont-app.igraph.graph/Graph`    The IGraph library comes with `ont-app.igraph.graph`, whose `Graph`  deftype is a very lightweight implementation of IGraph.    Its native representation is just Normal Form. Any hashable object can  technically be provided for any _s_, _p_, or _o_, but best practice is  to keep non-identifiers a the ""o"" level if you want to play easily  with other IGraph implementations.    ```  (require '[ont-app.igraph.graph :as g])  ```    <a name=""h4-graph-creation""></a>  #### Graph creation    Use `make-graph` to create a new graph, with an optional `:contents`  argument.    ```  > (def eg (g/make-graph))  eg  > (eg)  {}  >  > (def eg      (g/make-graph         :contents {:john {:isa #{:person}, :likes #{:beef}},                   :mary {:isa #{:person}, :likes #{:chicken}}}  eg  > (eg)  {:john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:chicken}}}  >  ```    The `:contents` argument must be in Normal Form.      <a name=""h4-querying""></a>  #### Querying    Querying is done with a very simple vector-of-triples graph pattern  using keywords starting with "":?"" to serve as variables. It returns an  unordered set of binding maps. This is very minimalistic. Any  selecting, ordering, grouping or aggregation needs to be done  downstream from the call.    ```  > (igraph/query eg [[:?liker :likes :?liked]])  #{{:?liker :john, :?liked :beef}     {:?liker :mary, :?liked :chicken}}  >  ```    Traversal functions can be specified in _p_ position:    ```  > (igraph/query eg-with-types [[:?liker :likes ?liked]                                 [?liked subClassOf* :?liked-class]])  #{{:?liked :beef, :?liked-class :consumable, :?liker :john}    {:?liked :beef, :?liked-class :beef, :?liker :john}    {:?liked :chicken, :?liked-class :food, :?liker :mary}    {:?liked :chicken, :?liked-class :chicken, :?liker :mary}    {:?liked :chicken, :?liked-class :consumable, :?liker :mary}    {:?liked :beef, :?liked-class :meat, :?liker :john}    {:?liked :beef, :?liked-class :food, :?liker :john}    {:?liked :beef, :?liked-class :thing, :?liker :john}    {:?liked :chicken, :?liked-class :thing, :?liker :mary}    {:?liked :chicken, :?liked-class :meat, :?liker :mary}}  >  ```      <a name=""h3-sparql-client""></a>  ### sparql-client    <https://github.com/ont-app/sparql-client>    Implements a mutable IGraph for a [SPARQL  endpoint](https://www.wikidata.org/wiki/Q26261192). Initializtion  requires configuring query and update endpoints, and the query  language is [SPARQL](https://www.wikidata.org/wiki/Q54871).    Keyword identifiers are expected to be namespaced, and rely on the [ont-app/vocabulary](https://github.com/ont-app/vocabulary) library, which uses namespace metadata to intercede between Clojure namespaces and RDF namespaces.    Set operations are not supported.    <a name=""h3-datascript-graph""></a>  ### datascript-graph    <https://github.com/ont-app/datascript-graph>    This implements IGraph for a  [datascript](https://github.com/tonsky/datascript) native  representation, and may as such may need to be initialized with some  schema declarations. Query language is datalog. Immutable, with set  operations.    <a name=""h3-datomic-client""></a>  ### datomic-client    https://github.com/ont-app/datomic-client    This implements IGraph for the [Datomic Client API](https://docs.datomic.com/cloud/client/client-api.html). The query language is datalog. Mutability model is Accumulate Only. Set operations are not supported.    <a name=""h2-future-work""></a>  ## Future work  - Ports to loom, ubergraph, and other graph-oriented libraries   - There will be a regime for providing annotations for reified triples    (for weights and such).  - Ports to table-based representations  - `igraph.graph` will have query planning and indexing  - Some kind of a scheme to bring all the various query formats under a    single tent    <a name=""h2-acknowledgements""></a>  ## Acknowledgements    Thanks to [Ram Krishnan](https://github.com/kriyative) for his  feedback and advice.    <a name=""h2-license""></a>  ## License      Copyright © 2019-21 Eric D. Scott    Distributed under the Eclipse Public License either version 1.0 or (at  your option) any later version.    <table>  <tr>  <td width=75>  <img src=""http://ericdscott.com/NaturalLexiconLogo.png"" alt=""Natural Lexicon logo"" :width=50 height=50/> </td>  <td>  <p>Natural Lexicon logo - Copyright © 2020 Eric D. Scott. Artwork by Athena M. Scott.</p>  <p>Released under <a href=""https://creativecommons.org/licenses/by-sa/4.0/"">Creative Commons Attribution-ShareAlike 4.0 International license</a>. Under the terms of this license, if you display this logo or derivates thereof, you must include an attribution to the original source, with a link to https://github.com/ont-app, or  http://ericdscott.com. </p>   </td>  </tr>  <table> """
Semantic web;https://github.com/jimregan/wordnet-lemon-to-w3c;"""wordnet-lemon-to-w3c  ====================    sameAs links for the Lemon conversion of WordNet 2.0 to the W3C conversion"""
Semantic web;https://github.com/yahoo/anthelion;"""# nutch-anth  Anthelion is a Nutch plugin for focused crawling of semantic data.  The project is an open-source project released under the Apache License 2.0.    Note: This project contains the complete Nutch 1.6 distribution. The plugin itself can be found in /src/plugin/parse-anth    Table of Contents  -----------------  * [Nutch-Anthelion Plugin](#nutch-anthelion plugin)    * [Plugin Overview] (#plugin-overview)    * [Usage and Development] (#usage-and-development)    * [Some Results] (#some-results)    * [3rd Party Libraries] (#3rd-party-libraries)  * [Anthelion](#anthelion)   * [References](#references)    Nutch-Anthelion Plugin  ---------  The plugin uses an online learning approach to predict data-rich web pages based on the context of the page as well as using feedback from the extraction of metadata from previously seen pages [1].    ### Plugin Overview    To perform the focused crawling the plugin implements three extensions:    1. **AnthelionScoringFilter** (implements the ScoringFilter interface): wraps around the Anthelion online classifier to classify newly discovered outlinks, as relevant or not. This extension gives score to each outlink, which is then used in the Generate stage, i.e., the URLs for the next fetch cycle are selected based on the score. This extension also pushes feedback to the classifier for the already parsed web pages. The online classifier can be configured and tuned (see [Usage and Development](#usage and development)).    2. **WdcParser** (implements the Parser interface): This extension parses the web page content and tries to extract semantic data. The parser is adaptation of an already existing Nutch parser plugin implemented in [2]. The parser is based on the [any23 library](https://any23.apache.org/) and is able to extract Microdata, Microformats and RDFa annotation from HTML. The extracted triples are stored in the *Content* field.    3. **TripleExtractor** (implements the IndexingFilter interface): This extension stores new fields to the index that can be later used for querying.    An overview of the complete crawling process using the Anthelion plugin is given in the following figure.    <p align=""center"">    <img src=""https://github.com/yahoo/anthelion/blob/master/documentation/architecture.png?raw=true"" alt=""Anthelion Architecture""/>  </p>      ### Usage and Development    As mentioned in the beginning of the document this project contains the complete Nutch 1.6 code, including the plugin. If you download the complete project, there is no need for any changes and settings. If you want to download only the plugin, please download only the nutch-anth.zip from the root of the folder and go to step 2 of the configuration. If you want to contribute to the plugin and/or want to use the sources with another version of Nutch, please follow the following instructions:    1. Download and copy the /src/plugin/parse-anth folder into your Nutch's plugins directory.    2. Enable the plugin in conf/nutch-site.xml by adding *parse-anth* in the *plugin.includes* property.    3. Copy the properties from nutch-anth.xml to conf/nutch-site.xml.    	3.1. Download the baseline.properties file and set the property *anth.scoring.classifier.PropsFilePath* conf/nutch-site.xml to point to the file. This file contains all configurations for the online classifier.    4. In order for ant to compile and deploy the plugin you need to edit the src/plugin/build.xml, by adding the following line in the *deploy* target:  	```xml  	<ant dir=""parse-anth"" target=""deploy""/>  	```  5. Add the following lines in conf/parse-plugins.xml:  	```xml  	<mimeType name=""text/html"">  			<plugin id=""parse-anth"" />  		</mimeType>  	  	        <mimeType name=""application/xhtml+xml"">  			<plugin id=""parse-anth"" />  		</mimeType>  	```  6. Add the following line in the *alias* property in conf/parse-plugins.xml:  	  	```xml  	<alias name=""parse-anth"" extension-id=""com.yahoo.research.parsing.WdcParser"" />  	```  7. Copy the *lib* folder into the root of the Nutch distribution.    8. Run `mvn package` inside the *anthelion* folder. This will create the jar ""Anthelion-1.0.0-jar-with-dependencies.jar"". Copy the jar to src/plugin/parse-anth/lib.    9. Add the following field in conf/schema.xml (also add it to the Solr schema.xml, if you are using Solr):  	```xml  	<field name=""containsSem"" type=""text_general"" stored=""true"" indexed=""true""/>  	```  10. Run ant in the root of your folder.    ### Some Results    In order to evaluate the focused crawler we measure the precision of the crawled pages, i.e., the ratio of the number of crawled web pages that contain semantic data and the total number of crawled web pages.  So far, we have evaluated using three different seeds sample, and several different configurations. An overview is given in the following table.    <table border=0 cellpadding=0 cellspacing=0 width=532 style='border-collapse:   collapse;table-layout:fixed;width:532pt'>   <col width=65 style='width:65pt'>   <col width=77 style='mso-width-source:userset;mso-width-alt:3285;width:77pt'>   <col width=65 span=2 style='mso-width-source:userset;mso-width-alt:2773;   width:65pt'>   <col class=xl65535 width=65 style='mso-width-source:userset;mso-width-alt:   2773;width:65pt'>   <col width=65 span=2 style='mso-width-source:userset;mso-width-alt:2773;   width:65pt'>   <col class=xl65535 width=65 style='mso-width-source:userset;mso-width-alt:   2773;width:65pt'>   <tr height=15 style='height:15.0pt'>    <td rowspan=2 height=30 class=xl65 width=65 style='height:30.0pt;width:65pt'>#seeds</td>    <td rowspan=2 class=xl68 width=77 style='width:77pt'>nutch options</td>    <td colspan=3 class=xl65 width=195 style='border-left:none;width:195pt'>standard    scoring</td>    <td colspan=3 class=xl65 width=195 style='border-left:none;width:195pt'>anthelion    scoring</td>   </tr>   <tr height=15 style='height:15.0pt'>    <td height=15 class=xl66 style='height:15.0pt;border-top:none;border-left:    none'>#total pages</td>    <td class=xl66 style='border-top:none;border-left:none'>#sem pages</td>    <td class=xl67 style='border-top:none;border-left:none'>precision</td>    <td class=xl66 style='border-top:none;border-left:none'>#total pages</td>    <td class=xl66 style='border-top:none;border-left:none'>#sem pages</td>    <td class=xl67 style='border-top:none;border-left:none'>precision</td>   </tr>   <tr height=15 style='height:15.0pt'>    <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>2</td>    <td class=xl69 style='border-top:none;border-left:none'>-depth 3 -topN 1<span    style='display:none'>5</span></td>    <td class=xl66 align=right style='border-top:none;border-left:none'>17</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>2</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.12</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>22</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>7</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.32</td>   </tr>   <tr height=15 style='height:15.0pt'>    <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>10</td>    <td class=xl69 style='border-top:none;border-left:none'>-depth 8 -topN 1<span    style='display:none'>5</span></td>    <td class=xl66 align=right style='border-top:none;border-left:none'>99</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>2</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.02</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>49</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>11</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.22</td>   </tr>   <tr height=15 style='height:15.0pt'>    <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>1000</td>    <td class=xl69 style='border-top:none;border-left:none'>-depth 4 -topN 1<span    style='display:none'>000</span></td>    <td class=xl66 align=right style='border-top:none;border-left:none'>3200</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>212</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.07</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>2910</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>1469</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.50</td>   </tr>   <tr height=15 style='height:15.0pt'>    <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>1000</td>    <td class=xl70 style='border-top:none;border-left:none'>    <meta charset=utf-8>    <span>-depth 5 -topN 2<span style='display:none'>000</span></span></td>    <td class=xl66 align=right style='border-top:none;border-left:none'>8240</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>511</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.06</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>    <meta charset=utf-8>    <span>9781</span></td>    <td class=xl66 align=right style='border-top:none;border-left:none'>7587</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.78</td>   </tr>  </table>    The pairwise comparison is given in the following chart:  <p align=""center"">    <img src=""https://github.com/yahoo/anthelion/blob/master/documentation/results_chart.png?raw=true"" alt=""Architecture""/>  </p>    ### 3rd Party Libraries  The Anthelion plugin uses several 3rd party open source libraries and tools.  Here we summarize the tools used, their purpose, and the licenses under which they're released.    1. This project includes the sources of Apache Nutch 1.6 (Apache License 2.0 - http://www.apache.org/licenses/LICENSE-2.0)  	* http://nutch.apache.org/    2. Apache Any23 1.2 (Apache License 2.0 - http://www.apache.org/licenses/LICENSE-2.0)  	* Used for extraction of semantic annotation from HTML.  	* https://any23.apache.org/  	* More information about the 3rd party dependencies used in the any23 library can be found [here](https://any23.apache.org/)      3. The classes com.yahoo.research.parsing.WdcParser and com.yahoo.research.parsing.FilterableTripleHandler are modified versions of existing Nutch plugins (Apache License 2.0 - http://www.apache.org/licenses/LICENSE-2.0)  	* Used for parsing the crawled web pages  	* Hellman et al. [2]; https://www.assembla.com/spaces/commondata/subversion/source/HEAD/extractorNutch    4. For the libraries and tools used in Anthelion, please check the Anthelion [README file] (https://github.com/yahoo/anthelion/blob/master/anthelion/README.md).     Anthelion  ---------  For more details about the Anthelion project please check the Anthelion [README file] (https://github.com/yahoo/anthelion/blob/master/anthelion/README.md).    References  ----------  [1]. Meusel, Robert, Peter Mika, and Roi Blanco. ""Focused Crawling for Structured Data."" Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, 2014.    [2]. Hellmann, Sebastian, et al. ""Knowledge Base Creation, Enrichment and Repair."" Linked Open Data--Creating Knowledge Out of Interlinked Data. Springer International Publishing, 2014. 45-69.  	      ###Troubleshooting  (TODO) """
Semantic web;https://github.com/Quetzal-RDF/quetzal;"""Quetzal (*Que*ry Tran*z*l*a*tion *L*ibraries)  =======    SPARQL to SQL translation engine for multiple backends, such as DB2, PostgreSQL and Apache Spark.       # Philosophy  The goal of Quetzal is to provide researchers with a framework to experiment with various techniques to store and query graph data efficiently.  To this end, we provide 3 modular components that:  * Store data:  In the current implementation, data is stored in using a schema similar to the one described in [SIGMOD 2013 paper](http://dl.acm.org/citation.cfm?id=2463718).  The schema lays out all outgoing (or incoming) labeled edges of a given vertex *based on the analysis of data characteristics* to optimize storage for a given dataset.  The goal in the layout is to store the data for a given vertex on a single row in table to optimize for STAR queries which are very common in SPARQL.    * Compile SPARQL to SQL:  In the current implementation, given a set of statistics about the dataset's characteristics, the compiler can compile SPARQL 1.1 queries into SQL.  The compiler will optimize the order in which it executes the SPARQL query based on statistics of the dataset.  * Support for SQL on multiple backends:  In the current implementation, we support DB2, PostgreSQL, and Apache Spark.  The first two are useful for workloads that require characteristics normally supported by relational backends (e.g., transactional support), the third targets analytic workloads that might mix graph analytic workloads with declarative query workloads.     # Overview of Components  * Data Layout:  The current implementation uses a *row based layout of graph data*, such that each vertex's incoming edges or outgoing edges are laid out as much as possible on the same row.  For a detailed set of experiments that examine when this layout is advantageous, see [SIGMOD 2013 paper](http://dl.acm.org/citation.cfm?id=2463718).  Outgoing edges are stored in a table called DPH (direct primary hashtable), and incoming edges are stored in a table called RPH (reverse primary hashtable).  Because RDF can have many thousand properties, dedicating a column per property is not an option (in fact, some datasets will exhaust most database systems limits on the number of columns).  RDF data is sparse though, so each vertex tends to have a small subset of the total number of properties.  The current implementation performs an analysis of which properties co-occur with which others, and uses graph coloring to build a hash function that maps properties to columns.  Properties that co-occur together are typically not assigned to the same row.  If they do get assigned to the same row because a single vertex has several hundred edges to all sorts of properties, then collisions are possible and the schema records this fact, and the SQL is adjusted appropriately.  Note that for multi-valued properties, DPH and RPH record only the existence of the property for a given vertex, actual values require a join with a DS (direct secondary) and RS (reverse secondary) table, respectively.  * SPARQL-SQL compiler:  In the current implementation, this compilation job is done by a class called ``com.ibm.research.rdf.store.sparql11.planner.Planner``, in a method called ``public Plan plan(Query q, Store store, SPARQLOptimizerStatistics stats)``.  The goal of the planner is to compile the SPARQL query into SQL, *re-ordering* the query in order to start with the most selective triples (triples with the least cost), joining it with the second most selective triple based on what becomes available when one evaluates the first triple, and so on.  In doing so, the planner must respect the semantics of SPARQL (e.g., not join two variables that are named the same but are on two separate brances of a UNION).  The Planner employs a greedy algorithm to evaluate what available nodes exist for planning, and which one should be planned first.  AND nodes get collapsed into a single ""region"" of QueryTriple nodes because any triples within an AND node can be targeted first.  Each triple node within an AND can evaluate its cost based on what variables are available, and each node has a notion of what variables it can produce bindings to based on the access method used (e.g., if the access method is DPH, it typically would produce an object variable binding; conversely if the access method is RPH, it would typically produce a subject variable binding).  The cost of producing these bindings is estimated based on the average number of outgoing (DPH) or incoming (RPH) edges in most cases, unless the triple happens to have a popular node which appears in a *top K* set.  Other complex nodes such as EXISTs, UNION or OPTIONAL nodes evaluate their costs recursively by planning for their subtrees. (See https://github.com/Quetzal-RDF/quetzal/tree/master/doc/QuetzalPlanner.pdf)  The planner then chooses the cheapest node to schedule first.  Once it has chosen a node, the set of available variables has changed, so a new of cost computations are performed to find the next step.  The planner proceeds in this manner till there are no more available nodes to plan.  The output of the planner is ``com.ibm.research.rdf.store.sparql11.planner.Plan``, which is basically a binary plan tree that is composed of AND plan nodes, LEFT JOIN nodes, etc.  This serves as the input for the next step.  * SQL generator:  In the current implementation, the plan serves as input to a number of SQL templates, which get created for every type of node in the plan tree.  The ``com.ibm.research.rdf.store.sparql11.sqltemplate`` package contains the templates, which generate SQL modularly per node in the plan tree using common table expressions (CTEs).  The template code is general purpose and keeps track of things such as the specific CTE to node mappings, what external variables need to be projected, which variables should be joined together etc.  The actual job of generating SQL for different backends is accomplished using specialized String Templates from the [String Template](http://www.stringtemplate.org) library.  Example files are ``com.ibm.research.rdf.store.sparql11.sqltemplate.common.stg`` which has the templates that are common to all backends.    For more information on how to get started, click on the Wiki to this repository    # Install and build issues  If you are building from source, get the following:  ``git clone https://github.com/themadcreator/rabinfingerprint`` and build using maven.  * Also install the latest JDBC driver from: https://cloud.google.com/bigquery/partners/simba-drivers/#current_jdbc_driver_releases_1151005 and drop it into lib to compile.  Then clone this repository and build using maven.    # Storage of graph data on cloud SQL backing stores such as Spanner and BigQuery  Since the time we worked on Quetzal, a number of cloud databases have emerged that support the complex SQL queries needed to access graph data. One question that we started to ask recently is whether storage of graph data is better suited for a column oriented, nested type data layout such as BigQuery, or whether a row store such as Spanner is better suited for storage of graph data.  There are tradeoffs to each, and this is by no means an exhaustive comparison of the two different approaches, but we performed some very initial experiments on the following layout on BigQuery versus Spanner for a simple graph query which is not just a 1 hop neighborhood of a node, and we note the rather interesting results here.    * The data and the query:  The graph data are generated from the Lehigh University Benchmark (LUBM) [LUBM](http://swat.cse.lehigh.edu/projects/lubm/) which has a set of students taking courses at a university, and they have advisors.  The data is sparse, and many entities have 1->many edges.  The query is query 9 from that benchmark, which is to find students taking courses taught by their advisors.  Students in that graph take many courses, and have a single advisor.  Each advisor teaches many courses.  And the query asks to find the 'triangle' between them, which is to specify which students take a class that is taught by their advisor.  The graph has 1 billion triples in it, which translates to ~174M rows in an entity oriented store, assuming that 1->many edges such as taking a course, or teaching a course are represented in a single row using arrays or nested data structures.  The dataset is about 79G when written as a JSON file.  * The layout:  Both Spanner and BigQuery provide support for nested data.  Following the entity oriented view of data in Quetzal, the data model is that of a 'subject' or entity, with various edge types mapped to distinct columns. Because BigQuery is ideal for storing columnar, sparse data, we used a 1-1 mapping of each edge type to columns. Furthermore, we did not actually need a reverse mapping since BigQuery has no indexes (every query is a scan).  Instead, it exploits the fact that only specific columns will ever be invoked in a given query. We maintained the same schema for Spanner just to ensure we had an apples to apples comparison.  The layout is therefore like just the DPH table in the [SIGMOD 2013 paper](http://dl.acm.org/citation.cfm?id=2463718), with the one change that we did not separate out the many valued edges into a separate table.  We used Spanner and BigQuery's support for array types to store multi valued predicates in the same column.  Note that Spanner also supports interleaving rows between the two tables which we could have used to support multi valued predicates but we did not do so in this first experiment.  All the code is checked into the spanner-loader and bigquery-loader directories.    * Here are the mappings of edge types to column names in LUBM:    `<http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#undergraduateDegreeFrom>=col_8  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#emailAddress>=col_6  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#mastersDegreeFrom>=col_5  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#memberOf>=col_12  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#worksFor>=col_3  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#teachingAssistantOf>=col_15  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#subOrganizationOf>=col_16  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#researchInterest>=col_9  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#teacherOf>=col_7  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#name>=col_2  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#headOf>=col_11  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#telephone>=col_4  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#publicationAuthor>=col_0  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#takesCourse>=col_14  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#doctoralDegreeFrom>=col_10  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#advisor>=col_13  <http\://www.w3.org/1999/02/22-rdf-syntax-ns\#type>=col_1`  * Here is the schema for all the edges in BigQuery for LUBM:    `{""schema"":{""fields"":[{""name"":""subject"",""type"":""string""},{""name"":""col_8"",""type"":""string""},{""name"":""col_6"",""type"":""string""},{""name"":""col_5"",""type"":""string""},{""name"":""col_12"",""type"":""string""},{""name"":""col_3"",""type"":""string""},{""name"":""col_15"",""type"":""string""},{""name"":""col_9"",""type"":""string""},{""name"":""col_16"",""type"":""string""},{""name"":""col_7"",""type"":""string"",""mode"":""repeated""},{""name"":""col_2"",""type"":""string""},{""name"":""col_11"",""type"":""string""},{""name"":""col_4"",""type"":""string""},{""name"":""col_0"",""type"":""string"",""mode"":""repeated""},{""name"":""col_14"",""type"":""string"",""mode"":""repeated""},{""name"":""col_10"",""type"":""string""},{""name"":""col_13"",""type"":""string""},{""name"":""col_1"",""type"":""string"",""mode"":""repeated""}]}}`    * Here is the corresponding schema for Spanner, written as Java code:    ` static void createDatabase(DatabaseAdminClient dbAdminClient, DatabaseId id) {      Operation<Database, CreateDatabaseMetadata> op = dbAdminClient.createDatabase(          id.getInstanceId().getInstance(), id.getDatabase(),          Arrays.asList(""CREATE TABLE DPH (\n"" + ""  subject STRING(MAX) NOT NULL,\n""              + ""  col_0  ARRAY<STRING(MAX)>, \n"" + ""  col_1  ARRAY<STRING(MAX)>, \n""              + ""  col_2  STRING(MAX), \n"" + ""  col_3  STRING(MAX), \n"" + ""  col_4  STRING(MAX), \n""              + ""  col_5  STRING(MAX), \n"" + ""  col_6  STRING(MAX), \n""              + ""  col_7  ARRAY<STRING(MAX)>, \n"" + ""  col_8  STRING(MAX), \n""              + ""  col_9  STRING(MAX), \n"" + ""  col_10  STRING(MAX), \n"" + ""  col_11  STRING(MAX), \n""              + ""  col_12  STRING(MAX), \n"" + ""  col_13  STRING(MAX), \n""              + ""  col_14  ARRAY<STRING(MAX)>, \n"" + ""  col_15  STRING(MAX), \n""              + ""  col_16  STRING(MAX)) \n"" + "" PRIMARY KEY (subject)""));      Database db = op.waitFor().getResult();      System.out.println(""Created database ["" + db.getId() + ""]"");    }`   * And now for the queries.  BigQuery supports common table expressions which were crucial in providing a nice abstraction to construct complex graph queries.  Here is the query for BigQuery:    `with        t1 as (select subject as student, col_13 as advisor from lubm.DPH where col_13 is not null),       t2 as (select subject as student, col_14 as course from lubm.DPH where col_14 is not null),       t3 as (select subject as teacher, col_7 as course from lubm.DPH where col_7 is not null),       t4 as (select teacher, course from t3 t, t.course course),       t5 as (select student, course from t2 t, t.course course)       select t5.student, t4.teacher, t4.course from t4, t5, t1 where t4.course = t5.course and t4.teacher = t1.advisor and t5.student = t1.student`  * Here is the corresponding query for Spanner because it has no support for Common Table Expressions (CTEs):    `select dph1.subject as student, dph1.col_13 as advisor, course, dph2.subject as teacher, c from DPH as dph1, DPH as dph2 cross join unnest(dph1.col_14) as course cross join unnest(dph2.col_7) as c where dph1.col_13 is not null and dph1.col_14 is not null and dph2.col_7 is not null and course = c and dph1.col_13 = dph2.subject`  * And the results.  BigQuery performed this query in 67.6s, and processed about 26.1 GB.  Spanner timed out after 15 minutes.  It is possible that Spanner does not handle un-nesting of arrays as well as BigQuery, but this is an interesting datapoint, and suggests that Spanner may need a different style schema for storing 1->many edges.  The performance of BigQuery is rather impressive, for comparison with some of the databases Quetzal supports on a 100M edge dataset [see here](https://github.com/Quetzal-RDF/quetzal/wiki/Benchmarks).   * Of course, this is a hand crafted experiment for now - but it seems to suggest that BigQuery has at least one key advantage over Spanner for querying graph data, which is its support for CTEs. """
Semantic web;https://github.com/AKSW/cubeviz.ontowiki;"""# CubeViz - THE RDF DATACUBE BROWSER     CubeViz is an [OntoWiki](http://aksw.org/Projects/OntoWiki.html) component providing faceted browsing in statistical   data in the Linked Data Web. It was set up and adopted to be part of the   [Data Portal](https://ec.europa.eu/digital-agenda/en/scoreboard) of the European Union.   CubeViz utilizing the  [RDF DataCube vocabulary](http://www.w3.org/TR/vocab-data-cube/)   which is the state-of-the-art in representing statistical data in [Resource Description Framework (RDF)](http://www.w3.org/RDF/).   This vocabulary is compatible with [SDMX](http://en.wikipedia.org/wiki/SDMX)   ([User Guide](http://sdmx.org/wp-content/uploads/2012/11/SDMX_2-1_User_Guide_draft_0-1.pdf)) and increasingly being adopted.   Based on the vocabulary and the encoded Data Cube, CubeViz is generating a facetted browsing widget that   can be used to filter interactively observations to be visualized in charts.   Based on the selected structure, CubeViz offer beneficiary chart types and options which can be selected by users.    Do you want to read further information about the project and its background, please have a look into   [**about**](https://github.com/AKSW/cubeviz.ontowiki/wiki/About-the-project) page.    ![](https://raw.github.com/wiki/AKSW/cubeviz.ontowiki/images/v0.5_IndexActionScreenshot.png)    ![](https://raw.github.com/wiki/AKSW/cubeviz.ontowiki/images/v0.6_visualization.png)    ## Getting started    You will find various information in our [Wiki](https://github.com/AKSW/cubeviz.ontowiki/wiki/Home).  For new users, please visit Page [installation](https://github.com/AKSW/cubeviz.ontowiki/wiki/Installation-and-setup-main)   to get an introduction about installation and setup of CubeViz.    Further information about the repository structure or other internals can be also found in the Wiki.    ### Docker container available    We providing a Docker container for everybody, who don't want to bother about getting OntoWiki running or struggles with Virtuoso. That container only needs a Docker and can be used with `docker pull` + `docker run`. How easy is that?!     Basically it ships with a fully fledged OntoWiki, pre-filled Virtuoso store and up and running CubeViz. After you started the container, you can use your browser and directly use CubeViz.    To pull the container just run:     `docker pull aksw/dld-present-cubeviz`    To run it, please use:    `docker run -d -p 8080:80 -p 8890:8890 aksw/dld-present-cubeviz`    For further information, please look in following the project page.    **Project page:** [Dockerizing/CubeViz](https://github.com/Dockerizing/CubeViz)    ## CubeViz compared to other tools    *(Last updated 2015-11-30)*    We created a comparison of CubeViz and other tools, which are supporting DataCube vocabulary. It is based on the support of elements which are mentioned in the vocabulary. The purpose of that overview is to provide users, who are interested in using CubeViz, further information of its DataCube vocabulary support in comparison to other tools.    *If you encounter erros or wanna mention something, please make a pull request, create an issue or send us an email.*    `(✓)` - Similar like CubeViz but not equivalent    | Supported Features                | CubeViz | OpenCube | LDCX | LSD Analysis    |  |-----------------------------------|:-------:|:--------:|:----:|:---------------:|  | **Explore RDF Data**              |         |          |      |                 |  | Select parts of the Dataset       |    ✓    |    ✓     |  ✓   |       (✓)       |  | Select Units and Measurements     |    ✓    |    ✓     |  ✓   |       (✓)       |  | Multiple Chart Visualization      |    ✓    |   (✓)    |  \-  |       \-        |  | Configure Dimensions              |    ✓    |    \-    |  ✓   |       \-        |  |                                   |         |          |      |                 |  | Collaborative Exploration/Edit    |    ✓    |    \-    |  \-  |   ✓ (Explore)   |  |                                   |         |          |      |                 |  | **Compare Datasets**              |         |          |      |       (✓)       |  | Compare Meta Information          |    ✓    |    \-    |  \-  |       \-        |  | Observation Values Normalization  |    ✓    |    \-    |  \-  |       \-        |  | Set Dimension Elements            |    ✓    |    \-    |  \-  |       \-        |  | Show Cluster                      |    ✓    |    \-    |  \-  |       \-        |  |                                   |         |          |      |                 |  | **Data Download**                 |         |          |      |                 |  | Download as CSV                   |    ✓    |    ✓     |  \-  |        ✓        |  | Download as Turtle                |    ✓    |    \-    |  \-  |       \-        |  |                                   |         |          |      |                 |  | Hierarchy Slices                  |   \-    |    \-    |  \-  |       \-        |  | OLAP Operations (Sum, Avg, Pivot) |   \-    |    ✓     |  \-  |       \-        |  | Mobile UI                         |   \-    |    \-    |  \-  |       \-        |  | Analysis Task (R Script)          |   \-    |    ✓     |  \-  | ✓ (Server-Side) |  | Geospatial Data Visualization     |   \-    |    ✓     |  \-  |       \-        |  | Visualizations with dimensions >2 |   \-    |    \-    |  \-  |       \-        |    #### Links to the mentioned tools    -	[OpenCube](http://opencube-toolkit.eu)    -	[LDCX](http://km.aifb.kit.edu/projects/ldcx/)    -	[LSD Analysis](http://stats.270a.info/analysis/worldbank:SP.DYN.IMRT.IN/transparency:CPI2011/year:2011.html)    ## Missing support of the Data Cube vocabulary    The following list contains all elements of the DataCube vocabulary which are not supported by CubeViz (yet):        *	**qb:ObservationGroup**: Indicates a group of observations. The domain of this property is left open so that a group may be attached to different resources and need not be restricted to a single DataSet.      *	**qb:CodedProperty**: Superclass of all coded component properties.      *	**qb:HierarchicalCodeList**: Represents a generalized hierarchy of concepts which can be used for coding. The hierarchy is defined by one or more roots together with a property which relates concepts in the hierarchy to their child concept . The same concepts may be members of multiple hierarchies provided that different qb:parentChildProperty values are used for each hierarchy.    *	**qb:hierarchyRoot**: Specifies a root of the hierarchy. A hierarchy may have multiple roots but must have at least one.    *	**qb:parentChildProperty**: Specifies a property which relates a parent concept in the hierarchy to a child concept. Note that a child may have more than one parent.    *	**qb:componentAttachment**: Indicates the level at which the component property should be attached, this might be an qb:DataSet, qb:Slice or qb:Observation, or a qb:MeasureProperty.    *	**qb:componentRequired:** Indicates whether a component property is required (true) or optional (false) in the context of a DSD. Only applicable to components corresponding to an attribute. Defaults to false (optional).      *	**qb:order**: Indicates a priority order for the components of sets with this structure, used to guide presentations - lower order numbers come before higher numbers, un-numbered components come last.    *	**qb:concept**: Gives the concept which is being measured or indicated by a ComponentProperty.    *	SKOS or SDMX related entities.    ## License    CubeViz is licensed under the terms of GNU General Public License 2 and it uses foreign libraries.   For further details have a look in [here](https://github.com/AKSW/cubeviz.ontowiki/blob/master/LICENSE.md). """
Semantic web;https://github.com/sage-org/sage-engine;"""# Sage: a SPARQL query engine for public Linked Data providers  [![Build Status](https://travis-ci.com/sage-org/sage-engine.svg?branch=master)](https://travis-ci.com/sage-org/sage-engine) [![PyPI version](https://badge.fury.io/py/sage-engine.svg)](https://badge.fury.io/py/sage-engine) [![Docs](https://img.shields.io/badge/docs-passing-brightgreen)](https://sage-org.github.io/sage-engine/)    [Read the online documentation](https://sage-org.github.io/sage-engine/)    SaGe is a SPARQL query engine for public Linked Data providers that implements *Web preemption*. The SPARQL engine includes a smart Sage client  and a Sage SPARQL query server hosting RDF datasets using [HDT](http://www.rdfhdt.org/), [postgres](https://www.postgresql.org/), [sqlite](https://www.sqlite.org/), or [hbase](https://hbase.apache.org/)  This repository contains the **Python implementation of the SaGe SPARQL query server**.    SPARQL queries are suspended by the web server after a fixed quantum of time and resumed upon client request. Using Web preemption, Sage ensures stable response times for query execution and completeness of results under high load.    The complete approach and experimental results are available in a Research paper accepted at The Web Conference 2019, [available here](https://hal.archives-ouvertes.fr/hal-02017155/document). *Thomas Minier, Hala Skaf-Molli and Pascal Molli. ""SaGe: Web Preemption for Public SPARQL Query services"" in Proceedings of the 2019 World Wide Web Conference (WWW'19), San Francisco, USA, May 13-17, 2019*.    We appreciate your feedback/comments/questions to be sent to our [mailing list](mailto:sage@univ-nantes.fr) or [our issue tracker on github](https://github.com/sage-org/sage-engine/issues).    # Table of contents    * [Installation](#installation)  * [Getting started](#getting-started)    * [Server configuration](#server-configuration)    * [PostgreSQL configuration](#postgresql-configuration)    * [Data ingestion](#data-ingestion)    * [Starting the server](#starting-the-server)  * [Sage Docker image](#sage-docker-image)  * [Command line utilities](#command-line-utilities)  * [Documentation](#documentation)    # Installation    Installation in a [virtualenv](https://virtualenv.pypa.io/en/stable/) is **strongly advised!**    Requirements:  * Python 3.7 (*or higher*)  * [pip](https://pip.pypa.io/en/stable/)  * **gcc/clang** with **c++11 support**  * **Python Development headers**  > You should have the `Python.h` header available on your system.     > For example, for Python 3.6, install the `python3.6-dev` package on Debian/Ubuntu systems.    ## Installation using pip    The core engine of the SaGe SPARQL query server with [HDT](http://www.rdfhdt.org/) as a backend can be installed as follows:  ```bash  pip install sage-engine[hdt,postgres,hbase]  ```  The SaGe query engine uses various **backends** to load RDF datasets.  The various backends available are installed as extras dependencies. The above command install both the HDT, the PostgreSQL and the HBase backends.    ## Manual Installation using poetry    The SaGe SPARQL query server can also be manually installed using the [poetry](https://github.com/sdispater/poetry) dependency manager.  ```bash  git clone https://github.com/sage-org/sage-engine  cd sage-engine  poetry install --extras ""hdt postgres hbase""  ```  As with pip, the various SaGe backends are installed as extras dependencies, using the  `--extras` flag.    # Getting started    ## Server configuration    A SaGe server is configured using a configuration file in [YAML syntax](http://yaml.org/).  You will find below a minimal working example of such a configuration file.  Full examples are available [in the `config_examples/` directory](https://github.com/sage-org/sage-engine/blob/master/config_examples/example.yaml)    ```yaml  name: SaGe Test server  maintainer: Chuck Norris  quota: 75  max_results: 2000  graphs:  -    name: dbpedia    uri: http://example.org/dbpedia    description: DBPedia    backend: hdt-file    file: datasets/dbpedia.2016.hdt  ```    The `quota` and `max_results` fields are used to set the maximum time quantum and the maximum number of results  allowed per request, respectively.    Each entry in the `graphs` field declare a RDF dataset with a name, description, backend and options specific to this backend.  Different backends are available:  - the `hdt-file` backend allows a SaGe server to load RDF datasets from [HDT files](http://www.rdfhdt.org/). SaGe uses [pyHDT](https://github.com/Callidon/pyHDT) to load and query HDT files.  - the `postgres` backend allows a SaGe server to create, query and update RDF datasets stored in [PostgreSQL](https://www.postgresql.org/). Each dataset is stored in a single table composed of 3 columns; S (subject), P (predicate) and O (object). Tables are created with B-Tree indexes on SPO, POS and OSP. SaGe uses [psycopg2](https://pypi.org/project/psycopg2/) to interact with PostgreSQL.  - the `postgres-catalog` backend uses a different schema than `postgres` to store datasets. Triples terms are mapped to unique identifiers and a dictionary table that is common to all datasets is used to map RDF terms with their identifiers. This schema allows to reduce the space required to store datasets.  - the `sqlite` backend allows a SaGe server to create, query and update RDF datasets stored in [SQLite](https://docs.python.org/3/library/sqlite3.html). Datasets are stored using the same schema as the `postgres` backend.  - the `sqlite-catalog` is another backend for SQLite that uses a dictionary based schema as the `postgres-catalog` backend.  - the `hbase` backend allows a SaGe server to create, query and update RDF datasets stored in [HBase](https://hbase.apache.org/). To have a sorted access on dataset triples, triples are inserted three times in three different tables using SPO, POS and OSP as triples keys. SaGe uses [happybase](https://happybase.readthedocs.io/en/latest/) to interact with HBase.    ## PostgreSQL configuration    This section is optional and can be skipped if you don't use one of the PostgreSQL backends.    To ensure stable performance when using PostgreSQL with SaGe, PostgreSQL needs to be configured. Open the file `postgresql.conf` in the PostgreSQL main directory and apply the following changes in the *Planner Method Configuration* section:  - Uncomment all enable_XYZ options  - Set *enable_indexscan*, *enable_indexonlyscan* and *enable_nestloop* to **on**  - Set all the other enable_XYZ options to **off**    These changes force the PostgreSQL query optimizer to generate the desired query plan for the SaGe resume queries.    ## Data ingestion    Different executables are available to load a RDF file depending on the backend you want to use.    To load a dataset from a HDT file, just declare a new dataset in your configuration file using the `hdt-file` backend.    To load a N-Triples file using one of the `postgres`, `postgres-catalog`, `hbase`, `sqlite` and `sqlite-catalog` backends, first declare a new dataset in your configuration file. For example, to load the file `my_dataset.nt` using the `sqlite` backend, we start by declaring a new dataset named `my_dataset` in our configuration file `my_config.yaml`.    ```yaml  quota: 75  max_results: 10000  graphs:  -    name: my_dataset    uri: http://example.org/my_dataset    backend: sqlite    database: sage-sqlite.db  ```    For each backend, an example that illustrate how to declare a new dataset is available in the [`config_examples/`](https://github.com/sage-org/sage-engine/blob/master/config_examples/example.yaml) directory.    To load a file into a dataset declared using one of the `SQLite` backends, use the following commands:    ```bash  # Create the required SQLite tables to store the dataset  sage-sqlite-init --no-index my_config.yaml my_dataset  # Insert the RDF triples in SQLite  sage-sqlite-put my_dataset.nt my_config.yaml my_dataset  # Create the SPO, OSP and POS indexes  sage-sqlite-index my_config.yaml my_dataset_name  ```    To load a file into a dataset declared using one of the `PostgreSQL` backends, use the following commands:    ```bash  # Create the required PostgreSQL tables to store the dataset  sage-postgres-init --no-index my_config.yaml my_dataset  # Insert the RDF triples in PostgreSQL  sage-postgres-put my_dataset.nt my_config.yaml my_dataset  # Create the SPO, OSP and POS indexes  sage-postgres-index my_config.yaml my_dataset_name  ```    To load a file into a dataset declared using the `hbase` backend, use the following commands:    ```bash  # Create the required HBase tables to store the dataset  sage-hbase-init my_config.yaml my_dataset  # Insert the RDF triples in HBase  sage-hbase-put my_dataset.nt my_config.yaml my_dataset  ```    ## Starting the server    The `sage` executable, installed alongside the SaGe server, allows to easily start a SaGe server from a configuration file using [Uvicorn](https://www.uvicorn.org/), a Python ASGI HTTP Server.    ```bash  # launch Sage server with 4 workers on port 8000  sage my_config.yaml -w 4 -p 8000  ```    The full usage of the `sage` executable is detailed below:  ```  Usage: sage [OPTIONS] CONFIG      Launch the Sage server using the CONFIG configuration file    Options:    -p, --port INTEGER              The port to bind  [default: 8000]    -w, --workers INTEGER           The number of server workers  [default: 4]    --log-level [debug|info|warning|error]                                    The granularity of log outputs  [default:                                    info]    --help                          Show this message and exit.  ```    Once started, you can interact with the SaGe server on http://localhost:8000/docs    # SaGe Docker image    The Sage server is also available through a [Docker image](https://hub.docker.com/r/callidon/sage/).  In order to use it, do not forget to [mount in the container](https://docs.docker.com/storage/volumes/) the directory that contains you configuration file and your datasets.    ```bash  docker pull callidon/sage  docker run -v path/to/config-file:/opt/data/ -p 8000:8000 callidon/sage sage /opt/data/config.yaml -w 4 -p 8000  ```    # Documentation    To generate the documentation, navigate in the `docs` directory and generate the documentation    ```bash  cd docs/  make html  open build/html/index.html  ```    Copyright 2017-2019 - [GDD Team](https://sites.google.com/site/gddlina/), [LS2N](https://www.ls2n.fr/?lang=en), [University of Nantes](http://www.univ-nantes.fr/) """
Semantic web;https://github.com/mmlab/TurtleValidator;"""TurtleValidator  ===========  RDF NTriples/Turtle validator using Ruben Verborgh's [N3 NodeJS library](https://github.com/RubenVerborgh/N3.js). Validate Turtle and Ntriples documents on syntax and XSD datatype errors through command line.    © 2014, 2015 - IDLab - Ghent University - imec  Source code: https://github.com/MMLab/TurtleValidator    Install:        npm install -g turtle-validator    Examples:        $ ttl <path-to-file ...>      $ curl http://data.linkeddatafragments.org/dbpedia -H ""accept: text/turtle"" | ttl      $ ttl http://triples.demo.thedatatank.com/demo.ttl    ## Or install the browser client    ```bash  # Equivalent to: npm build  npm install  browserify lib/validator.js -o public/js/ttl.js  ```    Then use it in your browser using the index.html in the public folder.  You can run this locally as follows.    ```bash  # Equivalent to: npm start  npm install  browserify lib/validator.js -o public/js/ttl.js  ws  ``` """
Semantic web;https://github.com/Claudenw/jdbc4sparql;"""Complete functionality has not been tested.  However, most code is functional and testing should   be completed in the near future.    # JDBC 4 SPARQL    JDBC 4 SPARQL is a JDBC Driver that uses a SPARQL endpoint (or Jena Model) as the data store.    ## Licenses    JDBC 4 SPARQL is licensed under Apache License V2.  However, there is one component that is Licensed under the GNU LGPL V3.0  this means that if you want to use this product in an environment that is not permitted under GNU LGPL V3.0 you will need to   find another implementation of the SparqlParser interface.  How to do this is covered in the Extensions Points section of this   document.  	  ## Maven    Group ID: org.xenei    Artifact ID: jdbc4sparql    ## Usage    ### URL  The URL for the J4SDriver is       jdbc:j4s:[?arg=value>[&arg2=value2[&arg3=value3...]]]:url    For current runtime configuration options and defaults execute the J4SDriver class as an application.    #### Valid arguments  * catalog The name of the catalogue to restrict queries for.  * type The type of the input.  * builder The builder to build the SQL Schema with.  Either the name of a registered builder or a class name.  See ""Registered Schema Builders"" below.  * parser The parser class.  A fully qualified SparqlParser implementation.  Defaults to org.xenei.jdbc4sparql.sparql.parser.jsqlparser.SparqlParserImpl    #### Valid Types:  * (Default) config - URL is a J4S configuration file (NOT YET IMPLEMENTED)  * sparql - URL is a sparql endpoint  * RDFXML or RDF/XML - URL is a RDF/XML formatted RDF file  * NTRIPLES or N-Triples - URL is a N-Triples formatted RDF file  * N3 or N3 - URL is a N3 formatted RDF file  * TURTLE or Turtle - URL is a Turtle formatted RDF file  * RDFJSON or RDF/JSON - URL is a RDF/JSON formatted RDF file  * NQUADS or N-Quads - URL is a N-Quads formatted RDF file  * TRIG or TriG - URL is a TriG formatted RDF file    #### Registered Schema Builders  (Default) Simple_Builder: A simple schema builder that builds tables based on RDFS Class names    #### Notes ####    Currently the catalogue is built at runtime by the builder, however future improvements should include a mechanism to store an entire configuration of multiple catalogues.    the Catalogue contains the URL for the SPARQL endpoint or RDF file so it will be possible to configure the driver to access multiple endpoints.    ## A Conflict Of Nomenclatures    SQL and SPARQL use nomenclatures that conflict with each other.  For the purposes of this documentation, unless otherwise specified,  SQL nomenclatures will be used.    ## How This Works    ### Mapping The Graph  In general an RDF Graph (and thus a SPARQL endpoint) is an instance of a navigational database, SQL is generally designed to work  against a relational model so there this application attempts to map a navigational database onto relational tables and keys.  It   does this through the use of a ""SchemaBuilder"" implementation to map the RDF Graph to tables and columns.  The default implementation looks for RDF resources that have a rdf:type of rdfs:Class.  It considers   each of these resources as tables and uses the resource name as the table name, we will call these ""Table Resources"".  It then scans the   RDF store looking for distinct properties of objects of the ""Table Resource"" types.  The properties are the columns in the table.  All tables generated by the ""SchemaBuilder"" are   stored in a SQL Schema, so multiple schemas may be created from a single RDF Schema, Vocabulary or datastore.    It is possible to override the default implementation by implementing a new SchemaBuilder and adding it to the /META-INF/services/org.xenei.jdbc4sparql.sparql.builders.SchemaBuilder file.  The first entry in the file is considered the default builder.    ### Generating A Query  A SQL query is parsed by a ""SparqlParser"" implementation the and converted into a SPARQL query.    The default implementation of SparqlParser is LGPL v3 and uses an LGPL v2.1 based package.  Other implementations may be used.  To register a different implementation place it as the first entry in the /META-INF/services/org.xenei.jdbc4sparql.sparql.parser.SparqlParser file.  The returned query is then executed against the SPARQL endpoint or the   Jena Model.  The SPARQL ResultSet is retrieved and the QuerySolutions stored in a local list.  A SQL ResultSet is   created against that list and returned.     """
Semantic web;https://github.com/AKSW/SparqlAnalytics;"""## Welcome to the SPARQL Analytics project.    This project aims to develop a java based middleware/proxy framework for live analysis of SPARQL queries. Publish live SPARQL endpoint metrics using embeddable HTML/JavaScript widgets.    ### Live Query Usage Stats  Although the goal of this project is more amibitious than ""just"" providing a live chart of SPARQL endpoint activity, this is still a pretty neat ""by-product"", which we intend to develop further.    #### Demo    A demo can be seen here:  * [FP7-ICT project partners dataset landing page](http://fp7-pp.publicdata.eu) shows the live chart (unfortunately requires IPv6 - if you know how to proxy websockets, please tell me :)  * [SNORQL-SPARQL explorer](http://fp7-pp.publicdata.eu/snorql) lets you do the queries (at the moment only Select queries are handled in the live chart)    ![Screenshot](https://raw.github.com/AKSW/SparqlAnalytics/master/images/2013-04-04-sparql-analytics-screenshot-fp7-pp.publicdata.eu.png)      #### Server Setup  A note in advance: currently the server is [CORS](http://enable-cors.org) enabled on all paths, so you *and anyone else* should be able to do cross site requests from JavaScript.    Clone the project and run        maven clean install    First, you need a postgres database. All query activity will be written to it.        sudo apt-get install postgres      # ... further configuration is up to you            # Create a DB called 'sparql_analytics'      createdb sparql_analytics            # Load the core schema      psql -d sparql_analytics -f sparql-analytics-core/schema.sql    An example server configuration is located under `sparql-analytics-server/config/example/sparql-analytics/platform.properties`. Either modify it directly, or better: create a copy of it and edit the copy:          mkdir sparql-analytics-server/config/myconf      cp -rf sparql-analytics-server/config/example/* sparql-analytics-server/config/myconf    Note that the `sparqlify-analytics` directory under your config directory (i.e. `example` or `myconf`) corresponds to the context path under which the server will run. So this is not optional!    Under `bin` you find the script to run the server:        cd bin      ./run-platform sparql-analytics-server/config/myconf    By default, the server will start on port 5522. Try your browser or curl to test:    [http://localhost:5522/sparql-analytics/api/sparql?query=Select { ?s ?p ?o } Limit 1](http://localhost:5522/sparql-analytics/api/sparql?query=Select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20Limit%201)        curl 'http://localhost:5522/sparql-analytics/api/sparql?query=Select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20Limit%201'    #### Client Setup    The client chart widget is in the `sparql-analytics-client` module. To build the minimized .js file, run        cd sparql-analytics-client      mvn package            # Link the built js file to the webapp js directory, because our HTML file in the next step references it      # CARE: Note the {version} placeholder in the next line :)            ln -s target/sparql-analytics-client-{version}/webapp/js/sparql-analytics-client.min.js src/main/webapp/js/sparql-analytics-client.min.js      Link the client HTML/JavaScript code to your webserver directory (requires you to allow your webserver to follow symlinks)        ln -s /path/to/repo/sparql-analytics-client/src/main/webapp /var/www/sparql-analytics-client    Now visit the following file [index-sparql-analytics-minimal.html](https://github.com/AKSW/SparqlAnalytics/blob/master/sparql-analytics-client/src/main/webapp/index-sparql-analytics-minimal.html) for a minimal example:     [http://localhost/sparql-analytics-client/index-sparql-analytics-minimal.html](http://localhost/sparql-analytics-client/index-sparql-analytics-minimal.html)    You can embad the chart widget by only integrating the following snippet (with properly adjusted paths) into your web page:        <html>      <body>          <div id=""histogram""></div>            <script type=""text/javascript"" src=""js/lib/jquery/1.9.1/jquery-1.9.1.js""></script>          <script type=""text/javascript"" src=""js/lib/jquery-atmosphere/jquery.atmosphere.js""></script>          <script type=""text/javascript"" src=""js/bootstrap.min.js""></script>          <script type=""text/javascript"" src=""js/lib/underscore/1.4.4/underscore.js""></script>          <script type=""text/javascript"" src=""js/lib/highcharts/2.2.5/js/highcharts.js""></script>          <script type=""text/javascript"" src=""js/lib/namespacedotjs/a28da387ce/Namespace.js""></script>            <script type=""text/javascript"" src=""js/sparql-analytics-client.min.js""></script>            <script type=""text/javascript"">              $(document).ready(function() {			                  new SparqlAnalytics.WidgetChartQueryLoad({                      el: '#histogram',                      apiUrl: 'http://localhost:5522/sparql-analytics/api/live'                  });              });          </script>        </body>      </html>      ### License  Will be clarified shortly.     """
Semantic web;https://github.com/tkurz/sesame-vocab-builder;"""# Sesame Vocabulary Builder    Sesame Vocabulary Builder provides a command line tool and maven plugin that allows to create constants for RDF primitives for a given namespace from RDF ontology files.    ## How To    1. Download the latest version [here](https://github.com/tkurz/sesame-vocab-builder/releases).  1. Run jar from command line (Java 7 required): `java -jar vocab-builder-cli-{VERSION}-exe.jar <input-file> [<output-file>]`  1. Additional information can be configured using command-line parameters    ## Command Line Options    ```    <input-file>                            the input file to read from    [<output-file>]                         the output file to write, StdOut if                                            omitted    -b,--languageBundles                    generate L10N LanguageBundles    -c,--constantCase <constantCase>        case to use for URI constants,                                            possible values: LOWER_UNDERSCORE,                                            LOWER_CAMEL, UPPER_CAMEL,                                            UPPER_UNDERSCORE    -C,--stringConstantCase <constantCase>  case to use for String constants, see                                            constantCase    -f,--format <input-format>              mime-type of the input file (will try                                            to guess if absent)    -h,--help                               print this help    -l,--language <prefLang>                preferred language for vocabulary                                            labels    -n,--name <ns>                          the name of the namespace (will try to                                            guess from the input file if absent)    -P,--stringConstantPrefix <prefix>      prefix to create string constants                                            (e.g. _)    -p,--package <package>                  package declaration (will use default                                            (empty) package if absent)    -s,--spaces <indent>                    use spaces for indentation (tabs if                                            missing, 4 spaces if no number given)    -S,--stringConstantSuffix <suffix>      suffix to create string constants                                            (e.g. _STRING)    -u,--uri <prefix>                       the prefix for the vocabulary (if not                                            available in the input file)  ```    ## Run from Git    1. Clone from https://github.com/tkurz/sesame-vocab-builder.git  1. Run `./sesame-vocab-builder  <input-file> <output-file>`  1. Additional information can be configured using command-line parameters    ## Maven Plugin    ```xml  <build>      <plugins>          <plugin>              <groupId>com.github.tkurz.sesame</groupId>              <artifactId>vocab-builder-maven-plugin</artifactId>              <version>1.3</version>              <executions>                  <execution>                      <id>generate-vocabularies</id>                      <phase>generate-sources</phase>                      <goals>                          <goal>generate</goal>                      </goals>                  </execution>              </executions>              <configuration>                  <output>${project.build.directory}/generated-sources/sesame-vocabs</output>                  <packageName>com.example.sesame.vocabularies</packageName>                  <mimeType>text/turtle</mimeType>                  <preferredLanguage>en</preferredLanguage>                  <createResourceBundles>true</createResourceBundles>                  <constantCase>UPPER_UNDERSCORE</constantCase>                  <createStringConstants>true</createStringConstants>                  <stringConstantCase>UPPER_UNDERSCORE</stringConstantCase>                  <stringConstantPrefix>_</stringConstantPrefix>                  <stringConstantSuffix>_STRING</stringConstantSuffix>                  <vocabularies>                      <vocabulary>                          <className>LDP</className>                          <prefix>http://www.w3.org/ns/ldp#</prefix>                          <file>sesame-vocab-builder-core/src/test/resources/ldp.ttl</file>                      </vocabulary>                      <vocabulary>                          <className>RDF</className>                          <url>http://www.w3.org/1999/02/22-rdf-syntax-ns</url>                      </vocabulary>                  </vocabularies>              </configuration>          </plugin>      </plugins>  </build>  ``` """
Semantic web;https://github.com/camwebb/owlconvert;"""owlconvert  ==========    > A very simple OWL format converter based on [OWLAPI](http://owlapi.sourceforge.net/2.x.x/index.html).    I wanted a local tool to handle format conversion similar to the  online converter at  [cs.Manchester](http://owl.cs.manchester.ac.uk/converter/), but  couldn’t find anything handy, ready made.  Before this hack, I knew  close to zero java, so please be lenient!    # Usage          $ owlconvert  manchester|functional|turtle|rdfxml  <owl infile>    sends to standard out.    # Installation    To compile, place in the same dir as `owlapi-bin.jar`          $ javac -classpath owlapi-bin.jar owlconvert.java    Run with (replacing with correct paths):          $ java -classpath ""owlapi-bin.jar:."" owlconvert    Make a shell script or alias to run from anywhere:          $ alias owlconvert 'java -classpath ""$OWLAPI:$OWLCONVERTPATH"" owlconvert'   """
Semantic web;https://github.com/chrdebru/r2rml;"""# R2RML-F: an R2RML Implementation    ## Building and using the code    Note: precompiled packages can be found [here](https://github.com/chrdebru/r2rml-distributions).    To build the project and copy its dependencies, execute    ```bash  $ mvn clean  $ mvn package  $ mvn dependency:copy-dependencies  ```    Note: in order to support connection to Oracle databases, we rely on a library that is not available from the Maven repository. If you have that library not installed manually, run `mvn clean` before `mvn package` and it will install the library locally prior to packaging.     You can also avail of a compiled from that resides in the `dist` directory.    The run the R2RML processor, execute the following command:    ```bash  $ java -jar r2rml.jar config.properties  ```  A fat jar is also provided with the [Apache Maven Shade Plugin](https://maven.apache.org/plugins/maven-shade-plugin/). It does not depend on the `dependency` folder and can be executed as follows:    ```bash  $ java -jar r2rml-fat.jar config.properties  ```    Where `config.properties` is a properties file containing:    - `connectionURL`, a JDBC connection URL to a database (required)  - `user`, username for the user connecting to the database  - `password`, password for the user connecting to the database  - `mappingFile`, the R2RML mapping file (required)  - `outputFile`, the output file (required)  - `format`, format of the output files (default ""TURTLE"")  - `filePerGraph`, flag to write the different graphs in separate files (default ""false"")  - `baseIRI`, used in resolving relative IRIs produced by the R2RML mapping  - `CSVFiles`, a list of paths to CSV files that are separated by semicolons  - `prefixFile`, an RDF file from which name space prefixes will be reused.    When named graphs are used in the R2RML mapping, one should use serialization that support graphs such as N-QUADS and TRIG. The use of other serializations formats (such as TURTLE) results in all triples of all graphs being written away to that file. When setting the flag `filePerGraph` to `true` for serialization formats that do not support graphs, however, the value for `outputFile` will be used to create a directory in which a file will be created for each graph in the RDF dataset.    Note that you cannot use both `CSVFiles` and `connectionURL` at the same time. For each CSV file, the name of the table will be the base name of that file.    ## Example    The directory `example` contains an example of a mapping and configuration file. The example assumes the MySQL database to be called `r2rml`, be running on `localhost` and accessible to the user `foo` with password `bar`. The configuration file looks as follows:    ```properties  connectionURL = jdbc:mysql://localhost/r2rml  user = foo  password = bar  mappingFile = mapping.ttl  outputFile = output.ttl  format = TURTLE  ```    The output, after passing the properties file as an argument to the R2RML processor, should look as follows:    ```turtle  <http://data.example.com/employee/7369>          a                             <http://example.com/ns#Employee> ;          <http://example.com/ns#name>  ""SMITH"" .  ```    ## Run with command line arguments    R2RML can be run with command line arguments similar to the configuration properties.     ```bash  $ java -jar r2rml.jar --connectionURL jdbc:mysql://localhost/r2rml \    --user foo --password bar \    --mappingFile mapping.ttl \    --outputFile output.ttl \    --format TURTLE  ```    ## Function with R2RML-F    This implementation of R2RML re-implemented the ideas presented in [1], allowing one to declare and use functions in ECMAScript as (Function Valued) TermMaps in the mapping. R2RML-F extends R2RML's vocabulary with predicates for declaring functions, function calls and parameter bindings. These are declared in the namespace [rrf](http://kdeg.scss.tcd.ie/ns/rrf/index.html).    ```turtle  @prefix rr: <http://www.w3.org/ns/r2rml#> .  @prefix ex: <http://example.com/ns#> .  @prefix rrf: <http://kdeg.scss.tcd.ie/ns/rrf#>    <#TriplesMap1>      rr:logicalTable [ rr:tableName ""EMP"" ];      rr:subjectMap [          rr:template ""http://data.example.com/employee/{EMPNO}"";          rr:class ex:Employee;      ];      rr:predicateObjectMap [          rr:predicate ex:name;          rr:objectMap [ rr:column ""ENAME"" ];      ];      rr:predicateObjectMap [          rr:predicate ex:test;          rr:objectMap [  	        rrf:functionCall [  	 			rrf:function <#Concat> ;  	 			rrf:parameterBindings (  	 				[ rr:column ""ENAME"" ]  	 				[ rr:column ""EMPNO"" ]  	 			) ;  	 		] ;   	 	]      ]          .        <#Concat>  	rrf:functionName ""concat"" ;  	rrf:functionBody """"""  		function concat(var1, var2) {  		return var1 + "" "" + var2 ;  	}  	"""""" ;  .  ```    ## License  This implementation of R2RML is written by [Christophe Debruyne](http://www.christophedebruyne.be/) and released under the [MIT license](http://opensource.org/licenses/MIT).    ## References    [1]  C. Debruyne and D. O'Sullivan. R2RML-F: Towards Sharing and Executing Domain Logic in R2RML Mappings. In Proceedings of the Workshop on Linked Data on the Web, LDOW 2016, co-located with the 25th International World Wide Web Conference (WWW 2016), Montreal, Canada, April 12th, 2016, 2016 """
Semantic web;https://github.com/phillord/horned-owl;"""Horned OWL  ==========    Horned-OWL is a library for manipulating OWL (Ontology Web Language)  data. While there are a number of libraries that manipulate this form  of data such as the [OWL API](https://github.com/owlcs/owlapi),  they can be quite slow. Horned-OWL is aimed at allowing ontologies  with millions of terms.    The library now implements all of OWL2, and we are working on further  parser functionality. We are testing it with real world tasks, such as  parsing the Gene Ontology, which is does in 2-3 seconds which is 20-40  times faster than the OWL API. """
Semantic web;https://github.com/sindice/sparqled;"""# SPARQLed    Assisted SPARQL Editor    ## General Information    - Project and contact information: [http://sindice.com/sparqled/](http://sindice.com/sparqled/)  = Instructions for installing SparQLed are available in the **Downloads** section    ## Live Demos    - [SparQLed](http://hcls.sindice.com/sparql-editor/) on the Health Care Life Science datasets  - [SparQLed](http://demo.sindice.net/dbpedia-sparqled/) on the English part of DBpedia 3.7    ## Distribution Content    This distribution includes 6 modules:  - _sparql-editor-client:_ the editor User Interface;  - _recommendation-servlet:_ the servlet for providing query element recommendations;  - _sesame-sparql-queryparser:_ [Sesame](http://www.openrdf.org/) SPARQL query parser extended for the SparQLed use case;  - _sesame-backend:_ Utility classes for operating on SPARQL endpoint through Sesame;  - _sparqled-commons:_ Utility classes used in the Data Graph Summary computation;  - _sparql-summary:_ an SPARQL-based Data Graph Summary Computation.    ## Acknowledgements    This software is based upon works supported by :    * the European FP7 project [LOD2](http://lod2.eu/Welcome.html) (257943)  * the IRCSET EMPOWER 2012 Government of Ireland Postdoctoral Fellowship (Renaud Delbru)    The SparQLed User Interface is based on the [Flint SPARQL Editor](https://github.com/TSO-Openup/FlintSparqlEditor).    ========    _Copyright 2014, National University of Ireland, Galway._ """
Semantic web;https://github.com/renespeck/fox-java;"""[1]: ./src/main/java/org/aksw/fox/binding/java/Examples.java    fox-java  ========    Java bindings for FOX - Federated Knowledge Extraction Framework      In [Examples.java][1] you can find an example.    ```Java  final IFoxApi fox = new FoxApi()       .setApiURL(new URL(""http://0.0.0.0:4444/fox""))       .setTask(FoxParameter.TASK.RE)       .setOutputFormat(FoxParameter.OUTPUT.JSONLD)       .setLang(FoxParameter.LANG.EN)       .setInput(""A. Einstein was born in Ulm."")       // .setLightVersion(FoxParameter.FOXLIGHT.ENBalie)       .send();     final String jsonld = fox.responseAsFile();   final FoxResponse response = fox.responseAsClasses();     List<Entity> entities = response.getEntities();   List<Relation> relations = response.getRelations();    ```    ### Maven      <dependencies>        <dependency>          <groupId>com.github.renespeck</groupId>          <artifactId>fox-java</artifactId>          <version>e67a2bd475</version>        </dependency>      </dependencies>        <repositories>          <repository>              <id>jitpack.io</id>              <url>https://jitpack.io</url>          </repository>      </repositories> """
Semantic web;https://github.com/oxigraph/rio;"""Rio  ===    [![actions status](https://github.com/oxigraph/rio/workflows/build/badge.svg)](https://github.com/oxigraph/rio/actions)    Rio is a low level library which provides conformant and fast parsers and formatters for RDF related file formats.    It currently provides [N-Triples](https://docs.rs/rio_turtle/latest/rio_turtle/struct.NTriplesParser.html), [N-Quads](https://docs.rs/rio_turtle/latest/rio_turtle/struct.NQuadsParser.html), [Turtle](https://docs.rs/rio_turtle/latest/rio_turtle/struct.TurtleParser.html), [TriG](https://docs.rs/rio_turtle/latest/rio_turtle/struct.TrigParser.html) and [RDF/XML](https://docs.rs/rio_xml/latest/rio_xml/struct.RdfXmlParser.html) parsers and formatters.    It is split into multiple crates:  * `rio_api` provides common traits and data structures to be used in Rio parsers (`Triple`, `TriplesParser`, `Iri`...).    [![Latest Version](https://img.shields.io/crates/v/rio_api.svg)](https://crates.io/crates/rio_api)     [![Released API docs](https://docs.rs/rio_api/badge.svg)](https://docs.rs/rio_api)  * `rio_turtle` provides conformant streaming parsers and formatters for [Turtle](https://www.w3.org/TR/turtle/), [TriG](https://www.w3.org/TR/trig/), [N-Triples](https://www.w3.org/TR/n-triples/) and [N-Quads](https://www.w3.org/TR/n-quads/).    [RDF-star](https://w3c.github.io/rdf-star/cg-spec/) syntaxes are also supported: [Turtle-star](https://w3c.github.io/rdf-star/cg-spec/#turtle-star), [TriG-star](https://w3c.github.io/rdf-star/cg-spec/#trig-star), [N-Triples-star](https://w3c.github.io/rdf-star/cg-spec/#n-triples-star) and [N-Quads-star](https://w3c.github.io/rdf-star/cg-spec/#n-quads-star).    [![Latest Version](https://img.shields.io/crates/v/rio_turtle.svg)](https://crates.io/crates/rio_turtle)    [![Released API docs](https://docs.rs/rio_turtle/badge.svg)](https://docs.rs/rio_turtle)  * `rio_xml` provides a conformant streaming parser and a formatter for [RDF/XML](https://www.w3.org/TR/rdf-syntax-grammar/).    [![Latest Version](https://img.shields.io/crates/v/rio_xml.svg)](https://crates.io/crates/rio_xml)    [![Released API docs](https://docs.rs/rio_xml/badge.svg)](https://docs.rs/rio_xml)    There is also the `rio_testsuite` crate that is used for testing Rio parsers against the [W3C RDF tests](http://w3c.github.io/rdf-tests/) to ensure their conformance.  It provides both an executable for building implementation reports and integration test to quickly ensure that the parsers stay conformant.  It is not designed to be used outside of Rio.      ## License    Copyright 2019-2021 The Rio developers.    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. """
Semantic web;https://github.com/banana-rdf/banana-rdf;"""banana-rdf  ==========    [![Build Status](https://secure.travis-ci.org/w3c/banana-rdf.png)](http://travis-ci.org/w3c/banana-rdf) [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/w3c/banana-rdf?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    The current published version 0.8.6, compiled with scala 2.13.6, is to be found on Maven Central under groupId [net/bblfish/rdf/](https://repo1.maven.org/maven2/net/bblfish/rdf/).    ```scala  val banana = (name: String) => ""net.bblfish.rdf"" %% name % ""0.8.6"" excludeAll (ExclusionRule(organization = ""org.scala-stm""))    //choose the packages you need for your dependencies  val bananaDeps = Seq(""banana"", ""banana-rdf"", ""banana-rdf4j"").map(banana)  ```  Snapshot releases can be found on [sonatype.org](https://oss.sonatype.org/content/repositories/snapshots/net/bblfish/rdf/). To use these you will need at add the sonatype resolver to your sbt build:  ```scala  resolvers += Resolver.sonatypeRepo(""snapshots"")  ```    A Scala3 version is being developed on the [scala-3](https://github.com/banana-rdf/banana-rdf/tree/scala-3) branch of this repository.     An RDF library in Scala  -----------------------    `banana-rdf` is a library for RDF, SPARQL and Linked Data technologies  in Scala.    It can be used with existing libraries without any added cost. There  is no wrapping involved: you manipulate directly the real objects. We  currently support Jena, RDF4J and Plantain, a pure Scala  implementation.    Features  --------    `banana-rdf` emphasizes type-safety and immutability, so it can come  with some cost when the underlying implementation is very mutable (I'm  looking at you, Jena and RDF4J). We try to keep a clear distinction  between the core concepts and the enhanced syntax that Scala can give  us.    [`RDF`](rdf/shared/src/main/scala/org/w3/banana/RDF.scala)  itself is defined as a record of types. Implementations just have to  _plug_ their own types. And because types alone are not enough, we  introduce the  [`RDFOps`](rdf/shared/src/main/scala/org/w3/banana/RDFOps.scala)  typeclass, which defines the mandatory operations that an RDF  implementation must  implement. [`SparqlOps`](rdf/shared/src/main/scala/org/w3/banana/SparqlOps.scala)  does the same for SPARQL.    With `banana-rdf`, you get _Diesel_, a nice DSL to build and navigate  within **pointed graphs** (graphs with a pointer to an inner  node). You also get an abstraction for **graph stores**  ([`GraphStore`](rdf/shared/src/main/scala/org/w3/banana/GraphStore.scala)),   which do not have to be **SPARQL engines**  ([`SparqlEngine`](rdf/shared/src/main/scala/org/w3/banana/SparqlEngine.scala)).   Of course, you can **serialize** and **deserialize**  most of the RDF syntaxes as well as JSON-LD (RDFa will come soon).    `banana-rdf` introduces the concept of **binders**, which let you  bridge the Scala and RDF worlds. Most of the common datastructures are  already available, and you can even map your own classes. Unlike usual  ORM techniques, this does not rely on annotation or reflection.    Until we write thorough documentation, the best place to understand  what you can do is to go through the [test  suite](https://github.com/w3c/banana-rdf/tree/series/0.8.x/rdf-test-suite).    How to start geeking  --------------------    To get going with banana-rdf  and get a feel for how to use it the easiest and  fastest way may well be to use it directly in the Ammonite shell as explained in the  [Scripting with Ammonite wiki page](https://github.com/banana-rdf/banana-rdf/wiki/Scripting-with-Ammonite).    It always helps to have the code available, as there are a lot of useful examples in   the test suite. You only need a recent version of Java, that's all:    ``` bash  $ git clone git@github.com:w3c/banana-rdf.git  $ cd banana-rdf  $ sbt  ```    It's also easy to just build specific target platforms:        ``` bash  $ sbt +banana_js/test    # for javascript only   $ sbt +banana_jvm/test   # for jvm only  ```    ( note: scala-js compilation uses more memory. see [travis.yml](.travis.yml) )    IDE Setup  =========    `banana-rdf` works with both [eclipse](https://www.eclipse.org/) and [IntelliJ IDEA](http://www.jetbrains.com/idea/).    global.sbt  ----------  Independent of your preferred IDE, optionally the add the following line to `~/.sbt/0.13/global.sbt` to prevent the   generation of empty source directories:    ```      unmanagedSourceDirectories in Compile ~= { _.filter(_.exists) }  ```    Eclipse  -------  Eclipse should work ""out of the box"" with the addition of the following global settings:    In `~/.sbt/0.13/global.sbt`:    ```      unmanagedSourceDirectories in Compile ~= { _.filter(_.exists) }  ```    In `~/.sbt/0.13/plugins/build.sbt`    ```      addSbtPlugin(""com.typesafe.sbteclipse"" % ""sbteclipse-plugin"" % ""2.5.0"")  ```    To generate eclipse project files, just run the command:    ``` bash  $ sbt eclipse  ```    IntelliJ IDEA  -------------    IntelliJ IDEA works out of the box since 2016.      Community  =========    For discussions that don't fit in the [issues tracker](https://github.com/w3c/banana-rdf/issues), you may try  either   *  the [w3c banana-rdf mailing list](http://lists.w3.org/Archives/Public/public-banana-rdf/), for longer discussions  *  the [banana-rdf gitter channel](https://gitter.im/banana-rdf/banana-rdf), for quick real time socialising    Code of Conduct  ---------------    **Banana-RDF contributors all agree to follow the [W3C Code of Ethics and Professional Conduct](http://www.w3.org/Consortium/cepc/).**    If you want to take action, feel free to contact Alexandre Bertails <alexandre@bertails.org>. You can also contact W3C Staff as explained in [W3C Procedures](http://www.w3.org/Consortium/pwe/#Procedures).    Licence  -------    This source code is made available under the [W3C Licence](http://opensource.org/licenses/W3C). This is a business friendly license. """
Semantic web;https://github.com/frmichel/morph-xr2rml;"""# What is it?    Morph-xR2RML is an implementation of the [xR2RML mapping language](http://i3s.unice.fr/~fmichel/xr2rml_specification.html) that enables the description of mappings from relational or non relational databases to RDF. xR2RML is an extension of [R2RML](http://www.w3.org/TR/r2rml/) and [RML](http://semweb.mmlab.be/rml/spec.html).    Morph-xR2RML comes with connectors for relational databases (MySQL, PostgreSQL, MonetDB) and the MongoDB NoSQL document store.  Two running modes are available:  - the *graph materialization* mode creates all possible RDF triples at once.  - the *query rewriting* mode translates a SPARQL 1.0 query into a target database query and returns a SPARQL answer. It can run as a SPARQL 1.0 endpoint or as a stand-alone application.    Morph-xR2RML was developed by the [I3S laboratory](http://www.i3s.unice.fr/) as an extension of the [Morph-RDB project](https://github.com/oeg-upm/morph-rdb) which is an implementation of R2RML. It is made available under the Apache 2.0 License.    #### SPARQL-to-SQL  The SPARQL-to-SQL rewriting is an adaptation of the former Morph-RDB implementation, it supports SPARQL SELECT and DESCRIBE queries.    #### SPARQL-to-MongoDB  The SPARQL-to-MongoDB rewriting is a fully new component, it supports the SELECT, ASK, CONSTRUCT and DESCRIBE query forms.        ## Publications  [1] F. Michel, L. Djimenou, C. Faron-Zucker, and J. Montagnat. Translation of Relational and Non-Relational Databases into RDF with xR2RML.  In Proceedings of the *11th International Confenrence on Web Information Systems and Technologies (WEBIST 2015)*, Lisbon, Portugal, 2015.    [2] F. Michel, L. Djimenou, C. Faron-Zucker, and J. Montagnat. xR2RML: Relational and Non-Relational Databases to RDF Mapping Language.  Research report, CNRS, 2015. https://hal.archives-ouvertes.fr/hal-01066663    [3] C. Callou, F. Michel, C. Faron-Zucker, C. Martin, J. Montagnat. Towards a Shared Reference Thesaurus for Studies on History of Zoology, Archaeozoology and Conservation Biology. In *Semantic Web For Scientific Heritage (SW4SH), Workshops of the ESWC’15 conference*.    [4] F. Michel, C. Faron-Zucker, and J. Montagnat. A Generic Mapping-Based Query Translation from SPARQL to Various Target Database Query Languages.  In Proceedings of the *12th International Confenrence on Web Information Systems and Technologies (WEBIST 2016)*, Roma, Italy, 2016.    [5] F. Michel, C. Faron-Zucker, and J. Montagnat. Mapping-based SPARQL access to a MongoDB database. Research report, CNRS, 2016.   https://hal.archives-ouvertes.fr/hal-01245883.    [6] F. Michel, C. Faron-Zucker, and J. Montagnat. A Mapping-Based Method to Query MongoDB Documents with SPARQL. In *27th International Conference on Database and Expert Systems Applications (DEXA 2016)*, 2016.      ## Limitations    ##### xR2RML Language support  - The generation of RDF collection and containers is supported in all cases (from a list of values resulting of the evaluation of a mixed syntax path typically, from the result of a join query implied by a referencing object map), except in the case of a regular R2RML join query applied to a relational database: the result of the join SQL query cannot be translated into an RDF collection or container.  - Named graphs are supported although they are not printed out in Turtle which does not support named graphs. It would be quite easy to extend it with a N-Quad or Trig serialization to allow for writing triples in named graphs.    The former limitation on NestedTermMaps was lifted in Sept. 2017. All types of NestedTermMaps are now fully implemented, so that any complex iterations and collection/container nesting can be defined.      ##### Query rewriting   The query rewriting is implemented for RDBs and MongoDB, with the restriction that _no mixed syntax paths be used_. Doing query rewriting with mixed syntax paths is a much more complex problem, that may not be possible in all situations (it would require to ""revert"" expressions such as JSONPath or XPath to retrieve source data base values).    Only one join condition is supported in a referencing object map.    ----------    # Code description    See a detailed [description of the project code and architecture](doc/README_code_architecture.md).    ----------    # Want to try it?    ### Download, Build    Pre-requisite: have **Java SDK 10** installed    You can download the last release or snapshot published in [this repository](https://www.dropbox.com/sh/djnztipsclvcskw/AABT1JagzD4K4aCALDNVj-yra?dl=0).  The latest on-going version is the 1.3.2 snapshot.    Alternatively, you can build the application using [Maven](http://maven.apache.org/): in a shell, CD to the root directory morph-xr2rml, then run the command: `mvn clean package`. A jar with all dependencies is generated in `morph-xr2rml-dist/target`.      ### Run it    The application takes two options: `--configDir` gives the configuration directory and `--configFile` give the configuration file within this directory. Option `--configFile` defaults to `morph.properties`.    Additionally, several parameter given in the configuration file can be overridden using the following options:   - mapping file: `--mappingFile`   - output file : `--output`  - maximum number of triples generated in a single output file: `--outputMaxTriples`      **From a command line interface**, CD to directory morph-xr2rml-dist and run the application as follows:    ```  java -jar target/morph-xr2rml-dist-<version>-jar-with-dependencies.jar \     --configDir <configuration directory> \     --configFile <configuration file within this directory>  ```    Besides, the logger configuration can be overriden by passing the `log4j.configuration` parameter to the JVM:    ```  java -Dlog4j.configuration=file:/path/to/my/log4j.configuration -jar ...  ```    **From an IDE** such as Eclipse or IntelliJ: In project morph-xr2rml-dist locate main class `fr.unice.i3s.morph.xr2rml.engine.MorphRunner`, and run it as a Scala application with arguments `--configDir` and `--configFile`.    ### SPARQL endpoint    To run Morph-xR2RML as a SPARQL endpoint, simply edit the configuration file (see reference) and set the property `sever.active=true`. The default access URL is:  ```  http://localhost:8080/sparql  ```  Property `query.file.path` is ignored and queries can be submitted using either HTTP GET or POST methods as described in the [SPARQL protocol](https://www.w3.org/TR/rdf-sparql-protocol/) recommendation.    For SPARQL SELECT and ASK queries, the XML, JSON, CSV and TSV serializations are supported.    For SPARQL DESCRIBE and CONSTRUCT queries, the supported serializations are RDF/XML, N-TRIPLE, N-QUAD, TURTLE, N3 and JSON-LD.    ### Examples for MongoDB    In directories `morph-xr2rml-dist/example_mongo` and `morph-xr2rml-dist/example_mongo_rewriting` we provide example databases and corresponding mappings. Directory `example_mongo` runs the graph materialization mode, `example_mongo_rewriting` runs the query rewriting mode.    - `testdb_dump.json` is a dump of the MongoDB test database: copy and paste the content of that file into a MongoDB shell window to create the database;  - `morph.properties` provides database connection details;  - `mapping1.ttl` to `mapping4.ttl` contain xR2RML mapping graphs illustrating various features of the language;  - `result1.txt` to `result4.txt` contain the expected result of the mappings 1 to 4;  - `query.sparql` (in directory `example_mongo_rewriting` only) contains a SPARQL query to be executed against the test database.    Edit `morph.properties` and change the database URL, name, user and password with appropriate values.    > _**Note about query optimization**_: the xR2RML xrr:uniqueRef notation is of major importance for query optimization as it allows for self-joins elimination. Check example in `morph-xr2rml-dist/example_taxref_rewriting`.    ### Examples for MySQL    In directories `morph-xr2rml-dist/example_mysql` and `morph-xr2rml-dist/example_mysql_rewriting` we provide example databases and corresponding mappings. Directory `example_mysql` runs the graph materialization mode, `example_mysql_rewriting` runs the query rewriting mode.    - `testdb_dump.sql` is a dump of the MySQL test database. You may import it into a MySQL instance by running command `mysql -u root -p test < testdb_dump.sql`;  - `morph.properties` provides database connection details;  - `mapping.ttl` contains an example xR2RML mapping graph;  - `result.txt` contains the expected result of applying this mapping to that database;  - `query.sparql` (in directory `example_mysql_rewriting` only) contains a SPARQL query to be executed against the test database.    Edit `morph.properties` and change the database url, name, user and password with appropriate values.    ----------    # Configuration file reference  ```  # -- xR2RML mapping file (Mandatory):  # path relative to the configuration directory given in parameter --configDir  mappingdocument.file.path=mapping1.ttl    # -- Server mode: true|false. Default: false  # false: stand-alone application that performs either graph materialization or query rewriting  # true:  SPARQL endpoint with query rewriting  server.active=false    # -- Server port number, ignored when ""server.active=false"". Default: 8080  server.port=8080    # -- Processing result output file, relative to --configDir. Default: result.txt  output.file.path=result.txt    # -- Max number of triples to generate in output file. Default: 0 (no limit)  # If the max number is reached, file name is suffixed with an index e.g. result.txt.0, result.txt.1, result.txt.2 etc.  output.file.max_triples=0    # -- Output RDF syntax: RDF/XML|N-TRIPLE|TURTLE|N3|JSON-LD. Default: TURTLE  # Applies to the graph materialization and the rewriting of SPARQL CONSTRUCT and DESCRIBE queries  output.syntax.rdf=TURTLE    # -- Output syntax for SPARQL result set (SPARQL SELECT and ASK queries): XML|JSON|CSV|TSV. Default: XML  # When ""server.active = true"", this may be overridden by the Accept HTTP header of the request  output.syntax.result=XML    # -- Display the result on the std output after the processing: true|false. Default: true  output.display=false    # -- File containing the SPARQL query to process, relative to --configDir. Default: none.   # Ignored when ""server.active = true""  query.file.path=query.sparql    # -- Database connection type and configuration  no_of_database=1  database.type[0]=MongoDB  database.driver[0]=  database.url[0]=mongodb://127.0.0.1:27017  database.name[0]=test  database.user[0]=user  database.pwd[0]=user      # -- Reference formulation: Column|JSONPath|XPath. Default: Column  database.reference_formulation[0]=JSONPath    # -- Runner factory. Mandatory.  # For MongoDB: fr.unice.i3s.morph.xr2rml.mongo.engine.MorphJsondocRunnerFactory  # For RDBs:    es.upm.fi.dia.oeg.morph.rdb.engine.MorphRDBRunnerFactory  runner_factory.class.name=fr.unice.i3s.morph.xr2rml.mongo.engine.MorphMongoRunnerFactory      # -- URL-encode reserved chars in database values. Default: true  # uricolumn.encode_unsafe_chars_dbvalues=true    # -- URL-encode reserved chars IRI template string. Default: true   # uricolumn.encode_uri=true      # -- Cache the result of previously executed queries for MongoDB. Default: false  # Caution: high memory consumption, to be used for RefObjectMaps only  querytranslator.cachequeryresult=false      # -- Primary SPARQL query optimization. Default: true  querytranslator.sparql.optimize=true    # -- Abstract query optimization: self join elimination. Default: true  querytranslator.abstract.selfjoinelimination=true    # -- Abstract query optimization: self union elimination. Default: true  querytranslator.abstract.selfunionelimination=true    # -- Abstract query optimization: propagation of conditions in a inner/left join. Default: true  querytranslator.abstract.propagateconditionfromjoin=true    ```   """
Semantic web;https://github.com/djogopatrao/SPARQLFederator;"""SPARQLFederator  ===============    Expand SPARQL queries to perform inference on multiple endpoints.      Main documentation  ==================    Find more information on our wiki page:    https://github.com/djogopatrao/SPARQLFederator/wiki    Example  =======    java -jar target/SPARQLFederator-1.1-SNAPSHOT-jar-with-dependencies.jar -domain_ontology examples/domain.owl -federation_ontology examples/federation.owl  -exec print -query 'http://www.cipe.accamargo.org.br/ontologias/domain.owl#A'    ""-exec print"" will print the expanded query; ""-exec run"" would execute it and yield results (that is, if there are working endpoints as defined on example/federation.owl)    ""http://www.cipe.accamargo.org.br/ontologias/domain.owl#A"" is the full IRI of the class you're querying for (see -domain_ns for saving space); try it with other classes (like B, or C). Add axioms and your own classes to domain.owl, but keep in mind SPARQLFederator implemented semantics (see the wiki).      Arguments  =========    It is mandatory to specify at least the domain ontology file, the federation ontology file, and one or more classes for querying.    usage: SPARQLFederator [options] <DOMAIN_CLASS> [...]     -query <query>               The query to be expanded (or run) in the syntax specified by -query_type     -domain_ns <arg>             The domain namespace (if specified, will be appended before each of the queryied DOMAIN_CLASSes)      -domain_ontology <arg>       The domain ontology file      -federation_ontology <arg>   The federation ontology file      -help                        Shows the help message      -ontocloud_ns <arg>          The federation namespace (default value: http://www.cipe.accamargo.org.br/ontologias/ontocloud2.owl)      -optimizer <arg>             Execute a query optimizer: 'simple' (default) or 'none'      -planner <arg>               Execute a query planner: 'simple' (default) or 'none'      -query_type <arg>            The accepted query type: 'simple' (default) or 'sparql' (not implemented)      -stats                       Display statistics for queries      DEBUG compiling and running:  ===========================    mvn clean package     mvn exec:java -Dexec.mainClass=""br.org.accamargo.cipe.gqe.SPARQLFederatorRun"" -Dexec.args=""-federation_ontology federation_ontology.owl -domain_ontology domain_ontology.owl  -domain_ns domainNamespace# class1 [,classn]""      Production Compiling  ====================    mvn clean compile assembly:single     """
Semantic web;https://github.com/sputniq-space/ontodia;"""# Ontodia [![npm](https://img.shields.io/npm/v/ontodia.svg)](https://www.npmjs.com/package/ontodia) [![CircleCI](https://circleci.com/gh/sputniq-space/ontodia.svg?style=svg)](https://circleci.com/gh/sputniq-space/ontodia) #    Ontodia is a JavaScript library that allows to visualize, navigate and explore data in the form of an interactive graph based on underlying data sources.    ## What is Ontodia for?    Ontodia allows you to create and persist diagrams made from existing data - relational, object, semantic.    It was designed to visualize RDF data sets in particular, but could be tailored to almost any data source by implementing a data provider interface.      ## Core features    - Visual navigation and diagramming over large graph data sets  - Rich graph visualization and context-aware navigation features   - Ability to store and retrieve diagrams  - User friendly - no graph query language or prior knowledge of the schema required  - Customizable user interface (by modifying templates for nodes and links) and data storage back-end    ## How to try it?    You can follow developer tutorials at the [developer documentation page](https://github.com/metaphacts/ontodia/wiki)    ## License    The Ontodia library is distributed under LGPL-2.1. A commercial license with additional features, support and custom development is available, please contact us at [info@metaphacts.com](info@metaphacts.com).         ## Developer documentation and contributing    Developer documentation is available at [wiki page](https://github.com/metaphacts/ontodia/wiki).    ## Giving Ontodia people credit    If you use the Ontodia library in your projects, please provide a link to this repository in your publication and a citation reference to the following paper:     Mouromtsev, D., Pavlov, D., Emelyanov, Y., Morozov, A., Razdyakonov, D. and Galkin, M., 2015. The Simple Web-based Tool for Visualization and Sharing of Semantic Data and Ontologies. In International Semantic Web Conference (Posters & Demos).    ```  @inproceedings{Mouromtsev2015,      author = {Mouromtsev, Dmitry and Pavlov, Dmitry and Emelyanov, Yury and          Morozov, Alexey and Razdyakonov, Daniil and Galkin, Mikhail},      year = {2015},      month = {10},      title = {The Simple Web-based Tool for Visualization and Sharing of Semantic Data and Ontologies},      booktitle = {International Semantic Web Conference (Posters & Demos)}  }  ```    It really helps our team to gain publicity and acknowledgment for our efforts.  Thank you for being considerate! """
Semantic web;https://github.com/dotnetrdf/dotnetrdf;"""# Welcome    [![Build status](https://ci.appveyor.com/api/projects/status/f8wtq0qh4k6620sl/branch/master?svg=true)](https://ci.appveyor.com/project/dotNetRDFadmin/dotnetrdf/branch/master)      dotNetRDF is a powerful and flexible API for working with RDF and SPARQL in .NET environments.    dotNetRDF is licensed under the MIT License, see the LICENSE.txt file in this repository    ## Getting Started    The easiest way to get dotNetRDF is via NuGet. We provide the following packages:    - **dotNetRDF** - contains the core libraries. This includes support for reading and writing RDF; and for managing and querying RDF data in-memory.  - **dotNetRDF.Data.DataTables** - a package which integrates RDF data with System.Data.DataTable  - **dotNetRDF.Data.Virtuoso** - provides support for using OpenLink Virtuoso as a backend store with dotNetRDF.  - **dotNetRDF.Query.FullText** - provides a full-text query plugin for dotNetRDF's Leviathan SPARQL query engine. The text indexing is provided by Lucene.  - **dotNetRDF.Query.Spin** - provides an implementation of [SPIN](http://spinrdf.org/) using dotNetRDF's Leviathan SPARQL query engine.  - **dotNetRDF.Web** - provides a framework for hosting RDF data in an IIS web application. This includes implementations of the SPARQL Protocol and SPARQL Graph Store Protocol.    We currently provide support for the following .NET frameworks:    - .NET 4.0  - .NET 4.0 Client Profile  - .NET Standard 2.0  	  ## Read The Docs!    To get started with using dotNetRDF you may want to check out the following resources:     - [User Guide](https://www.dotnetrdf.org/docs/stable/user_guide/index.html) - Series of articles detailing how to use various features of the library   - [Developer Guide](https://www.dotnetrdf.org/docs/stable/developer_guide/index.html) - Some advanced documentation    ## Asking Questions and Reporting Bugs    If you have a question about using dotNetRDF, please post it on [StackOverflow using the tag `dotnetrdf`](https://stackoverflow.com/questions/tagged/dotnetrdf).    Bugs and feature requests can be submitted to our [issues list on GitHub](https://github.com/dotnetrdf/dotnetrdf/issues). When submitting a bug report, please  include as much detail as possible. Code and/or data that reproduces the problem you are reporting will make it much more likely that your issue gets addressed   quickly.    ## Developers    dotNetRDF is developed by the following people:     - Rob Vesse   - Tomasz Pluskiewicz   - Ron Michael Zettlemoyer   - Max   - Khalil Ahmed   - Samu Lang   - Giacomo Citi    dotNetRDF also benefits from many community contributors who contribute in the form of bug reports, patches, suggestions and other feedback,   please see the [Acknowledgements](https://github.com/dotnetrdf/dotnetrdf/blob/master/Acknowledgments.txt) file for a full list.    ## Pull Requests    We are always pleased to receive pull requests that fix bugs or add features.   When fixing a bug, please make sure that it has been reported on the [issues list](https://github.com/dotnetrdf/dotnetrdf/issues) first.  If you plan to work on a new feature for dotNetRDF, it would be good to raise that on the issues list before you commit too much time to it.   """
Semantic web;https://github.com/linkedpipes/etl;"""# LinkedPipes ETL  [![Build Status](https://travis-ci.com/linkedpipes/etl.svg?branch=develop)](https://travis-ci.com/linkedpipes/etl)    LinkedPipes ETL is an RDF based, lightweight ETL tool.  - [REST API](https://github.com/linkedpipes/etl/wiki) based set of components for easy integration  - [Library of components](https://etl.linkedpipes.com/components) to get you started faster  - [Sharing of configuration](https://etl.linkedpipes.com/templates/) among individual pipelines using templates  - RDF configuration of transformation pipelines    ## Requirements  - Linux, Windows, iOS  - [Docker], [Docker Compose]    ### For building locally  - [Java] 11 or 16  - [Git]  - [Maven], 3.2.5 or newer  - [Node.js] & npm    ## Installation and startup  You can run LP-ETL in Docker, or build it from the source.    ### Docker  To start LP-ETL ```master``` branch on ```http://localhost:8080```, you can use a one-liner:  ```  curl https://raw.githubusercontent.com/linkedpipes/etl/master/docker-compose.yml | docker-compose -f - up  ```  Note that on Windows, there is an [issue with buildkit](https://github.com/moby/buildkit/issues/1684).  See the [temporary workaround](https://github.com/linkedpipes/etl/issues/851#issuecomment-814058925).    When running this on Windows, you might get a build error. There is a [workaround](https://github.com/linkedpipes/etl/issues/851) for that.    Alternatively, you can build docker images from GitHub sources using a one-liner:  ```  curl https://raw.githubusercontent.com/linkedpipes/etl/master/docker-compose-github.yml | docker-compose -f - up  ```    You may need to run the commands as ```sudo``` or be in the ```docker``` group.    #### Configuration  Each component (executor, executor-monitor, storage, frontend) has separate ```Dockerfile```.    Environment variables:   * ```LP_ETL_BUILD_BRANCH``` - The ```Dockerfiles``` are designed to run build from the github repository, the branch is set using this property, default is ```master```.   * ```LP_ETL_BUILD_JAVA_TEST``` - Set to empty to allow to run Java tests, this will slow down the build.   * ```LP_ETL_DOMAIN``` - The URL of the instance, this is used instead of the ```domain.uri``` from the configuration.   * ```LP_ETL_FTP``` - The URL of the FTP server, this is used instead of the ```executor-monitor.ftp.uri``` from the configuration.      For [Docker Compose], there are additional environment variables:   * ```LP_ETL_PORT``` - Specify port mapping for frontend, this is where you can connect to your instance.  This does NOT have to be the same as port in ```LP_ETL_DOMAIN``` in case of reverse-proxying.    For example to run LP-ETL from ```develop``` branch on ```http://localhost:9080``` use can use following command:  ```  curl https://raw.githubusercontent.com/linkedpipes/etl/develop/docker-compose-github.yml | LP_ETL_PORT=9080 LP_ETL_DOMAIN=http://localhost:9080 LP_ETL_BUILD_BRANCH=develop docker-compose -f - up  ```    ```docker-compose``` utilizes several volumes that can be used to access/provide data.  See ```docker-compose.yml``` comments for examples and configuration.  You may want to create your own ```docker-compose.yml``` for custom configuration.    ### From source on Linux    #### Installation    ```sh  $ git clone https://github.com/linkedpipes/etl.git  $ cd etl  $ mvn install  ```    #### Configuration  The configuration file ```deploy/configuration.properties``` can be edited, mainly changing paths to working, storage, log and library directories.     #### Startup    ```sh  $ cd deploy  $ ./executor.sh >> executor.log &  $ ./executor-monitor.sh >> executor-monitor.log &  $ ./storage.sh >> storage.log &  $ ./frontend.sh >> frontend.log &  ```    #### Running LP-ETL as a systemd service  See example service files in the ```deploy/systemd``` folder.    ### From source on Windows  Note that it is also possible to use [Bash on Ubuntu on Windows] or [Cygwin] and proceed as with Linux.    #### Installation  ```sh  git clone https://github.com/linkedpipes/etl.git  cd etl  mvn install  ```  #### Configuration  The configuration file ```deploy/configuration.properties``` can be edited, mainly changing paths to working, storage, log and library directories.     #### Startup  In the ```deploy``` folder, run   * ```executor.bat```   * ```executor-monitor.bat```   * ```storage.bat```   * ```frontend.bat```    ## Plugins - Components  The components live in the ```jars``` directory.  Detailed description of how to create your own is coming soon, in the meantime, you can copy an existing component and change it.     ## Update notes  > Update note 5: 2019-09-03 breaking changes in the configuration file. Remove ```/api/v1``` from the ```executor-monitor.webserver.uri```, so it loolks like: ```executor-monitor.webserver.uri = http://localhost:8081```. You can also remove ```executor.execution.uriPrefix``` as the value is derived from ```domain.uri```.    > Update note 4: 2019-07-03 we changed the way frontend is run. If you do not use our script to run it, you need to update yours.     > Update note 3: When upgrading from develop prior to 2017-02-14, you need to delete ```{deploy}/jars``` and ```{deploy}/osgi```.     > Update note 2: When upgrading from master prior to 2016-11-04, you need to move your pipelines folder from e.g., ```/data/lp/etl/pipelines``` to ```/data/lp/etl/storage/pipelines```, update the configuration.properites file and possibly the update/restart scripts as there is a new component, ```storage```.    > Update note: When upgrading from master prior to 2016-04-07, you need to delete your old execution data (e.g., in /data/lp/etl/working/data)    [Java]: <http://www.oracle.com/technetwork/java/javase/downloads/index.html>  [Git]: <https://git-scm.com/>  [Maven]: <https://maven.apache.org/>  [Node.js]: <https://nodejs.org>  [Cygwin]: <https://www.cygwin.com/>  [Bash on Ubuntu on Windows]: <https://msdn.microsoft.com/en-us/commandline/wsl/about>  [Docker]: <https://www.docker.com/>  [Docker Compose]: <https://docs.docker.com/compose/> """
Semantic web;https://github.com/AKSW/RDFUnit;"""RDFUnit - RDF Unit Testing Suite  ==========    [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.aksw.rdfunit/rdfunit-parent/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.aksw.rdfunit/rdfunit-parent)  [![Build Status](https://travis-ci.org/AKSW/RDFUnit.svg?branch=master)](https://travis-ci.org/AKSW/RDFUnit)  [![Coverity Scan Build Status](https://scan.coverity.com/projects/2650/badge.svg?flat=1)](https://scan.coverity.com/projects/2650)  [![Coverage Status](https://coveralls.io/repos/AKSW/RDFUnit/badge.svg?branch=master&service=github)](https://coveralls.io/github/AKSW/RDFUnit?branch=master)  [![Codacy Badge](https://api.codacy.com/project/badge/grade/02907c27b76141709e5a6e9682fc836c)](https://www.codacy.com/app/jimkont/RDFUnit)  [![codebeat badge](https://codebeat.co/badges/fc781acc-0a9f-4796-9d33-28d1ffb3b019)](https://codebeat.co/projects/github-com-aksw-rdfunit)  [![Project Stats](https://www.openhub.net/p/RDFUnit/widgets/project_thin_badge.gif)](https://www.ohloh.net/p/RDFUnit)        **Homepage**: http://rdfunit.aksw.org <br/>  **Documentation**: https://github.com/AKSW/RDFUnit/wiki  <br/>  **Slack #rdfunit**: https://dbpedia-slack.herokuapp.com/ <br/>  **Mailing list**: https://groups.google.com/d/forum/rdfunit (rdfunit [at] googlegroups.com)  <br/>  **Presentations**: http://www.slideshare.net/jimkont  <br/>  **Brief Overview**: https://github.com/AKSW/RDFUnit/wiki/Overview      RDFUnit is implemented on top of the [Test-Driven Data Validation Ontology](http://rdfunit.aksw.org/ns/core#) and designed to read and produce RDF that complies to that ontology only.  The main components that RDFUnit reads are  [TestCases (manual & automatic), TestSuites](https://github.com/AKSW/RDFUnit/wiki/TestCases),  [Patterns & TestAutoGenerators](https://github.com/AKSW/RDFUnit/wiki/Patterns-Generators).  RDFUnit also strictly defines the results of a TestSuite execution along with [different levels of result granularity](https://github.com/AKSW/RDFUnit/wiki/Results).    ### Contents   * [Basic Usage](#basic-usage)   * [Using Docker](#using-docker)   * [Supported Schemas](#supported-schemas)   * [Acknowledgements](#acknowledgements)      ### Basic usage    See [RDFUnit from Command Line](https://github.com/AKSW/RDFUnit/wiki/CLI) or `bin/rdfunit -h` for (a lot) more options but the simplest setting is as follows:    ```console  $ bin/rdfunit -d <local-or-remote-location-URI>  ```    What RDFUnit will do is:    1. Get statistics about all properties & classes in the dataset  1. Get the namespaces out of them and try to dereference all that exist in [LOV](http://lov.okfn.org)  1. Run our [Test Generators](https://github.com/AKSW/RDFUnit/wiki/Patterns-Generators) on the schemas and generate RDFUnit Test cases  1. Run the RDFUnit test cases on the dataset  1. You get a results report in html (by default) but you can request it in [RDF](http://rdfunit.aksw.org/ns/core#) or even multiple serializations with e.g.  `-o html,turtle,jsonld`    * The results are by default aggregated with counts, you can request different levels of result details using `-r {status|aggregate|shacl|shacllite}`. See [here](https://github.com/AKSW/RDFUnit/wiki/Results) for more details.    You can also run:  ```console  $ bin/rdfunit -d <dataset-uri> -s <schema1,schema2,schema3,...>  ```    Where you define your own schemas and we pick up from step 3. You can also use prefixes directly (e.g. `-s foaf,skos`) we can get everything that is defined in [LOV](http://lov.okfn.org).    ### Using Docker    A Dockerfile is provided to create a Docker image of the CLI of RDFUnit.    To create the Docker image:    ```console  $ docker build -t rdfunit .  ```    It is meant to execute a rdfunit command and then shutdown the container. If the output of rdfunit on stdout is not enough or you want to include files in the container, a directory could be mounted via Docker in order to create the output/result there or include files.    Here an example of usage:    ```console  $ docker run --rm -it rdfunit -d https://awesome.url/file -r aggregate  ```    This creates a temporary Docker container which runs the command, prints the results on stdout and stops plus removes itself. For further usage of CLI visit https://github.com/AKSW/RDFUnit/wiki/CLI.    ### Supported Schemas    RDFUnit supports the following types of schemas    1. **OWL** (using CWA): We pick the most commons OWL axioms as well as schema.org. (see [[1]](https://github.com/AKSW/RDFUnit/labels/OWL),[[2]](https://github.com/AKSW/RDFUnit/issues/20) for details)  1. **SHACL**: Full SHACL is almost available except for [a few SHACL constructs](https://github.com/AKSW/RDFUnit/issues/62). Whatever constructs we support can also run directly on SPARQL Endpoints  1. IBM **Resource Shapes**: The progress is tracked [here](https://github.com/AKSW/RDFUnit/issues/23) but as soon as SHACL becomes stable we will drop support for RS  1. **DSP** (Dublin Core Set Profiles): The progress is tracked [here](https://github.com/AKSW/RDFUnit/issues/22) but as soon as SHACL becomes stable we will drop support for DSP    Note that you can mix all of these constraints together and RDFUnit will validate the dataset against all of them.    ### Acknowledgements    The first version  of RDFUnit (formely known as Databugger) was developed by AKSW as part of the PhD thesis of Dimitris Kontokostas.   A lot of additional work for improvement, requirements & refactoring was performed through the [EU funded project ALIGNED](http://aligned-project.eu). Through the project, a lot of project partners provided feedback and contributed code like e.g.  Wolters Kluwers Germany and Semantic Web Company that are also users of RDFUnit.    There are also many [code contributors](https://github.com/AKSW/RDFUnit/graphs/contributors) as well as people submitted bug reports or provided constructive feedback.    In addition, RDFUnit used [Java profiler (JProfiler)](http://www.ej-technologies.com/products/jprofiler/overview.html) for optimizations """
Semantic web;https://github.com/wbsg/sieve;"""sieve  =====    Sieve - Linked Data Quality Assessment and Fusion"""
Semantic web;https://github.com/anapsid/anapsid;"""ANAPSID  =======    An adaptive query processing engine for SPARQL endpoints.    [1] Maribel Acosta, Maria-Esther Vidal, Tomas Lampo, Julio Castillo,  Edna Ruckhaus: ANAPSID: An Adaptive Query Processing Engine for SPARQL  Endpoints. International Semantic Web Conference (1) 2011: 18-34    [2] Gabriela Montoya, Maria-Esther Vidal, Maribel Acosta: A  Heuristic-Based Approach for Planning Federated SPARQL Queries. COLD  2012    Installing ANAPSID  ==================    ANAPSID is known to run on Debian GNU/Linux and OS X. These instructions were tested   on the latest Debian Stable and OS X. The recommended way to  execute ANAPSID is to use Python 2.7.       1. Download ANAPSID.       You can do this by cloning this repository using Git.       `$ git clone https://github.com/anapsid/anapsid.git ~/anapsid`          OR       You can download the latest release from Github [here](https://github.com/anapsid/anapsid/releases)     2. Go to your local copy of ANAPSID and run:       `$ pip install -r requirements.txt`       This will install ANAPSID's Python dependencies. Right now, the only library required to execute ANAPSID is ply 3.3 (https://pypi.python.org/pypi/ply/3.3)    3. When step 2 is done you can now install ANAPSID. This will install     it only to your current user caged VirtualEnv as to prevent     polluting Python's global site-packages.       `$ python setup.py install`    4. Go ahead and move to the next section on configuring ANAPSID.    Setting up ANAPSID  ==================    Running ANAPSID depends on a endpoint description file. This file  describes each endpoint URL and the predicates this endpoint  handles. ANAPSID comes bundled with a helper script to generate your  endpoints descriptions as to prevent errors.    1. Create a file, e.g. endpointsURLs.txt, with the URLs of your     endpoints, one per line.    2. Run the script. It will contact each endpoint and retrieve their     predicates, so it might take a while. This will save your endpoint     descriptions on endpointsDescriptions.txt       `$ get_predicates endpointsURLs.txt endpointsDescriptions.txt`    3. You are ready to run ANAPSID.    About supported endpoints  ------------------------    ANAPSID currently supports endpoints that answer queries either on XML  or JSON. Expect hard failures if you intend to use ANAPSID on  endpoints that answer in any other format.    Running ANAPSID  ===============    Once you have installed ANAPSID and retrieved endpoint descriptions,  you can run ANAPSID using our run_anapsid script.    `$ run_anapsid`    It will output a usage text and the options switches you can  select. We run our experiments, however, using the scripts bundled on  utils/ so you might want to check that out to get an idea.    ANAPSID Parameters  ------------------  Alternatively,  you can execute the following command to run a given query with ANAPSID:    ```  $python $ANAPSIDROOT/run_anapsid -e $ENDPOINTS -q $query -p <planType> -s False   -o False -d <TypeofDecompostion> -a True -w False [-k <special>]  [-V <typeOfEndpoint>] -r False  ```    Where:    `$ANAPSIDROOT`: directory where ANAPSID is stored.    `$ENDPOINTS`: path and name of the file where the description of the endpoints is stored.    `$query`: path and name of the filw where the query is stored.    `<planType>`: can be **b** if the plan is bushy, **ll** is the plan is left linear, and **naive** for naive binary tree plan.     `-o`: can be True or False. **True** indicates that the input query is in SPARQL1-1 and no decomposition is needed; **False**, otherwise.    `-d`: indicates the type of Decomposition. <TypeofDecompostion> can be **SSGM** (Star Shaped  Group Multiple Endpoints), **SSGS** (Star Shaped Group Single Endpoint), **EG** (Exclusive Groups), Recommended SSGM.    `-a`: indicates if the adaptive operators will be used. Recommended value **True**.    `-w`:  can be True or False. Indicates if the cardinality of the queries will be estimated by contacting the sources (**True**) or by using a cost model (**False**). Turning True this feature may affect execution time.    `-w`: can be y or c. The value **y** indicates that the plan will be produced, while **c** asks that decomposition. This parameter is optional, and should be set up only if the plan of the query wants to be produced.     `-r`: can be True or False. Use **True** if the answer of the query will be output and **False** if only a summary of the execution will be produced.     `-V`: can be True or False. **True** indicates if the endpoints to contact are Virtuoso, **False** is of any other type, e.g., OWLIM.      Included query decomposing heuristics  =====================================    We include three heuristics used for decomposing queries to be  evaluated by a federation of endpoints. These are:    1. Exclusive Groups (EG).  2. Star-Shaped Group Single endpoint selection (SSGS). See [2].  3. Star-Shaped Group Multiple endpoint selection (SSGM). See [2].     Running FedBench with ANAPSID  =============================  FedBench (see http://fedbench.fluidops.net) is a benchmark for testing federated query processing on RDF data sets.    In order to execute ANAPSID, it is necessary first to provide the endpoint descriptions. Endpoint descriptions are of the form `<URLEndpoint> <LISTOfPredicates>`. The file endpoints/endpointsFedBench provides   the description of the endpoints of the dataset collections in FedBench. The current URLs of the endpoints  have to be included as follows:  ```   <http://URLnytimes_dataset/sparql> URL of the NYTime endpoint   <http://URLchebi_dataset/sparql> URL of the Chebi endpoint   <http://URLSWDF_dataset/sparql> URL of the SW Dog Food endpoint   <http://URLdrugbank_dataset/sparql> URL of the Drugbank endpoint   <http://URLjamendo_dataset/sparql> URL of the Jamendo endpoint   <http://URLkegg_dataset/sparql> URL of the Kegg endpoint   <http://URLlinkedmdb_dataset/sparql> URL of the LinkedMDB endpoint   <http://URLSP2B/sparql> URL of the SP^2Bench 10M endpoint   <http://URLgeonames/sparql> URL of the Geonames endpoint  ```    The FedBench queries (see http://fedbench.fluidops.net/resource/Queries) are also available in the folder queries/fedbBench.      About and Contact  =================    ANAPSID was developed at  [Universidad Simón Bolívar](http://www.usb.ve) as an ongoing academic effort. You  can contact the current maintainers by email at mvidal[at]ldc[dot]usb[dot]ve.    We strongly encourage you to please report any issues you have with  ANAPSID. You can do that over our contact email or creating a new  issue here on Github.    - Simón Castillo: scastillo [at] ldc [dot] usb [dot] ve  - Guillermo Palma: gpalma [at] ldc [dot] usb [dot] ve  - Maria-Esther Vidal: mvidal [at] ldc [dot] usb [dot] ve  - Gabriela Montoya: Gabriela [dot] Montoya [at] univ-nantes [dot] fr  - Maribel Acosta: maribel [dot] acosta [at] kit [dot] edu      License  =======    This work is licensed under [GNU/GPL v2](https://www.gnu.org/licenses/gpl-2.0.html). """
Semantic web;https://github.com/andrewdbate/Sequoia;"""[![License: GPL v3](https://img.shields.io/badge/license-GNU%20GPL%20v3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)  [![Build Status](https://travis-ci.org/andrewdbate/Sequoia.svg?branch=master)](https://travis-ci.org/andrewdbate/Sequoia)  [![Build Status](https://ci.appveyor.com/api/projects/status/github/andrewdbate/Sequoia?branch=master&svg=true)](https://ci.appveyor.com/project/andrewdbate/sequoia)    # Sequoia: An Open Source OWL 2 DL Reasoner for Java    Welcome to the official repository for Sequoia!    Sequoia is an ontology reasoner that supports OWL 2 DL ontologies. Sequoia can be used from the command line, through  the Protégé plug-in, or via the OWL API.    Feel free to fork this repository and submit pull requests.    If you discover a bug, please report it [here](https://github.com/andrewdbate/Sequoia/issues) on GitHub.    Sequoia is free and open-source software that is licensed under the [GNU GPL v3](https://github.com/andrewdbate/Sequoia/blob/master/LICENSE) only.    _Note:_ To compile Sequoia you will need JDK 8 installed. Compiling with Java 9 is not supported at this time.    #### Current Limitations   * Datatypes are not supported.   * Neither nominals nor ABox assertions are supported.   * SWRL rules are not supported.    ## Publications  The following publications describe the algorithms and implementation of Sequoia.  1. Andrew Bate‚ Boris Motik‚ Bernardo Cuenca Grau‚ František Simančík, Ian Horrocks.     [**Extending Consequence−Based Reasoning to SRIQ**](https://www.cs.ox.ac.uk/files/8182/paper.pdf).     In _Principles of Knowledge Representation and Reasoning: Proceedings of the Fifteenth International Conference_.     Pages 187–196. AAAI Press. 2016.     **(Main reference on Sequoia)**          [Paper (PDF)](https://www.cs.ox.ac.uk/files/8182/paper.pdf) | [Slides (PDF)](https://www.cs.ox.ac.uk/files/8181/slides.pdf)       2. Andrew Bate‚ Boris Motik‚ Bernardo Cuenca Grau‚ František Simančík, Ian Horrocks.     [**Extending Consequence−Based Reasoning to SRIQ (Technical Report)**](https://arxiv.org/abs/1602.04498).     arXiv:1602.04498 \[cs.AI\]. February, 2016.          [Technical Report (PDF)](https://arxiv.org/pdf/1602.04498.pdf)       3. Andrew Bate‚ Boris Motik‚ Bernardo Cuenca Grau‚ František Simančík, Ian Horrocks.     [**Extending Consequence−Based Reasoning to SHIQ**](https://www.cs.ox.ac.uk/files/7444/paper.pdf)     In _Proceedings of the 28th International Workshop on Description Logics_.     Vol. 1350 of CEUR Workshop Proceedings. CEUR−WS.org. 2015.       [Paper (PDF)](https://www.cs.ox.ac.uk/files/7444/paper.pdf) | [Technical Report (PDF)](https://www.cs.ox.ac.uk/files/7864/techreport.pdf)    ## Development and Building    Sequoia is developed by [Andrew Bate](https://www.linkedin.com/in/andrewdbate/) and is actively maintained.    To build Sequoia, you will need [SBT](https://www.scala-sbt.org/) installed on your system.    The Sequoia reasoner is comprised of multiple subprojects:   * `reasoner-macros` contains Scala macros used throughout the other subprojects.   * `reasoner-kernel` contains the implementation of the core algorithm and data structures.   * `reasoner-owl-api` contains the implementation of the OWL API bindings.   * `reasoner-cli` contains the implementation of the command-line interface of the reasoner.   * `reasoner-protege-plugin` contains the implementation of the Protégé plugin.    To compile all subprojects and run all tests in a single command, first clone this repository,  and then from the root directory, type `sbt test`. The tests will take several minutes to complete.    #### Building the Command-Line Client  From the root directory, type `sbt` to launch the SBT REPL. Then type `project cli` and hit Enter, followed by `universal:packageBin` and hit Enter.    #### Building the Protégé Plugin  From the root directory, type `sbt` to launch the SBT REPL. Then type `project protegeplugin` and hit Enter, followed by `osgiBundle` and hit Enter.    ### Acknowledgements    Previous versions of Sequoia were developed at the  [Knowledge Representation and Reasoning group](https://www.cs.ox.ac.uk/activities/knowledge/)  at the  [Department of Computer Science](https://www.cs.ox.ac.uk/)  of the  [University of Oxford](https://www.ox.ac.uk). """
Semantic web;https://github.com/gh-rdf3x/gh-rdf3x;"""GH-RDF3X  ========  *GH-RDF3X* is a more complete and modified version of the original *RDF-3X* engine.    Changes over original version ( *rdf3x* ):    - **OPTIONAL** clause was implemented.    - new **GJOIN** clause was added.    - fixed bugs in **lang**, **langMatches**, **bound** and aritmetics relationals operators.    - *translatesparql* now translate more complex queries from SPARQL to SQL for postgresql and monetdb.    RDF-3X was created by: Thomas Neumann. Web site: http://www.mpi-inf.mpg.de/~neumann/rdf3x (c) 2008   RDF-3X was modified by: Hancel Gonzalez and Giuseppe De Simone (Advisor: Maria Esther Vidal). http://github.com/gh-rdf3x/gh-rdf3x (c) 2013 """
Semantic web;https://github.com/robstewart57/rdf4h;"""rdf4h - An RDF library for Haskell  =====    [![Available on Hackage][badge-hackage]][hackage]  [![License BSD3][badge-license]][license]  [![Build Status][badge-travis]][travis]    [badge-travis]: https://travis-ci.org/robstewart57/rdf4h.png?branch=master  [travis]: https://travis-ci.org/robstewart57/rdf4h  [badge-hackage]: https://img.shields.io/hackage/v/rdf4h.svg  [hackage]: http://hackage.haskell.org/package/rdf4h  [badge-license]: https://img.shields.io/badge/license-BSD3-green.svg?dummy  [license]: https://github.com/robstewart57/rdf4h/blob/master/LICENSE.txt    rdf4h is a library for working with RDF in Haskell.    For details see the GitHub project page:    http://robstewart57.github.io/rdf4h/    Supports GHC versions from 8.0.2 (stackage lts-9) to 8.8.3 (stackage lts-16.0).    ### Development with Nix and direnv    To enter a development environment, you can use [Nix](https://nixos.org/download.html) and [direnv](https://github.com/direnv/direnv) which install all required software, also allowing you to use your preferred shell. Once installed, just run    ```shell  $ direnv allow  ```    and you'll have a working development environment for now and the future whenever you enter this directory.    This development environment allows to use either Stack or Cabal for building the software.    ### RDF formats    The coverage of the W3C RDF standards are:    Format | Parsing | Serialising  --- | --- | ---  NTriples | complete | complete  Turtle | complete | complete  RDF/XML | complete | not supported    These results are produced with version 4.0 of this library.    These tests are run on the W3C unit tests for RDF formats: https://github.com/w3c/rdf-tests.    ### Feature requests    1. The parsers in this library parse large files/strings contents     entirely before generating RDF triples. This doesn't scale for very     large files. Implementing stream based RDF parsers would overcome     this problem, e.g. by creating input streams enabling output     streams in the     [io-streams](http://hackage.haskell.org/package/io-streams) library     to consume triples on-the-fly during parsing. This is discussed     here:     https://github.com/robstewart57/rdf4h/issues/56#issuecomment-497892024 and     https://github.com/robstewart57/rdf4h/issues/44#issuecomment-426054978    2. RDF/XML serialisation of RDF graphs.    ### Running tests    To run all the tests (parsers and the library API):    ```shell  $ git submodule update --init --recursive  $ git submodule foreach git pull origin gh-pages  $ stack test --test-arguments=""--quickcheck-tests 1000""  ```    To run specific parser tests when bug fixing:    ```shell  $ stack test --test-arguments=""--pattern /parser-w3c-tests-ntriples/""  $ stack test --test-arguments=""--pattern /parser-w3c-tests-turtle/""  $ stack test --test-arguments=""--pattern /parser-w3c-tests-xml/""  ```    ### Running benchmarks    To run the bencharks:    ```shell  $ wget https://www.dropbox.com/s/z1it340emcreowj/bills.099.actions.rdf  $ gzip -d bills.099.actions.rdf.gz  $ stack bench  ``` """
Semantic web;https://github.com/sebneu/csvw-parser;"""# pycsvw    Python implementation of the W3C CSV on the Web specification, cf. http://w3c.github.io/csvw/      ## Authors    - Sebastian Neumaier  - Jürgen Umbrich  - Mao Li """
Semantic web;https://github.com/earthquakesan/fox-py;"""fox-py  ======    Python bindings for FOX - Federated Knowledge Extraction Framework    For installing to virtualenv:  ```  pip install -e git+https://github.com/earthquakesan/fox-py#egg=fox-py  ``` """
Semantic web;https://github.com/scholtzan/rdf-rs;"""# rdf-rs    > Note: This project is work in progress and currently not stable.    `rdf` is a library for the [Resource Description Framework](https://www.w3.org/RDF/) (RDF) and [SPARQL](https://www.w3.org/TR/rdf-sparql-query/) implemented in Rust.    This project is a way for me to learn Rust and combine it with my interests in semantic web technologies.    ### Usage  Add this to your Cargo.toml:    ```toml  [dependencies]  rdf = ""0.1.4""  ```      ### Basic Examples      RDF triples can be stored and represented in a graph.    ```rust  use rdf::graph::Graph;  use rdf::uri::Uri;  use rdf::triple::Triple;    let mut graph = Graph::new(None);  let subject = graph.create_blank_node();  let predicate = graph.create_uri_node(&Uri::new(""http://example.org/show/localName"".to_string()));  let object = graph.create_blank_node();  let triple = Triple::new(&subject, &predicate, &object);    graph.add_triple(&triple);  ```    RDF graphs can be serialized to a supported format.    ```rust  use rdf::writer::n_triples_writer::NTriplesWriter;  use rdf::writer::rdf_writer::RdfWriter;  use rdf::graph::Graph;  use rdf::uri::Uri;  use rdf::triple::Triple;    let writer = NTriplesWriter::new();    let mut graph = Graph::new(None);  let subject = graph.create_blank_node();  let predicate = graph.create_uri_node(&Uri::new(""http://example.org/show/localName"".to_string()));  let object = graph.create_blank_node();  let triple = Triple::new(&subject, &predicate, &object);    graph.add_triple(&triple);  assert_eq!(writer.write_to_string(&graph).unwrap(),             ""_:auto0 <http://example.org/show/localName> _:auto1 .\n"".to_string());  ```    RDF syntax can also be parsed and transformed into an RDF graph.    ```rust  use rdf::reader::turtle_parser::TurtleParser;  use rdf::reader::rdf_parser::RdfParser;  use rdf::uri::Uri;    let input = ""@base <http://example.org/> .  @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .  @prefix foaf: <http://xmlns.com/foaf/0.1/> .    <http://www.w3.org/2001/sw/RDFCore/ntriples/> rdf:type foaf:Document ;          <http://purl.org/dc/terms/title> \""N-Triples\""@en-US ;          foaf:maker _:art ."";    let mut reader = TurtleParser::from_string(input.to_string());  match reader.decode() {    Ok(graph) => {      assert_eq!(graph.count(), 3);      assert_eq!(graph.namespaces().len(), 2);      assert_eq!(graph.base_uri(), &Some(Uri::new(""http://example.org/"".to_string())))    },    Err(_) => assert!(false)  }  ```    ## Current State    Currently `rdf-rs` provides basic data structures for representing RDF graphs, triples and nodes.  The following formats can be parsed and serialized:    * Turtle  * N-Triples      ## Future Work and Ideas    * Support querying with SPARQL  * Add support for more formats  * More comprehensive `Uri` data structure """
Semantic web;https://github.com/JeniT/linked-csv;"""# Linked CSV    Many open data sets are essentially tables, or sets of tables, which follow the same regular structure. Linked CSV is a set of conventions for CSV files that enable them to be linked together and to be interpreted as JSON, XML or RDF.    This repository contains the (work in progress) spec for linked CSV.   """
Semantic web;https://github.com/labra/shaclex;"""# SHaclEX    Scala implementation of SHEX and SHACL.    This project contains an implementation of  [SHACL](http://w3c.github.io/data-shapes/shacl/) and  [ShEx](http://www.shex.io)    [![Build Status](https://travis-ci.org/weso/shaclex.svg?branch=master)](https://travis-ci.org/weso/shaclex)  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/1c264d2087734a80b4cecf071bb5eaad)](https://www.codacy.com/gh/weso/shaclex?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=weso/shaclex&amp;utm_campaign=Badge_Grade)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1214239.svg)](https://doi.org/10.5281/zenodo.1214239)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/es.weso/shaclex_2.13/badge.svg)](https://maven-badges.herokuapp.com/maven-central/es.weso/shaclex_2.13)    ## Introduction    This project contains an implementation of [SHACL](https://www.w3.org/TR/shacl/) and [ShEx](http://shex.io/).    Both are implemented in Scala using the same underlying mechanism using a purely functional approach.    The library handles RDF using a  [simple RDF library](https://github.com/weso/srdf)  which has 2 implementations,  one using [Apache Jena](https://jena.apache.org/)  and another one using [RDF4j](http://rdf4j.org/),  this means that it is possible to use this library to validate RDF models from any of those RDF libraries,   as well as from external SPARQL endpoints.    ## Installation and compilation    The project uses [sbt](http://www.scala-sbt.org/) for compilation as well as Java 1.8.    * `sbt test` compiles and runs the tests    ## Command line usage    Once compiled, the program can be run as a command line tool.  It is possible to run the program inside `sbt` as:    ### Validating RDF data with SHACL    Example:    ```sh  sbt ""run --data examples/shacl/good1.ttl \           --engine shaclex \           --showValidationReport""  ```    It is also possible to use [Jena SHACL](https://jena.apache.org/documentation/shacl/) using:      ```sh  sbt ""run --data examples/shacl/good1.ttl \           --engine JenaSHACL \           --showValidationReport""  ```    or [Top Braid SHACL API] using:    ```sh  sbt ""run --data examples/shacl/good1.ttl \           --engine shacl-tq \           --showValidationReport""  ```      ### Validating RDF with ShEx     Example:    ```sh  sbt ""run --engine=ShEx            --schema examples/shex/good1.shex            --schemaFormat ShExC            --data examples/shex/good1.ttl""  ```    ### Validating RDF data through an SPARQL endpoint    The following example validates RDF nodes from wikidata using [Gene-wiki ShEx](https://github.com/SuLab/Genewiki-ShEx):    ```sh  sbt ""run --endpoint=https://query.wikidata.org/sparql            --schemaUrl=https://raw.githubusercontent.com/SuLab/Genewiki-ShEx/master/diseases/wikidata-disease-ontology.shex            --shapeMap=examples/shex/wikidata/disease1.shapeMap            --schemaFormat=ShExC            --engine=ShEx            --trigger=ShapeMap            --showResult            --resultFormat=JSON""  ```          ### Interactive mode with `sbt`     It is usually faster to run the `sbt` command, which opens the interactive `sbt` shell and inside that shell, execute   the different commands.     ```sh  $ sbt  ... several information about loading libraries  sbt> run -d examples/shacl/good1.ttl --engine ShaClex    ```    ### Binary mode    The fastest way to run Shaclex is to compile the code and generate a comand line binary file.     The following command will generate a binary file:    ```sh  $ sbt universal:packageBin  ...generates the file...  target/universal/shaclex-N.N.N.zip  ```    which contains the compressed binary code.    ## Programmatic usage    The Shaclex library can be invoked programmatically.    * [Simple project validating ShEx](https://github.com/weso/simpleShExScala)  * Simple example validating SHACL    ## Implementation details    * The engine is based on Monads using the [cats library](http://typelevel.org/cats/)  * The ShEx compact syntax parser    is implemented using the following [Antlr grammar](https://github.com/shexSpec/grammar/blob/master/ShExDoc.g4) (previous versions used Scala Parser Combinators)    which is based on this [grammar](https://github.com/shexSpec/shex.js/blob/master/doc/bnf)  * JSON encoding and decoding uses the Json structure [defined here](https://shexspec.github.io/spec/) and is implemented using [Circe](https://github.com/travisbrown/circe)    ## Compatibility tests    The current implementation passes all [shacl-core tests](https://w3c.github.io/data-shapes/data-shapes-test-suite/).    In order to generate the EARL report, run:      ```  $ sbt   [...]  sbt:shaclex> project shacl   sbt:shacl> testOnly es.weso.shacl.report.ReportGeneratorCompatTest  ```     We also aim to pass the [ShEx test-suite](https://github.com/shexSpec/shexTest).    In order to run the shex test-suite and generate the EARL report, you can do the following:    ```  sbt  ...  sbt:shaclex> project shex  sbt:shex> compat:test  ```    ## Convert between Schema formats    Shaclex can be used to convert between different schema formats.   The following example shows how to convert between ShExC to ShExJ:    ```  $ sbt ""run --schema examples/shex/good1.shex              --schemaFormat ShExC             --outSchemaFormat ShExJ             --showSchema""  ```    ## Convert between ShEx and SHACL    Shaclex can be used to convert schemas from ShEx to SHACL and viceversa.    The following example shows how to convert a SHACL schema to ShEx.     ```  $ sbt ""run --schema examples/shacl/good1.ttl              --schemaFormat Turtle              --outSchemaFormat ShExC              --engine SHACLEX              --outEngine SHEX              --showSchema              --no-validate""  ```    The conversion code is work in progress. This [issue tracks ShEx->SHACL conversion](https://github.com/weso/shaclex/issues/114) and this one tracks [SHACL->ShEx    conversion](https://github.com/weso/shaclex/issues/113).       ## Clingo validation    The project supports experimental Answer Set Programming based validation by converting the validation process to a [Clingo program])(https://potassco.org/clingo/). To run this, use the option `--showClingo` which will generate a Cling program. Example:    ```sh  sbt ""run --engine=ShEx            --schema examples/shex/good1.shex            --schemaFormat ShExC            --data examples/shex/good1.ttl           --showClingo           --clingoFile clingoProgram.pl""   ```    Once you generate the Clingo program and have installed Clingo itself, you can run the program with:    ```sh  clingo clingoProgram.pl  ```    This feature is experimental. This [issue tracks the Clingo conversion](https://github.com/weso/shaclex/issues/316).    ## More information    * The aim of Shaclex is to support both ShEx and SHACL and to provide conversions between both languages.     More information about both languages can be read in the [Validating RDF data](http://book.validatingrdf.com) written by the authors.  * An online demo based on this library is available at [http://rdfshape.weso.es](http://rdfshape.weso.es).  * Another online demo based on this library customized for Wikidata is available at [http://wikidata.weso.es](http://wikidata.weso.es).  * This project was based on [ShExcala](http://labra.github.io/ShExcala/) which was focused on Shape Expressions only.    ## Publishing to OSS-Sonatype    This project uses [the sbt ci release](https://github.com/olafurpg/sbt-ci-release) plugin for publishing to [OSS Sonatype](https://oss.sonatype.org/).    ##### SNAPSHOT Releases  Open a PR and merge it to watch the CI release a -SNAPSHOT version    ##### Full Library Releases  1. Push a tag and watch the CI do a regular release  2. `git tag -a v0.1.0 -m ""v0.1.0""`  3. `git push origin v0.1.0`  _Note that the tag version MUST start with v._    ## Author & contributors    * Author: [Jose Emilio Labra Gayo](http://labra.weso.es)    Contributors:    * [Eric Prud'hommeaux](https://www.w3.org/People/Eric/)  * [Bogdan Roman](https://github.com/bogdanromanx)  * [Toni Cebrían](http://www.tonicebrian.com/)  * [Andrew Berezovskyi](https://github.com/berezovskyi)    ## Adopters    * [RDFShape](http://rdfshape.weso.es): An online demo powered by this library.  * [Wikishape](http://wikishape.weso.es): An online demo powered by this library for Wikidata.  * [Eclipse lyo](http://www.eclipse.org/lyo/): An SDK and a modelling environment to design and develop linked data applications based on the [OSLC standards](http://open-services.net/). The validation library is [lyo-validation](https://github.com/eclipse/lyo-validation).    ## Contribution    Contributions are greatly appreciated.  Please fork this repository and open a  pull request to add more features or [submit issues](https://github.com/labra/shaclex/issues)      <a href=""https://github.com/weso/shaclex/graphs/contributors"">    <img src=""https://contributors-img.web.app/image?repo=weso/shaclex"" />  </a>    ```Made with [contributors-img](https://contributors-img.web.app). """
Semantic web;https://github.com/vocol/vocol;"""  VoCol - Vocabulary collaboration and build environment.  =====    Inspired by agile software and content development methodologies, the VoCol methodology and tool environment allows building leight-weight ontologies using version control systems such as Git and repository hosting platforms such as Github.   VoCol is implemented without dependencies on complex software components, it provides collaborators with comprehensible feedback on syntax and semantics errors in a tight loop, and gives access to a human-readable presentation of the vocabulary.   The VoCol environment is employing loose coupling of validation and documentation generation components on top of a standard Git repository.   All VoCol components, even the repository engine, can be exchanged with little effort.       ## Installation on a local machine or on a Web Server    The following steps are needed to setup the VoCol Environment either on a local machine or a web server. These steps are valid in the Linux-based operating systems and with slight modifications can be used in Windows-based as well.    1. You should have the following libraries installed: Java JDK, NodeJS, NPM, and Git packages with their respective versions or higher. For more info see in **[Required libraries and tools](https://github.com/vocol/vocol/wiki/Required-libraries-and-tools)**.     2. Create a new directory e.g. ""newFolder"", clone the VoCol repository, and give the execution permissions as follows:  ```  mkdir newFolder  cd newFolder  git clone https://github.com/vocol/vocol.git  chmod u+x  .  ```  4. Enter inside the ""VoCol"" folder and run the following script to clean up any not necessary file:  ```  cd vocol  ./helper/scripts/resetApp.sh  ```  5. Install the dependent packages (assuming that node package manager is installed already):  ```  sudo npm install  ```  Semantic-Ui framework is used in VoCol development, a couple of selections need to be given while installing it.   Select ""Skip install"" as follows:   ```  ? It looks like you have a semantic.json file already.    Yes, extend my current settings.  > Skip install  ```  Then ""Yes"" for that Vocol is using ""NPM Nice"".  ```  ? We detected you are using NPM Nice! Is this your project folder? D:\vocolrepo\vocol  > Yes    No, let me specify  ```  Finally, give ""public/semantic"" as the location of Sematic-Ui in VoCol Project.  ```  ? Where should we put Semantic UI inside your project? (semantic/) public/semantic/  ```  6. The last step is to start VoCol with **npm start [VocolPortNumber] [SparqlEndPointPortNumber]**. In the following command, we are going to start Vocol on port 3000 where Fuseki Server is runing at port 3030    ```  npm start 3000 3030  ```  8. You can access VoCol start page with http://localhost:3000 , if the port number was not changed. If you clear old data as step 4 describes, then the configuration page will be displayed. Otherwise, you can use http://localhost:3000/config URL for configuring of the VoCol. Sometimes, the port number is also changed during our project's development, for that, you have a possibility to look-up the vocol access's port number and as well change it, by opening **bin/www** file if you are on the root path of VoCol.    9. To keep your repository synchronized with VoCol instance (for example when you push something), you should configure **a webhook path** on the repository hosting platform such as Github, GitLab and BitBucket to point with the VoCol API: **http(s)://hostname(:port or /vocolInstancePath)/listener**. The connection between both hosting server and VoCol instance should be available in such a way that hosting platform can send the notification to the VoCol instance. Please the fundamental explanations of WebHooks in the following link: [https://developer.github.com/webhooks/](https://developer.github.com/webhooks/).    For more details about VoCol repository, please have a look on our [VoColWiki](https://github.com/vocol/vocol/wiki).        Check out a list of projects that are currently using [VoCol](https://vocol.iais.fraunhofer.de/).    Moreover, you can use the **docker image** of VoCol [here](https://hub.docker.com/r/ahemid/newvocol/) or use the included Dockerfile to build the docker image.    ## License    VoCol is licensed under the MIT License. See LICENSE.txt for more details. For respective licenses of individual components and libraries please refer to the **[Required libraries and tools](https://github.com/vocol/vocol/wiki/Required-libraries-and-tools)** section.     ## Current Work   We have extened this VoCol version to work As A Service in VoCoREG. Please, visit us on the following link: https://www.vocoreg.org or https://www.vocoreg.com   """
Semantic web;https://github.com/MakoLab/RomanticWeb;"""# [![romantic web logo](http://romanticweb.net/images/logo.png)](http://romanticweb.net/) [![teamcity build status](http://ci.t-code.pl/app/rest/builds/buildType:bt12/statusIcon)](http://ci.t-code.pl/viewType.html?buildTypeId=bt12)     ## Relational Object Model for SemanticWeb in .net!    While the hype about the Semantic Web technologies is rising, making the developers working with Resource Description Framework (RDF) data representing a graph might be not a good solution.    RomanticWeb is the world’s first ORM class solution for graph-based data written fully in C# that allows developers to work with the RDF data in a way the would work with any other data in an object oriented manner. This can be achieved by creating data models that can then be mapped to RDF statements in a fully transparent way.    RomanticWeb is also the first solution that in conjuction with it’s mapping abilities allows to query for the data in a native .net way with LINQ. Developers can use their natural approach while working with data and objects and query for them with strongly typed queries, which are then translated into a SPARQL Protocol And RDF Query Language.    Now it’s possible to work with the full power of elastic RDF data sets with a simplicity of a classic object oriented programming!    ============    __Read more on the [project page](http://romanticweb.net/)__ """
Semantic web;https://github.com/stain/profilechecker;"""# OWL API profile checker    (c) 2012-2017 The University of Manchester    License: [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)     (see `LICENSE` and `NOTICE` for required notices)    Author: Stian Soiland-Reyes <soiland-reyes@manchester.ac.uk>      ## Requirements    * [Java](https://java.com/en/download/) 8 or [OpenJDK](http://openjdk.java.net/) 8  * [Apache Maven](https://maven.apache.org/download.cgi) 3.3 or later       ## Building    Note: you don't need to compile from source code, you can also use one of the   [releases](https://github.com/stain/profilechecker/releases).          stain@ralph-ubuntu:~/src/profilechecker$ mvn clean package      [INFO] Scanning for projects...      [INFO]                                                                               [INFO] ------------------------------------------------------------------------      [INFO] Building OWL API profile checker 1.1.0      [INFO]       (..)      [INFO] Replacing /home/stain/src/profilechecker/target/profilechecker-1.1.0.jar with /home/stain/src/profilechecker/target/profilechecker-1.0-shaded.jar      [INFO] Dependency-reduced POM written at: /home/stain/src/profilechecker/dependency-reduced-pom.xml      [INFO] ------------------------------------------------------------------------      [INFO] BUILD SUCCESS      [INFO] ------------------------------------------------------------------------      [INFO] Total time: 29.912s      [INFO] Finished at: Thu Feb 07 15:34:12 GMT 2013      [INFO] Final Memory: 22M/169M      [INFO] ------------------------------------------------------------------------        ## Usage    ### Help        $ java -jar target/profilechecker-1.1.0.jar -h      Usage: profilechecker.jar <ontology.owl> [profile]            Available profiles:      OWL2_DL (OWL 2 DL)      OWL2_QL (OWL 2 QL)      OWL2_EL (OWL 2 EL)      OWL2_RL (OWL 2 RL)      OWL2_FULL (OWL 2 DL) -default-      --all    (Modify the version number `1.1.0` above to correspond to the output of your build)    The `<ontology.owl>` parameter can be given as a local file name or an  absolute IRI.    ### Default profile    With only ontology IRI or file name, will check against default profile  (OWL 2 Full):        $ java -jar target/profilechecker-1.1.0.jar https://cdn.rawgit.com/owlcs/pizza-ontology/v1.5.0/pizza.owl    Exit code is 0 if the ontology conforms to OWL 2 Full, in which case there should be no output on STDERR.          ### Specify OWL2 profile    Checking against a specific profile:            $ java -jar target/profilechecker-1.1.0.jar https://cdn.rawgit.com/owlcs/pizza-ontology/v1.5.0/pizza.owl OWL2QLProfile        Use of non-superclass expression in position that requires a        superclass expression:        ObjectAllValuesFrom(<http://www.co-ode.org/ontologies/pizza/pizza.owl#hasTopping>        ObjectUnionOf(<http://www.co-ode.org/ontologies/pizza/pizza.owl#MozzarellaTopping>        <http://www.co-ode.org/ontologies/pizza/pizza.owl#TomatoTopping>))        [SubClassOf(<http://www.co-ode.org/ontologies/pizza/pizza.owl#Margherita>        ObjectAllValuesFrom(<http://www.co-ode.org/ontologies/pizza/pizza.owl#hasTopping>        ObjectUnionOf(<http://www.co-ode.org/ontologies/pizza/pizza.owl#MozzarellaTopping>        <http://www.co-ode.org/ontologies/pizza/pizza.owl#TomatoTopping>)))        in <http://www.co-ode.org/ontologies/pizza/pizza.owl>]       (..)    Exit code is 0 if the ontology conforms to the specified profile, with errors logged to STDERR.    The ontology profile can be specified in any of these forms (in order of preference):    * `OWL2_DL` ([`Profiles`](http://owlcs.github.io/owlapi/apidocs_5/org/semanticweb/owlapi/profiles/Profiles.html) enum value)  * `http://www.w3.org/ns/owl-profile/DL` (IRI of OWL profile)  * `DL` (relative IRI of OWL profile)  * `OWL2DLProfile` (classname)      ### All profiles    Checking against all profiles:          $ java -jar target/profilechecker-1.1.0.jar https://cdn.rawgit.com/owlcs/pizza-ontology/v1.5.0/pizza.owl --all      OWL2_DL: OK      OWL2_QL: 52 violations      OWL2_EL: 66 violations      OWL2_RL: 188 violations      OWL2_FULL: OK      Exit code is 0 if the ontology conforms to all profiles.   The violation count per profile is output to STDOUT.      ### Warnings    Note that any warnings or errors logged from the OWLAPI (prefix `[main]`)  during ontology loading do not necessarily mean violation against the profile:        $ java -jar target/profilechecker-1.1.0.jar ~/Desktop/annotated.ttl --all      [main] ERROR uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl - Illegal redeclarations of entities: reuse of entity http://example.com/annotatedOntology#property1 in punning not allowed [Declaration(AnnotationProperty(<http://example.com/annotatedOntology#property1>)), Declaration(ObjectProperty(<http://example.com/annotatedOntology#property1>))]      OWL2_DL: 1 violations      OWL2_QL: 1 violations      OWL2_EL: 1 violations      OWL2_RL: 1 violations      OWL2_FULL: OK     """
Semantic web;https://github.com/zazuko/rdf-vocabularies;"""# @zazuko/rdf-vocabularies -- Zazuko's Default Ontologies & Prefixes  [![Build Status](https://travis-ci.org/zazuko/rdf-vocabularies.svg?branch=master)](https://travis-ci.org/zazuko/rdf-vocabularies)  [![Coverage Status](https://coveralls.io/repos/github/zazuko/rdf-vocabularies/badge.svg?branch=master)](https://coveralls.io/github/zazuko/rdf-vocabularies?branch=master)  [![npm version](https://badge.fury.io/js/%40zazuko%2Frdf-vocabularies.svg)](https://www.npmjs.com/package/@zazuko/rdf-vocabularies)    This package contains a distribution of the most commonly used RDF ontologies (schema/vocab, whatever you call it)  including their default prefixes, together with a set of utility functions to work with prefixes.    It is extending [RDFa Core Initial Context](http://www.w3.org/2011/rdfa-context/rdfa-1.1) and contains what we consider  commonly used prefixes. Some popular prefixes do not resolve to dereferencable RDF and are thus skipped.    The package is built for use in Node.js projects. We ship N-Quads files of the vocabularies so it could be useful for  other programming languages as well as you do not have to take care of downloading the ontologies yourself.    ## Installation    ```bash  $ npm install @zazuko/rdf-vocabularies  ```    ## Usage    (Read below and take a look at some [examples](./examples.js).)    ### Dataset-as-code modules    All vocabularies published by this package are also exported as JS modules so that then can be imported synchronously (no parsing required) and without additional dependencies when in web app setting (see the `raw-loader` instructions below).    Modules `@rdf-vocabularies/datasets` exports factories which returns an array of quads `Quad` and take RDF/JS `DataFactory` as parameter.    ```javascript  const $rdf = require('rdf-ext')  const { schema } = require('@zazuko/rdf-vocabularies/datasets')    const dataset = $rdf.dataset(schema($rdf))  ```    In a bundled web project it is also possible to directly import a single dataset like `import schema from '@zazuko/rdf-vocabularies/datasets/schema'`. At the time of writing this is not supported by newer versions of node (12-14) but has already been fixed and scheduled for release.    ### Vocabularies Metadata    See [`_index.nq`](./ontologies/_index.nq).    ### `vocabularies()`    The function (`require('@zazuko/rdf-vocabularies').vocabularies(options)`) accepts an optional `options` object:    * `options.only: Array?`, default: `undefined`, a subset of all available prefixes, will only load these.  * `options.factory: RDF/JS DatasetFactory`, default: [`rdf-ext`](https://github.com/rdf-ext/rdf-ext), a dataset  factory abiding by the [RDF/JS Dataset Specification](https://rdf.js.org/dataset-spec/), used to create the  returned datasets.  * `options.stream: Boolean`, default: `false`, whether to return a RDF/JS quad stream instead of regular objects/datasets.    #### Loading all Vocabularies as Datasets    In browser environment this will cause a request for each individual dataset.  It is thus recommended to always only [load the needed ontologies](#loading-only-some-ontologies-as-datasets)  to reduce the unnecessary traffic and save bandwidth.    ```js  const { vocabularies } = require('@zazuko/rdf-vocabularies')    vocabularies()    .then((datasets) => {      /* `datasets` is:      {        ""csvw"": Dataset,        ""sd"": Dataset,        ""ldp"": Dataset,        ""schema"": Dataset,        ""owl"": Dataset,        ""void"": Dataset,        ""sioc"": Dataset,        ""foaf"": Dataset,        ""time"": Dataset,        ""dcat"": Dataset,        ""oa"": Dataset,        ""gr"": Dataset,        ""rdf"": Dataset,        ""cc"": Dataset,        ""ssn"": Dataset,        ""rr"": Dataset,        ""rdfa"": Dataset,        ""org"": Dataset,        ""sosa"": Dataset,        ""dc11"": Dataset,        ""skos"": Dataset,        ""dqv"": Dataset,        ""prov"": Dataset,        ""og"": Dataset,        ""qb"": Dataset,        ""rdfs"": Dataset,        ""dc"": Dataset,        ""ma"": Dataset,        ""vcard"": Dataset,        ""grddl"": Dataset,        ""dcterms"": Dataset,        ""skosxl"": Dataset,        ""wgs"": Dataset,        ""dbo"": Dataset,        ""dbpedia"": Dataset,        ""dbpprop"": Dataset,        ""rss"": Dataset,        ""cnt"": Dataset,        ""vs"": Dataset,        ""hydra"": Dataset,        ""gn"": Dataset,        ""gtfs"": Dataset,        ""geo"": Dataset,        ""geof"": Dataset,        ""geor"": Dataset      }      */    })  ```    #### Loading only some Vocabularies as Datasets    ```js  const { vocabularies } = require('@zazuko/rdf-vocabularies')    vocabularies({ only: ['rdfs', 'owl', 'skos'] })    .then((datasets) => {      /* `datasets` is:      {        ""owl"": Dataset,        ""skos"": Dataset,        ""rdfs"": Dataset      }      */    })  ```    #### Getting a Readable Stream (Quad Stream)    ```js  const { vocabularies } = require('@zazuko/rdf-vocabularies')  const stream = await vocabularies({ stream: true, only: ['rdfs', 'owl', 'skos'] })  ```    ### Using `vocabularies` function in browser    The preferred usage in browser projects is to avoid importing from `@zazuko/rdf-vocabularies` because that will require additional bundling of dynamic n-quads modules.    Instead, import from the partial modules:    * `import { expand } from '@zazuko/rdf-vocabularies/expand'`  * `import { prefixes } from '@zazuko/rdf-vocabularies/prefixes'`  * `import { shrink } from '@zazuko/rdf-vocabularies/shrink'`    The module `@zazuko/rdf-vocabularies/expandWithCheck` requires `rdf-ext` and parses datasets. See the instructions below for examples how to configure the application.    The package's main module can also be used in browser albeit it needs a bundler such as webpack and additional steps to configure it:    The package can be used in browser albeit it needs a bundler such as webpack and additional steps to configure it:    * Enable [dynamic imports](https://medium.com/front-end-weekly/webpack-and-dynamic-imports-doing-it-right-72549ff49234).    In webpack it is done with [@babel/plugin-syntax-dynamic-import](https://babeljs.io/docs/en/babel-plugin-syntax-dynamic-import)  * Extend the bundler setup to have it load the contents of vocabulary files (all n-triples). In    In webpack it can be done with [`raw-loader`](https://github.com/webpack-contrib/raw-loader):          module: {          rules: [            {              test: /\.nq$/,              use: ['raw-loader']            }          ]        }  * Be careful with prefetching chunks. Some applications may generate prefetch links for dynamically loaded chunks.  Some of the ontology files are quite large and their number will grow over time. Hence, it may be desired to exclude  certain chunks from the being eagerly loaded. Check the [wiki](https://github.com/zazuko/rdf-vocabularies/wiki/Example-web-app-config) for examples.    ### Expanding a Prefix    `expand`ing means: `'xsd:dateTime' → 'http://www.w3.org/2001/XMLSchema#dateTime'`.  It is the opposite of [`shrink`](#shrinking-an-iri)ing:    `expand(shrink('http://www.w3.org/2001/XMLSchema#dateTime')) === 'http://www.w3.org/2001/XMLSchema#dateTime'`    There are two ways of expanding a prefix:    * `vocabularies.expand(prefixedTerm: String): String` synchronous        Expand without checks. It is similar to prefix.cc in the sense that prefix.cc would expand      `schema:ImNotInSchemaDotOrg` to `http://schema.org/ImNotInSchemaDotOrg`.    * `vocabularies.expand(prefixedTerm: String, types: Array<String|NamedNode>): Promise<String>` **asynchronous**        Expand with type checks. `types` is an array of strings or NamedNodes. See this example:        ```js      const { expand } = require('@zazuko/rdf-vocabularies')      const Class = expand('rdfs:Class')      const Property = expand('rdf:Property')        // Will return <schema:person> expanded to `http://schema.org/Person`      // iff the dataset contains either:      //   <schema:Person> <rdf:type> <rdfs:Class>      // or      //   <schema:Person> <rdf:type> <rdf:Property>      await expand('schema:Person', [Class, Property])      ```    ### Shrinking an IRI    `shrink`ing means: `'http://www.w3.org/2001/XMLSchema#dateTime' → 'xsd:dateTime'`.  It is the opposite of [`expand`](#expanding-a-prefix)ing:    `shrink(expand('xsd:dateTime')) === 'xsd:dateTime'`    * `vocabularies.shrink(iri: String): String`        **Note**: returns empty string when there is no corresponding prefix. Always check the output      when using `shrink` with user-provided strings.        ```js      const assert = require('assert')      const { shrink } = require('@zazuko/rdf-vocabularies')        assert(shrink('http://www.w3.org/2001/XMLSchema#dateTime') === 'xsd:dateTime')      assert(shrink('http://example.com#nothing') === '')        const iri = 'http://example.com#nothing'      const stringToDisplay = shrink(iri) || iri      console.log(stringToDisplay) // 'http://example.com#nothing'      ```    ### Accessing Prefixes: `vocabularies.prefixes`    Getting an object with prefixes and their base URI:    (Returns [this object](./src/prefixes.ts).)    ```js  const { prefixes } = require('@zazuko/rdf-vocabularies')    console.log(prefixes)  /*   {    v: 'http://rdf.data-vocabulary.org/#',    csvw: 'http://www.w3.org/ns/csvw#',    sd: 'http://www.w3.org/ns/sparql-service-description#',    …  }  */  ```    ### Accessing Data Files from the Package    Accessing the N-Quads files:    ```js  const path = require('path')  console.log(path.resolve(require.resolve('@zazuko/rdf-vocabularies'), '..', 'ontologies', 'skos.nq'))  ```    ### Command line    The package also includes a simple command line interface which forwards the vocabulary datasets to standard output. It can be used in two ways.    By prefix:    ```  rdf-vocab prefix foaf  ```    By namespace URI:    ```  rdf-vocab prefix http://schema.org/  ```    ## Versioning Scheme    This package is [vendoring ontologies](./ontologies/). These will be updated periodically.    This package is versioned using the date at which the data was pulled, e.g. `@zazuko/rdf-vocabularies@2019.04.30`.    Updating the vendored ontologies is achieved using `npm run fetch` in this package.    ## Adding new prefixes    New prefixes can be added by opening a pull request on Github. For new requests, first check if the creator/owner  of the namespace defined a prefix. If not check [prefix.cc](http://prefix.cc/). In case prefix.cc is ambiguous a  discussion should be raised before the pull-requests gets integrated. Last thing to check are the predefined namespaces  in the [DBpedia SPARQL endpoint](http://dbpedia.org/sparql?nsdecl) or other popular RDF resources like  [LOV](https://lov.linkeddata.es/dataset/lov/vocabs). If you find one please refer to it in the pull request.    ### Steps to add a prefix    1. Add an entry in [`src/prefixes.ts`](src/prefixes.ts)  1. If necessary, add an entry to [`overrides.ts`](overrides.ts), similar to the others     * for the `file` option, a `file:` scheme IRI can be used, with path relative to the repository root  1. Run `npm run fetch -- <prefix>` with the prefix passed as parameter.     * multiple prefixes can also be to fetch multiple ontologies  1. Commit changes and submit a PR    ### Project-specific prefixes    It is also possible to add prefix within a project so that it can be used with the functions [`expand`](#expanding-a-prefix) and [`shrink`](#shrinking-an-iri).    ```js  import { prefixes, expand, shrink } from '@zazuko/rdf-vocabularies'    prefixes['foo'] = 'http://example.com/'    // 'http://example.com/bar'  const foobar = expand('foo:bar')    // 'foo:bar'  const prefixed = shrink(foobar)  ``` """
Semantic web;https://github.com/klinovp/owlproofs;"""owlproofs  =========    Extension to the OWL API to request proofs of entailments from the reasoner """
Semantic web;https://github.com/knakk/rdf;"""# rdf    This package introduces data structures for representing RDF resources, and includes functions for parsing and serialization of RDF data.    For complete documentation see [godoc](http://godoc.org/github.com/knakk/rdf). """
Semantic web;https://github.com/ropensci/jsonld;"""# jsonld    > JSON for Linking Data    [![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)  [![Build Status](https://travis-ci.org/ropensci/jsonld.svg?branch=master)](https://travis-ci.org/ropensci/jsonld)  [![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/ropensci/jsonld?branch=master&svg=true)](https://ci.appveyor.com/project/jeroen/jsonld)  [![Coverage Status](https://codecov.io/github/ropensci/jsonld/coverage.svg?branch=master)](https://codecov.io/github/ropensci/jsonld?branch=master)  [![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/jsonld)](https://cran.r-project.org/package=jsonld)  [![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/jsonld)](https://cran.r-project.org/package=jsonld)  [![Github Stars](https://img.shields.io/github/stars/ropensci/jsonld.svg?style=social&label=Github)](https://github.com/ropensci/jsonld)      JSON-LD is a light-weight syntax for expressing linked data. It is primarily  intended for web-based programming environments, interoperable web services and for   storing linked data in JSON-based databases. This package provides bindings to the   JavaScript library for converting, expanding and compacting JSON-LD documents.    ## Hello World        Example from https://github.com/digitalbazaar/jsonld.js#quick-examples. Example data:      ```r  doc <- '{    ""http://schema.org/name"": ""Manu Sporny"",    ""http://schema.org/url"": {""@id"": ""http://manu.sporny.org/""},    ""http://schema.org/image"": {""@id"": ""http://manu.sporny.org/images/manu.png""}  }'    context <- '{    ""name"": ""http://schema.org/name"",    ""homepage"": {""@id"": ""http://schema.org/url"", ""@type"": ""@id""},    ""image"": {""@id"": ""http://schema.org/image"", ""@type"": ""@id""}  }'  ```    ### Compact and expand:      ```r  (out <- jsonld_compact(doc, context))  ```    ```  {    ""@context"": {      ""name"": ""http://schema.org/name"",      ""homepage"": {        ""@id"": ""http://schema.org/url"",        ""@type"": ""@id""      },      ""image"": {        ""@id"": ""http://schema.org/image"",        ""@type"": ""@id""      }    },    ""image"": ""http://manu.sporny.org/images/manu.png"",    ""name"": ""Manu Sporny"",    ""homepage"": ""http://manu.sporny.org/""  }   ```    ```r  (expanded <- jsonld_expand(out))  ```    ```  [    {      ""http://schema.org/url"": [        {          ""@id"": ""http://manu.sporny.org/""        }      ],      ""http://schema.org/image"": [        {          ""@id"": ""http://manu.sporny.org/images/manu.png""        }      ],      ""http://schema.org/name"": [        {          ""@value"": ""Manu Sporny""        }      ]    }  ]   ```    ### Convert between JSON and RDF:      ```r  cat(nquads <- jsonld_to_rdf(doc))  ```    ```  _:b0 <http://schema.org/image> <http://manu.sporny.org/images/manu.png> .  _:b0 <http://schema.org/name> ""Manu Sporny"" .  _:b0 <http://schema.org/url> <http://manu.sporny.org/> .  ```    ```r  jsonld_from_rdf(nquads)  ```    ```  [    {      ""@id"": ""_:b0"",      ""http://schema.org/image"": [        {          ""@id"": ""http://manu.sporny.org/images/manu.png""        }      ],      ""http://schema.org/name"": [        {          ""@value"": ""Manu Sporny""        }      ],      ""http://schema.org/url"": [        {          ""@id"": ""http://manu.sporny.org/""        }      ]    }  ]   ```    ### Other utilities:      ```r  jsonld_flatten(doc)  ```    ```  [    {      ""@id"": ""_:b0"",      ""http://schema.org/image"": [        {          ""@id"": ""http://manu.sporny.org/images/manu.png""        }      ],      ""http://schema.org/name"": [        {          ""@value"": ""Manu Sporny""        }      ],      ""http://schema.org/url"": [        {          ""@id"": ""http://manu.sporny.org/""        }      ]    }  ]   ```    ```r  cat(jsonld_normalize(doc, algorithm = 'URDNA2015', format = 'application/nquads'))  ```    ```  _:c14n0 <http://schema.org/image> <http://manu.sporny.org/images/manu.png> .  _:c14n0 <http://schema.org/name> ""Manu Sporny"" .  _:c14n0 <http://schema.org/url> <http://manu.sporny.org/> .  ```   """
Semantic web;https://github.com/sparksrdf/sparks;"""Sparks JavaScript Library  ==================    Sparks is a set of JavaScript libraries designed for simplifying the access to RDF data.  Sparks is licensed under the MIT license.    Disclaimer  ---------------  This is the first release of the library and it only contains an initial version of the Prism filtering framework.  There is some known bugs and the library doesn't support the upcoming Sparks Plug library.      Install  --------  Sparks depends on [JQuery](http://jquery.com/) and [JavascriptMVC](http://javascriptmvc.com/) in order to work properly.  Download [JavascriptMVC](http://javascriptmvc.com/)  and put the Sparks code in sparks directory inside [JavascriptMVC](http://javascriptmvc.com/).    Alternatively, you might use the file ''*sparks.prism.js*'' which includes everything except [JQuery](http://jquery.com/).    Usage  ---------  ```javascript  //You need to include the sparks library before doing the following:  //- Create a new prism endpoint:  var prism = new Sparks.Prism(""http://nebula.dcs.shef.ac.uk/sparks/sparql"",""?root a <http://ext.dcs.shef.ac.uk/~u0080/linkedPOI/core#POI>"");    //- Instantiate some lenses:  $('#tags-list').sparks_prism_lens_tags({prism: prism});  $('#search').sparks_prism_lens_search({prism: prism});  $('#list').sparks_prism_lens_list({prism: prism});  ``` """
Semantic web;https://github.com/konradreiche/jtriple;"""# JTriple    JTriple is a Java tool which creates a RDF data model out of a Java object model by making use of reflection, a small set of annotations and Jena's flexible RDF/OWL API.    ### Why another RDF binding for Java?    The most popular tool for persisting Java objects to RDF is [JenaBean]. JTriple was developed, respectively JenaBean was not modified due to the following reasons:    * JenaBean aims for a persistence layer (object serialization). This fact is often expressed by missing confguration, for instance a field cannot be declared as transient.    * Not the whole functionality of JenaBean is required. Additional data is serialized, for instance the serialization of the package names. Package names are vital for deserialization but for the pure data translation (one-way) it only interferes.    * Data (RDF) and schema (OWL) should be translated into two separate RDF graphs. JenaBean creates only one graph.    ## Getting Started    JTriple can be deployed through Maven. Before, the following repository has to be added to your pom.xml    ```xml  <repository>       <id>berlin.reiche.jtriple</id>       <url>https://github.com/platzhirsch/jtriple/raw/master/repository/releases</url>  </repository>  ```    Then it can be added with this dependency    ```xml  <dependency>       <groupId>berlin.reiche.jtriple</groupId>       <artifactId>jtriple</artifactId>       <version>0.1-RELEASE</version>       <scope>compile</scope>  </dependency>  ```    Not using Maven? You can also get the [JAR] directly.    ### Example    Considering the following example. A class Philosopher    ```java  public class Philosopher {    	@RdfIdentifier  	String name;    	String nationality;  	List<Branch> interests;  }  ```    with an enum type Branch    ```java  public enum Branch {    	EPISTEMOLOGY(""Epistemology""),  	MATHEMATIC(""Mathematic""),  	METAPHYSISC(""Metaphysic""),  	PHILOSOPHY_OF_MIND(""Philosophy of Mind"");  	  	String name;  	  	Branch(String name) {  		this.name = name;  	}  }  ```  The only requirement is to annotate one field or method of a class with `@RdfIdentifier`. Binding objects to RDF is as easy as follows      ```java  // create data  Philosopher locke = new Philosopher();  locke.setName(""John Locke"");  locke.setNationality(""English"");    List<Branch> branches = new ArrayList<>();  branches.add(METAPHYSISC);  branches.add(EPISTEMOLOGY);  branches.add(PHILOSOPHY_OF_MIND);  locke.setInterests(branches);    // bind object  Binding binding = new Binding(DEFAULT_NAMESPACE);  Model model = binding.getModel();  model.setNsPrefix(""philosophy"", NAMESPACE);    binding.bind(locke);    // output RDF  model.write(System.out, ""TURTLE"");  ```    It is sufficient to produce this RDF    ```  @prefix philosophy:  <http://konrad-reiche.com/philosophy/> .    <http://konrad-reiche.com/philosophy/philosopher/John_locke>        a       <http://dbpedia.org/page/Philosopher> ;        philosophy:interests                <http://konrad-reiche.com/philosophy/branch/Metaphysisc> ,                <http://konrad-reiche.com/philosophy/branch/Philosophy_of_mind> ,                <http://konrad-reiche.com/philosophy/branch/Epistemology> ;        philosophy:name ""John Locke""^^<http://www.w3.org/2001/XMLSchema#string> ;        philosophy:nationality                ""English""^^<http://www.w3.org/2001/XMLSchema#string> .    <http://konrad-reiche.com/philosophy/branch/Epistemology>        a       philosophy:branch ;        philosophy:name ""Epistemology""^^<http://www.w3.org/2001/XMLSchema#string> .    <http://konrad-reiche.com/philosophy/branch/Metaphysisc>        a       philosophy:branch ;        philosophy:name ""Metaphysic""^^<http://www.w3.org/2001/XMLSchema#string> .    <http://konrad-reiche.com/philosophy/branch/Philosophy_of_mind>        a       philosophy:branch ;        philosophy:name ""Philosophy of Mind""^^<http://www.w3.org/2001/XMLSchema#string> .  ```    Now, to get more sophisticated results, annotations help to provide neccessary information    ```java  @RdfType(""http://dbpedia.org/page/Philosopher"")  public class Philosopher {    	@Label  	@RdfIdentifier  	String name;    	@RdfProperty(""http://www.foafrealm.org/xfoaf/0.1/nationality"")  	String nationality;    	List<Branch> interests;  }  ```    ```java  public enum Branch {    	@SameAs({ ""http://dbpedia.org/resource/Epistemology"" })  	EPISTEMOLOGY(""Epistemology""),  	  	@SameAs({ ""http://dbpedia.org/resource/Mathematic"" })  	MATHEMATIC(""Mathematic""),    	@SameAs({ ""http://dbpedia.org/resource/Metaphysic"" })  	METAPHYSISC(""Metaphysic""),    	@SameAs({ ""http://dbpedia.org/resource/Philosophy_of_mind"" })  	PHILOSOPHY_OF_MIND(""Philosophy of Mind"");  	  	@Label  	String name;  	  	Branch(String name) {  		this.name = name;  	}  }  ```    Leading to this RDF:    ```  @prefix rdfs:    <http://www.w3.org/2000/01/rdf-schema#> .  @prefix xfoaf:   <http://www.foafrealm.org/xfoaf/0.1/> .  @prefix philosophy:  <http://konrad-reiche.com/philosophy/> .  @prefix dbpedia:  <http://dbpedia.org/resource/> .    <http://konrad-reiche.com/philosophy/philosopher/John_locke>        a       <http://dbpedia.org/page/Philosopher> ;        rdfs:label ""John Locke""^^<http://www.w3.org/2001/XMLSchema#string> ;        philosophy:interests                <http://konrad-reiche.com/philosophy/branch/Metaphysisc> ,                <http://konrad-reiche.com/philosophy/branch/Philosophy_of_mind> ,                <http://konrad-reiche.com/philosophy/branch/Epistemology> ;        xfoaf:nationality ""English""^^<http://www.w3.org/2001/XMLSchema#string> .    <http://konrad-reiche.com/philosophy/branch/Metaphysisc>        a       philosophy:branch ;        rdfs:label ""Metaphysic""^^<http://www.w3.org/2001/XMLSchema#string> ;        <http://www.w3.org/2002/07/owl#sameAs>                dbpedia:Metaphysic .    <http://konrad-reiche.com/philosophy/branch/Philosophy_of_mind>        a       philosophy:branch ;        rdfs:label ""Philosophy of Mind""^^<http://www.w3.org/2001/XMLSchema#string> ;        <http://www.w3.org/2002/07/owl#sameAs>                dbpedia:Philosophy_of_mind .    <http://konrad-reiche.com/philosophy/branch/Epistemology>        a       philosophy:branch ;        rdfs:label ""Epistemology""^^<http://www.w3.org/2001/XMLSchema#string> ;        <http://www.w3.org/2002/07/owl#sameAs>                dbpedia:Epistemology .    ```    ### Annotations    What annotations are there and how can they be used?    <table>    <tr>      <th>Name</th><th>Use</th><th>Effect</th>    </tr>    <tr>      <td><code>@RdfIdentifier</code></td><td>Fields, Methods</td><td>Value to be used for constructing the resource URI</td>    </tr>    <tr>      <td><code>@RdfProperty</code></td><td>Fields, Methods</td><td>Value to define another property URI</td>    </tr>    <tr>      <td><code>@RdfType</code></td><td>Classes</td><td>Value to define a rdfs:type property on the resource</td>    </tr>    <tr>      <td><code>@Transient</code></td><td>Fields</td><td>Indicate that this field must not be converted</td>    </tr>    <tr>      <td><code>@SameAs</code></td><td>Enum Constants</td><td>Value to define a owl:sameAs property on the resource</td>    </tr>    <tr>      <td><code>@Label</code></td><td>Fields, Methods</td><td>Value to define a rdfs:label property on the resource</td>    </tr>  </table>    ## Future Work    Some ideas for the future development:    * Implement OWL binding  * Increase the configuration flexibility    If something is amiss, feel free to open an issue or make a pull request. The implementation is lightweight and allows to change the functionality very quickly.    [JenaBean]: http://code.google.com/p/jenabean/  [Jena API]: http://jena.apache.org/  [JAR]: https://github.com/platzhirsch/jtriple/raw/master/repository/releases/berlin/reiche/jtriple/jtriple/0.1/jtriple-0.1.jar """
Semantic web;https://github.com/AKSW/AutoSPARQL;"""## Introduction    AutoSPARQL TBSL is a graphical user interface, which allows to answer natural  language queries over RDF knowledge bases. It is based on algorithms  implemented in the DL-Learner Semantic Web machine learning framework.    ## Requirements  * Java 7 or higher  * Maven 3 or higher  * Git  * an **IPv6-capable internet connection** (you can use a free IPv6 tunnel service like Miredo or Teredo if your connection does not support it natively)    ## Warning  AutoSPARQL TBSL is a research prototype that is not actively developed anymore.  As such, getting it to work may need some effort and depends on the availability of several other online services.  Feel free to create issues if you encounter problems and please share your fixes using pull requests.    ## Installation and Execution  1. clone the git repository  2. run `./compile` and then `./run` (Linux, Mac) or `compile.bat` and then `run.bat` (Windows)  * Manually: Do `mvn install -N` in the folders in that order: autosparql, commons, algorithm-tbsl. Then go into autosparql-tbsl and run `mvn jetty:run`.    Then, go into your browser and access `http://localhost:8080` and click on the link to the application.    ## Error Reporting  If you encounter errors, please look at the issues if the problem is already reported.  If not, please create a single issue including the command line output.  If the error occurs during compilation, please use `./createcompillelog` instead of `./compile` to create the compile log.  Feel free to create issues if you encounter problems and please share your fixes using pull requests.    ## Adding your own Dataset  Using your own datasource instead of DBpedia or Oxford is nontrivial.  It needs several days of work in addition to the time needed to familiarize yourself with the code base.    You need to:    - fork AutoSPARQL TBSL  - add your own domain dependent lexicon as an LTAG grammar at algorithm-tbsl/src/main/resources/tbsl/lexicon (see http://pub.uni-bielefeld.de/publication/2002961 and http://pub.uni-bielefeld.de/publication/2278529 as well as the existing files in that folder)  - add your knowledge base:   1. as a local model to algorithm-tbsl/src/main/resources/models/yourmodel (preferred for small knowledge bases as it is much faster and more reliable)   2. as a SPARQL (version 1.0 is enough) endpoint along with a SOLR server instance  - create a singleton for your knowledge base (see package org.aksw.autosparql.tbsl.algorithm.knowledgebase)  - extend org.aksw.autosparql.tbsl.algorithm.learning.TBSL with TbslYourKnowledgeBase  - finally, in case the dataset isn't a private model, please do a pull request so that it can be integrated in the main project    ## Modules and Packages  | Maven Module      | Package                             | Purpose       |  |-------------------|-------------------------------------|---------------|  | autosparql-parent |                  -                  | Parent Module |  | algorithm-tbsl    | org.aksw.autosparql.tbsl.algorithm  | Algorithm     |  | commons           | org.aksw.autosparql.commons         | Utilities     |  | autosparql-tbsl   | org.aksw.autosparql.tbsl.gui.vaadin | Web Interface | """
Semantic web;https://github.com/dgarijo/Widoco;"""# WIzard for DOCumenting Ontologies (WIDOCO)  [![DOI](https://zenodo.org/badge/11427075.svg)](https://zenodo.org/badge/latestdoi/11427075) [![](https://jitpack.io/v/dgarijo/Widoco.svg)](https://jitpack.io/#dgarijo/Widoco) [![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)    ![Logo](src/main/resources/logo/logo2.png)    WIDOCO helps you to publish and create an enriched and customized documentation of your ontology automatically, by following a series of steps in a GUI.    **Author**: Daniel Garijo Verdejo (@dgarijo)    **Contributors**: María Poveda, Idafen Santana, Almudena Ruiz, Miguel Angel García, Oscar Corcho, Daniel Vila, Sergio Barrio, Martin Scharm, Maxime Lefrancois, Alfredo Serafini, @kartgk, Pat Mc Bennett, Christophe Camel, Jacobus Geluk, Martin Scharm, @rpietzsch, Jonathan Leitschuh, Jodi Schneider, Giacomo Lanza, Alejandra Gonzalez-Beltran, Mario Scrocca, Miguel Angel García, Flores Bakker and @JohnnyMoonlight.    **Citing WIDOCO**: If you used WIDOCO in your work, please cite the ISWC 2017 paper: https://iswc2017.semanticweb.org/paper-138    ```bib  @inproceedings{garijo2017widoco,    title={WIDOCO: a wizard for documenting ontologies},    author={Garijo, Daniel},    booktitle={International Semantic Web Conference},    pages={94--102},    year={2017},    organization={Springer, Cham},    doi = {10.1007/978-3-319-68204-4_9},    funding = {USNSF ICER-1541029, NIH 1R01GM117097-01},    url={http://dgarijo.com/papers/widoco-iswc2017.pdf}  }  ```  If you want to cite the latest version of the software, you can do so by using: https://zenodo.org/badge/latestdoi/11427075.    ## Downloading the executable    To download WIDOCO, you need to download a JAR executable file. Check the latest release for more details: (https://github.com/dgarijo/WIDOCO/releases/latest).    ## Importing WIDOCO as a dependency  Just add the dependency and repository to your `pom.xml` file as follows. See the [WIDOCO JitPack](https://jitpack.io/#dgarijo/Widoco) page to find alternative means to incorporate WIDOCO to your project.    ```xml  <dependencies>    <dependency>        <groupId>com.github.dgarijo</groupId>        <artifactId>Widoco</artifactId>        <version>v1.4.16</version>    </dependency>  </dependencies>    [ ... ]    <repositories>  	<repository>  	    <id>jitpack.io</id>  	    <url>https://jitpack.io</url>  	</repository>  </repositories>  ```    ## Description  WIDOCO helps you to publish and create an enriched and customized documentation of your ontology, by following a series of steps in a wizard. We extend the LODE framework by Silvio Peroni to describe the classes, properties and data properties of the ontology, the OOPS! webservice by María Poveda to print an evaluation and the Licensius service by Victor Rodriguez Doncel to determine the license URI and title being used. In addition, we use WebVowl to visualize the ontology and have extended Bubastis to show a complete changelog between different versions of your ontology.    Features of WIDOCO:  * Automatic documentation of the terms in your ontology (based on [LODE](http://www.essepuntato.it/lode/)). Now you can use Markdown on your class descriptions (see [example](doc/gallery/index.html))  * Automatic annotation in JSON-LD snippets of the html produced.  * Association of a provenance page which includes the history of your vocabulary (W3C PROV-O compliant).  * Metadata extraction from the ontology plus the means to complete it on the fly when generating your ontology. Check the [best practice document](http://dgarijo.github.io/Widoco/doc/bestPractices/index-en.html) to know more about the terms recognized by WIDOCO.  * Guidelines on the main sections that your document should have and how to complete them.  * Integration with diagram creators ([WebVOWL](http://vowl.visualdataweb.org/webvowl/)).  * Automatic changelog of differences between the actual and the previous version of the ontology (based on [Bubastis](http://www.ebi.ac.uk/efo/bubastis/)).  * Separation of the sections of your html page so you can write them independently and replace only those needed.  * Content negotiation and serialization of your ontology according to [W3C best practices](https://www.w3.org/TR/swbp-vocab-pub/)  * Evaluation reports of your ontology (using the [OOPS! web service](http://oops.linkeddata.es/))  * Integration with license metadata services ([Licensius](http://licensius.com/)) to automatically describe the license used in your ontology.    ## Examples  Examples of the features of WIDOCO can be seen on [the gallery](http://dgarijo.github.io/Widoco/doc/gallery/)    ## GUI Tutorial  A tutorial explaining the main features of the GUI can be found [here](http://dgarijo.github.io/Widoco/doc/tutorial/)        ## How to use WIDOCO    ### JAR execution    Download the latest `.jar` [WIDOCO available release](https://github.com/dgarijo/WIDOCO/releases/latest) (it will be something like `widoco-VERSION-jar-with-dependencies.jar`). Then just double click the `.jar` file.    You may also execute WIDOCO through the command line. Usage:  ```bash  java -jar widoco-VERSION-jar-with-dependencies.jar [OPTIONS]  ```    ### Docker execution    First build the image using the `Dockerfile` in project folder:    ```bash  docker build -t dgarijo/widoco .  ```    You can now execute WIDOCO through the command line. Usage:    ```bash  docker run -ti --rm dgarijo/widoco [OPTIONS]  ```    If you want to share data between the Docker Container and your Host, for instance to load a local ontology file (from PATH), you will need to mount the container  with host directories. For instance:    ```bash  docker run -ti --rm \    -v `pwd`/test:/usr/local/widoco/in \    -v `pwd`/target/generated-doc:/usr/local/widoco/out \    dgarijo/widoco -ontFile in/bne.ttl -outFolder out -rewriteAll  ```    ### Options    `-ontFile PATH`  [required (unless -ontURI is used)]: Load a local ontology file (from PATH) to document. This option is incompatible with -ontURI    `-ontURI  URI`   [required (unless -ontFile is used)]: Load an ontology to document from its URI. This option is incompatible with -ontFile    `-outFolder folderName`: Specifies the name of the folder where to save the documentation. By default is 'myDocumentation'    `-confFile PATH`: Load your own configuration file for the ontology metadata. Incompatible with -getOntologyMetadata    `-getOntologyMetadata`: Extract ontology metadata from the given ontology    `-oops`: Create an html page with the evaluation from the OOPS service (http://oops.linkeddata.es/)    `-rewriteAll`: Replace any existing files when documenting an ontology (e.g., from a previous execution)    `-crossRef`: ONLY generate the overview and cross reference sections. The index document will NOT be generated. The htaccess, provenance page, etc., will not be generated unless requested by other flags. This flag is intended to be used only after a first version of the documentation exists.    `-saveConfig PATH`: Save a configuration file on PATH with the properties of a given ontology    `-useCustomStyle`: Export the documentation using alternate css files (by Daniel Vila).    `-lang LANG1-LANG2`: Generate documentation in multiple languages (separated by ""-""). Note that if the language is not supported, the system will load the labels in english. For example: en-pt-es    `-includeImportedOntologies`: Indicates whether the terms of the imported ontologies of the current ontology should be documented as well or not.    `-htaccess`: Create a bundle for publication ready to be deployed on your Apache server.    `-webVowl`: Create a visualization based on WebVowl (http://vowl.visualdataweb.org/webvowl/index.html#) in the documentation.    `-licensius`: Use the Licensius web services (http://licensius.com/apidoc/index.html) to retrieve license metadata. Only works if the -getOntologyMetadata  flag is enabled.    `-ignoreIndividuals`: Individuals will not be included in the documentation.    `-includeAnnotationProperties`: Include annotation properties defined in your ontology in the documentation (by default they are not included)    `-analytics CODE`: Add a code snippet for Google analytics to track your HTML documentation. You need to add your CODE next to the flag. For example: UA-1234    `-doNotDisplaySerializations`: The serializations of the ontology will not be displayed.    `-displayDirectImportsOnly`: Only those imported ontologies that are directly imported in the ontology being documented.    `-rewriteBase PATH`: Change the default rewrite base path. The default value is ""/"". This flag can only be used with the htaccess option.    `-excludeIntroduction`: Skip the introduction section in the documentation.    `-uniteSections`: Write all HTML sections into a single HTML document.    `-noPlaceHolderText`: Do not add any placeholder text (this will remove intro, abstract (if empty) and description sections).    `--help`: Shows a help message and exits.      ## How can I make WIDOCO automatically recognize my vocabulary annotations?  There are two alternative ways for making WIDOCO get your vocabulary metadata annotations and use them automatically to document the ontology.    * The recommended way: add them in your OWL file. For guidelines on which ones to include, follow our [best practices document](https://w3id.org/widoco/bestPractices), which indicates which ones we recommend.  * Alternatively, edit the project properties of /config/config.properties. This is a key-value pair file with metadata properties. Some people consider it easier than adding the property annotations to the OWL file, although I recommend doing the former option. Note that the character "";"" is used for lists (for instance first author; second author; third author).    ## Browser issues (Why can't I see the generated documentation / visualization?)  WIDOCO separates the contents of different sections in HTML files, which are then loaded in the `index.html` file. WIDOCO was designed this way because it's easier to edit your introduction or description sections independently without being all aggregated together in a huge HTML document.  **When all the contents generated by WIDOCO are stored in a server, you will be able to see the documentation of your ontology using any browser**. However, if you open the `index.html` file **on your local browser**, you may see a document missing most of the sections in your documentation. This happens because browsers don't allow loading separate content when opening a file locally for security reasons. If you want to explore how your ontology would look locally, you have two options:    * a) Execute WIDOCO with the `-uniteSections` flag; or select the option `add al sections in a single document` in the ""load sections"" step in the WIDOCO GUI. This will make all the sections of WIDOCO to be in the `index.html`; and you will be able to see it in your browser. Note that the **LODE visualization will not be available** when exploring your ontology locally.  * b) Create a local server: Set up a local server (e.g., using XAMPP or Tomcat) and serve the files WIDOCO generates (in the `htdocs` folder for Apache servers).    If you place the files generated by WIDOCO in a server and access them via its URL (for example, a Github page), you should be able to see your documentation appropriately.    ## Current improvements  For a complete list of the current improvements and next features, check the [project open issues](https://github.com/dgarijo/Widoco/issues) and [milestones](https://github.com/dgarijo/Widoco/milestones) in the repository.    ## Requirements  You will need Java 1.8 or higher (SDK 1.8 or JRE 8) for WIDOCO to work  Otherwise, you will probably experience an ""Unsupported major.minor version 52.0"" exception when executing the JAR file.    ## Contribution guidelines  Contributions to address any of the current issues are welcome. In order to push your contribution, just **push your pull request to the develop branch**. The master branch has only the code associated to the latest release. """
Semantic web;https://github.com/jsonld-java/jsonld-java;"""**JSONLD-Java is looking for a maintainer**    JSONLD-JAVA  ===========    This is a Java implementation of the [JSON-LD 1.0 specification](https://www.w3.org/TR/2014/REC-json-ld-20140116/) and the [JSON-LD-API 1.0 specification](https://www.w3.org/TR/2014/REC-json-ld-api-20140116/).    [![Build Status](https://travis-ci.org/jsonld-java/jsonld-java.svg?branch=master)](https://travis-ci.org/jsonld-java/jsonld-java) [![Coverage Status](https://coveralls.io/repos/jsonld-java/jsonld-java/badge.svg?branch=master)](https://coveralls.io/r/jsonld-java/jsonld-java?branch=master)    USAGE  =====    From Maven  ----------        <dependency>          <groupId>com.github.jsonld-java</groupId>          <artifactId>jsonld-java</artifactId>          <version>0.13.4</version>      </dependency>    Code example  ------------    ```java  // Open a valid json(-ld) input file  InputStream inputStream = new FileInputStream(""input.json"");  // Read the file into an Object (The type of this object will be a List, Map, String, Boolean,  // Number or null depending on the root object in the file).  Object jsonObject = JsonUtils.fromInputStream(inputStream);  // Create a context JSON map containing prefixes and definitions  Map context = new HashMap();  // Customise context...  // Create an instance of JsonLdOptions with the standard JSON-LD options  JsonLdOptions options = new JsonLdOptions();  // Customise options...  // Call whichever JSONLD function you want! (e.g. compact)  Object compact = JsonLdProcessor.compact(jsonObject, context, options);  // Print out the result (or don't, it's your call!)  System.out.println(JsonUtils.toPrettyString(compact));  ```    Processor options  -----------------    The Options specified by the [JSON-LD API Specification](https://json-ld.org/spec/latest/json-ld-api/#the-jsonldoptions-type) are accessible via the `com.github.jsonldjava.core.JsonLdOptions` class, and each `JsonLdProcessor.*` function has an optional input to take an instance of this class.    Controlling network traffic  ---------------------------    Parsing JSON-LD will normally follow any external `@context` declarations.  Loading these contexts from the network may in some cases not be desirable, or  might require additional proxy configuration or authentication.    JSONLD-Java uses the [Apache HTTPComponents Client](https://hc.apache.org/httpcomponents-client-ga/index.html) for these network connections,  based on the [SystemDefaultHttpClient](http://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/impl/client/SystemDefaultHttpClient.html) which reads  standard Java properties like `http.proxyHost`.     The default HTTP Client is wrapped with a  [CachingHttpClient](https://hc.apache.org/httpcomponents-client-ga/httpclient-cache/apidocs/org/apache/http/impl/client/cache/CachingHttpClient.html) to provide a   small memory-based cache (1000 objects, max 128 kB each) of regularly accessed contexts.    ### Loading contexts from classpath    Your application might be parsing JSONLD documents which always use the same  external `@context` IRIs. Although the default HTTP cache (see above) will  avoid repeated downloading of the same contexts, your application would still  initially be vulnerable to network connectivity.    To bypass this issue, and even facilitate parsing of such documents in an  offline state, it is possible to provide a 'warmed' cache populated  from the classpath, e.g. loaded from a JAR.    In your application, simply add a resource `jarcache.json` to the root of your  classpath together with the JSON-LD contexts to embed. (Note that you might  have to recursively embed any nested contexts).    The syntax of `jarcache.json` is best explained by example:  ```javascript  [    {      ""Content-Location"": ""http://www.example.com/context"",      ""X-Classpath"": ""contexts/example.jsonld"",      ""Content-Type"": ""application/ld+json""    },    {      ""Content-Location"": ""http://data.example.net/other"",      ""X-Classpath"": ""contexts/other.jsonld"",      ""Content-Type"": ""application/ld+json""    }  ]  ```  (See also [core/src/test/resources/jarcache.json](core/src/test/resources/jarcache.json)).    This will mean that any JSON-LD document trying to import the `@context`   `http://www.example.com/context` will instead be given  `contexts/example.jsonld` loaded as a classpath resource.     The `X-Classpath` location is an IRI reference resolved relative to the  location of the `jarcache.json` - so if you have multiple JARs with a  `jarcache.json` each, then the `X-Classpath` will be resolved within the  corresponding JAR (minimizing any conflicts).    Additional HTTP headers (such as `Content-Type` above) can be included,  although these are generally ignored by JSONLD-Java.     Unless overridden in `jarcache.json`, this `Cache-Control` header is  automatically injected together with the current `Date`, meaning that the  resource loaded from the JAR will effectively never expire (the real HTTP  server will never be consulted by the Apache HTTP client):    ```  Date: Wed, 19 Mar 2014 13:25:08 GMT  Cache-Control: max-age=2147483647  ```    The mechanism for loading `jarcache.json` relies on   [Thread.currentThread().getContextClassLoader()](http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#getContextClassLoader%28%29)  to locate resources from the classpath - if you are running on a command line,  within a framework (e.g. OSGi) or Servlet container (e.g. Tomcat) this should  normally be set correctly. If not, try:    ```java  ClassLoader oldContextCL = Thread.currentThread().getContextClassLoader();  try {       Thread.currentThread().setContextClassLoader(getClass().getClassLoader());      JsonLdProcessor.expand(input);   // or any other JsonLd operation  } finally {       // Restore, in case the current thread was doing something else      // with the context classloader before calling our method      Thread.currentThread().setContextClassLoader(oldContextCL);  }  ```    To disable all remote document fetching, when using the default DocumentLoader, set the   following Java System Property to ""true"" using:    ```java  System.setProperty(""com.github.jsonldjava.disallowRemoteContextLoading"", ""true"");  ```    You can also use the constant provided in DocumentLoader for the same purpose:    ```java  System.setProperty(DocumentLoader.DISALLOW_REMOTE_CONTEXT_LOADING, ""true"");  ```    Note that if you override DocumentLoader you should also support this setting for consistency and security.    ### Loading contexts from a string    Your application might be parsing JSONLD documents which reference external `@context` IRIs  that are not available as file URIs on the classpath. In this case, the `jarcache.json`  approach will not work. Instead you can inject the literal context file strings through  the `JsonLdOptions` object, as follows:    ```java  // Inject a context document into the options as a literal string  DocumentLoader dl = new DocumentLoader();  JsonLdOptions options = new JsonLdOptions();  // ... the contents of ""contexts/example.jsonld""  String jsonContext = ""{ \""@context\"": { ... } }"";  dl.addInjectedDoc(""http://www.example.com/context"",  jsonContext);  options.setDocumentLoader(dl);    InputStream inputStream = new FileInputStream(""input.json"");  Object jsonObject = JsonUtils.fromInputStream(inputStream);  Map context = new HashMap();  Object compact = JsonLdProcessor.compact(jsonObject, context, options);  System.out.println(JsonUtils.toPrettyString(compact));  ```    ### Customizing the Apache HttpClient    To customize the HTTP behaviour (e.g. to disable the cache or provide  [authentication  credentials)](https://hc.apache.org/httpcomponents-client-ga/tutorial/html/authentication.html),  you may want to create and configure your own `CloseableHttpClient` instance, which can  be passed to a `DocumentLoader` instance using `setHttpClient()`. This document  loader can then be inserted into `JsonLdOptions` using `setDocumentLoader()`  and passed as an argument to `JsonLdProcessor` arguments.      Example of inserting a credential provider (e.g. to load a `@context` protected  by HTTP Basic Auth):    ```java  Object input = JsonUtils.fromInputStream(..);  DocumentLoader documentLoader = new DocumentLoader();            CredentialsProvider credsProvider = new BasicCredentialsProvider();  credsProvider.setCredentials(          new AuthScope(""localhost"", 443),          new UsernamePasswordCredentials(""username"", ""password""));           CacheConfig cacheConfig = CacheConfig.custom().setMaxCacheEntries(1000)          .setMaxObjectSize(1024 * 128).build();    CloseableHttpClient httpClient = CachingHttpClientBuilder          .create()          // allow caching          .setCacheConfig(cacheConfig)          // Wrap the local JarCacheStorage around a BasicHttpCacheStorage          .setHttpCacheStorage(                  new JarCacheStorage(null, cacheConfig, new BasicHttpCacheStorage(                          cacheConfig)))....  		          // Add in the credentials provider          .setDefaultCredentialsProvider(credsProvider);          // When you are finished setting the properties, call build          .build();    documentLoader.setHttpClient(httpClient);            JsonLdOptions options = new JsonLdOptions();  options.setDocumentLoader(documentLoader);  // .. and any other options          Object rdf = JsonLdProcessor.toRDF(input, options);  ```    PLAYGROUND  ----------    The [jsonld-java-tools](https://github.com/jsonld-java/jsonld-java-tools) repository contains a simple application which provides command line access to JSON-LD functions    ### Initial clone and setup    ```bash  git clone git@github.com:jsonld-java/jsonld-java-tools.git  chmod +x ./jsonldplayground  ```    ### Usage    run the following to get usage details:    ```bash  ./jsonldplayground --help  ```    For Developers  --------------    ### Compiling & Packaging    `jsonld-java` uses maven to compile. From the base `jsonld-java` module run `mvn clean install` to install the jar into your local maven repository.    ### Running tests    ```bash  mvn test  ```    or    ```bash  mvn test -pl core  ```    to run only core package tests    ### Code style    The JSONLD-Java project uses custom Eclipse formatting and cleanup style guides to ensure that Pull Requests are fairly simple to merge.    These guides can be found in the /conf directory and can be installed in Eclipse using ""Properties>Java Code Style>Formatter"", followed by ""Properties>Java Code Style>Clean Up"" for each of the modules making up the JSONLD-Java project.    If you don't use Eclipse, then don't worry, your pull requests can be cleaned up by a repository maintainer prior to merging, but it makes the initial check easier if the modified code uses the conventions.    ### Submitting Pull Requests    Once you have made a change to fix a bug or add a new feature, you should commit and push the change to your fork.    Then, you can open a pull request to merge your change into the master branch of the main repository.    Implementation Reports for JSONLD-Java conformance with JSONLD-1.0  ==================================================================    The Implementation Reports documenting the conformance of JSONLD-Java with JSONLD-1.0 are available at:    https://github.com/jsonld-java/jsonld-java/tree/master/core/reports    ### Regenerating Implementation Report    Implementation Reports conforming to the [JSON-LD Implementation Report](http://json-ld.org/test-suite/reports/#instructions-for-submitting-implementation-reports) document can be regenerated using the following command:    ```bash  mvn test -pl core -Dtest=JsonLdProcessorTest -Dreport.format=<format>  ```    Current possible values for `<format>` include JSON-LD (`application/ld+json` or `jsonld`), NQuads (`text/plain`, `nquads`, `ntriples`, `nq` or `nt`) and Turtle (`text/turtle`, `turtle` or `ttl`). `*` can be used to generate reports in all available formats.    Integration of JSONLD-Java with other Java packages  ===================================================    This is the base package for JSONLD-Java. Integration with other Java packages are done in separate repositories.    Existing integrations  ---------------------    * [Eclipse RDF4J](https://github.com/eclipse/rdf4j)  * [Apache Jena](https://github.com/apache/jena/)  * [RDF2GO](https://github.com/jsonld-java/jsonld-java-rdf2go)  * [Apache Clerezza](https://github.com/jsonld-java/jsonld-java-clerezza)    Creating an integration module  ------------------------------    ### Create a repository for your module    Create a GitHub repository for your module under your user account, or have a JSONLD-Java maintainer create one in the jsonld-java organisation.    Create maven module  -------------------    ### Create pom.xml for your module    Here is the basic outline for what your module's pom.xml should look like    ```xml  <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""      xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">      <parent>      <groupId>com.github.jsonld-java</groupId>      <artifactId>jsonld-java-parent</artifactId>      <version>0.13.4</version>    </parent>    <modelVersion>4.0.0</modelVersion>    <artifactId>jsonld-java-{your module}</artifactId>    <version>0.13.4-SNAPSHOT</version>    <name>JSONLD Java :: {your module name}</name>    <description>JSON-LD Java integration module for {RDF Library your module integrates}</description>    <packaging>jar</packaging>      <developers>      <developer>        <name>{YOU}</name>        <email>{YOUR EMAIL ADDRESS}</email>      </developer>    </developers>      <dependencies>      <dependency>        <groupId>${project.groupId}</groupId>        <artifactId>jsonld-java</artifactId>        <version>${project.version}</version>        <type>jar</type>         <scope>compile</scope>       </dependency>      <dependency>        <groupId>${project.groupId}</groupId>        <artifactId>jsonld-java</artifactId>        <version>${project.version}</version>        <type>test-jar</type>        <scope>test</scope>      </dependency>      <dependency>        <groupId>junit</groupId>        <artifactId>junit</artifactId>        <scope>test</scope>      </dependency>      <dependency>        <groupId>org.slf4j</groupId>        <artifactId>slf4j-jdk14</artifactId>        <scope>test</scope>      </dependency>    </dependencies>  </project>  ```    Make sure you edit the following:   * `project/artifactId` : set this to `jsonld-java-{module id}`, where `{module id}` usually represents the RDF library you're integrating (e.g. `jsonld-java-jena`)   * `project/name` : set this to `JSONLD Java :: {Module Name}`, wher `{module name}` is usually the name of the RDF library you're integrating.   * `project/description`   * `project/developers/developer/...` : Give youself credit by filling in the developer field. At least put your `<name>` in ([see here for all available options](http://maven.apache.org/pom.html#Developers)).   * `project/dependencies/...` : remember to add any dependencies your project needs    ### Import into your favorite editor    For Example: Follow the first few steps in the section above to import the whole `jsonld-java` project or only your new module into eclipse.    Create RDFParser Implementation  -------------------------------    The interface `com.github.jsonldjava.core.RDFParser` is used to parse RDF from the library into the JSONLD-Java internal RDF format. See the documentation in [`RDFParser.java`](../core/src/main/java/com/github/jsonldjava/core/RDFParser.java) for details on how to implement this interface.    Create TripleCallback Implementation  ------------------------------------    The interface `com.github.jsonldjava.core.JSONLDTripleCallback` is used to generate a representation of the JSON-LD input in the RDF library. See the documentation in [`JSONLDTripleCallback.java`](../core/src/main/java/com/github/jsonldjava/core/JSONLDTripleCallback.java) for details on how to implement this interface.    Using your Implementations  --------------------------    ### RDFParser    A JSONLD RDF parser is a class that can parse your frameworks' RDF model  and generate JSON-LD.    There are two ways to use your `RDFParser` implementation.    Register your parser with the `JSONLD` class and set `options.format` when you call `fromRDF`    ```java  JSONLD.registerRDFParser(""format/identifier"", new YourRDFParser());  Object jsonld = JSONLD.fromRDF(yourInput, new Options("""") {{ format = ""format/identifier"" }});  ```    or pass an instance of your `RDFParser` into the `fromRDF` function    ```java  Object jsonld = JSONLD.fromRDF(yourInput, new YourRDFParser());  ```    ### JSONLDTripleCallback    A JSONLD triple callback is a class that can populate your framework's  RDF model from JSON-LD - being called for each triple (technically quad).    Pass an instance of your `TripleCallback` to `JSONLD.toRDF`    ```java  Object yourOutput = JSONLD.toRDF(jsonld, new YourTripleCallback());  ```    Integrate with your framework  -----------------------------  Your framework might have its own system of readers and writers, where  you should register JSON-LD as a supported format. Remember that here  the ""parse"" direction is opposite of above, a 'reader' may be a class   that can parse JSON-LD and populate an RDF Graph.    Write Tests  -----------    It's helpful to have a test or two for your implementations to make sure they work and continue to work with future versions.    Write README.md  ---------------    Write a `README.md` file with instrutions on how to use your module.    Submit your module  ------------------    Once you've `commit`ted your code, and `push`ed it into your github fork you can issue a [Pull Request](https://help.github.com/articles/using-pull-requests) so that we can add a reference to your module in this README file.    Alternatively, we can also host your repository in the jsonld-java organisation to give it more visibility.    CHANGELOG  =========  ### 2021-12-13  * Release 0.13.4  * Switch test logging from log4j to logback (Patch by @ansell)  * Improve Travis CI build Performance (Patch by @YunLemon)    ### 2021-03-06  * Release 0.13.3  * Fix @type when subject and object are the same (Reported by @barthanssens, Patch by @umbreak)  * Ignore @base if remote context is not relative (Reported by @whikloj, Patch by @dr0i)  * Fix throwing recursive context inclusion (Patch by @umbreak)    ### 2020-09-24  * Release 0.13.2  * Fix Guava dependency shading (Reported by @ggrasso)  * Fix @context issues when using a remote context (Patch by @umbreak)  * Deprecate Context.serialize (Patch by @umbreak)    ### 2020-09-09  * Release 0.13.1  * Fix java.net.URI resolution (Reported by @ebremer and @afs, Patch by @dr0i)  * Shade Guava failureaccess module (Patch by @peacekeeper)  * Don't minimize Guava class shading (Patch by @elahrvivaz)  * Follow link headers to @context files (Patch by @dr0i and @fsteeg)    ### 2019-11-28  * Release 0.13.0  * Bump Jackson versions to latest for security updates (Patch by @afs)  * Do not canonicalise XSD Decimal typed values (Patch by @jhg023)  * Bump dependency and plugin versions    ### 2019-08-03  * Release 0.12.5  * Bump Jackson versions to latest for security updates (Patches by @afs)  * IRI resolution fixes (Patch by @fsteeg)    ### 2019-04-20  * Release 0.12.4  * Bump Jackson version to 2.9.8  * Add a regression test for a past framing bug  * Throw error on empty key  * Add regression tests for workarounds to Text/URL dual definitions  * Persist JsonLdOptions through normalize/toRDF    ### 2018-11-24  * Release 0.12.3  * Fix NaN/Inf/-Inf raw value types on conversion to RDF  * Added fix for wrong rdf:type to @type conversion (Path by @umbreak)  * Open up Context.getTypeMapping and Context.getLanguageMapping for reuse    ### 2018-11-03  * W3c json ld syntax 34 allow container set on aliased type (Patch by @dr0i)  * Release 0.12.2    ### 2018-09-05  * handle omit graph flag (Patch by @eroux)  * Release 0.12.1  * Make pruneBlankNodeIdentifiers false by default in 1.0 mode and always true in 1.1 mode (Patch by @eroux)  * Fix issue with blank node identifier pruning when @id is aliased (Patch by @eroux)  * Allow wildcard {} for @id in framing (Patch by @eroux)    ### 2018-07-07  * Fix tests setup for schema.org with HttpURLConnection that break because of the inability of HttpURLConnection to redirect from HTTP to HTTPS    ### 2018-04-08  * Release 0.12.0  * Encapsulate RemoteDocument and make it immutable    ### 2018-04-03  * Fix performance issue caused by not caching schema.org and others that use ``Cache-Control: private`` (Patch by @HansBrende)  * Cache classpath scans for jarcache.json to fix a similar performance issue  * Add internal shaded dependency on Google Guava to use maintained soft and weak reference maps rather than adhoc versions  * Make JsonLdError a RuntimeException to improve its use in closures  * Bump minor version to 0.12 to reflect the API incompatibility caused by JsonLdError and protected field change and hiding in JarCacheStorage    ### 2018-01-25  * Fix resource leak in JsonUtils.fromURL on unsuccessful requests (Patch by @plaplaige)    ### 2017-11-15  * Ignore UTF BOM (Patch by @christopher-johnson)    ### 2017-08-26  * Release 0.11.1  * Fix @embed:@always support (Patch by @dr0i)    ### 2017-08-24  * Release 0.11.0    ### 2017-08-22  * Add implicit ""flag only"" subframe to fix incomplete list recursion (Patch by @christopher-johnson)  * Support pruneBlankNodeIdentifiers framing option in 1.1 mode (Patch by @fsteeg and @eroux)  * Support new @embed values (Patch by @eroux)    ### 2017-07-11  * Add injection of contexts directly into DocumentLoader (Patch by @ryankenney)  * Fix N-Quads content type (Patch by @NicolasRouquette)  * Add JsonUtils.fromJsonParser (Patch by @dschulten)    ### 2017-02-16  * Make literals compare consistently (Patch by @stain)  * Release 0.10.0    ### 2017-01-09  * Propagate causes for JsonLdError instances where they were caused by other Exceptions  * Remove schema.org hack as it appears to work again now...  * Remove deprecated and unused APIs  * Bump version to 0.10.0-SNAPSHOT per the removed/changed APIs    ### 2016-12-23  * Release 0.9.0  * Fixes schema.org support that is broken with Apache HTTP Client but works with java.net.URL    ### 2016-05-20  * Fix reported NPE in JsonLdApi.removeDependents    ### 2016-05-18  * Release 0.8.3  * Fix @base in remote contexts corrupting the local context    ### 2016-04-23  * Support @default inside of sets for framing    ### 2016-02-29  * Fix ConcurrentModificationException in the implementation of the Framing API    ### 2016-02-17  * Re-release version 0.8.2 with the refactoring work actually in it. 0.8.1 is identical in functionality to 0.8.0  * Release version 0.8.1  * Refactor JSONUtils and DocumentLoader to move most of the static logic into JSONUtils, and deprecate the DocumentLoader versions    ### 2016-02-10  * Release version 0.8.0    ### 2015-11-19  * Replace deprecated HTTPClient code with the new builder pattern  * Chain JarCacheStorage to any other HttpCacheStorage to simplify the way local caching is performed  * Bump version to 0.8.0-SNAPSHOT as some interface method parameters changed, particularly, DocumentLoader.setHttpClient changed to require CloseableHttpClient that was introduced in HttpClient-4.3    ### 2015-11-16  * Bump dependencies to latest versions, particularly HTTPClient that is seeing more use on 4.5/4.4 than the 4.2 series that we have used so far  * Performance improvements for serialisation to N-Quads by replacing string append and replace with StringBuilder  * Support setting a system property, com.github.jsonldjava.disallowRemoteContextLoading, to ""true"" to disable remote context loading.    ### 2015-09-30  * Release 0.7.0    ### 2015-09-27  * Move Tools, Clerezza and RDF2GO modules out to separate repositories. The Tools repository had a circular build dependency with Sesame, while the other modules are best located and managed in separate repositories    ### 2015-08-25  * Remove Sesame-2.7 module in favour of sesame-rio-jsonld for Sesame-2.8 and 4.0  * Fix bug where parsing did not fail if content was present after the end of a full JSON top level element    ### 2015-03-12  * Compact context arrays if they contain a single element during compaction  * Bump to Sesame-2.7.15    ### 2015-03-01  * Use jopt-simple for the playground cli to simplify the coding and improve error messages  * Allow RDF parsing and writing using all of the available Sesame Rio parsers through the playground cli  * Make the httpclient dependency OSGi compliant    ### 2014-12-31  * Fix locale sensitive serialisation of XSD double/decimal typed literals to always be Locale.US  * Bump to Sesame-2.7.14  * Bump to Clerezza-0.14    ### 2014-11-14  * Fix identification of integer, boolean, and decimal in RDF-JSONLD with useNativeTypes  * Release 0.5.1    ### 2014-10-29  * Add OSGi metadata to Jar files  * Bump to Sesame-2.7.13    ### 2014-07-14  * Release version 0.5.0  * Fix Jackson parse exceptions being propagated through Sesame without wrapping as RDFParseExceptions    ### 2014-07-02  * Fix use of Java-7 API so we are still Java-6 compatible  * Ensure that Sesame RDFHandler endRDF and startRDF are called in SesameTripleCallback    ### 2014-06-30  * Release version 0.4.2  * Bump to Sesame-2.7.12  * Remove Jena integration module, as it is now maintained by Jena team in their repository    ### 2014-04-22  * Release version 0.4  * Bump to Sesame-2.7.11  * Bump to Jackson-2.3.3  * Bump to Jena-2.11.1    ### 2014-03-26  * Bump RDF2GO to version 5.0.0    ### 2014-03-24  * Allow loading remote @context from bundled JAR cache  * Support JSON array in @context with toRDF   * Avoid exception on @context with default @language and unmapped key    ### 2014-02-24  * Javadoc some core classes, JsonLdProcessor, JsonLdApi, and JsonUtils  * Rename some core classes for consistency, particularly JSONUtils to JsonUtils and JsonLdTripleCallback  * Fix for a Context constructor that wasn't taking base into account    ### 2014-02-20  * Fix JsonLdApi mapping options in framing algorithm (Thanks Scott Blomquist @sblom)    ### 2014-02-06    * Release version 0.3  * Bump to Sesame-2.7.10  * Fix Jena module to use new API    ### 2014-01-29    * Updated to final Recommendation  * Namespaces supported by Sesame integration module  * Initial implementation of remote document loading  * Bump to Jackson-2.3.1    ### 2013-11-22    * updated jena writer    ### 2013-11-07    * Integration packages renamed com.github.jsonldjava.sesame,     com.github.jsonldjava.jena etc. (Issue #76)      ### 2013-10-07    * Matched class names to Spec   - Renamed `JSONLDException` to `JsonLdError`   - Renamed `JSONLDProcessor` to `JsonLdApi`   - Renamed `JSONLD` to `JsonLdProcessor`   - Renamed `ActiveContext` to `Context`   - Renamed `Options` to `JsonLdOptions`  * All context related utility functions moved to be members of the `Context` class    ### 2013-09-30  * Fixed JSON-LD to Jena to handle of BNodes    ### 2013-09-02    * Add RDF2Go integration  * Bump Sesame and Clerezza dependency versions    ### 2013-06-18    * Bump to version 0.2  * Updated Turtle integration  * Added Caching of contexts loaded from URI  * Added source formatting eclipse config  * Fixed up seasame integration package names  * Replaced depreciated Jackson code    ### 2013-05-19    * Added Turtle RDFParser and TripleCallback  * Changed Maven groupIds to `com.github.jsonld-java` to match github domain.  * Released version 0.1    ### 2013-05-16    * Updated core code to match [JSON-LD 1.0 Processing Algorithms and API / W3C Editor's Draft 14 May 2013](http://json-ld.org/spec/latest/json-ld-api/)  * Deprecated JSONLDSerializer in favor of the RDFParser interface to better represent the purpose of the interface and better fit in with the updated core code.  * Updated the JSONLDTripleCallback to better fit with the updated code.  * Updated the Playground tool to support updated core code.    ### 2013-05-07    * Changed base package names to com.github.jsonldjava  * Reverted version to 0.1-SNAPSHOT to allow version incrementing pre 1.0 while allowing a 1.0 release when the json-ld spec is finalised.  * Turned JSONLDTripleCallback into an interface.    ### 2013-04-18    * Updated to Sesame 2.7.0, Jena 2.10.0, Jackson 2.1.4  * Fixing a character encoding issue in the JSONLDProcessorTests  * Bumping to 1.0.1 to reflect dependency changes    ### 2012-10-30    * Brought the implementation up to date with the reference implementation (minus the normalization stuff)  * Changed entry point for the functions to the static functions in the JSONLD class  * Changed the JSONLDSerializer to an abstract class, requiring the implementation of a ""parse"" function. The JSONLDSerializer is now passed to the JSONLD.fromRDF function.  * Added JSONLDProcessingError class to handle errors more efficiently      Considerations for 1.0 release / optimisations  =========    * The `Context` class is a `Map` and many of the options are stored as values of the map. These could be made into variables, whice should speed things up a bit (the same with the termDefinitions variable inside the Context).  * some sort of document loader interface (with a mockup for testing) is required   """
Semantic web;https://github.com/davideceolin/rcsvw;"""# rcsvw    Package that implements the candidate recommendations from the W3C CSV on the Web Working Group.    # Copyright / License    ## rcsvw package    Copyright (C) 2011-2015  Davide Ceolin    ## Authors / Contributors    Author: Davide Ceolin    This package relies on an extended version of the rrdf package. The instructions below allow installing this version of rrdf, before installing csvw.    # Install from R        > install.packages(""rJava"") # if not present already      > install.packages(""devtools"") # if not present already      > library(devtools)      > install_github(""davideceolin/rrdf"", ref=""extended"", subdir=""rrdflibs"")      > install_github(""davideceolin/rrdf"", ref=""extended"", subdir=""rrdf"", build_vignettes = FALSE)      > install_github(""davideceolin/rcsvw"") """
Semantic web;https://github.com/clarkparsia/sparql-proxy;"""## Simple SPARQL Endpoint Proxy Servlet    Sparql Endpoint Proxy Servlet - Helps for bypassing CORS issues.    Simple proxy that redirects all elements in the request to a configured SPARQL endpoint. Headers in the request are also redirected.    ### Configuring the proxy    The only setting required to configure the SPARQL Endpoint proxy is `proxy.host` which refers to the host:port info in which the Stardog HTTP service is running, for example:        <init-param>  		<param-name>proxy.host</param-name>  		<param-value>http://localhost:5822</param-value>  	</init-param>    The previous indicats the a Stardog DB is running the HTTP protocol in `http://localhost:5822`, which can be modified to any other information, either a local reference or remote.    ### Building .war file with Ant    To build the proxy war, just execute:        ant clean web-dist        This will generate the `sparql-proxy.war` file that you just need to copy to your app container `webapps` directory.    ### Using the proxy    To use the proxy, simple send the request to it, following the same pattern as you would do with a Stardog DB directly. For instance, to query the DB `gov` directly to Stardog HTTP endpoint you'll do:        curl -X GET ""http://localhost:5822/gov/query?query=select%20*%20where%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20limit%2010""        Using the proxy (with CORS support), the same can be done:        curl -X GET ""http://localhost:8181/sparql-proxy/gov/query?query=select%20*%20where%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20limit%2010""        where:    * `http://localhost:8181/sparql-proxy` is the path for the sparql-proxy (with servlet containter running in port 8181)  * `gov` is the Stardog DB  * `query?query=` points to the encoded query.   """
Semantic web;https://github.com/guiveg/rdfsurveyor;"""RDF Surveyor  ==========  RDF Surveyor is an easy-to-use exploration tool of semantic datasets. It can be plugged in any CORS-enabled SPARQL 1.1 endpoint without requiring any installation.    These are some of the features of RDF Surveyor:    * Intuitive user interface that completely hides the RDF/SPARQL syntax    * It gives an overview of the repository contents, supports class navigation and individual visualization    * No installation required for exploring any RDF dataset    * Works with large datasets such as DBpedia    * Prepared to work with multilingual datasets    * The UI adapts to mobile devices    * RESTful design      Please cite RDF Surveyor as:    > G. Vega-Gorgojo, L. Slaughter, B.M. von-Zernichow, N. Nikolov, D. Roman. Linked Data Exploration with RDF Surveyor. IEEE Access. 7(1):172199-172213, December 2019.      Usage  ==========  RDF Surveyor is a web application developed in Javascript. You can easily deploy it in your web server or just try RDF Surveyor on [http://tools.sirius-labs.no/rdfsurveyor](http://tools.sirius-labs.no/rdfsurveyor)    To begin the exploration of a repository, you only need to copy the URI of the target SPARQL endpoint (and optionally the URI of the named graph). You can also import the URIs of some well-known repositories such as DBpedia with the ""Import configuration"" button.      Help us to improve  ==========  RDF Surveyor is available under an Apache 2 license. Please send us an email to [guiveg@ifi.uio.no](mailto:guiveg@ifi.uio.no) if you use or plan to use RDF Surveyor in a project. Drop us also a message if you have comments or suggestions for improvement.        Screenshots  ==========  Some screenshots of RDF Surveyor:    ![screenshot](/screenshots/config.png ""Config"")    ![screenshot](/screenshots/namespaces.png ""Namespaces"")    ![screenshot](/screenshots/upper.png ""Upper classes"")    ![screenshot](/screenshots/artwork.png ""Artwork class"")    ![screenshot](/screenshots/painting0.png ""The Surrender of Breda individual (1)"")    ![screenshot](/screenshots/painting1.png ""The Surrender of Breda individual (2)"")    ![screenshot](/screenshots/oslo.png ""Oslo"")      Configuration  ==========  You can edit the parameters of the configuration file at `etc/data/config.js`:    * `pagesize`: number of elements per page (default 10)    * `hidemax`: maximum number of elements to show in a list before including a *show more* button (default 8)    * `hidebegin`: element index to begin hiding (default 5)    * `repos`: preloaded configuration of repositories. Each element should have a name, an endpoint URI, and optionally a named graph URI    * `geoenabled`: if true, RDF Surveyor will try to find geographic coordinates for individuals and show a map widget provided by [Leaflet](http://leafletjs.com/).     * `geooptions`: the map widget provided by [Leaflet](http://leafletjs.com/) requires your own `accessToken`. The rest of parameters here should not be changed    * `gaenabled`: if true, RDF Surveyor will log events (requested resource, latency, and number of SPARQL queries) through [Google Analytics](https://www.google.com/analytics/)    * `gaproperty`: you have to provide your own property in order to log Google Analytics events         """
Semantic web;https://github.com/kbss-cvut/jb4jsonld;"""# Java Binding for JSON-LD    [![Build Status](https://kbss.felk.cvut.cz/jenkins/buildStatus/icon?job=jaxb-jsonld)](https://kbss.felk.cvut.cz/jenkins/job/jaxb-jsonld)    Java Binding for JSON-LD (JB4JSON-LD) is a simple library for serialization of Java objects into JSON-LD and vice versa.    Note that this is the core, abstract implementation. For actual usage, a binding like [https://github.com/kbss-cvut/jb4jsonld-jackson](https://github.com/kbss-cvut/jb4jsonld-jackson)  has to be used.      ## Usage    JB4JSON-LD is based on annotations from [JOPA](https://github.com/kbss-cvut/jopa), which enable POJO attributes  to be mapped to ontological constructs (i.e. to object, data or annotation properties) and Java classes to ontological  classes.    Use `@OWLDataProperty` to annotate data fields and `@OWLObjectProperty` to annotate fields referencing other mapped entities.    See [https://github.com/kbss-cvut/jopa-examples/tree/master/jsonld](https://github.com/kbss-cvut/jopa-examples/tree/master/jsonld) for  an executable example of JB4JSON-LD in action (together with Spring and Jackson).      ## Example    ### Java    ```Java  @OWLClass(iri = ""http://onto.fel.cvut.cz/ontologies/ufo/Person"")  public class User {        @Id      public URI uri;        @OWLDataProperty(iri = ""http://xmlns.com/foaf/0.1/firstName"")      private String firstName;        @OWLDataProperty(iri = ""http://xmlns.com/foaf/0.1/lastName"")      private String lastName;            @OWLDataProperty(iri = ""http://xmlns.com/foaf/0.1/accountName"")      private String username;        @OWLDataProperty(iri = ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/role"")      private Role role;  // Role is an enum        @Properties      private Map<String, Set<String>> properties;            // Getters and setters follow  }  ```    ### JSON-LD    ```JSON  {    ""@context"": {      ""firstName"": ""http://xmlns.com/foaf/0.1/firstName"",      ""lastName"": ""http://xmlns.com/foaf/0.1/lastName"",      ""accountName"": ""http://xmlns.com/foaf/0.1/accountName"",      ""isAdmin"": ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/isAdmin"",      ""role"": ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/role""    },    ""@id"": ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld#Catherine+Halsey"",    ""@type"": [      ""http://onto.fel.cvut.cz/ontologies/ufo/Person"",      ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/User"",      ""http://onto.fel.cvut.cz/ontologies/ufo/Agent""    ],    ""isAdmin"": true,    ""accountName"": ""halsey@unsc.org"",    ""firstName"": ""Catherine"",    ""lastName"": ""Halsey"",    ""role"": ""USER""  }  ```    ## Configuration    Parameter | Default value | Explanation  ----------|---------------|-----------  `ignoreUnknownProperties` | `false` | Whether to ignore unknown properties when deserializing JSON-LD. Default behavior throws an exception.  `scanPackage` | `""""` | Package in which the library should look for mapped classes. The scan is important for support for polymorphism in object deserialization.  It is highly recommended to specify this value, otherwise the library will attempt to load and scan all classes on the classpath.  `requireId` | `false` | Whether to require an identifier when serializing an object. If set to `true` and no identifier is found (either there is no `@Id` field or its value is `null`), an exception will be thrown. By default a blank node identifier is generated if no id is present.  `assumeTargetType` | `false` | Whether to allow assuming target type in case the JSON-LD object does not contain types (`@type`). If set to `true`, the provided Java type (deserialization invocation argument, field type) will be used as target type.  `enableOptimisticTargetTypeResolution` | `false` | Whether to enable optimistic target type resolution. If enabled, this allows to pick a target type even if there are multiple matching classes (which would normally end with an `AmbiguousTargetTypeException`).  `preferSuperclass` | `false` | Allows to further specify optimistic target type resolution. By default, any of the target classes may be selected. Setting this to `true` will make the resolver attempt to select a superclass of the matching classes (if it is also in the target set).     See `cz.cvut.kbss.jsonld.ConfigParam`.    ## Documentation    Documentation is on the [Wiki](https://github.com/kbss-cvut/jb4jsonld/wiki). API Javadoc is also [available](https://kbss.felk.cvut.cz/jenkins/view/Java%20Tools/job/jaxb-jsonld/javadoc/).    ## Getting JB4JSON-LD    There are two ways to get JB4JSON-LD:    * Clone repository/download zip and build it with Maven,  * Use a [Maven dependency](http://search.maven.org/#search%7Cga%7C1%7Ccz.cvut.kbss.jsonld):    ```XML  <dependency>      <groupId>cz.cvut.kbss.jsonld</groupId>      <artifactId>jb4jsonld</artifactId>  </dependency>  ```    Note that you will most likely need an integration with a JSON-serialization library like [JB4JSON-LD-Jackson](https://github.com/kbss-cvut/jb4jsonld-jackson).      ## License    LGPLv3 """
Semantic web;https://github.com/ontodia-org/ontodia;"""# Ontodia [![npm](https://img.shields.io/npm/v/ontodia.svg)](https://www.npmjs.com/package/ontodia) [![CircleCI](https://circleci.com/gh/sputniq-space/ontodia.svg?style=svg)](https://circleci.com/gh/sputniq-space/ontodia) #    Ontodia is a JavaScript library that allows to visualize, navigate and explore data in the form of an interactive graph based on underlying data sources.    ## What is Ontodia for?    Ontodia allows you to create and persist diagrams made from existing data - relational, object, semantic.    It was designed to visualize RDF data sets in particular, but could be tailored to almost any data source by implementing a data provider interface.      ## Core features    - Visual navigation and diagramming over large graph data sets  - Rich graph visualization and context-aware navigation features   - Ability to store and retrieve diagrams  - User friendly - no graph query language or prior knowledge of the schema required  - Customizable user interface (by modifying templates for nodes and links) and data storage back-end    ## How to try it?    You can follow developer tutorials at the [developer documentation page](https://github.com/metaphacts/ontodia/wiki)    ## License    The Ontodia library is distributed under LGPL-2.1. A commercial license with additional features, support and custom development is available, please contact us at [info@metaphacts.com](info@metaphacts.com).         ## Developer documentation and contributing    Developer documentation is available at [wiki page](https://github.com/metaphacts/ontodia/wiki).    ## Giving Ontodia people credit    If you use the Ontodia library in your projects, please provide a link to this repository in your publication and a citation reference to the following paper:     Mouromtsev, D., Pavlov, D., Emelyanov, Y., Morozov, A., Razdyakonov, D. and Galkin, M., 2015. The Simple Web-based Tool for Visualization and Sharing of Semantic Data and Ontologies. In International Semantic Web Conference (Posters & Demos).    ```  @inproceedings{Mouromtsev2015,      author = {Mouromtsev, Dmitry and Pavlov, Dmitry and Emelyanov, Yury and          Morozov, Alexey and Razdyakonov, Daniil and Galkin, Mikhail},      year = {2015},      month = {10},      title = {The Simple Web-based Tool for Visualization and Sharing of Semantic Data and Ontologies},      booktitle = {International Semantic Web Conference (Posters & Demos)}  }  ```    It really helps our team to gain publicity and acknowledgment for our efforts.  Thank you for being considerate! """
Semantic web;https://github.com/AKSW/Sparqlify;"""# Sparqlify SPARQL->SQL rewriter  [![Build Status](http://ci.aksw.org/jenkins/job/Sparqlify/badge/icon)](http://ci.aksw.org/jenkins/job/Sparqlify/)      ## Introduction    Sparqlify is a scalable SPARQL-SQL rewriter whose development began in April 2011 in the course of the [LinkedGeoData](http://linkedgeodata.org) project.    This system's features/traits are:  * Support of the ['Sparqlification Mapping Language' (SML)](http://sparqlify.org/wiki/SML), an intuitive language for expressing RDB-RDF mappings with only very little syntactic noise.  * Scalability: Sparqlify does not evaluate expressions in memory. All SPARQL filters end up in the corresponding SQL statement, giving the underlying RDBMS has maximum control over query planning.  * A powerful rewriting engine that analyzes filter expressions in order to eleminate self joins and joins with unsatisfiable conditions.  * Initial support for spatial datatypes and predicates.  * A subset of the SPARQL 1.0 query language plus sub queries are supported.  * Tested with PostgreSQL/Postgis and H2. Support for further databases is planned.  * CSV support  * R2RML will be supported soon    ## Supported SPARQL language features  * Join, LeftJoin (i.e. Optional), Union, Sub queries  * Filter predicates: comparison: (<=, <, =, >, >=) logical: (!, &&; ||) arithmetic: (+, -) spatial: st\_intersects, geomFromText; other: regex, lang, langMatches    * Aggregate functions: Count(\*)  * Order By is pushed into the SQL      ## Debian packages    Sparqlify Debian packages can be obtained by following means:  * Via the [Linked Data Stack](http://stack.linkeddata.org) (recommended)  * Download from the [Sparqlify website's download section](http://sparqlify.org/downloads/releases).  * Directly from source using maven (read down the README)    ### Public repositories    After setting up any of the repositories below, you can install sparqlify with apt using    * apt: `sudo apt-get install sparqlify-cli    #### Linked Data Stack (this is what you want)    Sparqlify is distributed at the [Linked Data Stack](http://stack.linkeddata.org), which offers many great tools done by various contributors of the Semantic Web community.    * The repository is available in the flavors `nightly`, `testing` and `stable` [here](http://stack.linkeddata.org/download/repo.php).    ```bash  # !!! Replace stable with nightly or testing as needed !!!    # Download the repository package  wget http://stack.linkeddata.org/ldstable-repository.deb    # Install the repository package  sudo dpkg -i ldstable-repository.deb    # Update the repository database  sudo apt-get update  ```      #### Bleeding Edge (Not recommended for production)  For the latest development version (built on every commit) perform the following steps    Import the public key with        wget -qO - http://cstadler.aksw.org/repos/apt/conf/packages.precise.gpg.key  | sudo apt-key add -    Add the repository        echo 'deb http://cstadler.aksw.org/repos/apt precise main contrib non-free' | sudo tee -a /etc/apt/sources.list.d/cstadler.aksw.org.list      Note that this also works with distros other than ""precise"" (ubuntu 12.04) such as ubuntu 14.04 or 16.04.        ## Building  Building the repository creates the JAR files providing the `sparqlify-*` tool suite.      ### Debian package  Building debian packages from this repo relies on the [Debian Maven Plugin](http://debian-maven.sourceforge.net]) plugin, which requires a debian-compatible environment.  If such an environment is present, the rest is simple:        # Install all shell scripts necessary for creating deb packages      sudo apt-get install devscripts        # Execute the follwing from the `<repository-root>/sparqlify-core` folder:      mvn clean install deb:package        # Upon sucessful completion, the debian package is located under `<repository-root>/sparqlify-core/target`      # Install using `dpkg`      sudo dpkg -i sparqlify_<version>.deb        # Uninstall using dpkg or apt:      sudo dpkg -r sparqlify      sudo apt-get remove sparqlify      ### Assembly based  Another way to build the project is run the following commands at `<repository-root>`        mvn clean install        cd sparqlify-cli      mvn assembly:assembly      This will generate a single stand-alone jar containing all necessary dependencies.  Afterwards, the shell scripts under `sparqlify-core/bin` should work.    ## Tool suite    If Sparqlify was installed from the debian package, the following commands are available system-wide:    * `sparqlify`: This is the main executable for running individual SPARQL queries, creating dumps and starting a stand-alone server.  * `sparqlify-csv`: This tool can create RDF dumps from CSV file based on SML view definitions.  * `sparqlify-platform`: A stand-alone server component integrating additional projects.    These tools write their output (such as RDF data in the N-TRIPLES format) to STDOUT. Log output goes to STDERR.    ### sparqlify  Usage: `sparqlify [options]`    Options are:    * Setup    * -m   SML view definition file    * Database Connectivity Settings    * -h   Hostname of the database (e.g. localhost or localhost:5432)    * -d   Database name    * -u   User name    * -p   Password    * -j   JDBC URI (mutually exclusive with both -h and -d)    * Quality of Service    * -n   Maximum result set size    * -t   Maximum query execution time in seconds (excluding rewriting time)    * Stand-alone Server Configuration    * -P   Server port [default: 7531]    * Run-Once (these options prevent the server from being started and are mutually exclusive with the server configuration)    * -D   Create an N-TRIPLES RDF dump on STDOUT     * -Q   [SPARQL query] Runs a SPARQL query against the configured database and view definitions    #### Example  The following command will start the Sparqlify HTTP server on the default port.        sparqlify -h localhost -u postgres -p secret -d mydb -m mydb-mappings.sml -n 1000 -t 30    Agents can now access the SPARQL endpoint at `http://localhost:7531/sparql`    ### sparqlify-csv  Usage: `sparqlify-csv [options]`    * Setup    * -m   SML view definition file    * -f   Input data file    * -v   View name (can be omitted if the view definition file only contains a single view)    * CSV Parser Settings    * -d   CSV field delimiter (default is '""')    * -e   CSV field escape delimiter (escapes the field delimiter) (default is '\')    * -s   CSV field separator (default is ',')    * -h   Use first row as headers. This option allows one to reference columns by name additionally to its index.      ### sparqlify-platform (Deprecated; about to be superseded by sparqlify-web-admin)  The Sparqlify Platform (under /sparqlify-platform) bundles Sparqlify with the Linked Data wrapper [Pubby](https://github.com/cygri/pubby) and the SPARQL Web interface [Snorql](https://github.com/kurtjx/SNORQL).    Usage: `sparqlify-platform config-dir [port]`     * `config-dir` Path to the configuration directory, e.g. `<repository-root/sparqlify-platform/config/example>`  * `port` Port on which to run the platform, default 7531.      For building, at the root of the project (outside of the sparqlify-\* directories), run `mvn compile` to build all modules.  Afterwards, lauch the platform using:        cd sparqlify-platform/bin      ./sparqlify-platform <path-to-config> <port>      Assuming the platform runs under `http://localhost:7531`, you can access the following services relative to this base url:  * `/sparql` is Sparqlify's SPARQL endpoint  * `/snorql` shows the SNORQL web frontend  * `/pubby` is the entry point to the Linked Data interface      #### Configuration  The configDirectory argument is mandatory and must contain a *sub-directory* for the context-path (i.e. `sparqlify-platform`) in turn contains the files:  * `platform.properties` This file contains configuration parameters that can be adjusted, such as the database connection.  * `views.sparqlify` The set of Sparqlify view definition to use.    I recommend to first create a copy of the files in `/sparqlify-platform/config/example` under a different location, then adjust the parameters and finally launch the platform with `-DconfigDirectory=...` set appropriately.    The platform *applies autoconfiguration to Pubby and Snorql*:  * Snorql: Namespaces are those of the views.sparqlify file.  * Pubby: The host name of all resources generated in the Sparqlify views is replaced with the URL of the platform (currently still needs to be configured via `platform.properties`)    Additionally you probably want to make the URIs nice by e.g. configuring an apache reverse proxy:    Enable the apache `proxy_http` module:    	sudo a2enmod proxy_http    Then in your `/etc/apache2/sites-available/default` add lines such as    	ProxyRequest Off  	ProxyPass /resource http://localhost:7531/pubby/bizer/bsbm/v01/ retry=1  	ProxyPassReverse /resource http://localhost:7531/pubby/bizer/bsbm/v01/    These entries will enable requests to `http://localhost/resource/...` rather than `http//localhost:7531/pubby/bizer/bsbm/v01/`.    The `retry=1` means, that apache only waits 1 seconds before retrying again when it encounters an error (e.g. HTTP code 500) from the proxied resource.    *IMPORTANT: ProxyRequests are off by default; DO NOT ENABLE THEM UNLESS YOU KNOW WHAT YOU ARE DOING. Simply enabling them potentially allows anyone to use your computer as a proxy.*      ## SML Mapping Syntax:  A Sparqlification Mapping Language (SML) configuration is essentially a set of CREATE VIEW statements, somewhat similar to the CREATE VIEW statement from SQL.  Probably the easiest way to learn to syntax is to look at the following resources:    * The [SML documentation](http://sparqlify.org/wiki/SML)  * The [SML test suite](https://github.com/AKSW/Sparqlify/tree/master/sparqlify-core/src/test/resources/org/aksw/sml/r2rml_tests) which is derived from the [R2RML test suite](https://github.com/AKSW/Sparqlify/tree/master/sparqlify-core/src/test/resources/org/w3c/r2rml_tests).    Two more examples are from    Additionally, for convenience, prefixes can be declared, which are valid throughout the config file.  As comments, you can use //, /\* \*/, and #.     For a first impression, here is a quick example:            /* This is a comment       * /* You can even nest them! */       */      // Prefixes are valid throughout the file      Prefix dbp:<http://dbpedia.org/ontology/>      Prefix ex:<http://ex.org/>        Create View myFirstView As          Construct {              ?s a dbp:Person .              ?s ex:workPage ?w .          }      With          ?s = uri('http://mydomain.org/person', ?id) // Define ?s to be an URI generated from the concatenation of a prefix with mytable's id-column.          ?w = uri(?work_page) // ?w is assigned the URIs in the column 'work_page' of 'mytable'      Constrain          ?w prefix ""http://my-organization.org/user/"" // Constraints can be used for optimization, e.g. to prune unsatisfiable join conditions      From          mytable; // If you want to use an SQL query, the query (without trailing semicolon) must be enclosed in double square brackets: [[SELECT id, work_page FROM mytable]]      ### Notes for sparqlify-csv  For `sparqlify-csv` view definition syntax is almost the same as above; the differences being:    * Instead of `Create View viewname As Construct` start your views with `CREATE VIEW TEMPLATE viewname As Construct`  * There is no FROM and CONSTRAINT clause    Colums can be referenced either by name (see the -h option) or by index (1-based).    #### Example        // Assume a CSV file with the following columns (osm stands for OpenStreetMap)      (city\_name, country\_name, osm\_entity\_type, osm\_id, longitude, latitude)        Prefix fn:<http://aksw.org/sparqlify/> //Needed for urlEncode and urlDecode.      Prefix rdfs:<http://www.w3.org/2000/01/rdf-schema#>      Prefix owl:<http://www.w3.org/2002/07/owl#>      Prefix xsd:<http://www.w3.org/2001/XMLSchema#>      Prefix geo:<http://www.w3.org/2003/01/geo/wgs84_pos#>        Create View Template geocode As        Construct {          ?cityUri            owl:sameAs ?lgdUri .            ?lgdUri            rdfs:label ?cityLabel ;            geo:long ?long ;            geo:lat ?lat .        }        With          ?cityUri = uri(concat(""http://fp7-pp.publicdata.eu/resource/city/"", fn:urlEncode(?2), ""-"", fn:urlEncode(?1)))          ?cityLabel = plainLiteral(?1)          ?lgdUri = uri(concat(""http://linkedgeodata.org/triplify/"", ?4, ?5))          ?long = typedLiteral(?6, xsd:float)          ?lat = typedLiteral(?7, xsd:float)       """
Semantic web;https://github.com/drobilla/sord;"""Sord  ====    Sord is a lightweight C library for storing RDF statements in memory.  For more information, see <http://drobilla.net/software/sord>.     -- David Robillard <d@drobilla.net>   """
Semantic web;https://github.com/RDFLib/graph-pattern-learner;"""Graph Pattern Learner  =====================    (Work in progress...)    In this repository you find the code for a graph pattern learner. Given a list  of source-target-pairs and a SPARQL endpoint, it will try to learn SPARQL  patterns. Given a source, the learned patterns will try to lead you to the right  target.    The algorithm was first developed on a list of human associations that had been  mapped to DBpedia entities, as can be seen in  [data/gt_associations.csv](./data/gt_associations.csv):    | source                            | target                            |  | --------------------------------- | --------------------------------- |  | http://dbpedia.org/resource/Bacon | http://dbpedia.org/resource/Egg   |  | http://dbpedia.org/resource/Baker | http://dbpedia.org/resource/Bread |  | http://dbpedia.org/resource/Crow  | http://dbpedia.org/resource/Bird  |  | http://dbpedia.org/resource/Elm   | http://dbpedia.org/resource/Tree  |  | http://dbpedia.org/resource/Gull  | http://dbpedia.org/resource/Bird  |  | ...                               | ...                               |    As you can immediately see, associations don't only follow a single pattern. Our  algorithm is designed to be able to deal with this. It will try to learn several  patterns, which in combination model your input list of source-target-pairs. If  your list of source-target-pairs is less complicated, the algorithm will happily  terminate earlier.    You can find more information about the algorithm and learning patterns for  human associations on https://w3id.org/associations . The page also includes  publications, as well as the resulting patterns learned for human associations  from a local DBpedia endpoint including wikilinks.      Requirements  ------------    To run the graph pattern learner, we recommend:  - 8 cores (for parallel execution)  - more than 8 GB free RAM  - Linux 64 bit with Python 2.7      Installation  ------------    For now, the suggested installation method is via git clone (also allows easier  contributions):        git clone https://github.com/RDFLib/graph-pattern-learner.git      cd graph-pattern-learner    Afterwards, to setup the virtual environment and install all dependencies in it:        virtualenv venv &&      . venv/bin/activate &&      pip install --upgrade pip setuptools &&      pip install -r requirements.txt &&      deactivate      Running the learner  -------------------    Before actually running the evolutionary algorithm, please consider that it will  issue a lot of queries to the endpoint you're specifying. Please don't run this  against public endpoints without asking the providers first. It is likely that  you will disrupt their service or get blacklisted. I suggest running against an  own local endpoint filled with the datasets you're interested in. If you really  want to run this against public endpoints, at least don't run the multi-process  version, but restrict yourself to one process.    Always feel free to reach out for help or feedback via the issue tracker or via  associations at joernhees de. We might even run the learner for you ;)    To get a list of all available options run:        . venv/bin/activate && python run.py --help ; deactivate    Don't be scared by the length, most options use sane defaults, but it's nice to  be able to change things once you become more familiar with your data and the  learner.    The options you will definitely be interested are:        --associations_filename (defaults to ./data/gt_associations.csv)      --sparql_endpoint (defaults to http://localhost:8890/sparql)    To run a full training cycle, you probably might want to execute this:        ./run_create_bundle.sh --processes=8 --sparql_endpoint=... --visualise \          ./results/your_bundle_name \          --associations_filename=... # ... other-options ...    The algorithm will then by default randomly split your input list of  source-target-pairs into a training and a test set, train on the training set,  visualise the resulting learned patterns in `./results/bundle_name/visualise`,  before evaluating predictions on first the training- and then the test-set.    To use a learned model for prediction, you can run:        . venv/bin/activate && \      PYTHONIOENCODING=utf-8 python \          -m scoop -n8 run.py --associations_filename=... --sparql_endpoint=... \          --RES_DIR=./results/your_bundle_name/results \          --predict=manual ; \      deactivate      Contributors  ------------   * Jörn Hees   * Rouven Bauer (visualise code) """
Semantic web;https://github.com/OpenTriply/YASGUI.legacy;"""IMPORTANT  ======  This is the old, unsupported (but relatively stable) YASGUI repository, still accessible at [http://legacy.yasgui.org][2]    The new YASGUI library is accessible at [http://yasgui.org][1].  For more info, visit [http://about.yasgui.org][3].      [1]: http://yasgui.org    [2]: http://legacy.yasgui.org    [3]: http://about.yasgui.org    [4]: http://yasqe.yasgui.org    [5]: http://yasr.yasgui.org """
Semantic web;https://github.com/AKSW/FOX;"""[4]: https://dice-research.org/FOX  [6]: https://fox.demos.dice-research.org/    [![Build Status](https://travis-ci.org/dice-group/FOX.svg?branch=master)](https://travis-ci.org/dice-group/FOX)  [![BCH compliance](https://bettercodehub.com/edge/badge/dice-group/FOX?branch=master)](https://bettercodehub.com/)  [![Project Stats](https://www.openhub.net/p/FOX-Framework/widgets/project_thin_badge.gif)](https://www.openhub.net/p/FOX-Framework)  <!---  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/348e14317ea140cbb98a110c40718d88)](https://www.codacy.com/app/renespeck/FOX?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=dice-group/FOX&amp;utm_campaign=Badge_Grade)  -->    ## FOX - Federated Knowledge Extraction Framework  FOX ([https://dice-research.org/FOX][4]) is a framework that integrates the Linked Data Cloud and makes use of the diversity of NLP algorithms to extract RDF triples of high accuracy out of NL.    In its current version, it integrates and merges the results of Named Entity Recognition tools as well as it integrates several Relation Extraction tools.    ## Requirements  Java 8, Maven 3, graphviz (for JavaDoc only)    ## Documentation:  [documentation](documentation/readme.md).    ## Demo  This version supports multiple languages for NER, NED and RE.    Live Demo: [https://fox.demos.dice-research.org/][6]    ## How to cite    English version with details:    ```Tex  @incollection{    year={2014},    isbn={978-3-319-11963-2},    booktitle={The Semantic Web – ISWC 2014},    volume={8796},    series={Lecture Notes in Computer Science},    title={Ensemble Learning for Named Entity Recognition},    publisher={Springer International Publishing},    author={Ren{\'e} Speck and Axel-Cyrille {Ngonga Ngomo}},  }  ```    The extended version for multiple languages:    ```Tex  @InProceedings{speck2017,     author={Ren{\'e} Speck and Axel-Cyrille {Ngonga Ngomo}},     title={{Ensemble Learning of Named Entity Recognition Algorithms using Multilayer Perceptron for the Multilingual Web of Data}},     booktitle={K-CAP 2017: Knowledge Capture Conference},     year={2017},     pages={4},     organization={ACM}   }   ```    ## License    FOX is licensed under the [GNU Affero General Public License v3.0](LICENSE) (license document is in the application folder).    FOX uses several other libraries. An incomplete list is as follows:  * Illinois NLP Pipeline  (University of Illinois Research and Academic Use License)  * Stanford CoreNLP (GNU GPL Version 2)  * Apache OpenNLP (Apache License, Version 2)  * Balie (GNU GPL Version 2)      ## Bugs  Found a :bug: bug? [Open an issue](https://github.com/dice-group/FOX/issues/new) with some [emojis](http://emoji.muan.co). Issues without emojis are not valid. :trollface: """
Semantic web;https://github.com/wallix/triplestore;"""[![Build Status](https://api.travis-ci.org/wallix/triplestore.svg?branch=master)](https://travis-ci.org/wallix/triplestore)  [![Go Report Card](https://goreportcard.com/badge/github.com/wallix/triplestore)](https://goreportcard.com/report/github.com/wallix/triplestore)  [![GoDoc](https://godoc.org/github.com/wallix/triplestore?status.svg)](https://godoc.org/github.com/wallix/triplestore)     # Triple Store    Triple Store is a library to manipulate RDF triples in a fast and fluent fashion.    RDF triples allow to represent any data and its relations to other data. It is a very versatile concept and is used in [Linked Data](https://en.wikipedia.org/wiki/Linked_data), graphs traversal and storage, etc....    Here the RDF triples implementation follows along the [W3C RDF concepts](https://www.w3.org/TR/rdf11-concepts/). (**Note that reification is not implemented**.). More digestible info on [RDF Wikipedia](https://en.wikipedia.org/wiki/Resource_Description_Framework)    ## Features overview    - Create and manage triples through a convenient DSL  - Snapshot and query RDFGraphs  - **Binary** encoding/decoding  - **Lenient NTriples** encoding/decoding (see W3C Test suite in _testdata/ntriples/w3c_suite/_)  - [DOT](https://en.wikipedia.org/wiki/DOT_(graph_description_language)) encoding  - Stream encoding/decoding (for binary & NTriples format) for memory conscious program   - CLI (Command line interface) utility to read and convert triples files.    ## Library     This library is written using the [Golang](https://golang.org) language. You need to [install Golang](https://golang.org/doc/install) before using it.    Get it:    ```sh  go get -u github.com/wallix/triplestore  ```    Test it:    ```  go test -v -cover -race github.com/wallix/triplestore  ```    Bench it:    ```  go test -run=none -bench=. -benchmem  ```    Import it in your source code:    ```go  import (  	""github.com/wallix/triplestore""  	// tstore ""github.com/wallix/triplestore"" for less verbosity  )  ```    Get the CLI with:    ```  go get -u github.com/wallix/triplestore/cmd/triplestore  ```    ## Concepts    A triple is made of 3 components:        subject -> predicate -> object    ... or you can also view that as:        entity -> attribute -> value    So    - A **triple** consists of a *subject*, a *predicate* and a *object*.  - A **subject** is a unicode string.  - A **predicate** is a unicode string.  - An **object** is a *resource* (or IRI) or a *literal* (blank node are not supported).  - A **literal** is a unicode string associated with a datatype (ex: string, integer, ...).  - A **resource**, a.k.a IRI, is a unicode string which point to another resource.    And    - A **source** is a persistent yet mutable source or container of triples.  - A **RDFGraph** is an **immutable set of triples**. It is a snapshot of a source and queryable .  - A **dataset** is a basically a collection of *RDFGraph*.    You can also view the library through the [godoc](https://godoc.org/github.com/wallix/triplestore)    ## Usage    #### Create triples    Although you can build triples the way you want to model any data, they are usually built from known RDF vocabularies & namespace. Ex: [foaf](http://xmlns.com/foaf/spec/), ...    ```go  triples = append(triples,  	SubjPred(""me"", ""name"").StringLiteral(""jsmith""),   	SubjPred(""me"", ""age"").IntegerLiteral(26),   	SubjPred(""me"", ""male"").BooleanLiteral(true),   	SubjPred(""me"", ""born"").DateTimeLiteral(time.Now()),   	SubjPred(""me"", ""mother"").Resource(""mum#121287""),  )  ```    or dynamically and even shorter with    ```go  triples = append(triples,   	SubjPredLit(""me"", ""age"", ""jsmith""), // String literal object   	SubjPredLit(""me"", ""age"", 26), // Integer literal object   	SubjPredLit(""me"", ""male"", true), // Boolean literal object   	SubjPredLit(""me"", ""born"", time.now()) // Datetime literal object   	SubjPredRes(""me"", ""mother"", ""mum#121287""), // Resource object  )  ```    or with blank nodes and language tag in literal    ```go  triples = append(triples,   	SubjPred(""me"", ""name"").Bnode(""jsmith""),   	BnodePred(""me"", ""name"").StringLiteral(""jsmith""),   	SubjPred(""me"", ""name"").StringLiteralWithLang(""jsmith"", ""en""),  )  ```    #### Create triples from a struct    As a convenience you can create triples from a singular struct, where you control embedding through bnode.    Here is an example.    ```go  type Address struct {  	Street string `predicate:""street""`  	City   string `predicate:""city""`  }    type Person struct {  	Name     string    `predicate:""name""`  	Age      int       `predicate:""age""`  	Size     int64     `predicate:""size""`  	Male     bool      `predicate:""male""`  	Birth    time.Time `predicate:""birth""`  	Surnames []string  `predicate:""surnames""`  	Addr     Address   `predicate:""address"" bnode:""myaddress""` // empty bnode value will make bnode value random  }    addr := &Address{...}  person := &Person{Addr: addr, ....}    tris := TriplesFromStruct(""jsmith"", person)    src := NewSource()  src.Add(tris)  snap := src.Snapshot()    snap.Contains(SubjPredLit(""jsmith"", ""name"", ""...""))  snap.Contains(SubjPredLit(""jsmith"", ""size"", 186))  snap.Contains(SubjPredLit(""jsmith"", ""surnames"", ""...""))  snap.Contains(SubjPredLit(""jsmith"", ""surnames"", ""...""))  snap.Contains(SubjPred(""me"", ""address"").Bnode(""myaddress""))  snap.Contains(BnodePred(""myaddress"", ""street"").StringLiteral(""5th avenue""))  snap.Contains(BnodePred(""myaddress"", ""city"").StringLiteral(""New York""))  ```    #### Equality    ```go  	me := SubjPred(""me"", ""name"").StringLiteral(""jsmith"")   	you := SubjPred(""me"", ""name"").StringLiteral(""fdupond"")     	if me.Equal(you) {   	 	...   	}  )  ```    ### Triple Source    A source is a persistent yet mutable source or container of triples    ```go  src := tstore.NewSource()    src.Add(  	SubjPredLit(""me"", ""age"", ""jsmith""),  	SubjPredLit(""me"", ""born"", time.now()),  )  src.Remove(SubjPredLit(""me"", ""age"", ""jsmith""))  ```    ### RDFGraph    A RDFGraph is an immutable set of triples you can query. You get a RDFGraph by snapshotting a source:    ```go  graph := src.Snapshot()    tris := graph.WithSubject(""me"")  for _, tri := range tris {  	...  }  ```    ### Codec    Triples can be encoded & decoded using either a simple binary format or more standard text format like NTriples, ...    Triples can therefore be persisted to disk, serialized or sent over the network.    For example    ```go  enc := NewBinaryEncoder(myWriter)  err := enc.Encode(triples)  ...    dec := NewBinaryDecoder(myReader)  triples, err := dec.Decode()  ```    Create a file of triples under the lenient NTriples format:    ```go  f, err := os.Create(""./triples.nt"")  if err != nil {  	return err  }  defer f.Close()    enc := NewLenientNTEncoder(f)  err := enc.Encode(triples)    ```     Encode to a DOT graph  ```go  tris := []Triple{          SubjPredRes(""me"", ""rel"", ""you""),          SubjPredRes(""me"", ""rdf:type"", ""person""),          SubjPredRes(""you"", ""rel"", ""other""),          SubjPredRes(""you"", ""rdf:type"", ""child""),          SubjPredRes(""other"", ""any"", ""john""),  }    err := NewDotGraphEncoder(file, ""rel"").Encode(tris...)  ...    // output  // digraph ""rel"" {  //  ""me"" -> ""you"";  //  ""me"" [label=""me<person>""];  //  ""you"" -> ""other"";  //  ""you"" [label=""you<child>""];  //}  ```    Load a binary dataset (i.e. multiple RDFGraph) concurrently from given files:    ```go  path := filepath.Join(fmt.Sprintf(""*%s"", fileExt))  files, _ := filepath.Glob(path)    var readers []io.Reader  for _, f := range files {  	reader, err := os.Open(f)  	if err != nil {  		return g, fmt.Errorf(""loading '%s': %s"", f, err)  	}  	readers = append(readers, reader)  }    dec := tstore.NewDatasetDecoder(tstore.NewBinaryDecoder, readers...)  tris, err := dec.Decode()  if err != nil {  	return err  }  ...  ```    ### triplestore CLI    This CLI is mainly ised for triples files conversion and inspection. Install it with `go get github.com/wallix/triplestore/cmd/triplestore`. Then `triplestore -h` for help.    Example of usage:    ```sh  triplestore -in ntriples -out bin -files fuzz/ntriples/corpus/samples.nt   triplestore -in ntriples -out bin -files fuzz/ntriples/corpus/samples.nt   triplestore -in bin -files fuzz/binary/corpus/samples.bin  ```    ### RDFGraph as a Tree    A tree is defined from a RDFGraph given:     * a specific predicate as an edge   * and considering triples pointing to RDF resource Object     You can then navigate the tree using the existing API calls    	tree := tstore.NewTree(myGraph, myPredicate)  	tree.TraverseDFS(...)  	tree.TraverseAncestors(...)  	tree.TraverseSiblings(...)    Have a look at the [godoc](https://godoc.org/github.com/wallix/triplestore) fro more info     Note that at the moment, constructing a new tree from a graph does not verify if the tree is valid namely no cycle and each child at most one parent. """
Semantic web;https://github.com/tomayac/ldf-client;"""&lt;ldf-client&gt;  ==========================    A declarative [Linked Data Fragments](http://linkeddatafragments.org) client in the form of a [Web Component](http://webcomponents.org/).  Simply insert the ```<ldf-client>``` in your page, fill in the required attribute values,  and you are ready to go.    Live Demo  =========    You can see a [live demo](http://tomayac.github.io/ldf-client/)  of the Web Component in action hosted on GitHub pages.    Installation  ============    This assumes that you have [bower](http://bower.io/) and  [polyserve](https://github.com/PolymerLabs/polyserve) installed.    ```sh  $ mkdir web-components  $ cd web-components  $ git clone git@github.com:tomayac/ldf-client.git  $ cd ldf-client  $ bower install  $ polyserve  ```    Finally visit [http://localhost:8080/components/ldf-client/](http://localhost:8080/components/ldf-client/)  and click the [demo](http://localhost:8080/components/ldf-client/demo/) link in the upper right corner.    Basic Usage  ===========    The example below shows basic usage instructions for the element.    ```html  <!doctype html>  <html>    <head>      <script src=""../webcomponentsjs/webcomponents-lite.min.js""></script>      <link rel=""import"" href=""ldf-client.html"">    </head>    <body>      <!-- Streaming example -->      <ldf-client          id=""ldf-client-streaming""          response-format=""streaming""          query=""SELECT DISTINCT ?frag WHERE {                   ?a a &lt;http://advene.org/ns/cinelab/ld#Annotation&gt; ;                     &lt;http://advene.org/ns/cinelab/ld#hasFragment&gt; ?frag ;                     &lt;http://advene.org/ns/cinelab/ld#taggedWith&gt;                       [ &lt;http://purl.org/dc/elements/1.1/title&gt; 'personnages: Margaret'],                       [ &lt;http://purl.org/dc/elements/1.1/title&gt; 'personnages: Grand Papa Pollitt'];                     .                 }""          start-fragment=""http://spectacleenlignes.fr/ldf/spectacle_en_lignes"">      </ldf-client>        <!-- Polling example -->      <ldf-client          id=""ldf-client-polling""          response-format=""polling""          query=""SELECT DISTINCT ?tag WHERE {                   [ a &lt;http://advene.org/ns/cinelab/ld#Annotation&gt; ;                     &lt;http://advene.org/ns/cinelab/ld#taggedWith&gt;                       [ &lt;http://purl.org/dc/elements/1.1/title&gt;  ?tag ]                    ]                 }""          start-fragment=""http://spectacleenlignes.fr/ldf/spectacle_en_lignes"">      </ldf-client>        <button value=""Poll"" id=""button"">Poll</button>        <script>        document.addEventListener('polymer-ready', function() {          /* Streaming example */          var ldfClientStreaming = document.querySelector('#ldf-client-streaming');          // Process data as it appears          ldfClientStreaming.addEventListener('ldf-query-streaming-response-partial',              function(e) {            var pre = document.createElement('pre');            pre.textContent = JSON.stringify(e.detail.response);            document.body.appendChild(pre);          });          // Get notified once all data is received          ldfClientStreaming.addEventListener('ldf-query-streaming-response-end', function() {            alert('Received all data');          });            /* Polling example */          var ldfClientPolling = document.querySelector('#ldf-client-polling');          // Poll for data          ldfClientPolling.addEventListener('ldf-query-polling-response', function(e) {            var pre = document.createElement('pre');            pre.textContent = JSON.stringify(e.detail.response);            document.body.appendChild(pre);          });          // Manually trigger polling          var button = document.querySelector('#button');          button.addEventListener('click', function() {            ldfClientPolling.showNext();          });        });      </script>    </body>  </html>  ```    About Linked Data Fragments  ===========================    The Web is full of high-quality Linked Data,  but we can't reliably query it.  Public SPARQL endpoints are often unavailable,  because they need to answer many unique queries.  One could set up a local endpoint using data dumps,  but that's not Web querying and  the data is never up-to-date.    [Linked Data Fragments](http://linkeddatafragments.org/) offer interfaces to  solve queries at the client side with server data.  Servers can offer data at low processing cost  in a way that enables client-side querying.  You can read more about [the concept of Linked Data Fragments](http://linkeddatafragments.org/concept/)  and learn about available [Linked Data Fragments software](http://linkeddatafragments.org/software/).    Acknowledgements  ================    Building this Web Component was pretty straight-forward thanks to  [Ruben Verborgh](http://ruben.verborgh.org/)'s  Linked Data Fragments browser client  [Browser.js](https://github.com/LinkedDataFragments/Browser.js).    License  =======  Copyright 2015 Thomas Steiner (tomac@google.com)    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. """
Semantic web;https://github.com/cosminbasca/jvmrdftools;"""jvmrdftools  ===========    Simple collection of RDF tools in scala    Important Notes  ---------------  This software is the product of research carried out at the [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and comes with no warranty whatsoever. Have fun!    TODO's  ------  * unit tests  * documentation  * examples    Gotcha's  --------  Every time the project version information is changed, BuildInfo needs to be regenerated. To do that simply run:    ```sh  $ sbt compile  ```    to generate the assembly (used by the [rdftools](https://github.com/cosminbasca/rdftools) python module) simply run    ```sh  $ sbt assembly  ```    Thanks a lot to  ---------------  * [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Swiss National Science Foundation](http://www.snf.ch/en/Pages/default.aspx) for generously funding the research that led to this software. """
Semantic web;https://github.com/ontola/rdf-serializers;"""# RDF Serializers    <a href=""https://travis-ci.org/ontola/rdf-serializers""><img src=""https://travis-ci.org/ontola/rdf-serializers.svg?branch=master"" alt=""Build Status""></a>    ## About    RDF Serializers enables serialization to RDF formats. It uses [fast-jsonapi](https://github.com/fast-jsonapi/fast_jsonapi) serializers, with a few modifications.  The serialization itself is done by the [rdf](https://github.com/ruby-rdf/rdf) gem.    This was built at [Ontola](https://ontola.io/). If you want to know more about our passion for open data, send us [an e-mail](mailto:ontola@argu.co).    ## Installation    Add this line to your application's Gemfile:    ```  gem 'rdf-serializers'  ```    And then execute:    ```  $ bundle  ```    ## Getting started    First, register the formats you wish to serialize to. For example, add the following to `config/initializers/rdf_serializers.rb`:  ```ruby  require 'rdf/serializers/renderers'    RDF::Serializers::Renderers.register(:ntriples)  ```  This automatically registers the MIME type.    In your controllers, add:  ```ruby  respond_to do |format|    format.nt { render nt: model }  end  ```    ## Configuration    You can configure the gem using `RDF::Serializers.configure`.  ```  RDF::Serializers.configure do |config|    config.always_include_named_graphs = false # true by default. Whether to include named graphs when the serialization format does not support quads.    config.default_graph = RDF::URI('https://example.com/graph') # nil by default.  end    ```    ## Formats    You can register multiple formats, if you add the correct gems. For example, add `rdf-turtle` to your gemfile and put this in the initializer:  ```ruby  require 'rdf/serializers/renderers'    opts = {    prefixes: {      ns:   'http://rdf.freebase.com/ns/',      key:  'http://rdf.freebase.com/key/',      owl:  'http://www.w3.org/2002/07/owl#',      rdfs: 'http://www.w3.org/2000/01/rdf-schema#',      rdf:  'http://www.w3.org/1999/02/22-rdf-syntax-ns#',      xsd:  'http://www.w3.org/2001/XMLSchema#'    }  }    RDF::Serializers::Renderers.register(%i[ntriples turtle], opts)    ```    The RDF gem has a list of available [RDF Serialization Formats](https://github.com/ruby-rdf/rdf#rdf-serialization-formats), which includes:  * NTriples  * Turtle  * N3  * RDF/XML  * JSON::LD    and more    ## Serializing    Add a predicate to the attributes and relations you wish to serialize.    It's recommended to reuse existing vocabularies provided by the `rdf` gem and the [rdf-vocab](https://github.com/ruby-rdf/rdf-vocab) gem,   and add your own vocab for missing predicates. One way to be able to access the different vocabs throughout your application is by defining a module:  ```  require 'rdf'  require ""rdf/vocab""    module NS    SCHEMA = RDF::Vocab::SCHEMA    MY_VOCAB = RDF::Vocabulary.new('http://example.com/')  end  ```    Now add the predicates to your serializers.     Old:   ```ruby  class PostSerializer    include JSONAPI::Serializer    attributes :title, :body    belongs_to :author    has_many :comments  end  ```    New:  ```ruby  class PostSerializer    include RDF::Serializers::ObjectSerializer    attribute :title, predicate: NS::SCHEMA[:name]    attribute :body, predicate: NS::SCHEMA[:text]    belongs_to :author, predicate: NS::MY_VOCAB[:author]    has_many :comments, predicate: NS::MY_VOCAB[:comments]  end  ```    For RDF serialization, you are required to add an `iri` method to your model, which must return a `RDF::Resource`. For example:  ```ruby    def iri      RDF::URI(Rails.application.routes.url_helpers.comment_url(object))    end  ```    In contrast to the JSON API serializer, this rdf serializers don't automatically serialize the `type` and `id` of your model.   It's recommended to add `attribute :type, predicate: RDF[:type]` and a method defining the type to your serializers to fix this.    ### Custom statements per model    You can add custom statements to the serialization of a model in the serializer, for example:  ```ruby  class PostSerializer    include RDF::Serializers::ObjectSerializer    statements :my_custom_statements        def my_custom_statements      [RDF::Statement.new(RDF::URI('https://example.com'), NS::MY_VOCAB[:fooBar], 1)]    end  end  ```    ### Meta statements    You can add additional statements to the serialization in the controller, for example:  ```ruby  render nt: model, meta: [RDF::Statement.new(RDF::URI('https://example.com'), NS::MY_VOCAB[:fooBar], 1)]  ```    ## Contributing    The usual stuff. Open an issue to discuss a change, open pull requests directly for bugfixes and refactors. """
Semantic web;https://github.com/solid/solid-spec;"""# Solid Specification Draft  [![](https://img.shields.io/badge/project-Solid-7C4DFF.svg?style=flat-square)](https://github.com/solid/solid)  [![Join the chat at https://gitter.im/solid/solid-spec](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/solid/solid-spec)    **Latest version:** [`v.0.7.0`](https://github.com/solid/solid-spec/tree/v0.7.0) (see [CHANGELOG.md](CHANGELOG.md))    **Publication status**: Unofficial Draft    **Current development version:** `v.0.7.0-next` (evolving)    **This document contains an informal description of implementation guidelines for Solid servers and clients.  A normative specification is in the making at https://github.com/solid/specification/.  For the time being, the present document contains the best approximation of expected server and client behavior.**    ## Table of Contents    1. [Overview](#overview)  2. [Identity](#identity)  3. [Profiles](#profiles)      * [WebID Profile Documents](#webid-profile-documents)  4. [Authentication](#authentication)      * [Primary Authentication](#primary-authentication)        * [WebID-TLS](#webid-tls)        * [Alternative Authentication            Mechanisms](#alternative-authentication-mechanisms)      * [Secondary Authentication: Account          Recovery](#secondary-authentication-account-recovery)  5. [Authorization and Access Control](#authorization-and-access-control)      * [Web Access Control](#web-access-control)  6. [Content Representation](#content-representation)  7. [Reading and Writing Resources](#reading-and-writing-resources)      * [HTTPS REST API](#https-rest-api)      * [WebSockets API](#websockets-api)  8. [Social Web App Protocols](#social-web-app-protocols)      * [Notifications](#notifications)      * [Friends Lists, Followers and          Following](#friends-lists-followers-and-following)  9. [Recommendations for Server        Implementation](#recommendations-for-server-implementations)  10. [Recommendations for Client App        Implementation](#recommendations-for-client-app-implementations)  11. [Examples](#examples)  12. [Current Implementations](#current-implementations)    ## Overview    [Solid](https://github.com/solid/solid)  is a proposed set of conventions and tools for building  *decentralized applications* based on [Linked  Data](https://www.w3.org/DesignIssues/LinkedData) principles. Solid is  modular and extensible. It relies as much as possible on existing  [W3C](http://www.w3.org/) standards and protocols.    See Also:    * [About Solid](https://github.com/solid/solid#about-solid)  * [Contributing to Solid](https://github.com/solid/solid#contributing-to-solid)    * [Pre-Requisites](https://github.com/solid/solid#pre-requisites)    * [Solid Project        Workflow](https://github.com/solid/solid#solid-project-workflow)  * [Standards Used](https://github.com/solid/solid#standards-used)  * [Platform Notes](https://github.com/solid/solid#solid-platform-notes)  * [Solid Project directory](https://github.com/solid/solid#project-directory)    ## Identity    Solid uses [WebID](http://www.w3.org/2005/Incubator/webid/spec/identity/) URIs  as universal usernames or actor identifiers. Frequently referred to simply as    *WebIDs*, these URIs form the basis of most other Solid-related technologies,  such as authentication, authorization, access control, user profiles, discovery  of user preferences and server capabilities, and more.    WebIDs provide globally unique decentralized identifiers, enable cross-service  federated signin, prevent service provider lock-in, and give users control over  their own identity. *The WebID URI's primary function is to point to the  location of a public [WebID Profile document](#profiles) (see below).*    **Example WebIDs:** `https://alice.databox.com/profile/card#me` or  `http://somepersonalsite.com/#webid`    ## Profiles    Solid uses WebID Profile Documents for management of user identity and security  credentials (such as public keys), and user preferences discovery.    Although here we mostly refer to them in the context of user profiles,  other types of actors use these profiles as well, such as groups, organizations,  devices, and software applications.    ### WebID Profile Documents    A WebID URI, when dereferenced, yields a WebID Profile Document in a  Linked Data format ([Turtle](http://www.w3.org/TR/turtle/) by default, but  often available as JSON-LD or HTML+RDFa). Parsing this document provides a  client application with useful information, such as the user's name and  profile image, links to user preferences and related documents, and lists of  public key certificates or other relevant identity credentials.    **See component spec:    [Solid WebID Profiles Specification](solid-webid-profiles.md)**    ## Authentication    Authentication is the process of determining a user’s identity, of asking the  question “How do I know you are who you say?”.    How do web applications typically authenticate users (that is, how do they  verify identity)? The most common method is usernames and passwords. A  *username* uniquely identifies a user (and ties them to a user profile), and a  *password* verifies that the user is who they say they are. Many applications or  services also have a *secondary authentication mechanism* (usually an external  email address) that they use for account recovery (in case the user forgets or  loses their primary authentication tokens, username and password).    Solid currently uses WebID-TLS as its primary authentication mechanism.  Alternative complementary mechanisms are also being actively investigated.  In addition, Solid recommends that server implementations also offer secondary  authentication available for users for Account Recovery (via email or some  other out-of-band mechanism).    ### Primary Authentication    Solid, being a decentralized web application platform, has a set of requirements  for its authentication mechanisms that are not commonly encountered by most  platforms and ecosystems. Specifically, it requires *cross-domain*,  de-centralized authentication mechanisms not tied to any particular identity  provider or certificate authority.    #### WebID-TLS    **Note:** Several browser vendors (Chrome, Firefox) have removed support  for the `KEYGEN` element, on which WebID-TLS relied for in-browser certificate  generation.    Solid uses the [WebID-TLS  protocol](http://www.w3.org/2005/Incubator/webid/spec/tls/) as one of its  primary authentication mechanism. Instead of usernames, it uses WebIDs as unique  identifiers, as previously mentioned. And instead of using passwords as bearer  tokens, it uses cryptographic certificates (stored and managed by the user's web  browser) to prove a user's identity.    When accessing a Solid server using WebID-TLS, a user is presented by their  web browsers with a popup asking them to select an appropriate security  certificate for that site. After a user makes their selection, the server  securely matches the private key stored by the browser with the public key  stored in that user's [WebID Profile Document](#webid-profile-documents), and  authenticates them.    **See component spec:    [Solid WebID-TLS Specification](authn-webid-tls.md)**    #### WebID-OIDC    The Solid team is currently implementing support for WebID-OIDC as another  primary authentication mechanism. It is based on the OAuth2/OpenID Connect  protocols, adapted for WebID based decentralized use cases.    **See component spec:    [WebID-OIDC Specification](https://github.com/solid/webid-oidc-spec)**    #### Alternative Authentication Mechanisms    There are several other authentication mechanisms that are  currently being investigated, such as combinations of traditional  username-and-password authentication and WebID-TLS Delegation).    ### Secondary Authentication: Account Recovery    Regardless of the primary authentication mechanism, bearer tokens and other  proofs of identity tend to get lost by users. Passwords can be forgotten,  browser certificates can be lost to hardware failure, and so on. Solid  recommends that secondary Account Recovery mechanisms are provided by server  implementers, to aid in these scenarios.    ## Authorization and Access Control    Authorization is the process of deciding whether a user has *access* to a  particular resource. If authentication asks ""who is the user?"", authorization  is concerned with ""what is the user allowed to do?"".    Solid currently uses the Web Access Control (WAC) mechanism for cross-domain  authorization for all its resources.    ### Web Access Control    [Web Access Control (WAC)](https://github.com/solid/web-access-control-spec) is  a decentralized system that allows different users and groups various forms of  access to resources where users and groups are identified by HTTP URIs. The  system is similar to the access control system used within many file systems  except that the documents controlled, the users, and the groups, are all  identified by URIs. Users are identified by WebIDs. Groups of users are  identified by the URI of a class of users which, if you look it up, returns a  list of users in the class. This means a WebID hosted by any server can be a  member of a group hosted some other server.    Users do not need to have an account (i.e. WebID) on a given server to have  access to documents on it.    **See component spec:  [Solid WAC Specification](https://github.com/solid/web-access-control-spec)**    ## Content Representation    Solid deals with reading and writing two kinds of resources:    1. Linked Data resources (RDF in the form of JSON-LD, Turtle, HTML+RDFa, etc)  2. Everything else (binary data and non-linked-data structured text)    While you can build Solid applications with non-linked data resources, using  actual RDF-based Linked Data provides you with considerable benefits in terms  of interoperability with the rest of the Solid app ecosystem.    Resources are grouped in directory-like **Containers** (currently conforming  to the [LDP Basic Container spec](https://www.w3.org/TR/ldp/#ldpbc)).    **See component spec: [Solid Content    Representation](content-representation.md)**    ## Reading and Writing Resources    ### HTTPS REST API    Solid extends the [Linked Data Platform spec](https://www.w3.org/TR/ldp/) to  provide a simple REST API for CRUD operations on resources and containers.    **See component spec: [HTTPS REST API](api-rest.md)**    ### WebSockets API    Solid also provides a WebSockets based API for a PubSub (Publish/Subscribe)  mechanism, through which clients can be notified in real time of  changes affecting a give resource.    **See component spec: [WebSockets API](api-websockets.md)**    ## Social Web App Protocols    In addition to read/write operations on resources, Solid provides a number of  specs and recommendations to help developers achieve interoperability between  various social web applications that are part of the ecosystem.    ### Notifications    **See component spec: [Linked Data Notifications](https://www.w3.org/TR/ldn/)**    ### Friends Lists, Followers and Following    API recommendations for managing subscriptions and friends lists are still  being discussed. TBD.    ## Recommendations for Server Implementations    **See component spec: [Recommendations for Server    Implementations](recommendations-server.md)**    ## Recommendations for Client App Implementations    **See component spec: [Recommendations for Client    Implementations](recommendations-client.md)**    ## Examples    * [User Posts a Note](examples/user-posts-note.md)    ## Current Implementations    **Server Implementations:** See  [solid/solid-platform](https://github.com/solid/solid-platform#servers) for a  list of Solid servers and developer tools.  Note: The Solid team uses  [`node-solid-server`](https://github.com/solid/node-solid-server) as  its main server implementation.    **Client App Implementations:** See  [`solid-client`](https://github.com/solid/solid-client) for the main client  library, and [solid/solid-apps](https://github.com/solid/solid-apps) for an  example list of Apps built using Solid. """
Semantic web;https://github.com/lukostaz/prissma;"""PRISSMA  ===========  ### Context-Aware Adaptation for Linked Data    [PRISSMA](http://wimmics.inria.fr/projects/prissma) is a presentation-level framework for [Linked Data](http://linkeddata.org) adaptation.    It is a Java rendering engine for [RDF](http://www.w3.org/TR/rdf11-primer/) that selects the most appropriate presentation of RDF triples according to [mobile context](http://en.wikipedia.org/wiki/Context_awareness).    PRISSMA is compatible with the [Fresnel vocabulary](http://www.w3.org/2005/04/fresnel-info/manual/) and is based on a graph edit distance algorithm that finds optimal error-tolerant subgraph isomorphisms between RDF context graphs.    PRISSMA is optimized for Android platforms, but can be used in regular Java Projects as well.      # Installation    PRISSMA is a Java library, optimized for Android applications.    ## Minimum Requirements    + Java 1.6  + Android 4.2.2    ## Download  The latest PRISSMA release is [v1.0.0](https://github.com/lukostaz/prissma/releases/tag/v1.0.0). Download it and add it to your build path.    Make sure `config.properties` is included in your build path and that it contains the  parameter values needed by the search algorithm (e.g. [similarity threshold, cost of edit distance operations, etc](http://2014.eswc-conferences.org/sites/default/files/papers/paper_81.pdf)).      ## Build from Source  Check out sources:    	$ git clone https://github.com/lukostaz/prissma.git    PRISSMA depends on the [Simmetrics](https://github.com/Simmetrics/simmetrics) and [WS4J](https://code.google.com/p/ws4j/) libraries, that are not available in Maven central. Simmetrics 1.6.2 and WS4J 1.0.1 are provided under `libs/`. You can install it in your local Maven repository:  	      $ cd prissma            $ mvn install:install-file \            -Dfile=libs/simmetrics-1.6.2.jar \            -DgroupId=simmetrics \            -DartifactId=simmetrics \            -Dversion=1.6.2 \            -Dpackaging=jar        $ mvn install:install-file \            -Dfile=libs/ws4j-1.0.1.jar \            -DgroupId=ws4j \            -DartifactId=ws4j \            -Dversion=1.0.1 \            -Dpackaging=jar    PRISSMA depends on the `xerces-impl` Android-optimized version available [available here](http://elite.polito.it/index.php/research/downloads/182-jena-on-android-download). A copy of the library is available under `libs/`.  Add it to local Maven repository:    	$ mvn install:install-file \            -Dfile=libs/xercesImpl-repack.jar \            -DgroupId=xerces \            -DartifactId=xercesImpl-repack \            -Dversion=1.0.0 \            -Dpackaging=jar      Build and install the library in local Maven repository:  	      $ mvn install    Add the following dependency in your `pom.xml`:    	<dependency>          <groupId>fr.inria.wimmics</groupId>          <artifactId>PRISSMA</artifactId>          <version>1.0.0</version>      </dependency>      # Designing Prisms    Prisms are context-aware presentation metadata for Linked Data visualization based on Fresnel and PRISSMA.    Prisms can be created manually, or with [PRISSMA Studio](http://luca.costabello.info/prissma-studio/).    ## Example    Prism to style a `dbpedia:Museum` when a user is walking in Paris.    First, define the Prism general information:	  ```turtle  :museumPrism a prissma:Prism ;     fresnel:purpose :walkingInParis ;     fresnel:stylesheetLink  <style.css>.  ```    Add some Fresnel Lenses:  ```turtle  :museumlens a fresnel:Lens;     fresnel:group :museumPrism;     fresnel:classLensDomain dbpedia:Museum;     fresnel:showProperties (                         dbpprop:location                        dbpprop:publictransit                        ex:telephone                       ex:openingHours                       ex:ticketPrice ) .  ```    Add Fresnel styling metadata:    ```turtle  :addressFormat a fresnel:Format ;     fresnel:group :museumPrism ;     fresnel:propertyFormatDomain                        dbpprop:location ;     fresnel:label ""Address"" ;     fresnel:labelStyle          ""css-class1""^^fresnel:styleClass ;     fresnel:valueStyle          ""css-class2""^^fresnel:styleClass .  ```    Finally, define a `prissma:Context` entity with the [PRISSMA vocabulary](http://ns.inria.fr/prissma/v2/prissma_v2.html):  ```turtle  # PRISSMA context description  :walkingInParisArtLover a prissma:Context ;     prissma:user :artLover ;      prissma:environment :parisWalking .        :artLover a prissma:User ;     foaf:interest ""art"".    :parisWalking a prissma:Environment ;     prissma:poi :paris ;     prissma:motion ""walking"" .  	  :paris geo:lat ""48.8567"" ;     geo:long ""2.3508"" ;     prissma:radius ""5000"" .  ```  Save the Prism locally, and store it in the Decomposition structure as explained below.        # API Overview    Make sure `config.properties` is included in your build path and that it contains the  parameters values needed by the search algorithm (e.g. [similarity threshold, cost of edit distance operations, etc](http://2014.eswc-conferences.org/sites/default/files/papers/paper_81.pdf)).    ## Step 1: Decomposing Prisms    ```java  // Instantiate a decomposer  Decomposer decomposer = new Decomposer();  // The Decomposition is the shared data structure for Prisms  Decomposition decomp = new Decomposition();    // The inputPrism Jena Model is the Prism read from local repository  Model inputPrism = ModelFactory.createDefaultModel();    // Get the path of the Prism local repository on Android devices.  // If executed on desktop environment p is a regular file path string.  String p = Environment.getExternalStorageDirectory().getAbsolutePath();  InputStream in = FileManager.get().open( p + ""/PRISSMA/prisms/prism.ttl"" );    // Decompose the Prism  if (in != null) {      inputPrism.read(in, null,  ""TURTLE"");      decomp = decomposer.decompose(inputPrism, decomp);  }    ```    ## Step 2: Running the search algorithm    ```java  // Read input context  Model actualCtx = ModelFactory.createDefaultModel();  InputStream inCtx = FileManager.get().open( p + ""/PRISSMA/ctx/ctx1.ttl"" );  if (inCtx != null) {      actualCtx.read(inCtx, null,  ""TURTLE"");  }    // Instantiate an error-tolerant matcher with a decomposition  Matcher matcher = new Matcher(decomp);  // get the prissma:Context element, i.e. the root element of input context  RDFNode ctxRoot = ContextUnitConverter.getRootCtxNode(actualCtx);  // Covnert core PRISSMA entities to their PRISSMA classes  ctxRoot = ContextUnitConverter.switchToClasses(ctxRoot, decomp);  // Execute error-tolerant match against Prisms in the decomposition  matcher.search(ctxRoot);    ```    ## Step 3: Rendering resources    ```java  // inputResource is the desired RDF resource to display.  Model prism = readPrismFromFS(matcher.results);  Renderer r = new Renderer();  String html = r.renderHTML(prism, inputResource, true);    ```      # Publications      + L. Costabello. [Error-Tolerant RDF Subgraph Matching for Adaptive Presentation of Linked Data on Mobile](http://2014.eswc-conferences.org/sites/default/files/papers/paper_81.pdf). 11th Extended Semantic Web Conference (ESWC), 2014.  + L. Costabello. [PRISSMA, Towards Mobile Adaptive Presentation of the Web of Data](http://iswc2011.semanticweb.org/fileadmin/iswc/Papers/DC_Proposals/70320273.pdf). ISWC 2011 Doctoral Consortium, Bonn, Germany    # Licence  	      Copyright (C) 2013-2015 Luca Costabello, v1.0.0        This program is free software; you can redistribute it and/or modify it      under the terms of the GNU General Public License as published by the      Free Software Foundation; either version 2 of the License, or (at your      option) any later version.        This program is distributed in the hope that it will be useful, but      WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY      or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License      for more details.        You should have received a copy of the GNU General Public License along      with this program; if not, see <http://www.gnu.org/licenses/>.    # Contacts  Further details on the [PRISSMA Project Page](http://wimmics.inria.fr/projects/prissma/), or contact [Luca Costabello](http://luca.costabello.info).   """
Semantic web;https://github.com/aschaetzle/Sempala;"""# Sempala      Sempala is a SPARQL-over-SQL approach to provide interactive-time SPARQL query processing on Hadoop. It stores RDF data in a columnar layout (Parquet) on HDFS and uses either Impala or Spark as the execution layer on top of it. SPARQL queries are translated into Impala/Spark SQL for execution.    http://dbis.informatik.uni-freiburg.de/forschung/projekte/DiPoS/Sempala.html      ### LICENSE  Unless explicitly stated otherwise all files in this repository are licensed under the Apache Software License 2.0    >   Copyright 2017 University of Freiburg  >  >   Licensed under the Apache License, Version 2.0 (the ""License"");  >   you may not use this file except in compliance with the License.  >   You may obtain a copy of the License at  >  >       http://www.apache.org/licenses/LICENSE-2.0  >  >   Unless required by applicable law or agreed to in writing, software  >   distributed under the License is distributed on an ""AS IS"" BASIS,  >   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  >   See the License for the specific language governing permissions and  >   limitations under the License.      ## BUILD project  You need to have Maven installed on your system.  Then simply run ""mvn package"" from the root directory.  It will build 'sempala-loader', 'sempala-translator' and finally 'sempala'.  You can also build 'sepala-loader' or 'sempala-translator' only by  running ""mvn package"" from the corresponding subdirectory.  NOTE: Two jars are generated for sempala translator - one for Impala (sempala-translator)   and one for Spark (spark-sempala-translator)      ## PURPOSE OF project_repo DIRECTORY    Cloudera Impala JDBC connector ships with several libraries. All but the   connector itself are available in the maven or cloudera central repositories  and are pulled at build time by maven. To fit in the maven architecture the  connector is installed in a in-project repository, which behaves like a remote   central repository.    To update the version of the Impala JDBC connector in the in-project repository,  you can install a newer version of it and update the POM of sempala-parent  (main POM in root directory of this project) to use that version.  To do this, get the JDBC driver by downloading it from cloudera.com [1]  and install it with the maven install plugin. This will take care of checksums:    ```    mvn install:install-file      -DlocalRepositoryPath=project_repo      -DcreateChecksum=true      -Dpackaging=jar      -Dfile=<path_to:jdbc_driver.jar>      -DgroupId=com.cloudera.impala      -DartifactId=impala-jdbc-4.1-connector      -Dversion=<version>  ```      ## Official guide to installing 3rd party JARs    Although rarely, but sometimes you will have 3rd party JARs that you need to put  in your local repository for use in your builds, since they don't exist in any  public repository like Maven Central. The JARs must be placed in the local  repository in the correct place in order for it to be correctly picked up by  Apache Maven. To make this easier, and less error prone, we have provide a goal  in the maven-install-plugin which should make this relatively painless. To  install a JAR in the local repository use the following command:    ```    mvn install:install-file -Dfile=<path-to-file> -DgroupId=<group-id> \      -DartifactId=<artifact-id> -Dversion=<version> -Dpackaging=<packaging>  ```    If there's a pom-file as well, you can install it with the following command:    ```    mvn install:install-file -Dfile=<path-to-file> -DpomFile=<path-to-pomfile>  ```    With version 2.5 of the maven-install-plugin it gets even better. If the JAR was  built by Apache Maven, it'll contain a pom.xml in a subfolder of the META-INF  directory, which will be read by default. In that case, all you need to do is:    ```    mvn install:install-file -Dfile=<path-to-file>  ```    (Source: https://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html)        [1] http://www.cloudera.com/downloads/connectors/impala/jdbc.html """
Semantic web;https://github.com/white-gecko/jekyll-rdf;"""# jekyll-rdf    A [Jekyll plugin](https://jekyllrb.com/docs/plugins/) for including RDF data in your static site.    [![Gem Version](https://badge.fury.io/rb/jekyll-rdf.svg)](https://badge.fury.io/rb/jekyll-rdf)  [![Build Status](https://travis-ci.org/white-gecko/jekyll-rdf.svg?branch=develop)](https://travis-ci.org/white-gecko/jekyll-rdf)  [![Coverage Status](https://coveralls.io/repos/github/white-gecko/jekyll-rdf/badge.svg?branch=develop)](https://coveralls.io/github/white-gecko/jekyll-rdf?branch=develop)    The API Documentation is available at [RubyDoc.info](http://www.rubydoc.info/gems/jekyll-rdf/).    # Contents    1. [Installation](#installation)  2. [Usage](#usage)      1. [Configuration](#configuration)      2. [Building the Jekyll Site](#building-the-jekyll-site)      3. [Defining Templates](#defining-templates)  3. [Parameters and configuration options at a glance](#parameters-and-configuration-options-at-a-glance)      1. [Resource Attributes](#resource-attributes)      2. [Liquid Filters](#liquid-filters)      3. [Plugin Configuration (\_config.yml)](#plugin-configuration-_configyml)  4. [Development](#development)  5. [License](#license)    # Installation    As a prerequisite for *Jekyll RDF* you of course need to install [*Jekyll*](https://jekyllrb.com/).  Please take a look at the installations instructions at https://jekyllrb.com/docs/installation/.    If you already have a working Jekyll installation you can add the the Jekyll-RDF plugin.  Probably you already using [Bundler](https://bundler.io/) and there is a [`Gemfile`](https://bundler.io/gemfile.html) in your Jekyll directory.  Add Jekyll-RDF to the plugins section:    ```  group :jekyll_plugins do      gem ""jekyll-rdf"", '~> 3.0.0.pre.a'      …  end  ```    Replace the version string with the currently available stable release as listed on [rubygems.org](https://rubygems.org/gems/jekyll-rdf).  After updating your `Gemfile` you probably want to run `bundle install` (or `bundle install --path vendor/bundle`) or `bundle update`.    If you are not using a `Gemfile` to manage your jekyll/ruby packages install Jekyll-RDF using `gem`:    ```  gem install jekyll-rdf  ```    If you want to build the plugin from source, please have a look at our [Development](#development) section.    # Usage    This section explains how to use Jekyll-RDF in three steps:    1. [Configuration](#configuration)  2. [Building the Jekyll Site](#building-the-jekyll-site)  3. [Defining Templates](#defining-templates)    All filters and methods to use in templates and configuration options are documented in the section “[Parameters and configuration options at a glance](#parameters-and-configuration-options-at-a-glance)”.    ## Configuration  First, you need a jekyll page. In order to create one, just do:  ```  jekyll new my_page  cd my_page  ```    Further there are some parameters required in your `_config.yml` for `jekyll-rdf`. I.e. the `url` and `baseurl` parameters are used for including the resource pages into the root of the site, the plug-in has to be configured, and the path to the RDF file has to be present.    ```yaml  baseurl: ""/simpsons""  url: ""http://example.org""    plugins:      - jekyll-rdf    jekyll_rdf:      path: ""_data/data.ttl""      default_template: ""default.html""      restriction: ""SELECT ?resourceUri WHERE { ?resourceUri ?p ?o . FILTER regex(str(?resourceUri), 'http://example.org/simpsons')  }""      class_template_mappings:          ""http://xmlns.com/foaf/0.1/Person"": ""person.html""      instance_template_mappings:          ""http://example.org/simpsons/Abraham"": ""abraham.html""  ```    ### Map resources to templates  It is possible to map a specific class (resp. RDF-type) or individual resources to a template.  ```yaml    class_template_mappings:        ""http://xmlns.com/foaf/0.1/Person"": ""person.html""    instance_template_mappings:        ""http://aksw.org/Team"": ""team.html""  ```    A template mapped to a class will be used to render each instance of that class and its subclasses.  Each instance is rendered with its most specific class mapped to a template.  If the mapping is ambiguous for a resource, a warning will be output to your command window, so watch out!    It is also possible to define a default template, which is used for all resources, which are not covered by the `class_template_mappings` or `instance_template_mappings`.    ```yaml    default_template: ""default.html""  ```    ### Restrict resource selection  You can restrict the resources selected to be built by adding a SPARQL query as `restriction` parameter to `_config.yml`. Please use `?resourceUri` as the placeholder for the resulting URIs:  ```yaml    restriction: ""SELECT ?resourceUri WHERE { ?resourceUri <http://www.ifi.uio.no/INF3580/family#hasFather> <http://www.ifi.uio.no/INF3580/simpsons#Homer> }""  ```    There are 3 pre-defined keywords for restrictions implemented:  * `subjects` will load all subject URIs  * `predicates` will load all predicate URIs  * `objects` will load all object URIs    Because some SPARQL endpoints have a built in limit for SELECT queries you can also define a list of resources to be built.  A file `_data/restriction.txt` cool have the following content:    ```  <http://example.org/resourceA>  <http://example.org/resourceB>  <http://example.org/resourceC>  <http://example.org/resourceD>  <http://example.org/resourceE>  ```    In the `_config.yml` you specify the file with the key `restriction_file`.  If both, a `restriction_file` and a `restriction`, are specified JekyllRDF will build pages for the union of the both.    ### Blank Nodes  Furthermore you can decide if you want to render blank nodes or not. You just need to add `include_blank`to `_config.yml`:  ```yaml  jekyll_rdf:    include_blank: true  ```    ### Preferred Language  Finally it is also possible to set a preferred language for the RDF-literals with the option `language`:  ```yaml  jekyll_rdf:    language: ""en""  ```    ## Building the Jekyll Site    Running `jekyll build` will render the RDF resources to the `_site/…` directory. Running `jekyll serve` will render the RDF resources and provide you with an instant HTTP-Server usually accessible at `http://localhost:4000/`.  RDF resources whose IRIs don't start with the configured jekyll `url` and `baseurl` are rendered to the `_site/rdfsites/…` subdirectory.    ## Defining Templates  To make use of the RDF data, create one or more files (e.g `rdf_index.html` or `person.html`) in the `_layouts`-directory. For each resource a page will be rendered. See example below:    ```html  ---  layout: default  ---  <div class=""home"">    <h1 class=""page-heading""><b>{{ page.rdf.iri }}</b></h1>    <p>      <h3>Statements in which {{ page.rdf.iri }} occurs as subject:</h3>      {% include statements_table.html collection=page.rdf.statements_as_subject %}    </p>    <p>      <h3>Statements in which {{ page.rdf.iri }} occurs as predicate:</h3>      {% include statements_table.html collection=page.rdf.statements_as_predicate %}    </p>    <p>      <h3>Statements in which {{ page.rdf.iri }} occurs as object:</h3>      {% include statements_table.html collection=page.rdf.statements_as_object %}    </p>  </div>  ```    ### Template Examples  We included some template examples at  * `test/source/_layouts/rdf_index.html`  * `test/source/_layouts/person.html`    ### Get the IRI of a resource        {{ page.rdf }}    Is the currently rendered resource.        {{ page.rdf.iri }}    Returns the IRI of the currently rendered resource.    To access objects which are connected to the current subject via a predicate you can use our custom liquid filters. For single objects or lists of objects use the `rdf_property`-filter (see [1](#single-objects) and [2](#multiple-objects)).    ### Single Objects  To access one object which is connected to the current subject through a given predicate please filter `page.rdf` data with the `rdf_property`-filter. Example:  ```  Age: {{ page.rdf | rdf_property: '<http://xmlns.com/foaf/0.1/age>' }}  ```    ### Optional Language Selection  To select a specific language please add a second parameter to the filter:  ```  Age: {{ page.rdf | rdf_property: '<http://xmlns.com/foaf/0.1/job>','en' }}  ```    ### Multiple Objects  To get more than one object connected to the current subject through a given predicate please use the filter `rdf_property` in conjunction with a third argument set to `true` (the second argument for the language can be omitted by setting it to `nil`):  ```html  Sisters: <br />  {% assign resultset = page.rdf | rdf_property: '<http://www.ifi.uio.no/INF3580/family#hasSister>', nil, true %}  <ul>  {% for result in resultset %}      <li>{{ result }}</li>  {% endfor %}  </ul>  ```    ### Optional Language Selection  To select a specific language please add a second parameter to the filter:  ```html  Book titles: <br />  {% assign resultset = page.rdf | rdf_property: '<http://xmlns.com/foaf/0.1/currentProject>','de' %}  <ul>  {% for result in resultset %}      <li>{{ result }}</li>  {% endfor %}  </ul>  ```    ### RDF Containers and Collections  To support [RDF Containers](https://www.w3.org/TR/rdf-schema/#ch_containervocab) and [RDF Collections](https://www.w3.org/TR/rdf-schema/#ch_collectionvocab) we provide the `rdf_container` and `rdf_collection` filters.    In both cases the respective container resource resp. head of the collection needs to be identified and then passed through the respective filter.  For containers we currently support explicit instances of `rdf:Bag`, `rdf:Seq` and `rdf:Alt` with the members identified using the `rdfs:ContainerMembershipProperty`s: `rdf:_1`, `rdf:_2`, `rdf:_3` ….  Collections are identified using `rdf:first`, `rdf:rest` and terminated with `L rdf:rest rdf:nil`.  Since the head of a collection needs to be identified you cannot use a blank node there, you can identify it indirectly through the predicate which contains the collection.    Example graph:    ```  @prefix ex: <http://example.org/> .  @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .    ex:Resource ex:lists ex:List ;              ex:directList (""hello"" ""from"" ""turtle"") .              ex:hasContainer ex:Container .    ex:List rdf:first ""hello"" ;          rdf:rest (""rdf"" ""list"") .    ex:Container a rdf:Bag ;               rdf:_1 ""hello"" ;               rdf:_2 ""rdf"" ;               rdf:_3 ""container"" .  ```    The template for `ex:Resource`:    ```  {% assign list = page.rdf | rdf_collection: '<http://example.org/directList>' %}  <ol>  {% for item in list %}  <li>{{ item }}</li>  {% endfor %}  </ol>    {% assign container = page.rdf | rdf_property: '<http://example.org/hasContainer>' | rdf_container %}  <ul>  {% for item in container %}  <li>{{ item }}</li>  {% endfor %}  </ul>  ```    ### Custom SPARQL Query  We implemented a liquid filter `sparql_query` to run custom SPARQL queries. Each occurence of `?resourceUri` gets replaced with the current URI.  *Caution:* You have to separate query and resultset variables because of Liquids concepts. Example:  ```html  {% assign query = 'SELECT ?sub ?pre WHERE { ?sub ?pre ?resourceUri }' %}  {% assign resultset = page.rdf | sparql_query: query %}  <table>  {% for result in resultset %}    <tr>      <td>{{ result.sub }}</td>      <td>{{ result.pre }}</td>    </tr>  {% endfor %}  </table>  ```    ### Defining Prefixes for RDF  It is possible to declare a set of prefixes which can be used in the `rdf_property` and `sparql_query` liquid-filters.  This allows to shorten the amount of text required for each liquid-filter.  The syntax of the prefix declarations is the same as for [SPARQL 1.1](https://www.w3.org/TR/2013/REC-sparql11-query-20130321/).  Just put your prefixes in a separate file and include the key `rdf_prefix_path` together with a relative path in the [YAML Front Matter](https://jekyllrb.com/docs/frontmatter/) of a file where your prefixes should be used.    For the prefixes the same rules apply as for other variables defined in the YAML Front Matter.  *These variables will then be available to you to access using Liquid tags both further down in the file and also in any layouts or includes that the page or post in question relies on.* (source: [YAML Front Matter](https://jekyllrb.com/docs/frontmatter/)).  This is especially relevant if you are using prefixes in includes.    ### Dealing with Fragment Identifiers  If the URI of a resource contains a [fragment identifier (`#…`)](https://en.wikipedia.org/wiki/Fragment_identifier) the resource can be hosted together with other resources with the same base URI up to the fragment identifier on a single page.  The page will by accessible through the base URI, while in the template the individual URIs with a fragment identifier are accessible through the collection `page.sub_rdf`.    **Example**    In the `_config.yml`:  ```yaml    'instance_template_mappings' :      'http://www.ifi.uio.no/INF3580/simpsons' : 'family.html'  ```    In `_layouts/family.html`:  ```html    {% for member in page.sub_rdf%}      {% include simPerson.html person = member%}    {% endfor %}  ```    The example uses the template `family.html` to render a single page containing every resource whose URI begins with `http://www.ifi.uio.no/INF3580/simpsons#`, was well as the resource `http://www.ifi.uio.no/INF3580/simpsons` itself.  Jekyll-rdf collects all resources with a fragment identifier in their URI (from here on called `subResources`) and passes them through `page.sub_rdf` into the templates of its `superResource` (resources whose base URI is the same as of its `subResources` except for the fragment identifier).    # Parameters and configuration options at a glance    ## Resource Attributes  Every resource returned by one of `jekyll-rdf`s filters is an object that liquid can also handle like a string. They all have the following methods usable in Liquid.    ### Resource.statements_as_subject  Return a list of statements whose subject is the current resource.  The statements in the returned list can be accessed by addressing their positions: `Statement.subject`, `Statement.predicate`, respective `Statement.object`.    ### Resource.statements_as_predicate  Return a list of statements whose predicate is the current resource.  The statements in the returned list can be accessed by addressing their positions: `Statement.subject`, `Statement.predicate`, respective `Statement.object`.    ### Resource.statements_as_object  Return a list of statements whose object is the current resource.  The statements in the returned list can be accessed by addressing their positions: `Statement.subject`, `Statement.predicate`, respective `Statement.object`.    ### Resource.page_url  Return the URL of the page representing this RdfResource.    ### Resource.render_path  Return the path to the page representing this RdfResource. Use it with care.    ### Resource.covered  This method is relevant for rendering pages for IRIs containing a fragment identifier (`http://superresource#anchor`).  This method returns true for the super-resource (`http://superresource`) if it is actually described in the given knowledgebase.    ### Resource.inspect  Returns a verbose String representing this resource.    ## Liquid Filters  ### rdf_get  **Synopsis:** `<resource_iri> | rdf_get`    **Parameters:**  - `<resource_iri>` is a string representing an RDF resource, with prefix (`prefix:name`) or a full IRI (`<http://ex.org/name>`). To reference the resource of the current page use `page.rdf`, `page`, or `nil`.    **Description:** Takes the provided IRI and returns the corresponding RdfResource object from your knowledge base.  On this object you can call the methods as described in the section [Resource](Resource).    **Example:**  ```  {{'<http://www.ifi.uio.no/INF3580/simpsons>' | rdf_get }}  ```    **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons  ```    ### rdf_property  **Synopsis:** `<rdf_resource> OR <rdf_resource_string> | rdf_property: <property>, [<lang>] OR [<lang>, <list>] OR [nil, <list>]`    **Parameters:**  - `<rdf_resource>` is an RdfResource. To reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<rdf_resource_string>` is a String representing the IRI of `<rdf_resource>`.  - `<property>` is a string representing an RDF predicate, with prefix (`prefix:name`) or a full IRI (`<http://ex.org/name>`).  - `<lang>` is a language tag (e.g. `de`). If this parameter is omitted replace it by `nil`.  - `<list>` is a boolean value (`true`, `false`).    **Description:** Returns the object, of the triple `<rdf_resource> <predicate> ?object`.  The returned object can by any of the kind, resource, literal, or blanknode.    **Example (default):**  ```  {assign resource = '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_get }  {{ resource | rdf_property: '<http://xmlns.com/foaf/0.1/job>' }}  ```  **Result:**  ```html  ""unknown""  ```  **Example (string):**  ```  {{ '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_property: '<http://xmlns.com/foaf/0.1/job>' }}  ```  **Result:**  ```html  ""unknown""    ```    **Example (with language):**  ```  {assign resource = '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_get }  {{ resource | rdf_property: '<http://xmlns.com/foaf/0.1/job>', 'de' }}  ```  **Result:**  ```html  ""unbekannt""  ```    **Example (return as list):**  ```  {assign resource = '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_get }  {% assign resultset = resource | rdf_property: '<http://xmlns.com/foaf/0.1/job>', nil, true %}  {% for result in resultset %}  <li>{{ result }}</li>  {% endfor %}  ```  **Result:**  ```html  <li>""unknown""</li>  <li>""unbekannt""</li>  <li>""unbekannter Job 2""</li>  <li>""unknown Job 2""</li>  ```    ### rdf_inverse_property  **Synopsis:** `<rdf_resource> OR <rdf_resource_string>| rdf_inverse_property: <property>, [<list>]`    **Parameters:**  - `<rdf_resource>` is an RdfResource. To reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<rdf_resource_string>` is a String representing the IRI of `<rdf_resource>`.  - `<property>` is a string representing an RDF predicate, with prefix (`prefix:name`) or a full IRI (`<http://ex.org/name>`).  - `<list>` is a boolean value (`true`, `false`).    **Description:** Same as rdf_property, but in inverse direction.  It returns the subject, of the triple `?subject <predicate> <rdf_resource>`.  The returned object can by any of the kind, resource, or blanknode.    **Examples (default):**  ```  {assign resource = '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_get }  {{ page.rdf | rdf_inverse_property: '<http://www.ifi.uio.no/INF3580/family#hasFather>' }}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Bart  ```    **Examples (string):**  ```  {{ '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_inverse_property: '<http://www.ifi.uio.no/INF3580/family#hasFather>' }}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Bart  ```      **Example (as list):**  ```  {assign resource = '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_get }  {% assign resultset = resource | rdf_property: '<http://www.ifi.uio.no/INF3580/family#hasFather>', true %}  {% for result in resultset %}  <li>{{ result }}</li>  {% endfor %}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie  ```    ### sparql_query  **Synopsis:** `<rdf_resource> | sparql_query: <query>` **OR** `<reference_array> | sparql_query: <query>` **OR** `<query> | sparql_query`    **Parameters:**  - `<rdf_resource>` is an RdfResource which will replace `?resourceUri` in the query. To omit this parameter or reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<reference_array>` an array containing IRIs as Strings or `rdf_resource`. They will consecutively replace each `?resourceUri_<index>` in your query.  - `<query>` a string containing a SPARQL query.    **Description:** Evaluates `query` on the given knowledge base and returns an array of results (result set).  Each entry object in the result set (result) contains the selected variables as resources or literals.  You can use `?resourceUri` inside the query to reference the resource which is given as `<rdf_resource>`.    **Example (page)**  ```  <!--Rendering the page of resource Lisa -->  {% assign query = 'SELECT ?sub ?pre WHERE { ?sub ?pre ?resourceUri }' %}  {% assign resultset = page | sparql_query: query %}  <table>  {% for result in resultset %}    <tr><td>{{ result.sub }}</td><td>{{ result.pre }}</td></tr>  {% endfor %}  </table>  ```  **Result:**  ```html  <table>  <tr><td>http://www.ifi.uio.no/INF3580/simpsons#TheSimpsons</td><td>http://www.ifi.uio.no/INF3580/family#hasFamilyMember</td></tr>  <tr><td>http://www.ifi.uio.no/INF3580/simpsons#Bart</td><td>http://www.ifi.uio.no/INF3580/family#hasSister</td></tr>  <tr><td>http://www.ifi.uio.no/INF3580/simpsons#Maggie</td><td>http://www.ifi.uio.no/INF3580/family#hasSister</td></tr>  ...  ```    **Example (array)**  ```  {% assign query = 'SELECT ?x WHERE {?resourceUri_0 ?x ?resourceUri_1}' %}  {% assign array = ""<http://www.ifi.uio.no/INF3580/simpsons#Homer>,<http://www.ifi.uio.no/INF3580/simpsons#Marge>"" | split: %}  {% assign resultset = array | sparql_query: query %}  <table>  {% for result in resultset %}    <tr><td>{{ result.x }}</td></tr>  {% endfor %}  </table>  ```  **Result:**  ```  <table>    <tr><td>http://www.ifi.uio.no/INF3580/family#hasSpouse</td></tr>  </table>  ```    **Example (query)**  ```  {% assign query = 'SELECT ?x WHERE {<http://www.ifi.uio.no/INF3580/simpsons#Homer> ?x <http://www.ifi.uio.no/INF3580/simpsons#Marge>}' %}  {% assign resultset = query | sparql_query %}  <table>  {% for result in resultset %}    <tr><td>{{ result.x }}</td></tr>  {% endfor %}  </table>  ```    **Result:**  ```  <table>    <tr><td>http://www.ifi.uio.no/INF3580/family#hasSpouse</td></tr>  </table>  ```    ### rdf_container  **Synopsis:** `<rdf_container_head> **OR** <rdf_container_head_string> | rdf_container`    **Parameters:**  - `<rdf_container_head>` is an RdfResource. To reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<rdf_container_head_string>` is a String representing the IRI of `<rdf_container_head>`.    **Description:** Returns an array with resources for each element in the container whose head is referenced by `rdf_container_head`.    **Examples:**  ```  {% assign resource = '<http://www.ifi.uio.no/INF3580/simpson-container#Container>' | rdf_get %}  {% assign array = resource | rdf_container %}  {% for item in array %}  {{ item }}  {% endfor %}  ```  ###### Result:  ```html  http://www.ifi.uio.no/INF3580/simpsons#Homer  http://www.ifi.uio.no/INF3580/simpsons#Marge  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie    ```    **Examples: (string)**  ```  {% assign array = '<http://www.ifi.uio.no/INF3580/simpson-container#Container>' | rdf_container %}  {% for item in array %}  {{ item }}  {% endfor %}  ```  ###### Result:  ```html  http://www.ifi.uio.no/INF3580/simpsons#Homer  http://www.ifi.uio.no/INF3580/simpsons#Marge  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie    ```    ### rdf_collection  **Synopsis:** `<rdf_collection_head> OR <rdf_collection_head_string> | rdf_collection` **OR** `<rdf_resource> | rdf_collection: ""<property>""`    **Parameters:**  - `<rdf_collection_head>` is an RdfResource. To reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<rdf_collection_head_string>` is a String representing the IRI of `<rdf_collection_head>`.  - `<rdf_resource>` is an RdfResource. To reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<property>` is a string representing an RDF predicate, with prefix (`prefix:name`) or a full IRI (`<http://ex.org/name>`).    **Description:** Returns an array with resources for each element in the collection whose head is referenced by `rdf_collection_head`.  Instead of directly referencing a head it is also possible to specify the property referencing the collection head.    **Example (specify head resource):**  ```  {% assign resource = '<http://www.ifi.uio.no/INF3580/simpson-collection#Collection>' | rdf_get %}  {% assign array = resource | rdf_collection %}  {% for item in array %}  {{ item }}  {% endfor %}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Homer  http://www.ifi.uio.no/INF3580/simpsons#Marge  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie  ```  **Example (specify head string):**  ```  {% assign array = '<http://www.ifi.uio.no/INF3580/simpson-collection#Collection>' | rdf_collection %}  {% for item in array %}  {{ item }}  {% endfor %}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Homer  http://www.ifi.uio.no/INF3580/simpsons#Marge  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie  ```  **Example (specify via property):**  ```  {% assign resource = '<http://www.ifi.uio.no/INF3580/simpsons>' | rdf_get %}  {% assign array = resource | rdf_collection: ""<http://www.ifi.uio.no/INF3580/simpsons#familycollection>"" %}  {% for item in array %}  {{ item }}  {% endfor %}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Homer  http://www.ifi.uio.no/INF3580/simpsons#Marge  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie  ```    ## Plugin Configuration (\_config.yml)  |Name|Parameter|Default|Description|Example|  |---	|---	|---	|---	|---	|  |path|Relative path to the RDF-File|no default|Specifies the path to the RDF file from where you want to render the website|```path: ""rdf-data/simpsons.ttl""```|  |remote|Section to specify a remote data source|no default|Has to contain the `endpoint` key. The `remote` paramter overrides the `path` parameter.||  |remote > endpoint|SPARQL endpoint to get the data from|no default|Specifies the URL to the SPARQL endpoint from where you want to render the website|```remote: endpoint: ""http://localhost:5000/sparql/""```|  |language|Language-Tag as String|no default|Specifies the preferred language when you select objects using our Liquid filters|```language: ""en""```|  |include_blank|Boolean-Expression|false|Specifies whether blank nodes should also be rendered or not|```include_blank: true```|  |restriction|SPARQL-Query as String or subjects/objects/predicates|no default|Restricts the resource-selection with a given SPARQL-Query to the results bound to the special variable `?resourceUri` or the three keywords `subjects` (only subject URIs), `objects`, `predicates`|```restriction: ""SELECT ?resourceUri WHERE { ?resourceUri <http://www.ifi.uio.no/INF3580/family#hasFather> <http://www.ifi.uio.no/INF3580/simpsons#Homer> }""```|  |restriction_file|File of resources to be rendered|no default|Restricts the resource-selection to the list of resources in the file|```restriction_file: _data/restriction.txt```|  |default_template|Filename of the default RDF-template in _layouts directory|no default|Specifies the template-file you want Jekyll to use to render all RDF resources|```default_template: ""rdf_index.html""```|  |instance_template_mappings|Target URI as String : filename of the template as String|no default|Maps given URIs to template-files for rendering an individual instance|```instance_template_mappings: ""http://www.ifi.uio.no/INF3580/simpsons#Abraham"": ""abraham.html""```|  |class_template_mappings|Target URI as String : filename of the template as String|no default|Maps given URIs to template-files for rendering all instances of that class|```class_template_mappings: ""http://xmlns.com/foaf/0.1/Person"": ""person.html""```|    # Development    ## Installation from source  To install the project with the git-repository you will need `git` on your system. The first step is just cloning the repository:  ```  git clone git@github.com:white-gecko/jekyll-rdf.git  ```  A folder named `jekyll-rdf` will be automatically generated. You need to switch into this folder and compile the ruby gem to finish the installation:  ```  cd jekyll-rdf  gem build jekyll-rdf.gemspec  gem install jekyll-rdf -*.gem  ```    ## Run tests  ```  bundle exec rake test  ```    ## Test page  Everytime the tests are executed, the Jekyll page inside of `test/source` gets processed. Start a slim web server to watch the results in web browser, e.g. Pythons `SimpleHTTPServer` (Python 2, for Python 3 it's `http.server`):  ```  cd test/source/_site  python -m SimpleHTTPServer 8000  ```    ## Build API Doc  To generate the API Doc please navigate to `jekyll-rdf/lib` directory and run  ```  gem install yard  yardoc *  ```    The generated documentation is placed into `jekyll-rdf/lib/doc` directory.    # License  jekyll-rdf is licensed under the [MIT license](https://github.com/DTP16/jekyll-rdf/tree/master/LICENSE). """
Semantic web;https://github.com/seebi/semweb.vim;"""# vim bundle for working with RDF knowledge bases    ## Installation    This bundle is created to work with `pathogen` so just copy the folder  into your bundle directory.    ## Features     * namespace completion   * syntax highlightning   * insert xsd:dateTime current time literal with abbrevation `xsdnow`    ## namespace completion    You can use ctrl-x+u (autocomplete user function) for completing  rdf prefixes (as `owl`, `skos` and `foaf`) to namespace URIs (e.g.  `http://xmlns.com/foaf/0.1/` for the `foaf` prefix).    This done with the multifunctional shell script  [rdf.sh](https://github.com/seebi/rdf.sh) which is a dependency and must  be available as `rdf` in the path.    ## syntax highlightning    ### N3 Syntax    I just pull the syntax file from [Niklas  Lindström](git://github.com/vim-scripts/n3.vim.git) here.    ### SPARQL Syntax    I just pull the syntax file from [Jeroen  Pulles](https://github.com/vim-scripts/sparql.vim) here.   """
Semantic web;https://github.com/IKCAP/wings;"""[![Test](https://github.com/KnowledgeCaptureAndDiscovery/wings/actions/workflows/maven.yml/badge.svg)](https://github.com/KnowledgeCaptureAndDiscovery/wings/actions/workflows/maven.yml)    # Wings    ## Installation    ### Docker     You must install [Docker](https://www.docker.com/) and [docker-compose](https://docs.docker.com/compose/install/).    Deploy the container with the following command:    ```bash  $ docker-compose up -d  ```    Open the browser [http://localhost:8080/wings-portal](http://localhost:8080/wings-portal) to access the Wings portal.      Go to [README Docker](wings-docker/) for additional instructions on running the Docker image.      ### Maven    Please follow the instructions in [README Maven](docs/maven.md) to install the Wings project."""
Semantic web;https://github.com/joepio/atomic-data-browser;"""![Atomic Data Browser](./logo.svg)    [![Discord chat][discord-badge]][discord-url]  [![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)    Create, share, fetch and model linked [Atomic Data](https://atomicdata.dev)!  This repo consists of three components: a javascript / typescript library, a react library, and a complete GUI: Atomic-Data Browser.    ## [`atomic-data-browser`](data-browser/README.md)    A GUI for viewing, editing and browsing Atomic Data.   Designed for interacting with [atomic-server](https://github.com/joepio/atomic-data-rust/).    **demo on [atomicdata.dev](https://atomicdata.dev)**    https://user-images.githubusercontent.com/2183313/139728539-d69b899f-6f9b-44cb-a1b7-bbab68beac0c.mp4    ```sh  # To run, simply run the following commands:  yarn # install dependencies  yarn bootstrap # symlink ./lib and ./react to ./data-browser  yarn start # run the server!  ```    [→ Read more](data-browser/README.md)    ## [`@tomic/lib`](lib/README.md)    <a href=""https://www.npmjs.com/package\/@tomic/lib"" target=""_blank"">    <img src=""https://img.shields.io/npm/v/@tomic/lib?color=cc3534"" />  </a>  <a href=""https://www.npmjs.com/package/@tomic/lib"" target=""_blank"">    <img src=""https://img.shields.io/npm/dm/@tomic/lib?color=%2344cc10"" />  </a>  <a href=""https://bundlephobia.com/result?p=@tomic/lib"" target=""_blank"">    <img src=""https://badgen.net/bundlephobia/minzip/@tomic/lib"">  </a>    Library with `Store`, `Commit`, `JSON-AD` parsing, and more.    [**docs**](https://joepio.github.io/atomic-data-browser/docs/modules/_tomic_lib.html)    [→ Read more](lib/README.md)    ## [`@tomic/react`](react/README.md)    <a href=""https://www.npmjs.com/package/@tomic/react"" target=""_blank"">    <img src=""https://img.shields.io/npm/v/@tomic/react?color=cc3534"" />  </a>  <a href=""https://www.npmjs.com/package/@tomic/react"" target=""_blank"">    <img src=""https://img.shields.io/npm/dm/@tomic/react?color=%2344cc10"" />  </a>  <a href=""https://bundlephobia.com/result?p=@tomic/react"" target=""_blank"">    <img src=""https://badgen.net/bundlephobia/minzip/@tomic/react"">  </a>      React library with many useful hooks for rendering and editing Atomic Data.    [**demo + template on codesandbox**](https://codesandbox.io/s/atomic-data-react-template-4y9qu?file=/src/MyResource.tsx:0-1223)    [**docs**](https://joepio.github.io/atomic-data-browser/docs/modules/_tomic_react.html)    [→ Read more](react/README.md)    ## Also check out    - [atomic-data-rust](https://github.com/joepio/atomic-data-rs), a rust [**library**](https://crates.io/crates/atomic-lib), [**server**](https://crates.io/crates/atomic-server) and [**cli**](https://crates.io/crates/atomic-cli) for Atomic Data.  - [sign up to the Atomic Data Newsletter](http://eepurl.com/hHcRA1)    ## Contribute    Issues and PR's are welcome!  And join our [Discord][discord-url]!  See [Contribute.md](CONTRIBUTE.md)    [discord-badge]: https://img.shields.io/discord/723588174747533393.svg?logo=discord  [discord-url]: https://discord.gg/a72Rv2P """
Semantic web;https://github.com/gjhiggins/RDFAlchemy;"""RDFAlchemy is an abstraction layer, allowing Python developers to use familiar  *dot notation* to access and update an RDF triplestore.            * RDFAlchemy is an **ORM** (Object Rdf Mapper) for graph data as:      * SQLAlchemy is an **ORM** (Object Relational Mapper) for relalational databases          See the the homepage at http://www.openvest.com/trac/wiki/RDFAlchemy,  and the docs at http://rdfalchemy.readthedocs.org/ for details.    [![Build Status](https://travis-ci.org/gjhiggins/RDFAlchemy.png?branch=master)](https://travis-ci.org/gjhiggins/RDFAlchemy) """
Semantic web;https://github.com/sisinflab-swot/ldp-coap-framework;"""LDP-CoAP: Linked Data Platform for the Constrained Application Protocol  ===================    [W3C Linked Data Platform 1.0 specification](http://www.w3.org/TR/ldp/) defines resource management primitives for HTTP only, pushing into the background not-negligible   use cases related to Web of Things (WoT) scenarios where HTTP-based communication and infrastructures are unfeasible.     LDP-CoAP proposes a mapping of the LDP specification for [RFC 7252 Constrained Application Protocol](https://tools.ietf.org/html/rfc7252) (CoAP)   and a complete Java-based framework to publish Linked Data on the WoT.     A general translation of LDP-HTTP requests and responses is provided, as well as a fully comprehensive framework for HTTP-to-CoAP proxying.     LDP-CoAP framework also supports the [W3C Linked Data Notifications](https://www.w3.org/TR/ldn/) (LDN) protocol aiming to generate, share and reuse notifications across different applications.    LDP-CoAP functionalities can be tested using the [W3C Test Suite for LDP](http://w3c.github.io/ldp-testsuite/) and the [LDN Test Suite](http://github.com/csarven/ldn-tests).    Modules  -------------    LDP-CoAP (version 1.2.x) consists of the following sub-projects:    - _ldp-coap-core_: basic framework implementation including the proposed LDP-CoAP mapping;  - _ldp-coap-test_: includes reference client/server implementation used to test the framework according to the test suites cited above;  - _ldp-coap-raspberry_: usage examples exploiting _ldp-coap-core_ on a [Raspberry Pi 1 Model B+](https://www.raspberrypi.com/products/raspberry-pi-1-model-b-plus/) board;  - _ldp-coap-android_: simple project using _ldp-coap-core_ on Android platform;    LDP-CoAP also requires [Californium-LDP](https://github.com/sisinflab-swot/californium-ldp), a fork of the _Eclipse Californium_ framework supporting LDP specification. In particular, the following modules were defined as local Maven dependencies:    - _californium-core-ldp_: a modified version of the [californium-core](https://github.com/eclipse/californium) library extended to support LDP-CoAP features;  - _californium-proxy-ldp_: a modified version of the [californium-proxy](http://github.com/eclipse/californium) used to translate LDP-HTTP request methods and headers   into the corresponding LDP-CoAP ones and then map back LDP-CoAP responses to LDP-HTTP;    Usage with Eclipse and Maven  -------------    Each module can be imported as Maven project in Eclipse. Make sure to have the following plugins before importing LDP-CoAP projects:    - [Eclipse EGit](http://www.eclipse.org/egit/)  - [M2Eclipse - Maven Integration for Eclipse](http://www.eclipse.org/m2e/)    Documentation  -------------    Hands-on introduction to LDP-CoAP using [basic code samples](http://swot.sisinflab.poliba.it/ldp-coap/usage.html) available on the project webpage.    More details about packages and methods can be found on the official [Javadoc](http://swot.sisinflab.poliba.it/ldp-coap/docs/javadoc/v1_1/).    References  -------------    If you want to refer to LDP-CoAP in a publication, please cite one of the following papers:    ```  @InProceedings{ldp-coap-framework,    author       = {Giuseppe Loseto and Saverio Ieva and Filippo Gramegna and Michele Ruta and Floriano Scioscia and Eugenio {Di Sciascio}},    title        = {Linked Data (in low-resource) Platforms: a mapping for Constrained Application Protocol},    booktitle    = {The Semantic Web - ISWC 2016: 15th International Semantic Web Conference, Proceedings, Part II},    series       = {Lecture Notes in Computer Science},    volume       = {9982},    pages        = {131--139},    month        = {oct},    year         = {2016},    editor       = {Paul Groth, Elena Simperl, Alasdair Gray, Marta Sabou, Markus Krotzsch, Freddy Lecue, Fabian Flock, Yolanda Gil},    publisher    = {Springer International Publishing},    address      = {Cham},  }  ```    ```  @InProceedings{ldp-coap-proposal,    author = {Giuseppe Loseto and Saverio Ieva and Filippo Gramegna and Michele Ruta and Floriano Scioscia and Eugenio {Di Sciascio}},    title = {Linking the Web of Things: LDP-CoAP mapping},    booktitle = {The 7th International Conference on Ambient Systems, Networks and Technologies (ANT 2016) / Affiliated Workshops},    series = {Procedia Computer Science},    volume = {83},    pages = {1182--1187},    month = {may},    year = {2016},    editor = {Elhadi Shakshuki},    publisher = {Elsevier}  }  ```    License  -------------    _ldp-coap-core_, _ldp-coap-android_ and _ldp-coap-raspberry_ modules are distributed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).    _californium-core-ldp_ and _ldp-coap-proxy_ are distributed under the [Eclipse Public License, Version 1.0](https://www.eclipse.org/legal/epl-v10.html) as derived projects.      Contact  -------------    For more information, please visit the [LDP-CoAP webpage](http://swot.sisinflab.poliba.it/ldp-coap/).      Contribute  -------------  The main purpose of this repository is to share and continue to improve the LDP-CoAP framework, making it easier to use. If you're interested in helping us any feedback you have about using LDP-CoAP would be greatly appreciated. There are only a few guidelines that we need contributors to follow reported in the CONTRIBUTING.md file.    --------- """
Semantic web;https://github.com/dice-group/CostFed;"""### CostFed: Cost-Based Query Optimization for SPARQL Endpoint Federation      CostFed is an index-assisted federation engine for federated SPARQL query processing over multiple SPARQL endpoints. CostFed makes use of statistical information collected from endpoints to perform efficient source selection and cost-based query planning. In contrast to the state of the art, it relies on a non-linear model for the estimation of the selectivity of joins. Therewith, it is able to generate better plans than the state-of-the-art federation engines. In an experimental evaluation based on FedBench benchmark, we show  that CostFed is 3 to 121 times faster than the state of the art SPARQL endpoint federation engines.   ## Live Demo  The CostFed live demo comprise the following two main applications:   * The endpoint manager is is available [here](http://manager.costfed.aksw.org). Using endpoint manager you can select the endpoints to be included in the federation. Also it allows to create/update CostFed's indexes.    * The query formulator/executer is availble [here](http://costfed.aksw.org). This is the main interface which allows executing both federated and non-federated queries.       The start CostFed-web and create your own local demo, the Dockerfile can be downloaded from [here](https://github.com/dice-group/CostFed/blob/master/Dokerfile)      To help user, we provided some federated queries [here](http://costfed.aksw.org/rdf4j-workbench/repositories/costfed/saved-queries) from FedBench and LargeRDFBench which can be directly executed.   ### How to Run CostFed?  * Checkout: the source code and import as new maven project. it will create three sub-projects, i.e, costfed, fex, and semagrow-bench.   * Create Index: Since CostFed is an index-assisted appraoch, the first step is to generate an index for all the endpoints in hand. The index generation, updation is given costfed/src/main/java/org/aksw/simba/quetsal/util/TBSSSummariesGenerator.java. Note for FedBench, LargeRDFBench, the index is already given at costfed/summaries/sum-localhost.n3.   * Configuration File: Set properties in /costfed/costfed.props or run with default  * Query Execution: costfed/src/main/java/org/aksw/simba/start/QueryEvaluation.java. Here you need to specify the URLs of the SPARQL endpoints which you want the given query to be federated and provide the configuration file, i.e., costfed.props as argument.      ### Used Benchmarks  The queries used in the evaluation can be downloaded from [FedBench](http://fedbench.fluidops.net/) and [LargeRDFBech](https://github.com/AKSW/largerdfbench) homepage.     ### Datasets Availability     All the datasets and corresponding virtuoso SPARQL endpoints can be downloaded from the links given below. You may start a SPARQL endpoint from bin/start.bat (for windows) and bin/start_virtuoso.sh (for linux).     | *Dataset*  | *Data-dump*  | *Windows Endpoint*  | *Linux Endpoint*  | *Local Endpoint Url*  | *Live Endpoint Url*|  |------------|--------------|---------------------|-------------------|-----------------------|--------------------|  | [ChEBI](https://www.ebi.ac.uk/chebi/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-Vk81dGVkNVNuY1E/edit?usp=sharing/ )| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-TUR6RF9jX2xoMFU/edit?usp=sharing/)|[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-Wk5LeHBzMUd3VHc/edit?usp=sharing )|your.system.ip.address:8890/sparql | - |  | [DBPedia-Subset](http://DBpedia.org/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-QWk5MVJud3cxUXM/edit?usp=sharing/ )|  [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-WjNkZEZrTTZzbW8/edit?usp=sharing/)|[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-OEgyXzBUVmlMQlk/edit?usp=sharing )|your.system.ip.address:8891/sparql |http://dbpedia.org/sparql |  | [ DrugBank](http://www.drugbank.ca/)|[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-cVp5QV9VUWRuYkk/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-QmMyOE9RWV9oNHM/edit?usp=sharing/ )| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-U0V5Y0xDWXhzam8/edit?usp=sharing/ )|your.system.ip.address:8892/sparql | http://wifo5-04.informatik.uni-mannheim.de/drugbank/sparql|  | [Geo Names](http://www.geonames.org/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-WEZZb2VwOG5vZkU/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-VC1HWmhBMlFncWc/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B_MUFqryVpByd3hJcHBPeHZhejA/edit?usp=sharing/ ) |your.system.ip.address:8893/sparql | http://factforge.net/sparql|  | [Jamendo](http://dbtune.org/jamendo/ ) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-cWpmMWxxQ3Z2eVk/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-YXV6U0ZzLUF0S0k/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-V3JMZjdfRkZxLUU/edit?usp=sharing/ ) |your.system.ip.address:8894/sparql  | http://dbtune.org/jamendo/sparql/|  | [KEGG](http://www.genome.jp/kegg/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-TUdUcllRMGVJaHM/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-c1BNQ0dVWTVkUEU/edit?usp=sharing/ )| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-R1dKbDlHNXZ6blk/edit?usp=sharing/ ) |your.system.ip.address:8895/sparql |http://cu.kegg.bio2rdf.org/sparql |  | [Linked MDB](http://linkedmdb.org/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-bU5VN25NLXZXU0U/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-eXpVSjd2Y25PaVk/edit?usp=sharing/ )| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-NjVTVERvajJUcGc/edit?usp=sharing/) |your.system.ip.address:8896/sparql |http://www.linkedmdb.org/sparql |  | [New York Times](http://data.nytimes.com/) |[ Download](https://drive.google.com/file/d/0B1tUDhWNTjO-dThoTm9DSmY4Wms/edit?usp=sharing/) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-VDhmNWJmZVcybm8/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-RG9GeVdxbDR4YjQ/edit?usp=sharing/ ) |your.system.ip.address:8897/sparql | - |  | [Semantic Web Dog Food](http://data.semanticweb.org/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-RjBWZXYyX2FDT1E/edit?usp=sharing/ )| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-c2h4al9VREF6bDg/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-UW5HaF9rekdialU/edit?usp=sharing/ ) |your.system.ip.address:8898/sparql | http://data.semanticweb.org/sparql|  | [Affymetrix](http://download.bio2rdf.org/release/2/affymetrix/affymetrix.html)| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-eHVlZ1RyVVFJQU0/edit?usp=sharing/ )| [ Download](https://drive.google.com/file/d/0B1tUDhWNTjO-RnV4SWtKelJTb0U/edit?usp=sharing/)|[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-Tm9oazNUdV9Cb1k/edit?usp=sharing )|your.system.ip.address:8899/sparql |http://cu.affymetrix.bio2rdf.org/sparql |    ### Evaluation Results and Runtime Errors     We have compared 5 - [FedX](https://www.mpi-inf.mpg.de/~khose/publications/ISWC2011.pdf ), [SPLENDID](http://ceur-ws.org/Vol-782/GoerlitzAndStaab_COLD2011.pdf ), [ANAPSID](http://link.springer.com/chapter/10.1007%2F978-3-642-25073-6_2#page-1 ), [SemaGrow](http://dl.acm.org/citation.cfm?id=2814886),  [HiBISUCuS](http://svn.aksw.org/papers/2014/HiBISCuS_ESWC/public.pdf) - state-of-the-art SPARQL endpoint federation systems with CostFed. Our complete evaluation results can be downloaded from [here](https://github.com/AKSW/CostFed/blob/master/Results.xlsx).        ### Authors    * [Muhammad Saleem](https://sites.google.com/site/saleemsweb/) (AKSW, University of Leipzig)   * Alexander Potocki (AKSW, University of Leipzig)   * [Tommaso Soru](http://tommaso-soru.it/) (AKSW, University of Leipzig)  * [Olaf Hartig](http://olafhartig.de/) (Linköping University, Sweden)  * [Axel-Cyrille Ngonga Ngomo](http://aksw.org/AxelNgonga.html) (AKSW, University of Leipzig)     We are especially thankful to Andreas Schwarte (fluid Operations, Germany), Olaf Görlitz (University Koblenz, Germany), and Angelos Charalambidis	(Institute of Informatics and Telecommunication, Paraskevi, Greece) for all their email conversations, feedbacks, and explanations. 	   """
Semantic web;https://github.com/lambdamusic/Sparql-cli;"""  # Sparql-cli    A command line SPARQL console. Works with endpoints as a well as local/remote graphs.       ### Status    Prototype. Runs but largely untested.       ### How-to    Suggested:    - Install via  `python setup.py install` then call `sparql-cli`      Otherwise, you can run in dev mode:     - get to the source top folder and run `python -m sparql-cli.main -h`      ### Summary    This is an attempt to build a sparql console using python-prompt toolkit    Idea:  either pass a sparql endpoint / or a graph URI which is loaded in memory using RDFLib  = useful to test things out quickly!    REQUIREMENTS  - click  - colorama  - rdflib  - pygments      @todo  - export results as html / web  - allow passing an endpoint @done  - add more saved queries   - store endpoints? eg via an extra command line  - add meta level cli eg .show or .info etc..  - namespaces and shortened URIs      ### Note     This tool relies on pygments for the syntax highlighting, however the current Sparql Lexer included in Pygments stable releases is broken (https://bitbucket.org/birkenfeld/pygments-main/issues/1236/sparql-lexer-error) hence you either have to update it from the dev branch or wait for the 2.2 release of  Sparql Lexer.        ### Changelog      **October 22, 2016**  - various improvements to print out  - added Endpoint connection and tested with DBpedia      **October 22, 2016**  - improved rendering of results   - added options via click       **October 21, 2016**  NOTE: > problem with Sparql Lexer  https://bitbucket.org/birkenfeld/pygments-main/issues/1236/sparql-lexer-error  ""fixed in 2.2 release soon""    Improved by installing this commit  https://bitbucket.org/birkenfeld/pygments-main/commits/60afc531aa2b  >> installed manually 'hg clone https://bitbucket.org/birkenfeld/pygments-main'  and it works     """
Semantic web;https://github.com/benjimor/odmtp-tpf;"""# ODMTP TPF Server  odmtp-tpf is a Triple Pattern Fragment server using Python, Django and ODMTP.    ODMTP (On Demand Mapper with Triple pattern matching) enables triple pattern matching over non-RDF datasources.    ODMTP support inference (see Semantic Reasoner (Inference) section)    # Online demo    ODMTP's implemented for Twitter API, Github API and Linkedin API are available online. You can run SPARQL queries using the online TPF client demo: [Here](http://query.linkeddatafragments.org/#datasources=http%3A%2F%2Fodmtp.priloo.univ-nantes.fr%2Ftwitter%2Fextended%2F&query=PREFIX%20it%3A%20%3Chttp%3A%2F%2Fwww.influencetracker.com%2Fontology%23%3E%0A%0ASELECT%20%3Fs%20WHERE%20%7B%0A%20%20%3Fs%20it%3AincludedHashtag%20%22SemanticWeb%22.%0A%7D) for Twitter,  [Here](http://query.linkeddatafragments.org/#datasources=http%3A%2F%2Fodmtp.priloo.univ-nantes.fr%2Fgithub%2Fextended%2F&query=PREFIX%20schema%3A%20%3Chttp%3A%2F%2Fschema.org%2F%3E%0A%0ASELECT%20%3Frepo%0AWHERE%20%7B%0A%09%3Frepo%20schema%3Alanguage%20%22Java%22%0A%7D) for Github (limited to 60 request per hour) and [Here](http://odmtp.priloo.univ-nantes.fr/linkedin/authentification/) for your Linkedin profile (you will need to login to access your personal LI profile).    ODMTP approach is already used on [OpenDataSoft Plateform](https://data.opendatasoft.com) and can be tested [Here](http://query.linkeddatafragments.org/#datasources=https%3A%2F%2Fpublic.opendatasoft.com%2Fapi%2Ftpf%2Froman-emperors%2F&query=PREFIX%20roman%3A%20%3Chttps%3A%2F%2Fpublic.opendatasoft.com%2Fld%2Fontologies%2Froman-emperors%2F%3E%0A%0ASELECT%20%3Fname%20WHERE%20%7B%0A%20%20%3Fs%20roman%3Abirth_cty%20%22Rome%22%5E%5Exsd%3Astring%20.%0A%20%20%3Fs%20roman%3Areign_start%20%3Fdate%20.%0A%20%20%20%20FILTER%20(%3Fdate%20%3E%20%220014-12-31T00%3A00%3A00%2B00%3A00%22%5E%5Exsd%3AdateTime)%0A%20%20%3Fs%20%20roman%3Aname%20%3Fname%20.%0A%7D).    # Instructions  ## Installation    #### macOS  You need to install [Homebrew](http://brew.sh/).    and then install Python 2.7:  ```bash  brew install python  ```  #### Ubuntu  ```  sudo apt-get install python-dev python-setuptools  ```    ## Dependencies  You need to install dependencies with pip:  ```bash  pip install -r requirements.txt  ```    ## Running The Platform  #### macOS & Ubuntu  From `~/odmtp-tpf` run the comand:  ```bash  python manage.py runserver  ```    The TPF server should run at: http://127.0.0.1:8000/    # Semantic Reasoner (Inference)  RDF data contains explicit and implicit triples that can be inferred using rules described in ontologies.  To support inference, ODMTP use ontologies to materialize implicit triples of mappings (extended mappings).  Each API can be queried using extended mappings at: `http://127.0.0.1:8000/{api}/extended`  Example: http://127.0.0.1:8000/twitter/extended    However, mappings only contains schema of the corresponding RDF dataset.  Thus, rules that apply on RDF instances cannot be applied on mappings.    ## Supported rules    All rules that apply to schema (class and properties) are supported by ODMTP Semantic Reasoner.    ### Implemented rules    | Rule Name                        |                    if dataset contains ...                   | ... then add                 |  |----------------------------------|:---------------------------------------------------------:|------------------------------|  | rdfs2 (domain)                   | aaa rdfs:domain xxx .<br>uuu aaa yyy .                       | uuu rdf:type xxx .           |  | rdfs3 (range)                    | aaa rdfs:range xxx  .<br>uuu aaa vvv .                        | vvv rdf:type xxx .           |  | rdfs5 (subProperty transitivity) | uuu rdfs:subPropertyOf vvv .<br>vvv rdfs:subPropertyOf xxx . | uuu rdfs:subPropertyOf xxx . |  | rdfs7 (subProperty)              | aaa rdfs:subPropertyOf bbb .<br>uuu aaa yyy .                | uuu bbb yyy .                |  | rdfs9 (subClassOf)               | uuu rdfs:subClassOf xxx .<br>vvv rdf:type uuu .              | vvv rdf:type xxx .           |  | rdfs11 (subClassOf transitivity) | uuu rdfs:subClassOf vvv .<br>vvv rdfs:subClassOf xxx .       | uuu rdfs:subClassOf xxx .    |  | owl sameAs Class                 | uuu owl:sameAs xxx .<br>vvv rdf:type uuu .                   | vvv rdf:type xxx .           |  | owl sameAs Property              | aaa owl:sameAs bbb .<br>uuu aaa yyy .                        | uuu bbb yyy .                |  | owl equivalentClass              | uuu owl:equivalentClass xxx .<br>vvv rdf:type uuu .          | vvv rdf:type xxx .           |  | owl equivalentProperty           | aaa owl:equivalentProperty bbb .<br>uuu aaa yyy .            | uuu bbb yyy .                |    ### Not implemented rules    Not all supported rules are yet implemented.  Complete list of [RDFS](https://www.w3.org/TR/rdf11-mt/#rdfs-entailment) and [OWL-LD](http://semanticweb.org/OWLLD/#Rules) rules.    ## Not supported rules    Rules that applies on instances are not supported by ODMTP Semantic Reasoner.  examples:    | Rule Name             |            if data contains ...           | ... then add     | Comment                                                                        |  |-----------------------|:-----------------------------------------:|------------------|--------------------------------------------------------------------------------|  | owl-ld eq-rep-subject | subj1 owl:sameAs subj2 .<br> subj1 aaa obj1 . | subj2 aaa obj1 . | Not supported if one of the two subject<br> is not defined in ontology or mapping  |  | owl-ld eq-rep-object  | obj1 owl:sameAs obj2 .<br> subj1 aaa obj1 .   | subj1 aaa obj2 . | Not supported if subject or objects are<br> not defined in ontology or mapping     |    # Mappings  Mappings are accessible at: `http://127.0.0.1:8000/{api}/mapping`  Example: http://127.0.0.1:8000/twitter/mapping    Extended mapping are accessible at `http://127.0.0.1:8000/{api}/mapping/extended`  Example: http://127.0.0.1:8000/twitter/mapping/extended    # Examples of Simple Queries  You can use any Triple Pattern Fragment client: http://linkeddatafragments.org/software/  to run SPARQL queries over twitter API and github Repo API V3  ## SPARQL queries over Twitter API  You can run this SPARQL query over http://127.0.0.1:8000/twitter/ to retrieve tweets using #iswc2017 hashtag.  ```sparql  PREFIX it: <http://www.influencetracker.com/ontology#>    SELECT ?s WHERE {   ?s it:includedHashtag ""SemanticWeb"".  }  ```    Retrieve tweets from a specific user.  ```sparql  PREFIX schema: <http://schema.org/>    SELECT ?s WHERE {   ?s schema:author ""NantesTech"".  }  ```    SPO query to browse tweets  ```sparql  SELECT ?s ?p ?o WHERE {   ?s ?p ?o  }  ```    To retrieve a specific tweet by ID  ```sparql  SELECT ?p ?o WHERE {   <https://twitter.com/statuses/889775221452005377> ?p ?o  }  ```    ## SPARQL queries over Github API  You can run this SPARQL query over http://127.0.0.1:8000/github/ to retrieve repositories using Java programming language.  ```sparql  PREFIX schema: <http://schema.org/>    SELECT ?repo  WHERE {  ?repo schema:language ""Java""  }  ```    SPO query to browse repositories  ```sparql  SELECT ?s ?p ?o WHERE {   ?s ?p ?o  }  ```    # Extras  To understand how ODTMP approach is working, the ISWC 2017 poster is available [here](https://docs.google.com/presentation/d/e/2PACX-1vT7fstdxp9LrqPdYpVpbDopBjBLJB5oUysFDp8iS3Z33MCqk-6Yq-2OrWZuWT1tqyFWLeAYcv2kshXe/embed?).    This is just a prototype, feel free to optimize it, improve mapping files, work on new api's etc. """
Semantic web;https://github.com/avicomp/ont-api;"""# ONT-API (ver. 1.4.2)    ## Notice  **The activity in this repository is frozen.  The v1.4.2 is the last release under the domain 'ru.avicomp'.   The new project's home is https://github.com/owlcs/ont-api**    ## Summary  ONT-API is an implementation of OWL-API over Apache Jena.    For more info see [wiki](https://github.com/avicomp/ont-api/wiki) page.     ## Dependencies  - **[Apache Jena](https://github.com/apache/jena)**, version **3.12.0**  - **[OWL-API](https://github.com/owlcs/owlapi)**, version **5.1.11**    ## License  * Apache License Version 2.0  * GNU LGPL Version 3.0   """
Semantic web;https://github.com/Swirrl/csv2rdf;"""# csv2rdf    [![CircleCI](https://circleci.com/gh/Swirrl/csv2rdf/tree/master.svg?style=svg)](https://circleci.com/gh/Swirrl/csv2rdf/tree/master)    Command line application (and clojure library) for converting [CSV to RDF](https://www.w3.org/TR/2015/REC-csv2rdf-20151217/) according to the specifications for [CSV on the web](https://w3c.github.io/csvw/).    ## Native Builds    We have some experimental native builds of the command line app here:    - For [linux x86](https://github.com/Swirrl/csv2rdf/releases/tag/graal-linux-0.4.7-SNAPSHOT-c8fe70c)  - For [mac os](https://github.com/Swirrl/csv2rdf/releases/tag/graal-0.4.7-SNAPSHOT-8839f3f)    ## Running    csv2rdf can be run from the command line given the location of either a tabular data file or metadata file referencing the described tabular file. The location  can be either a path on the local machine or URI for the document on the web.    To run from a tabular file:        java -jar csv2rdf-standalone.jar -t /path/to/tabular/file.csv    The resulting RDF is written to standard output in [turtle](https://www.w3.org/TR/turtle/) format. The output can instead be written to file with the -o option:        java -jar csv2rdf-standalone.jar -t /path/to/tabular/file.csv -o output.ttl    The extension of the output file is used to determine the output format. The full list of supported formats is defined by [rdf4j](http://docs.rdf4j.org/programming/#_detecting_the_file_format),  some common formats are listed below:    | Extension | Format                                               |  | --------- | -----------------------------------------------------|  | .ttl      | [turtle](https://www.w3.org/TR/turtle/)              |  | .nt       | [n-triples](https://www.w3.org/TR/n-triples/)        |  | .xml      | [rdf-xml](https://www.w3.org/TR/rdf-syntax-grammar/) |  | .trig     | [trig](https://www.w3.org/TR/trig/)                  |  | .nq       | [n-quads](https://www.w3.org/TR/n-quads/)            |    Note that for quad formats like trig and n-quads the graph will be nil.    The triples are generated according to CSVW standard mode by default. The mode to use can be specified by the -m parameter:        java -jar csv2rdf-standalone.jar -t /path/to/tabular/file.csv -m minimal    The supported values for the mode are `standard` and `minimal` and `annotated`. `annotated` mode is a non-standard mode which behaves like  `minimal` mode with the addition that any notes or non-standard annotations defined for table groups and tables will be output if the  corresponding metadata element specifies an `@id`.    The recommended way to start processing a tabular file is from a metadata document that describes the structure of a referenced tabular file. The tabular file does not  need to be provided when processing from a metadata file since the metadata should contain a reference to the tabular file(s).        java -jar csv2rdf-standalone.jar -u /path/to/metadata/file.json -o output.ttl    ## Using as a library    csv2rdf also exposes its functionality as a library - please see [the csv2rdf library](doc/library.md) for a description of the library and its interface.    - See [overview of the code](doc/code.md) for an overview of the codebase.  - See [Developing csv2rdf itself](doc/developing.md) for a quickstart guide on how to work on the library and application itself.    ## License    Copyright © 2018 Swirrl IT Ltd.    Distributed under the Eclipse Public License either version 1.0 or (at  your option) any later version. """
Semantic web;https://github.com/RubenVerborgh/N3.js;"""# Lightning fast, asynchronous, streaming RDF for JavaScript  [![Build Status](https://github.com/rdfjs/N3.js/workflows/CI/badge.svg)](https://github.com/rdfjs/N3.js/actions)  [![Coverage Status](https://coveralls.io/repos/github/rdfjs/N3.js/badge.svg)](https://coveralls.io/github/rdfjs/N3.js)  [![npm version](https://badge.fury.io/js/n3.svg)](https://www.npmjs.com/package/n3)  [![DOI](https://zenodo.org/badge/3058202.svg)](https://zenodo.org/badge/latestdoi/3058202)    The N3.js library is an implementation of the [RDF.js low-level specification](http://rdf.js.org/) that lets you handle [RDF](https://www.w3.org/TR/rdf-primer/) in JavaScript easily.  It offers:    - [**Parsing**](#parsing) triples/quads from    [Turtle](https://www.w3.org/TR/turtle/),    [TriG](https://www.w3.org/TR/trig/),    [N-Triples](https://www.w3.org/TR/n-triples/),    [N-Quads](https://www.w3.org/TR/n-quads/),    [RDF*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/)    and [Notation3 (N3)](https://www.w3.org/TeamSubmission/n3/)  - [**Writing**](#writing) triples/quads to    [Turtle](https://www.w3.org/TR/turtle/),    [TriG](https://www.w3.org/TR/trig/),    [N-Triples](https://www.w3.org/TR/n-triples/),    [N-Quads](https://www.w3.org/TR/n-quads/)    and [RDF*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/)  - [**Storage**](#storing) of triples/quads in memory    Parsing and writing is:  - **asynchronous** – triples arrive as soon as possible  - **streaming** – streams are parsed as data comes in, so you can parse files larger than memory  - **fast** – once the [fastest spec-compatible parser in JavaScript](https://github.com/rdfjs/N3.js/tree/master/perf)    (but then [graphy.js](https://github.com/blake-regalia/graphy.js) came along)    ## Installation  For Node.js, N3.js comes as an [npm package](https://npmjs.org/package/n3).    ```Bash  $ npm install n3  ```    ```JavaScript  const N3 = require('n3');  ```    N3.js seamlessly works in browsers via [webpack](https://webpack.js.org/)  or [browserify](http://browserify.org/).  If you're unfamiliar with these tools,  you can read  [_webpack: Creating a Bundle – getting started_](https://webpack.js.org/guides/getting-started/#creating-a-bundle)  or  [_Introduction to browserify_](https://writingjavascript.org/posts/introduction-to-browserify).  You will need to create a ""UMD bundle"" and supply a name (e.g. with the `-s N3` option in browserify).    You can also load it via CDN:  ```html  <script src=""https://unpkg.com/n3/browser/n3.min.js""></script>  ```    ## Creating triples/quads  N3.js follows the [RDF.js low-level specification](http://rdf.js.org/).    `N3.DataFactory` will give you the [factory](http://rdf.js.org/#datafactory-interface) functions to create triples and quads:    ```JavaScript  const { DataFactory } = N3;  const { namedNode, literal, defaultGraph, quad } = DataFactory;  const myQuad = quad(    namedNode('https://ruben.verborgh.org/profile/#me'),    namedNode('http://xmlns.com/foaf/0.1/givenName'),    literal('Ruben', 'en'),    defaultGraph(),  );  console.log(myQuad.termType);              // Quad  console.log(myQuad.value);                 // ''  console.log(myQuad.subject.value);         // https://ruben.verborgh.org/profile/#me  console.log(myQuad.object.value);          // Ruben  console.log(myQuad.object.datatype.value); // http://www.w3.org/1999/02/22-rdf-syntax-ns#langString  console.log(myQuad.object.language);       // en  ```    In the rest of this document, we will treat “triples” and “quads” equally:  we assume that a quad is simply a triple in a named or default graph.    ## Parsing    ### From an RDF document to quads    `N3.Parser` transforms Turtle, TriG, N-Triples, or N-Quads document into quads through a callback:  ```JavaScript  const parser = new N3.Parser();  parser.parse(    `PREFIX c: <http://example.org/cartoons#>     c:Tom a c:Cat.     c:Jerry a c:Mouse;             c:smarterThan c:Tom.`,    (error, quad, prefixes) => {      if (quad)        console.log(quad);      else        console.log(""# That's all, folks!"", prefixes);    });  ```  The callback's first argument is an optional error value, the second is a quad.  If there are no more quads,  the callback is invoked one last time with `null` for `quad`  and a hash of prefixes as third argument.  <br>  Pass a second callback to `parse` to retrieve prefixes as they are read.  <br>  If no callbacks are provided, parsing happens synchronously.    By default, `N3.Parser` parses a permissive superset of Turtle, TriG, N-Triples, and N-Quads.  <br>  For strict compatibility with any of those languages, pass a `format` argument upon creation:    ```JavaScript  const parser1 = new N3.Parser({ format: 'N-Triples' });  const parser2 = new N3.Parser({ format: 'application/trig' });  ```    Notation3 (N3) is supported _only_ through the `format` argument:    ```JavaScript  const parser3 = new N3.Parser({ format: 'N3' });  const parser4 = new N3.Parser({ format: 'Notation3' });  const parser5 = new N3.Parser({ format: 'text/n3' });  ```    It is possible to provide the base IRI of the document that you want to parse.  This is done by passing a `baseIRI` argument upon creation:  ```JavaScript  const parser = new N3.Parser({ baseIRI: 'http://example.org/' });  ```    By default, `N3.Parser` will prefix blank node labels with a `b{digit}_` prefix.  This is done to prevent collisions of unrelated blank nodes having identical  labels. The `blankNodePrefix` constructor argument can be used to modify the  prefix or, if set to an empty string, completely disable prefixing:  ```JavaScript  const parser = new N3.Parser({ blankNodePrefix: '' });  ```    ### From an RDF stream to quads    `N3.Parser` can parse [Node.js streams](http://nodejs.org/api/stream.html) as they grow,  returning quads as soon as they're ready.    ```JavaScript  const parser = new N3.Parser(),        rdfStream = fs.createReadStream('cartoons.ttl');  parser.parse(rdfStream, console.log);  ```    `N3.StreamParser` is a [Node.js stream](http://nodejs.org/api/stream.html) and [RDF.js Sink](http://rdf.js.org/#sink-interface) implementation.  This solution is ideal if your consumer is slower,  since source data is only read when the consumer is ready.    ```JavaScript  const streamParser = new N3.StreamParser(),        rdfStream = fs.createReadStream('cartoons.ttl');  rdfStream.pipe(streamParser);  streamParser.pipe(new SlowConsumer());    function SlowConsumer() {    const writer = new require('stream').Writable({ objectMode: true });    writer._write = (quad, encoding, done) => {      console.log(quad);      setTimeout(done, 1000);    };    return writer;  }  ```    A dedicated `prefix` event signals every prefix with `prefix` and `term` arguments.    ## Writing    ### From quads to a string    `N3.Writer` serializes quads as an RDF document.  Write quads through `addQuad`.    ```JavaScript  const writer = new N3.Writer({ prefixes: { c: 'http://example.org/cartoons#' } });  writer.addQuad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),    namedNode('http://example.org/cartoons#Cat')  );  writer.addQuad(quad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://example.org/cartoons#name'),    literal('Tom')  ));  writer.end((error, result) => console.log(result));  ```    By default, `N3.Writer` writes Turtle (or TriG if some quads are in a named graph).  <br>  To write N-Triples (or N-Quads) instead, pass a `format` argument upon creation:    ```JavaScript  const writer1 = new N3.Writer({ format: 'N-Triples' });  const writer2 = new N3.Writer({ format: 'application/trig' });  ```    ### From quads to an RDF stream    `N3.Writer` can also write quads to a Node.js stream.    ```JavaScript  const writer = new N3.Writer(process.stdout, { end: false, prefixes: { c: 'http://example.org/cartoons#' } });  writer.addQuad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),    namedNode('http://example.org/cartoons#Cat')  );  writer.addQuad(quad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://example.org/cartoons#name'),    literal('Tom')  ));  writer.end();  ```    ### From a quad stream to an RDF stream    `N3.StreamWriter` is a [Node.js stream](http://nodejs.org/api/stream.html) and [RDF.js Sink](http://rdf.js.org/#sink-interface) implementation.    ```JavaScript  const streamParser = new N3.StreamParser(),        inputStream = fs.createReadStream('cartoons.ttl'),        streamWriter = new N3.StreamWriter({ prefixes: { c: 'http://example.org/cartoons#' } });  inputStream.pipe(streamParser);  streamParser.pipe(streamWriter);  streamWriter.pipe(process.stdout);  ```    ### Blank nodes and lists  You might want to use the `[…]` and list `(…)` notations of Turtle and TriG.  However, a streaming writer cannot create these automatically:  the shorthand notations are only possible if blank nodes or list heads are not used later on,  which can only be determined conclusively at the end of the stream.    The `blank` and `list` functions allow you to create them manually instead:  ```JavaScript  const writer = new N3.Writer({ prefixes: { c: 'http://example.org/cartoons#',                                         foaf: 'http://xmlns.com/foaf/0.1/' } });  writer.addQuad(    writer.blank(      namedNode('http://xmlns.com/foaf/0.1/givenName'),      literal('Tom', 'en')),    namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),    namedNode('http://example.org/cartoons#Cat')  );  writer.addQuad(quad(    namedNode('http://example.org/cartoons#Jerry'),    namedNode('http://xmlns.com/foaf/0.1/knows'),    writer.blank([{      predicate: namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),      object:    namedNode('http://example.org/cartoons#Cat'),    },{      predicate: namedNode('http://xmlns.com/foaf/0.1/givenName'),      object:    literal('Tom', 'en'),    }])  ));  writer.addQuad(    namedNode('http://example.org/cartoons#Mammy'),    namedNode('http://example.org/cartoons#hasPets'),    writer.list([      namedNode('http://example.org/cartoons#Tom'),      namedNode('http://example.org/cartoons#Jerry'),    ])  );  writer.end((error, result) => console.log(result));  ```    ## Storing    `N3.Store` allows you to store triples in memory and find them fast.    In this example, we create a new store and add the triples `:Pluto a :Dog.` and `:Mickey a :Mouse`.  <br>  Then, we find triples with `:Mickey` as subject.    ```JavaScript  const store = new N3.Store();  store.addQuad(    namedNode('http://ex.org/Pluto'),    namedNode('http://ex.org/type'),    namedNode('http://ex.org/Dog')  );  store.addQuad(    namedNode('http://ex.org/Mickey'),    namedNode('http://ex.org/type'),    namedNode('http://ex.org/Mouse')  );    const mickey = store.getQuads(namedNode('http://ex.org/Mickey'), null, null)[0];  console.log(mickey);  ```    ### Addition and deletion of quads  The store provides the following manipulation methods  ([documentation](http://rdfjs.github.io/N3.js/docs/N3Store.html)):  - `addQuad` to insert one quad  - `addQuads` to insert an array of quads  - `removeQuad` to remove one quad  - `removeQuads` to remove an array of quads  - `remove` to remove a stream of quads  - `removeMatches` to remove all quads matching the given pattern  - `deleteGraph` to remove all quads with the given graph  - `createBlankNode` returns an unused blank node identifier    ### Searching quads or entities  The store provides the following search methods  ([documentation](http://rdfjs.github.io/N3.js/docs/N3Store.html)):  - `getQuads` returns an array of quads matching the given pattern  - `match` returns a stream of quads matching the given pattern  - `countQuads` counts the number of quads matching the given pattern  - `forEach` executes a callback on all matching quads  - `every` returns whether a callback on matching quads always returns true  - `some`  returns whether a callback on matching quads returns true at least once  - `getSubjects` returns an array of unique subjects occurring in matching quads  - `forSubjects` executes a callback on unique subjects occurring in matching quads  - `getPredicates` returns an array of unique predicates occurring in matching quad  - `forPredicates` executes a callback on unique predicates occurring in matching quads  - `getObjects` returns an array of unique objects occurring in matching quad  - `forObjects` executes a callback on unique objects occurring in matching quads  - `getGraphs` returns an array of unique graphs occurring in matching quad  - `forGraphs` executes a callback on unique graphs occurring in matching quads    ## Compatibility  ### Format specifications  The N3.js parser and writer is fully compatible with the following W3C specifications:  - [RDF 1.1 Turtle](https://www.w3.org/TR/turtle/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-turtle.ttl)  - [RDF 1.1 TriG](https://www.w3.org/TR/trig/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-trig.ttl)  - [RDF 1.1 N-Triples](https://www.w3.org/TR/n-triples/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-ntriples.ttl)  - [RDF 1.1 N-Quads](https://www.w3.org/TR/n-quads/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-nquads.ttl)    In addition, the N3.js parser also supports [Notation3 (N3)](https://www.w3.org/TeamSubmission/n3/) (no official specification yet).    The N3.js parser and writer are also fully compatible with the RDF* variants  of the W3C specifications.    The default mode is permissive  and allows a mixture of different syntaxes, including RDF*.  Pass a `format` option to the constructor with the name or MIME type of a format  for strict, fault-intolerant behavior.  If a format string contains `star` or `*`  (e.g., `turtlestar` or `TriG*`),  RDF* support for that format will be enabled.    ### Interface specifications  The N3.js submodules are compatible with the following [RDF.js](http://rdf.js.org) interfaces:    - `N3.DataFactory` implements    [`DataFactory`](http://rdf.js.org/data-model-spec/#datafactory-interface)    - the terms it creates implement [`Term`](http://rdf.js.org/data-model-spec/#term-interface)      and one of      [`NamedNode`](http://rdf.js.org/data-model-spec/#namednode-interface),      [`BlankNode`](http://rdf.js.org/data-model-spec/#blanknode-interface),      [`Literal`](http://rdf.js.org/data-model-spec/#literal-interface),      [`Variable`](http://rdf.js.org/data-model-spec/#variable-interface),      [`DefaultGraph`](http://rdf.js.org/data-model-spec/#defaultgraph-interface)    - the triples/quads it creates implement      [`Term`](http://rdf.js.org/data-model-spec/#term-interface),      [`Triple`](http://rdf.js.org/data-model-spec/#triple-interface)      and      [`Quad`](http://rdf.js.org/data-model-spec/#quad-interface)  - `N3.StreamParser` implements    [`Stream`](http://rdf.js.org/stream-spec/#stream-interface)    and    [`Sink`](http://rdf.js.org/stream-spec/#sink-interface)  - `N3.StreamWriter` implements    [`Stream`](http://rdf.js.org/stream-spec/#stream-interface)    and    [`Sink`](http://rdf.js.org/stream-spec/#sink-interface)  - `N3.Store` implements    [`Store`](http://rdf.js.org/stream-spec/#store-interface)    [`Source`](http://rdf.js.org/stream-spec/#source-interface)    [`Sink`](http://rdf.js.org/stream-spec/#sink-interface)    ## License and contributions  The N3.js library is copyrighted by [Ruben Verborgh](https://ruben.verborgh.org/)  and released under the [MIT License](https://github.com/rdfjs/N3.js/blob/master/LICENSE.md).    Contributions are welcome, and bug reports or pull requests are always helpful.  If you plan to implement a larger feature, it's best to contact me first. """
Semantic web;https://github.com/opensemanticsearch/solr-ontology-tagger;"""# solr-ontology-tagger  Automatic tagging of documents in an Apache Solr index for faceted search by RDF(S) Ontologies &amp; SKOS thesauri and converter for alternate labels in SKOS thesaurus to Solr synonyms config file. """
Semantic web;https://github.com/opencube-toolkit/tarql-component;"""OpenCube Toolkit - Tarql Extension  ===============      [Tarql](https://github.com/cygri/tarql) is a command-line tool for converting CSV files to RDF using SPARQL 1.1 syntax. It's written in Java and based on Apache ARQ.    ### How it works    TARQL Extension for data cubes is a tool for converting CSV files to RDF accordingly to SPARQL 1.1 syntax. The data cubes are generated based on the provided, easy to modify, query templates. The extension is integrated with the Information Workbench as a standard data provider. The interface delivered enables user to specify the basic information about the provider along with the location of the CSV file, polling intervals and to modify the cube mapping query via provided SPARQL editor. The interface includes information on status and the time that was needed to transform the data and enables to browse the output triples generated.    The TARQL extension can be stacked with other data components for instance the output RDF can be visualised with i.e. the OpenCube Map View.      ###Functionality    The component is built on top of Apache ARQ2. The OpenCube TARQL component includes the new release of TARQL. It brings several improvements, such as: streaming capabilities, multiple query patterns in one mapping file, convenient functions for typical mapping activities, validation rules included in mapping file and increased flexibility (dealing with CSV variants like TSV).    The OpenCube Toolkit user is able to create Cube directly from the input files and store the output in the SPARQL endpoint. The OpenCube TARQL extension offers the following options:    + Stream CSV processing  + Use of column headers as variable names  + Translate the CSV imported table into RDF by using a prepared mapping file (SPARQL construct schema)  + Test the mapping file (shows only the CONSTRUCT template, variable names, and a few input rows) """
Semantic web;https://github.com/phillord/tawny-owl;"""Tawny-OWL  ===========    [![Build Status](https://travis-ci.org/phillord/tawny-owl.png?branch=master)](https://travis-ci.org/phillord/tawny-owl)    ## Introduction    <img src=""docs/tawny-cartoon-only-owl.png""   alt=""Tawny Logo"" title=""Tawny OWL Logo"" height=""300"" align=""right"" />    Tawny-OWL allows construction of OWL ontologies, in a evaluative, functional  and fully programmatic environment. Think of it as the ontology engineering  equivalent of [R](http://www.r-project.org/). It has many advantages over  traditional ontology engineering tools, also described in a  [video introduction](https://vimeo.com/89782389).    - An interactive shell or REPL to explore and create ontologies.  - Source code, with comments, editable using any of a range of IDEs.  - Fully extensible -- new syntaxes, new data sources can be added by users  - Patterns can be created for individual ontologies; related classes can be    built easily, accurately and maintainably.  - A unit test framework with fully reasoning.  - A clean syntax for versioning with any VCS, integrated with the IDE  - Support for packaging, dependency resolution and publication  - Enabled continuous integration with both ontology and software dependencies    Tawny-OWL is implemented as a domain-specific language but built over a full  programming language called [Clojure](http://www.clojure.org). Many of the  features described (REPL, patterns, unit tests, extensibility) derive directly  from the Clojure language, or from general-purpose programming tools (IDEs,  versioning, continuous integration). The core ontology features are  implemented directly using the [OWL API](http://owlapi.sourceforge.net/).  These features are, therefore, industry strength, standards-compliant and  well-supported independently of the Tawny-OWL developers.    OWL is a W3C standard ontology representation language; an ontology is a fully  computable set of statements, describing the things and their relationships.  They are used, mostly notable in biomedicine, to describe complex areas of  knowledge such as [genetics](http://www.geneontology.org/) or  [clinical terminology](http://en.wikipedia.org/wiki/SNOMED_CT), but can  describe anything, including [e-commerce](http://purl.org/goodrelations/). For  more tutorial information, please see http://ontogenesis.knowledgeblog.org.    A [full-length manual](https://phillord.github.io/take-wing/) is also  [available](http://github.com/phillord/take-wing). The original,  slightly older [getting started](docs/getting-started.md) document is  available.    ## For the Clojure developer    Tawny-OWL is predominately designed as a programmatic application for ontology  development, but it can be used as an API. OWL ontologies are a set of  statements about things and their relationships; underneath these statements  map to a subset of first-order logic which makes it possible to answer   questions about these statements using highly-optimised reasoners.    Currently, the use of ontologies as a tool within general-purpose programming  is relatively under-developed. Part of the intention behind Tawny-OWL is to  embed ontologies deeply within a programmatic framework, to see whether  ontologies are useful in this way.    Further information on the use of Tawny-OWL is available in the  [documentation](docs/tawny-as-an-api.md).      ## Motivation    I discuss the development of this code base in my  [journal](http://www.russet.org.uk/blog). Two posts include one on the  [motivation](http://www.russet.org.uk/blog/2214) and another on making  the library more [""lispy""](http://www.russet.org.uk/blog/2254). All  revelevant posts are  [categorised](http://www.russet.org.uk/blog/category/all/professional/tech/tawny-owl).    ## Installation    Tawny-OWL requires no installation *per se* and is used as any Clojure  library. It is available from  [clojars](https://clojars.org/uk.org.russet/tawny-owl), so just add:    `[uk.org.russet/tawny-owl]` to your `project.clj` file.    I use Leiningen 2 on the current version 16.04 Ubuntu and, occasionally, on  Windows. Editing of both tawny-owl and the ontologies using it, is with Emacs  25 using Clojure mode and nrepl, currently both installed from their  respective versioning systems. The library should not depend on this  environment, however.    ## Author    Phillip Lord, Newcastle University.    http://www.russet.org.uk/blog    ## Mailing List    There is a [mailing list](mailto:tawny-owl@googlegroups.com).    ## Version    [![Clojars Project](http://clojars.org/uk.org.russet/tawny-owl/latest-version.svg)](http://clojars.org/uk.org.russet/tawny-owl)        ## License    The contents of this file are subject to the LGPL License, Version 3.0.    Copyright (C) 2012, 2013, Newcastle University    This program is free software: you can redistribute it and/or modify it under  the terms of the GNU Lesser General Public License as published by the Free  Software Foundation, either version 3 of the License, or (at your option) any  later version.    This program is distributed in the hope that it will be useful, but WITHOUT  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS  FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.    You should have received a copy of the GNU Lesser General Public License along  with this program. If not, see http://www.gnu.org/licenses/. """
Semantic web;https://github.com/joshsh/sesametools;"""<!-- This README can be viewed at https://github.com/joshsh/sesametools/wiki -->    ![SesameTools logo|width=322px|height=60px](https://github.com/joshsh/sesametools/wiki/graphics/SesameTools-logo.png)    Welcome to the SesameTools wiki!  SesameTools is a collection of general-purpose components for use with the [Sesame](http://rdf4j.org) RDF framework.  It includes:    * **SesameTools common utilities**: miscellaneous useful classes  * **CachingSail**: an in-memory cache for RDF data  * **ConstrainedSail**: a Sail implementation which interacts only with given named graphs.  Useful for simple access control.  * **DeduplicationSail**: a Sail implementation which avoids duplicate statements.  For use with triple stores such as AllegroGraph which otherwise allow duplicates.  * [LinkedDataServer](https://github.com/joshsh/sesametools/wiki/LinkedDataServer): a RESTful web service to publish a Sesame data store as Linked Data  * **MappingSail**: a Sail which translates between two URI spaces.  Used by LinkedDataServer.  * **RDFTransactionSail**: a write-only Sail which generates RDF transactions instead of performing per-call updates.  Useful for streaming RDF data as described [here](http://arxiv.org/abs/1011.3595)  * **ReadOnlySail**: a read-only Sail implementation  * **ReplaySail**: a pair of Sail implementations which allow Sail operations to be first recorded to a log file, then reproduced from the log file  * **RepositorySail**: a Sail implementation which wraps a Repository object.  This is essentially the inverse of Sesame's [SailRepository](http://rdf4j.org/sesame/2.7/apidocs/org/openrdf/repository/sail/SailRepository.html)  * **Sesamize**: command-line tools for Sesame  * **URI Translator**: a utility which runs SPARQL-1.1 Update queries against a Repository to convert URIs between different prefixes  * **WriteOnlySail**: a write-only Sail implementation    See also the [Sesametools API](http://fortytwo.net/projects/sesametools/api/latest/index.html).    For projects which use Maven, SesameTools snapshots and release packages can be imported by adding configuration like the following to the project's POM:    ```xml          <dependency>              <groupId>net.fortytwo.sesametools</groupId>              <artifactId>linked-data-server</artifactId>              <version>1.10</version>          </dependency>  ```    The latest Maven packages can be browsed [here](http://search.maven.org/#search%7Cga%7C1%7Csesametools).    **Credits**: SesameTools is written and maintained by [Joshua Shinavier](https://github.com/joshsh) and [Peter Ansell](https://github.com/ansell). Patches have been contributed by [Faisal Hameed](https://github.com/faisal-hameed), [Florian Kleedorfer](https://github.com/fkleedorfer), [Olaf Görlitz](https://github.com/goerlitz), and [Michal Klempa](https://github.com/michalklempa). An RDF/JSON parser and writer present in releases 1.7 and earlier contain code by [Hannes Ebner](http://ebner.wordpress.com/). """
Semantic web;https://github.com/kbss-cvut/jb4jsonld-jackson;"""# Java Binding for JSON-LD - Jackson    [![Build Status](https://kbss.felk.cvut.cz/jenkins/buildStatus/icon?job=jaxb-jsonld-jackson)](https://kbss.felk.cvut.cz/jenkins/job/jaxb-jsonld-jackson)    Java Binding for JSON-LD - Jackson (JB4JSON-LD-Jackson) is a binding of JB4JSON-LD for [Jackson](https://github.com/FasterXML/jackson).    The core implementation of JB4JSON-LD with a mapping example can be found at [https://github.com/kbss-cvut/jb4jsonld](https://github.com/kbss-cvut/jb4jsonld).    More info can be found at [https://kbss.felk.cvut.cz/web/kbss/jb4json-ld](https://kbss.felk.cvut.cz/web/kbss/jb4json-ld).    ## Usage    JB4JSON-LD is based on annotations from [JOPA](https://github.com/kbss-cvut/jopa), which enable POJO attributes  to be mapped to ontological constructs (i.e. to object, data or annotation properties) and Java classes to ontological  classes.    Use `@OWLDataProperty` to annotate data fields and `@OWLObjectProperty` to annotate fields referencing other mapped entities.    To integrate the library with Jackson, register a `cz.cvut.kbss.jsonld.jackson.JsonLdModule` in Jackson's `ObjectMapper` like this:    `objectMapper.registerModule(new JsonLdModule())`    and you should be good to go. See the `JsonLdSerializionTest` for examples.    See [https://github.com/kbss-cvut/jopa-examples/tree/master/jsonld](https://github.com/kbss-cvut/jopa-examples/tree/master/jsonld) for  an executable example of JB4JSON-LD-Jackson in action.    ## Serialization    The serializer's output has been verified to be a valid JSON-LD and is parseable by Java's JSON-LD reference implementation   [jsonld-java](https://github.com/jsonld-java/jsonld-java).    The output is basically a context-less compacted JSON-LD, which uses full IRIs for attribute names.    ## Deserialization    Since we are using jsonld-java to first process the incoming JSON-LD, it does not matter in which form (expanded, framed, flattened) the  input is.    ## Getting JB4JSON-LD-Jackson    There are two ways to get JB4JSON-LD-Jackson:    * Clone repository/download zip and build it with Maven,  * Use a [Maven dependency](http://search.maven.org/#search%7Cga%7C1%7Ccz.cvut.kbss.jsonld):    ```XML  <dependency>      <groupId>cz.cvut.kbss.jsonld</groupId>      <artifactId>jb4jsonld-jackson</artifactId>  </dependency>  ``` """
Semantic web;https://github.com/lanthaler/HydraConsole;"""HydraConsole  ============    [Hydra][1] is a lightweight vocabulary to create hypermedia-driven Web APIs.  By specifying a number of commonly used concepts it renders the creation of  generic API clients possible. The HydraConsole is such a generic API client  in the form of a single-page web application.    For a high level description of how the HydraConsole works, please refer to  my dissertation  [Third Generation Web APIs—Bridging the Gap between REST and Linked Data][2].      Installation  ------------    At the moment, the HydraConsole uses a [JSON-LD Processor][3] and a proxy  written in PHP to access and process responses of Web APIs. Thus, the  simplest way to install the HydraConsole is to use [Composer][4].    If you don't have Composer yet, download it following the instructions on  http://getcomposer.org/ or just run the following command:        curl -s http://getcomposer.org/installer | php    Then, use Composer's `create-project` command to download the HydraConsole  and install all it's dependencies:        php composer.phar create-project -s dev ml/hydra-console path/to/install    You can now serve the HydraConsole with PHP's built-in web server:        php -S localhost:8000 -t path/to/install    That's it. Just fire up your browser and point it to        http://localhost:8000      Collaboration  ------------    To participate in the development please file bugs and issues in the  issue tracker or submit pull requests. If there's enough interest I'll  create a dedicated mailing list in the future.    You can find more information about Hydra and a demo installation of the  HydraConsole on my homepage: http://www.markus-lanthaler.com/hydra/      [1]: http://www.markus-lanthaler.com/hydra/  [2]: http://m.lanthi.com/3gen-web-apis-p171  [3]: http://m.lanthi.com/json-ld  [4]: http://getcomposer.org/ """
Semantic web;https://github.com/architolk/Linked-Data-Studio;"""# Linked Data Studio  The Linked Data Studio (LDS) is a platform for the creation of Linked Data.    The LDS is an extension to the [Linked Data Theatre](https://github.com/architolk/Linked-Data-Theatre), and you should have a working version of the Linked Data Theatre if you want to use the Linked Data Theatre!    See [BUILD.md](BUILD.md) for instructions to build the Linked Data Studio. You can also try one of the releases:    - [lds-1.7.0.war](https://github.com/architolk/Linked-Data-Studio/releases/download/v1.7.0/lds-1.7.0.war ""ldt-1.7.0.war"")    If you want to create a new release of the LDS, please look into the instructions for creating a new release of the [Linked Data Theatre](https://github.com/architolk/Linked-Data-Theatre), the instructions are the same.    To deploy the Linked Data Studio in a tomcat container, follow the instructions in [DEPLOY.md](DEPLOY.md). You can also opt for a docker installation, see [DOCKER.md](DOCKER.md).    The Linked Data Studio uses a configuration graph containing all the triples that make up the LDS configuration. Instructions and examples how to create such a configuration can be found at the [wiki](https://github.com/architolk/Linked-Data-Theatre/wiki) of the Linked Data Theatre.    For security, firewall, proxy and certificates: use the documentation of the [Linked Data Theatre](https://github.com/architolk/Linked-Data-Theatre). """
Semantic web;https://github.com/mmlab/Git2PROV;"""  [![Build Status](https://travis-ci.org/vladistan/Git2PROV.svg?branch=feature%2Funit-tests)](https://travis-ci.org/vladistan/Git2PROV.svg?branch=feature%2Funit-tests)  [![Coverage Status](https://img.shields.io/coveralls/vladistan/Git2PROV.svg)](https://coveralls.io/r/vladistan/Git2PROV)    #Git2PROV  Check out our [One-minute Git2PROV tutorial on Vimeo](http://vimeo.com/70980809)    For an in-depth description of this tool and its creation, we refer to the following paper:    [Git2PROV: Exposing Version Control System Content as W3C PROV](http://www.iswc2013.semanticweb.org/sites/default/files/iswc_demo_32_0.pdf)  by Tom De Nies, Sara Magliacane, Ruben Verborgh, Sam Coppens, Paul Groth, Erik Mannens, and Rik Van de Walle  Published in 2013 in the Poster and Demo Proceedings of the 12th International Semantic Web Conference.    #Disclaimer and License  Git2PROV is a joint work of [Ghent University](http://www.ugent.be/) - [iMinds](http://www.iminds.be/) - [Multimedia Lab](http://mmlab.be/), and the [Data2Semantics](http://www.data2semantics.org/) project of the [VU University Amsterdam](http://www.vu.nl/) .    The people involved are:  * Tom De Nies (Ghent University - iMinds - MMLab)  * Sara Magliacane (VU Amsterdam)  * Ruben Verborgh (Ghent University - iMinds - MMLab)  * Sam Coppens (Ghent University - iMinds - MMLab)  * Paul Groth (VU Amsterdam)  * Erik Mannens (Ghent University - iMinds - MMLab)  * Rik Van de Walle (Ghent University - iMinds - MMLab)    We chose to make Git2PROV open source under [GPL Version 3 license](http://www.gnu.org/licenses/gpl.html) because we believe this will lead it to its full potential, and be of much more value to the Web community than a single isolated instance running on a server somewhere.    So in short, you are free to use and modify Git2PROV for non-commercial purposes, as long as you make your stuff open source as well and you properly credit us. This is most conveniently done by citing the paper mentioned above.    #Installation    Make sure you have node.js and git installed and in the system PATH variable. Then, run:  ```  [sudo] npm install -g git2prov  ```    ## Converting a repository  To convert a single repository, run:        git2prov git_url [serialization]    For example:        git2prov git@github.com:RubenVerborgh/N3.js.git PROV-JSON    ## Running the server  To run the server, use the following command:        git2prov-server [port]    For example:        git2prov-server 8905    Then go to your browser and enter the following url:  http://localhost:8905/    This will give you the [standard Git2PROV homepage](http://git2prov.org).    TO use the service directly, use the following URL:  http://localhost:8905/git2prov?giturl=<your open git repository>&serialization=<your serialization of choice>&[optional parameters]  The OPTIONAL parameters are:    serialization:  * PROV-N (default)  * PROV-JSON  * PROV-O  * PROV-XML    shortHashes  * true ---> This will force the git log to use short hashes, making the output more readable by humans      ignore  * <provenanceRelation> ---> This provenance relation will not appear in the output. Multiple values are possible.        Example:  http://localhost:8905/git2prov?giturl=<your open git repository>&serialization=PROV-JSON&shortHashes=true&ignore=wasInformedBy&ignore=used    To start a proxy server:      node proxy.js <port> <target port>  for example:      node proxy.js 80 8905    ##Running as a service on a Linux/UNIX machine  This script is used in combination with init.d. You could also modify it to work with upstart.    Copy the startup script ""git2prov"" to your /etc/init.d directory:  ```  sudo cp scripts/git2prov /etc/init.d/git2prov  ```  Make it executable  ```  sudo chmod a+x /etc/init.d/git2prov  ```  add it to the startup services  ```  update-rc.d git2prov defaults  ```  You can now do commands such as  ```  sudo service git2prov start  sudo service git2prov restart  sudo service git2prov stop  ```    And the service should automatically start when the machine is rebooted. """
Semantic web;https://github.com/GeoKnow/LinkedGeoData;"""## Welcome to LinkedGeoData: Providing OpenStreetMap data as RDF  LinkedGeoData (LGD) is an effort to add a spatial dimension to the Web of Data / Semantic Web. LinkedGeoData uses the information collected by the OpenStreetMap project and makes it available as an RDF knowledge base according to the Linked Data principles. It interlinks this data with other knowledge bases in the Linking Open Data initiative.    The project web site can be found [here](http://linkedgeodata.org).  If you are running [Ubuntu](http://www.ubuntu.com) then this repository contains everything you need to transform OpenStreetMap data to RDF yourself.  For other systems please consider contributing adaptions of the existing scripts.    ### Contributions Welcome  The docker-based architecture is aimed at making it easy to contribute new or alternative components that can sit side-by-side with the core of the system - which  is the a virtual knowledge graph view over an OSM database.  Please open issues for discussion.    Examples include but are not limited to:    * A more modern Linked Data Interface that displays GeoSPARQL geometries out-of-the-box  * Nicer SPARQL interface (YASGUI)  * Another data virtualization engine in order to ease comparision with the already integrated ones  * Updates to the existing OSM-RDF mappings, proposals about how this system could be improved.  * Proposals for a better IRI schema. For example, the 'triplify' in the IRIs is archaic. Migration to the pattern used by [Sophox](https://wiki.openstreetmap.org/wiki/Sophox) seems very worthwhile. Because of the virtual knowledge graph approach there should be no problem to use the legacy approach in parallel.  * General proposals for architecture improvements (e.g. config options in the docker setup to improve modularity)    Dockerfiles for services such as a Linked Data or SPARQL interfaces should be designed to allow configuration of the target SPARQL endpoint(s), ideally via the docker environment.    ### How It Works    The architecture shown in the image below. The docker setup is located in the [linkedgeodata-docker](linkedgeodata-docker) folder.    ![LGD Dockerized Architecture Overview](docs/assets/images/lgd-architecture-2021-01-18.png)    * This project first uses Osmosis to initialize a 'raw' OpenStreetMap postgis database (using simple schema) from a `.osm.pdf` file.  * Then, this database is extended with additional tables containing RDF mapping - and interlinking - information. Also, helper views are provided for simplifying access to the integrated information.  * Further, a nominatim setup (based on osm2pgqsl) is performed to further enrich the initial database osm data.  * A set of RDB2RDF mappings is provided that enables running SPARQL queries over the postgis database. The SPARQL-2-SQL rewriting engine we use is Sparqlify.  * Dumps are generated by simply running preconfigured SPARQL queries.        ### Debian package now available!  Technically, LinkedGeoData is set of SQL files, database-to-rdf (RDB2RDF) mappings, and bash scripts.  The actual RDF conversion is carried out by the SPARQL-to-SQL rewriter [Sparqlify](https://github.com/AKSW/Sparqlify).  You can [view the Sparqlify Mappings for LinkedGeoData here](https://raw.github.com/GeoKnow/LinkedGeoData/master/linkedgeodata-core/src/main/resources/org/aksw/linkedgeodata/sml/LinkedGeoData-Triplify-IndividualViews.sml).  Therefore, if you want to install the LinkedGeoData debian package, you also  Sparqlify one.    For the latest version of LinkedGeoData package, perform the following steps to set up the package source:    Register the repo        echo 'deb http://cstadler.aksw.org/repos/apt precise main contrib non-free' | sudo tee /etc/apt/sources.list.d/cstadler.aksw.org.list    Import the public key        wget -qO - http://cstadler.aksw.org/repos/apt/conf/packages.precise.gpg.key  | sudo apt-key add -    Now you can install LinkedGeoData using        sudo apt-get update      sudo apt-get install linkedgeodata      You can download and install packages manually, however installing their dependencies requires more work:  * [sparqlify-cli Debian Package](http://cstadler.aksw.org/repos/apt/pool/main/s/sparqlify-cli/)  * [linkedgeodata-nominatim Debian Package](http://cstadler.aksw.org/repos/apt/pool/main/l/linkedgeodata-nominatim-v2.5.1/)  * [linkedgeodata Debian Package](http://cstadler.aksw.org/repos/apt/pool/main/l/linkedgeodata/)      After installing these packages, the following essential commands will be available:  * `lgd-createdb` (provided by linkedgeodata) - Sets up a complete database ready for subsequent RDF conversion  * `lgd-createdb-snapshot` (provided by linkedgeodata) - Same as above, however using a different schema (experimental; not recommended)  * `sparqlify-tool` (provided by sparqlify) - Runs SPARQL queries on a relational (LGD) database  * Have a look at the [section for additional tools](#additional-tools)    Read the section on data conversion for their documentation.    ### Alternative set up  In [/bin](https://github.com/GeoKnow/LinkedGeoData/tree/master/linkedgeodata-cli/bin) you find the following setup helper scripts which are aimed at easing the LinkedGeoData setup directly from source; without a debian package:    * `[lgd-apt-install-ubuntu-16.04.sh](linkedgeodata-cli/bin/lgd-apt-install-ubuntu-16.04.sh)`: Installs all required system packages using apt (postgres, postgis, java, and several dependencies required for (building) nominatim)    The following scripts are just helpers to build and/or install the Sparqlify debian package. Mainly intended for development.    * `lgd-build-and-install-dependencies.sh`: Builds a Sparqlify debian package from source and installs it.  * `lgd-download-and-install-dependecies.sh`: Simply downloads and installs the latest Sparqlify debian package.      ### Do it yourself data conversion  This section describes how to create and query a LinkedGeoData database. After you installed the LinkedGeoData scripts, you need to obtain an OpenStreetMap dataset which you want to load.  Note: Make sure to read the section on database tuning when dealing with larger datasets!    As for obtaining datasets, a very good source for OSM datasets in bite-size chunks is [GeoFabrik](http://download.geofabrik.de). For full dumps, refer to the [planet downloads](http://planet.openstreetmap.org/).    In [/bin](https://github.com/GeoKnow/LinkedGeoData/tree/master/linkedgeodata-cli/bin) you find several scripts. Essentially they are designed to work both from a cloned LinkedGeoData Git repo and wrapped up as a debian package.  All of them are configured via `lgd.conf.dist`. You can override the default settings without changing this file by creating a `lgd.conf` file.  If you installed the debian package, instead of the lgd.conf.dist file, the file /etc/sparqlify/sparqlify.conf` is used.  If you are using the following scripts from the git repo, invoke them with `./scriptname.sh` (i.e. don't forget the `./` and `.sh`).    * (`lgd-createdb-snapshot`: An experimental, but possibly much faster, version of the `lgd-createdb` script. Probably the `lgd-createdb` command will eventually refer to this version.)    * `lgd-createdb`: Creates and loads an LGD database    * -h  postgres host name    * -d  postgres database name    * -U  postgres user name    * -W  postgres password (will be added to ~/.pgpass if not exists)    * -f  .pbf file to load (other formats currently not supported)    Example:        wget http://download.geofabrik.de/europe/monaco-latest.osm.pbf      lgd-createdb -h localhost -d lgd -U postgres -W mypwd -f monaco-latest.osm.pbf    The reason we chose Monaco for the example is simply that it is a small file (> 10MB).      * `sparqlify-tool`: This is a small wrapper for `sparqlify` command that adds a simple profile system for convenience.    * -P  profile name. Settings will be loaded from such a file (see below) and can be overridden by further options.    * -h  database host name    * -d  database name    * -U  database user name    * -W  database password (will be added to ~/.pgpass if not exists)    * -Q  SPARQL query string or named query      Here is an example of a profile file, which is assumed to be located at `/etc/sparqlify/profiles.d/lgd-example.conf`.  This file will be deployed when installing the linkedgeodata debian package.            dbHost=""localhost""          dbName=""lgd""          dbUser=""postgres""          dbPass=""postgres""          mappingFile=""/usr/share/linkedgeodata/sml/LinkedGeoData-Triplify-IndividualViews.sml /usr/share/linkedgeodata/sml/LinkedGeoData-Nominatim.sml""      A named query is just a SPARQL query that is referenced by a name.  The mapping of a name to a SPARQL is configured via `/etc/sparqlify/sparqlify.conf`.    Currently, the following named queries exist:    * `ontology`: Creates an N-Triple output with all classes and properties  * `dump`: Create a full dump of the database    Examples:            sparqlify-tool -P lgd-example ontology          sparqlify-tool -P lgd-example dump          sparqlify-tool -h localhost -d lgd -U postgres -W mypwd -Q 'Construct { ?s ?p ?o } { ?s a <http://linkedgeodata.org/ontology/Pub> . ?s ?p ?o }'          sparqlify-tool -P lgd-example -Q 'Select * { ?s ?p ?o . Filter(?s = <http://linkedgeodata.org/triplify/node2028098486>) }'    Again, note that Sparqlify is still in development and the supported features are a bit limited right now - still, basic graph patterns and equal-constraints should be working fine.      ### Additional tools    * `lgd-osm-replicate-sequences`: Convert a timestamp to a sequence ID. This is similar to [mazdermind's replicate sequences tool](https://github.com/MaZderMind/replicate-sequences), however, our version does not require a local index. Instead, our tools combines binary search with linear interpolation: First, the the two most recent state.txt files from the given repository url are fetched, then the time differnce is computed, and based on linear interpolation a sequence id close to the given timetstamp is computed. This process is repeated recursively.  ```bash  lgd-osm-replicate-sequences -u ""http://planet.openstreetmap.org/replication/hour/"" -t ""2017-05-28T15:00:00Z""    # The above command from the debian package is a wrapper for:    java -cp linkedgeodata-debian/target/linkedgeodata-debian-*-jar-with-dependencies.jar \      ""org.aksw.linkedgeodata.cli.command.osm.CommandOsmReplicateSequences"" \      -u ""http://planet.openstreetmap.org/replication/hour/"" -t ""2017-05-28T15:00:00Z""  ```  The output is a (presently subset) of the appropriate state.txt file whose timestamp is strictly less than that given as the argument.  ```  sequenceNumber=41263  timestamp=2017-05-28T14\:00\:00Z  ```  Note, that the timestamp format is compatible with `osmconvert`, which can check for the most recent data item in a osm data file. Hence,  these tools can be combined in order to find the state.txt file from which to proceed with replication.  ```bash  timestamp=`osmconvert --out-timestamp ""data.osm.pbf""`  lgd-osm-replicate-sequences -u ""url-to-repo"" -t ""$timestamp""  ```    ```bash  # Use the -d option to option the (d)uration between the most recently published files  lgd-osm-replicate-sequences -u ""http://planet.openstreetmap.org/replication/day/"" -d  # This yields simply the output (possibly off by a few seconds)  # 86400  ```    ### Postgresql Database Tuning  It is recommended to tune the database according to [these recommendations](http://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server). Here is a brief summary:  Edit `/etc/postgresql/9.1/main/postgresql.conf` and set the following properties:            shared_buffers       = 2GB #recommended values between 25% - 40% of available RAM, setting assumes 8GB RAM          effective_cache_size = 4GB #recommended values between 50% - 75% of available RAM, setting assumes 8GB RAM          checkpoint_segments  = 256          checkpoint_completion_target = 0.9          autovacuum = off # This can be re-enabled once loading has completed            work_mem             = 256MB (This memory is used for sorting, so each user may use this amount of memory for his sorts; You may want to use a significantly lower value if there are many connections doing sorts)          maintainance_work_mem = 256MB      Furthermore, allow more shared memory, otherwise postgres won't start:  Append the following line to `/etc/sysctl.conf`:            #Use more shared memory max          kernel.shmmax=4294967296            # Note: The amount (specified in bytes) for kernel.shmmax must be greater than the shared_buffers settings obove          #4GB = 4294967296          #8GB = 8589934592    Make the changes take effect:            sudo sysctl -p          sudo service postgresql restart    ## License  The content of this project are licensed under the [GPL v3 License](https://github.com/GeoKnow/LinkedGeoData/blob/master/LICENSE).   """
Semantic web;https://github.com/blokhin/genealogical-trees;"""Semantic Web Genealogical Trees  ======  [![DOI](https://zenodo.org/badge/18811/blokhin/genealogical-trees.svg)](https://zenodo.org/badge/latestdoi/18811/blokhin/genealogical-trees)    Rationale  ------    This is an example of logical reasoning applied to the graphs of genealogical trees. Defining the kinship types on top of genealogical trees seems to be the perfect use case for semantic technologies.    The source of data is the GEDCOM file format (**.ged**) common for exchange of genealogical information. With the aid of **RDFLib** and **gedcom** Python libraries GEDCOM files are converted into the OWL 2 ontologies (ABox) in Turtle syntax (**.ttl** file extension), adopting the TBox of the Family History Knowledge Base ([FHKB](http://ceur-ws.org/Vol-1207/paper_11.pdf), see ```data/header.ttl``` file). Note that the FHKB ontology is although very small but uses unusually complex role hierarchy and is rather hard for modern reasoners. After reasoning with the naive Python implementation of the OWL 2 RL Profile and inferring all possible triples the ontologies are finally converted to the JSON-formatted graphs for in-browser visualization. This is done inside the bundled ```index.html``` HTML5 web-app by means of **D3.js** JavaScript library.    Using this repository  ------    The above is summarized in the ```gedcom2json.sh``` script, which is used like this:    ```shell  ./gedcom2json.sh path/to/your/gedcom.ged path/to/entailed_graph.json  ```    Resulting file ```entailed_graph.json``` is to be uploaded and visualized in the bundled HTML5 web-app ```index.html``` (no server scripting is used). Its copy is currently hosted at GitHub: [http://blokhin.github.io/genealogical-trees](http://blokhin.github.io/genealogical-trees). Locally it should be served from a web-server (e.g. ```python -m SimpleHTTPServer``` or ```php -S localhost:8000```).    Before processing, the required Python libraries listed in ```requirements.txt``` should be installed (virtualenv is highly recommended).    Blog tutorial  ------    https://blog.tilde.pro/semantic-web-technologies-on-an-example-of-family-trees-7518f3f835a9    Remark on FHKB  ------    Note however the [following comment](http://www.researchgate.net/publication/271131820_Manchester_Family_History_Advanced_OWL_Tutorial) from FHKB authors:    > We probably do not wish to drive a genealogical application using an FHKB in this form. Its purpose is educational. It touches most of OWL 2 and shows a lot of what it can do, but also a considerable amount of what it cannot do.  > As inference is maximised, the FHKB breaks most of the OWL 2 reasoners at the time of writing. However, it serves its role to teach about OWL 2.  > OWL 2 on its own and using it in this style, really does not work for family history.    Remark on reasoning  ------    Reasoning with the naive Python implementation of the OWL 2 RL Profile is very slow and takes hours for relatively big family trees. Therefore use of the fast native reasoner (like Fact++) is very desirable. Wrapped in the [owl-cpp](http://owl-cpp.sourceforge.net) Python bindings, Fact++ performs up to two orders of magnitude faster. """
Semantic web;https://github.com/lambdamusic/Ontospy;"""# Ontospy    Python library and command-line interface for inspecting and visualizing RDF models.      #### Links    -   [Pypi](https://pypi.org/project/ontospy/)  -   [Github](https://github.com/lambdamusic/ontospy)  -   [Docs](http://lambdamusic.github.io/Ontospy/)  -   [YouTube Video](https://youtu.be/MkKrtVHi_Ks)    # Description    Ontospy is a lightweight Python library and command line tool for inspecting and visualizing vocabularies encoded using W3C Semantic Web standards, that is, RDF or any of its dialects (RDFS, OWL, SKOS).    The basic workflow is simple: load a graph by instantiating the `Ontospy` class with a file containing RDFS, OWL or SKOS definitions. You get back a object that lets you interrogate the ontology. That's all!    The same functionalities are accessible also via a command line application (`ontospy`). This is an interactive environment (like a repl) that allows to load ontologies from a local repository, interrogate them and cache them so that they can be quickly reloaded for inspection later on.    [![Downloads](https://pepy.tech/badge/ontospy)](https://pepy.tech/project/ontospy)      ## Generating ontology documentation    Ontospy can be used to generate HTML documentation for an ontology pretty easily. E.g. see the [Schema.org](https://lambdamusic.github.io/ontospy-examples/schema_org_topbraidttl/index.html) ontology, or [FOAF](https://lambdamusic.github.io/ontospy-examples/foafrdf/index.html) ontology.    This functionality relies on a module called _ontodocs_, which used to be maintained as a separate library but is now distributed with ontospy as an add-on:    -   `pip install ontospy[FULL]`    For more examples of the kind of documentation that can be generated out-of-the-box, [take a look at this page](https://lambdamusic.github.io/ontospy-examples/index.html).    ## Status    [![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity)    I have little time to spend on this project these days, so I'm mainly focusing on bug fixes and maintenance. Happy to review PRs if you want to add more functionalities!     ## Development    ```  # git clone the repo first  $ mkvirtualenv ontospy  $ pip install -r requirements.txt  $ pip install -e .  ```    ## Documentation    http://lambdamusic.github.io/Ontospy/ """
Semantic web;https://github.com/owlcs/owlapi;"""# OWLAPI  ======    ## OWL API main repository    The OWL API is a Java API for creating, manipulating and serialising OWL Ontologies.     * The latest version of the API supports OWL 2.    * It is available under Open Source licenses (LGPL and Apache).    The following components are included:    * An API for OWL 2 and an in-memory reference implementation.  * Read and write support for RDF/XML, OWL/XML, Functional syntax, Manchester syntax, Turtle, OBO.  * Write support for KRSS, DL syntax, LaTeX.  * Other formats via RIO integration (NTriples, JSON, etc.).  * Reasoner interfaces for working with reasoners such as FaCT++, HermiT, Pellet, Racer, JFact and Chainsaw.  * See documentation pages on the wiki for more details.    ## Release notes    ## 5.1.18 30 July 2021    ### Features:    *    Specify RioSetting values for Rio renderers #614    ### Bug fixes:    *    Fix sameAs failure when more than 2 entities included #994  *    Fix Trig and rdf/json should include a named graph. #1002  *    Fix ObjectHasSelf rendered wrongly in manchester syntax #1005    ## 5.1.17 6 November 2020    ### Features:    *    Remove @Deprecated annotations for Set based methods in OWLAPI 5 #981  *    Support RDF4J Rio HDT parser #931  *    OWLLiteral for XSD:Long #970    ### Bug fixes:    *    Fix Performance of signature checks during ontology changes #968  *    Fix Error on RIO renderer when expression has 6000 elements #971  *    Fix OWLOntology#datatypesInSignature to include ontology header #965  *    Fix EntitySearcher.getSuperProperties fails when parent is inverse #964  *    Update guava and junit versions  *    Fix OWLParser not ensuring streams are closed on exit #973  *    Error with undeclared classes in domain axioms #962  *    Fix Ontology caches should use weak keys #984    ## 5.1.16 28 July 2020    ### Bug fixes:    *    Fix follow multiple redirects across protocols #954  *    Javadoc fixes for deprecated stream methods #950    ## 5.1.15 02 July 2020    ### Features:    *    Allow creation of tautologies for n-ary axioms #776  *    Configurable fast pruning window size    ### Bug fixes:    *    Fix javadoc for OWLObject::nestedClassExpressions #937  *    Fix classAssertionAxioms with OWLClassExpression fails #930  *    Fix Include ontology annotations in signature #928  *    Fix Unable to set base directive for turtle writers #938  *    Fix OWLAPI accepts IRIs with leading spaces #940  *    Fix SWRL body reordered when structure shared #936  *    Fix roundtrip through OBO changes IRI of owl:versionInfo #947        ## 5.1.14 18 April 2020    ### Features:  *    General modularity classes contributed by Robn Nolte    ### Bug fixes:    *    Fix XSD datatypes are erroneously quoted in OBO writer #918  *    Fix referencingAxioms(OWLPrimitive) misses nested literals #912  *    Fix Empty line in META-INF/services/ files causes exceptions #924  *    Fix OWLOntologyWriterConfiguration does not disable banner comments #904      ## 5.1.13 27 January 2020    ### Bug fixes:    *    Fix OWLEntityRenamer and anonymous individuals #892  *    Fix Builtin annotation properties lost during parsing #895  *    Deal with OWLAnnotationProperty entities in OWLEntityURIConverter class #896      ## 5.1.12 20 October 2019    ### Bug fixes:    *    Implement Allow gzipped imports #887  *    Fix Race condition in Injector #883  *    Jackson update  *    Fix containsReference(OWLEntity) should be deprecated #864  *    Fix referencingAxioms(OWLPrimitive) misses IRI appearances #865  *    Fix Javadoc on applyChange/applyChanges and using the wrong manager #868  *    Fix OWLObjectPropertyExpression#getSimplified() used incorrectly #882  *    Fix Incomplete javadoc on OWLNaryAxiom#asPairwiseAxioms #884  *    Fix OWLNegative*AssertionAxiom#containsAnonymousIndividuals javadoc #885  *    Fix Annotated axiom with anon expression saved incorrectly #881  *    Fix Ontology with relative IRIs is serialized incorrectly #880  *    Fix Ann. annotation with anon individual saved in RDF incorrectly #877  *    Fix Annotations dropped if annotation property is undeclared #875  *    Fix SAXException from AutoIRIMapper at debug logging level #878  *    Ensure isAnonymous is implemented correctly #867  *    Fix Null pointers with imports and relation declarations #859  *    Amend base and escaped characters in Tutle parsing #857  *    Fix Exception when converting obi to obo #860      ## 5.1.11 02 June 2019    ### Features:    *    Add support to load an ontology from classpath #837  *    Implement Allow annotations to be skipped in module extraction #838  *    Add support for custom tags in obo files.    ### Bug fixes:    *    Fix Unescaping characters: OBOFormatParser#handleNextChar() #822  *    Fix IRI PREFIX_CACHE instance uses too much memory #825  *    Fix roundtrip of escaped values #833  *    Fix MaximumNumberOfNamedSuperclasses should count super classes #836  *    Fix OWLDataPropertyAxiom not a subinterface of OWLPropertyAxiom #831  *    Fix HTTP 307 and HTTP 308 redirects are not followed (in 4.x) #821  *    Fix Missing escape character in OBO output #828  *    Fix 5.1.10 Regression in OwlStringTools#translate(..) #829  *    Fixed several incorrect XSD datatype matching patterns.  *    Fix OWLDataFactory::getLiteral error with empty string and integer #846  *    Fix Unnecessary dc prefix added by Manchester syntax parser #845  *    Fix Multiple Ontology Definitions should obey strict parsing #840  *    Fix OWLLogicalEntity is not an OWLAnnotationProperty #847  *    Security: Jackson to 2.9.9  *    Fix Manchester syntax parser crashes on class/property punning #851  *    Fix OBO parser does not support qualifier block #852      ## 5.1.10 04 March 2019    ### Bug fixes:    *    Fix DLExpressivity checker never computes anything #810  *    Jackson version to 2.9.8  *    Fix ensure allValuesFrom axioms are not lost #808  *    Fix HTTP 307 and HTTP 308 redirects are not followed #821  *    Fix OBO renderer stuck with untranslatable axioms on concurrent managers  *    Fix Annotations on DifferentIndividualsAxioms lost #816  *    Fix No roundtrip for IRIs with colons in the fragment or path #817  *    Fix EOFException in CustomTokenizer #813  *    Fix Cyclic imports and missing declarations cause parsing error #798    ## 5.1.9 12 December 2018    ### Bug fixes:    *    Refactor OWLProfile implementations #638  *    Fix Missing user defined datatype violation in literals #639  *    Fix RDFGraph getSubjectsForObjects caught in infinite loop #809    ## 5.1.8 1 December 2018    ### Features:    *    Add OWLClassExpression.isNamed method #790  *    Fix injection problem under OSGi  *    Implement Allow Atomic Decomposition to skip assertions #796  *    Expressivity Checker for EL and FL #500    ### Bug fixes:    *    Fix ReadWriteLock should be injector singleton #785  *    Fix Cyclic import of versioned ontologies fails #788  *    Fix Annotate ontology annotations #791  *    Fix Incorrect documentation for OWLOntologyManager methods. #795  *    DisjointClasses with OWL:Thing produces incorrect axiom #747  *    Fix Concurrent managers with own lock shared with own ontologies #806    ## 5.1.7 2 September 2018    ### Features:    *    PROV and TIME vocabularies    ### Bug fixes:    *    Add representativeInstances() to OWLReasoner #772  *    SWRLRule hash code computed incorrectly  *    Fix OWLOntology with shared structure causes incorrect RDF/XML #780    ## 5.1.6 24 July 2018    ### Features:    *    Remove Guice dependencies  *    Upgrade to jsonld-java version 0.12.0 (performance) #763  *    Allow building with Java 10    ### Bug fixes:    *    Move from Trove4j to HPPC-RT #774  *    Fix incorrect sorting of OBO header tags  *    Fix Line breaks in rdfs:label cause invalid FS output #758  *    Fix AutoIriMapper chooses wrong IRIs. #755    ## 5.1.5 23 April 2018    ### Features:    *    Add an option to represent version build as strings  *    Implement #375 OWLZip reader and writer  *    Allow trimming to size after load to be disabled  *    do not register deprecated oboparser by default #729    ### Bug fixes:    *    Fix SWRL variable IRIs violate URN spec #732  *    default IRI for unnamed ontologies is not valid  *    doubling of # at the end of default namespace  *    IRI should return true for isIRI()  *    Fix OWL/XML writes langString unnecessarily #748  *    Fix importsDeclaration not returning imports for *.obo #727    ## 5.1.4 4 January 2018    ### Features:    *    Support Authorization header in remote loading    ### Bug fixes:    *    Fix Problem saving ontologies in Turtle #719  *    Fix Null pointer in OWLObjectPropertyManager #723  *    Remove com.google.inject and add exclusion of javax.annotation. #720    ## 5.1.3 4 November 2017    Features:    * Performance improvements on parsing of large ontologies.    ## 5.1.2 13 October 2017    ### Features:    *    Accept Headers to include all MIME types with supporting parsers #705  *    Add HasAnnotationValue interface with methods for mapping  *    Optional methods for OWLOntologyChange  *    Implement efficient way to test if an ontology refers entity type #698    ### Bug fixes:    *    Do not output xsd:string when unnecessary #640  *    OWL/XML should reject XML files that are not valid OWL/XML #657  *    getFragment advices an non existent replacement #684  *    OWLObject immutable collections sorted #702  *    Sort imports and imports closure #702  *    Sort namespace prefixes for XML serialization #702  *    OWLOntologyMerger fails with ConcurrentModificationException #673  *    Poor performance in OWLImmutableOntologyImpl.getImportsClosure #696  *    Fix AtomicDecomposition throws Nullpointer #695  *    Fix hashcode and equals on OWLOntology differ #694    ## 5.1.1 25 July 2017    ### Features:    *    Add REPAIR_ILLEGAL_PUNNINGS property to disable fix of illegal punnings  *    Move punning log to warning    ### Bug fixes:    *    Fix Profiles.OWL2_FULL returns Profiles.OWL2_DL #667  *    Fix EntitySearcher.getEquivalentClasses incorrectly returns itself #663  *    OWLEntityRenamer should rename annotation props in ontology annotations  *    Fix blank node ids should be NCNames in the RDF/XML output #689    ## 5.1.0 30 March 2017    ### Features:    *    Add stream support on reasoning interface.  *    Allow SyntacticLocalityModuleExtractor to exclude assertions. #462  *    Explanations: do not fail hard for profile violations  *    Latex formatting improvements  *    ensure ontology annotations are sorted  *    Implement HasApplyChanges.applyChanges returns ChangeApplied #544  *    recompile parsers with javacc 7  *    issue #612 : Add japicmp maven plugin to track API changes  *    Implement Zip with dependencies included for non Maven users #584  *    Dependencies update, move to rdf4j    ### Bug fixes:    *    Fix Relating to inferred axiom generators #646  *    Fix duplication and inverseOf properties in generators #646  *    Fix SimpleRenderer writes InverseOf instead of ObjectInverseOf #647  *    Fix nested annotation bug  *    Fix Turtle parser failure on some bioportal ontology #610  *    Fix Manchester expression parser bug with data cardinalities #609    ## 5.0.5 4 January 2017    ### Bug fixes:    *    Allow supplier for null error messages  *    IsAnonymous, isIndividual, isAxiom, isAnonymousExpression on OWLObject  *    Performance improved in creating OWLOntologyManager  *    Ensure XXE vulnerability is prevented  *    Fix Structural sharing in GCIs causes errors #564  *    Fix Issue with serialization - OWLAPI version 3.5.5 #586  *    Fix Problems compiling version5 for Android v24 Nougat #585  *    Fix Order of RDF triples affects annotation parsing #574  *    Fix Property Axiom Generator should check set size #527  *    Fix Imports not properly loaded after all ontologies removed #580  *    Fix Incomplete Parsing of DifferentIndividuals with distinctMembers #569  *    Fix Apache Harmoony SAX parser not supported #581  *    Fix line separators in `DLSyntaxHTMLStorer` #583  *    Fix IRIs with query string cause OFN unparseable output #570  *    Fix Unqualified data restriction considered qualified #576    ## 5.0.4 16 October 2016    ### Bug fixes:    *    Serializability issues    ## 5.0.3 10 September 2016    ### Bug fixes:    *    Fix OMN Parser mistakes punned class for object property #548  *    Fix OWL/XML format does not support SWRL variable IRIs #535  *    Fix Explicit prefix ignored during serialization #529  *    Fix Saving to OFN loses namespaces due to double default prefix #537  *    Fix DL-Syntax HTML formatter issues #536  *    Fix ManchesterOWLSyntaxRenderer SubPropertyOf render inconsistent #534  *    Fix Slow inferred disjoint generator #542  *    Fix Relative Ontology IRI causes missing declaration #557  *    Fix Invalid Turtle ontologies when hash IRIs are used #543  *    Fix Calling setOntologyDocumentIRI reset imports closure cache #541  *    Fix OWLOntology::annotationPropertiesInSignature has duplicates #555  *    Fix Latex Renderer produces bad LaTeX for property inverses #526  *    Fix ManchesterOWLSyntaxParser doesn't handle the OWL 2 datatypes #556  *    Fix Turtle Renderer doesn't escape periods #525  *    Fix Import IRI not resolved correctly in OBO input #523    ## 5.0.2 7 May 2016    ### Features:    *    Allow banning of parsers #510  *    Allow disabling banner comments  *    Move builders to api to be able to use them in parsers  *    Make AxiomType Comparable    ### Bug fixes:    *    Fix performance regression in String.intern() and improve memory footprint  *    Fix Default prefix overrides explicit prefixes #522  *    Fix Anonymous individuals parsing problems #494  *    Fix Noisy print messages from logging framework #516  *    Fix SWRL rules with incorrect equals() #512  *    Fix OWL 2 Full Profile #508  *    Fix Annotated entities not declared in RDF/XML #511  *    Fix XML literal is not self contained #509    ## 5.0.1 19 March 2016    ### Bug fixes:    *    Fix Fixing punnings fails on dp subDataPropertyOf rdfs:label #505  *    ""Dumping remaining triples"" displayed when no triples left #502  *    Fix Annotations lost on entity renaming of SWRLRules #501  *    Fix Profile validation throws exception on ontology annotations #498  *    Fix RDF/XML parser failing to load ontology containing XMLLiteral #496  *    Fix #489 makeLoadImportRequest log should be a warning #489  *    Fix Anonymous individuals parsing problems #494  *    Fix Saving fails for ontologies containing XMLLiteral #495  *    Fix Anonymous individuals parsing problems #494    ## 5.0.0 28 February 2015    ### Features:    *    Allow overridable and defaultable properties  *    FaCT++ AD implementation from OWLAPITOOLS  *    Add OWLObjectTransformer to replace any part of an ontology  *    Add HasOperands interface  *    Ad componentsWithoutAnnotations() to simplify equalsIgnoreAnnotations  *    Add componentsAnnotationsFirst() to OWLObject  *    Add component() to OWLObject to create a common interface for hashcode  *    Add rdf:langString data type  *    commons-rdf-api integration  *    Add InferenceDepth and convenience methods to OWLReasoner  *    OWLRDFConsumer fills guessed type declaration table with blank nodes  *    Marked methods returning sets where a stream is available as deprecated  *    Update guice version to beta 5  *    Replace Google Optional with Java 8 Optional (#250)  *    OWLProfileViolationVisitorEx returns Optional<T> rather than nulls  *    Add default methods to OWLOntology for direct add/remove of axioms  *    Added a getFormat method to OWLOntology  *    Merged visitor adapters into the interfaces with default methods  *    isAvailable/get pattern in OWLOntologyDocumentSource changed to Optional  *    OWLOntology.getReferencingAxioms() finds any of the OWLPrimitive objects  *    Enabled OWLOntologyManager to build and keep an OntologyConfigurator instance  *    Declaration of save methods on OWLOntology  *    OWLAPI 5 uses Java 8    ## 4.5.20 31 July 2021    ### Features:    *    Specify RioSetting values for Rio renderers #614    ### Bug fixes:    *    Fix sameAs failure when more than 2 entities included #994  *    Ordering of literals to be consistent with OWLAPI 5  *    Fix Trig and rdf/json should include a named graph. #1002  *    Fix ObjectHasSelf rendered wrongly in manchester syntax #1005      ## 4.5.19 7 November 2020    ### Bug fixes:    *    Fix OWLParser not ensuring streams are closed on exit #973  *    Error with undeclared classes in domain axioms #962  *    Fix Ontology caches should use weak keys #984      ## 4.5.18 23 October 2020    ### Bug fixes:    *    Fix Performance of signature checks during ontology changes #968  *    Fix Error on RIO renderer when expression has 6000 elements #971  *    Fix OWLOntology#datatypesInSignature to include ontology header #965  *    Fix Ontology not loaded in case of multiple HTTP redirects #954      ## 4.5.17 02 July 2020    ### Features:    *    Let OBO parser follow redirects  *    Allow creation of tautologies for n-ary axioms #776  *    Configurable fast pruning window size    ### Bug fixes:    *    Fix javadoc for OWLObject::nestedClassExpressions #937  *    Fix classAssertionAxioms with OWLClassExpression fails #930  *    Fix Include ontology annotations in signature #928  *    Fix Unable to set base directive for turtle writers #938  *    Fix OWLAPI accepts IRIs with leading spaces #940  *    Fix SWRL body reordered when structure shared #936  *    Fix roundtrip through OBO changes IRI of owl:versionInfo #947      ## 4.5.16 18 April 2020    ### Bug fixes:    *    Fix XSD datatypes are erroneously quoted in OBO writer #918  *    Fix referencingAxioms(OWLPrimitive) misses nested literals #912  *    Fix Empty line in META-INF/services/ files causes exceptions #924      ## 4.5.15 28 January 2020    ### Bug fixes:    *    Fix OWLEntityRenamer and anonymous individuals #892  *    Fix Builtin annotation properties lost during parsing #895  *    Deal with OWLAnnotationProperty entities in OWLEntityURIConverter class #896      ## 4.5.14 19 October 2019    ### Bug fixes:    *    Implement Allow gzipped imports #887  *    Fix Race condition in Injector #883  *    Jackson update  *    Fix containsReference(OWLEntity) should be deprecated #864  *    Fix Javadoc on applyChange/applyChanges and using the wrong manager #868  *    Fix OWLObjectPropertyExpression#getSimplified() used incorrectly #882  *    Fix Incomplete javadoc on OWLNaryAxiom#asPairwiseAxioms #884  *    Fix OWLNegative*AssertionAxiom#containsAnonymousIndividuals javadoc #885  *    Fix Annotated axiom with anon expression saved incorrectly #881  *    Fix Ontology with relative IRIs is serialized incorrectly #880  *    Fix Ann. annotation with anon individual saved in RDF incorrectly #877  *    Fix Annotations dropped if annotation property is undeclared #875  *    Fix SAXException from AutoIRIMapper at debug logging level #878  *    Ensure isAnonymous is implemented correctly #867  *    Fix Null pointers with imports and relation declarations #859  *    Amend base and escaped characters in Tutle parsing #857    ## 4.5.13 02 June 2019    ### Features:    *    Add support for custom tags in obo files. #848    ### Bug fixes:    *    Fix OWLLogicalEntity is not an OWLAnnotationProperty #847  *    Security: Jackson to 2.9.9  *    Fix Manchester syntax parser crashes on class/property punning #851  *    Fix OBO parser does not support qualifier block #852    ## 4.5.12 06 May 2019    ### Features:    *    Add support to load an ontology from classpath #837  *    Implement Allow annotations to be skipped in module extraction #838    ### Bug fixes:    *    Fix Multiple Ontology Definitions should obey strict parsing #840  *    Fix Unnecessary dc prefix added by Manchester syntax parser #845  *    Fix OWLDataFactory::getLiteral error with empty string and integer #846  *    Fixed several incorrect XSD datatype matching patterns #844    ## 4.5.11 17 April 2019    ### Bug fixes:    *    Fix HTTP 307 and HTTP 308 redirects are not followed (in 4.x) #821  *    Fix Missing escape character in OBO output #828  *    Fix OWLDataPropertyAxiom not a subinterface of OWLPropertyAxiom #831  *    Fix MaximumNumberOfNamedSuperclasses should count super classes #836  *    Fix OWLOntology::getGeneralClassAxioms slow #839  *    Fix roundtrip of escaped values #833    ## 4.5.10 14 March 2019    ### Bug fixes:    *    Fix HTTP 307 and HTTP 308 redirects are not followed (in 4.x) #821  *    Fix Ensure allValuesFrom axioms are not lost #808  *    Fix EOFException in CustomTokenizer #813  *    Fix Cyclic imports and missing declarations cause parsing error #798  *    Fix Unescaping characters: OBOFormatParser#handleNextChar() #822  *    Fix IRI PREFIX_CACHE instance uses too much memory #825      ## 4.5.9 1 February 2019    ### Bug fixes:    *    Jackson version to 2.9.8  *    Fix compatibility with Guava 27 #814  *    Fix OBO renderer stuck with untranslatable axioms on concurrent managers  *    Fix Annotations on DifferentIndividualsAxioms lost #816  *    Fix No roundtrip for IRIs with colons in the fragment or path #817    ## 4.5.8 22 December 2018    ### Features:    *    Refactor OWLProfile implementations #638    ### Bug fixes:    *    Fix Missing user defined datatype violation in literals #639  *    Fix RDFGraph getSubjectsForObjects caught in infinite loop #809  *    Fix DLExpressivity checker never computes anything #810    ## 4.5.7 1 December 2018    ### Features:    *    Add OWLClassExpression.isNamed method #790  *    Fix injection problem under OSGi  *    Expressivity Checker for EL and FL #500    ### Bug fixes:    *    Fix ReadWriteLock should be injector singleton #785  *    Fix Cyclic import of versioned ontologies fails #788  *    Fix Annotate ontology annotations #791  *    Fix Incorrect documentation for OWLOntologyManager methods. #795  *    DisjointClasses with OWL:Thing produces incorrect axiom #747  *    Fix Concurrent managers with own lock shared with own ontologies #806    ## 4.5.6 6 September 2018    ### Bug fixes:    *    OSGi issues fixed    ## 4.5.5 2 September 2018    ### Features:    *    PROV and TIME vocabularies    ### Bug fixes:    *    Prefix splitting at wrong place with percent encoded IRI #737  *    Fix OWLOntology with shared structure causes incorrect RDF/XML #780    ## 4.5.4 26 July 2018    ### Features:    *    Remove Guice dependencies  *    Upgrade to jsonld-java version 0.12.0 (performance) #763  *    Build with Java 10    ### Bug fixes:    *    Move from Trove4j to HPPC-RT #774  *    Literals with no lang and string literals must equal each other  *    Fix OWLLiteral.parseDouble should throw NumberFormatException #764  *    No need for Guice and Guava version updates  *    Amend pom files to build vaild osgi distribution #768  *    Fix incorrect sorting of OBO header tags  *    Fix Line breaks in rdfs:label cause invalid FS output #758  *    Fix AutoIriMapper chooses wrong IRIs. #755    ## 4.5.2 22 April 2018    ### Features:    *    Add an option to represent version build as strings  *    Implement #375 OWLZip reader and writer  *    Signature cache for ontologies  *    Allow trimming to size after load to be disabled  *    do not register deprecated oboparser by default #729    ### Bug fixes:    *    Fix SWRL variable IRIs violate URN spec #732  *    default IRI for unnamed ontologies is not valid  *    doubling of # at the end of default namespace  *    IRI should return true for isIRI()  *    Fix OWL/XML writes langString unnecessarily #748    ## 4.5.1 7 December 2017    ### Features:    *    Support Authorization header in remote loading    ## 4.5.0 13 October 2017    ### Features:    *    Accept Headers to include all MIME types with supporting parsers #705  *    Add HasAnnotationValue interface with methods for mapping  *    Optional methods for OWLOntologyChange  *    Implement efficient way to test if an ontology refers entity type #698    ### Bug fixes:    *    OWL/XML should reject XML files that are not valid OWL/XML #657  *    OWLObject immutable collections sorted #702  *    Sort imports and imports closure #702  *    Sort namespace prefixes for XML serialization #702  *    OWLOntologyMerger fails with ConcurrentModificationException #673  *    getFragment advices an non existent replacement #684  *    Fix Poor performance in OWLImmutableOntologyImpl.getImportsClosure #696    ## 4.3.2 25 July 2017    ### Features:    *    Add REPAIR_ILLEGAL_PUNNINGS property to disable fix of illegal punnings  *    Move punning log to warning    ### Bug fixes:    *    Fix Profiles.OWL2_FULL returns Profiles.OWL2_DL #667  *    Fix EntitySearcher.getEquivalentClasses incorrectly returns itself #663  *    Fix blank node ids should be NCNames in the RDF/XML output #689  *    OWLEntityRenamer should rename annotation props in ontology annotations    ## 4.3.1 27 March 2017    ### Features:    *    Allow SyntacticLocalityModuleExtractor to exclude assertions. #462  *    Explanations: do not fail hard for profile violations    ### Bug fixes:    *    Fix Relating to inferred axiom generators #646  *    Fix duplication and inverseOf properties in generators #646    ## 4.3.0 22 March 2017    ### Features:    *    Implement HasApplyChanges.applyChanges returns ChangeApplied #544  *    Implement Zip with dependencies included for non Maven users #584  *    issue #612 : Add japicmp maven plugin to track API changes  *    dependencies update    ### Bug fixes:    *    Fix OWLLiteral produce different hash codes in version 4 #645  *    Multiple nested annotations fail in RDF/XML #470  *    Parsing of nested anonymous nodes broken in 4.1.1 #478  *    ensure ontology annotations are sorted  *    Fix Turtle parser failure on some bioportal ontology #610  *    Fix Manchester expression parser bug with data cardinalities #609    ## 4.2.8 4 January 2017    ### Bug fixes:    *    Fix Structural sharing in GCIs causes errors #564  *    Fix Issue with serialization - OWLAPI version 3.5.5 #586  *    Fix Order of RDF triples affects annotation parsing #574  *    Fix Property Axiom Generator should check set size #527  *    Fix Imports not properly loaded after all ontologies removed #580  *    Performance improved in creating OWLOntologyManager  *    Fix Incomplete Parsing of DifferentIndividuals with distinctMembers #569  *    Fix Apache Harmoony SAX parser not supported #581  *    Ensure XXE vulnerability does not exist  *    Fix line separators in `DLSyntaxHTMLStorer` #583  *    Fix IRIs with query string cause OFN unparseable output #570  *    Fix Unqualified data restriction considered qualified #576  *    Fix OWLOntologyManager in OWLImmutableOntologyImpl should be @Nullable #568      ## 4.2.7 16 October 2016    ### Bug fixes:    *    Serialization issues      ## 4.2.6 10 September 2016    ### Bug fixes:    *    Fix OMN Parser mistakes punned class for object property #548  *    Fix OWL/XML format does not support SWRL variable IRIs #535  *    Fix Explicit prefix ignored during serialization #529  *    Fix Saving to OFN loses namespaces due to double default prefix #537  *    Fix DL-Syntax HTML formatter issues #536  *    Fix ManchesterOWLSyntaxRenderer SubPropertyOf render inconsistent #534  *    Fix Slow inferred disjoint generator #542  *    Fix Relative Ontology IRI causes missing declaration #557  *    Fix Invalid Turtle ontologies when hash IRIs are used #543  *    Fix Calling setOntologyDocumentIRI reset imports closure cache #541  *    Fix Latex Renderer produces bad LaTeX for property inverses #526  *    Fix ManchesterOWLSyntaxParser doesn't handle the OWL 2 datatypes #556    ## 4.2.5 17 May 2016    ### Bug fixes:    *    Fix Turtle Renderer doesn't escape periods #525  *    Fix Import IRI not resolved correctly in OBO input #523    ## 4.2.4 7 May 2016    ### Bug fixes:    *    Fix Default prefix overrides explicit prefixes #522  *    Fix Anonymous individuals parsing problems #494  *    Fix OWL 2 Full Profile #508  *    Fix Annotated entities not declared in RDF/XML #511  *    Fix XML literal is not self contained #509    ### Features:    *    Allow banning of parsers #510  *    Allow disabling banner comments  *    Fix Noisy print messages from logging framework #516    ## 4.2.3 19 March 2016    ### Bug fixes:    *    Fix Fixing punnings fails on dp subDataPropertyOf rdfs:label #505    ## 4.2.2 15 March 2016    ### Bug fixes:    *    Fix RDF/XML parser failing to load ontology containing XMLLiteral #496  *    Fix Profile validation throws exception on ontology annotations #498  *    Fix Annotations lost on entity renaming of SWRLRules #501  *    Fix ""Dumping remaining triples"" displayed when no triples left #502    ## 4.2.1 5 March 2016    ### Bug fixes:    *    Fix Anonymous individuals parsing problems #494  *    Fix makeLoadImportRequest log should be a warning #489  *    Fix Saving fails for ontologies containing XMLLiteral #495    ## 4.2.0 28 February 2016    ### Features:    *    Allow overridable and defaultable properties    ### Bug fixes:    *    Fix EntitySearcher.getDomains(OWLAnnotationProperty, OWLOntology) returns the range axioms instead of the domain axioms #492    ## 4.1.4 3 February 2016    ### Features:    *    Add OWLObjectTransformer to replace any part of an ontology #487  *    Add OWLLiteralReplacer to replace literals with different values #487  *    Throw error on malformed ontologies when parsing is strict (avoid Error1 classes) #444    ### Bug fixes:    *    Fix RBox axiom types #479  *    Fix Parsing of nested anonymous nodes broken in 4.1.1 #478  *    Fix Error in rdf handling of deep nested annotations  *    Fix EntitySearcher.subPropertiesFilter is broken #486  *    Fix Cyclic imports cause illegal punning #483  *    Fix OWLOntologyManager.copyOntology does not copy imports #480  *    Fix Changes not successfully applied broadcasted as applied #476  *    Add restriction on aduna packages #471  *    Fix BOM removal wrapper is applied to all input streams #212  *    Fix Serialization into incorrect OWL syntax #468  *    Fix Constraint Sesame Versions in OSGi Imports #471  *    Fix ExplanationOrdererImpl.SeedExtractor throws exception #469  *    Fix for Multiple nested annotations fail in RDF/XML #470    ## 4.1.0 25 October 2015    ### Features:    *    Port protege-owlapi concurrency support to the OWL API  *    Enabled the creation of a concurrent manager  *    Part of Add transactional support to OWLOntology #382 : ability to rollback the application of a set of changes if one fails. Added a ChangeApplied.NO_OPERATION to track those changes that did not have any effect (e.g., adding an axiom that is already present or removing an axiom that is not present).  *    ManchesterOWLSyntaxParser could use a convenience method parseClassExpression(String s) #384  *    Add setting to use labels as banner comment or use IRIs as before  *    Add config to switch saving all anon individuals  *    Enforce correct argument types for OWLObjectInverseProperty in default impl  *    Add control flag to structure walking to allow annotations to be walked  *    Reduce space used by OBO Parser  *    update jackson dependency to 2.5.1    ### Bug fixes:    *    OWLImmutableOntologyImpl.asSet() assumes input iterable is coming from a duplicate free collection #404  *    OWLOntologyManagerImpl.getVersions() calls get() on Optional without checking if the contained value is present #401  *    OWLOntologyManagerImpl tests storers with keys rather than canStoreOntology #400  *    setOntologyLoaderConfigurationProvider should accept a javax.inject.Provider #399  *    Cleaned distribution jar  *    Losing Annotations on Annotation Assertion Axioms #406  *    Return types of RemoveAxiom(s) vs. AddAxiom(s) #408  *    Examples file needs review #407  *    BFO fails to load in API version 4.0.X #405  *    Fix RDF/XML renderer does not like properties using urns (or almost urns) #301  *    All anonymous individuals are remapped consistently upon loading. #443  *    Only output node id for anonymous individuals if needed  *    Anon individuals appearing more than once as objects #443  *    Sort annotation assertions on output for manchester syntax #332  *    OWL 2 profile validation mistakes #435  *    [Typedef] created_by don't support space #419  *    ttl namespace abbreviations not preserved on serialization #421  *    Use EntitySearcher.getAnnotationObjects() when interested only in the object of annotation assertion axioms on an entity rather than the annotations on the annotation assertion axioms.  *    convincing owlapi-distribution to export rio dependencies  *    RDF issues testing 4.1.0 RC 1 #412  *    Annotation subproperty/domain/range disappear after save/load cycle (punning enhancement) #351      ## 4.0.2 17 April 2015    ### Features:    *    Simple OWL/FSS IRI sniffing for AutoIRIMapper.  *    Fix #373 Individuals in SWRL rules get extra type  *    XZ compression support  *    Sorting of FunctionalSyntaxObjectRenderer  *    Sort untyped IRI Annotation Assertions, general axioms.  *    Sort axiom Annotations, if they are output using the translateAnnotations method. Negative property assertions not yet ordered; neither are nary axioms and class expressions.  *    Add legacy files  to the OSGI distribution for  version 3.5 backwards compatibility.  *    Some reasoners may not support certain types of reasoning. They may also not support OWLReasoner::isEntailed, and hence may return false for all calls to  OWLReasoner::isEntailmentCheckingSupported.  *    Support preserving, transferring, and adding annotations during OBO Macro expansion  *    Add Extensions to link document formats and common file extensions  *    Optimise annotation lookup for module extraction enrichment  *    Change strategy for MapPointer storage: do all initial insertions in a list, and turn it into a THashSet upon first call to trimToSize or any get/size calls  *    Add benchmarking util to load ontology then dump to an hprof file for use with MAT  *    Improve conversion from OWL to OBO, add test case    ### Bug fixes:    *    Fix PriorityCollectionSorting should be fixed at manager creation time #395  *    Fix OWLOntologyManagerImpl.getVersions() calls get() on Optional without checking if the contained value is present #401  *    Fix XMLLiterals should be output as XML, not quoted XML #333  *    Fix Should RDF/XML Renderer include annotation assertions from the entire closure? #371  *    Fix Using a PriorityCollection for IRI mappers (and perhaps other things) is confusing #386  *    Fix THashSets got left using default load factor (0.50) after construction is finished. #380  *    Fix RDF Consumer failing to guess property types #322  *    Fix NonMappingOntologyIRIMapper confuses extra OWLOntologyIRIMapper implementations #383  *    Fix Data property with xsd:integer range parsed as Object property (RDF/XML source) #378  *    Fix imports in OWL Manchester Notation fail #347  *    Fix ELProfile incorrectly rejects DisjointClasses axioms. #343  *    Fix OWLRDFConsumer fills guessed type declaration table with blank nodes #336  *    Fix #338 OWL-API: DOCTYPE declaration missing in XML serialization  *    Fix #337 Axiom Equality short-circuits are in the wrong place  *    Fixed bug when rendering anonymous individual in Manchester OWL syntax.  *    Reverted incorrect setting of all packaging types to bundle    ## 4.0.1 22 November 2014    ### Features:    *    trimToSize available to cut ontology internals collections to minimal size after loading (Simon Spero)  *    various memory optimizations and caching of rarely used values removed (Simon Spero)  *    use of Trove collections for ontology internals  *    OBOFormat: Configuration option to skip validation before writing OBO file #290    ### Bug fixes:    *    fix a multithread bug revealed by owlapitools tests  *    fix #299 roundtrip errors on TriX and various other syntaxes  *    fix #292 SWRL rules saved by older versions of OWLAPI/Protege lose annotations  *    fix #288 Errors with object property assertions with inverseOf properties  *    fix #289 Manchester syntax errors: inverse() not recognized, SWRL rules rendered twice  *    fix #287 Error parsing RDF/Turtle with triple quoted literals - mismatching single and double quotes  *    fix #281 OntologyIRIShortFormProvider does wrong shortform generation      ## 4.0.0 10 September 2014    Supported Java versions: Java 7, Java 8    ### Features:    *    added HasAnnotationPropertiesInSignature to uniform treatment of annotation properties  *    added missing EntitySearcher methods for negative property assertions  *    Use Trove collections for ontology internals  *    improved performance of OWLAxiomImpl.equals  *    Transform functions from Collection<OWLOntoloy> and Collection<OWLOntologyID> to Collection<IRI>  *    PMD critical violations addressed  *    fix ServiceLoader use to be OSGi compatible  *    create osgidistribution: osgidistribution is a jar with embedded dependencies and including the compatibility module. It addresses the issues due to the OWLAPI dependencies not being wrapped in OSGi bundles separately.  *    enabled OWLOntologyManager to build and keep a loader configuration  *    OBO 1.2 ontologies cannot be parsed by the 1.4 parser. Added 1.2 parser from OWLAPITOOLS to compatibility package.  *    added saveOntology() methods to OWLMutableOntology  *    Add a copy/move ontology method to OWLOntologyManager #12  *    Search introduced to replace forSuperPosition, forSubPosition, ignoreAnnotations booleans  *    Imports.INCLUDED, Imports.EXCLUDED introduced instead of boolean arguments #156  *    introduced ChangeApplied to simplify code around applying changes and remove null warnings  *    Introduction of Searcher with transform functions  *    OWLOntology now exposes a new method for searching for axioms referring an entity, abstracting all various getAxiomsBy... methods.  *    Refactor OWLSignature for uniform imports closure use  *    Refactored OWLAxiomCollection for uniform use of import closure  *    Added targeted parsing to allow one and only one parser to be tried on a document input source. Takes into account MIME type as well.  *    Use MultiMap from Guava throughout V4.0.0 #153  *    OBO parser updated and replaced with oboformat 5.5 release  *    @Nonnull annotations and warnings  *    Centralised and uniformed sax parser factories  *    OWLOntologyID to use Optional #160  *    Refactored property hierarchy to remove generics.  *    Added an OWLParser implementation to enable actual use of DL parser  *    Added a pairwise visitor, changed storage strategy for nary axioms. The pairwise visitor interface is a functional interface that is applied to all distinct pairs in a collection of objects.  *    Added asPairwiseAxioms to all nary axiom types  *    Added HasPriority annotation and comparator for prioritizable objects  *    Added PriorityCollection for managing injected collections  *    Added ServiceLoader adapter for Guice injection  *    Added ManchesterOWLSyntaxParser interface to api  *    OWLAPITOOLS profiles and fixers included  *    Added Profiles enumeration and supported OWL profile for known reasoners.  *    Dependency injection enabled for fixers.  *    Added Guice module for creating OWLOntologyManager and OWLDataFactory  *    Introduced OWLOntologyBuilder to allow swapping OWLOntology implementations via Guice  *    Service loader module in api to be used independently of the owlapi-impl module  *    Add RDFa parser and use static META-INF/services files for owlapi-rio  *    Add JSON-LD support  *    Add rdf:langString from RDF 1.1  to OWL/RDF vocabulary.  *    Better BOM treatment through Apache BOMInputStream  *    Use SLFJ for logging  *    Guava 17  *    Guice 4.0-beta  *    Added jsr305 for Nonnull and Nullable annotations  *    Fix build and javadoc for Java 8   *    Do not raise violations from entities used only in declarations.  *    Do not add declarations for illegal punnings when saving ontologies #112  *    Do not cache any literals in OWLDataFactoryInternalsImpl. Overall improvement of about 9% when parsing Gene Ontology in FSS.  *    Implement Default prefix manager in prefix ontology format objects cannot be overriden #9  *    Implement automatic retries when loading from URLs, managed through OWLOntologyLoaderConfiguration #66  *    Switched OWLDataFactory IllegalArgumentExceptions to NullPointerExceptions #131  *    Handle annotations on punned types when rendering RDF #183  *    Add OWLOntologyManagerFactory and code to allow use of RioOWLRDFParser through an injector  *    Convert RioParserImpl to stream statements through wherever possible  *    Default datatype in RDFLiteral to PLAIN_LITERAL.  *    Made generated classes package protected  *    Rename OWLObjectRenderer to FunctionalSyntaxObjectRenderer  *    OWLReasoner change lists generic  *    Moved OWLOntologyFormat implementation to formats package  *    Performance improvement for rdf/xml rendering  *    Optimised functional renderer and prefix manager  *    feature: OBO parser replaced with oboformat.  *    OWL2Datatype implements HasIRI  *    Add XSD IRIs simple parsing support #56  *    GZip read/write ability  *    Manchester OWL syntax cleanup    ### Bug fixes:    *    Fix #278 AutoIRIMapper is not namespace aware  *    Direct imports result not updated correctly after manual load #277  *    fix #275 DL Syntax rendering of disjoint classes missing comma  *    fix #271 OWLOntology.getXXXInSignature(boolean) and similar methods  *    fix #270 Add OWLOntology.getReferencingAxioms(OWLPrimitive)  *    fix #268 Add documentation to OWLOntologyID to clarify the relationship between isAnonymous() and getOntologyIRI()  *    fix #267 Consider adding containsXInSignature methods that do not have an imports flag to OWLOntology.  *    fix #254 OWLAsymmetricObjectPropertyAxiom not rendered in DL Syntax  *    fix OWLDocumentFormat as interface #258 #259  *    fix #260 and fix #261 data and object cardinality are quantified restricitons  *    fix #255 PrefixOWLOntologyFormat is missing from the compatibility module  *    fix #253 StructuralReasoner.getSameIndividuals does not behave as advertised  *    Fixed #198 A SubClassOf B will not parse  *    Fixed IRI with a space: %20 escape #146  *    Old OWLEntityCollector brought forward as DeprecatedOWLEntityCollector  *    Fixed serialization warnings reported in #163  *    Use AtomicLong for concurrency support in gzip document sources  *    Moved transitive object property type to rbox  *    Set turtle default prefix slash aware #121   *    Fixed #116 import statement erroneously names anonymous ontology  *    Added test for roundtripping annotated SWRL rules and datatype definitions  *    Fixed annotations on datatype definition axioms fixed in OWL/XML  *    Fixed #20 and #60 use zipped buffer for StreamDocumentBase  *    Fixed #71 Misleading message on resolving illegal IRI  *    Fixed variable rendering in Manchester OWL Syntax to preserve ns.  *    Fixed #46 Round tripping error in functional syntax  *    Fixed #40 RDFGraph.getSortedTriplesForSubject throws exception in Java7  *    Fixed infinite recursion on cyclic import structures when declaration does not match location.    ## 3.4.10 18 January 2014    ### Features:    *    Improved feature 32 implementation (simple SWRL variables rendering)  *    javadoc warnings and errors removed to allow building with Java 8  *    refactored OWLAxiomVisitorExAdapter for general use  *    performance improvement for rdf/xml  *    Renamed variables, added explicit flush. Fixes #67 slow functional sytnax renderer on Java 6  *    default, final and abstract classes changed to allow interface verification  *    add oraclejdk8 early access build to travis  *    update oboformat class names to sync with oboformat project  *    make OBOFormatException extend OWLParserException    ### Bug fixes:    *    Fixes #72 Manchester syntax roundtrip of doubles and SWRL rules fails  *    Fixes #24 Manchester Syntax writer generates unparsable file  *    simple renderer writes duplicate annotations  *    Optimised functional renderer and prefix manager  *    Fix #63 null pointer exception loading an ontology and fix #64 empty ontology returned  *    Fixes #63 OBO Ontology Parsing throws NullPointer  *    updated OSGi settings for oboformat, module name, dependencies  *    fix bundle configuration for oboformat    ## 3.4.9 25 November 2013    ### Features:    *    Added default methods and return values for a few visitor adapters  *    #57 Reimplement NodeID so that it doesn't use an inner class  *    #56 parse things like xsd:string easily  *    OBO parser replaced with oboformat. Original code at: https://code.google.com/p/oboformat/    ### Bug fixes:    *    Fixed wrong naming of gzip file source and target and added stream only versions.  *    Restore UnparseableOntologyException interface  *    Added back deprecated parseConstant for compatibility with Pellet CLI code. Warning: it has bugs.      ## 3.4.8 02 November 2013    ### Features:    *    GZip read/write ability. Compressed gzip input and output streams, but make sure to close the stream in the calling code.  *    OSGI dependency removed  *    Added Namespaces.inNamespace  *    Made ParserException to extend OWLParserException, and OWLParserException a runtime exception.  *    Tidying up Manchester OWL Syntax parser code  *    Added to the documentation on getOWLDatatype(OWLDataFactory)  *    Added a convenience method to convert an OWL2Datatype object to an OWLDatatype.  *    mvn install and mvn deploy now make aggregate sources available.  *    Added more well known namespaces, removed non-explanatory comments  *    Added RDFa Core Initial Context prefixes and other well known prefixes and prefix names.  *    GITHUB-10 : Add RDFa Core Initial Context prefixes  *    Extend Namespaces enum with common prefixes    ### Bug fixes:    *    Fixes #40 Fixes #7 RDFGraph.getSortedTriplesForSubject throws exception in Java7 Refactored TripleComparator into RDFTriple and RDFNode as Comparable  *    DLQueryExample did not print the StringBuilder with the answers  *    Fixes #50 Null anonymous individual parsing ObjectHasValue from OWL/XML  *    Fixes SF 313 Man Syntax Parser gives facets unexpected datatype  *    Fixes SF 101 Manchester OWL Syntax Parser doesn't do precedence correctly  *    Fixes #43 Functional Syntax Parser does not support comments  *    Fixes #41 Parsing error of SWRL atom.  *    Fixes #46 Round tripping error in functional syntax  *    Fixes #37 setAddMissingTypes logging if disabled  *    Missed serialization transients and tests, serialization bug fixes  *    Patch IRI.create(String,String) to match IRI.create(String)    ## 3.4.5 25 July 2013    ### Features:    *    Refactored DefaultPrefixManager to expose addPrefixes  *    Updated pom files for OSGI compatible artifacts.  *    Allow override of DefaultPrefixManager in PrefixOWLOntologyFormat  *    Add CollectionFactory.getExpectedThreads  *    Reduce expected threads from 16 to 8 to improve general performance    ### Bug fixes:    *    literal parsing error was stopping HermiT builds with 3.4.4.  *    errors in OSGI support were stopping Protege from using the feature.  *    Fix #370 Typo in website/htdocs/reasoners.html.  *    Fixed a bug with large integer values being parsed as negative ints.  *    Fix infinite recursion on cyclic import structures.  *    Reverted the use of ThreadLocal as this causes memory leaks in webapps.  *    Bug #343 SWRL roundtrip in manchester syntax    ## 3.4.4 19 May 2013    Features and updates implemented:    *    feature 103 - Add methods to get EntityType names.  *    Updated IRI prefix caching to use thread local caches rather than synchronized caches.  *    added test for annotation retrieval for anonymous individuals.  *    WeakIndexCache vulnerability fixed.  *    Improved redirect following.  *    Added method to OWLOntologyLoaderConfiguration to disable cross protocol redirects on URLConnections.  *    Added parameter to OWLOntologyFormat for temp files, default disabled.  *    Update all JavaCharStream versions to fix the UTF BOM issue.  *    Travis Integration (on github).  *    Update KE interface with getBlocker method.  *    OSGI compatible build.  *    Removed IRI.toString() call when checking blank nodes.  *    Change logging of remaining triples to use less memory.  *    updates for javadoc.  *    Update Mockito to 1.9.5.    ### Bug fixes:    *    Bug #369 IRI.create(String) uses / and # to look for namespace and fragments.   *    Bug #348 ManchesterOWLSyntax round trip problem with disjoint classes.  *    Bug #269 single anonymous individual split in two on save to RDF.  *    Bug #319 redirects from http to https are not followed.  *    Bug #367 Serving ontologies from github as well.  *    Bug 4 (github) Missing method in OWLIndividual.  *    Bug 359 Incorrect Javadoc in OWL2DatatypeImpl and redundant javadoc.  *    Bug 360 - Axiom annotations are not parsed correctly.  *    Bug 364 Null pointer checking for containment of anonymous ontologies.  *    Bug #363 Turtle parser fails when loading valid Turtle document.  *    Bug 362 - owlapi files in temp folder.  *    Bug 357 duplicate axioms saving in OWL functional syntax.  *    Bug 355 - added assertions. Ability to parse 1+e7 as 10000000.  *    Bug 355 - Turtle Parser + Integer format Problem.  *    OntologyAlreadyExist bug when calls are made too fast.      ## 3.4.3 26 Jan 2013    Features implemented:    *    Updated OWLFunctionalSyntaxFactory with more methods.  *    Removed deprecated method calls.  *    Test package broken up in smaller packages.  *    Web site updated and added to version management.    ### Bug fixes:    *    Changed VersionInfo message default to provide more information.  *    Bug 354 fixed: There is AnnotationChange, but no AnnotationChangeData  *    Bug 353 fixed: SimpleRenderer renders SubDataPropertyOfAxiom wrong.  *    Bug 351 fixed: QNameShortFormProvider swapped prefix and iri.  *    Bug 352 fixed: SimpleRenderer does not escape double quotes in literals.      ## 3.4.2 4 December 2012    Features implemented:    *    Repository migrated to Git.  *    Fixed memory leak with OWLObject signatures never being released once cached.  *    Improved Serializable implementation.  *    Made timeout property functional.  *    3585007 	Make OWLAxiomChange.isAdd().  *    3579488 	Clean documentation page.  *    3578004 	Javadoc OWLRDFVocabulary.  *    3578003 	Add OWLOntologyManager.removeOntology(OntologyID).  *    3578002 	Add new constructor for SetOntologyID.  *    3576182 	Bump minimum java version to 1.6.  *    3575834 	Improvements to OWLOntologyManager and related contrib.  *    3566810 	Lack of support in OWLOntologyManager for version IRIs.  *    3521809 	KRSS parser throws Error instead of RuntimeException.    ### Bug fixes:    *    Some OWLLiterals cannot be optimised: -0.0^^xsd:float, 01^^xsd:int (optimisation with primitive types makes reasoners queasy in combination with W3C tests).  *    3590243 	hashCode for literals inconsistent.  *    3590084     misuse of XMLUtils.getNCNameSuffix().  *    3581575 	OWLOntologyDocumentAlreadyExistsException in OWL API 3.4.1.  *    3580114 	IRI isAbsolute method.  *    3579862 	OWLDataFactory.getRDFPlainLiteral creates new objects.  *    3579861 	rdf:PlainLiteral is not in the allowed OWL2EL datatypes.    ## 3.4.1 16 September 2012    ### Features:    *    3578004    OWLRDFVocabulary Javadoc  *    3575834    Improvements to OWLOntologyManager and related contrib    ### Bug fixes:    *    3463200    rdf/functional round trip problem (anonymous individuals)  *    3341637    round trip problem with owl functional or rdf/xml syntax  *    3576182    Bump minimum java version to 1.6 (Fixed the bugs, moved the request to feature requests)  *    3497161    Manchester syntax imports  *    3403855    Imports of multiple ont docs containing the same ont fails  *    3491516    unhelpful Manchester parse exception  *    3536150    TypeOntologyHandler/TPVersionIRIHandler overwrite ontologyID  *    3314432    Manchester OWL parser does not handle data ranges in rules  *    3186250    annotation assertion lost when annotation prop undeclared  *    3309666    [Turtle] Parsing turtle files containing relative IRIs  *    3562296    Switch version 3.4.1 to 3.4.1-SNAPSHOT  *    3560287    Is QNameShortFormProvider deprecated?  *    3566820    Missing Imports are not reported  *    3306980    file in Manchester OWL Syntax with BOM doesn't parse  *    3559116    Wrong VersionInfo in OWL-API 3.4 release  *    2887890    Parsing hasValue restrictions with punned properties fails  *    3178902    RDF/XML round trip problem with 3-way equivalent classes  *    3174734    ManSyntax fails to read ontology with single data property  *    3440117    Turtle parser doesn't handle qnames with empty name part    ## 3.4 12 August 2012    This version restructures the Maven modules into six modules: api, impl, parsers,   tools, apibinding and contract. Code is unchanged, only its organization in   the SVN is changed.    To split the dependencies accordingly, DefaultExplanationOrderer is in   contract and is only a shell for ExplanationOrdererImpl, which does not   depend on OWLManager directly. This enables the tools module to be   used with a different apibinfing without recompilation.    ### Bug fixes:    *    3554073 	Manchester Syntax Parser won't parse DisjointUnionOf  *    3552028 	DataFactory returns integer instead of double restrictions  *    3550607 	property cycle detection does not work  *    3545194 	OWLEquivalentObjectProperties as SubObjectPropertyOfAxioms  *    3541476 	OWLRDFConsumer.getErrorEntity counter is not thread safe  *    3541475 	Remove shared static in OWLOntologyXMLNamespaceManager  *    3535046 	Use ArrayList instead of TreeSet in TurtleRenderer  *    3532600 	Use AtomicInteger in OWLOntologyID for counter    ### 3.3 15 May 2012    This version wraps together some minor bug fixes and some performance improvements for loading time and memory footprint for large ontologies.  Maven support has been much improved, thanks to Thomas Scharrenbach's efforts and Peter Ansell's feedback and suggestions.    ### Features:    *    Performance improvements at loading time    ### Bug fixes:    *    OBO Parser updated to be more compliant with the latest draft of the OBO syntax and semantics spec.  *    OBO Parser doesn't expand XREF values  (3515525)  *    Integrating with OpenRDF  (3512217)  *    maven should use snapshot versions during development  (3511755)  *    junit should be in maven test scope  (3511754)  *    mvn clean install requires gpg key  (3511732)  *    OWLOntologyManager.contains(IRI) bug (3497086)  *    Functional and Manchester syntax writers prefix problem  (3479677)  *    SyntacticLocalityModuleExtractor and SubAnnotationPropertyOf  (3477470)  *    Ignored Imports List uses the wrong IRIs  (3472712)  *    OWLLiteralImpl corrupts non-ascii unicode (3452932)  *    OWLOntologyXMLNamespaceManager QName problem  (3449316)  *    MultiImportsTestCase assumes incorrect working directory  (3448125)  *    Mismatch between HasKeyTestCase and corresponding resource  (3447280)  *    functional Renderer: OWLObjectRenderer swaps SWRL variables (3442060)  *    maven build fails because of dependency issue  (3440757)  *    manchester owl syntax doesn't handle rules with anon classes  (3421317)  *    RDFTurtleFormatter doesn't escape '\'  (3415108)  *    RDF consumer does not check for all annotated axiom triples  (3405822)  *    mis-parsing/mis-serialization of haskey (3403359)  *    Literal not a builtin  (3305113)  *    OWL API throws NullPointerException loading ontology  (3302982)    ## 3.2.1 22 July 2011    This version of the API is released under both LGPL v3 and Apache license v2; developers are therefore free to choose which one to use.    ### Features:    *    Tutorial code has been added following the OWLED 2011 tutorial (subpackage tutorialowled2011);  *    A visitor to determine whether a set of axioms is a Horn-SHIQ ontology has been added (HornAxiomVisitorEx.java).    ### Bug fixes:    *    Some test files and extra modules had not had their copyright notices updated to the new license; this has now been fixed.    ## 3.2.3 27 May 2011    This version of the API is released under both LGPL v3 and Apache license v2; developers are therefore free to choose which one to use.    ### Bug fixes:    *    MAVEN support has been fixed (3296393).  *    Some minor problems with rdfs:Literal have been fixed (3305113).  *    Parsers and renderers for various languages have had some corner case errors fixed (3302982, 3300090, 3207844, 3235181, 3277496, 3294069, 3293620, 3235198).  *    An error in OWLOntologyManager.removeAxioms() which was throwing ConcurrentModificationExceptions when called on a specific axiom type has been fixed (3290632).  *    OWLOntologyLoaderConfiguration has been introduced to provide ontology wise parsing configuration (strict and lax parsing modes now enabled) (3203646) More options coming up.  *    Parsers no longer leave streams open when a parsing error is detected (3189947).      ## 3.2.2 17 February 2011    ### Bug fixes:    *    In RDF based serialisations, type triples for entities that don't have ""defining axioms"" don't get added even if the renderer is instructed to type undeclared entities (related to 3184131). Fixed.  *    The RDF parser would not recognise XSD datatypes that aren't OWL 2 Datatypes even in lax parsing mode (related to 3184131). Fixed.  *    OWLOntologyManager.createOntology() methods don't set the document IRI of the created ontologies as adverstised if there aren't any ontology IRI mappers installed (3184878). Fixed.      ## 3.2.1 4 February 2011    ### Bug fixes:    *    Issues with -INF as serialisation for -infinity for floating point literals. Fixed.  *    Null pointer exception can be thrown by OBO parser (3162800). Fixed.  *    OWLOntologyManager.loadOntology() can throw unchecked exceptions (IllegalArgumentException) (3158293). Fixed.  *    OWLOntologyAlreadyExistsException get wrapped as as a parse exception by the OBO parser. (3165446). Fixed.  *    OWLRDFConsumer (OWL RDFParser) used owl:Datatype instead of rdfs:Datatype. Fixed.  *    OWLOntology.getDisjointUnionAxioms() does not work as expected. (3165000). Fixed.  *    Anonymous ontologies do not get typed as owl:Ontology during saving in RDF. (3158177). Fixed.  *    OWLOntology hashCode is not implemented properly. (3165583). Fixed.    ## 3.2.0 14 January 2011    This version of the API includes various bug fixes and performance enhancements since version 3.1.0    ### Bug fixes:    *    Various round tripping problems for various syntaxes (3155509, 3154524, 3149789, 3141366, 3140693, 3137303, 3121903)  *    Plain Literals are rendered incorrectly. Fixed.  *    Cyclic imports cause an OntologyAlreadyExistsException to be thrown. Fixed.  *    OWLAxiom.getAnnotatedAxiom() does not merge annotations.  Fixed.  *    OWLObject.getSignature() returns an unmodifiable collection. Fixed.  *    Various problems with character encodings (3068076, 3077637, 3096546, 3140693). Fixed.  *    IRI.create resulted in a memory leak.  Fixed.  *    Imports by location does not work. Fixed.  *    Various problems with anonymous individuals (2998616, 2943908, 3073742). Fixed.  *    Dublin Core Vocabulary is built into OWL API parsers.  Fixed.  *    Files are left open on parse errors.  Fixed.    ## 3.1.0 20 August 2010    This version of the API includes various bug fixes and enhancements since version 3.0.0.    ### Features:    Changes to the representation of literals:    Please note that there is a slight incompatibility between version 3.1.0 and version 3.0.0.  This is due to some changes  in the way that literals are represented and handled in the API.  There was a disparity between how they are described  in the OWL 2 specification and how they are represented in version 3.0.0 of the API.  The changes bring 3.1.0 inline  with the OWL 2 specification.  The changes are as follows: OWLStringLiteral and OWLTypedLiteral have been removed and  replaced OWLLiteral.  In version 3.0.0 OWLLiteral was a super-interface of OWLStringLiteral and OWLTypedLiteral.  Clients therefore need to replace occurrences of OWLStringLiteral and OWLTypedLiteral with OWLLiteral.  Method calls  need not be changed - methods on OWLStringLiteral and OWLTypedLiteral have corresponding methods on OWLLiteral.  Note  that all literals are now typed.  What used to be regarded as OWLStringLiterals are now OWLLiterals with the datatype  rdf:PlainLiteral.  Although this change introduces a slight backward incompatibility with the previous version of the  API we believe that the handling of literals in 3.1.0 is much cleaner and follows the specification more closely.    Changes to the OWLReasoner interface:    The OWLReasoner interface has been updated.  There are now methods which allow for fine-grained control over reasoning  tasks.  For example, it is now possible to request that a reasoner just classifies the class hierarchy, when in   version 3.0.0 the prepare reasoner method caused realisation to occur as well.  The prepareReasoner method has been removed from  the interface due to the huge amount of confusion it caused (it was not necessary to call this method to get correct  results, but the name suggested that it was necessary).  Clients should update their code to replace any calls to  prepareReasoner with appropriate calls to precomputeInferences.    JavaDoc for various methods has been cleaned up.    ### Bug fixes:    *    Annotations on Declaration axioms were not save.  Fixed.  *    OWLObjectMaxCardinality restrictions incorrectly resulted in a profile violation of the OWL2RL profile. Fixed.  *    OWLOntology.containsAxiom() failed various round trip tests. Fixed.  *    OWLObjectPropertyExpression.getInverses() did not include properties that were asserted to be inverses of themselves. Fixed.  *    Writing large rdf:Lists can cause a stack overflow on saving. Fixed.  *    System.out is closed when using SystemOutDocumentTarget. Fixed.  *    An OWLOntologyAlreadyExists exception is thrown when an imports graph contains multiple imports of the same ontology. Fixed.  *    OWLReasoner.getSubclasses() is missing declarations in the throws list for runtime exceptions that get thrown. Fixed.  *    Parsing RDF graphs containing blank nodes with NodeIDs is broken.  Fixed.  *    Some methods on OWLOntology return sets of object that change with ontology changes. Fixed.  *    Double dashes (--) are not escaped in XML comments output by the XMLWriter. Fixed.  *    The API sometimes prints to System.err.  Fixed.  *    The functional syntax writer writes out annotation assertions twice. Fixed.  *    Ontologies with DataSomeValuesFrom restrictions are incorrectly considered non-OWL2QL. Fixed.  *    Ontologies with Functional data properties are incorrectly considered non-OWL2RL. Fixed.  *    OWL2Datatype.XSD_NAME has the wrong regex pattern. Fixed.  *    RDF rendering of GCIs with multiple elements is broken. Fixed.  *    SubObjectPropertyOf axioms with annotations and property chains don't get parsed correctly from RDF.  Fixed.  *    StructuralReasoner sometimes crashes when determining superclasses. Fixed.  *    SimpleRenderer renders declarations incorrectly.  Fixed.  *    Object property characteristics are not answered correctly.  Fixed.  *    OWLOntology.getReferencingAxioms gives mutable and incorrect results.  Fixed.  *    StructuralReasoner call OWLClassExpression.asOWLClass() on anonymous classes. Fixed.  *    SKOSVocabulary is not up to date.  Fixed.  *    Regular expression for xsd:double has extra white space. Fixed.  *    Structural reasoner sometimes misses subclasses of owl:Thing. Fixed.  *    OWLOntologyManager.getImportsClosure() fails on reload. Fixed.  *    PrefixOWLOntologyFormat has not methods to remove prefixes.  Fixed.  *    RDF/XML rendering of unicode characters is ugly. Fixed.  *    Turtle parser does parse shared blank nodes correctly. Fixed.    ## 3.0.0 28 January 2010    Version 3.0.0 is incompatible with previous releases.  Many interface names have been changed in order to acheive a  close alignment with the names used in the OWL 2 Structural Specification and Functional Style Syntax.  (See http://www.w3.org/TR/2009/REC-owl2-syntax-20091027/)    The opportunity to clean up method names was also taken.      ## 2.2.0  17 April 2008    ### Features:    *    Added support for building using ANT  *    OWL 1.1 namespaces changed to OWL 2.  Old ontologies that are written using the owl11 namespace will still load, but will be converted to use the owl2 namespace.  *    Updated the RDF parser and RDF rendere to support AllDisjointClasses and AllDisjointProperties  *    Added the ability to save ontologies in Turtle.  *    Added the ability to load ontologies that are written in Turtle  *    Added explanation code contributed by Clark & Parsia  *    Added a KRSS renderer (contributed by Olaf Noppens)  *    Added a new, more comprehensive KRSS parser (contributed by Olaf Noppens).  This parser can parser the version of the KRSS syntax that is used by Racer.  *    Added the ability to specify a connection timeout for URL connections via a system property (owlapi.connectionTimeOut) - the default value for the timeout is 20 seconds.  *    Added a method to OWLOntologyManager to clear all registered URI mappers  *    Added a method to OWLOntologyManager so that imports can be obtained by an imports declaration.  *    Added a convenience method to OWLOntologyManager to add a set of axioms to an ontology without having to create the AddAxiom changes  *    Added a makeLoadImportsRequest method on OWLOntologyManager which should be used by parsers and other loaders in order to load imports  *    Added the ability to set an option for silent missing imports handling on OWLOntologyManager.  When this option is set,  exceptions are not thrown when imports cannot be found or cannot be loaded.  It is possible to set a listeners that gets informed when an import cannot be found, so that the exception doesn't get lost entirely.  *    Added the ability to add a ontology loader listener to OWLOntologyManager.  The listener gets informed when the loading process for an ontology starts and finishes (which ontology is being loaded, from where and whether it was successfully loaded etc.).  *    Added a method to OWLReasonerFactory to obtain the human readable name of the reasoner that a factory creates.  *    Added a convenience method to OWLOntology to obtain all referenced entities  *    Added convenience methods to OWLEntity that check whether the entity is an OWLClass, OWLObjectProperty, OWLDataProperty, OWLIndividual or OWLDatatype.  Also added asXXX to obtain an entity in its more specific form.  *    Added convenience methods to OWLDataFactory for creating disjoint class axioms and equivalent classes axioms.  *    Added a general purpose renderer interface for OWLObjects  *    Added an OWLInconsistentOntologyException to the inference module.  *    Added SKOS core to the list of well known namespaces  *    Added a SKOS vocabulary enum  *    Added methods to the OWLOntologyManager interface, so that ontologies can be saved to an output target as well as a URI.  Added implementations of OWLOntologyOutputTarget to enable writing directly to OutputStreams and Writers.  *    Added a StringOutputTarget for writing ontologies into a buffer that can be obtained as a string.  *    Added some new input sources:  StreamInputSource, ReaderInputSource, FileInputSource  *    RDF Parser. Made the classExpression translator selector more intelligent so that when properties aren't typed as either object or data properties, other triples are examined to make the appropriate choice.  *    OWLRestrictedDataRangeFacetVocabulary.  Added methods to obtain facets by their symbolic name (e.g. >=)  *    BidirectionalShortFormProvider.  Added a method to obtain all short forms cached by the provider.  *    Added an option to turn tabbing on/off when rendering Manchester Syntax  *    Added more documentation for the method which adds ontology URI mappers  *    Improved error handling when loading ontologies: For errors that have nothing to do with parse errors e.g. unknown host exceptions, the factory will rethrow the error at the earliest opportunity rather than trying all parsers.  *    Updated parser to throw ManchesterOWLSyntaxOntologyParserException which is a more specific type of OWLParserException  *    Updated the BidirectionalShortFormProviderAdapter with functionality to track ontology changes and update the rendering cache depending on whether entities are referenced or not.  *    Added a latex renderer for rendering ontology axioms in a latex format  *    Added the ability to parse ontologies written in ManchesterOWLSyntax  *    Added URIShortFormProvider as a general purpose interface for providing short forms for URIs. Changed SimpleShortFormProvider to use the SimpleURIShortFormProvider as a base  *    Made the toString rendering of the default implementation pluggable via the ToStringRenderer singleton class.  *    Added some convenience methods to the OWLDataFactory to make creating certain types of objects less tedious.  Specifically: ObjectIntersectionOf, ObjectUnionOf, ObjectOneOf and DataOneOf can now be created using methods that take a variable number of arguments (OWLDescriptions, OWLIndividuals or OWLConstants as appropriate).  Also, added convenience methods that create typed literals directly from Java Strings, ints, doubles, floats and booleans.  For example, createOWLTypedConstant(3) will create a typed literal with a lexical value of ""3"" and a datatype of xsd:integer.  Added convenice methods for creating entity annotations without manually having to create OWLAnnotation objects.  *    Added a getAxiomType method on to the OWLAxiom interface for convenience.  *    Added functionality to the debugging module for ordering explanations  *    Added generics to the inferred axiom generator API  *    Added a new constructor to OWLOntologyNamespaceManager so that it is possible to override the ontology format that is used as a hint when generating namespaces.  *    Added a dlsyntax renderer module that can renderer axioms etc. in the traditional dlsyntax using unicode for the dlsyntax symbols.  *    Modified the RDFXMLNamespaceManager to select the minimal amount of entities for which namespaces need to be generated.  Namespaces are only generated for classes in OWLClassAssertionAxioms, and properties in OWLObjectPropertyAssertionAxioms and OWLDataPropertyAssertionAxioms.  This basically corresponds to the places where valid QNames are needed for entities.  *    Added code to add declarations for ""dangling entities"".  If an RDF graph contains  <ClsA> <rdfs:type> <owl:Class> and ClsA has not been referenced by any other axioms then this would have been dropped by the parser - this has been changed so that declaration axioms are added to the ontology in such cases.  (Hopefully, the OWL 1.1 spec will be updated to do something like this in the mapping to RDF graphs).  *    Added a utility class, AxiomSubjectProvider, which given an axiom returns an object which is regarded to be the ""subject"" of the axioms.  For example given SubClassOf(ClsA ClsB), ClsA is regarded as being the subject.  *    Modified the ontology URI short form provider to provide nicer looking short forms.  *    Added a convenience method to get the individuals that have been asserted to be an instance of an OWLClass.  *    Commons lang is no longer used in the API because it had been replaced with a lightweight utility class in order to escape strings.  *    Removed the fragments module and replaced it with the profiles module.  The EL++ profile is currently implemented.  *    Added support for extended visitors that can return objects in the visit method.  *    Turned off logging in the RDF parser classes by default.    ### Bug fixes:    *    The getOntologyURIs method on AutoURIMapper would return physical rather than logic URIs. Fixed.  *    Namespaces for annotation URIs weren't generated. Fixed.  *    Removing a subclass axiom from an ontology cause the axiom to be added to the ontology as a GCI. Fixed.  *    When parsing an ontology, the accept types has been set to include RDF/XML.  This means that ontologies can be parsed correctly from servers that are configured to return RDF or HTML depending on the request type.  *    OWL/XML writer has been modified to write the datatype URI attribute name correctly.  Previously the name was written as ""Datatype"", however it should be ""datatypeURI"".  *    OWL/XML parser. Modified the literal handler to parse literals using the correct datatype URI attribute name (was ""Datatype"" and should have been ""datatypeURI"").  *    The constructor that required a manager in BidirectionalShortFormProviderAdapter did not rebuild the cache. Fixed.  *    Unqualified cardinality restrictions were rendered out as qualified cardinality restrictions. Fixed.  *    Saving an ontology would fail if the necessary directories did not exist. Fixed.  *    Rendering anonymous property inverses in OWL/XML was incorrect. Fixed.  *    Label and Comment annotations in the functional syntax weren't parsed properly, they were parsed as regular annotations. Fixed.  *    In the OWLXMLParserHandler, no handler for negative data property assertions was registered. Fixed.  *    Annotations that have anonymous individuals as values weren't rendered correctly. Fixed.  *    RDFXMLOntologyStorer and RDFXMLRenderer always used the ontology format that is obtainable from the manager, regardless of whether or not a custom ontology format was specified - fixed.  *    Rules that contained individual or data value objects couldn't be rendered. Fixed.  *    Declaration axioms were automatically added for data properties whether an ontology contained declaredAs triples or not. Fixed.  *    Anonymous properties weren't rendered correcty. Fixed.  *    RDF rendering for sub property axioms whose sub property is a property chain used an old rendering.  The rendering now complies with the latest OWL 2 specification.  Ontologies that use the old rendering can still be parsed.  *    RDF lists were reordered on rendering. Fixed."""
Semantic web;https://github.com/protegeproject/swrlapi;"""SWRLAPI  =======    [![Build Status](https://travis-ci.org/protegeproject/swrlapi.svg?branch=master)](https://travis-ci.org/protegeproject/swrlapi)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi/badge.svg)](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi)    The SWRLAPI is a Java API for working with the [OWL](http://en.wikipedia.org/wiki/Web_Ontology_Language)-based [SWRL](http://www.w3.org/Submission/SWRL/) rule and [SQWRL](https://github.com/protegeproject/swrlapi/wiki/SQWRL) query languages.   It includes graphical tools for editing and executing rules and queries.    See the [SWRLAPI Wiki](https://github.com/protegeproject/swrlapi/wiki) for documentation.    A standalone [SWRLTab](https://github.com/protegeproject/swrltab) application and a [Protégé-based](http://protege.stanford.edu/)   [SWRLTab Plugin](https://github.com/protegeproject/swrltab-plugin), both built using this API, are also available.     #### Getting Started    The following examples can be used to quickly get started with the API. A sample SWRLAPI-based project can also be found [here](https://github.com/protegeproject/swrlapi-example).    The library's dependency information can be found here:    [![Maven Central](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi/badge.svg)](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi)    If you'd like to be able to execute SWRL rules or SQWRL queries you will need a SWRLAPI-based rule engine implementation. Currently, a [Drools-based SWRL rule engine implementation](https://github.com/protegeproject/swrlapi-drools-engine) is provided. This implementation is also hosted on Maven Central. Its dependency information can be found here:    [![Maven Central](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi-drools-engine/badge.svg)](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi-drools-engine)    The SWRLAPI uses the [OWLAPI](https://github.com/owlcs/owlapi) to manage OWL ontologies.  The following example illustrates how the library can be used to create a SWRL query engine using an ontology   created by the OWLAPI and then execute rules in that ontology.    ```java   // Create OWLOntology instance using the OWLAPI   OWLOntologyManager ontologyManager = OWLManager.createOWLOntologyManager();   OWLOntology ontology = ontologyManager.loadOntologyFromOntologyDocument(new File(""/ont/Ont1.owl""));     // Create a SWRL rule engine using the SWRLAPI   SWRLRuleEngine swrlRuleEngine = SWRLAPIFactory.createSWRLRuleEngine(ontology);     // Run the SWRL rules in the ontology   swrlRuleEngine.infer();  ```    This example shows how the API can be used to create a SQWRL query engine, create and execute a SQWRL query using  this engine, and then process the results.    ```java   // Create OWLOntology instance using the OWLAPI   OWLOntologyManager ontologyManager = OWLManager.createOWLOntologyManager();   OWLOntology ontology = ontologyManager.loadOntologyFromOntologyDocument(new File(""/ont/Ont1.owl""));     // Create SQWRL query engine using the SWRLAPI   SQWRLQueryEngine queryEngine = SWRLAPIFactory.createSQWRLQueryEngine(ontology);     // Create and execute a SQWRL query using the SWRLAPI   SQWRLResult result = queryEngine.runSQWRLQuery(""q1"",""swrlb:add(?x, 2, 2) -> sqwrl:select(?x)"");     // Process the SQWRL result   if (result.next())      System.out.println(""Name: "" + result.getLiteral(""x"").getInteger());  ```    Extensive documentation on the SWRLAPI can be found on the [SWRLAPI Wiki](https://github.com/protegeproject/swrlapi/wiki).    #### Building from Source    To build this library you must have the following items installed:    + [Java 8](http://www.oracle.com/technetwork/java/javase/downloads/index.html)  + A tool for checking out a [Git](http://git-scm.com/) repository  + Apache's [Maven](http://maven.apache.org/index.html)    Get a copy of the latest code:        git clone https://github.com/protegeproject/swrlapi.git     Change into the swrlapi directory:        cd swrlapi    Then build it with Maven:        mvn clean install    On build completion your local Maven repository will contain the generated swrlapi-${version}.jar file.    This JAR is used by the [Protégé](http://protege.stanford.edu/) [SWRLTab Plugin](https://github.com/protegeproject/swrltab-plugin)  and by the standalone [SWRLTab](https://github.com/protegeproject/swrltab) tool.    A [Build Project](https://github.com/protegeproject/swrlapi-project) is provided to build core SWRLAPI-related components.  A project containing a [library of integration tests](https://github.com/protegeproject/swrlapi-integration-tests) is also provided.    #### License    This software is licensed under the [BSD 2-clause License](https://github.com/protegeproject/swrlapi/blob/master/license.txt).    #### Questions    If you have questions about this library, please go to the main  Protégé website and subscribe to the [Protégé Developer Support  mailing list](http://protege.stanford.edu/support.php#mailingListSupport).  After subscribing, send messages to protege-dev at lists.stanford.edu. """
Semantic web;https://github.com/lanthaler/Hydra;"""Hydra: Hypermedia-Driven Web APIs  =====================================================================    Hydra is an effort to simplify the development of interoperable,  hypermedia-driven Web APIs. The two fundamental building blocks of  Hydra are [JSON-LD][1] and the [Hydra Core Vocabulary][2].    JSON-LD is the serialization format used in the communication between  the server and its clients. The Hydra Core Vocabulary represents the  shared vocabulary between them. By specifying a number of concepts  which are commonly used in Web APIs it can be used as the foundation  to build Web services that share REST's benefits in terms of loose  coupling, maintainability, evolvability, and scalability. Furthermore  it enables the creation of generic API clients instead of requiring  specialized clients for every single API.    To participate in the development of this specification please join  the [Hydra W3C Community Group][3].    More information about Hydra is currently available at:  http://www.markus-lanthaler.com/hydra      [1]: http://www.w3.org/TR/json-ld/  [2]: http://www.markus-lanthaler.com/hydra/spec/latest/core/  [3]: http://m.lanthi.com/HydraCG """
Semantic web;https://github.com/albertmeronyo/LSD-Dimensions;"""LSD Dimensions  ==============    All dimensions and Data Structure Definitions (DSDs) of Linked  Statistical Data, codes and concept schemes associated to them, and  endpoints using them.    ## What is this?    LSD Dimensions is an aggregator of all `qb:DataStructureDefinition`  and `qb:DimensionProperty` (and their associated triples) that can be  currently found in the Linked Data Cloud (read: the SPARQL endpoints  in Datahub.io). Its purpose is to improve the reusability of  statistical dimensions, codes and concept schemes in the Web of Data,  providing an interface for users (future work: to programs) to search  for resources commonly used to describe open statistical datasets. It  also looks for ways of using semantic descriptions of these resources  to enhance comparability of statistical datasets in Linked Data.    ## How does it work?    1. Querying the Datahub.io API for a list of endpoints  2. Querying these endpoints for DSDs, dimensions, codes and concept schemes  3. Storing all in a messy MongoDB instance  4. Displaying it using Boostrap Table    ## Online instance    See http://lsd-dimensions.org    ## Dependencies    - Python 2.7.5  - SPARQL Wrapper  - pymongo  - Bottle  - Bootstrap  - Bootstrap Table """
Semantic web;https://github.com/jpcik/morph-web;"""morph-web  =========    morph-web is a web demonstrator for morph-streams: [https://github.com/jpcik/morph-streams], an RDF stream query processor.    ##Install it yourself    You will need:  * java7  * sbt: www.scala-sbt.org/  * play framework: www.playframework.com/‎    Then you can:    * download the code  * start the application: `sbt run``  * go to a browser to `localhost:9000``  * that's it    ##Use cases    Follow the [tutorial](https://github.com/jpcik/morph-web/wiki/Tutorial:-Morph-streams) to learn about:  * Registering a query  * Pulling data from a registered query  * Pushing data with WebSockets  * Creating R2RML mappings """
Semantic web;https://github.com/apseyed/SemantGeo;"""SemantGeo  ========= """
Semantic web;https://github.com/goerlitz/rdffederator;"""# About #    SPLENDID provides a federation infrastructure for distributed, linked RDF data sources. SPARQL queries are executed transparently across a set of pre-configured SPARQL endpoints. Automatic source selection and query optimization is based on statistical information provided by VoiD descriptions.    This is an ongoing research project at the Institute for Web Science and Technologies, University of Koblenz, Germany. Currently, the software is offered as a stable alpha version. New features and updates may be added over time.    # Quick Start #      * check out the source code from svn (eclipse project)  > > `svn checkout http://rdffederator.googlecode.com/svn/trunk/ splendid`    * compile sources    * run `SPLENDID.sh` or `SPLENDID.bat` with a repository configuration and a SPARQL query file as parameters, e.g.  > > `./SPLENDID.sh SPLENDID-config.n3 eval/queries/cd/`    # Customizing SPLENDID federation #      * choose a set of data sources    * generate voiD statistics for all data sources      * download data source dump      * 1st option: run voiD generator shell script on N-Triples file  > > > `$>scripts/generate_void_description.sh dump.nt void.n3`      * 2nd option: run Java voiD generator on subject-sorted RDF file  > > > `$>scripts/run_voidgen dump_sorted_by_subject`    * add `fed:member` definition for each data source to the federation configuration (see `SPLENDID-config.n3`)      * `rep:repositoryType` is always ""west:VoidRepository""      * `fed:voidDescription` requires an URI pointing to the voiD file      * `void:sparqlEndpoint` is the URI of the source's SPARQL endpoint (overrides the endpoint definition in the voiD file)    * run SPLENDID with your custom configuration    # Technical Overview #    [Presentation at Consuming Linked Open Data Workshop (ISWC 2011):](http://www.slideshare.net/OlafGoerlitz/splendid-9858478)   """
Semantic web;https://github.com/Robsteranium/csvwr;"""# CSV on the Web R Package (csvwr) <img src=""man/figures/logo.png"" align=""right"" height=""139"" />    [![build](https://github.com/Robsteranium/csvwr/actions/workflows/r.yml/badge.svg)](https://github.com/Robsteranium/csvwr/actions/workflows/r.yml)  [![pkgdown](https://github.com/Robsteranium/csvwr/actions/workflows/pkgdown.yml/badge.svg)](https://github.com/Robsteranium/csvwr/actions/workflows/pkgdown.yml)    Read and write csv tables annotated with metadata according to the ""CSV on the Web"" standard (CSVW).    The [csvw model for tabular data](https://w3c.github.io/csvw/syntax/) describes how to annotate a group of csv tables to ensure they are interpreted correctly.    This package uses the [csvw metadata schema](https://w3c.github.io/csvw/metadata/) to find tables, identify column names and cast values to the correct types.    The aim is to reduce the amount of manual work needed to parse and prepare data before it can be used in analysis.      ## Usage    ### Reading CSVW    You can use `csvwr` to read a csv table with json annotations into a data frame:    ```r  library(csvwr)    # Parse a csv table using json metadata :  csvw <- read_csvw(""data.csv"", ""metadata.json"")    # To extract the parsed table (with syntactic variable names and typed-columns):  csvw$tables[[1]]$dataframe  ```    Alternatively, you can jump straight to the parsed table in one call:    ```  read_csvw_dataframe(""data.csv"", ""metadata.json"")  ```    ### Writing CSVW    You can also prepare annotations for a data frame:    ```r  # Given a data frame (saved as a csv)  d <- data.frame(x=c(""a"",""b"",""c""), y=1:3)  write.csv(d, ""table.csv"", row.names=FALSE)    # Derive a schema  s <- derive_table_schema(d)    # Create metadata (as a list)  m <- create_metadata(tables=list(list(url=""table.csv"", tableSchema=s)))    # Serialise the metadata to JSON  j <- jsonlite::toJSON(m)    # Write the json to a file  cat(j, file=""metadata.json"")  ```    For a complete introduction to the library please see the `vignette(""read-write-csvw"")`.      ## Installation    You'll need to use devtools to install this package from github:    ```r  install.packages(""devtools"")  devtools::install_github(""Robsteranium/csvwr"")  ```    ## Contributing    ### Roadmap    Broadly speaking, the objectives are as follows:    - parse csvw, creating dataframes with specified names and types (mostly implemented)  - connecting associated csv tables and json files according to the conventions set out in the csvw standard (partly implemented)  - support for validating a table according to a metadata document (a little implemented)  - support for multiple tables (mostly implemented)  - tools for writing csvw metadata, given an R data frame (partly implemented)  - vignettes and documentation (mostly implemented)  - scripts for running the most useful tools from the command line (not yet implemented)    It's not an urgent objective for the library to perform csv2rdf or csv2json translation although some support for csv2json is provided as this is used to test that the parsing is done correctly.    In terms of the csvw test cases provided by the standard, the following areas need to be addressed (in rough priority order):    - datatypes (most of simple datatypes and some complex ones are supported, but there are more types and constraints too)  - validations (there are a lot of these 😊)  - propagation of inherited properties  - http retrieval (`readr::read_csv` (and indeed `utils::read.csv`) accepts URIs, but the spec also involves link, dialect, and content-type headers)  - referential integrity (a foundation for this is in place)  - json nesting    ### Testing    The project currently incorporates two main parts of the [csvw test](https://w3c.github.io/csvw/tests/) suite:    - [Parsing with JSON output](https://github.com/Robsteranium/csvwr/blob/master/tests/testthat/test-csvw-parsing-json.R)  - [Validation](https://github.com/Robsteranium/csvwr/blob/master/tests/testthat/test-csvw-validation.R)    In each case, we're running only that subset of test entries that can be expected to pass given that part of the standard that has thus far been implemented. Some entries will be skipped (either permanently or) while other priorities are implemented.    You can find out what needs to be implemented next by widening the subset to include the next entry.    During development, you may find it convenient to recreate one of the test entries for exploration. There is a convenience function in [tests/csvw-tests-helpers.R](https://github.com/Robsteranium/csvwr/blob/master/tests/csvw-tests-helpers.R). This isn't exported by the package so you'll need to evaluate it explicitly. You can then use it as follows:    ```r  run_entry_in_dev(16) # index number in the list of entries  run_entry_in_dev(id=""manifest-json#test023"") # identifier for the test  ```    There are also some more [in-depth unit tests](https://github.com/Robsteranium/csvwr/blob/master/tests/testthat/test-parsing.R) written for this library.    ### Workflow    You can use `devtools::load_all()` (`CTRL + SHIFT + L` in RStudio) to load updates and `testthat::test_local()` (`CTRL + SHIFT + T`) to run the tests.    In order to check the vignettes, you need to do `devtools::install(build_vignettes=T)`. Then you can open e.g. `vignette(""read-write-csvw"")`.    ## License    GPL-3    To discuss other licensing terms, please [get in contact](mailto:csvw@infonomics.ltd.uk).    ## Other CSVW tools    There's another R implementation of csvw in the package [rcsvw](https://github.com/davideceolin/rcsvw).    If you're interested in csvw more generally, then the [RDF::Tabular](https://github.com/ruby-rdf/rdf-tabular/) ruby gem provides one of the more robust and comprehensive implementations, supporting both translation and validation.    If you're specifically interested in validation, take a look at the [ODI](https://theodi.org/)'s [csvlint](https://github.com/Data-Liberation-Front/csvlint.rb) which implements csvw and also the [OKFN](https://okfn.org/)'s [frictionless data table schemas](https://specs.frictionlessdata.io/).    If you want rdf translation, then you might like to check out [Swirrl](https://www.swirrl.com/)'s [csv2rdf](https://github.com/Swirrl/csv2rdf/) and also [table2qb](https://github.com/swirrl/table2qb) which generates csvw annotations from csv files to describe [RDF Data Cubes](https://www.w3.org/TR/vocab-data-cube/). """
Semantic web;https://github.com/uzh/signal-collect;"""Signal/Collect  ==============    Signal/Collect is a framework for computations on large graphs. The model allows to concisely express many iterated and data-flow algorithms, while the framework parallelizes and distributes the computation.    How to develop in Eclipse  -------------------------  Install the [Typesafe IDE for Scala 2.11](http://scala-ide.org/download/sdk.html).    Ensure that Eclipse uses a Java 8 library and JVM: Preferences → Java → Installed JREs → JRE/JDK 8 should be installed and selected.    Import the project into Eclipse: File → Import... → Maven → Existing Maven Projects → select ""signal-collect"" folder    Thanks a lot to  ---------------  * [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Hasler Foundation](http://www.haslerstiftung.ch/en/home) have generously funded the research on graph processing and the development of Signal/Collect.  * GitHub helps us by hosting our [code repositories](https://github.com/uzh/signal-collect).  * Travis.CI offers us very convenient [continuous integration](https://travis-ci.org/uzh/signal-collect).  * Codacy gives us automated [code reviews](https://www.codacy.com/public/uzh/signalcollect). """
Semantic web;https://github.com/AKSW/Sparqlify-Extensions;"""Sparqlify-Extensions  ====================    Extension projects for Sparqlify"""
Semantic web;https://github.com/AtomGraph/Processor;"""AtomGraph Processor is a server of declarative, read-write Linked Data applications. If you have a triplestore with RDF data that you want to serve Linked Data from, or write RDF over a RESTful HTTP interface, AtomGraph Processor is the only component you need.    What AtomGraph Processor provides for users as out-of-the-box generic features:  * API logic in a single [Linked Data Templates](https://atomgraph.github.io/Linked-Data-Templates/) ontology  * control of RDF input quality with SPARQL-based constraints  * SPARQL endpoint and Graph Store Protocol endpoint  * HTTP content negotiation and caching support    AtomGraph's direct use of semantic technologies results in extemely extensible and flexible design and leads the way towards declarative Web development. You can forget all about broken hyperlinks and concentrate on building great apps on quality data. For more details, see [articles and presentations](https://github.com/AtomGraph/Processor/wiki/Articles-and-presentations) about AtomGraph.    For a compatible frontend framework for end-user applications, see [AtomGraph Web-Client](https://github.com/AtomGraph/Web-Client).    # Getting started    * [how AtomGraph Processor works](https://github.com/AtomGraph/Processor/wiki/How-Processor-works)  * [Linked Data Templates](https://github.com/AtomGraph/Processor/wiki/Linked-Data-Templates)  * [installing AtomGraph Processor](https://github.com/AtomGraph/Processor/wiki/Installation)    For full documentation, see the [wiki index](https://github.com/AtomGraph/Processor/wiki).    # Usage    ## Docker    Processor is available from Docker Hub as [`atomgraph/processor`](https://hub.docker.com/r/atomgraph/processor/) image.  It accepts the following environment variables (that become webapp context parameters):    <dl>      <dt><code>ENDPOINT</code></dt>      <dd><a href=""https://www.w3.org/TR/sparql11-protocol/"">SPARQL 1.1 Protocol</a> endpoint</dd>      <dd>URI</dd>      <dt><code>GRAPH_STORE</code></dt>      <dd><a href=""https://www.w3.org/TR/sparql11-http-rdf-update/"">SPARQL 1.1 Graph Store Protocol</a> endpoint</dd>      <dd>URI</dd>      <dt><code>ONTOLOGY</code></dt>      <dd><a href=""https://atomgraph.github.io/Linked-Data-Templates/"">Linked Data Templates</a> ontology</dd>      <dd>URI</dd>      <dt><code>AUTH_USER</code></dt>      <dd>SPARQL service HTTP Basic auth username</dd>      <dd>string, optional</dd>      <dt><code>AUTH_PWD</code></dt>      <dd>SPARQL service HTTP Basic auth password</dd>      <dd>string, optional</dd>      <dt><code>PREEMPTIVE_AUTH</code></dt>      <dd>use premptive HTTP Basic auth?</dd>      <dd><code>true</code>/<code>false</code>, optional</dd>  </dl>    If you want to have your ontologies read from a local file rather than their URIs, you can define a custom [location mapping](https://jena.apache.org/documentation/notes/file-manager.html#the-locationmapper-configuration-file) that will be appended to the system location mapping.  The mapping has to be a file in N3 format and mounted to the `/usr/local/tomcat/webapps/ROOT/WEB-INF/classes/custom-mapping.n3` path. Validate the file syntax beforehand to avoid errors.    To enable logging, mount `log4j.properties` file to `/usr/local/tomcat/webapps/ROOT/WEB-INF/classes/log4j.properties`.    ### Examples    The examples show Processor running with combinations of  * default and custom LDT ontologies  * local and remote SPARQL services  * Docker commands    However different combinations are supported as well.    #### Default ontology and a local SPARQL service    The [Fuseki example](https://github.com/AtomGraph/Processor/tree/master/examples/fuseki) shows how to run a local [Fuseki](https://jena.apache.org/documentation/fuseki2/) SPARQL service together with Processor and how to setup [nginx](https://www.nginx.com) as a reverse proxy in front of Processor. Fuseki loads RDF dataset from a file. Processor uses a built-in LDT ontology.  It uses the [`docker-compose`](https://docs.docker.com/compose/) command.    Run the Processor container together with [Fuseki](https://hub.docker.com/r/atomgraph/fuseki) and [nginx](https://hub.docker.com/_/nginx) container:        cd examples/fuseki            docker-compose up    After that, open one of the following URLs in the browser and you will retrieve RDF descriptions:  * [`http://localhost:8080/`](http://localhost:8080/) - root resource  * [`http://localhost/`](http://localhost/) - root resource where the hostname of the Processor's base URI is rewritten to `example.org`    Alternatively you can run `curl http://localhost:8080/` etc. from shell.    In this setup Processor is also available on `http://localhost/` which is the nginx host.  The internal hostname rewriting is done by nginx and useful in situations when the Processor hostname is different from the application's dataset base URI and SPARQL queries do not match any triples.  The [dataset](https://github.com/AtomGraph/Processor/blob/master/examples/fuseki/dataset.ttl) for this example contains a second `http://example.org/` base URI, which works with the rewritten `example.org` hostname.    #### Custom ontology and a remote SPARQL service    The [Wikidata example](https://github.com/AtomGraph/Processor/tree/master/examples/wikidata) example shows to run Processor with a custom LDT ontology and a remote SPARQL service.  It uses the [`docker run`](https://docs.docker.com/engine/reference/run/) command.    Run the Processor container with the Wikidata example:        cd examples/wikidata            docker-compose up    After that, open one of the following URLs in the browser and you will retrieve RDF descriptions:  * [`http://localhost:8080/`](http://localhost:8080/) - root resource  * [`http://localhost:8080/birthdays`](http://localhost:8080/birthdays) - 100 people born today  * [`http://localhost:8080/birthdays?sex=http%3A%2F%2Fwww.wikidata.org%2Fentity%2FQ6581072`](http://localhost:8080/birthdays?sex=http%3A%2F%2Fwww.wikidata.org%2Fentity%2FQ6581072) - 100 females born today  * [`http://localhost:8080/birthdays?sex=http%3A%2F%2Fwww.wikidata.org%2Fentity%2FQ6581097`](http://localhost:8080/birthdays?sex=http%3A%2F%2Fwww.wikidata.org%2Fentity%2FQ6581097) - 100 males born today    Alternatively you can run `curl http://localhost:8080/` etc. from shell.    _Note that Wikidata's SPARQL endpoint [`https://query.wikidata.org/bigdata/namespace/wdq/sparql`](https://query.wikidata.org/bigdata/namespace/wdq/sparql) is very popular and therefore often overloaded. An error response received by the SPARQL client from Wikidata will result in `500 Internal Server Error` response by the Processor._    ## Maven    Processor is released on Maven central as [`com.atomgraph:processor`](https://search.maven.org/artifact/com.atomgraph/processor/).    # Datasource    AtomGraph Processor does *not* include an RDF datasource. It queries RDF data on the fly from a SPARQL endpoint using [SPARQL 1.1 Protocol](https://www.w3.org/TR/sparql11-protocol/) over HTTP. SPARQL endpoints are provided by most RDF [triplestores](http://en.wikipedia.org/wiki/Triplestore).    The easiest way to set up a SPARQL endpoint on an RDF dataset is Apache Jena [Fuseki](https://jena.apache.org/documentation/fuseki2/) as a Docker container using our [fuseki](https://hub.docker.com/r/atomgraph/fuseki) image. There is also a number of of [public SPARQL endpoints](http://sparqles.ai.wu.ac.at).    For a commercial triplestore with SPARQL 1.1 support see [Dydra](https://dydra.com).    # Test suite    Processor includes a basic HTTP [test suite](https://github.com/AtomGraph/Processor/tree/master/http-tests) for Linked Data Templates, SPARQL Protocol and the Graph Store Protocol.    ![master](https://github.com/AtomGraph/Processor/workflows/HTTP-tests/badge.svg?branch=master)  ![develop](https://github.com/AtomGraph/Processor/workflows/HTTP-tests/badge.svg?branch=develop)    # Support    Please [report issues](https://github.com/AtomGraph/Processor/issues) if you've encountered a bug or have a feature request.    Commercial consulting, development, and support are available from [AtomGraph](https://atomgraph.com).    # Community    Please join the W3C [Declarative Linked Data Apps Community Group](http://www.w3.org/community/declarative-apps/) to discuss  and develop AtomGraph and declarative Linked Data architecture in general. """
Semantic web;https://github.com/D2KLab/sparql-transformer;"""SPARQL Transformer  ===================    Write your SPARQL query directly in the JSON-LD you would like to have in output.    JavaScript package. Try it with the [Playground](https://d2klab.github.io/sparql-transformer/).    > Looking for the [Python one](https://github.com/D2KLab/py-sparql-transformer)?    ## News    - The parameter `$libraryMode` allows to perform the pagination on the merged objects, obtaining exactly `n=$limit` objects  - It is now possible to set a different **merging anchor** instead of `id`/`@id` using the `$anchor` modifier.    **Table of Contents**    - [Motivation](./motivation.md)  - [Query in JSON](#query-in-json)  - [How to use](#how-to-use)  - [Credits](#credits)    You want to learn more? Watch this [Tutorial](https://d2klab.github.io/swapi2020/slides.html)    ## Query in JSON    The core idea of this module is writing in a single file the query and the expected output in JSON.    Two syntaxes are supported: plain JSON and JSON-LD.  Here the examples in the 2 formats for the query of cities.    - plain JSON    ```json  {    ""proto"": [{      ""id"" : ""?id"",      ""name"": ""$rdfs:label$required"",      ""image"": ""$foaf:depiction$required""    }],    ""$where"": [      ""?id a dbo:City"",      ""?id dbo:country dbr:Italy""    ],    ""$limit"": 100  }  ```    - JSON-LD    ```json  {    ""@context"": ""http://schema.org/"",    ""@graph"": [{      ""@type"": ""City"",      ""@id"" : ""?id"",      ""name"": ""$rdfs:label$required"",      ""image"": ""$foaf:depiction$required""    }],    ""$where"": [      ""?id a dbo:City"",      ""?id dbo:country dbr:Italy""    ],    ""$limit"": 100  }  ```    The syntax is composed by two main parts.    ### The prototype    The `@graph`/`proto` property contains the prototype of the result as I expect it. When the value should be taken from the query result, I declare it using the following syntax:        $<SPARQL PREDICATE>[$modifier[:option...]...]    The subject of the predicate is the variable (declared of automatically assigned) of the closer **mergin anchor** in the structure, which is the `@id`/`id` property (if it exists, otherwise is the default `?id`).  The SPARQL variable name is manually (with the `$var` modifier) or automatically assigned.    Some modifiers can be present after, separated by the `$` sign. The `:` prepend the options for a given modifier.    |MODIFIER|OPTIONS|NOTE|  |---|---|---|  |`$required`|n/a| When omitted, the clause is wrapped by `OPTIONAL { ... }`.|  |`$sample`|n/a|Extract a single value for that property by adding a `SAMPLE(?v)` in the SELECT|  |`$lang`|`:lang`[string, optional]| FILTER by language. In absence of a language, pick the first value of `$lang` in the root.<br>Ex. `$lang:it`, `$lang:en`, `$lang`. |  |`$bestlang`|`:acceptedLangs`[string, optional]| Choose the best match (using `BEST_LANGMATCH`) over the languages according to the list expressed through the [Accept-Language standard](https://tools.ietf.org/html/rfc7231#section-5.3.5). This list can be appended after the `:` or expressed as `$lang` in the root.<br>Ex. `$bestlang`, `$bestlang:en;q=1, it;q=0.7 *;q=0.1`|  |`$var`|`:var`[string]| Specify the variable that will be assigned in the query, so that it can be referred in the root properties (like `$filter`). If missing, a `?` is prepended. <br> Ex. `$var:myVariable`, `$var:?name`|  |`$anchor`|n/a|Set this property as merging anchor. The set is valid for the current level in the JSON tree, ignoring eventual `id`/`@id` sibling properties. Ex. `""a"":""?example$anchor""` sets`?example` as subject of SPARQL statements and merges the final results on the `a` property.|  |`$reverse`|n/a|Set this property for use the current variable as subject of the SPARQL predicate, rather than object.|  |`$count` `$sum` `$min` `$max` `$avg`| n/a | Return the respective aggregate function (COUNT, SUM, MIN, MAX, AVG) on the variable. |  |`$langTag`|`""hide""`, `""show""` (default)| When `hide`, language tags are not included in the output.<br> Ex. `hide` => `""label"":""Bologna""` ;<br>  `show` => `""label"":{""value"": ""Bologna"", ""language"": ""it""}` |  |`$accept`|`""string""`, `""number""`, `""boolean""`| If set, values of type different from the specified one are discarded. |  |`$asList`|n/a| When set, the interested property value would always be a list, even if with a single element. |      In this way, I specify a mapping between the JSON-LD output properties and the ones in the endpoint. The values non prepended by a `$` are transferred as is to the output.    ### The root `$` properties    The `$`-something root properties allow to make the query more specific. They will be not present in the output, being used only at query level.  The supported properties are:    |PROPERTY|INPUT|NOTE|  |--------|-----|----|  |`$where`|string, array| Add where clause in the triple format.<br>Ex. `""$where"": ""?id a dbo:City""`|  |`$values`|object| Set `VALUES` for specified variables as a map. The presence of a lang tag or of the '$lang' attribute attached to the related property is taken in account. <br>Ex. `""$values"": {""?id"": [""dbr:Bari"", ""http://dbpedia.org/resource/Bologna""]}`|  |`$limit` |number| `LIMIT` the SPARQL results |  |`$limitMode` |`query` (default) or `library`| Perform the LIMIT operation in the query or on the obtained results (`library`) |  |`$from` |string(uri)| Define the graph `FROM` which selecting the results |  |`$offset` |number| `OFFSET` applied to the SPARQL results |  |`$distinct`|boolean (default `true`)| Set the `DISTINCT` in the select|  |`$orderby`|string, array| Build an `ORDER BY` on the variables in the input.<br> Ex. `""$orderby"":[""DESC(?name)"",""?age""]`|  |`$groupby`| string, array | Build an `GROUP BY` on the variables in the input. <br> Ex. `""$groupby"":""?id""`|  |`$having`| string, array | Allows to declare the content of `HAVING`. If it is an array, the items are concatenated by `&&`. |  |`$filter`| string, array |Add the content as a `FILTER`.<br>`""$filter"": ""?myNum > 3""`|  |`$prefixes`| object | set the prefixes in the format `""foaf"": ""http://xmlns.com/foaf/0.1/""`.|  |`$lang`|`:acceptedLangs`[string]| The default language to use as `$bestlang` (see above), expressed through the [Accept-Language standard](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4). <br>Ex. `$lang:en;q=1, it;q=0.7 *;q=0.1`|  |`$langTag`|`""hide""`, `""show""` (default)| When `hide`, language tags are not included in the output. Similar to the inline `$langTag`, but acting at a global level.<br> Ex. `hide` => `""label"":""Bologna""` ;<br>  `show` => `""label"":{""value"": ""Bologna"", ""language"": ""it""}` |    The `@context` property (for the JSON-LD version) will be transferred to the output.    The output of this query is intended to be:  - for the plain JSON, an array of object with the shape of the prototype;  - for the JSON-LD, an array of object with the shape of the prototype in the `@graph` property and with a sibling `@context`.    ## How to use    #### Install in nodeJS  Install by npm.    ```bash  npm install sparql-transformer  ```      Add to the application.    ```js  import sparqlTransformer from 'sparql-transformer';  ```    #### Install in the browser    SPARQL Transformer is exposed as [ES Module](https://jakearchibald.com/2017/es-modules-in-browsers/). We rely on [getlibs](https://www.npmjs.com/package/getlibs) until the technology will allow to use [""bare"" import specifier](https://github.com/WICG/import-maps#bare-specifiers).    ```html  <script src=""https://unpkg.com/getlibs""></script>  <script>sparqlTransformer = System.import('https://unpkg.com/sparql-transformer')</script>  ```    #### Use  ```js  sparqlTransformer(query, options)    .then(res => console.log(res))    .catch(err => console.error(err););    ```    The first parameter (`query`) is the query in the JSON-LD format. The JSON-LD can be:  - an already parsed JS object (or defined real time),  - **ONLY if running in NodeJS**, the local path of a JSON file (that will then be read and parsed).    The `options` parameter is optional, and can define the following:    | OPTION | DEFAULT | NOTE |  | --- | --- | --- |  |context | http://schema.org/ | The value in `@context`. It overwrites the one in the query.|  | sparqlFunction | `null` | A function receiving in input the transformed query in SPARQL, returning a Promise. If not specified, the module performs the query on its own<sup id=""a1"">[1](#f1)</sup> against the specified endpoint.  |  | endpoint | http://dbpedia.org/sparql | Used only if `sparqlFunction` is not specified. |  | debug | `false` | Enter in debug mode. This allow to print in console the generated SPARQL query. |      See [`test.js`](./test.js) for further examples.      ## Credits    If you use this module for your research work, please cite:    > Pasquale Lisena, Albert Meroño-Peñuela, Tobias Kuhn and Raphaël Troncy. Easy Web API Development with SPARQL Transformer. In 18th International Semantic Web Conference (ISWC), Auckland, New Zealand, October 26-30, 2019.    [BIB file](./bib/lisena2019easyweb.bib)      > Pasquale Lisena and Raphaël Troncy. Transforming the JSON Output of SPARQL Queries for Linked Data Clients. In WWW'18 Companion: The 2018 Web Conference Companion, April 23–27, 2018, Lyon, France.  https://doi.org/10.1145/3184558.3188739    [BIB file](./bib/lisena2018sparqltransformer.bib)    ---    <b id=""f1"">1</b>: Using a [lightweight SPARQL client](./src/sparql-client.mjs). """
Semantic web;https://github.com/alangrafu/turtle-in-html;"""# Introduction  ##Version 0.0.0.1    Nowadays it is possible to embed RDF in Turtle [1], however it is not easy to see if there is RDF in a page. Inspired by Crowbar [2], I created a tool that displays the triples available in Turtle format. Just copy the link available at http://graves.cl/turtle-in-html/ to your bookmarks and click on it whenever you are in a page with turtle embedded.   turtle-in-html uses Masahide Kanzaki's excellent Turtle parser [3] and is licenses under LGPL.      1. http://www.w3.org/TR/turtle/#in-html  2. http://nytimes.github.io/svg-crowbar/  3. http://www.kanzaki.com/works/2006/misc/0308turtle.html      # Installation and use    Simply go to http://graves.cl/turtle-in-html/ and add the link in your bookrmark. Later, in a page containing turtle (such as http://graves.cl/turtle-in-html/), you can click on that bookmark and you'll see the triples.  # Known bugs    Currently it only works in Firefox and Chrome and hasn't been tested in other browsers. Feel free to report any bugs. """
Semantic web;https://github.com/jpcik/morph;"""morph  =====    **If you don't care about compiling you can use morph**   as in this sample Java project: https://github.com/jpcik/morph-starter  using the library through Maven or Sbt.      To build morph you need:    * jvm7  * sbt 0.13 (www.scala-sbt.org)    The scala version is 2.10.3, but sbt will take care of that ;)  To compile it, run sbt after downloading the code:    ```  >sbt  >compile  ```    To run the R2RML test cases:    ```  >sbt  >project morph-r2rml-tc  >test  ```   """
Semantic web;https://github.com/ldodds/geonames;"""geonames  ========    Simple utility scripts for downloading, unpacking and reformatting the Geonames   RDF data dumps.    Author  ------    Leigh Dodds (leigh.dodds@talis.com)    Download  --------    [http://github.com/ldodds/geonames]    Overview  --------    Geonames is a great resource for geographical information. Helpfully they   publish data exports in a variety of formats, allowing others to process and   manipulate the data locally.    Unfortunately the RDF data dump that is available from:    [http://download.geonames.org/export/dump/all-geonames-rdf.txt.zip]    is a little idiosyncratic. Rather than provide a single ntriples or even RDF/XML file   the dump consists of a text file that consists of alternating lines like this:      ...feature URI....    <rdf:RDF>...RDF/XML description of feature....</rdf:RDF>    This means you need to script up unpacking the file in order to load it into a triple   store.    The Rakefile and script provided here make it easy to download, unpack and convert   the data dump into ntriples.    The conversion is written in Ruby and uses the RDF.rb and rdf/raptor libraries.  So before running the scripts you'll need to execute:      sudo gem install rdf rdf-raptor    To download and convert the files into ntriples simply run:      rake download convert    The converted data is stored in geonames.nt.    License  -------    These scripts are free and unencumbered public domain software. For more information, see http://unlicense.org/ or the accompanying UNLICENSE file. """
Semantic web;https://github.com/leipert/vsb;"""# Visual SPARQL Builder    Have a look at the [project page](http://leipert.github.io/vsb)    The Visual SPARQL Builder (VSB) is a tool which allows users to create and run SPARQL queries with a graphical interface within the browser.  For the creation of a query basic understanding of linked data is needed.    ## Deployment    1. Clone this repository and checkout the `dist` branch (or download a release).  1. Serve the VSB with an webserver like apache or nginx.  1. You probably want to configure the VSB for your own endpoint.     Therefore you need to create a file named `overwrite.js` in the root folder of your VSB copy.     For documentation of the structure of the file, please have a look [here](docs/overwrite.js.md)    ## Development    1. Clone this repository (or download a release).  1. Install [node.js](http://nodejs.org/) and npm  1. Install grunt `npm install -g bower gulp`  1. Run `npm install`  1. Run `bower install`  1. Run `gulp develop` to see the app running on `http://localhost:8123/`  1. Happy Development!"""
Semantic web;https://github.com/phenoscape/scowl;"""# Scowl    [![status](http://joss.theoj.org/papers/1b9d09aab8754997884a04c081cfc019/status.svg)](http://joss.theoj.org/papers/1b9d09aab8754997884a04c081cfc019)    Scowl provides a Scala DSL allowing a declarative approach to composing OWL expressions and axioms using the [OWL API](http://owlapi.sourceforge.net).    ## Usage    Since version 1.2.1, Scowl is available via Maven Central. Add the dependency to your `build.sbt`:    ```scala  libraryDependencies += ""org.phenoscape"" %% ""scowl"" % ""1.4.0""  ```    Import `org.phenoscape.scowl._`, and Scowl implicit conversions will add pseudo Manchester syntax methods to native OWL API objects. Additionally, functional syntax-style constructors and extractors will be in scope.    Scowl 1.2+ is built with OWL API 4.x. For OWL API 3.5, use Scowl 1.0.2. Scowl is cross-compiled to support Scala 2.13 and Scala 3.    ## Examples  The easiest way to get started is to see how the DSL can be used to implement all the examples from the [OWL 2 Web Ontology Language   Primer](https://www.w3.org/TR/owl2-primer/):    * [Manchester syntax style](https://github.com/phenoscape/scowl/blob/master/src/main/scala/org/phenoscape/scowl/example/OWL2PrimerManchester.scala)  * [Functional syntax style](https://github.com/phenoscape/scowl/blob/master/src/main/scala/org/phenoscape/scowl/example/OWL2PrimerFunctional.scala)    The examples below are also available in   [code](https://github.com/phenoscape/scowl/blob/master/src/main/scala/org/phenoscape/scowl/example/ReadMeExamples.scala).    ### Scowl expressions use and return native OWL API objects  ```scala  import org.phenoscape.scowl._  // import org.phenoscape.scowl._    val hasParent = ObjectProperty(""http://www.co-ode.org/roberts/family-tree.owl#hasParent"")  // hasParent: org.semanticweb.owlapi.model.OWLObjectProperty = <http://www.co-ode.org/roberts/family-tree.owl#hasParent>    val isParentOf = ObjectProperty(""http://www.co-ode.org/roberts/family-tree.owl#isParentOf"")  // isParentOf: org.semanticweb.owlapi.model.OWLObjectProperty = <http://www.co-ode.org/roberts/family-tree.owl#isParentOf>    val isSiblingOf = ObjectProperty(""http://www.co-ode.org/roberts/family-tree.owl#isSiblingOf"")  // isSiblingOf: org.semanticweb.owlapi.model.OWLObjectProperty = <http://www.co-ode.org/roberts/family-tree.owl#isSiblingOf>    val Person = Class(""http://www.co-ode.org/roberts/family-tree.owl#Person"")  // Person: org.semanticweb.owlapi.model.OWLClass = <http://www.co-ode.org/roberts/family-tree.owl#Person>    val FirstCousin = Class(""http://www.co-ode.org/roberts/family-tree.owl#FirstCousin"")  // FirstCousin: org.semanticweb.owlapi.model.OWLClass = <http://www.co-ode.org/roberts/family-tree.owl#FirstCousin>    val axiom = FirstCousin EquivalentTo (Person and (hasParent some (Person and (isSiblingOf some (Person and (isParentOf some Person))))))  // axiom: org.semanticweb.owlapi.model.OWLEquivalentClassesAxiom = EquivalentClasses(<http://www.co-ode.org/roberts/family-tree.owl#FirstCousin> ObjectIntersectionOf(<http://www.co-ode.org/roberts/family-tree.owl#Person> ObjectSomeValuesFrom(<http://www.co-ode.org/roberts/family-tree.owl#hasParent> ObjectIntersectionOf(<http://www.co-ode.org/roberts/family-tree.owl#Person> ObjectSomeValuesFrom(<http://www.co-ode.org/roberts/family-tree.owl#isSiblingOf> ObjectIntersectionOf(<http://www.co-ode.org/roberts/family-tree.owl#Person> ObjectSomeValuesFrom(<http://www.co-ode.org/roberts/family-tree.owl#isParentOf> <http://www.co-ode.org/roberts/family-tree.owl#Person>)))))) )  ```  ### Add some axioms and programmatically generated GCIs to an ontology  ```scala  val manager = OWLManager.createOWLOntologyManager()  val ontology = manager.createOntology()  val PartOf = ObjectProperty(""http://example.org/part_of"")  val HasPart = ObjectProperty(""http://example.org/has_part"")  val DevelopsFrom = ObjectProperty(""http://example.org/develops_from"")  val Eye = Class(""http://example.org/eye"")  val Head = Class(""http://example.org/head"")  val Tail = Class(""http://example.org/tail"")    manager.addAxiom(ontology, Eye SubClassOf (PartOf some Head))  manager.addAxiom(ontology, Eye SubClassOf (not(PartOf some Tail)))    val gcis = for {    term <- ontology.getClassesInSignature(true)  } yield {    (not(HasPart some term)) SubClassOf (not(HasPart some (DevelopsFrom some term)))  }  manager.addAxioms(ontology, gcis)  ```    ### Using pattern matching extractors to implement negation normal form  ```scala  def nnf(expression: OWLClassExpression): OWLClassExpression = expression match {    case Class(_)                                                          => expression    case ObjectComplementOf(Class(_))                                      => expression    case ObjectComplementOf(ObjectComplementOf(expression))                => nnf(expression)    case ObjectUnionOf(operands)                                           => ObjectUnionOf(operands.map(nnf))    case ObjectIntersectionOf(operands)                                    => ObjectIntersectionOf(operands.map(nnf))    case ObjectComplementOf(ObjectUnionOf(operands))                       => ObjectIntersectionOf(operands.map(c => nnf(ObjectComplementOf(c))))    case ObjectComplementOf(ObjectIntersectionOf(operands))                => ObjectUnionOf(operands.map(c => nnf(ObjectComplementOf(c))))    case ObjectAllValuesFrom(property, filler)                             => ObjectAllValuesFrom(property, nnf(filler))    case ObjectSomeValuesFrom(property, filler)                            => ObjectSomeValuesFrom(property, nnf(filler))    case ObjectMinCardinality(num, property, filler)                       => ObjectMinCardinality(num, property, nnf(filler))    case ObjectMaxCardinality(num, property, filler)                       => ObjectMaxCardinality(num, property, nnf(filler))    case ObjectExactCardinality(num, property, filler)                     => ObjectExactCardinality(num, property, nnf(filler))    case ObjectComplementOf(ObjectAllValuesFrom(property, filler))         => ObjectSomeValuesFrom(property, nnf(ObjectComplementOf(filler)))    case ObjectComplementOf(ObjectSomeValuesFrom(property, filler))        => ObjectAllValuesFrom(property, nnf(ObjectComplementOf(filler)))    case ObjectComplementOf(ObjectMinCardinality(num, property, filler))   => ObjectMaxCardinality(math.max(num - 1, 0), property, nnf(filler))    case ObjectComplementOf(ObjectMaxCardinality(num, property, filler))   => ObjectMinCardinality(num + 1, property, nnf(filler))    case ObjectComplementOf(ObjectExactCardinality(num, property, filler)) => ObjectUnionOf(ObjectMinCardinality(num + 1, property, nnf(filler)), ObjectMaxCardinality(math.max(num - 1, 0), property, nnf(filler)))    case _                                                                 => ???  }  ```    ### Using pattern matching extractors in for comprehensions  ```scala  // Print all properties and fillers used in existential restrictions in subclass axioms  for {    SubClassOf(_, subclass, ObjectSomeValuesFrom(property, filler)) <- ontology.getAxioms  } yield {    println(s""$property $filler"")  }    // Make an index of language tags to label values  val langValuePairs = for {    AnnotationAssertion(_, RDFSLabel, _, value @@ Some(lang)) <- ontology.getAxioms(Imports.INCLUDED)  } yield {    lang -> value  }  val langToValues: Map[String, Set[String]] = langValuePairs.foldLeft(Map.empty[String, Set[String]]) {    case (langIndex, (lang, value)) =>      langIndex.updated(lang, langIndex.getOrElse(value, Set.empty) ++ Set(value))  }  ```    ## Question or problem?  If you have questions about how to use Scowl, feel free to send an email to balhoff@gmail.com, or [open an issue on the tracker](https://github.com/phenoscape/scowl/issues). [Contributions are welcome](CONTRIBUTING.md).    ## Funding  Development of Scowl has been supported by National Science Foundation grant DBI-1062404 to the University of North Carolina.    ## License    Scowl is open source under the [MIT License](http://opensource.org/licenses/MIT).  See [LICENSE](LICENSE) for more information. """
Semantic web;https://github.com/dice-group/DEER;"""# DEER  [![Build Status](https://github.com/dice-group/deer/actions/workflows/run-tests.yml/badge.svg?branch=master&event=push)](https://github.com/dice-group/deer/actions/workflows/run-tests.yml)  [![DockerHub](https://badgen.net/badge/dockerhub/dicegroup%2Fdeer/blue?icon=docker)](https://hub.docker.com/r/dicegroup/deer)  [![GNU Affero General Public License v3.0](https://badgen.net/badge/license/GNU_Affero_General_Public_License_v3.0/orange)](./LICENSE)  ![Java 11+](https://badgen.net/badge/java/11+/gray?icon=maven)    <div style=""text-align: center;"">    ![LOGO](https://raw.githubusercontent.com/dice-group/deer/master/docs/_media/deer_logo.svg)  </div>    The RDF Dataset Enrichment Framework (DEER), is a modular, extensible software system for efficient  computation of arbitrary operations on RDF datasets.    The atomic operations involved in this process, dubbed *enrichment operators*,   are configured using RDF, making DEER a native semantic web citizen.    Enrichment operators are mapped to nodes of a directed acyclic graphs to build complex enrichment  models, in which the connections between two nodes represent intermediary datasets.    ## Running DEER    To bundle DEER as a single jar file, do    ```bash  mvn clean package shade:shade -Dmaven.test.skip=true  ```    Then execute it using    ```bash  java -jar deer-cli/target/deer-cli-${current-version}.jar path_to_config.ttl  ```    ## Using Docker    The Docker image declares two volumes:  - /plugins - this is where plugins are dynamically loaded from  - /data - this is where configuration as well as input/output data will reside    For running DEER server in Docker, we expose port 8080.  The image accepts the same arguments as the deer-cli.jar, i.e. to run a configuration at `./my-configuration`:    ```bash  docker run -it --rm \     -v $(pwd)/plugins:/plugins \     -v $(pwd):/data dicegroup/deer:latest \     /data/my-configuration.ttl  ```    To run DEER server:    ```bash  docker run -it --rm \     -v $(pwd)/plugins:/plugins \     -p 8080:8080 \     -s  ```    ## Maven    ```xml  <dependencies>    <dependency>      <groupId>org.aksw.deer</groupId>      <artifactId>deer-core</artifactId>      <version>2.3.1</version>    </dependency>  </dependencies>  ```  ```xml  <repositories>   <repository>        <id>maven.aksw.internal</id>        <name>University Leipzig, AKSW Maven2 Internal Repository</name>        <url>http://maven.aksw.org/repository/internal/</url>      </repository>        <repository>        <id>maven.aksw.snapshots</id>        <name>University Leipzig, AKSW Maven2 Snapshot Repository</name>        <url>http://maven.aksw.org/repository/snapshots/</url>      </repository>  </repositories>  ```      ## Documentation    For more detailed information about how to run or extend DEER, please read the  [manual](https://dice-group.github.io/deer/) and consult the  [Javadoc](https://dice-group.github.io/deer/javadoc/)    ## Developers    ### Release new version    ```bash  ./release ${new-version} ${new-snapshot-version}  ```"""
Semantic web;https://github.com/SmartDataAnalytics/Beast;"""# BEAST - Benchmarking, Evaluation, and Analysis Stack    Beast is a lightweight framework that makes it easy to build RDF-in/RDF-out workflows using Java8 streams and Jena.  For instance, if you want to execute a set of tasks described in RDF, Beast easily lets you create workflows that execute them as often as desired and record   any measurements directly in  RDF using the vocabulary of your choice (such as DataCube).    ## Charts in RDF - the Chart Vocabulary    The chart vocabulary enables embedding information about which charts to render from a dataset directly in RDF.  The full dataset example is [here](beast-core/src/test/resources/statistical-data.ttl).    ```turtle  eg:exp1    a cv:StatisticalBarChart ;    rdfs:label ""Performance Histogram"" ;    cv:xAxisTitle ""Workload"" ;    cv:yAxisTitle ""Time (s)"" ;    cv:width 1650 ;    cv:height 1050 ;    cv:style eg:exp1-style ;    cv:series eg:exp1-series ;    .    eg:exp1-style    a cv:ChartStyle ;    cv:legendPosition ""InsideNW"" ;    cv:yAxisLogarithmic true ;    cv:yAxisTicksVisible true ;    cv:xAxisLabelRotation 45 ;    cv:yAxisDecimalPattern ""###,###,###,###,###.#####"" ;    .            eg:exp1-series     a cv:ConceptBasedSeries ;    cv:sliceProperty bsbm:experimentId ;    cv:series ""some-triple-store"" ;    cv:valueProperty <http://bsbm.org/avgQet> ;    bsbm:experimentId eg:bsbm-exp1 ;    .  ```    Charts can be rendered using the class [`org.aksw.beast.cli.MainBeastChart`](beast-cli/src/main/java/org/aksw/beast/cli/MainBeastChart.java).  Installing the beast debian package gives you the convenient `ldcharts` command, which invokes the main class for rendering charts.    ```bash  cd beast-core/src/test/resources  ldcharts statistical-data.ttl  ```      ```bash  Usage [Options] file(s)    Option                 Description  ------                 -----------  --png                  Output charts in png format (Default if no other format is given)  --svg                  Output charts in svg format  --jgp                  Output charts in jpg format  --gui                  Display charts in a window  -o, --output <String>  Output folder  ```    ![LDChartScreenshot](docs/images/2018-02-10-ldchart-screenshot.png)      ## Features    * Construction of Resource-centric Java streams. Hence, plain RDF properties can be attached to resources as part of the stream execution.  * Extension to Jena which enhances Resources with support for attaching and retrieving Java objects by class. This means you can e.g. attach a parsed Jena Query object to a resource that represents a SPARQL query string.  * Looping with the loops state getting attached to the resource.  * No need to know the URI for resources in advance. You can painlessly give them a proper name *at the end* of the workflow *based on its properties*.    While technically Beast essentially provides utilities for chaining functions and streams, a great share of Beast's contribution lies in its the conceptual considerations.    ## Components    * Jena Extension: Attach Java objects to Jena resources by casting them to the enhanced resource `ResourceEnh`. Requires the `Model` to be created with `ModelFactoryEnh`:  ```java  Model m = ModelFactoryEnh.createModel();  m.createResource().as(ResourceEnh.class)      .addTrait(myObj.getClass(), myObj);      // Short-hand of above version      .addTrait(myObj);  ```  * RdfStream API: Enables construction of RDF Resource based workflows using the usual streaming methods, such as *map*, *flatMap*, *peek*, and additional ones such as *repeat*.  * Analysis: Compute new resources representing observations of aggregated values such as averages and standard deviations.  * Visualization: Plot series as charts.    ```java  RdfStream      .startWithCopy()      .peek(workloadRes -> workloadRes.as(ResourceEnh.class)          .addTrait(QueryFactory.create(workloadRes.getProperty(LSQ.text).getString())))      .map(workloadRes ->          // Create the blank observation resource          workloadRes.getModel().createResource().as(ResourceEnh.class)          // Copy the query object attached to the workload resource over to this observation resource          .copyTraitsFrom(workloadRes)          // Add some properties to the observation          .addProperty(RDF.type, QB.Observation)          .addProperty(IguanaVocab.workload, workloadRes)          .as(ResourceEnh.class))      .seq(          // Warm up run - the resources are processed, but filtered out          RdfStream.<ResourceEnh>start().repeat(1, IV.run, 1)              .peek(r -> r.addLiteral(IV.warmup, true))              .filter(r -> false),          // Actual evaluation          RdfStream.<ResourceEnh>start().repeat(3, IV.run, 1).peek(r -> r.addLiteral(IV.warmup, false))      )      // Give the observation resource a proper name      .map(r -> r.rename(""http://example.org/observation/{0}-{1}"", r.getProperty(IguanaVocab.workload).getResource().getLocalName(), IV.run))      .apply(() -> workloads.stream()).get()      // write out every observation resource      .forEach(observationRes -> RDFDataMgr.write(System.out, observationRes.getModel(), RDFFormat.TURTLE_BLOCKS));      ;  ```    Which generates output such as:  ```java  <http://example.org/observation/q1a-5>          <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>  <http://purl.org/linked-data/cube#Observation> ;          <http://iguana.aksw.org/ontology#workload>  <http://example.org/query/q1a> ;          <http://www.w3.org/ns/prov#startedAtTime>  ""2016-12-20T02:57:07.608Z""^^<http://www.w3.org/2001/XMLSchema#dateTime> ;          <http://www.w3.org/ns/prov#endAtTime>  ""2016-12-20T02:57:07.672Z""^^<http://www.w3.org/2001/XMLSchema#dateTime> ;          <http://www.w3.org/2006/time#numericDuration>  ""0.063747601""^^<http://www.w3.org/2001/XMLSchema#double> ;          <http://iv.aksw.org/vocab#run>  ""5""^^<http://www.w3.org/2001/XMLSchema#long> ;          <http://iv.aksw.org/vocab#warmup>  false .  ```    ## Examples    * [Performance Measurement](beast-examples/src/main/java/org/aksw/beast/examples/MainQueryPerformance.java) - [Test Data (queries.ttl)](beast-examples/src/main/resources/queries.ttl)  * [KFoldCrossValidation](beast-examples/src/main/java/org/aksw/beast/examples/MainKFoldCrossValidation.java) - [Test Data (folds.ttl)](beast-examples/src/main/resources/folds.ttl)      ## Dependencies    Beast only aggregates features from other (lower-level) projects, among them:    Core:  * jena  * guava    Visualization (Optional):  * JFreeChart  * XChart (probably in the future)           """
Semantic web;https://github.com/dice-group/IGUANA;"""[![GitLicense](https://gitlicense.com/badge/dice-group/IGUANA)](https://gitlicense.com/license/dice-group/IGUANA)  ![Java CI with Maven](https://github.com/dice-group/IGUANA/workflows/Java%20CI%20with%20Maven/badge.svg)[![BCH compliance](https://bettercodehub.com/edge/badge/AKSW/IGUANA?branch=master)](https://bettercodehub.com/)  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/9668460dd04c411fab8bf5ee9c161124)](https://www.codacy.com/app/TortugaAttack/IGUANA?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=AKSW/IGUANA&amp;utm_campaign=Badge_Grade)  [![Project Stats](https://www.openhub.net/p/iguana-benchmark/widgets/project_thin_badge.gif)](https://www.openhub.net/p/iguana-benchmark)      # IGUANA    <img src = ""https://github.com/dice-group/IGUANA/raw/develop/images/IGUANA_logo.png"" alt = ""IGUANA Logo"" width = ""400"" align = ""center"">    ## ABOUT      Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data.  Hence it is very important that the triple store must scale on the data and can handle several users.   Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily.  Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well.  Further on it was impossible to compare results for different benchmarks.     Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications.</br>  which solves all these issues.   It provides an enviroment which ...      + ... is highly configurable  + ... provides a realistic scneario benchmark  + ... works on every dataset  + ... works on SPARQL HTTP endpoints  + ... works on HTTP Get & Post endpoints  + ... works on CLI applications  + and is easily extendable      For further Information visit    [iguana-benchmark.eu](http://iguana-benchmark.eu)     [Documentation](http://iguana-benchmark.eu/docs/3.3/)      # Getting Started    # Prerequisites     You need to install Java 11 or greater.  In Ubuntu you can install these using the following commands    ```  sudo apt-get install java  ```    # Iguana Modules    Iguana consists of two modules    1. **corecontroller**: This will benchmark the systems   2. **resultprocessor**: This will calculate the Metrics and save the raw benchmark results     ## **corecontroller**    The **corecontroller** will benchmark your system. It should be started on the same machine the  is started.    ## **resultprocessor**    The **resultprocessor** will calculate the metrics.  By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store.   On the processing side, it calculates various metrics.    Per run metrics:  * Query Mixes Per Hour (QMPH)  * Number of Queries Per Hour (NoQPH)  * Number of Queries (NoQ)  * Average Queries Per Second (AvgQPS)    Per query metrics:  * Queries Per Second (QPS)      * Number of successful and failed queries      * result size      * queries per second      * sum of execution times    You can change these in the Iguana Benchmark suite config.    If you use the [basic configuration](https://github.com/dice-group/IGUANA/blob/master/example-suite.yml), it will save all mentioned metrics to a file called `results_{{DATE_RP_STARTED}}.nt`      # Setup Iguana    ## Download  Please download the release zip **iguana-x.y.z.zip** from the newest release available [here](https://github.com/dice-group/IGUANA/releases/latest):    ```  mkdir iguana  wget https://github.com/dice-group/IGUANA/releases/download/v3.3.0/iguana-3.3.0.zip  unzip iguana-3.3.0.zip  ```      It contains the following files:    * iguana.corecontroller-X.Y.Z.jar  * start-iguana.sh  * example-suite.yml    # Run Your Benchmarks    ## Create a Configuration    You can use the [basic configuration](https://github.com/dice-group/IGUANA/blob/master/example-suite.yml) we provide and modify it to your needs.  For further information please visit our [configuration](http://iguana-benchmark.eu/docs/3.2/usage/configuration/) and [Stresstest](http://iguana-benchmark.eu/docs/3.0/usage/stresstest/) wiki pages. For a detailed, step-by-step instruction please attend our [tutorial](http://iguana-benchmark.eu/docs/3.2/usage/tutorial/).        ## Execute the Benchmark    Use the start script   ```  ./start-iguana.sh example-suite.yml  ```  Now Iguana will execute the example benchmark suite configured in the example-suite.yml file      # How to Cite    ```bibtex  @InProceedings{10.1007/978-3-319-68204-4_5,  author=""Conrads, Felix  and Lehmann, Jens  and Saleem, Muhammad  and Morsey, Mohamed  and Ngonga Ngomo, Axel-Cyrille"",  editor=""d'Amato, Claudia  and Fernandez, Miriam  and Tamma, Valentina  and Lecue, Freddy  and Cudr{\'e}-Mauroux, Philippe  and Sequeda, Juan  and Lange, Christoph  and Heflin, Jeff"",  title=""Iguana: A Generic Framework for Benchmarking the Read-Write Performance of Triple Stores"",  booktitle=""The Semantic Web -- ISWC 2017"",  year=""2017"",  publisher=""Springer International Publishing"",  address=""Cham"",  pages=""48--65"",  abstract=""The performance of triples stores is crucial for applications driven by RDF. Several benchmarks have been proposed that assess the performance of triple stores. However, no integrated benchmark-independent execution framework for these benchmarks has yet been provided. We propose a novel SPARQL benchmark execution framework called Iguana. Our framework complements benchmarks by providing an execution environment which can measure the performance of triple stores during data loading, data updates as well as under different loads and parallel requests. Moreover, it allows a uniform comparison of results on different benchmarks. We execute the FEASIBLE and DBPSB benchmarks using the Iguana framework and measure the performance of popular triple stores under updates and parallel user requests. We compare our results (See https://doi.org/10.6084/m9.figshare.c.3767501.v1) with state-of-the-art benchmarking results and show that our benchmark execution framework can unveil new insights pertaining to the performance of triple stores."",  isbn=""978-3-319-68204-4""  }  ``` """
Semantic web;https://github.com/paulhoule/infovore;"""Overview  --------    Infovore is an RDF processing system that uses Hadoop to process RDF data  sets in the billion triple range and beyond.  Infovore was originally designed to process  the (old) proprietary Freebase dump into RDF,  but once Freebase came out with an official RDF  dump,  Infovore gained the ability to clean and purify the dump,  making it not just possible  but easy to process Freebase data with triple stores such as Virtuoso 7.    Every week we run Infovore in Amazon Elastic/Map reduce in order to produce a product known as  [:BaseKB](http://basekb.com/).    Infovore depends on the [Centipede](https://github.com/paulhoule/centipede/wiki) framework for packaging  and processing command-line arguments.  The [Telepath](https://github.com/paulhoule/telepath/wiki) project  extends the Infovore project in order to process Wikipedia usage information to produce a product called  [:SubjectiveEye3D](https://github.com/paulhoule/telepath/wiki/SubjectiveEye3D).      Supporting  ----------    It costs several hundreds of dollars per month to process and store files in connection with this work.  Please join <a href=""https://www.gittip.com/"">Gittip</a> and make a <a href=""https://www.gittip.com/paulhoule/"">small weekly donation</a> to keep this data free.      Building  --------    Infovore software requires JDK 7.    mvn clean install    Installing  ----------    The following cantrip, run from the top level ""infovore"" directory, initializes the bash shell  for the use of the ""haruhi"" program,  which can be used to run Infovore applications  packaged in the Bakemono Jar.    source haruhi/target/path.sh    More Information  ----------------    See     https://github.com/paulhoule/infovore/wiki     for documentation and join the discussion group at    https://groups.google.com/forum/#!forum/infovore-basekb         """
Semantic web;https://github.com/TopQuadrant/shacl;"""# TopBraid SHACL API    **An open source implementation of the W3C Shapes Constraint Language (SHACL) based on Apache Jena.**    Contact: Holger Knublauch (holger@topquadrant.com)    Can be used to perform SHACL constraint checking and rule inferencing in any Jena-based Java application.  This API also serves as a reference implementation of the SHACL spec.    Coverage:  * [SHACL Core and SHACL-SPARQL validation](https://www.w3.org/TR/shacl/)  * [SHACL Advanced Features (Rules etc)](https://www.w3.org/TR/shacl-af/)    Former Coverage until version 1.4.0  * [SHACL Compact Syntax](https://w3c.github.io/shacl/shacl-compact-syntax/)    Former Coverage until version 1.3.2  * [SHACL JavaScript Extensions](https://www.w3.org/TR/shacl-js/)    The TopBraid SHACL API is internally used by the European Commission's generic [SHACL-based RDF validator](https://www.itb.ec.europa.eu/shacl/any/upload) (used to validate RDF content against SHACL shapes)  and [SHACL shape validator](https://www.itb.ec.europa.eu/shacl/shacl/upload) (used to validate SHACL shapes themselves).    The same code is used in the TopBraid products (currently aligned with the TopBraid 7.1 release).    Feedback and questions should become GitHub issues or sent to TopBraid Users mailing list:  https://groups.google.com/forum/#!forum/topbraid-users  Please prefix your messages with [SHACL API]    To get started, look at the class ValidationUtil in  the package org.topbraid.shacl.validation.  There is also an [Example Test Case](../master/src/test/java/org/topbraid/shacl/ValidationExample.java)    ## Application dependency    Releases are available in the central maven repository:    ```  <dependency>    <groupId>org.topbraid</groupId>    <artifactId>shacl</artifactId>    <version>*VER*</version>  </dependency>  ```  ## Command Line Usage    Download the latest release from:    `https://repo1.maven.org/maven2/org/topbraid/shacl/`    The binary distribution is:    `https://repo1.maven.org/maven2/org/topbraid/shacl/*VER*/shacl-*VER*-bin.zip`.    Two command line utilities are included: shaclvalidate (performs constraint validation) and shaclinfer (performs SHACL rule inferencing).    To use them, set up your environment similar to https://jena.apache.org/documentation/tools/ (note that the SHACL download includes Jena).    For example, on Windows:    ```  SET SHACLROOT=C:\Users\Holger\Desktop\shacl-1.4.1-bin  SET PATH=%PATH%;%SHACLROOT%\bin  ```    As another example, for Linux, add to .bashrc these lines:    ```  # for shacl  export SHACLROOT=/home/holger/shacl/shacl-1.4.1-bin/shacl-1.4.1/bin  export PATH=$SHACLROOT:$PATH   ```    Both tools take the following parameters, for example:    `shaclvalidate.bat -datafile myfile.ttl -shapesfile myshapes.ttl`    where `-shapesfile` is optional and falls back to using the data graph as shapes graph.  Add -validateShapes in case you want to include the metashapes (from the tosh namespace in particular).    Currently only Turtle (.ttl) files are supported.    The tools print the validation report or the inferences graph to the output screen. """
Semantic web;https://github.com/read-write-web/rww-play;"""rww-play   ========    This is an implementation in Play of a number of tools to build a Read-Write-Web server using Play2.x and akka.  It is very early stages at present and it implements sketches of the following    * A [CORS](http://www.w3.org/TR/cors/) proxy  * An initial implementation of [Linked Data Basic Profile](http://www.w3.org/2012/ldp/wiki/Main_Page)    This currently works in the [TLS branch of the bblfish fork of Play 2.x](https://github.com/bblfish/Play20), which comes with TLS support and a few more patches.    We use [Travis CI](http://travis-ci.org/) to verify the build: [![Build Status](https://travis-ci.org/read-write-web/rww-play.png)](http://travis-ci.org/read-write-web/rww-play)        Getting going  -------------      * You need Java 7 at least - the official Oracle JVM or another one based on [the GPLed code](http://openjdk.java.net/): removing the dependency on Oracle's JVM will require [publishing of the GPLed java security libs](http://stackoverflow.com/questions/12982595/openjdk-sun-security-libs-on-maven)  * clone [this project](https://github.com/stample/rww-play)     ```bash   $ git clone git://github.com/stample/rww-play.git   ```     In the `rww-play` home directory, run the `build` bash script. It will download a precompiled tuned   version of play, build the application, and run it. (If there is no remotely downloadable version  it will build it from source in the `Play20` directory.)    ```bash  $ ./build  ```  Some network config.  --------------------    Download the JavaScript apps by running    ```bash  $ ./install-app.sh  ```    To start Play in secure mode with lightweight client certificate verification (for WebID)  In file:  conf/application.conf  set the smtp parameters: host= and user=  of your mail provider server.    In file:  /etc/hosts  add host names for the subdomains you will create, e.g. :  127.0.0.1 jmv.localhost  127.0.0.1 jmv1.localhost  127.0.0.1 jmv2.localhost    Installing RWW apps  ----------  The RWW apps are stored in other git repositories.  One can run the script `./install-app.sh` to install or update the RWW apps that we ship with the platform.  Check the script content, it is simply a git clone. ( If installing on a public server make sure the proxy  url is set. )    Running  -------  To start Play in secure mode with lightweight client certificate verification (for WebID); that is, a self-signed certificate:    ```bash   $ Play20/play   [RWWeb] $ idea with-sources=yes	// if you want to run intelliJ   [RWWeb] $ eclipse with-source=true	// if you want to run eclipse Scala IDE   [RWWeb] $ compile   [RWWeb] $ ~run -Dhttps.port=8443 -Dhttps.trustStore=noCA -Dakka.loglevel=DEBUG -Dakka.debug.receive=on -Drww.root.container.path=test_ldp    ```  Then you can direct your browser to:  [https://localhost:8443/2013/](https://localhost:8443/2013/)    If you want to have multiple users on your server, it is best to give each user a subdomain for JS security. This can  be had by starting the server with the following attributes.       For subdomains on your local machine you will need to edit `/etc/hosts` for each server. For  machines on the web you can just assign all domains to the same ip address.    ```bash  [RWWeb] $ run -Dhttps.port=8443 -Dhttps.trustStore=noCA -Drww.subdomains=true -Dhttp.hostname=localhost -Drww.subdomains=true -Dsmtp.password=secret  ```    You can the create yourself a subdomain by pointing your browser to the root domain:  [https://localhost:8443/](https://localhost:8443/). This will lead you to the account creation   page, which will allow you to create subdomains on your server. An e-mail will be sent to   your e-mail address for verification ( but you will be able to find the link in the logs   if the e-mail server is not set up).       Documentation  -------------    Further documentation can be found on the [rww-play wiki](https://github.com/stample/rww-play/wiki).    Licence  -------       Copyright 2013-2014 Henry Story       Licensed under the Apache License, Version 2.0 (the ""License"");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at          [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)       Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an ""AS IS"" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License. """
Semantic web;https://github.com/epimorphics/IntervalServer;"""# IntervalServer    IntervalServer repo contains the source code for the Servlet that provides the Interval service at reference.data.gov.uk. e.g.       - `http://reference.data.gov.uk/id/quarter/2016-Q1` [.rdf](http://reference.data.gov.uk/doc/quarter/2016-Q1) [.ttl](http://reference.data.gov.uk/doc/quarter/2016-Q1.ttl) [.json](http://reference.data.gov.uk/doc/quarter/2016-Q1.json)     - `http://reference.data.gov.uk/id/gregorian-interval/2016-06-30T00:00:00/P3M` [.rdf](http://reference.data.gov.uk/id/gregorian-interval/2016-06-30T00:00:00/P3M) [.ttl](http://reference.data.gov.uk/id/gregorian-interval/2016-06-30T00:00:00/P3M.ttl) [.json](http://reference.data.gov.uk/id/gregorian-interval/2016-06-30T00:00:00/P3M.json)    This source code is made available under an [Apache 2](http://www.apache.org/licenses/LICENSE-2.0) open source license.    [Interval URIs](interval-uris.md) provides more information about the interval service and the use of the interval URIs that it supports in statistical data publications.   """
Semantic web;https://github.com/rdf2h/rdf2h;"""RDF2h [![Build Status](https://travis-ci.org/rdf2h/rdf2h.svg)](https://travis-ci.org/rdf2h/rdf2h)  -------    RDF2h is a tool for rendering RDF resources using mustache templates.    <a href=""https://rdf2h.github.io/rdf2h-documentation/"">Try it out online and read the manual</a> to learn   more.    Install:    You need to have npm and nodejs installed in order to compile the sources,   for convenience the result of compilation is also in git in the directory `dist`.    This will download the dependencies and create (or update) dist/rdf2h.js        npm install """
Semantic web;https://github.com/WileyLabs/askos;"""# A SKOS browser and editor    Early stage [SKOS](https://www.w3.org/TR/skos-primer/) browser and editor.    Built with [Vue.js](https://vuejs.org/), [LevelGraph](https://levelgraph.io/),  and [Semantic-UI](https://semantic-ui.com/).    ## Usage    ```sh  $ npm i  $ npm run dev  ```    The `dev` script will build, watch, and serve Askos making it available at  `http://localhost:8888/`.    ## Design Drafts    Early drafts of the overall intent of the design...highly subject to change.    ![Concept Listing](concept-listing.png)  ![Browsing to a Narrower Concept](concept-narrower.png)    See also the [design.svg](design.svg) file--which you're welcome to contribute  to!    ## License    MIT """
Semantic web;https://github.com/usc-isi-i2/Web-Karma;"""Karma: A Data Integration Tool  ================================    ![travis ci](https://travis-ci.org/usc-isi-i2/Web-Karma.svg?branch=master)    The Karma tutorial at https://github.com/szeke/karma-tcdl-tutorial, also check out our [DIG web site](http://usc-isi-i2.github.io/dig/), where we use Karma extensively to process > 90M web pages.    See our [release stats](http://www.somsubhra.com/github-release-stats/?username=usc-isi-i2&repository=Web-Karma)      ## What is Karma?    Karma is an information integration tool that enables users to quickly and easily integrate data from a variety of data sources including databases, spreadsheets, delimited text files, XML, JSON, KML and Web APIs. Users integrate information by modeling it according to an ontology of their choice using a graphical user interface that automates much of the process. Karma learns to recognize the mapping of data to ontology classes and then uses the ontology to propose a model that ties together these classes. Users then interact with the system to adjust the automatically generated model. During this process, users can transform the data as needed to normalize data expressed in different formats and to restructure it. Once the model is complete, users can published the integrated data as RDF or store it in a database.    You can find useful tutorials on the project Website: [http://www.isi.edu/integration/karma/](http://www.isi.edu/integration/karma/)    ## Installation and Setup ##    Look in the Wiki [Installation](https://github.com/InformationIntegrationGroup/Web-Karma/wiki/Installation)    ## Frequently Asked Questions ##  ### How to perform offline RDF generation for a data source using a published model? ###  1. Model your source and publish it's model (the published models are located at `src/main/webapp/publish/R2RML/` inside the Karma directory).  2. To generate RDF of a CSV/JSON/XML file, go to the top level Karma directory and run the following command from terminal:  ```  cd karma-offline  mvn exec:java -Dexec.mainClass=""edu.isi.karma.rdf.OfflineRdfGenerator"" -Dexec.args=""--sourcetype   <sourcetype> --filepath <filepath> --modelfilepath <modelfilepath> --sourcename <sourcename> --outputfile <outputfile> --JSONOutputFile<outputJSON-LD>"" -Dexec.classpathScope=compile  ```    	Valid argument values for sourcetype are: CSV, JSON, XML. Also, you need to escape the double quotes that go inside argument values. Example invocation for a JSON file:  ```	  mvn exec:java -Dexec.mainClass=""edu.isi.karma.rdf.OfflineRdfGenerator"" -Dexec.args=""  --sourcetype JSON   --filepath \""/Users/shubhamgupta/Documents/wikipedia.json\""   --modelfilepath \""/Users/shubhamgupta/Documents/model-wikipedia.n3\""  --sourcename wikipedia  --outputfile wikipedia-rdf.n3  --JSONOutputFile wikipedia-rdf.json"" -Dexec.classpathScope=compile  ```  3. To generate RDF of a database table, go to the top level Karma directory and run the following command from terminal:  ```  cd karma-offline  mvn exec:java -Dexec.mainClass=""edu.isi.karma.rdf.OfflineRdfGenerator"" -Dexec.args=""--sourcetype DB  --modelfilepath <modelfilepath> --outputfile <outputfile> --dbtype <dbtype> --hostname <hostname>   --username <username> --password <password> --portnumber <portnumber> --dbname <dbname> --tablename <tablename> --JSONOutputFile<outputJSON-LD>"" -Dexec.classpathScope=compile  ```  	Valid argument values for `dbtype` are Oracle, MySQL, SQLServer, PostGIS, Sybase. Example invocation:  ```  mvn exec:java -Dexec.mainClass=""edu.isi.karma.rdf.OfflineRdfGenerator"" -Dexec.args=""  --sourcetype DB --dbtype SQLServer   --hostname example.com --username root --password secret   --portnumber 1433 --dbname Employees --tablename Person   --modelfilepath \""/Users/shubhamgupta/Documents/db-r2rml-model.ttl\""  --outputfile db-rdf.n3  --JSONOutputFile db-rdf.json"" -Dexec.classpathScope=compile  ```    You can do `mvn exec:java -Dexec.mainClass=""edu.isi.karma.rdf.OfflineRdfGenerator"" -Dexec.args=""--help""` to get information about required arguments.    ### How to set up password protection for accessing Karma? ###  - in /src/main/config/jettyrealm.properties change user/password (if you wish)  - in /src/main/webapp/WEB-INF/web.xml uncomment security section at the end of the file  - in pom.xml uncomment security section (search for loginServices)    ### Are there additional steps required to import data from Oracle database? ###  Yes. Due to Oracles binary license issues, we can't distribute the JAR file that is required for importing data from an Oracle database. Following are the steps to resolve the runtime error that you will get if you try to do it with the current source code:    1. Download the appropriate JDBC drive JAR file (for JDK 1.5 and above) that matches your Oracle DB version. Link: http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html  2. Put the downloaded JAR file inside `lib` folder of the Karma source code.   3. Add the following snippet in the `pom.xml` file (present inside the top level folder of Karma source code) inside the dependencies XML element:     ```  <dependency>       <groupId>com.oracle</groupId>       <artifactId>ojdbc</artifactId>       <version>14</version>       <scope>system</scope>       <systemPath>/Users/karma/Web-Karma/lib/ojdbc14.jar</systemPath>   </dependency>   ```  Make sure that the filename mentioned in the `systemPath` element matches with your downloaded JAR file; it is likely that your installation folder is different from `/Users/karma` so make sure you use the correct one.    ### Are there additional steps required to import data from MySQL database? ###  Yes. Due to MySQL binary license issues, we can't distribute the JAR file that is required for importing data from an MySQL database. Following are the steps to resolve the runtime error that you will get if you try to do it with the current source code:    1. Download the appropriate MySQL driver JAR file (for JDK 1.5 and above) that matches your MySQL version. Link: http://dev.mysql.com/downloads/connector/j/  2. Put the downloaded JAR file inside `lib` folder of the Karma source code.   3. Add the following snippet in the `pom.xml` file of the `karma-jdbc` project inside the dependencies XML element:     ```  <dependency>       <groupId>mysql</groupId>       <artifactId>mysql-connector-java</artifactId>       <version>5.1.32</version>       <scope>system</scope>       <systemPath>/Users/karma/Web-Karma/lib/mysql-connector-java-5.1.32-bin.jar</systemPath>   </dependency>   ```  Make sure that the filename mentioned in the `systemPath` element matches with your downloaded JAR file; it is likely that your installation folder is different from `/Users/karma` so make sure you use the correct one. The `version` will be the version of the JAR that you downloaded.     """
Semantic web;https://github.com/KNowledgeOnWebScale/unshacled;"""# UnSHACLed    A visual editor for RDF constraints  currently supporting the visual notations [ShapeUML](https://w3id.org/imec/unshacled/spec/shape-uml/20210118) and [ShapeVOWL](https://w3id.org/imec/unshacled/spec/shape-vowl/20210118/)  and import/export/validation of [SHACL](https://www.w3.org/TR/shacl/) constraints.    The [previous version](https://github.com/oSoc19/unshacled) of UnSHACLed was implemented during the Open Summer of Code 2019 in Belgium under the MIT license.  This is an updated version which adds support for different visual notations to interact visually with RDF constraints.    ## Contents  1. [Overview](#Overview)  2. [Setup](#Setup)  3. [Contribute](#Contribute)    ## Overview  At the time of writing this editor supports SHACL, future support for ShEx is envisioned. This editor makes abstraction of specific constraint languages and exposes concepts in a simple visual interface.    ### Functionalities  - [x] Drag and drop to rearrange the visualized shapes  - [x] Add, remove and edit shapes, constraints and relationships  - [x] View and edit namespaces and prefixes  - [x] Import SHACL files in JSON and Turtle  - [x] Export SHACL files in JSON and Turtle  - [x] Import data files in JSON and Turtle  - [x] View and edit data files in JSON format  - [x] Validate data files    ### Concepts  An [internal model](#Model) is used to represent shapes which can be edited in the browser. Using existing shape files requires these to be imported and [translated to this model](#Translation) before use. Editing is done in a [visual editor](#Interface).    ## Setup  To start the application, run the following commands:  1. Install dependencies  ```  npm install  ```  2. Compile and hot-reload for development  ```  npm run serve  ```    The documentation can be generated in `/docs` using the following command:  ```  npm run docs  ```    ### Useful while developing: testing and linting  ```  npm run test // Run tests  npm run lint // Check and fix code style  ```    ### Compile and minify for production  Execute this command, then move the contents of `/dist` into the `gh-pages` branch. The application will be automatically deployed to [UnSHACLed.com](https://unshacled.com).  ```  npm run build  ```    ## Contribute  This section contains information to help contribute to this project.  For more information about the project structure, the internal model et cetera, please consult [the wiki of the previous version](https://github.com/oSoc19/unshacled/wiki/Home).    ### Linting  To ensure code style consistency we use [ESLint](https://eslint.org/) and [Prettier](https://prettier.io/) which are configured in `.eslintrc.js`.    ### Testing  Testing is done with [Jest.js](https://jestjs.io/) and [Vue Jest](https://github.com/vuejs/vue-jest). Unit tests are kept in the same directory as the classes they test and share the same filename but with extension e.g. `somefile.js` and `somefile.test.js`. All tests can be executed using the following command:   ```  npm run test  ```    ### Documentation  Make sure to document your code in [JSDoc style](https://jsdoc.app/). Documentation is generated using the command:   ```  npm run docs  ```    ```  /* This comment should appear in the HTML documentation. */  // This is just a comment and should not be added to the HTML documentation.  ``` """
Semantic web;https://github.com/oeg-upm/morph-kgc;"""<p align=""center"">  <img src=""https://github.com/oeg-upm/morph-kgc/blob/main/logo.png"" height=""100"" alt=""morph"">  </p>    [![License](https://img.shields.io/pypi/l/morph-kgc.svg)](https://github.com/oeg-upm/morph-kgc/blob/main/LICENSE)  [![DOI](https://zenodo.org/badge/311956260.svg?style=flat)](https://zenodo.org/badge/latestdoi/311956260)  [![Latest PyPI version](https://img.shields.io/pypi/v/morph-kgc?style=flat)](https://pypi.python.org/pypi/morph-kgc)  [![Python Version](https://img.shields.io/pypi/pyversions/morph-kgc.svg)](https://pypi.python.org/pypi/morph-kgc)  [![PyPI status](https://img.shields.io:/pypi/status/morph-kgc?)](https://pypi.python.org/pypi/morph-kgc)  [![build](https://github.com/oeg-upm/morph-kgc/actions/workflows/continuous-integration.yml/badge.svg)](https://github.com/oeg-upm/morph-kgc/actions/workflows/continuous-integration.yml)    Morph-KGC is an engine that constructs [RDF](https://www.w3.org/TR/rdf11-concepts/) knowledge graphs from heterogeneous data sources with [R2RML](https://www.w3.org/TR/r2rml/) and [RML](https://rml.io/specs/rml/) mapping languages. Morph-KGC is built on top of [pandas](https://pandas.pydata.org/) and it leverages *mapping partitions* to significantly reduce execution times and memory consumption for large data sources.    ## Main Features    - Supports [R2RML](https://www.w3.org/TR/r2rml/) and [RML](https://rml.io/specs/rml/) mapping languages.  - Input data formats:    - Relational databases: [MySQL](https://www.mysql.com/), [PostgreSQL](https://www.postgresql.org/), [Oracle](https://www.oracle.com/database/), [Microsoft SQL Server](https://www.microsoft.com/sql-server), [MariaDB](https://mariadb.org/), [SQLite](https://www.sqlite.org/index.html).    - Tabular files: [CSV](https://en.wikipedia.org/wiki/Comma-separated_values), [TSV](https://en.wikipedia.org/wiki/Tab-separated_values), [Excel](https://www.microsoft.com/en-us/microsoft-365/excel), [Parquet](https://parquet.apache.org/documentation/latest/), [Feather](https://arrow.apache.org/docs/python/feather.html), [ORC](https://orc.apache.org/), [Stata](https://www.stata.com/), [SAS](https://www.sas.com), [SPSS](https://www.ibm.com/analytics/spss-statistics-software), [ODS](https://en.wikipedia.org/wiki/OpenDocument).    - Hierarchical files: [JSON](https://www.json.org/json-en.html), [XML](https://www.w3.org/TR/xml/).  - Output RDF serializations: [N-Triples](https://www.w3.org/TR/n-triples/), [N-Quads](https://www.w3.org/TR/n-quads/).  - Runs on Linux, Windows and macOS systems.  - Compatible with Python 3.7 or higher.  - Optimized to materialize large knowledge graphs.  - Highly configurable.  - Available under the [Apache License 2.0](https://github.com/oeg-upm/Morph-KGC/blob/main/LICENSE).    ## Installation and Usage    [PyPi](https://pypi.org/project/morph-kgc/) is the fastest way to install Morph-KGC:  ```  pip install morph-kgc  ```    To run the engine you just need to execute the following:  ```  python3 -m morph_kgc config.ini  ```    [Here](https://github.com/oeg-upm/Morph-KGC/wiki/Configuration) you can see how to generate the configuration file. It is also possible to run Morph-KGC as a library with [RDFlib](https://rdflib.readthedocs.io/en/stable/):  ```python  import morph_kgc    # generate the triples and load them to an RDFlib graph  graph = morph_kgc.materialize('/path/to/config.ini')    # work with the graph  graph.query(' SELECT DISTINCT ?classes WHERE { ?s a ?classes } ')  ```    ## Documentation    Check the **[wiki](https://github.com/oeg-upm/Morph-KGC/wiki)** with all the information:    **[Getting Started](https://github.com/oeg-upm/Morph-KGC/wiki/Getting-Started)**    **[Usage](https://github.com/oeg-upm/Morph-KGC/wiki/Usage)**    **[Configuration](https://github.com/oeg-upm/Morph-KGC/wiki/Configuration)**  - **[Engine](https://github.com/oeg-upm/Morph-KGC/wiki/Engine-Configuration)**  - **[Data Sources](https://github.com/oeg-upm/Morph-KGC/wiki/Data-Source-Configuration)**    - [Relational Databases](https://github.com/oeg-upm/Morph-KGC/wiki/Relational-Databases)    - [Data Files](https://github.com/oeg-upm/Morph-KGC/wiki/Data-Files)    **[Features](https://github.com/oeg-upm/Morph-KGC/wiki/Features)**    **[Academic Publications](https://github.com/oeg-upm/Morph-KGC/wiki/Academic-Publications)**    **[License](https://github.com/oeg-upm/Morph-KGC/wiki/License)**    **[FAQ](https://github.com/oeg-upm/Morph-KGC/wiki/FAQ)**    ## Contact    - **Julián Arenas-Guerrero (julian.arenas.guerrero@upm.es)**    *Ontology Engineering Group, Universidad Politécnica de Madrid | 2020 - Present* """
Semantic web;https://github.com/creativesoftwarefdn/weaviate;"""<h1>Weaviate <img alt='Weaviate logo' src='https://raw.githubusercontent.com/semi-technologies/weaviate/19de0956c69b66c5552447e84d016f4fe29d12c9/docs/assets/weaviate-logo.png' width='124' align='right' /></h1>    ## The ML-first vector search engine    [![Build Status](https://api.travis-ci.org/semi-technologies/weaviate.svg?branch=master)](https://travis-ci.org/semi-technologies/weaviate/branches)  [![Go Report Card](https://goreportcard.com/badge/github.com/semi-technologies/weaviate)](https://goreportcard.com/report/github.com/semi-technologies/weaviate)  [![Coverage Status](https://codecov.io/gh/semi-technologies/weaviate/branch/master/graph/badge.svg)](https://codecov.io/gh/semi-technologies/weaviate)  [![Slack](https://img.shields.io/badge/slack--channel-blue?logo=slack)](https://join.slack.com/t/weaviate/shared_invite/zt-goaoifjr-o8FuVz9b1HLzhlUfyfddhw)  [![Newsletter](https://img.shields.io/badge/newsletter-blue?logo=revue)](http://weaviate-newsletter.semi.technology/)    ## Description    **Weaviate in a nutshell**: Weaviate is a vector search engine and vector database. Weaviate uses machine learning to vectorize and store data, and to find answers to natural language queries. With Weaviate you can also bring your custom ML models to production scale.    **Weaviate in detail**: Weaviate is a low-latency vector search engine with out-of-the-box support for different media types (text, images, etc.). It offers Semantic Search, Question-Answer-Extraction, Classification, Customizable Models (PyTorch/TensorFlow/Keras), and more. Built from scratch in Go, Weaviate stores both objects and vectors, allowing for combining vector search with structured filtering with the fault-tolerance of a cloud-native database, all accessible through GraphQL, REST, and various language clients.    ## Weaviate helps ...    1. **Software Engineers** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate as an ML-first database for your applications.       * Out-of-the-box modules for: NLP/semantic search, automatic classification and image similarity search.      * Easy to integrate in your current architecture, with full CRUD support like you're used to from other OSS databases.      * Cloud-native, distributed, runs well on Kubernetes and scales with your workloads.    2. **Data Engineers** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate as a vector database that is built up from the ground with ANN at its core, and with the same UX they love from Lucene-based search engines.      * Weaviate has a modular setup that allows to use your own ML models inside Weaviate, but you can also use out-of-the-box ML models (e.g., SBERT, ResNet, fasttext, etc).      * Weaviate takes care of the scalability, so that you don't have to.      * Deploy and maintain ML models in production reliably and efficiently.    3. **Data Scientists** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate for a seamless handover of their Machine Learning models to MLOps.      * Deploy and maintain your ML models in production reliably and efficiently.      * Weaviate's modular design allows you to easily package any custom trained model you want.      * Smooth and accelerated handover of your Machine Learning models to engineers.    ## GraphQL interface demo    <a href=""https://weaviate.io/developers/weaviate/current/"" target=""_blank""><img src=""https://weaviate.io/img/weaviate-demo.gif?i=8"" alt=""Demo of Weaviate"" width=""100%""></a>    <sup>Weaviate GraphQL demo on news article dataset containing: Transformers module, GraphQL usage, semantic search, _additional{} features, Q&A, and Aggregate{} function. You can the demo on this dataset in the GUI here: <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Get%20%7B%0A%20%20%20%20Article(%0A%20%20%20%20%20%20nearText%3A%20%7B%0A%20%20%20%20%20%20%20%20concepts%3A%20%5B%22Housing%20prices%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20where%3A%20%7B%0A%20%20%20%20%20%20%20%20operator%3A%20Equal%0A%20%20%20%20%20%20%20%20path%3A%20%5B%22inPublication%22%2C%20%22Publication%22%2C%20%22name%22%5D%0A%20%20%20%20%20%20%20%20valueString%3A%20%22The%20Economist%22%0A%20%20%20%20%20%20%7D%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inPublication%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Publication%20%7B%0A%20%20%20%20%20%20%20%20%20%20name%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20certainty%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">semantic search</a>, <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Get%7B%0A%20%20%20%20Article(%0A%20%20%20%20%20%20ask%3A%20%7B%0A%20%20%20%20%20%20%20%20question%3A%20%22What%20did%20Jemina%20Packington%20predict%3F%22%0A%20%20%20%20%20%20%20%20properties%3A%20%5B%22summary%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%201%0A%20%20%20%20)%7B%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inPublication%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Publication%20%7B%0A%20%20%20%20%20%20%20%20%20%20name%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20answer%20%7B%0A%20%20%20%20%20%20%20%20%20%20endPosition%0A%20%20%20%20%20%20%20%20%20%20property%0A%20%20%20%20%20%20%20%20%20%20result%0A%20%20%20%20%20%20%20%20%20%20startPosition%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">Q&A</a>, <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Aggregate%20%7B%0A%20%20%20%20Article%20%7B%0A%20%20%20%20%20%20meta%20%7B%0A%20%20%20%20%20%20%20%20count%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">Aggregate</a>.</sup>    ## Features    Weaviate makes it easy to use state-of-the-art ML models while giving you the scalability, ease of use, safety and cost-effectiveness of a purpose-built vector database. Most notably:    * **Fast queries**<br>     Weaviate typically performs a 10-NN neighbor search out of millions of objects in considerably less than 100ms.     <br><sub></sub>    * **Any media type with Weaviate Modules**<br>    Use State-of-the-Art ML model inference (e.g. Transformers) for Text, Images, etc. at search and query time to let Weaviate manage the process of vectorizing your data for your - or import your own vectors.    * **Combine vector and scalar search**<br>    Weaviate allows for efficient combined vector and scalar searches, e.g “articles related to the COVID 19 pandemic published within the past 7 days”. Weaviate stores both your objects and the vectors and make sure the retrieval of both is always efficient. There is no need for a third party object storage.     * **Real-time and persistent**<br>  Weaviate let’s you search through your data even if it’s currently being imported or updated. In addition, every write is written to a Write-Ahead-Log (WAL) for immediately persisted writes - even when a crash occurs.    * **Horizontal Scalability**<br>    Scale Weaviate for your exact needs, e.g. High-Availability, maximum ingestion, largest possible dataset size, maximum queries per second, etc. (Multi-Node sharding since `v1.8.0`, Replication under development)     * **Cost-Effectiveness**<br>    Very large datasets do not need to be kept entirely in memory in Weaviate. At the same time available memory can be used to increase the speed of queries. This allows for a conscious speed/cost trade-off to suit every use case.    * **Graph-like connections between objects**<br>    Make arbitrary connections between your objects in a graph-like fashion to resemble real-life connections between your data points. Traverse those connections using GraphQL.    ## Documentation    You can find detailed documentation in the [developers section of our website](https://weaviate.io/developers/weaviate/current/) or directly go to one of the docs using the links in the list below.    ## Additional material    ### Video    - [Weaviate introduction video](https://www.youtube.com/watch?v=IExopg1r4fw)    ### Reading    - [Weaviate is an open-source search engine powered by ML, vectors, graphs, and GraphQL (ZDNet)](https://www.zdnet.com/article/weaviate-an-open-source-search-engine-powered-by-machine-learning-vectors-graphs-and-graphql/)  - [Weaviate, an ANN Database with CRUD support (DB-Engines.com)](https://db-engines.com/en/blog_post/87)  - [A sub-50ms neural search with DistilBERT and Weaviate (Towards Datascience)](https://towardsdatascience.com/a-sub-50ms-neural-search-with-distilbert-and-weaviate-4857ae390154)  - [Getting Started with Weaviate Python Library (Towards Datascience)](https://towardsdatascience.com/getting-started-with-weaviate-python-client-e85d14f19e4f)    ## Examples    You can find [code examples here](https://github.com/semi-technologies/weaviate-examples)    ## Support    - [Stackoverflow for questions](https://stackoverflow.com/questions/tagged/weaviate)  - [Github for issues](https://github.com/semi-technologies/weaviate/issues)  - [Slack channel to connect](https://join.slack.com/t/weaviate/shared_invite/zt-goaoifjr-o8FuVz9b1HLzhlUfyfddhw)  - [Newsletter to stay in the know](http://weaviate-newsletter.semi.technology/)    ## Contributing    - [How to Contribute](https://weaviate.io/developers/contributor-guide/current/) """
Semantic web;https://github.com/antoniogarrote/json-ld-macros;"""# JSON-LD Macros [![Build Status](https://travis-ci.org/antoniogarrote/json-ld-macros.svg?branch=master)](https://travis-ci.org/antoniogarrote/json-ld-macros)    JSON-LD Macros is a library to define declarative transformations of JSON objects obtained from a remote web service into JSON-LD objects. The ultimate goal of the library is to make it easier the process of consuming JSON APIs from RDF/JSON-LD applications. Similar ideas for transforming JSON documents into RDF have been explored in projects like [jsonGRDDL](http://buzzword.org.uk/2008/jsonGRDDL/spec.20100903).  JSON-LD Macros supports the serialisation of the macro itself as JSON-LD and the deserialisation back into the macro description.    A demo is available [here](http://antoniogarrote.github.com/json-ld-macros/) .    ## A Minimal example    ``` javascript        // requires the library      var macros = require('jsonld_macros');        macros.registerAPI({          // URI template for a remote service (Github Users' API)        ""https://api.github.com/users/{username}"":          {""$"": // selects the root node / list of root nodes of the JSON document            { // a JSON-LD context that will be added to all the slected nodes            ""@context"": {""data"":""http://socialrdf.org/github/datafeed""},            // removes the meta property and associated value from the selected nodes            ""@remove"":""meta""},            ""$.data"": // selects the root node/data objects             {// by default, all properties in the selected nodes will have the 'gh' prefix            ""@ns"": {""ns:default"": ""gh""},            // a JSON-LD context declaration that will be added to all the selecte nodes            ""@context"": {""gh"":""http://socialrdf.org/github/""},            // a JSON-LD type declaration that will be added to all the selecte nodes            ""@type"": ""http://socialrdf.org/github/User""}}      });        // We retrieve the data using whatever transport layer is      // available: AJAX, TCP sockets...      var resourceURI = ""https://api.github.com/users/1"";      retrieveRemoteData(resourceURI, function(data){           // we can apply the transformation to the retrieved data         // passing the URI used to retrieve the data         // as a selector for the transformation         var jsonld = macros.resolve(resourceURI, data);      });    ```    ## Definition of transformations    JSON-LD Macros fundamental concept is the description of JSON documents transformations encoded as JSON objects.  Transformation objects can be used to describe a service API associating a transformation to a list of URIs templates.  Transformations in turn are composed of pairs key-values where the key declares a 'selector' of nodes in the JSON object to transform. The value consist in a collection of transformation rules from a fixes set of possible rules: ""@context"", ""@id"", ""@type"", ""@remove"", ""@only"", ""@ns"" and ""@transform"".  When the transformation is applied to a JSON object retrieved from a URI matching one of the declared templates, each of the node selectors defined for that transformation is evaluated in the retrieved object. The output of this evaluation is a collection of nodes per node selector. For every collection of nodes, the transformation rules are applied inplace. After applying all the transformations, the resulting DOM document is returned as the final output.  Transformation bodies can consist, in some cases, in an array of objects containing functions that can be applied to the selected node to obtain the value that will be used by the transformation.    The following grammar describes the structure of an API transformation definition:    - API ::= {@declare:functionDeclarations}? , {URIPatterns: Transformation}*  - URIPatterns ::= URIPattern[\\n URIPattern]*  - Transformation ::= {NodeSelector: TransformationRules}  - TransformationRules ::= {TransformationRuleName: TransformationRuleBody}*  - TransformationRuleName ::= @context | @id | @type | @remove | @only | @ns | @transform  - TransformationRuleBody ::= FunctionsArray | JSON String | JSON Object    The following sections describe how to declare URI patterns, node selectors and transformations.    ### URI Patterns    URI patterns are regular URLs patterns as defined in [RFC6570](https://tools.ietf.org/html/rfc6570).    ### Node Selectors    Node selectors syntax is taken from [JSON Path](http://goessner.net/articles/JsonPath/) but the semantics are slightly modified to match the behaviour os selector libraries like jQuery.  Paths are chains of names identifying JSON objects propertes separated by '.' characters. Some characters can be used for special purposes:    - '$': Selects the root of the document. It can be a single JSON object if the document includes a single object or a collection of objects if the root object in the document is an array.  - '*': Selects all the objects linked to any property of the selected nodes.  - '..': Recursive evaluation of the rest of the path expression.  - 'propertyName[ * ]': if 'propertyName' returns an array of objects, 'propertyName[*]' aggregates all the objects in the selected arrays.      Evaluation of the selector is accomplished from left to right. For every component in the path, it is evaluated in the current set of selected nodes. After evaluation, the selected nodes set is replaced by the output of the evaluation. The set of selected nodes start with the empty set.      ### Transformation Rules    Transformation rules are JSON objects where the keys of the object identified certain standard transformations that can be performed in the input object and the values describe particular details of the transformation rule. A fixed set of transformation rules is available: ""@explode"", ""@compact"", ""@context"", ""@id"", ""@type"", ""@add"", ""@remove"", ""@only"", ""@ns"" and ""@transform"".    Rules are applied in the following order:    - @explode  - @add  - @context, @id, @type, @transform  - @remove  - @only  - @ns  - @compact    Rules are applied inplace in the target object without cloning or reserving any additional memory.  Some transformation rules like ""@id"", ""@type"" and ""@transform"" accept as the rule body an array of functions that will applied to the target object to obtain the final value generated by the rule.  Additional functions can be declared in the API definition.    This is a description of the different transformations    ### @explode    Transforms a pair property - value into  a pair property - nested object where the nested object has only the property value with a property specified in the transformation.    ``` javascript    // input node  - {""contact_url"": ""http://test.com/people/dmm4""}    // rule  - {""@explode"": ""@id""}    // output node  - {""contact_url"": { ""@id"": ""http://test.com/people/dmm4""} }    ```    ### @compact    Transforms a pair property - node into a pair property - value where the value is the value of the selected property in the node.    ``` javascript    // input node  {""contact_url"": {""$ref"": ""http://test.com/people/dmm4""}}    // rule  {""@compact"": ""$ref""}    // output node  {""contact_url"": ""http://test.com/people/dmm4""}    ```    ### @context    Defines a context JSON-LD object that is inserted in the target object. The body of the rule is the JSON object defining the JSON-LD context that will be inserted    ``` javascript    // input node  {""contact_url"": ""http://test.com/people/dmm4""}    // rule  {""@context"": {""contact_url"": { ""@id"": ""foaf:knowks"", ""@type"": ""@id""}, ""foaf"":""http://xmlns.com/foaf/0.1/"" }    // output node  {""@context"": {""contact_url"": { ""@id"": ""foaf:knowks"", ""@type"": ""@id""}, ""foaf"":""http://xmlns.com/foaf/0.1/""  },   ""contact_url"": ""http://test.com/people/dmm4"" }    ```    ### @id    Defines how the @id JSON-LD attribute will be generated in the transformed object. Possible rule values can be:    - JSON string: a fixed string that will be inserted as the value of the @id property in all the nodes  - An array of functions that will be applied to each selected node to obtain the value of the @id JSON-LD object.    ``` javascript    // input node  {""prop"":""value""}    // rule  {""@id"":""http://test.com/user#me""}    // output node  {""@id"":""http://test.com/user#me"", ""prop"":""value""}    ```    ### @type    Defines how the @type JSON-LD attribute will be generated in the transformed object. Possible rule values can be:    - JSON string: a fixed string that will be inserted as the value of the @type property in all the nodes  - JSON array: an array of fixed strings that will be inserted as the value of the @type property in all the nodes  - An array of functions that will be applied to each selected node to obtain the value of the @id JSON-LD object.    ``` javascript    // input node  {""prop"":""value""}    // rule  {""type"":[""http://test.com/vocab/Type1"", ""http://test.com/vocab/Type2""]}    // output node  {""@type"":[""http://test.com/vocab/Type1"", ""http://test.com/vocab/Type2""],   ""prop"":""value""}    ```    ### @transform    Defines a generic transformation for a property of the selected nodes that will be applied to the initial value of the property to obtain the final value for that property in the transformed object.  The body of the rule must be a JSON object with a single key with the name of the property to transform and a value containing the array of function to apply to the initial value.    ### @remove    This rule can be used to delete properties of the selected nodes. Possible values are a single string with the name of the property to remove or an array of properties that will be removed.    ### @add    This rule can be used to add properties of the selected nodes. The value must be a object with the properties and values to be added to the node.    ### @only    Collects a set of properties from the selected nodes and delete the remaining properties. Possible values for the this rule body are a single property to select or an array witht the properties that must be collected.    ### @ns    This rule transforms the names of the properties in the selected nodes. The rule body consist of an object containgin functions that will be applied to the object property names to obtain the final properties. This rule is applied after all other rules have been applied. When referring to property names in other rules, the name of the property before applying this rule must be used.  Possible functions that can be used in the rule body are:    - 'ns:default': the value of this function is a default prefix that will be prepended to all the properties in the current node to transform them into CURIEs  - 'ns:append': accepts an object with prefixes as keys and a property name or array of property names as value. When applied, this function prepends the prefix to all the selected property names.  - 'ns:replace': Similar to 'ns:append', but instead of a prefix, it accepts as key fo the rule body object a string that will replace enterily the selected property names  - 'ns:omit': accepts a single string property name or an array of properties name that will be not affected by any other function in the rule body.    ## Functions    Functions are expressed as a single object or an array of JSON objects where each object contains the declaration of a function application that will be issued to the selected node.  Function applications contain the name of the function as the key of the object and a parameter as the value.  When an array of functions is declared, each function application will be applied consequtively, receiving as parameters the argument defined in the function application, the output of the previous function application in the array and the selected node where the tansformation is being applied. The first function in the chain will receive null as the input value.  New functions can be defined in the API declaration using a prefixed name for the functions and invoked in the body of rules.    A collection of functions are already available for transformations:    - 'f:valueof': selects the value of function argument in the context object and returns it.  - 'f:defaultvalue': sets a default value if the current value in the function application chain is null  - 'f:select': selects the value of the function argument in the input object and returns it.  - 'f:prefix': adds a prefix passed as the function argument and add it to the input object before returning it.  - 'f:urlencode': Performs URL encoding into the input object. The function argument is ignored.  - 'f:apply': Accepts a string of JavaScript code as the function argument, evaluates it and applies the resulting function to the input object. Evaluation is scoped with the input object using code like: (new Function('with(this) { return '+functionArgumentTexT+';}')).call(inputObject)  - 'f:basetemplateurl': Transforms a URL template with terminal variables into the base URL without the variables. e.g.: 'https://api.github.com/users/octocat/starred{/owner}{/repo}' becomes 'https://api.github.com/users/octocat/starred'    ## Null properties and function application exception    One main problem when applying transformations with null properties. Some object in the input data may have optional values, or the application of a function may return an unexpected null value. The library can react to this events in two different ways depending of the value of the 'behaviour' property. If the 'behaviour' property is set to the value 'loose', exceptions in the application of function chains will be catched and a null value will be returned as the result of the function chain application. Additionally, after transforming a node, properties with null values will be removed, including the '@id' property.    If the value of the 'behaviour' property is set to 'strict', exceptions will not be catched and final values of the transformations will be returned including null values.      ## Function declarations    Additional functions can be declared in the definition of a API using the '@declare' property. Function declarations accepts as the value of the '@declare' property a JSON object containing  pairs of CURIEs and function literals. for every prefix used in the curies, an additional property must map the prefix to the URI prefix.    The following code shows an example of how a function can be declared in an API definition:    ``` javascript        {        '@declare':        {          // 'test' is a prefix          'test': 'http://socialrdf.org/functions/',          // declaration of the 'test:f' function          'test:f': 'function(argument, input, obj){ return ""the ""+argument+"" ""+input }'        },          ""https://api.github.com/users/{username}\\n\         https://api.github.com/users/{username}/following/{other_user}"":        {           '$': {'@ns': {'ns:default': 'gh'},                 '@context': {'gh':'http://socialrdf.org/github/'},                 '@type': 'http://socialrdf.org/github/User',                 '@transform': {           	      'name': [{'f:valueof':'name'},                             // we can apply the declared function            	               {'test:f': 'user name:'}]                        }                }         }      }    ```    Function will receive three arguments, the function argument declared in the rule body, the input object from the previous function application and the context object.    ## JSON-LD Serialisation    To export the registered macros as a JSON-LD document, the ```toJSONLD``` function can be used. The output of the serialisation is a JSON-LD document that uses a small vocabulary to expose the macro.  The main properties in the vocabulary are:    - jldm:JsonLDMacro : class for all the JSON-LD macro descriptions  - jldm:uriTemplate : a URI template used to match the transformations of the macro.  - jldm:specification : a property pointing to each node transformation in the macro.  - jldm:Specification : class for all JSON-LD node transformations.  - jldm:transformation : a transformation for a sigle JSON node.  - jldm:Transformation : class for particular node transformations.  - jldm:ruleName : type of node transformation '@add', '@remove', '@id', etc.  - jldm:ruleBody : JSON encoded body for the described rule.    ``` javascript    macro.clearAPIs();    macro.registerAPI({  ""https://api.github.com/users/{username}"":    {https://api.github.com/users/{username}/commits/{sha1}"":    {'$': {'@ns': {'ns:default': 'gh'},  	   '@context': {'gh':'http://socialrdf.org/github/'},  	   '@type': 'http://socialrdf.org/github/Commit'}}  });    var jsonld = macro.toJSONLD();    ```  The output of the previous code is the following JSON-LD document:    ``` json  [    {      ""@type"": ""jldm:JsonLDMacro"",      ""@context"": {        ""jldm"": ""http://jsonld-macros.org/vocab#""      },      ""jldm:uriTemplate"": [        ""https://api.github.com/users/{username}/commits/{sha1}""      ],      ""jldm:specification"": [        {          ""@type"": ""jldm:Specification"",          ""jldm:transformation"": [            {              ""@type"": ""jldm:Transformation"",              ""jldm:ruleName"": ""@ns"",              ""jldm:ruleBody"": ""{\""ns:default\"":\""gh\""}""            },            {              ""@type"": ""jldm:Transformation"",              ""jldm:ruleName"": ""@context"",              ""jldm:ruleBody"": ""{\""gh\"":\""http://socialrdf.org/github/\""}""            },            {              ""@type"": ""jldm:Transformation"",              ""jldm:ruleName"": ""@type"",              ""jldm:ruleBody"": ""\""http://socialrdf.org/github/Commit\""""            }          ],          ""jldm:pathSelector"": ""$""        }      ]    }  ]    ```      ## JSON-LD De-serialisation    Macros exported as JSON-LD documents can be de-serialised using the ```fromJSONLD``` function. The function requires an instance of the [RDFStore-JS](https://github.com/antoniogarrote/rdfstore-js) module to work.  This module is not included with the library to not increase the size of the library. If you want to use this functionality, you need to include rdfstore-js as an additional dependency into your project.    ``` javascript  var rdfstore = require('rdfstore');  macro.fromJSONLD(rdfstore, jsonld, function(err, macro){    // macro can be used here.  });  ```      ## RDFStore-JS integration    One goal in the development of the library was to make it easier to consume non RDF APIs from web applications using [RDFStore-JS](https://github.com/antoniogarrote/rdfstore-js) in the data layer. Once a API has been registered in the library, an instance of RDFStore-JS can be wrapped using the *wrapRDFStoreJSNetworkTransport* function.The wrapped store instance will use then the library to transform JSON objects loaded by the store using the *load* or a SPARQL ""LOAD"" query, matching one the registered API service URIs templates.      ## Author an license    This library is released under the LGPL V3 license. Copyright, Antonio Garrote 2012.  If you have any problem you can find me at antoniogarrote@gmail.com """
Semantic web;https://github.com/zazuko/d3-sparql;"""# d3-sparql    This module lets you request data from [SPARQL](https://www.w3.org/TR/sparql11-query/) [endpoints](https://www.w3.org/wiki/SparqlEndpoints) in the vein of [d3-csv](https://github.com/d3/d3-dsv) and friends. It is generating a JSON structure from SPARQL query results, you can use that in any way you like in your code, with or without D3.    The access through a SPARQL endpoint allows a faster and more efficient data preparation (once you got [the hang of SPARQL and the RDF data model](https://www.youtube.com/watch?v=FvGndkpa4K0)). Ultimately it keeps visualizations up to date. Think of SPARQL endpoints as the most flexible API imaginable.    Define the SPARQL query and endpoint:    ```js  // Author of Q3011087 (D3.js)  var mikeQuery = `SELECT ?developerName WHERE {    wd:Q3011087 wdt:P178 ?developer.    ?developer rdfs:label ?developerName.    FILTER(LANG(?developerName) = 'en')  }`    wikidataUrl = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'  ```    To query the endpoint and get the result:    ```js  d3.sparql(wikidataUrl, mikeQuery).then((data) => {    console.log(data); // [{'developerName': 'Mike Bostock'}]  })  ```    More [examples](https://github.com/zazuko/d3-sparql/tree/master/examples) are provided in the [repository](https://github.com/zazuko/d3-sparql).    ## Features    - Transformation of [XSD Datatypes](https://www.w3.org/2011/rdf-wg/wiki/XSD_Datatypes) (e.g. `xsd:dateTime`, `xsd:boolean`, ...) to native JavaScript types.  - Reformatting of the JSON Structure to a d3 style layout while using the provided variables names of the SPARQL Query.    ## Limitations    - Only `SELECT` queries are supported. (This provides a projection of the graph data onto a table structure used by d3.)  - Currently only supports endpoints which are able to respond with `application/sparql-results+json`.    ## Installing    Using NPM: `npm install d3-sparql`. You can also use a CDN, for instance <https://www.jsdelivr.com>.    See [CHANGELOG](CHANGELOG.md) for details about available versions.    ## API Reference    This package adds a `sparql` function to the global `d3` object: `d3.sparql(endpoint, query, options = {})`.    <a name=""request"" href=""#sparql"">#</a> d3.<b> sparql </b>(<i>endpoint</i>, <i>query</i>[, <i>options = {}</i>]) [<>](https://github.com/zazuko/d3-sparql/blob/master/src/sparql.js#L8 ""Source"")    `options` is an optional object that will get merged with the second argument of [`fetch()`](https://developer.mozilla.org/en-US/docs/Web/API/WindowOrWorkerGlobalScope/fetch).    ```js  d3.sparql(endpoint, query)    .then((data) => …);  ```    ## Acknowledgement  The initial development of this library by [Zazuko](http://www.zazuko.com) was supported by the [City of Zürich](https://www.stadt-zuerich.ch/). """
Semantic web;https://github.com/simonjupp/java-skos-api;"""SKOS API  ========    The SKOS API provides a Java interface and OWL API based implementation of the Simple Knowledge Organization System (SKOS). SKOS is a W3C vocabulary for describing Knowledge Organization Systems (KOS) such as thesauri or concept schemes. For more information about SKOS see [here](http://www.w3.org/2004/02/skos/). An implementation of the SKOS API is provided using the [OWL API](http://owlcs.github.io/owlapi/).    The SKOS API is available in a single jar in [Releases](https://github.com/simonjupp/java-skos-api/releases)    Build from source  ------------    The SKOS API can be built using Apache Maven. This will create a jar in the distribution/target/ folder.    _mvn clean install_    Documentation  -------------    Please refer to [skosapi.sourceforge.net](http://skosapi.sourceforge.net) for documentation. Some examples are available in the skos-example module.    Support  -------    Please post all question to skos-dev@googlegroups.com     """
Semantic web;https://github.com/rubensworks/jsonld-streaming-parser.js;"""# JSON-LD Streaming Parser    [![Build status](https://github.com/rubensworks/jsonld-streaming-parser.js/workflows/CI/badge.svg)](https://github.com/rubensworks/jsonld-streaming-parser.js/actions?query=workflow%3ACI)  [![Coverage Status](https://coveralls.io/repos/github/rubensworks/jsonld-streaming-parser.js/badge.svg?branch=master)](https://coveralls.io/github/rubensworks/jsonld-streaming-parser.js?branch=master)  [![npm version](https://badge.fury.io/js/jsonld-streaming-parser.svg)](https://www.npmjs.com/package/jsonld-streaming-parser)    A fast and lightweight _streaming_ and 100% _spec-compliant_ [JSON-LD 1.1](https://json-ld.org/) parser,  with [RDFJS](https://github.com/rdfjs/representation-task-force/) representations of RDF terms, quads and triples.    The streaming nature allows triples to be emitted _as soon as possible_, and documents _larger than memory_ to be parsed.    Make sure to enable the `streamingProfile` flag when parsing a JSON-LD document with a streaming profile  to exploit the streaming capabilities of this parser, as this is disabled by default.    ## Installation    ```bash  $ npm install jsonld-streaming-parser  ```    or    ```bash  $ yarn add jsonld-streaming-parser  ```    This package also works out-of-the-box in browsers via tools such as [webpack](https://webpack.js.org/) and [browserify](http://browserify.org/).    ## Require    ```javascript  import {JsonLdParser} from ""jsonld-streaming-parser"";  ```    _or_    ```javascript  const JsonLdParser = require(""jsonld-streaming-parser"").JsonLdParser;  ```      ## Usage    `JsonLdParser` is a Node [Transform stream](https://nodejs.org/api/stream.html#stream_class_stream_transform)  that takes in chunks of JSON-LD data,  and outputs [RDFJS](http://rdf.js.org/)-compliant quads.    It can be used to [`pipe`](https://nodejs.org/api/stream.html#stream_readable_pipe_destination_options) streams to,  or you can write strings into the parser directly.    ### Print all parsed triples from a file to the console    ```javascript  const myParser = new JsonLdParser();    fs.createReadStream('myfile.jsonld')    .pipe(myParser)    .on('data', console.log)    .on('error', console.error)    .on('end', () => console.log('All triples were parsed!'));  ```    ### Manually write strings to the parser    ```javascript  const myParser = new JsonLdParser();    myParser    .on('data', console.log)    .on('error', console.error)    .on('end', () => console.log('All triples were parsed!'));    myParser.write('{');  myParser.write(`""@context"": ""https://schema.org/"",`);  myParser.write(`""@type"": ""Recipe"",`);  myParser.write(`""name"": ""Grandma's Holiday Apple Pie"",`);  myParser.write(`""aggregateRating"": {`);  myParser.write(`""@type"": ""AggregateRating"",`);  myParser.write(`""ratingValue"": ""4""`);  myParser.write(`}}`);  myParser.end();  ```    ### Import streams    This parser implements the RDFJS [Sink interface](https://rdf.js.org/#sink-interface),  which makes it possible to alternatively parse streams using the `import` method.    ```javascript  const myParser = new JsonLdParser();    const myTextStream = fs.createReadStream('myfile.jsonld');    myParser.import(myTextStream)    .on('data', console.log)    .on('error', console.error)    .on('end', () => console.log('All triples were parsed!'));  ```    ### Capture detected contexts    Using a `context` event listener,  you can collect all detected contexts.    ```javascript  const myParser = new JsonLdParser();    const myTextStream = fs.createReadStream('myfile.jsonld');    myParser.import(myTextStream)    .on('context', console.log)    .on('data', console.error)    .on('error', console.error)    .on('end', () => console.log('All triples were parsed!'));  ```    ### Parse from HTTP responses    Usually, JSON-LD is published via the `application/ld+json` media type.  However, when a JSON-LD context is attached via a link header,  then it can also be published via `application/json` and `+json` extension types.    This library exposes the `JsonLdParser.fromHttpResponse`  function to abstract these cases,  so that you can call it for any HTTP response,  and it will return an appropriate parser  which may or may not contain a custom header-defined context:    ```javascript  const myParser = JsonLdParser.fromHttpResponse(    'http://example.org/my-file.json', // For example: response.url    'application/json', // For example: headers.get('content-type')    new Headers({ 'Link': '<my-context.jsonld>; rel=\""http://www.w3.org/ns/json-ld#context\""' }), // Optional: WHATWG Headers     {}, // Optional: Any options you want to pass to the parser  );    // Parse anything with myParser like usual  const quads = myParser.import(response.body);  ```    The `Headers` object must implement the [Headers interface from the WHATWG Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch#Headers).    This function will automatically detect the `http://www.w3.org/ns/json-ld#streaming` profile and set the `streamingProfile` flag.    ## Configuration    Optionally, the following parameters can be set in the `JsonLdParser` constructor:    * `dataFactory`: A custom [RDFJS DataFactory](http://rdf.js.org/#datafactory-interface) to construct terms and triples. _(Default: `require('@rdfjs/data-model')`)_  * `context`: An optional root context to use while parsing. This can by anything that is accepted by [jsonld-context-parser](https://github.com/rubensworks/jsonld-context-parser.js), such as a URL, object or array. _(Default: `{}`)_  * `baseIRI`: An initial default base IRI. _(Default: `''`)_  * `streamingProfile`: If this parser can assume that parsed documents follow the streaming JSON-LD profile. If true, and a non-streaming document is detected, an error may be thrown. If false, non-streaming documents will be handled by preemptively buffering entries, which will lose many of the streaming benefits of this parser. _(Default: `true`)_  * `documentLoader` A custom loader for fetching remote contexts. This can be set to anything that implements [`IDocumentLoader`](https://github.com/rubensworks/jsonld-context-parser.js/blob/master/lib/IDocumentLoader.ts) _(Default: [`FetchDocumentLoader`](https://github.com/rubensworks/jsonld-context-parser.js/blob/master/lib/FetchDocumentLoader.ts))_  * `ignoreMissingContextLinkHeader`: If the lack of JSON-LD context link headers on raw JSON documents should NOT result in an error. If true, raw JSON documents can be considered first-class JSON-LD documents. _(Default: `false`)_  * `produceGeneralizedRdf`: If blank node predicates should be allowed, they will be ignored otherwise. _(Default: `false`)_  * `processingMode`: The maximum JSON-LD version that should be processable by this parser. _(Default: `1.1`)_  * `strictValues`: By default, JSON-LD requires that all properties (or @id's) that are not URIs, are unknown keywords, and do not occur in the context should be silently dropped. When setting this value to true, an error will be thrown when such properties occur. This also applies to invalid values such as language tags. This is useful for debugging JSON-LD documents. _(Default: `false`)_  * `allowSubjectList`: If RDF lists can appear in the subject position. _(Default: `false`)_  * `validateValueIndexes`: If @index inside array nodes should be validated. I.e., nodes inside the same array with the same @id, should have equal @index values. This is not applicable to this parser as we don't do explicit flattening, but it is required to be spec-compliant. _(Default: `false`)_  * `defaultGraph`: The default graph for constructing [quads](http://rdf.js.org/#dom-datafactory-quad). _(Default: `defaultGraph()`)_  * `rdfDirection`: The [mode](https://w3c.github.io/json-ld-api/#dom-jsonldoptions-rdfdirection) under which `@direction` should be handled. If undefined, `@direction` is ignored. Alternatively, it can be set to either `'i18n-datatype'` or `'compound-literal'` _(Default: `undefined`)_  * `normalizeLanguageTags`: Whether or not language tags should be normalized to lowercase. _(Default: `false` for JSON-LD 1.1 (and higher), `true` for JSON-LD 1.0)_  * `streamingProfileAllowOutOfOrderPlainType`: When the streaming profile flag is enabled, `@type` entries MUST come before other properties since they may defined a type-scoped context. However, when this flag is enabled, `@type` entries that do NOT define a type-scoped context may appear anywhere just like a regular property.. _(Default: `false`)_  * `skipContextValidation`: If JSON-LD context validation should be skipped. This is useful when parsing large contexts that are known to be valid. _(Default: `false`)_    ```javascript  new JsonLdParser({    dataFactory: require('@rdfjs/data-model'),    context: 'https://schema.org/',    baseIRI: 'http://example.org/',    streamingProfile: true,    documentLoader: new FetchDocumentLoader(),    ignoreMissingContextLinkHeader: false,    produceGeneralizedRdf: false,    processingMode: '1.0',    errorOnInvalidIris: false,    allowSubjectList: false,    validateValueIndexes: false,    defaultGraph: namedNode('http://example.org/graph'),    rdfDirection: 'i18n-datatype',    normalizeLanguageTags: true,  });  ```    ## How it works    This parser does _not_ follow the [recommended procedure for transforming JSON-LD to RDF](https://www.w3.org/TR/json-ld/#serializing-deserializing-rdf),  because this does not allow stream-based handling of JSON.  Instead, this tool introduces an alternative _streaming_ algorithm that achieves spec-compliant JSON-LD parsing.    This parser builds on top of the [jsonparse](https://www.npmjs.com/package/jsonparse) library,  which is a sax-based streaming JSON parser.  With this, several in-memory stacks are maintained.  These stacks are needed to accumulate the required information to emit triples/quads.  These stacks are deleted from the moment they are not needed anymore, to limit memory usage.    The algorithm makes a couple of (soft) assumptions regarding the structure of the JSON-LD document,  which is true for most typical JSON-LD documents.    * If there is a `@context`, it is the first entry of an object.  * If there is an `@id`, it comes right after `@context`, or is the first entry of an object.    If these assumptions are met, (almost) each object entry corresponds to a triple/quad that can be emitted.  For example, the following document allows a triple to be emitted after each object entry (except for first two lines):    ```  {    ""@context"": ""http://schema.org/"",    ""@id"": ""http://example.org/"",    ""@type"": ""Person"",               // --> <http://example.org/> a schema:Person.    ""name"": ""Jane Doe"",              // --> <http://example.org/> schema:name ""Jane Doe"".    ""jobTitle"": ""Professor"",         // --> <http://example.org/> schema:jobTitle ""Professor"".    ""telephone"": ""(425) 123-4567"",   // --> <http://example.org/> schema:telephone ""(425) 123-4567"".    ""url"": ""http://www.janedoe.com""  // --> <http://example.org/> schema:url <http://www.janedoe.com>.  }  ```    If not all of these assumptions are met, entries of an object are buffered until enough information becomes available, or if the object is closed.  For example, if no `@id` was present, values will be buffered until an `@id` is read, or if the object closed.    For example:    ```  {    ""@context"": ""http://schema.org/"",    ""@type"": ""Person"",    ""name"": ""Jane Doe"",    ""jobTitle"": ""Professor"",    ""@id"": ""http://example.org/"",    // --> <http://example.org/> a schema:Person.                                     // --> <http://example.org/> schema:name ""Jane Doe"".                                     // --> <http://example.org/> schema:jobTitle ""Professor"".    ""telephone"": ""(425) 123-4567"",   // --> <http://example.org/> schema:telephone ""(425) 123-4567"".    ""url"": ""http://www.janedoe.com""  // --> <http://example.org/> schema:url <http://www.janedoe.com>.  }  ```    As such, JSON-LD documents that meet these requirements will be parsed very efficiently.  Other documents will still be parsed correctly as well, with a slightly lower efficiency.    ## Streaming Profile    This parser adheres to both the [JSON-LD 1.1](https://www.w3.org/TR/json-ld/) specification  and the [JSON-LD 1.1 Streaming specification](https://w3c.github.io/json-ld-streaming/).    By default, this parser assumes that JSON-LD document  are *not* in the [streaming document form](https://w3c.github.io/json-ld-streaming/#streaming-document-form).  This means that the parser may buffer large parts of the document before quads are produced,  to make sure that the document is interpreted correctly.    Since this buffering neglects the streaming benefits of this parser,  the `streamingProfile` flag *should* be enabled when a [streaming JSON-LD document](https://w3c.github.io/json-ld-streaming/#streaming-document-form)  is being parsed.    If non-streaming JSON-LD documents are encountered when the `streamingProfile` flag is enabled,  an error may be thrown.    ## Specification compliance    This parser implements the following [JSON-LD specifications](https://json-ld.org/test-suite/):    * JSON-LD 1.1 - Transform JSON-LD to RDF  * JSON-LD 1.1 - Error handling  * JSON-LD 1.1 - Streaming Transform JSON-LD to RDF    ## Performance    The following table shows some simple performance comparisons between JSON-LD Streaming Parser and [jsonld.js](https://www.npmjs.com/package/jsonld).    These basic experiments show that even though streaming parsers are typically significantly slower than regular parsers,  _JSON-LD Streaming Parser still achieves similar performance as jsonld.js_ for most typical JSON-LD files.  However, for expanded JSON-LD documents, JSON-LD Streaming Parser is around 3~4 times slower.    | File       | **JSON-LD Streaming Parser** | **jsonld.js** |  | ---------- | ---------------------------- | ------------- |  | [`toRdf-manifest.jsonld`](https://json-ld.org/test-suite/tests/toRdf-manifest.jsonld) (999 triples) | 683.964ms (38MB) | 708.975ms (40MB) |  | [`sparql-init.json`](https://raw.githubusercontent.com/comunica/comunica/master/packages/actor-init-sparql/config/sets/sparql-init.json) (69 triples) | 931.698ms (40MB) | 1088.607ms (47MB) |  | [`person.json`](https://json-ld.org/playground/) (5 triples) | 309.419ms (30MB) | 313.138ms (41MB) |  | `dbpedia-10000-expanded.json` (10,000 triples) | 785.557ms (70MB) | 202.363ms (62MB) |    Tested files:    * `toRdf-manifest.jsonld`: The JSON-LD toRdf test manifest. A typical JSON-LD file with a single context.  * `sparql-init.json`: A [Comunica](https://github.com/comunica/comunica) configuration file. A JSON-LD file with a large number of complex, nested, and remote contexts.  * `person.jsonld`: A very small JSON-LD example from the [JSON-LD playground](https://json-ld.org/playground/).  * `dbpedia-10000-expanded.json` First 10000 triples of DBpedia in expanded JSON-LD.    [Code for measurements](https://github.com/rubensworks/jsonld-streaming-parser.js/tree/master/perf)    ## License  This software is written by [Ruben Taelman](http://rubensworks.net/).    This code is released under the [MIT license](http://opensource.org/licenses/MIT). """
Semantic web;https://github.com/spy16/fabric;"""> WIP    # Fabric      [![GoDoc](https://godoc.org/github.com/spy16/fabric?status.svg)](https://godoc.org/github.com/spy16/fabric) [![Go Report Card](https://goreportcard.com/badge/github.com/spy16/fabric)](https://goreportcard.com/report/github.com/spy16/fabric)    Fabric is a triple-store written in `Go`. Fabric provides simple functions  and store options to deal with ""Subject->Predicate->Object"" relations or so called  triples.    ## Usage    Get fabric by using `go get -u github.com/spy16/fabric` (Fabric as a library has no external dependencies)    ```go  mem := &fabric.InMemoryStore{}    fab := fabric.New(mem)    fab.Insert(context.Background(), fabric.Triple{      Source: ""Bob"",      Predicate: ""Knows"",      Target: ""John"",  })    fab.Query(context.Background(), fabric.Query{      Source: fabric.Clause{          Type: ""equal"",          Value: ""Bob"",      },  })  ```    To use a SQL database for storing the triples, use the following snippet:    ```go  db, err := sql.Open(""sqlite3"", ""fabric.db"")  if err != nil {      panic(err)  }    store := &fabric.SQLStore{      DB: db,  }  store.Setup(context.Background()) // to create required tables    fab := fabric.New(store)  ```    > Fabric `SQLStore` uses Go's standard `database/sql` package. So any SQL database supported  > through this interface (includes most major SQL databases) should work.    Additional store support can be added by implementing the `Store` interface.    ```go  type Store interface {  	Insert(ctx context.Context, tri Triple) error  	Query(ctx context.Context, q Query) ([]Triple, error)  	Delete(ctx context.Context, q Query) (int, error)  }  ```    Optional `Counter` and `ReWeighter` can be implemented by the store implementations  to support extended query options. """
Semantic web;https://github.com/mommi84/Mandolin;"""![logo](https://github.com/mommi84/Mandolin/raw/master/mandolin-400px.png ""Mandolin logo"")    MANDOLIN  ========    *Markov Logic Networks for the Discovery of Links.*    ## Requirements    * Java 1.8+  * PostgreSQL 9.4.x  * Gurobi solver  * Maven  * Wget, Unzip    ## Quick start    * Download and decompress [Mandolin v0.4.0-alpha](https://github.com/mommi84/Mandolin/releases/download/v0.4.0-alpha/mandolin-binaries-v0.4.0-alpha.zip)  * Run `bash install.sh`    ## Experiments    The following command will discover new links of any predicate (`--aim`) on the WordNet dataset (`--input`) with mining threshold 0.8 (`--mining`) and 1 million Gibbs sampling iterations (`--sampling`).    ```bash  java -Xmx1g -jar target/Mandolin-0.4.0-jar-with-dependencies.jar plain --input data/benchmark/wn18/wordnet-mlj12-train.nt,data/benchmark/wn18/wordnet-mlj12-valid.nt --output eval/wn18 --mining 0.8 --sampling 1000000 --aim ""*""  ```    Discovered links can be found in the `--output` folder at `./eval/wn18/discovered_X.nt`, where `X` is the output threshold, meaning that a file contains all links whose confidence is greater or equal than `X`.    An excerpt of the discovered **rules and weights**:    ```text  0.990517419  wn18:_part_of(b, a) => wn18:_has_part(a, b)  0.862068966  wn18:_instance_hypernym(a, c) AND wn18:_synset_domain_topic_of(f, b) => wn18:_synset_domain_topic_of(a, b)  ```    An excerpt of the discovered **links** with confidence > 0.9:    ```text  wn18:08131530 wn18:_has_part wn18:08132046 .  wn18:09189411 wn18:_has_part wn18:08707917 .  wn18:10484858 wn18:_synset_domain_topic_of wn18:08441203 .  wn18:01941987 wn18:_synset_domain_topic_of wn18:00300441 .  ```    ### Basic documentation    Mandolin can be launched as follows.    ```bash  java -Xmx1g -jar target/Mandolin-0.4.0-jar-with-dependencies.jar <GOAL> <PARAMETERS>  ```    #### Goals    **Goal**|**Description**  :-----|:-----  `plain`|Launch a plain Mandolin execution.  `eval`|Evaluate MRR and hits@k.    #### Plain execution    Parameters for `plain` goal:    **Parameter**|**Description**|**Example value**  :-----|:-----|:-----  `--input`|Comma-separated N-Triple files.|`data1.nt,data2.nt`  `--output`|Workspace and output folder.|`eval/experiment1`  `--aim`|Aim predicate. For all predicates use wildcard `*`.|`http://www.w3.org/2002/07/owl#sameAs`  `--mining`|Rule mining threshold.|`0.9` (default: `0.0` support)  `--sampling`|Gibbs sampling iterations.|`1000000` (default: 100 x evidence size)  `--rules`|Maximum number of rules.|`1500` (default: none)  `--sim`|Enable similarity among literals as `min,step,max`.|`0.8,0.1,0.9` (default: none)  `--onto`|Enable ontology import.|`true` (default: `false`)  `--fwc`|Enable forward-chain.|`true` (default: `false`)    #### Evaluation    The `eval` goal takes two parameters: the N-Triples file of the test set and Mandolin's output directory.    Example run:    ```bash  java -Xmx1g -jar target/Mandolin-0.4.0-jar-with-dependencies.jar eval data/benchmark/wn18/wordnet-mlj12-test.nt eval/wn18  ```    ## Manual install    * Clone project:    ```bash  git clone https://github.com/mommi84/Mandolin.git  cd Mandolin  ```    * Get PostgreSQL 9.4.x - [Ubuntu/Debian binaries](http://oscg-downloads.s3.amazonaws.com/packages/postgresql-9.4.8-1-x64-bigsql.deb)    ### Alternative 1    * Launch `bash install.sh -c`    ### Alternative 2    * Insert PostgreSQL setting parameters into a file `./mandolin.properties`. Example:    ```properties  # GENERAL CONFIGURATION FOR MANDOLIN  pgsql_home=/usr/local/Cellar/postgresql/9.4.1  pgsql_username=tom  pgsql_password=  pgsql_url=localhost  ```    * Download [data](https://s3-eu-west-1.amazonaws.com/anonymous-folder/data.zip)    * Compile project:    ```bash  export MAVEN_OPTS=-Xss4m  mvn clean compile assembly:single  ```    ## Database handler    After using Mandolin, stop the DB instance with:    ```bash  sh pgsql-stop.sh  ```    The instance can be restarted with:    ```bash  sh pgsql-start.sh  ```    ## License(s)    **Mandolin** is licensed under GNU General Public License v2.0.  **AMIE** is licensed under Creative Commons Attribution-NonComercial license v3.0.  **ProbKB** is licensed under the BSD license.  **RockIt** is licensed under the MIT License.  **Gurobi** can be activated using a [free academic license](http://www.gurobi.com/academia/academia-center). """
Semantic web;https://github.com/jsonresume/resume-schema;"""# JSON Resume Schema    [![GitHub Releases](https://badgen.net/github/tag/jsonresume/resume-schema)](https://github.com/jsonresume/resume-schema/releases)  [![NPM Release](https://badgen.net/npm/v/resume-schema)](https://www.npmjs.com/package/resume-schema)  [![Latest Status](https://github.com/jsonresume/resume-schema/workflows/Latest/badge.svg)](https://github.com/vanillawc/wc-template/actions)  [![Release Status](https://github.com/jsonresume/resume-schema/workflows/Release/badge.svg)](https://github.com/vanillawc/wc-template/actions)    Standard, Specification, Schema    ### Getting started    ```  npm install --save resume-schema  ```    To use    ```js  const resumeSchema = require(""resume-schema"");  resumeSchema.validate(    { name: ""Thomas"" },    function (err, report) {      if (err) {        console.error(""The resume was invalid:"", err);        return;      }      console.log(""Resume validated successfully:"", report);    },    function (err) {      console.error(""The resume was invalid:"", err);    }  );  ```    More likely    ```js  var fs = require(""fs"");  var resumeSchema = require(""resume-schema"");  var resumeObject = JSON.parse(fs.readFileSync(""resume.json"", ""utf8""));  resumeSchema.validate(resumeObject);  ```    The JSON Resume schema is available from:    ```js  require(""resume-schema"").schema;  ```    ### Contribute    We encourage anyone who's interested in participating in the formation of this standard to join the discussions [here on GitHub](https://github.com/jsonresume/resume-schema/issues). Also feel free to fork this project and submit new ideas to add to the JSON Resume Schema standard. To make sure all formatting is kept in check, please install the [EditorConfig plugin](http://editorconfig.org/) for your editor of choice.    ### Versioning    JSON Resume Schema adheres to Semantic Versioning 2.0.0. If there is a violation of  this scheme, report it as a bug. Specifically, if a patch or minor version is  released and breaks backward compatibility, that version should be immediately  yanked and/or a new version should be immediately released that restores  compatibility. Any change that breaks the public API will only be introduced at  a major-version release. As a result of this policy, you can (and should)  specify any dependency on JSON Resume Schema by using the Pessimistic Version  Constraint with two digits of precision.    We use automatic semver system.    Pull requests titles should be formatted as such    ```  ""fix: added something"" - will bump the patch version  ""feat: added something"" - will bump the minor version  ```    `major` version bumps will be few and far between for this schema.    ### Other resume standards    - [HR-XML](https://schemas.liquid-technologies.com/HR-XML/2007-04-15/)  - [Europass](http://europass.cedefop.europa.eu/about-europass) """
Semantic web;https://github.com/carml/carml;"""  <p align=""center"">  <img src=""https://raw.githubusercontent.com/carml/carml.github.io/master/carml-logo.png"" height=""100"" alt=""carml"">  </p>    CARML  =====================  **A pretty sweet RML engine**    CARML was first developed by [Taxonic](http://www.taxonic.com) in cooperation with [Kadaster](https://www.kadaster.com/). And is now being maintained and developed further by [Skemu](https://skemu.com).      [![Build Status](https://api.travis-ci.org/carml/carml.svg?branch=master)](https://travis-ci.org/carml/carml)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.taxonic.carml/carml/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.taxonic.carml/carml)    Table of Contents  -----------------  - [CARML](#carml)    - [Table of Contents](#table-of-contents)    - [Releases](#releases)    - [Introduction](#introduction)    - [Getting started](#getting-started)    - [Validating your RML mapping](#validating-your-rml-mapping)    - [Input stream extension](#input-stream-extension)    - [Function extension](#function-extension)    - [XML namespace extension](#xml-namespace-extension)    - [Supported Data Source Types](#supported-data-source-types)    - [CARML in RML Test Cases](#carml-in-rml-test-cases)    - [Roadmap](#roadmap)    Releases  ----  20 Sep 2017 - CARML 0.0.1    21 Oct 2017 - CARML 0.1.0    05 Dec 2017 - CARML 0.1.1    12 Feb 2018 - CARML 0.1.2    20 May 2018 - CARML 0.2.0    03 Aug 2018 - CARML 0.2.1    13 Nov 2018 - CARML 0.2.2    17 Nov 2018 - CARML 0.2.3    11 Sep 2020 - CARML 0.3.0    31 Oct 2020 - CARML 0.3.1    22 Mar 2021 - CARML 0.3.2    Introduction  ------------  CARML is a java library that transforms structured sources to RDF based as declared in and [RML](http://rml.io) mapping, in accordance with the [RML spec](http://rml.io/spec.html).    The best place to start learning about RML is at the [source](http://rml.io), but basically  RML is defined as a superset of [R2RML](https://www.w3.org/TR/r2rml/) which is a W3C recommendation that describes a language for expressing mappings from relational databases to RDF datasets. RML allows not only the expression of mappings for relational databases, but generalizes this to any structured source. All you need is a way to iterate over and query the source.    Getting started  ---------------    CARML is available from the Central Maven Repository.    ```xml  <dependency>      <groupId>com.taxonic.carml</groupId>      <artifactId>carml-engine</artifactId>      <version>0.3.2</version>  </dependency>    <!-- Choose the resolvers to suit your need -->  <dependency>    <groupId>com.taxonic.carml</groupId>    <artifactId>carml-logical-source-resolver-jsonpath</artifactId>    <version>0.3.2</version>  </dependency>  <dependency>    <groupId>com.taxonic.carml</groupId>    <artifactId>carml-logical-source-resolver-xpath</artifactId>    <version>0.3.2</version>  </dependency>  <dependency>    <groupId>com.taxonic.carml</groupId>    <artifactId>carml-logical-source-resolver-csv</artifactId>    <version>0.3.2</version>  </dependency>    ```    CARML is built on [RDF4J](http://rdf4j.org/), and currently the Mapper directly outputs an [RDF4J Model](http://docs.rdf4j.org/javadoc/2.0/org/eclipse/rdf4j/model/package-summary.html).    ```java  Set<TriplesMap> mapping =    RmlMappingLoader      .build()      .load(RDFFormat.TURTLE, Paths.get(""path-to-mapping-file""));    RmlMapper mapper =    RmlMapper      .newBuilder()      // Add the resolvers to suit your need      .setLogicalSourceResolver(Rdf.Ql.JsonPath, new JsonPathResolver())      .setLogicalSourceResolver(Rdf.Ql.XPath, new XPathResolver())      .setLogicalSourceResolver(Rdf.Ql.Csv, new CsvResolver())        //-- optional: --        // specify IRI unicode normalization form (default = NFC)        // see http://www.unicode.org/unicode/reports/tr15/tr15-23.html      .iriUnicodeNormalization(Form.NFKC)        // set file directory for sources in mapping      .fileResolver(""/some/dir/"")        // set classpath basepath for sources in mapping      .classPathResolver(""some/path"")        // specify casing of hex numbers in IRI percent encoding (default = true)        // added for backwards compatibility with IRI encoding up until v0.2.3      .iriUpperCasePercentEncoding(false)      //---------------        .build();    Model result = mapper.map(mapping);  ```    Validating your RML mapping  ---------------------------  We're not set up for full mapping validation yet. But, to help you get those first nasty mapping errors out, we've created a [SHACL](https://www.w3.org/TR/shacl/) shapes graph ([here](https://github.com/carml/carml/tree/master/rml.sh.ttl)) that validates RML mappings. You can use the [SHACL playground](http://shacl.org/playground/) to easily test your mapping.    Input stream extension  ---------------------  When it comes to non-database sources, the current RML spec only supports the specification of file based sources in an `rml:LogicalSource`. However, it is often very useful to be able to transform stream sources.    To this end CARML introduces the notion of 'Named Streams'.  Which follows the ontology defined [here](https://github.com/carml/carml/tree/master/carml.ttl).    So now, you can define streams in your mapping like so:  ```  :SomeLogicalSource    rml:source [      a carml:Stream ;      # NOTE: name is not mandatory and can be left unspecified, when working with a single stream      carml:streamName ""stream-A"" ;    ];    rml:referenceFormulation ql:JSONPath;    rml:iterator ""$""  .  ```  In order to provide access to the input stream, it needs to be registered on the mapper.  ```java  RmlMapper mapper =    RmlMapper    .newBuilder()    .setLogicalSourceResolver(Rdf.Ql.JsonPath, new JsonPathResolver())    .build();  mapper.bindInputStream(""stream-A"", inputStream);  ```  Note that it is possible to register several streams, allowing you to combine several streams to create your desired RDF output.    Function extension  ------------------  A recent development related to RML is the possibility of adding functions to the mix. This is a powerful extension that increases the expressivity of mappings immensely. Do note that the function ontology is still under development at UGhent.    Because we believe that this extension can already be of great value, we've already adopted it in CARML.    <!--- TODO: explain that the function execution is a finisher, that is it runs the normal mapping, which creates the function execution triples, and the described execution is in turn evaluated and results in the term map value. --->  The way it works is, you describe the execution of a function in terms of the [Function Ontology (FnO)](https://fno.io/).    Take for example the SumFunction example of the [FnO spec](http://users.ugent.be/~bjdmeest/function/#complete-example). This defines an instance `ex:sumFunction` of class `fno:Function` that is able to compute the sum of two values provided as parameters of the function at execution time.    It also describes an instance `ex:sumExecution` of `fno:execution`, which `fno:executes` `ex:sumFunction` which descibes an instance of an execution of the defined sum function. In this case with parameters 2 and 4.    To be able to use this in RML mappings we use executions of instances of `fno:Function` to determine the value of a term map. The execution of a function can be seen as a post-processing step in the evaluation of a term map.    ```  @prefix rr: <http://www.w3.org/ns/r2rml#> .  @prefix rml: <http://semweb.mmlab.be/ns/rml#> .  @prefix fnml: <http://semweb.mmlab.be/ns/fnml#> .  @prefix xsd: <http://www.w3.org/2001/XMLSchema#> .  @prefix fno: <https://w3id.org/function/ontology#> .  @prefix ex: <http://example.org/> .    ex:sumValuePredicateObjectMap    rr:predicate ex:total ;    rr:objectMap [      a fnml:FunctionMap ;      fnml:functionValue [        rml:logicalSource ex:LogicalSource ;        rr:subjectMap [          rr:template ""functionExec"";          rr:termType rr:BlankNode ;          rr:class fno:Execution        ] ;        rr:predicateObjectMap          [            rr:predicate fno:executes ;            rr:objectMap [              rr:constant ex:sumFunction ;            ]          ] ,          [            rr:predicate ex:intParameterA ;            rr:objectMap [ rml:reference ""foo"" ]          ] ,          [            rr:predicate ex:intParameterB  ;            rr:objectMap [ rml:reference ""bar"" ]          ]      ] ;      rr:datatype xsd:integer ;    ]  .  ```    A function can be created in any `.java` class. The function should be annotated with `@FnoFunction`, providing the string value of the function IRI, and the function parameters with `@FnoParam`, providing the string value of the function parameter IRIs.    ```java  public class RmlFunctions {      @FnoFunction(""http://example.org/sumFunction"")    public int sumFunction(      @FnoParam(""http://example.org/intParameterA"") int intA,      @FnoParam(""http://example.org/intParameterB"") int intB      ) {        return intA + intB;      }    }  ```    The class or classes containing the annotated functions can then be registered on the mapper via the `RmlMapper#addFunctions` method.    ```java  RmlMapper mapper =    RmlMapper      .newBuilder()      .setLogicalSourceResolver(Rdf.Ql.JsonPath, new JsonPathResolver())      .addFunctions(new YourRmlFunctions())      .build();  Model result = mapper.map(mapping);  ```    It is recommended to describe and publish new functions in terms of FnO for interpretability of mappings, and, possibly, reuse of functions, but it's not mandatory for use in CARML.    Note that it is currently possible to specify and use function executions as parameters of other function executions in CARML, although this is not (yet?) expressible in FnO.    XML namespace extension  -----------------------    When working with XML documents, it is often necessary specify namespaces to identify a node's qualified name.  Most XPath implementations allow you to register these namespaces, in order to be able to use them in executing XPath expressions.  In order to convey these expressions to the CARML engine, CARML introduces the class `carml:XmlDocument` that can be used as a value of `rml:source`. An instance of  `carml:XmlDocument` can, if it is a file source, specify a location via the `carml:url` property, and specify namespace declarations via the `carml:declaresNamespace` property.    For example, given the following XML document:  ```xml  <?xml version=""1.0"" encoding=""UTF-8""?>  <ex:bookstore xmlns:ex=""http://www.example.com/books/1.0/"">    <ex:book category=""children"">      <ex:title lang=""en"">Harry Potter</ex:title>      <ex:author>J K. Rowling</ex:author>      <ex:year>2005</ex:year>      <ex:price>29.99</ex:price>    </ex:book>  </ex:bookstore>  ```    one can now use the following mapping, declaring namespaces, to use them in XPath expressions:  ```  @prefix rr: <http://www.w3.org/ns/r2rml#>.  @prefix rml: <http://semweb.mmlab.be/ns/rml#>.  @prefix ql: <http://semweb.mmlab.be/ns/ql#> .  @prefix carml: <http://carml.taxonic.com/carml/> .  @prefix ex: <http://www.example.com/> .    <#SubjectMapping> a rr:TriplesMap ;    rml:logicalSource [      rml:source [        a carml:Stream ;        # or in case of a file source use:        # carml:url ""path-to-source"" ;        carml:declaresNamespace [          carml:namespacePrefix ""ex"" ;          carml:namespaceName ""http://www.example.com/books/1.0/"" ;        ] ;      ] ;      rml:referenceFormulation ql:XPath ;      rml:iterator ""/ex:bookstore/*"" ;    ] ;      rr:subjectMap [      rr:template ""http://www.example.com/{./ex:title}"" ;      rr:class ex:Book ;      rr:termType rr:IRI ;    ] ;  .    ```    which yields:  ```  <http://www.example.com/Harry%20Potter> a <http://www.example.com/Book> .  ```    Supported Data Source Types  ---------------------------    | Data source type | Reference query language                           | Implementation                                                      |  | :--------------- | :------------------------------------------------- | :-------------------------------------------------------------      |  | JSON             | [JsonPath](http://goessner.net/articles/JsonPath/) | [Jayway JsonPath 2.4.0](https://github.com/json-path/JsonPath)      |  | XML              | [XPath](https://www.w3.org/TR/xpath-31/)           | [Saxon-HE 10.2](http://saxon.sourceforge.net/#F10HE)           |  | CSV              | n/a                                                | [Univocity 2.9.0](https://github.com/uniVocity/univocity-parsers)   |      CARML in RML Test Cases  -----------------------  See the [RML implementation Report](https://rml.io/implementation-report/) for how CARML does in the [RML test cases](https://rml.io/test-cases/).    > Note: currently we've raised [issues](https://github.com/RMLio/rml-test-cases/issues?q=is%3Aissue+author%3Apmaria+) for for some of the test cases which we believe are incorrect, or have an adverse effect on mapping data.    Roadmap  -------  * Better support for large sources  * Improved join / parent triples map performance  * Support RDF store connections  * Split off provisional RDF Mapper as separate library """
Semantic web;https://github.com/wikibus/rdf.vocabularies;"""# Rdf.Vocabularies [![Build status](https://ci.appveyor.com/api/projects/status/8uepsle8g54v101m/branch/master?svg=true)](https://ci.appveyor.com/project/tpluscode78631/rdf-vocabularies/branch/master) [![NuGet version](https://badge.fury.io/nu/rdf.vocabularies.svg)](https://badge.fury.io/nu/rdf.vocabularies) [![codefactor][codefactor-badge]][codefactor-link]     ## What     Statically typed RDF Vocabularies for .NET. Most generated automatically from respective RDF/OWL files:    * `bibo` - [Bibliographic Ontology](http://lov.okfn.org/dataset/lov/vocabs/bibo)  * `dc` - [Dublin Core Metadata Element Set](http://lov.okfn.org/dataset/lov/vocabs/dce)  * `dcterms` - [DCMI Metadata Terms](http://lov.okfn.org/dataset/lov/vocabs/dcterms)  * `foaf` - [Friend of a Friend Vocabulary](http://lov.okfn.org/dataset/lov/vocabs/foaf)  * `Hydra` - [A lightweight vocabulary for hypermedia-driven Web APIs](http://www.hydra-cg.com/)  * `opus` - [Ontology of Computer Science Publications](http://lov.okfn.org/dataset/lov/vocabs/opus)  * `owl` - [Ontology Web Language](http://lov.okfn.org/dataset/lov/vocabs/owl)  * `rdf` - [The RDF Concepts Vocabulary](http://lov.okfn.org/dataset/lov/vocabs/rdf)  * `rdfs` - [The RDF Schema vocabulary](http://lov.okfn.org/dataset/lov/vocabs/rdfs)  * `schema` - [Schema.org vocabulary](http://lov.okfn.org/dataset/lov/vocabs/schema)  * `sioc` - [Semantically-Interlinked Online Communities](http://lov.okfn.org/dataset/lov/vocabs/sioc)  * `skos` - [Simple Knowledge Organization System](http://lov.okfn.org/dataset/lov/vocabs/skos)    ## How    Install NuGet    ```  install-package Rdf.Vocabularies  ```    Use    ``` csharp  using Vocab;    var rdfsLabel = Rdfs.label;  ```    [codefactor-badge]: https://www.codefactor.io/repository/github/wikibus/Rdf.Vocabularies/badge/master  [codefactor-link]: https://www.codefactor.io/repository/github/wikibus/Rdf.Vocabularies/overview/master """
Semantic web;https://github.com/joshsh/rdfagents;"""RDFAgents is a messaging protocol for real-time, peer-to-peer knowledge sharing on the Semantic Web. It patterned after, and designed to interoperate with Linked Data, but extends Linked Data principles to support a ""word of mouth"" style of information discovery over a variety of communications protocols. It is geared towards ubiquitous Semantic Web computing, lightweight devices with variable network connectivity, and highly reactive, event-driven interfaces to Semantic Web services.    Read more in the [RDFAgents specification](http://fortytwo.net/2011/rdfagents/spec).   """
Semantic web;https://github.com/ML-Schema/core;"""# ML Schema    ML-Schema is a collaborative, community effort with a mission to develop, maintain, and promote standard schemas for data mining and machine learning algorithms, datasets, and experiment    See the wiki for more details: https://github.com/ML-Schema/core/wiki    **Guidelines**  * Open GitHub issues for any kind of discussion. Selected items will be discussed during telco's. For instance, see how this works for schema.org  * Store all schema in http://www.w3.org/TR/turtle/. This you can load in Protege, Jena,... if you want to edit them locally.  * Use the GitHub Wiki for information relevant to the group and users: https://github.com/ML-Schema/core/wiki  * Include structure (inheritance, value bounds) so arrive at a non-ambiguous schema.  * We use namespace 'mls' and will register this at PURL asap.  * If there is an issue to discuss, first create a GitHub issue, and then label it as 'discuss in call'  Conference calls    **Conference calls**    We do a conference call on Hangout every two weeks, on Monday 13:30 - 14:30 (European time zone)  * Next calls 2015: Nov 9, Nov 23, Dec 7, Dec 21  * Next calls 2016: Jan 4, Jan 18, Feb 1, ...    **Google group**    To receive updates about ML Schema, or contact everybody, please join our Google group: https://groups.google.com/forum/#!forum/mlw3c    When you join, please send a short introduction of yourself, e.g. who are you, what is your interest in ML schema, what would you like to contribute,... """
Semantic web;https://github.com/garlik/4store;"""# 4store    4store is an efficient, scalable and stable RDF database.    4store was designed by Steve Harris and developed at Garlik to underpin their  Semantic Web applications. It has been providing the base platform for around 3  years. At times holding and running queries over databases of 15GT, supporting a  Web application used by thousands of people.    ## Getting started    In this section:    1. Installing prerequisites.  2. Installing 4store.  3. Running 4store.  4. Installing frontend tools only.  5. Other installation hints.    ### Installing prerequisites    To install [Raptor](https://github.com/dajobe/raptor) (RDF parser) and  [Rasqal](https://github.com/dajobe/rasqal) (SPARQL parser):        # install a 64-bit raptor from freshly extracted source      ./configure --libdir=/usr/local/lib64 && make      sudo make install        # similarly for 64-bit rasqal      ./configure ""--enable-query-languages=laqrs sparql rdql"" \       --libdir=/usr/local/lib64 && make      sudo make install        # ensure PKG_CONFIG_PATH is set correctly      # ensure /etc/ld.so.conf.d/ includes /usr/local/lib64      sudo ldconfig    ### Installing 4store        ./autogen.sh      ./configure      make      make install    ### Running 4store        /usr/local/bin/4s-boss -D    ### Installing frontend tools only    To install just the frontend tools on non-cluster frontends:        # pre-requisites for installing the frontend tools      yum install pcre-devel avahi avahi-tools avahi-devel        # src/common      (cd src/common && make)        # src/frontend      (cd src/frontend && make && make install)      ### Other installation hints    Make sure `/var/lib/4store/` exists (in a cluster, it only needs to exist on  backend nodes) and that the user or users who will create new KBs have  permission to write to this directory.    For clusters (or to test cluster tools on a single machine) the frontend must  have a file `/etc/4s-cluster`, which lists all machines in the cluster.      To avoid problems running out of Avahi DBUS connections, modify  `/etc/dbus-1/system.d/avahi-dbus.conf` to:    * Increase `max_connections_per_user` to 200 or so  * Increase `max_match_rules_per_connection` to 512 or so (optional) """
Semantic web;https://github.com/wbsg/ldif;"""LDIF  ====    LDIF translates heterogeneous Linked Data from the Web into a clean, local target representation while keeping track of data provenance.    #### Get Started    To see LDIF in action, please follow these steps:    1. [Download](http://dl.mes-semantics.com/ldif/ldif-0.5.2.zip) the latest release  2. Unzip the archive and change into the extracted directory `ldif-0.5.2`  3. Run LDIF on the Music example            bin/ldif examples/music/light/schedulerConfig.xml        4. Monitor the progress of the scheduled jobs through the status interface, at [http://localhost:5343](http://localhost:5343)  5. Integration results will be written in the working directory, into `integrated_music_light.nq`      #### Learn More    Learn more about LDIF at [http://ldif.wbsg.de/](http://ldif.wbsg.de/).    #### Feedback    For questions and feedback please use the [LDIF Google Group](http://groups.google.com/group/ldif). """
Semantic web;https://github.com/cumulusrdf/cumulusrdf;"""<img src=""https://cloud.githubusercontent.com/assets/7569632/9528013/c54e198e-4cf2-11e5-864a-eb1f92433ee4.png"" width=""120"" height=""120""/>  <img src=""https://cloud.githubusercontent.com/assets/7569632/9528225/cbd9ae52-4cf3-11e5-9993-bf8c56dff5e1.png"" width=""500"" height=""120""/>        **CumulusRDF** (see Wikipedia[1] for details on ""Cumulus"") is an RDF store on a cloud-based architecture. CumulusRDF provides a REST-based API with CRUD operations to manage RDF data. The current version uses Apache Cassandra as storage backend.        To use CumulusRDF for your project, see the [GettingStarted](https://github.com/cumulusrdf/cumulusrdf/wiki/GettingStarted) wiki page!        ## Features  * By means of Apache Cassandra [2] CumulusRDF offers a highly scalable RDF store for write-intensive applications  * CumulusRDF acts as a Linked Data server  * It allows for fast and lightweight evaluation of triple pattern queries  * It has full support for triple and quad storage  * CumulusRDF comprises a SesameSail [3] implementation, see [CodeExamples](https://github.com/cumulusrdf/cumulusrdf/wiki/CodeExamples) wiki page.  * Further, CumulusRDF contains a SPARQL1.1 endpoint.     Please see our [Features](https://github.com/cumulusrdf/cumulusrdf/wiki/Features) as well as our [Roadmap](https://github.com/cumulusrdf/cumulusrdf/wiki/Roadmap) wiki page for further information.     ## Quick start using docker  If you have docker you can get up and running quickly by using the following commands.    ```  $ git clone git@github.com:cumulusrdf/cumulusrdf.git  $ docker build -t cumulusrdf .  $ docker run -d --name cumulusrdf -p 9090:9090 cumulusrdf  ```    CumulusRDF is now available on http://localhost:9090/cumulus    ## Want to contribute?  We welcome any kind of contribution to cumulusRDF. In particular, we have a developer mailing list. Feel free to sign up via the web interface or by emailing at cumulusrdf-dev-list+subscribe@googlegroups.com.     ## Support  You can sign up to the CumulusRDF mailing list via the web interface or by emailing at cumulusrdf-list+subscribe@googlegroups.com.     ## People Involved  CumulusRDF is developed by the Institute of Applied Informatics and Formal Desciption Methods (AIFB) as well as by developers from our community. The developers are (in random order)    ### Active Contributors  * Andreas Wagner   * <a href=""https://github.com/aharth"" target=""_new"">Andreas Harth</a>    * Sebastian Schmidt    * <a href=""https://github.com/agazzarini"" target=""_new"">Andrea Gazzarini</a>    * <a href=""https://github.com/fzancan"" target=""_new"">Federico Zancan</a>    * Yongtao Ma    * <a href=""https://github.com/kamir"" target=""_new"">Mirko Kämpf</a>      ### Previous Contributors    * Günter Ladwig    * Steffen Stadtmüller    * Felix Obenauer      ## Publications   ##### <a href=""http://iswc2011.semanticweb.org/fileadmin/iswc/Papers/Workshops/SSWS/Ladwig-et-all-SSWS2011.pdf"" target=""_new"">CumulusRDF: Linked Data Management on Nested Key-Value Stores</a>        ```Java  @Inproceedings{ladwig2011cldmonks,  author = {Guenter Ladwig and Andreas Harth}  title = {CumulusRDF: Linked Data Management on Nested Key-Value Stores},  booktitle = {Proceedings of the 7th International Workshop on   Scalable Semantic Web Knowledge Base Systems (SSWS2011)  at the 10th International Semantic Web Conference (ISWC2011)},  month = {October},  year = {2011}  }  ```    ##### <a href=""http://ribs.csres.utexas.edu/nosqlrdf/nosqlrdf_iswc2013.pdf"" target=""_new"">NoSQL Databases for RDF: An Empirical Evaluation</a>        ```Java  @incollection{  year={2013},  isbn={978-3-642-41337-7},  booktitle={The Semantic Web – ISWC 2013},  volume={8219},  series={Lecture Notes in Computer Science},  editor={Alani, Harith and Kagal, Lalana and Fokoue, Achille and Groth, Paul and Biemann, Chris and Parreira, JosianeXavier and Aroyo, Lora and Noy, Natasha and Welty, Chris and Janowicz, Krzysztof},  doi={10.1007/978-3-642-41338-4_20},  title={NoSQL Databases for RDF: An Empirical Evaluation},  url={http://dx.doi.org/10.1007/978-3-642-41338-4_20},  publisher={Springer Berlin Heidelberg},  author={Cudré-Mauroux, Philippe and Enchev, Iliya and Fundatureanu, Sever and Groth, Paul and Haque, Albert and Harth, Andreas and Keppmann, FelixLeif and Miranker, Daniel and Sequeda, JuanF. and Wylot, Marcin},  pages={310-325}  }  ```  ## Acknowledgements         CumulusRDF was supported by the German Federal Ministry of Economics and Technology in the iZEUS project.      The CumulusRDF logo was kindly provided by <a href=""https://github.com/danieleliberti"" target=""_new"">Daniele Liberti</a>.          -------------------------  [1] http://en.wikipedia.org/wiki/Cumulus     [2] http://cassandra.apache.org     [3] http://openrdf.org """
Semantic web;https://github.com/ontola/hextuples;"""# HexTuples    _Status: draft_    _Version: 0.3.0_    HexTuples is a simple datamodel for dealing with linked data.  This document both describes the model and concepts of HexTuples, as well as the (at this moment only) serialization format: HexTuples-NDJSON.  It is very **easy to parse**, can be used for **streaming parsing** and is designed to be **highly performant** in JS contexts.     ## Concepts    ### HexTuple    A single _HexTuple_ is an atomic piece of data, similar to an [RDF Triple](https://www.w3.org/TR/rdf-concepts/#section-triples) (also known as Statements or Quads).  A HexTuple cotains a small piece of information.   HexTuples consist of six fields: `subject`, `predicate`, `value`, `datatype`, `language` and `graph`.    Let's encode the following sentence in HexTuples:    _Tim Berners-Lee, the director of W3C, is born in London on the 8th of June, 1955._    | Subject    | Predicate     | Value | DataType | Language | Graph |  |---------|----------------|------------|-----|-----|----|  | [Tim](https://www.w3.org/People/Berners-Lee/)     |[birthPlace](http://schema.org/birthPlace) | [London](http://dbpedia.org/resource/London)     | | |  | [Tim](https://www.w3.org/People/Berners-Lee/)     |[birthDate](http://schema.org/birthDate) | 1955-06-08     | [xsd:date](http://www.w3.org/2001/XMLSchema#date) | |   | [Tim](https://www.w3.org/People/Berners-Lee/)     |[jobTitle](http://schema.org/jobTitle) | Director of W3C  | [xsd:string](http://www.w3.org/2001/XMLSchema#string) | en-US |     ### URI    URI stands for [Uniform Resource Identifier, specified in RDF 3986](https://tools.ietf.org/html/rfc3986).  The best known type of URI is the URL.  Although it is currently best practice to use mostly HTTPS URLs as URIs, HexTuples works with any type of URI.    ### Subject    - The _subject_ is identifier of the thing the statement is about.  - This field is required.  - It MUST be a URI.    ### Predicate    - The _predicate_ describes the abstract property of the statement.  - This field is required.  - It MUST be a URI.    ### Value    - The _value_ contains the object of the HexTuple.  - This field is required.  - It can be any datatype, specified in the `datatype` of the HexTuple.    ### Datatype    - The _datatype_ contains the object of the HexTuple.  - This field is optional.  - It MUST be a URI or an empty string.  - When the Datatype is a NamedNode, use: `globalId`  - When the Datatype is a BlankNode, use: `localId`    ### Language    - The _datatype_ contains the object of the HexTuple.  - This field is optional.  - It MUST be an [RFC 3066 language tag](https://tools.ietf.org/html/rfc3066) or an empty string.    ## Relation to RDF    The HexTuples datamodel closely resembles the RDF Data Model, which is the de-facto standard for linked data.  RDF statements are often called Triples, because they consist of a `subject`, `predicate` and `value`.  The `object` field is either a single URI (in Named Nodes), or a combination of three fields (in Literal): `value`, `datatype`, `language`.  This means that a single Triple can actually consist of _five_ fields: the `subject`, `predicate`, `value`, `datatype` and the `language`.   A Quad statement also has a `graph`, which totals to six fields, hence the name: HexTuples.  Instead of making a distinction between Literal statements and NamedNode statements (which have two different models), HexTuples uses a single model that describes both.  **Having a single model for all statements (HexTuples), makes it easier to serialize, query and store data.**    ## HexTuples-NDJSON    _This document serves as a work in progress / draft specification_    HexTuples-NDJSON is an [NDJSON](http://ndjson.org/) (Newline Delimited JSON) based HexTuples / RDF serialization format.  It is desgined to support streaming parsing and provide great performance in a JS context (i.e. the browser).    - A valid HexTuples document MUST be serialized using [NDJSON](http://ndjson.org/)  - HexTuples-NDJSON MIME type: `application/hex+x-ndjson; charset=utf-8`  - Each array MUST consist of six strings.  - Each array represents one RDF statement / quad / triple  - The six strings in each array respectively represent  `subject`, `predicate`, `value`, `datatype`, `lang` and `graph`.  - The `datatype` and `lang` fields are only used when the `value` represents a Literal value (i.e. not a URI, but a string / date / something else). In RDF, the combination of `value`, `datatype` and `lang` are known as `object`.  - When expressing an Object that is a NamedNode, use this string as the datatype: ""globalId"" ([discussion](https://github.com/ontola/hextuples/issues/1))  - When expressing an Object that is a BlankNode, use this string as the datatype: ""localId""  - If the `graph` is a blank node (i.e. anonymous), use an underscore as the URI scheme: `_:myNode`. ([discussion](https://github.com/ontola/hextuples/issues/2)). Parsers SHOULD interpret these as blank graphs, but MAY discard these if they have no support for them.  - When a field has no value, use an empty string: `""""`    ### Example    English:    _Tim Berners-Lee was born in London, on the 8th of june in 1955._    Turtle / N-Triples:    ```n-triples  <https://www.w3.org/People/Berners-Lee/> <http://schema.org/birthDate> ""1955-06-08""^^<http://www.w3.org/2001/XMLSchema#date>.  <https://www.w3.org/People/Berners-Lee/> <http://schema.org/birthPlace> <http://dbpedia.org/resource/London>.  ```    Expresed in HexTuples:    ```ndjson  [""https://www.w3.org/People/Berners-Lee/"", ""http://schema.org/birthDate"", ""1955-06-08"", ""http://www.w3.org/2001/XMLSchema#date"", """", """"]  [""https://www.w3.org/People/Berners-Lee/"", ""http://schema.org/birthPlace"", ""http://dbpedia.org/resource/London"", ""globalId"", """", """"]  ```    ## Implementations    ### Ontola TypeScript HexTuples Parser    * <https://github.com/ontola/hextuples-parser>    This Typescript code should give you some idea of how to write a parser for HexTuples.    ```ts  const object = (value: string, datatype: string, language: string): SomeTerm => {    if (language) {      return literal(value, language);    } else if (datatype === 'globalId') {      return namedNode(value);    } else if (datatype === 'localId') {      return blankNode(value);    }      return literal(value, namedNode(datatype));  };    const lineToQuad = (h: string[]) => quad(    h[0].startsWith('_:') ? blankNode(h[0]) : namedNode(h[0]),    namedNode(h[1]),    object(h[2], h[3], h[4]),    h[5] ? namedNode(h[5]) : defaultGraph(),  );  ```    ### Python RDFlib    * <https://pypi.org/project/rdflib/>  * RDFLib is a pure Python package for working with RDF.   * It supports parsing and serliazing RDF as HexTuples  * Internally (in Python objects), RDF parsed from HexTuples data is represented in a _Conjunctive Graph_, that is a multi-graph object  * HexTuples files must end in the file extension `.hext` for RDFlib to auto-recognise the format although files with any ending can be used if the format is given (`format=hext`)    An RDF format conversion tool using RDFLib that can convert from/to HexTuples is online at <http://rdftools.surroundaustralia.com/convert>.    ## Motivation for HexTuples-NDJSON    HexTuples was designed by [Thom van Kalkeren](https://github.com/fletcher91/) (CTO of Ontola) because he noticed that parsing / serialization was unnecessarily costly in our [full-RDF stack](https://ontola.io/blog/full-stack-linked-data/), even when using the relatively performant `n-quads` format.    - Since HexTuples is serialized in NDJSON, it benefits from the [highly optimised JSON parsers in browsers](https://v8.dev/blog/cost-of-javascript-2019#json).  - It uses NDJSON instead of regular JSON because it makes it easier to parse **concatenated responses** (multiple root objects in one document).  - NDJSON enables **streaming parsing** as well, which gives it another performance boost.  - Some JS RDF libraries ([link-lib](https://github.com/fletcher91/link-lib/), [link-redux](https://github.com/fletcher91/link-redux/)) have an internal RDF graph model which uses these HexTuples arrays as well, which means that there is minimal mapping cost when parsing Hex-Tuple statements.  This format is especially suitable for real front-end applications that use dynamic RDF data. """
Semantic web;https://github.com/esbranson/any2rdf;"""any2rdf  =======    Convert files (OpenOffice.org/LibreOffice UNO, GDAL, etc.) to RDF    shp2rdf  -------    	shp2rdf.py [-d] config.js    ### Dependencies:    * [RDFLib](https://pypi.python.org/pypi/rdflib)  * [GDAL](https://pypi.python.org/pypi/GDAL/) """
Semantic web;https://github.com/castagna/GeoARQ;"""GeoARQ   ======    GeoARQ uses Lucene Spatial via an ARQ property function to allow to search  nearby a location. All the RDF instances which use the WGS84 geo positioning   RDF vocabulary [1] to represent geographic coordinates are indexed.    This is *experimental* (and unsupported).      How to use it  -------------    This is how you build an index from a Jena Model:        ModelIndexerSubject indexer = new ModelIndexerSubject(""target/lucene"");      indexer.indexStatements(model.listStatements());      indexer.close();    This is how you configure ARQ to use the spatial Lucene index:                IndexSearcher searcher = IndexSearcherFactory.create(""target/lucene"");      GeoARQ.setDefaultIndex(searcher);    This is an example of a SPARQL query using the :nearby property function to find airports close to Bristol (i.e. latitude ~ 51.3000, longitude ~ -2.71000):         PREFIX : <http://example/>      PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>      PREFIX geoarq: <http://openjena.org/GeoARQ/property#>      PREFIX asc: <http://airports.dataincubator.org/schema/> .        SELECT ?label {          ?s a asc:LargeAirport .          ?s rdfs:label ?lavel .          ?s geoarq:nearby (51.3000 -2.71000) .      }    Or, within a bounded box:        PREFIX : <http://example/>      PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>      PREFIX geoarq: <http://openjena.org/GeoARQ/property#>      PREFIX asc: <http://airports.dataincubator.org/schema/> .        SELECT ?label {          ?s a asc:LargeAirport .          ?s rdfs:label ?lavel .          ?s geoarq:within (51.3727 -2.72909 51.3927 -2.70909) .      }      Todo  ----     * Add more tests   * Clean-up the code   * Add support for http://www.w3.org/2003/01/geo/wgs84_pos#lat_long   * Add ability to specify radius (i.e. geoarq:nearby (51.3000 -2.71000 20))   * Add ability to specify number of results within a radius     (i.e. geoarq:nearby (51.3000 -2.71000 20 10))   * Investigate Geohash [2]   * Double check lucene-misc dependency, is it necessary?   * Add ability to index remote SPARQL endpoints?   * ...       [1] http://www.w3.org/2003/01/geo/wgs84_pos#     [2] http://en.wikipedia.org/wiki/Geohash"""
Semantic web;https://github.com/Accenture/AmpliGraph;"""# ![AmpliGraph](docs/img/ampligraph_logo_transparent_300.png)    [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2595043.svg)](https://doi.org/10.5281/zenodo.2595043)    [![Documentation Status](https://readthedocs.org/projects/ampligraph/badge/?version=latest)](http://ampligraph.readthedocs.io/?badge=latest)    [Join the conversation on Slack](https://join.slack.com/t/ampligraph/shared_invite/enQtNTc2NTI0MzUxMTM5LTRkODk0MjI2OWRlZjdjYmExY2Q3M2M3NGY0MGYyMmI4NWYyMWVhYTRjZDhkZjA1YTEyMzBkMGE4N2RmNTRiZDg)  ![](docs/img/slack_logo.png)    **Open source library based on TensorFlow that predicts links between concepts in a knowledge graph.**    **AmpliGraph** is a suite of neural machine learning models for relational Learning, a branch of machine learning  that deals with supervised learning on knowledge graphs.      **Use AmpliGraph if you need to**:    * Discover new knowledge from an existing knowledge graph.  * Complete large knowledge graphs with missing statements.  * Generate stand-alone knowledge graph embeddings.  * Develop and evaluate a new relational model.      AmpliGraph's machine learning models generate **knowledge graph embeddings**, vector representations of concepts in a metric space:    ![](docs/img/kg_lp_step1.png)    It then combines embeddings with model-specific scoring functions to predict unseen and novel links:    ![](docs/img/kg_lp_step2.png)      ## Key Features      * **Intuitive APIs**: AmpliGraph APIs are designed to reduce the code amount required to learn models that predict links in knowledge graphs.  * **GPU-Ready**: AmpliGraph is based on TensorFlow, and it is designed to run seamlessly on CPU and GPU devices - to speed-up training.  * **Extensible**: Roll your own knowledge graph embeddings model by extending AmpliGraph base estimators.      ## Modules    AmpliGraph includes the following submodules:    * **Datasets**: helper functions to load datasets (knowledge graphs).  * **Models**: knowledge graph embedding models. AmpliGraph contains **TransE**, **DistMult**, **ComplEx**, **HolE**, **ConvE**, **ConvKB**. (More to come!)  * **Evaluation**: metrics and evaluation protocols to assess the predictive power of the models.  * **Discovery**: High-level convenience APIs for knowledge discovery (discover new facts, cluster entities, predict near duplicates).      ## Installation    ### Prerequisites    * Linux, macOS, Windows  * Python 3.7    #### Provision a Virtual Environment    Create and activate a virtual environment (conda)    ```  conda create --name ampligraph python=3.7  source activate ampligraph  ```    #### Install TensorFlow    AmpliGraph is built on TensorFlow 1.x.  Install from pip or conda:    **CPU-only**    ```  pip install ""tensorflow>=1.15.2,<2.0""    or    conda install tensorflow'>=1.15.2,<2.0.0'  ```    **GPU support**    ```  pip install ""tensorflow-gpu>=1.15.2,<2.0""    or    conda install tensorflow-gpu'>=1.15.2,<2.0.0'  ```        ### Install AmpliGraph      Install the latest stable release from pip:    ```  pip install ampligraph  ```    If instead you want the most recent development version, you can clone the repository  and install from source (your local working copy will be on the latest commit on the `develop` branch).  The code snippet below will install the library in editable mode (`-e`):    ```  git clone https://github.com/Accenture/AmpliGraph.git  cd AmpliGraph  pip install -e .  ```      ### Sanity Check    ```python  >> import ampligraph  >> ampligraph.__version__  '1.4.0'  ```      ## Predictive Power Evaluation (MRR Filtered)    AmpliGraph includes implementations of TransE, DistMult, ComplEx, HolE, ConvE, and ConvKB.  Their predictive power is reported below and compared against the state-of-the-art results in literature.  [More details available here](https://docs.ampligraph.org/en/latest/experiments.html).    |                              |FB15K-237 |WN18RR   |YAGO3-10   | FB15k      |WN18           |  |------------------------------|----------|---------|-----------|------------|---------------|  | Literature Best              | **0.35***| 0.48*   | 0.49*     | **0.84**** | **0.95***     |  | TransE (AmpliGraph)          |  0.31    | 0.22    | **0.51**  | 0.63       | 0.66          |  | DistMult (AmpliGraph)        |  0.31    | 0.47    | 0.50      | 0.78       | 0.82          |  | ComplEx  (AmpliGraph)        |  0.32    | **0.51**| 0.49      | 0.80       | 0.94          |  | HolE (AmpliGraph)            |  0.31    | 0.47    | 0.50      | 0.80       | 0.94          |  | ConvE (AmpliGraph)           |  0.26    | 0.45    | 0.30      | 0.50       | 0.93          |  | ConvE (1-N, AmpliGraph)      |  0.32    | 0.48    | 0.40      | 0.80       | **0.95**      |  | ConvKB (AmpliGraph)          |  0.23    | 0.39    | 0.30      | 0.65       | 0.80          |    <sub>  * Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for knowledge base   completion. In International Conference on Machine Learning, 2869–2878. 2018. <br/>  **  Kadlec, Rudolf, Ondrej Bajgar, and Jan Kleindienst. ""Knowledge base completion: Baselines strike back.   "" arXiv preprint arXiv:1705.10744 (2017).  </sub>    <sub>  Results above are computed assigning the worst rank to a positive in case of ties.   Although this is the most conservative approach, some published literature may adopt an evaluation protocol that assigns   the best rank instead.   </sub>      ## Documentation    **[Documentation available here](http://docs.ampligraph.org)**    The project documentation can be built from your local working copy with:    ```  cd docs  make clean autogen html  ```    ## How to contribute    See [guidelines](http://docs.ampligraph.org) from AmpliGraph documentation.      ## How to Cite    If you like AmpliGraph and you use it in your project, why not starring the project on GitHub!    [![GitHub stars](https://img.shields.io/github/stars/Accenture/AmpliGraph.svg?style=social&label=Star&maxAge=3600)](https://GitHub.com/Accenture/AmpliGraph/stargazers/)      If you instead use AmpliGraph in an academic publication, cite as:    ```  @misc{ampligraph,   author= {Luca Costabello and            Sumit Pai and            Chan Le Van and            Rory McGrath and            Nicholas McCarthy and            Pedro Tabacof},   title = {{AmpliGraph: a Library for Representation Learning on Knowledge Graphs}},   month = mar,   year  = 2019,   doi   = {10.5281/zenodo.2595043},   url   = {https://doi.org/10.5281/zenodo.2595043}  }  ```    ## License    AmpliGraph is licensed under the Apache 2.0 License."""
Semantic web;https://github.com/nelson-ai/semantic-graphql;"""# Semantic GraphQL    *UNMAINTAINED:* [Go to the new repo location](https://github.com/dherault/semantic-graphql) for issues and pull requests. """
Semantic web;https://github.com/edsu/csvw-template;"""This is a simple example of using [CSV on the Web] to document the semantics  of a CSV file. Fork it, and change it for your CSV data. With apologies   to [Dan Bricklin].    [CSV on the Web]: http://www.w3.org/2013/csvw/wiki/Main_Page  [Dan Bricklin]: https://en.wikipedia.org/wiki/Dan_Bricklin """
Semantic web;https://github.com/newsreader/eso-and-ceo;"""Event and Implied Situation Ontology (ESO) and the Circumstantial Event Ontology for Calamities (CEO)  ========================    This repository contains the Event and Implied Situation Ontology (ESO), developed in Newsreader (www.newsreader-project.eu) and the Circumstantial Event Ontology for Calamities (CEO).      ESO_version2.owl is the stable version of the ontology; the ontology itself is described in ESO_documentation. The folder manual-mappings comprises several files with manually mappings from ESO to FrameNet, SUMO and WordNet.    Further, this repository contains the CEO ontology, which builds upon the metamodel of ESO. CEO is designed for the calamity domain (murders, earthquakes, floodings, evacuations, etc) and allows to infer chains of events based on shared event properties. Current version is version 1.0.     From May 2018 onwards, this repository is further maintained and updated here: https://github.com/RoxaneSegers/CEO-Ontology and here https://github.com/RoxaneSegers/ESO-Ontology.      <a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by-sa/4.0/88x31.png"" /></a><br /><span xmlns:dct=""http://purl.org/dc/terms/"" property=""dct:title"">Event and Implied Situation Ontology (ESO)</span> by <span xmlns:cc=""http://creativecommons.org/ns#"" property=""cc:attributionName"">Roxane Segers and Marco Rospocher</span> is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/"">Creative Commons Attribution-ShareAlike 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=""http://purl.org/dc/terms/"" href=""https://github.com/newsreader/eso"" rel=""dct:source"">https://github.com/newsreader/eso</a>. """
Semantic web;https://github.com/webr3/js3;"""# JS3 - An insane integration of RDF in ECMAScript-262 V5 (Javascript) #    In short, with this library, all your javascript is also RDF, there are no special types or classes,  each variable and value is also an RDF Node, List or Graph.    All values are both the standard javascript values you expect (no extension or suchlike), and are the RDF types you expect    ## Example ##    This library doesn't inspect objects and then generate RDF, rather each value *is* RDF, and javascript:        true.toNT();         // ""true""^^<http://www.w3.org/2001/XMLSchema#boolean>      (12 * 1.4).toNT();   // ""12.3""^^<http://www.w3.org/2001/XMLSchema#decimal>        Here's a complicated yet simple example to illustrate, this is just a standard Object in js:        var me = {        a: 'foaf:Person',                                         // a String, a CURIE and a full IRI        name: 'Nathan',                                           // a String, and an RDF Plain Literal        age: new Date().getFullYear() - 1981,                     // a Number, and a Typed Literal with the type xsd:integer        homepage: 'http://webr3.org',                             // a String, and an IRI,         holdsAccount: {                                           // an Object, with a BlankNode reference for the .id          label: ""Nathan's twitter account"".l('en'),              // a String, and a Literal with a .language          accountName: 'webr3',                                   // noticed that you don't need the prefixes yet?          homepage: 'http://twitter.com/webr3'                  },        knows: bob,                                               // works with variables too of course        nick: ['webr3','nath']                                    // an Array, also a list of values, like in turtle and n3      }.ref("":me"");                                               // still an Object, but also has a .id now, it's subject is set.    If we now call *me.n3()* we'll get the following output:        <http://webr3.org/nathan#me> rdf:type foaf:Person;        foaf:name ""Nathan"";        foaf:age 29;        foaf:homepage <http://webr3.org>;        foaf:holdsAccount [          rdfs:label ""Nathan's twitter account""@en;          foaf:accountName ""webr3"";          foaf:homepage <http://twitter.com/webr3> ];        foaf:knows <http://example.com/bob#me>;        foaf:nick ""webr3"", ""nath"" .    It's just that simple, your javascript is your RDF, it's just plain old javascript:        me.gender = ""male"";                   // .gender will resolve to foaf:gender to http://xmlns.com/foaf/0.1/gender       if(me.age > 18) return true;          // it's all native values, just use like normal!    ### Implementation Notice ###    This library requires ECMAScript-262 V5, specifically it makes heavy usage of Object.defineProperties.    You can check the [compatibility chart](http://kangax.github.com/es5-compat-table/) to see if your platform / browser supports it.  The short version is that chrome 5+, ff4, webkit (safari) and ie9 all support this script, and on the server side node.js, rhino and besen are all fine.    Objects and values are not modified in the usual manner and they are not converted in to different types, rather this library automagically redefines  the property descriptors on objects to allow each value to be both a native javascript value, and an RDF compatible value.    ## Nodes & Values ##    All values of type string, number, boolean and date are also RDF Nodes, and are fully aligned (and compatible) with the Interfaces  from the RDFa API.    ### Standard Methods ####  All of the basic js types (string, number, boolean and date) are augmented with the following methods:    *   **.nodeType()** - returns one of PlainLiteral, TypedLiteral, BlankNode or IRI                true.nodeType();                  // TypedLiteral          (12 * 1.4).nodeType();            // TypedLiteral          new Date().nodeType();            // TypedLiteral          ""hello world"".nodeType();         // PlainLiteral          ""_:b12"".nodeType();               // BlankNode          ""foaf:name"".nodeType();           // IRI          ""http://webr3.org/"".nodeType();   // IRI        *   **.equals(other)** - returns boolean        RDF type safe equality test.                ""hello"" == ""hello"".l('en')        // true          ""hello"".equals( ""hello"".l('en') ) // false    *   **.toNT()** - returns string        Does what it says on the tin, returns the N-Triples formatted value.                true.toNT();                      // ""true""^^<http://www.w3.org/2001/XMLSchema#boolean>          (12 * 1.4).toNT();                // ""12.3""^^<http://www.w3.org/2001/XMLSchema#decimal>          new Date().toNT();                // ""2010-11-20T21:06:42Z""^^<http://www.w3.org/2001/XMLSchema#dateTime>          ""hello world"".toNT();             // ""hello world""          ""hello world"".l('en').toNT();     // ""hello world""@en          ""_:b12"".toNT();                   // _:b12          ""foaf:name"".toNT();               // <http://xmlns.com/foaf/0.1/name>          ""http://webr3.org/"".toNT();       // <http://webr3.org/>      *   **.toCanonical()**        Alias of .toNT(), RDFa API compatibility method.    *   **.n3()** returns string        Returns the value formatted for N3/Turtle.            true.n3();                        // true          (12 * 1.4).n3();                  // 12.3          new Date().n3();                  // ""2010-11-20T21:06:42Z""^^<http://www.w3.org/2001/XMLSchema#dateTime>          ""hello world"".n3();               // ""hello world""          ""hello world"".l('en').n3();       // ""hello world""@en          ""_:b12"".n3();                     // _:b12          ""foaf:name"".n3();                 // foaf:name          ""http://webr3.org/"".n3();         // <http://webr3.org/>       ### String Methods ####  A string can represent any of the RDF Node types, PlainLiteral (+language), TypedLiteral, BlankNode or IRI.  In js3 string exposes the following methods (in addition to the standard methods outlined above):    *   **.l()** - returns this        Set the language of a PlainLiteral - exposes the **.language** attribute after calling. (.language is non-enumerable, read-only)            var s = ""Hello World"".l('en');                                  s.language                        // 'en'          s.nodeType();                     // PlainLiteral          s.toNT();                         // ""Hello World""@en     *   **.tl()** - returns this        Set the type of a TypedLiteral - exposes the **.type** attribute after calling. (.type is non-enumerable, read-only)            var s = ""0FB7"".tl('xsd:hexBinary');          s.type                            // http://www.w3.org/2001/XMLSchema#hexBinary          s.nodeType();                     // TypedLiteral          s.toNT();                         // ""0FB7""^^<http://www.w3.org/2001/XMLSchema#hexBinary>                Note: this method also caters for the situations when you want a PlainLiteral to be an xsd:string, or an IRI to be a PlainLiteral                var u = ""http://webr3.org/"";      // <http://webr3.org/>          u.tl(""rdf:PlainLiteral);          // ""http://webr3.org/""                    var h = ""hello"";                  // ""hello""          ""hello"".tl('xsd:string');         // ""hello""^^<http://www.w3.org/2001/XMLSchema#string>     *   **.resolve()** - returns string IRI        Resolve a CURIE to a full IRI - note this is done automatically by .n3 and .toNT methods.            ""foaf:name"".resolve()             // returns string ""http://xmlns.com/foaf/0.1/name"" with nodeType IRI    Remember, all javascript values and types remain unchanged, so it's entirely backwards compatible with all existing data, and will not modify any js values,  ""Hello World"".l('en') is still a String, properties like .language and .type are non-enumerable, so they won't show up in for loops or when you JSON.stringify  the values. You cannot implement this library in non v5 ecmascript by simply adding a .language property to the String object, that simply won't work.    ### Numbers ####    js3 is fully aware of number types, it knows when a number is an integer, a double or a decimal.        12 .toNT()                            // ""12""^^<http://www.w3.org/2001/XMLSchema#integer>      12.1 .toNT()                          // ""12.1""^^<http://www.w3.org/2001/XMLSchema#decimal>      1267.43233E32 .toNT()                 // ""1.26743233e+35""^^<http://www.w3.org/2001/XMLSchema#double>    **Gotcha:** do note that you need to add a space between the number and the .method, or wrap the number in braces *(12.1).toNT()*,  since js expects any integer followed immediately by a period to be a decimal, like 12.145 - this only applies to hard coded in the source-code numbers, and not  those in variables or returned from functions, so generally isn't noticable, in the same way that you don't normally write 12.toString()!      ## Arrays and Lists ##    JS3 uses arrays for both lists of objects in property-object chains { name: ['nathan','nath'] } and as rdf Lists.    You can determine whether an array is a list or not by inspecting the boolean property **.list** on any array. To specify that an array is a list you simply call .toList() on it.    ### Array Methods and Properties ###    *   **.list** - boolean        Boolean flag indicating whether an array is an RDF list.         *   **.toList()** - returns this        Specifies that an array is to be used as an RDF list, sets the .list property to true.    *   **.n3()** returns string        Returns the value formatted for N3/Turtle.            [1,2,3,4].n3()                        // 1, 2, 3, 4          [1,2,3,4].toList().n3()               // ( 1 2 3 4 )            Note that there are no .toNT or .nodeType methods, or related, arrays and lists are not RDF Nodes.    ## Objects and Descriptions ##    In js3 each Object is by default just an Object with a single additional method exposed **.ref()**. When you call this method the object is RDF enabled,  whereby it is set to denote the description of something - identified by a blanknode or an IRI - the keys (properties) are mapped to RDF Properties,  a **.id** attribute is exposed on the object, and four methods are also exposed: **.n3()**, **.toNT()**, **.using()** and **.graphify()**.    ### The Basics ###  It's all really simple tbh, the properties on each object can either be:    - obj['http://xmlns.com/foaf/0.1/name'] - a full IRI  - obj['foaf:name'] - a normal CURIE  - obj.foaf$name - a more javascript friendly CURIE where the : is swapped for a $  - obj.name - a single property which maps up to a CURIE, which maps to an IRI    Each value can be a single value (of any type covered), or an array of values (which might be a list), or an object (which can be named with an IRI or a blanknode identifier).    And thus, just like normal javascript or JSON you can make an object structure as simple or as complicated as you like.    Objects can also have methods on them, and these are stripped from any output, so any existing object whether dumb or a full class with properties can be used.    They're just javascript objects with a .id set on them (non-enumerable and read-only), and where the properties are mapped to RDF properties. So, each object can be seen to describe one thing, one subject, the .id is the subject.  To set the .id all you do is call **.ref()** on the object, if you pass in a CURIE or an IRI as a param then that is set as the subject/.id, if you call .ref() with no argument then it is given a blanknode identifier as the .id.    The methods exposed after .ref'ing are also simple, .n3 dumps an n3 string of the object, .toNT dumps it out as ntriples, and .graphify gives you back an RDFGraph from the RDFa API, making it completely compatible and exposing all the functionality of my [rdfa-api](http://github.org/webr3/rdfa-api) library (and other compatible implementations of the RDFa API).    **.using()** is a bit more subtle, you can throw in the names of ontologies which properties your using come from, in order to provide an unambiguous mapping, for instance:        var article = {        description: ""A dc11:, not dc:, description"",        label: ""An rdfs:label""      }.ref(':me').using('dc11','rdfs');    If you don't pass in any names, then they are mapped up on a first-hit-first-used basis. This is covered more in the section about *propertymap* and *curiemap*.    ### Syntax, Variables and References ###    There is no special syntax, and variables + references are part of javascript, so they ""just work"". which means you can do things like this:        article.maker = me;      me.made = article;      article.maker.knows = bob;     // the same as me.knows      article.created = new Date();      { a: 'foaf:Document', primaryTopicOf: article }.ref(':this').graphify().turtle();        Because we referenced article by value then it'll be in the output graph too, we could use article.id instead then it won't be included.    You can also have X many Objects with the same .id, then when you .graphify them they all get smashed together as one - which is nice.    As for migrating IRIs or renaming subjects, that's as simple as calling .ref(':newid') on any object, no complex rdf replace routines needed.    ### Data Structures ###     When Objects are nested, they are by default considered to be blanknodes, *however!*, you can of course call .ref() on them in place, and thus  describe things in context, and name them there too.    So in this case the object in holdsAccount will be a blanknode:        var me = {        name: 'Nathan',        holdsAccount: {          label: ""Nathan's twitter account"".l('en'),          accountName: 'webr3',          homepage: 'http://twitter.com/webr3'                  },      }.ref("":me"");    But in this case it'll have it's own IRI:            var me = {        name: 'Nathan',        holdsAccount: {          label: ""Nathan's twitter account"".l('en'),          accountName: 'webr3',          homepage: 'http://twitter.com/webr3'                  }.ref(':twitter'),                                        // here's where we named it      }.ref("":me"");    ... of course we can code this however we want to get the same results, for example:        var me = { name: 'Nathan' }.ref("":me"");      var account = { accountName: 'webr3' };      account.label = ""Nathan's twitter account"";      account.label.l('en');      me.holdsAccount = account;      me.holdsAccount.foaf$homepage = ""twitter:webr3"".resolve();      me.holdsAccount.ref(':twitter');    ... or create structures just as complex as we like:        { deep: [        ""item1"",        [1, 2.745, [me,bob,""x:mary""].toList(), new Date(), bob].toList(),        ""something"".substr(3,4).l('en'),        [bob.id, me.id].toList(), { foo: ""bar"" }      ]};    ... and interact with our data however we want:        var somedata = {        values: [1,10,25,50].toList(),        created: new Date()      }.ref(':results');       with(Math) {        somedata.result = somedata.values.map(sqrt).reduce(function(p,c) { return max(p,c) });      }    ... and then call .n3():        :results        dc:created ""2010-11-20T21:06:42Z""^^<http://www.w3.org/2001/XMLSchema#dateTime>;        seq:values ( 1 10 25 50 );        seq:result 7.0710678118654755 .          It's all very flexible - as you can see as we just map reduced an RDF List and updated a graph in one line :)    ### Object Methods and Properties - after .ref()'ing ###    *   **.id** - string (read-only, non-enumerable)        BlankNode or IRI in a string, the subject / .id of this object.         *   **.n3()** returns string        Returns the object as N3/Turtle.    *   **.toNT()** returns string        Returns the object as NTriples.    *   **.graphify()** - returns RDFGraph        Returns the structure as an RDFGraph of RDFTriples as per the RDFa API core interfaces - compat++.    *   **.using(arg1, arg2 ... argN)** - returns this        Pass in string prefixes for ontologies to consider when mapping simple properties.            Do see the [wiki page on .using() js3.propertymap](https://github.com/webr3/js3/wiki/using-and-propertymap) for more details.      ## js3.curiemap and js3.propertymap ##    **js3.curiemap** is a simple object which maps prefixes to IRIs.    *   to add a CURIE mapping:                js3.curiemap.foaf = ""http://xmlns.com/foaf/0.1/"";            *   to get an IRI for a prefix:                var iri = js3.curiemap.foaf;            *   **.setDefault(iri)** - to set the default prefix ""**:**"" :                js3.curiemap.setDefault('http://webr3.org/nathan#');    *   **.getPrefix(iri)** - get the registered prefix for an IRI, returns null if no prefix is found:                js3.curiemap.getPrefix('http://xmlns.com/foaf/0.1/'); // 'foaf'            *   **.shrink(iri)** - turn an IRI in to a CURIE :        This method returns either a CURIE or the original IRI if no prefix is found.                js3.curiemap.shrink('http://xmlns.com/foaf/0.1/name'); // 'foaf:name'              **js3.propertymap** is a simple object which makes the lib aware of properties in ontologies.    *   to add the properties for an ontology:                js3.propertymap.foaf = ['name','mbox','page', ...];    note: the value must always be an array.    *   to get the properties for an ontology:                var properties = js3.propertymap.foaf;            *   **.ambiguities()** - returns an array of ambiguous properties:                var gotchas = js3.propertymap.ambiguities();    Do see the [wiki page on .using() js3.propertymap](https://github.com/webr3/js3/wiki/using-and-propertymap) for more details.    ## js3.graphify() ##    A simple method which can accept any number of objects, or an array of objects, and will return back an RDFGraph.      # Coming Soon #    *   **IRI.deref( callback )**        A web aware function that will get the description of a subject from the web (negotiating formats) and return back a js3 object to work with:                ""http://webr3.org/nathan#me"".deref( function(me) {            print( me.name ); // etc          });            *   **Obj.save()**        A web aware function that will PUT an updated description:                ""http://webr3.org/nathan#me"".deref( function(me) {            me.friends.push( ""somebody:new );            me.save();          });            *   **Full rdfa-api integration**        From the other side, so you can do graph.describe(subject) and get back an object, and suchlike.        Probably much more..    # Feedback #    All feedback, bugs etc via issues here, or, well you can get all my details from my FOAF profile using this lib if you like ;) """
Semantic web;https://github.com/OpenTriply/YASGUI;"""# YASGUI    Useful links:    - User documentation: https://triply.cc/docs/yasgui  - Developer documentation: https://triply.cc/docs/yasgui-api  - Documentation Github repository (feel free to add a PR for improvements): https://github.com/TriplyDB/Documentation    ## Installation    Below are instructions on how to include Yasgui in your project. If you only want to install Yasr or Yasqe, replace yasgui in the commands below    ### npm    ```sh  npm i @triply/yasgui  ```    ### Yarn    ```sh  yarn add @triply/yasgui  ```    ## Local development    #### Installing dependencies    Run `yarn install`.    #### Running Yasgui locally    To develop locally, run `yarn run dev`    #### Compiling Yasgui    Run `yarn run build`. It'll store the transpiled js/css files in the `build` directory    ## License    This software is written by Triply.    This code is released under the MIT license. """
Semantic web;https://github.com/daedafusion/cyber-ontology;"""#Cyber Intelligence Ontology    <a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by-sa/4.0/88x31.png"" /></a><br /><span xmlns:dct=""http://purl.org/dc/terms/"" href=""http://purl.org/dc/dcmitype/Text"" property=""dct:title"" rel=""dct:type"">Cyber</span> is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/"">Creative Commons Attribution-ShareAlike 4.0 International License</a>. """
Semantic web;https://github.com/robstewart57/hsparql;"""  [![Available on Hackage][badge-hackage]][hackage]  [![License BSD3][badge-license]][license]  [![Build Status][badge-travis]][travis]    [badge-travis]: https://travis-ci.org/robstewart57/hsparql.png?branch=master  [travis]: https://travis-ci.org/robstewart57/hsparql  [badge-hackage]: https://img.shields.io/hackage/v/hsparql.svg  [hackage]: http://hackage.haskell.org/package/hsparql  [badge-license]: https://img.shields.io/badge/license-BSD3-green.svg?dummy  [license]: https://github.com/robstewart57/hsparql/blob/master/LICENSE    ## Introduction    `hsparql` includes a DSL to easily create queries, as well as methods to  submit those queries to a SPARQL server, returning the results as  simple Haskell data structures.    ### Select Queries    Take the following SPARQL query:    ```sparql  PREFIX dbpedia: <http://dbpedia.org/resource/>  PREFIX dbprop: <http://dbpedia.org/property/>  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  SELECT ?name ?page  WHERE {    ?x dbprop:genre dbpedia:Web_browser    ?x foaf:name ?name    ?x foaf:page ?page  }  ```    Can be generated using the following Haskell code:    ```haskell  simpleSelect :: Query SelectQuery  simpleSelect = do      resource <- prefix ""dbpedia"" (iriRef ""http://dbpedia.org/resource/"")      dbpprop  <- prefix ""dbprop"" (iriRef ""http://dbpedia.org/property/"")      foaf     <- prefix ""foaf"" (iriRef ""http://xmlns.com/foaf/0.1/"")        x    <- var      name <- var      page <- var        triple_ x (dbpprop .:. ""genre"") (resource .:. ""Web_browser"")      triple_ x (foaf .:. ""name"") name      triple_ x (foaf .:. ""page"") page        selectVars [name, page]  ```      ### Construct Queries    Take the following SPARQL query:    ```sparql  PREFIX dbpedia: <http://dbpedia.org/resource/>  PREFIX dbprop: <http://dbpedia.org/property/>  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX example: <http://www.example.com/>  CONSTRUCT {    ?x example:hasName ?name  }  WHERE {    ?x dbprop:genre dbpedia:Web_browser    ?x foaf:name ?name    ?x foaf:page ?page  }  ```    Can be generated using the following Haskell code:    ```haskell  simpleConstruct :: Query ConstructQuery  simpleConstruct = do      resource <- prefix ""dbpedia"" (iriRef ""http://dbpedia.org/resource/"")      dbpprop  <- prefix ""dbprop"" (iriRef ""http://dbpedia.org/property/"")      foaf     <- prefix ""foaf"" (iriRef ""http://xmlns.com/foaf/0.1/"")      example  <- prefix ""example"" (iriRef ""http://www.example.com/"")        x    <- var      name <- var      page <- var        construct <- constructTriple x (example .:. ""hasName"") name        triple_ x (dbpprop .:. ""genre"") (resource .:. ""Web_browser"")      triple_ x (foaf .:. ""name"") name      triple_ x (foaf .:. ""page"") page        return ConstructQuery { queryConstructs = [construct] }  ```    ### Describe Queries    Take the following SPARQL query:    ```sparql  DESCRIBE <http://dbpedia.org/resource/Edinburgh>  ```    Can be generated using the following Haskell code:    ```haskell  simpleDescribe :: Query DescribeQuery  simpleDescribe = do      resource <- prefix ""dbpedia"" (iriRef ""http://dbpedia.org/resource/"")      uri <- describeIRI (resource .:. ""Edinburgh"")      return DescribeQuery { queryDescribe = uri }  ```    ### Ask Queries    Take the following SPARQL query:    ```sparql  PREFIX dbprop: <http://dbpedia.org/property/>  ASK { ?x dbprop:genre <http://dbpedia.org/resource/Web_browser> }  ```    Can be generated using the following Haskell code:    ```haskell  simpleAsk :: Query AskQuery  simpleAsk = do      resource <- prefix ""dbpedia"" (iriRef ""http://dbpedia.org/resource/"")      dbprop  <- prefix ""dbprop"" (iriRef ""http://dbpedia.org/property/"")        x <- var      ask <- askTriple x (dbprop .:. ""genre"") (resource .:. ""Web_browser"")        return AskQuery { queryAsk = [ask] }  ```    ## Output Types    ### Select Queries    `SELECT` queries generate a set of sparql query solutions. See:  http://www.w3.org/TR/rdf-sparql-XMLres/    ```haskell  selectExample :: IO ()  selectExample = do    (Just s) <- selectQuery ""http://dbpedia.org/sparql"" simpleSelect    putStrLn . take 500 . show $ s  ```      Here's the respective type:    ```haskell  selectQuery :: EndPoint -> Query SelectQuery -> IO (Maybe [[BindingValue]])  ```      ### Construct Queries    `CONSTRUCT` queries generate RDF, which is serialized in N3 in this  package. See: http://www.w3.org/TR/rdf-primer/#rdfxml    ```haskell  constructExample :: IO ()  constructExample = do    rdfGraph <- constructQuery ""http://dbpedia.org/sparql"" simpleConstruct    mapM_ print (triplesOf rdfGraph)  ```    Here's the respective type:    ```haskell  constructQuery :: EndPoint -> Query ConstructQuery -> IO MGraph  ```    ### Describe Queries    `DESCRIBE` queries generate RDF, which is serialized in N3 in this  package. See: http://www.w3.org/TR/rdf-sparql-query/#describe    ```haskell  describeExample :: IO ()  describeExample = do    rdfGraph <- describeQuery ""http://dbpedia.org/sparql"" simpleDescribe    mapM_ print (triplesOf rdfGraph  ```    Here's the respective type:    ```haskell  describeQuery :: EndPoint -> Query DescribeQuery -> IO MGraph  ```    ### Ask Queries    `ASK` queries inspects whether or not a triple exists. RDF is an  open-world assumption. See: http://www.w3.org/TR/rdf-sparql-query/#ask    ```haskell  askExample :: IO ()  askExample = do    res <- askQuery ""http://dbpedia.org/sparql"" simpleAsk    putStrLn $ ""result: "" ++ (show (res::Bool))  ```    Here's the respective type:    ```haskell  askQuery :: EndPoint -> Query AskQuery -> IO Bool  ```    ### More examples    Some extra examples can be found in [tests](tests/Database/HSparql/QueryGeneratorTest.hs).    ## TODOs    - Opt for a unified Type representation    This hsparql package and the [RDF4H][1] package use similar, but not    identical, types for triples, namespaces, prefixes and so on. Ideally,    one type representation for such concepts should be adopted for both packages.    - Develop a unified semantic web toolkit for Haskell    Combining the RDF4H and hsparql packages seems like a sensible goal to    achieve, to provide a semantic web toolkit similar to [Jena][2] for Java.      [1]: https://github.com/robstewart57/rdf4h  [2]: http://incubator.apache.org/jena/ """
Semantic web;https://github.com/peta/turtle.tmbundle;"""# turtle.tmbundle  ---------------------------------------------------------------------    Totally awesome bundle for Turtle – the terse RDF triple language.    It consists of:    + Language grammar for Turtle and SPARQL 1.1  + Powerful auto-completion (live-aggregated)  + Documentation for classes and roles/properties at your fingertips (live-aggregated)  + Interactive SPARQL query scratchpad  + Some snippets (prefixes and document skeleton)  + Solid syntax validation  + Commands for instant graph visualization of a knowledge base (requires Graphviz and Raptor)  + Automatic removal of unused prefixes  + Conversion between all common RDF formats    See [Screenshots](#screenshots)    __NOTE: If the HTML output of this GitHub-flavored Markdown document looks broken, please visit [https://github.com/peta/turtle.tmbundle/blob/master/README.md] to read the original version__    ## Language grammar     The language grammar now covers the official W3C parser spec (as proposed in the latest CR released on Feb 19th 2013). However, there are still one/two particularities that differ, but you shouldn't notice them during your daily work. In the case you notice some weird behaviour (most obvious sign: broken syntax highlighting), please file a bug in the [project's issue tracker](https://github.com/peta/turtle.tmbundle/issues ""Here at GitHub"").    The language grammar also recognizes keywords and builtin functions from the latest SPAR[QU]L 1.1 language specification. Further there is basic autocompletion (`⌥ + ⎋`) for the aforementioned.    ## Powerful auto-completion    The Turtle bundle offers auto-completion at two levels:    __NOTE: *When determining IRIs associated with a given prefix name, local prefix declarations always have precedence over those given by prefix.cc. So when you mess up IRIs in your @prefix directives, auto-completion might not work as expected.*__    ### Auto-completion for prefix directives    When you invoke the `Autocomplete` command (`⌥ + ⎋`) within the scope of a prefix directive (right after `@prefix` or `PREFIX`), the Turtle bundle fetches a list of all prefixes registered at [prefix.cc](http://prefix.cc) and displays them nicely in a auto-complete dropdown box. Once you have chosen an and confirmed your selection, the prefix directive is automagically updated with the prefix and its according URI. (Note: the fetched data is locally cached for 24h)    __NOTE: *Auto-completion for prefix declarations is case-insensitive*__    ### Auto-completion for the local part of prefixed name IRIs    When you invoke the `Autocomplete` command (`⌥ + ⎋`) within the scope of a prefixed name (e.g. right after `my:` or at `my:a...`), the Turtle bundle determines the actual URI that is abbreviated by the prefixed namespace and checks if there is a machine readable Vocabulary/Ontology document available (currently only RDF/S and OWL documents in the XML serialization format are supported -- but with known issues). When one is found, it is live-aggregated and all of its Classes and Roles/Properties are extracted (along with their documentation) and nicely presented in a auto-complete dropdown box. (Note: the fetched data is locally cached for 24h)    __NOTE: *Auto-completion for prefixed names is case-sensitive*__    ### Known issues    For now, the Turtle bundle relies on [prefix.cc](http://prefix.cc) for mapping prefixes to URIs (required for all live-aggregations). The problem however is, that the available listings contain only one IRI per prefix (the one with the highest ranking) and not every IRI offers a machine readable vocabulary/ontology representation, what in turn means that for certain prefixes no auto-completion data might be available. You can help to fix this, by visiting the according page at prefix.cc (URL scheme looks like `http://prefix.cc/<THE_PREFIX>`; without angle brackets ofc) and up/downvoting the according URIs.    The automatic aggregation of machine-readable vocabulary/ontology descriptions is working in principle but still has some shortcomings. (See the Github issue tracker) I will overwork that part when I have some more spare time and/or the need just arises. When you're told that data for a given prefix was fetched (green tooltip after a few seconds of freeze) but you will see no autocompletion dropdown later on, it probably means that the aggregator script failed and/or the IRI could not redirect to resource in a compatible format.    ## Documentation for classes, roles/properties and individuals    When you invoke the `Documentation for Resource` command (`⌃ + H`) within the scope of a prefixed name IRI (e.g. `my:Dog`), the Turtle bundle looks up if there are any informal descriptions available (like description texts, HTTP URLs to human-readable docs, asf.) and if so, displays them to the user. (Note: the fetched data is locally cached for 24h)    __NOTE: *That function also suffers from the issues outlined in the previous section.*__    ## Interactive SPARQL query scratchpad ##    All (web-based) query forms that crossed by cursor had one thing in common – they suck(ed). Syntax highlighting? Efficient workflows (like trial-error-roundtripping)? Of course NOT. So I decided to add something similar to TextMate. The *Execute SPARQL Query* command may seem self-explaining, but it hides some important features:    + first of all, it supports multiple sections in a single document  + it is aware of custom magic comments types (called ""markers"") for   	+ specifying different SPARQL Query/Update services (aka *endpoints*)  	+ including/calling other section from within the same document  + query results are nicely displayed together with the query log in a preview window    Just press `⌘ + R` and your query will be executed.    ### Multiple sections/snippets syntax    You can have one document contain multiple (independent) sections. This is e.g. useful when developing complex queries where we usually follow an iterative approach. When running the `Execute SPARQL Query` command, it will automatically figure out what your intention is and act accordingly. It assumes that you will always do one of the following tasks:    1. Execute the whole document  2. Execute your current selection  3. Execute the section where your cursor is currently positioned in     A simple document with multiple sections could look like this:    ```  #QUERY  <http://example1.com/ds/query>  #UPDATE <http://example1.com/ds/update>    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>  PREFIX : <urn:example#>    INSERT DATA { :instanceA rdfs:label 'Human-readable label of instanceA'. }    #---  PREFIX : <urn:example#>  SELECT (COUNT(?subj) AS ?n_subjs)  WHERE { ?subj a :ClassA }    #---  PREFIX : <urn:example#>  SELECT ?g  WHERE { GRAPH ?g {} }  ```    As you probably notice, multiple sections are separated with the marker string `#---` written on a separate line.    ###  Magic comment syntax for endpoints    These magic comments have the following syntax:    + `#QUERY <http://example.org/sparql` to describe a SPARQL endpoint (read-only)  + `#UPDATE <http://example.org/update` to describe a SPARUL endpoint (write-only)    You can specify multiple magic comments throughout your document. When executing the query command, it will automatically determine the required endpoint type depending on the SPARQL commands that occur in your query. After that, it scans your document for magic comments where it uses the following strategies:    + _Top-down:_ When the whole document is used as query, it scans every line beginning at the top until it finds a suitable magic comment   + _Bottom-up:_ When only the current/selected section is queried, it first checks if that section contains the required endpoints, and, when not the case, it then scans every line from bottom to top beginning at the line where the section/selection starts until it finds a suitable magic comment    When no suitable magic comment was found, the command consults TextMate's environment variables (can be set in the application preferences). More precisely it looks for the following two variables:     + `TM_SPARQL_QUERY`  + `TM_SPARQL_UPDATE`    that should contain the HTTP(S) URL to SPAR[QU]L endpoints. When even this fails, YOU (the user) are prompted to enter an endpoint URL. If you cancel, the execution is aborted and a puppy dies.    Here is an example document with multiple sections and multiple magic comments for endpoints:    ```  #QUERY  <http://example.com/ds/query>                   #UPDATE <http://example.com/tbox/update>  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>  \  PREFIX : <urn:example#>                               |  INSERT DATA {                                         | http://example.com/tbox/update      :ClassB a rdfs:Class;                             |          rdfs:subClassOf :ClassA.                      |  }                                                     /    #---  #UPDATE <http://example.com/abox/update>  PREFIX : <urn:example#>                               \  INSERT DATA {                                         |      :instA a :ClassB;                                 | http://example.com/abox/update          rdfs:label 'Instance of ClassB'.              |  }                                                     /    #---  PREFIX : <urn:example#>                               \  SELECT (COUNT(?subj) AS ?n_subjs)                     | http://example.com/ds/query  WHERE { ?subj a :ClassA }                             /    #---  #QUERY <http://dbpedia.org/sparql>                      SELECT DISTINCT ?s ?label                             \  WHERE {                                               | http://dbpedia.org/sparql      ?s <http://dbpedia.org/property/season> ?o .      |      ?s rdfs:label ?label                              |  }                                                     /    #---  BASE <http://dbpedia.org/>                            \  SELECT DISTINCT ?s ?label                             |  WHERE {                                               | http://dbpedia.org/sparql      ?s rdfs:label ?label                              |      FILTER( CONTAINS( STR(?label), 'The Wire') )      |  }                                                     /  ```    ### Additional magic comment types    #### Name your sections    You can create a named section by adding a `#name= my-section` marker. Named sections can be included from other sections within your document. For more details see the next section:       #### #INCLUDE <section-name>    By using Includes, you can reuse named sections all over the place. This way, it is possible to write typically used snippets once, and then just ""call"" them from anywhere else when needed. This is especially helpful for SPARQL beginners or when iteratively developing complex queries where one wants to see the effect of a previous query immediately.    When executed, every included snippet is listed separately in the web preview window of TextMate. That way it is easy to track down issues with intermediate states (and not just only after all queries have finished).    The order in which sections appear and are included in your document doesn't matter. So an included section doesn't have to be defined before the section it is included from. The following example shows a simple use-case:    ```  #INCLUDE <drop-all>  #INCLUDE <insert-some-bnode>  #INCLUDE <select-all-bnodes>  DELETE { ?s ?p ?o }  WHERE {   	[] ?p ?o .   	?s <urn:example#label> ?o .   	?s ?p 'bnode 2' .   	FILTER( isBLANK( ?s ) )  }  #INCLUDE <select-all-bnodes>    #---  #name= insert-some-bnode  PREFIX : <urn:example#>  INSERT DATA {  	[ a :MyClass ] :label 'bnode 1' .  	[ a :MyClass ] :label 'bnode 2' .  	[ a :MyClass ] :label 'bnode 3' .  }    #---  #name= drop-all  #descr= Drop all graphs – both, the default graph and all named ones  DROP ALL    #---  #name= select-all-bnodes  #descr= Selects all bnodes in the default graph  SELECT DISTINCT *  WHERE {  	?s ?p ?o   	FILTER isBLANK(?s)  }  ```    ![Screenshot of SPARQL result preview window](./Support/img/screenshot-sparql.png ""Screenshot of SPARQL result preview window"")    #### Notes    There is also basic support for simple metadata properties in the following form: `#propname= some prop value`. It's the SPARQL pendant to jsdoc or phpdoc comments. However, beside section names an description texts I have no idea how this could be used. Do you?    I have the vague idea of a pastebin for SPARQL in mind, where one can host reusable SPARQL snippets with support for parametrized SPARQL calls. What do you think about it?        ## Snippets    Right now the following snippets are included:    + Basic document skeleton  + ""Smart"" prefix/base directives (hit tab to see it work)  + A set of basic prefix directives (Boring! The cool kids instead use the fancy auto-completion)    ## Syntax validation    You can trigger a syntax validation of your Turtle by pressing `CTRL + SHIFT + V`. In order to make use of syntax validation you must a have a working installation of the [Raptor RDF syntax library](http://librdf.org/raptor/). For detailed instructions about wiring up Raptor with Textmate, see the [#graph-visualization](section below).    ![Screenshot of syntax validation error message](./Support/img/screenshot-syntaxval-error.png ""Screenshot of syntax validation error message"")    ![Screenshot of syntax validation success message](./Support/img/screenshot-syntaxval-success.png ""Screenshot of syntax validation success message"")    ## Graph visualization    In order to use this functionality you need a working installation of [Graphviz](http://graphviz.org) (especially the dot command) and the [Raptor RDF syntax library](http://librdf.org/raptor/). When properly installed (locatable through PATHs) everything should work fine ootb. However, in some cases you must explicitly tell Textmate where to find them. You can do this by introducing two configuration variables (Textmate -> Preferences -> Variables):    + `TM_DOT` absolute path to the dot binary (part of Graphviz)    + `TM_RAPPER` absoluter path to the rapper binary (part of Raptor)    By hitting `CMD + R` the active TTL document will be visualized on-the-fly in a Textmate HTML preview window. Because these preview windows are driven by Safari's WebKit rendering engine, PDF documents will be rendered right in-line. That way your ""edit knowledge base --> visualize"" workflow will be super intuitive and wont get interrupted by switching to separate PDF viewer app for viewing the visualization.    By hitting `SHIFT + ALT + CMD + S` the active TTL document will be visualized and saved to a PDF document.    ## Conversion between all common RDF formats    In order to make use of the converter functionality, you must need a working installation of the [Raptor RDF syntax library](http://librdf.org/raptor/). For detailed instructions about wiring up Raptor with Textmate, see the [#graph-visualization](section above).    ![Screenshot showing list of available target formats the user may choose from](./Support/img/screenshot-converter.png ""Screenshot showing list of available target formats the user may choose from"")    ## Installation    The Turtle bundle is now officially available through the Textate bundle installer (_Textmate -> Preferences -> Bundles_). However, it usually takes a few days until new releases are available through the bundle installer (make sure that you enabled 'Keep bundles updated' in the application preferences). If you know what you do, you can also install bundles (like Turtle) by hand. Just download/clone this repository, and place its root directory at `~/Library/Application Support/Avian/Bundles/Turtle.tmbundle`. That way it's kept distinct from bundles installed through the bundle installer. Textmate should notice the new bundle automatically; but when in doubt, just restart Textmate (`⌃ + ⌘ + Q`).     ## Screenshots    ![Screenshot of expanded bundle menu](./Support/img/screenshot-menu.png ""Screenshot of expanded bundle menu"")    ![Screenshot showing SPARQL execution](./Support/img/screenshot-sparql-exec.png ""Screenshot showing SPARQL execution"")    ![Screenshot editor showing auto-completion for resource identifier and documentation](./Support/img/screenshot-editor.png ""Screenshot editor showing auto-completion for resource identifier and documentation"")    ![Screenshot of knowledge base visualization](./Support/img/screenshot-visu.png ""Screenshot of knowledge base visualization"")    ## Meta    Turtle.tmbundle was created by [Peter Geil](http://github.com/peta). Feedback is highly welcome – if you find a bug, have a feature request or simply want to contribute something, please let me know. Just visit the official GitHub repository at [https://github.com/peta/turtle.tmbundle](https://github.com/peta/turtle.tmbundle) and open an [issue](https://github.com/peta/turtle.tmbundle/issues).    ### Please help making TextMate2 even more awesome!    One of the features EVERY user could greatly benefit from, is a more powerful auto-completion feature. However, the implementation of such a feature takes a considerable amount of time. Unfortunately time is one of those goods, Allan (the creator of TextMate) and the other guy(s) from Macromates don't have enough from. So I had to idea to start a crowdfunding campaign to raising enough funds for working two months as full-time contributor to the TextMate 2 project. [Visit my campaign page and contribute!](http://www.indiegogo.com/projects/textmate-dialog2-sprint).    ## Roadmap    + Extract individuals (for both, autocompletion and documentation)  + Work out documentation component  	+ Display resource documentation as HTML text (with clickable links to official sources) in a notification window  + Polish language grammar  + Add additional caching layer for speeding up things (vanilla MacOS Ruby 1.8.7 has only sloooow REXML module)  + Convert RDF/S and OWL documents from XML into Turtle and ""link"" resource identifiers to them, so that users can jump/navigate across all involved documents  + To be fixed  	+ Fix PN_LOCAL pattern so that semicolons inside POLs are marked up as terminators  + Rewrite Turtle.tmBundle as nodejs module and publish as plugin for web-based code editors like Cloud9 and Atom """
Semantic web;https://github.com/kasei/swift-hdt;"""# swift-hdt    ## An HDT RDF Parser    ### Build    On MacOS 10.14:    ```  % swift build -Xswiftc ""-target"" -Xswiftc ""x86_64-apple-macosx10.14""  ```    On Linux:    ```  % swift build  ```    ### Parse an HDT file    ```  % ./.build/release/hdt-parse swdf-2012-11-28.hdt  _:b1 <http://www.w3.org/1999/02/22-rdf-syntax-ns#_1> <http://data.semanticweb.org/person/barry-norton> .  _:b1 <http://www.w3.org/1999/02/22-rdf-syntax-ns#_2> <http://data.semanticweb.org/person/reto-krummenacher> .  _:b1 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq> .  _:b10 <http://www.w3.org/1999/02/22-rdf-syntax-ns#_1> <http://data.semanticweb.org/person/robert-isele> .  _:b10 <http://www.w3.org/1999/02/22-rdf-syntax-ns#_2> <http://data.semanticweb.org/person/anja-jentzsch> .  _:b10 <http://www.w3.org/1999/02/22-rdf-syntax-ns#_3> <http://data.semanticweb.org/person/christian-bizer> .  _:b10 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq> .  ...  ```    ### Limitations    This project is early in development, and has many limitations:    * Only serializing the entire HDT file is possible (triple pattern matching is planned for the future)  * Only ""Four Part"" dictionary encoding is currently supported  * Only ""Log64"" encoding of bitmap triples values is currently supported    ### References    * [HDT](http://www.rdfhdt.org)  * [HDT Binary Format](http://www.rdfhdt.org/hdt-binary-format/) """
Semantic web;https://github.com/Swirrl/grafter;"""# Grafter - Linked Data & RDF Processing    [![Clojars Project](https://img.shields.io/clojars/v/grafter.svg)](https://clojars.org/grafter)        ""For the hard graft of linked data processing.""    Grafter is a [Clojure](http://clojure.org/) library for linked data  processing.  It is mature and under active development.    It provides support for all common RDF serialisations and  includes a library of functions for querying and writing to SPARQL  repositories.    ## FAQ    *Where can I find the api-docs?*    [Latest docs](http://swirrl.github.io/grafter)    Legacy docs [api.grafter.org](http://api.grafter.org/)    *Didn't grafter also contain tools for tabular processing?*    As of 0.9.0 the `grafter.tabular` library has been moved into a  [separate repository](https://github.com/Swirrl/grafter.tabular) so  the core grafter library can focus on processing linked data.    This part of the library is now considered deprecated.  If you depend  on it you can still use it, and it may receive occaisional  maintainance updates.    If you're looking to start a greenfield project then you can easily  wire up any capable CSV/excel parser to the RDF processing side of  grafter.    ## License    Copyright © 2014 Swirrl IT Ltd.    Distributed under the Eclipse Public License version 1.0, the same as  Clojure. """
Semantic web;https://github.com/streamreasoning/TripleWave;"""# TripleWave    All the information regarding how to install, run and customize TripleWave can be found [here](http://streamreasoning.github.io/TripleWave/) """
Semantic web;https://github.com/lyrasis/docker-blazegraph;"""# Docker Blazegraph    Run Blazegraph in Docker.    ## Quickstart    ```bash  docker run --name blazegraph -d -p 8889:8080 lyrasis/blazegraph:2.1.5  docker logs -f blazegraph  ```    ## Local builds    ```bash  docker build -t blazegraph:2.1.5 2.1.5/  docker run --name blazegraph -d -p 8889:8080 blazegraph:2.1.5  docker logs -f blazegraph  ```    ## Loading data (example)    Files or directories need to be made available to the container:    ```bash  # using a host volume mount to make files available  mkdir -p /tmp/blazegraph/data/  cp data/authoritieschildrensSubjects.nt /tmp/blazegraph/data/    # set uid / gid for container, example is ubuntu primary user compatible  BLAZEGRAPH_UID=$UID  BLAZEGRAPH_GID=$GROUPS    # start container making files available under /data  docker run --name blazegraph -d \    -e BLAZEGRAPH_UID=$BLAZEGRAPH_UID \    -e BLAZEGRAPH_GID=$BLAZEGRAPH_GID \    -p 8889:8080 \    -v $PWD/data/RWStore.properties:/RWStore.properties \    -v /tmp/blazegraph/data/:/data \    lyrasis/blazegraph:2.1.5    # create payload config  cp data/dataloader.txt.example dataloader.txt    # trigger data import  curl -X POST \    --data-binary @dataloader.txt \    --header 'Content-Type:text/plain' \    http://localhost:8889/bigdata/dataloader  ```    Sample query:    ```sparql  prefix bds: <http://www.bigdata.com/rdf/search#>  select ?identifier ?value  where {    ?value bds:search ""Women"" .    ?value bds:matchAllTerms ""true"" .    ?identifier <http://www.loc.gov/mads/rdf/v1#authoritativeLabel> ?value .  }  ```    --- """
Semantic web;https://github.com/tomatophantastico/sparqlmap;"""#SparqlMap - Client      SparqlMap - A SPARQL to SQL rewriter based on [R2RML](http://www.w3.org/TR/r2rml/) specification.    It can be used in allows both extracting RDF from an relational database and rewrite SPARQL queries into SQL.    The SparqlMap Client provides command line and web access to the SparqlMap core.    ## Download    Get the latest release on the [release page](https://github.com/tomatophantastico/sparqlmap/releases).  Please note, that no MySQL driver is included. You will need to get it from the [MySQL page](https://dev.mysql.com/downloads/connector/j/)  and copy it into the ./lib folder.      ## Building    We use a [patched version](https://github.com/tomatophantastico/metamodel) of [Apache Metamode](http://metamodel.apache.org/), which improves quotation mark handling.    To include it into your SparqlMap-build please install it locally (using [maven](http://maven.apache.org))  ```  git clone --branch feature/quoteColumns https://github.com/tomatophantastico/metamodel && cd metamodel && mvn install  ```    SparqlMap utilizes gradle, so you can just run  ```  ./gradle installDist  ```  which will create a distribution in `build/install/sparqlmap`.            ## Overview over the mapping process    ![SparqlMap overview](https://raw.github.com/tomatophantastico/sparqlmap/doc/doc/sparqlMap.png)        ##Quick Start    Most of the time, dump creation will take place on the command line.  In the binary distributions use the sparqlmap command.   Calling sparqlmap without or with a wrong combination of options will present all options available.    Let's have a look at some samples:    ### RDF Dump    ```shell  ./bin/sparqlmap --action=dump --ds.type=JDBC --ds.url=""jdbc:mysql://192.168.59.103:3306/sparqlmaptest"" --ds.username=sparqlmap --ds.password=sparqlmap  --r2rmlFile=src/test/resources/hsql-bsbm/mapping.ttl     ```  Or if you do not have a R2RML mapping, you can create a dump based on a Direct Mapping    ```shell  ./bin/sparqlmap --action=dump --ds.type=JDBC --ds.url=""jdbc:mysql://192.168.59.103:3306/sparqlmaptest"" --ds.username=sparqlmap --ds.password=sparqlmap   ```      ## R2RML Mappings    ### Re-use an Existing Mapping  Is quite simple, just provide the ```-r2rml.file```parameter:   ```  -r2rml.file=<fileLocation>  ```    ### Creation of a Mapping    Creating a R2RML representation of a default mapping is as easy as this, just change the action:    ```shell  ./bin/sparqlmap --action=directmapping --ds.type=JDBC --ds.url=""jdbc:mysql://192.168.59.103:3306/sparqlmaptest"" -ds.username=sparqlmap  --ds.password=sparqlmap    ```    ## Direct Mapping options  With the following options, the generation of the direct mapping can be controlled.  These options are meant to ease manual editing of the generated R2RML file, or when the     * a given baseuriprefix is suffixed by either mapping, vocab or instance to produce the corresponding uri prefixes.  * the mappinguriprefix will only show up in the resulting r2rml-file for name the R2RML Resources, such as TriplesMaps  * vocaburiprefix helps constructing useful identifiers for predicates and class generated from the schema of the data source. It will therefore show up in the extraced data.  * Resources generated in the RDB-to-RDF translation process use the instanceuriprefix for IRI generation.    ```  --dm.baseuriprefix=<baseuri/>  --dm.mappinguriprefix=<baseuri/mapping/>  --dm.vocaburiprefix=<basuri/vocab/>  --dm.instanceuriprefix=<baseuri/instance>  --dm.separatorchar=#  ```              ## Rewrite SPARQL queries into SQL    For rewriting SPARQL queries into SQL SparqlMap can expose a SPARQL endpoint by an embedded tomcat.  The enpoint is started by     This will expose an SPARQL endpoint with a little snorql interface.        ## R2RML conformance    SparqlMap conforms with the R2RML specification and was tested with PostgreSQL, MySQL and HSQL.      ## Adding additional database drivers    Simply copy them into the lib folder.    ## Building and installation    SparqlMap requires the following dependencies, which are not present in the main repos.    2. Metamodel with a patch  ```shell  git clone https://github.com/tomatophantastico/metamodel.git  cd metamodel  mvn install  ```      # Actions    ## CLI  SparqlMap allows the following command line interactions, selected by the `--action=<action>´ parameter    ### dump    This creates a dump of the mapped database. You can specify the outputformat, using the `--format=<FORMAT>` parameter.  Supported is TURTLE,  TRIG, NTRIPLES and NQUADS.  Triple serializations simply ignore the graph definitions of the mappings. A triple that occurs in multiple graphs will consequently appear multiple times in the result.    ### directmapping    A R2RML mapping is created, based on the concept of a direct mapping, always yields TURTLE output.    ### query    A query is executed and the result is returned on the command line.    ## web  SparqlMap can be used as a SPARQL enndpoint, for example for exposing a ACCESS database:    ```shell  bin/sparqlmap --action=web --ds.type=ACCESS --ds.url=mydata.accdb  ```     The endpoint will be accessible on:  ```  localhost:8080/sparql  ```  Currently, a number of limitations apply:  * Some the query processing is executed in-memory, which degrades performance  * There is no further UI for entering and viewing queries        # Mapping Options  Existing mapping files can be provided via the r2rmlfile-parameter:  ```  --r2rmlFile=<filename>  ```  If this parameter is omitted, a direct mapping will be created.    The direct mapping generation can be modified by following attributes.  If you just define the  ```<baseUriPrefix>``` you should be fine in most cases.  ```  --dm.baseUriPrefix=http://localhost/baseiri/  --dm.mappingUriPrefix=<baseUriPrefix>/mapping/  --dm.vocabUriPrefix=<baseUriPrefix>/instance/  --dm.instanceUriPrefix=<baseUriPrefix>/vocab/  --dm.separatorChar=+  ```  # Data Sources      ## Using with MongoDB (```--ds.type=MONGODB2``` or ```--ds.type=MONGODB3```)  ```  --ds.type=MONGODB2 or --ds.type=MONGODB3  --ds.url  the host and port of the mongodb server, e.g.: localhost:11111  --ds.dbname the database name  --ds.username  username  --ds.password  password  ```    ## Using with a JDBC database  First, make sure that a JDBC4 driver is in the classpath. SparqlMap already contains drivers for the most important FOSS RDBMs, for closed source RDBMs, they have to be added manually.    ```  --ds.url  the full jdbc url, e.g. jdbc:mysql://localhost:3306/test  --ds.username  username of the RDBMS  --ds.password  password of the RDBMS  --ds.maxPoolSize max number of connections to the RDBMs, defaults to 10  ```    ## Using with CSV files (--ds.type=CSV)  For more details check the [Apache MetaModel CSV adapter wiki page](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65875503)    Required parameters  ```  --ds.type=CSV If it is not CSV, you will have to look at your other options  --ds.url=<path> required  The path to the file  ```  Optional parameters and their defaults  ```  --ds.quoteChar="" Encloses values  --ds.separatorChar=, default to , values in a row are split according to this value  --ds.escapeChar=\ for escaping special characters  --ds.encoding=UTF-8   --ds.columnNameLineNumber=1 Starting from 1,   --ds.failOnInconsistentRowLength=true defaults to true if the column count varies in the file, this pushes the parser further.  --ds.multilineValues=false allows multiline values  ```    ##Excel-Files  ```  --ds.type=EXCEL  Mandatory  --ds.url=<path> required  The path to the file  ```  Optional  ```  --ds.columnNameLineNumber=1  --ds.SkipEmptyLines=true  --ds.SkipEmptyColumns=true  ```    ## Access-Files  Besides the type, only the file location needs to be provided  ```  --ds.type=ACCESS  Mandatory  --ds.url=<path>  ```  ##CouchDB  ```  --ds.type=COUCHDB  Mandatory  --ds.url=<httpurl> required  The url of the couchdbserver (using ektorp)  ```  Optionally:  ```  --ds.username=<username>  --ds.password=<password>  ``` """
Semantic web;https://github.com/fluentdesk/FRESCA;"""The FRESH Resume Schema  ======  *A rational schema for your résumé or CV. Based on [FRESH][f].*    The [FRESH résumé schema][fresh] is an open source, standards-friendly,  JSON/YAML-driven format for résumé / CVs and other employment artifacts.    - [**View the official FRESH schema document.**][schema]  - [**View a sample FRESH resume.**][exemplar]    FRESH is supported natively by [HackMyResume][hmr] and can be trivially  converted to and from the [JSON Resume][jrs] format.    ## What It Does    FRESCA establishes an optimized, human-readable, computer-friendly  representation for your résumé and career data based on JSON or equivalent  YAML...    ```js  // Pared-down FRESH/FRESCA resume representation (JSON)  {    ""name"": ""Jane Doe"",    ""info"": { /* Basic info */ },    ""contact"": { /* Contact information */ },    ""location"": { /* Location / address */ },    ""meta"": { /* Resume metadata */ },    ""employment"": { /* Employment history */ },    ""projects"": [ /* Project history */ ],    ""skills"": [ /* Skills and technologies */ ],    ""education"": { /* Schools, training, certifications */ },    ""affiliation"": { /* Clubs, groups, and associations */ },    ""service"": { /* Volunteer, military, civilian service */ },    ""disposition"": { /* Disposition towards work, relocation, schedule */ },    ""writing"": [ /* Writing, blogging, and publications */ ],    ""reading"": [ /* Books and publication a la StackOverflow Careers */ ],    ""speaking"": [ /* Writing, blogging, and publications */ ],    ""governance"": [ /* Board memberships, committees, standards groups */ ],    ""recognition"": [ /* Awards and commendations */ ],    ""samples"": [ /* Work samples and portfolio pieces */ ],    ""social"": [ /* Social networking & engagement */ ],    ""references"": [ /* Candidate references */ ],    ""testimonials"": [ /* Public candidate testimonials */ ],      ""extracurricular"": [ /* Interests & hobbies */ ],    ""interests"": [ /* Interests & hobbies */ ],    ""languages"": [ /* languages spoken */ ]  }  ```    ...which you can use to generate resumes and other career artifacts in specific  concrete formats (HTML, LaTeX, Markdown, PDF, etc.) as well as enable  21st-century analysis of your resume and career data in a way that's not  possible with traditional, 20th-century resume tools and formats.    ## Anatomy of a FRESH Resume    FRESH resumes are:    - Text-based.  - Versionable.  - Standards-compliant.  - Human-readable/editable.  - Computer-friendly / easily parsable by tools.  - Built from JSON or equivalent YAML.  - Used to generate specific formats like HTML, PDF, or LaTeX.  - Free from proprietary structures or site- and/or tool-specific lock-in.    ## License    The FRESH resume schema is licensed under MIT. Go crazy.    [f]: https://freshstandard.org  [hmr]: https://fluentdesk.com/hackmyresume  [fresh]: https://resume.freshstandard.org  [schema]: schema/fresh-resume-schema.json  [cli]: https://www.npmjs.com/package/fluentcv  [fluentcv]: https://fluentdesk.com/fluentcv  [jrs]: http://jsonresume.org  [exemplar]: https://github.com/fluentdesk/jane-q-fullstacker/blob/master/resume/jane-resume.json  [npm]: https://www.npmjs.com/package/fluentcv """
Semantic web;https://github.com/arc-lasalle/AutoMap4OBDA;"""# AutoMap4OBDA  AutoMap4OBDA is a system which automatically generates R2RML mappings based on the intensive use of relational source contents and features of the target ontology. AutoMap4OBDA takes as inputs a relational database (i.e., PostgreSQL) and an ontology in OWL to produce a putative ontology from a relational database which is used as an intermediate element in the relational-to-ontology process. Moreover, AutoMap4OBDA has been designed to be used in OBDA scenarios and is able to generate fully compliant R2RML mappings without user intervention.    In AutoMap4OBDA, the database content and features of the target ontology are taken into account during the mapping generation process. We have developed three techniques that make the mapping process strongly dependent on the input database and the features of the target ontology to increase the performance of the relational-to-ontology mappings.    - Ontology learning technique to infer class hierarchies for development of a putative ontology  - String similarity metric selection based on target ontology labels for ontology alignment  - Short path strategy for R2RML mapping generation based on alignments    This is the first version which will require some code cleansing and refactoring. Currently it is only supporting PostgreSQL.    Use:  ```  java -jar automap4obda.jar -db <databaseURL> -schema <schemaname> -driver <databaseDriver> -u <username> - p ""<password>""   -n <ontologyname> -d <path-to-domainontology> -o <outputfiles>   [-attrasclass <0/1>] [-ol <0/1>] [-olclasstable <0/1>] [-olclassnamealone <0/1>] [-extendedmappings <0/1>]  ```    Example:  ```  java -jar automap4obda.jar -db jdbc:postgresql:postgres -schema sigkdd_structured -driver org.postgresql.Driver -u postgres -p ""postgres""   -n sigkdd_structured_putative -d ""c:\...\sigkdd_structured.ttl"" -o sigkdd_structured_putative   -attrasclass 1 -ol 1 -olclasstable 1 -olclassnamealone 1  -extendedmappings 1  ```      - Password can be empty  - attrasclass: Attributes as classes option (default 1)  - ol: Ontology learning technique option (default 1)  - olclasstable: New classes in ontology learning technique belongs to the table class (1) or to the column class (0) (default 1)  - olclassnamealone: New classes in ontology learning technique is taken as it is in the DB (1) or the name of the table/column is attached (0) (default 1)  - extendedmappings: Short path technique option (default 1)    http://arc.salleurl.edu/automap4obda    Copyright (C) 2016 ARC Engineering and Architecture La Salle, Ramon Llull University.     for comments please contact Alvaro Sicilia (ascilia@salleurl.edu)   """
Semantic web;https://github.com/castagna/EARQ;"""EARQ = ElasticSearch + ARQ   ==========================    EARQ is a combination of ARQ and ElasticSearch. It gives ARQ the ability to   perform free text searches using an ElasticSearch cluster. ElasticSearch   indexes are additional information for accessing the RDF graph, not storage   for the graph itself.    This is *experimental* (and unsupported).      How to use it  -------------    This is how you build an index from a Jena Model:        ModelIndexerString indexer = new ModelIndexerString(""earq_index"");      indexer.indexStatements(model.listStatements());      indexer.close();    This is how you configure ARQ to use ElasticSearch:                IndexSearcher searcher = IndexSearcherFactory.create(Type.ELASTICSEARCH, ""earq_index"") ;      EARQ.setDefaultIndex(searcher) ;    This is an example of a SPARQL query using the sarq:search property function:         PREFIX earq: <http://openjena.org/EARQ/property#>      SELECT * WHERE {          ?doc ?p ?lit .          (?lit ?score ) earq:search ""+text"" .      }      Acknowledgement  ---------------            The design and part of the code has been inspired from LARQ, see:     * [http://openjena.org/ARQ/lucene-arq.html](http://openjena.org/ARQ/lucene-arq.html)      Todo  ----     * ... still broken   * Remove LARQ reference from ModelIndexerSubject """
Semantic web;https://github.com/eBay/akutan;"""# Akutan    [![Build Status](https://travis-ci.com/eBay/akutan.svg?branch=master)](https://travis-ci.com/eBay/akutan)  [![GoDoc](https://godoc.org/github.com/ebay/akutan/src/github.com/ebay/akutan?status.svg)](https://godoc.org/github.com/ebay/akutan/src/github.com/ebay/akutan)    There's a blog post that's a [good introduction to Akutan](https://www.ebayinc.com/stories/blogs/tech/beam-a-distributed-knowledge-graph-store/).    Akutan is a distributed knowledge graph store, sometimes called an RDF store or a  triple store. Knowledge graphs are suitable for modeling data that is highly  interconnected by many types of relationships, like encyclopedic information  about the world. A knowledge graph store enables rich queries on its data, which  can be used to power real-time interfaces, to complement machine learning  applications, and to make sense of new, unstructured information in the context  of the existing knowledge.    How to model your data as a knowledge graph and how to query it will feel a bit  different for people coming from SQL, NoSQL, and property graph stores. In a  knowledge graph, data is represented as a single table of *facts*, where each  fact has a *subject*, *predicate*, and *object*. This representation enables the  store to sift through the data for complex queries and to apply inference rules  that raise the level of abstraction. Here's an example of a tiny graph:    subject         | predicate | object  ----------------|-----------|-----------------  `<John_Scalzi>` | `<born>`  | `<Fairfield>`  `<John_Scalzi>` | `<lives>` | `<Bradford>`  `<John_Scalzi>` | `<wrote>` | `<Old_Mans_War>`    To learn about how to represent and query data in Akutan, see  [docs/query.md](docs/query.md).    Akutan is designed to store large graphs that cannot fit on a single server. It's  scalable in how much data it can store and the rate of queries it can execute.  However, Akutan serializes all changes to the graph through a central log, which  fundamentally limits the total rate of change. The rate of change won't improve  with a larger number of servers, but a typical deployment should be able to  handle tens of thousands of changes per second. In exchange for this limitation,  Akutan's architecture is a relatively simple one that enables many features. For  example, Akutan supports transactional updates and historical global snapshots. We  believe this trade-off is suitable for most knowledge graph use cases, which  accumulate large amounts of data but do so at a modest pace. To learn more about  Akutan's architecture and this trade-off, see  [docs/central_log_arch.md](docs/central_log_arch.md).    Akutan isn't ready for production-critical deployments, but it's useful today for  some use cases. We've run a 20-server deployment of Akutan for development  purposes and off-line use cases for about a year, which we've most commonly  loaded with a dataset of about 2.5 billion facts. We believe Akutan's current  capabilities exceed this capacity and scale; we haven't yet pushed Akutan to its  limits. The project has a good architectural foundation on which additional  features can be built and higher performance could be achieved.    Akutan needs more love before it can be used for production-critical deployments.  Much of Akutan's code consists of high-quality, documented, unit-tested modules,  but some areas of the code base are inherited from Akutan's earlier prototype days  and still need attention. In other places, some functionality is lacking before  Akutan could be used as a critical production data store, including deletion of  facts, backup/restore, and automated cluster management. We have filed  GitHub issues for these and a few other things. There are also areas where Akutan  could be improved that wouldn't necessarily block production usage. For example,  Akutan's query language is not quite compatible with Sparql, and its inference  engine is limited.    So, Akutan has a nice foundation and may be useful to some people, but it also  needs additional love. If that's not for you, here are a few alternative  open-source knowledge and property graph stores that you may want to consider  (we have no affiliation with these projects):    - [Blazegraph](https://github.com/blazegraph/database): an RDF store. Supports    several query languages, including SPARQL and Gremlin. Disk-based,    single-master, scales out for reads only. Seems unmaintained. Powers    <https://query.wikidata.org/>.  - [Dgraph](https://github.com/dgraph-io/dgraph): a triple-oriented property    graph store. GraphQL-like query language, no support for SPARQL. Disk-based,    scales out.  - [Neo4j](https://github.com/neo4j/neo4j): a property graph store. Cypher query    language, no support for SPARQL. Single-master, scales out for reads only.  - See also Wikipedia's    [Comparison of Triplestores](https://en.wikipedia.org/wiki/Comparison_of_triplestores)    page.    The remainder of this README describes how to get Akutan up and running. Several  documents under the `docs/` directory describe aspects of Akutan in more  detail; see [docs/README.md](docs/README.md) for an overview.    ## Installing dependencies and building Akutan    Akutan has the following system dependencies:   - It's written in [Go](https://golang.org/). You'll need v1.11.5 or newer.   - Akutan uses [Protocol Buffers](https://developers.google.com/protocol-buffers/)     extensively to encode messages for [gRPC](https://grpc.io/), the log of data     changes, and storage on disk. You'll need protobuf version 3. We reccomend     3.5.2 or later. Note that 3.0.x is the default in many Linux distributions, but     doesn't work with the Akutan build.   - Akutan's Disk Views store their facts in [RocksDB](https://rocksdb.org/).    On Mac OS X, these can all be installed via [Homebrew](https://brew.sh/):    	$ brew install golang protobuf rocksdb zstd    On Ubuntu, refer to the files within the [docker/](docker/) directory for  package names to use with `apt-get`.    After cloning the Akutan repository, pull down several Go libraries and additional  Go tools:    	$ make get    Finally, build the project:    	$ make build    ## Running Akutan locally    The fastest way to run Akutan locally is to launch the in-memory log store:    	$ bin/plank    Then open another terminal and run:    	$ make run    This will bring up several Akutan servers locally. It starts an API server that  listens on localhost for gRPC requests on port 9987 and for HTTP requests on  port 9988, such as <http://localhost:9988/stats.txt>.    The easiest way to interact with the API server is using `bin/akutan-client`. See  [docs/query.md](docs/query.md) for examples. The API server exposes the  `FactStore` gRPC service defined in  [proto/api/akutan_api.proto](proto/api/akutan_api.proto).    ## Deployment concerns    ### The log    Earlier, we used `bin/plank` as a log store, but this is unsuitable for real  usage! Plank is in-memory only, isn't replicated, and by default, it only  keeps 1000 entries at a time. It's only meant for development.    Akutan also supports using [Apache Kafka](https://kafka.apache.org/) as its log  store. This is recommended over Plank for any deployment. To use Kafka, follow the  [Kafka quick start](https://kafka.apache.org/quickstart) guide to install  Kafka, start ZooKeeper, and start Kafka. Then create a topic called ""akutan""  (not ""test"" as in the Kafka guide) with `partitions` set to 1. You'll want to  configure Kafka to synchronously write entries to disk.    To use Kafka with Akutan, set the `akutanLog`'s `type` to `kafka` in your Akutan  configuration (default: `local/config.json`), and update the `locator`'s  `addresses` accordingly (Kafka uses port 9092 by default). You'll need to clear  out Akutan's Disk Views' data before restarting the cluster. The Disk Views  by default store their data in $TMPDIR/rocksdb-akutan-diskview-{space}-{partition}  so you can delete them all with `rm -rf $TMPDIR/rocksdb-akutan-diskview*`    ### Docker and Kubernetes    This repository includes support for running Akutan inside  [Docker](https://www.docker.com/) and  [Minikube](https://kubernetes.io/docs/setup/minikube/). These environments can  be tedious for development purposes, but they're useful as a step towards a  modern and robust production deployment.    See `cluster/k8s/Minikube.md` file for the steps to build and deploy Akutan  services in `Minikube`. It also includes the steps to build the Docker images.    ### Distributed tracing    Akutan generates distributed [OpenTracing](https://opentracing.io/) traces for use  with [Jaeger](https://www.jaegertracing.io/). To try it, follow the  [Jaeger Getting Started Guide](https://www.jaegertracing.io/docs/getting-started/#all-in-one-docker-image)  for running the all-in-one Docker image. The default `make run` is configured to  send traces there, which you can query at <http://localhost:16686>. The Minikube  cluster also includes a Jaeger all-in-one instance.    ## Development    ### VS Code    You can use whichever editor you'd like, but this repository contains some  configuration for [VS Code](https://code.visualstudio.com/Download). We  suggest the following extensions:   - [Go](https://marketplace.visualstudio.com/items?itemName=ms-vscode.Go)   - [Code Spell Checker](https://marketplace.visualstudio.com/items?itemName=streetsidesoftware.code-spell-checker)   - [Rewrap](https://marketplace.visualstudio.com/items?itemName=stkb.rewrap)   - [vscode-proto3](https://marketplace.visualstudio.com/items?itemName=zxh404.vscode-proto3)   - [Docker](https://marketplace.visualstudio.com/items?itemName=PeterJausovec.vscode-docker)    Override the default settings in `.vscode/settings.json` with  [./vscode-settings.json5](./vscode-settings.json5).    ### Test targets    The `Makefile` contains various targets related to running tests:    Target       | Description  ------------ | -----------  `make test`  | run all the akutan unit tests  `make cover` | run all the akutan unit tests and open the web-based coverage viewer  `make lint`  | run basic code linting  `make vet`   | run all static analysis tests including linting and formatting    ## License Information    Copyright 2019 eBay Inc.    Primary authors: Simon Fell, Diego Ongaro, Raymond Kroeker, Sathish Kandasamy    Licensed under the Apache License, Version 2.0 (the ""License""); you may not use  this file except in compliance with the License. You may obtain a copy of the  License at <https://www.apache.org/licenses/LICENSE-2.0>.    Unless required by applicable law or agreed to in writing, software distributed  under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR  CONDITIONS OF ANY KIND, either express or implied. See the License for the  specific language governing permissions and limitations under the License.      ----  **Note** the project was renamed to Akutan in July 2019. """
Semantic web;https://github.com/KMax/cqels;"""# CQELS (Continuous Query Evaluation over Linked Data)    This repository is a fork of https://code.google.com/p/cqels/ repository on Google Code.    _DISCLAMER: I'm not a developer of the original CQELS._    ## Install    Add the following repository to your pom.xml:  ```  <repository>      <id>cqels.mvn-repo</id>      <url>https://raw.github.com/KMax/cqels/mvn-repo/</url>      <snapshots>          <enabled>true</enabled>          <updatePolicy>always</updatePolicy>      </snapshots>  </repository>  ```    and declare the following dependency:  ```  <dependency>      <groupId>org.deri.cqels</groupId>      <artifactId>cqels</artifactId>      <version>...</version>  </dependency>  ```    ## Releases  ### 1.0.0  * Mavenized build,  * Support of [BIND](http://www.w3.org/TR/sparql11-query/#bind) operator,  * Initial support of remote SPARQL endpoints via [SPARQL Graph Protocol](http://www.w3.org/TR/sparql11-http-rdf-update/),  * Fixed an FileException (at ObjectFileStorage) exception _([commit](https://github.com/KMax/cqels/commit/4382fe7e2f15a8c205a47ab3cd0e25842e558c30))_.    ### 1.1.0  * Updated [Jena TDB](https://jena.apache.org/documentation/tdb/) up to 1.1.2 version    Code license: [LGPLv3.0](https://github.com/KMax/cqels/blob/master/LICENSE) """
Semantic web;https://github.com/notruthless/csv2rdf;"""csv2rdf  =======    Ruth Helfinstein  6/24/2012    CSV to RDF translator  Does a simple CSV to RDF translation.    - Assumes the first line contains the attribute names  - Assumes all instances are separated by newlines and contain the right number of elements  - Items in the csv file may have quotes or not, it works either way.      Outputs the rdf file as <filename>.rdf  NOTE: It does not currently check if the output file already exists and will overwrite it if it does.    Looks for a configuration file called <filename>-config.csv, where <filename>.csv is the input file.  Each line in the configuration file describes one item in the header line of the input csv file  <csv_name>, <type>(<optional rdf_name>), <class>    where <csv_name> is the name seen in the header of the column in the csv file.   <type> is either ""class"" or ""property"" or ""ignore"" (without quotes)  <rdf_name> in parenthesis is optional, indicates name to use for this property or class in the rdf file  <class> is the class (for a property) or superclass (for a class)     the lines in the config file do not have to be in the same order as the columns in the csv file  with the exception of any columns with a blank name.  These will be renamed ""unlabeled1"" ""unlabeled2""   and will only match if they are in the same order.    NOTE: any columns NOT listed in the configuration file will be ignored in the rdf file.    If no configuration file is found, the program will create a basic one    Files:  csv2rdf.jar  example.csv  - sample input file  example-config.csv - sample config file  src - source files    To run CSV to RDF from the console:  java -jar csv2rdf.jar <input csv file name>  *** NOTE You can specify the name of the .csv file to use on the command line, otherwise it will default to input.csv        for example   	java -jar csv2rdf.jar input.csv  creates  	input.rdf    Changes 2012.08.02  • Add configuration file    Changes 2012.07.04  • Ignore leading or trailing spaces in the csv  • Add a unlabeled1,unlabled2, unlabeld3... to any attribute in the csv that is not named.  • If there is a blank in the data, do not  add the property name to instance    • Add additional input question: Do you want an 'all' class?   If the user answers yes an ""all"" class is created as the  only subclass to Thing and everything  is subclassed to ""all"".         """
Semantic web;https://github.com/albertmeronyo/docker2rdf;"""# docker2rdf  Mapper to represent Dockerfiles as RDF triples """
Semantic web;https://github.com/jpcik/morph-streams;"""morph-streams  =============    To build morph-streams you need:    * jvm7  * sbt (www.scala-sbt.org)    To compile it, run sbt after downloading the code:    ```  >sbt  >compile  ```    To see some sparql-stream queries running with esper, you can check some simple tests (QueryExecutionTest):    ```  >sbt  >project adapter-esper  >test  ```   """
Semantic web;https://github.com/Omer/vim-sparql;"""# VIM-SPARQL    This is an unofficial fork of [http://www.vim.org/scripts/script.php?script_id=1755](http://www.vim.org/scripts/script.php?script_id=1755). This fork simply adds a filetype detection.    This is a first stab at syntax highlighting for SPARQL.  Very useful if you write SPARQL in, for example, `rq` files that you can use with the `roqet` command line query processor.     More information on SPARQL at [http://www.w3.org/TR/rdf-sparql-query/](http://www.w3.org/TR/rdf-sparql-query/).    Original by Jeroen Pulles, 2007-01-07 """
Semantic web;https://github.com/architolk/Linked-Data-Theatre;"""# Linked Data Theatre  The Linked Data Theatre (LDT) is a platform for an optimal presentation of Linked Data.    ### Installation and usage  The easiest way to use the LDT is to install the latest release in a Tomcat container, by following the instructions in [DEPLOY.md](docs/DEPLOY.md):    - [ldt-1.25.2.war](https://github.com/architolk/Linked-Data-Theatre/releases/download/v1.25.2/ldt-1.25.2.war ""ldt-1.25.2.war"")    **NB: Users that upgrade from version 1.17.0 or lower: from version 1.18.0 the LDT can work with any triplestore via the RDF4J interface. You might need to change your configuration a bit. Please look at [DEPLOY.md](docs/DEPLOY.md), sections 4.3 and 4.4.2 for further instructions.**    **NB: Users that upgrade from version 1.9.0 or lower: the config.xml has changed with release 1.10.0. Please make sure that a `<date/>` entry exists after the upgrade!**    The Linked Data Theatre uses a configuration graph containing all the triples that make up the LDT configuration. Instructions and examples how to create such a configuration can be found at the [wiki](https://github.com/architolk/Linked-Data-Theatre/wiki). A [basic-configuration](basic-configuration.ttl) is provided to get you started.    The wiki contains a [tutorial](https://github.com/architolk/Linked-Data-Theatre/wiki/Tutorial) to guide you through the most common features of the Theatre.    ### Build it yourself, linux and docker installations  See [BUILD.md](docs/BUILD.md) for instructions to build the Linked Data Theatre yourself. To deploy the Linked Data Theatre in a Tomcat container, follow the instructions in [DEPLOY.md](docs/DEPLOY.md). A step-by-step installation guide for Linux is also available: [LINUX_SETUP.md](docs/LINUX_SETUP.md). You can also opt for a docker installation, see [DOCKER.md](docs/DOCKER.md).    ### Advanced installation - production settings  * The default setting of the LDT is for development purposes. Read [PRODUCTION.md](docs/PRODUCTION.md) for information about securing the LDT for a production environment.    * To create linked data, the LDT can be extended with the [Linked Data Studio](https://github.com/architolk/Linked-Data-Studio) (LDS). If you install a version of the LDS, it includes a version of the LDT.    * If you want to create a new release of the LDT, please look into [BUILD-LICENSE.md](docs/BUILD-LICENSE.md) for instructions to create the approriate license headers. See [RELEASE.md](docs/RELEASE.md) for all steps to make a release, including upload to github.    * To add security to the Linked Data Theatre, follow the instructions in [SECURE.md](docs/SECURE.md).    * If you run the Linked Data Theatre behind a corporate firewall and access to the internet is restricted by a proxy, follow the instructions in [PROXY.md](docs/PROXY.md).    * If you want to access a secure endpoint (https), but the certificate is untrusted, you will have to set up a keystore. Follow the instructions in [KEYSTORE.md](docs/KEYSTORE.md). """
Semantic web;https://github.com/swirrl/table2qb;"""# table2qb <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Tesseract-1K.gif/240px-Tesseract-1K.gif"" align=""right"" height=""139"" alt=""tesseract animation""/>    [![Build Status](https://travis-ci.com/Swirrl/table2qb.svg?branch=master)](https://travis-ci.com/github/Swirrl/table2qb)    ## Build Statistical Linked-Data with CSV-on-the-Web    Create statistical linked-data by deriving CSV-on-the-Web annotations for your data tables using the [RDF Data Cube Vocabulary](https://www.w3.org/TR/vocab-data-cube/).    Build up a knowledge graph from spreadsheets without advanced programming skills or RDF modelling knowledge.    Simply prepare CSV inputs according to the templates and `table2qb` will output standards-compliant CSVW or RDF.    Once you're happy with the results you can adjust the configuration to tailor the URI patterns to your heart's content.    ## Turn Data Tables into Data Cubes     Table2qb expects three types of CSV tables as input:    - observations: a ['tidy data'](http://vita.had.co.nz/papers/tidy-data.pdf) table with one statistic per row (what the standard calls an _observation_)  - components: another table defining the columns used to describe observations (what the standard calls _component properties_ such as _dimensions_, _measures_, and _attributes_)  - codelists: a further set of tables that enumerate and describe the values used in cells of the observation table (what the standard calls _codes_, grouped into _codelists_)    For example, [the ONS says](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/articles/overviewoftheukpopulation/january2021) that:    > In mid-2019, the population of the UK reached an estimated 66.8 million    This is a single observation value (66.8 million) with two dimensions (date and place) which respectively have two code values (mid-2019 and UK), a single measure (population estimate), and implicitly an attribute for the unit (people).    The [regional-trade example](https://github.com/Swirrl/table2qb/tree/master/examples/regional-trade) goes into more depth. The [colour-coded spreadsheet](./all-colour-coded.ods) should help illustrate how the three types of table come together to describe a cube.    Each of these inputs is processed by it's own pipeline which will output [CSVW](https://w3c.github.io/csvw/metadata/) - i.e. a processed version of the CSV table along with a JSON metadata annotation which describes the translation into RDF. Optionally you can also ask `table2qb` to perform the translation outputting RDF directly that can be loaded into a graph database and queried with SPARQL.    Table2qb also relies on a fourth CSV table for configuration:    - columns: this describes how the observations table should be interpreted - i.e. which components and codelists should be used for each column in the observation tables    This configuration is designed to be used for multiple data cubes across a data collection (so that you can re-use e.g. a ""Year"" column without having to configure anew it each time) to encourage harmonisation and alignment of identifiers.    Ultimately `table2qb` provides a foundation to help you build a collection of interoperable statistical linked open data.    ## Install table2qb    ### Github release    Download the release from [https://github.com/Swirrl/table2qb/releases](https://github.com/Swirrl/table2qb/releases).     Currently the latest is 0.3.0.    Once downloaded, unzip.  The main 'table2qb' executable is in the directory `./target/table2qb-0.3.0` You can add this directory to your `PATH` environment variable, or just run it with the full file path on your system.    To get help on the available commands, type `table2qb help`.    To see the available pipelines (described in more detail below), type `table2qb list`.    To see the required command structure for one of the pipelines (for example the cube-pipeline), type `table2qb describe cube-pipeline`    ### Clojure CLI    Clojure now distributes `clojure` and `cli` command-line programs for running clojure programs. To run `table2qb` through the `clojure` command, first  [install the Clojure CLI tools](https://clojure.org/guides/getting_started). Then create a file `deps.edn` containing the following:       **deps.edn**  ```clojure  {:deps {swirrl/table2qb {:git/url ""https://github.com/Swirrl/table2qb.git""                           :sha ""8c4b22778db0c160b06f2f3b0b3df064d8f8452b""}          org.apache.logging.log4j/log4j-api {:mvn/version ""2.11.0""}          org.apache.logging.log4j/log4j-core {:mvn/version ""2.11.0""}          org.apache.logging.log4j/log4j-slf4j-impl {:mvn/version ""2.11.0""}}   :aliases   {:table2qb    {:main-opts [""-m"" ""table2qb.main""]}}}  ```    You can then run `table2qb` using        clojure -A:table2qb        More details about the `clojure` CLI and the format of the `deps.edn` file can be found [on the Clojure website](https://clojure.org/reference/deps_and_cli)    ## Compiling table2qb    Table2qb is written in Clojure and can be built using [Leiningen](https://leiningen.org/). It is recommended you use [Java 8](https://www.oracle.com/technetwork/java/javase/overview/java8-2100321.html) or later.    `table2qb` can be run through `leiningen` with `lein run` e.g.        lein run list        Alternatively it can be run from an uberjar built with `lein uberjar`. The resulting .jar file in the `target` directory can then be run:        java -jar target/table2qb.jar list    ## How to run table2qb    See [using table2qb](doc/usage.md) for documentation on how to generate RDF data cubes using `table2qb`.    ## Example    The [./examples/employment](./examples/employment) directory provides an example of creating a data cube from scratch with `table2qb`.    ## License    Copyright © 2018 Swirrl IT Ltd.    Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version.    ## Acknowledgements    The development of table2qb was funded by Swirrl, by the UK Office for National Statistics and by the European Union’s Horizon 2020 research and innovation programme under grant agreement No 693849 (the [OpenGovIntelligence](http://opengovintelligence.eu) project). """
Semantic web;https://github.com/vandenoever/rome;"""[![Build Status](https://travis-ci.org/vandenoever/rome.svg?branch=master)](https://travis-ci.org/vandenoever/rome)  [![Current Version](http://meritbadge.herokuapp.com/rome)](https://crates.io/crates/rome)    **Rome** is an **RDF library** written in safe Rust.    [Documentation](https://www.vandenoever.info/rust/rome/)    # Features    - Access any data in a uniform way as RDF by implementing a Graph.  - Read/write Turtle and N-Triple files.  - Iterate over triples in graphs.  - Wrap a graph in code generated from an ontology.  - Use the type system to distinguish between blank nodes, IRIs and literals at    compile time.    # Generated code    The ontology code is generated by these commands:    ```bash  cargo run --example generate_code src/ontology ontologies/*  cargo fmt  ```    # Testing    The Turtle parser passes the [W3 test suite](https://www.w3.org/2013/TurtleTests/).    Run the tests like this:    ```bash  wget https://www.w3.org/2013/TurtleTests/TESTS.tar.gz  tar xf TESTS.tar.gz  cargo run --example w3tests TurtleTests/manifest.ttl  ```    # License    Rome is licensed under AGPLv3.0 or any later version.    ## Contribution    Unless you explicitly state otherwise, any contribution intentionally submitted  for inclusion in the work by you, as defined in the AGPLv3.0 license, shall be licensed as above, without any additional terms or conditions. """
Semantic web;https://github.com/weblyzard/streaming-sparql;"""## Streaming SPARQL  [![Build Status](https://www.travis-ci.org/weblyzard/streaming-sparql.png?branch=master)](https://www.travis-ci.org/weblyzard/streaming-sparql)    Provides a robust, incremental processing of streaming results received from SPARQL servers.   The `StreamingResultSet` iterator yields results as they are received from the server.    ## Javadoc     http://javadoc.io/doc/com.weblyzard.sparql/streaming-sparql/    ## Example code:  ```java  try (StreamingResultSet s = StreamingQueryExecutor.getResultSet(""http://dbpedia.org/sparql"", ""SELECT ?s ?p ?o WHERE { ?s ?p ?o. } LIMIT 5"")) {      while (s.hasNext()) {          System.out.println(""Tupel "" + s.getRowNumber() + "": "" + s.next())      }  }  ```    ## Command line client    Streaming SPARQL also provides a command line client for testing queries.    ### Usage    ```bash  java -jar ./streaming-client-0.0.7-SNAPSHOT.jar  QueryEntitites [URL] [Query]    URL   ... URL to the linked data repository    Query ... The query to perform on the server  ```    ### Example  ```bash  java -jar ./streaming-client-0.0.7-SNAPSHOT.jar http://localhost:8080/rdf4j-sesame/test ""SELECT ?s ?p ?o WHERE { ?s ?p ?o. } LIMIT 5""  ```    ## Background    We have been using Fuseki and RDF4j together with comprehensive result sets (> 100 Mio. tuple) which lead to   instabilities with the native libraries that have been extremely difficult to debug.    Example error messages on the server site have been:    ```  [2017-05-04 19:50:14] Fuseki     WARN  [1450] Runtime IO Exception (client left?) RC = 500 : org.eclipse.jetty.io.EofException        org.apache.jena.atlas.RuntimeIOException: org.eclipse.jetty.io.EofException                                                           ```     ```  [2017-05-04 19:50:14] Fuseki    WARN  (HttpChannel.java:468) (and one from ServletHandler.java:631):  java.io.IOException: java.util.concurrent.TimeoutException: Idle timeout expired: 30001/30000 m  ```    These problems triggered the development of Streaming SPARQL which has proven to be very robust - even for queries that take more than one hour to process and transfer multiple gigabytes of results.  (Note: you will need to call `getResultSet` with a higher timeout to prevent TimeoutExceptions on the server).      ## Compatiblity    Streaming SPARQL is known to work with Jena, OpenRDF, RDF4j and Virtuoso.      ## Changelog    Please refer to the [release](https://github.com/weblyzard/streaming-sparql/releases) page. """
Semantic web;https://github.com/balhoff/whelk;"""<img align=""right"" src=""https://farm7.staticflickr.com/6205/6045158767_e70d43139d_m_d.jpg"">    # Whelk    Whelk is an OWL reasoner based on the algorithm implemented in [ELK](https://github.com/liveontologies/elk-reasoner), as described in [The Incredible ELK](https://doi.org/10.1007/s10817-013-9296-3).    Whelk is implemented as an [immutable functional data structure](https://en.wikipedia.org/wiki/Purely_functional_data_structure), so that each time axioms are added, a new reasoner state is created. References to the previous state remain unchanged. This allows Whelk to answer queries concurrently, and also allows rapid classification of multiple datasets which build upon the same ontology Tbox, which can be classified ahead of time and stored. However, for basic classification of a single ontology, Whelk is much slower than ELK.    In addition to OWL EL classification, Whelk provides OWL RL and a subset of SWRL for reasoning on individuals.    ## Use cases    Whelk is based on ELK, and ELK is much faster at classifying an ontology. Some reasons to use Whelk include:  - object property assertion materialization  - SWRL rules (class and object property atoms)  - OWL RL features for Abox  - some classification for unions in superclass position (e.g., infer least common subsumer of operands)    - Example:       ```      C SubClassOf B      D SubClassOf B      E EquivalentTo C or D      then      E SubClassOf B      ```    - *this feature is useful, but not guaranteed to be complete*  - extended support for Self restrictions, supporting rolification  - in application code, submitting many DL queries programmatically (Whelk is much faster at this)  - in application code, performing many DL queries in parallel, non-blocking  - in application code, storing a reasoning state (e.g., Tbox classification) and simultaneously extending it with multiple independent new axiom sets (e.g., Aboxes); or, quickly rolling back to a previous, saved reasoning state    ## Status    Whelk is under development. It works, but its Scala API is in flux. A basic implementation of the OWL API OWLReasoner interface is included, but to make use of its immutability features, Whelk is best used from Scala. """
Semantic web;https://github.com/amgadmadkour/knowledgecubes;"""![KNOWLEDGECUBES_LOGO](src/main/resources/logo-svg.png)    ## About    A Knowledge Cube, or KC for short, is a semantically-guided data management architecture, where data semantics influences the data management architecture rather than a predefined scheme. KC relies on semantics to define how the data is fetched, organized, stored, optimized, and queried. Knowledge cubes use RDF to store data. This allows knowledge cubes to store Linked Data from the Web of Data. Knowledge cubes envisions breaking down the centralized architecture into multiple specialized cubes, each having its own index and data store.    ## Quick Start Guide    #### Create Encode Data    ```bash  java -cp uber-knowledgecubes-0.1.0.jar:scala-library-2.11.0.jar edu.purdue.knowledgecubes.DictionaryEncoderCLI -i src/main/resources/datasets/original/sample.nt -o /home/amadkour/kclocal/encoded.nt -l /home/amadkour/kclocal -s space  ```  The ```kclocal``` will contain the created dictionaries, the initial data structure used by the store    #### Create Store    ```bash  spark-submit --master local[*] --class edu.purdue.knowledgecubes.StoreCLI target/uber-knowledgecubes-0.1.0.jar -i /home/amadkour/kclocal/encoded.nt -l /home/amadkour/kclocal -f 0.01 -t roaring -d /home/amadkour/kcdb  ```  The database ```kcdb``` directory contains the actual data and reductions for the input NT file. The following is the directory structure of the local store:    #### Run Query Workload    ```bash  spark-submit --master local[*] --class edu.purdue.knowledgecubes.BenchmarkReductionsCLI target/uber-knowledgecubes-0.1.0.jar -l /home/amadkour/kclocal -f 0.01 -t roaring -d /home/amadkour/kcdb -q src/main/resources/queries/original  ```  The command generates the workload reductions under the ```kcdb/reductions/join``` directory. The partitions are saved using parquet format.     #### Local Store Overview    ```  $ ls  amadkour@amadkour:~/kclocal$ ls  GEFI  dbinfo.yaml  dictionary  encoded.nt  join-reductions.yaml    results-20200625115017.txt  tables.yaml  ```    * ```GEFI```: directory represents the generalized filters created for the input datasets.   * ```dbinfo.yaml```: file lists meta-data about the store datasets.   * ```dictionary```: directory containts the string to id mappings created by the dictionary module.   * ```join-reductions.yaml```: directory contains metadata about the generated reductions.  * ```results-20200625115017.txt```: is the output file containing the query performance output when running the benchmarking modules.   * ```tables.yaml```: file lists the meta-data about the tables.    #### Database Directory Overview    ```  amadkour@amadkour:~/kcdb$ ls  data  reductions  ```  The database contains parquet formatted files that represents the original data and reductions:  * ```data```: contains the original data created based on the input NT files.  * ```reductions```: contains the workload-driven reductions created after running a query workload (e.g. after running the Benchmark CLI tool mentioned below).     Program such as spark-shell can be used to view the parquet file content:    ```bash  scala> var data = spark.read.parquet(""/home/amadkour/kcdb/reductions/join/13_TRPO_JOIN_13_TRPS"")  data: org.apache.spark.sql.DataFrame = [s: int, p: int ... 1 more field]    scala> data.show()  +---+---+---+  |  s|  p|  o|  +---+---+---+  | 11| 13|  3|  | 11| 13|  4|  | 12| 13|  5|  |  8| 13|  1|  +---+---+---+  ```    ## WORQ: Workload-Driven RDF Query Processing    KC uses a workload-driven RDF query processing technique, or WORQ for short, for filtering non-matching entries during join evaluation as early as possible to reduce the communication and computation overhead. WORQ generates a reduced sets of triples (or reductions, for short) to represent join pattern(s) of query workloads. WORQ can materialize the reductions on disk or in memory and reuses the reductions that share the same join pattern(s) to answer queries. Furthermore, these reductions are not computed beforehand, but are rather computed in an online fashion. KC also answer complex analytical queries that involve unbound properties. Based on a realization of KC on top of Spark, extensive experimentation demonstrates an order of magnitude enhancement in terms of preprocessing, storage, and query performance compared to the state-of-the-art cloud-based solutions.    ## Features    * A spark-based API for SPARQL querying  * Efficient execution of frequent workload join patterns  * Materialze workload join patterns in memory or on disk  * Efficiently answer unbound property queries    ## Usage    KC provide spark-based API for issuing RDF related operations. There are three steps necessary for running the system:     * Dictionary Encoding  * Store Creation  * Querying    #### Dictionary Encoding     KC requires that the dataset be dictionary encoded. The dictionary encoding allows adding resources (subjects or objects) as integers to the filters.     ```bash  java -cp target/uber-knowledgecubes-0.1.0.jar edu.purdue.knowledgecubes.DictionaryEncoderCLI -i [NT File] -o [Output File] -l [Local Path for the new store] -s space  ```    The command generates a dictionary encoded version of the dataset. This encoded NT file is used for creating the store. KC automatically encodes and decodes SPARQL queries and the corresponding results.     #### Store Creation    KC provide the Store class for creation of an RDF store. The input to the store is a spark session, database path where the RDF dataset will be stored, and a local configuration path.    ```scala  import org.apache.spark.sql.SparkSession    import edu.purdue.knowledgecubes.GEFI.GEFIType  import edu.purdue.knowledgecubes.GEFI.join.GEFIJoinCreator  import edu.purdue.knowledgecubes.storage.persistent.Store    val localPath = ""/path/to/local/path""  val dbPath = ""/path/to/db/path""  val ntPath = ""/path/to/rdf/file""    val spark = SparkSession.builder              .appName(s""KnowledgeCubes Store Creator"")              .getOrCreate()    val store = Store(spark, dbPath, localPath)  store.create(ntPath)  ```    #### SPARQL Querying    KC provides a SPARQL query processor that takes as input the spark session, database path of where the RDF dataset was created, local configuration file path, a filter type, and a false postivie rate (if any).    ```scala  import org.apache.spark.sql.SparkSession    import edu.purdue.knowledgecubes.queryprocessor.QueryProcessor  import edu.purdue.knowledgecubes.GEFI.GEFIType    val spark = SparkSession.builder              .appName(s""Knowledge Cubes Query"")              .getOrCreate()    val localPath = ""/path/to/local/path""  val dbPath = ""/path/to/db/path""  val filterType = GEFIType.ROARING // Roaring bitmap  val falsePositiveRate = 0    val queryProcessor = QueryProcessor(spark, dbPath, localPath, filterType, falsePositiveRate)    val query =    """"""      SELECT ?GivenName ?FamilyName WHERE{          ?p <http://yago-knowledge.org/resource/hasGivenName> ?GivenName .           ?p <http://yago-knowledge.org/resource/hasFamilyName> ?FamilyName .           ?p <http://yago-knowledge.org/resource/wasBornIn> ?city .           ?p <http://yago-knowledge.org/resource/hasAcademicAdvisor> ?a .          ?a <http://yago-knowledge.org/resource/wasBornIn> ?city .      }    """""".stripMargin    // Returns a Spark DataFrame containing the results  val r = queryProcessor.sparql(query)    ```    #### Constructing Filters    Additionaly, KC provides an API for creating additional filters. KC provides exact and approximate structures for filtering data. Currently KC supports ```GEFIType.BLOOM```, ```GEFIType.ROARING```, and ```GEFIType.BITSET```.    ```scala  import org.apache.spark.sql.SparkSession    import edu.purdue.knowledgecubes.GEFI.GEFIType  import edu.purdue.knowledgecubes.GEFI.join.GEFIJoinCreator  import edu.purdue.knowledgecubes.utils.Timer    val spark = SparkSession.builder              .appName(s""KnowledgeCubes Filter Creator"")              .getOrCreate()                var localPath = ""/path/to/db/path""  var dbPath = ""/path/to/local/path""  var filterType = GEFIType.ROARING  var fp = 0    val filter = new GEFIJoinCreator(spark, dbPath, localPath)  filter.create(filterType, fp)  ```    #### Query Execution Benchmarking    KC provides a set of benchmarking classes    * **BenchmarkFilteringCLI:** For benchmarking the query execution when using filters  * **BenchamrkReductionsCLI:** For benchmarking the query execution when using reductions only          ## Publications    * Amgad Madkour, Ahmed M. Ali, Walid G. Aref, ""WORQ: Workload-driven RDF Query Processing"", ISWC 2018 [[Paper](https://amgadmadkour.github.io/files/papers/worq.pdf)][[Slides](https://amgadmadkour.github.io/files/presentations/WORQ-ISWC2018.pdf)]    * Amgad Madkour, Walid G. Aref, Ahmed M. Aly, ""SPARTI: Scalable RDF Data Management Using Query-Centric Semantic Partitioning"", Semantic Big Data (SBD18) [[Paper](https://amgadmadkour.github.io/files/papers/sparti.pdf)][[Slides](https://amgadmadkour.github.io/files/presentations/SPARTI-SBD2018.pdf)]    * Amgad Madkour, Walid G. Aref, Sunil Prabhakar, Mohamed Ali, Siarhei Bykau, ""TrueWeb: A Proposal for Scalable Semantically-Guided Data Management and Truth Finding in Heterogeneous Web Sources"", Semantic Big Data (SBD18) [[Paper](https://amgadmadkour.github.io/files/papers/trueweb.pdf)][[Slides](https://amgadmadkour.github.io/files/presentations/TrueWeb-SBD2018.pdf)]    * Amgad Madkour, Walid G. Aref, Saleh Basalamah, “Knowledge Cubes - A Proposal for Scalable and Semantically-Guided Management of Big Data”, IEEE BigData 2013 [[Paper](https://amgadmadkour.github.io/files/papers/bigdata2013.pdf)][[Slides](https://amgadmadkour.github.io/files/presentations/KnowledgeCubes.pdf)]    ## Contact    If you have any problems running KC please feel free to send an email.     * Amgad Madkour <amgad@alumni.purdue.edu> """
Semantic web;https://github.com/zazuko/trifid;"""# Zazuko Trifid - Lightweight Linked Data Server and Proxy  <img src=""https://cdn.rawgit.com/zazuko/trifid/master/logo.svg"" width=""140px"" height=""140px"" align=""right"" alt=""Trifid-ld Logo""/>    [![Join the chat at https://gitter.im/zazuko/trifid](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/zazuko/trifid?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    Trifid provides a lightweight and easy way to access Linked Data URIs via HTTP.  In the Linked Data world this is often called [dereferencing](http://en.wikipedia.org/wiki/Dereferenceable_Uniform_Resource_Identifier).  Trifid is inspired by [Pubby](https://www.w3.org/2001/sw/wiki/Pubby) and written in (server side) JavaScript.    ### Features    * Provides a Linked Data interface to SPARQL protocol servers  * Provides a file based interface for testing  * Provides a customizable HTML renderer with embedded RDF  * Takes care of content-negotiation  * Provides a SPARQL proxy and [YASGUI](http://about.yasgui.org/) as web frontend    ### Requirements    * A SPARQL endpoint  * Or for development some triples in a local file.    Trifid supports all content-types provided by the SPARQL endpoint and does not do additional format conversion.    ### Trifid in the wild    Trifid can be completely themed according to your needs. Example resources using Trifid:    * Default view: http://lod.opentransportdata.swiss/didok/8500011  * Customized for one gov entity in Switzerland: https://ld.geo.admin.ch/boundaries/municipality/296    ## Installation    Trifid is a [Node.js](http://nodejs.org/) based application.  To install and run it you will need to install [Node.js](http://nodejs.org/) on your system.    Clone the Github repository and run        npm install    to install all module dependencies.    ## Usage    To start the server execute the following command:        npm start    The server script is also a command line program which can be called like this:        trifid --config=my-trifid-config.json    If you want to run Trifid using a SPARQL endpoint and default settings, you can run it even without a config file:        trifid --sparql-endpoint-url=http://localhost:3030/sparql         ### Parameters    The following parameters are available:    - `-v` or `--verbose`: Verbose output, will show the actual config after expanding  - `-c` or `--config`: Expects a path to a config as value, which will be used by Trifid  - `-p` or `--port`: Expects a port number as value, which will be used by the HTTP listener of Trifid  - `--sparql-endpoint-url`: Expects a SPARQL HTTP query interface URL value, which will be used by the Trifid SPARQL handler  - `--dataset-base-url`: Expects a Base URL value, which will be used to translate the request URLs    ## Configuration    Trifid uses JSON configuration files and supports comments in JavaScript style.  One configuration file can use another file as base.  The `baseConfig` property must point to the other file.  Values of the base file will be overwritten.    ### Examples    #### Default configuration    The default configuration `config.json` uses the file system handler and a [sample dataset](https://github.com/zazukoians/tbbt-ld) with characters from _The Big Bang Theory_.  The following command will run it:        npm start    You will then be able to access its content, e.g. <http://localhost:8080/data/person/amy-farrah-fowler>.    In a production environment the SPARQL handler may be the better choice.    #### SPARQL configuration    For production systems we recommend data access via the [SPARQL 1.1 Protocol](http://www.w3.org/TR/sparql11-protocol/) interface.  `config-sparql.json` can be used as base configuration.  The following lines defines a configuration using a Fuseki SPARQL endpoint:    ```JSON  {    ""baseConfig"": ""trifid:config-sparql.json"",    ""sparqlEndpointUrl"": ""http://localhost:3030/dataset/sparql""  }  ```    The `baseConfig` property defines which file should be used as base configuration.  The `trifid:` prefix prepends the Trifid module path.  The value of the `sparqlEndpointUrl` property is used in the handler and also the SPARQL proxy.    Sometimes SPARQL endpoints are running on TLS/SSL but provide an incomplete configuration or a self-signed certificate. In that case one can disable strict certificate checking by setting the environment variable `NODE_TLS_REJECT_UNAUTHORIZED`. For example:        $ export NODE_TLS_REJECT_UNAUTHORIZED=0    ### Properties    Usually only the following properties must be configured:    - `baseConfig`: Base configuration file for the current configuration file.  - `sparqlEndpointUrl`: URL of the SPARQL HTTP query interface.  - `datasetBaseUrl`: If the dataset is stored with a different base URL this property is used to translate the request URL.    The following properties are already defined in the default configurations:    - `listener`: `port` and `host` of the listener.  - `express`: Express settings as key vale pairs.  - `patchHeaders`: Settings for the `patch-headers` middleware.  - `mediaTypeUrl`: Settings for the `format-to-accept` middleware.  - `rewrite`: Settings for the camouflage-rewrite middleware.  - `handler`: Settings for the graph handler.    ### Prefixes    It is possible to use prefixes in the property values of the configuration.  These prefixes will be translated to specific paths or environment variable values.    - `cwd`: Prepends the current working directory to the value.  - `env`: Uses the value of the environment variable with the name matching the value after the prefix.    (e.g. `""env:SPARQL_ENDPOINT_URL""` will be replaced with the environment variable value of `$SPARQL_ENDPOINT_URL`)  - `trifid`: Prepends the Trifid module path to the value.    ### Multiple Configurations    Most plugins support multiple configurations to support path or hostname specific configurations.  These plugins have an additional level in the config with the config name as key and the actual configuration as value.  Each config can have a `path` property.  If it's not defined, `/` will be used.  Also a `hostname` can be specified to use the config only for matching host names.  The `priority` may be required if multiple configs could match to an URL.    Example:  ```JSON  ""pluginName"": {    ""root"": {      // ""path"": ""/"" will be automatically added if path is not given      ""priority"": 200      ...    },    ""otherPath"": {      ""path"": ""/other/"",      ""priority"": 100      ...    },    ""otherHostname"": {      ""hostname"": ""example.org""      ""priority"": 150    }  }  ```    ### Static Files    With the `staticFiles` property, folders can be mapped into URL paths for static file hosting.  This plugin supports multiple configurations.  The key for a static file hosting can be used to replace values defined in a configuration, which is used as `baseConfig`.  If the first folder does not contain the requested file, the next folder will be used and so on.  The `folder` property points to the folder in the file system.  It's possible to use prefixes in the folder value.    Example:    ```JSON  ""staticFiles"": {    ""rendererFiles"": {      ""hostname"": ""example.org"",      ""path"": ""/"",      ""folder"": ""renderer:public""    }  }  ```    ### Handler    The handler plugin supports multiple configurations.  Properties for the handler configuration:    - `module`: The handler JS file or module.  - `options`: Handler specific options.    More details about the handler specific options can be found in the documentation of the handlers:    - [Fetch files](https://github.com/zazukoians/trifid-handler-fetch)  - [SPARQL](https://github.com/zazukoians/trifid-handler-sparql)    ### SPARQL Proxy    The SPARQL proxy plugin supports multiple configurations.  Properties:    - `options`: Options for the SPARQL proxy.    Options:    - `endpointUrl`: URL to the SPARQL HTTP query interface. (default: sparqlEndpointUrl)  - `authentication`: `user` and `password` for basic authentication.    See `config-virtuoso.json` and `config-stardog.json` for default configuration in case you use either of these stores.    Note that SPARQL is currently not supported by the in-memory store.    ### Patch Headers    The patch headers plugin supports multiple configurations.  See the [patch-headers](https://www.npmjs.com/package/patch-headers) module documentation for more details.    ### Rewrite    The rewrite plugin supports multiple configurations.  See the [camouflage-rewrite](https://www.npmjs.com/package/camouflage-rewrite) module documentation for more details.    Note that this module does _not_ work for most content-types, see the documentation for details. By default it should work for HTML and Turtle. It is merely for testing purposes and should not be active on production.    ## Production Best Practices    Note that it is not recommended to run Node applications on [well-known ports](http://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers#Well-known_ports) (< 1024). You should use a reverse proxy instead.    ### Installing/Using with Docker    Trifid can be installed using Docker. With this method you only need to have Docker installed, see https://docs.docker.com/installation/ for installation instructions for your platform.    Once Docker is installed clone the Github repository and run        docker build -t trifid .    This creates an image named `trifid` that you can execute with        docker run -ti -p 8080:8080 trifid    Once it is started you can access for example http://localhost:8080/data/person/sheldon-cooper . An example on using Docker can be found at [lod.opentransportdata.swiss](https://github.com/zazuko/lod.opentransportdata.swiss).    #### Trifid environment variables    You can change its behavior by changing the following environment variable:        TRIFID_CONFIG config-sparql.json    This overrides the default configuration `config.json`.    #### Use the pre built image    If you do not want to build your own Docker image, you can pull the official image from [Docker Hub](https://hub.docker.com/r/zazuko/trifid/):        docker pull zazuko/trifid      ### Reverse Proxy    If you run Trifid behind a reverse proxy, the proxy must set the `X-Forwarded-Host` header field.    ## Debugging    This package uses [`debug`](https://www.npmjs.com/package/debug), you can get debug logging via: `DEBUG=trifid:`.  Trifid plugins should also implement `debug` under the `trifid:` prefix, enabling logging from all packages  implementing it can be done this way: `DEBUG=trifid:*`.    ## Support    Issues & feature requests should be reported on Github.    Pull requests are very welcome.    ## License    Copyright 2015-2019 Zazuko GmbH    Trifid is licensed under the Apache License, Version 2.0. Please see LICENSE and NOTICE for details. """
Semantic web;https://github.com/njh/redstore;"""RedStore  ========  Nicholas J. Humfrey <njh@aelius.com>    For the latest version of RedStore, please see:  <http://www.aelius.com/njh/redstore/>      What is RedStore ?  ------------------  RedStore is a lightweight RDF triplestore written in C using the [Redland] library.    It has a HTTP interface and supports the following W3C standards:    * [SPARQL 1.0 Query]  * [SPARQL 1.1 Protocol for RDF]  * [SPARQL 1.1 Graph Store HTTP Protocol]  * [SPARQL 1.1 Service Description]    Features  --------    * Built-in HTTP server  * Mac OS X app available  * Supports a wide range of RDF formats  * Only runtime dependancy is [Redland].  * Compatible with rdfproc command line tool for offline operations  * Unit and integration test suite.    Usage  -----      redstore [options] [<name>]         -p <port>       Port number to run HTTP server on (default 8080)         -b <address>    Bind to specific address (default all)         -s <type>       Set the graph storage type (default hashes)         -t <options>    Storage options         -n              Create a new store / replace old (default no)         -f <filename>   Input file to load at startup         -F <format>     Format of the input file (default guess)         -v              Enable verbose mode         -q              Enable quiet mode      Start RedStore on port 8080, bound to localhost, using a new sqlite store:        redstore -p 8080 -b localhost -n -s sqlite    Load a URI into the triplestore:        curl --data uri=http://example.com/file.rdf http://localhost:8080/load    Add a file to the triplestore:        curl -T foaf.rdf 'http://localhost:8080/data/foaf.rdf'    Add a file to the triplestore with full URI specified:        curl -T foaf.rdf 'http://localhost:8080/data/?graph=http://example.com/foaf.rdf'    Add a file to the triplestore with type specified:        curl -T foaf.ttl -H 'Content-Type: application/x-turtle' 'http://localhost:8080/data/foaf.rdf'     You can delete graphs with in the same manner, using the DELETE HTTP verb:        curl -X DELETE 'http://localhost:8080/data/foaf.rdf'    Query using the [SPARQL Query Tool]:        sparql-query http://localhost:8080/sparql 'SELECT * WHERE { ?s ?p ?o } LIMIT 10'      Requirements  ------------    The minimum required versions of the [Redland] RDF Libraries are:    - [raptor2-2.0.4]  - [rasqal-0.9.27]  - [redland-1.0.14]      Installation  ------------  RedStore uses a standard automake build process:        ./configure      make      make install      Supported Storage Modules  -------------------------    You can use any of the [Redland Storage Modules] that supports contexts:    - [hashes] (Default)  - [mysql]  - [memory]  - [postgresql]  - [sqlite]  - [virtuoso]      License  -------    This program is free software: you can redistribute it and/or modify  it under the terms of the [GNU General Public License] as published by  the Free Software Foundation, either version 3 of the License, or  (at your option) any later version.    This program is distributed in the hope that it will be useful,  but WITHOUT ANY WARRANTY; without even the implied warranty of  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the  GNU General Public License for more details.        [Redland]:                     http://librdf.org/  [Redland Storage Modules]:     http://librdf.org/docs/api/redland-storage-modules.html  [SPARQL Query Tool]:           http://github.com/tialaramex/sparql-query  [GNU General Public License]:  http://www.gnu.org/licenses/gpl.html    [SPARQL 1.0 Query]:                     http://www.w3.org/TR/rdf-sparql-query/  [SPARQL 1.1 Protocol for RDF]:          http://www.w3.org/TR/sparql11-protocol/  [SPARQL 1.1 Graph Store HTTP Protocol]: http://www.w3.org/TR/sparql11-http-rdf-update/  [SPARQL 1.1 Service Description]:       http://www.w3.org/TR/sparql11-service-description/    [raptor2-2.0.4]:               http://download.librdf.org/source/raptor2-2.0.4.tar.gz  [rasqal-0.9.27]:               http://download.librdf.org/source/rasqal-0.9.27.tar.gz  [redland-1.0.14]:              http://download.librdf.org/source/redland-1.0.14.tar.gz    [hashes]:                      http://librdf.org/docs/api/redland-storage-module-hashes.html  [mysql]:                       http://librdf.org/docs/api/redland-storage-module-mysql.html  [memory]:                      http://librdf.org/docs/api/redland-storage-module-memory.html  [postgresql]:                  http://librdf.org/docs/api/redland-storage-module-postgresql.html  [sqlite]:                      http://librdf.org/docs/api/redland-storage-module-sqlite.html  [virtuoso]:                    http://librdf.org/docs/api/redland-storage-module-virtuoso.html """
Semantic web;https://github.com/cmungall/sparqlprog;"""# sparqlprog - programming with SPARQL    [![Build Status](https://travis-ci.org/cmungall/sparqlprog.svg?branch=master)](https://travis-ci.org/cmungall/sparqlprog)  [![Join the chat at https://gitter.im/sparqlprog/Lobby](https://badges.gitter.im/sparqlprog/Lobby.svg)](https://gitter.im/sparqlprog/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  [**pack**](http://www.swi-prolog.org/pack/list?p=sparqlprog)    sparqlprog is a programming language and environment that can be used  to write composable modular building blocks that can be executed as  federated SPARQL queries.    Example of use (command line):    ```  pl2sparql  -u sparqlprog/ontologies/ebi -u sparqlprog/ontologies/faldo  -s ebi ""\    protein_coding_gene(G), \    location(G,L,B,E,grcm38:'11'), \    B >= 101100523,E =< 101190725, \    orthologous_to(G,H),in_taxon(H,taxon:'9606')"" \    ""h(G,H)""  ```    The command passes a *logic program query* to sparqlprog. In this  case, the query is a conjunction of conditions involving different  variables (each indicated with a leading upper-case letter):     1. `G` is a *protein coding gene*   2. `G` is located on mouse chromosome 11, with an interval bounded by `B` (begin) and `E` (end)   3. The interval is within a certain range   4. `G` is *homologus to* `H`   5. `H` is a human gene (indicated by taxon ID 9606)   6. The results are bound to a tuples `h(G,H)` (i.e. two column table)    This logic query compiles down to a SPARQL query for fetching G and  H. The query is then executed on the [EBI RDF  Platform](https://www.ebi.ac.uk/rdf/services/sparql), giving:    |Mouse Gene|Human Gene|  |---|---|  |ensembl:ENSMUSG00000035198|ensembl:ENSG00000131462|  |ensembl:ENSMUSG00000017167|ensembl:ENSG00000108797|  |ensembl:ENSMUSG00000044052|ensembl:ENSG00000184451|  |ensembl:ENSMUSG00000017802|ensembl:ENSG00000141699|  |ensembl:ENSMUSG00000045007|ensembl:ENSG00000037042|  |ensembl:ENSMUSG00000035172|ensembl:ENSG00000068137|    How does this work? The query compilation makes use of pre-defined  n-ary predicates, such as this one defined in the [faldo  module](https://www.swi-prolog.org/pack/file_details/sparqlprog/prolog/sparqlprog/ontologies/faldo.pl):    ```  location(F,L,B,E,R) :-    rdf(F,faldo:location,L),    begin(L,PB),position(PB,B),reference(PB,R),    end(L,PE),position(PE,E),reference(PE,R).  ```    The `:-` connects a rule head to a rule body. In this case the body is  a conjuncation of goals. Each of these may be defined in their own  rules. Typically everything bottoms out at a call over a 3-ary  predicate `rdf(S,P,O)` which maps to a single triple. In this case the vocabulary used for genomic locations is [faldo](https://github.com/OBF/FALDO).    This approach allows for *composability* of queries. Rather that  repeating the same verbose SPARQL each time in different queries,  reusable modules can be defined.    In addition to providing a composable language that compiles to  SPARQL, this package provides a complete turing-complete environment  for mixing code and queries in a relational/logic programming  paradigm. See below for examples.    ## Quick Start (for prolog hackers)    See the [sparqlprog module docs](https://www.swi-prolog.org/pack/file_details/sparqlprog/prolog/sparqlprog.pl)    See also the [specification](SPECIFICATION.md)    ## Quick Start (for Python hackers)    See the [sparqlprog-python](https://github.com/cmungall/sparqlprog-python) package    This provides a Python interface to a sparqlprog service    You can also see demonstration notebooks:     * [Basic SPARQLProg](https://nbviewer.jupyter.org/github/cmungall/sparqlprog-python/blob/master/Notebook_01_Basics.ipynb)   * [sending programs over the wire](https://nbviewer.jupyter.org/github/cmungall/sparqlprog-python/blob/master/Notebook_02_Programs.ipynb)    ## Quick Start (for everyone else)    There are a variety of ways to use this framework:     * Executing queries on remote services via command line   * Compiling logic queries to SPARQL queries, for use in another framework   * Programmatically within a logic program (interleaving remote and local operations)   * Programmatically from a language like python/javascript, using a __sparqlprog service__    Consult the appropriate section below:    ### Running queries from the command line    See the [examples](./examples/) directory for all command line examples    First [install](INSTALL.md), making sure the [bin](bin) directory is  in your path. This will give you access to the the pl2sparql script.    For full options, run:    ```  pl2sparql --help  ```    Note you should also have a number of convenience scripts in your  path. For example the `pq-wd` script is simply a shortcut for    ```  pl2sparql -s wikidata -u sparqlprog/ontologies/wikidata  ARGS  ```    This will give you access to a number of convenience predicates such  as positive_therapeutic_predictor/2 (for drug queries). The `-u`  option uses the wikidata module, and the `-s` option sets the service  to the one with handle `dbpedia` (the mapping from a handle to the  full service URL is defined in the wikidata module).    The best way to learn is to look at the [examples/](examples),  together with the corresponding set of rules in  [prolog/sparqlprog/ontologies](prolog/sparqlprog/ontologies).    For example [examples/monarch-examples.sh](examples/monarch-examples.sh) has:    ```  pq-mi  'label(D,DN),literal_exact_match(DN,""peroxisome biogenesis disorder""),\     rdfs_subclass_of(D,C),owl_equivalent_class(C,E),has_phenotype(E,Z)'\     'x(C,CN,E,Z)'  ```    This finds a disease with a given name, finds equivalent classes of  transitive reflexive subclasses, and then finds phenotypes for each    ### Compiling logic programs to SPARQL    You can use pl2sparql (see above for installation) to compile a  program with bindings to a SPARQL query by using the `-C` option. The  SPARQL query can then be used without any dependence on  sparqlprog. E.g.    ```  pq-ebi -C ""\    protein_coding_gene(G), \    location(G,L,B,E,grcm38:'11'), \    B >= 101100523,E =< 101190725, \    homologous_to(G,H),in_taxon(H,taxon:'9606')"" \    ""h(G,H)""  ```    will generate the following SPARQL:    ```  SELECT ?g ?h WHERE {?g <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.obolibrary.org/obo/SO_0001217> . ?g <http://biohackathon.org/resource/faldo#location> ?l . ?l <http://biohackathon.org/resource/faldo#begin> ?v0 . ?v0 <http://biohackathon.org/resource/faldo#position> ?b . ?v0 <http://biohackathon.org/resource/faldo#reference> <http://rdf.ebi.ac.uk/resource/ensembl/90/mus_musculus/GRCm38/11> . ?l <http://biohackathon.org/resource/faldo#end> ?v1 . ?v1 <http://biohackathon.org/resource/faldo#position> ?e . ?v1 <http://biohackathon.org/resource/faldo#reference> <http://rdf.ebi.ac.uk/resource/ensembl/90/mus_musculus/GRCm38/11> . FILTER (?b >= 101100523) . FILTER (?e <= 101190725) . ?g <http://semanticscience.org/resource/SIO_000558> ?h . ?h <http://purl.obolibrary.org/obo/RO_0002162> <http://identifiers.org/taxonomy/9606>}  ```    withOUT executing it remotely    note: indentation and URI shortening are on the cards for future releases.    ### Using a public sparqlprog service    Public pengines service: https://evening-falls-87315.herokuapp.com/pengine    [Pengines](http://pengines.swi-prolog.org/) is a framework for running logic program environments as a  web service. They can be used by clients in any language (client  libraries in python, javascript seem to be mature; as well as separate  prolog clients as well).    See the docs on the [pengines framework](http://pengines.swi-prolog.org/).    There is an example of how to contact this service in javascript in  [bin/sprog-client.js](bin/sprog-client.js). You will need to do a `npm  install pengines`, and change the server URL.    Pengines allows the client to send logic programs to the server, and  then to invoke them. For example:    ```  pengines = require('pengines');    peng = pengines({      server: ""https://evening-falls-87315.herokuapp.com/pengine"",      ask: ""q(X)"",      chunk: 100,      sourceText: ""q(X):- (wd ?? continent(X)).\n""  }  ).on('success', handleSuccess).on('error', handleError);  function handleSuccess(result) {      console.log('# Results: '+ result.data.length);      for (var i = 0; i < result.data.length; i++) {          console.log(result.data[i])      }      if (result.data.length == 0) {          console.log(""No results!"")      }  }  function handleError(result) {      console.error(result)  }  ```    Note that *any* safe subset of prolog can be passed as a program. In  this case we are passing a small program:    `q(X):- (wd ?? continent(X))`    This trivially defines a unary predicate `q/1`. The argument is bound  to any continent. The `??` is a special infix binary predicate, the  left side is the service name and the right side is the query to be  compiled.    The `ask` portion of the javascript will simply pass the query to the  server.    ### Using a local sparqlprog service    You can start a sparqlprog service running locally:        docker run -p 9083:9083 cmungall/sparqlprog    (requires docker)    This creates a pengines service at http://localhost:9083/pengine    There is an example of how to contact this service in javascript in  [sprog-client.js](bin/sprog-client.js). You will need to do:        npm install pengines    ### SWISH    TODO    ### Use within logic programs    For this example, consider writing a music band recommender, based on  similarity of genres. dbpedia has triples linking bands to genres, so  we will use that.    We will write a program  [dbpedia_rules.pl](examples/dbpedia/dbpedia_rules.pl) that contains  definitions of predicates we will use.    First we define a binary predicate that counts the number of bands per genre:    ```  genre_num_bands(G,Count) :-          aggregate_group(count(distinct(B)),[G],(rdf(B,dbont:genre,G),band(B)),Count).  ```    you can try this with:    `pq-dbpedia -c examples/dbpedia/dbpedia_rules.pl ""genre_num_bands(G,Count)""`    this will give results like:    ```  http://dbpedia.org/resource/Independent_music,184  http://dbpedia.org/resource/Funky_Club_Music,1  http://dbpedia.org/resource/Ghettotech,2  http://dbpedia.org/resource/Indian_folk_music,1  http://dbpedia.org/resource/Bakersfield_Sound,1  http://dbpedia.org/resource/Punk_Rawk,1  http://dbpedia.org/resource/Go-go,6  http://dbpedia.org/resource/Jazz_pop,3  http://dbpedia.org/resource/Dubstep,74  http://dbpedia.org/resource/Alt.folk,1  http://dbpedia.org/resource/AfroHouse,1  http://dbpedia.org/resource/Electro-disco,1  http://dbpedia.org/resource/Math_Rock,15  ```    we are doing this because we want to weight band similarity according  to how rare a genre is. If two bands share the genre of 'independent  music' it is not remarkable, but if two bands share a rarer genre like  'Ghettotech' then we will weight that higher.    we can explicitly bind this to dbpedia using `??/2`:    ```  get_genre_num_bands(G,Count) :-          ??(dbpedia,genre_num_bands(G,Count)).  ```    we can define the Information Content (IC) of a genre `G` as `-log2(Pr(G))`:    ```  genre_ic(G,IC) :-          get_genre_num_bands(G,Count),          get_num_bands(Total),          seval(-log(Count/Total)/log(2), IC).  ```    This makes use of:    ```  :- table get_num_bands/1.  get_num_bands(Count) :-          ??(dbpedia,num_bands(Count)).  num_bands(Count) :-          aggregate(count(distinct(B)),band(B),Count).  ```    Note we are tabling (memoizing) the call to fetch the total number of  bands. This means it will only be called once per sparqlprog session.    Finally we can define a 3-ary predicate that compares any two bands  and bindings the 3rd arg to a similarity score that is the sum of the  ICs of all genres held in common. (for simplicity, we do not penalize  unmatched genres, or try to use sub/super genre categories yet):    ```  pair_genre_ic(A,B,SumIC) :-          get_all_genres(A,SA),          get_all_genres(B,SB),          ord_intersection(SA,SB,I),          aggregate(sum(IC),G^(member(G,I),genre_ic(G,IC)),SumIC).  ```    This is a normal prolog goal and can be executed in a normal prolog context, or from the command line:    `pq-dbpedia -c examples/dbpedia/dbpedia_rules.pl -e  ""pair_genre_ic(dbr:'Metallica',dbr:'Megadeth',IC)""`    The `-e` option tells the script to execute the query directly rather  than try and compile everything to a single SPARQL query (this may be  possible, but could be highly inefficient). It is only when the prolog  engine executes the `??` goals that a remote SPARQL will be executed.    If we want to adapt this program to search rather than compare two  given bands, we can modify it slightly so that it does not waste  cycles querying on bands that have no genres in common:    ```  pair_genre_ic(A,B,SumIC) :-          get_all_genres(A,SA),          ??(dbpedia,has_shared_genre(A,B,_)),          get_all_genres(B,SB),          ord_intersection(SA,SB,I),          aggregate(sum(IC),G^(member(G,I),genre_ic(G,IC)),SumIC).  ```    Example of running this:    `pq-dbpedia -c examples/dbpedia/dbpedia_rules.pl -e  ""pair_genre_ic(dbr:'Voivod_(band)',B,IC),IC>=10""`    Note this is slow, as it will iterate across each band performing  queries to gather stats. There are various approaches to optimizing  this, but the core idea here is that the logic can be shuffled back  and forth between the portion that is compiled to SPARQL and executed  remotely, and the portion that is executed locally by a logic engine.    ### Using a local triplestore    You can use sparqlprog with any local or remote triplestore that  supports the SPARQL protocol. If you have RDF files and want to get  started, here is one quick route (assuming you have docker):     1. Place your files in [data](examples/data)   2. Run `make bg-run`    This will run blazegraph within a docker container    ## Discussion      SPARQL provides a declarative way of querying a triplestore. One of  its limitations is the lack of ability to *compose* queries and reuse  repeated patterns across multiple queries. Sparqlprog is an extension  of SPARQL and a subset of Prolog for relational rule-oriented  programming using SPARQL endpoints.    ## Prolog programmers guide    This package provides a more natural (from a Prolog point of view) interface  to SPARQL endpoints. There are two layers. The first, lower layer, defines a  DCG for generating SPARQL queries from a structured term. The second provides  a translation from representation that looks more or less like a Prolog goal  built from rdf/3 goals (with conjunction, disjunction etc) to a term in the  term language understood by the SPARQL DCG.    In addition, the library provides a mechanism to register known SPARQL endpoints  so that they can be referred to by a short name, or to enable a query to be  run against all registered endpoints.    The library is based on the idea implemented in Yves Raimond's swic package,  but the code has been completely re-implemented.    You just need SWI Prolog with its Semantic Web libraries.    ## Simple usage    The `(??)/2`  and `(??)/1` operators have a high precedence so that conjuction and disjunctive  queries can be written to the right of it without parentheses:    ```  ?- rdf_register_prefix(foaf,'http://xmlns.com/foaf/0.1/')  ?- rdf_register_prefix(dbont,'http://dbpedia.org/ontology/')  ?- sparql_endpoint( dbp, 'http://dbpedia.org/sparql/').  ?- debug(sparkle).  % to show queries    ?-	dbp ?? rdf(Class,rdf:type,owl:'Class'), rdf(Instance,rdf:type,Class).  ?- dbp ?? rdf(Person,rdf:type,foaf:'Person'),             rdf(Person,foaf:Name,Name),            filter(regex('Colt.*',Name)).  ?- dbp ?? rdf(A,rdf:type,dbont:'Photographer'); rdf(A, rdf:type, dbont:'MusicalArtist').  ```      ## Clause expansion    If the following clause is defined:    ```  cls(Class) :-          rdf(Class,rdf:type,owl:'Class').  ```    Then cls/1 can be used in queries, e.g.    ```  ?-  dbp ?? cls(X).  ```    The cls/1 goal will be expanded.    More complex goals can be defined; for example, this queries for existential restrictions:    ```  subclass_of(C,D) :- rdf(C,rdfs:subClassOf,Restr).  svf_edge(C,P,D) :-          subclass_of(C,Restr),          rdf(Restr,owl:onProperty,P),          rdf(Restr,owl:someValuesFrom,D).  ```    Only a subset of prolog can be expanded in this way. Conjunction,  disjunction (or multiple clauses), negation are supported. Terminals  rdf/3, rdf/4, and some predicates from the rdfs library are  supported. In future a wider set of constructs may be supported,  e.g. setof/3.    It is also possible to use create_sparql_construct/3 and  create_sparl_construct/4 to generate SPARQL queries for a  limited subset of pure prolog that can be executed outside  the prolog environment - effectively a limited prolog to SPARQL  compiler.    ## Comparison with SPIN    TODO https://spinrdf.org/    ## Credits    The majority of code in this repo was developed by Samer Abdallah, as  part of the [sparkle  package](http://www.swi-prolog.org/pack/list?p=sparkle). Some of this  code came from Yves Raimond's swic package.    Extensions were implemented by Chris Mungall. In particular     - goal rewriting   - DCG extensions: aggregates, filter operators   - predicate definitions for vocabularies used by various triplestores (faldo, ebi, wikidata, dbpedia, go, monarch) """
Semantic web;https://github.com/deiu/rdf2go;"""# rdf2go    [![Build Status](https://api.travis-ci.org/deiu/rdf2go.svg?branch=master)](https://travis-ci.org/deiu/rdf2go)  [![Coverage Status](https://coveralls.io/repos/github/deiu/rdf2go/badge.svg?branch=master)](https://coveralls.io/github/deiu/rdf2go?branch=master)    Native golang parser/serializer from/to Turtle and JSON-LD.    # Installation    Just go get it!    `go get -u github.com/deiu/rdf2go`    # Example usage    ## Working with graphs    ```golang  // Set a base URI  baseUri := ""https://example.org/foo""    // Create a new graph  g, err := NewGraph(baseUri)  if err != nil {  	// deal with err  }    // Add a few triples to the graph  triple1 := NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""c""))  g.Add(triple1)  triple2 := NewTriple(NewResource(""a""), NewResource(""d""), NewResource(""e""))  g.Add(triple2)    // Get length of Graph (nr of triples)  g.Len() // -> 2    // Dump graph contents to NTriples  out := g.String()  // <a> <b> <c> .  // <a> <d> <e> .    // Delete a triple  g.Remove(triple2)  ```    ## Looking up triples from the graph    ### Returning a single match    The `g.One()` method returns the first triple that matches against any (or all) of Subject, Predicate, Object patterns.    ```golang  // Create a new graph  g, _ := NewGraph(""https://example.org"")    // Add a few triples  g.Add(NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""c"")))    // Look up one triple matching the given subject  triple := g.One(NewResource(""a""), nil, nil) // -> <a> <b> <c> .    // Look up one triple matching the given predicate  triple = g.One(nil, NewResource(""b""), nil) // -> <a> <b> <c> .    // Look up one triple matching the given object  triple = g.One(nil, nil, NewResource(""c"")) // -> <a> <b> <c> .    // Look up one triple matching the given subject and predicate  triple = g.One(NewResource(""a""), NewResource(""b""), nil) // -> <a> <b> <c> .    // Look up one triple matching the a bad predicate  triple = g.One(nil, NewResource(""z""), nil) // -> nil  ```    ### Returning a list of matches    Similar to `g.One()`, `g.All()` returns all triples that match the given pattern.    ```golang  // Create a new graph  g, _ := NewGraph(""https://example.org"")    // Add a few triples  g.Add(NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""c"")))  g.Add(NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""d"")))    // Look up one triple matching the given subject  triples := g.All(nil, nil, NewResource(""c"")) //  for triple := range triples {  	triple.String()  }  // Returns a single triple that matches object <c>:  // <a> <b> <c> .    triples = g.All(nil, NewResource(""b""), nil)  for triple := range triples {  	triple.String()  }  // Returns all triples that match subject <b>:   // <a> <b> <c> .  // <a> <b> <d> .  ```    ## Different types of terms (resources)    ### IRIs    ```golang  // Create a new IRI  iri := NewResource(""https://example.org"")  iri.String() // -> <https://example.org>  ```    ### Literals    ```golang  // Create a new simple Literal  lit := NewLiteral(""hello world"")  lit.String() // -> ""hello word""    // Create a new Literal with language tag  lit := NewLiteralWithLanguage(""hello world"", ""en"")  lit.String() // -> ""hello word""@en    // Create a new Literal with a data type  lit := NewLiteralWithDatatype(""newTypeVal"", NewResource(""https://datatype.com""))  lit.String() // -> ""newTypeVal""^^<https://datatype.com>  ```    ### Blank Nodes    ```golang  // Create a new Blank Node with a given ID  bn := NewBlankNode(9)  bn.String() // -> ""_:n9""    // Create an anonymous Blank Node with a random ID  abn := NewAnonNode()  abn.String() // -> ""_:n192853""  ```      ## Parsing data    The parser takes an `io.Reader` as first parameter, and the string containing the mime type as the second parameter.    Currently, the supported parsing formats are Turtle (with mime type `text/turtle`) and JSON-LD (with mime type `application/ld+json`).    ### Parsing Turtle from an io.Reader    ```golang  // Set a base URI  baseUri := ""https://example.org/foo""    // Create a new graph  g, _ := NewGraph(baseUri)    // r is of type io.Reader  g.Parse(r, ""text/turtle"")  ```    ### Parsing JSON-LD from an io.Reader    ```golang  // Set a base URI  baseUri := ""https://example.org/foo""    // Create a new graph  g, _ := NewGraph(baseUri)    // r is an io.Reader  g.Parse(r, ""application/ld+json"")  ```    ### Parsing either Turtle or JSON-LD from a URI on the Web    In this case you don't have to specify the mime type, as the internal http client will try to content negotiate to either Turtle or JSON-LD. An error will be returned if it fails.    **Note:** The `NewGraph()` function accepts an optional parameter called `skipVerify` that is used to tell the internal http client whether or not to ignore bad/self-signed server side certificates. By default, it will not check if you omit this parameter, or if you set it to `true`.    ```golang  // Set a base URI  uri := ""https://example.org/foo""    // Check remote server certificate to see if it's valid   // (don't skip verification)  skipVerify := false    // Create a new graph. You can also omit the skipVerify parameter  // and accept invalid certificates (e.g. self-signed)  g, _ := NewGraph(uri, skipVerify)    err := g.LoadURI(uri)  if err != nil {  	// deal with the error  }  ```      ## Serializing data      The serializer takes an `io.Writer` as first parameter, and the string containing the mime type as the second parameter.    Currently, the supported serialization formats are Turtle (with mime type `text/turtle`) and JSON-LD (with mime type `application/ld+json`).      ### Serializing to Turtle    ```golang  // Set a base URI  baseUri := ""https://example.org/foo""    // Create a new graph  g, _ := NewGraph(baseUri)    triple := NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""c""))  g.Add(triple)    // w is of type io.Writer  g.Serialize(w, ""text/turtle"")  ```    ### Serializing to JSON-LD    ```golang  // Set a base URI  baseUri := ""https://example.org/foo""    // Create a new graph  g, _ := NewGraph(baseUri)    triple := NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""c""))  g.Add(triple)    // w is of type io.Writer  g.Serialize(w, ""application/ld+json"")  ``` """
Semantic web;https://github.com/egonw/jqudt;"""[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3883588.svg)](https://doi.org/10.5281/zenodo.3883588)  [![Build Status](https://travis-ci.org/egonw/jqudt.svg?branch=master)](https://travis-ci.org/egonw/jqudt)  [![Maven Central](https://img.shields.io/maven-central/v/com.github.egonw/jqudt.svg?label=Maven%20Central)](https://search.maven.org/search?q=g:%22com.github.egonw%22%20AND%20a:%22jqudt%22)    Introduction  ============    Java Library to deal with QUDT units and conversions between them.    QUDT is ""Quantities, Units, Dimensions and Data Types in OWL and XML""        http://www.qudt.org/    QUDT is a CC-SA-BY project by NASA Ames Research Center and TopQuadrant, Inc.    License of this Java library: new BSD    Installation  ============    Maven:    ```xml  <dependency>    <groupId>com.github.egonw</groupId>    <artifactId>jqudt</artifactId>    <version>1.4.0</version>  </dependency>  ```    Groovy:    ```groovy  @Grab(group='com.github.egonw', module='jqudt', version='1.4.0')  ```    Quick demo  ==========    Keep in mind, that the below conversions are purely derived from the information  defined in the QUDT ontology, taking advantage from the fact that the have the  same unit type, qudt:MolarConcentrationUnit and qudt:TemperatureUnit respectively.    Source:    ```java  Quantity obs = new Quantity(0.1, ConcentrationUnit.MICROMOLAR);  System.out.println(obs + "" = "" +  obs.convertTo(ConcentrationUnit.NANOMOLAR));    Quantity temp = new Quantity(20, TemperatureUnit.CELSIUS);  System.out.println(temp + "" = "" +  temp.convertTo(TemperatureUnit.KELVIN));  ```    Output    ```  0.1 μM = 100.00000000000001 nM  20.0 C = 293.0 K  ``` """
Semantic web;https://github.com/d2rq/r2rml-kit;"""# r2rml-kit – Export relational databases to RDF with R2RML    **r2rml-kit** is an implementation of W3C's [R2RML](https://www.w3.org/TR/r2rml/) and [Direct Mapping](https://www.w3.org/TR/rdb-direct-mapping/) standards. It can:    - Generate an R2RML mapping by inspecting a relational database schema  - Validate R2RML mapping files  - Dump the contents of a database to an RDF file according to an R2RML mapping  - Dump the contents of a database to an RDF file according to the W3C Direct Mapping  - Access the contents of a database through the Jena API    Besides R2RML, **r2rml-kit** also supports the [D2RQ mapping language](http://d2rq.org/d2rq-language).    **r2rml-kit** is an offshoot of [D2RQ](http://d2rq.org/), based on its abandoned `develop` branch. Unlike D2RQ, it does not support SPARQL, and does not include a server application equivalent to D2RQ's D2R Server.    **r2rml-kit** is currently in pre-alpha stage. It is not yet fully separated from the D2RQ codebase, and many things will not yet work. It does not support R2RML's named graph features. See [`TODO.md`](https://github.com/d2rq/r2rml-kit/blob/master/TODO.md) for a short-term roadmap.    ## Running r2rml-kit    After building with `mvn compile`, you can test-run the various components. Let's assume you have a MySQL database called `mydb` on your machine.    ### Generating a default mapping file    ```./generate-mapping -u root -o mydb.ttl jdbc:mysql:///mydb```    This generates a mapping file `mydb.ttl` for your database.    ### Validating an R2RML mapping    ```./validate mydb.ttl```    This validates the mapping file `mydb.ttl`.    ### Dumping the database    ```./dump-rdf -m mydb.ttl -o dump.nt```    This creates `dump.nt`, a dump containing the mapped RDF in N-Triples format.    ### Running the unit tests    The unit tests can be executed with `mvn test`.    Some unit tests rely on MySQL being present, and require that two databases are created:    1. A database called `iswc` that contains the data from `src/test/resources/example/iswc-mysql.sql`:        echo ""CREATE DATABASE iswc"" | mysql -u root      mysql -u root iswc < src/test/resources/example/iswc-mysql.sql    2. An empty database called `D2RQ_TEST`. """
Semantic web;https://github.com/ssrangan/gm-sparql;"""# gm-sparql  Graph Mining Using SPARQL    The Resource Description Framework (RDF) and SPARQL Protocol and RDF Query Language (SPARQL) were introduced about a decade ago to enable flexible schema-free data interchange on the Semantic Web. Today, data scientists use the framework as a scalable graph representation for integrating, querying, exploring and analyzing data sets hosted at different sources. With increasing adoption, the need for graph mining capabilities for the Semantic Web has emerged. We address that need through implementation of popular iterative Graph Mining algorithms (degree distribution, diameter, radius, node eccentricity, triangle count, connected component analysis, and PageRank). We implement these algorithms as SPARQL queries, wrapped within Python scripts. These graph mining algorithms (that have a linear-algebra formulation) can indeed be unleashed on data represented as RDF graphs using the SPARQL query interface.    Although primarily developed for Cray's Urika hosted at the Department of Energy's Oak Ridge National Laboratory, this open source version works on Apache Jena triplestore. We have tested EAGLE to work on desktops, laptops and cloud services such as Amazon EC2. EAGLE can play a critical role in the exploratory analysis of massive heterogeneous graph data (e.g. semantic knowledge graphs). We believe with more support and user feedback we can create a ""MATLAB"" for LinkedData.    We really appreciate citing the following paper if our code was useful in anyway to your work.    S.Lee (lees4@ornl.gov) , S.R. Sukumar (sukumarsr@ornl.gov) and S- H. Lim (lims1@ornl.gov), ”Graph mining meets the Semantic Web”, in the Proc. of the Workshop on Data Engineering meets the Semantic Web in conjunction with International Conference on Data Engineering, Korea, 2015.    List of ORNL team members (in alphabetical order)  * Sangkeun Lee (lees4@ornl.gov, leesangkeun@gmail.com) - Post-doc  * Seung-Hwan. Lim (lims1@ornl.gov)  * Seokyong Hong (shong3@ncsu.edu) - Intern  * Sreenivas R. Sukumar (sukumarsr@ornl.gov)  * Tyler C. Brown (browntc@ornl.gov) - Intern   """
Semantic web;https://github.com/egonw/rrdf;"""  # About    Package to bring RDF and SPARQL functionality to R, using the Jena libraries (needs Java7 or higher).        java/ -> Helper classes for the R rdf package.      rdf/ -> R package for dealing with RDF, using Jena.      rrdflibs/ -> Jena libraries    Information about this package can be found in this preprint:        Willighagen E. (2014) Accessing biological data in R with semantic web      technologies. PeerJ PrePrints 2:e185v3    See https://dx.doi.org/10.7287/peerj.preprints.185v3    (Please cite this paper if you use this package.)    To cite a specific release, you can use these http://zenodo.org/ DOIs:    * rrdf 2.1.2 [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.34307.svg)](http://dx.doi.org/10.5281/zenodo.34307)  * rrdf 2.1.1 [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.20717.svg)](http://dx.doi.org/10.5281/zenodo.20717)    # User mailing list        https://groups.google.com/forum/#!forum/rrdf-user    # Copyright / License    ## rrdflibs package    Copyright (C) 2011-2015  Egon Willighagen and contributors    Apache License 2.0 for for the rrdflibs package files.    Copyright for Jena is described in the LICENSE and java/NOTICE  files. Please also visit https://jena.apache.org/.    ## rrdf package    Copyright (C) 2011-2015  Egon Willighagen and contributors    License AGPL v3 for the rrdf package.    ## Authors / Contributors    Authors:    Egon Willighagen    Contributions from:    Carl Boettiger,  Ryan Kohl    (See: https://github.com/egonw/rrdf/graphs/contributors)    # Install from R    Previously, the packages were available from CRAN, but this is no longer the case.    Mind you, the below install_github() method will attempt to rebuild the vignette  and therefore at this moment require a LaTeX distribution with pdflatex and a few  packages installed. See also issue https://github.com/egonw/rrdf/issues/28 and  https://github.com/egonw/rrdf/issues/29.        > install.packages(""rJava"") # if not present already      > install.packages(""devtools"") # if not present already      > library(devtools)      > install_github(""egonw/rrdf"", subdir=""rrdflibs"")      > install_github(""egonw/rrdf"", subdir=""rrdf"", build_vignettes = FALSE)    # Compile from source        $ cd rrdf/java      $ groovy build.groovy      $ cd ../..      $ R CMD build rrdflibs      $ R CMD check --as-cran rrdflibs_1.4.0.tar.gz      $ R CMD build rrdf      $ tar xvf rrdf_2.0.4.tar.gz rrdf/inst/doc/tutorial.pdf      $ R CMD check --as-cran rrdf_2.0.4.tar.gz    # Error Handling    In case of issues on Mac, follow the instructions below:    - check that the $JAVA_PATH variable is correctly set. If not:   1. Run `touch ~/.bash_profile; open ~/.bash_profile` in a Terminal window.   2. add the following lines to your .bash_profile and then save:      export JAVA_HOME=$(/usr/libexec/java_home)      export LD_LIBRARY_PATH=/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/server       export PATH=$PATH:$JAVA_HOME/bin   3. Run `source ~/.bash_profile` in a terminal window.    - check that that Java >=1.7 is installed in your system. If not, go to [https://java.com/it/download/](https://java.com/it/download/)  - check that R sees the latest Java version (`.jinit();.jcall(""java/lang/System"", ""S"", ""getProperty"", ""java.runtime.version"")`). If not [1,2]:   1. Download and install Apple’s Java version 1.6 like you were asked to.   2. Reconfigure your R installation by typing `sudo R CMD javareconf` in a Terminal window.   3. Trigger a recompile by reinstalling rJava by typing `install.packages('rJava', type='source')`.   4. Run `sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/lib` in a Terminal window. """
Semantic web;https://github.com/RDFLib/geosparql-dggs;"""# RDFlib GeoSPARQL Functions for DGGS    This library provides support for the [GeoSPARQL 1.1 Simple Features Relation Family](https://opengeospatial.github.io/ogc-geosparql/geosparql11/spec.html#_simple_features_relation_family_relation_familysimple_features)  for geometries expressed as [DGGS Literals](https://opengeospatial.github.io/ogc-geosparql/geosparql11/spec.html#_rdfs_datatype_geodggsliteral).  Currently, [rHEALPix DGGS](https://iopscience.iop.org/article/10.1088/1755-1315/34/1/012012/pdf) Grids are supported.      ## Installation  From the python package index, PyPi: https://pypi.org/project/geosparql-dggs/    `pip install geosparql-dggs`    This package depends on to support the functions' use against graphs [RDFlib](https://pypi.org/project/rdflib/). The functions themselves depend on the [rHEAL-sf](https://github.com/surroundaustralia/rhealpix-sf/)   library, which in turn depends on the [rHEAL-geo](https://github.com/surroundaustralia/rhealpix-geo/) library for the base classes  which represent DGGS Cells and collections of Cells.   ## Use  These functions are implemented in RDFlib Python in the file `gsdggs/sf_functions.py` and are imported into `gsdggs/__init__.py` and registered there in RDFlib as SPARQL extension functions with their IRIs.    This means they can be used like this (full working script):    ```python  from rdflib import Literal, Graph, Namespace, URIRef  from gsdggs import DGGS    EX = Namespace(""http://example.com/"")  GEO = Namespace(""http://www.opengis.net/ont/geosparql#"")    # Define the DGGS Geometries, add them to an in-memory RDF graph  g = Graph()  g.add((URIRef('https://geom-a'), GEO.asDGGS, Literal('CELLLIST ((R0 R10 R13 R16 R30 R31 R32 R40))', EX.ausPixLiteral)))  g.add((URIRef('https://geom-b'), GEO.asDGGS, Literal('CELLLIST ((R06 R07 R30 R31))', EX.ausPixLiteral)))  g.add((URIRef('https://geom-c'), GEO.asDGGS, Literal('CELLLIST ((R11 R12 R14 R15))', EX.ausPixLiteral)))    # Query the in-memory graph  q = """"""      PREFIX geo: <http://www.opengis.net/ont/geosparql#>      PREFIX dggs: <https://placeholder.com/dggsfuncs/>            SELECT ?a ?b       WHERE {          ?a geo:asDGGS ?a_geom .          ?b geo:asDGGS ?b_geom .                    FILTER dggs:sfWithin(?a_geom, ?b_geom)      }""""""  # Interate through and print results  for r in g.query(q):      print(f""{r['a']} is within {r['b']}"")  ```  The above stript outputs:    ```bash  https://geom-b is within https://geom-a  ```    The functions can also be used directly (without RDFLib) by direct import from _source, for example:  ```python  from _source import sfEquals    sfEquals(""R1"", ""R1"")  ```  The above stript outputs:    ```bash  True  ```  ## Function Definitions  The Simple Feature relations have been interpreted in the following way for the context of a nested square DGGS grid (such as rHEALPix grids).      * **dggs:sfEqual:** Two sets of cells are equal if they have the same identifier.    * **dggs:sfWithin:** One set of cells (A) is within some other set of cells (B) if the addition of A's cells to B results in a set of cells equal to B, where A is not equal to B.    * **dggs:sfContains:** One set of cells (A) is contains some other set of cells (B) if the addition of A's cells to B results in a set of cells equal to A, where A is not equal to B.    * **dggs:sfIntersects:** One set of cells (A) intersects some other set of cells (B) where they share any two cells, or any cell in A is the parent or child of a cell in B, or any cell in A or B touches.    * **dggs:sfTouches:** One set of cells (A) touches some other set of cells (B) where the cells meet at an edge, or vertex.    * **dggs:sfDisjoint:** One set of cells (A) is disjoint with some other set of cells (B) where they do not share any two cells, no cell in A is the parent or child of a cell in B, and no cells in A and B touch.    * **dggs:sfOverlaps:** One set of cells (A) overlaps some other set of cells (B) where the addition of A's cells to B results in a set of cells different from A and B, and A and B are not disjoint and do not touch.    ## Testing  All tests are in `tests/` and implemented using [pytest](https://docs.pytest.org/en/6.2.x/index.html).    There are individual tests for each function, along with more granular tests for supporting Python classes (Cells and CellCollections), as well as application of the functions without RDF.     ## Contributing  Via GitHub, Issues & Pull Requests:     * <https://github.com/rdflib/geosparql-dggs>    ## License  This code is licensed with the BSD 3-clause license as per [LICENSE](LICENSE) which is the same license as used for [rdflib](https://pypi.org/project/rdflib/).    ## Citation  ```bibtex  @software{https://github.com/rdflib/geosparql-dggs,    author = {{David Habgood}},    title = {RDFlib GeoSPARQL Functions for DGGS},    version = {0.0.1},    date = {2021},    url = {https://github.com/rdflib/geosparql-dggs}  }  ```    ## Contact  _Creator & maintainer:_    **David Habgood**    _Application Architect_    [SURROUND Australia Pty Ltd](https://surroundaustralia.com)    <david.habgood@surroundaustrlaia.com>      https://orcid.org/0000-0002-3322-1868 """
Semantic web;https://github.com/lanthaler/HydraClient;"""HydraClient  ===========    This is work in progress. Documentation will follow. In the meantime, check  out [Hydra](http://www.markus-lanthaler.com/hydra/). """
Semantic web;https://github.com/gniezen/n3pygments;"""==========  n3pygments  ==========    This is a [Pygments](http://http://pygments.org/) lexer that performs syntax highlighting for:    * n3, turtle : Turtle/N3/NT (*.ttl, *.n3 and *.NT)   * sparql : SPARQL (*.sparql)    Make sure you're running Pygments 1.7 or higher and run        sudo python setup.py install        to install and e.g.        pygmentize -l turtle filename.ttl    to run Pygments.    This is mostly code from [Openvest](http://www.openvest.com/trac/wiki/n3SyntaxHighlighting#Pygments) which seems to be abandoned. The original instructions on that site only works when using `pygmentize` from the command-line. This implementation registers the package as a proper Pygments plugin which you can use from within Python, e.g.:        from pygments.lexers import (get_lexer_by_name,get_lexer_for_filename)      get_lexer_by_name(""turtle"")    should return `<pygments.lexers.Notation3Lexer>`.     n3pygments was created based on [this answer](http://tex.stackexchange.com/a/14929/8419) on the TeX StackExchange site. So yes, you can use it to perform using syntax highlighting on your code in LaTeX using [Minted](http://code.google.com/p/minted/). I have also used it with success to perform syntax highlighting on an [Octopress 2.0](http://octopress.org) blog.    Thanks go out to [Raphaël Pinson](http://www.raphink.info) and [Philip Cooper](http://Openvest.com).      """
Semantic web;https://github.com/sebferre/sparklis;"""<meta charset=""UTF-8""/>    # What is Sparklis?    Sparklis is a query builder in natural language that allows people to explore and query SPARQL endpoints with all the power of SPARQL and without any knowledge of SPARQL, nor of the endpoint vocabulary.    Sparklis is a Web client running entirely in the browser. It directly connects to SPARQL endpoints to retrieve query results and suggested query elements. It covers a large subset of SPARQL 1.1 `SELECT` queries: basic graph patterns including cycles, `UNION`, `OPTIONAL`, `NOT EXISTS`, `FILTER`, `BIND`, complex expressions, aggregations, `GROUP BY`, `ORDER BY`. All those features can be combined in a flexible way, like in SPARQL. Results are presented as tables, and also on maps. A  configuration panel offers a few configuration options to adapt to different endpoints (e.g., GET/POST, labelling properties and language tags). Sparklis also includes the YASGUI editor to let advanced users access and modify the SPARQL translation of the query.    Sparklis reconciles expressivity and usability in semantic search by tightly combining a Query Builder, a Natural Language Interface, and a Faceted Search system. As a *Query Builder* it lets users build complex queries by composing elementary queries in an incremental fashion. An elementary query can be a class (e.g., ""a film""), a property (e.g., ""that has a director""), a RDF node (e.g., ""Tim Burton""), a reference to another node (e.g., ""the film""), or an operator (e.g., ""not"", ""or"", ""highest-to-lowest"", ""+"", ""average"").    As a *Faceted Search system*, at every step, the query under construction is well-formed, query results are computed and displayed, and the suggested query elements are derived from actual data - not only from schema - so as to prevent the construction of non-sensical or empty results. The display of results and data-relevant suggestions at every step provides constant and acurate feedback to users during the construction process. This supports exploratory search, serendipity, and confidence about final results.    As a *Natural Language Interface*, everything presented to users - queries, suggested query elements, and results - are verbalized in natural language, completely hidding SPARQL behind the user interface. Compared to Query Answering (QA) systems, the hard problem of spontaneous NL understanding is avoided by controlling query formulation through guided query construction, and replaced by the simpler problem of NL generation. The user interface lends itself to multilinguality, and is so far available in English, French, Spanish, and Dutch.    When refering to Sparklis in scientific documents, please use the following citation.    > Sébastien Ferré: *Sparklis: An expressive query builder for SPARQL endpoints with guidance in natural language.* Semantic Web 8(3): 405-418 (2017)    # Where can I try Sparklis?    Simply follow those steps:  1. Go to the [online application](http://www.irisa.fr/LIS/ferre/sparklis/)  2. Select a SPARQL endpoint in the dropdown list at the top (the default one is *Core English DBpedia*, a core subset of DBpedia)  3. Build your query incrementally by clicking suggested query elements (in the three lists of suggestions, and through the hamburger menu in the query), and clicking different parts of the query to change focus. The suggestions are relative to the current focus.    We recommend to visit the [*Examples page*](http://www.irisa.fr/LIS/ferre/sparklis/examples.html) where there are 100+ example queries over several datasets. Every example query can be opened in Sparklis in one click, and the query can then be further modified. For a number of them, there is a YouTube screencast to show how they are built step by step.    # How can I use Sparklis on my own RDF dataset?    It is enough to have a SPARQL endpoint for your dataset that is visible from your machine. It can be a publicly open endpoint (like for DBpedia or Wikidata), or a localhost endpoint (I personally use Apache Jena Fuseki but other stores should work too). The one important condition is that the endpoint server be [CORS-enabled](https://www.w3.org/wiki/CORS_Enabled) so that HTTP requests can be made to it from your browser, where Sparklis runs.    Here a few recommendations about the contents of your store for best results:  * include RDFS/OWL triples declaring classes (`rdfs:Class`, `owl:Class`) and properties (`rdf:Property`, `owl:ObjectProperty`, `owl:DataProperty`), as well as their labels (`rdfs:label`) and their hierarchy (`rdfs:subClassOf`, `rdfs:subPropertyOf`)  * ensure that all URI-resources have their label defined, preferably with `rdfs:label` and possibly with other standard properties (e.g., `skos:prefLabel`)  * if named graphs are used, make sure to configure your store so that the default graphs contains the union of those named graphs    The *Configure* menu offers a number of options to adapt Sparklis to your endpoint, and control the display. Here is a non-exhaustive list:  * Endpoint and queries: max numbers of results/suggestions, GET vs POST, credentials  * Ontology: display of class/property hierarchies, filtering of classes/properties, use of Sparklis-specific schema properties (see below)  * Language and labels: interface language, labelling properties, fulltext search support    Sparklis makes use of standard and non-standard properties to get more control on the building of queries, and on the display of suggestions and results. For ech property, there is generally a configuration option to activate its use.  * `sdo:position` (`sdo: = https://schema.org/`): it is possible to control the ordering of suggestions (classes, properties, and individuals) by setting this property with the desired rank of the suggestion. The related option is in *Configure advanced features*.  * `sdo:logo`: it is possible to have small icons in front of entity labels by setting this property with URLs to those icons. Several icons can be attached to a same entity. The related option is in *Configure advanced features*, where the size of icons can be defined.  * `rdfs:inheritsThrough`: suppose you have a `ex:location` property whose range `ex:Place` is organized into a hierarchy through the property `ex:isPartOf`. By adding to your dataset the triple `ex:location rdfs:inheritsThrough ex:isPartOf`, you get that whenever property `ex:location` is inserted into the query, inheritance through the place hierarchy is performed, and place suggestions are displayed as a tree. This is a generalization of the well-known `rdf:type` inheritance through `rdfs:subClassOf`. By adding triple `ex:Place rdfs:inheritsThrough ex:isPartOf`, the same effect is obtained when inserting class `ex:Place`. The related option in *Configure the ontology* must be activated.  * `lis-owl:transitiveOf` (`lis-owl: = http://www.irisa.fr/LIS/ferre/vocab/owl#`): the use of `rdfs:inheritsThrough` entails the insertion of transitive paths (e.g., `ex:isPartOf*`) in the SPARQL query, which are very costly to evaluate. One solution is to materialize the transitive closure as a new property `ex:isPartOf_star` in the dataset, and to add the triple `ex:isPartOf_star lis-owl:transitiveOf ex:isPartOf`. By activating the related option in *Configure the ontology*, property path `ex:isPartOf*` will be replaced by `ex:isPartOf_star`.  * `nary:subjectObject` (`nary: = http://www.irisa.fr/LIS/ferre/vocab/nary#`): this property handles the case of a reified relationship where there is a property `PS` from reification node to subject, and a property `PO` from reification node to object. By adding triple `PS nary:subjectObject PO`, the reification becomes transparent in Sparkls. See cyan properties on the Mondial endpoint for examples. The related option in *Configure the ontology* must be activated.  * `nary:eventObject`: this is similar as above, except there is a property `PE` from subject to reification node (instead of the inverse `PS`). See cyan properties in the Wikidata endpoint for examples.    Once you have found a good configuration of your endpoint, you can generate a *permalink* with the button at the top, which you can share to the endpoint users. Those permalinks also include the current query and current focus, so you can also share example queries and template queries. You can also save queries by simply adding bookmarks in your browser.    If you find your endpoint of general interest, you are welcome to suggest me to add it to the list of SPARQL endpoints.    # How do I reuse Sparklis in my web site?    As Sparklis is only client-side code, it is possible to integrate Sparklis into your website by simply copying the contents of the `webapp` folder among your website files, and adding links to it from your web pages, with URL arguments containing the endpoint, the configuration, and possibly an initial query. To get those URLs, simply navigate in Sparklis and copy (and adapt as needed) the browser URL.    You can adapt the appearance of the main HTML file (`osparklis.html`, `osparklis.css`) as long as you retain the *Sparklis* name, and the credits in the page footer. You can for instance hide some configuration options and elements, you can change the look-and-feel, and the layout of elements. Be careful not to delete element ids and classes that are used by the JS code of Sparklis.    Let me know of successful integrations, and also of problems you encounter in the process.    # Credits    Author: [Sébastien Ferré](http://people.irisa.fr/Sebastien.Ferre/)    Affiliation: Univ. Rennes 1, team [SemLIS](http://www-semlis.irisa.fr/) at IRISA    Copyright © 2013 Sébastien Ferré, IRISA, Université de Rennes 1, France    Licence: Apache Licence 2.0    Citation: *Ferré, Sébastien. ‘Sparklis: An Expressive Query Builder for SPARQL Endpoints with Guidance in Natural Language’. Semantic Web 8(3) : 405-418. IOS Press, 2017.* [PDF](https://hal.inria.fr/hal-01485093/file/sparklis-preprint.pdf) """
Semantic web;https://github.com/drobilla/serd;"""Serd  ====    Serd is a lightweight C library for RDF syntax which supports reading and  writing [Turtle][], [TriG][], [NTriples][], and [NQuads][].  Serd is suitable  for performance-critical or resource-limited applications, such as serialising  very large data sets or embedded systems.    Features  --------     * **Free:** Serd is [Free Software][] released under the extremely liberal     [ISC license][].     * **Portable and Dependency-Free:** Serd has no external dependencies other     than the C standard library.  It is known to compile with GCC, Clang, and     MSVC (as C++), and is tested on GNU/Linux, MacOS, and Windows.     * **Small:** Serd is implemented in a few thousand lines of C.  It typically     compiles to about 100 KiB, or about 50 KiB stripped with size optimizations.     * **Fast and Lightweight:** Serd can stream abbreviated Turtle, unlike many     tools which must first build an internal model.  This makes it particularly     useful for writing very large data sets, since it can do so using only a     small amount of memory.  Serd is, to the author's knowledge, the fastest     Turtle reader/writer by a wide margin (see [Performance](#performance)     below).     * **Conformant and Well-Tested:** Serd passes all tests in the Turtle and TriG     test suites, correctly handles all ""normal"" examples in the URI     specification, and includes many additional tests which were written     manually or discovered with fuzz testing.  The test suite is run     continuously on many platforms, has 100% code coverage by line, and runs     with zero memory errors or leaks.    Performance  -----------    The benchmarks below compare `serdi`, [rapper][], and [riot][] re-serialising  Turtle data generated by [sp2b][] on an i7-4980HQ running Debian 9.  Of the  three, `serdi` is the fastest by a wide margin, and the only one that uses a  constant amount of memory (a single page) for all input sizes.    ![Time](doc/serdi-time.svg)  ![Throughput](doc/serdi-throughput.svg)  ![Memory](doc/serdi-memory.svg)    Documentation  -------------     * [API reference (single page)](https://drobilla.gitlab.io/serd/c/singlehtml)   * [API reference (paginated)](https://drobilla.gitlab.io/serd/c/html)   * [`serdi` man page](https://drobilla.gitlab.io/serd/man/serdi.html)     -- David Robillard <d@drobilla.net>    [Turtle]: https://www.w3.org/TR/turtle/  [TriG]: https://www.w3.org/TR/trig/  [NTriples]: https://www.w3.org/TR/n-triples/  [NQuads]: https://www.w3.org/TR/n-quads/  [Free Software]: http://www.gnu.org/philosophy/free-sw.html  [ISC license]: http://opensource.org/licenses/isc  [rapper]: http://librdf.org/raptor/  [riot]: https://jena.apache.org/  [sp2b]: http://www2.informatik.uni-freiburg.de/~mschmidt/docs/sp2b.pdf """
Semantic web;https://github.com/sisinflab-swot/cowl;"""# Cowl    *Cowl* is a lightweight C API for working with OWL 2 ontologies, developed by  [SisInf Lab][swot] at the [Polytechnic University of Bari][poliba].    ### Documentation    Documentation and build instructions are available [online][docs].    ### Changelog    See [CHANGELOG.md](CHANGELOG.md)    ### Copyright and License    Copyright (c) 2019-2021 [SisInf Lab][swot], [Polytechnic University of Bari][poliba]    Cowl is distributed under the [Eclipse Public License, Version 2.0][epl2].    [docs]: http://swot.sisinflab.poliba.it/cowl  [epl2]: https://www.eclipse.org/legal/epl-2.0  [poliba]: http://www.poliba.it  [swot]: http://swot.sisinflab.poliba.it """
Semantic web;https://github.com/jindrichmynarz/sparqlab;"""# SPARQLab    SPARQLab is a lab for exercising the SPARQL query language. Your task in SPARQLab is to formulate SPARQL queries that produce results satisfying the given requirements. Each exercise is provided with an exemplary solution. The application is based on data on pension statistics from the Czech social security administration. The exercises focus on data exploration, such as for finding the dimensions used in the data, but also on domain-specific analyses, such as for detecting regions with the largest pay gap between male and female retirees.    ## Deployment    SPARQLab can be deployed via [docker-compose](https://docs.docker.com/compose). It uses [Stardog Community Edition](http://stardog.com) as its RDF store, which can be installed via [this Docker image](https://github.com/jindrichmynarz/sparqlab-stardog).    SPARQLab is a web application based on the [Luminus](https://luminusweb.com) framework. If you need to run it on a non-root path of your web server, such as when in a web application container, you can set the path via the `APP_CONTEXT` enviroment variable in `docker-compose.yml`. Similarly, you can customize the SPARQL endpoint SPARQLab connects to via the `SPARQL_ENDPOINT` environment variable.    ## Acknowledgements    SPARQLab was funded by developement grants of the University of Economics no. 6/2016 and no. 50/2017. The application received additional support from the prize for the best student application based on Czech open data in the Czech Open Data Challenge 2016.    ## License    Copyright © 2016 Jindřich Mynarz, Vojtěch Svátek, and Jan Kučera    Distributed under the Eclipse Public License version 1.0. """
Semantic web;https://github.com/AKSW/NSpM;"""# 🤖 Neural SPARQL Machines    [![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-370/)    A Machine-Translation Approach for Question Answering over Knowledge Graphs.    ![What does a NSpM do?](http://www.liberai.org/img/NSpM-image.png ""What does a NSpM do?"")    ## IMPORTANT    If you are looking for the code for papers _""SPARQL as a Foreign Language""_ and _""Neural Machine Translation for Query Construction and Composition""_ please checkout tag [v0.1.0-akaha](https://github.com/LiberAI/NSpM/tree/v0.1.0-akaha) or branch [v1](https://github.com/LiberAI/NSpM/tree/v1).    ## Install    ### Via pip    Coming soon!    ### Local setup    Clone the repository.    ```bash  pip install -r requirements.txt  ```    ## Example of usage    ### The Generator module    #### Pre-generated data    You can extract pre-generated data and model checkpoints from [here](https://nspm-models.s3.eu-west-2.amazonaws.com/v2/art_30.zip) (1.1 GB) in folders having the respective names.    #### Manual Generation (Alternative to using pre-generated data)    The template used in the paper can be found in a file such as `Annotations_F30_art.csv`. `data/art_30` will be the ID of the working dataset used throughout the tutorial. To generate the training data, launch the following command.    ```bash  mkdir -p data/art_30  python nspm/generator.py --templates data/templates/Annotations_F30_art.csv --output data/art_30  ```    Launch the command if you want to build dataset seprately else it will internally be called while training.    ```bash  python nspm/data_gen.py --input data/art_30 --output data/art_30  ```    ### The Learner module    Now go back to the initial directory and launch `learner.py` to train the model.     ```bash  python nspm/learner.py --input data/art_30 --output data/art_30  ```    This command will create a model checkpoints in `data/art_30` and some pickle files in `data/art_30/pickle_objects`.    ### The Interpreter module    Predict the SPARQL query for a given question it will store the detailed output in output_query.    ```bash  python nspm/interpreter.py --input data/art_30 --output data/art_30 --query ""yuncken freeman has architected in how many cities?""  ```  or, if you want to use NSpM with [airml](https://github.com/sahandilshan/airML) to install pre-trained models, follow these steps,  1. Install airML latest version from [here](https://pypi.org/project/airML/)  2. Navigate to the table.kns [here](https://github.com/sahandilshan/KBox/blob/dev/kns/2.0/table.kns) and check if your model is listed in that file.  3. Then copy the name of that model and use it with the `interpreter.py` as follows  ```bash  python interpreter.py --airml http://nspm.org/art --output data/art_30 --inputstr ""yuncken freeman has architected in how many cities?""  ```    ## Use cases & integrations    * Components of the [Adam Medical platform](https://www.graphen.ai/products/mi_feature.html) partly developed by [Jose A. Alvarado](https://www.linkedin.com/in/josealvaradoguzman/) at Graphen (including a humanoid robot called Dr Adam), rely on NSpM technology.  * The [Telegram NSpM chatbot](https://github.com/AKSW/NSpM/wiki/NSpM-Telegram-Bot) offers an integration of NSpM with the Telegram messaging platform.  * The [Google Summer of Code](https://summerofcode.withgoogle.com/) program has been supporting 6 students to work on NSpM-backed project ""[A neural question answering model for DBpedia](https://github.com/dbpedia/neural-qa)"" since 2018.  * A [question answering system](https://github.com/qasim9872/question-answering-system) was implemented on top of NSpM by [Muhammad Qasim](https://github.com/qasim9872).    ## Publications    ### SPARQL as a Foreign Language (2017)    * arXiv: https://arxiv.org/abs/1708.07624    ```  @inproceedings{soru-marx-2017,      author = ""Tommaso Soru and Edgard Marx and Diego Moussallem and Gustavo Publio and Andr\'e Valdestilhas and Diego Esteves and Ciro Baron Neto"",      title = ""{SPARQL} as a Foreign Language"",      year = ""2017"",      journal = ""13th International Conference on Semantic Systems (SEMANTiCS 2017) - Posters and Demos"",      url = ""https://arxiv.org/abs/1708.07624"",  }  ```    ### Neural Machine Translation for Query Construction and Composition (2018)    * NAMPI Website: https://uclnlp.github.io/nampi/  * arXiv: https://arxiv.org/abs/1806.10478    ```  @inproceedings{soru-marx-nampi2018,      author = ""Tommaso Soru and Edgard Marx and Andr\'e Valdestilhas and Diego Esteves and Diego Moussallem and Gustavo Publio"",      title = ""Neural Machine Translation for Query Construction and Composition"",      year = ""2018"",      journal = ""ICML Workshop on Neural Abstract Machines \& Program Induction (NAMPI v2)"",      url = ""https://arxiv.org/abs/1806.10478"",  }  ```    ### Exploring Sequence-to-Sequence Models for SPARQL Pattern Composition (2020)    * arXiv: https://arxiv.org/abs/2010.10900    ```  @inproceedings{panchbhai-2020,      author = ""Anand Panchbhai and Tommaso Soru and Edgard Marx"",      title = ""Exploring Sequence-to-Sequence Models for {SPARQL} Pattern Composition"",      year = ""2020"",      journal = ""First Indo-American Knowledge Graph and Semantic Web Conference"",      url = ""https://arxiv.org/abs/2010.10900"",  }  ```    ### Liber AI on Medium (2020)    * [What is a Neural SPARQL Machine?](https://medium.com/liber-ai/what-is-a-neural-sparql-machine-c35945a5d278)    ## Contact    ### Questions?  * Primary contacts: [Tommaso Soru](http://tommaso-soru.it) and [Edgard Marx](http://emarx.org).  * Neural SPARQL Machines [mailing list](https://groups.google.com/forum/#!forum/neural-sparql-machines).  * Join the conversation on [Gitter](https://gitter.im/LiberAI/community).    ### Follow us  * Follow the [project on ResearchGate](https://www.researchgate.net/project/Neural-SPARQL-Machines).  * Follow Liber AI Research on [Twitter](https://twitter.com/theLiberAI).    <p align=""center""><img tooltip=""Liber AI"" src=""http://www.liberai.org/img/Liber-AI-logo-name-200px.png"" alt=""Liber AI logo"" border=""0""></p> """
Semantic web;https://github.com/levelgraph/levelgraph;"""LevelGraph&nbsp;&nbsp;&nbsp;[![Dependency Status](https://david-dm.org/levelgraph/levelgraph.svg?theme=shields.io)](https://david-dm.org/levelgraph/levelgraph)  ===========    ![Logo](https://raw.githubusercontent.com/levelgraph/levelgraph/master/logo.png)    [![NPM](https://nodei.co/npm/levelgraph.png)](https://nodei.co/npm/levelgraph/)    __LevelGraph__ is a Graph Database. Unlike many other graph database,  __LevelGraph__ is built on the uber-fast key-value store  [LevelDB](http://code.google.com/p/leveldb/) through the powerful  [LevelUp](https://github.com/rvagg/node-levelup) library.  You can use it inside your node.js application or in any  IndexedDB-powered Browser.    __LevelGraph__ loosely follows the __Hexastore__ approach as presented in the article:  [Hexastore: sextuple indexing for semantic web data management  C Weiss, P Karras, A Bernstein - Proceedings of the VLDB Endowment,  2008](https://sci-hub.se/10.14778/1453856.1453965).  Following this approach, __LevelGraph__ uses six indices for every triple,  in order to access them as fast as it is possible.    __LevelGraph__ was presented in the paper [Graph databases in the  browser: using LevelGraph  to explore New Delhi - A. Maccioni, M. Collina - Proceedings of the VLDB Endowment, 2016](http://www.vldb.org/pvldb/vol9/p1469-maccioni.pdf).    Check out a [slideshow](http://nodejsconfit.levelgraph.io)  that introduces you to LevelGraph by  [@matteocollina](http://twitter.com/matteocollina) at  http://nodejsconf.it.    Also, give the [LevelGraph Playground](http://wileylabs.github.io/levelgraph-playground) to get a quick feel for adding JSON-LD and N3/Turtle documents to a filter-able subject, predicate, object table. The `db` variable in the browser console is very useful for checking out the full power of LevelGraph.    **LevelGraph** is an **OPEN Open Source Project**, see the <a href=""#contributing"">Contributing</a> section to find out what this means.      ## Table of Contents    * [Install](#install)  * [Usage](#usage)    * [Get and Put](#get-and-put)      * [Triple Properties](#triple-properties)      * [Limit and Offset](#limit-and-offset)      * [Reverse Order](#reverse-order)      * [Updating](#updating)      * [Multiple Puts](#multiple-puts)    * [Deleting](#deleting)    * [Searches](#searches)      * [Search Without Streams](#search-without-streams)      * [Triple Generation](#triple-generation)      * [Limit and Offset](#limit-and-offset-1)    * [Filtering](#filtering)    * [Putting and Deleting through Streams](#putting-and-deleting-through-streams)    * [Generate batch operations](#generate-batch-operations)    * [Generate levelup query](#generate-levelup-query)  * [Navigator API](#navigator-api)  * [LevelUp integration](#levelup-integration)  * [Browserify](#browserify)  * [RDF support](#rdf-support)  * [Extensions](#extensions)  * [TODO](#todo)  * [Contributing](#contributing)  * [Credits](#credits)  * [Contributors](#contributors)  * [LICENSE - ""MIT License""](#license---mit-license)      ## Install  ### On Node.js    ```  npm install levelgraph level-browserify --save  ```    At the moment it requires node v0.10.x, but the port to node v0.8.x  should be straighforward.  If you need it, just open a pull request.    ### In the Browser    Just download  [levelgraph.min.js](https://github.com/levelgraph/levelgraph/blob/master/build/levelgraph.min.js)  and you are done!    Alternatively, you can use [browserify](http://browserify.org/).    ## Usage    The LevelGraph API remains the same for Node.js and the browsers,  however the initialization change slightly.    Initializing a database is very easy:  ```javascript  var level = require(""level-browserify"");  var levelgraph = require(""levelgraph"");    // just use this in the browser with the provided bundle  var db = levelgraph(level(""yourdb""));  ```    ### Get and Put    Inserting a triple in the database is extremely easy:  ```javascript  var triple = { subject: ""a"", predicate: ""b"", object: ""c"" };  db.put(triple, function(err) {    // do something after the triple is inserted  });  ```    Retrieving it through pattern-matching is extremely easy:  ```javascript  db.get({ subject: ""a"" }, function(err, list) {    console.log(list);  });  ```    It even supports a Stream interface:  ```javascript  var stream = db.getStream({ predicate: ""b"" });  stream.on(""data"", function(data) {    console.log(data);  });  ```    #### Triple Properties    LevelGraph supports adding properties to triples with very  little overhead (apart from storage costs). It is very easy:  ```javascript  var triple = { subject: ""a"", predicate: ""b"", object: ""c"", ""someStuff"": 42 };  db.put(triple, function() {    db.get({ subject: ""a"" }, function(err, list) {      console.log(list);    });  });  ```    #### Limit and Offset    It is possible to implement pagination of get results by using  `'offset'` and `'limit'`, like so:    ```javascript  db.get({ subject: ""a"", limit: 4, offset: 2}, function(err, list) {    console.log(list);  });  ```    #### Reverse Order    It is possible to get results in reverse lexicographical order  using the `'reverse'` option. This option is only supported by  `get()` and `getStream()` and not available in `search()`.    ```javascript  db.get({ predicate: ""b"", reverse: true }, function (err, list) {    console.log(list);  });  ```      #### Updating    __LevelGraph__ does not support in-place update, as there are no  constraint in the graph.  In order to update a triple, you should first delete it:  ```javascript  var triple = { subject: ""a"", predicate: ""b"", object: ""c"" };  db.put(triple, function(err) {    db.del(triple, function(err) {      triple.object = 'd';      db.put(triple, function(err) {        // do something with your update      });    });  });  ```    #### Multiple Puts    __LevelGraph__ also supports putting multiple triples:  ```javascript  var triple1 = { subject: ""a1"", predicate: ""b"", object: ""c"" };  var triple2 = { subject: ""a2"", predicate: ""b"", object: ""d"" };  db.put([triple1, triple2],  function(err) {    // do something after the triples are inserted  });  ```    ### Deleting    Deleting is easy too:  ```javascript  var triple = { subject: ""a"", predicate: ""b"", object: ""c"" };  db.del(triple, function(err) {    // do something after the triple is deleted  });  ```      ### Searches    __LevelGraph__ also supports searches:  ```javascript  db.put([{    subject: ""matteo"",    predicate: ""friend"",    object: ""daniele""  }, {    subject: ""daniele"",    predicate: ""friend"",    object: ""matteo""  }, {    subject: ""daniele"",    predicate: ""friend"",    object: ""marco""  }, {    subject: ""lucio"",    predicate: ""friend"",    object: ""matteo""  }, {    subject: ""lucio"",    predicate: ""friend"",    object: ""marco""  }, {    subject: ""marco"",    predicate: ""friend"",    object: ""davide""  }], function () {      var stream = db.searchStream([{      subject: ""matteo"",      predicate: ""friend"",      object: db.v(""x"")    }, {      subject: db.v(""x""),      predicate: ""friend"",      object: db.v(""y"")    }, {      subject: db.v(""y""),      predicate: ""friend"",      object: ""davide""    }]);      stream.on(""data"", function(data) {      // this will print ""{ x: 'daniele', y: 'marco' }""      console.log(data);    });  });  ```    #### Search Without Streams    It also supports a similar API without streams:  ```javascript  db.put([{   //...  }], function () {      db.search([{      subject: ""matteo"",      predicate: ""friend"",      object: db.v(""x"")    }, {      subject: db.v(""x""),      predicate: ""friend"",      object: db.v(""y"")    }, {      subject: db.v(""y""),      predicate: ""friend"",      object: ""davide""    }], function(err, results) {      // this will print ""[{ x: 'daniele', y: 'marco' }]""      console.log(results);    });  });  ```    #### Triple Generation    It also allows to generate a stream of triples, instead of a solution:  ```javascript    db.search([{      subject: db.v(""a""),      predicate: ""friend"",      object: db.v(""x"")    }, {      subject: db.v(""x""),      predicate: ""friend"",      object: db.v(""y"")    }, {      subject: db.v(""y""),      predicate: ""friend"",      object: db.v(""b"")    }], {      materialized: {        subject: db.v(""a""),        predicate: ""friend-of-a-friend"",        object: db.v(""b"")      }    }, function(err, results) {      // this will print all the 'friend of a friend triples..'      // like so: {      //   subject: ""lucio"",      //   predicate: ""friend-of-a-friend"",      //   object: ""daniele""      // }    });  ```    #### Limit and Offset    It is possible to implement pagination of search results by using  `'offset'` and `'limit'`, like so:    ```javascript  db.search([{      subject: db.v(""a""),      predicate: ""friend"",      object: db.v(""x"")    }, {      subject: db.v(""x""),      predicate: ""friend"",      object: db.v(""y"")    }], { limit: 4, offset: 2 }, function(err, list) {      console.log(list);  });  ```    ### Filtering    __LevelGraph__ supports filtering of triples when calling `get()`   and solutions when calling `search()`, and streams are supported too.    It is possible to filter the matching triples during a `get()`:  ```javascript  db.get({      subject: 'matteo'    , predicate: 'friend'    , filter: function filter(triple) {        return triple.object !== 'daniele';      }  }, function process(err, results) {    // results will not contain any triples that    // have 'daniele' as object  });  ```    Moreover, it is possible to filter the triples during a `search()`  ```javascript  db.search({      subject: 'matteo'    , predicate: 'friend'    , object: db.v('x')    , filter: function filter(triple) {        return triple.object !== 'daniele';      }  }, function process(err, solutions) {    // results will not contain any solutions that    // have { x: 'daniele' }  });  ```    Finally, __LevelGraph__ supports filtering full solutions:  ```javascript  db.search({      subject: 'matteo'    , predicate: 'friend'    , object: db.v('x')  }, {      filter: function filter(solution, callback) {        if (solution.x !== 'daniele') {          // confirm the solution          callback(null, solution);        } else {          // refute the solution          callback(null);        }      }  }, function process(err, solutions) {    // results will not contain any solutions that    // have { x: 'daniele' }  });  ```    Thanks to solultion filtering, it is possible to implement a negation:  ```javascript  db.search({      subject: 'matteo'    , predicate: 'friend'    , object: db.v('x')  }, {      filter: function filter(solution, callback) {        db.get({            subject: solution.x          , predicate: 'friend'          , object: 'marco'        }, function (err, results) {          if (err) {            callback(err);            return;          }          if (results.length > 0) {            // confirm the solution            callback(null, solution);          } else {            // refute the solution            callback();          }        });      }  }, function process(err, solutions) {    // results will not contain any solutions that    // do not satisfy the filter  });  ```    The heavier method is filtering solutions, so we recommend filtering the  triples whenever possible.      ### Putting and Deleting through Streams    It is also possible to `put` or `del` triples from the store  using a `Stream2` interface:    ```javascript  var t1 = { subject: ""a"", predicate: ""b"", object: ""c"" };  var t2 = { subject: ""a"", predicate: ""b"", object: ""d"" };  var stream = db.putStream();    stream.write(t1);  stream.end(t2);    stream.on(""close"", function() {    // do something, the writes are done  });  ```    ### Generate batch operations    You can also generate a `put` and `del` batch, so you can  manage the batching yourself:    ```javascript  var triple = { subject: ""a"", predicate: ""b"", object: ""c"" };    // Produces a batch of put operations  var putBatch = db.generateBatch(triple);    // Produces a batch of del operations  var delBatch = db.generateBatch(triple, 'del');  ```    ### Generate levelup query    Return the leveldb query for the given triple.    ```js  var query = db.createQuery({ predicate: ""b""});  leveldb.createReadStream(query);  ```    ## Navigator API    The Navigator API is a fluent API for LevelGraph, loosely inspired by  [Gremlin](http://markorodriguez.com/2011/06/15/graph-pattern-matching-with-gremlin-1-1/)  It allows to specify how to search our graph in a much more compact way and navigate  between vertexes.    Here is an example, using the same dataset as before:  ```javascript      db.nav(""matteo"").archIn(""friend"").archOut(""friend"").        solutions(function(err, results) {        // prints:        // [ { x0: 'daniele', x1: 'marco' },        //   { x0: 'daniele', x1: 'matteo' },        //   { x0: 'lucio', x1: 'marco' },        //   { x0: 'lucio', x1: 'matteo' } ]        console.log(results);      });  ```    The above example match the same triples of:  ```javascript      db.search([{        subject: db.v(""x0""),        predicate: 'friend',        object: 'matteo'      }, {        subject: db.v(""x0""),        predicate: 'friend',        object: db.v(""x1"")      }], function(err, results) {        // prints:        // [ { x0: 'daniele', x1: 'marco' },        //   { x0: 'daniele', x1: 'matteo' },        //   { x0: 'lucio', x1: 'marco' },        //   { x0: 'lucio', x1: 'matteo' } ]        console.log(results);      });  ```    It allows to see just the last reached vertex:  ```javascript      db.nav(""matteo"").archIn(""friend"").archOut(""friend"").        values(function(err, results) {        // prints [ 'marco', 'matteo' ]        console.log(results);      });  ```    Variable names can also be specified, like so:  ```javascript  db.nav(""marco"").archIn(""friend"").as(""a"").archOut(""friend"").archOut(""friend"").as(""a"").        solutions(function(err, friends) {      console.log(friends); // will print [{ a: ""daniele"" }]  });  ```    Variables can also be bound to a specific value, like so:  ```javascript  db.nav(""matteo"").archIn(""friend"").bind(""lucio"").archOut(""friend"").bind(""marco"").        values(function(err, friends) {    console.log(friends); // this will print ['marco']  });  ```    A materialized search can also be produced, like so:  ```javascript  db.nav(""matteo"").archOut(""friend"").bind(""lucio"").archOut(""friend"").bind(""marco"").        triples({:          materialized: {          subject: db.v(""a""),          predicate: ""friend-of-a-friend"",          object: db.v(""b"")        }      }, function(err, results) {      // this will return all the 'friend of a friend triples..'    // like so: {    //   subject: ""lucio"",    //   predicate: ""friend-of-a-friend"",    //   object: ""daniele""    // }      console.log(results);  });  ```    It is also possible to change the current vertex:  ```javascript  db.nav(""marco"").archIn(""friend"").as(""a"").go(""matteo"").archOut(""friend"").as(""b"").        solutions(function(err, solutions) {       //  solutions is: [{     //    a: ""daniele"",     //    b: ""daniele""     //   }, {     //     a: ""lucio"",     //     b: ""daniele""     //   }]    });  ```    ## LevelUp integration    LevelGraph allows to leverage the full power of all  [LevelUp](https://github.com/rvagg/node-levelup) plugins.    Initializing a database with LevelUp support is very easy:  ```javascript  var levelup = require(""level"");  var levelgraph = require(""levelgraph"");  var db = levelgraph(levelup(""yourdb""));  ```    ### Usage with SubLevel    An extremely powerful usage of LevelGraph is to partition your  LevelDB with [SubLevel](http://npm.im/level-sublevel):    ```javascript  var levelup = require(""level"");  var sublevel = require(""level-sublevel"");  var levelgraph = require(""levelgraph"");  var db = sublevel(levelup(""yourdb""));  var graph = levelgraph(db.sublevel('graph'));  ```    ## Browserify    You can use [browserify](https://github.com/substack/node-browserify) to bundle your module and all the dependencies, including levelgraph, into a single script-tag friendly js file for use in webpages. For the convenience of people unfamiliar with browserify, a pre-bundled version of levelgraph is included in the build folder.    Simply `require(""levelgraph"")` in your browser modules and use [level.js](https://github.com/maxogden/level.js) instead of `level`:    ```javascript  var levelgraph = require(""levelgraph"");  var leveljs = require(""level-js"");  var levelup = require(""levelup"");  var factory = function (location) { return new leveljs(location) };    var db = levelgraph(levelup(""yourdb"", { db: factory }));  ```    ### Testling    Follow the [Testling install instructions](https://github.com/substack/testling#install) and run `testling` in the levelgraph directory to run the test suite against a headless browser using level.js    ## RDF support    __LevelGraph__ does not support out of the box loading serialized RDF or storing it. Such functionality is provided by extensions:  * [LevelGraph-N3](https://github.com/levelgraph/levelgraph-n3) - __N3/Turtle__  * [LevelGraph-JSONLD](https://github.com/levelgraph/levelgraph-jsonld) - __JSON-LD__    ## Extensions    You can use multiple extensions at the same time. Just check if one depends on another one  to nest them in correct order! *(LevelGraph-N3 and LevelGraph-JSONLD are  independent)*    ```javascript  var lg = require('levelgraph');  var lgN3 = require('levelgraph-n3');  var lgJSONLD = require('levelgraph-jsonld');    var db = lgJSONLD(lgN3(lg(""yourdb"")));  // gives same result as  var db = lgN3(lgJSONLD(lg(""yourdb"")));  ```    ## TODO    There are plenty of things that this library is missing.  If you feel you want a feature added, just do it and __submit a  pull-request__.    Here are some ideas:    * [x] Return the matching triples in the search results.  * [x] Support for Query Planning in search.  * [x] Added a Sort-Join algorithm.  * [ ] Add more database operators (grouping, filtering).  * [x] Browser support    [#10](https://github.com/levelgraph/levelgraph/issues/10)  * [ ] Live searches    [#3](https://github.com/levelgraph/node-levelgraph/issues/3)  * Extensions    * [ ] RDFa    * [ ] RDF/XML    * [ ] Microdata    ## Contributing    LevelGraph is an **OPEN Open Source Project**. This means that:    > Individuals making significant and valuable contributions are given commit-access to the project to contribute as they see fit. This project is more like an open wiki than a standard guarded open source project.    See the [CONTRIBUTING.md](https://github.com/levelgraph/levelgraph/blob/master/CONTRIBUTING.md) file for more details.    ## Credits    *LevelGraph builds on the excellent work on both the LevelUp community  and the LevelDB and Snappy teams from Google and additional contributors.  LevelDB and Snappy are both issued under the [New BSD Licence](http://opensource.org/licenses/BSD-3-Clause).*    ## Contributors    LevelGraph is only possible due to the excellent work of the following contributors:    <table><tbody>  <tr><th align=""left"">Matteo Collina</th><td><a href=""https://github.com/mcollina"">GitHub/mcollina</a></td><td><a href=""https://twitter.com/matteocollina"">Twitter/@matteocollina</a></td></tr>  <tr><th align=""left"">Jeremy Taylor</th><td><a  href=""https://github.com/jez0990"">GitHub/jez0990</a></td></tr>  <tr><th align=""left"">Elf Pavlik</th><td><a href=""https://github.com/elf-pavlik"">GitHub/elf-pavlik</a></td><td><a href=""https://twitter.com/elfpavlik"">Twitter/@elfpavlik</a></td></tr>  <tr><th align=""left"">Riceball LEE</th><td><a href=""https://github.com/snowyu"">GitHub/snowyu</a></td><td></td></tr>  <tr><th align=""left"">Brian Woodward</th><td><a href=""https://github.com/doowb"">GitHub/doowb</a></td><td><a href=""https://twitter.com/doowb"">Twitter/@doowb</a></td></tr>  <tr><th align=""left"">Leon Chen</th><td><a href=""https://github.com/transcranial"">GitHub/transcranial</a></td><td><a href=""https://twitter.com/transcranial"">Twitter/@transcranial</a></td></tr>  </tbody></table>    ## LICENSE - ""MIT License""    Copyright (c) 2013-2017 Matteo Collina and LevelGraph Contributors    Permission is hereby granted, free of charge, to any person  obtaining a copy of this software and associated documentation  files (the ""Software""), to deal in the Software without  restriction, including without limitation the rights to use,  copy, modify, merge, publish, distribute, sublicense, and/or sell  copies of the Software, and to permit persons to whom the  Software is furnished to do so, subject to the following  conditions:    The above copyright notice and this permission notice shall be  included in all copies or substantial portions of the Software.    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES  OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT  HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,  WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR  OTHER DEALINGS IN THE SOFTWARE. """
Semantic web;https://github.com/Data2Semantics/fox-ui;"""# Fox-UI  Web UI for [FoxPSL](https://github.com/uzh/fox/uzh) """
Semantic web;https://github.com/AKSW/IGUANA;"""[![GitLicense](https://gitlicense.com/badge/dice-group/IGUANA)](https://gitlicense.com/license/dice-group/IGUANA)  ![Java CI with Maven](https://github.com/dice-group/IGUANA/workflows/Java%20CI%20with%20Maven/badge.svg)[![BCH compliance](https://bettercodehub.com/edge/badge/AKSW/IGUANA?branch=master)](https://bettercodehub.com/)  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/9668460dd04c411fab8bf5ee9c161124)](https://www.codacy.com/app/TortugaAttack/IGUANA?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=AKSW/IGUANA&amp;utm_campaign=Badge_Grade)  [![Project Stats](https://www.openhub.net/p/iguana-benchmark/widgets/project_thin_badge.gif)](https://www.openhub.net/p/iguana-benchmark)      # IGUANA    <img src = ""https://github.com/dice-group/IGUANA/raw/develop/images/IGUANA_logo.png"" alt = ""IGUANA Logo"" width = ""400"" align = ""center"">    ## ABOUT      Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data.  Hence it is very important that the triple store must scale on the data and can handle several users.   Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily.  Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well.  Further on it was impossible to compare results for different benchmarks.     Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications.</br>  which solves all these issues.   It provides an enviroment which ...      + ... is highly configurable  + ... provides a realistic scneario benchmark  + ... works on every dataset  + ... works on SPARQL HTTP endpoints  + ... works on HTTP Get & Post endpoints  + ... works on CLI applications  + and is easily extendable      For further Information visit    [iguana-benchmark.eu](http://iguana-benchmark.eu)     [Documentation](http://iguana-benchmark.eu/docs/3.3/)      # Getting Started    # Prerequisites     You need to install Java 11 or greater.  In Ubuntu you can install these using the following commands    ```  sudo apt-get install java  ```    # Iguana Modules    Iguana consists of two modules    1. **corecontroller**: This will benchmark the systems   2. **resultprocessor**: This will calculate the Metrics and save the raw benchmark results     ## **corecontroller**    The **corecontroller** will benchmark your system. It should be started on the same machine the  is started.    ## **resultprocessor**    The **resultprocessor** will calculate the metrics.  By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store.   On the processing side, it calculates various metrics.    Per run metrics:  * Query Mixes Per Hour (QMPH)  * Number of Queries Per Hour (NoQPH)  * Number of Queries (NoQ)  * Average Queries Per Second (AvgQPS)    Per query metrics:  * Queries Per Second (QPS)      * Number of successful and failed queries      * result size      * queries per second      * sum of execution times    You can change these in the Iguana Benchmark suite config.    If you use the [basic configuration](https://github.com/dice-group/IGUANA/blob/master/example-suite.yml), it will save all mentioned metrics to a file called `results_{{DATE_RP_STARTED}}.nt`      # Setup Iguana    ## Download  Please download the release zip **iguana-x.y.z.zip** from the newest release available [here](https://github.com/dice-group/IGUANA/releases/latest):    ```  mkdir iguana  wget https://github.com/dice-group/IGUANA/releases/download/v3.3.0/iguana-3.3.0.zip  unzip iguana-3.3.0.zip  ```      It contains the following files:    * iguana.corecontroller-X.Y.Z.jar  * start-iguana.sh  * example-suite.yml    # Run Your Benchmarks    ## Create a Configuration    You can use the [basic configuration](https://github.com/dice-group/IGUANA/blob/master/example-suite.yml) we provide and modify it to your needs.  For further information please visit our [configuration](http://iguana-benchmark.eu/docs/3.2/usage/configuration/) and [Stresstest](http://iguana-benchmark.eu/docs/3.0/usage/stresstest/) wiki pages. For a detailed, step-by-step instruction please attend our [tutorial](http://iguana-benchmark.eu/docs/3.2/usage/tutorial/).        ## Execute the Benchmark    Use the start script   ```  ./start-iguana.sh example-suite.yml  ```  Now Iguana will execute the example benchmark suite configured in the example-suite.yml file      # How to Cite    ```bibtex  @InProceedings{10.1007/978-3-319-68204-4_5,  author=""Conrads, Felix  and Lehmann, Jens  and Saleem, Muhammad  and Morsey, Mohamed  and Ngonga Ngomo, Axel-Cyrille"",  editor=""d'Amato, Claudia  and Fernandez, Miriam  and Tamma, Valentina  and Lecue, Freddy  and Cudr{\'e}-Mauroux, Philippe  and Sequeda, Juan  and Lange, Christoph  and Heflin, Jeff"",  title=""Iguana: A Generic Framework for Benchmarking the Read-Write Performance of Triple Stores"",  booktitle=""The Semantic Web -- ISWC 2017"",  year=""2017"",  publisher=""Springer International Publishing"",  address=""Cham"",  pages=""48--65"",  abstract=""The performance of triples stores is crucial for applications driven by RDF. Several benchmarks have been proposed that assess the performance of triple stores. However, no integrated benchmark-independent execution framework for these benchmarks has yet been provided. We propose a novel SPARQL benchmark execution framework called Iguana. Our framework complements benchmarks by providing an execution environment which can measure the performance of triple stores during data loading, data updates as well as under different loads and parallel requests. Moreover, it allows a uniform comparison of results on different benchmarks. We execute the FEASIBLE and DBPSB benchmarks using the Iguana framework and measure the performance of popular triple stores under updates and parallel user requests. We compare our results (See https://doi.org/10.6084/m9.figshare.c.3767501.v1) with state-of-the-art benchmarking results and show that our benchmark execution framework can unveil new insights pertaining to the performance of triple stores."",  isbn=""978-3-319-68204-4""  }  ``` """
Semantic web;https://github.com/clarkparsia/csv2rdf;"""csv2rdf  =======    csv2rdf is a simple tool for generating RDF output from CSV/TSV files. The conversion is done by a template file  that shows how the RDF output will look for one row. See [examples/cars](examples/cars) for details.     Building  --------    `ant clean dist` will create a local build in the `dist` sub-directory.    Running  -------    You can run the tool with the command `java -jar dist/lib/csv2rdf.jar` followed by arguments.    You can see the help screen with the command `java -jar dist/lib/csv2rdf.jar help convert`.    You can run the conversion for the example using `java -jar dist/lib/csv2rdf.jar examples/cars/template.ttl examples/cars/cars.csv cars.ttl`. """
Semantic web;https://github.com/ontola/rdfdev-js;"""# js.rdf.dev  [![GitHub stars](https://img.shields.io/github/stars/ontola/rdfdev-js?style=social)](https://github.com/ontola/rdfdev-js)  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/)  [![Maintainability](https://api.codeclimate.com/v1/badges/292914da43d93b43addd/maintainability)](https://codeclimate.com/github/ontola/rdfdev-js/maintainability)    Collection of libraries to ease in JavaScript RDF development.  Open source (MIT licensed).    ## Packages  ### IRI  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/iri)  [![npm (tag)](https://img.shields.io/npm/v/@rdfdev/iri)](https://npmjs.com/package/@rdfdev/iri)  [![npm bundle size](https://img.shields.io/bundlephobia/minzip/@rdfdev/iri)](https://bundlephobia.com/result?p=@rdfdev/iri)    A lot of IRI/URI manipulation can happen while working with linked data, this package provides  utility functions to do just that.    ### Actions  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/actions)  [![npm (tag)](https://img.shields.io/npm/v/@rdfdev/actions)](https://npmjs.com/package/@rdfdev/actions)  [![npm bundle size](https://img.shields.io/bundlephobia/minzip/@rdfdev/actions)](https://bundlephobia.com/result?p=@rdfdev/actions)    Utilities for working with [link actions](https://github.com/rescribet/link-lib/wiki/Hypermedia-API)  and link middleware.    ### Collections  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/collections)  [![npm (tag)](https://img.shields.io/npm/v/@rdfdev/collections)](https://npmjs.com/package/@rdfdev/collections)  [![npm bundle size](https://img.shields.io/bundlephobia/minzip/@rdfdev/collections)](https://bundlephobia.com/result?p=@rdfdev/collections)    Utilities for reading and manipulating different kinds of RDF collections (rdf:Seq, rdf:List)    ### Delta  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/delta)  [![npm (tag)](https://img.shields.io/npm/v/@rdfdev/delta)](https://npmjs.com/package/@rdfdev/delta)  [![npm bundle size](https://img.shields.io/bundlephobia/minzip/@rdfdev/delta)](https://bundlephobia.com/result?p=@rdfdev/delta)    Utilities to quickly create [linked deltas](https://github.com/ontola/linked-delta), an rdf-native  way to express and process changes in state.    ### Prop types  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/prop-types)  [![npm (tag)](https://img.shields.io/npm/v/@rdfdev/prop-types)](https://npmjs.com/package/@rdfdev/prop-types)  [![npm bundle size](https://img.shields.io/bundlephobia/minzip/@rdfdev/prop-types)](https://bundlephobia.com/result?p=@rdfdev/prop-types)    React [prop-type](https://reactjs.org/docs/typechecking-with-proptypes.html) declarations for the  RDF data structures.    ## See also  The following libraries are used by these packages.  ### @ontologies/core  [![GitHub stars](https://img.shields.io/github/stars/ontola/ontologies?style=social)](https://github.com/ontola/ontologies)  [![npm (tag)](https://img.shields.io/npm/v/@ontologies/core/next?label=npm)](https://npmjs.com/package/@ontologies/core)  ![npm bundle size](https://img.shields.io/bundlephobia/minzip/@ontologies/core@next)    Makes working with RDF a breeze:  * Types for RDF including quads, literals, resources.  * A data factory for creating RDF data.  * Access a run-time set datafactory in static context!  * Typeguards to check if something is an RDF object.    ### Link lib    [![GitHub stars](https://img.shields.io/github/stars/rescribet/link-lib?style=social)](https://github.com/rescribet/link-lib)  [![npm (tag)](https://img.shields.io/npm/v/link-lib/light?label=npm)](https://npmjs.com/package/link-lib)  ![npm bundle size](https://img.shields.io/bundlephobia/minzip/link-lib@light)  ![CircleCI](https://img.shields.io/circleci/build/gh/rescribet/link-lib/use-data-factory-and-ontologies)  [![Maintainability](https://api.codeclimate.com/v1/badges/e8824bb0fb4bcf689749/maintainability)](https://codeclimate.com/github/rescribet/link-lib/maintainability)    Fetch, store, write and render linked data.    ### Link redux  [![GitHub stars](https://img.shields.io/github/stars/rescribet/link-redux?style=social)](https://github.com/rescribet/link-redux)  [![npm (tag)](https://img.shields.io/npm/v/link-redux/light?label=npm)](https://npmjs.com/package/link-redux)  ![npm bundle size](https://img.shields.io/bundlephobia/minzip/link-redux@light)  ![CircleCI](https://img.shields.io/circleci/build/gh/rescribet/link-redux/datafactory)  [![Maintainability](https://api.codeclimate.com/v1/badges/6801255f84f20aa73420/maintainability)](https://codeclimate.com/github/rescribet/link-redux/maintainability)    All the tools needed to quickly create interactive web apps consuming RDF.    ## Need help with linked data?    All these package are brought to you by [Ontola](https://ontola.io), we build production-grade  linked data solutions and can help you from advice to building custom web services. """
Semantic web;https://github.com/apache/marmotta;"""# Apache Marmotta    This repository contains the source code for [Apache Marmotta](https://marmotta.apache.org/)    [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.marmotta/marmotta-core/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.apache.marmotta/marmotta-core/)  [![Docker Image](https://images.microbadger.com/badges/version/apache/marmotta.svg)](https://hub.docker.com/r/apache/marmotta/)  [![Docker Image layers](https://images.microbadger.com/badges/image/apache/marmotta.svg)](https://microbadger.com/images/apache/marmotta)  [![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)      ## Building the Source Distribution    Apache Marmotta uses Maven to build, test, and install the software. A basic  build requires downloading and installing Maven and then running:        mvn clean install    This will compile, package and test all Apache Marmotta modules and install  it in your local Maven repository. In case you want to build your own  projects based on some of the libraries provided by Apache Marmotta, this  usually suffices.    The default loglevel for most unit and integration tests executed during the  build is INFO. To change the loglevel for either more or less output, you  can pass the loglevel as system property:        mvn clean install -Droot-level=TRACE|DEBUG|INFO|WARN|ERROR    Note that some of the integration tests start up parts of the Marmotta  platform during execution. The log level for these tests cannot be  changed, as Marmotta is taking over the log configuration in these  cases.    ## Building, Running and Deploying the Wep Application    Apache Marmotta also includes a default configuration for building a Java  Web Application that can be deployed in any Java Application Server. To  build the web application, first run        mvn clean install    in the project root. Then change to the launchers/marmotta-webapp directory  and run        mvn package    This will create a marmotta.war file in the target/ directory. You can  deploy this archive to any Java Application Server by copying it into  its deployment directory; [more details](http://marmotta.apache.org/installation.html).    Alternatively, you can directly startup the Apache Marmotta Web Application  from Maven with a default configuration suitable for development. To try this  out, run        mvn tomcat7:run    wait until the system is started up and point your browser to `http://localhost:8080`    When developing it is sometimes useful to always start with a clean confi-  guration of the system. Therefore, you can also start up the web application  as follows:        mvn clean tomcat7:run -Pcleanall    This command will remove any existing configuration directory before startup.    ## Building the Standalone Installer    The build environment also offers to automatically build an installer package  that guides users through the installation with an easy-to-use installation  wizard. The installer is based on izPack and dynamically assembled when  building the package. To build the installer, first run        mvn clean install    in the project root. Then change to the launchers/marmotta-installer directory  and run        mvn package -Pinstaller    The build process will automatically create an appropriate installer confi-  guration from the Maven dependencies through the Apache Marmotta refpack  build plugin.    The installer can then be tried out by running        java -jar target/marmotta-installer-x.x.x.jar      ## Building a Docker image    Marmotta also comes with support for creating a Docker images that you can use for development   or testing:    1. Locate at the root of the source repository  2. Build image: `docker build -t marmotta .`  3. Run the container: `docker run -p 8080:8080 marmotta`  4. Access Marmotta at [localhost:8080/marmotta](http://localhost:8080/marmotta) (IP address may      be different, see information bellow).    An official images is [available from Docker Hub](https://hub.docker.com/r/apache/marmotta/) as   an automated build, so you just need to pull it from there to replace the second step above:         docker pull apache/marmotta      ## Building with a Clean Repository    Sometimes it is useful to check if the build runs properly on a clean local  repository, i.e. simulate what happens if a user downloads the source and  runs the build. This can be achieved by running Maven as follows:        mvn clean install -Dmaven.repo.local=/tmp/testrepo    The command changes the local repository location from ~/.m2 to the  directory passed as argument      ## Simulating a Release    To test the release build without actually deploying the software, we have  created a profile that will deploy to the local file system. You can  simulate the release by running        mvn clean deploy -Pdist-local,marmotta-release,installer    Please keep in mind that building a release involves creating digital  signatures, so you will need a GPG key and a proper GPG configuration to run  this task.    Read more about [our release process](https://wiki.apache.org/marmotta/ReleaseProcess).   """
Semantic web;https://github.com/sspider/etalis;"""# etalis  Exported from code.google.com/p/etalis    ETALIS is an open source system for Complex Event Processing with two accompanied languages called: ETALIS Language for Events (ELE) and Event Processing SPARQL (EP-SPARQL). ETALIS is based on a declarative semantics, grounded in Logic Programming. Complex events are derived from simpler events by means of deductive rules. Due to its root in logic, ETALIS also supports reasoning about events, context, and real-time complex situations (i.e., Knowledge-based Event Processing). ETALIS stands for Event TrAnsaction Logic Inference System.    ETALIS is implemented in Prolog. The engine runs on many Prolog systems: YAP, SWI, SICStus, XSB, tuProlog and LPA Prolog. Download ETALIS from here. We installed ETALIS and EP-SPARQL on several operating systems including: Windows XP, Vista and 7, Mac OS, Android OS (with tuProlog) and Linux-based systems (Ubuntu, RedHat, SUSE).    Features:    * declarative rule-based language for event processing;  * detection of complex events and reasoning over states (with logic rules);  * classic event operators (e.g., sequence, concurrent conjunction, disjunction, negation etc.). The language supports all operators from Allen's interval algebra (e.g., during, meets, starts, finishes etc.);  * count-based sliding windows;  * event aggregation for count, avg, sum, min, max etc.;  * event filtering, enrichment, projection, translation, and multiplication are supported;  * alarm events to trigger events after certain durations of time or at absolute datimes (working only under SWI Prolog);  * processing of out-of-order events (i.e. events that are delayed due to different circumstances e.g. network anomalies etc.);  * event retraction (revision);  * shared computation plan for evaluation of complex event rules;  * support for Event Processing SPARQL (EP-SPARQL) language.    See the ETALIS Manual for more information about ETALIS.    To evaluate ETALIS we have implemented the Fast Flower Delivery use case created by Opher Etzion and Peter Niblett in their book: Event Processing in Action. Description of the use case can be found in the aforementioned book or from the Event Processing Technical Society website. The use case implementation in ETALIS can be found in examples/flower_delivery or on a corresponding Fast_Flower_Delivery_Use_Case wiki page.    Contact:  Darko Anicic <darko.anicic@fzi.de>: for questions related to ETALIS design, architecture and underlying algorithms;  Paul Fodor <pfodor@cs.stonybrook.edu>: for questions related to implementation of the ETALIS engine or any problems with the execution of the ETALIS programs. """
Semantic web;https://github.com/ncbo/umls2rdf;"""This project takes a MySQL Unified Medical Language System (UMLS) database and converts the ontologies to RDF using OWL and SKOS as the main schemas.    Virtual Appliance users can review the [documentation in the OntoPortal Administration Guide}(https://ontoportal.github.io/administration/ontologies/handling_umls/).    To use it:    * Specify your database connection conf.py  * Specify the SAB ontologies to export in umls.conf    The umls.conf configuration file must contain one ontology per line. The lines are comma separated tuples where the elements are:    <em>The following list needs updating.</em>  <pre>  (0) SAB  (1) BioPortal Virtual ID. This is optional, any value works.  (2) Output file name  (3) Conversion strategy. Accepted values (load_on_codes, load_on_cuis).  </pre>    Note that 'CCS COSTAR DSM3R DSM4 DXP ICPC2ICD10ENG MCM MMSL MMX MTHCMSFRF MTHMST MTHSPL MTH NDFRT SNM' have no code and should not be loaded on loads_on_codes.    umls2rdf.py is designed to be an offline, run-once process.   It's memory intensive and exports all of the default ontologies in umls.conf in 3h 30min.   The ontologies listed in umls.conf are the UMLS ontologies accessible in [BioPortal](https://bioportal.bioontology.org/).    If you get an error when installing the MySQL-python python library, https://stackoverflow.com/questions/12218229/my-config-h-file-not-found-when-intall-mysql-python-on-osx-10-8 may be of help.    If running a Windows 10 OS with MySQL, the following tips may be of help.    - Install [MySQL 5.5](https://dev.mysql.com/downloads/mysql/5.5.html#downloads) to avoid the InnoDB space [disclaimer](https://www.nlm.nih.gov/research/umls/implementation_resources/scripts/README_RRF_MySQL_Output_Stream.html) by NLM.   - [Python 2.7.x](https://www.python.org/downloads/) should be used to avoid syntax errors on 'raise Attribute'  - For installtion of the MySQLdb module <pre>python -m pip install MySQLdb</pre> is error prone. Install with executable [MySQL-python-1.2.3.win-amd64-py2.7](http://www.codegood.com/archives/129) (last known location).  - Create your RRF subset(s) using mmsys with the MySQL load option, load your database, edit conf.py and umls.py to specifications, run umsl2rdf.py """
Semantic web;https://github.com/stardog-union/stardog.js;"""# Stardog.js    Universal Javascript fetch wrapper for communicating with the Stardog HTTP server.    [![npm](https://img.shields.io/npm/v/stardog.svg?style=flat-square)](https://www.npmjs.com/package/stardog)    <a href=""http://stardog.com""><img src=""https://d33wubrfki0l68.cloudfront.net/66e9dcff51317cfc11b9f3d4ce2917a11ba81681/543c1/img/stardog-logo-optimized.svg"" style=""margin: 0 auto; display: block; width: 250px""/></a>    ## What is it?    This framework wraps all the functionality of a client for the Stardog DBMS, and provides access to a full set of functions such as executing SPARQL queries, administrative tasks on Stardog, and the use of the Reasoning API.    All the implementation uses the HTTP protocol, since most of Stardog functionality is available using this protocol. For more information, go to the Stardog's [HTTP Programming](http://www.stardog.com/docs/#_network_programming) documentation.    This is a universal library and as such can be used in both the browser and Node.js.    ## Installation    To install stardog.js run:    ```bash  npm install stardog  ```    ## Usage    Stardog.js conforms to the [Universal Module Definition](https://github.com/umdjs/umd) API. To use it in Node.js, simply `require` or `import` it as you would any other Node module. To use it in the browser, you can either:    1. Do the same as you would with Node.js, in which case you'll have to use [webpack](https://webpack.js.org/), [parcel](https://parceljs.org/), [browserify](http://browserify.org/), or some other module bundler,  2. Use [require.js](https://requirejs.org/) or some other module loader, or  3. Directly import the built stardog.js file in your HTML (e.g., `<script src=""./node_modules/stardog/dist/stardog.js""></script>`) and then reference the global `stardogjs` object (e.g., `stardogjs.query.execute(/* . . . */)`).    ## Development    To get started, just clone the project. You'll need a local copy of Stardog to be able to run the tests. For more information on starting the Stardog DB service and how it works, go to [Stardog's documentation](http://docs.stardog.com), where you'll find everything you need to get up and running with Stardog.    Go to [http://stardog.com](http://stardog.com), download and install the database and load the data provided in `data/` using the script in the repository.    1. Start the Stardog server  ```bash  stardog-admin server start  ```  2. Install `stardog.js` dependencies:  ```bash  npm install  ```    ### Running Tests    In order to contribute changes, all test cases must pass. With the Stardog server running, execute the following command to run all test cases in `test/spec`:    ```bash  npm test  ```    To test the cluster commands you will need to first start a Stardog cluster then run the cluster suite. The easiest way to do this is to run docker-compose to start a cluster:    ```bash  docker-compose -f .circleci/docker-compose.yml up  ```    Then run the cluster test suite in `test/cluster`:    ```bash  npm run test:cluster  ```    ### Contributing    Fork, clone and develop, write or amend tests, and then open a PR. All PRs go against ""master"". This project uses [prettier](https://github.com/prettier/prettier) on file commit, so don't worry about style as it'll just get rewritten when you commit your changes.    ### Releasing    If you have publishing rights, BE SURE TO RUN `npm version (major|minor|patch)` IMMEDIATELY BEFORE PUBLISHING. This will ensure that the build is up-to-date and will also (1) bump the version number in package.json accordingly, (2) create a git tag matching the version number, and (3) automatically update the README and the CHANGELOG using our type declarations and data from the stardog.js GitHub repo. For this process to work correctly, you will need to have generated a GitHub OAuth token and assigned it to the `MDCHANGELOG_TOKEN` environment variable (the name of the token is a relic of the fact that this repo once used [mdchangelog](https://www.npmjs.com/package/mdchangelog) to generate changelogs; it now uses a custom script). In order to ensure that this process is followed, there will be a very annoying alert triggered whenever you publish; if you're all set, just ignore the alert.    After releasing, be sure to push to master, including the tags (so that the release is reflected on GitHub).    ## Version/Support Details    Each release of stardog.js is tested against the most recent version of Stardog available at the time of the release. The relationship between versions of stardog.js and versions of Stardog is detailed in the following table:    | stardog.js Version  | Supported Stardog Version(s) |  | ------------------  | ---------------------------- |  | 3.x.x               | 7.x.x                        |  | 2.x.x               | 6.x.x                        |  | 1.x.x*              | 5.x.x                        |  | 0.x.x*              | any version < 5              |    _* = No longer supported_    We support and maintain a particular version of stardog.js only if the corresponding Stardog version(s) is (are) officially supported and maintained. For example, we no longer support v0.x.x of stardog.js, as the corresponding Stardog versions are no longer supported. (That said, later versions of stardog.js will often _mostly_ work with earlier Stardog versions. We just don't test this or make any guarantees to that effect.)    ## Quick Example  ```js  const { Connection, query } = require('stardog');    const conn = new Connection({    username: 'admin',    password: 'admin',    endpoint: 'http://localhost:5820',  });    query.execute(conn, 'myDatabaseName', 'select distinct ?s where { ?s ?p ?o }', 'application/sparql-results+json', {    limit: 10,    reasoning: true,    offset: 0,  }).then(({ body }) => {    console.log(body.results.bindings);  });  ```    <!--- API Goes Here --->  # API    ## <a name=""http"">HTTP</a>    #### <a name=""rdfmimetype"">RdfMimeType</a>    One of the following values:    `'application/ld+json'              | 'text/turtle'              | 'application/rdf+xml'              | 'application/n-triples'              | 'application/n-quads'              | 'application/trig'`  #### <a name=""sparqlmimetype"">SparqlMimeType</a>    One of the following values:    `'application/sparql-results+json'              | 'application/sparql-results+xml'`  #### <a name=""acceptmimetype"">AcceptMimeType</a>    One of the following values:    `RdfMimeType              | SparqlMimeType              | 'text/plain'              | 'text/boolean'              | 'application/json'              | '*/*'`  #### <a name=""explainacceptmimetype"">ExplainAcceptMimeType</a>    One of the following values:    `'text/plain'              | 'application/json'`  #### <a name=""body"">Body</a>    Object with the following values:    - status (`number`)  - statusText (`string`)  - result (`object | string | boolean | null`)  - ok (`boolean`)  - headers (`Headers`)  - body (`any`)    #### <a name=""connectionoptions"">ConnectionOptions</a>    Object with the following values:    - endpoint (`string`)  - username (`string`)  - password (`string`)  - token (`string`)  - meta (`ConnectionMeta`)    #### <a name=""requestconstructor"">RequestConstructor</a>    One of the following values:    `{        new (input: string | Request, init?: RequestInit): Request;      }`  #### <a name=""requestcreator"">RequestCreator</a>    One of the following values:    `({ uri, Request }: { uri: string; Request: Constructor }) => ReturnType`  #### <a name=""connectionmeta"">ConnectionMeta</a>    Object with the following values:    - createRequest (`RequestCreator<RequestConstructor, string | Request>`)  - createHeaders (`(defaults: { headers: Headers; }) => Headers`)    ## <a name=""connection"">Connection</a> (Class)    Constructed with:  - options ([`ConnectionOptions`](#connectionoptions))  ### <a name=""config"">Connection.config(options, meta)</a>    Takes the following params:  - options ([`ConnectionOptions`](#connectionoptions))  - meta ([`ConnectionMeta`](#connectionmeta))    Returns [`void`](#void)  ### <a name=""headers"">Connection.headers()</a>    Returns [`Headers`](#headers)  ### <a name=""uri"">Connection.uri(resource)</a>    Takes the following params:  - resource (`string[]`)    Returns `string`  ## <a name=""server"">server</a>    #### <a name=""shutdown"">`server.shutdown(conn, params)`</a>    Shuts down a Stardog server.     Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""status"">`server.status(conn, params)`</a>    Retrieves general status information about a Stardog server. By  default, also includes status information about all databases on  that server. If `params.databases` is `false`, however, then the  information about databases is omitted.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`{ databases?: boolean; }`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""db"">db</a>    #### <a name=""create"">`db.create(conn, database, databaseOptions, options, params)`</a>    Creates a new database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - databaseOptions (`object`)    - options (`{ files: { filename: string}[] }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""drop"">`db.drop(conn, database, params)`</a>    Deletes a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`db.get(conn, database, params)`</a>    Gets an RDF representation of a database. See: https://www.w3.org/TR/sparql11-http-rdf-update/#http-get    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""offline"">`db.offline(conn, database, params)`</a>    Sets a database offline.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""online"">`db.online(conn, database, params)`</a>    Sets a database online.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""optimize"">`db.optimize(conn, database, params)`</a>    Optimizes a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""list"">`db.list(conn, params)`</a>    Gets a list of all databases on a Stardog server.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""size"">`db.size(conn, database, params)`</a>    Gets number of triples in a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""model"">`db.model(conn, database, options, params)`</a>    Gets the model for a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - options (`object`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""clear"">`db.clear(conn, database, transactionId, params)`</a>    Clears the contents of a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`db.add(conn, database, transactionId, content, options, params)`</a>    Adds data within a transaction.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - content (`string`)    - options ([`TransactionOptions`](#transactionoptions))    - params (`object`)    Returns [`Promise<transaction.TransactionResponse>`](#transactionresponse)    #### <a name=""remove"">`db.remove(conn, database, transactionId, content, options, params)`</a>    Removes data within a transaction.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - content (`string`)    - options ([`TransactionOptions`](#transactionoptions))    - params (`object`)    Returns [`Promise<transaction.TransactionResponse>`](#transactionresponse)    #### <a name=""exportdata"">`db.exportData(conn, database, options, params)`</a>    Exports the contents of a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - options ({ mimetype: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""options"">options</a>    #### <a name=""getavailable"">`db.options.getAvailable(conn)`</a>    Gets all available database options with their default values.     Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`db.options.get(conn, database, params)`</a>    Gets set of options on a database.     Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""getall"">`db.options.getAll(conn, database)`</a>    Gets all options on a database.     Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""set"">`db.options.set(conn, database, databaseOptions, params)`</a>    Sets options on a database.     Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - databaseOptions (`object`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""graph"">graph</a>    #### <a name=""doget"">`db.graph.doGet(conn, database, graphUri, accept, params)`</a>    Retrieves the specified named graph    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - graphUri (`string`)    - accept ([`RdfMimeType`](#rdfmimetype))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""doput"">`db.graph.doPut(conn, database, graphData, graphUri, contentType, params)`</a>    Stores the given RDF data in the specified named graph    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - graphData (`string`)    - graphUri (`string`)    - contentType ([`RdfMimeType`](#rdfmimetype))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""dodelete"">`db.graph.doDelete(conn, database, graphUri, params)`</a>    Deletes the specified named graph    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - graphUri (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""dopost"">`db.graph.doPost(conn, database, graphUri, options, params)`</a>    Merges the given RDF data into the specified named graph    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - graphUri (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""transaction"">transaction</a>    #### <a name=""encodings"">Encodings</a>    One of the following values:    `'gzip' |                  'compress' |                  'deflate' |                  'identity' |                  'br'`  #### <a name=""transactionresponse"">TransactionResponse</a> extends [HTTP.Body](#body)    Object with the following values:    - transactionId (`string`)    #### <a name=""transactionoptions"">TransactionOptions</a>    Object with the following values:    - contentType (`HTTP.RdfMimeType`)  - encoding (`Encodings`)    #### <a name=""begin"">`db.transaction.begin(conn, database, params)`</a>    Begins a new transaction.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<TransactionResponse>`](#transactionresponse)    #### <a name=""rollback"">`db.transaction.rollback(conn, database, transactionId, params)`</a>    Rolls back a transaction, removing the transaction and undoing all changes    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - params (`object`)    Returns [`Promise<TransactionResponse>`](#transactionresponse)    #### <a name=""commit"">`db.transaction.commit(conn, database, transactionId, params)`</a>    Commits a transaction to the database, removing the transaction and making its changes permanent.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - params (`object`)    Returns [`Promise<TransactionResponse>`](#transactionresponse)    ## <a name=""icv"">icv</a>    #### <a name=""get"">`db.icv.get(conn, database, params)`</a>    Gets the set of integrity constraints on a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`db.icv.add(conn, database, icvAxioms, options, params)`</a>    Adds integrity constraints to a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - icvAxioms (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`db.icv.remove(conn, database, icvAxioms, options, params)`</a>    Removes integrity constraints from a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - icvAxioms (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""clear"">`db.icv.clear(conn, database, params)`</a>    Removes all integrity constraints from a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""convert"">`db.icv.convert(conn, database, icvAxioms, options, params)`</a>    Converts a set of integrity constraints into an equivalent SPARQL query for a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - icvAxioms (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""validate"">`db.icv.validate(conn, database, constraints, options, params)`</a>    Checks constraints to see if they are valid    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - constraints (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""validateintx"">`db.icv.validateInTx(conn, database, transactionId, constraints, options, params)`</a>    Checks constraints to see if they are valid within a transaction    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - constraints (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""violations"">`db.icv.violations(conn, database, constraints, options, params)`</a>    Accepts integrity constraints as RDF and returns the violation explanations, if any, as RDF.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - constraints (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""violationsintx"">`db.icv.violationsInTx(conn, database, transactionId, constraints, options, params)`</a>    Accepts integrity constraints as RDF and returns the violation explanations, if any, as RDF.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - constraints (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""report"">`db.icv.report(conn, database, constraints, options, params)`</a>    Accepts integrity constraints as RDF and returns a validation report.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - constraints (`string`)    - options ({ contentType?: HTTP.RdfMimeType, accept?: [`AcceptMimeType`](#acceptmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""reportintx"">`db.icv.reportInTx(conn, database, transactionId, constraints, options, params)`</a>    Accepts integrity constraints as RDF and returns a validation report.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - constraints (`string`)    - options ({ contentType?: HTTP.RdfMimeType, accept?: [`AcceptMimeType`](#acceptmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""reasoning"">reasoning</a>    #### <a name=""consistency"">`db.reasoning.consistency(conn, database, options, params)`</a>    Returns if the database is consistent    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - options (`{ namedGraph: string }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""explaininference"">`db.reasoning.explainInference(conn, database, inference, config, params)`</a>    Provides an explanation for an inference    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - inference (`string`)    - config (`{ contentType: string }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""explaininconsistency"">`db.reasoning.explainInconsistency(conn, database, options, params)`</a>    Provides the reason why a database is inconsistent, as reported by db.reasoning.consistency    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - options (`{ namedGraph: string }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""explaininferenceintransaction"">`db.reasoning.explainInferenceInTransaction(conn, database, transactionId, inference, config, params)`</a>    Provides an explanation for an inference within a transaction    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - inference (`string`)    - config ([`TransactionOptions`](#transactionoptions))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""explaininconsistencyintransaction"">`db.reasoning.explainInconsistencyInTransaction(conn, database, transactionId, options, params)`</a>    Provides the reason why a database is inconsistent, as reported by db.reasoning.consistency    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - options (`{ namedGraph: string }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""schema"">`db.reasoning.schema(conn, database, params)`</a>    Gets the reasoning schema of the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""docs"">docs</a>    #### <a name=""size"">`db.docs.size(conn, database, params)`</a>    Retrieves the size of the document store    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""clear"">`db.docs.clear(conn, database, params)`</a>    Clears the document store    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`db.docs.add(conn, database, fileName, fileContents, params)`</a>    Adds a document to the document store    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - fileName (`string`)    - fileContents (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`db.docs.remove(conn, database, fileName, params)`</a>    Removes a document from the document store    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - fileName (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`db.docs.get(conn, database, fileName, params)`</a>    Retrieves a document from the document store    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - fileName (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""namespaces"">namespaces</a>    #### <a name=""get"">`db.namespaces.get(conn, database)`</a>    Gets a mapping of the namespaces used in a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`db.namespaces.add(conn, database, fileOrContents, options)`</a>    Extracts namespaces from an RDF file or RDF string and adds new  and updates existing namespaces in the database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - fileOrContents (`object | string`)    - options ({ contentType?: [`RdfMimeType`](#rdfmimetype) })    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""query"">query</a>    #### <a name=""querytype"">QueryType</a>    One of the following values:    `'select' | 'ask' | 'construct' | 'describe' | 'update' | 'paths' | null`  #### <a name=""propertyoptions"">PropertyOptions</a>    Object with the following values:    - uri (`string`)  - property (`string`)    #### <a name=""additionalhandlers"">AdditionalHandlers</a>    Object with the following values:    - onResponseStart (`(res: Response) => boolean | void`)    #### <a name=""property"">`query.property(conn, database, config, params)`</a>    Gets the values for a specific property of a URI individual.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - config ([`PropertyOptions`](#propertyoptions))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""explain"">`query.explain(conn, database, query, accept, params)`</a>    Gets the query plan generated by Stardog for a given SPARQL query.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - query (`string`)    - accept ([`ExplainAcceptMimeType`](#explainacceptmimetype))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""execute"">`query.execute(conn, database, query, accept, params, additionalHandlers)`</a>    Executes a query against a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - query (`string`)    - accept ([`AcceptMimeType`](#acceptmimetype))    - params (`object`)    - additionalHandlers ([`AdditionalHandlers`](#additionalhandlers))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""executeintransaction"">`query.executeInTransaction(conn, database, transactionId, query, options, params)`</a>    Executes a query against a database within a transaction.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - query (`string`)    - options ({ accept: [`RdfMimeType`](#rdfmimetype) })    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""list"">`query.list(conn)`</a>    Gets a list of actively running queries.    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""kill"">`query.kill(conn, queryId)`</a>    Kills an actively running query.    Expects the following parameters:    - conn ([`Connection`](#connection))    - queryId (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`query.get(conn, queryId)`</a>    Gets information about an actively running query.    Expects the following parameters:    - conn ([`Connection`](#connection))    - queryId (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""storedqueryoptions"">StoredQueryOptions</a>    Object with the following values:    - name (`string`)  - database (`string`)  - query (`string`)  - shared (`boolean`)  - reasoning (`boolean`)  - description (`boolean`)    ## <a name=""stored"">stored</a>    #### <a name=""create"">`query.stored.create(conn, config, params)`</a>    Stores a query in Stardog, either on the system level or for a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - config ([`StoredQueryOptions`](#storedqueryoptions))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""list"">`query.stored.list(conn, params)`</a>    Lists all stored queries.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""update"">`query.stored.update(conn, config, params, useUpdateMethod)`</a>    Updates a given stored query and creates it if the name does not refer to an existing stored query.    Expects the following parameters:    - conn ([`Connection`](#connection))    - config ([`StoredQueryOptions`](#storedqueryoptions))    - params (`object`)    - useUpdateMethod (`boolean`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`query.stored.remove(conn, storedQuery, params)`</a>    Removes a given stored query.    Expects the following parameters:    - conn ([`Connection`](#connection))    - storedQuery (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""graphql"">graphql</a>    #### <a name=""execute"">`query.graphql.execute(conn, database, query, variables, params, additionalHandlers)`</a>    Executes a GraphQL query    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - query (`string`)    - variables (`object`)    - params (`object`)    - additionalHandlers ([`AdditionalHandlers`](#additionalhandlers))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""listschemas"">`query.graphql.listSchemas(conn, database, params)`</a>    Retrieves a list of GraphQL schemas in the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""addschema"">`query.graphql.addSchema(conn, database, name, schema, params)`</a>    Adds a GraphQL schema to the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - name (`string`)    - schema (`object`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""updateschema"">`query.graphql.updateSchema(conn, database, name, schema, params)`</a>    Updates (or adds if non-existent) a GraphQL schema to the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - name (`string`)    - schema (`object`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""getschema"">`query.graphql.getSchema(conn, database, name, params)`</a>    Retrieves a GraphQL schema from the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - name (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""removeschema"">`query.graphql.removeSchema(conn, database, name, params)`</a>    Removes a GraphQL schemafrom  the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - name (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""clearschemas"">`query.graphql.clearSchemas(conn, database, params)`</a>    Clears all GraphQL schemas in the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""utils"">utils</a>    #### <a name=""querytype"">`query.utils.queryType(query)`</a>    Returns the QueryType (as a string or null) for the given query.    Expects the following parameters:    - query (`string`)    Returns [`QueryType`](#querytyp)    #### <a name=""mimetype"">`query.utils.mimeType(query)`</a>    Returns the default HTTP `Accept` MIME type for the given query.    Expects the following parameters:    - query (`string`)    Returns [`HTTP.AcceptMimeType`](#acceptmimetyp)    ## <a name=""user"">user</a>    #### <a name=""user"">User</a>    Object with the following values:    - username (`string`)  - password (`string`)  - superuser (`boolean`)    #### <a name=""action"">Action</a>    One of the following values:    `'CREATE' |              'DELETE' |              'READ' |              'WRITE' |              'GRANT' |              'REVOKE' |              'EXECUTE'`  #### <a name=""resourcetype"">ResourceType</a>    One of the following values:    `'db' |              'user' |              'role' |              'admin' |              'metadata' |              'named-graph' |              'icv-constraints'`  #### <a name=""list"">`user.list(conn, params)`</a>    Gets a list of users.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`user.get(conn, username, params)`</a>    Gets all information for a given user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""create"">`user.create(conn, user, params)`</a>    Creates a new user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - user ([`User`](#user))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""changepassword"">`user.changePassword(conn, username, password, params)`</a>    Changes a user's password.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - password (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""valid"">`user.valid(conn, params)`</a>    Verifies that a Connection's credentials are valid.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""enabled"">`user.enabled(conn, username, params)`</a>    Verifies that a user is enabled.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""enable"">`user.enable(conn, username, enabled, params)`</a>    Enables/disables a user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - enabled (`boolean`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""setroles"">`user.setRoles(conn, username, roles, params)`</a>    Sets roles for a user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - roles (`string[]`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""listroles"">`user.listRoles(conn, username, params)`</a>    Gets a list of roles assigned to a user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""assignpermission"">`user.assignPermission(conn, username, permission, params)`</a>    Creates a new permission for a user over a given resource.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - permission ([`Permission`](#permission))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""deletepermission"">`user.deletePermission(conn, username, permission, params)`</a>    Removes a permission for a user over a given resource.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - permission ([`Permission`](#permission))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""permissions"">`user.permissions(conn, username, params)`</a>    Gets a list of permissions assigned to user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""effectivepermissions"">`user.effectivePermissions(conn, username, params)`</a>    Gets a list of a user's effective permissions.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""superuser"">`user.superUser(conn, username, params)`</a>    Specifies whether a user is a superuser.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`user.remove(conn, username, params)`</a>    Deletes a user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""token"">`user.token(conn)`</a>    Returns a token for the user if the connection is valid.    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""permission"">Permission</a>    Object with the following values:    - action (`Action`)  - resourceType (`ResourceType`)  - resources (`string[]`)    ## <a name=""role"">role</a>    #### <a name=""create"">`user.role.create(conn, role, params)`</a>    Creates a new role.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`{ name: string }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""list"">`user.role.list(conn, params)`</a>    Lists all existing roles.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`user.role.remove(conn, role, params)`</a>    Deletes an existing role from the system.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""userswithrole"">`user.role.usersWithRole(conn, role, params)`</a>    Lists all users that have been assigned a given role.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""assignpermission"">`user.role.assignPermission(conn, role, permission, params)`</a>    Adds a permission over a given resource to a given role.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`string`)    - permission ([`Permission`](#permission))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""deletepermission"">`user.role.deletePermission(conn, role, permission, params)`</a>    Removes a permission over a given resource from a given role.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`string`)    - permission ([`Permission`](#permission))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""permissions"">`user.role.permissions(conn, role, params)`</a>    Lists all permissions assigned to a given role.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""virtualgraphs"">virtualGraphs</a>    #### <a name=""sharedoptions"">SharedOptions</a>    Object with the following values:    - base (`string`)  - mappings.syntax (`string`)  - percent.encode (`boolean`)  - optimize.import (`boolean`)  - query.translation (`'DEFAULT' | 'LEGACY'`)    #### <a name=""rdbmsoptions"">RdbmsOptions</a> extends [SharedOptions](#sharedoptions)    Object with the following values:    - jdbc.url (`string`)  - jdbc.username (`string`)  - jdbc.password (`string`)  - jdbc.driver (`string`)  - parser.sql.quoting (`'NATIVE' | 'ANSI'`)  - sql.functions (`string`)  - sql.schemas (`string`)  - default.mappings.include.tables (`string`)  - default.mappings.exclude.tables (`string`)    #### <a name=""mongooptions"">MongoOptions</a> extends [SharedOptions](#sharedoptions)    Object with the following values:    - mongodb.uri (`string`)    #### <a name=""csvoptions"">CsvOptions</a> extends [SharedOptions](#sharedoptions)    Object with the following values:    - csv.separator (`string`)  - csv.quote (`string`)  - csv.escape (`string`)  - csv.header (`boolean`)  - csv.skip.empty (`boolean`)    #### <a name=""allvgoptions"">AllVgOptions</a>    One of the following values:    `SharedOptions & RdbmsOptions & MongoOptions & CsvOptions`  #### <a name=""mappingsrequestoptions"">MappingsRequestOptions</a>    Object with the following values:    - preferUntransformed (`boolean`)  - syntax (`string`)    #### <a name=""vgmeta"">VgMeta</a>    Object with the following values:    - db (`string`)  - dataSource (`string`)    #### <a name=""list"">`virtualGraphs.list(conn)`</a>    Retrieve a list of virtual graphs    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""listinfo"">`virtualGraphs.listInfo(conn)`</a>    Retrieve a list of virtual graphs info    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`virtualGraphs.add(conn, name, mappings, options, meta)`</a>    Add a virtual graph to the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - mappings (`string`)    - options ([`T`](#t))    - meta ([`VgMeta`](#vgmeta))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""update"">`virtualGraphs.update(conn, name, mappings, options, meta)`</a>    Update a virtual graph in the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - mappings (`string`)    - options ([`T`](#t))    - meta ([`VgMeta`](#vgmeta))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`virtualGraphs.remove(conn, name)`</a>    Remove a virtual graph from the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""online"">`virtualGraphs.online(conn, name)`</a>    Bring a virtual graph online    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""available"">`virtualGraphs.available(conn, name)`</a>    Determine if the named virtual graph is available    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""options"">`virtualGraphs.options(conn, name)`</a>    Retrieve a virtual graph's options    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""mappings"">`virtualGraphs.mappings(conn, name, requestOptions)`</a>    Retrieve a virtual graph's mappings    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - requestOptions ([`MappingsRequestOptions`](#mappingsrequestoptions))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""importfile"">`virtualGraphs.importFile(conn, file, fileType, database, importOptions)`</a>    Import a JSON or CSV file into a database via `virtual import`.    Expects the following parameters:    - conn ([`Connection`](#connection))    - file (`object`)    - fileType (`string`)    - database (`string`)    - importOptions (`{            mappings?: string,            properties?: string,            namedGraph?: string,          }`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""storedfunctions"">storedFunctions</a>    #### <a name=""add"">`storedFunctions.add(conn, functions, params)`</a>    Adds one or more stored functions to the server    Expects the following parameters:    - conn ([`Connection`](#connection))    - functions (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`storedFunctions.get(conn, name, params)`</a>    Retrieves the specified function definition    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`storedFunctions.remove(conn, name, params)`</a>    Removes a stored function from the server    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""clear"">`storedFunctions.clear(conn, params)`</a>    Removes all stored functions from the server    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""getall"">`storedFunctions.getAll(conn, params)`</a>    Retrieves an export of all stored functions on the server    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""cluster"">cluster</a>    #### <a name=""info"">`cluster.info(conn)`</a>    Retrieves basic information about a Stardog cluster.    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""status"">`cluster.status(conn)`</a>    Retrieves detailed status information about a Stardog cluster.    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""datasources"">dataSources</a>    #### <a name=""list"">`dataSources.list(conn)`</a>    Retrieve a list of data sources    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""listinfo"">`dataSources.listInfo(conn)`</a>    Retrieve a list of data sources info    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""info"">`dataSources.info(conn, name)`</a>    Retrieve the named data source info    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`dataSources.add(conn, name, options)`</a>    Add a data source to the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - options ([`T`](#t))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""update"">`dataSources.update(conn, name, options, requestOptions)`</a>    Update the named data source in the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - options ([`T`](#t))    - requestOptions (`{ force: boolean }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`dataSources.remove(conn, name, params)`</a>    Remove the named data source from the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""share"">`dataSources.share(conn, name)`</a>    Change a private data source to a shared one    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""online"">`dataSources.online(conn, name)`</a>    Bring the named data source online    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""available"">`dataSources.available(conn, name)`</a>    Determine if the named data source is available    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""options"">`dataSources.options(conn, name)`</a>    Retrieve the named data source options    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""getmetadata"">`dataSources.getMetadata(conn, name, options)`</a>    Retrieve the named data source metadata    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - options (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""updatemetadata"">`dataSources.updateMetadata(conn, name, metadata, options)`</a>    Update the named data source metadata    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - metadata ([`T`](#t))    - options (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""query"">`dataSources.query(conn, name, dataSourceQuery)`</a>    Query data source    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - dataSourceQuery (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""refreshcounts"">`dataSources.refreshCounts(conn, name, tableName)`</a>    Refresh table row-count estimates    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - tableName (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""refreshmetadata"">`dataSources.refreshMetadata(conn, name, tableName)`</a>    Refresh metadata    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - tableName (`string`)    Returns [`Promise<HTTP.Body>`](#body)   """
Semantic web;https://github.com/jindrichmynarz/sparql-to-csv;"""# sparql-to-csv    A command-line tool to stream SPARQL results to CSV. The tool is primarily intended to support data preparation for analyses that require tabular input. It helps you avoid writing ad hoc scripts to piece larger tabular datasets out of results of many SPARQL queries. It allows to generate queries from [Mustache](https://mustache.github.io) templates, either to execute paged queries or to execute queries based on results of other queries.     ## Usage    Use a [released executable](https://github.com/jindrichmynarz/sparql-to-csv/releases) or compile using [Leiningen](http://leiningen.org) and [lein-binplus](https://github.com/BrunoBonacci/lein-binplus):    ```sh  git clone https://github.com/jindrichmynarz/sparql-to-csv.git  cd sparql-to-csv  lein bin  ```    Then you can run the created executable file to find out about the configuration options:     ```sh  target/sparql_to_csv --help  ```    Example of use:    ```sh  target/sparql_to_csv --endpoint http://localhost:8890/sparql \                       --page-size 1000 \                       paged_query.mustache > results.csv  ```    There are two main use cases for this tool: paged queries and piped queries.    ### Paged queries    The first one is paged execution of SPARQL `SELECT` queries. RDF stores often limit the number of rows a SPARQL `SELECT` query can retrieve in one go and thus avoid the load such queries impose on the store. For queries that select more results than the limit per one request their execution must be split into several requests if complete results need to be obtained. One way to partition such queries is to split them into pages delimited by `LIMIT` and `OFFSET`, indicating the size and the start index, respectively, of a page. Paging requires the results to have a deterministic order, which can be achieved by using an `ORDER BY` clause. Due to limitations of some RDF stores (see [Virtuoso's documentation on this topic](https://virtuoso.openlinksw.com/dataspace/doc/dav/wiki/Main/VirtTipsAndTricksHowToHandleBandwidthLimitExceed)), the paged queries may need to contain an inner sub-`SELECT` that with an `ORDER BY` clause wrapped by an outer `SELECT` that slices a page from the ordered results using `LIMIT` and `OFFSET`, like this:    ```sparql  PREFIX dbo: <http://dbpedia.org/ontology/>    SELECT ?person   WHERE {    {      SELECT DISTINCT ?person      WHERE {        ?person a dbo:Person .      }      ORDER BY ?person    }  }  LIMIT 10000  OFFSET 40000  ```    In order to run paged queries you need to provide the tool with a Mustache template to generate the queries for the individual pages. These queries must contain a `{{limit}}` and `{{offset}}` parameters, like so:    ```sparql  PREFIX dbo: <http://dbpedia.org/ontology/>    SELECT ?person   WHERE {    {      SELECT DISTINCT ?person      WHERE {        ?person a dbo:Person .      }      ORDER BY ?person    }  }  LIMIT {{limit}}  OFFSET {{offset}}  ```    The `limit` is set by the `--page-size` parameter. The offset is incremented by the page size in each successive request. The execution of paged queries stops when an individual query returns empty results.    ### Piped queries     It may be desirable to decompose complex queries into several simpler queries to avoid limit on demanding queries due to performance. For example, for each person in a dataset we may want to retrieve its complex description. While this may be possible to achieve by using a sub-`SELECT` to page through the individual persons and an outer `SELECT` to compose their descriptions, such query would be more demanding since it both sorts the persons and selects their descriptions. Consequently, it may not be possible to run such query since it would end with a time-out. Instead, this query can be decomposed into two queries. The first one selects persons in the paged manner described above, while the second one receives results of the first query one by one and fetches their descriptions.    This approach is also useful when you need to query one SPARQL endpoint using data from another SPARQL endpoint. While this is feasible using [federated queries](https://www.w3.org/TR/sparql11-federated-query), they too suffer from performance problems.    Piped queries take CSV input and for each line they execute a query generated from a Mustache template that is provided with the line's data as parameters. For example, the CSV generated by the query above contains a column `person`, which can be used in a query template as `{{person}}`:    ```sparql  PREFIX dbo: <http://dbpedia.org/ontology/>  PREFIX dbp: <http://dbpedia.org/property/>    SELECT (<{{person}}> AS ?person) ?name ?birthDate ?deathDate  WHERE {    <{{person}}> dbp:name ?name ;      dbo:birthDate ?birthDate .    OPTIONAL {      <{{person}}> dbo:deathDate ?deathDate .    }  }  ```    The input CSV must have a header with column names. In order to be usable in Mustache template, the column names in the input CSV can contain only ASCII characters, `?`, `!`, `/`, `.`, or `-`. For example, `right!` is allowed, while `mélangé` is not.    Piped queries enable to create data processing pipelines. For instance, if the first query is stored in the `persons.mustache` file and the second query is stored as `describe_person.mustache`, then we can run them in pipeline using the following command using `--piped` to indicate that it is a piped query:     ```sh  sparql_to_csv -e http://dbpedia.org/sparql persons.mustache |    sparql_to_csv -e http://dbpedia.org/sparql --piped describe_person.mustache  ```    By default the piped input is replaced by the output query results. However, using the `--extend` parameter extends the input with the results. Each result row is append to its input row. This allows you to combine data from multiple queries. Piped queries can be arbitrarily chained and allow joining data across many SPARQL endpoints.    ## License    Copyright © 2016 Jindřich Mynarz    Distributed under the Eclipse Public License version 1.0. """
Semantic web;https://github.com/ad-freiburg/QLever;"""# QLever      [![Build Status via Docker](https://github.com/ad-freiburg/QLever/actions/workflows/docker.yml/badge.svg)](https://github.com/ad-freiburg/QLever/actions/workflows/docker.yml)  [![Build Status via G++10/Clang++11](https://github.com/ad-freiburg/QLever/actions/workflows/cmake.yml/badge.svg)](https://github.com/ad-freiburg/QLever/actions/workflows/cmake.yml)        QLever (pronounced ""Clever"") is a SPARQL engine that can efficiently index and query very large knowledge graphs with up to 100 billion triples on a single standard PC or server.  In particular, QLever is fast for queries that involve large intermediate or final results, which are notoriously hard for engines like Blazegraph or Virtuoso.  QLever also supports search in text associated with the knowledge base, as well as SPARQL autocompletion.  [Here are demos of QLever](http://qlever.cs.uni-freiburg.de) on a variety of large knowledge graphs, including the complete Wikidata and OpenStreetMap.  Those demos also feature QLever's context-sensitiv autocompletion, which makes SPARQL query construction so much easier.    QLever was first described and evaluated in this [CIKM'17  paper](http://ad-publications.informatik.uni-freiburg.de/CIKM_qlever_BB_2017.pdf).  QLever has developed a lot since then.  Qlever's autocompletion functionality and some other new features are described and evaluated in [this paper](https://ad-publications.cs.uni-freiburg.de/ARXIV_sparql_autocompletion_BKKKS_2021.pdf).  If you use QLever in your work, please cite those papers.  QLever supports standard SPARQL 1.1 constructs like:  LIMIT, OFFSET, ORDER BY, GROUP BY, HAVING, COUNT, DISTINCT, SAMPLE, GROUP_CONCAT, FILTER, REGEX, LANG, OPTIONAL, UNION, MINUS, VALUES, BIND.  Predicate paths and subqueries are also supported.  The SERVICE keyword is not yet supported.  We aim at full SPARQL 1.1 support and we are almost there (except for SPARQL Update operations, which are a longer-term project).    # Quickstart    For easy step-by-step instructions on how to build an index using QLever and  then start a SPARQL endpoint using that index, see our [Quickstart Guide](docs/quickstart.md).  This will take you through a simple example dataset ([120 Years of Olympics](https://github.com/wallscope/olympics-rdf), with 1.8M triples)  as well as a very large dataset ([the complete Wikidata](https://www.wikidata.org), with 16 billion triples as of 30.09.2021).    # Advanced feature and more in-depth information    QLever's [advanced features are described here](docs/advanced_features.md).    For more in-depth information, see the various other `.md` files in [this folder](docs). """
Semantic web;https://github.com/nkons/r2rml-parser;"""# R2RML Parser    An R2RML implementation that can export relational database contents as RDF graphs, based on an [R2RML](http://www.w3.org/TR/r2rml/) mapping document. Contains an R2RML [mapping document](https://github.com/nkons/r2rml-parser/blob/master/dspace/dspace-mapping.rdf) for the [DSpace](http://www.dspace.org/) institutional repository solution.    For more information, please see the [wiki](https://github.com/nkons/r2rml-parser/wiki).    Please send any feedback or questions to [nkons@live.com](mailto:nkons@live.com), or [open a new issue](https://github.com/nkons/r2rml-parser/issues). Happy to discuss how to get value from your data.    If you use R2RML Parser, please cite it in your publications as follows:  ```bibtex  @article{Konstantinou2014,  author = {Nikolaos Konstantinou and Dimitrios-Emmanuel Spanos and Nikos Houssos and Nikolaos Mitrou},  title = {Exposing scholarly information as Linked Open Data: RDFizing DSpace contents},  journal = {The Electronic Library},  volume = {32},  number = {6},  pages = {834-851},  year = {2014},  doi = {10.1108/EL-12-2012-0156}  }  ```    ### Implementation details    R2RML implementation written fully in Java 7, using Apache Jena 2.11, Spring 4.0, JUnit 4.9, and Maven 3.1. Tested against MySQL 5.6, PostgreSQL 9.2 and Oracle 11g.    ### Licence    This work is licensed under the Apache License 2.0.    ### Publications    You can find in the wiki a list of [publications](https://github.com/nkons/r2rml-parser/wiki/Publications) based on the tool. """
Semantic web;https://github.com/cavendish-ldp/cavendish;"""# Cavendish  ## LDP on BlazeGraph  Cavendish endeavours to be both an implementation of [LDP](https://www.w3.org/TR/ldp/) and a laboratory for elaborating [Fedora 4](http://duraspace.org/about_fedora) APIs as extensions of the LDP specification. We are also interested in better understanding how the constituent frameworks of a Fedora 4 implementation influence the way its core model is understood- in this case, building a Fedora on a triplestore. Cavendish is built on [BlazeGraph](https://www.blazegraph.com/).  ## Running Cavendish  ```bash  cd cavendish  mvn install  cd cavendish-jetty  JETTY_HOME=file:`pwd`/src/main/webapp  BG_CONFIG=`pwd`/src/main/webapp/WEB-INF/RWStore.properties  JETTY_XML=`pwd`/src/main/resources/jetty.xml  mvn exec:java -Djetty.home=$JETTY_HOME -Dbigdata.propertyFile=$BG_CONFIG -DjettyXml=$JETTY_XML  ```  ## Colophon  > By this Poetical Description, you may perceive, that my ambition is not onely to be Empress, but Authoress of a whole World;  > and that the Worlds I have made, both the Blazing- and the other Philosophical World, mentioned in the first part of this Description, are framed and composed of the most pure, that is, the Rational parts of Matter, which are the parts of my Mind;  > which Creation was more easily and suddenly effected, than the Conquests of the two famous Monarchs of the World.  >  > ... as for the Blazing-world, it having an Empress already, who rules it with great Wisdom and Conduct, which Empress is my dear Platonick Friend;  > I shall never prove so unjust, treacherous and unworthy to her, as to disturb her Government, much less to depose her from her Imperial Throne, for the sake of any other, but rather chuse to create another World for another Friend.  >  > <cite>Margaret Cavendish, in the Epilogue to [The Blazing-World](http://digital.library.upenn.edu/women/newcastle/blazing/blazing.html)</cite> """
Semantic web;https://github.com/lukostaz/shi3ld-http;"""Shi3ld for HTTP operations  ===========    [Shi3ld for HTTP](http://wimmics.inria.fr/projects/shi3ld-ldp) is an access control module for enforcing authorization on triple stores.   Shi3ld for HTTP protects HTTP operations on [Linked Data](linkeddata.org) and relies on attribute-based access policies.    ### Features    * Authorization for r/w HTTP Methods on RDF resources  * RDF Resource-oriented  * Policy Language in RDF/SPARQL or RDF only  * Attribute-based  * ""Context-aware"" Policies    The policy vocabularies namespace documents are available at:  * [S4AC](http://ns.inria.fr/s4ac) - for modelling Access Policies.  * [PRISSMA](http://ns.inria.fr/prissma) - for modelling context client attributes.     ### Scenarios    Shi3ld for HTTP supports three different scenarios and are available in this repository branches:    * Shi3ld for [SPARQL Graph Store Protocol](http://www.w3.org/TR/sparql11-http-rdf-update/)  * Shi3ld for [Linked Data Platform](http://www.w3.org/TR/ldp-ucr/) (SPARQL-based)  * Shi3ld for [Linked Data Platform](http://www.w3.org/TR/ldp-ucr/) (SPARQL-less)    Scenarios are detailed in our paper [Access Control for HTTP Operations on Linked Data](http://hal.inria.fr/docs/00/81/50/67/PDF/eswc2013_shi3ld.pdf)       ### Installation    All Shi3ld scenarios are Java server side modules that run in a java application server (e.g. Tomcat)    The `config.properties` property file needs to be customized with the policy storage path and the triple storage path.     The Shi3ld-GSP scenario is compatible with the GSP-compliant [Fuseki SPARQL engine](http://jena.apache.org/documentation/serving_data/index.html) needs the Fuseki server URL and the Fuseki dataset name.    The Shi3ld-LDP scenarios embed the [Corese/KGRAM](http://wimmics.inria.fr/corese) RDF store and SPARQL processor.    ### Testing    Shi3ld-HTTP can be tested with a [standalone client](http://wimmics.inria.fr/projects/shi3ld-ldp/shi3ld-test-client.zip) shipped with sample client attributes.    Sample Access Policies can be found [here](http://wimmics.inria.fr/projects/shi3ld-ldp/shi3ld-test-policies.zip)."""
Semantic web;https://github.com/seebi/rdf.sh;"""# rdf.sh    A multi-tool shell script for doing Semantic Web jobs on the command line.    [![Build Status](https://travis-ci.org/seebi/rdf.sh.svg?branch=develop)](https://travis-ci.org/seebi/rdf.sh)      # contents    * [installation (manually, debian/ubuntu/, brew, docker)](#installation)  * [configuration](#configuration)  * [usage / features](#usage-features)      * [overview](#overview)      * [namespace lookup](#nslookup)      * [resource description](#description)      * [SPARQL graph store protocol](#gsp)      * [linked data platform client](#ldp)      * [WebID requests](#webid)      * [syntax highlighting](#highlighting)      * [resource listings](#listings)      * [resource inspection / debugging](#inspection)      * [re-format RDF files in turtle](#turtleize)      * [prefix distribution for data projects](#prefixes)      * [autocompletion and resource history](#autocompletion)      <a name=""installation""></a>  ## installation    ### manually    rdf.sh is a single bash shell script so installation is trivial ... :-)  Just copy or link it to you path, e.g. with        $ sudo ln -s /path/to/rdf.sh /usr/local/bin/rdf    ### debian / ubuntu    You can download a debian package from the [release  section](https://github.com/seebi/rdf.sh/releases) and install it as root with  the following commands:    ```  $ sudo dpkg -i /path/to/your/rdf.sh_X.Y_all.deb  $ sudo apt-get -f install  ```    The `dpkg` run will probably fail due to missing dependencies but the `apt-get`  run will install all dependencies as well as `rdf`.    Currently, `zsh` is a hard dependency since the zsh completion ""needs"" it.    ### brew based    You can install `rdf.sh` by using the provided recipe:    ```  brew install https://raw.githubusercontent.com/seebi/rdf.sh/develop/brew/rdf.sh.rb  ```    ### docker based    You can install `rdf.sh` by using the provided docker image:    ```  docker pull seebi/rdf.sh  ```    After that, you can e.g. run this command:    ```  docker run -i -t --rm seebi/rdf.sh rdf desc foaf:Person  ```    <a name=""dependencies""></a>  ### dependencies    Required tools currently are:    * [roqet](http://librdf.org/rasqal/roqet.html) (from rasqal-utils)  * [rapper](http://librdf.org/raptor/rapper.html) (from raptor-utils or raptor2-utils)  * [curl](http://curl.haxx.se/)    Suggested tools are:     * [zsh](http://zsh.sourceforge.net/) (without the autocompletion, it is not the same)    <a name=""files""></a>  ### files    These files are available in the repository:    * `README.md` - this file  * `_rdf` - zsh autocompletion file  * `CHANGELOG.md` - version change log  * `doap.ttl` - doap description of rdf.sh  * `rdf.1` - rdf.sh man page  * `rdf.sh` - the script  * `Screenshot.png` - a screeny of rdf.sh in action  * `example.rc` - an example config file which can be copied    These files are used by rdf.sh:    * `$HOME/.cache/rdf.sh/resource.history` - history of all processed resources  * `$HOME/.cache/rdf.sh/prefix.cache` - a cache of all fetched namespaces  * `$HOME/.config/rdf.sh/prefix.local` - locally defined prefix / namespaces  * `$HOME/.config/rdf.sh/rc` - config file    rdf.sh follows the  [XDG Base Directory Specification](http://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html)  in order to allow different cache and config directories.      <a name=""usage-features""></a>  ## usage / features    <a name=""overview""></a>  ### overview    rdf.sh currently provides these subcommands:    * color: get a html color for a resource URI  * count: count distinct triples  * delete: deletes an existing linked data resource via LDP  * desc: outputs description of the given resource in a given format (default: turtle)  * diff: diff of triples from two RDF files  * edit: edit the content of an existing linked data resource via LDP (GET + PUT)  * get: fetches an URL as RDF to stdout (tries accept header)  * get-ntriples: curls rdf and transforms to ntriples  * gsp-delete: delete a graph via SPARQL 1.1 Graph Store HTTP Protocol  * gsp-get: get a graph via SPARQL 1.1 Graph Store HTTP Protocol  * gsp-put: delete and re-create a graph via SPARQL 1.1 Graph Store HTTP Protocol  * head: curls only the http header but accepts only rdf  * headn: curls only the http header  * help: outputs the manpage of rdf  * list: list resources which start with the given URI  * ns: curls the namespace from prefix.cc  * nscollect: collects prefix declarations of a list of ttl/n3 files  * nsdist: distributes prefix declarations from one file to a list of other ttl/n3 files  * put: replaces an existing linked data resource via LDP  * split: split an RDF file into pieces of max X triple and output the file names  * turtleize: outputs an RDF file in turtle, using as much as possible prefix declarations      <a name=""nslookup""></a>  ### namespace lookup (`ns`)    rdf.sh allows you to quickly lookup namespaces from [prefix.cc](http://prefix.cc) as well as locally defined prefixes:    ```  $ rdf ns foaf  http://xmlns.com/foaf/0.1/  ```    These namespace lookups are cached (typically  `$HOME/.cache/rdf.sh/prefix.cache`) in order to avoid unneeded network  traffic. As a result of this subcommand, all other rdf command can get  qnames as parameters (e.g. `foaf:Person` or `skos:Concept`).    To define you own lookup table, just add a line    ```  prefix|namespace  ```    to `$HOME/.config/rdf.sh/prefix.local`. rdf.sh will use it as a priority  lookup table which overwrites cache and prefix.cc lookup.    rdf.sh can also output prefix.cc syntax templates (uncached):     ```  $ rdf ns skos sparql  PREFIX skos: <http://www.w3.org/2004/02/skos/core#>    SELECT *  WHERE {    ?s ?p ?o .  }    $ rdf ns dct n3      @prefix dct: <http://purl.org/dc/terms/>.  ```      <a name=""description""></a>  ### resource description (`desc`)    Describe a resource by querying for statements where the resource is the  subject. This is extremly useful to fastly check schema details.    ```  $ rdf desc foaf:Person  @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .  @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .  @prefix owl: <http://www.w3.org/2002/07/owl#> .  @prefix foaf: <http://xmlns.com/foaf/0.1/> .  @prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .  @prefix contact: <http://www.w3.org/2000/10/swap/pim/contact#> .    foaf:Person      a rdfs:Class, owl:Class ;      rdfs:comment ""A person."" ;      rdfs:isDefinedBy <http://xmlns.com/foaf/0.1/> ;      rdfs:label ""Person"" ;      rdfs:subClassOf contact:Person, geo:SpatialThing, foaf:Agent ;      owl:disjointWith foaf:Organization, foaf:Project ;      <http://www.w3.org/2003/06/sw-vocab-status/ns#term_status> ""stable"" .  ```    In addition to the textual representation, you can calculate a color for visual  resource representation with the `color` command:    ```  $ rdf color http://sebastian.tramp.name  #2024e9  ```    Refer to the [cold webpage](http://cold.aksw.org) for more information :-)    <a name=""gsp""></a>  ### SPARQL graph store protocol client    The [SPARQL 1.1 Graph Store HTTP Protocol](https://www.w3.org/TR/sparql11-http-rdf-update/) describes the use of HTTP operations for the purpose of managing a collection of RDF graphs.  rdf.sh supports the following commands in order to manipulate graphs:    ```  Syntax: rdf gsp-get <graph URI | Prefix:LocalPart> <store URL | Prefix:LocalPart (optional)>  (get a graph via SPARQL 1.1 Graph Store HTTP Protocol)  ```    ```  Syntax: rdf gsp-put <graph URI | Prefix:LocalPart> <path/to/your/file.rdf> <store URL | Prefix:LocalPart (optional)>  (delete and re-create a graph via SPARQL 1.1 Graph Store HTTP Protocol)  ```    ```  Syntax: rdf gsp-delete <graph URI | Prefix:LocalPart> <store URL | Prefix:LocalPart (optional)>  (delete a graph via SPARQL 1.1 Graph Store HTTP Protocol)  ```    If the store URL **is not given**, the [Direct Graph Identification](https://www.w3.org/TR/sparql11-http-rdf-update/#direct-graph-identification) is assumed, which means the store URL is taken as the graph URL.  If the store URL **is given**, [Indirect Graph Identification](https://www.w3.org/TR/sparql11-http-rdf-update/#indirect-graph-identification) is used.      <a name=""ldp""></a>  ### linked data platform client    The [Linked Data Platform](http://www.w3.org/TR/ldp/) describe a read-write  Linked Data architecture, based on HTTP access to web resources that describe  their state using the RDF data model. rdf.sh supports  [DELETE](http://www.w3.org/TR/ldp/#http-delete),  [PUT](http://www.w3.org/TR/ldp/#http-put) and edit (GET, followed by an edit  command, followed by a PUT request)  of Linked Data Platform Resources (LDPRs).    ```  Syntax: rdf put <URI | Prefix:LocalPart> <path/to/your/file.rdf>  (replaces an existing linked data resource via LDP)  ```    ```  Syntax: rdf delete <URI | Prefix:LocalPart>  (deletes an existing linked data resource via LDP)  ```    ```  Syntax: rdf edit <URI | Prefix:LocalPart>  (edit the content of an existing linked data resource via LDP (GET + PUT))  ```    The edit command uses the `EDITOR` variable to start the editor of your choice  with a prepared turtle file.  You can change the content of that file (add or remove triple) and you can use  any prefix you've already declared via config or which is cached.  Used prefix declarations are added automatically afterwards and the file is the  PUTted to the server.      <a name=""webid""></a>  ### WebID requests    In order to request ressources with your WebID client certificate, you need to  setup the rdf.sh `rc` file (see configuration section).  Curl allows for using client certs with the  [-E parameter](http://curl.haxx.se/docs/manpage.html#-E), which needs a  [pem](https://en.wikipedia.org/wiki/X.509#Certificate_filename_extensions) file  with your private key AND the certificate.    To use your proper created WebID pem file, just add this to your rc file:    ```  RDFSH_CURLOPTIONS_ADDITONS=""-E $HOME/path/to/your/webid.pem""  ```    <a name=""highlighting""></a>  ### syntax highlighting    rdf.sh supports the highlighted output of turtle with  [pygmentize](http://pygments.org/) and a proper  [turtle lexer](https://github.com/gniezen/n3pygments). If everything is  available (`pygmentize -l turtle` does not throw an error), then it will look  like this.    <img src=""https://raw.github.com/seebi/rdf.sh/master/Screenshot.png"" />    If you do not want syntax highlighting for some reason, you can disable it by  setting the shell environment variable `RDFSH_HIGHLIGHTING_SUPPRESS` to `true`  e.g with        export RDFSH_HIGHLIGHTING_SUPPRESS=true    before you start `rdf.sh`.      <a name=""listings""></a>  ### resource listings (`list`)    To get a quick overview of an unknown RDF schema, rdf.sh provides the  `list` command which outputs a distinct list of subject resources of the  fetched URI:    ```  $ rdf list geo:  http://www.w3.org/2003/01/geo/wgs84_pos#  http://www.w3.org/2003/01/geo/wgs84_pos#SpatialThing  http://www.w3.org/2003/01/geo/wgs84_pos#Point  http://www.w3.org/2003/01/geo/wgs84_pos#lat  http://www.w3.org/2003/01/geo/wgs84_pos#location  http://www.w3.org/2003/01/geo/wgs84_pos#long  http://www.w3.org/2003/01/geo/wgs84_pos#alt  http://www.w3.org/2003/01/geo/wgs84_pos#lat_long  ```    You can also provide a starting sequence to constrain the output    ```  $ rdf list skos:C     http://www.w3.org/2004/02/skos/core#Concept  http://www.w3.org/2004/02/skos/core#ConceptScheme  http://www.w3.org/2004/02/skos/core#Collection  http://www.w3.org/2004/02/skos/core#changeNote  http://www.w3.org/2004/02/skos/core#closeMatch  ```    **Note:** Here the `$GREP_OPTIONS` environment applies to the list. In  my case, I have a `--ignore-case` in it, so e.g. `skos:changeNote` is  listed as well.    This feature only works with schema documents which are available by  fetching the namespace URI (optionally with linked data headers to be  redirected to an RDF document).       <a name=""inspection""></a>  ### resource inspection (`get`, `count`, `head` and `headn`)    Fetch a resource via linked data and print it to stdout:    ```  $ rdf get http://sebastian.tramp.name >me.rdf  ```    Count all statements of a resource:      ```  $ rdf count http://sebastian.tramp.name  58  ```    Inspect the header of a resource. Use `head` for header request with  content negotiation suitable for linked data and `headn` for a normal  header request as sent by browsers.    ```  $ rdf head http://sebastian.tramp.name  HTTP/1.1 302 Found  [...]  Location: http://sebastian.tramp.name/index.rdf  [...]  ```      <a name=""prefixes""></a>  ### prefix distribution for data projects (`nscollect` and `nsdist`)    Often I need to create a lot of n3/ttl files as a data project which consists  of schema and instance resources. These projects are split over several files  for a better handling and share a set if used namespaces.    When introducing a new namespace to such projects, I need to add the `@prefix`  line to each of the ttl files of this project.    `rdf.sh` has two subcommands which handle this procedure:    * `rdf nscollect` collects all prefixes from existing n3/ttl files in the    current directory and collect them in the file `prefixes.n3`  * `rdf nsdist *.n3` firstly removes all `@prefix` lines from the target files    and then add `prefixes.n3` on top of them.      <a name=""turtleize""></a>  ### re-format RDF files in turtle (`turtleize`)    Working with RDF files often requires to convert and reformat different files.  With `rdf turtleize`, its easy to get RDF files in turtle plus they are nicely  formatted because all needed prefix declarations are added.    turtleize uses rapper and tries to detect all namespaces which are cached in  your `prefix.cache` file, as well as which a defined in the `prefix.local` file.    To turtleize your current buffer in vim for example, you can do a `:%! rdf turtleize %`.      <a name=""autocompletion""></a>  ### autocompletion and resource history    `rdf.sh` can be used with a   [zsh](http://en.wikipedia.org/wiki/Zsh)  [command-line completion](http://en.wikipedia.org/wiki/Command-line_completion)  function.  This boosts the usability of  this tool to a new level!  The completion features support for the base commands as well as for  auto-completion of resources.  These resources are taken from the resource history.  The resource history is written to `$HOME/.cache/rdf.sh/resource.history`.    When loaded, the completion function could be used in this way:    ```  rdf de<tab> tramp<tab>  ```    This could result in the following commandline:    ```  rdf desc http://sebastian.tramp.name  ```    Notes:    * The substring matching feature of the zsh [completion system](http://linux.die.net/man/1/zshcompsys) should be turned on.    * e.g. with `zstyle ':completion:*' matcher-list 'r:|[._-]=* r:|=*' 'l:|=* r:|=*'`  * This assumes that at least one resource exists in the history which matches `.*tramp.*`    <a name=""configuration""></a>  ## configuration    rdf.sh imports `$HOME/.config/rdf.sh/rc` at the beginning of each execution so  this is the place to setup personal configuration options such as    * WebID support  * syntax highlighting suppression  * setup of preferred accept headers  * setup of alternate ntriples fetch program such as any23's rover (see [this feature request](https://github.com/seebi/rdf.sh/issues/8) for background infos)    Please have a look at the [example rc file](https://github.com/seebi/rdf.sh/blob/master/example.rc).   """
Semantic web;https://github.com/RDFLib/rdflib;"""![](docs/_static/RDFlib.png)        RDFLib  ======  [![Build Status](https://drone.rdflib.ashs.dev/api/badges/RDFLib/rdflib/status.svg?ref=refs/heads/master)](https://drone.rdflib.ashs.dev/RDFLib/rdflib/branches)  [![Coveralls branch](https://img.shields.io/coveralls/RDFLib/rdflib/master.svg)](https://coveralls.io/r/RDFLib/rdflib?branch=master)  [![GitHub stars](https://img.shields.io/github/stars/RDFLib/rdflib.svg)](https://github.com/RDFLib/rdflib/stargazers)  [![PyPI](https://img.shields.io/pypi/v/rdflib.svg)](https://pypi.python.org/pypi/rdflib)  [![PyPI](https://img.shields.io/pypi/pyversions/rdflib.svg)](https://pypi.python.org/pypi/rdflib)    RDFLib is a pure Python package for working with [RDF](http://www.w3.org/RDF/). RDFLib contains most things you need to work with RDF, including:    * parsers and serializers for RDF/XML, N3, NTriples, N-Quads, Turtle, TriX, Trig and JSON-LD  * a Graph interface which can be backed by any one of a number of Store implementations  * store implementations for in-memory, persistent on disk (Berkeley DB) and remote SPARQL endpoints  * a SPARQL 1.1 implementation - supporting SPARQL 1.1 Queries and Update statements  * SPARQL function extension mechanisms    ## RDFlib Family of packages  The RDFlib community maintains many RDF-related Python code repositories with different purposes. For example:    * [rdflib](https://github.com/RDFLib/rdflib) - the RDFLib core  * [sparqlwrapper](https://github.com/RDFLib/sparqlwrapper) - a simple Python wrapper around a SPARQL service to remotely execute your queries  * [pyLODE](https://github.com/RDFLib/pyLODE) - An OWL ontology documentation tool using Python and templating, based on LODE.    Please see the list for all packages/repositories here:    * <https://github.com/RDFLib>    ## Versions & Releases    * `6.2.0-alpha` current `master` branch  * `6.x.y` current release and support Python 3.7+ only. Many improvements over 5.0.0      * see [Releases](https://github.com/RDFLib/rdflib/releases)  * `5.x.y` supports Python 2.7 and 3.4+ and is [mostly backwards compatible with 4.2.2](https://rdflib.readthedocs.io/en/stable/upgrade4to5.html).    See <https://rdflib.dev> for the release overview.    ## Documentation  See <https://rdflib.readthedocs.io> for our documentation built from the code. Note that there are `latest`, `stable` `5.0.0` and `4.2.2` documentation versions, matching releases.    ## Installation  The stable release of RDFLib may be installed with Python's package management tool *pip*:        $ pip install rdflib    Alternatively manually download the package from the Python Package  Index (PyPI) at https://pypi.python.org/pypi/rdflib    The current version of RDFLib is 6.1.1, see the ``CHANGELOG.md`` file for what's new in this release.    ### Installation of the current master branch (for developers)    With *pip* you can also install rdflib from the git repository with one of the following options:        $ pip install git+https://github.com/rdflib/rdflib@master    or        $ pip install -e git+https://github.com/rdflib/rdflib@master#egg=rdflib    or from your locally cloned repository you can install it with one of the following options:        $ python setup.py install    or        $ pip install -e .    ## Getting Started  RDFLib aims to be a pythonic RDF API. RDFLib's main data object is a `Graph` which is a Python collection  of RDF *Subject, Predicate, Object* Triples:    To create graph and load it with RDF data from DBPedia then print the results:    ```python  from rdflib import Graph  g = Graph()  g.parse('http://dbpedia.org/resource/Semantic_Web')    for s, p, o in g:      print(s, p, o)  ```  The components of the triples are URIs (resources) or Literals  (values).    URIs are grouped together by *namespace*, common namespaces are included in RDFLib:    ```python  from rdflib.namespace import DC, DCTERMS, DOAP, FOAF, SKOS, OWL, RDF, RDFS, VOID, XMLNS, XSD  ```    You can use them like this:    ```python  from rdflib import Graph, URIRef, Literal  from rdflib.namespace import RDFS, XSD    g = Graph()  semweb = URIRef('http://dbpedia.org/resource/Semantic_Web')  type = g.value(semweb, RDFS.label)  ```  Where `RDFS` is the RDFS namespace, `XSD` the XML Schema Datatypes namespace and `g.value` returns an object of the triple-pattern given (or an arbitrary one if multiple exist).    Or like this, adding a triple to a graph `g`:    ```python  g.add((      URIRef(""http://example.com/person/nick""),      FOAF.givenName,      Literal(""Nick"", datatype=XSD.string)  ))  ```  The triple (in n-triples notation) `<http://example.com/person/nick> <http://xmlns.com/foaf/0.1/givenName> ""Nick""^^<http://www.w3.org/2001/XMLSchema#string> .`  is created where the property `FOAF.givenName` is the URI `<http://xmlns.com/foaf/0.1/givenName>` and `XSD.string` is the  URI `<http://www.w3.org/2001/XMLSchema#string>`.    You can bind namespaces to prefixes to shorten the URIs for RDF/XML, Turtle, N3, TriG, TriX & JSON-LD serializations:     ```python  g.bind(""foaf"", FOAF)  g.bind(""xsd"", XSD)  ```  This will allow the n-triples triple above to be serialised like this:   ```python  print(g.serialize(format=""turtle""))  ```    With these results:  ```turtle  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>    <http://example.com/person/nick> foaf:givenName ""Nick""^^xsd:string .  ```    New Namespaces can also be defined:    ```python  dbpedia = Namespace('http://dbpedia.org/ontology/')    abstracts = list(x for x in g.objects(semweb, dbpedia['abstract']) if x.language=='en')  ```    See also [./examples](./examples)      ## Features  The library contains parsers and serializers for RDF/XML, N3,  NTriples, N-Quads, Turtle, TriX, JSON-LD, RDFa and Microdata.    The library presents a Graph interface which can be backed by  any one of a number of Store implementations.    This core RDFLib package includes store implementations for  in-memory storage and persistent storage on top of the Berkeley DB.    A SPARQL 1.1 implementation is included - supporting SPARQL 1.1 Queries and Update statements.    RDFLib is open source and is maintained on [GitHub](https://github.com/RDFLib/rdflib/). RDFLib releases, current and previous  are listed on [PyPI](https://pypi.python.org/pypi/rdflib/)    Multiple other projects are contained within the RDFlib ""family"", see <https://github.com/RDFLib/>.    ## Running tests    ### Running the tests on the host    Run the test suite with `pytest`.  ```shell  pytest  ```    ### Running test coverage on the host with coverage report    Run the test suite and generate a HTML coverage report with `pytest` and `pytest-cov`.  ```shell  pytest --cov  ```    ### Running the tests in a Docker container    Run the test suite inside a Docker container for cross-platform support. This resolves issues such as installing BerkeleyDB on Windows and avoids the host and port issues on macOS.  ```shell  make tests  ```    Tip: If the underlying Dockerfile for the test runner changes, use `make build`.    ### Running the tests in a Docker container with coverage report    Run the test suite inside a Docker container with HTML coverage report.  ```shell  make coverage  ```    ### Viewing test coverage    Once tests have produced HTML output of the coverage report, view it by running:  ```shell  pytest --cov --cov-report term --cov-report html  python -m http.server --directory=htmlcov  ```    ## Contributing    RDFLib survives and grows via user contributions!  Please read our [contributing guide](https://rdflib.readthedocs.io/en/stable/developers.html) to get started.  Please consider lodging Pull Requests here:    * <https://github.com/RDFLib/rdflib/pulls>    You can also raise issues here:    * <https://github.com/RDFLib/rdflib/issues>    ## Support & Contacts  For general ""how do I..."" queries, please use https://stackoverflow.com and tag your question with `rdflib`.  Existing questions:    * <https://stackoverflow.com/questions/tagged/rdflib>    If you want to contact the rdflib maintainers, please do so via:    * the rdflib-dev mailing list: <https://groups.google.com/group/rdflib-dev>  * the chat, which is available at [gitter](https://gitter.im/RDFLib/rdflib) or via matrix [#RDFLib_rdflib:gitter.im](https://matrix.to/#/#RDFLib_rdflib:gitter.im) """
Semantic web;https://github.com/theodi/csv2json;"""[![License](http://img.shields.io/:license-mit-blue.svg)](http://theodi.mit-license.org)    # CSV 2 JSON    A ruby gem to convert CSV to JSON, following the CSV on the Web specification."""
Semantic web;https://github.com/yyz1989/NoSPA-RDF-Data-Cube-Validator;"""NoSPA RDF Data Cube Validator  =============================    ### Introduction    This is an RDF Data Cube Validator. Its significant difference from other existing validators is that it is not based on SPARQL queries, as its name ""NoSPA"". Jena library is used to manipulate RDF models. The official SPARQL queries for constraint checks are interpreted and parsed by this validator to search functions with nested statement listing functions provided by Jena and filters for different conditions. It has an outstanding performance because the entire process is executed in memory. I believe that it is valuable to sacrifice some memory for saving time.    Here are some references and knowledge background for this tool:    * The official RDF data cube spec: [The RDF Data Cube Vocabulary](http://www.w3.org/TR/vocab-data-cube/)    * Jena API: [Apache Jena](http://jena.apache.org/index.html)    * The official SPARQL spec: [SPARQL 1.1 Query Language](http://www.w3.org/TR/sparql11-query/)    ### Updates in the latest release 0.9.9    1.  Rewrote some functions to boost the performance on validating constraints 11 and 12, which occupies more than 99% computation time among all constraints. Now NoSPA is capable of handling data cube with million level observations.    2.  Added a progress monitor for the validation of 11 and 12.    ### Requirements    JDK (>=5) and Maven if you want to compile by yourself    or     JVM (>=5) if you want to execute a jar directly    ### Installation    This tool is written in Java and managed by Maven so you can compile it easily by yourself. The first thing you need to do is ``git clone`` this repository.    *Updates: now the packaged jar files are already uploaded and can be found at the release page so you don't need to do it by yourself any more*    Then you need to do a ``mvn package`` at the root directory of this repository and find the jar file at ``NoSPA-RDF-Data-Cube-Validator/target/nospa-rdf-data-cube-validator-0.9.9.jar``. Note that in this case the library for Jena and Log4j is not included in this package.    In the case that you need to run this package independently, you will need to do a ``mvn package assembly:single`` at the root directory of this repository and find the jar file at ``NoSPA-RDF-Data-Cube-Validator/target/nospa-rdf-data-cube-validator-0.9.9-jar-with-dependencies.jar``, which includes all the required libraries to run it.    ### Validation    Basically, there are 3 ways to use it:    1.  Use an IDE to hack into the code by yourself and run the Main class, without making any packages.    2.  In the case that you need to integrate it into your own project, you have to import the package ``rdf-data-cube-validator-0.9.9.jar``, create a new validator instance and call the functions to normalize and validate the cube.        ``Validator validator = ValidatorFactory.createValidator(""NOSPA"", inputPath, inputFormat);``            ``validator.normalize();``            ``validator.validateAll();``        The first argument for the createValidaotr method is the type of validator. Options are ""NOSPA"" and ""SPARQL"" since they are implemented in this software. The ``inputPath`` is the path of the cube file and ``inputFormat`` indicates the RDF format of the cube file such as RDF/XML, N3, TURTLE, N-TRIPLES, etc.        You may also want to check constraints selectively, in that case you cannot use the ValidatorFactory because the two types of validator have different implementions to validate constraints individually and it is a bit difficulty to unify them with an interface. For example, validate with NoSPA validator:            ``NospaValidator nospaValidator = new NospaValidator(inputPath, inputFormat);``            ``nospaValidator.normalize();``            ``nospaValidator.validateIC1();``            ``nospaValidator.validateIC20_21();``            Validate with SPARQL validator:            ``SparqlValidator sparqlValidator = new SparqlValidator(inputPath, inputFormat);``            ``sparqlValidator.normalize();``            ``sparqlValidator.validate(""IC1"");``            ``sparqlValidator.validateIC20_21(""IC20"");``            You will know why there is such difference if you can take a look at the code. Maybe I will get better ideas to unify them in the future. Besides, please make sure that you have normalized the cube before checking constraints if it is in the abbreviated form. You don't need to normalize it if you are sure that it is in the normalized form.        Note that the validation result of this tool will be recorded as logs so you need to turn on the logs for this package in the log configuration of your own project. Additionally you have to set a system property ``current.timestamp`` with the value of current time as part of the name of the validation result. Finally, the validation result can be found at ``${user.dir}/validation_result_${current.timestamp}.md``.    3.  In the case that you need to validate the cube file manually and independently, you need to run ``java -jar nospa-rdf-data-cube-validator-0.9.9-jar-with-dependencies.jar <cube-file.(xml|rdf|nt|n3|ttl)> <(nospa|sparql)>``, where the first argument is the file path of the cube to be validated and the second argument is the name of validator respectively. Currently only 5 RDF format are supported, as can be seen from the file extension name. The validator can be ""nospa"" power by this tool, or ""sparql"" which runs the official validation SPARQL queries against the cube with Jena ARQ.    ### Performance    The constraint check IC-12, ""No duplicate observations"", is the most time-consuming procedure for the entire validation. The motivation of developing this tool is mainly to tackle this issue.     Test file: a data cube containing 13970 observations    Test environment: Ubuntu 14.04 with VMWare, 2 CPU cores of I5-2450M @ 2GHz, 2 GB memory, ordinary HHD    Time consumption for validating IC-12:      * Validation by SPARQL queries with Virtuoso: 1 hour 22 min      * Validation by SPARQL queries with Jena Parser: 58 min      * Validation by NoSPA: 10 sec    *Updates for the performance of the latest release:*    Test file: a 230MB cube file including 540K observations    Test environment: A Web server with 4 Intel(R) Xeon(R) CPUs E5-2630L 0 @ 2.00GHz and 8 GB memory    Time consumption: 52 sec    ### Prospects    Due to lack of faluty datasets, my tests may not cover all cases. Please give me any feedback and suggestion if you are using this software so I can keep improving its quality.    I am still working on some minor changes related to the functionalities. I am planning to make a fair front end when it gets stabilized. """
Semantic web;https://github.com/oxigraph/oxigraph;"""Oxigraph  ========    [![Latest Version](https://img.shields.io/crates/v/oxigraph.svg)](https://crates.io/crates/oxigraph)   [![Released API docs](https://docs.rs/oxigraph/badge.svg)](https://docs.rs/oxigraph)  [![PyPI](https://img.shields.io/pypi/v/pyoxigraph)](https://pypi.org/project/pyoxigraph/)  [![npm](https://img.shields.io/npm/v/oxigraph)](https://www.npmjs.com/package/oxigraph)  [![actions status](https://github.com/oxigraph/oxigraph/workflows/build/badge.svg)](https://github.com/oxigraph/oxigraph/actions)  [![dependency status](https://deps.rs/repo/github/oxigraph/oxigraph/status.svg)](https://deps.rs/repo/github/oxigraph/oxigraph)  [![Gitter](https://badges.gitter.im/oxigraph/community.svg)](https://gitter.im/oxigraph/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)  [![Twitter URL](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Ftwitter.com%2Foxigraph)](https://twitter.com/oxigraph)    Oxigraph is a graph database implementing the [SPARQL](https://www.w3.org/TR/sparql11-overview/) standard.    Its goal is to provide a compliant, safe, and fast graph database based on the [RocksDB](https://rocksdb.org/) key-value store.  It is written in Rust.  It also provides a set of utility functions for reading, writing, and processing RDF files.    Oxigraph is in heavy development and SPARQL query evaluation has not been optimized yet.  The development roadmap is using [GitHub milestones](https://github.com/oxigraph/oxigraph/milestones?direction=desc&sort=completeness&state=open).  Oxigraph internal design [is described on the wiki](https://github.com/oxigraph/oxigraph/wiki/Architecture).    It is split into multiple parts:  * [The database written as a Rust library](https://crates.io/crates/oxigraph). Its source code is in the `lib` directory.  [![Latest Version](https://img.shields.io/crates/v/oxigraph.svg)](https://crates.io/crates/oxigraph)   [![Released API docs](https://docs.rs/oxigraph/badge.svg)](https://docs.rs/oxigraph)  * [`pyoxigraph` that exposes Oxigraph to the Python world](https://oxigraph.org/pyoxigraph/). Its source code is in the `python` directory. [![PyPI](https://img.shields.io/pypi/v/pyoxigraph)](https://pypi.org/project/pyoxigraph/)  * [JavaScript bindings for Oxigraph](https://www.npmjs.com/package/oxigraph). WebAssembly is used to package Oxigraph into a NodeJS compatible NPM package. Its source code is in the `js` directory.  [![npm](https://img.shields.io/npm/v/oxigraph)](https://www.npmjs.com/package/oxigraph)  * [Oxigraph server](https://crates.io/crates/oxigraph_server) that provides a standalone binary of a web server implementing the [SPARQL 1.1 Protocol](https://www.w3.org/TR/sparql11-protocol/) and the [SPARQL 1.1 Graph Store Protocol](https://www.w3.org/TR/sparql11-http-rdf-update/). Its source code is in the `server` directory.  [![Latest Version](https://img.shields.io/crates/v/oxigraph_server.svg)](https://crates.io/crates/oxigraph_server)   [![Docker Image Version (latest semver)](https://img.shields.io/docker/v/oxigraph/oxigraph?sort=semver)](https://hub.docker.com/r/oxigraph/oxigraph)    Oxigraph implements the following specifications:  * [SPARQL 1.1 Query](https://www.w3.org/TR/sparql11-query/), [SPARQL 1.1 Update](https://www.w3.org/TR/sparql11-update/), and [SPARQL 1.1 Federated Query](https://www.w3.org/TR/sparql11-federated-query/).  * [Turtle](https://www.w3.org/TR/turtle/), [TriG](https://www.w3.org/TR/trig/), [N-Triples](https://www.w3.org/TR/n-triples/), [N-Quads](https://www.w3.org/TR/n-quads/), and [RDF XML](https://www.w3.org/TR/rdf-syntax-grammar/) RDF serialization formats for both data ingestion and retrieval using the [Rio library](https://github.com/oxigraph/rio).  * [SPARQL Query Results XML Format](http://www.w3.org/TR/rdf-sparql-XMLres/), [SPARQL 1.1 Query Results JSON Format](https://www.w3.org/TR/sparql11-results-json/) and [SPARQL 1.1 Query Results CSV and TSV Formats](https://www.w3.org/TR/sparql11-results-csv-tsv/).    A preliminary benchmark [is provided](bench/README.md). There is also [a document describing Oxigraph technical architecture](https://github.com/oxigraph/oxigraph/wiki/Architecture).      ## Help    Feel free to use [GitHub discussions](https://github.com/oxigraph/oxigraph/discussions) or [the Gitter chat](https://gitter.im/oxigraph/community) to ask questions or talk about Oxigraph.  [Bug reports](https://github.com/oxigraph/oxigraph/issues) are also very welcome.    If you need advanced support or are willing to pay to get some extra features, feel free to reach out to [Tpt](https://github.com/Tpt/).      ## License    This project is licensed under either of     * Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or     http://www.apache.org/licenses/LICENSE-2.0)   * MIT license ([LICENSE-MIT](LICENSE-MIT) or     http://opensource.org/licenses/MIT)       at your option.      ### Contribution    Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in Oxigraph by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions. """
Semantic web;https://github.com/sparql-generate/sparql-generate;"""# SPARQL-Generate    This project contains the sources of the implementations of SPARQL-Generate and STTL (a.k.a. SPARQL-Template) over Apache Jena.  """
Semantic web;https://github.com/Wimmics/gephi-semantic-web-import;"""# Gephi Plugins    This repository is an out-of-the-box development environment for Gephi plugins. Gephi plugins are implemented in Java and can extend [Gephi](https://gephi.org) in many different ways, adding or improving features. Getting started is easy with this repository but also checkout the [Bootcamp](https://github.com/gephi/gephi-plugins-bootcamp) for examples of plugins you can create.     ## Migrate Gephi 0.8 plugins    The process in which plugins are developed and submitted had an overhaul when Gephi 0.9 was released. Details can be read on this article: [Plugin development gets new tools and opens-up to the community](https://gephi.wordpress.com/2015/12/16/plugin-development-gets-new-tools-and-opens-up-to-the-community/).    This section is a step-by-step guide to migrate 0.8 plugins. Before going through the code and configuration, let's summerize the key differences between the two environements.    - The 0.8 base is built using Ant, whereas the 0.9 uses Maven. These two are significantly different. If you aren't familiar with Maven, you can start with [Maven in 5 Minutes]( https://maven.apache.org/guides/getting-started/maven-in-five-minutes.html). Maven configurations are defined in the `pom.xml` files.  - The 0.8 base finds the Gephi modules into the `platform` folder checked in the repository, whereas the 0.9 base downloads everything from the central Maven repository, where all Gephi modules are available.  - Maven requires to separate source files (e.g. .java) and resources files (e.g. .properties) into distinct folders. Sources are located in `src/main/java` and resources in `src/main/resources`.    A custom `migrate` goal is available in the [Gephi Maven Plugin](https://github.com/gephi/gephi-maven-plugin) to facilitate the migration from 0.8 to 0.9. This automated process migrates ant-based plugins to maven and takes care of copying the configuration and code. Follow these steps to migrate your plugin:    - Fork and checkout this repository:            git clone git@github.com:username/gephi-plugins.git    If you've already had a forked repository based on 0.8 we suggest to save your code somewhere, delete it and fork again as the history was cleared.    - Copy your plugin folder at the root of this directory.    - Run this command:            mvn org.gephi:gephi-maven-plugin:migrate    This command will detect the ant-based plugin and migrate it. The resulting folder is then located into the `modules` folder.    The plugin code can then be inspected in Netbeans or built via command line with `mvn clean package`.    ## Get started    ### Requirements    Developing Gephi plugins requires [JDK 7](http://www.oracle.com/technetwork/java/javase/downloads/index.html) or later and [Maven](http://maven.apache.org/). Although any IDE/Editor can be used, [Netbeans IDE](https://netbeans.org/) is recommend as Gephi itself is based on [Netbeans Platform](https://netbeans.org/features/platform/index.html).    ### Create a plugin    The creation of a new plugin is simple thanks to our custom [Gephi Maven Plugin](https://github.com/gephi/gephi-maven-plugin). The `generate` goal asks a few questions and then configures everything for you.    - Fork and checkout the latest version of this repository:            git clone git@github.com:username/gephi-plugins.git  - Run the following command and answer the questions:            mvn org.gephi:gephi-maven-plugin:generate    This is an example of what this process will ask:            Name of organization (e.g. my.company): org.foo          Name of artifact (e.g my-plugin): my-plugin          Version (e.g. 1.0.0): 1.0.0          Directory name (e.g MyPlugin): MyPlugin          Branding name (e.g My Plugin): My Plugin          Category (e.g Layout, Filter, etc.): Layout          Author: My Name          Author email (optional):          Author URL (optional):          License (e.g Apache 2.0): Apache 2.0          Short description (i.e. one sentence): Plugin catch-phrase          Long description (i.e multiple sentences): Plugin features are great          Would you like to add a README.md file (yes|no): yes    The plugin configuration is created. Now you can (in any order):    - Add some Java code in the `src/main/java` folder of your plugin  - Add some resources (e.g. Bundle.properties, images) into the `src/main/resources/` folder of your plugin  - Change the version, author or license information into the `pom.xml` file, which is in your plugin folder  - Edit the description or category details into the `src/main/nbm/manifest.mf` file in your plugin folder     ### Build a plugin    Run the following command to compile and build your plugin:           mvn clean package    In addition of compiling and building the JAR and NBM, this command uses the `Gephi Maven Plugin` to verify the plugin's configuration. In care something is wrong it will fail and indicte the reason.    ### Run Gephi with plugin    Run the following command to run Gephi with your plugin pre-installed. Make sure to run `mvn package` beforehand to rebuild.           mvn org.gephi:gephi-maven-plugin:run    In Gephi, when you navigate to `Tools` > `Plugins` you should see your plugin listed in `Installed`.    ## Submit a plugin    Submitting a Gephi plugin for approval is a simple process based on GitHub's [pull request](https://help.github.com/articles/using-pull-requests/) mechanism.    - First, make sure you're working on a fork of [gephi-plugins](https://github.com/gephi/gephi-plugins). You can check that by running `git remote -v` and look at the url, it should contain your GitHub username, for example `git@github.com:username/gephi-plugins.git`.    - Add and commit your work. It's recommended to keep your fork synced with the upstream repository, as explained [here](https://help.github.com/articles/syncing-a-fork/), so you can run `git merge upstream/master` beforehand.    - Push your commits to your fork with `git push origin master`.    - Navigate to your fork's URL and create a pull request. Select `master-forge` instead of `master` as base branch.    - Submit your pull request.    ## Update a plugin    Updating a Gephi plugin has the same process as submiting it for the first time. Don't forget to merge from upstream's master branch.    ## IDE Support    ### Netbeans IDE    - Start Netbeans and go to `File` and then `Open Project`. Navigate to your fork repository, Netbeans automatically recognizes it as Maven project.   - Each plugin module can be found in the `Modules` folder.    To run Gephi with your plugin pre-installed, right click on the `gephi-plugins` project and select `Run`.    To debug Gephi with your plugin, right click on the `gephi-plugins` project and select `Debug`.    ### IntelliJ IDEA    - Start IntelliJ and `Open` the project by navigating to your fork repository. IntelliJ may prompt you to import the Maven project, select yes.    To run Gephi with your plugin pre-installed when you click `Run`, create a `Maven` run configuration and enter `org.gephi:gephi-maven-plugin:run` in the command field. The working directory is simply the current project directory.    To debug Gephi with your plugin, create a `Remote` configuration and switch the `Debugger mode` option to `Listen`. Then create a `Maven` run configuration like abobe but add `-Drun.params.debug=""-J-Xdebug -J-Xnoagent -J-Xrunjdwp:transport=dt_socket,suspend=n,server=n,address=5005""` into the `Runner` > `VM Options` field. Then, go to the `Run` menu and first run debug with the remote configuration and then only run debug with the Maven configuration.    ## FAQ    #### What kind of plugins can I create?    Gephi can be extended in many ways but the major categories are `Layout`, `Export`, `Import`, `Data Laboratory`, `Filter`, `Generator`, `Metric`, `Preview`, `Tool`, `Appearance` and `Clustering`. A good way to start is to look at examples with the [bootcamp](https://github.com/gephi/gephi-plugins-bootcamp).    #### In which language can plugins be created?    Plugins can use any JVM languages (e.g. Scala, Python, Groovy) but the default option is Java.     #### Can native librairies be used?    Yes, native librairies can be used in modules.    #### How is this repository structured?    The `modules` folder is where plugin modules go. Each plugin is defined in a in single folder in this directory. A plugin can be composed of multiple modules (it's called a suite then) but usually one is enough to do what you want.    The `pom.xml` file in `modules` is the parent pom for plugins. A Maven pom can inherit configurations from a parent and that is something we use to keep each plugin's pom very simple. Notice that each plugin's pom (i.e. the `pom.xml` file in the plugin folder) has a `<parent>` defined.    The `pom.xml` file at the root folder makes eveything fit together and notably lists the modules.    #### How are the manifest settings defined?    There are two options. The first option is what the `generate` task does: it puts entries `OpenIDE-Module-Short-Description`, `OpenIDE-Module-Long-Description`, `OpenIDE-Module-Display-Category` and `OpenIDE-Module-Name` into the `src/main/nbm/manifest.mf` file. The second option sets a `  OpenIDE-Module-Localizing-Bundle` entry into the `manifest.mf` so values are defined elsewhere in `Bundle.properties` file. The value is then simply the path to the file (e.g. `OpenIDE-Module-Localizing-Bundle: org/project/Bundle.properties`).    The second option is preferable when the short or long description have too many characters as the manifest format is pretty restrictive.      #### How to add a new module?    This applies for suite plugins with multiple modules. Besides creating the module folder, edit the `pom.xml` file and add the folder path to `<modules>`, like in this example:    ```      <!-- List of modules -->      <modules>          <!-- Add here the paths of all modules (e.g. <module>modules/MyModule</module>) -->          <module>modules/ExampleModule</module>       </modules>  ```    #### Where are dependencies configured?    Dependencies are configured in the `<dependencies>` section in the plugin folder's `pom.xml`. Each dependency has a `groupId`, an `artifactId` and a `version`. There are three types of dependencies a plugin can have: an external library, a Gephi module or a Netbeans module.    The list of Gephi and Netbeans dependencies one can use can be found in the `modules/pom.xml` file. All possible dependencies are listed in the `<dependencyManagement>` section. Because each plugin module inherits from this parent pom the version can be omitted when the dependency is set. For instance, this is how a plugin depends on `GraphAPI` and Netbeans's `Lookup`.    ```  <dependencies>       <dependency>           <groupId>org.netbeans.api</groupId>           <artifactId>org-openide-util-lookup</artifactId>       </dependency>       <dependency>           <groupId>org.gephi</groupId>           <artifactId>graph-api</artifactId>      </dependency>  </dependencies>  ```    #### What are public packages for?    This applies for suite plugins with multiple modules. A module should declare the packages it wants to nake accessible to other modules. For instance, if a module `B` depends on the class `my.org.project.ExampleController` defined in a module `A`, the `A` module should declare `my.org.project` as public package.    Public packages are configured in the module's `pom.xml` file. Edit the `<publicPackages>` entry. Example:    ```  <publicPackages>      <publicPackage>my.org.project</publicPackage>  </publicPackages>  ```    #### What is the difference between plugin and module?    It's the same thing. We say module because Gephi is a modular application and is composed of many independent modules. Plugins also are modules but we call them plugin because they aren't in the _core_ Gephi.    #### When running the plugin in Netbeans I get an error ""Running standalone modules or suites requires...""    This error appears when you try to run a module. To run Gephi with your plugin you need to run the `gephi-plugins` project, not your module. """
Semantic web;https://github.com/ucbl/HyLAR-Reasoner;"""# HyLAR-Reasoner ![HyLAR icon](https://raw.githubusercontent.com/ucbl/HyLAR-Reasoner/master/hylar-icon.png)     A rule-based incremental reasoner for the Web.    To cite HyLAR: [HyLAR+: improving Hybrid Location-Agnostic Reasoning  with Incremental Rule-based Update](https://hal.archives-ouvertes.fr/hal-01276558/file/Demo_www2016.pdf)    ## Table of contents    - [Description](#description)  - [Use HyLAR locally](#use-hylar-locally)  - [Use HyLAR in a browser](#use-hylar-in-a-browser)  - [Use HyLAR as a server](#use-hylar-as-a-server)  - [Supported Inferences](#supported-inferences)  - [Publications](#publications)      ## Description    HyLAR is a **Hy**brid **L**ocation-**A**gnostic incremental **R**easoner that uses known rdf-based librairies such as rdfstore.js, sparqljs and rdf-ext while providing an additional incremental reasoning engine. HyLAR can be either used locally as a npm module or globally as a server, and comes with a browserified version.    HyLAR relies on the rdfstore.js triplestore and therefore supports JSON-LD, N3 and Turtle serializations.  SPARQL support is detailed [here](https://github.com/antoniogarrote/rdfstore-js#sparql-support). The inferences initially supported by HyLAR are described [at the bottom of this page](#supported-inferences). HyLAR supports custom business rules.    ## Use HyLAR locally    ### Installation    To use HyLAR locally, just launch  `npm install --save hylar`    ### Loading an ontology    Import HyLAR, then classify your ontology and query it using `load()`,  which takes three parameters:  - rawOntology: A string, the raw ontology.  - mimeType: A string, either `text/turtle`, `text/n3` or `application/ld+json`.  - keepOldValues: A boolean: true to keep old values while classfying, false to overwrite the KB. Default is **false**.    ```javascript  const Hylar = require('hylar');  const h = new Hylar();        // async function  h.load(rawOntology, mimeType, keepOldValues);  ```    ### Querying an ontology    Once loaded, HyLAR is able to process SPARQL queries using `query()`, with the following parameters:    - query: A string, the SPARQL query    ```javascript  let results = await h.query(query);  ```    ### Create your own rules    HyLAR supports insertion of custom forward-chaining conjunctive rules in the form:  ```  triple_head_1 ^ ... ^ triple_head_n -> triple_body_3  ```  Where `triple_head_x` and `triple_body_x` are respectively ""cause"" triples (*i.e.* the input) and ""consequence"" triples (*i.e.* the inferred output) in the form:  ```  (subject predicate object)  ```  Each subject/predicate/object can be one of the following:  - A variable, *e.g.* `?x`  - An URI, *e.g.* `http://www.w3.org/2000/01/rdf-schema#subClassOf`  - A literal, *e.g.* `""0.5""`, `""Hello world!""`    A predicate can also be any of these comparison operators: `<`, `>`, `=`, `<=`, `=>`.    **Rule add example (first param: the 'raw' rule, second param: the rule name)**    ```javascript  h.parseAndAddRule('(?p1 http://www.w3.org/2002/07/owl#inverseOf ?p2) ^ (?x ?p1 ?y) -> (?y ?p2 ?x)', 'inverse-1');  ```  **Rule removal example (first and only param: either the rule name or the raw rule)**    ```javascript  h.removeRule('inverse-1');  // Outputs ""[HyLAR] Removed rule (?p1 inverseOf ?p2) ^ (?x ?p1 ?y) -> (?y ?p2 ?x)"" if succeeded.  ```    ## Use HyLAR in a browser    Run `npm run clientize`, which will generate the file `hylar-client.js`.  Include this script in your page with this line:  ```html  <script src=""path-to/hylar-client.js""></script>  ```  As in the node module version, you can instantiate HyLAR with `const h = new Hylar();` and call the same methods `query()`, `load()` and `parseAndAddRule()`.    ## Use HyLAR as a server    ### Installation    `npm install -g hylar`    ### Run the server    Command `hylar` with the following optional parameters    - `--port <port_number>` (port 3000 by default)  - `--no-persist` deactivates database persistence (activated by default)  - `--graph-directory </your/base/graph/directory/>` where local datasets are saved  - `--entailment` either ```OWL2RL``` (default) or ```RDFS```  - `--reasoning-method` either `incremental` (default) or `tag-based` (provides *reasoning proofs*)    ### Hylar server API    - `/classify/{FILE_NAME}` (GET)    Loads, parses and classify the file `{FILE_NAME}` from the ontology directory.  > **Note:** You don't have to specify the ontology file's mimetype as it is detected automatically using its extension.    - `/classify/` (GET)    Allows classifying an ontology as a string, which requires its original serialization type.  > **Body parameters**   >`filename` the absolute path of the ontology file to be processed.  > `mimetype` the serialization of the ontology (mimetype, one of text/turtle, text/n3 or application/ld+json).    - `/query`(GET)    SPARQL queries your loaded ontology as does `Hylar.query()`.    > **Body parameters**  > `query` the SPARQL query string.    - `/rule` (PUT)    Puts an list of custom rules and adds it to the reasoner.    > **Body parameters**  > `rules` the array of conjunctive rules.    ## Supported inferences    HyLAR supports a subset of OWL 2 RL and RDFS.  - [RDFS](https://www.w3.org/TR/rdf-mt/#RDFSRules)      - Rules:  `rdf1, rdfs2, rdfs3, rdfs4a, rdfs4b, rdfs5, dfs6, rdfs7, rdfs8, rdfs9, rdfs10, rdfs11, rdfs12, rdfs13`.      - Supports all RDFS axiomatic triples, except axioms related to `rdf:Seq` and `rdf:Bag`.      - [OWL 2 RL](https://www.w3.org/TR/owl2-profiles/#Reasoning_in_OWL_2_RL_and_RDF_Graphs_using_Rules)      - Rules: `prp-dom, prp-rng, prp-fp, prp-ifp, prp-irp, prp-symp, prp-asyp, prp-trp, prp-spo1, prp-spo2, prp-eqp1, prp-eqp2, prp-pdw, prp-inv1, prp-inv2, prp-npa1, prp-npa2, cls-nothing2, cls-com, cls-svf1, cls-svf2, cls-avf, cls-hv1, cls-hv2, cls-maxc1, cls-maxc2, cls-maxqc1, cls-maxqc2, cls-maxqc3, cls-maxqc4, cax-sco, cax-eqc1, cax-eqc2, cax-dw, scm-cls, scm-sco, scm-eqc1, scm-eqc2, scm-op, scm-dp, scm-spo, scm-eqp1, scm-eqp2, scm-dom1, scm-dom2, scm-rng1, scm-rng2, scm-hv, scm-svf1, scm-svf2, scm-avf1, scm-avf2`      - Axiomatic triples are *not yet* supported.    ## Publications    ### Location-agnostic mechanism    Terdjimi, M., Médini, L., & Mrissa, M. (2015, May). [Hylar: Hybrid location-agnostic reasoning 📚](https://hal.archives-ouvertes.fr/hal-01154549/file/hylar.pdf) In ESWC Developers Workshop 2015 (p. 1).    ### Incremental reasoning on the Web with HyLAR    Terdjimi, M., Médini, L., & Mrissa, M. (2016, April). [HyLAR+: improving hybrid location-agnostic reasoning with incremental rule-based update 📚](https://hal.archives-ouvertes.fr/hal-01276558/file/Demo_www2016.pdf) In Proceedings of the 25th International Conference Companion on World Wide Web (pp. 259-262). International World Wide Web Conferences Steering Committee.     ### Tag-based maintenance    Terdjimi, M., Médini, L., & Mrissa, M. (2018, April). [Web Reasoning Using Fact Tagging 📚](http://mmrissa.perso.univ-pau.fr/pub/Accepted-papers/2018-TheWebConf-RoD.pdf) In Companion of the The Web Conference 2018 on The Web Conference 2018 (pp. 1587-1594). International World Wide Web Conferences Steering Committee.   """
Semantic web;https://github.com/lanthaler/HydraBundle;"""HydraBundle  ==============    Hydra is a lightweight vocabulary to create hypermedia-driven Web APIs. By  specifying a number of concepts commonly used in Web APIs it renders the  creation of generic API clients possible.    This is a [Symfony2](http://www.symfony.com/) bundle which shows how easily  Hydra can be integrated in modern Web frameworks. It acts as a proof of  concept to show how Hydra can simplify the implementation of interoperable  and evolvable RESTful APIs.    ***WARNING: This is highly experimental stuff that isn't ready for  production use yet.***    To participate in the development of this bundle, please file bugs and  issues in the issue tracker or submit pull requests. If you have questions  regarding Hydra in general, join the  [Hydra W3C Community Group](http://bit.ly/HydraCG).    You can find an online demo of this bundle as well as more information about  Hydra on my homepage:  http://www.markus-lanthaler.com/hydra      Installation  ------------    You can install this bundle by running        composer require ml/hydra-bundle dev-master    or by adding the package to your composer.json file directly    ```json  {      ""require"": {          ""ml/hydra-bundle"": ""dev-master""      }  }  ```    After you have installed the package, you just need to add the bundle  to your `AppKernel.php` file:    ```php  // in AppKernel::registerBundles()  $bundles = array(      // ...      new ML\HydraBundle\HydraBundle(),      // ...  );  ```    and import the routes in your `routing.yml` file:    ```yaml  hydra:      resource: ""@HydraBundle/Controller/""      type:     annotation      prefix:   /  ```      Credits  ------------    This bundle heavily uses the  [Doctrine Common project](http://www.doctrine-project.org/projects/common.html)  and is inspired by its  [object relational mapper](http://www.doctrine-project.org/projects/orm.html).  The code generation is based on Sensio's  [SensioGeneratorBundle](https://github.com/sensio/SensioGeneratorBundle). """
Semantic web;https://github.com/hyrise/hyrise;"""[![Build Status](https://hyrise-ci.epic-hpi.de/buildStatus/icon?job=Hyrise/hyrise/master)](https://hyrise-ci.epic-hpi.de/blue/organizations/jenkins/hyrise%2Fhyrise/activity/)  [![Coverage Status](https://hyrise-coverage-badge.herokuapp.com/coverage_badge.svg)](https://hyrise-ci.epic-hpi.de/job/Hyrise/job/hyrise/job/master/lastStableBuild/Llvm-cov_5fReport/)  [![CodeFactor](https://www.codefactor.io/repository/github/hyrise/hyrise/badge)](https://www.codefactor.io/repository/github/hyrise/hyrise)    # Welcome to Hyrise    Hyrise is a research in-memory database system that has been developed [by HPI since 2009](https://www.vldb.org/pvldb/vol4/p105-grund.pdf) and has been entirely [rewritten in 2017](https://openproceedings.org/2019/conf/edbt/EDBT19_paper_152.pdf). Our goal is to provide a clean and flexible platform for research in the area of in-memory data management. Its architecture allows us, our students, and other researchers to conduct experiments around new data management concepts. To enable realistic experiments, Hyrise features comprehensive SQL support and performs powerful query plan optimizations. Well-known benchmarks, such as TPC-H or TPC-DS, can be executed with a single command and without any preparation.    This readme file focuses on the technical aspects of the repository. For more background on our research and for a list of publications, please visit the [Hyrise project page](https://hpi.de/plattner/projects/hyrise.html).    You can still find the (archived) previous version of Hyrise on [Github](https://github.com/hyrise/hyrise-v1).    ## Citation    When referencing this version of Hyrise, please use the following bibtex entry:  <details><summary>(click to expand)</summary>      ```bibtex  @inproceedings{DBLP:conf/edbt/DreselerK0KUP19,    author    = {Markus Dreseler and                 Jan Kossmann and                 Martin Boissier and                 Stefan Klauck and                 Matthias Uflacker and                 Hasso Plattner},    editor    = {Melanie Herschel and                 Helena Galhardas and                 Berthold Reinwald and                 Irini Fundulaki and                 Carsten Binnig and                 Zoi Kaoudi},    title     = {Hyrise Re-engineered: An Extensible Database System for Research in                 Relational In-Memory Data Management},    booktitle = {Advances in Database Technology - 22nd International Conference on                 Extending Database Technology, {EDBT} 2019, Lisbon, Portugal, March                 26-29, 2019},    pages     = {313--324},    publisher = {OpenProceedings.org},    year      = {2019},    url       = {https://doi.org/10.5441/002/edbt.2019.28},    doi       = {10.5441/002/edbt.2019.28},    timestamp = {Mon, 18 Mar 2019 16:09:00 +0100},    biburl    = {https://dblp.org/rec/conf/edbt/DreselerK0KUP19.bib},    bibsource = {dblp computer science bibliography, https://dblp.org}  }  ```  </details>    ## Supported Systems  Hyrise is developed for Linux (preferrably the most current Ubuntu version) and optimized to run on server hardware. We support Mac to facilitate the local development of Hyrise, but do not recommend it for benchmarking.    ## Supported Benchmarks  We support a number of benchmarks out of the box. This makes it easy to generate performance numbers without having to set up the data generation, loading CSVs, and finding a query runner. You can run them using the `./hyriseBenchmark*` binaries.    | Benchmark  | Notes                                                                                                                    |  | ---------- | ------------------------------------------------------------------------------------------------------------------------ |  | TPC-DS     | [Query Plans](https://hyrise-ci.epic-hpi.de/job/hyrise/job/hyrise/job/master/lastStableBuild/artifact/query_plans/tpcds) |  | TPC-H      | [Query Plans](https://hyrise-ci.epic-hpi.de/job/hyrise/job/hyrise/job/master/lastStableBuild/artifact/query_plans/tpch)  |  | Join Order | [Query Plans](https://hyrise-ci.epic-hpi.de/job/hyrise/job/hyrise/job/master/lastStableBuild/artifact/query_plans/job)   |  | JCC-H      | Call the hyriseBenchmarkTPCH binary with the -j flag.                                                                    |   | TPC-C      | In development, no proper optimization done yet                                                                          |    # Getting started    *Have a look at our [contributor guidelines](CONTRIBUTING.md)*.    You can find definitions of most of the terms and abbreviations used in the code in the [glossary](GLOSSARY.md). If you cannot find something that you are looking for, feel free to open an issue.    The [Step by Step Guide](https://github.com/hyrise/hyrise/wiki/Step-by-Step-Guide) is a good starting point to get to know Hyrise.    ## Native Setup  You can install the dependencies on your own or use the install_dependencies.sh script (**recommended**) which installs all of the therein listed dependencies and submodules.  The install script was tested under macOS Big Sur (10.16) and Ubuntu 20.10 (apt-get).    See [dependencies](DEPENDENCIES.md) for a detailed list of dependencies to use with `brew install` or `apt-get install`, depending on your platform. As compilers, we generally use the most recent version of clang and gcc (Linux only). Please make sure that the system compiler points to the most recent version or use cmake (see below) accordingly.  Older versions may work, but are neither tested nor supported.    **Note about LLVM 13 and TBB 2021:** Hyrise can currently not be built with LLVM 13. We hope to get LLVM 13 running soon. For TBB, please use a `2020*` version until https://github.com/oneapi-src/oneTBB/issues/378 is resolved. On MacOS with brew, LLVM 12 and TBB 2020 can be installed as follows: `brew install tbb@2020 && brew install llvm@12`. Keep in mind that these package versions are alternate versions and, thus, not symlinked into `/usr/local`. `brew link tbb@2020` and `brew link llvm@12` symlinks these packages.    ## Setup using Docker  If you want to create a Docker-based development environment using CLion, head over to our [dedicated tutorial](https://github.com/hyrise/hyrise/wiki/Use-Docker-with-CLion).     Otherwise, to get all dependencies of Hyrise into a Docker image, run  ```  docker build -t hyrise .  ```    You can start the container via  ```  docker run -it hyrise  ```    Inside the container, you can then checkout Hyrise and run `./install_dependencies.sh` to download the required submodules.    ## Building and Tooling  It is highly recommended to perform out-of-source builds, i.e., creating a separate directory for the build.  Advisable names for this directory would be `cmake-build-{debug,release}`, depending on the build type.  Within this directory call `cmake ..` to configure the build.  By default, we use very strict compiler flags (beyond `-Wextra`, including `-Werror`). If you use one of the officially supported environments, this should not be an issue. If you simply want to test Hyrise on a different system and run into issues, you can call `cmake -DHYRISE_RELAXED_BUILD=On ..`, which will disable these strict checks.  Subsequent calls to CMake, e.g., when adding files to the build will not be necessary, the generated Makefiles will take care of that.    ### Compiler choice  CMake will default to your system's default compiler.  To use a different one, call `cmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ ..` in a clean build directory. See [dependencies](DEPENDENCIES.md) for supported compiler versions.    ### Unity Builds  Starting with cmake 3.16, you can use `-DCMAKE_UNITY_BUILD=On` to perform unity builds. For a complete (re-)build or when multiple files have to be rebuilt, these are usually faster, as the relative cost of starting a compiler process and loading the most common headers is reduced. However, this only makes sense for debug builds. See our [blog post](https://medium.com/hyrise/reducing-hyrises-build-time-8523135aed72) on reducing the compilation time for details.    ### ccache  For development, you may want to use [ccache](https://ccache.samba.org/), which reduces the time needed for recompiles significantly. Especially when switching branches, this can reduce the time to recompile from several minutes to one or less. On the downside, we have seen random build failures on our CI server, which is why we do not recommend ccache anymore but merely list it as an option. To use ccache, add `-DCMAKE_CXX_COMPILER_LAUNCHER=ccache` to your cmake call. You will need to [adjust some ccache settings](https://ccache.dev/manual/latest.html#_precompiled_headers) either in your environment variables or in your [ccache config](https://ccache.dev/manual/latest.html#_configuration) so that ccache can handle the precompiled headers. On our CI server, this worked for us: `CCACHE_SLOPPINESS=file_macro,pch_defines,time_macros CCACHE_DEPEND=1`.    ### Build  Simply call `make -j*`, where `*` denotes the number of threads to use.    Usually debug binaries are created.  To configure a build directory for a release build make sure it is empty and call CMake like `cmake -DCMAKE_BUILD_TYPE=Release`    ### Lint  `./scripts/lint.sh` (Google's cpplint is used for the database code. In addition, we use _flake8_ for linting the Python scripts under /scripts.)    ### Format  `./scripts/format.sh` (clang-format is used for the database code. We use _black_ for formatting the Python scripts under /scripts.)    ### Test  Calling `make hyriseTest` from the build directory builds all available tests.  The binary can be executed with `./<YourBuildDirectory>/hyriseTest`.  Subsets of all available tests can be selected via `--gtest_filter=`.    ### Coverage  `./scripts/coverage.sh` will print a summary to the command line and create detailed html reports at ./coverage/index.html    *Supports only clang on MacOS and only gcc on linux*    ### Address/UndefinedBehavior Sanitizers  `cmake -DENABLE_ADDR_UB_SANITIZATION=ON` will generate Makefiles with AddressSanitizer and Undefined Behavior options.  Compile and run them as normal - if any issues are detected, they will be printed to the console.  It will fail on the first detected error and will print a summary.  To convert addresses to actual source code locations, make sure llvm-symbolizer is installed (included in the llvm package) and is available in `$PATH`.  To specify a custom location for the symbolizer, set `$ASAN_SYMBOLIZER_PATH` to the path of the executable.  This seems to work out of the box on macOS - If not, make sure to have llvm installed.  The binary can be executed with `LSAN_OPTIONS=suppressions=asan-ignore.txt ./<YourBuildDirectory>/hyriseTest`.    `cmake -DENABLE_THREAD_SANITIZATION=ON` will work as above but with the ThreadSanitizer. Some sanitizers are mutually exclusive, which is why we use two configurations for this.    ### Compile Times  When trying to optimize the time spent building the project, it is often helpful to have an idea how much time is spent where.  `scripts/compile_time.sh` helps with that. Get usage instructions by running it without any arguments.    ## Maintainers  - Jan Kossmann  - Marcel Weisgut  - Martin Boissier  - Stefan Halfpap    Contact: firstname.lastname@hpi.de    ## Maintainer emeritus  - Markus Dreseler    ## Contributors  -   Yannick   Bäumer  -   Lawrence  Benson  -   Timo      Djürken  -   Alexander Dubrawski  -   Fabian    Dumke  -   Leonard   Geier  -   Richard   Ebeling  -   Fabian    Engel  -   Moritz    Eyssen  -   Martin    Fischer  -   Christian Flach  -   Pedro     Flemming  -   Mathias   Flüggen  -   Johannes  Frohnhofen  -   Pascal    Führlich  -   Carl      Gödecken  -   Adrian    Holfter  -   Ben       Hurdelhey  -   Sven      Ihde  -   Ivan      Illic  -   Jonathan  Janetzki  -   Michael   Janke  -   Max       Jendruk  -   David     Justen  -   Youri     Kaminsky  -   Marvin    Keller  -   Mirko     Krause  -   Eva       Krebs  -   Sven      Lehmann  -   Till      Lehmann  -   Tom       Lichtenstein  -   Daniel    Lindner  -   Alexander Löser  -   Jan       Mattfeld  -   Arne      Mayer  -   Dominik   Meier  -   Julian    Menzler  -   Torben    Meyer  -   Leander   Neiß  -   Hendrik   Rätz  -   Alexander Riese  -   Johannes  Schneider  -   David     Schumann  -   Simon     Siegert  -   Arthur    Silber  -   Toni      Stachewicz  -   Daniel    Stolpe  -   Jonathan  Striebel  -   Nils      Thamm  -   Hendrik   Tjabben  -   Justin    Trautmann  -   Carsten   Walther  -   Lukas     Wenzel  -   Fabian    Wiebe  -   Tim       Zimmermann """
Semantic web;https://github.com/mifeet/LD-FusionTool;"""LD-FusionTool  ==========  ###Data Fusion and Conflict Resolution tool for Linked Data        LD-FusionTool is a standalone tool (and [a module](https://github.com/mifeet/FusionTool-DPU) for [UnifiedViews](http://unifiedviews.eu) ETL framework) executing the Data Fusion and Conflict Resolution steps in the integration process for RDF, where data are merged to produce consistent and clean representations of objects, and conflicts which emerged during data integration need to be resolved.    **Please visit [the official page of LD-FusionTool](http://mifeet.github.io/LD-FusionTool/) for more information about what LD-FusionTool is, how it works and how you can download it and run.**        Building from sources  ========    In order to use LD-FusionTool, download the sources, build them with Maven (run <code>mvn clean install</code> in the <code>sources</code> directory of the project). Locate the built binaries in <code>sources/odcsft-application/target</code> and execute<br/> <code>java -jar odcsft-application-&lt;version&gt;-executable.jar &lt;configuration-file&gt;.xml</code>. Running the executable without parameters shows more usage options and sample configuration files can be found at <a href=""https://github.com/mifeet/LD-FusionTool/tree/master/examples"">examples</a> (file <code>sample-config-full.xml</code> serves as the working documentation of the configuration file).     """
Semantic web;https://github.com/jpcik/srbench;"""srbench  =======    SRBench SPARQL RDF Bench"""
Semantic web;https://github.com/AKSW/ShacShifter;"""# The ShacShifter    [![Travis CI Build Status](https://travis-ci.org/AKSW/ShacShifter.svg)](https://travis-ci.org/AKSW/ShacShifter/)  [![Coverage Status](https://coveralls.io/repos/github/AKSW/ShacShifter/badge.svg?branch=master)](https://coveralls.io/github/AKSW/ShacShifter?branch=master)    The *ShacShifter* is a shape shifter for the [*Shapes Constraint Language (SHACL)*](https://www.w3.org/TR/shacl/) to various other format.  Currently our focus is on convertig a SHACL NodeShape to an [RDForms template](http://rdforms.org/#!templateReference.md).    ## Installation and Usage    You have to install the python dependencies with `pip install -r requirements.txt`.    To run start with:        $ bin/ShacShifter --help      usage: ShacShifter [-h] [-s SHACL] [-o OUTPUT] [-f {rdforms,wisski,html}]        optional arguments:        -h, --help            show this help message and exit        -s SHACL, --shacl SHACL                              The input SHACL file        -o OUTPUT, --output OUTPUT                              The output file        -f {rdforms,wisski,html}, --format {rdforms,wisski,html}                              The output format """
Semantic web;https://github.com/ktym/d3sparql;"""d3sparql.js  ===========    JavaScript library for executing SPARQL query and transforming resulted JSON for visualization in D3.js.    ### Description    Semantic Web technologies are getting widely used in information sciences along with the Linked Open Data (LOD) initiative and RDF data are exposed at SPARQL endpoints around the world. SPARQL query is used to search those endpoints and the results are obtained as a SPARQL Query Results XML Format or a SPARQL Query Results JSON Format, both are essentially tabular structured data. To effectively represent the SPARQL results, appropriate visualization methods are highly demanded. To create and control dynamic graphical representation of data on the Web, the D3.js JavaScript library is getting popularity as a generic framework based on the widely accepted Web standards such as SVG, JavaScript, HTML5 and CSS. A variety of visualization examples implemented with the D3.js library are already freely available, however, each of them depends on a predefined JSON data structure that differs from the JSON structure returned from SPARQL endpoints. Therefore, it is expected to largely reduce development costs of Semantic Web visualization if a JavaScript library is available which can transform SPARQL Query Results JSON Format into JSON data structures consumed by the D3.js. The d3sparql.js is developed as a generic JavaScript library to fill this gap which can be used to query SPARQL endpoints as an AJAX call and provides various callback functions to visualize the obtained results.    ### Currently supports    * Charts    * barchart, piechart, scatterplot  * Graphs    * force graph, sankey graph  * Trees    * roundtree, dendrogram, treemap, sunburst, circlepack  * Maps    * coordmap, namedmap  * Tables    * htmltable, htmlhash    ### Usage    ```html  <!DOCTYPE html>  <meta charset=""utf-8"">  <html>   <head>    <script src=""http://d3js.org/d3.v3.min.js""></script>    <script src=""d3sparql.js""></script>    <script>    function exec() {      /* Uncomment to see debug information in console */      // d3sparql.debug = true      var endpoint = d3.select(""#endpoint"").property(""value"")      var sparql = d3.select(""#sparql"").property(""value"")      d3sparql.query(endpoint, sparql, render)    }    function render(json) {      /* set options and call the d3spraql.xxxxx function in this library ... */      var config = {  	  ""selector"": ""#result""  	}      d3sparql.xxxxx(json, config)    }    </script>    <style>    <!-- customize CSS -->    </style>   </head>   <body onload=""exec()"">    <form style=""display:none"">     <input id=""endpoint"" value=""http://dbpedia.org/sparql"" type=""text"">     <textarea id=""sparql"">      PREFIX ...      SELECT ...      WHERE { ... }     </textarea>    </form>    <div id=""result""></div>   </body>  </html>  ```    ### Live demo    * http://biohackathon.org/d3sparql    ### Codebase    * https://github.com/ktym/d3sparql    ### Publication    * http://ceur-ws.org/Vol-1320/paper_39.pdf    ### Presentation    * http://www.slideshare.net/ToshiakiKatayama/d3sparqljs-demo-at-swat4ls-2014-in-berlin    ### License    * The d3sparql.js library is distributed under the same license as D3.js's ([BSD license](http://opensource.org/licenses/BSD-3-Clause)).    ### ChangeLog    See details at https://github.com/ktym/d3sparql/commits/master/d3sparql.js    * 2013-01-28 Project started  * 2014-07-03 Made publicly available at GitHub  * 2014-07-14 Added bar/line charts ```barchart()``` with scales  * 2014-07-17 Added default SVG attributes equivalent to CSS styles    * Visualizations look good without CSS by default (user can customize style by CSS)    * Added descriptions to each visualization function  * 2014-07-19 Introduced ```d3sparql``` name space  * 2014-07-20 Added Pie, Doughnut ```piechart()```, Sankey diagram ```sankey()``` and a name based map ```namedmap()```  * 2014-11-13 Merged a pull request to visualize a coordination based map ```coordmap()```  * 2014-12-11 Updated to set default values in options  * 2015-02-03 Added README file    * updated namedmap to use an option for scale    * merged a pull request to insert visualization at the specified DOM ID instead of appending to the body  * 2015-02-04 Improved to customize the target DOM ID  * 2015-02-06 Changed to clear the DOM contents before appending elements to update the visualization  * 2015-05-21 Updated ```tree()``` and ```graph()``` to keep values associated to nodes    * Values are reflected in the ```treemap()```, ```sunburst()``` and ```forcegraph()``` visualizations  * 2015-05-21 Debug mode is introduced    * Assign ```d3sparql.debug = true``` at anytime to enable verbose console log  * 2015-05-25 Incorporated ```treemapzoom()``` useful to dig into a nested tree (in which each leaf may have a value)     """
Semantic web;https://github.com/linkeddata/gold;"""# gold       [![](https://img.shields.io/badge/project-Solid-7C4DFF.svg?style=flat-square)](https://github.com/solid/solid)  [![Join the chat at https://gitter.im/linkeddata/gold](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/linkeddata/gold?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    `gold` is a reference Linked Data Platform server for the  **[Solid platform](https://github.com/solid/solid-spec)**.    Written in Go, based on  [initial work done by William Waites](https://bitbucket.org/ww/gold).    [![Build Status](https://travis-ci.org/linkeddata/gold.svg?branch=master)](https://travis-ci.org/linkeddata/gold)    ## Installing    ### From docker repository:    ```  sudo docker pull linkeddata/gold  sudo docker run -p ip:port:443 linkeddata/gold  ```  Replace `ip` and `port` with your host computer's IP address and port number.    To check the status of the container, type:    ```  sudo docker ps  ```    `IMPORTANT`: if you want to mount a host directory into the container, you can use the -v parameter:    ```  sudo docker run -p ip:port:443 -v /home/user/data:/data linkeddata/gold  ```    This will mount the host directory, `/home/user/data`, into the container as the `/data/` directory. Doing this will allow you to reuse the data directory without worrying about persistence inside the container.    ### From Github:    1. Setup Go:        * **Mac OS X**: `brew install go`      * **Ubuntu**: `sudo apt-get install golang-go`      * **Fedora**: `sudo dnf install golang`    1. Set the `GOPATH` variable (required by Go):          ```bash        mkdir ~/go        export GOPATH=~/go        ```              (Optionally consider adding `export GOPATH=~/go` to your `.bashrc` or profile).    1. Check that you have the required Go version (**Go 1.4 or later**):          ```        go version        ```              If you don't, please [install](http://golang.org/doc/install) a more recent        version.    1. Use the `go get` command to install the server and all the dependencies:        ```      go get github.com/linkeddata/gold/server      ```        1. Install dependencies:      * **Mac OS X**: `brew install raptor libmagic`      * **Ubuntu**: `sudo apt-get install libraptor2-dev libmagic-dev`      * **Fedora**: `sudo dnf install raptor2-devel file-devel`        1. (Optional) Install extra dependencies used by the tests:        ```      go get github.com/stretchr/testify/assert      ```    ## Running the Server    **IMPORTANT**: Among other things, `gold` is a web server. Please consider  running it as a regular user instead of root. Since gold treats all files  equally, and even though uploaded files are not made executable, it will not  prevent clients from uploading malicious shell scripts.    Pay attention to the data root parameter, `-root`. By default, it will serve  files from its current directory (so, for example, if you installed it from  Github, its data root will be `$GOPATH/src/github.com/linkeddata/gold/`).  Otherwise, make sure to pass it a dedicated data directory to serve, either  using a command-line parameter or the [config file](#configuration).  Something like: `-root=/var/www/data/` or `-root=~/data/`.    1. If you installed it from package via `go get`, you can run it by:      ```    $GOPATH/bin/server -http="":8080"" -https="":8443"" -debug    ```    2. When developing locally, you can `cd` into the repo cloned by `go get`:      ```    cd $GOPATH/src/github.com/linkeddata/gold    ```      And launch the server by:      ```    go run server/*.go -http="":8080"" -https="":8443"" -debug -boltPath=/tmp/bolt.db    ```      Alternatively, you can compile and run it from the source dir in one command:      ```    go run $GOPATH/src/github.com/linkeddata/gold/server/*.go -http="":8080"" -https="":8443"" \      -root=/home/user/data/ -debug -boltPath=/tmp/bolt.db    ```      ## Configuration    You can use the provided `gold.conf-example` file to create your own  configuration file, and specify it with the `-conf` parameter.    ```bash  cd $GOPATH/src/github.com/linkeddata/gold/  cp gold.conf-example server/gold.conf    # edit the configuration file  nano server/gold.conf    # pass the config file when launching the gold server  $GOPATH/bin/server -conf=$GOPATH/src/github.com/linkeddata/gold/server/gold.conf  ```    To see a list of available options:        ~/go/bin/server -help    Some important options and defaults:    * `-conf` - Optional path to a config file.    * `-debug` - Outputs config parameters and extra logging. Default: `false`.    * `-root` - Specifies the data root directory which `gold` will be serving.    Default: `.` (so, likely to be `$GOPATH/src/github.com/linkeddata/gold/`).    * `-http` - HTTP port on which the server listens. For local development,    the default HTTP port, `80`, is likely to be reserved, so pass in an    alternative. Default: `"":80""`. Example: `-http="":8080""`.    * `-https` - HTTPS port on which the server listens. For local development,    the default HTTPS port, `443`, is likely to be reserved, so pass in an    alternative. Default: `"":443""`. Example: `-https="":8443""`.    ## Testing  To run the unit tests (assuming you've installed `assert` via  `go get github.com/stretchr/testify/assert`):    ```  make test  ```    ## Notes    * HOWTO : [Get an example X.509 cert](https://gist.github.com/melvincarvalho/e14753a7137d02d756f19299fed292b4)  * HOWTO : [Login after getting a 401](https://gist.github.com/melvincarvalho/72eaff2fbf1b51a805846320e0bff0cc)  * HOWTO : [Recover an account](https://gist.github.com/melvincarvalho/bcc04e1529dd3a4509892346109b1d37)    ## License  [MIT](http://joe.mit-license.org/) """
Semantic web;https://web.archive.org/web/20180627155808/https://github.com/dice-group/triplestore-benchmarks;"""### How Representative is a SPARQL Benchmark? An Analysis of RDF Triplestore Benchmarks (TheWebConf2019 paper)  We provide a fine-grained comparative analysis of existing triplestore benchmarks. In particular, we have analyzed the data and queries, provided with the existing triplestore benchmarks in addition to several real-world datasets. Further, we have measured the correlation between the query execution time and various SPARQL query features and ranked those features based on their significance levels. Our experiments have revealed  several interesting insights about the design of such benchmarks. We can hope such fine-grained evaluation will be helpful for SPARQL benchmark designers to design diverse benchmarks in the future.  ### Persistent URI, Licence   All of the data and results presented in our evaluation are available online from  https://github.com/dice-group/triplestore-benchmarks under [GNU General Public License v3.0](https://github.com/dice-group/triplestore-benchmarks/blob/master/LICENSE).    ### Benchmark Datasets and Queries  We provide the datasets and queries of the benchmarmks and real-world datasets used in our evaluation. The datasets are also provided as portable virtuoso triplestores which can be started from bin/start_virtuoso.sh.    | *Benchmark/Dataset*   | *RDF Dump* | *Virtuoso Store* | *Queries* |  |-----------------------|------------|---------------------|-----------|  |[Bowlogna](https://exascale.info/assets/pdf/BowlognaBenchSIMPDA2011.pdf)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/datasets-dumps/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[TrainBench](http://docs.inf.mit.bme.hu/trainbenchmark/)|[Download](https://www.dropbox.com/s/n7s02dzf0dyf4by/trainbenchmark-models-1-1024.zip?dl=0)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[BSBM](https://pdfs.semanticscholar.org/0efc/d1d38ad020da7c01613b7818eb123cb34121.pdf)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/datasets-dumps/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[SP2Bench](https://arxiv.org/pdf/0806.4627.pdf)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/datasets-dumps/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[WatDiv](https://link.springer.com/chapter/10.1007/978-3-319-11964-9_13)|[Download](http://dsg.uwaterloo.ca/watdiv/watdiv.100M.tar.bz2)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[LDBC-SNB](https://ldbc.github.io/ldbc_snb_docs/wiki)|[Download](https://www.dropbox.com/s/uyocuxmx85dce4m/social_network_ttl_sf1.zip?dl=0)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[FEASIBLE](https://svn.aksw.org/papers/2015/ISWC_FEASIBLE/public.pdf)|[Download](http://downloads.dbpedia.org/3.5.1/en/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[FishMark](http://ceur-ws.org/Vol-943/SSWS_HPCSW2012_paper1.pdf)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/datasets-dumps/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[DBPSB](https://link.springer.com/chapter/10.1007/978-3-642-25073-6_29)|[Download](http://downloads.dbpedia.org/3.5.1/en/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[BioBench](https://jbiomedsem.biomedcentral.com/track/pdf/10.1186/2041-1480-5-32)|ftp://ftp.dbcls.jp/togordf/bmtoyama/|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[DBpedia3.5.1](http://wiki.dbpedia.org/)|[Download](http://downloads.dbpedia.org/3.5.1/en/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz)|  |[SWDF](https://old.datahub.io/dataset/semantic-web-dog-food)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/datasets-dumps/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz)|  |[NCBIGene](http://download.openbiocloud.org/release/3/ncbigene/ncbigene.html)|[Download](http://download.bio2rdf.org/#/release/3/ncbigene/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz)|  |[SIDER](http://download.openbiocloud.org/release/3/sider/sider.html)|[Download](http://download.bio2rdf.org/#/release/3/sider/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz)|  |[DrugBank](http://download.openbiocloud.org/release/3/drugbank/drugbank.html)|[Download](http://download.bio2rdf.org/#/release/3/drugbank/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz)|      ### Analysis  Our analysis is based on the following benchmark design features:  * Dataset structuredness (**dataset related**)  * Dataset relationship specialty (**dataset related**)  * Overall queries diversity score based on important SPARQL query features, i.e., number of triple patterns, number of projection variables, result set sizes, query execution time, number of BGPs, number of join vertices, mean join vertex degree, mean triple pattern selectivities, BGP-restricted and join-restricted triple pattern selectivities, and join vertex types. (**queries related**)  * Percentages-wise distribution of the use of important SPARQL clauses (e.g., LIMIT, OPTIONAL, ORDER BY, DISTINCT,  UNION, FILTER, REGEX) in benchmark queries  (**queries related**)  * SPearsman's correlation of the query runtimes and important SPARQL query features (**queries related**)    The first two features are related to benchmark datasets and later three are related to benchmark queries. Please refer to the manuscript for the details of above design features.    ### Reproducing Results  Please follow the following steps to reproduce the complete results presented in the paper.   1. Download the folder [CLI](https://github.com/AKSW/triplestore-benchmarks/tree/master/cli) which contains a runable jar **benchmark-util.jar**.     2. To calculate the structuredness or relationship specialty of an RDF datasets, we need to first load the dataset into a triple store and provide the endpoint url as input to the jar file. The linux based virtuoso SPARQL endpoints of the datasets of all the triplestore benchmarks and real-world datasets used in our evaluation can be downloaded from [here](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/). Note the virtuoso can be started from bin/start_virtuoso.sh script. The utility work for any SPARQL endpoint.   3. Download the [Virtuoso](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz) which contains the LSQ datasets of all the selected 10 triplestores benchmarks and 5 real-world datasets. Please refer to [LSQ homepage](https://github.com/aksw/lsq) for generating an LSQ dataset of the queries of a new RDF benchmark. This step is only required to generate queries related results.     ```html  ###Command line arguments ###  java -jar benchmark-util.jar  -m <measure> -e <endpoint> -g <graph> -q <queriesFile>    where    measure = structuredness or specialty or diversity or percentage or correlation  endpoint = endpoint url  graph = graph name (optional)  queriesFile = queries file (only required to produce queries related statistics, i.e., diversity, percentages, and correlation).   please note the queries files are already provided with the cli folder downloaded in step 1.    An example formats:  java -jar benchmark-util.jar -m structuredness -e http://localhost:8890/sparql  java -jar benchmark-util.jar -m specialty -e http://localhost:8890/sparql -g http://benchmark-eval.aksw.org/feasible    java -jar benchmark-util.jar -m diversity -e http://localhost:8890/sparql -g http://benchmark-eval.aksw.org/feasible -q queries-diversity.txt  java -jar benchmark-util.jar -m percentage -e http://localhost:8890/sparql -q queries-percent.txt -g http://benchmark-eval.aksw.org/biobench  java -jar benchmark-util.jar -m correlation -e http://localhost:8890/sparql -q queries-correlation.txt -g http://benchmark-eval.aksw.org/dbpsb    You can run SPARQL SELECT DISTINCT ?g WHERE { GRAPH ?g {?s ?p ?o }} on the virtuoso downloded in step 2 to get the graph names of all the selected benchmarks and real-world datasets. Note you can add more queries into the input files in -q argument to get results for other features.  ```    ### Complette Evaluation Results  Our complete evaluation results can be found [here](https://github.com/AKSW/triplestore-benchmarks/raw/master/complete-evaluation-results.xlsx)  ### Authors    * [Muhammad Saleem](https://sites.google.com/site/saleemsweb/) (AKSW, University of Leipzig)    * [Gábor	Szárnyas](https://inf.mit.bme.hu/en/members/szarnyasg/) (MTA-BME Lendület Cyber-Physical Systems Research Group, Budapest University of Technology and Economics)    * [Felix Conrads](http://aksw.org/FelixConrads.html) (AKSW, University of Leipzig)    * [Syed Ahmad Chan	Bukhari](http://ahmadchan.com) (Department of Pathology, Yale University School of Medicine)    * [Qaiser Mehmood](https://www.insight-centre.org/users/qaiser-mehmood) (INSIGHT, University of Galway)    * [Axel-Cyrille Ngonga Ngomo](http://aksw.org/AxelNgonga.html) (AKSW, University of Leipzig) """
Semantic web;https://github.com/SDM-TIB/FunMap;"""# FunMap: Functional Mappings for Scaled-Up Knowledge Graph Creation    We present FunMap, an interpreter of [RML](https://rml.io/docs/rml/introduction/)+[FnO](https://fno.io/),that converts a data integration system defined using RML+FnO into an equivalent data integration system where RML mappings are function-free. FunMap resembles existing mapping translation proposals and empowers the  knowledge  graph  creation  process  with  optimization  techniques  to  reduce execution  time.  Transformations  of  data  sources  include  the  projection  of  the attributes used in the RML+FnO mappings. They are supported on well-known properties of the relational algebra, e.g., the pushing down of projections and selections into the data sources, and enable not only the reduction of the size of  data  sources  but  also  the  elimination  of  duplicates.  Additionally,  FunMap materializes  functions  –expressed  in  FnO–  and  represents  the  results  as  data sources of the generated data integration system; the translation of RML+FnO into RML mappings that integrate the materialization of functions is performed by means of joins between the generated RML mappings.     ![FunMap-workflow](images/architecture.png?raw=true ""FunMap-workflow"")      ## Research papers:    Samaneh Jozashoori, David Chaves-Fraga, Enrique Iglesias, Oscar Corcho, and Maria-Esther Vidal. 2020. FunMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation. The 19th International Semantic Web Conference - Research Track (ISWC 2020). *[Fully Reproduced Paper](https://github.com/SDM-TIB/FunMap/tree/eval-iswc2020)* [Online](https://www.researchgate.net/publication/346220361_FunMap_Efficient_Execution_of_Functional_Mappings_for_Knowledge_Graph_Creation)      ## How to run FunMap?    ### Configuration file  ```  [default]  main_directory: /     [datasets]  number_of_datasets: 1  # name of dataset  name: funmap   # path of results  output_folder: ${default:main_directory}results/   # yes executes FunMap, no executes FunMap- (without projections)  enrichment: yes  # only for RDB instance  dbType: mysql     [dataset1]  name: funmap  # mapping path  mapping: ${default:main_directory}mappings/mapping.ttl   # only for RDB  user: user   password: pass  host: 127.0.0.1  port: 3306   db: dbName  ```    ### Run with Docker   ```  # Preparation  docker build -t funmap .    # For CSV files  docker-compose up -d  # The path of the files in the mappings has to be: /data/nameOfTheFile.csv.  cp csvFiles.csv data/  cp mapping.ttl mappings/  cp config.ini funmap/    # For RDB instance  mkdir sql  cp sqlScript.sql sql/  docker-compose up -d   cp mapping.ttl mappings/  cp config.ini funmap/    # Execution  docker exec -it funmap python3 /funmap/run_translator.py /funmap/config[_rdb].ini  ```    ### Run with Python3  ```  pip install -r requirements.txt  python3 run_translator.py config[_rdb].ini  ```    ## Authors    - Samaneh Jozashoori (samaneh.jozashoori@tib.eu)  - David Chaves-Fraga (dchaves@fi.upm.es)  - Enrique Iglesias ( s6enigle@uni-bonn.de)  - Oscar Corcho (ocorcho@fi.upm.es)  - Maria-Esther Vidal (maria.vidal@tib.eu) """
Semantic web;https://github.com/DataFabricRus/datastudio-sparql-connector;"""# SPARQL Connector for Google Data Studio    It allows to load data from a SPARQL endpoint using SELECT queries.    ![Connector in the gallery](screenshot_ingallery.png)    ## Example report    ![Example report on Google Datastudio](screenshot_examplereport.png)    Here is [link](https://datastudio.google.com/open/1Mbwa4VUue5Z9Ke4TsN6FcsQMiQX2uDPC) to the example report.    ## Getting started    1. Open the [link](https://datastudio.google.com/datasources/create?connectorId=AKfycbzDHEBN9qHXPni4xO4P2cIZtyQ3rnYmzkCnVsnh9oEJrnhGe4MntBF-t1zAu2Lm-Vjc) to create a new Data Source.  1. Once authorization has successfully completed, you're ready to configure the parameters. You should see the form:        ![Screenshot of the configuration page](screenshot_parameters.png)    1. Enter the SPARQL endpoint URL, e.g. http://dbpedia.org/sparql  1. Enter your SELECT query, e.g.        ```      PREFIX dbr: <http://dbpedia.org/resource/>      PREFIX dbo: <http://dbpedia.org/ontology/>      PREFIX foaf: <http://xmlns.com/foaf/0.1/>      PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>        SELECT ?name ?gender ?birthDate WHERE {        ?person dbo:birthPlace dbr:Berlin ;                dbo:birthDate ?birthDate ;                foaf:name ?name ;                foaf:gender ?gender .           FILTER (?birthDate > ""{dateRange.startDate}""^^xsd:date && ?birthDate < ""{dateRange.endDate}""^^xsd:date) .      }      ```        The following variables are supported:        * `dateRange.startDate` - format `YYYY-MM-DD`, e.g. 2018-10-01,      * `dateRange.endDate` - format `YYYY-MM-DD`, e.g. 2018-10-01,      * `dateRange.numDays` - it's a positive integer value.        If you don't use `dateRange.startDate` or `dateRange.endDate`, then **Date range** filter won't be able to control the date range.    1. Enter the schema of projections, e.g.        ```      [          {""name"": ""name"", ""dataType"": ""STRING""},          {""name"": ""gender"", ""dataType"": ""STRING""},          {""name"": ""birtDate"", ""dataType"": ""STRING""}      ]      ```        At this step is enough to set only data types for each projection, at the next step you'll be able to refine it. More about the schema elements, data types you can read in https://developers.google.com/datastudio/connector/semantics.    1. Press **Connect** button and the next page is the same for all connectors.    ## Supported data type conversions    Google Data Studio uses it's own formats for some of the data types. Therefore the connector automatically converts them. The following data types are supported:    * `xsd:date` is converted to `YYYYMMDD`,  * `xsd:dateTime` is converted to `YYYYMMDDHH`,  * `xsd:duration` is converted to an integer corresponding to the number of seconds.    ## Default values    The connector may apply default values in query results which don't a value for a requested field. The default values:    Datatype   | Default value  -----------|--------------  `NUMBER`   | `0`  `BOOLEAN`  | `false`  `STRING`   | `""""`    If you don't like these defaults, then write your query in a way when it can't have missing/empty values, especially in case of `GROUP BY`.    ## License    MIT License   """
Semantic web;https://github.com/magnetik/node-webid;"""#node-webid    Node.js module with tools to help using WebID (http://www.webid.info).    ##Installation    ### [npm](https://npmjs.org/package/webid)    Just require the module `webid`:    ```  var webid = require('webid');  ```    ### Manual    Start `cake build` and get the webid.js in the bin folder.    ## Usage    Check the project [webid-demo](https://github.com/magnetik/node-webid-demo) to see a working example.     Basic usage:    ```  var webid = require('webid');  var verifAgent = new webid.VerificationAgent(certificate);  	verifAgent.verify(function (result) {  		//Success! User is identified  		var foaf = new webid.Foaf(result);  		req.session.profile = foaf.parse();  	}, function(result) {  		//An error occured  	});  ```    ##Licence    The lib is available under MIT Licence: http://www.opensource.org/licenses/MIT   """
Semantic web;https://github.com/Claudenw/PA4RDF;"""# PA4RDF  Persistence Annotation for RDF (PA4RDF) is a set of annotations and an entity manager that provides JPA like functionality on top of an RDF store while accounting for and exploiting the fundamental differences between graph storage and relational storage.  	PA4RDF introduces three (3) annotations that map a RDF triple (subject, predicate, object) to a Plain Old Java Object (POJO) using Java's dynamic proxy capabilities.    Documentation is at http://claudenw.github.io/PA4RDF/ """
Semantic web;https://github.com/Callidon/sparql-engine;"""# sparql-engine  [![build package](https://github.com/Callidon/sparql-engine/actions/workflows/test.yaml/badge.svg?branch=master)](https://github.com/Callidon/sparql-engine/actions/workflows/test.yaml)  [![codecov](https://codecov.io/gh/Callidon/sparql-engine/branch/master/graph/badge.svg)](https://codecov.io/gh/Callidon/sparql-engine) [![npm version](https://badge.fury.io/js/sparql-engine.svg)](https://badge.fury.io/js/sparql-engine) [![JavaScript Style Guide](https://img.shields.io/badge/code_style-standard-brightgreen.svg)](https://standardjs.com)    An open-source framework for building SPARQL query engines in Javascript/Typescript.    [Online documentation](https://callidon.github.io/sparql-engine/)    **Main features**:  * Build a [SPARQL](https://www.w3.org/TR/2013/REC-sparql11-overview-20130321/) query engine on top of any data storage system.  * Supports [the full features of the SPARQL syntax](https://www.w3.org/TR/sparql11-query/) by *implementing a single class!*  * Support for all [SPARQL property Paths](https://www.w3.org/TR/sparql11-query/#propertypaths).  * Implements advanced *SPARQL query rewriting techniques* for transparently optimizing SPARQL query processing.  * Supports [full text search queries](#full-text-search).  * Supports [Custom SPARQL functions](#custom-functions).  * Supports [Semantic Caching](#enable-caching), to speed up query evaluation of reccurent patterns.  * Supports the [SPARQL UPDATE protocol](https://www.w3.org/TR/2013/REC-sparql11-update-20130321/).  * Supports Basic [Federated SPARQL queries](https://www.w3.org/TR/2013/REC-sparql11-federated-query-20130321/) using **SERVICE clauses**.  * Customize every step of SPARQL query processing, thanks to *a modular architecture*.  * Support for [SPARQL Graph Management protocol](https://www.w3.org/TR/2013/REC-sparql11-update-20130321/#graphManagement).    # Table of contents  * [Installation](#installation)  * [Getting started](#getting-started)    * [Examples](#examples)    * [Preliminaries](#preliminaries)    * [RDF Graphs](#rdf-graphs)    * [RDF Datasets](#rdf-datasets)    * [Running a SPARQL query](#running-a-sparql-query)  * [Enable caching](#enable-caching)  * [Full text search](#full-text-search)  * [Federated SPARQL Queries](#federated-sparql-queries)  * [Custom Functions](#custom-functions)  * [Advanced Usage](#advanced-usage)    * [Customize the pipeline implementation](#customize-the-pipeline-implementation)    * [Customize query execution](#customize-query-execution)  * [Documentation](#documentation)  * [Aknowledgments](#aknowledgments)  * [References](#references)    # Installation    ```bash  npm install --save sparql-engine  ```    # Getting started    The `sparql-engine` framework allow you to build a custom SPARQL query engine on top of any data storage system.    In short, to support SPARQL queries on top of your data storage system, you need to:  * [Implements a subclass of `Graph`](#rdf-graphs), which provides access to the data storage system.  * Gather all your Graphs as a `Dataset` (using your own implementation or [the default one](#rdf-datasets)).  * [Instantiate a `PlanBuilder`](#running-a-sparql-query) and use it to execute SPARQL queries.    ## Examples    As a starting point, we provide you with two examples of integration:  * With [N3.js](https://github.com/rdfjs/N3.js), available [here](https://github.com/Callidon/sparql-engine/tree/master/examples/n3.js).  * With [LevelGraph](https://github.com/levelgraph/levelgraph), available [here](https://github.com/Callidon/sparql-engine/tree/master/examples/levelgraph.js).    ## Preliminaries    ### SPARQL.js algebra and TypeScript    The `sparql-engine` framework use the [`SPARQL.js`](https://github.com/RubenVerborgh/SPARQL.js/) library for parsing and manipulating SPARQL queries as JSON objects. For TypeScript compiltation, we use a custom package [`sparqljs-legacy-type`](https://github.com/Callidon/sparqljs-legacy-type) for providing the types information.     Thus, **if you are working with `sparql-engine` in TypeScript**, you will need to install the [`sparqljs-legacy-type`](https://github.com/Callidon/sparqljs-legacy-type) package.    If want to know why we use a custom types package, see [the discussion of this issue](https://github.com/Callidon/sparql-engine/issues/58).    ### RDF triples representation    This framework represents RDF triples using Javascript Object.  You will find below, in Java-like syntax, the ""shape"" of such object.    ```typescript  interface TripleObject {    subject: string; // The Triple's subject    predicate: string; // The Triple's predicate    object: string; // The Triple's object  }  ```    ### PipelineStage    The `sparql-engine` framework uses a pipeline of iterators to execute SPARQL queries. Thus, many methods encountered in this framework needs to return `PipelineStage<T>`, *i.e.*, objects that generates items of type `T` in a pull-based fashion.    An `PipelineStage<T>` can be easily created from one of the following:  * An **array** of elements of type `T`  * A [**Javascript Iterator**](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols), which yields elements of type `T`.  * An [**EventEmitter**](https://nodejs.org/api/events.html#events_class_eventemitter) which emits elements of type `T` on a `data` event.  * A [**Readable stream**](https://nodejs.org/api/stream.html#stream_readable_streams) which produces elements of type `T`.    To create a new `PipelineStage<T>` from one of these objects, you can use the following code:  ```javascript  const { Pipeline } = require('sparql-engine')    const sourceObject = // the object to convert into a PipelineStage    const stage = Pipeline.getInstance().from(sourceObject)  ```    Fore more information on how to create and manipulate the pipeline, please refers to the documentation of [`Pipeline`](https://callidon.github.io/sparql-engine/classes/pipelinee.html) and [`PipelineEngine`](https://callidon.github.io/sparql-engine/classes/pipelineengine.html).    ## RDF Graphs    The first thing to do is to implement a subclass of the `Graph` abstract class. A `Graph` represents an [RDF Graph](https://www.w3.org/TR/rdf11-concepts/#section-rdf-graph) and is responsible for inserting, deleting and searching for RDF triples in the database.    The main method to implement is `Graph.find(triple)`, which is used by the framework to find RDF triples matching  a [triple pattern](https://www.w3.org/TR/sparql11-query/#basicpatterns) in the RDF Graph.  This method must return an `PipelineStage<TripleObject>`, which will be consumed to find matching RDF triples. You can find an **example** of such implementation in the [N3 example](https://github.com/Callidon/sparql-engine/tree/master/examples/n3.js).    Similarly, to support the [SPARQL UPDATE protocol](https://www.w3.org/TR/2013/REC-sparql11-update-20130321/), you have to provides a graph that implements the `Graph.insert(triple)` and `Graph.delete(triple)` methods, which insert and delete RDF triple from the graph, respectively. These methods must returns [Promises](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise), which are fulfilled when the insertion/deletion operation is completed.    Finally, the `sparql-engine` framework also let your customize how [Basic graph patterns](https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#BasicGraphPatterns) (BGPs) are evaluated against  the RDF graph. The engine provides a **default implementation** based on the `Graph.find` method and the  *Index Nested Loop Join algorithm*. However, if you wish to supply your own implementation for BGP evaluation, you just have to implement a `Graph` with an `evalBGP(triples)` method.  This method must return a `PipelineStage<Bindings>`. You can find an example of such implementation in the [LevelGraph example](https://github.com/Callidon/sparql-engine/tree/master/examples/levelgraph.js).    You will find below, in Java-like syntax, an example subclass of a `Graph`.  ```typescript    const { Graph } = require('sparql-engine')      class CustomGraph extends Graph {      /**       * Returns an iterator that finds RDF triples matching a triple pattern in the graph.       * @param  triple - Triple pattern to find       * @return An PipelineStage which produces RDF triples matching a triple pattern       */      find (triple: TripleObject, options: Object): PipelineStage<TripleObject> { /* ... */ }        /**       * Insert a RDF triple into the RDF Graph       * @param  triple - RDF Triple to insert       * @return A Promise fulfilled when the insertion has been completed       */      insert (triple: TripleObject): Promise { /* ... */ }        /**       * Delete a RDF triple from the RDF Graph       * @param  triple - RDF Triple to delete       * @return A Promise fulfilled when the deletion has been completed       */      delete (triple: : TripleObject): Promise { /* ... */ }    }  ```    ## RDF Datasets    Once you have your subclass of `Graph` ready, you need to build a collection of RDF Graphs, called a [RDF Dataset](https://www.w3.org/TR/rdf11-concepts/#section-dataset). A default implementation, `HashMapDataset`, is made available by the framework, but you can build your own by subclassing [`Dataset`](https://callidon.github.io/sparql-engine/classes/dataset.html).    ```javascript   const { HashMapDataset } = require('sparql-engine')   const CustomGraph = require(/* import your Graph subclass */)     const GRAPH_A_IRI = 'http://example.org#graph-a'   const GRAPH_B_IRI = 'http://example.org#graph-b'   const graph_a = new CustomGraph(/* ... */)   const graph_b = new CustomGraph(/* ... */)     // we set graph_a as the Default RDF dataset   const dataset = new HashMapDataset(GRAPH_A_IRI, graph_a)     // insert graph_b as a Named Graph   dataset.addNamedGraph(GRAPH_B_IRI, graph_b)  ```    ## Running a SPARQL query    Finally, to run a SPARQL query on your RDF dataset, you need to use the `PlanBuilder` class. It is responsible for parsing SPARQL queries and building a pipeline of iterators to evaluate them.    ```javascript    const { PlanBuilder } = require('sparql-engine')      // Get the name of all people in the Default Graph    const query = `      PREFIX foaf: <http://xmlns.com/foaf/0.1/>      SELECT ?name      WHERE {        ?s a foaf:Person .        ?s foaf:name ?name .      }`      // Creates a plan builder for the RDF dataset    const builder = new PlanBuilder(dataset)      // Get an iterator to evaluate the query    const iterator = builder.build(query)      // Read results    iterator.subscribe(      bindings => console.log(bindings),      err => console.error(err),      () => console.log('Query evaluation complete!')    )  ```    # Enable caching    The `sparql-engine` provides support for automatic caching of Basic Graph Pattern evaluation using the [Semantic Cache algorithm](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1161590). Basically, the cache will save the results of BGPs already evaluated and, when the engine wants to evaluates a BGP, it will look for the largest subset of the BGP in the cache. If one is available, it will re-use the cached results to speed up query processing.    By default, semantic caching is disabled. You can turn it on/off using the `PlanBuilder.useCache` and `PlanBuilder.disableCache` methods, respectively. The `useCache` method accepts an optional parameter, so you can provide your own implementation of the semantic cache. By defaults, it uses an in-memory [LRU cache](https://callidon.github.io/sparql-engine/classes/lrubgpcache.html) which stores up to 500MB of items for 20 minutes.    ```javascript  // get an instance of a PlanBuilder  const builder = new PlanBuilder(/* ... */)    // activate the cache  builder.useCache()    // disable the cache  builder.disableCache()  ```    # Full Text Search    The `sparql-engine` provides a non-standard full text search functionnality,  allowing users to execute [approximate string matching](https://en.wikipedia.org/wiki/Approximate_string_matching) on RDF Terms retrieved by SPARQL queries.  To accomplish this integration, it follows an approach similar to [BlazeGraph](https://wiki.blazegraph.com/wiki/index.php/FullTextSearch) and defines several **magic predicates** that are given special meaning, and when encountered in a SPARQL query, they are interpreted as configuration parameters for a full text search query.    The simplest way to integrate a full text search into a SPARQL query is to use the magic predicate `ses:search` inside of a SPARQL join group. In the following query, this predicate is used to search for the keywords `neil` and `gaiman` in the values binded to the `?o` position of the triple pattern.  ```  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX ses: <https://callidon.github.io/sparql-engine/search#>  SELECT * WHERE {    ?s foaf:knows ?o .    ?o ses:search “neil gaiman” .  }  ```  In a way, full text search queries allows users to express more complex SPARQL filters that performs approximate string matching over RDF terms.  Each result is annotated with a *relevance score* (how much it matches the keywords, higher is better) and a *rank* (they represent the descending order of relevance scores). These two values are not binded by default into the query results, but you can use magic predicates to get access to them (see below). Note that the meaning of relevance scores is specific to the implementation of the full text search.    The full list of magic predicates that you can use in a full text search query is:  * `ses:search` defines keywords to search as a list of keywords separated by spaces.  * `ses:matchAllTerms` indicates that only values that contain all of the specified search terms should be considered.  * `ses:minRelevance`and `ses:maxRelevance` limits the search to matches with a minimum/maximum  relevance score, respectively. In the default implementation, scores are floating numbers, ranging from 0.0 to 1.0 with a precision of 4 digits.  * `ses:minRank` and `ses:maxRank` limits the search to matches with a minimum/maximum  rank value, respectively. In the default implementation, ranks are positive integers starting at 0.  * `ses:relevance` binds each term's relevance score to a SPARQL variable.  * `ses:rank` binds each term's rank to a SPARQL variable.    Below is a more complete example, that use most of these keywords to customize the full text search.  ```  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX ses: <https://callidon.github.io/sparql-engine/search#>  SELECT ?s ?o ?score ?rank WHERE {    ?s foaf:knows ?o .    ?o ses:search “neil gaiman” .    ?o ses:minRelevance “0.25” .    ?o ses:maxRank “1000” .    ?o ses:relevance ?score .    ?o ses:rank ?rank .    ?o ses:matchAllTerms “true” .  }  ```    To provide a custom implementation for the full text search that is more integrated with your backend,  you simply need to override the `fullTextSearch` method of the `Graph` class.  You can find the full signature of this method in the [relevant documentation](https://callidon.github.io/sparql-engine/classes/graph.html#fullTextSearch).    The `sparql-engine` framework provides a default implementation of this method, which computes relevance scores as the average ratio of keywords matched by words in the RDF terms.  Notice that **this default implementation is not suited for production usage**.  It will performs fine for small RDF datasets, but,   when possible, you should always provides a dedicated implementation that leverages your backend.  For example, for SQL databases, you could use [GIN or GIST indexes](https://www.postgresql.org/docs/12/gin-intro.html).    # Federated SPARQL Queries    The `sparql-engine` framework provides automatic support for evaluating [federated SPARQL queries](https://www.w3.org/TR/2013/REC-sparql11-federated-query-20130321/), using the [`SERVICE` keyword](https://www.w3.org/TR/sparql11-query/#basic-federated-query).    To enable them, you need to set **a Graph Factory** for the RDF dataset used to evaluate SPARQL queries.  This Graph factory is used by the dataset to create new RDF Graph on-demand.  To set it, you need to use the [`Dataset.setGraphFactory`](https://callidon.github.io/sparql-engine/classes/dataset.html#setgraphfactory) method, as detailed below.  It takes *a callback* as parameter, which will be invoked to create a new graph from an IRI.  It's your responsibility to define the graph creation logic, depending on your application.    ```typescript  const { HashMapDataset } = require('sparql-engine')  const CustomGraph = require(/* import your Graph subclass */)    const my_graph = new CustomGraph(/* ... */)    const dataset = new HashMapDataset('http://example.org#graph-a', my_graph)    // set the Graph factory of the dataset  dataset.setGraphFactory(iri => {    // return a new graph for the provided iri    return new CustomGraph(/* .. */)  })  ```    Once the Graph factory is set, you have nothing more to do!  Juste execute your federated SPARQL queries as regular queries, like before!    # Custom Functions    SPARQL allows custom functions in expressions so that queries can be used on domain-specific data.  The `sparql-engine` framework provides a supports for declaring such custom functions.    A SPARQL value function is an extension point of the SPARQL query language that allows URI to name a function in the query processor.  It is defined by an `IRI` in a `FILTER`, `BIND` or `HAVING BY` expression.  To register custom functions, you must create a JSON object that maps each function's `IRI` to a Javascript function that takes a variable number of **RDF Terms** arguments and returns one of the following:  * A new RDF Term (an IRI, a Literal or a Blank Node) in RDF.js format.  * An array of RDF Terms.  * An [Iterable](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols) or a [Generator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Generator) that yields RDF Terms.  * The `null` value, to indicates that the function's evaluation has failed.    RDF Terms are represented using the [RDF.js data model](http://rdf.js.org/data-model-spec/).  The [`rdf` subpackage](https://callidon.github.io/sparql-engine/modules/rdf.html) exposes a lot  of utilities methods to create and manipulate RDF.js terms in the context of custom SPARQL functions.    The following shows a declaration of some simple custom functions.  ```javascript  // load the utility functions used to manipulate RDF terms  const { rdf } = require('sparql-engine')    // define some custom SPARQL functions  const customFunctions = {    // reverse a RDF literal    'http://example.com#REVERSE': function (rdfTerm) {      const reverseValue = rdfTerm.value.split("""").reverse().join("""")      return rdf.shallowCloneTerm(rdfTerm, reverseValue)    },    // Test if a RDF Literal is a palindrome    'http://example.com#IS_PALINDROME': function (rdfTerm) {      const result = rdfTerm.value.split("""").reverse().join("""") === rdfTerm.value      return rdf.createBoolean(result)    },    // Test if a number is even    'http://example.com#IS_EVEN': function (rdfTerm) {      if (rdf.termIsLiteral(rdfTerm) && rdf.literalIsNumeric(rdfTerm)) {        const jsValue = rdf.asJS(rdfTerm.value, rdfTerm.datatype.value)        const result = jsValue % 2 === 0        return rdf.createBoolean(result)      }      return terms.createFalse()    }  }  ```    Then, this JSON object is passed into the constructor of your PlanBuilder.    ```javascript  const builder = new PlanBuilder(dataset, {}, customFunctions)  ```    Now, you can execute SPARQL queries with your custom functions!  For example, here is a query that uses our newly defined custom SPARQL functions.    ```  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX example: <http://example.com#>  SELECT ?length  WHERE {    ?s foaf:name ?name .      # this bind is not critical, but is here for illustrative purposes    BIND(<http://example.com#REVERSE>(?name) as ?reverse)      BIND(STRLEN(?reverse) as ?length)      # only keeps palindromes    FILTER (!example:IS_PALINDROME(?name))  }  GROUP BY ?length  HAVING (example:IS_EVEN(?length))  ```    # Advanced usage    ## Customize the pipeline implementation    The class `PipelineEngine` (and its subclasses) is the main component used by `sparql-engine` to evaluate all SPARQL operations. It defines basic operations (`map`, `filter`, etc) that can be used  to manipulate intermediate results and evaluate SPARQL queries.    By default, the framework uses an implementation of `PipelineEngine` based on [`rxjs`](https://rxjs-dev.firebaseapp.com/), to implements a SPARQL query execution plan as a pipeline of iterators.  However, **you are able to switch to others implementations** of `PipelineEngine`, using `Pipeline.setInstance`.    ```javascript  const { Pipeline, PipelineEngine } = require('sparql-engine')    class CustomEngine extends PipelineEngine {    // ...  }    // add this before creating a new plan builder  Pipeline.setInstance(new CustomEngine())  // ...  ```    Two implementations of `PipelineEngine` are provided by default.  * `RxjsPipeline`, based on [`rxjs`](https://rxjs-dev.firebaseapp.com/), which provides a pure pipeline approach. This approach is **selected by default** when loading the framework.  * `VectorPipeline`, which materializes all intermediate results at each pipeline computation step. This approach is more efficient CPU-wise, but also consumes a lot more memory.    These implementations can be imported as follows:  ```javascript  const { RxjsPipeline, VectorPipeline } = require('sparql-engine')  ```    ## Customize query execution    A `PlanBuilder` implements a [Builder pattern](https://en.wikipedia.org/wiki/Builder_pattern) in order to create a physical query execution plan for a given SPARQL query.  Internally, it defines [*stages builders*](https://callidon.github.io/sparql-engine/classes/stagebuilder) to generates operators for executing all types of SPARQL operations.  For example, the [`OrderByStageBuilder`](https://callidon.github.io/sparql-engine/classes/orderbystagebuilder.html) is invoked when the `PlanBuilder` needs to evaluate an `ORDER BY` modifier.    If you want to customize how query execution plans are built, you have to implement your own stage builders, by extending existing ones.  Then, you need to configure your plan builder to use them, with the [`use` function](https://callidon.github.io/sparql-engine/classes/planbuilder.html#use).    ```javascript    const { PlanBuilder, stages } = require('sparql-engine')      class MyOrderByStageBuilder extends stages.OrderByStageBuilder {      /* Define your custom execution logic for ORDER BY */    }      const dataset = /* a RDF dataset */      // Creates a plan builder for the RDF dataset    const builder = new PlanBuilder(dataset)      // Plug-in your custom stage builder    builder.use(stages.SPARQL_OPERATION.ORDER_BY, MyOrderByStageBuilder(dataset))      // Now, execute SPARQL queries as before with your PlanBuilder  ```    You will find below a reference table of all stage builders used by `sparql-engine` to evaluate SPARQL queries. Please see [the API documentation](https://callidon.github.io/sparql-engine/classes/stagebuilder) for more details.    **Executors**    | SPARQL Operation | Default Stage Builder | Symbol |  |------------------|-----------------------|--------|  | [Aggregates](https://www.w3.org/TR/sparql11-query/#aggregates) | [AggregateStageBuilder](https://callidon.github.io/sparql-engine/classes/aggregatestagebuilder.html) | `SPARQL_OPERATION.AGGREGATE` |  | [Basic Graph Patterns](https://www.w3.org/TR/sparql11-query/#BasicGraphPatterns) | [BGPStageBuilder](https://callidon.github.io/sparql-engine/classes/bgpstagebuilder.html) | `SPARQL_OPERATION.BGP` |  | [BIND](https://www.w3.org/TR/sparql11-query/#bind) | [BindStageBuilder](https://callidon.github.io/sparql-engine/classes/bindstagebuilder.html) | `SPARQL_OPERATION.BIND` |  | [DISTINCT](https://www.w3.org/TR/sparql11-query/#neg-minus) | [DistinctStageBuilder](https://callidon.github.io/sparql-engine/classes/distinctstagebuilder.html) | `SPARQL_OPERATION.DISTINCT` |  | [FILTER](https://www.w3.org/TR/sparql11-query/#expressions) | [FilterStageBuilder](https://callidon.github.io/sparql-engine/classes/filterstagebuilder.html) | `SPARQL_OPERATION.FILTER` |  | [Property Paths](https://www.w3.org/TR/sparql11-query/#propertypaths) | [PathStageBuilder](https://callidon.github.io/sparql-engine/classes/pathstagebuilder.html) | `SPARQL_OPERATION.PROPERTY_PATH` |  | [GRAPH](https://www.w3.org/TR/sparql11-query/#rdfDataset) | [GraphStageBuilder](https://callidon.github.io/sparql-engine/classes/graphstagebuilder.html) | `SPARQL_OPERATION.GRAPH` |  | [MINUS](https://www.w3.org/TR/sparql11-query/#neg-minus) | [MinusStageBuilder](https://callidon.github.io/sparql-engine/classes/minusstagebuilder.html) | `SPARQL_OPERATION.MINUS` |  | [OPTIONAL](https://www.w3.org/TR/sparql11-query/#optionals) | [OptionalStageBuilder](https://callidon.github.io/sparql-engine/classes/optionalstagebuilder.html) | `SPARQL_OPERATION.OPTIONAL` |  | [ORDER_BY](https://www.w3.org/TR/sparql11-query/#modOrderBy) | [OrderByStageBuilder](https://callidon.github.io/sparql-engine/classes/orderbystagebuilder.html) | `SPARQL_OPERATION.ORDER_BY` |  | [SERVICE](https://www.w3.org/TR/sparql11-query/#basic-federated-query) | [ServiceStageBuilder](https://callidon.github.io/sparql-engine/classes/servicestagebuilder.html) | `SPARQL_OPERATION.SERVICE` |  | [UNION](https://www.w3.org/TR/sparql11-query/#alternatives) | [UnionStageBuilder](https://callidon.github.io/sparql-engine/classes/unionstagebuilder.html) | `SPARQL_OPERATION.UNION` |  | [UPDATE](https://www.w3.org/TR/2013/REC-sparql11-update-20130321/) | [UpdateStageBuilder](https://callidon.github.io/sparql-engine/classes/updatestagebuilder.html) | `SPARQL_OPERATION.UPDATE` |      # Documentation    To generate the documentation in the `docs` director:  ```bash  git clone https://github.com/Callidon/sparql-engine.git  cd sparql-engine  yarn install  npm run doc  ```    # Acknowledgments    This framework is developed since 2018 by many contributors, and we thanks them very much for their contributions to this project! Here is the full list of our amazing contributors.    * [Corentin Marionneau](https://github.com/Slaanaroth) (@Slaanaroth)    * Corentin created the first version of `sparql-engine` during its research internship at the [Laboratoire des Sciences du Numérique de Nantes](https://www.ls2n.fr/) (LS2N). He is now a Web developer at SII Atlantique.  * [Merlin Barzilai](https://github.com/Rintarou) (@Rintarou)    * Merlin designed the first SPARQL compliance tests for the framework during its research internship at the [LS2N](https://www.ls2n.fr/).  * [Dustin Whitney](https://github.com/dwhitney) (@dwhitney)    * Dustin implemented the support for custom SPARQL functions and provided a lot of feedback during the early stages of development.  * [Julien Aimonier-Davat](https://github.com/Lastshot97) (@Lastshot97)    * Julien implemented the support for SPARQL Property Paths evaluation during its research internship at the [LS2N](https://www.ls2n.fr/). He is now a Ph.D. Student at the University of Nantes.  * [Arnaud Grall](https://github.com/folkvir) (@folkvir)    * Arnaud contributed to many bugfixes and provided a lot of feedback throughout the development of the framework. He is now a Software Engineer at SII Atlantique.  * [Thomas Minier](https://github.com/Callidon) (@Callidon)    * Thomas developed the framework during his PhD thesis in the [Team ""Gestion des Données Distribuées""](https://sites.google.com/site/gddlina/) (GDD) and supervise its evolution ever since. He is now a Software Engineer at SII Atlantique.    # References    * [Official W3C RDF specification](https://www.w3.org/TR/rdf11-concepts)  * [Official W3C SPARQL specification](https://www.w3.org/TR/2013/REC-sparql11-query-20130321/) """
Semantic web;https://github.com/magnetik/tac;""""""
Semantic web;https://github.com/SmartDataAnalytics/jena-sparql-api;"""## Welcome to the Jena SPARQL API project  An advanced Jena-based SPARQL processing stack for building Semantic Web applications.    Highlights:  * Fluent SPARQL Query API - Transparently enhance query execution with caching, pagination, rewriting, transformations, and so on, without having to worry about that in your application logic.  * Transparent basic (normalized) string caching - Just the usual string based caching as it has been implemented over and over again  * Query Transformations  * SPARQL sub graph isomorphism checker  * Transparent sub graph isomorphy cache - Uses the isomorphism checker for caching - Detects whether prior result sets fit into a current query - regardless of variable naming.  * JPA-based Java<->RDF mapper: Run JPA criteria queries over Java classes which are actually backed by SPARQL.      [![Build Status](http://ci.aksw.org/jenkins/job/jena-sparql-api/badge/icon)](http://ci.aksw.org/jenkins/job/jena-sparql-api/)    This library offers several [Jena](http://jena.apache.org/)-compatible ways to *transparently* add delays, caching, pagination, retry and even query transformations before sending off your original SPARQL query. This frees your application layer from the hassle of dealing with those issues. Also, the server module bundles Jena with the [Atmosphere](https://github.com/Atmosphere/atmosphere) framework, giving you a kickstart for REST and websocket implementations.     ### Maven  Releases are available on [maven central](http://search.maven.org/#search%7Cga%7C1%7Cjena-sparql-api).  Snapshots are presently published in our own archiva:    ```xml  <repositories>  	<repository>  	    <id>maven.aksw.snapshots</id>  	    <name>University Leipzig, AKSW Maven2 Repository</name>  	    <url>http://maven.aksw.org/archiva/repository/snapshots</url>  	</repository>  </repositories>    <dependencies>          <!-- This is the core artifact; several other ones build on that. -->  	<dependency>  		<groupId>org.aksw.jena-sparql-api</groupId>  		<artifactId>jena-sparql-api-core</artifactId>  		<version>{check available versions with the link below}</version>  	</dependency>	  	...  </dependencies>  ```    Latest version(s): [jena-sparql-api on maven central](http://search.maven.org/#search%7Cga%7C1%7Cjena-sparql-api)      ### Project structure    This library is composed of the following modules:  * `jena-sparql-api-core`: Contains the core interfaces and basic implementations.  * `jena-sparql-api-server`: An abstract SPARQL enpdoint class that allows you to easily create your own SPARQL endpoint. For example, the SPARQL-SQL rewriter [Sparqlify](http://github.com/AKSW/Sparqlify) is implemented against these interfaces.  * `jena-sparql-api-utils`: Utilities common to all packages.  * `jena-sparql-api-example-proxy`: An example how to create a simple SPARQL proxy. You can easily adapt it to add pagination, caching and delays.  * `jena-sparql-api-sparql-ext`: SPARQL extensions for processing non-RDF data as part of query evaluation. Most prominently features support for querying JSON documents and unnesting JSON arrays to triples. (We should also add CSV processing for completeness, although covered by the TARQL tool).  * `jena-sparql-api-jgrapht`: Provides a JGraphT wrapper for Jena's Graph interface. Yes, we were aware that RDF is not a plain graph, but a labeled directed pseudo graph and implemented it accordingly. Also contains conversions of SPARQL queries to graphs. Enables e.g. subgraph isomorphism analysis.  * `jena-sparql-api-mapper`: Powerful module to query RDF data transparently with the Java Persistence API (JPA) criteria queries. I.e. queries and updates are expressed over (annotated) Java classes, and no RDF specifics are exposed to the developer.        ### Usage    Here is a brief summary of what you can do. A complete example is avaible [here](https://github.com/AKSW/jena-sparql-api/blob/master/jena-sparql-api-core/src/main/java/org/aksw/jena_sparql_api/example/Example.java).    Http Query Execution Factory  ```Java  QueryExecutionFactory qef = new QueryExecutionFactoryHttp(""http://dbpedia.org/sparql"", ""http://dbpedia.org"");  ```  Adding a 2000 millisecond delay in order to be nice to the backend  ```Java  qef = new QueryExecutionFactoryDelay(qef, 2000);  ```  Set up a cache    ```Java  // Some boilerplace code which may get simpler soon  long timeToLive = 24l * 60l * 60l * 1000l;   CacheCoreEx cacheBackend = CacheCoreH2.create(""sparql"", timeToLive, true);  CacheEx cacheFrontend = new CacheExImpl(cacheBackend);    qef = new QueryExecutionFactoryCacheEx(qef, cacheFrontend);  ```  Add pagination with (for the sake of demonstration) 900 entries per page (we could have used 1000 as well).  Note: Should the pagination abort, such as because you ran out of memory and need to adjust your settings, you can resume from cache!  ```Java  qef = new QueryExecutionFactoryPaginated(qef, 900);  ```  Create and run a query on this fully buffed QueryExecutionFactory  ```Java  String queryString = ""SELECT ?s { ?s a <http://dbpedia.org/ontology/City> } LIMIT 5000"";  QueryExecution qe = qef.createQueryExecution(queryString);  		  ResultSet rs = qe.execSelect();  System.out.println(ResultSetFormatter.asText(rs));  ```    ### Proxy Server Example  This example demonstrates how you can create your own SPARQL web service.  You only have to subclass `SparqlEndpointBase` and override the `createQueryExecution` method.  Look at the [Source Code](https://github.com/AKSW/jena-sparql-api/blob/master/jena-sparql-api-example-proxy/src/main/java/org/aksw/jena_sparql_api/example/proxy/SparqlEndpointProxy.java) to see how easy it is.    Running the example:  ```bash  cd jena-sparql-api-example-proxy  mvn jetty:run  # This will now start the proxy on part 5522  ```  In your browser or a terminal visit:    [http://localhost:5522/sparql?service-uri=http://dbpedia.org/sparql&query=Select * { ?s ?p ?o } Limit 10](http://localhost:5522/sparql?service-uri=http%3A%2F%2Fdbpedia.org%2Fsparql&query=Select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20Limit%2010)      ## License  The source code of this repo is published under the [Apache License Version 2.0](https://github.com/AKSW/jena-sparql-api/blob/master/LICENSE).    This project makes use of several dependencies: When in doubt, please cross-check with the respective projects:  * [Apache Jena](https://jena.apache.org/) (Apache License 2.0)  * [Atmosphere](https://github.com/Atmosphere/atmosphere) (Apache License 2.0/Partially CDDL License)  * [Guava](http://code.google.com/p/guava-libraries/) (Apache License 2.0)  * [commons-lang](http://commons.apache.org/proper/commons-lang/) (Apache License 2.0)  * [rdf-json-writer](https://github.com/kasabi/rdf-json-writer) (currently copied but also under Apache 2.0 license, will be changed to maven dep)       """
Semantic web;https://github.com/gbv/ssso;"""  This repository contains the specification of **Simple Service Status Ontology  (SSSO)**.    The URI of this ontology is <http://purl.org/ontology/ssso> and it's URI  namespace is <http://purl.org/ontology/ssso#>.    See <http://gbv.github.io/ssso> for a full documentation. RDF serializations  are available at <http://gbv.github.io/ssso/ssso.ttl> and  <http://gbv.github.io/ssso/ssso.owl>.    [Feedback](https://github.com/gbv/ssso/issues) is welcome!    # Overview    The following diagram illustrates the classes and properties definied in this ontology:    ``` {.ditaa}      nextService / previousService                 ------                |      |                v      v         +--------------------+         |    ServiceEvent    |         |                    |         |   ReservedService  |         |   PreparedService  |         |   ProvidedService  |         |   ExecutedService  |         |   RejectedService  |         |                    |         | ServiceFulfillment |         +-----^--------------+               |      ^               |      |                ------  dcterms:hasPart / dcterms:partOf  ```   """
Semantic web;https://github.com/tkurz/squebi;"""Squebi  ======    Squebi is a SPARQL editor and SPARQL result visualizer with some nice features:    * customization of SPARQL result visualization  * support for SPARQL 1.1 (update and select)  * bookmarkable uris that define queries and the visualization type  * support for SPARQL query editing (URIs, ontologies and prefixes)  * fast backend switch (quite useful for demo)  * nice GUI    Squebi is in use in the following projects and products:    * [Apache Marmotta](http://marmotta.apache.org)  * [Redlink Dashboard](http://redlink.co)  * [MICO - Media in Context](http://mico-project.eu)  * [Salzburgerland Data Hub](http://data.salzburgerland.com/dataset/events)    Installation  ------------    Squeby uses [bower](http://bower.io/) for dependency management. To get all dependencies, execute `bower install`.  The current version is v1.0.1.    Configuration  -------------    For configuration set your SPARQL endpoints (and additional parameters) by script and include it before the requirejs.    ### selectService : String (required)  The URI of the SPARQL select webservice (used via POST).    ### updateService : String (required)  The URI of the SPARQL update webservice (used via POST).    ### configurable : Boolean  If squebi allows dynamic changes of SPARQL endpoints.    **default: false**    ### automaticQuery : Boolean  If squebi automatically sends the current query after the page is loaded.    **default: true**    ### queryParams : Object  Additional query parameters as property:'value' pairs.    ### updateAllowed : Boolean  If UI allows SPARQL update queries.    **default: true**    ### namespaces : Object  Namespace prefixes as path:'prefix' pairs. Als prefixes that are not defined here will be looked up on prefix.cc.    ### container : String  The identifier (#id or .class) for the container that should be used for the application.    **default: '#squebi'**    ### appLoader : String  The identifier (#id or .class) for the container that is shown before the app is loaded completely. This container is hided on app startup complete.    **default: '#appLoader'**    ### app : String  The uri of the squebi app.    **default: '.'**    ### bower : String  The uri of the bower dependencies.    **default: 'bower_components'**    ### responseMessage : Object  Custom response messages based on http response as 'number':'message' pairs. If no response message is defined, the server response is used for display.    ### samples : List <Object>  A List if sample objects with properties 'name' (what is displayd), 'query', and 'type' (the id of the writer).    ## writers : List <String>  A list that includes the ids of the writers which are enabled.    ## Sample Configuration    ```javascript  SQUEBI = {      selectService : ""https://api.redlink.io/1.0-BETA/data/example/sparql/select"",      updateService : ""https://api.redlink.io/1.0-BETA/data/example/sparql/update"",      queryParams : {          key : ""mykeyasadditionalqueryparameter""      }  };  ```    ## Use Squebi as Webjar  You can use squebi in version 1.0.1 as webjar, too. The webjar is hosted on Maven Central. Put this dependencies to your pom    ```xml  <dependency>      <groupId>com.github.tkurz.webjars</groupId>      <artifactId>squebi</artifactId>      <version>1.0.1</version>  </dependency>  ```    Important: If you want to build your own webjar, please download the required bower dependencies first into the folder `bower_components`. """
Semantic web;https://github.com/MM2-0/rdf4a;"""# RDF4A - Porting RDF4J to Android    This is a port of RDF4J to the Android platform.    ## What is RDF4J?    > Eclipse RDF4J (formerly known as Sesame) is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions.    [RDF4J homepage](http://rdf4j.org/)    ## Setup    Install to local Maven repository  ```  mvn package -DskipTests && mvn install -DskipTests  ```    Include in an Android project  ```  //build.gradle  repositories {      mavenLocal()  }  dependencies {      compile 'de.mm20.rdf4a:rdf4j-model:1.1-SNAPSHOT'      [...]  }  ``` """
Semantic web;https://github.com/trellis-ldp/trellis;"""# Trellis Linked Data Server    A scalable platform for building [linked data](https://www.w3.org/TR/ldp/) applications.    ![Build Status](https://github.com/trellis-ldp/trellis/workflows/GitHub%20CI/badge.svg)  [![Coverage](https://sonarcloud.io/api/project_badges/measure?project=org.trellisldp%3Atrellis&metric=coverage)](https://sonarcloud.io/dashboard?id=org.trellisldp%3Atrellis)  ![Maven Central](https://img.shields.io/maven-central/v/org.trellisldp/trellis-api.svg)    Trellis is a rock-solid, enterprise-ready linked data server.  The quickest way to get started with Trellis is to use  a pre-built [docker container](https://hub.docker.com/r/trellisldp/trellis).    Trellis is built on existing [Web standards](https://github.com/trellis-ldp/trellis/wiki/Web-Standards).  It is modular, extensible and fast.    * [Wiki](https://github.com/trellis-ldp/trellis/wiki)  * [Mailing List](https://groups.google.com/group/trellis-ldp)  * [API Documentation](https://www.trellisldp.org/docs/trellis/current/apidocs/) (JavaDocs)  * [Website](https://www.trellisldp.org)    All source code is open source and licensed as Apache 2. Contributions are always welcome.    ## Docker Containers    Docker containers for Trellis are published on [Docker Hub](https://hub.docker.com/u/trellisldp).  Container environments are published with every commit to the `main` branch and are available for all stable  releases. More details are available on the  [Trellis Wiki](https://github.com/trellis-ldp/trellis/wiki/Dockerized-Trellis).    Docker pull command    ```bash  docker pull trellisldp/trellis-triplestore  ```    Or, for the PostgreSQL-based persistence layer    ```bash  docker pull trellisldp/trellis-postgresql  ```    ## Building Trellis    In most cases, you won't need to compile Trellis. Released components are available on Maven Central,  and the deployable application can be [downloaded](https://www.trellisldp.org/download.html) directly  from the Trellis website. However, if you want to build the latest snapshot, you will need, at the very least,  to have Java 11+ available. The software can be built with [Gradle](https://gradle.org) using this command:    ```bash  ./gradlew install  ```    ## Related projects    * [py-ldnlib](https://github.com/trellis-ldp/py-ldnlib) A Python3 library for linked data notifications  * [static-ldp](https://github.com/trellis-ldp/static-ldp) A PHP application that serves static files as LDP resources  * [camel-ldp-recipes](https://github.com/trellis-ldp/camel-ldp-recipes) Integration workflows built with [Apache Camel](https://camel.apache.org)   """
Semantic web;https://github.com/kasei/swift-serd;"""serd-swift  ===    Swift package wrapper for the [Serd RDF library](http://drobilla.net/software/serd). """
Semantic web;https://github.com/newres/aesopica;"""# Aesopica    A Clojure library designed to help create Semantic Web, and in particular Linked Data/RDF based applications.   It allows the user to create Linked Data using idiomatic Clojure datastructures, and translate them to various RDF formats.    ## Example Usage      ```clojure  (ns example     (:require [aesopica.core :as aes]               [aesopica.converter :as conv]))    (def fox-and-stork-edn    {::aes/context     {nil ""http://www.newresalhaider.com/ontologies/aesop/foxstork/""      :rdf ""http://www.w3.org/1999/02/22-rdf-syntax-ns#""}     ::aes/facts     #{[:fox :rdf/type :animal]       [:stork :rdf/type :animal]       [:fox :gives-invitation :invitation1]       [:invitation1 :has-invited :stork]       [:invitation1 :has-food :soup]       [:invitation1 :serves-using :shallow-plate]       [:stork :gives-invitation :invitation2]       [:invitation2 :has-invited :fox]       [:invitation2 :has-food :crumbled-food]       [:invitation2 :serves-using :narrow-mouthed-jug]       [:fox :can-eat-food-served-using :shallow-plate]       [:fox :can-not-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-not-eat-food-served-using :shallow-plate]}})      (conv/convert-to-turtle fox-and-stork-edn)  ```  ## Features    ### String, Integer, Boolean, Long and Custom Datatypes    ```clojure  (def fox-and-stork-literals-edn    {::aes/context     {nil ""http://www.newresalhaider.com/ontologies/aesop/foxstork/""      :rdf ""http://www.w3.org/1999/02/22-rdf-syntax-ns#""      :foaf ""http://xmlns.com/foaf/0.1/""      :xsd ""http://www.w3.org/2001/XMLSchema#""}     ::aes/facts     #{[:fox :rdf/type :animal]       [:fox :foaf/name ""vo""]       [:fox :foaf/age 2]       [:fox :is-cunning true]       [:fox :has-weight 6.8]       [:stork :rdf/type :animal]       [:stork :foaf/name ""ooi""]       [:stork :foaf/age 13]       [:stork :is-cunning true]       [:dinner1 :has-date {::aes/value ""2002-05-30T18:00:00"" ::aes/type :xsd/dateTime}]}})  ```  ### Quads/Named Graphs      ```clojure  (def fox-and-stork-reif-edn    {::aes/context     {nil ""http://www.newresalhaider.com/ontologies/aesop/foxstork/""      :rdf ""http://www.w3.org/1999/02/22-rdf-syntax-ns#""}     ::aes/facts     #{[:fox :rdf/type :animal :dinner1]       [:stork :rdf/type :animal :dinner1]       [:fox :gives-invitation :invitation1 :dinner1]       [:invitation1 :has-invited :stork :dinner1]       [:invitation1 :has-food :soup :dinner1]       [:invitation1 :serves-using :shallow-plate :dinner1]       [:stork :gives-invitation :invitation2 :dinner2]       [:invitation2 :has-invited :fox :dinner2]       [:invitation2 :has-food :crumbled-food :dinner2]       [:invitation2 :serves-using :narrow-mouthed-jug :dinner2]       [:fox :can-eat-food-served-using :shallow-plate]       [:fox :can-not-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-not-eat-food-served-using :shallow-plate]}})  ```    ### Blank Nodes       ```clojure  (def fox-and-stork-blank-node-edn    {::aes/context     {nil ""http://www.newresalhaider.com/ontologies/aesop/foxstork/""      :rdf ""http://www.w3.org/1999/02/22-rdf-syntax-ns#""}     ::aes/facts     #{[:fox :rdf/type :animal]       [:stork :rdf/type :animal]       [:fox :gives-invitation 'invitation1]       ['invitation1 :has-invited :stork]       ['invitation1 :has-food :soup]       ['invitation1 :serves-using :shallow-plate]       [:stork :gives-invitation 'invitation2]       ['invitation2 :has-invited :fox]       ['invitation2 :has-food :crumbled-food]       ['invitation2 :serves-using :narrow-mouthed-jug]       [:fox :can-eat-food-served-using :shallow-plate]       [:fox :can-not-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-not-eat-food-served-using :shallow-plate]}})  ```    ### Conversion to Common Formats such as Turtle, Trig, N-Quads, JSON-LD    The conversion utilizes the [Apache Jena](https://jena.apache.org/) library for conversion.   First the Clojure EDN representation needs to be converted to a [Jena DataSetGraph](http://jena.apache.org/documentation/javadoc/arq/org/apache/jena/sparql/core/DatasetGraph.html) (a Jena representation of a set of graphs).  Afterwards the Clojure functions that utilize and wrap Jena's [RDF I/O technology (RIOT)](https://jena.apache.org/documentation/io/) can be called.     Assuming `fox-and-stork-edn` is a Clojure EDN representation of RDF, and `conv` the shorthand for the `aesopica.converter` namespace, a conversion to Turtle can be written as:    ```clojure  (conv/convert-to-turtle fox-and-stork-edn)  ```  See the `aesopica.converter` namespace and related tests for more examples.     Note that certain formats, such as Turtle, are not designed with quads/named graphs in mind.  In cases such as these, a converter to a format that supports quads need to be used (e.g.: TriG, N-Quads) to not lose information.    ## Design Decisions and Tutorial    I have been writing a number of articles about the use of Clojure for creating Linked Data, that is interlinked with the creation of this library:    1. [General Introduction](https://www.newresalhaider.com/post/aesopica-1/)  2. [Datatypes](https://www.newresalhaider.com/post/aesopica-2/)  2. [Named Graphs](https://www.newresalhaider.com/post/aesopica-3/)    ## License    Copyright © 2018 Newres Al Haider    Distributed under the Eclipse Public License (see the LICENSE file).  """
Semantic web;https://github.com/kasei/swift-sparql-syntax;"""# SPARQLSyntax    ## SPARQL 1.1 Parser and Abstract Syntax     - [Features](#features)   - [Building](#building)   - [Swift Package Manager](#swift-package-manager)   - [Command Line Usage](#command-line-usage)   - [API](#api)     - [Term](#term)     - [Triple, Quad, TriplePattern and QuadPattern](#triple-quad-triplepattern-and-quadpattern)     - [Algebra](#algebra)     - [Expression](#expression)     - [Query](#query)     - [SPARQLParser](#sparqlparser)     - [SPARQLSerializer](#sparqlserializer)   - [Extensions](#extensions)     - [Window Functions](#window-functions)    ### Features    * [SPARQL 1.1] Parser, Tokenizer, and Serializer available via both API and command line tool  * Abstract syntax representation of SPARQL queries, aligned with the [SPARQL Algebra]  * Supported extensions:    - [Window Functions](#window-functions)    ### Building    ```  % swift build -c release  ```    ### Swift Package Manager    To use SPARQLSyntax with projects using the [Swift Package Manager],  add the following to your project's `Package.swift` file:      ```swift    dependencies: [      .package(url: ""https://github.com/kasei/swift-sparql-syntax.git"", .upToNextMinor(from: ""0.0.91""))    ]    ```    ### Command Line Usage    A command line tool, `sparql-parser`, is provided to parse a SPARQL query and  print its parsed query algebra, its tokenization, or a pretty-printed SPARQL  string:    ```  % ./.build/release/sparql-parser   Usage: ./.build/release/sparql-parser [-v] COMMAND [ARGUMENTS]         ./.build/release/sparql-parser parse query.rq         ./.build/release/sparql-parser lint query.rq         ./.build/release/sparql-parser tokens query.rq  ```    To ""lint"", or ""pretty print"", a SPARQL query:    ```  % cat examples/messy.rq  prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>  select    ?s  where{  ?s geo:lat ?lat ;geo:long ?long   ;  	FILTER(?long < -117.0)  FILTER(?lat >= 31.0)    FILTER(?lat <= 33.0)  } ORDER BY ?s    % ./.build/release/sparql-parser lint examples/messy.rq   PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>  SELECT ?s WHERE {      ?s geo:lat ?lat ;          geo:long ?long ;      FILTER (?long < - 117.0)      FILTER (?lat >= 31.0)      FILTER (?lat <= 33.0)  }  ORDER BY ?s    ```    To parse the query and print the resulting query algebra:    ```  % ./.build/release/sparql-parser parse examples/messy.rq  Query    Select { ?s }          Project { ?s }            OrderBy { ?s }              Filter (((?long < -117.0) && (?lat >= 31.0)) && (?lat <= 33.0))                BGP                  ?s <http://www.w3.org/2003/01/geo/wgs84_pos#lat> ?lat .                  ?s <http://www.w3.org/2003/01/geo/wgs84_pos#long> ?long .    ```    ### API    The `SPARQLSyntax` library provides an API for parsing SPARQL queries  and accessing the resulting abstract data structures.  The primary components of this API are:    * `struct Term` - A representation of an RDF Term (IRI, Literal, or Blank node)  * `enum Algebra` - A representation of the query pattern closely aligned with the formal SPARQL Algebra  * `enum Expression` - A representation of a logical expression  * `struct Query` - A representation of a SPARQL query including: a query form (`SELECT`, `ASK`, `DESCRIBE`, or `CONSTRUCT`), a query `Algebra`, and optional base URI and dataset specification  * `struct SPARQLParser` - Parses a SPARQL query String/Data and returns a `Query`  * `struct SPARQLSerializer` - Provides the ability to serialize a query, optionally applying ""pretty printing"" formatting    #### `Term`    `struct Term` represents an [RDF Term] (an IRI, a blank node, or an RDF Literal).  `Term` also provides some support for XSD numeric types,  bridging between `Term`s and `enum NumericValue` which provides numeric functions and [type-promoting operators](https://www.w3.org/TR/xpath20/#promotion).    #### `Triple`, `Quad`, `TriplePattern`, and `QuadPattern`    `struct Triple` and `struct Quad` combine `Term`s into RDF triples and quads.  `struct TriplePattern` and `struct QuadPattern` represent patterns which can be matched by concrete `Triple`s and `Quad`s.  Instead of `Term`s, patterns are comprised of a `enum Node` which can be either a bound `Term`, or a named `variable`.    #### `Algebra`    `enum Algebra` is an representation of a query pattern aligned with the [SPARQL Algebra].  Cases include simple graph pattern matching such as `triple`, `quad`, and `bgp`,  and more complex operators that can be used to join other `Algebra` values  (e.g. `innerJoin`, `union`, `project`, `distinct`).    `Algebra` provides functions and properties to access features of graph patterns including:  variables used; and in-scope, projectable, and ""necessarily bound"" variables.  The structure of `Algebra` values can be modified using a rewriting API that can:  bind values to specific variables; replace entire `Algebra` sub-trees; and rewrite `Expression`s used within the `Algebra`.    #### `Expression`    `enum Expression` represents a logical expression of variables, values, operators, and functions  that can be evaluated within the context of a query result to produce a  `Term` value.  `Expression`s are used in the following `Algebra` operations: filter, left outer join (""OPTIONAL""), extend (""BIND""), and aggregate.    `Expression`s may be modified using a similar rewriting API to that provided by `Algebra` that can:  bind values to specific variables; and replace entire `Expression` sub-trees.    #### `Query`    `struct Query` represents a SPARQL Query and includes:    * a query form (`SELECT`, `ASK`, `DESCRIBE`, or `CONSTRUCT`, and any associated data such as projected variables, or triple patterns used to `CONSTRUCT` a result graph)  * a graph pattern (`Algebra`)  * an optional base URI  * an optional dataset specification    #### `SPARQLParser`    `struct SPARQLParser` provides an API for parsing a SPARQL 1.1 query string and producing a `Query`.    #### `SPARQLSerializer`    `struct SPARQLSerializer` provides an API for serializing SPARQL 1.1 queries, optionally applying ""pretty printing"" rules to produce consistently formatted output.  It can serialize both structured queries (`Query` and `Algebra`) and unstructured queries (a query `String`).  In the latter case, serialization can be used even if the query contains syntax errors (with data after the error being serialized as-is).    ### Extensions    #### Window Functions    Parsing of window functions is supported as an extension to the SPARQL 1.1 syntax.  A SQL-like syntax is supported for projecting window functions in a `SELECT` clause, as well as in a `HAVING` clause.  In addition to the built-in aggregate functions, the following window functions are supported:  `RANK`, `ROW_NUMBER`.    Shown below are some examples of the supported syntax.    ```swift  # ""Limit By Resource""  # This query limits results to two name/school pairs per person  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  SELECT ?name ?school WHERE {  	?s a foaf:Person ;  		foaf:name ?name ;  		foaf:schoolHomepage ?school  }  HAVING (RANK() OVER (PARTITION BY ?s) < 2)  ```    ```swift  # Use window framing to compute a moving average over the trailing four results  PREFIX : <http://example.org/>  SELECT (AVG(?value) OVER (ORDER BY ?date ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS ?movingAverage) WHERE {  	VALUES (?date ?value) {  		(1 1.0)  		(2 2.0)  		(3 3.0)  		(4 2.0)  		(5 0.0)  		(6 0.0)  		(7 1.0)  	}  }  ```      [SPARQL 1.1]: https://www.w3.org/TR/sparql11-query  [SPARQL Algebra]: https://www.w3.org/TR/sparql11-query/#sparqlAlgebra  [Swift Package Manager]: https://swift.org/package-manager  [RDF Term]: https://www.w3.org/TR/sparql11-query/#sparqlBasicTerms """
Semantic web;https://github.com/aldonline/fbrs;"""# FBRS = Facebook RDF Sync    Facebook's Graph API can return (almost) proper Linked Data when asked to in a polite manner. ( Accept: text/turtle ).  However, we can still not SPARQL the complete dataset.    Our solution is to load a subset of Facebook's graph at any given time.  Which subset to load is a tricky question and will depend on the use cases.  This framework supports some common ways of loading data from Facebook, including batch, incremental,  as well as keeping data in sync using Facebook's Real-Time updates.    ## Setup    * You will need the following dependencies:   * Node v0.6.x   * OpenLink Virtuoso >= 6.1.3   * Virtuoso's iSQL client library on the path    FBRS will access Virtuoso via both the iSQL client library and the SPARQL HTTP endpoint.  You can configure the ports later on.    Clone this repo:        git@github.com:aldonline/fbrs.git    And install dependencies:        cd fbrs      npm install -d    For example:    * Load all data   ...    ## Usage    You can run FBRS in batch mode or via an HTTP Server. Bath is simpler, but does not   keep the data in sync.    ### Batch ( load everything once or incremental )        ### Web Server ( With Real-Time Updates )        FBRS_SPARQL_ENDPOINT=""http://localhost:8890/sparql""      FBRS_VIRTUOSO_PORT=1111      FBRS_VIRTUOSO_USERNAME=""dba""      FBRS_VIRTUOSO_PASSWORD=""dba""      FBRS_PORT=3008      FBRS_CALLBACK_PORT=3009      FBRS_ACCESS_TOKEN=...       """
Semantic web;https://github.com/R2RML-api/R2RML-api;"""[![Build Status](https://travis-ci.org/R2RML-api/R2RML-api.svg?branch=master)](https://travis-ci.org/R2RML-api)    R2RML-api  =========    ## How to use R2RML-api as a Maven dependency    * put the following fragments into your `pom.xml`    ```xml              <dependencies>  		<!-- Optique R2RML API -->  		<dependency>  			<groupId>eu.optique-project</groupId>  			<artifactId>r2rml-api-core</artifactId>  			<version>0.6.0</version>  		</dependency>            <!-- Optique R2RML API RDF4J Binding -->  		<dependency>  			<groupId>eu.optique-project</groupId>  			<artifactId>r2rml-api-rdf4j-binding</artifactId>  			<version>0.6.0</version>  		</dependency>            <!-- Optique R2RML API Jena Binding -->          <dependency>  			<groupId>eu.optique-project</groupId>  			<artifactId>r2rml-api-jena-binding</artifactId>  			<version>0.6.0</version>  		</dependency>  	</dependencies>  ```      ## Release history    <a name=""v0.6.0""></a>  ### 1 March, 2017 ::  Version 0.6.0   * code refactor      <a name=""v0.5.0""></a>  ### 13 January, 2017 ::  Version 0.5.0   * A major rewriting using commons-rdf   * retired owlapi-binding     <a name=""v0.4.0""></a>  ### 3 January, 2017 ::  Version 0.4.0   * Upgrade Sesame to RDF4J 2.1.4      <a name=""v0.3.0""></a>  ### 26 Feb, 2016 ::  Version 0.3.0   * Upgrade Jena to v3    <a name=""v0.2.1""></a>  ### 23 Feb, 2016 ::  Version 0.2.1   * Fix an issue of deploying `r2rml-api-jena-bridge` to central repository    <a name=""v0.2.0""></a>  ### 23 Feb, 2016 ::  Version 0.2.0  * Upgrade OWL-API to v4  * Deployed to central maven repository    <a name=""v0.1.0""></a>  ### 2014 ::  Version 0.1.0  * First release    ## Reference    [1] ""An R2RML Mapping Management API in Java"", Marius Strandhaug, Master’s Thesis Spring 2014, University of Oslo.    [2] The javadoc documentation of the API v0.1 can be found by following this URL: http://folk.uio.no/marstran/doc/.   """
Semantic web;https://github.com/abcoates/sublime-text-turtle-sparql;"""README for sublime-text-turtle-sparql  =====================================    Sublime Text syntax and completions files for Turtle (and, later, SPARQL). Developed for Sublime Text 3 beta.    Copy the 'Turtle' and 'SPARQL' directories into your Sublime text 'Packages' directory (use 'Preferences | Browse Packages...' in Sublime Text to find where that is).    Sublime Text uses the same syntax mechanism as TextMate and other editors.  [See unofficial Sublime Text documentation for details.](http://docs.sublimetext.info/en/latest/extensibility/syntaxdefs.html)    For information on Sublime Text completions, [see the unofficial Sublime Text documentation for details.](http://docs.sublimetext.info/en/latest/extensibility/completions.html) """
Semantic web;https://github.com/AKSW/SemanticPingback;"""# Semantic Pingback Vocabulary    This small vocabulary defines resources which are used in the context  of Semantic Pingback. The Semantic Pingback mechanism is an extension  of the well-known Pingback method, a technological cornerstone of the  blogosphere, thus supporting the interlinking within the Data Web.    More information about Semantic Pingback are available at:  * http://aksw.org/Projects/SemanticPingback  * https://aksw.github.io/SemanticPingback/    Semantic Pingback also used in the architecture of the  [Distributed Semantic Social Network (DSSN)](http://aksw.org/Projects/DSSN)  and in the [Structured Feedback](http://feedback.aksw.org/) protocol. """
Semantic web;https://github.com/ldp4j/ldp4j;"""LDP4j  =====    A Java-based framework for the development of read-write Linked Data applications based on the W3C Linked Data Platform 1.0 (LDP) specification.    LDP4j is distributed under the Apache License, version 2.0.    For more information, please visit [LDP4j website](http://www.ldp4j.org/).    [![Build Status](https://travis-ci.org/ldp4j/ldp4j.svg?branch=master)](https://travis-ci.org/ldp4j/ldp4j)  [![Coverage](https://img.shields.io/sonar/http/analysis.ldp4j.org/sonar/org.ldp4j:ldp4j-parent:master/coverage.svg)](http://analysis.ldp4j.org/sonar/)  [![Technical Debt](https://img.shields.io/sonar/http/analysis.ldp4j.org/sonar/org.ldp4j:ldp4j-parent:master/tech_debt.svg)](http://analysis.ldp4j.org/sonar/)  [![Version](https://img.shields.io/maven-central/v/org.ldp4j/ldp4j-parent.svg?style=flat)](https://github.com/ldp4j/ldp4j/releases)  [![License](https://img.shields.io/github/license/ldp4j/ldp4j.svg)](http://www.apache.org/licenses/LICENSE-2.0)"""
Semantic web;https://github.com/LinkedBrainz/MusicBrainz-R2RML;"""MusicBrainz-R2RML  =================    R2RML mappings for the MusicBrainz schema on an entity-by-entity basis.     These can be run on the MusicBrainz server using ultrawrap, for which a script is provided (`dump.sh`).  You must set an environment variable `ULTRAWRAP_HOME`.    Running `musicbrainz-r2rml/dump.sh entity` (where entity is artist, track, etc.) runs the appropriate set of mappings (e.g. `mappings/artist.ttl`) to produce output in the form of NTriples (e.g. `output/artist.nt`).    A virtual machine is available (for use with VirtualBox, VMware, etc.) with a replicated MusicBrainz database.    Note that the file `musicbrainz_compile_config.properties` must reflect your DB name:  * `musicbrainz_db` is the default for a snapshot  * `musicbrainz_db_slave` is the default for a replicated database    Please report any issues on [our Jira tracker](https://tickets.metabrainz.org/), under the `LINKB` project. """
Semantic web;https://github.com/CLARIAH/COW;"""## CoW: Integrated CSV to RDF Converter    > CoW (Csv on the Web) is an integrated CSV to RDF converter that uses the W3C standard [CSVW](https://www.w3.org/TR/tabular-data-primer/) for rich semantic table specificatons, and [nanopublications](http://nanopub.org/) as an output RDF model        ### What is CoW    CoW is a command-line utility to convert any CSV file into an RDF dataset. Its distinctive features are:    - Expressive CSVW-compatible schemas based on the [Jinja](https://github.com/pallets/jinja) template enginge  - Highly efficient implementation leveraging multithreaded and multicore architectures  - Available as a pythonic [CLI tool](#cli), [library](#library), and [web service](#web-service)  - Supports Python 3    ### Install (requires Python to be installed)    `pip3` is the recommended method of installing COW in your system:    ```  pip3 install cow-csvw  ```    You can upgrade your currently installed version with:    ```  pip3 install cow-csvw --upgrade  ```    Possible issues:    - Permission issues. You can get around them by installing CoW in user space: `pip3 install cow-csvw --user`. Make sure your binary user directory (typically something like `/Users/user/Library/Python/3.7/bin` in MacOS or `/home/user/.local/bin` in Linux) is in your PATH. For Windows/MacOS we recommend to install Python via the [official distribution page](https://www.python.org/downloads/). You can also use [virtualenv](https://virtualenv.pypa.io/en/latest/) to avoid conflicts with your system libraries  - Please [report your unlisted issue](https://github.com/CLARIAH/CoW/issues/new)    If you can't/don't want to deal with installing CoW, you can use the [cattle](http://cattle.datalegend.net/) [web service version](#web-service) (deprecated).    ### Usage    #### CLI    The CLI (command line interface) is the recommended way of using CoW for most users. The straightforward CSV to RDF conversion is done in two steps. First:    ```  cow_tool build myfile.csv  ```    This will create a file named `myfile.csv-metadata.json` (from now on: JSON schema file or just JSF). You don't need to worry about this file if you only want a syntactic conversion. Then:    ```  cow_tool convert myfile.csv  ```    Will output a `myfile.csv.nq` RDF file (nquads by default; you can control the output RDF serialization with e.g. ``--format turtle``). That's it!    If you want to control the base URI namespace, URIs used in predicates, virtual columns, and the many other features of CoW, you'll need to edit the `myfile.csv-metadata.json` JSF and/or use CoW arguments. Have a look at the [CLI options](#options) below, the examples in the [wiki](https://github.com/CLARIAH/CoW/wiki), and the [technical documentation](http://csvw-converter.readthedocs.io/en/latest/).    ##### Options    Check the ``--help`` for a complete list of options:    ```  usage: cow_tool [-h] [--dataset DATASET] [--delimiter DELIMITER]                  [--quotechar QUOTECHAR] [--encoding ENCODING] [--processes PROCESSES]                  [--chunksize CHUNKSIZE] [--base BASE]                  [--format [{xml,n3,turtle,nt,pretty-xml,trix,trig,nquads}]]  				[--gzip] [--version]                  {convert,build} file [file ...]    Not nearly CSVW compliant schema builder and RDF converter    positional arguments:    {convert,build}       Use the schema of the `file` specified to convert it                          to RDF, or build a schema from scratch.    file                  Path(s) of the file(s) that should be used for                          building or converting. Must be a CSV file.    optional arguments:    -h, --help            show this help message and exit    --dataset DATASET     A short name (slug) for the name of the dataset (will                          use input file name if not specified)    --delimiter DELIMITER                          The delimiter used in the CSV file(s)    --quotechar QUOTECHAR                          The character used as quotation character in the CSV                          file(s)    --encoding ENCODING   The character encoding used in the CSV file(s)      --processes PROCESSES                          The number of processes the converter should use    --chunksize CHUNKSIZE                          The number of rows processed at each time    --base BASE           The base for URIs generated with the schema (only                          relevant when `build`ing a schema)    --gzip 				Compress the output file using gzip    --format [{xml,n3,turtle,nt,pretty-xml,trix,trig,nquads}], -f [{xml,n3,turtle,nt,pretty-xml,trix,trig,nquads}]                          RDF serialization format    --version             show program's version number and exit  ```    #### Web service    There is web service and interface running CoW, called [cattle](http://cattle.datalegend.net/). Two public instances are running at:    - http://cattle.datalegend.net/ - runs CoW in Python3  - http://legacy.cattle.datalegend.net/ - runs CoW in Python2 for legacy reasons    Beware of the web service limitations:    - There's a limit to the size of the CSVs you can upload  - It's a public instance, so your conversion could take longer  - Cattle is no longer being maintained and these public instances will eventually be taken offline    #### Library    Once installed, CoW can be used as a library as follows:    ```  from cow_csvw.csvw_tool import CoW  import os    COW(mode='build', files=[os.path.join(path, filename)], dataset='My dataset', delimiter=';', quotechar='\""')    COW(mode='convert', files=[os.path.join(path, filename)], dataset='My dataset', delimiter=';', quotechar='\""', processes=4, chunksize=100, base='http://example.org/my-dataset', format='turtle', gzipped=False)  ```    ### Documentation    Technical documentation for CoW are maintained in this GitHub repository (under <docs>), and published through [Read the Docs](http://readthedocs.org) at <http://csvw-converter.readthedocs.io/en/latest/>.    To build the documentation from source, change into the `docs` directory, and run `make html`. This should produce an HTML version of the documentation in the `_build/html` directory.    ### Examples    The [wiki](https://github.com/CLARIAH/COW/wiki) provides more hands-on examples of transposing CSVs into Linked Data    ### License    MIT License (see [license.txt](license.txt))    ### Acknowledgements    **Authors:**    Albert Meroño-Peñuela, Roderick van der Weerdt, Rinke Hoekstra, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Melvin Roest, Xander Wilcke    **Copyright:**  Vrije Universiteit Amsterdam, Utrecht University, International Institute of Social History      CoW is developed and maintained by the CLARIAH project and funded by NWO. """
Semantic web;https://github.com/fafalios/sparql-ld;"""# SPARQL-LD: A SPARQL Extension for Fetching and Querying Linked Data    This Jena ARQ SERVICE extension allows to fetch, query and integrate in the same SPARQL query:  - *data stored in the (local) endpoint*  - *data coming from online RDF files (in any standard format)*  - *data embedded in Web pages as RDFa*  - *data coming from JSON-LD files*  - *data coming from dereferenceable URIs*  - *data (in RDF) dynamically created by a Web Service (Web API)*  - *data coming by querying other SPARQL endpoints*    by simply using the SERVICE operator of [SPARQL 1.1 Federated Query](http://www.w3.org/TR/sparql11-federated-query/).    A distinctive characteristic of SPARQL-LD is that it enables to   fetch and query even data in datasets returned by a portion of the query,  i.e. discovered at query-execution time.     SPARQL-LD is actually a generalization of SPARQL  in the sense that every query that can be answered by the original SPARQL can  be also answered by SPARQL-LD. Specifically, if the IRI given to the service  operator corresponds to a SPARQL endpoint, then it works exactly as the original  SPARQL (the remote endpoint evaluates the query and returns the result).  Otherwise, instead of returning an error (and no bindings), it tries to fetch and  query the triples that may exist in the given resource.      SPARQL-LD has been tested with Jena 2.13.0 ARQ (nevertheless, it may also work with other Jena ARQ releases).       ### Demo    A prototype implemention of a SPARQL endpoint that support SPARQL-LD is available at:   * https://demos.isl.ics.forth.gr/sparql-ld-endpoint/    ### Related publications    Full Paper describing SPARQL-LD:  ```  P. Fafalios, T. Yannakis and Y. Tzitzikas,  ""Querying the Web of Data with SPARQL-LD"",   20th International Conference on Theory and Practice of Digital Libraries (TPDL'16),   Hannover, Germany, September 5-9, 2016  ```   [PDF](http://l3s.de/~fafalios/files/pubs/fafalios_2016_tpdl.pdf) | [BIB](http://l3s.de/~fafalios/files/bibs/fafalios2016sparql-ld.bib)     Demo Paper describing SPARQL-LD:  ```  P. Fafalios and Y. Tzitzikas,  ""SPARQL-LD: A SPARQL Extension for Fetching and Querying Linked Data"",   14th International Semantic Web Conference (Demo Paper) - ISWC'15,   Bethlehem, Pennsylvania, USA, October 11-15, 2015   ```   [PDF](http://users.ics.forth.gr/~fafalios/files/pubs/fafalios_2015_sparql-ld.pdf) | [BIB](http://users.ics.forth.gr/~fafalios/files/bibs/fafalios2015sparql.bib)    Paper on optimizing the execution of SPARQL-LD queries through query reordering:   ```  T. Yannakis, P. Fafalios, and Y. Tzitzikas,  ""Heuristics-based Query Reordering for Federated Queries in SPARQL 1.1 and SPARQL-LD"",   2nd Workshop on Querying the Web of Data (QuWeDa), in conjunction with the 15th Extended Semantic Web Conference (ESWC'18),   Heraklion, Greece, June 3-7, 2018  ```   [PDF](http://l3s.de/~fafalios/files/pubs/fafalios2018_QuWeDa.pdf) | [BIB](http://l3s.de/~fafalios/files/bibs/fafalios2018_QuWeDa.bib) | [SOURCE CODE](https://github.com/TYannakis/SPARQL-LD-Query-Optimizer)    Paper on how a SPARQL query (to be evaluated on a SPARQL endpoint) can be transformed to a SPARQL-LD query that is answered through   link traversal, without accessing the endpoint:  ```  P. Fafalios, and Y. Tzitzikas,  ""How Many and What Types of SPARQL Queries can be Answered through Zero-Knowledge Link Traversal?"",   34th ACM/SIGAPP Symposium On Applied Computing (Semantic Web and Applications track),   Limassol, Cyprus, April 8-12, 2019  ```   [PDF](http://users.ics.forth.gr/~fafalios/files/pubs/SAC2019_ZeroKnowledgeLinkTraversal.pdf) | [BIB](http://users.ics.forth.gr/~fafalios/files/bibs/fafalios2019_SAC_LinkTraversal.bib) | [SOURCE CODE](https://github.com/fafalios/LDaQ)     ## Example Query    The following query   can be answered by an implementation of SPARQL-LD.  The query returns all co-authors of Pavlos Fafalios (main contributor of this repository)  together with the number of their publications and the number of different conferences  in which they have a publication.  Notice that this query combines and integrates:  i) data embedded in the HTML Web page http://users.ics.forth.gr/~fafalios as RDFa (lines 3-4),  ii) data coming from dereferenceable URIs derived at *query-execution* time (lines 5-6), and  iii) data coming by querying another endpoint (lines 7-9).  Note also that this query can be answered by any endpoint that implements  this extension (independently of its ""local"" contents).    ```  1.  SELECT DISTINCT ?authorURI (count(distinct ?paper)  AS ?numOfPapers)  2.                             (count(distinct ?series) AS ?numOfDiffConfs) WHERE {  3.    SERVICE <http://users.ics.forth.gr/~fafalios/> {  4.      ?p <http://purl.org/dc/terms/creator> ?authorURI }  5.    SERVICE ?authorURI {   6.      ?paper <http://purl.org/dc/elements/1.1/creator> ?authorURI }  7.    SERVICE <http://dblp.l3s.de/d2r/sparql> {  8.      ?p2 <http://purl.org/dc/elements/1.1/creator> ?authorURI .  9.      ?p2 <http://swrc.ontoware.org/ontology#series> ?series  }  10. } GROUP BY ?authorURI ORDER BY ?numOfPapers  ```     ## Source code    For implementing SPARQL-LD, we have created the following 4 classes:    - com.hp.hpl.jena.sparql.engine.http.**ReadRDFFromIRI**  - com.hp.hpl.jena.sparql.engine.http.**ResourcesCache**  - com.hp.hpl.jena.sparql.engine.http.**EndpointsIndex**  - arq.**SPARQL_LD_QueryExamples**    We have also updated the following 2 classes of Jena 2.13.0 ARQ:    - com.hp.hpl.jena.sparql.engine.**QueryExecutionBase**  - com.hp.hpl.jena.sparql.engine.http.**Service**      This repository contains only the above 6 classes.   We also provide a zip containing the *original* Jena 2.13.0 ARQ source code  (as downloaded from [https://jena.apache.org/download](https://jena.apache.org/download) in April 17, 2015)  as well as the extended, already built, Jena ARQ JAR file (**jena-arq-2.13.0_SPARQL-LD-1.1.jar**) and the corresponding extended Jena sources (**jena-arq-2.13.0-sources_SPARQL-LD-1.1.jar**).     ## Installation    - Directly use the provided (already built) extended Jena ARQ jar:        **jena-arq-2.13.0_SPARQL-LD-1.1.jar**    OR    - Download the original Jena 2.13.0 ARQ source code  - Add the 4 new classes  - Replace the 2 updated classes  - Add the *endpoints.lst* file to the project folder (same level as pom.xml)  - Add the following dependency to pom.xml (which allows to load and query RDFa data):  ```   <dependency>     <groupId>org.semarglproject</groupId>     <artifactId>semargl-jena</artifactId>     <version>0.6.1</version>     <exclusions>       <exclusion>         <groupId>org.apache.jena</groupId>         <artifactId>jena-core</artifactId>       </exclusion>     </exclusions>   </dependency>  ```	  - Build the sources  - Try to run the main class ""arq.SPARQL_LD_QueryExamples""   """
Semantic web;https://github.com/jbmusso/awesome-graph;"""# awesome-graph    A curated list of resources for graph databases and graph computing tools    ## Graph databases    * [AgensGraph](https://bitnine.net/agensgraph-2/) - multi-model graph database with SQL and Cypher support  * [AnzoGraph](https://www.cambridgesemantics.com/anzograph/) - Massively parallel graph database with advanced analytics (SPARQL, Cypher, OWL/RDFS+, LPG)   * [Atomic-Server](https://crates.io/crates/atomic-server/) - open-source type-safe graph database server with GUI, written in rust. Supports [Atomic Data](docs.atomicdata.dev/), JSON & RDF.  * [ArangoDB](https://www.arangodb.com/) - highly available Multi-Model NoSQL database  * [Blazegraph](https://github.com/blazegraph/database) - GPU accelerated graph database  * [Cayley](https://github.com/cayleygraph/cayley) - open source database written in Go  * [CosmosDB](https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction) - cloud-based multi-model database with support for TinkerPop3  * [Dgraph](https://dgraph.io) - Fast, Transactional, Distributed Graph Database (open source, written in Go)  * [DSE Graph](https://www.datastax.com/products/datastax-enterprise-graph) - Graph layer on top of DataStax Enterprise (Cassandra, SolR, Spark)  * [Grafito](https://github.com/arturo-lang/grafito) - Portable, Serverless & Lightweight SQLite-based Graph Database in Arturo  * [Grakn.AI](https://grakn.ai/) - a distributed hyper-relational database for knowledge-oriented systems, i.e. a distributed knowledge base  * [Graphd](https://github.com/google/graphd) - the Metaweb/Freebase Graph Repository  * [JanusGraph](http://janusgraph.org) - an open-source, distributed graph database with pluggable storage and indexing backends  * [Memgraph](https://memgraph.com/) - High Performance, In-Memory, Transactional Graph Database  * [Neo4j](http://tinkerpop.apache.org/docs/current/#neo4j-gremlin) - OLTP graph database  * [Nebula Graph](http://nebula-graph.io/) - A distributed, fast open-source graph database featuring horizontal scalability and high availability  * [RedisGraph](https://oss.redislabs.com/redisgraph/) - Property graph database, based on linear algebra constructs (GraphBLAS)  * [Sparksee](http://www.sparsity-technologies.com/#sparksee) - makes space and performance compatible with a small footprint and a fast analysis of large networks  * [Stardog](http://stardog.com/) - RDF graph database with OLTP and OLAP support  * [OrientDB](http://orientdb.com/orientdb/) - Distributed Multi-Model NoSQL Database with a Graph Database Engine  * [TerminusDB](https://github.com/terminusdb/terminusdb) is an open source graph database and document store. It is designed for collaboratively building data-intensive applications and knowledge graphs.  * [TigerGraph](https://www.tigergraph.com/) - The First Native Parallel Graph capable of real-time analytics on web-scale data  * [Weaviate](https://github.com/semi-technologies/weaviate) - Weaviate is a cloud-native, modular, real-time vector search engine with a graph data model (GraphQL interface) built to scale your machine learning models.    ### Triple stores  * [Akutan](https://github.com/eBay/akutan) - Akutan is a distributed knowledge graph store, sometimes called an RDF store or a triple store  * [AllegroGraph](https://franz.com/agraph/allegrograph/) - high-performance, persistent graph database that scales to billions of quads  * [Apache Jena](https://jena.apache.org/) - open source Java framework for building Semantic Web and Linked Data applications  * [Dydra]( http://docs.dydra.com/dydra) - Dydra is a cloud-based graph database. Dydra stores data is natively stored as a property graph, directly representing the relationships in the underlying data.  * [Eclipse RDF4J](http://rdf4j.org/) - (formerly known as Sesame) is an open source Java framework for processing RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.  * [GraphDB](http://graphdb.ontotext.com/graphdb/) - enterprise ready Semantic Graph Database, compliant with W3C Standards  * [Virtuoso](https://virtuoso.openlinksw.com/) - a ""Data Junction Box"" that drives enterprise and individual agility by deriving a Semantic Web of Linked Data from existing data silos  * [Hoply](https://github.com/amirouche/hoply/) - explore bigger than RAM relational data in the comfort of Python.    ## Graph computing frameworks    * [Apache Giraph](https://giraph.apache.org/) - an iterative graph processing system built for high scalability  * [Apache TinkerPop](https://tinkerpop.apache.org/) - a graph computing framework for both graph databases (OLTP) and graph analytic systems (OLAP)  * [Apache Spark - GraphX](https://spark.apache.org/graphx/) - Apache Spark's API for graphs and graph-parallel computation  * [GraphScope](https://github.com/alibaba/GraphScope) - A one-stop large-scale graph computing system from Alibaba    ## Languages    * [Cypher](http://www.opencypher.org/)  * [Datalog](https://en.wikipedia.org/wiki/Datalog)  * [Gremlin](https://tinkerpop.apache.org/gremlin.html)  * [SPARQL](https://en.wikipedia.org/wiki/SPARQL)  * [GSQL](https://docs.tigergraph.com/)    ## Managed hosting services    * [CosmosDB @ Microsoft](https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction)  * [JanusGraph @ IBM Compose](https://www.compose.com/databases/janusgraph)  * [JanusGraph @ Google Cloud Platform](https://cloud.google.com/solutions/running-janusgraph-with-bigtable) - JanusGraph on Google Kubernetes Engine backed by Google Cloud Bigtable  * [JanusGraph @ Amazon Web Services Labs](https://github.com/awslabs/dynamodb-janusgraph-storage-backend)  * [Neo4j @ Graphene](https://www.graphenedb.com/)  * [Neptune @ Amazon Web Services](https://aws.amazon.com/neptune/) - a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets    ## Learning materials    ### Official documentations  * [Cypher](https://neo4j.com/developer/cypher-query-language/) - reference documentation  * [Gremlin](http://tinkerpop.apache.org/docs/current/reference/#traversal) - reference documentation    ### Community effort  * [Graph Book](https://github.com/krlawrence/graph) - TinkerPop3 centric book written by [Kelvin R. Lawrence](https://twitter.com/gfxman)  * [SQL2Gremlin](http://sql2gremlin.com/) - transition from SQL to Gremlin by [Daniel Kuppitz](https://twitter.com/dkuppitz)  * [The Gremlin Compendium](http://www.doanduyhai.com/blog/?p=13460) - minimum survival kit for any Gremlin user, 10 blog post series by [Doan DuyHai](https://twitter.com/doanduyhai)    ### Blogs  * [TigerGraph Blog](https://www.tigergraph.com/blog/)    ## Conferences    * [Graph Connect](http://graphconnect.com/) - powered by Neo4j  * [Graph Day](http://graphday.com/) - an Independent Graph Conference from the Data Day folks    ## License    [![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)    To the extent possible under law, [Jean-Baptiste Musso](https://github.com/jbmusso) has waived all copyright and related or neighboring rights to this work. """
Semantic web;https://github.com/edumbill/doap;"""# DOAP: Description Of A Project    DOAP is a project to create an XML/RDF vocabulary to describe software  projects, and in particular open source projects.    In addition to developing an RDF schema and examples, the DOAP project aims to  provide tool support in all the popular programming languages.    Homepage: [DOAP wiki](https://github.com/ewilderj/doap/wiki)    Maintainers are:    * Edd Wilder-James [ewilderj](https://github.com/ewilderj)  * Kjetil Kjernsmo [kjetilk](https://github.com/kjetilk)    ## Schema    The live version of the schema is at the namespace URI,  http://usefulinc.com/ns/doap    It will be kept synchronized with the `master` branch in the Github  repository. """
Semantic web;https://github.com/plt-tud/r43ples;"""# R43ples    R43ples (Revision for triples) is an open source Revision Management Tool for the Semantic Web.    It provides different revisions of named graphs via a SPARQL interface. All information about revisions, changes, commits, branches and tags are stored in additional named graphs beside the original graph in an attached external triple store.    [![Build Status](https://travis-ci.org/plt-tud/r43ples.png?branch=develop)](https://travis-ci.org/plt-tud/r43ples)  [![Coverity Scan Build Status](https://scan.coverity.com/projects/2125/badge.svg)](https://scan.coverity.com/projects/2125)  [![codecov](https://codecov.io/gh/plt-tud/r43ples/graph/badge.svg)](https://codecov.io/gh/plt-tud/r43ples)  [![codebeat badge](https://codebeat.co/badges/8b09853d-1312-44a0-979b-579fe8551468)](https://codebeat.co/projects/github-com-plt-tud-r43ples-develop)  [![Ohloh Project Status](https://www.ohloh.net/p/r43ples/widgets/project_thin_badge.gif)](https://www.ohloh.net/p/r43ples)      This project provides an enhanced SPARQL endpoint for revision management of named graphs.  R43ples uses an internal Jena TDB is attached to an existing SPARQL endpoint of a triplestore and acts as another endpoint both for normal SPARQL queries  as well as for revision-enhanced SPARQL queries, named R43ples queries.  The R43ples endpoint allows to specify revisions which should be queried for each named graph used inside a SPARQL query.  The whole revision information is stored in additional graphs in the attached Jena TDB.    The [website](http://plt-tud.github.io/r43ples) of R43ples contains further [project information](http://plt-tud.github.io/r43ples/site/project-reports.html) including [Javadocs](http://plt-tud.github.io/r43ples/site/apidocs/) of the *develop* branch.  A running test server should be available under [http://eatld.et.tu-dresden.de:9998/r43ples/sparql](http://eatld.et.tu-dresden.de:9998/r43ples/sparql)      ## Getting Started  ### Dependencies  * JDK 1.8  * Maven    	sudo apt-get install maven default-jdk      ### Compiling  Maven is used for compiling        mvn compile exec:java    Packages (JAR with dependencies for the webservice) can be be built with:        mvn package    ### Running    R43ples runs with standalone web server    ``` bash  java -jar target/r43ples.jar  ```    ### Releases    Releases are stored on [GitHub](https://github.com/plt-tud/r43ples/releases).    There are also *stable* and *latest* docker images available:  ```  docker pull plttud/r43ples  ```    Run default r43ples via docker  ```  docker run -p 9998:9998 plttud/r43ples  ```    Run with specific configuration  ```  docker run -p 9998:9998 -v $PWD/r43ples.conf:/r43ples.conf plttud/r43ples  ```      ## Configuration    There is a configuration file named *resources/r43ples.conf*. The most important ones are the following:    * *triplestore.type* - type of attached triplestore (can be tdb, virtuoso [not working right now], http)  * *triplestore.uri* - URI or path under which R43ples can access the attached triplestore  * *triplestore.user* - user of attached triplestore if necessary  * *triplestore.password* - password of attached triplestore if necessary  * *revision.graph* - named graph which is used by R43ples to store revision graph information  * *evolution.graph* - named graph which is used by R43ples to store all information regarding evolutions  * *sdg.graph* - named graph for storing the SDG  * *sdg.graph.defaultContent* - default content of SDG which should be stored within named graph (sdg.graph)  * *sdg.graph.defaultSDG* -  Structural Definition Group within the named graph (sdg.graph) which should be associated with new graphs under revision control (mmo:hasDefaultSDG)  * *rules.graph* - named graph for storing the high level change aggregation and co-evolution rules  * *rules.graph.defaultContent* - default content of rules  * *service.host* - host which provides R43ples  * *service.port* - port which should provide R43ples  * *service.path* - path of host which should provide R43ples    The logging configuration is stored in *resources/log4j.properties*      ## Interfaces    ### Extended SPARQL endpoint  SPARQL endpoint is available at:        [uri]:[port]/r43ples/sparql    The endpoint directly accepts SPARQL queries with HTTP GET or HTTP POST parameters for *query* and *format*:        [uri]:[port]/r43ples/sparql?query=[]&format=[]    #### Supported Formats    The formats can be specified as URL Path Parameter *format*, as HTTP post paramter *format* or as HTTP header parameter *Accept*:    * text/turtle  * application/json  * application/rdf+xml  * text/html  * text/plain      #### R43ples keywords    There are some additional keywords which extends SPARQL and can be used to control the revisions of graphs:    * Create graph            CREATE GRAPH <graph>    * Select query            SELECT *          WHERE {          	GRAPH <graph> REVISION ""23"" {?s ?p ?o}      	}      	          SELECT *          WHERE {              GRAPH <graph> REVISION ""master"" {?s ?p ?o}          }    * Update query            USER ""mgraube"" MESSAGE ""test commit""          INSERT {              GRAPH <test> BRANCH ""master"" {                  <a> <b> <c> .              }          }                    USER ""mgraube"" MESSAGE ""test commit""          DELETE {              GRAPH <test> BRANCH ""develop"" {                  <a> <b> <c> .              }          }    * Branching            USER ""mgraube""          MESSAGE ""test commit""          BRANCH GRAPH <test> REVISION ""2"" TO ""unstable""    * Tagging            USER ""mgraube""          MESSAGE ""test commit""          TAG GRAPH <test> REVISION ""2"" TO ""v0.3-alpha""    * Merging    		USER ""Mister X.""  		MESSAGE ""merge example for a common merge""  		MERGE GRAPH <test> BRANCH ""branch-1"" INTO BRANCH ""branch-2""  		  		USER ""Mister X.""          MESSAGE ""merge example for automatica conflict resolution based upon specified SDD""          MERGE AUTO GRAPH <test> BRANCH ""branch-1"" INTO BRANCH ""branch-2""                    USER ""Mister X.""          MESSAGE ""merge example for a common merge with conflict resolution in WITH part""          MERGE GRAPH <test> BRANCH ""branch-1"" INTO BRANCH ""branch-2"" WITH {              <http://test.com/Carlos> <http://test.com/knows> <http://test.com/Danny> .              <http://test.com/Franz> <http://test.com/knows> <http://test.com/Silvia> .          }                    USER ""Mister X.""          MESSAGE ""merge example for manual specification of merged revision content""          MERGE MANUAL GRAPH <test> BRANCH ""branch-1"" INTO BRANCH ""branch-2"" WITH {              <http://test.com/Carlos> <http://test.com/knows> <http://test.com/Danny> .              <http://test.com/Franz> <http://test.com/knows> <http://test.com/Silvia> .          }  		  * Pick a revision into a branch                    USER ""Mister X.""          MESSAGE ""pick single revision example""          PICK GRAPH <test> REVISION ""56"" INTO BRANCH ""develop""                    USER ""Mister X.""          MESSAGE ""pick multiple revisions example""          PICK GRAPH <test> REVISION ""56"" TO REVISION ""62"" INTO BRANCH ""develop""            * Aggregate atomic changes to high level ones (semantic changes)                    AGG GRAPH <test> REVISION ""1"" TO REVISION ""2""            * Coevolve semantic changes to dependent revised graphs                    USER ""Mister X.""          MESSAGE ""Coevolution example""          COEVO GRAPH <test> REVISION ""1"" TO REVISION ""2""      #### Query Rewriting option    There is a new option for R43ples which improves the performance. The necessary revision is not temporarily generated anymore.  The SPARQL query is rewritten in such a way that the branch and the change sets are directly joined inside the query. This includes the order of the change sets.  It is currently under development and further research.    The option can be enabled by passing an additional parameter ""query_rewriting=true""    It currently supports:    * Multiple Graphs  * Multiple TriplePath  * FILTER  * MINUS    For more details, have a look into the *doc/* directory.    ### Debug SPARQL endpoint    R43ples redirects the queries performed on the debug endpoint directly to the attached triplstore.  Thus, this endpoint can be used for debugging purposes.    	[uri]:[port]/r43ples/debug    ### API  R43ples provides some functionalities additionally via an external API, even if all information can also be queried directly from the triplestore    * *api/getRevisedGraphs* lists all graphs managed by R43ples  * *createSampleDataset* generates some sample datasets with revision information  * *revisiongraph?graph=<test>&format=application/json* provides the revision graph for the specified graph (    ## Concept of R43ples    ### Extended SPARQL proxy    R43ples itself does not story any information. All information in the revised graphs and about the revised graphs  are stored in the attached triplestore. R43ples acts only as a proxy which evaluates additional revision information  in the SPARQL queries.    ![System Structure](./doc/r43ples-system.png)      ### Revision information    All information about the revision history of all named graphs is stored in the named graph **http://eatld.et.tu-dresden.de/r43ples-revisions** (as long as not configured otherwise in the configuration file).    Here, the Revision Management Ontology (RMO) is used to model revisions, branches and tags. Furthermore commits are stored which connect each revision, tag and branch with its prior revision.    The RMO is derived from the PROV ontology:  ![RMO example](./doc/ontology/RMO_UML.png)    An exemplary revision graph is shown here:  ![RMO example](./doc/revision management description/r43ples-creategraph.png)        ### HTTP Header information    Each response header contains information about the revision information of the graphs specified in the requests in the *r43ples-revisiongraph* HTTP header field. This information follows the RMO and is transferred as Turtle serialization.    Clients can also pass this information in R43ples update queries to the R43ples server via the *r43ples-revisiongraph* HTTP header attribute.  The server will check if the client is aware of the most recent version of the involved revised graphs. If this is not the case,  the update query will be rejected.        ## Used libraries and frameworks    Following libraries are used in R43ples:    * [Jersey](https://jersey.java.net/) for RestFul web services in Java  * [Grizzly](https://grizzly.java.net/) as web server  * [Jena ARQ](https://jena.apache.org/documentation/query/index.html) for processing SPARQL results  * [Jena TDB](https://jena.apache.org/documentation/tdb/index.html) as triplestore  * [jQuery](http://jquery.com/) as JavaScript framework  * [Bootstrap](http://getbootstrap.com/) as HTML, CSS and JS framework  * [Mustache](https://mustache.github.io/) as template engine  * [SpinRDF](https://github.com/spinrdf/spinrdf) as SPIN engine      """
Semantic web;https://github.com/blake-regalia/graphy.js;"""[![NPM version][npm-image]][npm-url] [![Dependency Status][daviddm-image]][daviddm-url]     # graphy.js 🍌  `graphy` is a collection of *high-performance* RDF libraries for JavaScript developers with a focus on usability. API works in both the browser and Node.js. Expressive CLI tool also available for Node.js.    [https://graphy.link/](https://graphy.link/)      ## Performance Benchmarks  🚀 [See how well `graphy` outperforms all others](https://github.com/blake-regalia/graphy.js/blob/master/perf/README.md).      ## Command Line Interface  📑 [See documentation for CLI here](https://graphy.link/cli).    ### Install the `graphy` bin CLI   - npm:     ```console     $ npm install --global graphy     $ graphy --help     ```     - yarn:     ```console     $ yarn global add graphy     $ graphy --help     ```      ## [Features](https://graphy.link/)   - [Read RDF documents](https://graphy.link/content.textual#verb_read) using streams. Includes support for N-Triples (.nt), N-Quads (.nq), Turtle (.ttl), and TriG (.trig).   - [Write RDF data](https://graphy.link/content.textual#verb_write) using streaming transforms with the awesome and intuitive [concise triples and concise quads language](https://graphy.link/concise).   - [Construct RDF data](https://graphy.link/concise#hash_c3) using ES object literals that reflect the tree-like structure of quads, `graph -> subject -> predicate -> object`, including nested blank nodes and RDF collections.   - [Compute the union, intersection, difference or subtraction](https://graphy.link/memory.dataset.fast) between multiple RDF graphs analagous to [Set Algebra](https://en.wikipedia.org/wiki/Algebra_of_sets).   - [Compare two RDF graphs](https://graphy.link/memory.dataset.fast#method_canonicalize) for isomoprhic equivalence, containment, and disjointness by first canonicalizing them with the [RDF Dataset Normalization Algorithm](https://json-ld.github.io/normalization/spec/).   - [Transform RDF data from the command-line](https://graphy.link/cli) by piping them through a series of sub-commands.   - [Scan RDF documents](https://graphy.link/content.textual#verb_scan) and run custom code using multiple threads for maximum throughput.      ## [See API Documentation](https://graphy.link/api)  🔎 Find the package you need _or_ install the super-package `npm install --save graphy` .    ### Core   - [DataFactory](https://graphy.link/core.data.factory)    ### Memory   - [FastDataset](https://graphy.link/memory.dataset.fast)    ### Content   - **N-Triples**: [NTriplesReader](https://graphy.link/content.textual#verb_read), [NTriplesScanner](https://graphy.link/content.textual#verb_scan), [NTriplesWriter](https://graphy.link/content.textual#verb_write), [NTriplesScriber](https://graphy.link/content.textual#verb_scribe)   - **N-Quads**: [NQuadsReader](https://graphy.link/content.textual#verb_read), [NQuadsScanner](https://graphy.link/content.textual#verb_scan), [NQuadsWriter](https://graphy.link/content.textual#verb_write), [NQuadsScriber](https://graphy.link/content.textual#verb_scribe)   - **Turtle**: [TurtleReader](https://graphy.link/content.textual#verb_read), [TurtleWriter](https://graphy.link/content.textual#verb_write), [TurtleScriber](https://graphy.link/content.textual#verb_scribe)   - **TriG**: [TriGReader](https://graphy.link/content.textual#verb_read), [TriGWriter](https://graphy.link/content.textual#verb_write), [TriGScriber](https://graphy.link/content.textual#verb_scribe)   - **RDF/XML**: [RdfXmlScriber](https://graphy.link/content.textual#verb_scribe)    ## Changelog  🍭⚡︎🔧  [See history of changes here](https://github.com/blake-regalia/graphy.js/blob/master/CHANGELOG.md).    ## Roadmap  🚧  [See the list of planned features](https://github.com/blake-regalia/graphy.js/blob/master/ROADMAP.md).    ## License    ISC © [Blake Regalia]()      [npm-image]: https://badge.fury.io/js/graphy.svg  [npm-url]: https://npmjs.org/package/graphy  <!-- [travis-image]: https://travis-ci.org/blake-regalia/graphy.js.svg?branch=master -->  <!-- [travis-url]: https://travis-ci.org/blake-regalia/graphy.js -->  [daviddm-image]: https://david-dm.org/blake-regalia/graphy.js.svg?theme=shields.io  [daviddm-url]: https://david-dm.org/blake-regalia/graphy.js """
Semantic web;https://github.com/opencube-toolkit/OpenCube-Expander;"""OpenCube Expander  ===============    The role of the OpenCube Expander component is:  + to search for compatible cubes and   + to create a new expanded cube by merging two compatible cubes.     ###How it works    he OpenCube Expander is developed as a separate component of the OpenCube toolkit and is part of the �Data Expanding� lifecycle step. It supports the identification of compatible cubes stored either at the native triple store or at remote SPARQL end-points. The Expander can be initialized by creating a widget.    Widget configuration for use with the native triple store:    ```  {{#widget: CubeSelection|asynch='true'}}  ```    Widget configuration for use with the remote SPARQL end-point containing data for the 2011 Irish Census:    ```  {{#widget:CubeSelection| sparqlService='<http://data.cso.ie/sparql>'| asynch='true' }}  ```       ###Functionality    The functionality of the OpenCube Expander is based:    + On the links (dimensionValueCompatible and MeasureCompatible) created by the OpenCube Compatibility Explorer in order to detect external compatible cubes  + On the aggregations (across a dimension and across a hierarchy) to detect compatible pre-computed aggregate cubes. The links enable the fast detection of the compatible cubes since no complex computations are made.       At the beginning the component presents the structure of the cube: i) the cube dimensions, ii) the values for each dimension and ii) the cube measures. Then the user can search for compatible cubes based on the following operations:    + **Add measure**. This operation identifies and presents cubes that are compatible to add new measures to the original cubes i.e. associated cubes using the property MeasureCompatible.  + **Add value to dimension**. In this case the user selects an expansion dimension and the operation identifies and presents compatible cubes that can be used to add new values to the selected dimension i.e. associated cubes using the property dimensionValueCompatible.  + **Add hierarchy**. This operation identifies and presents cubes that are compatible to add a hierarchy to the original cube i.e. pre-computed aggregations across a hierarchy created by the OpenCube Aggregator (for simplicity reasons this functionality has been integrated to the OpenCube OLAP Browser).  + **Add dimension**. This operation identifies and presents cubes that are compatible to add a dimension to the original cube i.e. pre-computed aggregations across a dimension created by the OpenCube Aggregator (for simplicity reasons this functionality has been integrated to the OpenCube OLAP Browser).        """
Semantic web;https://github.com/drlivingston/kr;"""# Clojure API for RDF and SPARQL    The Knowledge Representation and Reasoning Tools library enables easy Clojure use of RDF and SPARQL, provinging a unified interface for both Jena and Sesame.  (KR can be extended for other APIs and underlying triplestores.)      ## Overview    Currently it facilitates use of RDF-based representations backed by triple-/quad- stores.  It provides a consistent clojure based way of interacting with its backing implementations, which currently include the Jena and Sesame APIs. The library enables easy working with knowledge representations and knowledge bases, and provides support for some common tasks including forward-chaining and reification.    [Release Notes]    update: see the note on [Sesame Versions]      ## Basic Setup    The primary api functions you're likely to use come from the kr-core apis:  ```clj  (use 'edu.ucdenver.ccp.kr.kb)  (use 'edu.ucdenver.ccp.kr.rdf)  (use 'edu.ucdenver.ccp.kr.sparql)  ```    To actually get a KB instance to work with you'll need to make sure the implementation-specific code is loaded:  ```clj  (require 'edu.ucdenver.ccp.kr.sesame.kb)  ;; OR  (require 'edu.ucdenver.ccp.kr.jena.kb)  ```    a kb instance can then be acquired with the kb function, for example:  ```clj  (kb :sesame-mem)  ; an in-memory sesame kb  ```  The `kb` function can take keyword arguments such as `:sesame-mem` or `:jena-mem` or it can take names of several native jena or sesame objects or pre-constructed jena or sesame instances to create a `kb` wrapper around (e.g., a jena `Model` or a sesame `Sail`).    kb's need some help knowing what the namespace mappings are, the server mappings can be brought down from a third party kb by calling `(synch-ns-mappings my-kb)` or you can add a few:  ```clj  (register-namespaces my-kb                       '((""ex"" ""http://www.example.org/"")                          (""rdf"" ""http://www.w3.org/1999/02/22-rdf-syntax-ns#"")                         (""foaf"" ""http://xmlns.com/foaf/0.1/"")))  ;;the return value is the new modified kb - hang onto it  ```    ## Basic Use    Once you have a KB you can load rdf triple or files:  ```clj    ;;in parts    (add my-kb 'ex/KevinL 'rdf/type 'ex/Person)    ;;as a triple    (add my-kb '(ex/KevinL foaf/name ""Kevin Livingston""))  ```    Query for RDF triples:  ```clj  (ask-rdf my-kb nil nil 'ex/Person)  ;;true    (query-rdf my-kb nil nil 'ex/Person)  ;;((ex/KevinL rdf/type ex/Person))  ```    Query with triple patterns (SPARQL):  ```clj  (query my-kb '((?/person rdf/type ex/Person)                 (?/person foaf/name ?/name)                 (:optional ((?/person foaf/mbox ?/email)))))  ;;({?/name ""Kevin Livingston"", ?/person ex/KevinL})  ```    ## More Details    The examples also provide details on how to interact with a KB, with run-able poms:  https://github.com/drlivingston/kr/tree/master/kr-examples    These include examples of connecting to a remote repository and a local in-memory repository.      More detailed uses can be found in the test cases for both the KB, RDF, and SPARQL APIs.  They are here:  https://github.com/drlivingston/kr/tree/master/kr-core/src/test/clojure/edu/ucdenver/ccp/test/kr      ## Maven    releases are deployed to clojars:  ```xml  <repository>    <id>clojars.org</id>    <url>http://clojars.org/repo</url>  </repository>  ```    the core dependency is kr-core:  ```xml  <dependency>    <groupId>edu.ucdenver.ccp</groupId>    <artifactId>kr-core</artifactId>    <version>1.4.17</version>  </dependency>  ```    but the core dependency is unnecessary if you are brining in either the sesame or jena implementations:  ```xml  <dependency>    <groupId>edu.ucdenver.ccp</groupId>    <artifactId>kr-sesame-core</artifactId>    <version>1.4.17</version>  </dependency>    <dependency>    <groupId>edu.ucdenver.ccp</groupId>    <artifactId>kr-jena-core</artifactId>    <version>1.4.17</version>  </dependency>  ```      ## Acknowledgements  open sourced by: <br />  [CCP Lab][] <br />  [University of Colorado Denver][] <br />  primary developer: [Kevin Livingston][]    ----      [CCP Lab]: http://compbio.ucdenver.edu/Hunter_lab/CCP_website/index.html  [University of Colorado Denver]: http://www.ucdenver.edu/  [Kevin Livingston]: https://github.com/drlivingston  [Sesame Versions]:https://github.com/drlivingston/kr/wiki/versions-and-sesame  [Release Notes]:https://github.com/drlivingston/kr/wiki/Release-notes """
Semantic web;https://github.com/UCLALibrary/fester;"""# Fester  [![Maven Build](https://github.com/uclalibrary/fester/workflows/Maven%20PR%20Build/badge.svg)](https://github.com/UCLALibrary/fester/actions) [![Codacy Badge](https://app.codacy.com/project/badge/Grade/990b5c316e0a45d092c83d58f148e0e8)](https://www.codacy.com/gh/UCLALibrary/fester?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=UCLALibrary/fester&amp;utm_campaign=Badge_Grade) [![Codacy Badge](https://app.codacy.com/project/badge/Coverage/990b5c316e0a45d092c83d58f148e0e8)](https://www.codacy.com/gh/UCLALibrary/fester?utm_source=github.com&utm_medium=referral&utm_content=UCLALibrary/fester&utm_campaign=Badge_Coverage) [![Known Vulnerabilities](https://snyk.io/test/github/uclalibrary/fester/badge.svg)](https://snyk.io/test/github/uclalibrary/fester)    A microservice for facilitating the creation, storage, and retrieval of IIIF manifests and collections.    ## Prerequisites    There are just a few prerequisites that must be installed, and configured correctly, in order to build Fester:    * [Java Development Kit (JDK)](https://openjdk.java.net/install/): version 11 or greater  * [Docker](https://docs.docker.com/get-docker/): version 19.03 or greater  * [Maven](https://maven.apache.org/download.cgi): version 3.6 or greater  * [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) [Not required, but useful]    These packages may also be available through your system's package repository. If they are, it's better to install from that source so that they will be kept up to date for you.    You will also need an account on AWS and have the ability to create [IAM](https://aws.amazon.com/iam/) accounts and [S3](https://aws.amazon.com/s3/) buckets.    ## Configuring the Build    Fester uses an S3 bucket for back-end storage. To be able to run the project's tests, several configuration values must be supplied:    * fester.s3.bucket  * fester.s3.access_key  * fester.s3.secret_key  * fester.s3.region    These values can be set as properties in your system's Maven settings.xml file (or be supplied on the command line at build time).    ## Building the Project    The project builds an executable Jar that can be run to start the microservice. To build the project, run:        mvn package    This will put the executable Jar in the `target/build-artifact` directory.    To generate the site's documentation, run:        mvn site    This will generate the documentation in the `target/site` directory.    ## Configuring the Tests    The project contains unit, functional, and integration tests, with controls on how to control which tests are run. In order to run the functional and integration tests, the build machine must have a working Docker environment. Setting up Docker on your machine will depend on the type of machine you have (e.g., Linux, Mac, or Windows). Docker's [documentation](https://docs.docker.com/get-docker/) should be consulted on how to do this.    When running the build using the 'package' phase (as described above), only the unit tests are run. If you want to run all the possible tests, the project can be built with:        mvn integration-test    or        mvn verify    This will run the functional, feature flag, and integration tests, in addition to the unit tests. If you want to skip a particular type of test but still run the 'verify' phase, you can use one of the following arguments to your Maven command:        -DskipUTs      -DskipITs      -DskipFTs      -DskipFfTs    The first will skip the unit tests; the second will skip the integration tests; the third will skip the functional tests; and, the fourth will skip the feature flag tests. They can also be combined so that two types of tests are skipped. For instance, only the functional tests will be run if the following is typed:        mvn verify -DskipUTs -DskipITs    When running the integration and functional tests, it may be desirable to turn on logging for the containers that run the tests. This can be useful in debugging test failures that happen within the container. To do this, supply one (or any) of the following arguments to your build:        -DseeLogsFT      -DseeLogsIT      -DseeLogsFfT    This will tunnel the container's logs (including the application within the container's logs) to Maven's logging mechanism so that you will be able to see what's happening in the container as the tests are being run against it.    You might also want to adjust the logging level on the tests themselves. By default, the test loggers are configured to write DEBUG logs to a log file in the `target` directory and ERROR logs to standard out. To change the log level of the standard out logging, run Maven with the `logLevel` argument; for instance:        mvn -DlogLevel=DEBUG test    If you want more fine-grained control over the logging, you can copy the `src/test/resources/logback-test.xml` file to the project's root directory and modify it. A `logback-test.xml` file in the project's home directory will be used instead of the standard one in `src/rest/resources` if it's available. That hypothetical file has also been added to the project's `.gitignore` so you don't need to worry about checking it into Git.    ## Running a Single Test    It is sometimes useful to run a single test (instead of the whole test suite). The Surefire Maven plugin allows for this, but it's worth noting that when a single test is run in this way the test suite's pre-configured system properties are not picked up from the plugin's configuration. To work around this, a dev who wants to run a single test must supply the necessary properties theirself. For example, if one wanted to run the functional test that checks that missing images get a placeholder image in the manifest, the command to do that would be:        mvn integration-test -Dtest=MissingImageFT -Dfester.s3.bucket=iiif-fester -Dfester.placeholder.url=""https://iiif.library.ucla.edu/iiif/2/blank"" -Dfester.logs.output=true    You would want to supply your own values for `fester.s3.bucket` and `fester.placeholder` of course. This command will spin up the Docker container that the functional test is run against, but it will only run the `MissingImageFT` test, skipping all the integration and other functional tests in the suite.    ## Running the Application for Development    You can run a development instance of Fester by typing the following within the project root:        mvn -Plive test    Once run, the service can be verified/accessed at [http://localhost:8888/fester/status](http://localhost:8888/fester/status). The API documentation can be accessed at [http://localhost:8888/fester/docs](http://localhost:8888/fester/docs)    ## Debugging with Eclipse IDE    There are two ways to debug Fester:    - **Debugging the tests.** This enables the developer to step through both the test and application code as the test suite runs.  - **Debugging a running instance.** This enables the developer to step through the application code as they interact with the HTTP API.    The following setup instructions were tested with [Eclipse IDE](https://www.eclipse.org/eclipseide/) 4.14.0 (2019-12).    ### Debugging the tests    From within Eclipse:    1. Create a new debug configuration      - In the top-level menu, select *Run* > *Debug Configurations...*      - In the pop-up window:          - Create a new configuration of type *Remote Java Application*          - Set *Name* to something like `Fester (JDWP server for containerized instances created by test suite)`          - In the *Connect* tab:              - Set *Project* to the Fester project directory              - Set *Connection Type* to `Standard (Socket Listen)`              - Set *Port* to `5556`              - Set *Connection limit* to `16`              - Check *Allow termination of remote VM* (optional)  2. Create another debug configuration *      - In the top-level menu, select *Run* > *Debug Configurations...*      - In the pop-up window:          - Create a new configuration of type *Maven Build*          - Set *Name* to something like `Fester (debug test suite)`          - In the *Main* tab:              - Set *Base directory* to the Fester project directory              - Set *Goals* to `integration-test`              - Set *Profiles* to `debug`              - Set *User settings* to the path to a `settings.xml` that contains your AWS S3 credentials  3. Run the debug configuration created in Step 1 **  4. Run the debug configuration created in Step 2 **    _* As an alternative to step 2 (and 4), run the following from the command line (after completing steps 1 and 3):_        mvn -Pdebug integration-test    _** If you're doing this for the first time, you may need to bring back the pop-up window where you created the configuration in order to invoke it. Otherwise, you can use toolbar buttons, or hotkeys <kbd>Ctrl</kbd> <kbd>F11</kbd> (Run) or <kbd>F11</kbd> (Debug)._    ### Debugging a running instance    This procedure will start an instance of Fester with port `5555` open for incoming JDWP connections.    From within Eclipse:    1. Create a new run configuration ***      - In the top-level menu, select *Run* > *Run Configurations...*      - In the pop-up window:          - Create a new configuration of type *Maven Build*          - Set *Name* to something like `Fester (debugging mode)`          - In the *Main* tab:              - Set *Base directory* to the Fester project directory              - Set *Goals* to `test`              - Set *Profiles* to `runDebug`              - Set *User settings* to the path to a `settings.xml` that contains your AWS S3 credentials  2. Create a new debug configuration      - In the top-level menu, select *Run* > *Debug Configurations...*      - In the pop-up window:          - Create a new configuration of type *Remote Java Application*          - Set *Name* to something like `Fester (JDWP client)`          - In the *Connect* tab:              - Set *Project* to the Fester project directory              - Set *Connection Type* to `Standard (Socket Attach)`              - Set *Host* to `localhost`              - Set *Port* to `5555`              - Check *Allow termination of remote VM* (optional)  3. Run the new run configuration created in Step 1  4. Run the new debug configuration created in Step 2    _*** As an alternative to step 1 (and 3), run the following from the command line:_        mvn -PrunDebug test    _and then proceed with steps 2 and 4._    ## Load Testing    A [Locust](https://docs.locust.io/en/stable/index.html) test file is included, it only tests PUTs of manifests. If you wish to run the test, you need to have Locust installed, and then run the following command from the src/test/scripts/locust folder:        locust --host=url-of-the-server-you-are-testing    For example, if you wish to run a Locust test against a dev instance on your own machine, you would enter:        locust --host=http://localhost:8888    ## Git Hooks    To prevent accidentally pushing commits that would cause the CI build to fail, you can configure your Git client to use a pre-push hook:        ln -s ../../src/test/scripts/git-hooks/pre-push .git/hooks    ## Working with Pinned OS Packages    We pin the versions of packages that we install into our base image. What this means is that periodically a pinned version will become obsolete and the build will break. We have a nightly build that should catch this issues for us, but in the case that you find the breakage before us, there is a handy way to tell which pinned version has broken the build. To see the current versions inside the base image, run:        mvn validate -Dversions    This will output a list of current versions, which can be compared to the pinned versions defined in the project's POM file (i.e., pom.xml).    ## Festerize    [Festerize](https://github.com/UCLALibrary/festerize) may be used to interact with Fester, as an alternative to the built-in CSV upload form.    ## Contact    We use an internal ticketing system, but we've left the GitHub [issues](https://github.com/UCLALibrary/fester/issues) open in case you'd like to file a ticket or make a suggestion. You can also contact Kevin S. Clarke at <a href=""mailto:ksclarke@ksclarke.io"">ksclarke@ksclarke.io</a> if you have a question about the project. """
Semantic web;https://github.com/marcelotto/rdf-ex;"""<img src=""rdf-logo.png"" align=""right"" />    # RDF.ex    [![CI](https://github.com/rdf-elixir/rdf-ex/workflows/CI/badge.svg?branch=master)](https://github.com/rdf-elixir/rdf-ex/actions?query=branch%3Amaster+workflow%3ACI)  [![Hex.pm](https://img.shields.io/hexpm/v/rdf.svg?style=flat-square)](https://hex.pm/packages/rdf)  [![Hex Docs](https://img.shields.io/badge/hex-docs-lightgreen.svg)](https://hexdocs.pm/rdf/)  [![Total Download](https://img.shields.io/hexpm/dt/rdf.svg)](https://hex.pm/packages/rdf)  [![License](https://img.shields.io/hexpm/l/rdf.svg)](https://github.com/rdf-elixir/rdf-ex/blob/master/LICENSE.md)      An implementation of the [RDF](https://www.w3.org/TR/rdf11-primer/) data model in Elixir.    The API documentation can be found [here](https://hexdocs.pm/rdf/). For a guide and more information about RDF.ex and it's related projects, go to <https://rdf-elixir.dev>.    Migration guides for the various versions can be found in the [Wiki](https://github.com/rdf-elixir/rdf-ex/wiki).      ## Features    - fully compatible with the RDF 1.1 specification  - support of the [RDF-star] extension  - in-memory data structures for RDF descriptions, RDF graphs and RDF datasets  - basic graph pattern matching against the in-memory data structures with streaming-support  - execution of [SPARQL] queries against the in-memory data structures with the [SPARQL.ex] package or against any SPARQL endpoint with the [SPARQL.Client] package  - RDF vocabularies as Elixir modules for safe, i.e. compile-time checked and concise usage of IRIs  - most of the important XML schema datatypes for RDF literals  - support for custom datatypes for RDF literals, incl. as derivations of XSD datatypes via facets   - sigils for the most common types of nodes, i.e. IRIs, literals, blank nodes and lists  - a description DSL resembling Turtle in Elixir  - implementations for the [N-Triples], [N-Quads] and [Turtle] serialization formats (including the respective RDF-star extensions); [JSON-LD] and [RDF-XML] are available with the separate [JSON-LD.ex] and [RDF-XML.ex] packages  - validation of RDF data against [ShEx] schemas with the [ShEx.ex] package  - mapping of RDF data structures to Elixir structs and back with [Grax]       ## Contributing    There's still much to do for a complete RDF ecosystem for Elixir, which means there are plenty of opportunities to contribute. Here are some suggestions:    - more serialization formats, like [RDFa], [N3], [CSVW], [HDT] etc.  - more XSD datatypes  - improving the documentation    See [CONTRIBUTING](CONTRIBUTING.md) for details.      ## Consulting    If you need help with your Elixir and Linked Data projects, just contact [NinjaConcept](https://www.ninjaconcept.com/) via <contact@ninjaconcept.com>.      ## Acknowledgements    The development of this project was partly sponsored by [NetzeBW](https://www.netze-bw.de/) for [NETZlive](https://www.netze-bw.de/unsernetz/netzinnovationen/digitalisierung/netzlive).    [JetBrains](https://www.jetbrains.com/?from=RDF.ex) supports the project with complimentary access to its development environments.      ## License and Copyright    (c) 2017-present Marcel Otto. MIT Licensed, see [LICENSE](LICENSE.md) for details.      [RDF.ex]:               https://hex.pm/packages/rdf  [JSON-LD.ex]:           https://hex.pm/packages/json_ld  [RDF-XML.ex]:           https://hex.pm/packages/rdf_xml  [SPARQL.ex]:            https://hex.pm/packages/sparql  [SPARQL.Client]:        https://hex.pm/packages/sparql_client  [ShEx.ex]:              https://hex.pm/packages/shex  [Grax]:                 https://hex.pm/packages/grax  [RDF-star]:             https://w3c.github.io/rdf-star/cg-spec  [N-Triples]:            https://www.w3.org/TR/n-triples/  [N-Quads]:              https://www.w3.org/TR/n-quads/  [Turtle]:               https://www.w3.org/TR/turtle/  [N3]:                   https://www.w3.org/TeamSubmission/n3/  [JSON-LD]:              https://www.w3.org/TR/json-ld/  [RDFa]:                 https://www.w3.org/TR/rdfa-syntax/  [RDF-XML]:              https://www.w3.org/TR/rdf-syntax-grammar/  [CSVW]:                 https://www.w3.org/TR/tabular-data-model/  [HDT]:                  http://www.rdfhdt.org/  [SPARQL]:               https://www.w3.org/TR/sparql11-overview/  [ShEx]:                 https://shex.io/ """
Semantic web;https://github.com/owlcs/ont-api;"""# ONT-API (ver. 2.1.0)    ## Summary  ONT-API is a RDF-centric Java library to work with OWL.    For more info see [wiki](https://github.com/owlcs/ont-api/wiki).     ## Dependencies  - **[Apache Jena](https://github.com/apache/jena)** (**3.x.x**)  - **[OWL-API](https://github.com/owlcs/owlapi)** (**5.x.x**)    ## Requirements:    - Java **8+**    ## License  * Apache License Version 2.0  * GNU LGPL Version 3.0   """
Semantic web;https://github.com/larsga/Duke;"""# Duke    Duke is a fast and flexible deduplication (or entity resolution, or  record linkage) engine written in Java on top of Lucene.  The latest  version is 1.2 (see [ReleaseNotes](https://github.com/larsga/Duke/wiki/ReleaseNotes)).    Duke can find duplicate customer records, or other kinds of records in  your database. Or you can use it to connect records in one data set  with other records representing the same thing in another data set.  Duke has sophisticated comparators that can handle spelling  differences, numbers, geopositions, and more. Using a probabilistic  model Duke can handle noisy data with good accuracy.    Features      * High performance.    * Highly configurable.    * Support for [CSV, JDBC, SPARQL, NTriples, and JSON](https://github.com/larsga/Duke/wiki/DataSources).    * Many built-in [comparators](https://github.com/larsga/Duke/wiki/Comparator).    * Plug in your own data sources, comparators, and [cleaners](https://github.com/larsga/Duke/wiki/Cleaner).    * [Genetic algorithm](https://github.com/larsga/Duke/wiki/GeneticAlgorithm) for automatically tuning configurations.    * Command-line client for getting started.    * [API](https://github.com/larsga/Duke/wiki/UsingTheAPI) for embedding into any kind of application.    * Support for batch processing and continuous processing.    * Can maintain database of links found via JNDI/JDBC.    * Can run in multiple threads.    The [GettingStarted page](https://github.com/larsga/Duke/wiki/GettingStarted) explains how to get started and has links to  further documentation. The [examples of use](https://github.com/larsga/Duke/wiki/ExamplesOfUse) page  lists real examples of using Duke, complete with data and  configurations. [This  presentation](http://www.slideshare.net/larsga/linking-data-without-common-identifiers)  has more of the big picture and background.    Contributions, whether issue reports or patches, are very much  welcome.  Please fork the repository and make pull requests.    Supports Java 1.7 and 1.8.    [![Build status](https://travis-ci.org/larsga/Duke.png?branch=master)](https://travis-ci.org/larsga/Duke)    If you have questions or problems, please register an issue in the  issue tracker, or post to the [the mailing  list](http://groups.google.com/group/duke-dedup). If you don't want to  join the list you can always write to me at `larsga [a]  garshol.priv.no`, too.    ## Using Duke with Maven    Duke is hosted in Maven Central, so if you want to use Duke it's as  easy as including the following in your pom file:    ```  <dependency>    <groupId>no.priv.garshol.duke</groupId>    <artifactId>duke</artifactId>    <version>1.2</version>  </dependency>  ```    ## Building the source    If you have [Maven](https://maven.apache.org/) installed, this is as  easy as giving the command `mvn package` in the root directory. This  will produce a `.jar` file in the `target/` subdirectory of each  module.    ## Older documentation    [This blog post](http://www.garshol.priv.no/blog/217.html) describes  the basic approach taken to match records. It does not deal with the  Lucene-based lookup, but describes an early, slow O(n^2)  prototype. [This early  presentation](http://www.slideshare.net/larsga/deduplication)  describes the ideas behind the engine and the intended architecture"""
Semantic web;https://github.com/RDFLib/pySHACL;"""![](pySHACL-250.png)    # pySHACL  A Python validator for SHACL.    [![Build Status](https://drone.rdflib.ashs.dev/api/badges/RDFLib/pySHACL/status.svg)](https://drone.rdflib.ashs.dev/RDFLib/pySHACL)    [![DOI](https://zenodo.org/badge/147505799.svg)](https://zenodo.org/badge/latestdoi/147505799) [![Downloads](https://pepy.tech/badge/pyshacl)](https://pepy.tech/project/pyshacl) [![Downloads](https://pepy.tech/badge/pyshacl/month)](https://pepy.tech/project/pyshacl/month) [![Downloads](https://pepy.tech/badge/pyshacl/week)](https://pepy.tech/project/pyshacl/week)    This is a pure Python module which allows for the validation of [RDF](https://www.w3.org/2001/sw/wiki/RDF) graphs against Shapes Constraint Language ([SHACL](https://www.w3.org/TR/shacl/)) graphs. This module uses the [rdflib](https://github.com/RDFLib/rdflib) Python library for working with RDF and is dependent on the [OWL-RL](https://github.com/RDFLib/OWL-RL) Python module for [OWL2 RL Profile](https://www.w3.org/TR/owl2-overview/#ref-owl-2-profiles) based expansion of data graphs.    This module is developed to adhere to the SHACL Recommendation:  > Holger Knublauch; Dimitris Kontokostas. *Shapes Constraint Language (SHACL)*. 20 July 2017. W3C Recommendation. URL: <https://www.w3.org/TR/shacl/> ED: <https://w3c.github.io/data-shapes/shacl/>    # Community for Help and Support  The SHACL community has a discord server for discussion of topics around SHACL and the SHACL specification.    [Use this invitation link: https://discord.gg/RTbGfJqdKB to join the server](https://discord.gg/RTbGfJqdKB)    There is a \#pyshacl channel in which discussion around this python library can held, and you can ask for general pyshacl help too.    ## Installation  Install with PIP (Using the Python3 pip installer `pip3`)  ```bash  $ pip3 install pyshacl  ```    Or in a python virtualenv _(these example commandline instructions are for a Linux/Unix based OS)_  ```bash  $ python3 -m virtualenv --python=python3 --no-site-packages .venv  $ source ./.venv/bin/activate  $ pip3 install pyshacl  ```    To exit the virtual enviornment:  ```bash  $ deactivate  ```    ## Command Line Use  For command line use:  _(these example commandline instructions are for a Linux/Unix based OS)_  ```bash  $ pyshacl -s /path/to/shapesGraph.ttl -m -i rdfs -a -j -f human /path/to/dataGraph.ttl  ```  Where   - `-s` is an (optional) path to the shapes graph to use   - `-e` is an (optional) path to an extra ontology graph to import   - `-i` is the pre-inferencing option   - `-f` is the ValidationReport output format (`human` = human-readable validation report)   - `-m` enable the meta-shacl feature   - `-a` enable SHACL Advanced Features   - `-j` enable SHACL-JS Features (if `pyhsacl[js]` is installed)    System exit codes are:  `0` = DataGraph is Conformant  `1` = DataGraph is Non-Conformant  `2` = The validator encountered a RuntimeError (check stderr output for details)  `3` = Not-Implemented; The validator encountered a SHACL feature that is not yet implemented.    Full CLI Usage options:  ```bash  $ pyshacl -h  $ python3 -m pyshacl -h  usage: pyshacl [-h] [-s [SHACL]] [-e [ONT]] [-i {none,rdfs,owlrl,both}] [-m]                 [-im] [-a] [-j] [-it] [--abort] [--allow-infos] [-w] [-d]                 [-f {human,table,turtle,xml,json-ld,nt,n3}]                 [-df {auto,turtle,xml,json-ld,nt,n3}]                 [-sf {auto,turtle,xml,json-ld,nt,n3}]                 [-ef {auto,turtle,xml,json-ld,nt,n3}] [-V] [-o [OUTPUT]]                 DataGraph    PySHACL 0.18.1 command line tool.    positional arguments:    DataGraph             The file containing the Target Data Graph.    optional arguments:    -h, --help            show this help message and exit    -s [SHACL], --shacl [SHACL]                          A file containing the SHACL Shapes Graph.    -e [ONT], --ont-graph [ONT]                          A file path or URL to a document containing extra                          ontological information to mix into the data graph.    -i {none,rdfs,owlrl,both}, --inference {none,rdfs,owlrl,both}                          Choose a type of inferencing to run against the Data                          Graph before validating.    -m, --metashacl       Validate the SHACL Shapes graph against the shacl-                          shacl Shapes Graph before validating the Data Graph.    -im, --imports        Allow import of sub-graphs defined in statements with                          owl:imports.    -a, --advanced        Enable features from the SHACL Advanced Features                          specification.    -j, --js              Enable features from the SHACL-JS Specification.    -it, --iterate-rules  Run Shape's SHACL Rules iteratively until the                          data_graph reaches a steady state.    --abort               Abort on first invalid data.    --allow-infos         Shapes marked with severity of Info will not cause                          result to be invalid.    -w, --allow-warnings  Shapes marked with severity of Warning or Info will                          not cause result to be invalid.    -d, --debug           Output additional runtime messages.    -f {human,table,turtle,xml,json-ld,nt,n3}, --format {human,table,turtle,xml,json-ld,nt,n3}                          Choose an output format. Default is ""human"".    -df {auto,turtle,xml,json-ld,nt,n3}, --data-file-format {auto,turtle,xml,json-ld,nt,n3}                          Explicitly state the RDF File format of the input                          DataGraph file. Default=""auto"".    -sf {auto,turtle,xml,json-ld,nt,n3}, --shacl-file-format {auto,turtle,xml,json-ld,nt,n3}                          Explicitly state the RDF File format of the input                          SHACL file. Default=""auto"".    -ef {auto,turtle,xml,json-ld,nt,n3}, --ont-file-format {auto,turtle,xml,json-ld,nt,n3}                          Explicitly state the RDF File format of the extra                          ontology file. Default=""auto"".    -V, --version         Show PySHACL version and exit.    -o [OUTPUT], --output [OUTPUT]                          Send output to a file (defaults to stdout).  ```    ## Python Module Use  For basic use of this module, you can just call the `validate` function of the `pyshacl` module like this:    ```python  from pyshacl import validate  r = validate(data_graph,        shacl_graph=sg,        ont_graph=og,        inference='rdfs',        abort_on_first=False,        allow_infos=False,        allow_warnings=False,        meta_shacl=False,        advanced=False,        js=False,        debug=False)  conforms, results_graph, results_text = r  ```    Where:  * `data_graph` is an rdflib `Graph` object or file path of the graph to be validated  * `shacl_graph` is an rdflib `Graph` object or file path or Web URL of the graph containing the SHACL shapes to validate with, or None if the SHACL shapes are included in the data_graph.  * `ont_graph` is an rdflib `Graph` object or file path or Web URL a graph containing extra ontological information, or None if not required.  * `inference` is a Python string value to indicate whether or not to perform OWL inferencing expansion of the `data_graph` before validation.  Options are 'rdfs', 'owlrl', 'both', or 'none'. The default is 'none'.  * `abort_on_first` (optional) `bool` value to indicate whether or not the program should abort after encountering the first validation failure or to continue. Default is to continue.  * `allow_infos` (optional) `bool` value, Shapes marked with severity of Info will not cause result to be invalid.  * `allow_warnings` (optional) `bool` value, Shapes marked with severity of Warning or Info will not cause result to be invalid.  * `meta_shacl` (optional) `bool` value to indicate whether or not the program should enable the Meta-SHACL feature. Default is False.  * `advanced`: (optional) `bool` value to enable SHACL Advanced Features  * `js`: (optional) `bool` value to enable SHACL-JS Features (if `pyshacl[js]` is installed)  * `debug` (optional) `bool` value to indicate whether or not the program should emit debugging output text, including violations that didn't lead to non-conformance overall. So when debug is True don't judge conformance by absense of violation messages. Default is False.    Some other optional keyword variables available on the `validate` function:  * `data_graph_format`: Override the format detection for the given data graph source file.  * `shacl_graph_format`: Override the format detection for the given shacl graph source file.  * `ont_graph_format`: Override the format detection for the given extra ontology graph source file.  * `iterate_rules`: Interate SHACL Rules until steady state is found (only works with advanced mode).  * `do_owl_imports`: Enable the feature to allow the import of subgraphs using `owl:imports` for the shapes graph and the ontology graph. Note, you explicitly cannot use this on the target data graph.  * `serialize_report_graph`: Convert the report results_graph into a serialised representation (for example, 'turtle')  * `check_dash_result`: Check the validation result against the given expected DASH test suite result.  * `check_sht_result`: Check the validation result against the given expected SHT test suite result.    Return value:  * a three-component `tuple` containing:    * `conforms`: a `bool`, indicating whether or not the `data_graph` conforms to the `shacl_graph`    * `results_graph`: a `Graph` object built according to the SHACL specification's [Validation Report](https://www.w3.org/TR/shacl/#validation-report) structure    * `results_text`: python string representing a verbose textual representation of the [Validation Report](https://www.w3.org/TR/shacl/#validation-report)      ## Python Module Call    You can get an equivalent of the Command Line Tool using the Python3 executable by doing:    ```bash  $ python3 -m pyshacl  ```      ## Errors  Under certain circumstances pySHACL can produce a `Validation Failure`. This is a formal error defined by the SHACL specification and is required to be produced as a result of specific conditions within the SHACL graph.  If the validator produces a `Validation Failure`, the `results_graph` variable returned by the `validate()` function will be an instance of `ValidationFailure`.  See the `message` attribute on that instance to get more information about the validation failure.    Other errors the validator can generate:  - `ShapeLoadError`: This error is thrown when a SHACL Shape in the SHACL graph is in an invalid state and cannot be loaded into the validation engine.  - `ConstraintLoadError`: This error is thrown when a SHACL Constraint Component is in an invalid state and cannot be loaded into the validation engine.  - `ReportableRuntimeError`: An error occurred for a different reason, and the reason should be communicated back to the user of the validator.  - `RuntimeError`: The validator encountered a situation that caused it to throw an error, but the reason does concern the user.    Unlike `ValidationFailure`, these errors are not passed back as a result by the `validate()` function, but thrown as exceptions by the validation engine and must be  caught in a `try ... except` block.  In the case of `ShapeLoadError` and `ConstraintLoadError`, see the `str()` string representation of the exception instance for the error message along with a link to the relevant section in the SHACL spec document.      ## Windows CLI    [Pyinstaller](https://www.pyinstaller.org/) can be  [used](https://pyinstaller.readthedocs.io/en/stable/usage.html) to create an  executable for Windows that has the same characteristics as the Linux/Mac  CLI program.  The necessary ``.spec`` file is already included in ``pyshacl/pyshacl-cli.spec``.  The ``pyshacl-cli.spec`` PyInstaller spec file creates a ``.exe`` for the  pySHACL Command Line utility. See above for the pySHACL command line util usage instructions.    See [the PyInstaller installation guide](https://pyinstaller.readthedocs.io/en/stable/installation.html#installing-in-windows) for info on how to install PyInstaller for Windows.    Once you have pyinstaller, use pyinstaller to generate the ``pyshacl.exe`` CLI file like so:  ```bash powershell  $ cd src/pyshacl  $ pyinstaller pyshacl-cli.spec  ```  This will output ``pyshacl.exe`` in the ``dist`` directory in ``src/pyshacl``.    You can now run the pySHACL Command Line utility via ``pyshacl.exe``.  See above for the pySHACL command line util usage instructions.      ## Compatibility  PySHACL is a Python3 library. For best compatibility use Python v3.7 or greater. Python3 v3.6 or below is _**not supported**_ and this library _**does not work**_ on Python v2.7.x or below.    PySHACL is now a PEP518 & PEP517 project, it uses `pyproject.toml` and `poetry` to manage dependencies, build and install.    For best compatibility when installing from PyPI with `pip`, upgrade to pip v18.1.0 or above.    - If you're on Ubuntu 16.04 or 18.04, you will need to run `sudo pip3 install --upgrade pip` to get the newer version.      ## Features  A features matrix is kept in the [FEATURES file](https://github.com/RDFLib/pySHACL/blob/master/FEATURES.md).      ## Changelog  A comprehensive changelog is kept in the [CHANGELOG file](https://github.com/RDFLib/pySHACL/blob/master/CHANGELOG.md).      ## Benchmarks  This project includes a script to measure the difference in performance of validating the same source graph that has been inferenced using each of the four different inferencing options. Run it on your computer to see how fast the validator operates for you.      ## License  This repository is licensed under Apache License, Version 2.0. See the [LICENSE deed](https://github.com/RDFLib/pySHACL/blob/master/LICENSE.txt) for details.      ## Contributors  See the [CONTRIBUTORS file](https://github.com/RDFLib/pySHACL/blob/master/CONTRIBUTORS.md).      ## Citation  DOI: [10.5281/zenodo.4750840](https://doi.org/10.5281/zenodo.4750840) (For all versions/latest version)    ## Contacts  Project Lead:  **Nicholas Car**  *Senior Experimental Scientist*  CSIRO Land & Water, Environmental Informatics Group  Brisbane, Qld, Australia  <nicholas.car@csiro.au>  <http://orcid.org/0000-0002-8742-7730>    Lead Developer:  **Ashley Sommer**  *Informatics Software Engineer*  CSIRO Land & Water, Environmental Informatics Group  Brisbane, Qld, Australia  <Ashley.Sommer@csiro.au>  <https://orcid.org/0000-0003-0590-0131> """
Semantic web;https://github.com/anno4j/anno4j;"""# Anno4j    > This library mainly provides programmatic access to the [W3C Web Annotation Data Model](http://www.w3.org/TR/annotation-model/) (formerly known as the [W3C Open Annotation Data Model](http://www.openannotation.org/spec/core/)) to allow Annotations to be written from and to local or remote SPARQL endpoints. An easy-to-use and extensible Java API allows creation and querying of Annotations even for non-experts. This API is augmented with various supporting functionalities to increase the usability of using the W3C Web Annotations.  >   > With the last iteration, Anno4j has also been developed to be able to work with generic metadata models. It is now possible to parse a RDFS or OWL Lite schema and generate the respective Anno4j classes on the fly via code generation.    ## Build Status  master branch: [![Build Status](https://travis-ci.org/anno4j/anno4j.svg?branch=master)](https://travis-ci.org/anno4j/anno4j) develop branch: [![Build Status](https://travis-ci.org/anno4j/anno4j.svg?branch=develop)](https://travis-ci.org/anno4j/anno4j)    ## Table of Content    The use of the Anno4j library and its features is documented in the respective [GitHub Anno4j Wiki](https://github.com/anno4j/anno4j/wiki). Its features are the following:    - Extensible creation of Web/Open Annotations based on Java Annotations syntax (see [Getting Started](https://github.com/anno4j/anno4j/wiki/Getting-started))  - Built-in and predefined implementations for nearly all RDF classes conform to the W3C Web Annotation Data Model  - Created (and annotated) Java POJOs are transformed to RDF and automatically transmitted to local/remote SPARQL 1.1 endpoints using the SPARQL Update functionality  - Querying of annotations with path-based criteria (see [Querying](https://github.com/anno4j/anno4j/wiki/Querying))      - [x] Basic comparisons like ""equal"", ""greater"", and ""lower""      - [x] String comparisons: ""equal"", ""contains"", ""starts with"", and ""ends with""      - [x] Union of different paths      - [x] Type condition      - [x] Custom filters  - Addition of custom behaviours of otherwise simple Anno4j classes through partial/support classes (see [Support Classes](https://github.com/anno4j/anno4j/wiki/Support-Classes))  - Input and Output to and from different standardised RDF serialisation standards (see [RDF Input and Output](https://github.com/anno4j/anno4j/wiki/RDF-Input-and-Output))  - Parsing of RDFS or OWL Lite schemata to automatically generate respective Anno4j classes (see [Java File Generation](https://github.com/anno4j/anno4j/wiki/Java-File-Generation))  - Schema/Validation annotations that can be added to Anno4j classes to induce schema-correctness which is indicated at the point of creation (see [Schema Validation](https://github.com/anno4j/anno4j/wiki/Schema-Validation) and [Schema Annotations](https://github.com/anno4j/anno4j/wiki/Schema-Annotations))  - A tool to support the generation of so-called proxy classes, that speed up the creation of instances of large and deep schemata    ## Status of Anno4j and the implemented WADM specification    The current version 2.4 of Anno4j supports the [most current W3C recommendation of the Web Annotation Data Model](https://www.w3.org/TR/annotation-model/).    ## Development Guidelines    ### Snapshot  Each push on the development branch triggers the build of a snapshot version. Snapshots are publicly available:  ```xml  	<dependency>   	<groupId>com.github.anno4j</groupId>     	<artifactId>anno4j-core</artifactId>     	<version>X.X.X-SNAPSHOT</version>  	</dependency>  ```         ### Compile, Package and Install    Package with:  ```        mvn package  ```         Install to your local repository  ```        mvn install  ```         ### Participate  1. Create an issue  2. Fork Anno4j  3. Add features  4. Add JUnit Tests  5. Create pull request to anno4j/develop      ### 3rd party integration of custom LDPath expressions    To contribute custom LDPath (test) functions and thereby custom LDPath syntax, the following two classes have to be provided:    1. Step:     Create a Java class that extends either the *SelectorFunction* class or the *TestFunction* class. This class defines the actual syntax  that has to be injected into the Anno4j evaluation process.    ```java      public class GetSelector extends SelectorFunction<Node> {                @Override          protected String getLocalName() {              return ""getSelector"";          }                @Override          public Collection<Node> apply(RDFBackend<Node> backend, Node context, Collection<Node>... args) throws IllegalArgumentException {              return null;          }                @Override          public String getSignature() {              return ""fn:getSelector(Annotation) : Selector"";          }                @Override          public String getDescription() {              return ""Selects the Selector of a given annotation object."";          }      }    ```     2. Step:    Create a Java class that actually evaluates the newly provided LDPath expression. This class needs  to be flagged with the *@Evaluator* Java annotation. The *@Evaluator* annotation requires the class   of the description mentioned in the first step. Besides that, the evaluator has to implement either  the *QueryEvaluator* or the *TestEvaluator* interface. Inside the prepared evaluate method, the actual  SPARQL query has to be generated using the Apache Jena framework.    ```java      @Evaluator(GetSelector.class)      public class GetSelectorFunctionEvaluator implements QueryEvaluator {          @Override          public Var evaluate(NodeSelector nodeSelector, ElementGroup elementGroup, Var var, LDPathEvaluatorConfiguration evaluatorConfiguration) {              Var evaluate = new SelfSelectionEvaluator().evaluate(nodeSelector, elementGroup, var, evaluatorConfiguration);              Var target = Var.alloc(""target"");              Var selector = Var.alloc(""selector"");                    elementGroup.addTriplePattern(new Triple(evaluate.asNode(), new ResourceImpl(OADM.HAS_TARGET).asNode(), target));              elementGroup.addTriplePattern(new Triple(target.asNode(), new ResourceImpl(OADM.HAS_SELECTOR).asNode(), selector));              return selector;          }      }  ```     ## Contributors    - Kai Schlegel (University of Passau)  - Andreas Eisenkolb (University of Passau)  - Emanuel Berndl (University of Passau)  - Thomas Weißgerber (University of Passau)  - Matthias Fisch (University of Passau)    > This software was partially developed within the [MICO project](http://www.mico-project.eu/) (Media in Context - European Commission 7th Framework Programme grant agreement no: 610480) and the [ViSIT project](http://www.phil.uni-passau.de/dh/projekte/visit/) (Virtuelle Verbund-Systeme und Informations-Technologien für die touristische Erschließung von kulturellem Erbe - Interreg Österreich-Bayern 2014-2020, project code: AB78).    ## License   Apache License Version 2.0 - http://www.apache.org/licenses/LICENSE-2.0    """
Semantic web;https://github.com/ruby-rdf/rdf-tabular;"""# Tabular Data RDF Reader and JSON serializer    [CSV][] reader for [RDF.rb][] and fully JSON serializer.    [![Gem Version](https://badge.fury.io/rb/rdf-tabular.png)](https://badge.fury.io/rb/rdf-tabular)  [![Build Status](https://github.com/ruby-rdf/rdf-tabular/workflows/CI/badge.svg?branch=develop)](https://github.com/ruby-rdf/rdf-tabular/actions?query=workflow%3ACI)  [![Coverage Status](https://coveralls.io/repos/ruby-rdf/rdf-tabular/badge.svg?branch=develop)](https://coveralls.io/github/ruby-rdf/rdf-tabular?branch=develop)  [![Gitter chat](https://badges.gitter.im/ruby-rdf/rdf.png)](https://gitter.im/ruby-rdf/rdf)    ## Features    RDF::Tabular parses CSV or other Tabular Data into [RDF][] and JSON using the [W3C CSVW][] specifications, currently undergoing development.    * Parses [number patterns](https://www.unicode.org/reports/tr35/tr35-39/tr35-numbers.html#Number_Patterns) from [UAX35][]  * Parses [date formats](https://www.unicode.org/reports/tr35/tr35-39/tr35-dates.html#Contents) from [UAX35][]  * Returns detailed errors and warnings using optional `Logger`.    ## Installation  Install with `gem install rdf-tabular`    ## Description  RDF::Tabular parses CSVs, TSVs, and potentially other tabular data formats. Using rules defined for [W3C CSVW][], it can also parse metadata files (in JSON-LD format) to find a set of tabular data files, or locate a metadata file given a CSV:    * Given a CSV `http://example.org/mycsv.csv` look for `http://example.org/mycsv.csv-metadata.json` or `http://example.org/metadata.json`. Metadata can also be specified using the `describedby` link header to reference a metadata file.  * Given a metadata file, locate one or more CSV files described within the metadata file.  * Also, extract _embedded metadata_ from the CSV (limited to column titles right now).    Metadata can then provide datatypes for the columns, express foreign key relationships, and associate subjects and predicates with columns. An example [metadata file for the project DOAP description](https://raw.githubusercontent.com/ruby-rdf/rdf-tabular/develop/etc/doap.csv-metadata.json) is:        {        ""@context"": ""http://www.w3.org/ns/csvw"",        ""url"": ""doap.csv"",        ""tableSchema"": {          ""aboutUrl"": ""https://rubygems.org/gems/rdf-tabular"",          ""propertyUrl"": ""http://usefulinc.com/ns/doap#{_name}"",          ""null"": """",          ""columns"": [            {""titles"": ""name""},            {""titles"": ""type"", ""propertyUrl"": ""rdf:type"", ""valueUrl"": ""{+type}""},            {""titles"": ""homepage"", ""valueUrl"": ""{+homepage}""},            {""titles"": ""license"", ""valueUrl"": ""{+license}""},            {""titles"": ""shortdesc"", ""lang"": ""en""},            {""titles"": ""description"", ""lang"": ""en""},            {""titles"": ""created"", ""datatype"": {""base"": ""date"", ""format"": ""M/d/yyyy""}},            {""titles"": ""programming_language"", ""propertyUrl"": ""http://usefulinc.com/ns/doap#programming-language""},            {""titles"": ""implements"", ""valueUrl"": ""{+implements}""},            {""titles"": ""category"", ""valueUrl"": ""{+category}""},            {""titles"": ""download_page"", ""propertyUrl"": ""http://usefulinc.com/ns/doap#download-page"", ""valueUrl"": ""{+download_page}""},            {""titles"": ""mailing_list"", ""propertyUrl"": ""http://usefulinc.com/ns/doap#mailing-list"", ""valueUrl"": ""{+mailing_list}""},            {""titles"": ""bug_database"", ""propertyUrl"": ""http://usefulinc.com/ns/doap#bug-database"", ""valueUrl"": ""{+bug_database}""},            {""titles"": ""blog"", ""valueUrl"": ""{+blog}""},            {""titles"": ""developer"", ""valueUrl"": ""{+developer}""},            {""titles"": ""maintainer"", ""valueUrl"": ""{+maintainer}""},            {""titles"": ""documenter"", ""valueUrl"": ""{+documenter}""},            {""titles"": ""maker"", ""propertyUrl"": ""foaf:maker"", ""valueUrl"": ""{+maker}""},            {""titles"": ""dc_title"", ""propertyUrl"": ""dc:title""},            {""titles"": ""dc_description"", ""propertyUrl"": ""dc:description"", ""lang"": ""en""},            {""titles"": ""dc_date"", ""propertyUrl"": ""dc:date"", ""datatype"": {""base"": ""date"", ""format"": ""M/d/yyyy""}},            {""titles"": ""dc_creator"", ""propertyUrl"": ""dc:creator"", ""valueUrl"": ""{+dc_creator}""},            {""titles"": ""isPartOf"", ""propertyUrl"": ""dc:isPartOf"", ""valueUrl"": ""{+isPartOf}""}          ]        }      }    This associates the metadata with the CSV [doap.csv](https://raw.githubusercontent.com/ruby-rdf/rdf-tabular/develop/etc/doap.csv), creates a common subject for all rows in the file, and a common predicate using the URI Template [URI Template](https://tools.ietf.org/html/rfc6570) `http://usefulinc.com/ns/doap#\{_name\}` which uses the `name` of each column (defaulted from `titles`) to construct a URI in the DOAP vocabulary, and constructs object URIs for object-valued properties from the contents of the column cells. In some cases, the predicates are changed on a per-column basis by using a different `propertyUrl` property on a given column.    This results in the following Turtle:        @prefix csvw: <http://www.w3.org/ns/csvw#> .      @prefix dc: <http://purl.org/dc/terms/> .      @prefix doap: <http://usefulinc.com/ns/doap#> .      @prefix foaf: <http://xmlns.com/foaf/0.1/> .      @prefix prov: <http://www.w3.org/ns/prov#> .      @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .      @prefix xsd: <http://www.w3.org/2001/XMLSchema#> .        <https://rubygems.org/gems/rdf-tabular> a doap:Project,           <http://www.w3.org/ns/earl#TestSubject>,           <http://www.w3.org/ns/earl#Software>;         dc:title ""RDF::Tabular"";         dc:creator <http://greggkellogg.net/foaf#me>;         dc:date ""2015-01-05""^^xsd:date;         dc:description ""RDF::Tabular processes tabular data with metadata creating RDF or JSON output.""@en;         dc:isPartOf <https://rubygems.org/gems/rdf>;         doap:blog <http://greggkellogg.net/>;         doap:bug-database <https://github.com/ruby-rdf/rdf-tabular/issues>;         doap:category <http://dbpedia.org/resource/Resource_Description_Framework>,           <http://dbpedia.org/resource/Ruby_(programming_language)>;         doap:created ""2015-01-05""^^xsd:date;         doap:description ""RDF::Tabular processes tabular data with metadata creating RDF or JSON output.""@en;         doap:developer <http://greggkellogg.net/foaf#me>;         doap:documenter <http://greggkellogg.net/foaf#me>;         doap:download-page <https://rubygems.org/gems/rdf-tabular>;         doap:homepage <http://ruby-rdf.github.com/rdf-tabular>;         doap:implements <http://www.w3.org/TR/tabular-data-model/>,           <http://www.w3.org/TR/tabular-metadata/>,           <http://www.w3.org/TR/csv2rdf/>,           <http://www.w3.org/TR/csv2json/>;         doap:license <https://unlicense.org/1.0/>;         doap:mailing-list <http://lists.w3.org/Archives/Public/public-rdf-ruby/>;         doap:maintainer <http://greggkellogg.net/foaf#me>;         doap:name ""RDF::Tabular"";         doap:programming-language ""Ruby"";         doap:shortdesc ""Tabular Data RDF Reader and JSON serializer.""@en;         foaf:maker <http://greggkellogg.net/foaf#me> .         [          a csvw:TableGroup;          csvw:table [            a csvw:Table;            csvw:row [              a csvw:Row;              csvw:describes <https://rubygems.org/gems/rdf-tabular>;              csvw:rownum 1;              csvw:url <file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=2>            ],  [              a csvw:Row;              csvw:describes <https://rubygems.org/gems/rdf-tabular>;              csvw:rownum 2;              csvw:url <file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=3>            ],  [              a csvw:Row;              csvw:describes <https://rubygems.org/gems/rdf-tabular>;              csvw:rownum 3;              csvw:url <file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=4>            ],  [              a csvw:Row;              csvw:describes <https://rubygems.org/gems/rdf-tabular>;              csvw:rownum 4;              csvw:url <file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=5>            ];            csvw:url <file://users/gregg/Projects/rdf-tabular/etc/doap.csv>          ];          prov:wasGeneratedBy [            a prov:Activity;            prov:endedAtTime ""2015-04-11T12:33:26Z""^^xsd:dateTime;            prov:qualifiedUsage [              a prov:Usage;              prov:entity <file://users/gregg/Projects/rdf-tabular/etc/doap.csv>;              prov:hadRole csvw:csvEncodedTabularData            ],  [              a prov:Usage;              prov:entity <file://users/gregg/Projects/rdf-tabular/etc/doap.csv-metadata.json>;              prov:hadRole csvw:tabularMetadata            ];            prov:startedAtTime ""2015-04-11T12:33:25Z""^^xsd:dateTime;            prov:wasAssociatedWith <https://rubygems.org/gems/rdf-tabular>          ]       ] .    The provenance on table-source information can be excluded by using the `:minimal` option to the reader.    It can also generate JSON output (not complete JSON-LD, but compatible with it), using the {RDF::Tabular::Reader#to_json} method:        {        ""table"": [          {            ""url"": ""file://users/gregg/Projects/rdf-tabular/etc/doap.csv"",            ""row"": [              {                ""url"": ""file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=2"",                ""rownum"": 1,                ""describes"": [                  {                    ""@id"": ""https://rubygems.org/gems/rdf-tabular"",                    ""http://usefulinc.com/ns/doap#name"": ""RDF::Tabular"",                    ""@type"": ""http://usefulinc.com/ns/doap#Project"",                    ""http://usefulinc.com/ns/doap#homepage"": ""http://ruby-rdf.github.com/rdf-tabular"",                    ""http://usefulinc.com/ns/doap#license"": ""https://unlicense.org/1.0/"",                    ""http://usefulinc.com/ns/doap#shortdesc"": ""Tabular Data RDF Reader and JSON serializer."",                    ""http://usefulinc.com/ns/doap#description"": ""RDF::Tabular processes tabular data with metadata creating RDF or JSON output."",                    ""http://usefulinc.com/ns/doap#created"": ""2015-01-05"",                    ""http://usefulinc.com/ns/doap#programming-language"": ""Ruby"",                    ""http://usefulinc.com/ns/doap#implements"": ""http://www.w3.org/TR/tabular-data-model/"",                    ""http://usefulinc.com/ns/doap#category"": ""http://dbpedia.org/resource/Resource_Description_Framework"",                    ""http://usefulinc.com/ns/doap#download-page"": ""https://rubygems.org/gems/rdf-tabular"",                    ""http://usefulinc.com/ns/doap#mailing-list"": ""http://lists.w3.org/Archives/Public/public-rdf-ruby/"",                    ""http://usefulinc.com/ns/doap#bug-database"": ""https://github.com/ruby-rdf/rdf-tabular/issues"",                    ""http://usefulinc.com/ns/doap#blog"": ""http://greggkellogg.net/"",                    ""http://usefulinc.com/ns/doap#developer"": ""http://greggkellogg.net/foaf#me"",                    ""http://usefulinc.com/ns/doap#maintainer"": ""http://greggkellogg.net/foaf#me"",                    ""http://usefulinc.com/ns/doap#documenter"": ""http://greggkellogg.net/foaf#me"",                    ""foaf:maker"": ""http://greggkellogg.net/foaf#me"",                    ""dc:title"": ""RDF::Tabular"",                    ""dc:description"": ""RDF::Tabular processes tabular data with metadata creating RDF or JSON output."",                    ""dc:date"": ""2015-01-05"",                    ""dc:creator"": ""http://greggkellogg.net/foaf#me"",                    ""dc:isPartOf"": ""https://rubygems.org/gems/rdf""                  }                ]              },              {                ""url"": ""file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=3"",                ""rownum"": 2,                ""describes"": [                  {                    ""@id"": ""https://rubygems.org/gems/rdf-tabular"",                    ""@type"": ""http://www.w3.org/ns/earl#TestSubject"",                    ""http://usefulinc.com/ns/doap#implements"": ""http://www.w3.org/TR/tabular-metadata/"",                    ""http://usefulinc.com/ns/doap#category"": ""http://dbpedia.org/resource/Ruby_(programming_language)""                  }                ]              },              {                ""url"": ""file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=4"",                ""rownum"": 3,                ""describes"": [                  {                    ""@id"": ""https://rubygems.org/gems/rdf-tabular"",                    ""@type"": ""http://www.w3.org/ns/earl#Software"",                    ""http://usefulinc.com/ns/doap#implements"": ""http://www.w3.org/TR/csv2rdf/""                  }                ]              },              {                ""url"": ""file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=5"",                ""rownum"": 4,                ""describes"": [                  {                    ""@id"": ""https://rubygems.org/gems/rdf-tabular"",                    ""http://usefulinc.com/ns/doap#implements"": ""http://www.w3.org/TR/csv2json/""                  }                ]              }            ]          }        ]      }    ## Tutorials    * [CSV on the Web](https://www.greggkellogg.net/2015/08/csv-on-the-web-presentation/)  * [Implementing CSV on the Web](https://greggkellogg.net/2015/04/implementing-csv-on-the-web/)    ## Command Line  When the `linkeddata` gem is installed, RDF.rb includes a `rdf` executable which acts as a wrapper to perform a number of different  operations on RDF files using available readers and writers, including RDF::Tabular. The commands specific to RDF::Tabular is     * `tabular-json`: Parse the CSV file and emit data as Tabular JSON    To use RDF::Tabular specific features, you must use the `--input-format tabular` option to the `rdf` executable.    Other `rdf` commands and options treat CSV as a standard RDF format.    Example usage:        rdf serialize https://raw.githubusercontent.com/ruby-rdf/rdf-tabular/develop/etc/doap.csv \        --output-format ttl      rdf tabular-json --input-format tabular https://raw.githubusercontent.com/ruby-rdf/rdf-tabular/develop/etc/doap.csv      rdf validate https://raw.githubusercontent.com/ruby-rdf/rdf-tabular/develop/etc/doap.csv --validate    Note that the `--validate` option must be used with the `validate` (or other) command to detect parse-time errors in addition to validating any resulting RDF triples.    ## RDF Reader  RDF::Tabular also acts as a normal RDF reader, using the standard RDF.rb Reader interface:        graph = RDF::Graph.load(""etc/doap.csv"", minimal: true)    ## Documentation  Full documentation available on [RubyDoc](https://rubydoc.info/gems/rdf-tabular/file/README.md)    ### Principal Classes  * {RDF::Tabular}    * {RDF::Tabular::JSON}    * {RDF::Tabular::Format}    * {RDF::Tabular::Metadata}    * {RDF::Tabular::Reader}    ## Dependencies  * [Ruby](https://ruby-lang.org/) (>= 2.6)  * [RDF.rb](https://rubygems.org/gems/rdf) (~> 3.2)  * [JSON](https://rubygems.org/gems/json) (>= 2.6)    ## Installation  The recommended installation method is via [RubyGems](https://rubygems.org/).  To install the latest official release of the `RDF::Tabular` gem, do:        % [sudo] gem install rdf-tabular    ## Mailing List  * <https://lists.w3.org/Archives/Public/public-rdf-ruby/>    ## Author  * [Gregg Kellogg](https://github.com/gkellogg) - <https://greggkellogg.net/>    ## Contributing  * Do your best to adhere to the existing coding conventions and idioms.  * Don't use hard tabs, and don't leave trailing whitespace on any line.  * Do document every method you add using [YARD][] annotations. Read the    [tutorial][YARD-GS] or just look at the existing code for examples.  * Don't touch the `rdf-tabular.gemspec`, `VERSION` or `AUTHORS` files. If you need to change them, do so on your private branch only.  * Do feel free to add yourself to the `CREDITS` file and the corresponding list in the the `README`. Alphabetical order applies.  * Do note that in order for us to merge any non-trivial changes (as a rule    of thumb, additions larger than about 15 lines of code), we need an    explicit [public domain dedication][PDD] on record from you,    which you will be asked to agree to on the first commit to a repo within the organization.    Note that the agreement applies to all repos in the [Ruby RDF](https://github.com/ruby-rdf/) organization.    License  -------    This is free and unencumbered public domain software. For more information,  see <https://unlicense.org/> or the accompanying {file:UNLICENSE} file.    [Ruby]:             https://ruby-lang.org/  [RDF]:              https://www.w3.org/RDF/  [YARD]:             https://yardoc.org/  [YARD-GS]:          https://rubydoc.info/docs/yard/file/docs/GettingStarted.md  [PDD]:              https://unlicense.org/#unlicensing-contributions  [RDF.rb]:           https://rubygems.org/gems/rdf  [CSV]:              https://en.wikipedia.org/wiki/Comma-separated_values  [W3C CSVW]:         https://www.w3.org/2013/csvw/wiki/Main_Page  [URI template]:     https://tools.ietf.org/html/rfc6570  [UAX35]:            https://www.unicode.org/reports/tr15/ """
Semantic web;https://github.com/epimorphics/elda;"""<h1>Elda, an implementation of the Linked Data API</h1>    <p>  	Elda is a Java implementation of the   	<a href=""http://code.google.com/p/linked-data-api/"" rel=""nofollow"">Linked Data API</a>,  	which provides a configurable way to access RDF data using simple   	RESTful URLs that are translated into queries to a SPARQL endpoint.   	The API developer (probably you) writes an API spec (in RDF) which   	specifies how to translate URLs into queries.   </p>    <p>  	Elda is the   	<a href=""http://www.epimorphics.com/web/"">Epimorphics</a> implementation of the LDA. The <i>standalone jar</i>  	comes with pre-built examples which allow you to experiment with the style   	of query and get started with building your own configurations using  	<i>elda common</i> or your own webapps.  </p>    <p>  	See <a href=""http://epimorphics.github.io/elda/current/index.html"">  		the current Elda documentation,  	</a> or the forthcoming release's documentation linked from  	<a href=""http://epimorphics.github.io/elda/index.html"">  		the documentation index  	</a>.  </p>   """
Semantic web;https://github.com/utapyngo/owl2vcs;"""owl2vcs is a set of tools designed to facilitate version control of  [OWL 2 ontologies][owl2] using version control systems.        Contents  --------    -   owl2diff - a command line diff tool for OWL 2 ontologies;    -   a set of scripts to integrate the tools with [Git][git], [Mercurial][hg] and [Subversion][svn].        Features  --------    -   Detects axioms additions and removals;    -   Detects imports additions and removals;    -   Detects ontology annotations additions and removals;    -   Detects prefix additions, removals, modifications and renames;    -   Detects ontology IRI and version IRI changes;    -   Detects ontology format changes;    -   Supports RDF/XML, OWL/XML, OWL Functional Syntax, Manchester OWL Syntax,      Turtle;    -   Changeset serializer and parser;    -   Two formats of changes: compact (like OWL Functional Syntax) and indented      (same but uses indents instead of parentheses, more readable);    -   Four formats of IRIs: Simple, QName, Full, Label.        Requirements  ------------    -   Java 1.6 or higher and `java` in `PATH`;    -   For [Git][git]: `git` in `PATH;`    -   For [Mercurial][hg]: `hg` in `PATH`;        Installation instructions  -------------------------    1.  [Download][owl2vcs-latest];    2.  Unzip;    3.  [Add][path] to `PATH`.        Standalone usage  ----------------    After adding the directory to `PATH` you can use the `owl2diff` command to compare two versions of an ontology. See `owl2diff --help` for more information.        Usage with Git/Mercurial  ------------------------    1.  Open command shell and `cd` into your repository;    2.  Type `owl2enable`;    3.  Now you can view informative diffs for \*.owl, \*.rdf, and \*.ttl files with either `hg owl2diff` or `git diff`.    * If `git diff` hangs on Windows, use `sh -c ""git diff""` or `git difftool`.    4.  If you want owl2vcs to compare files with other extensions, edit your `.hg/hgrc` or `.git/info/attributes`.        Please help out  ---------------    This project is still under development. Feedback and suggestions are very welcome and I encourage you to use the [Issues list][issues] on Github to provide that feedback.    Feel free to [fork][fork] this repo and to commit your additions.    Contributing  ------------    1.  [Fork it][fork].    2.  Clone the **develop** branch to your machine: `git clone -b develop git@github.com:utapyngo/owl2vcs.git`.    3.  Create your feature branch: `git checkout -b my-new-feature`.    4.  Commit your changes: `git commit -am 'Added some feature'`.    5.  Push to the branch `git push origin my-new-feature`.    6.  Create new Pull Request.    [owl2]:   http://www.w3.org/TR/owl2-overview/    [git]:    http://git-scm.com/    [hg]:     http://mercurial.selenic.com/    [svn]:    http://subversion.apache.org/    [owl2vcs-latest]: http://j.mp/owl2vcs-latest    [path]:   https://github.com/utapyngo/owl2vcs/wiki/How-to-add-owl2vcs-to-PATH    [issues]: http://github.com/utapyngo/owl2vcs/issues    [fork]:   https://github.com/utapyngo/owl2vcs/fork_select"""
Semantic web;https://github.com/antoniogarrote/rdfstore-js;"""#rdfstore-js [![Build Status](https://travis-ci.org/antoniogarrote/rdfstore-js.svg?branch=master)](https://travis-ci.org/antoniogarrote/rdfstore-js) [![Join the chat at https://gitter.im/antoniogarrote/rdfstore-js](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/antoniogarrote/rdfstore-js?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    ## Important Note    Many features present in versions 0.8.X have been removed in the 0.9.X. Some of them, will be added in the next versions, other like the MongoDB backend will be discarded.  Please read this README file carefully to find the current set of features.    ## Table of Contents    - [Introduction](#introduction)  - [Documentation](#documentation)  - [SPARQL support](#sparql-support)  - [Installation](#installation)  - [Building](#building)  - [Tests](#tests)  - [API](#api)  	- [Store creation](#store-creation)  	- [Query execution](#query-execution)  	- [Construct queries RDF Interfaces API](#construct-queries-rdf-interfaces-api)  	- [Loading remote graphs](#loading-remote-graphs)  	- [High level interface](#high-level-interface)  	- [RDF Interface API](#rdf-interface-api)  	- [Default Prefixes](#default-prefixes)  	- [JSON-LD Support](#json-ld-support)  	- [Events API](#events-api)  	- [Custom Filter Functions](#custom-filter-functions)  	- [Persistence](#persistence)  - [Dependencies](#dependencies)  - [Frontend](#frontend)  - [Contributing](#contributing)  - [Author](#author)  - [License](#license)      ## Introduction    rdfstore-js is a pure Javascript implementation of a RDF graph store with support for the SPARQL query and data manipulation language.  ```javascript  var rdfstore = require('rdfstore');    rdfstore.create(function(err, store) {    store.execute('LOAD <http://dbpedia.org/resource/Tim_Berners-Lee> INTO GRAPH <http://example.org/people>', function() {    	store.setPrefix('dbp', 'http://dbpedia.org/resource/');    	store.node(store.rdf.resolve('dbp:Tim_Berners-Lee'),  ""http://example.org/people"", function(err, graph) {    	  var peopleGraph = graph.filter(store.rdf.filters.type(store.rdf.resolve(""foaf:Person"")));    	  store.execute('PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\  					 PREFIX foaf: <http://xmlns.com/foaf/0.1/>\  					 PREFIX : <http://example.org/>\  					 SELECT ?s FROM NAMED :people { GRAPH ?g { ?s rdf:type foaf:Person } }',  					 function(err, results) {    					   console.log(peopleGraph.toArray()[0].subject.valueOf() === results[0].s.value);    					 });  	});      });  });  ```    rdfstore-js can be executed in a web browser or can be included as a library in a node.js application. It can also be executed as a stand-alone SPARQL end-point accepting SPARQL RDF Protocol HTTP requests. Go to the bottom of this page to find some application examples using the library.    The current implementation is far from complete but it already passes all the test cases for the SPARQL 1.0 query language and supports data manipulation operations from the SPARQL 1.1/Update version of the language.    Some other features included in the library are the following:    - SPARQL 1.0 support  - SPARQL 1.1/Update support  - Partial SPARQL 1.1 query support  - JSON-LD parser  - Turtle/N3 parser  - W3C RDF Interfaces API  - RDF graph events API  - Custom filter functions  - Browser persistence using IndexedDB    ## Documentation    Documentation for the store can be found [here](http://antoniogarrote.github.com/rdfstore-js/doc/index.html).    ## SPARQL support    rdfstore-js supports at the moment SPARQL 1.0 and most of SPARQL 1.1/Update.  Only some parts of SPARQL 1.1 query have been implemented yet.    This is a list of the different kind of queries currently implemented:    - SELECT queries  - UNION, OPTIONAL clauses  - NAMED GRAPH identifiers  - LIMIT, OFFSET  - ORDER BY clauses  - SPARQL 1.0 filters and builtin functions  - variable aliases  - variable aggregation: MAX, MIN, COUNT, AVG, SUM functions  - GROUP BY clauses  - DISTINCT query modifier  - CONSTRUCT queries  - ASK queries  - INSERT DATA queries  - DELETE DATA queries  - DELETE WHERE queries  - WITH/DELETE/INSERT/WHERE queries  - LOAD queries  - CREATE GRAPH clauses  - DROP DEFAULT/NAMED/ALL/GRAPH clauses  - CLEAR DEFAULT/NAMED/ALL/Graph clauses  - FILTER EXISTS / NOT EXISTS operators  - BIND  - FILTER IN / NOT IN operators      ##Installation    The library can be installed using NPM:    ```bash  $ npm install rdfstore  ```    The library can also be installed via bower using a global module:    ```bash  $ bower install rdfstore  ```    ##Building    Before running the build script, you must install JavaScript dependencies with [npm](https://npmjs.org/doc/install.html) (`npm` is shipped with [node](http://nodejs.org/download/)):    ```bash  $ npm install  ```    The library can be built using gulp:    ```bash  $ gulp  ```    The browser version can be built using the 'browser' gulp target:    ```bash  $ gulp browser  ```    ## Tests    To execute the whole test suite of the library, including the DAWG  test cases for SPARQL 1.0 and the test cases for SPARQL 1.1  implemented at the moment, a gulp target can be executed:    ```bash  $ gulp specs  ```    Additionally, there are some smoke tests for both browser versions that can be found ithe 'spec/browser'' directory.    ## API    This is a small overview of the rdfstore-js API.    ###Store creation    ```javascript  //nodejs only  var rdfstore = require('rdfstore');    // in the browser the rdfstore object  // is already defined    // alt 1  rdfstore.create(function(err, store) {    // the new store is ready  });      // alt 2  new rdfstore.Store(function(err, store) {    // the new store is ready  });  ```    ###Query execution    ```javascript  // simple query execution  store.execute(""SELECT * { ?s ?p ?o }"", function(err, results){    if(!err) {  	// process results  	if(results[0].s.token === 'uri') {  	  console.log(results[0].s.value);  	}    }  });    // execution with an explicit default and named graph    var defaultGraph = [{'token':'uri', 'value': graph1}, {'token':'uri', 'value': graph2}, ...];  var namedGraphs  = [{'token':'uri', 'value': graph3}, {'token':'uri', 'value': graph4}, ...];    store.executeWithEnvironment(""SELECT * { ?s ?p ?o }"",defaultGraph,    namedGraphs, function(err, results) {    if(err) {  	// process results    }  });  ```    ###Construct queries RDF Interfaces API    ```javascript  var query = ""CONSTRUCT { <http://example.org/people/Alice> ?p ?o } \  			 WHERE { <http://example.org/people/Alice> ?p ?o  }"";    store.execute(query, function(err, graph){    if(graph.some(store.rdf.filters.p(store.rdf.resolve('foaf:name')))) {  	nameTriples = graph.match(null,  							  store.rdf.createNamedNode(rdf.resolve('foaf:name')),  							  null);    	nameTriples.forEach(function(triple) {  	  console.log(triple.object.valueOf());  	});    }  });  ```    ###Loading remote graphs    rdfstore-js will try to retrieve remote RDF resources across the network when a 'LOAD' SPARQL query is executed.  The node.js build of the library will use regular TCP sockets and perform proper content negotiation. It will also follow a limited number of redirections.  The browser build, will try to perform an AJAX request to retrieve the resource using the correct HTTP headers. Nevertheless, this implementation is subjected to the limitations of the Same Domain Policy implemented in current browsers that prevents cross domain requests. Redirections, even for the same domain, may also fail due to the browser removing the 'Accept' HTTP header of the original request.  rdfstore-js relies in on the jQuery Javascript library to peform cross-browser AJAX requests. This library must be linked in order to exeucte 'LOAD' requests in the browser.    ```javascript  store.execute('LOAD <http://dbpedialite.org/titles/Lisp_%28programming_language%29>\  			   INTO GRAPH <lisp>', function(err){    if(err) {  	var query = 'PREFIX foaf:<http://xmlns.com/foaf/0.1/> SELECT ?o \  				 FROM NAMED <lisp> { GRAPH <lisp> { ?s foaf:page ?o} }';  	store.execute(query, function(err, results) {  	  // process results  	});    }  })  ```    ###High level interface    The following interface is a convenience API to work with Javascript code instead of using SPARQL query strings. It is built on top of the RDF Interfaces W3C API.    ```javascript  /* retrieving a whole graph as JS Interafce API graph object */    store.graph(graphUri, function(err, graph){    // process graph  });      /* Exporting a graph to N3 (this function is not part of W3C's API)*/  store.graph(graphUri, function(err, graph){    var serialized = graph.toNT();  });      /* retrieving a single node in the graph as a JS Interface API graph object */    store.node(subjectUri, function(err, node) {    //process node  });    store.node(subjectUri, graphUri, function(err, node) {    //process node  });        /* inserting a JS Interface API graph object into the store */    // inserted in the default graph  store.insert(graph, function(err) {}) ;    // inserted in graphUri  store.insert(graph, graphUri, function(err) {}) ;        /* deleting a JS Interface API graph object into the store */    // deleted from the default graph  store.delete(graph, function(err){});    // deleted from graphUri  store.delete(graph, graphUri, function(err){});        /* clearing a graph */    // clears the default graph  store.clear(function(err){});    // clears a named graph  store.clear(graphUri, function(err){});        /* Parsing and loading a graph */    // loading local data  store.load(""text/turtle"", turtleString, function(err, results) {});    // loading remote data  store.load('remote', remoteGraphUri, function(err, results) {});        /* Registering a parser for a new media type */    // The parser object must implement a 'parse' function  // accepting the data to parse and a callback function.    store.registerParser(""application/rdf+xml"", rdXmlParser);  ```    ###RDF Interface API    The store object includes a 'rdf' object implementing a RDF environment as described in the [RDF Interfaces 1.0](http://www.w3.org/TR/rdf-interfaces/) W3C's working draft.  This object can be used to access to the full RDF Interfaces 1.0 API.    ```javascript  var graph = store.rdf.createGraph();  graph.addAction(rdf.createAction(store.rdf.filters.p(store.rdf.resolve(""foaf:name"")),  								 function(triple){ var name = triple.object.valueOf();  												   var name = name.slice(0,1).toUpperCase()  												   + name.slice(1, name.length);  												   triple.object = store.rdf.createNamedNode(name);  												   return triple;}));    store.rdf.setPrefix(""ex"", ""http://example.org/people/"");  graph.add(store.rdf.createTriple( store.rdf.createNamedNode(store.rdf.resolve(""ex:Alice"")),  								  store.rdf.createNamedNode(store.rdf.resolve(""foaf:name"")),  								  store.rdf.createLiteral(""alice"") ));    var triples = graph.match(null, store.rdf.createNamedNode(store.rdf.resolve(""foaf:name"")), null).toArray();    console.log(""worked? ""+(triples[0].object.valueOf() === 'Alice'));  ```    ###Default Prefixes    Default RDF name-spaces can be specified using the *registerDefaultNamespace*. These names will be included automatically in all queries. If the same name-space is specified by the client in the query string the new prefix will shadow the default one.  A collection of common name-spaces like rdf, rdfs, foaf, etc. can be automatically registered using the *registerDefaultProfileNamespace* function.    ```javascript  new Store({name:'test', overwrite:true}, function(err,store){  	store.execute('INSERT DATA {  <http://example/person1> <http://xmlns.com/foaf/0.1/name> ""Celia"" }', function(err){    	   store.registerDefaultProfileNamespaces();    	   store.execute('SELECT * { ?s foaf:name ?name }', function(err,results) {  		   test.ok(results.length === 1);  		   test.ok(results[0].name.value === ""Celia"");  	   });  	});  });  ```    ###JSON-LD Support    rdfstore-js implements parsers for Turtle and JSON-LD. The specification of JSON-LD is still an ongoing effort. You may expect to find some inconsistencies between this implementation and the actual specification.    ```javascript  		jsonld = {  		  ""@context"":  		  {  			 ""rdf"": ""http://www.w3.org/1999/02/22-rdf-syntax-ns#"",  			 ""xsd"": ""http://www.w3.org/2001/XMLSchema#"",  			 ""name"": ""http://xmlns.com/foaf/0.1/name"",  			 ""age"": {""@id"": ""http://xmlns.com/foaf/0.1/age"", ""@type"": ""xsd:integer"" },  			 ""homepage"": {""@id"": ""http://xmlns.com/foaf/0.1/homepage"", ""@type"": ""xsd:anyURI"" },  			 ""ex"": ""http://example.org/people/""  		  },  		  ""@id"": ""ex:john_smith"",  		  ""name"": ""John Smith"",  		  ""age"": ""41"",  		  ""homepage"": ""http://example.org/home/""  		};    store.setPrefix(""ex"", ""http://example.org/people/"");    store.load(""application/ld+json"", jsonld, ""ex:test"", function(err,results) {    store.node(""ex:john_smith"", ""ex:test"", function(err, graph) {  	// process graph here    });  });  ```    ###Events API    rdfstore-js implements an experimental events API that allows clients to observe changes in the RDF graph and receive notifications when parts of this graph changes.  The two main event functions are *subscribe* that makes possible to set up a callback function that will be invoked each time triples matching a certain pattern passed as an argument are added or removed, and the function *startObservingNode* that will be invoked with the modified version of the node each time triples are added or removed from the node.    ```javascript  var cb = function(event, triples){    // it will receive a notifications where a triple matching    // the pattern s:http://example/boogk, p:*, o:*, g:*    // is inserted or removed.    if(event === 'added') {  	console.log(triples.length+"" triples have been added"");    } else if(event === 'deleted') {  	console.log(triples.length+"" triples have been deleted"");    }  }    store.subscribe(""http://example/book"",null,null,null,cb);      // .. do something;    // stop receiving notifications  store.unsubscribe(cb);  ```    The main difference between both methods is that *subscribe* receives the triples that have changed meanwhile *startObservingNode* receives alway the whole node with its updated triples. *startObservingNode* receives the node as a RDF Interface graph object.    ```javascript  var cb = function(node){    // it will receive the updated version of the node each    // time it is modified.    // If the node does not exist, the graph received will    // not contain triples.    console.log(""The node has now ""+node.toArray().length+"" nodes"");  }    // if only tow arguments are passed, the default graph will be used.  // A graph uri can be passed as an optional second argument.  store.startObservingNode(""http://example/book"",cb);      // .. do something;    // stop receiving notifications  store.stopObservingNode(cb);  ```    In the same way, there are *startObservingQuery* and *stopObservingQuery* functions that makes possible to set up callbacks for whole SPARQL queries.  The store will try to be smart and not perform unnecessary evaluations of these query after quad insertion/deletions. Nevertheless too broad queries must be used carefully with the events API.    ###Custom Filter Functions    Custom filter function can be registered into the store using the *registerCustomFunction* function. This function receives two argument, the name of the custom function and the associated implementation. This functions will be available in a SPARQL query using the prefix *custom*.  You can also use a full URI to identify the function that is going to be registered.  The function implementation will receive two arguments, an object linking to the store query filters engine and a list with the actual arguments. Arguments will consist of literal or URIs objects. Results from the function must also be literal or URI objects.    The query filters engine can be used to access auxiliary function to transform literals into JavaScript types using the *effectiveTypeValue* function, boolean values using the *effectiveBooleanValue*, to build boolean literal objects (*ebvTrue*, *ebvFalse*) or return an error with the *ebvError*. Documentation and source code for the *QueryFilters* object n the 'js-query-engine' module can be consulted to find information about additional helper functions.    The following test shows a simple examples of how custom functions can be invoked:    ```javascript  new Store({name:'test', overwrite:true}, function(err,store) {  	store.load(  		'text/n3',  		'@prefix test: <http://test.com/> .\  		 test:A test:prop 5.\  		 test:B test:prop 4.\  		 test:C test:prop 1.\  		 test:D test:prop 3.',  		function(err) {    			var invoked = false;              // instead of 'my_addition_check' a full URI can be used 'http://test.com/my_fns/my_addition_check'  			store.registerCustomFunction('my_addition_check', function(engine,args) {  		// equivalent to var v1 = parseInt(args[0].value), v2 = parseInt(args[1].value);    		var v1 = engine.effectiveTypeValue(args[0]);  		var v2 = engine.effectiveTypeValue(args[1]);    		// equivalent to return {token: 'literal', type:""http://www.w3.org/2001/XMLSchema#boolean"", value:(v1+v2<5)};    		return engine.ebvBoolean(v1+v2<5);  	});    	   store.execute(  				'PREFIX test: <http://test.com/> \  				 SELECT * { ?x test:prop ?v1 .\  							?y test:prop ?v2 .\  							filter(custom:my_addition_check(?v1,?v2)) }',  				function(err) {  				   test.ok(results.length === 3);  		   for(var i=0; i<results.length; i++) {  			test.ok(parseInt(results[i].v1.value) + parseInt(results[i].v2.value) < 5 );  		}  		test.done()  		}  	);    });  });  ```    ###Persistence    The store can be persisted in the browser using IndexedDB as the backend. In order to make the store persistent,  the 'persistent' flag must be set to true in the store creation options.  Additionally, a 'name' option can also be passed for the store. Different persistent instances of the store can be  opened using different names.    ###Controlling the frequency of function yielding    Performance of the store can be improved by reducing the frequency the 'nexTick' mechanism is used to cancel the the calls stack.  You can reduce this frequency by invoking the `yieldFrequency` function on the Store object and passing a bigger number:    ``` javascript  var rdfstore = require('rdfstore')  rdfstore.Store.yieldFrequency(200); // will only yield after 200 invocations of nextTick    ```  If the number is too big a number can produce stack overflow errors during execution. If you find this problem, reduce the value provided for `yieldFrequency`.    ##Dependencies    The library include dependencies to two semantic-web libraries for  parsing:    - [N3.js library](https://github.com/RubenVerborgh/N3.js/), developed    by Ruben Verborgh and released under the MIT license.    - [jsonld](https://github.com/digitalbazaar/jsonld.js), developed by Digital Bazaar and released under the New BSD license.    ##Frontend    A stand-along frontend for the store built using electron has been added in version 0.9.7.  You can build the frontend running the command:    ```bash  $ gulp frontend  ```    The file will be added under the releases directory.    ##Contributing    rdfstore-js is still at the beginning of its development. If you take a look at the library and find a way to improve it, please ping us. We'll be very greatful for any bug report or pull-request.    ## Author    Antonio Garrote, email:antoniogarrote@gmail.com, twitter:@antoniogarrote.      ## License    Licensed under the [MIT License](http://opensource.org/licenses/MIT), copyright Antonio Garrote 2011-2015 """
Semantic web;https://github.com/VeritasOS/fedx;"""# Welcome to the FedX Repository    FedX is a practical framework for transparent access to Linked Data sources through a federation.   It incorporates new sophisticated optimization techniques combined with effective variants of existing  techniques and is thus a highly scalable solution for practical federated query processing.      # FedX has moved    Veritas [contributed](https://rdf4j.org/news/2019/10/15/fedx-joins-rdf4j/) FedX to the Eclipse [RDF4J project](https://rdf4j.org/). Development continues there, please see https://github.com/eclipse/rdf4j.   """
Semantic web;https://github.com/nxparser/nxparser;"""# Welcome to NxParser #    NxParser is a Java open source, streaming, non-validating parser for the Nx format, where x = Triples, Quads, or any other number. For more details see the specification for the [NQuads format](http://sw.deri.org/2008/07/n-quads/), a extension for the [N-Triples](http://www.w3.org/TR/rdf-testcases/#ntriples) RDF format. Note that the parser handles any combination (cf. [generalised triples](http://www.w3.org/TR/rdf11-concepts/#section-generalized-rdf)) or number of N-Triples syntax terms on each line (the number of terms per line can also vary).    It ate 2 mil. quads (~4GB, (~240MB GZIPped)) on a T60p (Win7, 2.16 GHz)  in ~1 min 35 s (1:18min). Overall, it's more than twice as fast as the previous version when it comes to reading Nx.    The NxParser is non-validating, meaning that, e.g., it will happily eat non-conformant N-Triples. Also, the NxParser will not parse certain valid N-Triples files where the RDF terms are not separated by whitespace. We pass all positive W3C N-Triples test cases except one, where the RDF terms are not separated by whitespace (surprise!).    ## Other formats ##  The NxParser Parser family also includes a [RDF/XML](http://www.w3.org/TR/rdf-syntax-grammar/) and a [Turtle](http://www.w3.org/TR/turtle/) parser. Moreover, we attached a [JSON-LD](http://json-ld.org/) parser ([jsonld-java](https://github.com/jsonld-java/jsonld-java)) and a [RDFa](http://www.w3.org/TR/rdfa-core/) parser ([semargl](https://github.com/levkhomich/semargl)) such that they emit Triples in the NxParser API.    ## Binaries ##  Compiles are available on Maven Central. The groupId is `org.semanticweb.yars`. Depending on what part you need, you have to choose the artifactId accordingly: For example, if you only want to use the data model, use `nxparser-model`. If you want to make use of the parsers, use `nxparser-parsers`. If you want to use the RDF support for JAX-RS, use `nxparser-jax-rs`. The modules are linked as required.  ```xml  <dependency>    <groupId>org.semanticweb.yars</groupId>    <artifactId>nxparser-parsers</artifactId>    <version>2.3.3</version>  </dependency>    ```    ### Legacy binaries ###  Find old compiles in the repository on Google Code, which we do not maintain any more. To use it nevertheless, add  ```xml  <repository>   <id>nxparser-repo</id>   <url>    http://nxparser.googlecode.com/svn/repository   </url>  </repository>  <repository>   <id>nxparser-snapshots</id>   <url>    http://nxparser.googlecode.com/svn/snapshots   </url>  </repository>  ```  to your pom.xml.    ## Code Examples ##  ### Read Nx from a file ###  ```java  FileInputStream is = new FileInputStream(""path/to/file.nq"");    NxParser nxp = new NxParser();  nxp.parse(is);    for (Node[] nx : nxp)    // prints the subject, eg. <http://example.org/>    System.out.println(nx[0]);  ```    ### Use a blank node ###  ```java  // true means you are supplying proper N-Triples RDF terms that do not need to be processed  Resource subjRes = new Resource(""<http://example.org/123>"", true);  Resource predRes = new Resource(""<http://example.org/123>"", true);  BNode bn = new BNode(""_:bnodeId"", true);    Node[] triple = new Node[]{subjRes, predRes, bn};  // yields <http://example.org/123> <http://example.org/123> _:bnodeId  System.out.println(Arrays.toString(triple));  ```    ### Use Unicode-characters ###  ```java  String japaneseString = (""祝福は、チーズのメーカーです。"");  Literal japaneseLiteral = new Literal(japaneseString, ""ja"");    // yields ""\u795D\u798F\u306F\u3001\u30C1\u30FC\u30BA\u306E\u30E1\u30FC\u30AB\u30FC\u3067\u3059\u3002""@ja  System.out.println(japaneseLiteral);    // yields 祝福は、チーズのメーカーです。  System.out.println(japaneseLiteral.getLabel());  ```    ### Use datatyped literals ###  Example: Get a Calendar object from an `xsd:dateTime`-typed Literal  ```java  Literal dtl; // parser-generated  XSDDateTime dt = (XSDDateTime)DatatypeFactory.getDatatype(dtl);   GregorianCalendar cal = dt.getValue();  ```    ### Use from Python ###  Provided you use the Jython implementation (thanks to Uldis Bojars, this is saved from his now offline blog).    ```python  import sys  sys.path.append(""./nxparser.jar"")  	   from org.semanticweb.yars.nx.parser import *  from java.io import FileInputStream  from java.util.zip import GZIPInputStream  	   def all_triples(fname, use_gzip=False):    in_file = FileInputStream(fname)    if use_gzip:        in_file = GZIPInputStream(in_file)  	     nxp = NxParser()    nxp.parse(in_file)  	     while nxp.hasNext():      triple = nxp.next()      n3 = ([i.toString() for i in triple])      yield n3  ```  The code above defines a generator function which will yield a stream of NQuad records. We can now add some demo code in order to see it in action:  ```python  def main():    gzfname = ""sioc-btc-2009.gz""       for line in all_triples(gzfname, use_gzip=True):      print line  	     if __name__ == ""__main__"":      main()  ```  results in:  ```python  [u'<http://2008.blogtalk.net/node/29>', u'<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>', u'<http://rdfs.org/sioc/ns#Post>', u'<http://2008.blogtalk.net/sioc/node/29>']  [u'<http://2008.blogtalk.net/node/65>', u'<http://rdfs.org/sioc/ns#content>', u'""We\'ve created a map showing the main places of interest (event locations, restaurants, pubs, shopping locations and tourist sights) during BlogTalk 2008.  The conference venue is shown on the left-hand side of the map.  We will also have a hardcopy for all attendees. View Larger Map""', u'<http://2008.blogtalk.net/sioc/node/65>']  ```  	  #### issues with Eclipse ####  we had an issue with eclipse not being able to create his folder structure for nxparser-parsers, ```` mvn eclipse:eclipse ```` did the trick. """
Semantic web;https://github.com/correndo/mediation;"""# Mediation toolkit    It's a lightweight toolkit to implement ontological mediation over RDF.  It uses ontology mappings in order to rewrite SPARQL SELECT queries and to generate SPARQL CONSTRUCT queries to import an external data set.     API  --------     The tool is divided in the following packages:    * [uk.soton.service.dataset](https://github.com/correndo/mediation/tree/master/src/uk/soton/service/dataset) Provides the classes and interfaces necessaries to manages distributed datasets.  * [uk.soton.service.mediation](https://github.com/correndo/mediation/tree/master/src/uk/soton/service/mediation) Provides the classes and interfaces necessaries to mediate RDF documents and SPARQL queries using graph rewriting rules.  * [uk.soton.service.mediation.algebra](https://github.com/correndo/mediation/tree/master/src/uk/soton/service/mediation/algebra) Provides the classes and interfaces necessaries to manipulate SPARQL at the algebra level.  * [uk.soton.service.mediation.algebra.operation](https://github.com/correndo/mediation/tree/master/src/uk/soton/service/mediation/algebra/operation) Provides the implementation of SPARQL XPath functions.  * [uk.soton.service.mediation.edoal](https://github.com/correndo/mediation/tree/master/src/uk/soton/service/mediation/edoal) Provides the classes and interfaces necessaries to interface with the [EDOAL][edoal] ontology alignment format.    [edoal]: http://alignapi.gforge.inria.fr/edoal.html     The ontology alignments are represented as RDF files and describe rewriting rules that allows to define class mappings:  ```    	[]    <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                <http://ecs.soton.ac.uk/om.owl#Alignment> ;        <http://ecs.soton.ac.uk/om.owl#hasEntityAlignment>                [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                          <http://ecs.soton.ac.uk/om.owl#EntityAlignment> ;                  <http://ecs.soton.ac.uk/om.owl#hasRelation>                          <http://ecs.soton.ac.uk/om.owl#EQ> ;                  <http://ecs.soton.ac.uk/om.owl#lhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    <http://correndo.ecs.soton.ac.uk/ontology/target#Boiler> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b1                          ] ;                  <http://ecs.soton.ac.uk/om.owl#rhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    <http://correndo.ecs.soton.ac.uk/ontology/source#Kettle> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b1                          ]                ] ;  ```              ...property mappings:    ```        <http://ecs.soton.ac.uk/om.owl#hasEntityAlignment>                [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                          <http://ecs.soton.ac.uk/om.owl#EntityAlignment> ;                  <http://ecs.soton.ac.uk/om.owl#hasRelation>                          <http://ecs.soton.ac.uk/om.owl#EQ> ;                  <http://ecs.soton.ac.uk/om.owl#lhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    _:b2 ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://correndo.ecs.soton.ac.uk/ontology/target#boiler> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b3                          ] ;                  <http://ecs.soton.ac.uk/om.owl#rhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    _:b2 ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://correndo.ecs.soton.ac.uk/ontology/source#hasKettle> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b3                          ]                ] ;  ```    ...and data manipulation:    ```        <http://ecs.soton.ac.uk/om.owl#hasEntityAlignment>                [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                          <http://ecs.soton.ac.uk/om.owl#EntityAlignment> ;                  <http://ecs.soton.ac.uk/om.owl#hasFunctionalDependency>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                              <http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq> ;                                      <http://www.w3.org/1999/02/22-rdf-syntax-ns#_1>                                              _:b4 ;                                      <http://www.w3.org/1999/02/22-rdf-syntax-ns#_2>                                              273.15                                    ] ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://www.w3.org/2005/xpath-functions/sub> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b5                          ] ;                          <http://ecs.soton.ac.uk/om.owl#hasRelation>                          <http://ecs.soton.ac.uk/om.owl#EQ> ;                  <http://ecs.soton.ac.uk/om.owl#lhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    _:b4 ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://correndo.ecs.soton.ac.uk/ontology/target#temp> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b6                          ] ;                  <http://ecs.soton.ac.uk/om.owl#rhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    _:b5 ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://correndo.ecs.soton.ac.uk/ontology/source#hasTemperature> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b6                          ]                ] ;  ```                                 Once loaded an alignment the tool allows to rewrite a SPARQL SELECT query in order to fit a given schema:      [kettle-boiler] original query:  ```   	PREFIX  rdfs: <http://www.w3.org/2000/01/rdf-schema#>  	PREFIX  source: <http://correndo.ecs.soton.ac.uk/ontology/source#>  	PREFIX  owl:  <http://www.w3.org/2002/07/owl#>  	PREFIX  rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>  	SELECT DISTINCT  ?v ?y ?z ?lt  	WHERE  	{ ?v rdf:type source:Person .  	?v source:hasKettle ?y .  	?v source:hasKettle ?l .  	?y source:hasTemperature 10 .  	?l source:hasTemperature ?lt .  	} LIMIT   10  ```    [kettle-boiler] translated query:    ```  	SELECT DISTINCT  ?v ?y ?z ?lt  	WHERE  	{ ?v   <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>  <http://correndo.ecs.soton.ac.uk/ontology/target#User> ;  	       <http://correndo.ecs.soton.ac.uk/ontology/target#boiler>  ?y ;  	       <http://correndo.ecs.soton.ac.uk/ontology/target#boiler>  ?l ;  	?y   <http://correndo.ecs.soton.ac.uk/ontology/target#temp>  283.15 .  	?l  <http://correndo.ecs.soton.ac.uk/ontology/target#temp>  ?_12 .  	LET (?lt := ( ?_12 - 273.15 ))  	} LIMIT   10  ```	 """
Semantic web;https://github.com/white-gecko/TriplePlace;"""TriplePlace  ===========    This is TriplePlace a light weight and flexible Triple Store for Android. It uses a indexing structure similar to the  one in [Hexastore](http://www.zora.uzh.ch/8938/2/hexastore.pdf). TriplePlace uses  [TokyoCabinet](http://fallabs.com/tokyocabinet/) as persistent storage system. I've also published a patched version of [TokyoCabinet](https://github.com/white-gecko/TokyoCabinet) and TokyoCabinet-Java-API resp. [TokyoCabinet-Android-API](https://github.com/white-gecko/TokyoCabinet-Android-API).    TriplePlace is free software: you can redistribute it and/or modify it under the terms of the GNU General Public  License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later  version.    TriplePlace is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied  warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.    You should have received a copy of the GNU General Public License along with TriplePlace.  If not, see <[http://www.gnu.org/licenses/](http://www.gnu.org/licenses/)>.    To be done/To be implemented  ----------    - Implement some kind of logging to implement atomicity and consistency (ACID) between the indices  - Implement edit and maybe delete operations (norman says delete would be to expencive, maybe we have to mark those  triples as deleted)  - Implement query on graph patherns  - Implement RDF/XML, N-Triples/Turtle/N3 and ... import/export """
Semantic web;https://github.com/jiemakel/visu;"""VISU  ====    Visual SPARQL query tool, available at http://demo.seco.tkk.fi/visu/. Combines [YASQE](http://yasqe.yasgui.org/) and [YASR](http://yasr.yasgui.org/) from [YASGUI](http://yasgui.org/) with the [Google Visualization API](https://developers.google.com/chart/interactive/docs/reference), so that the results of SPARQL queries can be instantly visualized.    The tool is also able to save its state in the URL, so queries and visualizations can be shared. For example, [this query](http://goo.gl/QtDrzm) aggregates births and deaths by place and time in DBpedia, and visualizes the results as a motion chart. The chart shows for example that for some reason Wikipedia contains a disproportionate number of French people who died in the 1930s, while the number of Chinese people appearing is altogether very low. """
Semantic web;https://github.com/Wimmics/corese;"""<!-- markdownlint-configure-file { ""MD004"": { ""style"": ""consistent"" } } -->  <!-- markdownlint-disable MD033 -->    #    <p align=""center"">      <a href=""https://project.inria.fr/corese/"">          <img src=""https://user-images.githubusercontent.com/5692787/151987397-316a61f0-8098-4d37-a4e8-69180e33261a.svg"" width=""300"" height=""149"" alt=""Corese-logo"">      </a>      <br>      <strong>Software platform for the Semantic Web of Linked Data</strong>  </p>  <!-- markdownlint-enable MD033 -->    Corese is a software platform implementing and extending the standards of the Semantic Web. It allows to create, manipulate, parse, serialize, query, reason and validate RDF data.    Corese implement W3C standarts [RDF](https://www.w3.org/RDF/), [RDFS](https://www.w3.org/2001/sw/wiki/RDFS), [SPARQL1.1 Query & Update](https://www.w3.org/2001/sw/wiki/SPARQL), [OWL RL](https://www.w3.org/2005/rules/wiki/OWLRL), [SHACL](https://www.w3.org/TR/shacl/) …  It also implements extensions like [STTL SPARQL](https://files.inria.fr/corese/doc/sttl.html), [SPARQL Rule](https://files.inria.fr/corese/doc/rule.html) and [LDScript](https://files.inria.fr/corese/doc/ldscript.html).    There are three versions of Corese:    - **Corese-library:** Java library to process RDF data and use Corese features via an API.  - **Corese-server:** Tool to easily create, configure and manage SPARQL endpoints.  - **Corese-gui:** Graphical interface that allows an easy and visual use of Corese features.    ## Download and install    ### Corese-library    - Download from [maven-central](https://search.maven.org/search?q=g:fr.inria.corese)    ```xml  <dependency>      <groupId>fr.inria.corese</groupId>      <artifactId>corese-core</artifactId>      <version>4.3.0</version>  </dependency>    <dependency>      <groupId>fr.inria.corese</groupId>      <artifactId>corese-rdf4j</artifactId>      <version>4.3.0</version>  </dependency>  ```    Documentation: [Getting Started With Corese](https://notes.inria.fr/s/hiiedLfVe#)    ### Corese-server    - Download from [Docker-hub](https://hub.docker.com/r/wimmics/corese)    ```sh  docker run --name my-corese \      -p 8080:8080 \      -d wimmics/corese  ```    - Download [Corese-server jar file](https://project.inria.fr/corese/download/).    ```sh  # Replace ${VERSION} with the desired version number (e.g: 4.3.0)  wget ""files.inria.fr/corese/distrib/corese-server-${VERSION}.jar""  java -jar ""corese-server-${VERSION}.jar""  ```    ### Corese-GUI    - Download [Corese-gui jar file](https://project.inria.fr/corese/download/).    ```sh  # Replace ${VERSION} with the desired version number (e.g: 4.3.0)  wget ""files.inria.fr/corese/distrib/corese-gui-${VERSION}.jar""  java -jar ""corese-gui-${VERSION}.jar""  ```    ## Compilation from source    Download source code and compile.    ```shell  git clone ""https://github.com/Wimmics/corese.git""  cd corese  mvn package  ```    ## Contributions and discussions    For support questions, comments, and any ideas for improvements you'd like to discuss, please use our [discussion forum](https://github.com/Wimmics/corese/discussions/).  We welcome everyone to contribute to [issue reports](https://github.com/Wimmics/corese/issues), suggest new features, and create [pull requests](https://github.com/Wimmics/corese/pulls).    ## General informations    - [Corese website](https://project.inria.fr/corese)  - [Source code](https://github.com/Wimmics/corese)  - [Corese server demo](http://corese.inria.fr/)  - [Changelog](https://notes.inria.fr/s/TjriAbX14#)  - **Mailing list:** corese-users at inria.fr  - **Subscribe to mailing list:** corese-users-request at inria.fr **subject:** subscribe """
Semantic web;https://github.com/semarglproject/semargl;"""Welcome to the home of Semargl!  ===============================    Semargl is a modular framework for crawling [linked data](http://en.wikipedia.org/wiki/Linked_data)  from structured documents. The main goal of the project is to provide lightweight  and performant tool without excess dependencies.    At this moment Semargl offers high-performant streaming parsers for RDF/XML,  [RDFa](http://en.wikipedia.org/wiki/Rdfa), N-Triples, JSON-LD,  streaming serializers for Turtle, NTriples, NQuads and integration with Jena, Clerezza and Sesame.    Small memory footprint, and CPU requirements allow framework to be embedded in any system.  It runs seamlessly on Android and GAE.    You can check some framework capabilities via [RDFa parser demo](http://demo.semarglproject.org).    [![Maven Central](https://img.shields.io/maven-central/v/org.semarglproject/semargl-core.svg?style=flat-square)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.semarglproject%22%20semargl)  [![Build Status](https://img.shields.io/travis/semarglproject/semargl/master.svg?style=flat-square)](https://travis-ci.org/semarglproject/semargl)  [![Coverage Status](https://img.shields.io/coveralls/semarglproject/semargl.svg?style=flat-square)](https://coveralls.io/r/semarglproject/semargl?branch=master)    Why use Semargl?  ================    Lightweight  -----------    Semargl’s code is small and simple to understand. It has no external dependencies and  it will never [read a mail](http://en.wikipedia.org/wiki/Zawinski's_law_of_software_envelopment).  Internally it operates with a raw strings and creates as few objects as possible,  so your Android or GAE applications will be happy.    Standard conformant  -------------------    All parsers and serializers fully support  [corresponding W3C specifications](http://semarglproject.org/conformance.html) and test suites.    Dead Simple  -----------    No jokes!    ```xml  <dependency>      <groupId>org.semarglproject</groupId>      <artifactId>semargl-rdfa</artifactId>      <version>0.7</version>  </dependency>  ```    ```java  // just init triple store you want  MGraph graph = ... // Clerezza calls  // create processing pipe  StreamProcessor sp = new StreamProcessor(NTriplesParser.connect(ClerezzaSink.connect(graph));  // and run it  sp.process(file, docUri);  ```    If you want to use Semargl as a standalone framework, you can find useful internal  serializers and easily extendable API.    Build  =====    To build framework just run `mvn clean install`. RDFa tests require direct Internet connection. """
Semantic web;https://github.com/wastl/rdfdot;"""RDFDot  ======    Tools for drawing graphs from RDF files with GraphViz implemented in Java 8 using Java Native Interface calls  to GraphViz.    # Demo #    You can access the demo at [our demo server](http://demo4.newmedialab.at/rdfdot/), it should be running most of  the time. Simply cut & paste a Turtle or RDF/XML document, optionally change the configuration and press ""Render"".    # Building #    RDFDot comes with several different renderers for accessing the Graphviz layouting library. There are two different  approaches:    * call the _dot_ process as external shell command from Java; requires graphviz to be installed and accessible on your server  * use the Java Native Interface library that is provided by RDFDot; faster but requires manual compilation    ## External Shell Command ##    The first approach is simple to build using standard Maven:        mvn clean install -DskipTests    The -DskipTests is necessary at the moment, because some tests require the JNI library to work properly.    ## Java Native Interface ##    If you want to use the JNI library, please follow the following sequence:        mvn clean      cd rdfdot-core/src/main/native      make      cd ../../../..      mvn install    Calling make will download all the necessary C libraries, compile them and link them statically into a JNI library  for RDFdot. Note that this has only been tested to work on Linux.      # Installing #    RDFDot currently consists of the libraries rdfdot-api and rdfdot-core, which you can use in your own projects, and the  web application rdfdot-web, which can be deployed in any Java web container. When RDFDot has been properly installed,  simply add the approprate Maven dependencies to your project:        <dependency>          <groupId>net.wastl.rdfdot</groupId>          <artifactId>rdfdot-core</artifactId>          <version>1.0.0-SNAPSHOT</version>      </dependency>    If you want to use the (faster) JNI rendering library, it is necessary that you copy the libgraphviz.so library to  an appropriate location and call Java with -Djava.library.path=/path/to/location.      # Library Usage #    ## Configuration ##    The rdfdot-api library contains a class GraphConfiguration, which can be used for setting layouting configuration  options for the graph. It currently supports changing the node style, shape, color, and fillcolor for URI, BNode and  Literal nodes, as well as the arrow shape, style and color for edges. Furthermore, it is possible to change the layout  direction of the graph (default: left-right). All available options are defined using the enums Arrows, Layouts, Shapes  and Styles.    ## RDFHandler ##    The visualization library is implemented as a Sesame RDFHandler, so it can be used anywhere Sesame accepts an  RDFHandler, e.g.    * in a RIO parser (using RDFParser.setRDFHandler(...))  * in a repository query (using RepositoryConnection.exportStatements(...))  * in a SPARQL graph query (using GraphQuery.evaluate(...))    To initialize the RDFHandler, use e.g. the following sequence of statements:            GraphConfiguration configuration = new GraphConfiguration();          GraphvizSerializer serializer = new GraphvizSerializerNative(configuration);            RDFParser parser = Rio.createParser(RDFFormat.TURTLE);          parser.setRDFHandler(new GraphvizHandler(serializer));          parser.parse(in, ""http://localhost/"");            byte[] image = serializer.getResult();    Different GraphSerializers are available, including GraphSerializerNative (using JNI calls) and GraphSerializerCommand  (executing a shell command).   """
Semantic web;https://github.com/CLARIAH/grlc;"""<p algin=""center""><img src=""https://raw.githubusercontent.com/CLARIAH/grlc/master/src/static/grlc_logo_01.png"" width=""250px""></p>    [![PyPI version](https://badge.fury.io/py/grlc.svg)](https://badge.fury.io/py/grlc)  [![DOI](https://zenodo.org/badge/46131212.svg)](https://zenodo.org/badge/latestdoi/46131212)  [![Build Status](https://travis-ci.org/CLARIAH/grlc.svg?branch=master)](https://travis-ci.org/CLARIAH/grlc)      grlc, the <b>g</b>it <b>r</b>epository <b>l</b>inked data API <b>c</b>onstructor, automatically builds Web APIs using shared SPARQL queries. http://grlc.io/    If you use grlc in your work, please cite it as:    ```  @InProceedings{merono2016grlc,   author = {Mero{\~{n}}o-Pe{\~{n}}uela, Albert and Hoekstra, Rinke},   title = {{grlc Makes GitHub Taste Like Linked Data APIs}},   booktitle = {The Semantic Web: ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 -- June 2,  2016},   year = {2016},   publisher = {Springer},   pages = {342--353},   isbn = {978-3-319-47602-5},   doi = {10.1007/978-3-319-47602-5_48}  }  ```    ## What is grlc?  grlc is a lightweight server that takes SPARQL queries (stored in a GitHub repository, in your local filesystem, or listed in a URL), and translates them to Linked Data Web APIs. This enables universal access to Linked Data. Users are not required to know SPARQL to query their data, but instead can access a web API.    ## Quick tutorial  For a quick usage tutorial check out our wiki [walkthrough](https://github.com/CLARIAH/grlc/wiki/Quick-tutorial) and [list of features](https://github.com/CLARIAH/grlc/wiki/Features).    ## Usage  grlc assumes that you have a collection of SPARQL queries as .rq files (like [this](https://github.com/CLARIAH/grlc-queries)). grlc will create one API operation for each SPARQL query/.rq file in the collection.    Your queries can add API parameters to each operation by using the [parameter mapping](https://github.com/CLARIAH/grlc/wiki/Parameter-Mapping) syntax. This allows your query to define query variables which will be mapped to API parameters for your API operation ([see here](https://github.com/CLARIAH/grlc-queries/blob/master/enumerate.rq) for an example).    Your queries can include special [decorators](#decorator-syntax) to add extra functionality to your API.    ### Query location  grlc can load your query collection from different locations: from a GitHub repository (`api-git`), from local storage (`api-local`), and from a specification file (`api-url`). Each type of location has specific features and is accessible via different paths. However all location types produce the same beautiful APIs.    #### From a GitHub repository  > API path:  `http://grlc-server/api-git/<user>/<repo>`    grlc can build an API from any Github repository, specified by the GitHub user name of the owner (`<user>`) and repository name (`<repo>`).    For example, assuming your queries are stored on a Github repo: `https://github.com/CLARIAH/grlc-queries/`, point your browser to the following location  `http://grlc.io/api-git/CLARIAH/grlc-queries/`    grlc can make use of git's version control mechanism to generate an API based on a specific version of queries in the repository. This can be done by including the commit sha in the URL path (`http://grlc-server/api-git/<user>/<repo>/commit/<sha>`), for example: `http://grlc.io/api-git/CLARIAH/grlc-queries/commit/79ceef2ee814a12e2ec572ffaa2f8212a22bae23`    grlc can also use a subdirectory inside your Github repo. This can be done by including a subdirectory in the URL path (`http://grlc-server/api-git/<user>/<repo>/subdir/<subdir>`).    #### From local storage  > API path:  `http://grlc-server/api-local/`    grlc can generate an API from a local directory in the computer where your grlc server runs. You can configure the location of this directory in your [grlc server configuration file](#grlc-server-configuration). See also [how to install and run your own grlc instance](#install-and-run).    When the API is generated from a local directory, API information can be loaded from a configuration file in that folder. This file must be called `local-api-config.ini` and it has the following format:  ```ini  [repo_info]  repo_title = Some title  api_description = Description of my API  contact_name = My name  contact_url = https://mypage/  licence_url = https://mylicence/  ```    #### From a specification file  > API path:  `http://grlc-server/api-url/?specUrl=<specUrl>`    grlc can generate an API from a yaml specification file accessible on the web.    For example, assuming your queries are listed on spec file: `https://raw.githubusercontent.com/CLARIAH/grlc-queries/master/urls.yml`, point your browser to the following location  `http://grlc.io/api-url?specUrl=https://raw.githubusercontent.com/CLARIAH/grlc-queries/master/urls.yml`    ##### Specification file syntax  A grlc API specification file is a YAML file which includes the necessary information to create a grlc API, most importantly a list of URLs to decorated and HTTP-dereferenceable SPARQL queries. This file should contain the following fields     - `title`: Title of my API   - `description`: API description   - `contact`: Contact details of the API owner. This should include the `name` and `url` properties.   - `licence`: A URL pointing to the licence file for the API.   - `queries`: A list of URLs of SPARQL queries (with header decorators).    For example:  ```YAML  title: Title of my API  description: Description of my API  contact:    name: Contact Name    url: https://www.mywebsite.org  licence: http://example.org/licence.html  queries:    - https://www.mywebsite.org/query1.rq    - https://www.mywebsite.org/query2.rq    - https://www.otherwebsite.org/query3.rq  ```    ### grlc generated API    The API paths of all location types point to the generated [swagger-ui](https://swagger.io/) style API documentation. On the API documentation page, you can explore available API calls and execute individual API calls.    You can also view the swagger spec of your API, by visiting `<API-path>/swagger`, for example: `http://grlc.io/api-git/CLARIAH/grlc-queries/swagger`    ### grlc query execution  When you call an API endpoint, grlc executes the SPARQL query for that endpoint by combining supplied parameters and decorators.    There are 4 options to specify your own endpoint:    * Add a `sparql_endpoint` on your [`config.ini`](#grlc-server-configuration)  * Add a `endpoint` parameter to your request: 'http://grlc.io/user/repo/query?endpoint=http://sparql-endpoint/'. You can add a `#+ endpoint_in_url: False` decorator if you DO NOT want to see the `endpoint` parameter in the swagger-ui of your API.  * Add the `#+ endpoint:` [decorator](#`endpoint`).  * Add the URL of the endpoint on a single line in an `endpoint.txt` file within the GitHub repository that contains the queries.    The endpoint call will return the result of executing the query as a json representation of rdflib.query.QueryResult (for other result formats, you can use content negotiation via HTTP `Accept` headers). For json responses, the schema of the response can be modified by using the `#+ transform:` [decorator](#`transform`).    ## Decorator syntax  Special decorators are available to make your swagger-ui look nicer and to increase functionality. These are provided as comments at the start of your query file, making it still syntactically valid SPARQL. All decorators start with `#+ `, for example:    ```SPARQL  #+ decorator_1: decorator value  #+ decorator_1: decorator value    SELECT * WHERE {    ?s ?p ?o .  }  ```  The following is a list of available decorators and their function:    ### `summary`  Creates a summary of your query/operation. This is shown next to your operation name in the swagger-ui.    Syntax:  ```  #+ summary: This is the summary of my query/operation  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/summary.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_summary).    ### `description`  Creates a description of your query/operation. This is shown as the description of your operation in the swagger-ui.    Syntax:  ```  #+ description: Extended description of my query/operation.  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/description.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_description).    ### `endpoint`  Specifies a query-specific endpoint.    Syntax:  ```  #+ endpoint: http://example.com/sparql  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/endpoint.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_endpoint).    ### `pagination`  Paginates the results in groups of (for example) 100. Links to previous, next, first, and last result pages are provided as HTTP response headers to avoid polluting the payload (see details [here](https://developer.github.com/v3/guides/traversing-with-pagination/))    Syntax:  ```  #+ pagination: 100  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/pagination.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_pagination).    ### `method`  Indicates the HTTP request method (`GET` and `POST` are supported).    Syntax:  ```  #+ method: GET  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/method.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/post_method).    ### `tags`  Assign tags to your query/operation. Query/operations with the same tag are grouped together in the swagger-ui.    Syntax:  ```  #+ tags:  #+   - firstTag  #+   - secondTag  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/tags.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/group1/get_tags).    ### `defaults`  Set the default value in the swagger-ui for a specific parameter in the query.    Syntax:  ```  #+ defaults:  #+   - param_name: default_value  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/defaults.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_defaults).    ### `enumerate`  Indicates which parameters of your query/operation should get enumerations (and get dropdown menus in the swagger-ui) using the given values from the SPARQL endpoint. The values for each enumeration variable can also be specified into the query decorators to save endpoint requests and speed up the API generation.    Syntax:  ```  #+ enumerate:  #+   - var1:  #+     - value1  #+     - value2  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/enumerate.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_enumerate).    Notice that these should be plain variable names without SPARQL/BASIL conventions (so `var1` instead of `?_var1_iri`)    ###  `endpoint_in_url`  Allows/disallows the `endpoint` parameter from being provided as a URL parameter (allowed by default).    Syntax:  ```  #+ endpoint_in_url: False  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/endpoint_url.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_endpoint_url).    ###  `transform`  Allows  query results to be converted to the specified JSON structure, by using [SPARQLTransformer](https://github.com/D2KLab/py-sparql-transformer) syntax. Notice that the response content type must be set to `application/json` for the transformation to take effect.    Syntax:  ```  #+ transform: {  #+     ""key"": ""?p"",  #+     ""value"": ""?o"",  #+     ""$anchor"": ""key""  #+   }  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/transform.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_transform).    ### Example APIs    Check these out:  - http://grlc.io/api-git/CLARIAH/grlc-queries  - http://grlc.io/api-url?specUrl=https://raw.githubusercontent.com/CLARIAH/grlc-queries/master/urls.yml  - http://grlc.io/api-git/CLARIAH/wp4-queries-hisco  - http://grlc.io/api-git/albertmeronyo/lodapi  - http://grlc.io/api-git/albertmeronyo/lsq-api  - https://grlc.io/api-git/CEDAR-project/Queries    You'll find the sources of these and many more in [GitHub](https://github.com/search?o=desc&q=endpoint+summary+language%3ASPARQL&s=indexed&type=Code&utf8=%E2%9C%93)    Use [this GitHub search](https://github.com/search?q=endpoint+summary+language%3ASPARQL&type=Code&utf8=%E2%9C%93) to see examples from other grlc users.    ## Install and run  You can use grlc in different ways:   - [Via grlc.io](#grlc.io): you can use the [grlc.io service](https://grlc.io/)   - [Via Docker](#Docker): you can use the [grlc docker image](https://hub.docker.com/r/clariah/grlc) and start your own grlc server.   - [Via pip](#Pip): you can install the [grlc Python package](https://pypi.org/project/grlc/) and start your own grlc server or use grlc as a Python library.    More details for each of these options are given below.    ### grlc.io  The easiest way to use grlc is by visiting [grlc.io](http://grlc.io/) and using this service to convert SPARQL queries into a RESTful API. Your queries can be [stored on a github repo](#from-a-github-repository) or can be [listed on a specification file](#from-a-specification-file).    ### Docker  To run grlc via [docker](https://www.docker.com/), you'll need a working installation of docker. To deploy grlc, just pull the [latest image from Docker hub](https://hub.docker.com/r/clariah/grlc/). :  ```bash  docker run -it --rm -p 8088:80 clariah/grlc  ```    The docker image allows you to setup several environment variable such as `GRLC_SERVER_NAME` `GRLC_GITHUB_ACCESS_TOKEN` and `GRLC_SPARQL_ENDPOINT`:  ```bash  docker run -it --rm -p 8088:80 -e GRLC_SERVER_NAME=grlc.io -e GRLC_GITHUB_ACCESS_TOKEN=xxx -e GRLC_SPARQL_ENDPOINT=http://dbpedia.org/sparql -e DEBUG=true clariah/grlc  ```    ### Pip  If you want to run grlc locally or use it as a library, you can install grlc on your machine. Grlc is [registered in PyPi](https://pypi.org/project/grlc/) so you can install it using pip.    #### Prerequisites  grlc has the following requirements:  - Python3  - development files (depending on your OS):  ```bash  sudo apt-get install libevent-dev python-all-dev  ```    #### pip install  Once the base requirements are satisfied, you can install grlc like this:  ```bash  pip install grlc  ```    Once grlc is installed, you have several options:   - [Stand alone server](#Standalone-server)   - [Using a WSGI server](#Using-a-WSGI-server)   - [As a python library](#Grlc-library)    #### Standalone server  grlc includes a command line tool which you can use to start your own grlc server:  ```bash  grlc-server  ```    #### Using a WSGI server  You can run grlc using a WSGI server such as gunicorn as follows:  ```bash  gunicorn grlc.server:app  ```    If you want to use your own gunicorn configuration, for example `gunicorn_config.py`:  ```python  workers = 5  worker_class = 'gevent'  bind = '0.0.0.0:8088'  ```  Then you can run it as:  ```bash  gunicorn -c gunicorn_config.py grlc.server:app  ```    **Note:** Since `gunicorn` does not work under Windows, you can use `waitress` instead:  ```bash  waitress-serve --port=8088 grlc.server:app  ```    If you want to run grlc at system boot as a service, you can find example upstart scripts at [upstart/](upstart/grlc-docker.conf)    #### grlc library  You can use grlc as a library directly from your own python script. See the [usage example](https://github.com/CLARIAH/grlc/blob/master/doc/notebooks/GrlcFromNotebook.ipynb) to find out more.    #### grlc server configuration  Regardless of how you are running your grlc server, you will need to configure it using the `config.ini` file. Have a look at the [example config file](./config.default.ini) to see how it this file is structured.    The configuration file contains the following variables:   - `github_access_token` [access token](#github-access-token) to communicate with Github API.   - `local_sparql_dir` local storage directory where [local queries](#from-local-storage) are located.   - `server_name` name of the server (e.g. grlc.io)   - `sparql_endpoint` default SPARQL endpoint   - `user` and `password` SPARQL endpoint default authentication (if required, specify `'none'` if not required)   - `debug` enable debug level logging.    ##### GitHub access token  In order for grlc to communicate with GitHub, you'll need to tell grlc what your access token is:    1. Get a GitHub personal access token. In your GitHub's profile page, go to _Settings_, then _Developer settings_, _Personal access tokens_, and _Generate new token_  2. You'll get an access token string, copy it and save it somewhere safe (GitHub won't let you see it again!)  3. Edit your `config.ini` or `docker-compose.yml` as value of the environment variable `GRLC_GITHUB_ACCESS_TOKEN`.    # Contribute!  grlc needs **you** to continue bringing Semantic Web content to developers, applications and users. No matter if you are just a curious user, a developer, or a researcher; there are many ways in which you can contribute:    - File in bug reports  - Request new features  - Set up your own environment and start hacking    Check our [contributing](CONTRIBUTING.md) guidelines for these and more, and join us today!    If you cannot code, that's no problem! There's still plenty you can contribute:    - Share your experience at using grlc in Twitter (mention the handle **@grlcldapi**)  - If you are good with HTML/CSS, [let us know](mailto:albert.meronyo@gmail.com)    ## Related tools  - [SPARQL2Git](https://github.com/albertmeronyo/SPARQL2Git) is a Web interface for editing SPARQL queries and saving them in GitHub as grlc APIs.  - [grlcR](https://github.com/CLARIAH/grlcR) is a package for R that brings Linked Data into your R environment easily through grlc.  - [Hay's tools](https://tools.wmflabs.org/hay/directory/#/showall) lists grlc as a Wikimedia-related tool :-)    ## This is what grlc users are saying  - [Flavour your Linked Data with grlc](https://blog.esciencecenter.nl/flavour-your-linked-data-with-garlic-98bfbb358e06), by Carlos Martinez  - [Converting any SPARQL endpoint to an OpenAPI](http://chem-bla-ics.blogspot.com/2018/07/converting-any-sparql-endpoint-to.html) by Egon Willighagen    Quotes from grlc users:  > A cool project that can convert a random SPARQL endpoint into an OpenAPI endpoint    > It enables us to quickly integrate any new API requirements in a matter of seconds, without having to worry about configuration or deployment of the system    > You can store your SPARQL queries on GitHub and then you can run your queries on your favourite programming language (Python, Javascript, etc.) using a Web API (including swagger documentation) just as easily as loading data from a web page    **Contributors:**	[Albert Meroño](https://github.com/albertmeronyo), [Rinke Hoekstra](https://github.com/RinkeHoekstra), [Carlos Martínez](https://github.com/c-martinez)    **Copyright:**	Albert Meroño, Rinke Hoekstra, Carlos Martínez    **License:**	MIT License (see [LICENSE.txt](LICENSE.txt))    ## Academic publications    - Albert Meroño-Peñuela, Rinke Hoekstra. “grlc Makes GitHub Taste Like Linked Data APIs”. The Semantic Web – ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 – June 2, 2016, Revised Selected Papers. LNCS 9989, pp. 342-353 (2016). ([PDF](https://link.springer.com/content/pdf/10.1007%2F978-3-319-47602-5_48.pdf))  - Albert Meroño-Peñuela, Rinke Hoekstra. “SPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git”. In: Proceedings of the 14th Extended Semantic Web Conference (ESWC 2017), Poster and Demo Track. Portoroz, Slovenia, May 28th – June 1st, 2017 (2017). ([PDF](https://www.albertmeronyo.org/wp-content/uploads/2017/04/sparql2git-transparent-sparql-4.pdf))  - Albert Meroño-Peñuela, Rinke Hoekstra. “Automatic Query-centric API for Routine Access to Linked Data”. In: The Semantic Web – ISWC 2017, 16th International Semantic Web Conference. Lecture Notes in Computer Science, vol 10587, pp. 334-339 (2017). ([PDF](https://www.albertmeronyo.org/wp-content/uploads/2017/07/ISWC2017_paper_430.pdf))  - Pasquale Lisena, Albert Meroño-Peñuela, Tobias Kuhn, Raphaël Troncy. “Easy Web API Development with SPARQL Transformer”. In: The Semantic Web – ISWC 2019, 18th International Semantic Web Conference. Lecture Notes in Computer Science, vol 11779, pp. 454-470 (2019). ([PDF](https://www.albertmeronyo.org/wp-content/uploads/2019/06/ISWC2019_paper_237.pdf)) """
Semantic web;https://github.com/tarql/tarql;"""# Tarql: SPARQL for Tables    Tarql is a command-line tool for converting CSV files to RDF using SPARQL 1.1 syntax. It's written in Java and based on Apache ARQ.    **See http://tarql.github.io/ for documentation.**    ## Building    Get the code from GitHub: http://github.com/tarql/tarql    Tarql uses Maven. To create executable scripts for Windows and Unix in `/target/appassembler/bin/tarql`:        mvn package appassembler:assemble    Otherwise it's standard Maven. """
Semantic web;https://github.com/wikier/djubby;"""This project is currently **not maintained**, so please use it under your own risk.    # Djubby, a Linked Data frontend for SPARQL endpoints    Djubby is a Linked Data frontend for SPARQL endpoints for the Django Web framework.  It's quite inspired by Richard Cyganiak's [Pubby](http://wifo5-03.informatik.uni-mannheim.de/pubby/),   and with the exception of the HTML style, all the code has beed written from scratch   due the many differences between languages (Java vs. Python) and the frameworks (JavaEE vs. Django).    ![djubby](https://raw.githubusercontent.com/wikier/djubby/master/doc/images/djubby.png)    More information at: https://github.com/wikier/djubby    ## Features    * Provides a Linked Data interface to local or remote SPARQL protocol servers.  * Provides dereferenceable URIs by rewriting URIs found in the SPARQL-exposed dataset into the djubby server's namespace.  * Provides a simple HTML interface showing the data available about each resource.  * Takes care of handling 303 redirects and content negotiation.  * Compatible with the Django Web framework.    ### Planned     * Include a metadata extension to add metadata to the provided data.    ### Limitations    * Only works for SPARQL endpoint that can answer ASK and DESCRIBE queries.  * Multiple dataset support may not work as expected, so it is recommended to simply set up a separate djubby instance for each dataset.    ## Dependencies    * RDFLib >= 2.4.0  * SPARQLWrapper >= 1.3.2  * Django >= 1.1.0  * mimeparse >= 0.1.2    ## Usage    ### Installation        cd lib/      sudo python setup.py install    ## Authors    * [Sergio Fernández](http://www.wikier.org)    ## License    GNU Library or Lesser General Public License (LGPL) v3, http://www.gnu.org/licenses/lgpl.html """
Semantic web;https://github.com/owlcs/owlapitools;"""# OWLAPITOOLS 4.1.1  ## Built for OWLAPI version: 4.1.1    ### concurrentimpl  Multithread safe internals for the OWLAPI. Access with ThreadSafeOWLManager.    ### suggestor  An API to simplify recurring tasks that use a reasoner. Uses OWLKnowledgeExplorerReasoner, which works with FaCT++ or JFact.    ### atomicdecomposition  A reasoner independent, self contained implementation of atomic decomposition and modularisation. It is a port of the same tools implemented by Dmitry Tsarkov in FaCT++, but can be used in Java without JNI and without FaCT++. """
Semantic web;https://github.com/pchampin/sophia_rs;"""# Sophia    A Rust toolkit for RDF and Linked Data.    [![Actions Status](https://github.com/pchampin/sophia_rs/actions/workflows/lint_and_test.yml/badge.svg)](https://github.com/pchampin/sophia_rs/actions)  [![Latest Version](https://img.shields.io/crates/v/sophia.svg)](https://crates.io/crates/sophia)  [![Documentation](https://docs.rs/sophia/badge.svg)](https://docs.rs/sophia/)    It comprises the following crates:    * [`sophia_api`] defines a generic API for RDF and linked data,    as a set of core traits and types;    more precisely, it provides traits for describing    - terms, triples and quads,    - graphs and datasets,    - parsers and serializers  * [`sophia_iri`] provides functions, types and traits for validating and resolving IRIs.  * [`sophia_term`] defines implementations of the `TTerm` trait from `sophia_api`.  * [`sophia_inmem`] defines in-memory implementations of the `Graph` and `Dataset` traits from `sophia_api`.  * [`sophia_turtle`] provides parsers and serializers for the Turtle-family of concrete syntaxes.  * [`sophia_xml`] provides parsers and serializers for RDF/XML.  * [`sophia_jsonld`] provides preliminary support for JSON-LD.  * [`sophia_indexed`] and [`sophia_rio`] are lower-level crates, used by the ones above.     and finally:  * [`sophia`] is the “all-inclusive” crate,    re-exporting symbols from all the crates above.      ## Licence    [CECILL-B] (compatible with BSD)    ## Testing    The test suite depends on the [the [JSON-LD test-suite]  which is included as a `git` submodule.  In order to run all the tests, you need to execute the following commands:  ```bash  $ git submodule init  $ git submodule update  ```    ## Citation    When using Sophia, please use the following citation:    > Champin, P.-A. (2020) ‘Sophia: A Linked Data and Semantic Web toolkit for Rust’, in Wilde, E. and Amundsen, M. (eds). The Web Conference 2020: Developers Track, Taipei, TW. Available at: https://www2020devtrack.github.io/site/schedule.    Bibtex:  ```bibtex  @misc{champin_sophia_2020,          title = {{Sophia: A Linked Data and Semantic Web toolkit for Rust},          author = {Champin, Pierre-Antoine},          howpublished = {{The Web Conference 2020: Developers Track}},          address = {Taipei, TW},          editor = {Wilde, Erik and Amundsen, Mike},          month = apr,          year = {2020},          language = {en},          url = {https://www2020devtrack.github.io/site/schedule}  }  ```    ## History    An outdated comparison of Sophia with other RDF libraries is still available  [here](https://github.com/pchampin/sophia_benchmark/blob/master/benchmark_results.ipynb).      [`sophia_api`]: https://crates.io/crates/sophia_api  [`sophia_iri`]: https://crates.io/crates/sophia_iri  [`sophia_term`]: https://crates.io/crates/sophia_term  [`sophia_inmem`]: https://crates.io/crates/sophia_inmem  [`sophia_turtle`]: https://crates.io/crates/sophia_turtle  [`sophia_xml`]: https://crates.io/crates/sophia_xml  [`sophia_jsonld`]: https://crates.io/crates/sophia_jsonld  [`sophia_indexed`]: https://crates.io/crates/sophia_indexed  [`sophia_rio`]: https://crates.io/crates/sophia_rio  [`sophia`]: https://crates.io/crates/sophia  [CECILL-B]: https://cecill.info/licences/Licence_CeCILL-B_V1-en.html  [RDF test-suite]: https://github.com/w3c/rdf-tests/  [JSON-LD test-suite]: https://github.com/w3c/json-ld-api/ """
Semantic web;https://github.com/linkeddata/rabel;"""# rabel - linked data format converter    Program for reading and writing linked data in various formats.    To install,        npm install -g rabel    ## Command line    Commands look like unix options are executed *in order* from left to right. They  include:  ```  -base=rrrr    Set the current base URI (relative URI, default is file:///$PWD)  -clear        Clear the current store  -dump         Serialize the current store in current content type  -format=cccc  Set the current content-type  -help         This message  -in=uri       Load a web resource or file  -out=filename Output in the current content type  -report=file  set the report file destination for future validation  -size         Give the current store  -spray=base   Write out linked data to lots of different linked files CAREFUL!  -test=manifest   Run tests as described in the test manifest  -validate=shapeFile   Run a SHACL validator on the data loaded by previous in=x  -version      Give the version of this program  ```    Formats cccc are given as MIME types. These can be used for input or output:     * text/turtle   *(default)*   * application/rdf+xml    whereas these can only input:     * application/rdfa   * application/xml     #### Examples    ```    rabel -format=application/xml -in=foo.xml -format=text/turtle -out=foo.ttl    rabel part*.ttl -out=whole.ttl  ```  ## Details  Currently rabel can read from the web or files, and write only to files.  Filenames are deemed to be relative URIs just taken relative to file:///{pwd}/ where {pwd} is the  current working directory.    One use case is testing all the parsers. Another is providing a stable serialization. The output serialization is designed to be stable under small changes of the the data, to allow data files to be checked into source code control systems.    The name comes from RDF and Babel.    ### XML    When loading XML, elements are mapped to arcs, and text content to trimmed RDF strings. For the XML namespace used for IANA registry documents, custom mapping is done, both of properties and datatypes, and local identifier generation.  (See the source for details!) """
Semantic web;https://github.com/vastix/blazegraph-service;"""Vert.x Blazegraph Service  =========================    This service provides an asynchronous interface around Blazegraph.    Before use this service, place it into Maven local repo using this task:    ```  $ gradle publishToMavenLocal  ```    If you want to use this service from Maven local, use this in your dependencies part of your `build.gradle` file:    ```  dependencies {      ...      ...      compile 'name.bpdp.vertx:blazegraph-service:1.0.0'      ...      ...  }  ``` """
Semantic web;https://github.com/stardog-union/pellet;"""Pellet: An Open Source OWL DL reasoner for Java  -----------------------------------------------    [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/complexible/pellet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    Pellet is the OWL 2 DL reasoner:      * [open source](https://github.com/complexible/pellet/blob/master/LICENSE.txt) (AGPL) or commercial license  * pure Java  * developed and [commercially supported](http://complexible.com/) by Complexible Inc.     Pellet can be used with Jena or OWL-API libraries. Pellet provides functionality to check consistency of ontologies, compute the classification hierarchy,   explain inferences, and answer SPARQL queries.    _Pellet 3.0, a closed source, next-gen version of Pellet, is embedded and available in [Stardog](http://stardog.com/), the RDF database._    Feel free to fork this repository and submit pull requests if you want to  see changes, new features, etc. in Pellet.    Documentation about how to use Pellet is in the doc/ directory and there are some   code samples in the examples/ directory.                                        Commercial support for Pellet is [available](http://complexible.com/). The [Pellet FAQ](http://clarkparsia.com/pellet/faq) answers some frequently asked questions.    There is a [pellet-users mailing list](https://groups.google.com/forum/?fromgroups#!forum/pellet-users) for questions and feedback. You can search [pellet-users archives](http://news.gmane.org/gmane.comp.web.pellet.user).   Bug reports and enhancement requests should be sent to the mailing list. Issues are on [Github](http://github.com/complexible/pellet/issues).    Thanks for using Pellet. """
Semantic web;https://github.com/blake-regalia/linked-data.syntaxes;"""# Syntax highlighting for Linked Data developers    Each syntax highlighter in this package covers the *entire* grammar specification for its language. This provides very high-resolution scopes and immediately shows invalid syntax using special alert highlighting and [all tokens are inspectable](https://superuser.com/questions/848836/how-do-i-see-what-the-current-scope-is-in-sublimetext).    ### Install:  Available on [Package Control](https://packagecontrol.io/packages/LinkedData) as `LinkedData` .    Alternatively, you can download the `.sublime-package` file (or the source code archives) from the [Releases](https://github.com/blake-regalia/linked-data.syntaxes/releases).    #### Features:   - Highly-resolution scoping allows for very detailed color schemes.   - Malformed syntax detection. Expected token(s) are [inspectable via scope name](https://superuser.com/questions/848836/how-do-i-see-what-the-current-scope-is-in-sublimetext).   - Auto-completion and validation for prefix mappings registered on [prefix.cc](http://prefix.cc).    #### Currently supported languages:   - [SPARQL 1.1](https://www.w3.org/TR/sparql11-query/) and [SPARQL*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/)   - [RDF 1.1 Turtle](https://www.w3.org/TR/turtle/) (TTL), [Turtle*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/), and [RDF 1.1 TriG](https://www.w3.org/TR/trig/)   - [ShExC 2.1](https://shex.io/shex-semantics/#shexc)   - [RDF 1.1 N-Triples](https://www.w3.org/TR/n-triples/) (NT) and [RDF 1.1 N-Quads](https://www.w3.org/TR/n-quads/) (NQ)   - [Notation3](https://www.w3.org/TeamSubmission/n3/) (N3)    #### Currently supported platforms:   - Sublime Text 3    #### Currently supported color themes:   - Macaron Dark   - Macaron Light (in beta)    #### *Planned langauage support*:   - OWL Manchester   - OWL Functional-Style   - RDFa   - JSON-LD    #### *Planned platform support*:   - Atom   - CodeMirror   - Emacs   - minted (LaTeX)   - ~Ace~    #### *Planned color theme support*   - *Suggestions?*      ### Activating the Light Color Scheme  This package ships with two color schemes which were designed specifically for the high-resolution scopes that the syntax highlighting definitions create. By default, this package will use the [Macaron Dark](#macaron-dark) color scheme. If you prefer to use [Macaron Light](#macaron-light), you'll need to create a settings file to override the syntaxes defined below:    First, create a new file in Sublime and paste these contents into it:  ```json  // These settings will override both User and Default settings for the specific LinkedData syntaxes  {  	""color_scheme"": ""Packages/LinkedData/macaron-light.sublime-color-scheme""  }  ```    Next, save this file as `LinkedData.sublime-settings` under the `User/` folder in the [Sublime Text 3 Packages directory](https://stackoverflow.com/a/49967132/1641160). That is, the path should end with: `[...]/Packages/User/LinkedData.sublime-settings` .    Finally, create a new symbolic link to this file for each syntax. The files should be in the same `User` subdirectory as the other file:    For Linux and Mac, open terminal in this directory and run:  ```bash  ln -s LinkedData.sublime-settings n-triples.sublime-settings  ln -s LinkedData.sublime-settings n-quads.sublime-settings  ln -s LinkedData.sublime-settings turtle.sublime-settings  ln -s LinkedData.sublime-settings trig.sublime-settings  ln -s LinkedData.sublime-settings notation3.sublime-settings  ln -s LinkedData.sublime-settings shex.sublime-settings  ln -s LinkedData.sublime-settings sparql.sublime-settings  ```    For Windows, open command prompt in this directory and run:  ```cmd  mklink n-triples.sublime-settings  LinkedData.sublime-settings  mklink n-quads.sublime-settings    LinkedData.sublime-settings  mklink turtle.sublime-settings     LinkedData.sublime-settings  mklink trig.sublime-settings       LinkedData.sublime-settings  mklink notation3.sublime-settings  LinkedData.sublime-settings  mklink shex.sublime-settings       LinkedData.sublime-settings  mklink sparql.sublime-settings     LinkedData.sublime-settings  ```    This will override the default color scheme when any of these syntaxes are loaded in the current view.      ---    ## Previews:    ### Macaron Dark    #### Turtle:  ![Turtle Preview](doc/preview/macaron-dark/turtle.png)    #### SPARQL:  ![SPARQL Preview](doc/preview/macaron-dark/sparql.png)    #### ShExC:  ![ShExC Preview](doc/preview/macaron-dark/shex.png)    ### Macaron Light    #### Turtle:  ![Turtle Preview](doc/preview/macaron-light/turtle.png)    #### SPARQL:  ![SPARQL Preview](doc/preview/macaron-light/sparql.png)    #### ShExC:  ![ShExC Preview](doc/preview/macaron-light/shex.png)   """
Semantic web;https://github.com/amgadmadkour/sparti;"""# Query-Centric Semantic Partitioning (SPARTI)    * Adaptively partition based on query-workload  * Precompute Bloom join between the most frequent triples joins (MF-TJ) combinations  * Partition related properties based on a greedy algorithm and a cost model  * Current version is implemented to run over <b>Apache Spark</b>"""
Semantic web;https://github.com/dbiir/jdbc-for-rdf3x;"""# jdbc-for-rdf3x  This is a jdbc connector for rdf-3x. """
Semantic web;https://github.com/castagna/SARQ;"""SARQ - Free Text Indexing for SPARQL  ====================================    SARQ is a combination of ARQ and Solr. It gives ARQ the ability to perform  free text searches using a remote Solr server. Lucene indexes in Solr are   additional information for accessing the RDF graph, not storage for the   graph itself.    This is *experimental* (and unsupported).      How to use it  -------------    This is how you build an index from a Jena Model:        IndexBuilderModel builder = new IndexBuilderString(""http://127.0.0.1:8983/solr/sarq"");      builder.indexStatements(model.listStatements());      builder.commit();    This is how you configure ARQ to use Solr:                SARQ.setDefaultIndex(builder.getSolrServer());    This is an example of a SPARQL query using the sarq:search property function:         PREFIX sarq:     <http://openjena.org/SARQ/property#>      SELECT * WHERE {          ?doc ?p ?lit .          (?lit ?score ) sarq:search ""+text"" .      }      Acknowledgement  ---------------            The design and part of the code has been taken from LARQ, see:     * [http://openjena.org/ARQ/lucene-arq.html](http://openjena.org/ARQ/lucene-arq.html)      TODO  ----     * Fix the failing test and add more tests. [DONE]   * Double check the id as unique key, does it make sense? [DONE]   * Consider using multiValue=""true"" when index by subject, is it possible/useful?   * Test with Solr, add id=x title=foo then add id=x title=bar... if title is multi value field, what happens?   * Add required=""true"" in the schema.xml where appropriate?   * Add custom rank field and how to combined it with existing rank?   * Investigate allowDups=true|false, when is it appropriate/safe to use?   * Use [EmbeddedSolrServer](http://lucene.apache.org/solr/api/org/apache/solr/client/solrj/embedded/EmbeddedSolrServer.html) for testing   * ... """
Semantic web;https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark;"""# GeoSPARQL Compliance Benchmark    This is the GeoSPARQL Compliance Benchmark, integrated into the [HOBBIT Platform](https://github.com/hobbit-project/platform).    The GeoSPARQL Compliance Benchmark aims to evaluate the GeoSPARQL compliance of RDF storage systems. The benchmark uses  206 SPARQL queries to test the extent to which the benchmarked system supports the 30 requirements defined in the [GeoSPARQL standard](https://www.ogc.org/standards/geosparql).    As a result, the benchmark provides two metrics:   * **Correct answers**: The number of correct answers out of all GeoSPARQL queries, i.e. tests.   * **GeoSPARQL compliance percentage**: The percentage of compliance with the requirements of the GeoSPARQL standard.    ## Results    You can find a set of results from the [latest experiments on the hosted instance of the HOBBIT Platform](https://master.project-hobbit.eu/experiments/1612476122572,1612477003063,1612476116049,1625421291667,1612477500164,1612661614510,1612637531673,1612828110551,1612477849872)  (log in as Guest). [last update: 07.07.2021]    If you want your RDF triplestore tested, you can [add it as a system to the HOBBIT Platform](https://hobbit-project.github.io/system_integration.html),  and then [run an experiment](https://hobbit-project.github.io/benchmarking.html) using the [hosted instance of the HOBBIT Platform](https://hobbit-project.github.io/master.html).    ## Publications     * Milos Jovanovik, Timo Homburg, Mirko Spasić. ""[A GeoSPARQL Compliance Benchmark](https://www.mdpi.com/2220-9964/10/7/487)"". ISPRS International Journal of Geo-Information 10(7):487, 2021.   * Milos Jovanovik, Timo Homburg, Mirko Spasić. ""[Software for the GeoSPARQL Compliance Benchmark](https://doi.org/10.1016/j.simpa.2021.100071)"". Software Impacts 8:100071, 2021.   * (preprint) Milos Jovanovik, Timo Homburg, Mirko Spasić. ""[A GeoSPARQL Compliance Benchmark](https://arxiv.org/abs/2102.06139)"". arXiv:2102.06139.    ## Mapping Requirements to Queries    | Req. | Set of corresponding queries | Description  | :--: | :--- | :---   | <tr><th colspan=""3"">Core component (CORE)</th></tr>  | [R1](http://www.opengis.net/spec/geosparql/1.0/req/core/sparql-protocol) | [Q01.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r01.rq) | Selecting of the first triple where geometry A is the subject  | [R2](http://www.opengis.net/spec/geosparql/1.0/req/core/spatial-object-class ) | [Q02.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r02.rq) | Selecting of the first entity of type [geo:SpatialObject](http://www.opengis.net/ont/geosparql#SpatialObject)  | [R3](http://www.opengis.net/spec/geosparql/1.0/req/core/feature-class) | [Q03.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r03.rq) | Selecting of the first entity of type [geo:Feature](http://www.opengis.net/ont/geosparql#Feature)  | <tr><th colspan=""3"">Topology vocabulary extension (TOP)</th></tr>  | [R4](http://www.opengis.net/spec/geosparql/1.0/req/topology-vocab-extension/sf-spatial-relations) | [Q04-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-1.rq), [Q04-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-2.rq),  [Q04-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-3.rq), [Q04-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-4.rq), <br /> [Q04-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-5.rq), [Q04-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-6.rq),  [Q04-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-7.rq), [Q04-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-8.rq) | Testing the properties [geo:sfEquals](http://www.opengis.net/ont/geosparql#sfEquals), [geo:sfDisjoint](http://www.opengis.net/ont/geosparql#sfDisjoint), [geo:sfIntersects](http://www.opengis.net/ont/geosparql#sfIntersects),  [geo:sfTouches](http://www.opengis.net/ont/geosparql#sfTouches), [geo:sfCrosses](http://www.opengis.net/ont/geosparql#sfCrosses), [geo:sfWithin](http://www.opengis.net/ont/geosparql#sfWithin), [geo:sfContains](http://www.opengis.net/ont/geosparql#sfContains) and  [geo:sfOverlaps](http://www.opengis.net/ont/geosparql#sfOverlaps)  | [R5](http://www.opengis.net/spec/geosparql/1.0/req/topology-vocab-extension/eh-spatial-relations) | [Q05-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-1.rq), [Q05-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-2.rq),  [Q05-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-3.rq), [Q05-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-4.rq), <br /> [Q05-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-5.rq), [Q05-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-6.rq),  [Q05-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-7.rq), [Q05-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-8.rq) | Testing the properties [geo:ehEquals](http://www.opengis.net/ont/geosparql#ehEquals), [geo:ehDisjoint](http://www.opengis.net/ont/geosparql#ehDisjoint), [geo:ehMeet](http://www.opengis.net/ont/geosparql#ehMeet),  [geo:ehOverlap](http://www.opengis.net/ont/geosparql#ehOverlap), [geo:ehCovers](http://www.opengis.net/ont/geosparql#ehCovers), [geo:ehCoveredBy](http://www.opengis.net/ont/geosparql#ehCoveredBy), [geo:ehInside](http://www.opengis.net/ont/geosparql#ehInside) and  [geo:ehContains](http://www.opengis.net/ont/geosparql#ehContains)  | [R6](http://www.opengis.net/spec/geosparql/1.0/req/topology-vocab-extension/rcc8-spatial-relations) | [Q06-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-1.rq), [Q06-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-2.rq),  [Q06-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-3.rq), [Q06-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-4.rq), <br /> [Q06-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-5.rq), [Q06-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-6.rq),  [Q06-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-7.rq), [Q06-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-8.rq) | Testing the properties [geo:rcc8eq](http://www.opengis.net/ont/geosparql#rcc8eq), [geo:rcc8dc](http://www.opengis.net/ont/geosparql#rcc8dc), [geo:rcc8ec](http://www.opengis.net/ont/geosparql#rcc8ec), [geo:rcc8po](http://www.opengis.net/ont/geosparql#rcc8po),  [geo:rcc8tppi](http://www.opengis.net/ont/geosparql#rcc8tppi), [geo:rcc8tpp](http://www.opengis.net/ont/geosparql#rcc8tpp), [geo:rcc8ntpp](http://www.opengis.net/ont/geosparql#rcc8ntpp) and [geo:rcc8ntppi](http://www.opengis.net/ont/geosparql#rcc8ntppi)  | <tr><th colspan=""3"">Geometry extension (GEOEXT)</th></tr>  | [R7](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/geometry-class) | [Q07.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r07.rq) | Selecting of all entities of type [geo:Geometry](http://www.opengis.net/ont/geosparql#Geometry)  | [R8](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/feature-properties) | [Q08-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r08-1.rq), [Q08-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r08-2.rq) | Selecting of the value of geometry A denoted by the properties  [geo:hasGeometry](http://www.opengis.net/ont/geosparql#hasGeometry) and [geo:hasDefaultGeometry](http://www.opengis.net/ont/geosparql#hasDefaultGeometry)  | [R9](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/geometry-properties) | [Q09-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-1.rq), [Q09-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-2.rq),  [Q09-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-3.rq), [Q09-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-4.rq), <br /> [Q09-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-5.rq), [Q09-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-6.rq) | Selecting of the value of geometry A denoted by the properties  [geo:dimension](http://www.opengis.net/ont/geosparql#dimension), [geo:coordinateDimension](http://www.opengis.net/ont/geosparql#coordinateDimension), [geo:spatialDimension](http://www.opengis.net/ont/geosparql#spatialDimension),  [geo:isEmpty](http://www.opengis.net/ont/geosparql#isEmpty), [geo:isSimple](http://www.opengis.net/ont/geosparql#isSimple) and [geo:hasSerialization](http://www.opengis.net/ont/geosparql#hasSerialization)|   | [R10](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/wkt-literal) | [Q10.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r10.rq) | Checking of the datatype of a correctly defined WKT literal from the dataset  | [R11](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/wkt-literal-default-srs) | [Q11.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r11.rq) | Checking of the equality of two geometries from the dataset  | [R12](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/wkt-axis-order) | [Q12.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r12.rq) | Checking if the system interprets the axis order within a point geometry  according to the spatial reference system being used  | [R13](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/wkt-literal-empty) | [Q13-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r13-1.rq), [Q13-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r13-2.rq) | Checking if an empty RDFS Literal of type [geo:wktLiteral](http://www.opengis.net/ont/geosparql#wktLiteral) is interpreted as  an empty geometry  | [R14](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/geometry-as-wkt-literal) | [Q14.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r14.rq) | Checking of the [geo:asWKT](http://www.opengis.net/ont/geosparql#asWKT) value of geometry A against the expected  literal value  | [R15](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/gml-literal) | [Q15.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r15.rq) | Checking whether all the values of the [geo:asGML](http://www.opengis.net/ont/geosparql#asGML) property contain a valid  GM_Object subtype in it and whether its datatype is [geo:gmlLiteral](http://www.opengis.net/ont/geosparql#gmlLiteral)  | [R16](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/gml-literal-empty) | [Q16-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r16-1.rq), [Q16-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r16-2.rq) | Checking if an empty [geo:gmlLiteral](http://www.opengis.net/ont/geosparql#gmlLiteral) is interpreted as an empty geometry  | [R17](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/gml-profile) | --- | ---  | [R18](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/geometry-as-gml-literal) | [Q18.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r18.rq) | Checking the [geo:asGML](http://www.opengis.net/ont/geosparql#asGML) value of geometry A against the expected literal value  | [R19](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/query-functions) | [Q19-1-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-1-1.rq), [Q19-1-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-1-2.rq),  [Q19-1-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-1-3.rq), [Q19-1-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-1-4.rq), <br /> [Q19-2-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-2-1.rq), [Q19-2-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-2-2.rq),  [Q19-3-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-3-1.rq), [Q19-3-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-3-2.rq), <br /> [Q19-4-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-4-1.rq), [Q19-4-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-4-2.rq),  [Q19-4-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-4-3.rq), [Q19-4-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-4-4.rq), <br /> [Q19-5-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-5-1.rq), [Q19-5-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-5-2.rq),  [Q19-5-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-5-3.rq), [Q19-5-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-5-4.rq), <br /> [Q19-6-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-6-1.rq), [Q19-6-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-6-2.rq),  [Q19-6-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-6-3.rq), [Q19-6-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-6-4.rq), <br /> [Q19-7-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-7-1.rq), [Q19-7-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-7-2.rq),  [Q19-7-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-7-3.rq), [Q19-7-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-7-4.rq), <br /> [Q19-8-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-8-1.rq), [Q19-8-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-8-2.rq),  [Q19-9-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-9-1.rq), [Q19-9-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-9-2.rq) | Checking a support of the geospatial functions [geof:distance](http://www.opengis.net/def/function/geosparql/distance), [geof:buffer](http://www.opengis.net/def/function/geosparql/buffer),  [geof:convexHull](http://www.opengis.net/def/function/geosparql/convexHull), [geof:intersection](http://www.opengis.net/def/function/geosparql/intersection), [geof:union](http://www.opengis.net/def/function/geosparql/union), [geof:difference](http://www.opengis.net/def/function/geosparql/difference),  [geof:symDifference](http://www.opengis.net/def/function/geosparql/symDifference), [geof:envelope](http://www.opengis.net/def/function/geosparql/envelope) and [geof:boundary](http://www.opengis.net/def/function/geosparql/boundary)  | [R20](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/srid-function) | [Q20-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r20-1.rq), [Q20-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r20-2.rq) | Checking a support of the geospatial function [geof:getSRID](http://www.opengis.net/def/function/geosparql/getSRID)  | <tr><th colspan=""3"">Geometry topology extension (GTOP)</th></tr>  | [R21](http://www.opengis.net/spec/geosparql/1.0/req/geometry-topology-extension/relate-query-function) | [Q21-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r21-1.rq), [Q21-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r21-2.rq),  [Q21-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r21-3.rq), [Q21-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r21-4.rq) | Checking a support of the geospatial operator [geof:relate](http://www.opengis.net/def/function/geosparql/relate)  | [R22](http://www.opengis.net/spec/geosparql/1.0/req/geometry-topology-extension/sf-query-functions) | [Q22-1-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-1-1.rq), [Q22-1-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-1-2.rq),  [Q22-1-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-1-3.rq), [Q22-1-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-1-4.rq), <br /> [Q22-2-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-2-1.rq), [Q22-2-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-2-2.rq),  [Q22-2-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-2-3.rq), [Q22-2-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-2-4.rq), <br /> [Q22-3-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-3-1.rq), [Q22-3-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-3-2.rq),  [Q22-3-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-3-3.rq), [Q22-3-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-3-4.rq), <br /> [Q22-4-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-4-1.rq), [Q22-4-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-4-2.rq),  [Q22-4-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-4-3.rq), [Q22-4-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-4-4.rq), <br /> [Q22-5-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-5-1.rq), [Q22-5-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-5-2.rq),  [Q22-5-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-5-3.rq), [Q22-5-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-5-4.rq), <br /> [Q22-6-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-6-1.rq), [Q22-6-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-6-2.rq),  [Q22-6-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-6-3.rq), [Q22-6-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-6-4.rq), <br /> [Q22-7-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-7-1.rq), [Q22-7-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-7-2.rq),  [Q22-7-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-7-3.rq), [Q22-7-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-7-4.rq), <br /> [Q22-8-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-8-1.rq), [Q22-8-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-8-2.rq),  [Q22-8-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-8-3.rq), [Q22-8-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-8-4.rq) | Checking a support of the geospatial functions [geof:sfEquals](http://www.opengis.net/def/function/geosparql/sfEquals), [geof:sfDisjoint](http://www.opengis.net/def/function/geosparql/sfDisjoint),  [geof:sfIntersects](http://www.opengis.net/def/function/geosparql/sfIntersects), [geof:sfTouches](http://www.opengis.net/def/function/geosparql/sfTouches), [geof:sfCrosses](http://www.opengis.net/def/function/geosparql/sfCrosses), [geof:sfWithin](http://www.opengis.net/def/function/geosparql/sfWithin), [geof:sfContains](http://www.opengis.net/def/function/geosparql/sfContains)  and [geof:sfOverlaps](http://www.opengis.net/def/function/geosparql/sfOverlaps)  | [R23](http://www.opengis.net/spec/geosparql/1.0/geometry-topology-extension/eh-query-functions) | [Q23-1-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-1-1.rq), [Q23-1-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-1-2.rq),  [Q23-1-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-1-3.rq), [Q23-1-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-1-4.rq), <br /> [Q23-2-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-2-1.rq), [Q23-2-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-2-2.rq),  [Q23-2-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-2-3.rq), [Q23-2-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-2-4.rq), <br /> [Q23-3-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-3-1.rq), [Q23-3-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-3-2.rq),  [Q23-3-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-3-3.rq), [Q23-3-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-3-4.rq), <br /> [Q23-4-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-4-1.rq), [Q23-4-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-4-2.rq),  [Q23-4-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-4-3.rq), [Q23-4-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-4-4.rq), <br /> [Q23-5-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-5-1.rq), [Q23-5-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-5-2.rq),  [Q23-5-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-5-3.rq), [Q23-5-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-5-4.rq), <br /> [Q23-6-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-6-1.rq), [Q23-6-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-6-2.rq),  [Q23-6-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-6-3.rq), [Q23-6-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-6-4.rq), <br /> [Q23-7-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-7-1.rq), [Q23-7-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-7-2.rq),  [Q23-7-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-7-3.rq), [Q23-7-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-7-4.rq), <br /> [Q23-8-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-8-1.rq), [Q23-8-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-8-2.rq),  [Q23-8-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-8-3.rq), [Q23-8-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-8-4.rq) | Checking a support of the geospatial functions [geof:ehEquals](http://www.opengis.net/def/function/geosparql/ehEquals), [geof:ehDisjoint](http://www.opengis.net/def/function/geosparql/ehDisjoint),  [geof:ehMeet](http://www.opengis.net/def/function/geosparql/ehMeet), [geof:ehOverlap](http://www.opengis.net/def/function/geosparql/ehOverlap), [geof:ehCovers](http://www.opengis.net/def/function/geosparql/ehCovers), [geof:ehCoveredBy](http://www.opengis.net/def/function/geosparql/ehCoveredBy), [geof:ehInside](http://www.opengis.net/def/function/geosparql/ehInside)  and [geof:ehContains](http://www.opengis.net/def/function/geosparql/ehContains)  | [R24](http://www.opengis.net/spec/geosparql/1.0/req/geometry-topology-extension/rcc8-query-functions) | [Q24-1-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-1-1.rq), [Q24-1-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-1-2.rq),  [Q24-1-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-1-3.rq), [Q24-1-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-1-4.rq), <br /> [Q24-2-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-2-1.rq), [Q24-2-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-2-2.rq),  [Q24-2-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-2-3.rq), [Q24-2-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-2-4.rq), <br /> [Q24-3-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-3-1.rq), [Q24-3-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-3-2.rq),  [Q24-3-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-3-3.rq), [Q24-3-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-3-4.rq), <br /> [Q24-4-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-4-1.rq), [Q24-4-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-4-2.rq),  [Q24-4-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-4-3.rq), [Q24-4-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-4-4.rq), <br /> [Q24-5-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-5-1.rq), [Q24-5-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-5-2.rq),  [Q24-5-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-5-3.rq), [Q24-5-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-5-4.rq), <br /> [Q24-6-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-6-1.rq), [Q24-6-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-6-2.rq),  [Q24-6-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-6-3.rq), [Q24-6-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-6-4.rq), <br /> [Q24-7-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-7-1.rq), [Q24-7-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-7-2.rq),  [Q24-7-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-7-3.rq), [Q24-7-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-7-4.rq), <br /> [Q24-8-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-8-1.rq), [Q24-8-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-8-2.rq),  [Q24-8-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-8-3.rq), [Q24-8-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-8-4.rq) | Checking a support of the geospatial functions [geof:rcc8eq](http://www.opengis.net/def/function/geosparql/rcc8eq), [geof:rcc8dc](http://www.opengis.net/def/function/geosparql/rcc8dc), [geof:rcc8ec](http://www.opengis.net/def/function/geosparql/rcc8ec),  [geof:rcc8po](http://www.opengis.net/def/function/geosparql/rcc8po), [geof:rcc8tppi](http://www.opengis.net/def/function/geosparql/rcc8tppi), [geof:rcc8tpp](http://www.opengis.net/def/function/geosparql/rcc8tpp), [geof:rcc8ntpp](http://www.opengis.net/def/function/geosparql/rcc8ntpp) and [geof:rcc8ntppi](http://www.opengis.net/def/function/geosparql/rcc8ntppi)  | <tr><th colspan=""3"">RDFS entailment extension (RDFSE)</th></tr>  | [R25](http://www.opengis.net/spec/geosparql/1.0/req/rdfs-entailment-extension/bgp-rdfs-ent) | [Q25-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r25-1.rq), [Q25-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r25-2.rq),  [Q25-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r25-3.rq) | Checking if the system supports selecting both materialized RDF triples, as well as  inferred RDF triples based on the RDFS Entailment Regime  | [R26](http://www.opengis.net/spec/geosparql/1.0/req/rdfs-entailment-extension/wkt-geometry-types) | [Q26-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r26-1.rq), [Q26-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r26-2.rq) | Checking if the system supports selecting both materialized RDF triples, as well as  inferred RDF triples based on the RDFS/OWL class hierarchy of geometry types from  Simple Features  | [R27](http://www.opengis.net/spec/geosparql/1.0/req/rdfs-entailment-extension/gml-geometry-types) | [Q27.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r27.rq) | Checking if the system supports selecting both materialized RDF triples, as well as  inferred RDF triples based on the RDFS/OWL class hierarchy of geometry types of  the GML schema   | <tr><th colspan=""3"">Query rewrite extension (QRW)</th></tr>  | [R28](http://www.opengis.net/spec/geosparql/1.0/req/query-rewrite-extension/sf-query-rewrite) | [Q28-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-1.rq), [Q28-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-2.rq),  [Q28-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-3.rq), [Q28-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-4.rq), <br /> [Q28-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-5.rq), [Q28-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-6.rq),  [Q28-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-7.rq), [Q28-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-8.rq) | Testing the properties [geor:sfEquals](http://www.opengis.net/def/rule/geosparql/sfEquals), [geor:sfDisjoint](http://www.opengis.net/def/rule/geosparql/sfDisjoint), [geor:sfIntersects](http://www.opengis.net/def/rule/geosparql/sfIntersects), [geor:sfTouches](http://www.opengis.net/def/rule/geosparql/sfTouches),  [geor:sfCrosses](http://www.opengis.net/def/rule/geosparql/sfCrosses), [geor:sfWithin](http://www.opengis.net/def/rule/geosparql/sfWithin), [geor:sfContains](http://www.opengis.net/def/rule/geosparql/sfContains) and [geor:sfOverlaps](http://www.opengis.net/def/rule/geosparql/sfOverlaps) using both  materialized RDF triples and inferred RDF triples   | [R29](http://www.opengis.net/spec/geosparql/1.0/req/query-rewrite-extension/eh-query-rewrite) | [Q29-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-1.rq), [Q29-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-2.rq),  [Q29-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-3.rq), [Q29-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-4.rq), <br /> [Q29-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-5.rq), [Q29-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-6.rq),  [Q29-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-7.rq), [Q29-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-8.rq) | Testing the properties [geor:ehEquals](http://www.opengis.net/def/rule/geosparql/ehEquals), [geor:ehDisjoint](http://www.opengis.net/def/rule/geosparql/ehDisjoint), [geor:ehMeet](http://www.opengis.net/def/rule/geosparql/ehMeet), [geor:ehOverlap](http://www.opengis.net/def/rule/geosparql/ehOverlap),  [geor:ehCovers](http://www.opengis.net/def/rule/geosparql/ehCovers), [geor:ehCoveredBy](http://www.opengis.net/def/rule/geosparql/ehCoveredBy), [geor:ehInside](http://www.opengis.net/def/rule/geosparql/ehInside) and [geor:ehContains](http://www.opengis.net/def/rule/geosparql/ehContains) using both  materialized RDF triples and inferred RDF triples   | [R30](http://www.opengis.net/spec/geosparql/1.0/req/query-rewrite-extension/rcc8-query-rewrite) | [Q30-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-1.rq), [Q30-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-2.rq),  [Q30-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-3.rq), [Q30-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-4.rq), <br /> [Q30-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-5.rq), [Q30-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-6.rq),  [Q30-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-7.rq), [Q30-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-8.rq) | Testing the properties [geor:rcc8eq](http://www.opengis.net/def/rule/geosparql/rcc8eq), [geor:rcc8dc](http://www.opengis.net/def/rule/geosparql/rcc8dc), [geor:rcc8ec](http://www.opengis.net/def/rule/geosparql/rcc8ec), [geor:rcc8po](http://www.opengis.net/def/rule/geosparql/rcc8po), [geor:rcc8tppi](http://www.opengis.net/def/rule/geosparql/rcc8tppi),  [geor:rcc8tpp](http://www.opengis.net/def/rule/geosparql/rcc8tpp), [geor:rcc8ntpp](http://www.opengis.net/def/rule/geosparql/rcc8ntpp) and [geor:rcc8ntppi](http://www.opengis.net/def/rule/geosparql/rcc8ntppi) using both materialized RDF triples  and inferred RDF triples       ## Acknowledgement    The benchmark has been developed as part of the [HOBBIT](https://project-hobbit.eu) and [SAGE](https://sage.cs.uni-paderborn.de/sage/) research projects. """
Semantic web;https://github.com/ruby-rdf/rdf-reasoner;"""# RDF::Reasoner    A partial RDFS/OWL/schema.org reasoner for [RDF.rb][].    [![Gem Version](https://badge.fury.io/rb/rdf-reasoner.png)](https://badge.fury.io/rb/rdf-reasoner)  [![Build Status](https://github.com/ruby-rdf/rdf-reasoner/workflows/CI/badge.svg?branch=develop)](https://github.com/ruby-rdf/rdf-reasoner/actions?query=workflow%3ACI)  [![Coverage Status](https://coveralls.io/repos/ruby-rdf/rdf-reasoner/badge.svg?branch=develop)](https://coveralls.io/github/ruby-rdf/rdf-reasoner?branch=develop)  [![Gitter chat](https://badges.gitter.im/ruby-rdf/rdf.png)](https://gitter.im/ruby-rdf/rdf)    ## Description    Reasons over RDFS/OWL vocabularies and schema.org to generate statements which are entailed based on base RDFS/OWL rules along with vocabulary information. It can also be used to ask specific questions, such as if a given object is consistent with the vocabulary ruleset. This can be used to implement [SPARQL Entailment][] Regimes and [RDF Schema][] entailment.    ## Features    * Entail `rdfs:subClassOf` generating an array of terms which are ancestors of the subject.  * Entail `rdfs:subPropertyOf` generating an array of terms which are ancestors of the subject.  * Entail `rdfs:domain` and `rdfs:range` adding `rdf:type` assertions on the subject or object.  * Inverse `rdfs:subClassOf` entailment, to find descendant classes of the subject term.  * Inverse `rdfs:subPropertyOf` entailment, to find descendant properties of the subject term.  * Entail `owl:equivalentClass` generating an array of terms equivalent to the subject.  * Entail `owl:equivalentProperty` generating an array of terms equivalent to the subject.  * `domainCompatible?` determines if a particular resource is compatible with the domain definition of a given predicate, based on the intersection of entailed subclasses with the property domain.  * `rangeCompatible?` determines if a particular resource is compatible with the range definition of a given predicate, based on the intersection of entailed subclasses or literal types with the property domain.  * adds `entail` and `lint` commands to the `rdf` command line interface    Domain and Range entailment include specific rules for schema.org vocabularies.    * A plain literal is an acceptable value for any property.  * If `resource` is of type `schema:Role`, `resource` is domain acceptable if any other resource references `resource` using the same property.  * If `resource` is of type `schema:Role`, it is range acceptable if it has the same property with an acceptable value.  * If `resource` is of type `rdf:List` (must be previously entailed), it is range acceptable if all members of the list are otherwise range acceptable on the same property.    ### Limiting vocabularies used for reasoning    As loading vocabularies can dominate processing time, the `RDF::Vocabulary.limit_vocabs` method can be used to set a specific set of vocabularies over which to reason. For example:        RDF::Vocabulary.limit_vocabs(:rdf, :rdf, :schema)    will limit the vocabularies which are returned from `RDF::Vocabulary.each`, which is used for reasoning and other operations over vocabularies and terms.        ## Examples  ### Determine super-classes of a class        require 'rdf/reasoner'        RDF::Reasoner.apply(:rdfs)      term = RDF::Vocabulary.find_term(""http://xmlns.com/foaf/0.1/Person"")      term.entail(:subClassOf)        # => [          foaf:Agent,          http://www.w3.org/2000/10/swap/pim/contact#Person,          geo:SpatialThing,          foaf:Person        ]    ### Determine sub-classes of a class        require 'rdf/reasoner'        RDF::Reasoner.apply(:rdfs)      term = RDF::Vocab::FOAF.Person      term.entail(:subClass) # => [foaf:Person, mo:SoloMusicArtist]    ### Determine if a resource is compatible with the domains of a property        require 'rdf/reasoner'      require 'rdf/turtle'        RDF::Reasoner.apply(:rdfs)      graph = RDF::Graph.load(""etc/doap.ttl"")      subj = RDF::URI(""https://rubygems.org/gems/rdf-reasoner"")      RDF::Vocab::DOAP.name.domain_compatible?(subj, graph) # => true    ### Determine if a resource is compatible with the ranges of a property        require 'rdf/reasoner'      require 'rdf/turtle'        RDF::Reasoner.apply(:rdfs)      graph = RDF::Graph.load(""etc/doap.ttl"")      obj = RDF::Literal(Date.new)      RDF::Vocab::DOAP.created.range_compatible?(obj, graph) # => true    ### Perform equivalentClass entailment on a graph        require 'rdf/reasoner'      require 'rdf/turtle'        RDF::Reasoner.apply(:owl)      graph = RDF::Graph.load(""etc/doap.ttl"")      graph.entail!(:equivalentClass)    ### Yield all entailed statements for all entailment methods        require 'rdf/reasoner'      require 'rdf/turtle'        RDF::Reasoner.apply(:rdfs, :owl)      graph = RDF::Graph.load(""etc/doap.ttl"")      graph.enum_statement.entail.count # >= graph.enum_statement.count    ### Lint an expanded graph        require 'rdf/reasoner'      require 'rdf/turtle'        RDF::Reasoner.apply(:rdfs, :owl)      graph = RDF::Graph.load(""etc/doap.ttl"")      graph.entail!      messages = graph.lint      messages.each do |kind, term_messages|        term_messages.each do |term, messages|          options[:output].puts ""#{kind}  #{term}""          messages.each {|m| options[:output].puts ""  #{m}""}        end      end    ## Command-Line interface  The `rdf` command-line interface is extended with `entail` and `lint` commands. `Entail` can be used in combination, with `serialize` to generate an output graph representation including entailed triples.    ## Dependencies    * [Ruby](https://ruby-lang.org/) (>= 2.6)  * [RDF.rb](https://rubygems.org/gems/rdf) (~> 3.2)    ## Mailing List    * <https://lists.w3.org/Archives/Public/public-rdf-ruby/>    ## Authors    * [Gregg Kellogg](https://github.com/gkellogg) - <https://greggkellogg.net/>    ## Contributing    * Do your best to adhere to the existing coding conventions and idioms.  * Don't use hard tabs, and don't leave trailing whitespace on any line.    Before committing, run `git diff --check` to make sure of this.  * Do document every method you add using [YARD][] annotations. Read the    [tutorial][YARD-GS] or just look at the existing code for examples.  * Don't touch the `.gemspec`, `VERSION` or `AUTHORS` files. If you need to    change them, do so on your private branch only.  * Do feel free to add yourself to the `CREDITS` file and the corresponding    list in the the `README`. Alphabetical order applies.  * Do note that in order for us to merge any non-trivial changes (as a rule    of thumb, additions larger than about 15 lines of code), we need an    explicit [public domain dedication][PDD] on record from you,    which you will be asked to agree to on the first commit to a repo within the organization.    Note that the agreement applies to all repos in the [Ruby RDF](https://github.com/ruby-rdf/) organization.    ## License    This is free and unencumbered public domain software. For more information,  see <https://unlicense.org/> or the accompanying {file:UNLICENSE} file.    [Ruby]:             https://ruby-lang.org/  [RDF]:              https://www.w3.org/RDF/  [YARD]:             https://yardoc.org/  [YARD-GS]:          https://rubydoc.info/docs/yard/file/docs/GettingStarted.md  [PDD]:              https://unlicense.org/#unlicensing-contributions  [SPARQL]:           https://en.wikipedia.org/wiki/SPARQL  [SPARQL Query]:     https://www.w3.org/TR/2013/REC-sparql11-query-20130321/  [SPARQL Entailment]:https://www.w3.org/TR/sparql11-entailment/  [RDF 1.1]:          https://www.w3.org/TR/rdf11-concepts  [RDF.rb]:           https://www.rubydoc.info/github/ruby-rdf/rdf/  [RDF Schema]:       https://www.w3.org/TR/rdf-schema/  [Rack]:             https://rack.github.io/ """
Semantic web;https://github.com/protegeproject/sparql-dl-api;"""# sparq-dl-api  A query engine for SPARQL-DL.  Originally written by Derivo Systems        http://www.derivo.de/en/resources/sparql-dl-api.html """
Semantic web;https://github.com/EBIBioSamples/java2rdf;"""# java2rdf    A simple library to map Java objects and Java beans onto RDF/OWL. Contrary to other similar tools, java2rdf is based on declaring mappings between JavaBeans and RDF/OWL entities in dedicated mapping Java classes, so not in configuration files (you don't have to learn yet another XML schema), not via Java annotations (you don't always have access to, or want to spoil the source model).      We have a [5-min presentation about java2rdf](http://www.slideshare.net/mbrandizi/java2rdf). That shows code exerpts from [this example](https://github.com/EBIBioSamples/java2rdf/tree/master/src/test/java/uk/ac/ebi/fg/java2rdf/mapping/foaf_example), we also have  [another example](https://github.com/EBIBioSamples/java2rdf/blob/master/src/test/java/uk/ac/ebi/fg/java2rdf/mapping/MappersTest.java), showing a slightly different, 'quick-n-dirty' way to define object mappings (we recommend the former approach in real applications).      [Here](http://www.marcobrandizi.info/mysite/node/153) you can read something more about java2rdf and the BioSD Linked Data Project, for which it was built.      [Ondex](https://github.com/Rothamsted/ondex-knet-builder) is another example where java2rdf is used to [export RDF from Ondex knowledge network format](https://github.com/Rothamsted/ondex-knet-builder/tree/master/modules/rdf-export-2).     Note that version 2 has been re-implemented based on Commons-RDF, so that now you can choose to configure java2rdf   to use Jena or RDF4j as underlining RDF framework (we don't support OWLAPI anymore).   """
Semantic web;https://github.com/mdesalvo/RDFSharp;"""# RDFSharp [![NuGet Badge](https://buildstats.info/nuget/RDFSharp)](https://www.nuget.org/packages/RDFSharp) [![codecov](https://codecov.io/gh/mdesalvo/RDFSharp/branch/master/graph/badge.svg?token=wtP1B77d3e)](https://codecov.io/gh/mdesalvo/RDFSharp)    RDFSharp has a modular API made up of 4 layers:     <b><a href=""https://github.com/mdesalvo/RDFSharp/releases/download/v2.26.0/RDFSharp.Model-2.26.0.pdf"">RDFSharp.Model</a></b>  <ul>      <li>Create and manage <b>RDF models</b> (resources, literals, triples, graphs, namespaces, ...)</li>      <li>Exchange them using standard <b>RDF formats</b> (N-Triples, TriX, Turtle, RDF/Xml)</li>      <li>Create and validate <b>SHACL shapes</b> (shape graphs, shapes, targets, constraints, reports, ...)</b></li>  </ul>    <b><a href=""https://github.com/mdesalvo/RDFSharp/releases/download/v2.26.0/RDFSharp.Store-2.26.0.pdf"">RDFSharp.Store</a></b>  <ul>      <li>Create and manage <b>RDF stores</b> for context-aware modeling of RDF data (contexts, quadruples, ...)</li>      <li>Exchange them using standard <b>RDF formats</b> (N-Quads, TriX)</li>  </ul>    <b><a href=""https://github.com/mdesalvo/RDFSharp/releases/download/v2.26.0/RDFSharp.Query-2.26.0.pdf"">RDFSharp.Query</a></b>  <ul>      <li>Create and execute <b>SPARQL queries</b> on graphs, stores, federations and <i>SPARQL endpoints</i></li>      <li>Create and execute <b>SPARQL operations</b> on graphs, stores and <i>SPARQL UPDATE endpoints</i></li>  </ul>    <b><a href=""https://github.com/mdesalvo/RDFSharp/releases/download/v2.26.0/RDFSharp.Semantics-2.26.0.pdf"">RDFSharp.Semantics</a></b>  <ul>      <li>Create and manage <b>OWL-DL ontologies</b> (classes, restrictions, properties, facts, assertions, annotations, ...)</li>      <li>Validate them against a wide set of intelligent semantic rules analyzing <b>T-BOX</b> and <b>A-BOX</b></li>      <li>Create and execute <b>SWRL reasoners</b> with forward-chaining materialization of ontology inferences</li>      <li>Create and manage <b>SKOS schemes</b> (concepts, collections, relations, annotations, labels, ...)</li>  </ul> """
Semantic web;https://github.com/angelo-v/groovyrdf;"""# groovyrdf [![CircleCI](https://circleci.com/gh/angelo-v/groovyrdf/tree/master.svg?style=svg)](https://circleci.com/gh/angelo-v/groovyrdf/tree/master) [![Known Vulnerabilities](https://snyk.io/test/github/angelo-v/groovyrdf/badge.svg)](https://snyk.io/test/github/angelo-v/groovyrdf)    Library for building and processing RDF data with Groovy. groovyrdf helps you to build and consume RDF and Linked Data ""the groovy way"".    Version 0.2.7    ## Install     ### Maven    ```xml  <dependency>    <groupId>de.datenwissen.util</groupId>    <artifactId>groovyrdf</artifactId>    <version>0.2.7</version>    <type>pom</type>  </dependency>  ```    ### Gradle    ```groovy  compile 'de.datenwissen.util:groovyrdf:0.2.7'  ```    ### Ivy    ```xml  <dependency org='de.datenwissen.util' name='groovyrdf' rev='0.2.7'>    <artifact name='groovyrdf' ext='pom' ></artifact>  </dependency>  ```    ## Usage    Please take a look at the [user guide] on how to use groovyrdf.    [user guide]: http://angelo-v.github.com/groovyrdf/    ## Contact    Please contact me for any questions & feedback: [angelo.veltens@online.de](mailto:angelo.veltens@online.de)    ## Release Notes    Version 0.2.2 - 0.2.7    - security & dependency updates    Version 0.2.1    - adding WebIDs to resources    Version 0.2    - reading & processing RDF data from linked data resources    Version 0.1    - building RDF data    ## License    Copyright (c) 2017, Angelo Veltens    All rights reserved.    Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:    - Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.  - Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. """
Semantic web;https://github.com/mcollina/levelgraph-jsonld;"""LevelGraph-JSONLD  ===========    ![Logo](https://github.com/levelgraph/levelgraph/raw/master/logo.png)    [![Build Status](https://travis-ci.org/levelgraph/levelgraph-jsonld.png)](https://travis-ci.org/levelgraph/levelgraph-jsonld)  [![Coverage Status](https://coveralls.io/repos/levelgraph/levelgraph-jsonld/badge.png)](https://coveralls.io/r/levelgraph/levelgraph-jsonld)  [![Dependency Status](https://david-dm.org/levelgraph/levelgraph-jsonld.png?theme=shields.io)](https://david-dm.org/levelgraph/levelgraph-jsonld)  [![Sauce Labs Tests](https://saucelabs.com/browser-matrix/levelgraph-jsonld.svg)](https://saucelabs.com/u/levelgraph-jsonld)    __LevelGraph-JSONLD__ is a plugin for  [LevelGraph](http://github.com/levelgraph/levelgraph) that adds the  ability to store, retrieve and delete JSON-LD objects.  In fact, it is a full-blown Object-Document-Mapper (ODM) for  __LevelGraph__.    ## Install    ### Node.js    Adding support for JSON-LD to LevelGraph is easy:  ```shell  $ npm install level levelgraph levelgraph-jsonld --save  ```  Then in your code:  ```javascript  var level      = require('level'),      yourDB     = level('./yourdb'),      levelgraph = require('levelgraph'),      jsonld     = require('levelgraph-jsonld'),      db         = jsonld(levelgraph(yourDB));  ```    At the moment it requires node v0.10.x, but the port to node v0.8.x  should be straighforward.  If you need it, just open a pull request.    ## Browser    If you use [browserify](http://browserify.org/) you can use this package  in a browser just as in node.js. Please also take a look at [Browserify  section in LevelGraph package](https://github.com/levelgraph/levelgraph#browserify)    You can also use standalone browserified version from `./build`  directory or use [bower](http://bower.io)    ```shell  $ bower install levelgraph-jsonld --save  ```  It will also install its dependency levelgraph! Now you can simply:    ```html  <script src=""bower_components/levelgraph/build/levelgraph.js""></script>  <script src=""bower_components/levelgraph-jsonld/build/levelgraph-jsonld.js""></script>  <script>    var db = levelgraphJSONLD(levelgraph('yourdb'));  </script>  ```    ## Usage    We assume in following examples that you created database as explained  above!  ```js  var level  = require('level'),      yourDB = level('./yourdb'),      db     = levelgraphJSONLD(levelgraph(yourDB));  ```    `'base'` can also be specified when you create the db:  ```javascript  var level      = require('level'),      yourDB     = level('./yourdb'),      levelgraph = require('levelgraph'),      jsonld     = require('levelgraph-jsonld'),      opts       = { base: 'http://matteocollina.com/base' },      db         = jsonld(levelgraph(yourDB), opts);  ```    > From v1, overwriting and deleting is more conservative. If you rely on the previous behavior you can set the `overwrite` option to `true` (when creating the db or as options to `put` and `del`) to:  >  - overwrite all existing triples when using `put`  >  - delete all blank nodes recursively when using `del` (cf upcoming `cut` function)  > This old api will be phased out.    ### Put    Please keep in mind that LevelGraph-JSONLD __doesn't store the original  JSON-LD document but decomposes it into triples__! It stores literals  double quoted with datatype if other then string. If you use plain  LevelGraph methods, instead trying to match number `42` you need to try  matching `""42""^^http://www.w3.org/2001/XMLSchema#integer`     Storing triples from JSON-LD document is extremely easy:  ```javascript  var manu = {    ""@context"": {      ""name"": ""http://xmlns.com/foaf/0.1/name"",      ""homepage"": {        ""@id"": ""http://xmlns.com/foaf/0.1/homepage"",        ""@type"": ""@id""      }    },    ""@id"": ""http://manu.sporny.org#person"",    ""name"": ""Manu Sporny"",    ""homepage"": ""http://manu.sporny.org/""  };    db.jsonld.put(manu, function(err, obj) {    // do something after the obj is inserted  });  ```    if the top level objects have no `'@id'` key, one will be generated for  each, using a UUID and the `'base'` argument, like so:  ```javascript  delete manu['@id'];  db.jsonld.put(manu, { base: 'http://this/is/an/iri' }, function(err, obj) {    // obj['@id'] will be something like    // http://this/is/an/iri/b1e783b0-eda6-11e2-9540-d7575689f4bc  });  ```    `'base'` can also be [specified when you create the db](#usage).    __LevelGraph-JSONLD__ also support nested objects, like so:  ```javascript  var nested = {    ""@context"": {      ""name"": ""http://xmlns.com/foaf/0.1/name"",      ""knows"": ""http://xmlns.com/foaf/0.1/knows""    },    ""@id"": ""http://matteocollina.com"",    ""name"": ""Matteo"",    ""knows"": [{      ""name"": ""Daniele""    }, {      ""name"": ""Lucio""    }]  };    db.jsonld.put(nested, function(err, obj) {    // do something...  });  ```    ### Get    Retrieving a JSON-LD object from the store requires its `'@id'`:  ```javascript  db.jsonld.get(manu['@id'], { '@context': manu['@context'] }, function(err, obj) {    // obj will be the very same of the manu object  });  ```    The format of the loaded object is entirely specified by the  `'@context'`, so have fun :).    As with `'put'` it correctly support nested objects. If nested objects didn't originally include `'@id'` properties, now they will have them since `'put'` generates them by using UUID and formats  them as *blank node identifiers*:  ```javascript  var nested = {    ""@context"": {      ""name"": ""http://xmlns.com/foaf/0.1/name"",      ""knows"": ""http://xmlns.com/foaf/0.1/knows""    },    ""@id"": ""http://matteocollina.com"",    ""name"": ""Matteo"",    ""knows"": [{      ""name"": ""Daniele""    }, {      ""name"": ""Lucio""    }]  };    db.jsonld.put(nested, function(err, obj) {    // obj will be    // {    //   ""@context"": {    //     ""name"": ""http://xmlns.com/foaf/0.1/name"",    //     ""knows"": ""http://xmlns.com/foaf/0.1/knows""    //   },    //   ""@id"": ""http://matteocollina.com"",    //   ""name"": ""Matteo"",    //   ""knows"": [{    //     ""@id"": ""_:7053c150-5fea-11e3-a62e-adadc4e3df79"",    //     ""name"": ""Daniele""    //   }, {    //     ""@id"": ""_:9d2bb59d-3baf-42ff-ba5d-9f8eab34ada5"",    //     ""name"": ""Lucio""    //   }]    // }  });  ```    ### Delete    In order to delete an object, you need to pass the document to the `'del'` method which will delete only the properties specified in the document:  ```javascript  db.jsonld.del(manu, function(err) {    // do something after it is deleted!  });  ```    Note that blank nodes are ignored, so to delete blank nodes you need to pass the `cut: true` option (you can also add the `recurse: true`option) or use the `'cut'` method below.    > Note that since v1 `'del'` doesn't support passing an IRI anymore.    ### Cut    In order to delete the blank nodes object, you can just pass it's `'@id'` to the  `'cut'` method:  ```javascript  db.jsonld.cut(manu['@id'], function(err) {    // do something after it is cut!  });  ```    You can also pass an object, but in this case the properties are not used to determine which triples will be deleted and only the `@id`s are considered.    Using the `recurse` option you can follow all links and blank nodes (which might result in deleting more data than you expect)  ```javascript  db.jsonld.cut(manu['@id'], { recurse: true }, function(err) {    // do something after it is cut!  });  ```    ### Searching with LevelGraph    __LevelGraph-JSONLD__ does not support searching for objects, because  that problem is already solved by __LevelGraph__ itself. This example  search finds friends living near Paris:  ```javascript  var manu = {    ""@context"": {      ""@vocab"": ""http://xmlns.com/foaf/0.1/"",      ""homepage"": { ""@type"": ""@id"" },      ""knows"": { ""@type"": ""@id"" },      ""based_near"": { ""@type"": ""@id"" }    },    ""@id"": ""http://manu.sporny.org#person"",    ""name"": ""Manu Sporny"",    ""homepage"": ""http://manu.sporny.org/"",    ""knows"": [{      ""@id"": ""https://my-profile.eu/people/deiu/card#me"",      ""name"": ""Andrei Vlad Sambra"",      ""based_near"": ""http://dbpedia.org/resource/Paris""    }, {      ""@id"": ""http://melvincarvalho.com/#me"",      ""name"": ""Melvin Carvalho"",      ""based_near"": ""http://dbpedia.org/resource/Honolulu""    }, {      ""@id"": ""http://bblfish.net/people/henry/card#me"",      ""name"": ""Henry Story"",      ""based_near"": ""http://dbpedia.org/resource/Paris""    }, {      ""@id"": ""http://presbrey.mit.edu/foaf#presbrey"",      ""name"": ""Joe Presbrey"",      ""based_near"": ""http://dbpedia.org/resource/Cambridge""    }]  };    var paris = 'http://dbpedia.org/resource/Paris';    db.jsonld.put(manu, function(){    db.search([{      subject: manu['@id'],      predicate: 'http://xmlns.com/foaf/0.1/knows',      object: db.v('webid')    }, {      subject: db.v('webid'),      predicate: 'http://xmlns.com/foaf/0.1/based_near',      object: paris    }, {      subject: db.v('webid'),      predicate: 'http://xmlns.com/foaf/0.1/name',      object: db.v('name')    }    ], function(err, solution) {      // solution contains      // [{      //   webid: 'http://bblfish.net/people/henry/card#me',      //   name: '""Henry Story""'      // }, {      //   webid: 'https://my-profile.eu/people/deiu/card#me',      //   name: '""Andrei Vlad Sambra""'      // }]    });  });  ```  ## Changes    [CHANGELOG.md](https://github.com/levelgraph/levelgraph-jsonld/blob/master/CHANGELOG.md)  **including migration info for breaking changes**      ## Contributing to LevelGraph-JSONLD    * Check out the latest master to make sure the feature hasn't been    implemented or the bug hasn't been fixed yet  * Check out the issue tracker to make sure someone already hasn't    requested it and/or contributed it  * Fork the project  * Start a feature/bugfix branch  * Commit and push until you are happy with your contribution  * Make sure to add tests for it. This is important so I don't break it    in a future version unintentionally.  * Please try not to mess with the Makefile and package.json. If you    want to have your own version, or is otherwise necessary, that is    fine, but please isolate to its own commit so I can cherry-pick around    it.    ## LICENSE - ""MIT License""    Copyright (c) 2013-2017 Matteo Collina and LevelGraph-JSONLD contributors    Permission is hereby granted, free of charge, to any person  obtaining a copy of this software and associated documentation  files (the ""Software""), to deal in the Software without  restriction, including without limitation the rights to use,  copy, modify, merge, publish, distribute, sublicense, and/or sell  copies of the Software, and to permit persons to whom the  Software is furnished to do so, subject to the following  conditions:    The above copyright notice and this permission notice shall be  included in all copies or substantial portions of the Software.    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES  OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT  HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,  WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR  OTHER DEALINGS IN THE SOFTWARE. """
Semantic web;https://github.com/the-open-university/basil;"""# BASIL #    ![Maven build JDK 1.8](https://github.com/basilapi/basil/actions/workflows/main.yml/badge.svg)  ![MAven build Java 11](https://github.com/basilapi/basil/actions/workflows/mvn-Java11.yml/badge.svg)    BASIL is designed as middleware system that mediates between SPARQL endpoints and applications.    With BASIL you can build Web APIs on top of SPARQL endpoints.    BASIL stores SPARQL queries and builds APIs with standard and customizable formats.    ## Build ##  The basil project is managed and built with Maven.    ```  mvn clean install  ```  Note: to also run tests, you need an active internet connection (as they use public SPARQL endpoints).  If you want to skip tests, you can:    ```  mvn install -DskipTests  ```    ## Run ##  You need to:     - Have a MySQL server.   - Prepare a database running the [db.sql](db.sql) queries (at the root of the codebase).   - Prepare the configuration file (the connection parameters), see [this file](basil.ini) as an example.   - Prepare a log4j2 configuration file (if you want logging). See [this file](server/src/test/resources/log4j2.xml) as an example.     When ready, execute:    ```  $ java -jar -Dbasil.configurationFile=../basil.ini -Dlog4j.configurationFile=src/test/resources/log4j2.xml basil-server-0.3.0.jar -p 8080  #1: welcome to the world's helthiest food  #2: basil is starting on port 8080  #3: done  #4: enjoy  ```      ## Releasing ##  The following command will pack a release, sign the artefacts, and push them to maven central.  ```  mvn deploy -DperformRelease=true  ```"""
Semantic web;https://github.com/rdfostrich/ostrich;"""# OSTRICH  _Offset-enabled TRIple store for CHangesets_    [![Build Status](https://travis-ci.org/rdfostrich/ostrich.svg?branch=master)](https://travis-ci.org/rdfostrich/ostrich)  [![Docker Automated Build](https://img.shields.io/docker/automated/rdfostrich/ostrich.svg)](https://hub.docker.com/r/rdfostrich/ostrich/)  [![DOI](https://zenodo.org/badge/97819866.svg)](https://zenodo.org/badge/latestdoi/97819866)    **OSTRICH** is an _RDF triple store_ that allows _multiple versions_ of a dataset to be stored and queried at the same time.    The store is a hybrid between _snapshot_, _delta_ and _timestamp-based_ storage,  which provides a good trade-off between storage size and query time.  It provides several built-in algorithms to enable efficient iterator-based queries _at_ a certain version, _between_ any two versions, and _for_ versions. These queries support limits and offsets for any triple pattern.    Insertion is done by first inserting a dataset snapshot, which is encoded in [HDT](rdfhdt.org).  After that, deltas can be inserted, which contain additions and deletions based on the last delta or snapshot.    More details on OSTRICH can be found in our [**journal**](https://rdfostrich.github.io/article-jws2018-ostrich/) or [demo](https://rdfostrich.github.io/article-demo/) articles.    ## Building    OSTRICH requires ZLib, Kyoto Cabinet and CMake (compilation only) to be installed.    Compile:  ```bash  $ mkdir build  $ cd build  $ cmake ..  $ make  ```    ## Running    The OSTRICH dataset will always be loaded from the current directory.    ### Tests  ```bash  build/ostrich_test  ```    ### Query  ```bash  build/ostrich-query-version-materialized patch_id s p o  build/ostrich-query-delta-materialized patch_id_start patch_id_end s p o  build/ostrich-query-version patch_id_start s p o  ```    ### Insert  ```bash  build/ostrich-insert [-v] patch_id [+|- file_1.nt [file_2.nt [...]]]*  ```    Input deltas must be sorted in SPO-order.    ### Evaluate  Only load changesets from a path structured as `path_to_patch_directory/patch_id/main.nt.additions.txt` and `path_to_patch_directory/patch_id/main.nt.deletions.txt`.  ```bash  build/ostrich-evaluate path_to_patch_directory patch_id_start patch_id_end  ```  CSV-formatted insert data will be emitted: `version,added,durationms,rate,accsize`.    Load changesets AND query with triple patterns from the given file on separate lines, with the given number of replications.  ```bash  build/ostrich-evaluate path_to_patch_directory patch_id_start patch_id_end patch_to_queries/queries.txt s|p|o nr_replications  ```  CSV-formatted query data will be emitted (time in microseconds) for all versions for the three query types: `patch,offset,limit,count-ms,lookup-mus,results`.    ## Docker    Alternatively, OSTRICH can be built and run using Docker.    ### Build  ```bash  docker build -t ostrich .  ```    Instead of building the container yourself, you can use the pre-built image from [DockerHub](https://hub.docker.com/r/rdfostrich/ostrich/).  ```bash  docker pull rdfostrich/ostrich  ```    ### Test  ```bash  docker run --rm -it --entrypoint /opt/patchstore/build/ostrich_test ostrich  ```    ### Query  ```bash  docker run --rm -it --entrypoint /opt/ostrich/build/ostrich-query-version-materialized ostrich patch_id s p o  docker run --rm -it --entrypoint /opt/ostrich/build/ostrich-delta-version-materialized ostrich patch_id_start patch_id_end s p o  docker run --rm -it --entrypoint /opt/ostrich/build/ostrich-query-version ostrich s p o  ```    ### Insert  ```bash  docker run --rm -it --entrypoint /opt/ostrich/build/ostrich-insert ostrich [-v] patch_id [+|- file_1.nt [file_2.nt [...]]]*  ```    ### Evaluate    Only load changesets from a path structured as `path_to_patch_directory/patch_id/main.nt.additions.txt` and `path_to_patch_directory/patch_id/main.nt.deletions.txt`.  ```bash  docker run --rm -it -v path_to_patch_directory:/var/patches ostrich /var/patches patch_id_start patch_id_end  ```    Load changesets AND query with triple patterns from the given file on separate lines, with the given number of replications.  ```bash  docker run --rm -it -v path_to_patch_directory:/var/patches -v patch_to_queries:/var/queries ostrich /var/patches patch_id_start patch_id_end /var/queries/queries.txt s|p|o nr_replications  ```    Enable debug mode:  ```bash  docker run --rm -it -v path_to_patch_directory:/var/patches -v patch_to_queries:/var/queries -v path_to_crash_dir:/crash --privileged=true ostrich --debug /var/patches patch_id_start patch_id_end /var/queries/queries.txt s|p|o nr_replications  ```    ## Compiler variables  `PATCH_INSERT_BUFFER_SIZE`: The size of the triple parser buffer during patch insertion. (default `100`)    `FLUSH_POSITIONS_COUNT`: The amount of triples after which the patch positions should be flushed to disk, to avoid memory issues. (default `500000`)    `FLUSH_TRIPLES_COUNT`: The amount of triples after which the store should be flushed to disk, to avoid memory issues. (default `500000`)    `KC_MEMORY_MAP_SIZE`: The KC memory map size per tree. (default `1LL << 27` = 128MB)    `KC_PAGE_CACHE_SIZE`: The KC page cache size per tree. (default `1LL << 25` = 32MB)    `MIN_ADDITION_COUNT`: The minimum addition triple count so that it will be stored in the db. Changing this value only has effect during insertion time. Lookups are compatible with any value. (default `200`)    ## Cite    If you are using or extending OSTRICH as part of a scientific publication,  we would appreciate a citation of our [article](https://rdfostrich.github.io/article-jws2018-ostrich/).    ```bibtex  @article{taelman_jws_ostrich_2018,    author = {Taelman, Ruben and Vander Sande, Miel and Van Herwegen, Joachim and Mannens, Erik and Verborgh, Ruben},    title = {Triple Storage for Random-Access Versioned Querying of RDF Archives},    journal = {Journal of Web Semantics},    year = {2018},    month = aug,    url = {https://rdfostrich.github.io/article-jws2018-ostrich/}  }  ```    ## License  This software is written by [Ruben Taelman](http://rubensworks.net/) and colleagues.    This code is copyrighted by [Ghent University – imec](http://idlab.ugent.be/)  and released under the [MIT license](http://opensource.org/licenses/MIT). """
Semantic web;https://github.com/gushakov/sparql-template;"""## SPARQL Template    [![Build status](https://travis-ci.org/gushakov/sparql-template.svg?branch=master)](https://travis-ci.org/gushakov/sparql-template)    Small library for traversing an RDF store using automatic mapping of triples to annotated POJOs.    ## Highlights     * Support of any store exposing HTTP SPARQL endpoint   * Uses [Jena API](https://jena.apache.org/) to load and process RDF triples   * Uses [MappingContext](https://github.com/spring-projects/spring-data-commons/blob/master/src/main/java/org/springframework/data/mapping/context/MappingContext.java) from Spring Data Commons to process class annotations   * On-demand (lazy) loading of relations using automatic proxying with [ByteBuddy](http://bytebuddy.net/)   * Easily extended for conversion from any `org.apache.jena.graph.Node` to a custom Java type   * Some useful converters are registered by default, see `ch.unil.sparql.template.convert.ExtendedRdfJavaConverter`    + `java.util.Date`    + `java.time.ZonedDateTime`    + `java.time.Duration`    + `java.net.URL`     ## Examples    Assume we want to retrieve some information about a person from the [DBPedia](http://dbpedia.org) using the [SPARQL endpoint](http://dbpedia.org/sparql).  We annotate our domain POJO as following.    ```java  // marks this as an RDF entity  @Rdf  public class Person {        // will be mapped from the value of http://dbpedia.org/ontology/birthName      @Predicate(DBP_NS)      private String birthName;        // will be mapped from the value of http://www.w3.org/2000/01/rdf-schema#label for the Russian language      @Predicate(value = RDFS_NS, language = ""ru"")      private String label;        // will be mapped from the value of http://dbpedia.org/property/birthDate, automatic conversion to java.time.ZonedDateTime      @Predicate(DBP_NS)      private ZonedDateTime birthDate;        // will be mapped from the values of http://dbpedia.org/property/spouse, lazy load of relationships      @Predicate(DBP_NS)      @Relation      private Collection<Person> spouse;  }  ```    Then we can just use `ch.unil.sparql.template.SparqlTemplate` to load the triples from the DBPedia converting  them automatically to the required Java instance.    ```java      // get the default SPARQL template      final SparqlTemplate sparqlTemplate = new SparqlTemplate(""https://dbpedia.org/sparql"");        // load information about Angelina Jolie      final Person person = sparqlTemplate.load(DBR_NS + ""Angelina_Jolie"", Person.class);        System.out.println(person.getBirthName());      // Angelina Jolie Voight        System.out.println(person.getLabel());      // Джоли, Анджелина        System.out.println(person.getBirthDate().format(DateTimeFormatter.ofPattern(""dd/MM/yyyy (EEE)"", Locale.ENGLISH)));      // 04/06/1975 (Wed)        System.out.println(person.getSpouse().stream()              .filter(p -> p.getBirthName() != null && p.getBirthName().contains(""Pitt""))              .findAny().get().getBirthName());      // William Bradley Pitt    ``` """
Semantic web;https://github.com/arces-wot/SEPA;"""<div align=""center"">    <a href=""https://github.com/arces-wot/SEPA"">      <img width=""300px"" src=""./doc/logo.png"">    </a>    <br>    <br>    <a href=""https://travis-ci.org/arces-wot/SEPA"">      <img  src=""https://travis-ci.org/arces-wot/SEPA.svg?branch=master"">    </a>    <a href=""https://bintray.com/arces-wot/sepa-java-libs/client-api"">      <img  src=""https://img.shields.io/badge/client%20api-latest-cyan.svg"">    </a>    <a href=""https://github.com/arces-wot/SEPA/releases"">      <img  src=""https://img.shields.io/github/downloads/arces-wot/SEPA/total.svg?colorB=blue"">    </a>    <a href=""https://github.com/arces-wot/SEPA/tree/dev"">      <img  src=""https://img.shields.io/badge/unstable-dev-violet.svg"">    </a>    <a href=""https://gitter.im/sepa_dev/Lobby#"">      <img  src=""https://img.shields.io/badge/chat-on%20gitter-red.svg"">    </a>    <br>     <a href=""https://www.gnu.org/licenses/gpl-3.0"">      <img  src=""https://img.shields.io/badge/License-GPLv3-blue.svg"">    </a>    <a href=""hhttps://www.gnu.org/licenses/lgpl-3.0"">      <img  src=""https://img.shields.io/badge/License-LGPL%20v3-blue.svg"">    </a>      </div>    ## Table of Contents  - [Introduction](#introduction)  - [Demo](#demo)  - [Quick start](#quick-start)  - [Configuration](#configuration)  - [Usage](#usage)  - [Contributing](#contributing)  - [History](#history)  - [Credits](#credits)    ## Introduction  SEPA (**S**PARQL **E**vent **P**rocessing **A**rchitecture) is a publish-subscribe architecture designed to support information level interoperability. The architecture is built on top of generic SPARQL endpoints (conformant with [SPARQL 1.1 protocol](https://www.w3.org/TR/sparql11-protocol/)) where publishers and subscribers use standard **SPARQL 1.1** [Updates](https://www.w3.org/TR/sparql11-update/) and [Queries](https://www.w3.org/TR/sparql11-query/). Notifications about events (i.e., changes in the **RDF** knowledge base) are expressed in terms of added and removed SPARQL binding results since the previous notification. To know more about SEPA architecture and vision please refer to this [paper](https://www.mdpi.com/1999-5903/10/4/36/htm). SEPA proposal has been formalized in the following *unofficial dratfs*:  - [SPARQL Event Processing Architecture (SEPA)](http://mml.arces.unibo.it/TR/sepa.html) contribute [here](https://github.com/arces-wot/SEPADocs/blob/master/sepa.html)  - [SPARQL 1.1 Secure Event Protocol](http://mml.arces.unibo.it/TR/sparql11-se-protocol.html) contribute [here](https://github.com/arces-wot/SEPADocs/blob/master/sparql11-se-protocol.html)  - [SPARQL 1.1 Subscribe Language](http://mml.arces.unibo.it/TR/sparql11-subscribe.html) contribute [here](https://github.com/arces-wot/SEPADocs/blob/master/sparql11-subscribe.html)  - [JSON SPARQL Application Profile (JSAP)](http://mml.arces.unibo.it/TR/jsap.html) contribute [here](https://github.com/arces-wot/SEPADocs/blob/master/jsap.html)    ## Demo    ![Demo showing subscription and notifications](./doc/SEPADemo.gif)    ## Quick start    - Download the [SEPA Engine](https://github.com/arces-wot/SEPA/releases/latest) and run it: `java -jar engine-x.y.z.jar`    - Download [Blazegraph](https://sourceforge.net/projects/bigdata/files/latest/download) (or use any other SPARQL 1.1 Protocol compliant service) and run it as shown [here](https://wiki.blazegraph.com/wiki/index.php/Quick_Start)     - Use the [SEPA Playground](http://mml.arces.unibo.it/apps/dashboard?mode=local) to check basic functionalities of the engine.    ### For Hackers 💻👩‍💻👨‍💻  <a href=""https://asciinema.org/a/251211"">    <img width=""300px"" src=""https://asciinema.org/a/251211.svg"">  </a>    ## Configuration  The SEPA engine can be used with different SPARQL endpoints which must support SPARQL 1.1 protocol. The endpoint can be configured using  a JSON file `endpoint.jpar`. Furthermore, the engine has various parameters that can be used to configure the standard behavior; they  can be set using another JSON file called `engine.jpar`.    In the repository, you will find some versions of `endpoint-{something}.jpar` file. According to your underlying SPARQL endpoint, you have to rename the correct file to `endpoint.jpar`.  The default version of `endpoint.jpar` configures the engine to use use a local running instance of Blazegraph as [SPARQL 1.1 Protocol Service](https://www.w3.org/TR/sparql11-protocol/).    ```json  {  ""host"":""localhost"",  ""sparql11protocol"":{    ""protocol"":""http"",    ""port"":9999,    ""query"":{      ""path"":""/blazegraph/namespace/kb/sparql"",      ""method"":""POST"",      ""format"":""JSON""},    ""update"":{      ""path"":""/blazegraph/namespace/kb/sparql"",      ""method"":""POST"",      ""format"":""JSON""}}}  ```  The default version of  `engine.jpar` configures the engine to listen for incoming [SPARQL 1.1 SE Protocol](http://mml.arces.unibo.it/TR/sparql11-se-protocol/) requests at the following URLs:    1. Query: http://localhost:8000/query  2. Update: http://localhost:8000/update  3. Subscribe/Unsubscribe: ws://localhost:9000/subscribe  4. SECURE Query: https://localhost:8443/secure/query  5. SECURE Update: https://localhost:8443/secure/update  6. SECURE Subscribe/Unsubscribe: wss://localhost:9443/secure/subscribe   7. Regitration: https://localhost:8443/oauth/register  8. Token request: https://localhost:8443/oauth/token  ```json  {""parameters"":{    ""scheduler"":{     ""queueSize"":100,     ""timeout"":5000},    ""processor"":{     ""updateTimeout"":5000,     ""queryTimeout"":5000,     ""maxConcurrentRequests"":5,     ""reliableUpdate"":true},    ""spu"":{""timeout"":5000},    ""gates"":{     ""security"":{      ""tls"":false,      ""enabled"":false,      ""type"":""local""},     ""paths"":{      ""secure"":""/secure"",      ""update"":""/update"",      ""query"":""/query"",      ""subscribe"":""/subscribe"",      ""unsubscribe"":""/unsubscribe"",      ""register"":""/oauth/register"",      ""tokenRequest"":""/oauth/token""},     ""ports"":{      ""http"":8000,      ""https"":8443,      ""ws"":9000,      ""wss"":9443}}}}  ```  ### Logging  SEPA uses [log4j2](http://logging.apache.org/log4j/2.x/) by Apache. A default configuration is stored in the file log4j2.xml provided with the distribution. If the file resides in the engine folder, but it is not used, add the following JVM directive to force using it:    java `-Dlog4j.configurationFile=./log4j2.xml` -jar engine-x.y.z.jar    ### Security  By default, the engine implements a simple in-memory [OAuth 2.0 client-credential flow](https://auth0.com/docs/flows/client-credentials-flow). It uses a JKS for storing the keys and certificates for [SSL](http://docs.oracle.com/cd/E19509-01/820-3503/6nf1il6ek/index.html) and [JWT](https://tools.ietf.org/html/rfc7519) signing/verification. A default `sepa.jks` is provided including a single X.509 certificate (the password for both the store and the key is: `sepa2017`). If you face problems using the provided JKS, please delete the `sepa.jks` file and create a new one as follows: `keytool -genkey -keyalg RSA -alias sepakey -keystore sepa.jks -storepass sepa2017 -validity 360 -keysize 2048`  Run `java -jar engine-x.y.z.jar -help` for a list of options. The Java [Keytool](https://docs.oracle.com/javase/6/docs/technotes/tools/solaris/keytool.html) can be used to create, access and modify a JKS.   SEPA also implements other two security mechanisms:  - LDAP: it extends the default one by storing clients's information into an LDAP server (tested with [Apache Directory](https://directory.apache.org/))  - KEYCLOAK: authentication based on OpenID Connect in managed by [Keycloak](https://www.keycloak.org/)    Security is configured within the `engine.jpar` as follows:  ```json  {""gates"":{    ""security"":{      ""tls"": false,      ""enabled"": true,      ""type"": ""local""  }}}  ```  where   - `type` can assume one of the following values: `local`,`ldap`,`keycloak`  - `tls` is used when `type`=`ldap` to enable or not LDAP StartTLS    ### JMX monitoring  The SEPA engine is also distributed with a default [JMX](http://www.oracle.com/technetwork/articles/java/javamanagement-140525.html) configuration `jmx.properties` (including the `jmxremote.password` and `jmxremote.access` files for password and user grants). Remember to change password file permissions using: `chmod 600 jmxremote.password`. To enable remote JMX, the engine must be run as follows: `java -Dcom.sun.management.config.file=jmx.properties -jar engine-x.y.z.jar`. Using [`jconsole`](http://docs.oracle.com/javase/7/docs/technotes/guides/management/jconsole.html) is possible to monitor and control the most important engine parameters. By default, the port is `5555` and the `root:root` credentials grant full control (read/write).    ### Usage  The SEPA engine can be configured from the command line. Run `java -jar engine-x.y.z.jar -help` for the list of available settings.    `java [JMX] [JVM] [LOG4J] -jar SEPAEngine_X.Y.Z.jar [-help] [-secure=true] [-engine=engine.jpar] [-endpoint=endpoint.jpar] [JKS OPTIONS] [LDAP OPTIONS] [ISQL OPTIONS]`    - `secure` : overwrite the current secure option of engine.jpar  - `engine` : can be used to specify the JSON configuration parameters for the engine (default: engine.jpar)  - `endpoint` : can be used to specify the JSON configuration parameters for the endpoint (default: endpoint.jpar)  - `help` : to print this help    [JMX]  - `Dcom.sun.management.config.file=jmx.properties` : to enable JMX remote managment    [JVM]  - `XX:+UseG1GC`    [LOG4J]  - `Dlog4j.configurationFile=path/to/log4j2.xml`    [JKS OPTIONS]  - `sslstore` <jks> : JKS for SSL CA      			(default: ssl.jks)  - `sslpass` <pwd> : password of the JKS        	(default: sepastore)  - `jwtstore` <jks> : JKS for the JWT key       	(default: jwt.jks)  - `jwtalias` <alias> : alias for the JWT key   	(default: jwt)  - `jwtstorepass` <pwd> : password for the JKS  	(default: sepakey)  - `jwtaliaspass` <pwd> : password for the JWT key  (default: sepakey)  		  [LDAP OPTIONS]  - `ldaphost` <name> : host     		         (default: localhost)  - `ldapport` <port> : port                      (default: 10389)  - `ldapdn` <dn> : domain                        (default: dc=sepatest,dc=com)  - `ldapusersdn` <dn> : domain                   (default: null)  - `ldapuser` <usr> : username                   (default: null)  - `ldappwd` <pwd> : password                    (default: null)  		  [ISQL OPTIONS]  - `isqlpath` <path> : location of isql     		 (default: /usr/local/virtuoso-opensource/bin/)  - `isqlhost` <host> : host of Virtuoso     		 (default: localhost)  - `isqluser` <user> : user of Virtuoso     		 (default: dba)  - `isqlpass` <pass> : password of Virtuoso     	 (default: dba)    ## Contributing  You are very welcome to be part of SEPA community. If you find any bug feel free to open an issue here on GitHub, but also feel free to  ask any question. For more details check [Contributing guidelines](CONTRIBUTING.md). Besides, if you want to help the SEPA development follow this simple steps:    1. Fork it!  2. Create your feature branch: `git checkout -b my-new-feature`  3. Check some IDE specific instruction below  4. Do your stuff  5. Provide tests for your features if applicable  5. Commit your changes: `git commit -am 'Add some feature'`  6. Push to the branch: `git push origin my-new-feature`  7. Submit a pull request :D    Pull request with unit tests have an higher likelihood to be accepted, but we are not to restrictive. So do not be afraid to send your contribution!    ### Clone in Eclipse  There is no particular restriction in your IDE choice. Here we provide a short guide to import the GitHub cloned project inside Eclipse. Any   other IDEs work fine.     1. Open Eclipse  2. File > Import > Maven  3. Choose ""Check out Maven Projects from SCM""  4. In the field SCM URL choose 'git' and add the clone address from Github. If 'git' is not found, tap into ""Find more SCM connectors in the m2e Marketplace""  5. go on...  The project is cloned. Enjoy!    ### Build with Maven  SEPA engine is a Maven project composed by two sub-projects:  - Client-api  - Engine    As first, you need to build client-api skipping JUnit tests:  ```bash  mvn install -DskipTests  ```  In fact, clien-api JUnit tests include integration tests that require a SEPA engine running    Then you can build the engine with this command:  ```bash  mvn install  ```  That create an executable inside the target directory. To know more about Maven please refer to the [official documentation](https://maven.apache.org/).    ## History    SEPA has been inspired and influenced by [Smart-M3](https://sourceforge.net/projects/smart-m3/). SEPA authors have been involved in the development of Smart-M3 since its [origin](https://artemis-ia.eu/project/4-sofia.html).     The main differences beetween SEPA and Smart-M3 are the protocol (now compliant with the [SPARQL 1.1 Protocol](https://www.w3.org/TR/sparql11-protocol/)) and the introduction of a security layer (based on TLS and JSON Web Token for client authentication).     All the SEPA software components have been implemented from scratch.    ## Credits    SEPA stands for *SPARQL Event Processing Architecture*. SEPA is promoted and maintained by the [**Dynamic linked data and Web of Things Research Group**](https://site.unibo.it/wot/en) @ [**ARCES**](http://www.arces.unibo.it), the *Advanced Research Center on Electronic Systems ""Ercole De Castro""* of the [**University of Bologna**](http://www.unibo.it).    ## License    SEPA Engine is released under the [GNU GPL](https://github.com/arces-wot/SEPA/blob/master/engine/LICENSE), SEPA APIs are released under the  [GNU LGPL](https://github.com/arces-wot/SEPA/blob/master/client-api/LICENSE)"""
Semantic web;https://github.com/levelgraph/levelgraph-n3;"""LevelGraph-N3  ===========    ![Logo](https://github.com/levelgraph/node-levelgraph/raw/master/logo.png)    [![Build Status](https://travis-ci.org/levelgraph/levelgraph-n3.png)](https://travis-ci.org/levelgraph/levelgraph-n3)  [![Coverage Status](https://coveralls.io/repos/levelgraph/levelgraph-n3/badge.png)](https://coveralls.io/r/levelgraph/levelgraph-n3)  [![Dependency Status](https://david-dm.org/levelgraph/levelgraph-n3.png?theme=shields.io)](https://david-dm.org/levelgraph/levelgraph-n3)  [![Sauce Labs  Tests](https://saucelabs.com/browser-matrix/levelgraph-n3.svg)](https://saucelabs.com/u/levelgraph-n3)    __LevelGraph-N3__ is a plugin for  [LevelGraph](http://github.com/levelgraph/levelgraph) that adds the  ability to store, fetch and process N3 and turtle files.    ## Install    ### Node.js    Adding support for N3 to LevelGraph is easy:  ```shell  $ npm install level levelgraph levelgraph-n3 --save  ```  Then in your code:  ```js  var level = require('level'),      levelgraph = require('levelgraph'),      levelgraphN3 = require('levelgraph-n3'),      db = levelgraphN3(levelgraph(level('yourdb')));  ```        ### Browser    If you use [browserify](http://browserify.org/) you can use this package  in a browser just as in node.js. Please also take a look at [Browserify  section in LevelGraph package](https://github.com/levelgraph/levelgraph#browserify)      ## Usage    We assume in following examples that you created database as explained  above!  ```js  var db = levelgraphN3(levelgraph(level(""yourdb"")));  ```    ### Importing n3 files    In code:    ```js  var fs = require(""fs"");    var stream = fs.createReadStream(""./triples.n3"")                 .pipe(db.n3.putStream());    stream.on(""finish"", function() {    console.log(""Import completed"");  });  ```    Alternatively, you can run the import CLI tool by running `npm install`, then:    ```  ./import.js path/to/n3/file(s)  ```    with the following optional flags:    `-o` or `--output` followed by the desired DB path. If not specified, path will be at `./db`.    `-q` or `--quiet` will silence status updates during the import process. Otherwise, progress information is displayed.    File extensions must be `.n3` or `.nt`. Additionally, there is glob support, so for example `*.nt` will import all the matching n-triple files.      ### Get and Put    Storing an N3 file in the database is extremey easy:  ```js  var turtle = ""@prefix c: <http://example.org/cartoons#>.\n"" +               ""c:Tom a c:Cat.\n"" +               ""c:Jerry a c:Mouse;\n"" +               ""        c:smarterThan c:Tom;\n"" +               ""        c:place \""fantasy\""."";    db.n3.put(turtle, function(err) {    // do something after the triple is inserted  });  ```    Retrieving it through pattern-matching is extremely easy:  ```js  db.n3.get({ subject: ""http://example.org/cartoons#Tom"" }, function(err, turtle) {    // turtle is ""<http://example.org/cartoons#Tom> a <http://example.org/cartoons#Cat> .\n"";  });  ```    It even support a Stream interface:  ```js  var stream = db.n3.getStream({ subject: ""http://example.org/cartoons#Tom"" });  stream.on(""data"", function(data) {    // data is ""<http://example.org/cartoons#Tom> a <http://example.org/cartoons#Cat> .\n"";  });  stream.on(""end"", done);  ```    ### Exporting NTriples from LevelGraph    __LevelGraph-N3__ allows to export ntriples from a __LevelGraph__ database.  __LevelGraph-N3__ augments the a standard `search` method with a `{ n3: ... }` option  that specifies the subject, predicate and object of the created triples.  It follows the same structure of the `{ materialized: ... }` option (see https://github.com/levelgraph/levelgraph#searches).    Here is an example:  ```js  db.search([{    subject: db.v(""s""),    predicate: ""http://example.org/cartoons#smarterThan"",    object: db.v(""o"")  }], {    n3: {      subject: db.v(""o""),      predicate: ""http://example.org/cartoons#dumberThan"",      object: db.v(""s"")    }  }, function(err, turtle) {    // turtle is ""<http://example.org/cartoons#Tom> <http://example.org/cartoons#dumberThan> <http://example.org/cartoons#Jerry> .\n""  });  ```    It also supported by the `searchStream` method.    ## Changes    [CHANGELOG.md](https://github.com/levelgraph/levelgraph-n3/blob/master/CHANGELOG.md)  **including migration info for breaking changes**      ## Contributing to LevelGraph-N3    * Check out the latest master to make sure the feature hasn't been    implemented or the bug hasn't been fixed yet  * Check out the issue tracker to make sure someone already hasn't    requested it and/or contributed it  * Fork the project  * Start a feature/bugfix branch  * Commit and push until you are happy with your contribution  * Make sure to add tests for it. This is important so I don't break it    in a future version unintentionally.  * Please try not to mess with the Makefile and package.json. If you    want to have your own version, or is otherwise necessary, that is    fine, but please isolate to its own commit so I can cherry-pick around    it.    ## LICENSE - ""MIT License""    Copyright (c) 2013-2015 Matteo Collina (http://matteocollina.com)    Permission is hereby granted, free of charge, to any person  obtaining a copy of this software and associated documentation  files (the ""Software""), to deal in the Software without  restriction, including without limitation the rights to use,  copy, modify, merge, publish, distribute, sublicense, and/or sell  copies of the Software, and to permit persons to whom the  Software is furnished to do so, subject to the following  conditions:    The above copyright notice and this permission notice shall be  included in all copies or substantial portions of the Software.    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES  OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT  HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,  WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR  OTHER DEALINGS IN THE SOFTWARE. """
Semantic web;https://github.com/SPARQL-Anything/sparql.anything;"""# sparql.everything"""
Semantic web;https://github.com/cosminbasca/cysparql;"""CySparql  ========    CySparql is a python wrapper over the excellent heavy-duty C [Rasqal RDF](http://librdf.org/rasqal/) v0.9.33+ [SPARQL](http://www.w3.org/TR/rdf-sparql-query/) parser. The library is intended to give a pythonic feel to parsing SPARQL queries. There are several goodies included as well like:  * simple and fast star-pattern extraction from SPARQL queries  * node-edge (graph) visualizations of SPARQL queries  * simple descriptive command line utility to describe a given SPARQL query (from a file or read from stdin)  * auto pretty formatter for SPARQL queries (slower, than just parsing)    Important Notes  ---------------  This software is the product of research carried out at the [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and comes with no warranty whatsoever. Have fun!    TODO's  ------  * The *librasqal* is not fully supported (e.g. filters, etc)  * The project is not documented (yet)    How to Compile the Project  --------------------------  Ensure that *librasqal* v0.9.33+ and *libraptor2* v2.0.13+ are installed on your system (either using the package manager of the OS or compiled from source).    To install **CySparql** you have two options: 1) manual installation (install requirements first) or 2) automatic with **pip**    **Manual** installation:  ```sh  $ git clone https://github.com/cosminbasca/cysparql  $ cd cysparql  $ python setup.py install  ```    Install the project with **pip**:  ```sh  $ pip install https://github.com/cosminbasca/cysparql  ```    Also have a look at the build.sh, clean.sh, test.sh scripts included in the codebase     Basic Example  -------------  ```python  from cysparql import *  q_string = """"""  PREFIX example: <http://www.example.org/rdf#>  SELECT * WHERE {      ?a example:p ?b1.      ?a example:p ?b2.      ?a example:p ?b3.      ?a example:p ?b4.      ?a example:p ?b5.      ?a example:p ?b6.      ?a example:q ?b6.      ?b5 example:p ?x .      ?b6 example:p ?y .  }  """"""    query = Query(q_string, pretty=True)  # should print:  #[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.]  # [ 0.  0.  0.  0.  0.  1.  0.  0.  0.]  # [ 0.  0.  0.  1.  1.  1.  1.  1.  1.]  # [ 0.  0.  1.  0.  0.  0.  0.  0.  0.]  # [ 1.  0.  1.  0.  0.  0.  0.  0.  0.]  # [ 0.  1.  1.  0.  0.  0.  0.  0.  0.]  # [ 0.  0.  1.  0.  0.  0.  0.  0.  0.]  # [ 0.  0.  1.  0.  0.  0.  0.  0.  0.]  # [ 0.  0.  1.  0.  0.  0.  0.  0.  0.]]  print query.adjacency_matrix    # sould print: triple_patterns =  <cysparql.pattern.TriplePatternSequence object at 0x1049a4730>  print 'triple_patterns = ',query.triple_patterns    # should print:  #STAR (0):   # [< ?a, http://www.example.org/rdf#p, ?b1 ,None>, < ?a, http://www.example.org/rdf#p, ?b2 ,None>, < ?a, http://www.example.org/rdf#p, ?b3 ,None>, < ?a, http://www.example.org/rdf#p, ?b4 ,None>, < ?a, http://www.example.org/rdf#p, ?b5 ,None>, < ?a, http://www.example.org/rdf#p, ?b6 ,None>, < ?a, http://www.example.org/rdf#q, ?b6 ,None>]  #  #STAR (1):   # [< ?a, http://www.example.org/rdf#p, ?b5 ,None>, < ?b5, http://www.example.org/rdf#p, ?x ,None>]  #  #STAR (2):   # [< ?a, http://www.example.org/rdf#p, ?b6 ,None>, < ?a, http://www.example.org/rdf#q, ?b6 ,None>, < ?b6, http://www.example.org/rdf#p, ?y ,None>]  stars = get_stars(query.triple_patterns)  for i,s in enumerate(stars):      print '\nSTAR (%s): \n %s'%(i,s)    # if asciinet is installed  # should print:  #ASCII:   #          ┌─────────────┐             #          │     ?a      │             #          └┬┬──┬─────┬┬┬┘             #           ││  │     │││              #           ││  │     ││└───┐          #    ┌──────┼┘  │     ││    │          #    │      │   │     ││    │          #    v      v   │     ││    │          #  ┌───┐  ┌───┐ │     ││    │          #  │?b6│  │?b5│ │     ││    │          #  └┬──┘  └─┬─┘ │     ││    │          #   │       │   │     ││    │          #   │     ┌─┘   │     │└────┼─────┐    #   │     │     │     │     │     │    #   v     v     v     v     v     v    # ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐  # │?y │ │?x │ │?b3│ │?b2│ │?b1│ │?b4│  # └───┘ └───┘ └───┘ └───┘ └───┘ └───┘  print 'ASCII: \n',query.ascii    # print the librasqal debug information  # should print:  #query verb: SELECT  #data graphs: []  #named variables: [variable(a), variable(b1), variable(b2), variable(b3), variable(b4), variable(b5), variable(b6), variable(x), variable(y)]  #anonymous variables: []  #projected variable names: a, b1, b2, b3, b4, b5, b6, x, y  #  #bound variables: [variable(a), variable(b1), variable(b2), variable(b3), variable(b4), variable(b5), variable(b6), variable(x), variable(y)]  #triples: [triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b1)), triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b2)), triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b3)), triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b4)), triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b5)), triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b6)), triple(variable(a), uri<http://www.example.org/rdf#q>, variable(b6)), triple(variable(b5), uri<http://www.example.org/rdf#p>, variable(x)), triple(variable(b6), uri<http://www.example.org/rdf#p>, variable(y))]  #prefixes: [prefix(example as http://www.example.org/rdf#)]  #query graph pattern: graph pattern[0] Basic(over 9 triples[triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b1)) ,triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b2)) ,triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b3)) ,triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b4)) ,triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b5)) ,triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b6)) ,triple(variable(a), uri<http://www.example.org/rdf#q>, variable(b6)) ,triple(variable(b5), uri<http://www.example.org/rdf#p>, variable(x)) ,triple(variable(b6), uri<http://www.example.org/rdf#p>, variable(y))])  query.debug()  ```    Thanks a lot to  ---------------  * [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Swiss National Science Foundation](http://www.snf.ch/en/Pages/default.aspx) for generously funding the research that led to this software. """
Semantic web;https://github.com/RubenVerborgh/LDflex;"""# LDflex makes Linked Data in JavaScript fun  LDflex is a domain-specific language  for querying Linked Data on the Web  as if you were browsing a local JavaScript graph.    [![npm version](https://img.shields.io/npm/v/ldflex.svg)](https://www.npmjs.com/package/ldflex)  [![Build Status](https://travis-ci.com/LDflex/LDflex.svg?branch=master)](https://travis-ci.com/LDflex/LDflex)  [![Coverage Status](https://coveralls.io/repos/github/LDflex/LDflex/badge.svg?branch=master)](https://coveralls.io/github/LDflex/LDflex?branch=master)  [![Dependency Status](https://david-dm.org/LDflex/LDflex.svg)](https://david-dm.org/LDflex/LDflex)  [![DOI](https://zenodo.org/badge/148931900.svg)](https://zenodo.org/badge/latestdoi/148931900)    You can write things like `person.friends.firstName`  to get a list of your friends.  Thanks to the power of [JSON-LD contexts](https://www.w3.org/TR/json-ld/#the-context)  and [JavaScript's Proxy](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy),  these properties are not hard-coded in LDflex,  but can be chosen at runtime.  They feel as if you're traversing a local object,  while you're actually querying the Web—without  pulling in all data first.    [Tim Berners-Lee](https://www.w3.org/People/Berners-Lee/)  came up with the idea for such a fluid JavaScript interface to Linked Data,  in a discussion on how to make Linked Data easier for developers.    ## Articles and tutorials  - [Tutorial slides](https://comunica.github.io/Tutorial-ISWC2019-Slides-LDflex/)    and [walkthrough](https://github.com/comunica/Tutorial-ISWC2019-LDflex-on-React/wiki/Tutorial-Walkthrough)  - [Cheatsheet](https://vincenttunru.gitlab.io/tripledoc/docs/cheatsheet)  - [Designing a Linked Data developer experience](https://ruben.verborgh.org/blog/2018/12/28/designing-a-linked-data-developer-experience/),    discussing the design of LDflex  - [Solid Chess](https://pieterheyvaert.com/blog/2019/02/10/solid-world-summary),    an app built with LDflex    ## Installation  ```bash  npm install ldflex  ```    In order to execute queries,  you will also need a query engine:  ```bash  npm install @ldflex/comunica  ```    ## Usage  When you have obtained a starting subject,  you can navigate through its properties  using standard JavaScript dot property syntax.    In order to query for the result,  use `await` if you want a single value,  or `for await` to iterate over all values.    ### Initialization  ```javascript  const { PathFactory } = require('ldflex');  const { default: ComunicaEngine } = require('@ldflex/comunica');  const { namedNode } = require('@rdfjs/data-model');    // The JSON-LD context for resolving properties  const context = {    ""@context"": {      ""@vocab"": ""http://xmlns.com/foaf/0.1/"",      ""friends"": ""knows"",      ""label"": ""http://www.w3.org/2000/01/rdf-schema#label"",      ""rbn"": ""https://ruben.verborgh.org/profile/#""    }  };  // The query engine and its source  const queryEngine = new ComunicaEngine('https://ruben.verborgh.org/profile/');  // The object that can create new paths  const path = new PathFactory({ context, queryEngine });  ```    ### Looking up data on the Web  ```javascript  const ruben = path.create({ subject: namedNode('https://ruben.verborgh.org/profile/#me') });  showPerson(ruben);    async function showPerson(person) {    console.log(`This person is ${await person.name}`);      console.log(`${await person.givenName} is interested in:`);    for await (const name of person.interest.label)      console.log(`- ${name}`);      console.log(`${await person.givenName} is friends with:`);    for await (const name of person.friends.givenName)      console.log(`- ${name}`);  }  ```    ### Inspecting the generated path expression  ```javascript  (async person => {    console.log(await person.friends.givenName.pathExpression);  })(ruben);    ```    ### Getting all subjects of a document  ```javascript  (async document => {    for await (const subject of document.subjects)      console.log(`${subject}`);  })(ruben);  ```    ### Getting all properties of a subject  ```javascript  (async subject => {    for await (const property of subject.properties)      console.log(`${property}`);  })(ruben);    ```    ### Converting an LDflex expression into a SPARQL query  ```javascript  (async person => {    console.log(await person.friends.givenName.sparql);  })(ruben);    ```    ### Sorting path results  ```javascript  (async person => {    for await (const uri of person.interest.sort('label'))      console.log(`- ${uri}`);  })(ruben);    ```    The sort function takes multiple arguments,  creating a path that sorts on the last argument.  The path can also continue after the sort:  `person.friends.sort('country', 'label').givenName`  will sort the friends based on the label of their country,  and then return their names.    ### Modifying data    ```javascript  // Add a new value  await person['http://xmlns.com/foaf/0.1/name'].add(literal(name));  await person['http://xmlns.com/foaf/0.1/nick'].add(literal(nickname));    // Set a new value and override existing values  await person['http://xmlns.com/foaf/0.1/name'].set(literal(name));  await person['http://xmlns.com/foaf/0.1/nick'].set(literal(nickname));    // Delete object values  await person['http://xmlns.com/foaf/0.1/name'].delete();  await person['http://xmlns.com/foaf/0.1/nick'].delete();    // Replace object values  await person['http://xmlns.com/foaf/0.1/name'].replace(literal(oldName), literal(name));  ```    ### Accessing collections  Handle `rdf:List`, `rdf:Bag`, `rdf:Alt`, `rdf:Seq` and `rdf:Container`.    For `rdf:List`s  ```javascript  (async publication => {    // Returns an Array of Authors    const authors = await publication['bibo:authorList'].list();  })(ordonez_medellin_2014);  ```    For `rdf:Alt`, `rdf:Seq` and `rdf:Container`s  ```javascript  (async data => {    // Returns an Array of elements    const elements = await data['ex:myContainer'].container();  })(data);  ```    For `rdf:Bag`s  ```javascript  (async data => {    // Returns a Set of elements    const elements = await data['ex:myBag'].containerAsSet();  })(data);  ```    Alternatively, `.collection` can be used for *any* collection (i.e. `rdf:List`, `rdf:Bag`, `rdf:Alt`, `rdf:Seq` and `rdf:Container`) **provided the collection has the correct `rdf:type` annotation in the data source**    ```javascript  (async publication => {    // Returns an Array of Authors    const authors = await publication['bibo:authorList'].collection();  })(ordonez_medellin_2014);  ```    ### NamedNode URI utilities  ```js  ruben.namespace // 'https://ruben.verborgh.org/profile/#'  ruben.fragment // 'me'  await ruben.prefix // 'rbn'  ```    ## Additional Handlers    The following libraries provide handlers that extend the functionality of LDflex:   - [async-iteration-handlers](https://github.com/LDflex/async-iteration-handlers) Provides methods such as `.map`, `.filter` and `.reduce` for the async-iterable results returned by LDflex.    ## License  ©2018–present  [Ruben Verborgh](https://ruben.verborgh.org/),  [Ruben Taelman](https://www.rubensworks.net/).  [MIT License](https://github.com/LDflex/LDflex/blob/master/LICENSE.md). """
Semantic web;https://github.com/gtfierro/reasonable;"""# Reasonable    ![Nightly Build](https://github.com/gtfierro/reasonable/workflows/Nightly%20Build/badge.svg)  ![Build](https://github.com/gtfierro/reasonable/workflows/Build/badge.svg)  [![PyPI version](https://badge.fury.io/py/reasonable.svg)](https://badge.fury.io/py/reasonable)    An OWL 2 RL reasoner with reasonable performance    ## Performance    Comparing performance of `reasonable` with [OWLRL](https://github.com/RDFLib/OWL-RL) and [Allegro](https://franz.com/agraph/support/documentation/current/materializer.html). Evaluation consisted of loading Brick models of different sizes into the respective reasoning engine and timing how long it took to produce the materialization. `reasonable` is about 7x faster than Allegro and 38x faster than OWLRL on this workload.    ![benchmark](img/benchmark.png)    ## How to Use    ### Python    To facilitate usage, we use the [pyo3](https://pyo3.rs/) project to generate Python 3.x bindings to this project.  Installing these *should* be as easy as `pip install reasonable`.    See also the [`brickschema`](https://github.com/BrickSchema/py-brickschema) package for working with Brick models. The package provides a generic interface to this reasoner and several others.    Usage looks like:    ```python  import reasonable    # import triples from an rdflib Graph  import rdflib  g = rdflib.Graph()  g.parse(""example_models/ontologies/Brick.n3"", format=""n3"")  g.parse(""example_models/small1.n3"", format=""n3"")    r = reasonable.PyReasoner()  r.from_graph(g)  triples = r.reason()  print(""from rdflib:"", len(triples))    # import triples from files on disk  r = reasonable.PyReasoner()  r.load_file(""example_models/ontologies/Brick.n3"")  r.load_file(""example_models/small1.n3"")  triples = r.reason()  print(""from files:"", len(triples))  ```    ### Rust    See [Rust docs](https://docs.rs/reasonable)    Example of usage from Rust:    ```rust  use ::reasonable::owl::Reasoner;  use std::env;  use std::time::Instant;  use log::info;    fn main() {      env_logger::init();      let mut r = Reasoner::new();      env::args().skip(1).map(|filename| {          info!(""Loading file {}"", &filename);          r.load_file(&filename).unwrap()      }).count();      let reasoning_start = Instant::now();      info!(""Starting reasoning"");      r.reason();      info!(""Reasoning completed in {:.02}sec"", reasoning_start.elapsed().as_secs_f64());      r.dump_file(""output.ttl"").unwrap();  }  ```      ## OWL 2 Rules    Using rule definitions from [here](https://www.w3.org/TR/owl2-profiles/#Reasoning_in_OWL_2_RL_and_RDF_Graphs_using_Rules).    **TODO**: implement RDF/RDFS entailment semantics as described [here](https://www.w3.org/TR/rdf11-mt/)    **Note**: haven't implemented rules that produce exceptions; waiting to determine the best way of handling these errors.    ### Equality Semantics    |Completed| Rule name | Notes |  |---------|----------|-------|  | no     | `eq-ref` | implementation is very inefficient; causes lots of flux       |  | **yes**| `eq-sym` |       |  | **yes**| `eq-trans` |       |  | **yes**| `eq-rep-s` |       |  | **yes**| `eq-rep-p` |       |  | **yes**| `eq-rep-o` |       |  | no     | `eq-diff1` | throws exception |  | no     | `eq-diff2` | throws exception |  | no     | `eq-diff3` | throws exception |    ### Property Axiom Semantics    |Completed| Rule name | Notes |  |---------|----------|-------|  | no        | `prp-ap` |       |  | **yes**   | `prp-dom` |       |  | **yes**   | `prp-rng` |       |  | **yes**   | `prp-fp` |       |  | **yes**   | `prp-ifp` |       |  | **yes**   | `prp-irp` | throws exception |  | **yes**   | `prp-symp` |       |  | **yes**   | `prp-asyp` | throws exception |  | **yes**   | `prp-trp` |       |  | **yes**   | `prp-spo1` |       |  | no        | `prp-spo2` |       |  | **yes**   | `prp-eqp1` |       |  | **yes**   | `prp-eqp2` |       |  | **yes**   | `prp-pdw` | throws exception |  | no        | `prp-adp` | throws exception |  | **yes**   | `prp-inv1` |       |  | **yes**   | `prp-inv2` |       |  | no        | `prp-key` |       |  | no        | `prp-npa1` | throws exception |  | no        | `prp-npa2` | throws exception |    ### Class Semantics    |Completed| Rule name | Notes |  |---------|----------|-------|  | **yes**| `cls-thing` |       |  | **yes**| `cls-nothing1` |       |  | **yes**| `cls-nothing2` | throws exception       |  | **yes**| `cls-int1` |       |  | **yes**| `cls-int2` |       |  | **yes**| `cls-uni` |       |  | **yes**| `cls-com` | throws exception    |  | **yes**| `cls-svf1` |       |  | **yes**| `cls-svf2` |       |  | **yes**| `cls-avf` |       |  | **yes**| `cls-hv1` |       |  | **yes**| `cls-hv2` |       |  | no     | `cls-maxc1` | throws exception       |  | no     | `cls-maxc2` |       |  | no     | `cls-maxqc1` | throws exception       |  | no     | `cls-maxqc2` | throws exception      |  | no     | `cls-maxqc3` |       |  | no     | `cls-maxqc4` |       |  | no     | `cls-oo` |       |    ### Class Axiom Semantics    |Completed| Rule name | Notes |  |---------|----------|-------|  | **yes**| `cax-sco` |       |  | **yes**| `cax-eqc1` |       |  | **yes**| `cax-eqc2` |       |  | **yes**| `cax-dw` | throws exception      |  | no     | `cax-adc` |  throws exception     |    ### Other    - no datatype semantics for now    ## Development Notes    To publish new versions of `reasonable`, tag a commit with the version (e.g. `v1.3.2`) and push the tag to GitHub. This will execute the `publish` action which builds an uploads to PyPi. """
Semantic web;https://github.com/TheOntologist/OntoVerbal;"""OntoVerbal  ==========    OntoVerbal is a Protege 4.2 plugin that generates natural language descriptions for classes for an ontology written in OWL (roughly the OWL EL profile). OntoVerbal was written by Fennie Liang, in collaboraitn with Donia Scott, Alan Rector and Robert Stevens, as part of the EPSRC funded Semantic Web Authoring Tool Project (EP/G032459/1).  """
Semantic web;https://github.com/stardog-union/stardog-groovy;"""Stardog Groovy  ==========    Licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)    _Current Version **5.3.5**_     Stardog Groovy - Groovy language bindings to use to develop apps with the [Stardog Graph / RDF Database](http://stardog.com).      ![Stardog](http://stardog.com/img/stardog.png)       ## What is it? ##    This bindings provides a set of idiomatic Groovy APIs for interacting with the Stardog database, similar to the Stardog Spring project - an easy to use method for creating connection pools, and the ability run queries over them. To run the queries, Stardog Groovy uses standard Groovy patterns, such as passing in a closure to iterate over result sets.  Common use cases for Stardog-groovy are ETL scripts, command line applications, usage with Grails, or other Groovy frameworks.       ## How to use it    1. Download Stardog from stardog.com, and follow the installation instructions  2. Add the `com.complexible.stardog:stardog-groovy:<version>` dependency declaration to your build tool, such as Maven or Gradle  3. Make sure your build prioritizes your local Maven repository (i.e. `~/.m2/repository`), where the core Stardog binaries were installed by step 2  4. Enjoy!    There is also a `shadowJar` task available via the Shadow plugin to produce a fatjar with all of the Stardog dependencies.    ## Quickstart    ```  @Grab('com.complexible.stardog:stardog-groovy:5.3.5')  import com.complexible.stardog.ext.groovy.Stardog    def stardog = new Stardog(url: ""http://localhost:5820"", to:""testdb"", username: ""admin"", password:""admin"", reasoning: true)    stardog.query(""select ?s ?p ?o where { ?s ?p ?o } limit 2"", { println it })  ```    ## Examples ##    Create a new embedded database in one line  ```groovy  	def stardog = new Stardog(home:""/opt/stardog"", to:""testgroovy"", username:""admin"", password:""admin"")  ```    Collect query results via a closure  ```groovy  	def list = []  	stardog.query(""select ?x ?y ?z WHERE { ?x ?y ?z } LIMIT 2"") { list << it }   	// list has the two Sesame BindingSet's added to it, ie TupleQueryResult.next called per each run on the closure  ```    Collect query results via projected result values  ```groovy      stardog.each(""select ?x ?y ?z WHERE { ?x ?y ?z } LIMIT 2"", {         println x // whatever x is bound to in the result set         println y // ..         println z //       }  ```    Like query, this is executed over each TupleQueryResult    Insert multidimensional arrays, single triples also works  ```groovy  	stardog.insert([ [""urn:test3"", ""urn:test:predicate"", ""hello world""], [""urn:test4"", ""urn:test:predicate"", ""hello world2""] ])  ```    Remove triples via a simple groovy list  ```groovy  	stardog.remove([""urn:test3"", ""urn:test:predicate"", ""hello world""])  ```    ## Upgrading from Prior Releases    Significant changes in 2.1.3:    *    Installation now available via Maven Central and ""com.complexible.stardog:stardog-groovy:2.1.3"" dependency  *    No longer a dependency on Spring, i.e. the Stardog-Spring DataSource can no longer be passed as a constructor.  The Stardog Groovy class performs all the same operations.  *    Stardog-groovy 4.2.1 and later should be built with Gradle 2.3      ## Development ##    To get started, just clone the project. You'll need a local copy of Stardog to be able to run the build. For more information on starting the Stardog DB service and how it works, go to [Stardog's documentation](http://stardog.com/docs/), where you'll find everything you need to get up and running with Stardog.    Once you have the local project, start up a local Stardog and create a testdb with `stardog-admin db create -n testdb $STARDOG/data/examples/lumbSchema.owl $STARDOG/data/examples/University0_0.owl`.     You can then build the project        gradle build    # validate all the test pass      gradle install  # install jar into local m2    That will run all the JUnit tests and create the jar in build/libs.  The test does use a running Stardog, and if you receive error during the test it is likely you're Stardog server is not running or has an invalid license.  This usually manifests in an exit of a Gradle worker, which is the JVM running the JUnit tests.       ## Contributing ##    This framework is in continuous development, please check the [issues](https://github.com/clarkparsia/stardog-groovy/issues) page. You're welcome to contribute.    ## License    Copyright 2015 - 2018 Stardog Union  Copyright 2012 - 2015 Clark & Parsia, LLC  Copyright 2012 Al Baker    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    * [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)      Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.     """
Semantic web;https://github.com/shaoxiongji/awesome-knowledge-graph;"""# Knowledge Graphs  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  [![GitHub issues](https://img.shields.io/github/issues/shaoxiongji/knowledge-graphs)](https://github.com/shaoxiongji/knowledge-graphs/issues)  [![GitHub forks](https://img.shields.io/github/forks/shaoxiongji/knowledge-graphs)](https://github.com/shaoxiongji/knowledge-graphs/network)  [![GitHub stars](https://img.shields.io/github/stars/shaoxiongji/knowledge-graphs)](https://github.com/shaoxiongji/knowledge-graphs/stargazers)  [![Twitter](https://img.shields.io/twitter/url?style=social)](https://twitter.com/intent/tweet?text=Wow:&url=https%3A%2F%2Fgithub.com%2Fshaoxiongji%2Fknowledge-graphs)    A collection of knowledge graph papers, codes, and reading notes.    - [Knowledge Graphs](#knowledge-graphs)    - [Survey](#survey)    - [Papers by venues](#papers-by-venues)    - [Papers by categories](#papers-by-categories)    - [Data](#data)      - [General Knowledge Graphs](#general-knowledge-graphs)      - [Domain-specific Data](#domain-specific-data)      - [Entity Recognition](#entity-recognition)      - [Other Collections](#other-collections)    - [Libraries, Softwares and Tools](#libraries-softwares-and-tools)      - [KRL Libraries](#krl-libraries)      - [Knowledge Graph Database](#knowledge-graph-database)      - [Others](#others)      - [Interactive APP](#interactive-app)    - [Courses, Tutorials and Seminars](#courses-tutorials-and-seminars)      - [Courses](#courses)    - [Related Repos](#related-repos)    - [Acknowledgements](#acknowledgements)      ## Survey  __A Survey on Knowledge Graphs: Representation, Acquisition and Applications__. IEEE TNNLS 2021. _Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, Philip S. Yu_. [[Paper](https://arxiv.org/pdf/2002.00388)]     __Knowledge Graphs__. Preprint 2020. _Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d'Amato, Gerard de Melo, Claudio Gutierrez, José Emilio Labra Gayo, Sabrina Kirrane, Sebastian Neumaier, Axel Polleres, Roberto Navigli, Axel-Cyrille Ngonga Ngomo, Sabbir M. Rashid, Anisa Rula, Lukas Schmelzeisen, Juan Sequeda, Steffen Staab, Antoine Zimmermann_. [[Paper](https://arxiv.org/abs/2003.02320)]     __Knowledge Representation Learning: A Quantitative Review__. Preprint 2018. _Lin, Yankai and Han, Xu and Xie, Ruobing and Liu, Zhiyuan and Sun, Maosong_. [[Paper](https://arxiv.org/pdf/1812.10901)]    __Knowledge graph embedding: A survey of approaches and applications__. TKDE 2017. _Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li_. [[Paper](https://persagen.com/files/misc/Wang2017Knowledge.pdf)]    __Knowledge graph refinement: A survey of approaches and evaluation methods__. Semantic Web 2017. _Paulheim, Heiko_. [[Paper](http://www.semantic-web-journal.net/system/files/swj1167.pdf)]    __A review of relational machine learning for knowledge graphs__. Proceedings of the IEEE 2015. _Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy_. [[Paper](https://arxiv.org/pdf/1503.00759)]    ## Papers by venues  | Year      | WWW                           | AAAI                            |  ACL  |   | --------  | --------                      | --------                        |  --------                        |                | 2020      | [20](./conferences/www20.md)  | [28](./conferences/aaai20.md)   |  [53](./conferences/acl20.md)    |    ## Papers by categories  - [Knowledge Graph Embedding](./papers/KG-embedding.md)  - [Cross-Modal KG Embedding](./papers/KG-cross-modal.md)  - Knowledge Acquisition    - [Knowledge Graph Completion](./papers/KG-KGC.md)    - [Relation Extraction](./papers/KG-RE.md)    - [Entity Discovery](./papers/KG-entity.md)  - Knowledge-aware Applications    - [Natural Language Understanding](./papers/KG-applications.md#natural-langauge-understanding)    - [Commonsense Knowledge](./papers/KG-applications.md#commonsense-knowledge)    - [Question Answering](./papers/KG-applications.md#question-answering)    - [Dialogue Systems](./papers/KG-applications.md#dialogue-systems)    - [Recommendation Systems](./papers/KG-applications.md#recommendation-systems)    - [Information Retrieval](./papers/KG-applications.md#information-retrieval)  - [Temporal Knowledge Graph](./papers/KG-temporal.md)  - [Knowledge Graph Reasoning](./papers/KG-reasoning.md)  - [One/few-Shot and Zero-Shot Learning](./papers/KG-few-shot.md)  - [Domain-specific Knowledge Graphs](./papers/KG-domain.md)  - [KG Database Systems](./papers/KG-database.md)    ## Data  ### General Knowledge Graphs  - WordNet, https://wordnet.princeton.edu  - OpenCyc, https://www.cyc.com/opencyc/  - Cyc, https://www.cyc.com  - YAGO, http://www.mpii.mpg.de/∼suchanek/yago  - DBpedia, https://wiki.dbpedia.org/develop/datasets  - Freebase, https://developers.google.com/freebase/  - NELL, http://rtw.ml.cmu.edu/rtw/  - Wikidata, https://www.wikidata.org/wiki  - Probase IsA, https://concept.research.microsoft.com/Home/Download  - Google KG, https://developers.google.com/knowledge-graph  - A large-scale Chinese knowledge graph from [OwnThink](https://github.com/ownthink/KnowledgeGraph)  - GDELT（Global Database of Events, Language, and Tone）[Web](https://www.gdeltproject.org)    ### Domain-specific Data  __OpenKG knowledge graphs about the novel coronavirus COVID-19__  - 新冠百科图谱 [[链接](http://www.openkg.cn/dataset/2019-ncov-baike)] Knowledge graph from encyclopedia[[Link](http://www.openkg.cn/dataset/2019-ncov-baike)]    - 新冠科研图谱 [[链接](http://www.openkg.cn/dataset/2019-ncov-research)] Knowledge graph of COVID-19 research [[Link](http://www.openkg.cn/dataset/2019-ncov-research)]    - 新冠临床图谱 [[链接](http://www.openkg.cn/dataset/2019-ncov-clinic)] Clinical knowledge graph [[Link](http://www.openkg.cn/dataset/2019-ncov-clinic)]    - 新冠英雄图谱 [[链接](http://www.openkg.cn/dataset/2019-ncov-hero)] Knowledge graph of people, experts, and heroes [[Link](http://www.openkg.cn/dataset/2019-ncov-hero)]    - 新冠热点事件图谱 [[链接](http://www.openkg.cn/dataset/2019-ncov-event)] Knowledge graph of public events [[Link](http://www.openkg.cn/dataset/2019-ncov-event)]    __COVID❋GRAPH  COVID-19 virus__  [[Web](http://www.odbms.org/2020/03/we-build-a-knowledge-graph-on-covid-19/)]    __KgBase COVID-19 knowledge graph__ [[Web](https://covid19.kgbase.com)]  __Academic graphs__  - OAG, Open Academic Graph, https://www.aminer.cn/open-academic-graph    ### Entity Recognition  CORD-19, a comprehensieve named entity annotation dataset, CORD-NER, on the COVID-19 Open Research Dataset Challenge (CORD-19) corpus [[Data](https://xuanwang91.github.io/2020-03-20-cord19-ner/)]    ### Other Collections  Baidu BROAD datasets [[Web](https://ai.baidu.com/broad/introduction)]    __ASER: A Large-scale Eventuality Knowledge Graph__  WWW 2020. _Zhang et al._ [[Paper](https://dl.acm.org/doi/abs/10.1145/3366423.3380107)]      ## Libraries, Softwares and Tools  ### KRL Libraries  [Grakn](https://github.com/graknlabs/kglib), Grakn Knowledge Graph Library (ML R&D) https://grakn.ai    [AmpliGraph](https://github.com/Accenture/AmpliGraph), Python library for Representation Learning on Knowledge Graphs https://docs.ampligraph.org    [OpenKE](https://github.com/thunlp/OpenKE), An Open-Source Package for Knowledge Embedding (KE)    [Fast-TransX](https://github.com/thunlp/Fast-TransX), An Efficient implementation of TransE and its extended models for Knowledge Representation Learning    [scikit-kge](https://github.com/mnick/scikit-kge), Python library to compute knowledge graph embeddings    [OpenNRE](https://github.com/thunlp/OpenNRE), An Open-Source Package for Neural Relation Extraction (NRE)    ### Knowledge Graph Database  [akutan](https://github.com/eBay/akutan), A distributed knowledge graph store    ### Others  - [OpenNRE](https://github.com/thunlp/OpenNRE)    ### Interactive APP  Knowledge graph APP, Simple knowledge graph applications can be easily built using JSON data managed entirely via a GraphQL layer. [[Github](https://github.com/epistemik-co/staple-api-kg-demo)] [[Website](http://demo.staple-api.org)]    ## Courses, Tutorials and Seminars  ### Courses  - Stanford CS 520 Knowledge Graphs: How should AI explicitly represent knowledge? _Vinay K. Chaudhri, Naren Chittar, Michael Genesereth_. [[Web](https://web.stanford.edu/class/cs520/)]  - Stanford CS 224W: Machine Learning with Graphs. _Jure Leskovec_. [[Web](http://web.stanford.edu/class/cs224w/index.html)]  - University of Bonn: Analysis of Knowledge Graphs. _Jens Lehmmann_. [[Web](https://sewiki.iai.uni-bonn.de/teaching/lectures/kga/2017/start)] [[GitHub](https://github.com/SmartDataAnalytics/Knowledge-Graph-Analysis-Programming-Exercises)]  - Knowledge Graphs. _Harald Sack, Mehwish Alam_. [[Web](https://open.hpi.de/courses/knowledgegraphs2020)]      ## Related Repos  A repo about knowledge graph in Chinese - [husthuke/awesome-knowledge-graph](https://github.com/husthuke/awesome-knowledge-graph)    A repo about NLP, KG, Dialogue Systems in Chinese - [lihanghang/NLP-Knowledge-Graph](https://github.com/lihanghang/NLP-Knowledge-Graph)    Top-level Conference Publications on Knowledge Graph - [wds-seu/Knowledge-Graph-Publications](https://github.com/wds-seu/Knowledge-Graph-Publications)    Geospatial Knowledge Graphs - [semantic-geospatial](https://github.com/laurentlefort/semantic-geospatial/wiki/Geospatial-Knowledge-Graphs)    ## Acknowledgements    Acknowledgments give to the following people who comment or contribute to this repository (listed chronologically).    - [DonaldTsang](https://github.com/DonaldTsang)  - [NYXFLOWER](https://github.com/NYXFLOWER)  - [Arun-George-Zachariah](https://github.com/Arun-George-Zachariah)      __[⬆](#awesome-knowledge-graph)__ """
Semantic web;https://github.com/mff-uk/ODCS;"""ODCleanStore  ============    The tool uses data processing pipelines for obtaining, processing, and storing  RDF data. It makes data processing highly customizable by employing custom data  processing units, also provides data processing monitoring, debugging, and  scheduling capabilities.      Documentation  -------------    Please see http://www.ksi.mff.cuni.cz/~knap/odcs      Installation steps  ------------------    Please see http://www.ksi.mff.cuni.cz/~knap/odcs/doc.html#install      Licenses  -------    The following modules of the software are licensed under GNU Lesser General Public License, Version 3, https://www.gnu.org/licenses/lgpl-3.0.txt:       * commons    * commons-module    * commons-web    * dataunit-rdf    * dataunit-file    * ontology    * module-test    * module-base      The following modules of the software are licensed under GNU General Public License, Version 3, https://www.gnu.org/licenses/gpl-3.0.txt:      * commons-app    * frontend    * backend    * dataunit    * dataunit-file-impl    * dataunit-rdf-impl    * RDF_File_Extractor    * RDF_File_Loader     * SPARQL_Extractor     * SPARQL_Loader     * SPARQL_Transformer    * RDF_Data_Validator     * Silk_Linker_Extractor    For details, please see the particular module.       """
Big data;https://github.com/ziadoz/awesome-php;"""# Awesome PHP ![](https://github.com/ziadoz/awesome-php/workflows/Awesome%20Bot/badge.svg)    A curated list of amazingly awesome PHP libraries, resources and shiny things.    ## Contributing and Collaborating  Please see [CONTRIBUTING](https://github.com/ziadoz/awesome-php/blob/master/CONTRIBUTING.md), [CODE-OF-CONDUCT](https://github.com/ziadoz/awesome-php/blob/master/CODE-OF-CONDUCT.md) and [COLLABORATING](https://github.com/ziadoz/awesome-php/blob/master/COLLABORATING.md) for details.    ## Table of Contents  - [Awesome PHP](#awesome-php)      - [Composer Repositories](#composer-repositories)      - [Dependency Management](#dependency-management)      - [Dependency Management Extras](#dependency-management-extras)      - [Frameworks](#frameworks)      - [Framework Extras](#framework-extras)      - [Content Management Systems](#content-management-systems-cms)      - [Components](#components)      - [Micro Frameworks](#micro-frameworks)      - [Micro Framework Extras](#micro-framework-extras)      - [Routers](#routers)      - [Templating](#templating)      - [Static Site Generators](#static-site-generators)      - [HTTP](#http)      - [Scraping](#scraping)      - [Middlewares](#middlewares)      - [URL](#url)      - [Email](#email)      - [Files](#Files)      - [Streams](#streams)      - [Dependency Injection](#dependency-injection)      - [Imagery](#imagery)      - [Testing](#testing)      - [Continuous Integration](#continuous-integration)      - [Documentation](#documentation)      - [Security](#security)      - [Passwords](#passwords)      - [Code Analysis](#code-analysis)      - [Code Quality](#code-quality)      - [Static Analysis](#static-analysis)      - [Architectural](#architectural)      - [Debugging and Profiling](#debugging-and-profiling)      - [Build Tools](#build-tools)      - [Task Runners](#task-runners)      - [Navigation](#navigation)      - [Asset Management](#asset-management)      - [Geolocation](#geolocation)      - [Date and Time](#date-and-time)      - [Event](#event)      - [Logging](#logging)      - [E-commerce](#e-commerce)      - [PDF](#pdf)      - [Office](#office)      - [Database](#database)      - [Migrations](#migrations)      - [NoSQL](#nosql)      - [Queue](#queue)      - [Search](#search)      - [Command Line](#command-line)      - [Authentication and Authorization](#authentication-and-authorization)      - [Markup and CSS](#markup-and-css)      - [JSON](#json)      - [Strings](#strings)      - [Numbers](#numbers)      - [Filtering and Validation](#filtering-and-validation)      - [API](#api)      - [Caching and Locking](#caching-and-locking)      - [Data Structure and Storage](#data-structure-and-storage)      - [Notifications](#notifications)      - [Deployment](#deployment)      - [Internationalisation and Localisation](#internationalisation-and-localisation)      - [Serverless](#serverless)      - [Configuration](#configuration)      - [Third Party APIs](#third-party-apis)      - [Extensions](#extensions)      - [Miscellaneous](#miscellaneous)  - [Software](#software)      - [PHP Installation](#php-installation)      - [Development Environment](#development-environment)      - [Virtual Machines](#virtual-machines)      - [Text Editors and IDEs](#text-editors-and-ides)      - [Web Applications](#web-applications)      - [Infrastructure](#infrastructure)  - [Resources](#resources)      - [PHP Websites](#php-websites)      - [PHP Books](#php-books)      - [PHP Videos](#php-videos)      - [PHP Podcasts](#php-podcasts)      - [PHP Newsletters](#php-newsletters)      - [PHP Reading](#php-reading)      - [PHP Internals Reading](#php-internals-reading)    ### Composer Repositories  *Composer Repositories.*    * [Firegento](https://packages.firegento.com/) - Magento Module Composer Repository.  * [Packagist](https://packagist.org/) - The PHP Package Repository.  * [Private Packagist](https://packagist.com/) - Composer package archive as a service for PHP.  * [WordPress Packagist](https://wpackagist.org/) - Manage your plugins with Composer.    ### Dependency Management  *Libraries for dependency and package management.*    * [Composer Installers](https://github.com/composer/installers) - A  multi framework Composer library installer.  * [Composer](https://getcomposer.org/) - A package and dependency manager.  * [Phive](https://phar.io/) - A PHAR manager.  * [Pickle](https://github.com/FriendsOfPHP/pickle) - A PHP extension installer.    ### Dependency Management Extras  *Extras related to dependency management.*    * [Composed](https://github.com/joshdifabio/composed) - A library to parse your project's Composer environment at runtime.  * [Composer Merge Plugin](https://github.com/wikimedia/composer-merge-plugin) - A composer plugin to merge several `composer.json` files.  * [Composer Normalize](https://github.com/ergebnis/composer-normalize) - A plugin for normalising `composer.json` files.   * [Composer Patches](https://github.com/cweagans/composer-patches) - A plugin for Composer to apply patches.  * [Composer Require Checker](https://github.com/maglnet/ComposerRequireChecker) - CLI tool to analyze composer dependencies and verify that no unknown symbols are used in the sources of a package.  * [Composer Unused](https://github.com/composer-unused/composer-unused) - A CLI Tool to scan for unused composer packages.  * [Prestissimo](https://github.com/hirak/prestissimo) - A composer plugin which enables parallel install process.  * [Repman](https://repman.io) - A private PHP package repository manager and Packagist proxy.  * [Satis](https://github.com/composer/satis) - A static Composer repository generator.  * [Tooly](https://github.com/tommy-muehle/tooly-composer-script) - A library to manage PHAR files in project using Composer.  * [Toran Proxy](https://toranproxy.com) - A static Composer repository and proxy.    ### Frameworks  *Web development frameworks.*    * [CakePHP](https://cakephp.org/) - A rapid application development framework.  * [Laminas](https://getlaminas.org/) - A framework comprised of individual components (previously Zend Framework).  * [Laravel](https://laravel.com/) - A web application framework with expressive, elegant syntax.  * [Nette](https://nette.org) - A web framework comprised of mature components.  * [Phalcon](https://phalcon.io/en-us) - A framework implemented as a C extension.  * [Spiral](https://spiral.dev/) - A high performance PHP/Go framework.  * [Symfony](https://symfony.com/) - A set of reuseable components and a web framework.  * [Yii2](https://github.com/yiisoft/yii2/) - A fast, secure, and efficient web framework.    ### Framework Extras  *Extras related to web development frameworks.*    * [CakePHP CRUD](https://github.com/friendsofcake/crud) - A Rapid Application Development (RAD) plugin for CakePHP.  * [Knp RAD Components](https://rad.knplabs.com/) - A set of Rapid Application Development (RAD) components for Symfony.  * [LaravelS](https://github.com/hhxsv5/laravel-s) - Glue for using Swoole in Laravel or Lumen.  * [Symfony CMF](https://github.com/symfony-cmf/symfony-cmf) - A Content Management Framework to create custom CMS.    ### Content Management Systems (CMS)  *Tools for managing digital content.*    * [Backdrop](https://backdropcms.org) - A CMS targeting small-to-medium sized business and non-profits (a fork of Drupal).  * [Concrete5](https://www.concrete5.org/) - A CMS targeting users with a minimum of technical skills.  * [CraftCMS](https://github.com/craftcms/cms) - A flexible, user-friendly CMS for creating custom digital experiences on the web and beyond.  * [Drupal](https://www.drupal.org) - An enterprise level CMS.  * [Grav](https://github.com/getgrav/grav) - A modern flat-file CMS.  * [Joomla](https://www.joomla.org/) - Another leading CMS.  * [Kirby](https://getkirby.com/) - A flat-file CMS that adapts to any project.  * [Magento](https://magento.com/) - The most popular ecommerce platform.  * [Moodle](https://moodle.org/) - An open-source learning platform.  * [Pico CMS](http://picocms.org/) - A stupidly simple, blazing fast, flat file CMS.  * [Statamic](https://statamic.com/) - Build beautiful, easy to manage websites.  * [WordPress](https://wordpress.org/) - A blogging platform and CMS.    ### Components  *Standalone components from web development frameworks and development groups.*    * [Aura](http://auraphp.com/) - Independent components, fully decoupled from each other and from any framework.  * [CakePHP Plugins](https://plugins.cakephp.org/) - A directory of CakePHP plugins.  * [Hoa Project](https://hoa-project.net/En/) - Another package of PHP components.  * [Laravel Components](https://github.com/illuminate) - The Laravel Framework components.  * [League of Extraordinary Packages](https://thephpleague.com/) - A PHP package development group.  * [Spatie Open Source](https://spatie.be/open-source) - A collection of open source PHP and Laravel packages.  * [Symfony Components](https://symfony.com/components) - The components that make Symfony.  * [Laminas Components](https://docs.laminas.dev/components/) - The components that make the Laminas Framework.    ### Micro Frameworks  *Micro frameworks and routers.*    * [Laravel-Zero](https://laravel-zero.com) - A micro-framework for console applications.  * [Lumen](https://lumen.laravel.com) - A micro-framework by Laravel.  * [Mezzio](https://getexpressive.org/) - A micro-framework by Laminas.  * [Radar](https://github.com/radarphp/Radar.Adr) - An Action-Domain-Responder implementation for PHP.  * [Silly](https://github.com/mnapoli/silly) - A micro-framework for CLI applications.  * [Slim](https://www.slimframework.com/) - Another simple micro framework.    ### Micro Framework Extras  *Extras related to micro frameworks and routers.*    * [Slim Skeleton](https://github.com/slimphp/Slim-Skeleton) - A skeleton for Slim.  * [Slim Twig View](https://github.com/slimphp/Slim-Views) - Integrate Twig into Slim.  * [Slim PHP View](https://github.com/slimphp/PHP-View) - A simple PHP renderer for Slim.    ### Routers  *Libraries for handling application routing.*    * [Aura.Router](https://github.com/auraphp/Aura.Router) - A full-featured routing library.  * [Fast Route](https://github.com/nikic/FastRoute) - A fast routing library.  * [Klein](https://github.com/klein/klein.php) - A flexible router.  * [Pux](https://github.com/c9s/Pux) - Another fast routing library.  * [Route](https://github.com/thephpleague/route) - A routing library built on top of Fast Route.    ### Templating  *Libraries and tools for templating and lexing.*    * [MtHaml](https://github.com/arnaud-lb/MtHaml) - A PHP implementation of the HAML template language.  * [Mustache](https://github.com/bobthecow/mustache.php) - A PHP implementation of the Mustache template language.  * [PHPTAL](https://phptal.org/) - A PHP implementation of the [TAL](https://en.wikipedia.org/wiki/Template_Attribute_Language) templating language.  * [Plates](http://platesphp.com/) - A native PHP templating library.  * [Smarty](https://www.smarty.net/) - A template engine to complement PHP.  * [Twig](https://twig.symfony.com/) - A comprehensive templating language.    ### Static Site Generators  *Tools for pre-processing content to generate web pages.*    * [Couscous](http://couscous.io) - Couscous turns Markdown documentation into beautiful websites. It's GitHub Pages on steroids.  * [Jigsaw](http://jigsaw.tighten.co/) - Simple static sites with Laravel's Blade.  * [Sculpin](https://sculpin.io) - A tool that converts Markdown and Twig into static HTML.  * [Spress](http://spress.yosymfony.com) - An extensible tool that converts Markdown and Twig into HTML.    ### HTTP  *Libraries for working with HTTP.*    * [Buzz](https://github.com/kriswallsmith/Buzz) - Another HTTP client.  * [Guzzle]( https://github.com/guzzle/guzzle) - A comprehensive HTTP client.  * [HTTPlug](http://httplug.io) - An HTTP client abstraction without binding to a specific implementation.  * [Nyholm PSR-7](https://github.com/Nyholm/psr7) - A super lightweight PSR-7 implementation. Very strict and very fast.  * [PHP VCR](https://php-vcr.github.io/) - A library for recording and replaying HTTP requests.  * [Requests](https://github.com/rmccue/Requests) - A simple HTTP library.  * [Retrofit](https://github.com/tebru/retrofit-php) - A library to ease creation of REST API clients.  * [Symfony HTTP Client](https://github.com/symfony/http-client) - A component to fetch HTTP resources synchronously or asynchronously.  * [Laminas Diactoros](https://github.com/laminas/laminas-diactoros) - PSR-7 HTTP Message implementation.    ### Scraping  *Libraries for scraping websites.*    * [Chrome PHP](https://github.com/chrome-php/chrome) - Instrument headless Chrome/Chromium instances from PHP.   * [DiDOM](https://github.com/Imangazaliev/DiDOM) - A super fast HTML scrapper and parser.  * [Embed](https://github.com/oscarotero/Embed) - An information extractor from any web service or page.  * [Goutte](https://github.com/FriendsOfPHP/Goutte) - A simple web scraper.  * [Symfony Panther](https://github.com/symfony/panther) - A browser testing and web crawling library for PHP and Symfony.  * [PHP Spider](https://github.com/mvdbos/php-spider) - A configurable and extensible PHP web spider.    ### Middlewares  *Libraries for building application using middlewares.*    * [PSR-7 Middlewares](https://github.com/oscarotero/psr7-middlewares) - Inspiring collection of handy middlewares.  * [Relay](https://github.com/relayphp/Relay.Relay) - A PHP 5.5 PSR-7 middleware dispatcher.  * [Stack](https://github.com/stackphp) - A library of stackable middleware for Symfony.  * [Laminas Stratigility](https://github.com/laminas/laminas-stratigility) - Middleware for PHP built on top of PSR-7.    ### URL  *Libraries for parsing URLs.*    * [PHP Domain Parser](https://github.com/jeremykendall/php-domain-parser) - A domain suffix parser library.  * [Purl](https://github.com/jwage/purl) - A URL manipulation library.  * [sabre/uri](https://github.com/sabre-io/uri) - A functional URI manipulation library.  * [Uri](https://github.com/thephpleague/uri) - Another URL manipulation library.    ### Email  *Libraries for sending and parsing email.*    * [CssToInlineStyles](https://github.com/tijsverkoyen/CssToInlineStyles) - A library to inline CSS in email templates.  * [Email Reply Parser](https://github.com/willdurand/EmailReplyParser) - An email reply parser library.  * [Email Validator](https://github.com/nojacko/email-validator) - A small email address validation library.  * [Fetch](https://github.com/tedious/Fetch) - An IMAP library.  * [Mautic](https://github.com/mautic/mautic) - Email marketing automation  * [PHPMailer](https://github.com/PHPMailer/PHPMailer) - Another mailer solution.  * [PHP IMAP](https://github.com/barbushin/php-imap) - A library to access mailboxes via POP3, IMAP and NNTP.  * [Stampie](https://github.com/Stampie/Stampie) - A library for email services such as [SendGrid](https://sendgrid.com/), [PostMark](https://postmarkapp.com), [MailGun](https://www.mailgun.com/) and [Mandrill](https://mailchimp.com/features/transactional-email/).  * [SwiftMailer](https://swiftmailer.symfony.com) - A mailer solution.  * [Symfony Mailer](https://github.com/symfony/mailer) - A powerful library for creating and sending emails.    ### Files  *Libraries for file manipulation and MIME type detection.*    * [CSV](https://github.com/thephpleague/csv) - A CSV data manipulation library.  * [Flysystem](https://github.com/thephpleague/Flysystem) - Abstraction for local and remote filesystems.  * [Gaufrette](https://github.com/KnpLabs/Gaufrette) - A filesystem abstraction layer.  * [Hoa Mime](https://github.com/hoaproject/Mime) - Another MIME detection library.  * [PHP FFmpeg](https://github.com/PHP-FFmpeg/PHP-FFmpeg/) - A wrapper for the [FFmpeg](https://www.ffmpeg.org/) video library.  * [UnifiedArchive](https://github.com/wapmorgan/UnifiedArchive) - A unified reader and writer of compressed archives.    ### Streams  *Libraries for working with streams.*    * [ByteStream](https://amphp.org/byte-stream/) - An asynchronous stream abstraction.  * [Streamer](https://github.com/fzaninotto/Streamer) - A simple object-orientated stream wrapper library.    ### Dependency Injection  *Libraries that implement the dependency injection design pattern.*    * [Aura.Di](https://github.com/auraphp/Aura.Di) - A serializable dependency injection container with constructor and setter injection, interface and trait awareness, configuration inheritance, and much more.  * [Acclimate](https://github.com/AcclimateContainer/acclimate-container) - A common interface to dependency injection containers and service locators.  * [Auryn](https://github.com/rdlowrey/Auryn) - A recursive dependency injector.  * [Container](https://github.com/thephpleague/container) - Another flexible dependency injection container.  * [Disco](https://github.com/bitExpert/disco) - A PSR-11 compatible, annotation-based dependency injection container.  * [PHP-DI](https://php-di.org/) - A dependency injection container that supports autowiring.  * [Pimple](https://pimple.symfony.com/) - A tiny dependency injection container.  * [Symfony DI](https://github.com/symfony/dependency-injection) - A dependency injection container component.    ### Imagery  *Libraries for manipulating images.*    * [Color Extractor](https://github.com/thephpleague/color-extractor) - A library for extracting colours from images.  * [Glide](https://github.com/thephpleague/glide) - An on-demand image manipulation library.  * [Image Hash](https://github.com/jenssegers/imagehash) - A library for generating perceptual image hashes.  * [Image Optimizer](https://github.com/psliwa/image-optimizer) - A library for optimizing images.  * [Imagine](https://imagine.readthedocs.io/en/latest/index.html) - An image manipulation library.  * [Intervention Image](https://github.com/Intervention/image) - Another image manipulation library.  * [PHP Image Workshop](https://github.com/Sybio/ImageWorkshop) - Another image manipulation library.    ### Testing  *Libraries for testing codebases and generating test data.*    * [Alice](https://github.com/nelmio/alice) - An expressive fixture generation library.  * [AspectMock](https://github.com/Codeception/AspectMock) - A mocking framework for PHPUnit/Codeception.  * [Atoum](https://github.com/atoum/atoum) - A simple testing library.  * [Behat](https://docs.behat.org/en/latest/) - A behaviour driven development (BDD) testing framework.  * [Codeception](https://github.com/Codeception/Codeception) - A full stack testing framework.  * [Faker](https://github.com/fakerphp/faker) - A fake data generator library.  * [HTTP Mock](https://github.com/InterNations/http-mock) - A library for mocking HTTP requests in unit tests.  * [Infection](https://github.com/infection/infection) - An AST-based PHP Mutation testing framework.  * [Kahlan](https://github.com/kahlan/kahlan) - Full stack Unit/BDD testing framework with built-in stub, mock and code-coverage support.  * [Mink](http://mink.behat.org/en/latest/) - Web acceptance testing.  * [Mockery](https://github.com/mockery/mockery) - A mock object library for testing.  * [ParaTest](https://github.com/paratestphp/paratest) - A parallel testing library for PHPUnit.  * [Pest](https://pestphp.com/) - A testing framework with a focus on simplicity.  * [Peridot](https://github.com/peridot-php/peridot) - An event driven test framework.  * [Phake](https://github.com/mlively/Phake) - Another mock object library for testing.  * [Pho](https://github.com/danielstjules/pho) - Another behaviour driven development testing framework.  * [PHP-Mock](https://github.com/php-mock/php-mock) - A mock library for built-in PHP functions (e.g. time()).  * [PHP MySQL Engine](https://github.com/vimeo/php-mysql-engine) -  A MySQL engine written in pure PHP.   * [PHPSpec](https://github.com/phpspec/phpspec) - A design by specification unit testing library.  * [PHPT](https://qa.php.net/write-test.php) - A test tool used by PHP itself.  * [PHPUnit](https://github.com/sebastianbergmann/phpunit) - A unit testing framework.  * [Prophecy](https://github.com/phpspec/prophecy) - A highly opinionated mocking framework.  * [VFS Stream](https://github.com/bovigo/vfsStream) - A virtual filesystem stream wrapper for testing.    ### Continuous Integration  *Libraries and applications for continuous integration.*    * [CircleCI](https://circleci.com) - A continuous integration platform.  * [GitlabCi](https://about.gitlab.com/stages-devops-lifecycle/continuous-integration/) - Let GitLab CI test, build, deploy your code. TravisCi like.  * [Jenkins](https://www.jenkins.io/) - A continuous integration platform with [PHP support](https://www.jenkins.io/solutions/php/).  * [JoliCi](https://github.com/jolicode/JoliCi) - A continuous integration client written in PHP and powered by Docker.  * [PHPCI](https://github.com/dancryer/phpci) - An open source continuous integration platform for PHP.  * [SemaphoreCI](https://semaphoreci.com/) - A continuous integration platform for open source and private projects.  * [Shippable](https://www.shippable.com/) - A Docker based continious integration platform for open source and private projects.  * [Travis CI](https://travis-ci.org/) - A continuous integration platform.  * [Setup PHP](https://github.com/shivammathur/setup-php) - A GitHub Action for PHP.    ### Documentation  *Libraries for generating project documentation.*    * [APIGen](https://github.com/apigen/apigen) - Another API documentation generator.  * [daux.io](https://github.com/dauxio/daux.io) - A documentation generator which uses Markdown files.  * [PHP Documentor 2](https://github.com/phpDocumentor/phpDocumentor) - A documentation generator.  * [phpDox](http://phpdox.de/) - A documentation generator for PHP projects (that is not limited to API documentation).    ### Security  *Libraries for generating secure random numbers, encrypting data and scanning and testing for vulnerabilities.*    * [Halite](https://paragonie.com/project/halite) - A simple library for encryption using [libsodium](https://github.com/jedisct1/libsodium).  * [HTML Purifier](https://github.com/ezyang/htmlpurifier) - A standards compliant HTML filter.  * [IniScan](https://github.com/psecio/iniscan) - A tool that scans PHP INI files for security.  * [Optimus](https://github.com/jenssegers/optimus) - Id obfuscation based on Knuth's multiplicative hashing method.  * [PHPGGC](https://github.com/ambionics/phpggc) - A library of PHP unserializeable payloads along with a tool to generate them.  * [PHP Encryption](https://github.com/defuse/php-encryption) - Secure PHP Encryption Library.  * [PHP SSH](https://github.com/Herzult/php-ssh) - An experimental object orientated SSH wrapper library.  * [PHPSecLib](http://phpseclib.sourceforge.net/) - A pure PHP secure communications library.  * [random_compat](https://github.com/paragonie/random_compat) - PHP 5.x support for `random_bytes()` and `random_int()`  * [RandomLib](https://github.com/ircmaxell/RandomLib) - A library for generating random numbers and strings.  * [Symfony Security Monitoring](https://security.symfony.com/) - A web tool to check your Composer dependencies for security advisories, previously known as ""SensioLabs Security Check"".  * [SQLMap](https://github.com/sqlmapproject/sqlmap) - An automatic SQL injection and database takeover tool.   * [TCrypto](https://github.com/timoh6/TCrypto) - A simple encrypted key-value storage library.  * [VAddy](https://vaddy.net/) - A continuous security testing platform for web applications.  * [Zap](https://owasp.org/www-project-zap/) - An integrated penetration testing tool for web applications.    ### Passwords  *Libraries and tools for working with and storing passwords.*    * [GenPhrase](https://github.com/timoh6/GenPhrase) - A library for generating secure random passphrases.  * [Password Compat](https://github.com/ircmaxell/password_compat) - A compatibility library for the new PHP 5.5 password functions.  * [Password Policy](https://github.com/ircmaxell/password-policy) - A password policy library for PHP and JavaScript.  * [Password Validator](https://github.com/jeremykendall/password-validator) - A library for validating and upgrading password hashes.  * [Password-Generator](https://github.com/hackzilla/password-generator) - PHP library to generate random passwords.  * [PHP Password Lib](https://github.com/ircmaxell/PHP-PasswordLib) - A library for generating and validating passwords.  * [phpass](https://www.openwall.com/phpass/) - A portable password hashing framework.  * [Zxcvbn PHP](https://github.com/bjeavons/zxcvbn-php) - A realistic PHP password strength estimate library based on Zxcvbn JS.    ### Code Analysis  *Libraries and tools for analysing, parsing and manipulating codebases.*    * [Better Reflection](https://github.com/Roave/BetterReflection) - AST-based reflection library that allows analysis and manipulation of code  * [Code Climate](https://codeclimate.com) - An automated code review.  * [GrumPHP](https://github.com/phpro/grumphp) - A PHP code-quality tool.  * [PHP Parser](https://github.com/nikic/PHP-Parser) - A PHP parser written in PHP.  * [PHP Semantic Versioning Checker](https://github.com/tomzx/php-semver-checker) - A command line utility that compares two source sets and determines the appropriate semantic versioning to apply.  * [Phpactor](https://github.com/phpactor/phpactor) - PHP completion, refactoring and introspection tool.  * [PHPLOC](https://github.com/sebastianbergmann/phploc) - A tool for quickly measuring the size of a PHP project.  * [PHPQA](https://github.com/EdgedesignCZ/phpqa) - A tool for running QA tools (phploc, phpcpd, phpcs, pdepend, phpmd, phpmetrics).  * [Qafoo Quality Analyzer](https://github.com/Qafoo/QualityAnalyzer) - A tool to visualize metrics and source code.  * [Rector](https://github.com/rectorphp/rector) - A tool to upgrade and refactor code.  * [Scrutinizer](https://scrutinizer-ci.com/) - A web tool to [scrutinise PHP code](https://github.com/scrutinizer-ci/php-analyzer).  * [UBench](https://github.com/devster/ubench) - A simple micro benchmark library.    ### Code Quality  *Libraries for managing code quality, formatting and linting.*    * [CaptainHook](https://github.com/captainhookphp/captainhook) - An easy-to-use and flexible Git hook library.   * [PHP CodeSniffer](https://github.com/squizlabs/PHP_CodeSniffer) - A library that detects PHP, CSS and JS coding standard violations.  * [PHP CS Fixer](https://github.com/FriendsOfPHP/PHP-CS-Fixer) - A coding standards fixer library.  * [PHP Mess Detector](https://github.com/phpmd/phpmd) - A library that scans code for bugs, sub-optimal code, unused parameters and more.  * [PHPCheckstyle](https://github.com/PHPCheckstyle/phpcheckstyle) - A tool to help adhere to certain coding conventions.  * [PHPCPD](https://github.com/sebastianbergmann/phpcpd) - A library that detects copied and pasted code.    ### Static Analysis  *Libraries for performing static analysis of PHP code.*    * [Exakat](https://github.com/exakat/exakat) - A static analysis engine for PHP.  * [Deptrac](https://github.com/sensiolabs-de/deptrac) - A static code analysis tool that helps to enforce rules for dependencies between software layers.  * [Mondrian](https://github.com/Trismegiste/Mondrian) - A code analysis tool using Graph Theory.  * [phan](https://github.com/phan/phan) - A static analyzer based on PHP 7+ and the php-ast extension.  * [PHP Architecture Tester](https://github.com/carlosas/phpat) - Easy to use architecture testing tool for PHP.  * [PHPCompatibility](https://github.com/PHPCompatibility/PHPCompatibility) - A PHP compatibility checker for PHP CodeSniffer.  * [PhpDependencyAnalysis](https://github.com/mamuz/PhpDependencyAnalysis) - A tool to create customisable dependency graphs.  * [PHP Metrics](https://github.com/phpmetrics/PhpMetrics) - A static metric library.  * [PHP Migration](https://github.com/monque/PHP-Migration) - A static analyzer for PHP version migration.  * [PHPStan](https://github.com/phpstan/phpstan) - A PHP Static Analysis Tool.  * [Psalm](https://github.com/vimeo/psalm) - A static analysis tool for finding errors in PHP applications.    ### Architectural  *Libraries related to design patterns, programming approaches and ways to organize code.*    * [Design Patterns PHP](https://github.com/domnikl/DesignPatternsPHP) - A repository of software patterns implemented in PHP.  * [Finite](https://yohan.giarel.li/Finite/) - A simple PHP finite state machine.  * [Functional PHP](https://github.com/lstrojny/functional-php) - A functional programming library.  * [Iter](https://github.com/nikic/iter) - A library that provides iteration primitives using generators.  * [Patchwork](http://patchwork2.org/) - A library for redefining userland functions.  * [Pipeline](https://github.com/thephpleague/pipeline) - A pipeline pattern implementation.  * [Porter](https://github.com/ScriptFUSION/Porter) - Data import abstraction library for consuming Web APIs and other data sources.  * [Ruler](https://github.com/bobthecow/Ruler) - A simple stateless production rules engine.  * [RulerZ](https://github.com/K-Phoen/rulerz) - A powerful rule engine and implementation of the Specification pattern.    ### Debugging and Profiling  *Libraries and tools for debugging errors and profiling code.*    * [APM](https://pecl.php.net/package/APM) - Monitoring extension collecting errors and statistics into SQLite/MySQL/StatsD.  * [Barbushin PHP Console](https://github.com/barbushin/php-console) - Another web debugging console using Google Chrome.  * [Blackfire.io](https://blackfire.io) - A low-overhead code profiler.  * [Kint](https://github.com/kint-php/kint) - A debugging and profiling tool.  * [Metrics](https://github.com/beberlei/metrics) - A simple metrics API library.  * [PCOV](https://github.com/krakjoe/pcov) - A self contained code coverage compatible driver.  * [PHP Console](https://github.com/Seldaek/php-console) - A web debugging console.  * [PHP Debug Bar](http://phpdebugbar.com/) - A debugging toolbar.  * [PHPBench](https://github.com/phpbench/phpbench) - A benchmarking Framework.  * [PHPSpy](https://github.com/adsr/phpspy) - A low-overhead sampling profiler.  * [Symfony VarDumper](https://github.com/symfony/var-dumper) - A variable dumper component.  * [Tideways.io](https://tideways.com/) - Monitoring and profiling tool.  * [Tracy](https://github.com/nette/tracy) - A simple error detection, logging and time measuring library.  * [Whoops](https://github.com/filp/whoops) - A pretty error handling library.  * [xDebug](https://github.com/xdebug/xdebug) - A debug and profile tool for PHP.  * [XHProf](https://github.com/phacility/xhprof) - A profiling tool originally developed by Facebook.  * [Z-Ray](https://www.zend.com/products/z-ray) - A debug and profile tool for Zend Server.    ### Build Tools  *Project build and automation tools.*    * [Box](https://github.com/box-project/box) - A utility to build PHAR files.  * [Construct](https://github.com/jonathantorres/construct) - A PHP project/micro-package generator.  * [Phing](https://www.phing.info/) - A PHP project build system inspired by Apache Ant.  * [RMT](https://github.com/liip/RMT) - A library for versioning and releasing software.    ### Task Runners  *Libraries for automating and running tasks.*    * [Bldr](https://bldr.io/) - A PHP Task runner built on Symfony components.  * [Jobby](https://github.com/jobbyphp/jobby) - A PHP cron job manager without modifying crontab.  * [Robo](https://github.com/consolidation/Robo) - A PHP Task runner with object-orientated configurations.  * [Task](https://taskphp.github.io/) - A pure PHP task runner inspired by Grunt and Gulp.    ### Navigation  *Tools for building navigation structures.*    * [KnpMenu](https://github.com/KnpLabs/KnpMenu) - A menu library.  * [Menu](https://github.com/spatie/menu) - A flexible menu library with a fluent interface.    ### Asset Management  *Tools for managing, compressing and minifying website assets.*    * [JShrink](https://github.com/tedious/JShrink) - A JavaScript minifier library.  * [Laravel Mix](https://github.com/JeffreyWay/laravel-mix) - An elegant wrapper around Webpack for the 80% use case.  * [Symfony Asset](https://github.com/symfony/asset) - Manages URL generation and versioning of web assets.  * [Symfony Encore](https://github.com/symfony/webpack-encore) - A simple but powerful API for processing and compiling assets built around Webpack.    ### Geolocation  *Libraries for geocoding addresses and working with latitudes and longitudes.*    * [Country List](https://github.com/umpirsky/country-list) - A list of all countries with names and ISO 3166-1 codes.  * [GeoCoder](https://geocoder-php.org/) - A geocoding library.  * [GeoJSON](https://github.com/jmikola/geojson) - A GeoJSON implementation.  * [GeoTools](https://github.com/thephpleague/geotools) - A library of geo-related tools.  * [PHPGeo](https://github.com/mjaschen/phpgeo) - A simple geo library.    ### Date and Time  *Libraries for working with dates and times.*    * [CalendR](https://yohan.giarel.li/CalendR/) - A calendar management library.  * [Carbon](https://github.com/briannesbitt/Carbon) - A simple DateTime API extension.  * [Chronos](https://github.com/cakephp/chronos) - A DateTime API extension supporting both mutable and immutable date/time.  * [Moment.php](https://github.com/fightbulc/moment.php) - Moment.js inspired PHP DateTime handler with i18n support.  * [Yasumi](https://github.com/azuyalabs/yasumi) - An library to help you calculate the dates and names of holidays.    ### Event  *Libraries that are event-driven or implement non-blocking event loops.*  * [Amp](https://github.com/amphp/amp) - An event driven non-blocking I/O library.  * [Broadway](https://github.com/broadway/broadway) - An event source and CQRS library.  * [CakePHP Event](https://github.com/cakephp/event) - An event dispatcher library.  * [Elephant.io](https://github.com/Wisembly/Elephant.io) - Yet another web socket library.  * [Evenement](https://github.com/igorw/evenement) - An event dispatcher library.  * [Event](https://github.com/thephpleague/event) - An event library with a focus on domain events.  * [Hoa EventSource](https://github.com/hoaproject/Eventsource) - An event source library.  * [Hoa WebSocket](https://github.com/hoaproject/Websocket) - Another web socket library.  * [Pawl](https://github.com/ratchetphp/Pawl) - An asynchronous web socket client.  * [Prooph Event Store](https://github.com/prooph/event-store) - An event source component to persist event messages  * [PHP Defer](https://github.com/php-defer/php-defer) - Golang's defer statement for PHP.  * [Ratchet](https://github.com/ratchetphp/Ratchet) - A web socket library.  * [ReactPHP](https://github.com/reactphp/reactphp) - An event driven non-blocking I/O library.  * [RxPHP](https://github.com/ReactiveX/RxPHP) - A reactive extension library.  * [Swoole](https://github.com/swoole/swoole-src) - An event-driven asynchronous and concurrent networking communication framework with high performance for PHP written in C.  * [Workerman](https://github.com/walkor/Workerman) - An event driven non-blocking I/O library.    ### Logging  *Libraries for generating and working with log files.*    * [Monolog](https://github.com/Seldaek/monolog) - A comprehensive logger.    ### E-commerce  *Libraries and applications for taking payments and building online e-commerce stores.*    * [Money](https://github.com/moneyphp/money) - A PHP implementation of Fowler's money pattern.  * [Brick\Money](https://github.com/brick/money) - A money library for PHP, with support for contexts, cash roundings, currency conversion.  * [OmniPay](https://github.com/thephpleague/omnipay) - A framework agnostic multi-gateway payment processing library.  * [Payum](https://github.com/payum/payum) - A payment abstraction library.  * [Shopware](https://github.com/shopware/shopware) - Highly customizable e-commerce software  * [Swap](https://github.com/florianv/swap) - An exchange rates library.  * [Sylius](https://sylius.com/) - An open source e-commerce solution.    ### PDF  *Libraries and software for working with PDF files.*    * [Dompdf](https://github.com/dompdf/dompdf) - A HTML to PDF converter.  * [PHPPdf](https://github.com/psliwa/PHPPdf) - A library for generating PDFs and images from XML.  * [Snappy](https://github.com/KnpLabs/snappy) - A PDF and image generation library.  * [WKHTMLToPDF](https://github.com/wkhtmltopdf/wkhtmltopdf) - A tool to convert HTML to PDF.    ### Office  *Libraries for working with office suite documents.*    * [PHPPowerPoint](https://github.com/PHPOffice/PHPPresentation) - A library for working with Microsoft PowerPoint Presentations.  * [PHPWord](https://github.com/PHPOffice/PHPWord) - A library for working with Microsoft Word documents.  * [PHPSpreadsheet](https://github.com/PHPOffice/PhpSpreadsheet) - A pure PHP library for reading and writing spreadsheet files (successor of PHPExcel).  * [Spout](https://github.com/box/spout) - Read and write spreadsheet files (CSV, XLSX and ODS), in a fast and scalable way .    ### Database  *Libraries for interacting with databases using object-relational mapping (ORM) or datamapping techniques.*    * [Atlas.Orm](https://github.com/atlasphp/Atlas.Orm) - A data mapper implementation for your persistence model in PHP.  * [Aura.Sql](https://github.com/auraphp/Aura.Sql) - Provides an extension to the native PDO along with a profiler and connection locator.  * [Aura.SqlQuery](https://github.com/auraphp/Aura.SqlQuery) - Independent query builders for MySQL, PostgreSQL, SQLite, and Microsoft SQL Server.  * [Baum](https://github.com/etrepat/baum) - A nested set implementation for Eloquent.  * [CakePHP ORM](https://github.com/cakephp/orm) - Object-Relational Mapper, implemented using the DataMapper pattern.  * [Cycle ORM](https://github.com/cycle/orm) - PHP DataMapper, ORM.  * [Doctrine Extensions](https://github.com/Atlantic18/DoctrineExtensions) - A collection of Doctrine behavioural extensions.  * [Doctrine](https://www.doctrine-project.org/) - A comprehensive DBAL and ORM.  * [Laravel Eloquent](https://github.com/illuminate/database) - A simple ORM.  * [Pomm](https://github.com/chanmix51/Pomm) - An Object Model Manager for PostgreSQL.  * [ProxyManager](https://github.com/Ocramius/ProxyManager) - A set of utilities to generate proxy objects for data mappers.  * [RedBean](https://redbeanphp.com/index.php) - A lightweight, configuration-less ORM.  * [Slimdump](https://github.com/webfactory/slimdump) - An easy dumper tool for MySQL.  * [Spot2](https://github.com/spotorm/spot2) - A MySQL datamapper ORM.    ### Migrations  Libraries to help manage database schemas and migrations.    * [Doctrine Migrations](https://www.doctrine-project.org/projects/migrations.html) - A migration library for Doctrine.  * [Migrations](https://github.com/icomefromthenet/Migrations) - A migration management library.  * [Phinx](https://github.com/cakephp/phinx) - Another database migration library.  * [PHPMig](https://github.com/davedevelopment/phpmig) - Another migration management library.  * [Ruckusing](https://github.com/ruckus/ruckusing-migrations) - Database migrations for PHP ala ActiveRecord Migrations with support for MySQL, Postgres, SQLite.    ### NoSQL  *Libraries for working with ""NoSQL"" backends.*    * [PHPMongo](https://github.com/sokil/php-mongo) - A MongoDB ORM.  * [Predis](https://github.com/predis/predis) - A feature complete Redis library.    ### Queue  *Libraries for working with event and task queues.*    * [Bernard](https://github.com/bernardphp/bernard) - A multibackend abstraction library.  * [BunnyPHP](https://github.com/jakubkulhan/bunny) - A performant pure-PHP AMQP (RabbitMQ) sync and also async (ReactPHP) library.  * [Pheanstalk](https://github.com/pheanstalk/pheanstalk) - A Beanstalkd client library.  * [PHP AMQP](https://github.com/php-amqplib/php-amqplib) - A pure PHP AMQP library.  * [Tarantool Queue](https://github.com/tarantool-php/queue) - PHP bindings for Tarantool Queue.  * [Thumper](https://github.com/php-amqplib/Thumper) - A RabbitMQ pattern library.  * [Enqueue](https://github.com/php-enqueue/enqueue-dev) - A message queue packages for PHP that supports RabbitMQ, AMQP, STOMP, Amazon SQS, Redis and Doctrine transports.     ### Search  *Libraries and software for indexing and performing search queries on data.*    * [Elastica](https://github.com/ruflin/Elastica) - A client library for ElasticSearch.  * [ElasticSearch PHP](https://github.com/elastic/elasticsearch-php) - The official client library for [ElasticSearch](https://www.elastic.co/).  * [Solarium](https://www.solarium-project.org/) - A client library for [Solr](https://lucene.apache.org/solr/).  * [SphinxQL Query Builder](https://foolcode.github.io/SphinxQL-Query-Builder/) - A query library for the [Sphinx](https://sphinxsearch.com/) and [Manticore](https://manticoresearch.com/) search engines.    ### Command Line  *Libraries related to the command line.*    * [Aura.Cli](https://github.com/auraphp/Aura.Cli) - Provides the equivalent of request ( Context ) and response ( Stdio ) objects for the command line interface, including Getopt support, and an independent Help object for describing commands.  * [Boris](https://github.com/borisrepl/boris) - A tiny PHP REPL.  * [Cilex](https://github.com/Cilex/Cilex) - A micro framework for building command line tools.  * [CLI Menu](https://github.com/php-school/cli-menu) - A library for building CLI menus.  * [CLIFramework](https://github.com/c9s/CLIFramework) - A command-line framework supports zsh/bash completion generation, subcommands and option constraints. It also powers phpbrew.  * [CLImate](https://github.com/thephpleague/climate) - A library for outputting colours and special formatting.  * [Commando](https://github.com/nategood/commando) - Another simple command line opt parser.  * [Cron Expression](https://github.com/mtdowling/cron-expression) - A library to calculate cron run dates.  * [GetOpt](https://github.com/getopt-php/getopt-php) - A command line opt parser.  * [GetOptionKit](https://github.com/c9s/GetOptionKit) - Another command line opt parser.  * [Hoa Console](https://github.com/hoaproject/Console) - Another command line library.  * [PsySH](https://github.com/bobthecow/psysh) - Another PHP REPL.  * [ShellWrap](https://github.com/MrRio/shellwrap) - A simple command line wrapper library.    ### Authentication and Authorization  *Libraries for implementing user authentication and authorization.*    * [Aura.Auth](https://github.com/auraphp/Aura.Auth) - Provides authentication functionality and session tracking using various adapters.  * [SocialConnect Auth](https://github.com/socialConnect/auth) - An open source social sign (OAuth1\OAuth2\OpenID\OpenIDConnect).  * [Json Web Token](https://github.com/lcobucci/jwt) - Json Tokens to authenticate and transmit information.  * [OAuth 1.0 Client](https://github.com/thephpleague/oauth1-client) - An OAuth 1.0 client library.  * [OAuth 2.0 Client](https://github.com/thephpleague/oauth2-client) - An OAuth 2.0 client library.  * [OAuth2 Server](https://bshaffer.github.io/oauth2-server-php-docs/) - Another OAuth2 server implementation.  * [OAuth2 Server](https://oauth2.thephpleague.com/) - An OAuth2 authentication server, resource server and client library.  * [Opauth](https://github.com/opauth/opauth) - A multi-provider authentication framework.  * [Paseto](https://github.com/paragonie/paseto) - Platform-Agnostic Security Tokens.  * [PHP oAuthLib](https://github.com/Lusitanian/PHPoAuthLib) - Another OAuth library.  * [Sentinel Social](https://cartalyst.com/manual/sentinel-social/2.0) - A library for social network authentication.  * [Sentinel](https://cartalyst.com/manual/sentinel/2.0) - A framework agnostic authentication & authorisation library.  * [TwitterOAuth](https://github.com/abraham/twitteroauth) - A Twitter OAuth library.    ### Markup and CSS  *Libraries for working with markup and CSS formats.    * [Cebe Markdown](https://github.com/cebe/markdown) - An fast and extensible Markdown parser.  * [CommonMark PHP](https://github.com/thephpleague/commonmark) - Highly-extensible Markdown parser which fully supports the [CommonMark spec](https://spec.commonmark.org/).  * [Decoda](https://github.com/milesj/decoda) - A lightweight markup parser library.  * [Essence](https://github.com/essence/essence) - A library for extracting web media.  * [Embera](https://github.com/mpratt/Embera) - An Oembed consumer library.  * [HTML to Markdown](https://github.com/thephpleague/html-to-markdown) - Converts HTML into Markdown.  * [HTML5 PHP](https://github.com/Masterminds/html5-php) - An HTML5 parser and serializer library.  * [Parsedown](https://github.com/erusev/parsedown) - Another Markdown parser.  * [PHP CSS Parser](https://github.com/sabberworm/PHP-CSS-Parser) - A Parser for CSS Files written in PHP.  * [PHP Markdown](https://github.com/michelf/php-markdown) - A Markdown parser.  * [Shiki PHP](https://github.com/spatie/shiki-php) - A [Shiki](https://github.com/shikijs/shiki) code highlighting package in PHP.  * [VObject](https://github.com/sabre-io/vobject) - A library for parsing VCard and iCalendar objects.    ### JSON  *Libraries for working with JSON.*    * [JSON Lint](https://github.com/Seldaek/jsonlint) - A JSON lint utility.  * [JSONMapper](https://github.com/JsonMapper/JsonMapper) - A library for mapping JSON to PHP objects.    ### Strings  *Libraries for parsing and manipulating strings.*    * [Agent](https://github.com/jenssegers/agent) - A PHP desktop/mobile user agent parser, based on Mobiledetect.  * [ANSI to HTML5](https://github.com/sensiolabs/ansi-to-html) - An ANSI to HTML5 converter library.  * [Color Jizz](https://github.com/mikeemoo/ColorJizz-PHP) - A library for manipulating and converting colours.  * [Device Detector](https://github.com/matomo-org/device-detector) - Another library for parsing user agent strings.  * [Hoa String](https://github.com/hoaproject/Ustring) - Another UTF-8 string library.  * [Jieba-PHP](https://github.com/fukuball/jieba-php) - A PHP port of Python's jieba. Chinese text segmentation for natural language processing.  * [Mobile-Detect](https://github.com/serbanghita/Mobile-Detect) - A lightweight PHP class for detecting mobile devices (including tablets).  * [Patchwork UTF-8](https://github.com/nicolas-grekas/Patchwork-UTF8) - A portable library for working with UTF-8 strings.  * [Portable UTF-8](https://github.com/voku/portable-utf8) - A string manipulation library with UTF-8 safe replacement methods.  * [Slugify](https://github.com/cocur/slugify) - A library to convert strings to slugs.  * [SQL Formatter](https://github.com/jdorn/sql-formatter/) - A library for formatting SQL statements.  * [Stringy](https://github.com/voku/Stringy) - A string manipulation library with multibyte support.  * [UA Parser](https://github.com/tobie/ua-parser/tree/master/php) - A library for parsing user agent strings.  * [URLify](https://github.com/jbroadway/urlify) - A PHP port of Django's URLify.js.  * [UUID](https://github.com/ramsey/uuid) - A library for generating UUIDs.    ### Numbers  *Libraries for working with numbers.*    * [Brick\Math](https://github.com/brick/math) - A library providing large number support: `BigInteger`, `BigDecimal` and `BigRational`.  * [ByteUnits](https://github.com/gabrielelana/byte-units) - A library to parse, format and convert byte units in binary and metric systems.  * [DecimalObject](https://github.com/spryker/decimal-object) - A value object to handle decimals/floats easily and more precisely.  * [IP](https://github.com/darsyn/ip) - An immutable value object for working with IPv4 and IPv6 addresses.  * [LibPhoneNumber for PHP](https://github.com/giggsey/libphonenumber-for-php) - A PHP implementation of Google's phone number handling library.  * [PHP Conversion](https://github.com/Crisu83/php-conversion) - Another library for converting between units of measure.  * [PHP Units of Measure](https://github.com/triplepoint/php-units-of-measure) - A library for converting between units of measure.  * [MathPHP](https://github.com/markrogoyski/math-php) - A math library for PHP.     ### Filtering and Validation  *Libraries for filtering and validating data.*    * [Assert](https://github.com/beberlei/assert) - A validation library with a rich set of assertions. Supports assertion chaining and lazy assertions.  * [Aura.Filter](https://github.com/auraphp/Aura.Filter) - Provides tools to validate and sanitize objects and arrays.  * [CakePHP Validation](https://github.com/cakephp/validation) - Another validation library.  * [Filterus](https://github.com/ircmaxell/filterus) - A simple PHP filtering library.  * [ISO-codes](https://github.com/ronanguilloux/IsoCodes) - A library for validating inputs according standards from ISO, International Finance, Public Administrations, GS1, Book Industry, Phone numbers & Zipcodes for many countries.  * [JSON Schema](https://github.com/justinrainbow/json-schema) - A [JSON Schema](https://json-schema.org/) validation library.  * [MetaYaml](https://github.com/romaricdrigon/MetaYaml) - A schema validation library that supports YAML, JSON and XML.  * [Respect Validation](https://github.com/Respect/Validation) - A simple validation library.  * [Upload](https://github.com/brandonsavage/Upload) - A library for handling file uploads and validation.  * [Valitron](https://github.com/vlucas/valitron) - Another validation library.  * [Volan](https://github.com/serkin/Volan) - Another simplified validation library.    ### API  *Libraries and web tools for developing APIs.*    * [API Platform](https://api-platform.com ) - Expose in minutes an hypermedia REST API that embraces JSON-LD, Hydra format.  * [Laminas API Tool Skeleton](https://github.com/laminas-api-tools/api-tools-skeleton) - An API builder built with the Laminas Framework.  * [Drest](https://github.com/leedavis81/drest) - A library for exposing Doctrine entities as REST resource endpoints.  * [HAL](https://github.com/blongden/hal) - A Hypertext Application Language (HAL) builder library.  * [Hateoas](https://github.com/willdurand/Hateoas) - A HATEOAS REST web service library.  * [Negotiation](https://github.com/willdurand/Negotiation) - A content negotiation library.  * [Restler](https://github.com/Luracast/Restler) - A lightweight framework to expose PHP methods as RESTful web API.  * [wsdl2phpgenerator](https://github.com/wsdl2phpgenerator/wsdl2phpgenerator) - A tool to generate PHP classes from SOAP WSDL files.    ### Caching and Locking  *Libraries for caching data and acquiring locks.*    * [APIx Cache](https://github.com/apix/cache) - A thin PSR-6 cache wrapper to various caching backends emphasising cache tagging and indexing.  * [CacheTool](https://github.com/gordalina/cachetool) - A tool to clear APC/opcode caches from the command line.  * [CakePHP Cache](https://github.com/cakephp/cache) - A caching library.  * [Doctrine Cache](https://github.com/doctrine/cache) - A caching library.  * [Metaphore](https://github.com/sobstel/metaphore) - Cache slam defense using a semaphore to prevent dogpile effect.  * [Stash](https://github.com/tedious/Stash) - Another library for caching.  * [Laminas Cache](https://github.com/laminas/laminas-cache) - Another caching library.  * [Lock](https://github.com/php-lock/lock) - A lock library to provide exclusive execution.    ### Data Structure and Storage  *Libraries that implement data structure or storage techniques.*    * [CakePHP Collection](https://github.com/cakephp/collection) - A simple collections library.  * [Fractal](https://github.com/thephpleague/fractal) - A library for converting complex data structures to JSON output.  * [Ginq](https://github.com/akanehara/ginq) - Another PHP library based on .NET's LINQ.  * [JsonMapper](https://github.com/cweiske/jsonmapper) - A library that maps nested JSON structures onto PHP classes.  * [JSON Machine](https://github.com/halaxa/json-machine) - Provides iteration over huge JSONs using simple `foreach`  * [Knapsack](https://github.com/DusanKasan/Knapsack) - Collection library inspired by Clojure's sequences.  * [msgpack.php](https://github.com/rybakit/msgpack.php) - A pure PHP implementation of the [MessagePack](https://msgpack.org/) serialization format.  * [PINQ](https://github.com/TimeToogo/Pinq) - A PHP library based on .NET's LINQ (Language Integrated Query).  * [Serializer](https://github.com/schmittjoh/serializer) - A library for serialising and de-serialising data.  * [YaLinqo](https://github.com/Athari/YaLinqo) - Yet Another LINQ to Objects for PHP.  * [Laminas Serializer](https://github.com/laminas/laminas-serializer) - Another library for serialising and de-serialising data.    ### Notifications  *Libraries for working with notification software.*    * [JoliNotif](https://github.com/jolicode/JoliNotif) - A cross-platform library for desktop notification (support for Growl, notify-send, toaster, etc)  * [Notification Pusher](https://github.com/Ph3nol/NotificationPusher) - A standalone library for device push notifications.  * [Notificato](https://github.com/mac-cain13/notificato) - A library for handling push notifications.  * [Notificator](https://github.com/namshi/notificator) - A lightweight notification library.  * [Php-pushwoosh](https://github.com/gomoob/php-pushwoosh) - A PHP Library to easily send push notifications with the Pushwoosh REST Web Services.    ### Deployment  *Libraries for project deployment.*    * [Deployer](https://github.com/deployphp/deployer) - A deployment tool.  * [Envoy](https://github.com/laravel/envoy) - A tool to run SSH tasks with PHP.  * [Rocketeer](https://github.com/rocketeers/rocketeer) - A fast and easy deployer for the PHP world.    ### Internationalisation and Localisation  *Libraries for Internationalization (I18n) and Localization (L10n).*    * [Aura.Intl](https://github.com/auraphp/Aura.Intl) - Provides internationalization (I18N) tools, specifically package-oriented per-locale message translation.  * [CakePHP I18n](https://github.com/cakephp/i18n) - Message translation and localization for dates and numbers.    ### Serverless  *Libraries and tools to help build serverless web applications.*    * [Bref](https://bref.sh/) - Serverless PHP on AWS Lambda.  * [OpenWhisk](http://openwhisk.apache.org/) - An open-source serverless cloud platform.  * [Serverless Framework](https://www.serverless.com/open-source/) - An open-source framework for building serverless applications.  * [Laravel Vapor](https://vapor.laravel.com/) - A serverless deployment platform for Laravel, powered by AWS.    ## Configuration  *Libraries and tools for configuration.*    * [PHP Dotenv](https://github.com/vlucas/phpdotenv) - Parse and load environment variables from `.env` files.  * [Symfony Dotenv](https://github.com/symfony/dotenv)- Parse and load environment variables from `.env` files.  * [Yo! Symfony TOML](https://github.com/yosymfony/toml) - A PHP parser for [TOML](https://github.com/toml-lang/toml).     ### Third Party APIs  *Libraries for accessing third party APIs.*    * [Amazon Web Service SDK](https://github.com/aws/aws-sdk-php) - The official PHP AWS SDK library.  * [AsyncAWS](https://async-aws.com/) - An unofficial asynchronous PHP AWS SDK.  * [Campaign Monitor](https://campaignmonitor.github.io/createsend-php/) - The official Campaign Monitor PHP library.  * [Github](https://github.com/KnpLabs/php-github-api) - A library to interface with the Github API.  * [Mailgun](https://github.com/mailgun/mailgun-php) The official Mailgun PHP API.  * [Square](https://github.com/square/connect-php-sdk) - The official Square PHP SDK for payments and other Square APIs.  * [Stripe](https://github.com/stripe/stripe-php) - The official Stripe PHP library.  * [Twilio](https://github.com/twilio/twilio-php) - The official Twilio PHP REST API.    ### Extensions  *Libraries to help build PHP extensions.*    * [PHP CPP](https://www.php-cpp.com/) - A C++ library for developing PHP extensions.  * [Zephir](https://github.com/phalcon/zephir) - A compiled language between PHP and C++ for developing PHP extensions.    ### Miscellaneous  *Useful libraries or utilities that don't fit into the categories above.*    * [Annotations](https://github.com/doctrine/annotations) - An annotation library (part of Doctrine).  * [BotMan](https://github.com/botman/botman) - A framework agnostic PHP library to build cross-platform chat bots.  * [ClassPreloader](https://github.com/ClassPreloader/ClassPreloader) - A library for optimising autoloading.  * [Hprose-PHP](https://github.com/hprose/hprose-php) - A cross-language RPC.  * [noCAPTCHA](https://github.com/ARCANEDEV/noCAPTCHA) - Helper for Google's noCAPTCHA (reCAPTCHA).  * [Pagerfanta](https://github.com/whiteoctober/Pagerfanta) - A pagination library.  * [Safe](https://github.com/thecodingmachine/safe) - All PHP functions, rewritten to throw exceptions instead of returning false.  * [SuperClosure](https://github.com/jeremeamia/super_closure) - A library that allows Closures to be serialized.    # Software  *Software for creating a development environment.*    ### PHP Installation  *Tools to help install and manage PHP on your computer.*    * [Brew PHP Switcher](https://github.com/philcook/brew-php-switcher) - Brew PHP switcher.  * [HomeBrew](https://brew.sh/) - A package manager for OSX.  * [Laravel Valet](https://laravel.com/docs/master/valet) - A development environment for macOS.  * [PHP Brew](https://github.com/phpbrew/phpbrew) - A PHP version manager and installer.  * [PHP Build](https://github.com/php-build/php-build) - Another PHP version installer.  * [PHP OSX](https://php-osx.liip.ch/) - A PHP installer for OSX.    ### Development Environment  *Software and tools for creating and sharing a development environment.*    * [Ansible](https://www.ansible.com/) - A radically simple orchestration framework.  * [Docker](https://www.docker.com/) - A containerization platform.  * [Docker PHP Extension Installer](https://github.com/mlocati/docker-php-extension-installer) - Easily install PHP extensions in Docker containers.  * [Expose](https://github.com/beyondcode/expose) - An open source PHP tunneling service.  * [Lando](https://lando.dev/) - Push-button development environments.  * [Laravel Homestead](https://laravel.com/docs/master/homestead) - A local development environment for Laravel.   * [Laradock](http://laradock.io/) - A full PHP development environment based on Docker.  * [Puppet](https://puppet.com/) - A server automation framework and application.  * [Takeout](https://github.com/tighten/takeout) - A Docker-based development-only dependency manager.  * [Vagrant](https://www.vagrantup.com/) - A portable development environment utility.    ### Virtual Machines  *Alternative PHP virtual machines.*    * [Hack](https://hacklang.org/) - A programming language for HHVM.  * [HHVM](https://github.com/facebook/hhvm) - A Virtual Machine, Runtime and JIT for PHP by Facebook.  * [PeachPie](https://github.com/peachpiecompiler/peachpie) - PHP compiler and runtime for .NET and .NET Core.    ### Text Editors and IDEs  *Text Editors and Integrated Development Environments (IDE) with support for PHP.*    * [Eclipse for PHP Developers](https://www.eclipse.org/downloads/) - A PHP IDE based on the Eclipse platform.  * [Apache NetBeans](https://netbeans.apache.org/) - An IDE with support for PHP and HTML5.  * [PhpStorm](https://www.jetbrains.com/phpstorm/) - A commercial PHP IDE.  * [VS Code](https://code.visualstudio.com/) - An open source code editor.    ### Web Applications  *Web-based applications and tools.*    * [3V4L](https://3v4l.org/) - An online PHP & HHVM shell.  * [Adminer](https://www.adminer.org/) - Database management in a single PHP file.  * [Cachet](https://github.com/cachethq/cachet) - The open source status page system.  * [DBV](https://github.com/victorstanciu/dbv) - A database version control application.  * [Lychee](https://github.com/electerious/Lychee) - An easy to use and great looking photo-management-system.  * [MailCatcher](https://github.com/sj26/mailcatcher) - A web tool for capturing and viewing emails.  * [phpMyAdmin](https://github.com/phpmyadmin/phpmyadmin) - A web interface for MySQL/MariaDB.  * [PHP Queue](https://github.com/CoderKungfu/php-queue) - An application for managing queueing backends.  * [phpRedisAdmin](https://github.com/ErikDubbelboer/phpRedisAdmin) - A simple web interface to manage [Redis](https://redis.io/) databases.  * [PHPSandbox](https://phpsandbox.io) - An online IDE for PHP in the browser.    ### Infrastructure  *Infrastructure for providing PHP applications and services.*    * [appserver.io](https://github.com/appserver-io/appserver) - A multithreaded application server for PHP, written in PHP.  * [php-pm](https://github.com/php-pm/php-pm) - A process manager, supercharger and load balancer for PHP applications.  * [RoadRunner](https://github.com/spiral/roadrunner) - High-performance PHP application server, load-balancer and process manager.    # Resources  Various resources, such as books, websites and articles, for improving your PHP development skills and knowledge.    ### PHP Websites  *Useful PHP-related websites.*    * [libs.garden: PHP](https://libs.garden/php) - An overview of fastest growing PHP libraries.  * [Nomad PHP](https://nomadphp.com/) - A online PHP learning resource.  * [Laravel News](https://laravel-news.com/) - The official Laravel blog.  * [PHP Annotated Monthly](https://blog.jetbrains.com/phpstorm/category/php-annotated-monthly/) - A monthly digest of PHP news.  * [PHP Best Practices](https://phpbestpractices.org/) - A PHP best practice guide.  * [PHP FIG](https://www.php-fig.org/) - The PHP Framework Interoperability Group.  * [PHP Package Development Standards](http://php-pds.com) - Package development standards for PHP.  * [PHP School](https://www.phpschool.io/) - Open Source Learning for PHP.  * [PHP Security](https://phpsecurity.readthedocs.io/en/latest/index.html) - A guide to PHP security.  * [PHP The Right Way](https://phptherightway.com/) - A PHP best practice quick reference guide.  * [PHP UG](https://php.ug) - A website to help people locate their nearest PHP user group (UG).  * [PHP Versions](http://phpversions.info/) - Lists which versions of PHP are available on several popular web hosts.  * [PHP Watch](https://php.watch/) - PHP articles, news, upcoming changes, RFCs and more.  * [PHP Weekly](http://www.phpweekly.com/archive.html) - A weekly PHP newsletter.  * [Securing PHP](https://www.securingphp.com/) - A newsletter about PHP security and library recommendations.  * [Seven PHP](https://7php.com/) - A website that interviews members of the PHP community.    ### PHP Books  *Fantastic PHP-related books.*    * [Domain-Driven Design in PHP](https://leanpub.com/ddd-in-php) - Real examples written in PHP showcasing DDD Architectural Styles.  * [Functional Programming in PHP](https://www.functionalphp.com/) - This book will show you how to leverage these new PHP5.3+ features by understanding functional programming principles  * [Grumpy PHPUnit](https://leanpub.com/grumpy-phpunit) - A book about unit testing with PHPUnit by Chris Hartjes.  * [Mastering Object-Orientated PHP](https://www.brandonsavage.net/) - A book about object-orientated PHP by Brandon Savage.  * [Modern PHP New Features and Good Practices](https://www.oreilly.com/library/view/~/9781491905173/) - A book about new PHP features and best practices by Josh Lockhart.  * [Modernizing Legacy Applications in PHP](https://leanpub.com/mlaphp) - A book about modernizing legacy PHP applications by Paul M. Jones.  * [PHP 7 Upgrade Guide](https://leanpub.com/php7) - An ebook covering all of the features and changes in PHP 7 by Colin O'Dell.  * [PHP Pandas](https://daylerees.com/php-pandas/) - A book about learning to write PHP by Dayle Rees.  * [Scaling PHP Applications](https://www.scalingphpbook.com) - An ebook about scaling PHP applications by Steve Corona.  * [Securing PHP: Core Concepts](https://leanpub.com/securingphp-coreconcepts) - A book about common security terms and practices for PHP by Chris Cornutt.  * [Signaling PHP](https://leanpub.com/signalingphp) - A book about catching PCNTL signals in CLI scripts by Cal Evans.  * [The Grumpy Programmer's Guide to Building Testable PHP Applications](https://leanpub.com/grumpy-testing) - A book about building testing PHP applications by Chris Hartjes.  * [XML Parsing with PHP](https://www.phparch.com/books/xml-parsing-with-php/) - This book covers parsing and validating XML documents, leveraging XPath expressions, and working with namespaces as well as how to create and modify XML files programmatically.    ### PHP Videos  *Fantastic PHP-related videos.*    * [Nomad PHP Lightning Talks](https://www.youtube.com/c/nomadphp) - 10 to 15 minute Lightning Talks by PHP community members.  * [PHP UK Conference](https://www.youtube.com/user/phpukconference/videos) - A collection of videos from the PHP UK Conference.  * [Programming with Anthony](https://www.youtube.com/playlist?list=PLM-218uGSX3DQ3KsB5NJnuOqPqc5CW2kW) - A video series by Anthony Ferrara.  * [Taking PHP Seriously](https://www.infoq.com/presentations/php-history/) - A talk outlining PHP's strengths by Keith Adams of Facebook.  * [Laracasts](https://laracasts.com) - Screencasts about Laravel, Vue JS and more.  * [Laravel YouTube Channel](https://www.youtube.com/channel/UCfO2GiQwb-cwJTb1CuRSkwg) - The official Laravel YouTube channel.  * [SymfonyCasts](https://symfonycasts.com/) - Screencasts and tutorials about PHP and Symfony.    ### PHP Podcasts  *Podcasts with a focus on PHP topics.*    * [Laravel Podcast](https://laravelpodcast.com/) - Laravel and PHP development news and discussion.  * [PHP Internals News](https://phpinternals.news) - A podcast about PHP internals.  * [PHP Roundtable](https://www.phproundtable.com/) - The PHP Roundtable is a casual gathering of developers discussing topics that PHP nerds care about.  * [PHP Town Hall](https://phptownhall.com/) - A casual PHP podcast by Ben Edmunds and Phil Sturgeon.  * [Voices of the ElePHPant](https://voicesoftheelephpant.com/) Interviews with the people that make the PHP community special.    ### PHP Newsletters  *PHP-related news directly to your inbox.*    * [PHP Weekly](http://www.phpweekly.com/) - A weekly newsletter about PHP.    ### PHP Reading  *PHP-releated reading materials.*    * [php[architect]](https://www.phparch.com/magazine/) - A monthly magazine dedicated to PHP.    ### PHP Internals Reading  *Reading materials related to the PHP internals or performance.*    * [PHP RFCs](https://wiki.php.net/rfc) - The home of PHP RFCs (Request for Comments).  * [Externals](https://externals.io/) - PHP internal discussions.   * [PHP RFC Watch](https://php-rfc-watch.beberlei.de/) - Watch the latest PHP [RFCs](https://wiki.php.net/rfc).  * [PHP Internals Book](http://www.phpinternalsbook.com) - An online book about PHP internals, written by three core developers. """
Big data;https://github.com/facebookincubator/beringei;"""** THIS REPO HAS BEEN ARCHIVED AND IS NO LONGER BEING ACTIVELY MAINTAINED **    # Beringei [![CircleCI](https://circleci.com/gh/facebookincubator/beringei/tree/master.svg?style=svg)](https://circleci.com/gh/facebookincubator/beringei/tree/master)  A high performance, in memory time series storage engine    <img src=""./beringei_logo_clear.png"" height=200 width=200>    In the fall of 2015, we published the [paper “Gorilla: A Fast, Scalable, In-Memory Time Series Database”](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf) at VLDB 2015. Beringei is the open source representation of the ideas presented in this paper.    Beringei is a high performance time series storage engine. Time series are commonly used as a representation of statistics, gauges, and counters for monitoring performance and health of a system.     ## Features    Beringei has the following features:    * Support for very fast, in-memory storage, backed by disk for persistence. Queries to the storage engine are always served out of memory for extremely fast query performance, but backed to disk so the process can be restarted or migrated with very little down time and no data loss.  * Extremely efficient streaming compression algorithm. Our streaming compression algorithm is able to compress real world time series data by over 90%. The delta of delta compression algorithm used by Beringei is also fast - we see that a single machine is able to compress more than 1.5 million datapoints/second.  * Reference sharded service implementation, including a client implementation.  * Reference http service implementation that enables direct Grafana integration.    ## How can I use Beringei?    Beringei can be used in one of two ways.     1. We have created a simple, sharded service, and reference client implementation, that can store and serve  time series query requests.   1. You can use Beringei as an embedded library to handle the low-level details of efficiently storing time series data. Using Beringei in this way is similar to [RocksDB](https://rocksdb.org) - the Beringei library can be the high performance storage system underlying your performance monitoring solution.      ## Requirements    Beringei is tested and working on:    * Ubuntu 16.10    We also depend on these open source projects:    * [fbthrift](https://github.com/facebook/fbthrift)  * [folly](https://github.com/facebook/folly)  * [wangle](https://github.com/facebook/wangle)  * [proxygen](https://github.com/facebook/proxygen)  * [gtest](https://github.com/google/googletest)  * [gflags](https://github.com/gflags/gflags)    ## Building Beringei    Our instructions are for Ubuntu 16.10 - but you will probably be able to modify  the install scripts and directions to work with other linux distros.    - Run `sudo ./setup_ubuntu.sh`.    - Build beringei.    ```  mkdir build && cd build && cmake .. && make  ```    - Generate a beringei configuration file.    ```  ./beringei/tools/beringei_configuration_generator --host_names $(hostname) --file_path /tmp/beringei.json  ```    - Start beringei.    ```  ./beringei/service/beringei_main \      -beringei_configuration_path /tmp/beringei.json \      -create_directories \      -sleep_between_bucket_finalization_secs 60 \      -allowed_timestamp_behind 300 \      -bucket_size 600 \      -buckets $((86400/600)) \      -logtostderr \      -v=2  ```    - Send data.    ```  while [[ 1 ]]; do      ./beringei/tools/beringei_put \          -beringei_configuration_path /tmp/beringei.json \          testkey ${RANDOM} \          -logtostderr -v 3      sleep 30  done  ```    - Read the data back.    ```  ./beringei/tools/beringei_get \      -beringei_configuration_path /tmp/beringei.json \      testkey \      -logtostderr -v 3  ```    ## License    Beringei is BSD-licensed. We also provide an additional patent grant. """
Big data;https://github.com/Netflix/suro;"""# Suro: Netflix's Data Pipeline    Suro is a data pipeline service for collecting, aggregating, and dispatching large volume of application events including log data. It has the following features:    - It is distributed and can be horizontally scaled.  - It supports streaming data flow, large number of connections, and high throughput.  - It allows dynamically dispatching events to different locations with flexible dispatching rules.  - It has a simple and flexible architecture to allow users to add additional data destinations.  - It fits well into NetflixOSS ecosystem  - It is a best-effort data pipeline with support of flexible retries and store-and-forward to minimize message loss    Learn more about Suro on the <a href=""https://github.com/Netflix/suro/wiki"">Suro Wiki</a> and the <a href=""http://techblog.netflix.com/2013/12/announcing-suro-backbone-of-netflixs.html"">Netflix TechBlog post</a> where Suro was introduced.    ## Master Build Status    <a href='https://netflixoss.ci.cloudbees.com/job/suro-master/'><img src='https://netflixoss.ci.cloudbees.com/job/suro-master/badge/icon'></a>    ## Pull Request Build Status    <a href='https://netflixoss.ci.cloudbees.com/job/suro-pull-requests/'><img src='https://netflixoss.ci.cloudbees.com/job/suro-pull-requests/badge/icon'></a></img></a></img></a>    Build  -----    NetflixGraph is built via Gradle (www.gradle.org). To build from the command line:        ./gradlew build    See the `build.gradle` file for other gradle targets, like `distTar`, `distZip`, `installApp`, and `runServer`.    Running the server  ------------------    You can run the server locally by just running `./gradlew runServer`.    More more advanced usage you may wish to run `./gradlew installApp` and then:    	cd suro-server  	java -cp ""build/install/suro-server/lib/*"" com.netflix.suro.SuroServer -m conf/routingmap.json -s conf/sink.json -i conf/input.json    To enable basic logging you can downloaded `slf4j-simple-1.7.7.jar` and copy it into `suro-server` then run:    	cd suro-server  	java -cp ""build/install/suro-server/lib/*:slf4j-simple-1.7.7.jar"" com.netflix.suro.SuroServer -m conf/routingmap.json -s conf/sink.json -i conf/input.json    Support  -----    We will use the Google Group, Suro Users, to discuss issues: https://groups.google.com/forum/#!forum/suro-users """
Big data;https://github.com/krestenkrab/hanoidb;"""# HanoiDB Indexed Key/Value Storage    [![Build Status](https://travis-ci.org/krestenkrab/hanoidb.svg?branch=master)](https://travis-ci.org/krestenkrab/hanoidb)    HanoiDB implements an indexed, key/value storage engine.  The primary index is  a log-structured merge tree (LSM-BTree) implemented using ""doubling sizes""  persistent ordered sets of key/value pairs, similar is some regards to  [LevelDB](http://code.google.com/p/leveldb/).  HanoiDB includes a visualizer  which when used to watch a living database resembles the ""Towers of Hanoi""  puzzle game, which inspired the name of this database.    ## Features  - Insert, Delete and Read all have worst case *O*(log<sub>2</sub>(*N*)) latency.  - Incremental space reclaimation: The cost of evicting stale key/values    is amortized into insertion    - you don't need a separate eviction thread to keep memory use low    - you don't need to schedule merges to happen at off-peak hours  - Operations-friendly ""append-only"" storage    - allows you to backup live system    - crash-recovery is very fast and the logic is straight forward    - all data subject to CRC32 checksums    - data can be compressed on disk to save space  - Efficient range queries    - Riak secondary indexing    - Fast key and bucket listing  - Uses bloom filters to avoid unnecessary lookups on disk  - Time-based expiry of data    - configure the database to expire data older than n seconds    - specify a lifetime in seconds for any particular key/value pair  - Efficient resource utilization    - doesn't store all keys in memory    - uses a modest number of file descriptors proportional to the number of levels    - I/O is generally balanced between random and sequential    - low CPU overhead  - ~2000 lines of pure Erlang code in src/*.erl    HanoiDB is developed by Trifork, a Riak expert solutions provider, and Basho  Technologies, makers of Riak.  HanoiDB can be used in Riak via the  `riak_kv_tower_backend` repository.    ### Configuration options    Put these values in your `app.config` in the `hanoidb` section    ```erlang   {hanoidb, [            {data_root, ""./data/hanoidb""},              %% Enable/disable on-disk compression.            %%            {compress, none | gzip},              %% Expire (automatically delete) entries after N seconds.            %% When this value is 0 (zero), entries never expire.            %%            {expiry_secs, 0},              %% Sync strategy `none' only syncs every time the            %% nursery runs full, which is currently hard coded            %% to be evert 256 inserts or deletes.            %%            %% Sync strategy `sync' will sync the nursery log            %% for every insert or delete operation.            %%            {sync_strategy, none | sync | {seconds, N}},              %% The page size is a minimum page size, when a page fills            %% up to beyond this size, it is written to disk.            %% Compression applies to such units of page size.            %%            {page_size, 8192},              %% Read/write buffer sizes apply to merge processes.            %% A merge process has two read buffers and a write            %% buffer, and there is a merge process *per level* in            %% the database.            %%            {write_buffer_size, 524288},  % 512kB            {read_buffer_size, 524288},  % 512kB              %% The merge strategy is one of `fast' or `predictable'.            %% Both have same log2(N) worst case, but `fast' is            %% sometimes faster; yielding latency fluctuations.            %%            {merge_strategy, fast | predictable},              %% ""Level0"" files has 2^N KVs in it, defaulting to 1024.            %% If the database is to contain very small KVs, this is            %% likely too small, and will result in many unnecessary            %% file operations.  (Subsequent levels double in size).            {top_level, 10}  % 1024 Key/Values           ]},  ```      ### Contributors    - Kresten Krab Thorup @krestenkrab  - Greg Burd @gburd  - Jesper Louis Andersen @jlouis  - Steve Vinoski @vinoski  - Erik Søe Sørensen, @eriksoe  - Yamamoto Takashi @yamt  - Joseph Wayne Norton @norton """
Big data;https://github.com/IBMStreams/streamsx.topology;"""# streamsx.topology  A project that supports building streaming topologies (applications)  for IBM Streams in different programming languages, such as Python, Java, and Scala.  http://ibmstreams.github.io/streamsx.topology/      ## Changes  [CHANGELOG.md](com.ibm.streamsx.topology/CHANGELOG.md)    ## Development  [DEVELOPMENT.md](DEVELOPMENT.md)    ## Testing  [TESTING.md](TESTING.md) """
Big data;https://github.com/gojek/feast;"""<!--Do not modify this file. It is auto-generated from a template (infra/templates/README.md.jinja2)-->    <p align=""center"">      <a href=""https://feast.dev/"">        <img src=""docs/assets/feast_logo.png"" width=""550"">      </a>  </p>  <br />    [![unit-tests](https://github.com/feast-dev/feast/actions/workflows/unit_tests.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/unit_tests.yml)  [![integration-tests-and-build](https://github.com/feast-dev/feast/actions/workflows/master_only.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/master_only.yml)  [![java-integration-tests](https://github.com/feast-dev/feast/actions/workflows/java_master_only.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/java_master_only.yml)  [![linter](https://github.com/feast-dev/feast/actions/workflows/linter.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/linter.yml)  [![Docs Latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://docs.feast.dev/)  [![Python API](https://img.shields.io/readthedocs/feast/master?label=Python%20API)](http://rtd.feast.dev/)  [![License](https://img.shields.io/badge/License-Apache%202.0-blue)](https://github.com/feast-dev/feast/blob/master/LICENSE)  [![GitHub Release](https://img.shields.io/github/v/release/feast-dev/feast.svg?style=flat&sort=semver&color=blue)](https://github.com/feast-dev/feast/releases)    ## Overview    Feast is an open source feature store for machine learning. Feast is the fastest path to productionizing analytic data for model training and online inference.    Please see our [documentation](https://docs.feast.dev/) for more information about the project.    ## 📐 Architecture  ![](docs/assets/feast-marchitecture.png)    The above architecture is the minimal Feast deployment. Want to run the full Feast on Snowflake/GCP/AWS? Click [here](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws).    ## 🐣 Getting Started    ### 1. Install Feast  ```commandline  pip install feast  ```    ### 2. Create a feature repository  ```commandline  feast init my_feature_repo  cd my_feature_repo  ```    ### 3. Register your feature definitions and set up your feature store  ```commandline  feast apply  ```    ### 4. Explore your data in the web UI (experimental)    ![Web UI](ui/sample.png)    ### 5. Build a training dataset  ```python  from feast import FeatureStore  import pandas as pd  from datetime import datetime    entity_df = pd.DataFrame.from_dict({      ""driver_id"": [1001, 1002, 1003, 1004],      ""event_timestamp"": [          datetime(2021, 4, 12, 10, 59, 42),          datetime(2021, 4, 12, 8,  12, 10),          datetime(2021, 4, 12, 16, 40, 26),          datetime(2021, 4, 12, 15, 1 , 12)      ]  })    store = FeatureStore(repo_path=""."")    training_df = store.get_historical_features(      entity_df=entity_df,      features = [          'driver_hourly_stats:conv_rate',          'driver_hourly_stats:acc_rate',          'driver_hourly_stats:avg_daily_trips'      ],  ).to_df()    print(training_df.head())    # Train model  # model = ml.fit(training_df)  ```  ```commandline              event_timestamp  driver_id  conv_rate  acc_rate  avg_daily_trips  0 2021-04-12 08:12:10+00:00       1002   0.713465  0.597095              531  1 2021-04-12 10:59:42+00:00       1001   0.072752  0.044344               11  2 2021-04-12 15:01:12+00:00       1004   0.658182  0.079150              220  3 2021-04-12 16:40:26+00:00       1003   0.162092  0.309035              959    ```    ### 6. Load feature values into your online store  ```commandline  CURRENT_TIME=$(date -u +""%Y-%m-%dT%H:%M:%S"")  feast materialize-incremental $CURRENT_TIME  ```    ```commandline  Materializing feature view driver_hourly_stats from 2021-04-14 to 2021-04-15 done!  ```    ### 7. Read online features at low latency  ```python  from pprint import pprint  from feast import FeatureStore    store = FeatureStore(repo_path=""."")    feature_vector = store.get_online_features(      features=[          'driver_hourly_stats:conv_rate',          'driver_hourly_stats:acc_rate',          'driver_hourly_stats:avg_daily_trips'      ],      entity_rows=[{""driver_id"": 1001}]  ).to_dict()    pprint(feature_vector)    # Make prediction  # model.predict(feature_vector)  ```  ```json  {      ""driver_id"": [1001],      ""driver_hourly_stats__conv_rate"": [0.49274],      ""driver_hourly_stats__acc_rate"": [0.92743],      ""driver_hourly_stats__avg_daily_trips"": [72]  }  ```    ## 📦 Functionality and Roadmap    The list below contains the functionality that contributors are planning to develop for Feast    * Items below that are in development (or planned for development) will be indicated in parentheses.  * We welcome contribution to all items in the roadmap!  * Want to influence our roadmap and prioritization? Submit your feedback to [this form](https://docs.google.com/forms/d/e/1FAIpQLSfa1nRQ0sKz-JEFnMMCi4Jseag\_yDssO\_3nV9qMfxfrkil-wA/viewform).  * Want to speak to a Feast contributor? We are more than happy to jump on a call. Please schedule a time using [Calendly](https://calendly.com/d/x2ry-g5bb/meet-with-feast-team).    * **Data Sources**    * [x] [Snowflake source](https://docs.feast.dev/reference/data-sources/snowflake)    * [x] [Redshift source](https://docs.feast.dev/reference/data-sources/redshift)    * [x] [BigQuery source](https://docs.feast.dev/reference/data-sources/bigquery)    * [x] [Parquet file source](https://docs.feast.dev/reference/data-sources/file)    * [x] [Synapse source (community plugin)](https://github.com/Azure/feast-azure)    * [x] [Hive (community plugin)](https://github.com/baineng/feast-hive)    * [x] [Postgres (community plugin)](https://github.com/nossrannug/feast-postgres)    * [x] [Spark (community plugin)](https://github.com/Adyen/feast-spark-offline-store)    * [x] Kafka source (with [push support into the online store](https://docs.feast.dev/reference/alpha-stream-ingestion))    * [ ] HTTP source  * **Offline Stores**    * [x] [Snowflake](https://docs.feast.dev/reference/offline-stores/snowflake)    * [x] [Redshift](https://docs.feast.dev/reference/offline-stores/redshift)    * [x] [BigQuery](https://docs.feast.dev/reference/offline-stores/bigquery)    * [x] [Synapse (community plugin)](https://github.com/Azure/feast-azure)    * [x] [Hive (community plugin)](https://github.com/baineng/feast-hive)    * [x] [Postgres (community plugin)](https://github.com/nossrannug/feast-postgres)    * [x] [Trino (community plugin)](https://github.com/Shopify/feast-trino)    * [x] [Spark (community plugin)](https://github.com/Adyen/feast-spark-offline-store)    * [x] [In-memory / Pandas](https://docs.feast.dev/reference/offline-stores/file)    * [x] [Custom offline store support](https://docs.feast.dev/how-to-guides/adding-a-new-offline-store)  * **Online Stores**    * [x] [DynamoDB](https://docs.feast.dev/reference/online-stores/dynamodb)    * [x] [Redis](https://docs.feast.dev/reference/online-stores/redis)    * [x] [Datastore](https://docs.feast.dev/reference/online-stores/datastore)    * [x] [SQLite](https://docs.feast.dev/reference/online-stores/sqlite)    * [x] [Azure Cache for Redis (community plugin)](https://github.com/Azure/feast-azure)    * [x] [Postgres (community plugin)](https://github.com/nossrannug/feast-postgres)    * [x] [Custom online store support](https://docs.feast.dev/how-to-guides/adding-support-for-a-new-online-store)    * [ ] Bigtable (in progress)    * [ ] Cassandra  * **Streaming**    * [x] [Custom streaming ingestion job support](https://docs.feast.dev/how-to-guides/creating-a-custom-provider)    * [x] [Push based streaming data ingestion](reference/alpha-stream-ingestion.md)    * [ ] Streaming ingestion on AWS    * [ ] Streaming ingestion on GCP  * **Feature Engineering**    * [x] On-demand Transformations (Alpha release. See [RFC](https://docs.google.com/document/d/1lgfIw0Drc65LpaxbUu49RCeJgMew547meSJttnUqz7c/edit#))    * [ ] Batch transformation (SQL. In progress. See [RFC](https://docs.google.com/document/d/1964OkzuBljifDvkV-0fakp2uaijnVzdwWNGdz7Vz50A/edit))    * [ ] Streaming transformation  * **Deployments**    * [x] AWS Lambda (Alpha release. See [RFC](https://docs.google.com/document/d/1eZWKWzfBif66LDN32IajpaG-j82LSHCCOzY6R7Ax7MI/edit))    * [x] Kubernetes (See [guide](https://docs.feast.dev/how-to-guides/running-feast-in-production#4.3.-java-based-feature-server-deployed-on-kubernetes))    * [ ] Cloud Run    * [ ] KNative  * **Feature Serving**    * [x] Python Client    * [x] REST Feature Server (Python) (Alpha release. See [RFC](https://docs.google.com/document/d/1iXvFhAsJ5jgAhPOpTdB3j-Wj1S9x3Ev\_Wr6ZpnLzER4/edit))    * [x] gRPC Feature Server (Java) (See [#1497](https://github.com/feast-dev/feast/issues/1497))    * [x] Push API    * [ ] Java Client    * [ ] Go Client    * [ ] Delete API    * [ ] Feature Logging (for training)  * **Data Quality Management (See [RFC](https://docs.google.com/document/d/110F72d4NTv80p35wDSONxhhPBqWRwbZXG4f9mNEMd98/edit))**    * [x] Data profiling and validation (Great Expectations)    * [ ] Training-serving skew detection (in progress)    * [ ] Metric production    * [ ] Drift detection  * **Feature Discovery and Governance**    * [x] Python SDK for browsing feature registry    * [x] CLI for browsing feature registry    * [x] Model-centric feature tracking (feature services)    * [x] Amundsen integration (see [Feast extractor](https://github.com/amundsen-io/amundsen/blob/main/databuilder/databuilder/extractor/feast_extractor.py))    * [ ] Feast Web UI (in progress)    * [ ] REST API for browsing feature registry    * [ ] Feature versioning      ## 🎓 Important Resources    Please refer to the official documentation at [Documentation](https://docs.feast.dev/)   * [Quickstart](https://docs.feast.dev/getting-started/quickstart)   * [Tutorials](https://docs.feast.dev/tutorials/tutorials-overview)   * [Running Feast with Snowflake/GCP/AWS](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws)   * [Change Log](https://github.com/feast-dev/feast/blob/master/CHANGELOG.md)   * [Slack (#Feast)](https://slack.feast.dev/)    ## 👋 Contributing  Feast is a community project and is still under active development. Please have a look at our contributing and development guides if you want to contribute to the project:  - [Contribution Process for Feast](https://docs.feast.dev/project/contributing)  - [Development Guide for Feast](https://docs.feast.dev/project/development-guide)  - [Development Guide for the Main Feast Repository](./CONTRIBUTING.md)    ## ✨ Contributors    Thanks goes to these incredible people:    <a href=""https://github.com/feast-dev/feast/graphs/contributors"">    <img src=""https://contrib.rocks/image?repo=feast-dev/feast"" />  </a>"""
Big data;https://github.com/NationalSecurityAgency/timely;"""![Timely](timely-readme-logo.png)    [![Apache License][li]][ll]    Timely is a time series database application that provides secure access to time series data. Timely is written in Java and designed to work with [Apache Accumulo](https://accumulo.apache.org/) and [Grafana](https://www.grafana.com). Documentation is located [here](https://nationalsecurityagency.github.io/timely/).    [li]: https://img.shields.io/badge/license-ASL-blue.svg  [ll]: https://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/mesosphere/marathon;"""# Project Status    Support for DC/OS ends on October 31, 2021. We will continue to provide support for our current DC/OS customers per their contracts, of course. However, we will no longer be investing in new features or capabilities or maintaining the related repositories. If a customer wishes to continue use of the DC/OS Enterprise platform or other non-free DC/OS components, the customer can purchase an End-of-Life License or Perpetual Use License, however support is not included in these licenses and continued use of DC/OS will be at your own discretion and risk. We apologize for any inconvenience this may have caused.    We want to thank all of our loyal customers, particularly those DC/OS users who were fellow pioneers in the growth of the cloud native landscape from the beginning.    # [Marathon](https://mesosphere.github.io/marathon/) [![Build Status](https://jenkins.mesosphere.com/service/jenkins/buildStatus/icon?job=marathon-pipelines/master)](https://jenkins.mesosphere.com/service/jenkins/buildStatus/icon?job=marathon-pipelines/master) [![Issues](https://img.shields.io/badge/Issues-JIRA-ff69b4.svg?style=flat)](https://jira.mesosphere.com/projects/MARATHON/issues/)    Marathon is a production-proven [Apache Mesos][Mesos] framework for container orchestration. [DC/OS](https://dcos.io/get-started/#marathon) is the easiest way to start using Marathon. Issues are tracked in [JIRA](https://jira.mesosphere.com/projects/MARATHON/issues/).    Marathon provides a  [REST API](https://mesosphere.github.io/marathon/docs/rest-api.html) for  starting, stopping, and scaling applications. Marathon is written in Scala and  can run in highly-available mode by running multiple copies. The  state of running tasks gets stored in the Mesos state abstraction.    Marathon is also used as a *meta framework*: you can start other Mesos frameworks such as  Chronos or [Storm][Storm] with it to ensure they survive machine failures.  It can launch anything that can be launched in a standard shell. In fact, you  can even start other Marathon instances via Marathon.    ## Features    * *HA* - run any number of Marathon schedulers, but only one gets elected as      leader; if you access a non-leader, your request gets proxied to the      current leader  * *[Constraints](https://mesosphere.github.io/marathon/docs/constraints.html)* - e.g., only one instance of an application per rack, node, etc.  * *[Service Discovery &amp; Load Balancing](https://mesosphere.github.io/marathon/docs/service-discovery-load-balancing.html)* via HAProxy or the events API (see below).  * *[Health Checks](https://mesosphere.github.io/marathon/docs/health-checks.html)*: check your application's health via HTTP or TCP checks.  * *[Event Subscription](https://mesosphere.github.io/marathon/docs/rest-api.html#event-subscriptions)* lets you supply an HTTP endpoint to receive notifications, for example to integrate with an external load balancer.  * *[Marathon UI](https://mesosphere.github.io/marathon/docs/marathon-ui.html)*  * *[JSON/REST API](https://mesosphere.github.io/marathon/docs/rest-api.html)* for easy integration and scriptability  * *[*Basic Auth* and *SSL*](https://mesosphere.github.io/marathon/docs/ssl-basic-access-authentication.html)*  * *[Metrics](https://mesosphere.github.io/marathon/docs/metrics.html)*:    query them at `/metrics` in JSON format, push them to systems like Graphite, StatsD and DataDog, or scrape them using Prometheus.    ## Documentation    Marathon documentation is available on the [Marathon GitHub pages site](http://mesosphere.github.io/marathon/).    Documentation for installing and configuring the full Mesosphere stack including Mesos and Marathon is available on the [Mesosphere website](http://docs.mesosphere.com).    ## Issue Tracking    Marathon uses [JIRA](https://jira.mesosphere.com/projects/MARATHON) to track issues. You can [browse](https://jira.mesosphere.com/projects/MARATHON/issues/) existing issues or [file a new issue](https://jira.mesosphere.com/secure/CreateIssue!default.jspa?pid=10401) with your GitHub account.    Note for users of GitHub issues: All existing issues have been migrated and closed, and a reference to the related [JIRA](https://jira.mesosphere.com/projects/MARATHON) has been added as a comment.  We leave the GitHub issues available for reference. Going forward please use [JIRA](https://jira.mesosphere.com/projects/MARATHON) always.    ### Contributing    We heartily welcome external contributions to Marathon's documentation. Documentation should be committed to the `master` branch and published to our GitHub pages site using the instructions in [docs/README.md](https://github.com/mesosphere/marathon/tree/master/docs).    ## Setting Up And Running Marathon    ### Dependencies  Marathon has the following compile-time dependencies:  * sbt - A build tool for scala. You can find the instructions for installing sbt for Mac OS X and Linux over [here](http://www.scala-sbt.org/0.13/tutorial/Setup.html).  * JDK 1.8+    For run-time, Marathon has the following dependencies:  * libmesos - JNI bindings for talking to Apache Mesos master. Look at the *Install Mesos* section for instructions to get libmesos.  * Apache Zookeeper - You can have a separate Zookeeper installation specifically for Marathon, or you can use the same Zookeeper used by Mesos.    ### Installation    #### Getting started with [DC/OS](https://dcos.io/get-started/#marathon)  The by far easiest way to get Marathon running is to use [DC/OS](https://dcos.io/get-started/#marathon). Marathon is pre-bundled into [DC/OS](https://dcos.io/get-started/#marathon).    #### Install Mesos  Marathon requires libmesos, a shared object library, that contains JNI bindings for Marathon to talk to the Mesos master. *libmesos* comes as part of the Apache Mesos installation. There are three options for installing Apache Mesos.    ##### Installing Mesos from prepackaged releases  Instructions on how to install prepackaged releases of Mesos are available [in the Marathon docs](https://mesosphere.github.io/marathon/docs/).    ##### Building Mesos from source  **NOTE:** *Choose this option only if building Marathon from source, else there might be version incompatibility between pre-packaged releases of Marathon and Mesos built from source.*    You can find the instructions for compiling Mesos from source in the [Apache Mesos getting started docs](http://mesos.apache.org/getting-started/). If you want Mesos to install libraries and executables in a non-default location use the --prefix option during configuration as follows:    ```console  ./configure --prefix=<path to Mesos installation>  ```    The `make install` will install libmesos (libmesos.so on Linux and libmesos.dylib on Mac OS X) in the install directory.    ##### Using the Mesos Version Manager  **NOTE:** *Choose this option only if building Marathon from source, else there might be version incompatibility between pre-packaged releases of Marathon and Mesos built from source.*      The Mesos Version Manager (mvm) compiles, configures, and manages multiple versions of Apache Mesos.  It allows switching between versions quickly, making it easy to test Marathon against different versions of Mesos.    ###### Prerequisites    The Mesos Version Manager assumes that all dependencies of Apache Mesos are readily installed.    Please refer to the [Apache Mesos getting started docs](http://mesos.apache.org/gettingstarted/) for instructions on how to set up the build environment.    MVM compiles Mesos with SSL support by default, which requires openssl and libevent to be installed.    On macOS, the packages can be installed using brew: `brew install openssl libevent`    On CentOS, the packages can be installed using yum: `sudo yum install -y libevent-devel openssl-devel`    ###### Usage    The script can be run as follows:            cd marathon          cd tools          ./mvm.sh <VERSION> [SHELL]    The following command will launch a bash shell configured for Mesos 1.2.0: `./mvm.sh 1.2.0 bash`    You should consider placing the script into a folder in your shell's `PATH` if you are using it regularly.    The mvm script accepts three different formats as version name:    1. Version tags from the Mesos repository. Use `./mvm.sh --tags` in order to obtain a list of available tags.  2. Commit hashes from the Mesos repository.  3. The `--latest` flag, which automatically chooses the latest development version: `./mvm.sh --latest`.    MVM Will automatically download & compile Apache Mesos if necessary.  It will then spawn a new bash shell with the chosen version of Mesos activated.    For more information see `./mvm.sh --help`.    **Note**: You will have to re-run the script if you wish to use Mesos after closing the shell.  See `./mvm.sh --help` information on how to  permanently configure your shell for mvm to avoid this.    #### Install Marathon    Instructions on how to install prepackaged releases are available [in the Marathon docs](https://mesosphere.github.io/marathon/docs/). Alternatively, you can build Marathon from source.    ##### Building from Source    1.  To build Marathon from source, check out this repo and use sbt to build a universal:            git clone https://github.com/mesosphere/marathon.git          cd marathon          sbt 'run --master localhost:5050 --zk zk://localhost:2181/marathon'            **Troubleshooting**      1. Failure in retrieval of IP address of the local machine will result in an error and may look like this:                `Failed to obtain the IP address for '<local-machine>'; the DNS service may not be able to resolve it: nodename nor servname provided, or not known`                    Make sure that `LIBPROCESS_IP` environment variable is set.          ```          export LIBPROCESS_IP=""127.0.0.1""          ```      1. When the `MESOS_NATIVE_JAVA_LIBRARY` environment variable is not set, the following error may occur,                    `java.lang.UnsatisfiedLinkError: no mesos in java.library.path...`                    Make sure that `MESOS_NATIVE_JAVA_LIBRARY` environment variable is set.          ```          export MESOS_NATIVE_JAVA_LIBRARY=""/path/to/mesos/lib/libmesos.dylib""          ```       1.  Run `sbt universal:packageZipTarball` to package Marathon as an txz file      containing bin/marathon fully packaged.    1. Run `cd tools/packager; make tag-docker` for a local Marathon docker image.    ### Running in Development Mode    Mesos local mode allows you to run Marathon without launching a full Mesos  cluster. It is meant for experimentation and not recommended for production  use. Note that you still need to run ZooKeeper for storing state. The following  command launches Marathon on Mesos in *local mode*. Point your web browser to  `http://localhost:8080` to see the Marathon UI.            mesos-local          sbt 'run --master localhost:5050 --zk zk://localhost:2181/marathon'    For more information on how to run Marathon in production and configuration  options, see [the Marathon docs](https://mesosphere.github.io/marathon/docs/).    ## Developing Marathon    See [developing Marathon](./docs/docs/developing.md) in the docs.    ## Marathon Clients    * [marathonctl](https://github.com/shoenig/marathonctl) A handy CLI tool for controlling Marathon  * [Ruby gem and command line client](https://rubygems.org/gems/marathon-api)        Running Chronos with the Ruby Marathon Client:            marathon -M http://foo.bar:8080 start -i chronos -u https://s3.amazonaws.com/mesosphere-binaries-public/chronos/chronos.tgz \              -C ""./chronos/bin/demo ./chronos/config/nomail.yml \              ./chronos/target/chronos-1.0-SNAPSHOT.jar"" -c 1.0 -m 1024  * [Ruby gem marathon_deploy](https://github.com/eBayClassifiedsGroup/marathon_deploy) alternative command line tool to deploy using json or yaml files with ENV macros.  * [Scala client](https://github.com/guidewire/marathon-client), developed at Guidewire  * [Java client](https://github.com/mesosphere/marathon-client), developed at [Mesosphere](https://mesosphere.com)  * [Java client](https://github.com/mohitsoni/marathon-client) by Mohit Soni  * [Maven plugin](https://github.com/dcos-labs/dcos-maven-plugin), developed by [Johannes Unterstein](https://github.com/unterstein)  * [Maven plugin](https://github.com/holidaycheck/marathon-maven-plugin), developed at [HolidayCheck](http://www.holidaycheck.com/)  * [Python client](https://github.com/thefactory/marathon-python), developed at [The Factory](http://www.thefactory.com)  * [Python client](https://github.com/Wizcorp/marathon-client.py), developed at [Wizcorp](http://www.wizcorp.jp)  * [Go client](https://github.com/gambol99/go-marathon) by Rohith Jayawardene  * [Go client](https://github.com/jbdalido/gomarathon) by Jean-Baptiste Dalido  * [Node client](https://github.com/silas/node-mesos) by Silas Sewell  * [Clojure client](https://github.com/codemomentum/marathonclj) by Halit Olali  * [sbt plugin](https://github.com/Tapad/sbt-marathon), developed at [Tapad](https://tapad.com)    ## Companies using Marathon    Across all installations Marathon is managing applications on more than 100,000 nodes world-wide. These are some of the companies using it:    * [Adform](http://site.adform.com/)  * [Alauda](http://www.alauda.cn/)  * [Allegro](http://allegro.tech)  * [AllUnite](https://allunite.com/)  * [Argus Cyber Security](http://argus-sec.com/)  * [Artirix](http://www.artirix.com/)  * [Arukas](https://arukas.io/)  * [Avast](https://www.avast.com/)  * [AVENTER](https://www.aventer.biz/)  * [bol.com](https://www.bol.com/)  * [Brand24](https://brand24.com/)  * [Branding Brand](http://www.brandingbrand.com/)  * [China Mobile](http://www.chinamobileltd.com/en/global/home.php)  * [China Unicom](http://eng.chinaunicom.com/index/index.html)  * [Corvisa](https://www.corvisa.com/)  * [Criteo](http://www.criteo.com/)  * [Daemon](http://www.daemon.com.au/)  * [DataMan](http://www.shurenyun.com/)  * [Deutsche Telekom](https://www.telekom.com/)  * [DHL Parcel](https://www.dhlparcel.nl/)  * [Disqus](https://disqus.com/)  * [DueDil](https://www.duedil.com/)  * [eBay](http://www.ebay.com/)  * [Erasys](http://www.erasys.de/)  * [The Factory](https://github.com/thefactory/)  * [Football Radar](http://www.footballradar.com)  * [Guidewire](https://www.guidewire.com/)  * [Groupon](https://www.groupon.com/)  * [GSShop](http://www.gsshop.com/)  * [GrowingIO](https://www.growingio.com/)  * [HolidayCheck](http://www.holidaycheck.com/)  * [Human API](https://humanapi.co/)  * [Indix](http://www.indix.com/)  * [ING](http://www.ing.com/)  * [Intent HQ](https://www.intenthq.com/)  * [iQIYI](http://www.iqiyi.com/)  * [LaunchKey](https://launchkey.com/)  * [Mapillary](https://www.mapillary.com/)  * [Measurence](http://www.measurence.com/)  * [Motus](http://www.motus.com/)  * [Notonthehighstreet](http://www.notonthehighstreet.com/)  * [OpenTable](http://www.opentable.com/)  * [Opera](https://www.opera.com)  * [Orbitz](http://www.orbitz.com/)  * [Otto](https://www.otto.de/)  * [OVH](https://ovh.com/)  * [PayPal](https://www.paypal.com)  * [Qubit](http://www.qubit.com/)  * [RelateIQ](https://www.salesforceiq.com/)  * [Refinery29](https://www.refinery29.com)  * [Sailthru](http://www.sailthru.com/)  * [SAKURA Internet Inc](https://www.sakura.ad.jp/)  * [sloppy.io](http://sloppy.io/)  * [SmartProcure](https://smartprocure.us/)  * [Strava](https://www.strava.com)  * [Sveriges Television](http://www.svt.se)  * [Salesforce](http://www.salesforce.com)  * [T2 Systems](http://t2systems.com)  * [Tapad](https://tapad.com)  * [Teradata](http://www.teradata.com)  * [Toss](https://www.toss.im/)  * [trivago](http://www.trivago.com/)  * [tronc / L.A. Times](http://www.tronc.com/about-us/)  * [VANAD Enovation](http://www.vanadenovation.nl/)  * [Viadeo](http://www.viadeo.com)  * [Wikia](http://www.wikia.com/Wikia)  * [William Hill](https://www.williamhill.com)  * [WooRank](https://www.woorank.com/)  * [Yelp](http://www.yelp.com/)    Not in the list? Open a pull request and add yourself!    ## Help    Have you found an issue? Feel free to report it using our [JIRA Issues](https://jira.mesosphere.com/projects/MARATHON/summary) page.  In order to speed up response times, we ask you to provide as much  information on how to reproduce the problem as possible. If the issue is related   in any way to the web UI, we kindly ask you to use the `gui` label.    If you have questions, please post on the [Marathon Framework](https://groups.google.com/forum/?hl=en#!forum/marathon-framework) email list.     You can find Marathon support in the `#marathon` channel, and Mesos support in the `#mesos` channel, on [freenode][freenode] (IRC). Alternatively, check out the same channels on the [Mesos Slack](https://mesos.slack.com/) ([request an invitation here](https://mesos-slackin.herokuapp.com/)).     The team at [Mesosphere][Mesosphere] is also happy to answer any questions.    If you'd like to take part in design research and test new features in Marathon before they're released, please add your name to our [UX Research](http://uxresearch.mesosphere.com) list.    ## Authors    Marathon was created by [Tobias Knaup](https://github.com/guenter) and  [Florian Leibert](https://github.com/florianleibert) and continues to be  developed by the team at Mesosphere and by many contributors from  the community.    [Chronos]: https://github.com/mesos/chronos ""Airbnb's Chronos""  [Mesos]: https://mesos.apache.org/ ""Apache Mesos""  [Zookeeper]: https://zookeeper.apache.org/ ""Apache Zookeeper""  [Storm]: http://storm.apache.org ""distributed realtime computation""  [freenode]: https://freenode.net/ ""IRC channels""  [upstart]: http://upstart.ubuntu.com/ ""Ubuntu's event-based daemons""  [init]: https://en.wikipedia.org/wiki/Init ""init""  [Mesosphere]: https://mesosphere.com/ ""Mesosphere""    ## Acknowledgements    **YourKit, LLC**    ![YourKit, LLC](https://www.yourkit.com/images/yklogo.png)    YourKit supports open source projects with its full-featured Java  Profiler.  YourKit, LLC is the creator of <a  href=""https://www.yourkit.com/java/profiler/index.jsp"">YourKit Java  Profiler</a>  and <a href=""https://www.yourkit.com/.net/profiler/index.jsp"">YourKit  .NET Profiler</a>,  innovative and intelligent tools for profiling Java and .NET  applications. """
Big data;https://github.com/getredash/redash;"""<p align=""center"">    <img title=""Redash"" src='https://redash.io/assets/images/logo.png' width=""200px""/>  </p>    [![Documentation](https://img.shields.io/badge/docs-redash.io/help-brightgreen.svg)](https://redash.io/help/)  [![Datree](https://s3.amazonaws.com/catalog.static.datree.io/datree-badge-20px.svg)](https://datree.io/?src=badge)  [![Build Status](https://circleci.com/gh/getredash/redash.png?style=shield&circle-token=8a695aa5ec2cbfa89b48c275aea298318016f040)](https://circleci.com/gh/getredash/redash/tree/master)    Redash is designed to enable anyone, regardless of the level of technical sophistication, to harness the power of data big and small. SQL users leverage Redash to explore, query, visualize, and share data from any data sources. Their work in turn enables anybody in their organization to use the data. Every day, millions of users at thousands of organizations around the world use Redash to develop insights and make data-driven decisions.    Redash features:    1. **Browser-based**: Everything in your browser, with a shareable URL.  2. **Ease-of-use**: Become immediately productive with data without the need to master complex software.  3. **Query editor**: Quickly compose SQL and NoSQL queries with a schema browser and auto-complete.  4. **Visualization and dashboards**: Create [beautiful visualizations](https://redash.io/help/user-guide/visualizations/visualization-types) with drag and drop, and combine them into a single dashboard.  5. **Sharing**: Collaborate easily by sharing visualizations and their associated queries, enabling peer review of reports and queries.  6. **Schedule refreshes**: Automatically update your charts and dashboards at regular intervals you define.  7. **Alerts**: Define conditions and be alerted instantly when your data changes.  8. **REST API**: Everything that can be done in the UI is also available through REST API.  9. **Broad support for data sources**: Extensible data source API with native support for a long list of common databases and platforms.    <img src=""https://raw.githubusercontent.com/getredash/website/8e820cd02c73a8ddf4f946a9d293c54fd3fb08b9/website/_assets/images/redash-anim.gif"" width=""80%""/>    ## Getting Started    * [Setting up Redash instance](https://redash.io/help/open-source/setup) (includes links to ready-made AWS/GCE images).  * [Documentation](https://redash.io/help/).    ## Supported Data Sources    Redash supports more than 35 SQL and NoSQL [data sources](https://redash.io/help/data-sources/supported-data-sources). It can also be extended to support more. Below is a list of built-in sources:    - Amazon Athena  - Amazon DynamoDB  - Amazon Redshift  - Axibase Time Series Database  - Cassandra  - ClickHouse  - CockroachDB  - CSV  - Databricks (Apache Spark)  - DB2 by IBM  - Druid  - Elasticsearch  - Firebolt  - Google Analytics  - Google BigQuery  - Google Spreadsheets  - Graphite  - Greenplum  - Hive  - Impala  - InfluxDB  - JIRA  - JSON  - Apache Kylin  - OmniSciDB (Formerly MapD)  - MemSQL  - Microsoft Azure Data Warehouse / Synapse  - Microsoft Azure SQL Database  - Microsoft SQL Server  - MongoDB  - MySQL  - Oracle  - PostgreSQL  - Presto  - Prometheus  - Python  - Qubole  - Rockset  - Salesforce  - ScyllaDB  - Shell Scripts  - Snowflake  - SQLite  - TiDB  - TreasureData  - Vertica  - Yandex AppMetrrica  - Yandex Metrica    ## Getting Help    * Issues: https://github.com/getredash/redash/issues  * Discussion Forum: https://discuss.redash.io/    ## Reporting Bugs and Contributing Code    * Want to report a bug or request a feature? Please open [an issue](https://github.com/getredash/redash/issues/new).  * Want to help us build **_Redash_**? Fork the project, edit in a [dev environment](https://redash.io/help-onpremise/dev/guide.html) and make a pull request. We need all the help we can get!    ## Security    Please email security@redash.io to report any security vulnerabilities. We will acknowledge receipt of your vulnerability and strive to send you regular updates about our progress. If you're curious about the status of your disclosure please feel free to email us again. If you want to encrypt your disclosure email, you can use [this PGP key](https://keybase.io/arikfr/key.asc).    ## License    BSD-2-Clause. """
Big data;https://github.com/benedekrozemberczki/awesome-graph-classification;"""# Awesome Graph Classification  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-graph-embedding.svg?color=blue)  [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-graph-classification.svg)](https://github.com/benedekrozemberczki/awesome-graph-classification/archive/master.zip) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)     A collection of graph classification methods, covering embedding, deep learning, graph kernel and factorization papers with reference implementations.    Relevant graph classification benchmark datasets are available [[here]](https://github.com/shiruipan/graph_datasets).    Similar collections about [community detection](https://github.com/benedekrozemberczki/awesome-community-detection), [classification/regression tree](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [fraud detection](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers), [Monte Carlo tree search](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers), and [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers) papers with implementations.    <p align=""center"">    <img width=""400"" src=""atlas.png"">  </p>    -----------------------------------------------------    ## Contents      1. [Matrix Factorization](https://github.com/benedekrozemberczki/awesome-graph-classification/blob/master/chapters/matrix_factorization.md)    2. [Spectral and Statistical Fingerprints](https://github.com/benedekrozemberczki/awesome-graph-classification/blob/master/chapters/fingerprints.md)  3. [Deep Learning](https://github.com/benedekrozemberczki/awesome-graph-classification/blob/master/chapters/deep_learning.md)    4. [Graph Kernels](https://github.com/benedekrozemberczki/awesome-graph-classification/blob/master/chapters/kernels.md)    -------------------------------------------------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-graph-classification/blob/master/LICENSE) """
Big data;https://github.com/Sdogruyol/awesome-ruby;"""# Awesome Ruby    A curated list of awesome Ruby frameworks, libraries and resources. Inspired by [awesome-php](https://github.com/ziadoz/awesome-php) & [awesome-python](https://github.com/vinta/awesome-python).    ## Contribution    Your Pull requests are welcome! Let's make this the awesomest resource for Ruby :purple_heart:    - [Awesome Ruby](#awesome-ruby)    - [Admin Panels](#admin-panels)    - [Anti-spam](#anti-spam)    - [Asset Management](#asset-management)    - [Audio](#audio)    - [Auditing](#auditing)    - [Authentication and OAuth](#authentication-and-oauth)    - [Build Tools](#build-tools)    - [Caching](#caching)    - [Cloud Services](#cloud-services)    - [CMS](#cms)    - [Code Analysis and Linter](#code-analysis-and-linter)    - [Command-line Tools](#command-line-tools)    - [Configuration](#configuration)    - [CSS and Styling](#css-and-styling)    - [Data Validation](#data-validation)    - [Data Visualization](#data-visualization)    - [Database Drivers](#database-drivers)    - [Date and Time](#date-and-time)    - [Debugging Tools](#debugging-tools)    - [DevOps Tools](#devops-tools)    - [Distribution](#distribution)    - [Documentation](#documentation)    - [Downloader](#downloader)    - [E-Commerce & Online Paying](#e-commerce--online-paying)    - [E-Mail](#e-mail)    - [Environment Management](#environment-management)    - [File Uploading](#file-uploading)    - [Feature Flipping](#feature-flipping)    - [Foreign Function Interface](#foreign-function-interface)    - [Forms](#forms)    - [Game Development](#game-development)    - [Geolocation](#geolocation)    - [GUI](#gui)    - [High Performance](#high-performance)    - [HTML/XML/CSS Manipulation](#htmlxmlcss-manipulation)    - [HTTP](#http)    - [Imagery](#imagery)    - [Internationalization](#internationalization)    - [Logging](#logging)    - [Machine Learning](#machine-learning)    - [MapReduce](#mapreduce)    - [Multi-tenancy](#multi-tenancy)    - [Natural Language Processing](#natural-language-processing)    - [Networking](#networking)    - [ORM](#orm)    - [Package Management](#package-management)    - [Presentation Tools](#presentation-tools)    - [Processes and Threads](#processes-and-threads)    - [Push Notification](#push-notification)    - [Queue](#queue)    - [Serverless](#serverless)    - [Spreadsheets](#spreadsheets)    - [RESTful API](#restful-api)    - [Science and Data Analysis](#science-and-data-analysis)    - [Search](#search)    - [Site Monitoring](#site-monitoring)    - [Starter Apps](#starter-apps)    - [Tagging](#tagging)    - [Template Engine](#template-engine)    - [Testing](#testing)    - [Text Processing](#text-processing)    - [Third-party APIs](#third-party-apis)    - [URL Manipulation](#url-manipulation)    - [Video](#video)    - [Web Content Extracting](#web-content-extracting)    - [Web Crawling](#web-crawling)    - [Web Frameworks](#web-frameworks)    - [Web Servers](#web-servers)    - [WebSocket](#websocket)  - [Miscellaneous](#miscellaneous)    - [Editor Plugins](#editor-plugins)  - [Resources](#resources)    - [People to Follow](#people-to-follow)  - [Other Awesome Lists](#other-awesome-lists)  - [Contributing](#contributing)      ## Admin Panels    *Libraries for administrative interfaces.*      * [active_admin](https://github.com/activeadmin/activeadmin) The administration framework for Ruby on Rails applications    * [rails_admin](https://github.com/sferik/rails_admin) A Rails engine that provides an easy-to-use interface for managing your data    * [administrate](https://github.com/thoughtbot/administrate) A framework for creating flexible, powerful admin dashboards in Rails.    ## Anti-spam    *Libraries for fighting spam.*      * [RubySpamAssassin](https://github.com/noeticpenguin/RubySpamAssassin) Kills Spam Dead. Perhaps before it's sent!    ## Asset Management    *Tools for managing, compressing and minifying website assets.*      * [sprockets](https://github.com/sstephenson/sprockets) Rack-based asset packaging system    * [rails-assets](https://github.com/rails-assets/rails-assets/) is the frictionless proxy between Bundler and Bower    ## Audio      * [seal](https://github.com/zhangsu/seal) A C library (with Ruby binding) for 3D audio rendering    ## Auditing      *Tools for logging changes to ActiveRecord models*      * [Audited](https://github.com/collectiveidea/audited) - Audited (formerly acts_as_audited) is an ORM extension that logs all changes to your Rails models.    * [Logidze](https://github.com/palkan/logidze) - Logs model changes via database triggers (PL/SQL functions). Fastest model diffs. PostgreSQL 9.5+ only.    ## Authentication and OAuth    *Libraries for implementing authentications schemes.*      * [Devise](https://github.com/plataformatec/devise) - Devise is a flexible authentication solution for Rails based on Warden    * [Omniauth](https://github.com/intridea/omniauth) - OmniAuth is a flexible authentication system utilizing Rack middleware    * [Warden](https://github.com/hassox/warden) - General Rack Authentication Framework    * [AuthLogic](https://github.com/binarylogic/authlogic) - A simple ruby authentication solution    * [Sorcery](https://github.com/NoamB/sorcery) - Magical authentication for Rails 3 & 4    * [CanCanCan](https://github.com/CanCanCommunity/cancancan) Authorization gem for Rails (continued version of CanCan from ryanb)    * [pundit](https://github.com/elabs/pundit) - Minimal authorization using object oriented design.    * [authority](https://github.com/nathanl/authority) - ORM neutral authorization.    * [doorkeeper](https://github.com/doorkeeper-gem/doorkeeper) An OAuth 2 provider for Rails    * [tiddle](https://github.com/adamniedzielski/tiddle/) - Devise strategy for token authentication in API-only Ruby on Rails applications    ## Build Tools    *Compile software from source code.*      * [teapot](https://github.com/ioquatix/teapot) A decentralised build tool for managing complex cross-platform projects    ## Caching    *Libraries for caching data.*      * [rack-cache](https://github.com/rtomayko/rack-cache) HTTP Caching for Ruby Web Apps    * [Dalli](https://github.com/mperham/dalli) - a high performance pure Ruby client for accessing memcached servers.    ## Cloud Services      * [fog](https://github.com/fog/fog) The Ruby cloud services library    * [aws-sdk-ruby](https://github.com/aws/aws-sdk-ruby) The official AWS SDK for Ruby    ## CMS    *Content management systems*      * [Refinery CMS](http://www.refinerycms.com) An extendable Ruby on Rails CMS that supports Rails 3.2 and 4.2    * [Comfortable Mexican Sofa](https://github.com/comfy/comfortable-mexican-sofa) A powerful Rails 4/5 CMS Engine    * [Browser](http://www.browsercms.org/) Humane Content Management for Rails    * [Locomotive](http://www.locomotivecms.com/) a brand new CMS system with super sexy UI and cool features    * [Radiant](http://radiantcms.org/) A no-fluff, open source content management system    * [Nesta](http://nestacms.com/) A lightweight CMS, implemented in Sinatra    * [alchemy_cms](https://github.com/AlchemyCMS/alchemy_cms) the most powerful, user friendly and flexible Rails CMS    * [weby](https://github.com/cercomp/weby) Newbie CMS in Ruby on Rails    ## Code Analysis and Linter    *Libraries and tools for analyzing, parsing and manipulation codebases.*    * [sonarlint-intellij](https://github.com/SonarSource/sonarlint-intellij) - An IDE extension that helps you detect and fix quality issues as you write code.    * [Rubocop](https://github.com/bbatsov/rubocop) - A Ruby static code analyzer, based on the community Ruby style guide.    * [ruby-lint](https://github.com/YorickPeterse/ruby-lint) - ruby-lint is a static code analysis tool for Ruby    * [brakeman](https://github.com/presidentbeef/brakeman) - Static analysis tool which checks Ruby on Rails applications for security vulnerabilities    * [reek](https://github.com/troessner/reek) - Code smell detector for Ruby    * [Breezer](https://github.com/lambda2/breezer) - Lock your Gemfile dependencies to safe versions.    ## Command-line Tools    *Libraries for building command-line application.*      * [Commander](http://visionmedia.github.io/commander/) - The complete solution for Ruby command-line executables    * [Thor](https://github.com/erikhuda/thor) - Thor is a toolkit for building powerful command-line interfaces    ## Configuration    *Libraries for storing configuration options.*    ## CSS and Styling      * [sass](https://github.com/sass/sass) A CSS preproccessor      * [sass-rails](https://github.com/rails/sass-rails) Rails stylesheet engine for Sass    * [less-rails](https://github.com/metaskills/less-rails) The dynamic stylesheet language for the Rails    * [compass](https://github.com/Compass/compass) A a Stylesheet Authoring Environment    * [bootstrap-sass](https://github.com/twbs/bootstrap-sass) Official Sass port of Bootstrap    * [foundation-rails](https://github.com/zurb/foundation-rails) Foundation for Rails    * [bootswatch-rails](https://github.com/maxim/bootswatch-rails) Bootswatches converted to SCSS ready to use in Rails    * [bourbon](https://github.com/thoughtbot/bourbon) A lightweight mixin library for Sass    ## Data Validation    *Libraries for validating data. Used for forms in many cases.*      * [kangal](https://github.com/lab2023/kangal) - Extended validation gem for email, subdomain, credit card, tax number etc    * [bin_checker](https://github.com/lab2023/bin_checker) - BIN validator for Turkish Banks    ## Data Visualization    *Libraries for visualizing data.*      * [prosperity](https://github.com/smathieu/prosperity) The easiest way to graph data from your Rails models    ## Database Drivers    *Libraries for connecting and operating databases.*      * Relational Databases      * [ruby-pg](https://bitbucket.org/ged/ruby-pg) Ruby interface to the PostgreSQL >= 8.4      * [mysql2](https://github.com/brianmario/mysql2) A modern, simple and very fast Mysql library for Ruby      * [sqlite3-ruby](https://github.com/sparklemotion/sqlite3-ruby) Ruby bindings for the SQLite3 embedded database      * NoSQL Databases    ## Date and Time    *Libraries for working with dates and times.*      * [stamp](https://github.com/jeremyw/stamp) Date and time formatting for humans    * [chronic](https://github.com/mojombo/chronic) Natural language date/time parser      ## Debugging Tools    *Libraries for debugging and developing.*      * [byebug](https://github.com/deivid-rodriguez/byebug) - Debugging in Ruby 2    * [debugger](https://github.com/cldwalker/debugger) - port of ruby-debug that works on 1.9.2 and 1.9.3    * [puts_debuggerer](https://github.com/AndyObtiva/puts_debuggerer) - Debugger-less debugging FTW    ## DevOps Tools    *Software and libraries for DevOps.*      * [Puppet](https://github.com/puppetlabs/puppet) - Server automation framework and application    * [Chef](https://github.com/chef/chef) - A systems integration framework, built to bring the benefits of configuration management to your entire infrastructure.    * [Vagrant](https://www.vagrantup.com/) - Vagrant is a tool for building and distributing development environments.    * [Capistrano](http://capistranorb.com/) - Remote multi-server automation tool    * [Mina](https://github.com/mina-deploy/mina) Really fast deployer and server automation tool    * [Nanobox](https://github.com/nanobox-io/nanobox) - A micro-PaaS (μPaaS) for creating consistent, isolated, Ruby environments deployable anywhere https://nanobox.io.    ## Distribution    *Libraries to create packaged executables for release distribution.*      * [fpm](https://github.com/jordansissel/fpm)  Building packages for multiple platforms (deb, rpm, etc) with great ease and sanity.    ## Documentation    *Libraries for generating project documentation.*      * [Dictum](https://github.com/Wolox/dictum) - A tool that let's you create automatic documentation of your Rails API endpoints through your tests.    * [rdoc](https://github.com/rdoc/rdoc) HTML and online documentation for Ruby projects    * [yard](https://github.com/lsegal/yard) A Ruby Documentation tool    ## Downloader    *Libraries for downloading.*      * [GitHub Starred Repos Downloader](https://github.com/LeonardoCardoso/gsrd) -  gsrd downloads your public starred repos. Just in case you want to keep a backup of them from time to time.    ## E-Commerce & Online Paying      * [Active Merchant](https://github.com/activemerchant/active_merchant) - A simple payment abstraction library extracted from Shopify.    * [Spree](https://github.com/spree/spree) - A complete open source e-commerce solution for Ruby on Rails.    * [Square SDK](https://github.com/square/square-ruby-sdk) - Use this gem to integrate Square payments into your app and grow your business with Square APIs including Catalog, Customers, Employees, Inventory, Labor, Locations, and Orders.    * [PayPal Merchant SDK](https://github.com/paypal/merchant-sdk-ruby) - Provides Ruby APIs for processing payments, recurring payments, subscriptions and transactions using PayPal's Merchant APIs.    ## E-Mail    *Libraries for sending and parsing email.*      * [mail](https://github.com/mikel/mail) A Really Ruby Mail Library    * [mailman](https://github.com/mailman/mailman) An incoming mail processing microframework in Ruby    ## Environment Management    *Libraries for Ruby version and environment management.*      * [chruby](https://github.com/postmodern/chruby) - Changes the current Ruby    * [chgems](https://github.com/postmodern/chgems) - Chroot for RubyGems    * [rvm](https://rvm.io/) - Ruby Version Manager    * [rbenv](http://rbenv.org/) - Groom your app’s Ruby environment    * [ruby-install](https://github.com/postmodern/ruby-install) - Installs Ruby, JRuby, Rubinius, MagLev or MRuby    * [ruby-build](https://github.com/sstephenson/ruby-build) - Compile and install Ruby    * [Nanobox](https://github.com/nanobox-io/nanobox) - A tool for creating isolated Ruby environments for consistency across teams and application stages (dev, staging, production, etc.) https://nanobox.io.    ## Error Handling    *Libraries for exception and error handling.*      * [Exception Notification](https://github.com/smartinez87/exception_notification) - A set of notifiers for sending notifications when errors occur in a Rack/Rails application    * [Errbit](http://errbit.github.io/errbit) - The open source, self-hosted error catcher    * [Airbrake](https://github.com/airbrake/airbrake) - The official Airbrake library for Ruby on Rails (and other Rack based frameworks)    * [Better Errors](https://github.com/charliesome/better_errors) - Better error page for Rack apps    ## File Uploading    *Libraries for handling file uploads.*      * [paperclip](https://github.com/thoughtbot/paperclip) Easy file attachment management for ActiveRecord    * [dragonfly](https://github.com/markevans/dragonfly) On-the-fly processing - suitable for image uploading in Rails, Sinatra and much more    * [carrierwave](https://github.com/carrierwaveuploader/carrierwave) Classier solution for file uploads for Rails, Sinatra and other Ruby web frameworks    * [attache](https://github.com/choonkeat/attache) - Yet another approach to file upload https://attache-demo.herokuapp.com    ## Feature flipping    *Libraries for flipping features*      * [abstract_feature_branch](https://github.com/AndyObtiva/abstract_feature_branch) a gem that enables developers to easily branch by abstraction    * [helioth](https://github.com/gmontard/helioth) Manage feature flipping and rollout    * [flipper](https://github.com/jnunemaker/flipper) feature flipping for ANYTHING    * [flip](https://github.com/pda/flip) Flip lets you declare and manage feature flags, backed by cookies (private testing) and database (site-wide)    * [rollout](https://github.com/FetLife/rollout) Feature flippers.    ## Foreign Function Interface    *Libraries for providing foreign function interface.*    ## Forms    *Libraries for working with forms.*      * [simple_form](https://github.com/plataformatec/simple_form) Forms made easy for Rails    * [formtastic](https://github.com/justinfrench/formtastic) A Rails form builder plugin with semantically rich and accessible markup    * [bootstrap_form](https://github.com/bootstrap-ruby/bootstrap_form) A Rails form builder that makes it super easy to integrate Bootstrap v4-style forms into your Rails application    ## Game Development    *Awesome game development libraries.*      * [Gosu](https://www.libgosu.org/) - A 2D game development library for the Ruby and C++ programming languages      ## Geolocation    *Libraries for geocoding addresses and working with latitudes and longitudes.*      * [geocoder](https://github.com/alexreisner/geocoder) Complete Ruby geocoding solution    * [Geokit](https://github.com/geokit/geokit) - Geokit gem provides geocoding and distance/heading calculations.    ## Git Tools    *Libraries for working with Git VCS*      * [katip](https://github.com/lab2023/katip) - Change logger for Git initialized projects    ## GUI    *Libraries for working with graphical user interface applications.*      * [glimmer](https://github.com/AndyObtiva/glimmer) Ruby Desktop Development GUI Library    * [shoes](https://github.com/shoes/shoes) A tiny graphical app kit for ruby    * [shoes4](https://github.com/shoes/shoes4) the next version of Shoes    ## High Performance    *Libraries for making Ruby faster.*      * [EventMachine](https://github.com/eventmachine/eventmachine) - EventMachine: fast, simple event-processing library for Ruby programs    * [Celluloid](https://celluloid.io/) - Actor-based concurrent object framework for Ruby. It has its own [awesomeness](https://github.com/sashaegorov/awesome-celluloid).    ## HTML/XML/CSS Manipulation    *Libraries for working with HTML, XML & CSS.*      * [Nokogiri](http://www.nokogiri.org/)    * [loofah](https://github.com/flavorjones/loofah) A general library for manipulating and transforming HTML/XML documents and fragments    ## HTTP    *Libraries for working with HTTP.*      * [httparty](https://github.com/jnunemaker/httparty) Makes http fun again!    * [faraday](https://github.com/lostisland/faraday) Simple, but flexible HTTP client library, with support for multiple backends.    * [http](https://github.com/httprb/http) A simple Ruby DSL for making HTTP requests    * [excon](https://github.com/excon/excon) Usable, fast, simple HTTP(S) 1.1 for Ruby    * [nestful](https://github.com/maccman/nestful) Simple Ruby HTTP/REST client with a sane API    * [response_code](https://github.com/torokmark/response_code) Response Code in readable way    ## Imagery    *Libraries for manipulating images.*      * [rmagick](https://github.com/rmagick/rmagick) An interface to the ImageMagick and GraphicsMagick image processing libraries      *  [minimagick](https://github.com/minimagick/minimagick) Minified version of rmagick    * [chunky_png](https://github.com/wvanbergen/chunky_png) Read/write access to PNG images in pure Ruby    * [image_optim](https://github.com/toy/image_optim) Optimize images using multiple utilities    * [magickly](https://github.com/afeld/magickly) image manipulation as a (plugin-able) service    ## Internationalization    *Libraries for woking with i18n.*      * [i18n](https://github.com/svenfuchs/i18n) - Basic internationalization(i18n) library for Ruby    * [globalize](https://github.com/globalize/globalize) Rails I18n de-facto standard library for ActiveRecord model/data translation    * [i18n-tasks](https://github.com/glebm/i18n-tasks) Manage translations in ruby applications with the awesome power of static analysis    ## Logging    *Libraries for generating and working with log files.*      * [Logstash](https://github.com/elastic/logstash) Logstash is a tool for managing events and logs.    ## Machine Learning    *Libraries for Machine Learning.*      * [PredictionIO Ruby SDK](https://github.com/PredictionIO/PredictionIO-Ruby-SDK) - The PredictionIO Ruby SDK provides a convenient API to quickly record your users' behavior and retrieve personalized predictions for them    * [m2cgen](https://github.com/BayesWitnesses/m2cgen) - A CLI tool to transpile trained classic ML models into a native Ruby code with zero dependencies.    ## MapReduce    *Frameworks and libraries for MapReduce.*    ## Multi-tenancy    *Libraries for managing multi-tenant apps.*    * [Apartment](https://github.com/influitive/apartment) - Database multi-tenancy for Rack (and Rails) applications    ## Natural Language Processing    *Libraries for working with human languages.*    * [Treat](https://github.com/louismullie/treat) - Treat is a toolkit for natural language processing and computational linguistics in Ruby    ## Networking    *Libraries for network programming.*    ## ORM    *Libraries that implement Object-Relational Mapping or data mapping techniques.*    * Relational Databases      * [ActiveRecord](https://www.ruby-toolbox.com/projects/activerecord) - Databases on Rails. Build a persistent domain model by mapping database tables to Ruby classes    * [DataMapper](http://datamapper.org/) - DataMapper is an Object Relational Mapper written in Ruby. The goal is to create an ORM which is fast, thread-safe and feature rich.    * [Sequel](http://sequel.jeremyevans.net/) - The Database Toolkit for Ruby    * NoSQL Databases      * [Mongoid](http://mongoid.org) - Mongoid (pronounced mann-goyd) is an Object-Document-Mapper (ODM) for MongoDB written in Ruby.    * [Ohm](https://github.com/soveran/ohm) - Object-Hash Mapping for Redis      ## Package Management    *Libraries for package and dependency management.*      * [RubyGems](https://rubygems.org/) - RubyGems is a package manager for the Ruby programming language that provides a standard format for distributing Ruby programs and libraries    * [Bundler](http://bundler.io) - Bundler provides a consistent environment for Ruby projects by tracking and installing the exact gems and versions that are needed    * [Homebrew](http://brew.sh) - Homebrew installs the stuff you need that Apple didn’t    * [Homebrew Cask](http://caskroom.io/) - Cask provides a friendly homebrew-style CLI workflow for the administration of Mac applications distributed as binaries    ## Pagination      * [kaminari](https://github.com/amatsuda/kaminari) A Scope & Engine based, clean, powerful, customizable and sophisticated paginator    * [will_paginate](https://github.com/mislav/will_paginate) Pagination library for Rails 3, Sinatra, Merb, DataMapper, and more    * [order_query](https://github.com/glebm/order_query) Keyset pagination to find the next or previous record(s) relative to the current one efficiently, e.g. for infinite scroll.    ## PDF Processing      * [DocRaptor](https://github.com/DocRaptor/docraptor-ruby) Wrapper library for [DocRaptor's](https://docraptor.com) Ruby-based HTML-to-PDF API    * [wicked_pdf](https://github.com/mileszs/wicked_pdf) PDF generator (from HTML) plugin for Ruby on Rails    * [pdfkit](https://github.com/pdfkit/pdfkit) HTML+CSS to PDF using wkhtmltopdf    * [prawn](https://github.com/prawnpdf/prawn) Fast, Nimble PDF Writer for Ruby    * [InvoicePrinter](https://github.com/strzibny/invoice_printer) - Super simple PDF invoicing in Ruby (built on top of Prawn).    ## Presentation Tools      * [rabbit](https://github.com/rabbit-shocker/rabbit) A programable presentaton tool by Ruby    * [reveal-ck](https://github.com/jedcn/reveal-ck) Reveal.js presentations with a Ruby toolset    ## Processes and Threads    *Libraries for woking with processes or threads*      * [Parallel](https://github.com/grosser/parallel) - Ruby parallel processing made simple and fast    ## Profiling      * [bullet](https://github.com/flyerhzm/bullet) - help to kill N+1 queries and unused eager loading    ## Push Notification      * [Rpush](https://github.com/rpush/rpush) - The push notification service for Ruby.    * [apn_sender](https://github.com/arthurnn/apn_sender) - Background worker to send Apple Push Notifications over a persistent TCP socket.    * [Houston](https://github.com/nomad/houston) - A simple gem for sending Apple Push Notifications.    * [webpush](https://github.com/zaru/webpush) - Encryption Utilities for Web Push protocol    ## Queue    *Libraries for working with event and task queues.*      * [Resque](https://github.com/resque/resque) A Redis-backed Ruby library for creating background jobs, placing them on multiple queues.    * [Delayed::Job](https://github.com/tobi/delayed_job) — Database backed asynchronous priority queue.    * [Qu](https://github.com/bkeepers/qu) A Ruby library for queuing and processing background jobs.    * [Sidekiq](https://github.com/mperham/sidekiq) Simple, efficient background processing for Ruby    ## RESTful API    *Libraries for developing RESTful APIs.*      * [Grape](http://intridea.github.io/grape/) - An opinionated micro-framework for creating REST-like APIs in Ruby.    * [Rails::API](https://github.com/rails-api/rails-api) - Rails for API only applications    * [jbuilder](https://github.com/rails/jbuilder) - Create JSON structures via a Builder-style DSL    * [rabl](https://github.com/nesquena/rabl) - General Ruby templating with json, bson, xml, plist and msgpack support    * [active_model_serializers](https://github.com/rails-api/active_model_serializers) - ActiveModel::Serializer implementation and Rails hooks    * [oat](https://github.com/ismasan/oat) - Adapters-based API serializers with Hypermedia support for Ruby apps (HAL, Siren, JSONAPI).    * [APIcasso](https://github.com/ErvalhouS/APIcasso) - An abstract API design as a Rails-based mountable engine. RESTfullize your legacy code.    ## Serverless  * [FaaStRuby](https://faastruby.io) - Serverless Software Development Platform for Ruby and Crystal developers.    ## Spreadsheets    *Libraries for manipulating Excel, Google Spreadsheets, Numbers, OpenOffice and LibreOffice files*      * [spreadsheet](https://github.com/zdavatz/spreadsheet) - The Spreadsheet Library is designed to read and write Spreadsheet Documents.    * [caxlsx](https://github.com/caxlsx/caxlsx) - Caxlsx excels at helping you generate beautiful Office Open XML Spreadsheet documents.    * [caxlsx_rails](https://github.com/caxlsx/caxlsx_rails) - Axlsx-Rails provides an Axlsx renderer so you can move all your spreadsheet code from your controller into view files.    * [roo](https://github.com/roo-rb/roo) - Roo implements read access for all spreadsheet types and read/write access for Google spreadsheets.    * [google-spreadsheet-ruby](https://github.com/gimite/google-spreadsheet-ruby) - This is a library to read/write Google Spreadsheet.    * [rubyXL](https://github.com/weshatheleopard/rubyXL) - rubyXL is a gem which allows the parsing, creation, and manipulation of Microsoft Excel (.xlsx/.xlsm) Documents    * [Odf-report](https://github.com/sandrods/odf-report) - Generates ODF files, given a template (.odt) and data, replacing tags    * [simple_xlsx_writer](https://github.com/harvesthq/simple_xlsx_writer) - Just as the name says, simple writter for Office 2007+ Excel files    * [remote_table](https://github.com/seamusabshere/remote_table) - Open local or remote XLSX, XLS, ODS, CSV (comma separated), TSV (tab separated), other delimited, fixed-width files, and Google Docs.    * [acts_as_caxlsx](https://github.com/caxlsx/acts_as_caxlsx) - acts_as_caxlsx lets you turn any ActiveRecord::Base inheriting class into an excel spreadsheet.    * [activeadmin-caxlsx](https://github.com/caxlsx/activeadmin-caxlsx) - This gem uses caxlsx to provide excel/xlsx downloads for resources in Active Admin.    * [to_spreadsheet](https://github.com/glebm/to_spreadsheet) - Render XLSX from Rails using existing views    * [write_xlsx](https://github.com/cxn03651/write_xlsx) - write_xlsx is a gem to create a new file in the Excel 2007+ XLSX format.    * [excel_rails](https://github.com/asanghi/excel_rails) - Allows you to program spreadsheets using .rxls views    * [sheets](https://github.com/bspaulding/Sheets) - Work with spreadsheets easily in a native ruby format.    * [workbook](https://github.com/murb/workbook) - Workbook contains workbooks, as in a table, contains rows, contains cells, reads/writes excel, ods and csv and tab separated files...    * [Spreadsheet report](https://github.com/gnoso/spreadsheet_report) - Simple tool for running queries against ActiveRecord and putting them into a Google Spreadsheet.    * [oxcelix](https://github.com/gbiczo/oxcelix) - A fast Excel 2007/2010 (.xlsx) file parser that returns a collection of Matrix objects    * [wrap_excel](https://github.com/tomiacannondale/wrap_excel) - WrapExcel is to wrap the win32ole, and easy to use Excel operations with ruby. Detailed description please see the README.    * [write_xlsx_rails](https://github.com/maxd/write_xlsx_rails) - xlsx renderer for Rails base on write_xlsx gem    * [Fastsheet](https://github.com/dkkoval/fastsheet) - Fast spreadsheet reader using Rust native extensions.    ## Scheduling      * [whenever](https://github.com/javan/whenever) Cron jobs in Ruby    * [resque-scheduler](https://github.com/resque/resque-scheduler) A light-weight job scheduling system built on top of resque    * [rufus-scheduler](https://github.com/jmettraux/rufus-scheduler) Scheduler for Ruby    * [Clockwork](https://github.com/tomykaira/clockwork) Clockwork is a cron replacement. It runs as a lightweight, long-running Ruby process which sits alongside your web processes (Mongrel/Thin) and your worker processes (DJ/Resque/Minion/Stalker) to schedule recurring work at particular times or dates.    ## Science and Data Analysis    *Libraries for scientific computing and data analyzing.*    ## Search    *Libraries and software for indexing and performing search queries on data.*      * [Thinking Sphinx](https://github.com/pat/thinking-sphinx) - Sphinx plugin for ActiveRecord/Rails    * [elasticsearch-ruby](https://github.com/elastic/elasticsearch-ruby) - Ruby integrations for Elasticsearch    * [Searchkick](https://github.com/ankane/searchkick) - Intelligent search made easy    * [Algoliasearch Rails](https://github.com/algolia/algoliasearch-rails/) - AlgoliaSearch integration to your favorite ORM    * [PgSearch](https://github.com/Casecommons/pg_search) - PostgreSQL's full text search    * [Rroonga](https://github.com/ranguba/rroonga) - The Ruby bindings of Groonga    * [Sunspot](https://github.com/sunspot/sunspot) - Solr-powered search for Ruby objects    ## Site Monitoring    *Libs for analytics, monitoring*      * [rack-google-analytics](https://github.com/kangguru/rack-google-analytics) Simple Rack middleware for implementing google analytics tracking    * [DataDog](https://github.com/DataDog/dogapi-rb) A monitoring service for IT, operations and development teams    * [Instrumental](https://github.com/Instrumental/instrumental_agent-ruby) High-scale, non-blocking agent for [Instrumental](https://instrumentalapp.com) application monitoring    * [Keen IO](https://github.com/keenlabs/keen-gem) Build analytics features directly into your Ruby apps    ## Static Page Generation      * [jekyll](https://github.com/jekyll/jekyll) A blog-aware, static site generator in Ruby    * [middleman](https://github.com/middleman/middleman)    ## Starter Apps    *App templates for creating apps quickly*      * [suspenders](https://github.com/thoughtbot/suspenders) A Rails template with our standard defaults, ready to deploy to Heroku    * [ruby2-rails4-bootstrap-heroku](https://github.com/diowa/ruby2-rails4-bootstrap-heroku) A starter application based on Ruby 2, Rails 4 and Bootstrap for Sass, deployable on Heroku    * [rails-bootstrap](https://github.com/RailsApps/rails-bootstrap) Rails 4.1 starter app with the Bootstrap front-end framework    * [rails4-starterkit](https://github.com/starterkits/rails4-starterkit) Rails 4.1 starter app with production ready performance, security, and authentication    * [cybele](https://github.com/lab2023/cybele) - Rails 4.x template with responder, simple form, haml, exception notification, etc ...    ## Text Processing    *Libraries for parsing and manipulating texts.*      * General      * Specific Formats      * Parser        * [Yomu](https://github.com/Erol) - Read text and metadata from files and documents (.doc, .docx, .pages, .odt, .rtf, .pdf)    ## Tagging    *Libraries for tagging items.*      * [acts-as-taggable-on](https://github.com/mbleigh/acts-as-taggable-on) - A tagging plugin for Rails applications that allows for custom tagging along dynamic contexts.    ## Template Engine    *Libraries and tools for templating and lexing.*      * [Slim](https://github.com/slim-template/slim) A templating lang that reduce the syntax to the essential parts without becoming cryptic.      * [slim-rails](https://github.com/slim-template/slim-rails) Rails port of Slim lang    * [Haml](https://github.com/haml/haml) HTML Abstraction Markup Language - A Markup Haiku      * [haml-rails](https://github.com/indirect/haml-rails) Rails port of Haml lang    * [Tilt](https://github.com/rtomayko/tilt)    * [Liquid](https://github.com/Shopify/liquid)    ## Testing    *Libraries for testing codebases and generating test data.*      * Testing Frameworks      * [RSpec](http://rspec.info/) - BDD for Ruby      * [MiniTest](https://github.com/seattlerb/minitest) - minitest provides a complete suite of testing facilities supporting TDD, BDD, mocking, and benchmarking      * [Cucumber]         * [Cucumber Github](https://github.com/cucumber/cucumber/wiki) - Cucumber is a tool that executes plain-text functional descriptions as automated tests         * [Cucumber Site](https://cucumber.io/) - Behaviour Driven Development with elegacy and joy      * [Spinach](https://github.com/codegram/spinach) - Spinach is a high-level BDD framework that leverages the expressive Gherkin language (used by Cucumber) to help you define executable specifications of your application or library's acceptance criteria.      * [Rubytest](http://rubyworks.github.io/rubytest) - Rubytest is a testing meta-framework useful for creating highly customize test suites or building whole new test frameworks.         * [BRASS](http://rubyworks.github.io/brass) - Bare-metal Ruby assertion system standard used by Rubytest.         * [Lemon](http://rubyworks.github.io/lemon) - Strict unit test system built on top of Rubytest.      * [shoulda-matchers](https://github.com/thoughtbot/shoulda-matchers) - Collection of testing matchers extracted from Shoulda      * [capybara](https://github.com/jnicklas/capybara) - Acceptance test framework for web applications    * Mock      * [RSpec-mocks](https://github.com/rspec/rspec-mocks) - RSpec's 'test double' framework, with support for stubbing and mocking      * [Mocha](http://gofreerange.com/mocha/docs/) - Mocking and stubbing library with JMock/SchMock syntax, which allows mocking and stubbing of methods on real (non-mock) classes.      * [FlexMock](https://github.com/jimweirich/flexmock) - Flexible mocking for Ruby testing    * Fake Data      * [Faker](https://github.com/stympy/faker) - A library for generating fake data such as names, addresses, and phone numbers      * [ffaker](https://github.com/ffaker/ffaker) - Faker Refactored.      * [Forgery](https://github.com/sevenwire/forgery) - Easy and customizable generation of forged data.    * Code Coverage      * [simplecov](https://github.com/colszowka/simplecov) Code coverage for Ruby 1.9+ with a powerful configuration library and automatic merging of coverage    * Load Testing      * Error Handler    ## Third-party APIs    *Libraries for accessing third party APIs.*      * [koala](https://github.com/arsduo/koala) A lightweight, flexible library for Facebook    * [fb_graph](https://github.com/nov/fb_graph) A full-stack Facebook Graph API wrapper    * [twitter](https://github.com/sferik/twitter) A Ruby interface to the Twitter API    * [tweetstream](https://github.com/tweetstream/tweetstream) A simple library for consuming Twitter's Streaming API    * [gitlab](https://github.com/NARKOZ/gitlab) Ruby wrapper and CLI for the GitLab API    * [octokit.rb](https://github.com/octokit/octokit.rb) Ruby toolkit for the GitHub API    * [instagram](https://github.com/Instagram/instagram-ruby-gem) The official gem for the Instagram API    * [linkedin](https://github.com/hexgnu/linkedin) Ruby wrapper for the LinkedIn API    * [twilio-ruby](https://github.com/twilio/twilio-ruby) A Ruby gem for communicating with the Twilio API and generating TwiML    * [viewpoint-spws](https://github.com/zenchild/viewpoint-spws) A Microsoft Sharepoint Web Services library for Ruby.    * [youtube_it](https://github.com/kylejginavan/youtube_it) An object-oriented Ruby wrapper for the YouTube GData API    * [flickraw](https://github.com/hanklords/flickraw) Flickraw is a library to access flickr api    * [f00px](https://github.com/500px/f00px) Official 500px api ruby gem    * [rspotify](https://github.com/guilhermesad/rspotify) Ruby wrapper for the Spotify Web API    ## URL Manipulation    *Libraries for parsing URLs.*    ## Video    *Libraries for manipulating video and GIFs.*      * [streamio-ffmpeg](https://github.com/streamio/streamio-ffmpeg) Simple yet powerful ruby FFmpeg wrapper for reading metadata and transcoding movies    ## Web Content Extracting    *Libraries for extracting web contents.*    ## Web Crawling    *Libraries for scraping websites.*      * [upton](https://github.com/propublica/upton) A batteries-included framework for easy web-scraping    * [metainspector](https://github.com/jaimeiniesta/metainspector)    ## Web Frameworks    *Web development frameworks.*      * [Ruby On Rails](http://rubyonrails.org/) - Ruby on Rails is a full-stack web framework optimized for programmer happiness and sustainable productivity    * [Sinatra](http://www.sinatrarb.com/) - Sinatra is a DSL for quickly creating web applications in Ruby with minimal effort.    * [Padrino](http://www.padrinorb.com/) - The Godfather of Sinatra provides a full-stack agnostic framework on top of Sinatra    * [Cramp](http://cramp.in/) - Cramp is a fully asynchronous real-time web application framework in Ruby    * [Lotus](http://lotusrb.org/) - A newborn complete Ruby web framework that is simple, fast and lightweight.    * [Cuba](http://cuba.is/) - Cuba is a microframework for web development originally inspired by Rum, a tiny but powerful mapper for Rack applications.    * [Pakyow](https://pakyow.com/) - Pakyow is an open-source framework for the modern web. Build working software faster with a development process that remains friendly to both designers and developers. It's built for getting along.    * [Hyperstack](https://hyperstack.org/) - A complete Isomorphic Ruby Framework using React, Opal and Rails    ## Web Servers    *App server interface*      * [puma](https://github.com/puma/puma) A simple, fast, threaded, and highly concurrent HTTP 1.1 server for Ruby/Rack applications.    * [thin](https://github.com/macournoyer/thin) A thin and fast web server    * [trinidad](https://github.com/trinidad/trinidad) Run Rails or Rack applications within an embedded Apache Tomcat container.    * [unicorn](https://github.com/defunkt/unicorn) An HTTP server for Rack applications designed to only serve fast clients.    * [passenger](https://github.com/phusion/passenger) A modern web server and application server for Ruby, Python, and Node.js.    * [pow](https://github.com/basecamp/pow) Pow treats files and directories as ruby objects giving you more power and flexibility.    * [goliath](https://github.com/postrank-labs/goliath) is a non-blocking Ruby web server framework.    ## WebSocket    *Libraries for woking with WebSocket.*      * [Faye](http://faye.jcoglan.com/ruby.html) - Simple pub/sub messaging for the web    * [websocket-rails](https://github.com/websocket-rails/websocket-rails) - Plug and play websocket support for ruby on rails.    # Miscellaneous    *Useful libraries or tools that don't fit in the categories above.*      * [packetfu](https://github.com/packetfu/packetfu) A mid-level packet manipulation library for Ruby.    * [chatterbot](https://github.com/muffinista/chatterbot) A straightforward ruby-based Twitter Bot Framework, using OAuth to authenticate    * [sneakers](https://github.com/jondot/sneakers) A fast background processing framework for Ruby and RabbitMQ    * [ransack](https://github.com/activerecord-hackery/ransack) Object-based searching.    * [cinch](https://github.com/cinchrb/cinch) The IRC Bot Building Framework    * [pry](https://github.com/pry/pry) An IRB alternative and runtime developer console    * [rib](http://rib.godfat.org/) A lightweight and extensible IRB replacement    * [jazz_hands](https://github.com/nixme/jazz_hands/) Pry-based enhancements for the default Rails 3 and 4 consoles    * [awesome_print](https://github.com/awesome-print/awesome_print) A Ruby library that pretty prints Ruby objects in full color exposing their internal structure with proper indentation.    * [friendly_id](https://github.com/norman/friendly_id) Slugging and permalink plugins for ActiveRecord    * [backup](https://github.com/backup/backup) An elegant DSL in Ruby for performing backups on UNIX-like systems    * [kss](https://github.com/kneath/kss) Documenting CSS and generating styleguides    * [AASM](https://github.com/aasm/aasm) - A library for adding finite state machines to Ruby classes    * [JsonCompare](https://github.com/a2design-inc/json-compare) - Returns the difference between two JSON files    * [blankable](https://github.com/lab2023/blankable) - Adds blank slates to index view in Rails    * [tcmb_currency](https://github.com/lab2023/tcmb_currency) - T.C.M.B. currencies for Money Gem    * [enumerize](https://github.com/brainspec/enumerize) - Enumerated attributes with I18n and ActiveRecord/Mongoid support    * [lol_dba](https://github.com/plentz/lol_dba) - lol_dba is a small package of rake tasks that scan your application models and displays a list of columns that probably should be indexed.    * [annotate-models](https://github.com/ctran/annotate_models) - Annotate ActiveRecord models    * [fast_attributes](https://github.com/applift/fast_attributes) - FastAttributes adds attributes with their types to the class    * [Github Changelog Generator](https://github.com/skywinder/Github-Changelog-Generator) — automatically generate change log from your tags, issues, labels and pull requests on GitHub.    * [Letter Opener](https://github.com/ryanb/letter_opener) — Preview email in the default browser instead of sending it.    * [Auto HTML](https://github.com/dejan/auto_html) — Transforming URLs to appropriate resource (image, link, YouTube, Vimeo video,...).    * [OctoLinker](https://github.com/OctoLinker/browser-extension) - Navigate through projects on GitHub.com efficiently with the OctoLinker browser extension.    * [BetterDocs](https://github/khusnetdinov/betterdocs) - Documentation with collection practices in ruby. Good for new ruby developers and beginners.      ## Editor Plugins    *Plugins for various editors.*      * [vim-ruby](https://github.com/vim-ruby/vim-ruby) Vim/Ruby Configuration Files    * [vim-rails](https://github.com/tpope/vim-rails) rails.vim: Ruby on Rails power tools    # Resources    *Where to discover things (libraries, news e.g) about Ruby.*      * [The Ruby Toolbox](https://www.ruby-toolbox.com/)    * [RubyGems](https://rubygems.org/)    * [RubyDaily](http://rubydaily.org) - Community driven news    * [Ruby Weekly](http://rubyweekly.com/) - A free, once–weekly e-mail round-up of Ruby news and articles.    * [Ruby5](https://ruby5.codeschool.com/) - The latest news in the Ruby and Rails community    * [RubyFlow](http://www.rubyflow.com) - Ruby Programming Community Link Blog    * [Ruby Curated Resources](https://hackr.io/tutorials/learn-ruby)    ## People to Follow    *People in Ruby World*      * [Yukihiro ""Matz"" Matsumoto](https://twitter.com/yukihiro_matz) - Creator of Ruby lang    * [David Heinemeier Hansson](https://twitter.com/dhh) - Creator of Rails framework    * [Koichi Sasada](https://github.com/ko1) - Ruby core committer and the developer of YARV    * [Aaron Patterson](http://tenderlovemaking.com/) - Committer to Nokogiri, Ruby, and Ruby on Rails    * [Avdi Grimm](http://devblog.avdi.org/) - Host of Ruby Tapas webcasts    * [Aman Gupta](http://tmm1.net/)- Ruby core committer      # Other Awesome Lists    Other amazingly awesome lists can be found in the [awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness) list. """
Big data;https://github.com/caskdata/tigon;"""# Tigon    ![Tigon Logo](/tigon-docs/developer-guide/source/_images/tigon.png)    **Introduction**    **Tigon** is an open-source, real-time, low-latency, high-throughput stream processing framework.    Tigon is a collaborative effort between Cask Data, Inc. and AT&T that combines   technologies from these companies to create a disruptive new framework to handle a diverse  set of real-time streaming requirements.    Cask Data has built technology that provides scalable, reliable, and persistent high-throughput  event processing with high-level Java APIs using Hadoop and HBase.    AT&T has built a streaming engine that provides massively scalable, flexible, and in-memory  low-latency stream processing with a SQL-like query Language.    Together, they have combined to create **Tigon**.    There are many applications that can take advantage of its features:    - Ability to handle extremely large data flows;  - Exactly-once event processing using an app-level Java API with consistency, reliability, and persistence;  - Streaming database using a SQL-like language to filter, group and join data streams in-memory;  - Runs collections of queries using pipelined query plans;  - Able to transparently handle complex record routing in large parallelized implementations;  - Runs and scales as a native Apache Hadoop YARN Application;  - Reads, writes, and tightly integrates with HDFS and HBase;  - Supports a significant amount of parallelization;  - Fault-tolerance and horizontal scalability without burdening the developer;  - Enterprise security features with debugging, logging, and monitoring tools; and  - Simpler programming model, tools and UI; and  - Open-source software and development process.    For more information, see our collection of   [Guides and other documentation](http://docs.cask.co/tigon/current/en/index.html).    ## Is It Building?    Builds                                                              ------------------------------------------------------------------  [Bamboo Build](https://builds.cask.co/browse/TIGON)                   [GitHub Version](https://github.com/caskdata/tigon/releases/latest)       ## Getting Started    ### Prerequisites    Tigon is supported on *NIX systems such as Linux and Macintosh OS X.  It is not supported on Microsoft Windows.    To install and use Tigon and its included examples, there are a few prerequisites:      1. JDK 6 or JDK 7 (required to run Tigon; note that $JAVA_HOME should be set)    2. GCC    3. G++    4. Apache Maven 3.0+ (required to build the example applications)      Note: To run the TigonSQL Stream Engine outside of Tigon, libz, Perl 5.x, and Python 3.x are required.    ### Download    Pre-compiled sources and related files can be downloaded in a zip file:   [tigon-developer-release-0.2.0.zip.](http://repository.cask.co/downloads/co/cask/tigon/tigon-developer-release/0.2.0/tigon-developer-release-0.2.0.zip)    ### Install     Once the download has completed, unzip the file in a suitable location.    ### Run Instructions    To run Tigon in standalone mode:        $ run_standalone.sh <path-to-flow-jar> <flow-class-name> <run-time-args>    To run Tigon in distributed mode:        $ run_distributed.sh <zookeeper-quorum> <hdfs-namespace>    ### Building from Source    You can also build Tigon directly from the latest source code:        git clone https://github.com/caskdata/tigon.git      cd tigon      mvn clean package -DskipTests -Pdist    After the build completes, you will have a distribution of Tigon under the  `tigon-distribution/target/` directory.      Take the `tigon-sdk-<version>.zip` file and unzip it into a suitable location.      ## Getting Started Guide    Visit our web site for a [Getting Started Guide](http://docs.cask.co/docs/tigon/current/en/getting-started.html)  that will guide you through installing Tigon and running an example.        ## Where to Go Next    Now that you've had a look at the Tigon SDK, take a look at:    - Examples, located in the `/tigon-examples` directory of Tigon  - [Online Examples](http://docs.cask.co/tigon/current/en/examples/index.html)     (demonstrating basic features of Tigon) are located on-line  - [Reference Applications:](https://github.com/caskdata/tigon-apps)    - [AdNetworkFlow:](https://github.com/caskdata/tigon-apps/tree/develop/AdNetworkFlow)      Demonstrates using Tigon to write a realtime bidding (RTB) advertisement framework  - Developer Guides, located in the source distribution in `/tigon-docs/developer-guide/source`    or [online](http://docs.cask.co/tigon/current/en/developer.html)      ## How to Contribute    Interested in helping to improve Tigon? We welcome all contributions, whether in filing detailed  bug reports, submitting pull requests for code changes and improvements, or by asking questions and  assisting others on the mailing list.    ### Bug Reports & Feature Requests    Bugs and tasks are tracked in a public JIRA issue tracker. Details on access will be forthcoming.    ### Pull Requests    We have a simple pull-based development model with a consensus-building phase, similar to Apache's  voting process. If you’d like to help make Tigon better by adding new features, enhancing existing  features, or fixing bugs, here's how to do it:    1. If you are planning a large change or contribution, discuss your plans on the `cask-tigon-dev`     mailing list first.  This will help us understand your needs and best guide your solution in a     way that fits the project.  2. Fork Tigon into your own GitHub repository.  3. Create a topic branch with an appropriate name.  4. Work on the code to your heart's content.  5. Once you’re satisfied, create a pull request from your GitHub repo (it’s helpful if you fill in     all of the description fields).  6. After we review and accept your request, we’ll commit your code to the cask/tigon     repository.    Thanks for helping to improve Tigon!    ### Mailing List    Tigon User Group:   [tigon-user@googlegroups.com](https://groups.google.com/d/forum/tigon-user)    Tigon Development Discussion:   [tigon-dev@googlegroups.com](https://groups.google.com/d/forum/tigon-dev)      ### IRC Channel    Tigon IRC Channel #tigon on irc.freenode.net      ## License and Trademarks    Copyright © 2014 Cask Data, Inc.    Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except  in compliance with the License. You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software distributed under the   License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,   either express or implied. See the License for the specific language governing permissions   and limitations under the License.    Cask is a trademark of Cask Data, Inc. All rights reserved.    Apache, Apache HBase, and HBase are trademarks of The Apache Software Foundation. Used with  permission. No endorsement by The Apache Software Foundation is implied by the use of these marks. """
Big data;https://github.com/monksy/awesome-kafka;"""# awesome-kafka    This list is for anyone wishing to learn about [Apache Kafka](http://kafka.apache.org/), but do not have a starting point.    #### How to contribute    Fork the repository, create a contribution, and create a pull request against monksy/awesome-kafka:master.      Table of Contents  =================       * [Learning/Resources](learning.md)     * [Tools/Utilities/Monitoring](tools.md)     * [Client Libraries](clients.md)     * [Libraries with Kafka Support](libraries.md)     * [Kafka Connectors](connectors.md)     * [Testing](testing.md)     * [Projects with Kafka Integrations](integrations.md)     * [Social Resources](social.md)       ## Requests for Help/TODO      - Learning: Organize articles into tutorials, case studies, product related, reference, etc   - Create a section for resources that will help to quickstart learning for kafka and kafka streams   - Add about sections per each page.   - Merge in repo: https://github.com/semantalytics/awesome-kafka   - Find Example projects with kafka   - Organize learning.md by the types/subsystem that they are using   - Add section headers   - Find blogs that are dedicated to Kafka   - Create sections for all the sub-sections      - Kafka Streams       - KSQL       - Kafka Connect      - etc   - Solicit for help from the community    - Get involved with Kafka community (gitter/slack/irc/mailing list)   - Fix links in learning   - Verify links work     ## Where did this information come from?     - Originally from: https://github.com/infoslack/awesome-kafka   - Client Libraries, Tools, etc from https://github.com/dharmeshkakadia/awesome-kafka   - Awesome-data-engineering list for kafka: https://github.com/monksy/awesome-data-engineering"""
Big data;https://github.com/dagster-io/dagster;"""<p align=""center"">  <img src=""assets/dagster-logo.png"" />  <br /><br />  <a href=""https://badge.fury.io/py/dagster""><img src=""https://badge.fury.io/py/dagster.svg""></>  <a href=""https://coveralls.io/github/dagster-io/dagster?branch=master""><img src=""https://coveralls.io/repos/github/dagster-io/dagster/badge.svg?branch=master""></a>  <a href=""https://buildkite.com/dagster/dagster""><img src=""https://badge.buildkite.com/888545beab829e41e5d7303db15525a2bc3b0f0e33a72759ac.svg?branch=master""></a>  <a href=""https://dagster-slackin.herokuapp.com/""><img src=""https://dagster-slackin.herokuapp.com/badge.svg""></a>  </p>    # Dagster    An orchestration platform for the development, production, and observation of data assets.    Dagster lets you define jobs in terms of the data flow between reusable, logical components, then test locally and run anywhere. With a unified view of jobs and the assets they produce, Dagster can schedule and orchestrate Pandas, Spark, SQL, or anything else that Python can invoke.    Dagster is designed for data platform engineers, data engineers, and full-stack data scientists. Building a data platform with Dagster makes your stakeholders more independent and your systems more robust. Developing data pipelines with Dagster makes testing easier and deploying faster.    ### Develop and test locally, then deploy anywhere    With Dagster’s pluggable execution, the same computations can run in-process against your local file system, or on a distributed work queue against your production data lake. You can set up Dagster’s web interface in a minute on your laptop, deploy it on-premise, or in any cloud.    ### Model and type the data produced and consumed by each step    Dagster models data dependencies between steps in your orchestration graph and handles passing data between them. Optional typing on inputs and outputs helps catch bugs early.    ### Link data to computations    Dagster’s Asset Manager tracks the data sets and ML models produced by your jobs, so you can understand how they were generated and trace issues when they don’t look how you expect.    ### Build a self-service data platform    Dagster helps platform teams build systems for data practitioners. Jobs are built from shared, reusable, configurable data processing and infrastructure components. Dagit, Dagster’s web interface, lets anyone inspect these objects and discover how to use them.    ### Avoid dependency nightmares    Dagster’s repository model lets you isolate codebases so that problems in one job don’t bring down the rest. Each job can have its own package dependencies and Python version. Jobs are run in isolated processes so user code issues can't bring the system down.    ### Debug pipelines from a rich UI    Dagit, Dagster’s web interface, includes expansive facilities for understanding the jobs it orchestrates. When inspecting a run of your job, you can query over logs, discover the most time consuming tasks via a Gantt chart, re-execute subsets of steps, and more.    ## Getting Started    ### Installation    Dagster is available on PyPI, and officially supports Python 3.6+.    ```bash  $ pip install dagster dagit  ```    This installs two modules:    - **Dagster**: the core programming model and abstraction stack; stateless, single-node,    single-process and multi-process execution engines; and a CLI tool for driving those engines.  - **Dagit**: the UI for developing and operating Dagster pipelines, including a DAG browser, a    type-aware config editor, and a live execution interface.    ### Learn    Next, jump right into our [tutorial](https://docs.dagster.io/tutorial/), or read our [complete  documentation](https://docs.dagster.io). If you're actively using Dagster or have questions on  getting started, we'd love to hear from you:    <br />  <p align=""center"">  <a href=""https://join.slack.com/t/dagster/shared_invite/enQtNjEyNjkzNTA2OTkzLTI0MzdlNjU0ODVhZjQyOTMyMGM1ZDUwZDQ1YjJmYjI3YzExZGViMDI1ZDlkNTY5OThmYWVlOWM1MWVjN2I3NjU""><img src=""https://user-images.githubusercontent.com/609349/63558739-f60a7e00-c502-11e9-8434-c8a95b03ce62.png"" width=160px; /></a>  </p>    ## Contributing    For details on contributing or running the project for development, check out our [contributing  guide](https://docs.dagster.io/community/contributing/). <br />    ## Integrations    Dagster works with the tools and systems that you're already using with your data, including:    <table>  	<thead>  		<tr style=""background-color: #ddd"" align=""center"">  			<td colspan=2><b>Integration</b></td>  			<td><b>Dagster Library</b></td>  		</tr>  	</thead>  	<tbody>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/57987547-a7e36b80-7a37-11e9-95ae-4c4de2618e87.png""></td>  			<td style=""border-left: 0px""> <b>Apache Airflow</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-airflow"" />dagster-airflow</a><br />Allows Dagster pipelines to be scheduled and executed, either containerized or uncontainerized, as <a href=""https://github.com/apache/airflow"">Apache Airflow DAGs</a>.</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/57987976-5ccc5700-7a3d-11e9-9fa5-1a51299b1ccb.png""></td>  			<td style=""border-left: 0px""> <b>Apache Spark</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-spark"" />dagster-spark</a> &middot; <a href=""https://docs.dagster.io/_apidocs/libraries/dagster-pyspark"" />dagster-pyspark</a>  			<br />Libraries for interacting with Apache Spark and PySpark.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/58348728-48f66b80-7e16-11e9-9e9f-1a0fea9a49b4.png""></td>  			<td style=""border-left: 0px""> <b>Dask</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-dask"" />dagster-dask</a>  			<br />Provides a Dagster integration with Dask / Dask.Distributed.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/58349731-f36f8e00-7e18-11e9-8a2e-86e086caab66.png""></td>  			<td style=""border-left: 0px""> <b>Datadog</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-datadog"" />dagster-datadog</a>  			<br />Provides a Dagster resource for publishing metrics to Datadog.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987809-bf245800-7a3b-11e9-8905-494ed99d0852.png"" />  			&nbsp;/&nbsp; <img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987827-fa268b80-7a3b-11e9-8a18-b675d76c19aa.png"">  			</td>  			<td style=""border-left: 0px""> <b>Jupyter / Papermill</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagstermill"" />dagstermill</a><br />Built on the <a href=""https://github.com/nteract/papermill"">papermill library</a>, dagstermill is meant for integrating productionized Jupyter notebooks into dagster pipelines.</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle""  src=""https://user-images.githubusercontent.com/609349/57988016-f431aa00-7a3d-11e9-8cb6-1309d4246b27.png""></td>  			<td style=""border-left: 0px""> <b>PagerDuty</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-pagerduty"" />dagster-pagerduty</a>  			<br />A library for creating PagerDuty alerts from Dagster workflows.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/58349397-fcac2b00-7e17-11e9-900c-9ab8cf7cb64a.png""></td>  			<td style=""border-left: 0px""> <b>Snowflake</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-snowflake"" />dagster-snowflake</a>  			<br />A library for interacting with the Snowflake Data Warehouse.  			</td>  		</tr>  		<tr style=""background-color: #ddd"">  			<td colspan=2 align=""center""><b>Cloud Providers</b></td>  			<td><b></b></td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987557-c2b5e000-7a37-11e9-9310-c274481a4682.png""> </td>  			<td style=""border-left: 0px""><b>AWS</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-aws"" />dagster-aws</a>  			<br />A library for interacting with Amazon Web Services. Provides integrations with Cloudwatch, S3, EMR, and Redshift.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/84176312-0bbb4680-aa36-11ea-9580-a70758b12161.png""> </td>  			<td style=""border-left: 0px""><b>Azure</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-azure"" />dagster-azure</a>  			<br />A library for interacting with Microsoft Azure.  			</td>  		</tr>  		<tr>  			<td align=""center"" style=""border-right: 0px""><img style=""vertical-align:middle"" src=""https://user-images.githubusercontent.com/609349/57987566-f98bf600-7a37-11e9-81fa-b8ca1ea6cc1e.png""> </td>  			<td style=""border-left: 0px""><b>GCP</b></td>  			<td><a href=""https://docs.dagster.io/_apidocs/libraries/dagster-gcp"" />dagster-gcp</a>  			<br />A library for interacting with Google Cloud Platform. Provides integrations with GCS, BigQuery, and Cloud Dataproc.  			</td>  		</tr>  	</tbody>  </table>    This list is growing as we are actively building more integrations, and we welcome contributions! """
Big data;https://github.com/chihming/awesome-network-embedding;"""# awesome-network-embedding  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  [![Gitter chat for developers at https://gitter.im/dmlc/xgboost](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/awesome-network-embedding/Lobby)    Also called network representation learning, graph embedding, knowledge embedding, etc.    The task is to learn the representations of the vertices from a given network.    CALL FOR HELP: I'm planning to re-organize the papers with clear classification index in the near future. Please feel free to submit a commit if you find any interesting related work:)    <img src=""NE.png"" width=""480"">    # Paper References with the implementation(s)  - **GraphGym**    - A platform for designing and evaluating Graph Neural Networks (GNN), NeurIPS 2020    - [[Paper]](https://proceedings.neurips.cc/paper/2020/file/c5c3d4fe6b2cc463c7d7ecba17cc9de7-Paper.pdf)    - [[Python]](https://github.com/snap-stanford/graphgym)  - **FEATHER**    - Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models, CIKM 2020    - [[Paper]](https://arxiv.org/abs/2005.07959)    - [[Python]](https://github.com/benedekrozemberczki/FEATHER)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **HeGAN**    - Adversarial Learning on Heterogeneous Information Networks, KDD 2019    - [[Paper]](https://fangyuan1st.github.io/paper/KDD19_HeGAN.pdf)    - [[Python]](https://github.com/librahu/HeGAN)  - **NetMF**    - Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec, WSDM 2018    - [[Paper]](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **GL2Vec**    - GL2vec: Graph Embedding Enriched by Line Graphs with Edge Features, ICONIP 2019    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **NNSED**    - A Non-negative Symmetric Encoder-Decoder Approach for Community Detection, CIKM 2017    - [[Paper]](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)   - **SymmNMF**    - Symmetric Nonnegative Matrix Factorization for Graph Clustering, SDM 2012    - [[Paper]](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **RECT**    - Network Embedding with Completely-Imbalanced Labels, TKDE 2020    - [[Paper]](https://zhengwang100.github.io/pdf/TKDE20_wzheng.pdf)    - [[Python]](https://github.com/zhengwang100/RECT)   - **GEMSEC**    - GEMSEC: Graph Embedding with Self Clustering, ASONAM 2019    - [[Paper]](https://arxiv.org/abs/1802.03997)    - [[Python]](https://github.com/benedekrozemberczki/GEMSEC)   - **AmpliGraph**    - Library for learning knowledge graph embeddings with TensorFlow     - [[Project]](http://docs.ampligraph.org)    - [[code]](https://github.com/Accenture/AmpliGraph)  - **jodie**    - Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks, KDD'19    - [[Project]](http://snap.stanford.edu/jodie/)    - [[Code]](https://github.com/srijankr/jodie/)  - **PyTorch-BigGraph**    - Pytorch-BigGraph - a distributed system for learning graph embeddings for large graphs, SysML'19    - [[github]](https://github.com/facebookresearch/PyTorch-BigGraph)  - **ATP**    - ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation, AAAI'19    - [[paper]](https://arxiv.org/abs/1811.00839)    - [[code]](https://github.com/zhenv5/atp)  - **MUSAE**    - Multi-scale Attributed Node Embedding, ArXiv 2019    - [[paper]](https://arxiv.org/abs/1909.13021)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/MUSAE)  - **SEAL-CI**    - Semi-Supervised Graph Classification: A Hierarchical Graph Perspective, WWW'19    - [[paper]](https://arxiv.org/pdf/1904.05003.pdf)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/SEAL-CI)  - **N-GCN and MixHop**    - A Higher-Order Graph Convolutional Layer, NIPS'18 (workshop)    - [[paper]](http://sami.haija.org/papers/high-order-gc-layer.pdf)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/MixHop-and-N-GCN)  - **CapsGNN**    - Capsule Graph Neural Network, ICLR'19    - [[paper]](https://openreview.net/forum?id=Byl8BnRcYm)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/CapsGNN)  - **Splitter**    - Splitter: Learning Node Representations that Capture Multiple Social Contexts, WWW'19    - [[paper]](http://epasto.org/papers/www2019splitter.pdf)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/Splitter)  - **REGAL**    - REGAL: Representation Learning-based Graph Alignment. International Conference on Information and Knowledge Management, CIKM'18    - [[arxiv]](https://arxiv.org/pdf/1802.06257.pdf)    - [[paper]](https://dl.acm.org/citation.cfm?id=3271788)    - [[code]](https://github.com/GemsLab/REGAL)  - **PyTorch Geometric**    - Fast Graph Representation Learning With PyTorch Geometric    - [[paper]](https://arxiv.org/pdf/1903.02428.pdf)    - [[Python PyTorch]](https://github.com/rusty1s/pytorch_geometric)  - **TuckER**    - Tensor Factorization for Knowledge Graph Completion, Arxiv'19    - [[paper]](https://arxiv.org/pdf/1901.09590.pdf)    - [[Python PyTorch]](https://github.com/ibalazevic/TuckER)  - **HypER**    - Hypernetwork Knowledge Graph Embeddings, Arxiv'18    - [[paper]](https://arxiv.org/pdf/1808.07018.pdf)    - [[Python PyTorch]](https://github.com/ibalazevic/HypER)  - **GWNN**    - Graph Wavelet Neural Network, ICLR'19    - [[paper]](https://openreview.net/forum?id=H1ewdiR5tQ)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork)    - [[Python TensorFlow]](https://github.com/Eilene/GWNN)  - **APPNP**    - Combining Neural Networks with Personalized PageRank for Classification on Graphs, ICLR'19    - [[paper]](https://arxiv.org/abs/1810.05997)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/APPNP)    - [[Python TensorFlow]](https://github.com/klicperajo/ppnp)  - **role2vec**    - Learning Role-based Graph Embeddings, IJCAI'18    - [[paper]](https://arxiv.org/pdf/1802.02896.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/role2vec)  - **AttentionWalk**    - Watch Your Step: Learning Node Embeddings via Graph Attention, NIPS'18    - [[paper]](https://arxiv.org/pdf/1710.09599.pdf)    - [[Python]](http://sami.haija.org/graph/context)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/AttentionWalk)    - [[Python TensorFlow]](https://github.com/google-research/google-research/tree/master/graph_embedding/watch_your_step/)  - **GAT**    - Graph Attention Networks, ICLR'18    - [[paper]](https://arxiv.org/pdf/1710.10903.pdf)    - [[Python PyTorch]](https://github.com/Diego999/pyGAT)    - [[Python TensorFlow]](https://github.com/PetarV-/GAT)  - **SINE**    - SINE: Scalable Incomplete Network Embedding, ICDM'18    - [[paper]](https://github.com/benedekrozemberczki/SINE/blob/master/paper.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python PyTorch]](https://github.com/benedekrozemberczki/SINE/)    - [[C++]](https://github.com/daokunzhang/SINE)  - **SGCN**    - Signed Graph Convolutional Network, ICDM'18    - [[paper]](https://github.com/benedekrozemberczki/SGCN/blob/master/sgcn.pdf)    - [[Python]](https://github.com/benedekrozemberczki/SGCN)  - **TENE**    - Enhanced Network Embedding with Text Information, ICPR'18    - [[paper]](https://github.com/benedekrozemberczki/TENE/blob/master/tene_paper.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/TENE)   - **DANMF**    - Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection, CIKM'18    - [[paper]](https://smartyfh.com/Documents/18DANMF.pdf)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/DANMF)    - [[Matlab]](https://github.com/smartyfh/DANMF)    - **BANE**    - Binarized Attributed Network Embedding, ICDM'18    - [[paper]](https://www.researchgate.net/publication/328688614_Binarized_Attributed_Network_Embedding)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/BANE)    - [[Matlab]](https://github.com/ICDM2018-BANE/BANE)  - **GCN Insights**    - Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, AAAI'18    - [[Project]](https://liqimai.github.io/blog/AAAI-18/)    - [[code]](https://github.com/liqimai/gcn/tree/AAAI-18/)  - **PCTADW**    - Learning Embeddings of Directed Networks with Text-Associated Nodes---with Applications in Software Package Dependency Networks    - [[paper]](https://arxiv.org/pdf/1809.02270.pdf)    - [[Python]](https://github.com/shudan/PCTADW)    - [[dataset]](https://doi.org/10.5281/zenodo.1410669)  - **LGCN**    - Large-Scale Learnable Graph Convolutional Networks, KDD'18    - [[paper]](http://www.kdd.org/kdd2018/accepted-papers/view/large-scale-learnable-graph-convolutional-networks)    - [[Python]](https://github.com/HongyangGao/LGCN)  - **AspEm**    - AspEm: Embedding Learning by Aspects in Heterogeneous Information Networks    - [[paper]](http://yushi2.web.engr.illinois.edu/sdm18.pdf)    - [[Python]](https://github.com/ysyushi/aspem)  - **Walklets**    - Don't Walk, Skip! Online Learning of Multi-scale Network Embeddings    - [[paper]](https://arxiv.org/pdf/1605.02115.pdf)    - [[Python Karateclub]](https://github.com/benedekrozemberczki/karateclub)      - [[Python]](https://github.com/benedekrozemberczki/walklets)    - **gat2vec**    - gat2vec: Representation learning for attributed graphs    - [[paper]](https://doi.org/10.1007/s00607-018-0622-9)    - [[Python]](https://github.com/snash4/GAT2VEC)  - **FSCNMF**    - FSCNMF: Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks    - [[paper]](https://arxiv.org/abs/1804.05313)    - [[Python Karateclub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/sambaranban/FSCNMF)      - [[Python]](https://github.com/benedekrozemberczki/FSCNMF)  - **SIDE**    - SIDE: Representation Learning in Signed Directed Networks    - [[paper]](https://datalab.snu.ac.kr/side/resources/side.pdf)    - [[Python]](https://datalab.snu.ac.kr/side/resources/side.zip)    - [[Site]](https://datalab.snu.ac.kr/side/)  - **AWE**    - Anonymous Walk Embeddings, ICML'18    - [[paper]](https://www.researchgate.net/publication/325114285_Anonymous_Walk_Embeddings)    - [[Python]](https://github.com/nd7141/Anonymous-Walk-Embeddings)  - **BiNE**    - BiNE: Bipartite Network Embedding, SIGIR'18    - [[paper]](http://staff.ustc.edu.cn/~hexn/papers/sigir18-bipartiteNE.pdf)    - [[Python]](https://github.com/clhchtcjj/BiNE)  - **HOPE**    - Asymmetric Transitivity Preserving Graph Embedding    - [[KDD 2016]](http://www.kdd.org/kdd2016/papers/files/rfp0184-ouA.pdf)    - [[Python]](https://github.com/AnryYang/HOPE)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)   - **VERSE**    - VERSE, Versatile Graph Embeddings from Similarity Measures    - [[Arxiv]](https://arxiv.org/abs/1803.04742) [[WWW 2018]]    - [[Python]](https://github.com/xgfs/verse)   - **AGNN**    - Attention-based Graph Neural Network for semi-supervised learning    - [[ICLR 2018 OpenReview (rejected)]](https://openreview.net/forum?id=rJg4YGWRb)    - [[Python]](https://github.com/dawnranger/pytorch-AGNN)  - **SEANO**    - Semi-supervised Embedding in Attributed Networks with Outliers    - [[Paper]](https://arxiv.org/pdf/1703.08100.pdf) (SDM 2018)    - [[Python]](http://jiongqianliang.com/SEANO/)     - **Hyperbolics**    - Representation Tradeoffs for Hyperbolic Embeddings     - [[Arxiv]](https://arxiv.org/abs/1804.03329)    - [[Python]](https://github.com/HazyResearch/hyperbolics)     - **DGCNN**    - An End-to-End Deep Learning Architecture for Graph Classiﬁcation     - [[AAAI 2018]](http://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf)    - [[Lua]](https://github.com/muhanzhang/DGCNN) [[Python]](https://github.com/muhanzhang/pytorch_DGCNN)    - **structure2vec**    - Discriminative Embeddings of Latent Variable Models for Structured Data     - [[Arxiv]](https://arxiv.org/abs/1603.05629)    - [[Python]](https://github.com/Hanjun-Dai/pytorch_structure2vec)    - **Decagon**    - Decagon, Graph Neural Network for Multirelational Link Prediction     - [[Arxiv]](https://arxiv.org/abs/1802.00543) [[SNAP]](http://snap.stanford.edu/decagon/) [[ISMB 2018]]    - [[Python]](https://github.com/marinkaz/decagon)      - **DHNE**    - Structural Deep Embedding for Hyper-Networks    - [[AAAI 2018]](http://nrl.thumedialab.com/Structural-Deep-Embedding-for-Hyper-Networks)[[Arxiv]](https://arxiv.org/abs/1711.10146)    - [[Python]](https://github.com/tadpole/DHNE)    - **Ohmnet**    - Feature Learning in Multi-Layer Networks     - [[Arxiv]](https://arxiv.org/abs/1707.04638) [[SNAP]](http://snap.stanford.edu/ohmnet/)     - [[Python]](https://github.com/marinkaz/ohmnet)    - **SDNE**    - Structural Deep Network Embedding     - [[KDD 2016]](http://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf)    - [[Python]](https://github.com/xiaohan2012/sdne-keras)   - **STWalk**    - STWalk: Learning Trajectory Representations in Temporal Graphs]     - [[Arxiv]](https://arxiv.org/abs/1711.04150)    - [[Python]](https://github.com/supriya-pandhre/STWalk)  - **LoNGAE**    - Learning to Make Predictions on Graphs with Autoencoders     - [[Arxiv]](https://arxiv.org/abs/1802.08352)    - [[Python]](https://github.com/vuptran/graph-representation-learning)    - **RSDNE**    - [RSDNE: Exploring Relaxed Similarity and Dissimilarity from Completely-imbalanced Labels for Network Embedding.](https://zhengwang100.github.io/AAAI18_RSDNE.pdf), AAAI 2018    - [[Matlab]](https://github.com/zhengwang100/RSDNE)   - **FastGCN**    - FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling     - [[Arxiv]](https://arxiv.org/abs/1801.10247), [[ICLR 2018 OpenReview]](https://openreview.net/forum?id=rytstxWAW)    - [[Python]](https://github.com/matenure/FastGCN)  - **diff2vec**    - [Fast Sequence Based Embedding with Diffusion Graphs](http://homepages.inf.ed.ac.uk/s1668259/papers/sequence.pdf), CompleNet 2018    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/diff2vec)   - **Poincare**    - [Poincaré Embeddings for Learning Hierarchical Representations](https://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations), NIPS 2017    - [[PyTorch]](https://github.com/facebookresearch/poincare-embeddings) [[Python]](https://radimrehurek.com/gensim/models/poincare.html) [[C++]](https://github.com/TatsuyaShirakawa/poincare-embedding)  - **PEUNE**    - [PRUNE: Preserving Proximity and Global Ranking for Network Embedding](https://papers.nips.cc/paper/7110-prune-preserving-proximity-and-global-ranking-for-network-embedding), NIPS 2017    - [[code]](https://github.com/ntumslab/PRUNE)  - **ASNE**    - Attributed Social Network Embedding, TKDE'18    - [[arxiv]](https://arxiv.org/abs/1706.01860)    - [[Python]](https://github.com/lizi-git/ASNE)    - [[Fast Python]](https://github.com/benedekrozemberczki/ASNE)  - **GraphWave**    - [Spectral Graph Wavelets for Structural Role Similarity in Networks](http://snap.stanford.edu/graphwave/),     - [[arxiv]](https://arxiv.org/abs/1710.10321), [[ICLR 2018 OpenReview]](https://openreview.net/forum?id=rytstxWAW)    - [[Python]](https://github.com/snap-stanford/graphwave) [[faster version]](https://github.com/benedekrozemberczki/GraphWaveMachine)  - **StarSpace**    - [StarSpace: Embed All The Things!](https://arxiv.org/pdf/1709.03856), arxiv'17    - [[code]](https://github.com/facebookresearch/Starspace)  - **proNet-core**    - Vertex-Context Sampling for Weighted Network Embedding, arxiv'17    - [[arxiv]](https://arxiv.org/abs/1711.00227) [[code]](https://github.com/cnclabs/proNet-core)  - **struc2vec**    - [struc2vec: Learning Node Representations from Structural Identity](https://dl.acm.org/citation.cfm?id=3098061), KDD'17    - [[Python]](https://github.com/leoribeiro/struc2vec)  - **ComE**    - Learning Community Embedding with Community Detection and Node Embedding on Graphs, CIKM'17    - [[Python]](https://github.com/andompesta/ComE)  - **BoostedNE**    - [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627), '18    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)    - [[Python]](https://github.com/benedekrozemberczki/BoostedFactorization)  - **M-NMF**    - Community Preserving Network Embedding, AAAI'17    - [[Python TensorFlow]](https://github.com/benedekrozemberczki/M-NMF)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **GraphSAGE**    - Inductive Representation Learning on Large Graphs, NIPS'17    - [[arxiv]](https://arxiv.org/abs/1706.02216) [[TF]](https://github.com/williamleif/GraphSAGE) [[PyTorch]](https://github.com/williamleif/graphsage-simple/)   - **ICE**    - [ICE: Item Concept Embedding via Textual Information](http://dl.acm.org/citation.cfm?id=3080807), SIGIR'17    - [[demo]](https://cnclabs.github.io/ICE/) [[code]](https://github.com/cnclabs/ICE)  - **GuidedHeteEmbedding**    - Task-guided and path-augmented heterogeneous network embedding for author identification, WSDM'17    - [[paper]](https://arxiv.org/pdf/1612.02814.pdf) [[code]](https://github.com/chentingpc/GuidedHeteEmbedding)  - **metapath2vec**    - metapath2vec: Scalable Representation Learning for Heterogeneous Networks, KDD'17    - [[paper]](https://www3.nd.edu/~dial/publications/dong2017metapath2vec.pdf) [[project website]](https://ericdongyx.github.io/metapath2vec/m2v.html)  - **GCN**    - Semi-Supervised Classification with Graph Convolutional Networks, ICLR'17    - [[arxiv]](https://arxiv.org/abs/1609.02907)  [[Python Tensorflow]](https://github.com/tkipf/gcn)  - **GAE**    - Variational Graph Auto-Encoders, arxiv    - [[arxiv]](https://arxiv.org/abs/1611.07308) [[Python Tensorflow]](https://github.com/tkipf/gae)  - **CANE**    - CANE: Context-Aware Network Embedding for Relation Modeling, ACL'17    - [[paper]](http://www.thunlp.org/~tcc/publications/acl2017_cane.pdf) [[Python]](https://github.com/thunlp/cane)  - **TransNet**    - TransNet: Translation-Based Network Representation Learning for Social Relation Extraction, IJCAI'17    - [[Python Tensorflow]](https://github.com/thunlp/TransNet)  - **cnn_graph**    - Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, NIPS'16    - [[Python]](https://github.com/mdeff/cnn_graph)  - **ConvE**    - [Convolutional 2D Knowledge Graph Embeddings](https://arxiv.org/pdf/1707.01476v2.pdf), arxiv    - [[source]](https://github.com/TimDettmers/ConvE)  - **node2vec**    - [node2vec: Scalable Feature Learning for Networks](http://dl.acm.org/citation.cfm?id=2939672.2939754), KDD'16    - [[arxiv]](https://arxiv.org/abs/1607.00653) [[Python]](https://github.com/aditya-grover/node2vec) [[Python-2]](https://github.com/apple2373/node2vec) [[Python-3]](https://github.com/eliorc/node2vec) [[C++]](https://github.com/xgfs/node2vec-c)    - **DNGR**    - [Deep Neural Networks for Learning Graph Representations](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12423), AAAI'16    - [[Matlab]](https://github.com/ShelsonCao/DNGR) [[Python Keras]](https://github.com/MdAsifKhan/DNGR-Keras)  - **HolE**    - [Holographic Embeddings of Knowledge Graphs](http://dl.acm.org/citation.cfm?id=3016172), AAAI'16    - [[Python-sklearn]](https://github.com/mnick/holographic-embeddings) [[Python-sklearn2]](https://github.com/mnick/scikit-kge)  - **ComplEx**    - [Complex Embeddings for Simple Link Prediction](http://dl.acm.org/citation.cfm?id=3045609), ICML'16    - [[arxiv]](https://arxiv.org/abs/1606.06357) [[Python]](https://github.com/ttrouill/complex)  - **MMDW**    - Max-Margin DeepWalk: Discriminative Learning of Network Representation, IJCAI'16    - [[paper]](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/ijcai2016_mmdw.pdf)  [[Java]](https://github.com/thunlp/MMDW)  - **planetoid**    - Revisiting Semi-supervised Learning with Graph Embeddings, ICML'16    - [[arxiv]](https://arxiv.org/abs/1603.08861) [[Python]](https://github.com/kimiyoung/planetoid)  - **graph2vec**    - graph2vec: Learning Distributed Representations of Graphs, KDD'17 MLGWorkshop    - [[arxiv]](https://arxiv.org/abs/1707.05005)    - [[Python gensim]](https://github.com/benedekrozemberczki/graph2vec) [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **PowerWalk**    - [PowerWalk: Scalable Personalized PageRank via Random Walks with Vertex-Centric Decomposition](http://dl.acm.org/citation.cfm?id=2983713), CIKM'16    - [[code]](https://github.com/lqhl/PowerWalk)  - **LINE**    - [LINE: Large-scale information network embedding](http://dl.acm.org/citation.cfm?id=2741093), WWW'15    - [[arxiv]](https://arxiv.org/abs/1503.03578) [[C++]](https://github.com/tangjianpku/LINE) [[Python TF]](https://github.com/snowkylin/line) [[Python Theano/Keras]](https://github.com/VahidooX/LINE)  - **PTE**    - [PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks](http://dl.acm.org/citation.cfm?id=2783307), KDD'15    - [[C++]](https://github.com/mnqu/PTE)  - **GraRep**    - [Grarep: Learning graph representations with global structural information](http://dl.acm.org/citation.cfm?id=2806512), CIKM'15    - [[Matlab]](https://github.com/ShelsonCao/GraRep)    - [[Julia]](https://github.com/xgfs/GraRep.jl)    - [[Python]](https://github.com/benedekrozemberczki/GraRep)    - [[Python KarateClub]](https://github.com/benedekrozemberczki/karateclub)  - **KB2E**    - [Learning Entity and Relation Embeddings for Knowledge Graph Completion](http://dl.acm.org/citation.cfm?id=2886624), AAAI'15    - [[paper]](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2015_transr.pdf) [[C++]](https://github.com/thunlp/KB2E)  [[faster version]](https://github.com/thunlp/Fast-TransX)  - **TADW**    - [Network Representation Learning with Rich Text Information](http://dl.acm.org/citation.cfm?id=2832542), IJCAI'15    - [[paper]](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) [[Matlab]](https://github.com/thunlp/tadw) [[Python]](https://github.com/benedekrozemberczki/TADW)  - **DeepWalk**    - [DeepWalk: Online Learning of Social Representations](http://dl.acm.org/citation.cfm?id=2623732), KDD'14    - [[arxiv]](https://arxiv.org/abs/1403.6652) [[Python]](https://github.com/phanein/deepwalk)  [[C++]](https://github.com/xgfs/deepwalk-c)  - **GEM**    - Graph Embedding Techniques, Applications, and Performance: A Survey    - [[arxiv]](https://arxiv.org/abs/1705.02801) [[Python]](https://github.com/palash1992/GEM)  - **DNE-SBP**    - Deep Network Embedding for Graph Representation Learning in Signed Networks    - [[paper]](https://ieeexplore.ieee.org/document/8486671) [[Code]](https://github.com/shenxiaocam/Deep-network-embedding-for-graph-representation-learning-in-signed-networks)    # Paper References    [A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/abs/1901.00596), arxiv'19    [Hierarchical Graph Representation Learning with Differentiable Pooling](https://arxiv.org/pdf/1806.08804.pdf), NIPS'18    **SEMAC**, [Link Prediction via Subgraph Embedding-Based Convex Matrix Completion](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16442), AAAI 2018, [Slides](https://www.slideshare.net/gdm3003/semac-graph-node-embeddings-for-link-prediction)    **MILE**, [MILE: A Multi-Level Framework for Scalable Graph Embedding](https://arxiv.org/pdf/1802.09612.pdf), arxiv'18    **MetaGraph2Vec**, [MetaGraph2Vec: Complex Semantic Path Augmented Heterogeneous Network Embedding](https://arxiv.org/abs/1803.02533)    **PinSAGE**, [Graph Convolutional Neural Networks for Web-Scale Recommender Systems](https://arxiv.org/abs/1806.01973)    [Curriculum Learning for Heterogeneous Star Network Embedding via Deep Reinforcement Learning](https://dl.acm.org/citation.cfm?id=3159711), WSDM '18    [Adversarial Network Embedding](https://arxiv.org/abs/1711.07838), arxiv    **Role2Vec**, [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896)    **edge2vec**, [Feature Propagation on Graph: A New Perspective to Graph Representation  Learning](https://arxiv.org/abs/1804.06111)    **MINES**, [Multi-Dimensional Network Embedding with Hierarchical Structure](http://cse.msu.edu/~mayao4/downloads/Multidimensional_Network_Embedding_with_Hierarchical_Structure.pdf)    [Walk-Steered Convolution for Graph Classification](https://arxiv.org/abs/1804.05837)    [Deep Feature Learning for Graphs](https://arxiv.org/abs/1704.08829), arxiv'17    [Fast Linear Model for Knowledge Graph Embeddings](https://arxiv.org/abs/1710.10881), arxiv'17    [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec](https://arxiv.org/abs/1710.02971), arxiv'17    [A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications](https://arxiv.org/abs/1709.07604), arxiv'17    [Representation Learning on Graphs: Methods and Applications](https://arxiv.org/pdf/1709.05584.pdf), IEEE DEB'17    **CONE**, [CONE: Community Oriented Network Embedding](https://arxiv.org/abs/1709.01554), arxiv'17    **LANE**,   [Label Informed Attributed Network Embedding](http://dl.acm.org/citation.cfm?id=3018667), WSDM'17    **Graph2Gauss**,  [Deep Gaussian Embedding of Attributed Graphs: Unsupervised Inductive Learning via Ranking](https://arxiv.org/abs/1707.03815), arxiv  [[Bonus Animation]](https://twitter.com/abojchevski/status/885502050133585925)    [Scalable Graph Embedding for Asymmetric Proximity](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14696), AAAI'17    [Query-based Music Recommendations via Preference Embedding](http://dl.acm.org/citation.cfm?id=2959169), RecSys'16    [Tri-party deep network representation](http://dl.acm.org/citation.cfm?id=3060886), IJCAI'16    [Heterogeneous Network Embedding via Deep Architectures](http://dl.acm.org/citation.cfm?id=2783296), KDD'15    [Neural Word Embedding As Implicit Matrix Factorization](http://dl.acm.org/citation.cfm?id=2969070), NIPS'14    [Distributed large-scale natural graph factorization](http://dl.acm.org/citation.cfm?id=2488393), WWW'13    [From Node Embedding To Community Embedding](https://arxiv.org/abs/1610.09950), arxiv    [Walklets: Multiscale Graph Embeddings for Interpretable Network Classification](https://arxiv.org/abs/1605.02115), arxiv    [Comprehend DeepWalk as Matrix Factorization](https://arxiv.org/abs/1501.00358), arxiv    # Conference & Workshop    [Graph Neural Networks for Natural Language Processing](https://github.com/svjan5/GNNs-for-NLP), **EMNLP'19**    [SMORe : Modularize Graph Embedding for Recommendation](https://github.com/cnclabs/smore), **RecSys'19**    [13th International Workshop on Mining and Learning with Graphs](http://www.mlgworkshop.org/2017/), **MLG'17**    [WWW-18 Tutorial Representation Learning on Networks](http://snap.stanford.edu/proj/embeddings-www/), **WWW'18**    # Related List    [awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification)    [awesome-community-detection](https://github.com/benedekrozemberczki/awesome-community-detection)    [awesome-embedding-models](https://github.com/Hironsan/awesome-embedding-models)    [Must-read papers on network representation learning (NRL) / network embedding (NE)](https://github.com/thunlp/NRLPapers)    [Must-read papers on knowledge representation learning (KRL) / knowledge embedding (KE)](https://github.com/thunlp/KRLPapers)    [Network Embedding Resources](https://github.com/nate-russell/Network-Embedding-Resources)    [awesome-embedding-models](https://github.com/Hironsan/awesome-embedding-models)    [2vec-type embedding models](https://github.com/MaxwellRebo/awesome-2vec)    [Must-read papers on GNN](https://github.com/thunlp/GNNPapers)    [LiteratureDL4Graph](https://github.com/DeepGraphLearning/LiteratureDL4Graph)    [awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification)    # Related Project    **Stanford Network Analysis Project** [website](http://snap.stanford.edu/)    **StellarGraph Machine Learning Library** [website](https://www.stellargraph.io) [GitHub](https://github.com/stellargraph/stellargraph) """
Big data;https://github.com/nathanmarz/elephantdb;"""[![Build Status](https://travis-ci.org/nathanmarz/elephantdb.png?branch=develop)](https://travis-ci.org/nathanmarz/elephantdb)    # ElephantDB 0.5.1 (cascalog 2.x)    ## ElephantDB 0.4.5 (cascalog 1.x)    # About    ElephantDB is a database that specializes in exporting key/value data  from Hadoop. ElephantDB is composed of two components. The first is a  library that is used in MapReduce jobs for creating an indexed  key/value dataset that is stored on a distributed filesystem. The  second component is a daemon that can download a subset of a dataset  and serve it in a read-only, random-access fashion. A group of  machines working together to serve a full dataset is called a ring.    Since ElephantDB server doesn't support random writes, it is almost  laughingly simple. Once the server loads up its subset of the data, it  does very little. This leads to ElephantDB being rock-solid in  production, since there's almost no moving parts.    ElephantDB server has a Thrift interface, so any language can make  reads from it. The database itself is implemented in Clojure.    An ElephantDB datastore contains a fixed number of shards of a ""Local  Persistence"". ElephantDB's local persistence engine is pluggable, and  ElephantDB comes bundled with local persistence implementations for  Berkeley DB Java Edition and LevelDB. On the MapReduce side, each  reducer creates or updates a single shard into the DFS, and on the  server side, each server serves a subset of the shards.    ElephantDB support hot-swapping so that a live server can be updated  with a new set of shards without downtime.    # Questions    Google group: [elephantdb-user](http://groups.google.com/group/elephantdb-user)    # Introduction    [Introduction to ElephantDB](https://speakerdeck.com/sorenmacbeth/introduction-to-elephantdb)    # Tutorials    TODO: Write an updated tutorial for ElephantDB 0.4.x    # Using ElephantDB in MapReduce Jobs    ElephantDB is hosted at [Clojars](http://clojars.org/elephantdb).  Clojars is a maven repo that is trivially easy to use with maven or  leiningen. You should use this dependency when using ElephantDB within  your MapReduce jobs to create ElephantDB datastores. ElephantDB  contains a module elephantdb-cascading which allows you to easily create  datastores from your Cascading workflows. elephantdb-cascalog is available  for use with [Cascalog](http://github.com/nathanmarz/cascalog) >= 1.10.1.    # Deploying ElephantDB server    TODO: Documentation on how to deploy ElephantDB.    # Running the EDB Jar    TODO: Documentation on how to run ElephantDB """
Big data;https://github.com/OryxProject/oryx;"""<img align=""right"" src=""http://oryx.io/img/OryxLogoMedium.png"" />    Oryx 2 is a realization of the lambda architecture built on [Apache Spark](http://spark.apache.org)   and [Apache Kafka](http://kafka.apache.org), but with specialization for real-time large scale machine   learning. It is a framework for building applications, but also includes packaged, end-to-end   applications for collaborative filtering, classification, regression and clustering.    Proceed to the [Oryx 2 site](http://oryx.io/) for full documentation.    Just looking to deploy a ready-made, end-to-end application for collaborative filtering, clustering or classification? Easy.  Proceed directly to:    - Prepare your Hadoop cluster with [Cluster Setup](http://oryx.io/docs/admin.html)  - Get a [Release](https://github.com/OryxProject/oryx/releases)  - Prepare a config file from the [Configuration Reference](http://oryx.io/docs/endusers.html#Configuration)  - Run the binaries with [Running Oryx](http://oryx.io/docs/endusers.html#Running)  - Learn about the REST API endpoints you can call in the [API Endpoint Reference](http://oryx.io/docs/endusers.html#API_Endpoint_Reference)    Developers can consume Oryx 2 as a framework for building custom applications as well.   Following the architecture overview below, proceed to   [Making an Oryx App](http://oryx.io/docs/developer.html#Making_an_Oryx_App)   to learn how to create a new application. You can review a [module diagram](https://sourcespy.com/github/oryx/)   as well to understand the project structure.    <img src=""http://oryx.io/img/Architecture.png""/>    ------    [![Build Status](https://travis-ci.org/OryxProject/oryx.svg?branch=master)](https://travis-ci.org/OryxProject/oryx)  [![Coverity](https://scan.coverity.com/projects/2697/badge.svg)](https://scan.coverity.com/projects/2697)  [![codecov.io](https://codecov.io/github/OryxProject/oryx/coverage.svg?branch=master)](https://codecov.io/github/OryxProject/oryx?branch=master) """
Big data;https://github.com/paulhoule/infovore;"""Overview  --------    Infovore is an RDF processing system that uses Hadoop to process RDF data  sets in the billion triple range and beyond.  Infovore was originally designed to process  the (old) proprietary Freebase dump into RDF,  but once Freebase came out with an official RDF  dump,  Infovore gained the ability to clean and purify the dump,  making it not just possible  but easy to process Freebase data with triple stores such as Virtuoso 7.    Every week we run Infovore in Amazon Elastic/Map reduce in order to produce a product known as  [:BaseKB](http://basekb.com/).    Infovore depends on the [Centipede](https://github.com/paulhoule/centipede/wiki) framework for packaging  and processing command-line arguments.  The [Telepath](https://github.com/paulhoule/telepath/wiki) project  extends the Infovore project in order to process Wikipedia usage information to produce a product called  [:SubjectiveEye3D](https://github.com/paulhoule/telepath/wiki/SubjectiveEye3D).      Supporting  ----------    It costs several hundreds of dollars per month to process and store files in connection with this work.  Please join <a href=""https://www.gittip.com/"">Gittip</a> and make a <a href=""https://www.gittip.com/paulhoule/"">small weekly donation</a> to keep this data free.      Building  --------    Infovore software requires JDK 7.    mvn clean install    Installing  ----------    The following cantrip, run from the top level ""infovore"" directory, initializes the bash shell  for the use of the ""haruhi"" program,  which can be used to run Infovore applications  packaged in the Bakemono Jar.    source haruhi/target/path.sh    More Information  ----------------    See     https://github.com/paulhoule/infovore/wiki     for documentation and join the discussion group at    https://groups.google.com/forum/#!forum/infovore-basekb         """
Big data;https://github.com/linkedin/cleo;"""What is Cleo?  =======================    Cleo is a flexible software library for enabling rapid development of partial, out-of-order and real-time typeahead search.  It is suitable for data sets of varying sizes and types. Cleo has been used extensively to power LinkedIn typeahead search  covering professional network connections, companies, groups, questions, skills and other site features.    ### Homepage:    Find out more about Cleo at: http://sna-projects.com/cleo    ### License:    Apache Public License (APL) 2.0    ### Artifacts:    1. cleo.jar <-- core library    ### Maven:    groupId: com.sna-projects.cleo    artifactId: cleo    version: 1.2.6    ### Code Examples:    https://github.com/linkedin/cleo/tree/master/src/examples    https://github.com/jingwei/cleo-primer    ### Eclipse:    Set up Eclipse for Cleo by executing the command below:    mvn eclipse:eclipse    Inside Eclipse, select Preferences > Java > Build Path > Classpath Variables. Define a new classpath variable M2_REPO and assign maven repository.    For more information, check out http://maven.apache.org/guides/mini/guide-ide-eclipse.html   """
Big data;https://github.com/SnappyDataInc/snappydata;"""<span style=""background-color:yellow"">  This repository is provided for legacy users and informational purposes only. It may contain security vulnerabilities in the code itself or its dependencies. TIBCO provides no updates, including security updates, to this code. Consistent with the terms of the Apache License 2.0 that apply to the TIBCO code in this repository, the code is provided on an ""as is"" basis, without any warranties or conditions of any kind and in no event and under no legal theory shall TIBCO be liable to you for damages arising as a result of the use or inability to use the code.  </span>      ## Introduction   SnappyData (aka TIBCO ComputeDB) is a distributed, in-memory optimized analytics database. SnappyData delivers high throughput, low latency, and high concurrency for unified analytics workload. By fusing an in-memory hybrid database inside Apache Spark, it provides analytic query processing, mutability/transactions, access to virtually all big data sources and stream processing all in one unified cluster.    One common use case for SnappyData is to provide analytics at interactive speeds over large volumes of data with minimal or no pre-processing of the dataset. For instance, there is no need to often pre-aggregate/reduce or generate cubes over your large data sets for ad-hoc visual analytics. This is made possible by smartly managing data in-memory, dynamically generating code using vectorization optimizations and maximizing the potential of modern multi-core CPUs.  SnappyData enables complex processing on large data sets in sub-second timeframes.     ![SnappyData Positioning](docs/Images/Snappy_intro.1.png)    !!!Note  	*SnappyData is not another Enterprise Data Warehouse (EDW) platform, but rather a high performance computational and caching cluster that augments traditional EDWs and data lakes.*    ### Important Capabilities    *	**Easily discover and catalog big data sets**</br>  	You can connect and discover datasets in SQL DBs, Hadoop, NoSQL stores, file systems, or even cloud data stores such as S3 by using SQL, infer schemas automatically and register them in a secure catalog. A wide variety of data formats are supported out of the box such as JSON, CSV, text, Objects, Parquet, ORC, SQL, XML, and more.    *	**Rich connectivity**</br>  	SnappyData is built with Apache Spark inside. Therefore, any data store that has an Apache Spark connector can be accessed using SQL or by using the Apache Spark RDD/Dataset API. Virtually all modern data stores do have Apache Spark connector. See [Apache Spark Packages](https://spark-packages.org/). You can also dynamically deploy connectors to a running SnappyData cluster.    *	**Virtual or in-memory data**</br>  	You can decide which datasets need to be provisioned into distributed memory or left at the source. When the data is left at source, after being modeled as a virtual/external tables, the analytic query processing is parallelized, and the query fragments are pushed down wherever possible and executed at high speed.  When speed is essential, applications can selectively copy the external data into memory using a single SQL command.    *	**In-memory Columnar + Row store** </br>  	You can choose in-memory data to be stored in any of the following forms:      *	**Columnar**: The form that is compressed and designed for scanning/aggregating large data sets.      *	**Row store**: The form that has an extremely fast key access or highly selective access.  	The columnar store is automatically indexed using a skipping index. Applications can explicitly add indexes for the row store.    *	**High performance** </br>  	When data is loaded, the engine parallelizes all the accesses by carefully taking into account the available distributed cores, the available memory, and whether the source data can be partitioned to deliver extremely high-speed loading. Therefore, unlike a traditional warehouse, you can bring up SnappyData whenever required, load, process, and tear it down. Query processing uses code generation and vectorization techniques to shift the processing to the modern-day multi-core processor and L1/L2/L3 caches to the possible extent.    *	**Flexible rich data transformations** </br>  	External data sets when discovered automatically through schema inference will have the schema of the source. Users can cleanse, blend, reshape data using a SQL function library (Apache Spark SQL+) or even submit Apache Spark jobs and use custom logic. The entire rich Apache Spark API is at your disposal. This logic can be written in SQL, Java, Scala, or even Python.*    *	**Prepares data for data science**</br>   	Through the use of apache Apache Spark API for statistics and machine learning, raw or curated datasets can be easily prepared for machine learning. You can understand the statistical characteristics such as correlation, independence of different variables and so on. You can generate distributed feature vectors from your data that is by using processes such as one-hot encoder, binarizer, and a range of functions built into the Apache Spark ML library. These features can be stored back into column tables and shared across a group of users with security and avoid dumping copies to disk, which is slow and error-prone.     *	**Stream ingestion and liveness** </br>  	While it is common to see query service engines today, most resort to periodic refreshing of data sets from the source as the managed data cannot be mutated — for example query engines such as Presto, HDFS formats like parquet, etc. Moreover, when updates can be applied pre-processing, re-shaping of the data is not necessarily simple.      In SnappyData, operational systems can feed data updates through Kafka to SnappyData. The incoming data can be CDC(Change-data-capture) events (insert, updates, or deletes) and can be easily ingested into in-memory tables with ease, consistency, and exactly-once semantics. The Application can apply custom logic to do sophisticated transformations and get the data ready for analytics. This incremental and continuous process is far more efficient than batch refreshes. Refer [Stream Processing with SnappyData](docs/howto/use_stream_processing_with_snappydata.md) </br>      *	**Approximate Query Processing(AQP)** </br>  	When dealing with huge data sets, for example, IoT sensor streaming time-series data, it may not be possible to provision the data in-memory, and if left at the source (say Hadoop or S3) your analytic query processing can take too long. In SnappyData, you can create one or more stratified data samples on the full data set. The query engine automatically uses these samples for aggregation queries, and a nearly accurate answer returned to clients. This can be immensely valuable when visualizing a trend, plotting a graph or bar chart. Refer [AQP](docs/sde/index.md).    *	**Access from anywhere** </br>  	You can use JDBC, ODBC, REST, or any of the Apache Spark APIs. The product is fully compatible with Apache Spark 2.1.1. SnappyData natively supports modern visualization tools such as [TIBCO Spotfire](docs/howto/connecttibcospotfire.md), [Tableau](docs/howto/tableauconnect.md), and [Qlikview](docs/setting_up_jdbc_driver_qlikview.md). Refer       ## Downloading and Installing SnappyData  You can download and install the latest version of SnappyData from [github](https://github.com/TIBCOSoftware/snappydata/releases).  Refer to the [documentation](docs/install/index.md) for installation steps.    ## Getting Started  Multiple options are provided to get started with SnappyData. Easiest way to get going with SnappyData is on your laptop. You can also use any of the following options:    *	On-premise clusters    *	AWS    *	Docker  *	Kubernetes    You can find more information on options for running SnappyData [here](docs/quickstart/index.md).    ## Quick Test to Measure Performance of SnappyData vs Apache Spark    If you are already using Apache Spark, you can experience upto 20x speedup for your query performance with SnappyData. Try this [test](https://github.com/TIBCOSoftware/snappydata/blob/master/examples/quickstart/scripts/Quickstart.scala) using the Spark Shell.    ## Documentation  To understand SnappyData and its features refer to the [documentation](http://tibcosoftware.github.io/snappydata/).    ### Other Relevant content  - [Paper](http://cidrdb.org/cidr2017/papers/p28-mozafari-cidr17.pdf) on Snappydata at Conference on Innovative Data Systems Research (CIDR) - Info on key concepts and motivating problems.  - [Another early Paper](https://www.snappydata.io/snappy-industrial) that focuses on overall architecture, use cases, and benchmarks. ACM Sigmod 2016.  - [TPC-H benchmark](https://www.snappydata.io/whitepapers/snappydata-tpch) comparing Apache Spark with SnappyData  - Checkout the [SnappyData blog](https://www.snappydata.io/blog) for developer content  -	[TIBCO community page](https://community.tibco.com/products/tibco-computedb) for the latest info.    ## Community Support    We monitor the following channels comments/questions:    *	[Stackoverflow](http://stackoverflow.com/questions/tagged/snappydata) ![Stackoverflow](http://i.imgur.com/LPIdp12.png)    *	[Slack](http://snappydata-slackin.herokuapp.com/) ![Slack](http://i.imgur.com/h3sc6GM.png)    *	[Gitter](https://gitter.im/SnappyDataInc/snappydata) ![Gitter](http://i.imgur.com/jNAJeOn.jpg)    *	[Mailing List](https://groups.google.com/forum/#!forum/snappydata-user) ![Mailing List](http://i.imgur.com/YomdH4s.png)    *	[Reddit](https://www.reddit.com/r/snappydata) ![Reddit](http://i.imgur.com/AB3cVtj.png)              *	[JIRA](https://jira.snappydata.io/projects/SNAP/issues) ![JIRA](http://i.imgur.com/E92zntA.png)    ## Link with SnappyData Distribution    ### Using Maven Dependency    SnappyData artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:    ```  groupId: io.snappydata  artifactId: snappydata-cluster_2.11  version: 1.3.0  ```    ### Using SBT Dependency    If you are using SBT, add this line to your **build.sbt** for core SnappyData artifacts:    ```  libraryDependencies += ""io.snappydata"" % ""snappydata-core_2.11"" % ""1.3.0""  ```    For additions related to SnappyData cluster, use:    ```  libraryDependencies += ""io.snappydata"" % ""snappydata-cluster_2.11"" % ""1.3.0""  ```    You can find more specific SnappyData artifacts [here](http://mvnrepository.com/artifact/io.snappydata)    !!!Note  	If your project fails when resolving the above dependency (that is, it fails to download `javax.ws.rs#javax.ws.rs-api;2.1`), it may be due an issue with its pom file. </br> As a workaround, you can add the below code to your **build.sbt**:    ```  val workaround = {    sys.props += ""packaging.type"" -> ""jar""    ()  }  ```    For more details, refer [https://github.com/sbt/sbt/issues/3618](https://github.com/sbt/sbt/issues/3618).      ## Building from Source  If you would like to build SnappyData from source, refer to the [documentation on building from source](docs/install/building_from_source.md).      ## How is SnappyData Different than Apache Spark?    Apache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and is capable of working with disparate data sources. While this provides rich unified access to data, this can also be quite inefficient and expensive. Analytic processing requires massive data sets to be repeatedly copied and data to be reformatted to suit Apache Spark. In many cases, it ultimately fails to deliver the promise of interactive analytic performance.  For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Apache Spark to do the aggregation. Caching within Apache Spark is immutable and results in stale insight.    ### The SnappyData Approach    ##### Snappy Architecture    ![SnappyData Architecture](docs/Images/SnappyArchitecture.png)    SnappyData takes a different approach. SnappyData fuses a low latency, highly available in-memory transactional database ((Pivotal GemFire/Apache Geode) into Apache Spark with shared memory management and optimizations. Data can be managed in columnar form similar to Apache Spark caching or in a row oriented manner commonly used in popular relational databases like postgres). But, many query engine operators are significantly more optimized through better vectorization, code generation and indexing. </br>  The net effect is, an order of magnitude performance improvement when compared to native Apache Spark caching, and more than two orders of magnitude better performance when Apache Spark is used in conjunction with external data sources.  Apache Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with Streams (Apache Spark) and running analytic SQL queries without losing the computational richness in Apache Spark.      ## Streaming Example - Ad Analytics  Here is a stream + Transactions + Analytics use case example to illustrate the SQL as well as the Apache Spark programming approaches in SnappyData - [Ad Analytics code example](https://github.com/TIBCOSoftware/snappy-poc). Here is a [screencast](https://www.youtube.com/watch?v=bXofwFtmHjE) that showcases many useful features of SnappyData. The example also goes through a benchmark comparing SnappyData to a Hybrid in-memory database and Cassandra.    ## Contributing to SnappyData    If you are interested in contributing, please visit the [community page](http://www.snappydata.io/community) for ways in which you can help.   """
Big data;https://github.com/cockroachdb/cockroach;"""<p align=""center"">    <img src='docs/media/cockroach_db.png?raw=true' width='70%'>  </p>    ---    CockroachDB is a cloud-native distributed SQL database designed to build,   scale, and manage modern, data-intensive applications.    - [What is CockroachDB?](#what-is-cockroachdb)  - [Docs](#docs)  - [Starting with Cockroach Cloud](#starting-with-cockroachcloud)  - [Starting with CockroachDB](#starting-with-cockroachdb)  - [Client Drivers](#client-drivers)  - [Deployment](#deployment)  - [Need Help?](#need-help)  - [Contributing](#contributing)  - [Design](#design)  - [Comparison with Other Databases](#comparison-with-other-databases)  - [See Also](#see-also)    ## What is CockroachDB?    CockroachDB is a distributed SQL database built on a transactional and  strongly-consistent key-value store. It **scales** horizontally;  **survives** disk, machine, rack, and even datacenter failures with  minimal latency disruption and no manual intervention; supports  **strongly-consistent** ACID transactions; and provides a familiar  **SQL** API for structuring, manipulating, and querying data.    For more details, see our [FAQ](https://cockroachlabs.com/docs/stable/frequently-asked-questions.html) or [architecture document](  https://www.cockroachlabs.com/docs/stable/architecture/overview.html).    <p align=""center"">    <a href='https://www.youtube.com/watch?v=VgXiMcbGwzQ'> <img src='docs/media/explainer-video-preview.png' width='70%'> </a>  </p>    ## Docs    For guidance on installation, development, deployment, and administration, see our [User Documentation](https://cockroachlabs.com/docs/stable/).    ## Starting with CockroachCloud    We can run CockroachDB for you, so you don't have to run your own cluster.    See our online documentation: [Quickstart with CockroachCloud](https://www.cockroachlabs.com/docs/cockroachcloud/quickstart.html)    ## Starting with CockroachDB    1. Install CockroachDB:  [using a pre-built executable](https://www.cockroachlabs.com/docs/stable/install-cockroachdb.html) or [build it from source](https://www.cockroachlabs.com/docs/v21.1/install-cockroachdb-linux#build-from-source).  2. [Start a local cluster](https://www.cockroachlabs.com/docs/stable/start-a-local-cluster.html) and connect to it via the [built-in SQL client](https://www.cockroachlabs.com/docs/stable/use-the-built-in-sql-client.html).  3. [Learn more about CockroachDB SQL](https://www.cockroachlabs.com/docs/stable/learn-cockroachdb-sql.html).  4. Use a PostgreSQL-compatible driver or ORM to [build an app with CockroachDB](https://www.cockroachlabs.com/docs/stable/hello-world-example-apps.html).  5. [Explore core features](https://www.cockroachlabs.com/docs/stable/demo-data-replication.html), such as data replication, automatic rebalancing, and fault tolerance and recovery.    ## Client Drivers    CockroachDB supports the PostgreSQL wire protocol, so you can use any available PostgreSQL client drivers to connect from various languages.    - For recommended drivers that we've tested, see [Install Client Drivers](https://www.cockroachlabs.com/docs/stable/install-client-drivers.html).  - For tutorials using these drivers, as well as supported ORMs, see [Example Apps](https://www.cockroachlabs.com/docs/stable/example-apps.html).    ## Deployment    - [CockroachCloud](https://www.cockroachlabs.com/docs/cockroachcloud/quickstart) - Steps to create a [free CockroachCloud cluster](https://cockroachlabs.cloud/signup?referralId=githubquickstart) on your preferred Cloud platform.  - [Manual](https://www.cockroachlabs.com/docs/stable/manual-deployment.html) - Steps to deploy a CockroachDB cluster manually on multiple machines.  - [Cloud](https://www.cockroachlabs.com/docs/stable/cloud-deployment.html) - Guides for deploying CockroachDB on various cloud platforms.  - [Orchestration](https://www.cockroachlabs.com/docs/stable/orchestration.html) - Guides for running CockroachDB with popular open-source orchestration systems.    ## Need Help?    - [CockroachDB Community Slack](https://go.crdb.dev/p/slack) - Join our slack to connect with our engineers and other users running CockroachDB.  - [CockroachDB Forum](https://forum.cockroachlabs.com/) and [Stack Overflow](https://stackoverflow.com/questions/tagged/cockroachdb) - Ask questions, find answers, and help other users.  - [Troubleshooting documentation](https://www.cockroachlabs.com/docs/stable/troubleshooting-overview.html) - Learn how to troubleshoot common errors, cluster setup, and SQL query behavior.  - For filing bugs, suggesting improvements, or requesting new features, help us out by [opening an issue](https://github.com/cockroachdb/cockroach/issues/new).    ## Building from source    See [our wiki](https://wiki.crdb.io/wiki/spaces/CRDB/pages/181338446/Getting+and+building+from+source) for more details.    ## Contributing    We welcome your contributions! If you're looking for issues to work on, try looking at the [good first issue list](https://github.com/cockroachdb/cockroach/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22). We do our best to tag issues suitable for new external contributors with that label, so it's a great way to find something you can help with!    See [our wiki](https://wiki.crdb.io/wiki/spaces/CRDB/pages/73204033/Contributing+to+CockroachDB) for more details.    Engineering discussions take place on our public mailing list, [cockroach-db@googlegroups.com](https://groups.google.com/forum/#!forum/cockroach-db). Also please join our [Community Slack](https://go.crdb.dev/p/slack) (there's a dedicated #contributors channel!) to ask questions, discuss your ideas, and connect with other contributors.    ## Design    For an in-depth discussion of the CockroachDB architecture, see our  [Architecture  Guide](https://www.cockroachlabs.com/docs/stable/architecture/overview.html).  For the original design motivation, see our [design  doc](https://github.com/cockroachdb/cockroach/blob/master/docs/design.md).    ## Licensing    Current CockroachDB code is released under a combination of two licenses, the [Business Source License (BSL)](https://www.cockroachlabs.com/docs/stable/licensing-faqs.html#bsl) and the [Cockroach Community License (CCL)](https://www.cockroachlabs.com/docs/stable/licensing-faqs.html#ccl).    When contributing to a CockroachDB feature, you can find the relevant license in the comments at the top of each file.    For more information, see the [Licensing FAQs](https://www.cockroachlabs.com/docs/stable/licensing-faqs.html).    ## Comparison with Other Databases    To see how key features of CockroachDB stack up against other databases,  check out [CockroachDB in Comparison](https://www.cockroachlabs.com/docs/stable/cockroachdb-in-comparison.html).    ## See Also    - [Tech Talks](https://www.cockroachlabs.com/community/tech-talks/) (by CockroachDB founders, engineers, and customers!)  - [CockroachDB User Documentation](https://cockroachlabs.com/docs/stable/)  - [The CockroachDB Blog](https://www.cockroachlabs.com/blog/)  - Key design documents    - [Serializable, Lockless, Distributed: Isolation in CockroachDB](https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/)    - [Consensus, Made Thrive](https://www.cockroachlabs.com/blog/consensus-made-thrive/)    - [Trust, But Verify: How CockroachDB Checks Replication](https://www.cockroachlabs.com/blog/trust-but-verify-cockroachdb-checks-replication/)    - [Living Without Atomic Clocks](https://www.cockroachlabs.com/blog/living-without-atomic-clocks/)    - [The CockroachDB Architecture Document](https://github.com/cockroachdb/cockroach/blob/master/docs/design.md) """
Big data;https://github.com/krotik/eliasdb;"""EliasDB  =======    <p align=""center"">    <img height=""300px"" style=""height:300px;"" src=""eliasdb_logo.png"">  </p>    EliasDB is a graph-based database which aims to provide a lightweight solution for projects which want to store their data as a graph.    [![Code coverage](https://void.devt.de/pub/eliasdb/test_result.svg)](https://void.devt.de/pub/eliasdb/coverage.txt)  [![Go Report Card](https://goreportcard.com/badge/devt.de/krotik/eliasdb?style=flat-square)](https://goreportcard.com/report/devt.de/krotik/eliasdb)  [![Go Reference](https://pkg.go.dev/badge/krotik/eliasdb.svg)](https://pkg.go.dev/devt.de/krotik/eliasdb)  [![Mentioned in Awesome Go](https://awesome.re/mentioned-badge-flat.svg)](https://github.com/avelino/awesome-go)    Features  --------  - Build on top of a custom key-value store which supports transactions and memory-only storage.  - Data is stored in nodes (key-value objects) which are connected via edges.  - Stored graphs can be separated via partitions.  - Stored graphs support cascading deletions - delete one node and all its ""children"".  - All stored data is indexed and can be quickly searched via a full text phrase search.  - EliasDB has a GraphQL interface which can be used to store and retrieve data.  - For more complex queries EliasDB has an own query language called EQL with an sql-like syntax.  - Includes a scripting interpreter to define alternative actions for database operations or writing backend logic.  - Written in Go from scratch. Only uses gorilla/websocket to support websockets for GraphQL subscriptions.  - The database can be embedded or used as a standalone application.  - When used as a standalone application it comes with an internal HTTPS webserver which provides user management, a REST API and a basic file server.  - When used as an embedded database it supports transactions with rollbacks, iteration of data and rule based consistency management.    Getting Started (standalone application)  ----------------------------------------  You can download a pre-compiled package for Windows (win64) or Linux (amd64) [here](https://void.devt.de/pub/eliasdb).    Extract it and execute the executable with:  ```  eliasdb server  ```  The executable should automatically create 3 subfolders and a configuration file. It should start an HTTPS server on port 9090. To see a terminal point your webbrowser to:  ```  https://localhost:9090/db/term.html  ```  After accepting the self-signed certificate from the server you should see a web terminal. EliasDB can be stopped with a simple CTRL+C or by overwriting the content in eliasdb.lck with a single character.    Getting Started (docker image)  ------------------------------  You can pull the latest docker image of EliasDB from [Dockerhub](https://hub.docker.com/r/krotik/eliasdb):  ```  docker pull krotik/eliasdb  ```    Create an empty directory, change into it and run the following to start the server:  ```  docker run --user $(id -u):$(id -g) -v $PWD:/data -p 9090:9090 krotik/eliasdb server  ```  This exposes port 9090 from the container on the local machine. All runtime related files are written to the current directory as the current user/group.    Connect to the running server with a console by running:  ```  docker run --rm --network=""host"" -it -v $PWD:/data --user $(id -u):$(id -g) -v $PWD:/data krotik/eliasdb console  ```    ### Tutorial:    To get an idea of what EliasDB is about have a look at the [tutorial](examples/tutorial/doc/tutorial.md). This tutorial will cover the basics of EQL and show how data is organized.    There is a separate [tutorial](examples/tutorial/doc/tutorial_graphql.md) on using ELiasDB with GraphQL.    ### REST API:    The terminal uses a REST API to communicate with the backend. The REST API can be browsed using a dynamically generated swagger.json definition (https://localhost:9090/db/swagger.json). You can browse the API of EliasDB's latest version [here](http://petstore.swagger.io/?url=https://devt.de/krotik/eliasdb/raw/master/swagger.json).    ### Scripting    EliasDB supports a scripting language called [ECAL](ecal.md) to define alternative actions for database operations such as store, update or delete. The actions can be taken before, instead (by calling `db.raiseGraphEventHandled()`) or after the normal database operation. The language is powerful enough to write backend logic for applications.    There is a [VSCode integration](https://devt.de/krotik/ecal/src/master/ecal-support/README.md) available which supports syntax highlighting and debugging via the debug server. More information can be found in the [code repository](https://devt.de/krotik/ecal) of the interpreter.    ### Clustering:    EliasDB supports to be run in a cluster by joining multiple instances of EliasDB together. You can read more about it [here](cluster.md).    ### Command line options  The main EliasDB executable has two main tools:  ```  Usage of ./eliasdb <tool>    EliasDB graph based database    Available commands:        console   EliasDB server console      server    Start EliasDB server  ```  The most important one is server which starts the database server. The server has several options:  ```  Usage of ./eliasdb server [options]      -export string      	Export the current database to a zip file    -help      	Show this help message    -import string      	Import a database from a zip file    -no-serv      	Do not start the server after initialization  ```  If the `EnableECALScripts` configuration option is set the following additional option is available:  ```  -ecal-console      Start an interactive interpreter console for ECAL  ```  The interactive console can be used to inspect and modify the runtime state of the ECAL interpreter.    Once the server is started the console tool can be used to interact with the server. The options of the console tool are:  ```  Usage of ./eliasdb console [options]      -exec string      	Execute a single line and exit    -file string      	Read commands from a file and exit    -help      	Show this help message    -host string      	Host of the EliasDB server (default ""localhost"")    -port string      	Port of the EliasDB server (default ""9090"")  ```  On the console type 'q' to exit and 'help' to get an overview of available commands:  ```  Command Description  export  Exports the last output.  find    Do a full-text search of the database.  help    Display descriptions for all available commands.  info    Returns general database information.  part    Displays or sets the current partition.  ver     Displays server version information.  ```  It is also possible to directly run EQL and GraphQL queries on the console. Use the arrow keys to cycle through the command history.    ### Configuration  EliasDB uses a single configuration file called eliasdb.config.json. After starting EliasDB for the first time it should create a default configuration file. Available configurations are:    | Configuration Option | Description |  | --- | --- |  | ClusterConfigFile | Cluster configuration file. |  | ClusterLogHistory | File which is used to store the console history. |  | ClusterStateInfoFile | File which is used to store the cluster state. |  | CookieMaxAgeSeconds | Lifetime for cookies used by EliasDB. |  | ECALDebugServerHost | Hostname the ECAL debug server should listen to. |  | ECALDebugServerPort | Port on which the debug server should listen on. |  | ECALEntryScript | Entry script for ECAL interpreter. |  | ECALLogFile | Logfile for ECAL interpreter. An empty string will cause the logger to write to the console. |  | ECALLogLevel | Log level for ECAL interpreter. Can be debug, info or error. |  | ECALScriptFolder | Directory for ECAL scripts. |  | ECALWorkerCount | Number of worker threads in the ECA engine's thread pool. |  | EnableAccessControl | Flag if access control for EliasDB should be enabled. This provides user authentication and authorization features. |  | EnableCluster | Flag if EliasDB clustering support should be enabled. EXPERIMENTAL! |  | EnableClusterTerminal | Flag if the cluster terminal file /web/db/cluster.html should be created. |  | EnableECALDebugServer | Flag if the ECAL debug server should be started. Note: This will slow ECAL performance significantly. |  | EnableECALScripts | Flag if ECAL scripts should be executed on startup. |  | EnableReadOnly | Flag if the datastore should be open read-only. |  | EnableWebFolder | Flag if the files in the webfolder /web should be served up by the webserver. If false only the REST API is accessible. |  | EnableWebTerminal | Flag if the web terminal file /web/db/term.html should be created. |  | HTTPSCertificate | Name of the webserver certificate which should be used. A new one is created if it does not exist. |  | HTTPSHost | Hostname the webserver should listen to. This host is also used in the dynamically generated swagger definition. |  | HTTPSKey | Name of the webserver private key which should be used. A new one is created if it does not exist. |  | HTTPSPort | Port on which the webserver should listen on. |  | LocationAccessDB | File which is used to store access control information. This file can be edited while the server is running and changes will be picked up immediately. |  | LocationDatastore | Directory for datastore files. |  | LocationHTTPS | Directory for the webserver's SSL related files. |  | LocationUserDB | File which is used to store (hashed) user passwords. |  | LocationWebFolder | Directory of the webserver's webfolder. |  | LockFile | Lockfile for the webserver which will be watched duing runtime. Replacing the content of this file with a single character will shutdown the webserver gracefully. |  | MemoryOnlyStorage | Flag if the datastore should only be kept in memory. |  | ResultCacheMaxAgeSeconds | EQL queries create result sets which are cached. The value describes the amount of time in seconds a result is kept in the cache. |  | ResultCacheMaxSize | EQL queries create result sets which are cached. The value describes the number of results which can be kept in the cache. |    Note: It is not (and will never be) possible to access the REST API via HTTP.    Enabling Access Control  -----------------------  It is possible to enforce access control by enabling the `EnableAccessControl` configuration option. When started with enabled access control EliasDB will only allow known users to connect. Users must authenticate with a password before connecting to the web interface or the REST API. On the first start with the flag enabled the following users are created by default:    |Username|Default Password|Groups|Description|  |---|---|---|---|  |elias|elias|admin/public|Default Admin|  |johndoe|doe|public|Default unprivileged user|    Users can be managed from the console. Please do either delete the default users or change their password after starting EliasDB.    Users are organized in groups and permissions are assigned to groups. Permissions are given to endpoints of the REST API. The following permissions are available:    |Type|Allowed HTTP Request Type|Description|  |---|---|---|  |Create|Post|Creating new data|  |Read|Get|Read data|  |Update|Put|Modify existing data|  |Delete|Delete|Delete data|    The default group permissions are:    |Group|Path|Permissions|  |---|---|---|  |admin|/db/*|`CRUD`|  |public|/|`-R--`|  ||/css/*|`-R--`|  ||/db/*|`-R--`|  ||/img/*|`-R--`|  ||/js/*|`-R--`|  ||/vendor/*|`-R--`|      Building EliasDB  ----------------  To build EliasDB from source you need to have Go installed (go >= 1.12):    - Create a directory, change into it and run:  ```  git clone https://github.com/krotik/eliasdb/ .  ```    - You can build EliasDB's executable with:  ```  go build cli/eliasdb.go  ```    Building EliasDB as Docker image  --------------------------------  EliasDB can be build as a secure and compact Docker image.    - Create a directory, change into it and run:  ```  git clone https://github.com/krotik/eliasdb/ .  ```    - You can now build the Docker image with:  ```  docker build --tag krotik/eliasdb .  ```    Example Applications  --------------------  - [Chat](examples/chat/doc/chat.md) - A simple chat application showing node modification via ECAL script, user management and subscriptions.  - [Data-mining](examples/data-mining/doc/data-mining.md) - A more complex application which uses the cluster feature of EliasDB and GraphQL for data queries.  - [Game](examples/game/doc/game.md) - A multiplayer game example using ECAL for simulating the game scene in the backend.    Further Reading  ---------------  - A design document which describes the different components of the graph database. [Link](eliasdb_design.md)  - A reference for EliasDB's custom query language EQL. [Link](eql.md)  - A reference for EliasDB's support for GraphQL. [Link](graphql.md)  - A quick overview of what you can do when you embed EliasDB in your own Go project. [Link](embedding.md)    License  -------  EliasDB source code is available under the [Mozilla Public License](/LICENSE). """
Big data;https://github.com/benpickles/peity;"""# Peity    [![Build Status](https://travis-ci.org/benpickles/peity.svg?branch=master)](https://travis-ci.org/benpickles/peity)    Peity (sounds like deity) is a jQuery plugin that converts an element's content into a mini `<svg>` pie, donut, line or bar chart.    ## Basic Usage    ### HTML    ```html  <span class=""pie"">3/5</span>  <span class=""donut"">5,2,3</span>  <span class=""line"">3,5,1,6,2</span>  <span class=""bar"">2,5,3,6,2,1</span>  ```    ### JavaScript (jQuery)    ```js  $("".pie"").peity(""pie"");  $("".donut"").peity(""donut"");  $("".line"").peity(""line"");  $("".bar"").peity(""bar"");  ```    ## Docs    More detailed usage can be found at [benpickles.github.io/peity](http://benpickles.github.io/peity/).    ## Development    Run the automated visual regression tests with:        make test    Run a filtered set of tests with:        ARGS=""--grep bar"" make test    To manually view all test cases run:        make server    And hit <http://localhost:8080/>.    ## Release    Update the version string in `jquery.peity.js`, run `make release`, and follow the instructions.    ## Copyright    Copyright 2009-2020 [Ben Pickles](http://benpickles.com/). See [LICENCE](https://github.com/benpickles/peity/blob/master/LICENCE) for details. """
Big data;https://github.com/jacomyal/sigma.js;"""[![Build Status](https://travis-ci.org/jacomyal/sigma.js.svg)](https://travis-ci.org/jacomyal/sigma.js)    sigma.js - v1.2.1  =================    Sigma is a JavaScript library dedicated to graph drawing, mainly developed by [@jacomyal](https://github.com/jacomyal) and [@Yomguithereal](https://github.com/Yomguithereal).    ### Resources    [The website](http://sigmajs.org) provides a global overview of the project, and the documentation is available in the [GitHub Wiki](https://github.com/jacomyal/sigma.js/wiki).    Also, the `plugins` and `examples` directories contain various use-cases that might help you understand how to use sigma.    ### How to use it    To use it, clone the repository:    ```  git clone git@github.com:jacomyal/sigma.js.git  ```    To build the code:     - Install [Node.js](http://nodejs.org/).   - Install [gjslint](https://developers.google.com/closure/utilities/docs/linter_howto?hl=en).   - Use `npm install` to install sigma development dependencies.   - Use `npm run build` to minify the code with [Uglify](https://github.com/mishoo/UglifyJS). The minified file `sigma.min.js` will then be accessible in the `build/` folder.    Also, you can customize the build by adding or removing files from the `coreJsFiles` array in `Gruntfile.js` before applying the grunt task.    ### Contributing    You can contribute by submitting [issues tickets](http://github.com/jacomyal/sigma.js/issues) and proposing [pull requests](http://github.com/jacomyal/sigma.js/pulls). Make sure that tests and linting pass before submitting any pull request by running the command `grunt`.    The whole source code is validated by the [Google Closure Linter](https://developers.google.com/closure/utilities/) and [JSHint](http://www.jshint.com/), and the comments are written in [JSDoc](http://en.wikipedia.org/wiki/JSDoc) (tags description is available [here](https://developers.google.com/closure/compiler/docs/js-for-compiler)). """
Big data;https://github.com/deeplearning4j/rl4j;"""# RL4J: Reinforcement Learning for Java    For support questions regarding RL4J, please contact help@pathmind.com.    RL4J is a reinforcement learning framework integrated with deeplearning4j and released under an Apache 2.0 open-source license.     * DQN (Deep Q Learning with double DQN)  * Async RL (A3C, Async NStepQlearning)    Both for Low-Dimensional (array of info) and high-dimensional (pixels) input.      ![DOOM](docs/images/doom.gif)      ![Cartpole](docs/images/cartpole.gif)    A useful blog post to introduce you to reinforcement learning, DQN and Async RL:    [Blog post](https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html)      # Quickstart    * mvn install    # Visualisation    [webapp-rl4j](https://github.com/rubenfiszel/webapp-rl4j)    # Doom    Doom is not ready yet but you can make it work if you feel adventurous with some additional steps:    * You will need vizdoom, compile the native lib and move it into the root of your project in a folder  * export MAVEN_OPTS=-Djava.library.path=THEFOLDEROFTHELIB  * mvn compile exec:java -Dexec.mainClass=""YOURMAINCLASS""    # Malmo (Minecraft)    ![Malmo](docs/images/malmo.gif)    * Download and unzip Malmo from [here](https://github.com/Microsoft/malmo/releases)  * export MALMO_HOME=YOURMALMO_FOLDER  * export MALMO_XSD_PATH=$MALMO_HOME/Schemas  * launch malmo per [instructions](https://github.com/Microsoft/malmo#launching-minecraft-with-our-mod)    # WIP    * Documentation  * Serialization/Deserialization (load save)  * Compression of pixels in order to store 1M state in a reasonnable amount of memory  * Async learning: A3C and nstep learning (requires some missing features from dl4j (calc and apply gradients)).    # Author    [Ruben Fiszel](http://rubenfiszel.github.io/)  """
Big data;https://github.com/dalmatinerdb/dalmatinerdb;"""Read more at the [official site](https://dalmatiner.io/) and the [documentation](https://docs.dalmatiner.io).    # DalmatinerDB  DalmatinerDB is a metric database written in pure Erlang. It takes advantage of some special properties of metrics to make some tradeoffs. The goal is to make a store for metric data (time, value of a metric) that is fast, has a low overhead, and is easy to query and manage.    # Tradeoffs  I try here to be explicit about the tradeoffs we made, so people can decide if they are happy with them (costs vs gains). The acceptable tradeoffs differ from case to case, but I hope the choices we made fit a metric store quite well. If you are comparing DalmatinerDB with X, please don't assume that just because X does not list the tradeoffs they made, they have none; be inquisitive and make a decision based on facts, not marketing.    A detailed comparison between databases can be found here:    [https://docs.google.com/spreadsheets/d/1sMQe9oOKhMhIVw9WmuCEWdPtAoccJ4a-IuZv4fXDHxM/edit#gid=0](https://docs.google.com/spreadsheets/d/1sMQe9oOKhMhIVw9WmuCEWdPtAoccJ4a-IuZv4fXDHxM/edit#gid=0)    ## Let the Filesystem handle it  A lot of work is handed down to the file system, ZFS is exceptionally smart and can do things like checksums, compressions and caching very well. Handing down these tasks to the filesystem simplifies the codebase, and builds on very well tested and highly performant code, instead of trying to reimplement it.    ## Prioritise the overall writes over individual ones  DalmatinerDB offers a 'best effort' on storing the metrics, there is no log for writes (if enabled in ZFS, the ZIL (ZFS Intent Log) can log write operations) or forced sync after each write. This means that if your network fails, packets can get lost, and if your server crashes, unwritten data can be lost.    The point is that losing one or two metric points in a huge series is a non-problem, the importance of a metric is often seen in aggregates, and DalmatinerDB fills in the blanks with the last written value. However there is explicitly no guarantee that data is written, so *this can be an issue if every single point of metric is of importance!*    ## Flat files  Data is stored in a flat binary format, this means that reads and writes can be calculated to a filename+offset by simple math, there is no need for traversing data-structures. This means however that if a metric stops unwritten, points can 'linger' around for a while depending on how the file size was picked.    As an example: if metrics are stored with a precision down to the second, and 1 week of data is stored per file, up to one week of unused data can be stored, but it should be taken into account that with compression this data will be compressed quite well. """
Big data;https://github.com/google/leveldb;"""**LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.**    [![ci](https://github.com/google/leveldb/actions/workflows/build.yml/badge.svg)](https://github.com/google/leveldb/actions/workflows/build.yml)    Authors: Sanjay Ghemawat (sanjay@google.com) and Jeff Dean (jeff@google.com)    # Features      * Keys and values are arbitrary byte arrays.    * Data is stored sorted by key.    * Callers can provide a custom comparison function to override the sort order.    * The basic operations are `Put(key,value)`, `Get(key)`, `Delete(key)`.    * Multiple changes can be made in one atomic batch.    * Users can create a transient snapshot to get a consistent view of data.    * Forward and backward iteration is supported over the data.    * Data is automatically compressed using the [Snappy compression library](https://google.github.io/snappy/).    * External activity (file system operations etc.) is relayed through a virtual interface so users can customize the operating system interactions.    # Documentation      [LevelDB library documentation](https://github.com/google/leveldb/blob/main/doc/index.md) is online and bundled with the source code.    # Limitations      * This is not a SQL database.  It does not have a relational data model, it does not support SQL queries, and it has no support for indexes.    * Only a single process (possibly multi-threaded) can access a particular database at a time.    * There is no client-server support builtin to the library.  An application that needs such support will have to wrap their own server around the library.    # Getting the Source    ```bash  git clone --recurse-submodules https://github.com/google/leveldb.git  ```    # Building    This project supports [CMake](https://cmake.org/) out of the box.    ### Build for POSIX    Quick start:    ```bash  mkdir -p build && cd build  cmake -DCMAKE_BUILD_TYPE=Release .. && cmake --build .  ```    ### Building for Windows    First generate the Visual Studio 2017 project/solution files:    ```cmd  mkdir build  cd build  cmake -G ""Visual Studio 15"" ..  ```  The default default will build for x86. For 64-bit run:    ```cmd  cmake -G ""Visual Studio 15 Win64"" ..  ```    To compile the Windows solution from the command-line:    ```cmd  devenv /build Debug leveldb.sln  ```    or open leveldb.sln in Visual Studio and build from within.    Please see the CMake documentation and `CMakeLists.txt` for more advanced usage.    # Contributing to the leveldb Project    The leveldb project welcomes contributions. leveldb's primary goal is to be  a reliable and fast key/value store. Changes that are in line with the  features/limitations outlined above, and meet the requirements below,  will be considered.    Contribution requirements:    1. **Tested platforms only**. We _generally_ will only accept changes for     platforms that are compiled and tested. This means POSIX (for Linux and     macOS) or Windows. Very small changes will sometimes be accepted, but     consider that more of an exception than the rule.    2. **Stable API**. We strive very hard to maintain a stable API. Changes that     require changes for projects using leveldb _might_ be rejected without     sufficient benefit to the project.    3. **Tests**: All changes must be accompanied by a new (or changed) test, or     a sufficient explanation as to why a new (or changed) test is not required.    4. **Consistent Style**: This project conforms to the     [Google C++ Style Guide](https://google.github.io/styleguide/cppguide.html).     To ensure your changes are properly formatted please run:       ```     clang-format -i --style=file <file>     ```    We are unlikely to accept contributions to the build configuration files, such  as `CMakeLists.txt`. We are focused on maintaining a build configuration that  allows us to test that the project works in a few supported configurations  inside Google. We are not currently interested in supporting other requirements,  such as different operating systems, compilers, or build systems.    ## Submitting a Pull Request    Before any pull request will be accepted the author must first sign a  Contributor License Agreement (CLA) at https://cla.developers.google.com/.    In order to keep the commit timeline linear  [squash](https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History#Squashing-Commits)  your changes down to a single commit and [rebase](https://git-scm.com/docs/git-rebase)  on google/leveldb/main. This keeps the commit timeline linear and more easily sync'ed  with the internal repository at Google. More information at GitHub's  [About Git rebase](https://help.github.com/articles/about-git-rebase/) page.    # Performance    Here is a performance report (with explanations) from the run of the  included db_bench program.  The results are somewhat noisy, but should  be enough to get a ballpark performance estimate.    ## Setup    We use a database with a million entries.  Each entry has a 16 byte  key, and a 100 byte value.  Values used by the benchmark compress to  about half their original size.        LevelDB:    version 1.1      Date:       Sun May  1 12:11:26 2011      CPU:        4 x Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz      CPUCache:   4096 KB      Keys:       16 bytes each      Values:     100 bytes each (50 bytes after compression)      Entries:    1000000      Raw Size:   110.6 MB (estimated)      File Size:  62.9 MB (estimated)    ## Write performance    The ""fill"" benchmarks create a brand new database, in either  sequential, or random order.  The ""fillsync"" benchmark flushes data  from the operating system to the disk after every operation; the other  write operations leave the data sitting in the operating system buffer  cache for a while.  The ""overwrite"" benchmark does random writes that  update existing keys in the database.        fillseq      :       1.765 micros/op;   62.7 MB/s      fillsync     :     268.409 micros/op;    0.4 MB/s (10000 ops)      fillrandom   :       2.460 micros/op;   45.0 MB/s      overwrite    :       2.380 micros/op;   46.5 MB/s    Each ""op"" above corresponds to a write of a single key/value pair.  I.e., a random write benchmark goes at approximately 400,000 writes per second.    Each ""fillsync"" operation costs much less (0.3 millisecond)  than a disk seek (typically 10 milliseconds).  We suspect that this is  because the hard disk itself is buffering the update in its memory and  responding before the data has been written to the platter.  This may  or may not be safe based on whether or not the hard disk has enough  power to save its memory in the event of a power failure.    ## Read performance    We list the performance of reading sequentially in both the forward  and reverse direction, and also the performance of a random lookup.  Note that the database created by the benchmark is quite small.  Therefore the report characterizes the performance of leveldb when the  working set fits in memory.  The cost of reading a piece of data that  is not present in the operating system buffer cache will be dominated  by the one or two disk seeks needed to fetch the data from disk.  Write performance will be mostly unaffected by whether or not the  working set fits in memory.        readrandom  : 16.677 micros/op;  (approximately 60,000 reads per second)      readseq     :  0.476 micros/op;  232.3 MB/s      readreverse :  0.724 micros/op;  152.9 MB/s    LevelDB compacts its underlying storage data in the background to  improve read performance.  The results listed above were done  immediately after a lot of random writes.  The results after  compactions (which are usually triggered automatically) are better.        readrandom  : 11.602 micros/op;  (approximately 85,000 reads per second)      readseq     :  0.423 micros/op;  261.8 MB/s      readreverse :  0.663 micros/op;  166.9 MB/s    Some of the high cost of reads comes from repeated decompression of blocks  read from disk.  If we supply enough cache to the leveldb so it can hold the  uncompressed blocks in memory, the read performance improves again:        readrandom  : 9.775 micros/op;  (approximately 100,000 reads per second before compaction)      readrandom  : 5.215 micros/op;  (approximately 190,000 reads per second after compaction)    ## Repository contents    See [doc/index.md](doc/index.md) for more explanation. See  [doc/impl.md](doc/impl.md) for a brief overview of the implementation.    The public interface is in include/leveldb/*.h.  Callers should not include or  rely on the details of any other header files in this package.  Those  internal APIs may be changed without warning.    Guide to header files:    * **include/leveldb/db.h**: Main interface to the DB: Start here.    * **include/leveldb/options.h**: Control over the behavior of an entire database,  and also control over the behavior of individual reads and writes.    * **include/leveldb/comparator.h**: Abstraction for user-specified comparison function.  If you want just bytewise comparison of keys, you can use the default  comparator, but clients can write their own comparator implementations if they  want custom ordering (e.g. to handle different character encodings, etc.).    * **include/leveldb/iterator.h**: Interface for iterating over data. You can get  an iterator from a DB object.    * **include/leveldb/write_batch.h**: Interface for atomically applying multiple  updates to a database.    * **include/leveldb/slice.h**: A simple module for maintaining a pointer and a  length into some other byte array.    * **include/leveldb/status.h**: Status is returned from many of the public interfaces  and is used to report success and various kinds of errors.    * **include/leveldb/env.h**:  Abstraction of the OS environment.  A posix implementation of this interface is  in util/env_posix.cc.    * **include/leveldb/table.h, include/leveldb/table_builder.h**: Lower-level modules that most  clients probably won't use directly. """
Big data;https://github.com/linkedin/ambry;"""# Ambry    [![Github Actions CI](https://github.com/linkedin/ambry/actions/workflows/github-actions.yml/badge.svg)](https://github.com/linkedin/ambry/actions/workflows/github-actions.yml)  [![codecov.io](https://codecov.io/github/linkedin/ambry/branch/master/graph/badge.svg)](https://codecov.io/github/linkedin/ambry)  [![license](https://img.shields.io/github/license/linkedin/ambry.svg)](LICENSE)    Ambry is a distributed object store that supports storage of trillion of small immutable objects (50K -100K) as well as billions of large objects. It was specifically designed to store and serve media objects in web companies. However, it can be used as a general purpose storage system to store DB backups, search indexes or business reports. The system has the following characterisitics:     1. Highly available and horizontally scalable  2. Low latency and high throughput  3. Optimized for both small and large objects  4. Cost effective  5. Easy to use    Requires at least JDK 1.8.    ## Documentation  Detailed documentation is available at https://github.com/linkedin/ambry/wiki    ## Research  Paper introducing Ambry at [SIGMOD 2016](http://sigmod2016.org/) -> http://dprg.cs.uiuc.edu/data/files/2016/ambry.pdf    Reach out to us at ambrydev@googlegroups.com if you would like us to list a paper that is based off of research on Ambry.    ## Getting Started  ##### Step 1: Download the code, build it and prepare for deployment.  To get the latest code and build it, do        $ git clone https://github.com/linkedin/ambry.git       $ cd ambry      $ ./gradlew allJar      $ cd target      $ mkdir logs  Ambry uses files that provide information about the cluster to route requests from the frontend to servers and for replication between servers. We will use a simple clustermap that contains a single server with one partition. The partition will use `/tmp` as the mount point.  ##### Step 2: Deploy a server.      $ nohup java -Dlog4j.configuration=file:../config/log4j.properties -jar ambry.jar --serverPropsFilePath ../config/server.properties --hardwareLayoutFilePath ../config/HardwareLayout.json --partitionLayoutFilePath ../config/PartitionLayout.json > logs/server.log &    Through this command, we configure the log4j properties, provide the server with configuration options and cluster definitions and redirect output to a log. Note down the process ID returned (`serverProcessID`) because it will be needed for shutdown.    The log will be available at `logs/server.log`. Alternately, you can change the log4j properties to write the log messages to a file instead of standard output.  ##### Step 3: Deploy a frontend.      $ nohup java -Dlog4j.configuration=file:../config/log4j.properties -cp ""*"" com.github.ambry.frontend.AmbryFrontendMain --serverPropsFilePath ../config/frontend.properties --hardwareLayoutFilePath ../config/HardwareLayout.json --partitionLayoutFilePath ../config/PartitionLayout.json > logs/frontend.log &    Note down the process ID returned (`frontendProcessID`) because it will be needed for shutdown. Make sure that the frontend is ready to receive requests.        $ curl http://localhost:1174/healthCheck      GOOD  The log will be available at `logs/frontend.log`. Alternately, you can change the log4j properties to write the log messages to a file instead of standard output.  ##### Step 4: Interact with Ambry !  We are now ready to store and retrieve data from Ambry. Let us start by storing a simple image. For demonstration purposes, we will use an image `demo.gif` that has been copied into the `target` folder.  ###### POST      $ curl -i -H ""x-ambry-service-id:CUrlUpload""  -H ""x-ambry-owner-id:`whoami`"" -H ""x-ambry-content-type:image/gif"" -H ""x-ambry-um-description:Demonstration Image"" http://localhost:1174/ --data-binary @demo.gif      HTTP/1.1 201 Created      Location: AmbryID      Content-Length: 0  The CUrl command creates a `POST` request that contains the binary data in demo.gif. Along with the file data, we provide headers that act as blob properties. These include the size of the blob, the service ID, the owner ID and the content type.    In addition to these properties, Ambry also has a provision for arbitrary user defined metadata. We provide `x-ambry-um-description` as user metadata. Ambry does not interpret this data and it is purely for user annotation.  The `Location` header in the response is the blob ID of the blob we just uploaded.  ###### GET - Blob Info  Now that we stored a blob, let us verify some properties of the blob we uploaded.        $ curl -i http://localhost:1174/AmbryID/BlobInfo      HTTP/1.1 200 OK      x-ambry-blob-size: {Blob size}      x-ambry-service-id: CUrlUpload      x-ambry-creation-time: {Creation time}      x-ambry-private: false      x-ambry-content-type: image/gif      x-ambry-owner-id: {username}      x-ambry-um-desc: Demonstration Image      Content-Length: 0  ###### GET - Blob  Now that we have verified that Ambry returns properties correctly, let us obtain the actual blob.        $ curl http://localhost:1174/AmbryID > demo-downloaded.gif      $ diff demo.gif demo-downloaded.gif       $  This confirms that the data that was sent in the `POST` request matches what we received in the `GET`. If you would like to see the image, simply point your browser to `http://localhost:1174/AmbryID` and you should see the image that was uploaded !  ###### DELETE  Ambry is an immutable store and blobs cannot be updated but they can be deleted in order to make them irretrievable. Let us go ahead and delete the blob we just created.        $ curl -i -X DELETE http://localhost:1174/AmbryID      HTTP/1.1 202 Accepted      Content-Length: 0  You will no longer be able to retrieve the blob properties or data.        $ curl -i http://localhost:1174/AmbryID/BlobInfo      HTTP/1.1 410 Gone      Content-Type: text/plain; charset=UTF-8      Content-Length: 17      Connection: close        Failure: 410 Gone  ##### Step 5: Stop the frontend and server.      $ kill -15 frontendProcessID      $ kill -15 serverProcessID  You can confirm that the services have been shut down by looking at the logs.  ##### Additional information:  In addition to the simple APIs demonstrated above, Ambry provides support for `GET` of only user metadata and `HEAD`. In addition to the `POST` of binary data that was demonstrated, Ambry also supports `POST` of `multipart/form-data` via CUrl or web forms.  Other features of interest include:  * **Time To Live (TTL)**: During `POST`, a TTL in seconds can be provided through the addition of a header named `x-ambry-ttl`. This means that Ambry will stop serving the blob after the TTL has expired. On `GET`, expired blobs behave the same way as deleted blobs.  * **Private**: During `POST`, providing a header named `x-ambry-private` with the value `true` will mark the blob as private. API behavior can be configured based on whether a blob is public or private. """
Big data;https://github.com/polyaxon/polyaxon;"""[![License: Apache 2](https://img.shields.io/badge/License-apache2-blue.svg?style=flat&longCache=true)](LICENSE)  [![Polyaxon API](https://img.shields.io/docker/pulls/polyaxon/polyaxon-api)](https://hub.docker.com/r/polyaxon/polyaxon-api)  [![Slack](https://img.shields.io/badge/Slack-1.4k%20members-blue.svg?style=flat&logo=slack&longCache=true)](https://polyaxon.com/slack/)    [![Docs](https://img.shields.io/badge/docs-stable-brightgreen.svg?style=flat&longCache=true)](https://polyaxon.com/docs/)  [![Release](https://img.shields.io/badge/release-v1.16.1-brightgreen.svg?longCache=true)](https://polyaxon.com/docs/releases/1-16/)  [![GitHub](https://img.shields.io/badge/issue_tracker-github-blue?style=flat&logo=github&longCache=true)](https://github.com/polyaxon/polyaxon/issues)  [![GitHub](https://img.shields.io/badge/roadmap-github-blue?style=flat&logo=github&longCache=true)](https://github.com/orgs/polyaxon/projects/5)    [![CLI](https://github.com/polyaxon/polyaxon/actions/workflows/core.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/core.yml)  [![Traceml](https://github.com/polyaxon/polyaxon/actions/workflows/traceml.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/traceml.yml)  [![Datatile](https://github.com/polyaxon/polyaxon/actions/workflows/datatile.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/datatile.yml)  [![Platform](https://github.com/polyaxon/polyaxon/actions/workflows/platform.yml/badge.svg)](https://github.com/polyaxon/polyaxon/actions/workflows/platform.yml)  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/90c05b6b112548c1a88b950beceacb69)](https://www.codacy.com/app/polyaxon/polyaxon?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=polyaxon/polyaxon&amp;utm_campaign=Badge_Grade)    <br>  <p align=""center"">    <p align=""center"">      <a href=""https://polyaxon.com/?utm_source=github&utm_medium=logo"" target=""_blank"">        <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/logo/vector/primary-white-default-monochrome.svg"" alt=""polyaxon"" height=""100"">      </a>    </p>    <p align=""center"">      Reproduce, Automate, Scale your data science.    </p>  </p>  <br>      Welcome to Polyaxon, a platform for building, training, and monitoring large scale deep learning applications.  We are making a system to solve reproducibility, automation, and scalability for machine learning applications.    Polyaxon deploys into any data center, cloud provider, or can be hosted and managed by Polyaxon, and it supports all the major deep learning frameworks such as Tensorflow, MXNet, Caffe, Torch, etc.    Polyaxon makes it faster, easier, and more efficient to develop deep learning applications by managing workloads with smart container and node management. And it turns GPU servers into shared, self-service resources for your team or organization.    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/demo.gif"" alt=""demo"" width=""80%"">  </p>  <br>    # Install    #### TL;DR;    * Install CLI        ```bash      # Install Polyaxon CLI      $ pip install -U polyaxon      ```     * Create a deployment        ```bash      # Create a namespace      $ kubectl create namespace polyaxon        # Add Polyaxon charts repo      $ helm repo add polyaxon https://charts.polyaxon.com        # Deploy Polyaxon      $ polyaxon admin deploy -f config.yaml        # Access API      $ polyaxon port-forward      ```    Please check [polyaxon installation guide](https://polyaxon.com/docs/setup/)    # Quick start    #### TL;DR;     * Start a project        ```bash      # Create a project      $ polyaxon project create --name=quick-start --description='Polyaxon quick start.'      ```     * Train and track logs & resources        ```bash      # Upload code and start experiments      $ polyaxon run -f experiment.yaml -u -l      ```     * Dashboard        ```bash      # Start Polyaxon dashboard      $ polyaxon dashboard        Dashboard page will now open in your browser. Continue? [Y/n]: y      ```    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/compare.png"" alt=""compare"" width=""400"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/dashboards.png"" alt=""dashboards"" width=""400"">  </p>  <br>     * Notebook      ```bash      # Start Jupyter notebook for your project      $ polyaxon run --hub notebook      ```    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/notebook.png"" alt=""compare"" width=""400"">  </p>  <br>     * Tensorboard      ```bash      # Start TensorBoard for a run's output      $ polyaxon run --hub tensorboard -P uuid=UUID      ```    <br>  <p align=""center"">    <img src=""https://raw.githubusercontent.com/polyaxon/polyaxon/master/artifacts/tensorboard.png"" alt=""tensorboard"" width=""400"">  </p>  <br>    Please check our [quick start guide](https://polyaxon.com/docs/intro/quick-start/) to start training your first experiment.    # Distributed job    Polyaxon supports and simplifies distributed jobs.  Depending on the framework you are using, you need to deploy the corresponding operator, adapt your code to enable the distributed training,  and update your polyaxonfile.    Here are some examples of using distributed training:      * [Distributed Tensorflow](https://polyaxon.com/docs/experimentation/distributed/tf-jobs/)   * [Distributed Pytorch](https://polyaxon.com/docs/experimentation/distributed/pytorch-jobs/)   * [Distributed MPI](https://polyaxon.com/docs/experimentation/distributed/mpi-jobs/)   * [Horovod](https://polyaxon.com/integrations/horovod/)   * [Spark](https://polyaxon.com/docs/experimentation/distributed/spark-jobs/)   * [Dask](https://polyaxon.com/docs/experimentation/distributed/dask-jobs/)    # Hyperparameters tuning    Polyaxon has a concept for suggesting hyperparameters and managing their results very similar to Google Vizier called experiment groups.  An experiment group in Polyaxon defines a search algorithm, a search space, and a model to train.     * [Grid search](https://polyaxon.com/docs/automation/optimization-engine/grid-search/)   * [Random search](https://polyaxon.com/docs/automation/optimization-engine/random-search/)   * [Hyperband](https://polyaxon.com/docs/automation/optimization-engine/hyperband/)   * [Bayesian Optimization](https://polyaxon.com/docs/automation/optimization-engine/bayesian-optimization/)   * [Hyperopt](https://polyaxon.com/docs/automation/optimization-engine/hyperopt/)   * [Custom Iterative Optimization](https://polyaxon.com/docs/automation/optimization-engine/iterative/)    # Parallel executions    You can run your processing or model training jobs in parallel, Polyaxon provides a [mapping](https://polyaxon.com/docs/automation/mapping/) abstraction to manage concurrent jobs.    # DAGs and workflows    [Polyaxon DAGs](https://polyaxon.com/docs/automation/flow-engine/) is a tool that provides container-native engine for running machine learning pipelines.   A DAG manages multiple operations with dependencies. Each operation is defined by a component runtime.   This means that operations in a DAG can be jobs, services, distributed jobs, parallel executions, or nested DAGs.       # Architecture    ![Polyaxon architecture](artifacts/polyaxon_architecture.png)    # Documentation    Check out our [documentation](https://polyaxon.com/docs/) to learn more about Polyaxon.    # Dashboard    Polyaxon comes with a dashboard that shows the projects and experiments created by you and your team members.    To start the dashboard, just run the following command in your terminal    ```bash  $ polyaxon dashboard -y  ```    # Project status    Polyaxon is stable and it's running in production mode at many startups and Fortune 500 companies.     # Contributions    Please follow the contribution guide line: *[Contribute to Polyaxon](CONTRIBUTING.md)*.      # Research    If you use Polyaxon in your academic research, we would be grateful if you could cite it.    Feel free to [contact us](mailto:contact@polyaxon.com), we would love to learn about your project and see how we can support your custom need. """
Big data;https://github.com/vega/vega;"""# Vega: A Visualization Grammar <a href=""https://vega.github.io/vega/""><img align=""right"" src=""https://github.com/vega/logos/blob/master/assets/VG_Color@64.png?raw=true"" height=""38""></img></a>    <a href=""https://vega.github.io/vega/examples"">  <img src=""https://vega.github.io/vega/assets/banner.png"" alt=""Vega Examples"" width=""900""></img>  </a>    **Vega** is a *visualization grammar*, a declarative format for creating, saving, and sharing interactive visualization designs. With Vega you can describe data visualizations in a JSON format, and generate interactive views using either HTML5 Canvas or SVG.    For documentation, tutorials, and examples, see the [Vega website](https://vega.github.io/vega). For a description of changes between Vega 2 and later versions, please refer to the [Vega Porting Guide](https://vega.github.io/vega/docs/porting-guide/).    ## Build Instructions    For a basic setup allowing you to build Vega and run examples:    - Clone `https://github.com/vega/vega`.  - Run `yarn` to install dependencies for all packages. If you don't have yarn installed, see https://yarnpkg.com/en/docs/install. We use [Yarn workspaces](https://yarnpkg.com/lang/en/docs/workspaces/) to manage multiple packages within this [monorepo](https://en.wikipedia.org/wiki/Monorepo).  - Once installation is complete, run `yarn test` to run test cases, or run `yarn build` to build output files for all packages.  - After running either `yarn test` or `yarn build`, run `yarn serve` to launch a local web server &mdash; your default browser will open and you can browse to the `""test""` folder to view test specifications.    This repository includes the Vega website and documentation in the `docs` folder. To launch the website locally, first run `bundle install` in the `docs` folder to install the necessary Jekyll libraries. Afterwards, use `yarn docs` to build the documentation and launch a local webserver. After launching, you can open [`http://127.0.0.1:4000/vega/`](http://127.0.0.1:4000/vega/) to see the website.    ## ES5 Support  For backwards compatibility, Vega includes a [babel-ified](https://babeljs.io/) ES5-compatible version of the code in `packages/vega/build-es5` directory. Older browser would also require several polyfill libraries:    ```html  <script src=""https://cdnjs.cloudflare.com/ajax/libs/babel-polyfill/7.4.4/polyfill.min.js""></script>  <script src=""https://cdn.jsdelivr.net/npm/regenerator-runtime@0.13.3/runtime.min.js""></script>  <script src=""https://cdn.jsdelivr.net/npm/whatwg-fetch@3.0.0/dist/fetch.umd.min.js""></script>  ```    ## Contributions, Development, and Support    Interested in contributing to Vega? Please see our [contribution and development guidelines](CONTRIBUTING.md), subject to our [code of conduct](https://vega.github.io/vega/about/code-of-conduct/).    Looking for support, or interested in sharing examples and tips? Post to the [Vega discussion forum](https://groups.google.com/forum/#!forum/vega-js) or join the [Vega slack organization](https://bit.ly/join-vega-slack-2020)! We also have examples available as [Observable notebooks](https://observablehq.com/@vega).    If you're curious about system performance, see some [in-browser benchmarks](https://observablehq.com/@vega/vega-performance-tests). Read about future plans in [our roadmap](https://github.com/vega/roadmap/projects/1). """
Big data;https://github.com/sonalgoyal/hiho;"""# HIHO: Hadoop In, Hadoop Out.     > Hadoop Data Integration, deduplication, incremental update and more.      This branch is for support for HIHO on Apache Hadoop 0.21.    ## Import from a database to HDFS    **query based import**      Join multiple tables, provide where conditions, dynamically bind parameters to SQL queries to get data to Hadoop. As simple as creating a simple config and running the job.    	bin/hadoop jar hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf dbInputQueryDelimited.xml    or     	${HIHO_HOME}/scripts/hiho import   		-jdbcDriver <jdbcDriver>   		-jdbcUrl <jdbcUrl>   		-jdbcUsername <jdbcUsername>   		-jdbcPassword <jdbcPassword>   		-inputQuery <inputQuery>   		-inputBoundingQuery <inputBoundingQuery>   		-outputPath <outputPath>   		-outputStrategy <outputStrategy>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-inputOrderBy <inputOrderBy>      **table based import**      	bin/hadoop jar hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf dbInputTableDelimited.xml    or      	${HIHO_HOME}/scripts/hiho import   		-jdbcDriver <jdbcDriver>   		-jdbcUrl <jdbcUrl>   		-jdbcUsername <jdbcUsername>   		-jdbcPassword <jdbcPassword>  		-outputPath <outputPath>   		-outputStrategy <outputStrategy>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-inputOrderBy <inputOrderBy>   		-inputTableName <inputTableName>   		-inputFieldNames <inputFieldNames>    **incremental import** by appending to existing `HDFS` location so that all data is in one place.  just specify `isAppend = true` in the configurations and import. Import will be written to existing HDFS folder.    **configurable format for data import: delimited, avro** by specifying the `mapreduce.jdbc.hiho.input.outputStrategy` as DELIMITED or AVRO.     **Note:**     1. Please specify delimiter in double qoutes, because in some cases such as semi colon ';' it breaks For example `-delimiter "";""`. If you are specifing * for `inputFieldNames` then also you put in double qoutes      ## Export to Databases    **high performance MySQL loading using LOAD DATA INFILE**    	${HIHO_HOME}/scripts/hiho export mysql  		-inputPath <inputPath>   		-url <url>   		-userName <userName>   		-password <password>   		-querySuffix  <querySuffix>    **high performance Oracle loading by creating external tables.** See [expert opinion](http://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:6611962171229)    For information on external tables, check [here](http://download.oracle.com/docs/cd/B12037_01/server.101/b10825/et_concepts.htm)    On the Oracle server    1. Make a folder    		mkdir -p ageTest    2. Create a directory through the Oracle Client (sqlplus) and grant it privileges.    		sqlplus>create or replace directory age_ext as '/home/nube/age';    3. Allow ftp to the Oracle server  	  		${HIHO_HOME}/scripts/hiho export oracle   			-inputPath <inputPath>   			-oracleFtpAddress <oracleFtpAddress>   			-oracleFtpPortNumber <oracleFtpPortNumber>   			-oracleFtpUserName <oracleFtpUserName>   			-oracleFtpPassword <oracleFtpPassword>   			-oracleExternalTableDirectory <oracleExternalTableDirectory>   			-driver <driver>   			-url <url>   			-userName <userName>   			-password <password>   			-externalTable <createExternalTableQuery>    **custom loading and export to any database** by emitting own `GenericDBWritables`. Check `DelimitedLoadMapper`    ## Export to SalesForce      **send computed map reduce results to Salesforce.**    For this, you need to have a developer account with Bulk API enabled. You can join at http://developer.force.com/join     If you get message:     >[LoginFault [ApiFault  exceptionCode='INVALID_LOGIN' exceptionMessage='Invalid username, password, security token; or user locked out.'  ""Invalid username, password, security token; or user locked out. Are you at a new location? When accessing Salesforce--either via a desktop client or the API--from outside of your company’s trusted networks, you must add a security token to your password to log in. To receive a new security token, log in to [Salesforce](http://www.salesforce.com) and click Setup | My Personal Information | Reset Security Token.""  login and get the security token.     then try    	sfUserName - Name of Salesforce account  	sfPassword - Password and security token. The Security Token can be obtained by logging in to the Salesforce.com site and clicking on Reset Security Token.  	sfObjectType - The Salesforce object to export  	sfHeaders - header describing the Salesforce object properties. For more information, refer to the Bulk API Developer's Guide.    	${HIHO_HOME}/scripts/hiho export saleforce   		-inputPath <inputPath>   		-sfUserName <sfUserName>   		-sfPassword <sfPassword>   		-sfObjectType <sfObjectType>   		-sfHeaders <sfHeaders>      ## Export results to an FTP Server.    Use the `co.nubetech.hiho.mapreduce.lib.output.FTPOutputFormat` directly in your job, just like `FileOutputFormat`. For usage, check `co.nubetech.hiho.job.ExportToFTPserver`. This job writes the output directly to an FTP server.   It can be invoked as:    	${HIHO_HOME}/scripts/hiho export ftp   		-inputPath <inputPath>   		-outputPath <outputPath>   		-ftpUserName <ftpUserName>   		-ftpAddress <ftpAddress>   		-ftpPortNumper <ftpPortNumper>   		-ftpPassword <ftpPassword>    Where:    	ftpUserName - FTP server login username  	ftpAddress - FTP server address  	ftpPortNumper - FTP port  	ftpPassword - FTP server password  	outputPath is the location on the FTP server to which the output will be written. It should be a complete directory path - /home/sgoyal/output        ## Export to Hive  This is used to export data from any other database to Hive database.   Hive export can be done in two method query base and table based configuration needed are     	mapreduce.jdbc.hiho.input.loadTo - this configuration defines in which database you want to load your data from HDFS for eg:- hive  	mapreduce.jdbc.hiho.input.loadToPath - Our program also generates script for all queries , this configuration defines where to store that script on your local system    	mapreduce.jdbc.hiho.hive.driver - name of hive jdbc driver For eg:- org.apache.hadoop.hive.jdbc.HiveDriver  	mapreduce.jdbc.hiho.hive.url - hive url for jdbc connection For eg:- jdbc:hive:// (for embedded mode),jdbc:hive://localhost:10000/default for standalone mode  	mapreduce.jdbc.hiho.hive.usrName - user name for jdbc connection   	mapreduce.jdbc.hiho.hive.password - password for jdbc connection  	mapreduce.jdbc.hiho.hive.partitionBy - This configuration is when we want to create partitioned hive table. For eg :- country:string:us;name:string:jack (basic partition), country:string:us;name:string (static and one dynamic partition), country:string (dynamic partition) till now we allow only one dynamic partition  											We also allow to store data in a table for multiple partition at a time for that value is given as country:string:us,uk,aus for this we need to define three different queries or table in there respective configurations   	mapreduce.jdbc.hiho.hive.ifNotExists - set true if you want include 'if not exits' clause in your create table query  	mapreduce.jdbc.hiho.hive.tableName - write the name for the table in the hive you want to create  	mapreduce.jdbc.hiho.hive.sortedBy - this can be only used if clusteredBy configuration is defined, in this give the name of column by which u want to sort your data  	mapreduce.jdbc.hiho.hive.clusteredBy - This configuration defines name of column by which you want to cluster your data and define the number of buckets you want to create. For eg:- name:2     Execution command for table based     	bin/hadoop jar ~/workspace/hiho/build/classes/hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf  ~/workspace/hiho/conf/dbInputTableDelimitedHive.xml    or    	${HIHO_HOME}/scripts/hiho import   		-jdbcDriver <jdbcDriver>   		-jdbcUrl <jdbcUrl>   		-jdbcUsername <jdbcUsername>   		-jdbcPassword <jdbcPassword>   		-outputPath <outputPath>   		-outputStrategy <outputStrategy>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-inputOrderBy <inputOrderBy>   		-inputTableName <inputTableName>   		-inputFieldNames <inputFieldNames>   		-inputLoadTo hive   		-inputLoadToPath <inputLoadToPath>   		-hiveDriver <hiveDriver>    		-hiveUrl <hiveUrl>   		-hiveUsername <hiveUsername>   		-hivePassword <hivePassword>   		-hivePartitionBy <hivePartitionBy>   		-hiveIfNotExists <hiveIfNotExists>   		-hiveTableName <hiveTableName>   		-hiveSortedBy <hiveSortedBy>   		-hiveClusteredBy <hiveClusteredBy>      For query based    	bin/hadoop jar ~/workspace/hiho/build/classes/hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf  ~/workspace/hiho/conf/dbInputQueryDelimitedHive.xml  or    	${HIHO_HOME}/scripts/hiho import   		-jdbcDriver <jdbcDriver>   		-jdbcUrl <jdbcUrl>   		-jdbcUsername <jdbcUsername>   		-jdbcPassword <jdbcPassword>   		-outputPath <outputPath>   		-outputStrategy <outputStrategy>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-inputOrderBy <inputOrderBy>   		-inputLoadTo hive   		-inputLoadToPath <inputLoadToPath>   		-hiveDriver <hiveDriver>    		-hiveUrl <hiveUrl>   		-hiveUsername <hiveUsername>   		-hivePassword <hivePassword>   		-hivePartitionBy <hivePartitionBy>   		-hiveIfNotExists <hiveIfNotExists>   		-hiveTableName <hiveTableName>   		-hiveSortedBy <hiveSortedBy>   		-hiveClusteredBy <hiveClusteredBy>    **Notes:**      1. Hive table name is mandatory when you are quering more than one query or table that is the case of multiple partition  2. Please note that sorted feature will not work untill clustered feature is defined      ## Dedup details    	bin/hadoop jar ~/workspace/HIHO/deploy/hiho.jar co.nubetech.hiho.dedup.DedupJob -inputFormat <inputFormat> -dedupBy <""key"" or ""value""> -inputKeyClassName <inputKeyClassName> -inputValueClassName <inputValueClassName> -inputPath <inputPath> -outputPath <outputPath> -delimeter <delimeter> -column <column> -outputFormat <outputFormat>    Alternatively Dedup can also be executed as:-  Running HadoopTransform script present in `$HIHO_HOME/scripts/`    	${HIHO_HOME}/scripts/hiho dedup   		-inputFormat <inputFormat>   		-dedupBy <""key"" or ""value"">   		-inputKeyClassName <inputKeyClassName>   		-inputValueClassName <inputValueClassName>   		-inputPath <inputPath>   		-outputPath <outputPath>   		-delimeter <delimeter> -column <column>    **Example For Deduplication with key:**      For Sequence Files:     	${HIHO_HOME}/scripts/hiho dedup   		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat   		-inputKeyClassName org.apache.hadoop.io.IntWritable   		-inputValueClassName org.apache.hadoop.io.Text   		-inputPath testData/dedup/inputForSeqTest   		-outputPath output -dedupBy key    For Delimited Text Files:     	${HIHO_HOME}/scripts/hiho dedup   		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat   		-inputKeyClassName org.apache.hadoop.io.Text   		-inputValueClassName org.apache.hadoop.io.Text   		-inputPath testData/dedup/textFilesForTest   		-outputPath output -delimeter ,   		-column 1   		-dedupBy key    **Example For Deduplication with value:**      For Sequence Files:     	${HIHO_HOME}/scripts/hiho dedup   		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat   		-inputKeyClassName org.apache.hadoop.io.IntWritable   		-inputValueClassName org.apache.hadoop.io.Text   		-inputPath testData/dedup/inputForSeqTest   		-outputPath output -dedupBy value    For Delimited Text Files:     	${HIHO_HOME}/scripts/hiho dedup   		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat   		-inputKeyClassName org.apache.hadoop.io.Text   		-inputValueClassName org.apache.hadoop.io.Text   		-inputPath testData/dedup/textFilesForTest   		-outputPath output   		-dedupBy value    ## Merge details:     	${HIHO_HOME}/scripts/hiho merge   		-newPath <newPath>   		-oldPath <oldPath>   		-mergeBy <""key"" or ""value"">   		-outputPath <outputPath>   		-inputFormat <inputFormat>   		-inputKeyClassName <inputKeyClassName>   		-inputValueClassName <inputValueClassName>   		-outputFormat <outputFormat>    **Example For Merge with key:**      For Sequence Files:    	${HIHO_HOME}/scripts/hiho merge   		-newPath testData/merge/inputNew/input1.seq   		-oldPath testData/merge/inputOld/input2.seq   		-mergeBy key -outputPath output    		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat   		-inputKeyClassName org.apache.hadoop.io.IntWritable   		-inputValueClassName org.apache.hadoop.io.Text    For Delimited Text Files:    	${HIHO_HOME}/scripts/hiho merge   		-newPath testData/merge/inputNew/fileInNewPath.txt   		-oldPath testData/merge/inputOld/fileInOldPath.txt   		-mergeBy key   		-outputPath output   		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat   		-inputKeyClassName org.apache.hadoop.io.Text   		-inputValueClassName org.apache.hadoop.io.Text    **Example For Merge with value:**      For Sequence Files:    	${HIHO_HOME}/scripts/hiho merge   		-newPath testData/merge/inputNew/input1.seq   		-oldPath testData/merge/inputOld/input2.seq   		-mergeBy value   		-outputPath output    		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat   		-inputKeyClassName org.apache.hadoop.io.IntWritable   		-inputValueClassName org.apache.hadoop.io.Text    For Delimited Text Files:    	${HIHO_HOME}/scripts/hiho merge   		-newPath testData/merge/inputNew/fileInNewPath.txt   		-oldPath testData/merge/inputOld/fileInOldPath.txt   		-mergeBy value   		-outputPath output   		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat   		-inputKeyClassName org.apache.hadoop.io.Text   		-inputValueClassName org.apache.hadoop.io.Text    ## Export to DB:    	bin/hadoop jar deploy/hiho-0.4.0.jar co.nubetech.hiho.job.ExportToDB    		-jdbcDriver <jdbcDriverName>    		-jdbcUrl <jdbcUrl>    		-jdbcUsername <jdbcUserName>    		-jdbcPassword <jdbcPassword>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-tableName <tableName>   		-columnNames <columnNames>   		-inputPath <inputPath>   or    	${HIHO_HOME}/scripts/hiho export db   		-jdbcDriver <jdbcDriverName>    		-jdbcUrl <jdbcUrl>    		-jdbcUsername <jdbcUserName>    		-jdbcPassword <jdbcPassword>   		-delimiter <delimiter>   		-numberOfMappers <numberOfMappers>   		-tableName <tableName>   		-columnNames <columnNames>   		-inputPath <inputPath>     ## New Features in this release  - incremental import and introduction of AppendFileInputFormat  - Oracle export  - FTP Server integration  - Salesforce  - Support for Apache Hadoop 0.20  - Support for Apache Hadoop 0.21  - Generic dedup and merge    ## Other improvements  - Ivy based build and dependency management  - Junit and mockito based test cases     **Note:**     1. To run `TestExportToMySQLDB` we need to add `hiho-0.4.0.jar`, all jars of hadoop and hadoop lib with also `mysql-connector-java.jar`  		in the `classpath`. """
Big data;https://github.com/jnv/lists;"""# Lists    List of useful, silly and [awesome](#awesome-) lists curated on GitHub. Contributions welcome!    ✨ Now also available [in CSV](https://github.com/jnv/lists/blob/gh-pages/lists.csv)! ✨    - [Lists](#lists)    - [Non-technical](#non-technical)    - [Technical](#technical)      - [awesome-*](#awesome-)    - [Lists of lists](#lists-of-lists)      - [Lists of lists of lists](#lists-of-lists-of-lists)        - [Lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists)          - [Lists of lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists-of-lists)            - [Lists of lists of lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists-of-lists-of-lists)            - [Lists of lists of lists of lists of lists of lists of lists](#lists-of-lists-of-lists-of-lists-of-lists-of-lists-of-lists)    - [License](#license)    <!-- lists-start -->    ## Non-technical    * [aksh](https://github.com/svaksha/aksh) – Bibliography of STEM (Science, Technology, Engineering & Mathematics) resources and grey literature.  * [amas](https://github.com/sindresorhus/amas) – Awesome & Marvelous Amas (Ask Me Anything) on GitHub  * [Annual-Reading-List](https://github.com/davidskeck/Annual-Reading-List) – Things to read every year.  * [awesomebandnames](https://github.com/jnv/awesomebandnames) – The open-source list of awesome band names.  * [awesome-belarus-online](https://github.com/Friz-zy/awesome-belarus-online) – Useful belarusian online resources.  * [awesome-biology](https://github.com/raivivek/awesome-biology) – Learning resources, research papers, tools and other resources related to Biology.  * [awesome-bitclout](https://github.com/barrymode/awesome-bitclout) – BitClout is social media on a blockchain. Everyone gets their own coin.  * [awesome-board-games](https://github.com/edm00se/awesome-board-games) – Awesome and exceptional board games    - https://awesomeboard.games  * [awesome-ethics](https://github.com/HussainAther/awesome-ethics)  * [awesome-fantasy](https://github.com/RichardLitt/awesome-fantasy) – Fantasy literature worth reading.  * [awesome-gif](https://github.com/Kikobeats/awesome-gif) – GIF /dʒ/ links and resources.  * [awesome-glasgow](https://github.com/allyjweir/awesome-glasgow) – Some highlights around Glasgow, Scotland.  * [awesome-hacking-locations](https://github.com/daviddias/awesome-hacking-locations) – Hacking places, organised by Country and City, listing if it features power and wifi.  * [awesome-health](https://github.com/prabhic/awesome-health) – Useful health resources.  * [awesome-images](https://github.com/heyalexej/awesome-images) – Free (stock) photo resources for your projects.  * [awesome-kimchi](https://github.com/jeyraof/awesome-kimchi) – Kimchi of the people, by the people, for the people.  * [awesome-lego](https://github.com/ad-si/awesome-lego)  * [awesome-lockpicking](https://github.com/fabacab/awesome-lockpicking) – Guides, tools, and other resources related to the security and compromise of locks, safes, and keys.  * [awesome-maps](https://github.com/simsieg/awesome-maps) – Various Online Maps  * [awesome-mental-health](https://github.com/dreamingechoes/awesome-mental-health) – Articles, websites and resources about mental health in the software industry.    - https://dreamingechoes.github.io/awesome-mental-health  * [awesome-parasite](https://github.com/ecohealthalliance/awesome-parasite) – Parasites and host-pathogen interactions.  * [awesome-philosophy](https://github.com/HussainAther/awesome-philosophy) – Philosophy  * [awesome-reddit-channels](https://github.com/MadhuNimmo/awesome-reddit-channels) – Reddit Channels every programmer must follow.  * [awesome-scifi](https://github.com/sindresorhus/awesome-scifi) – Sci-Fi worth consuming.  * [awesome-speaking](https://github.com/matteofigus/awesome-speaking) – Resources about public speaking  * [awesome-stock-resources](https://github.com/neutraltone/awesome-stock-resources) – Stock photography, video and illustration websites.  * [awesome-theravada](https://github.com/johnjago/awesome-theravada) – Theravada Buddhist teachings  * [awesome-uncopyright](https://github.com/johnjago/awesome-uncopyright) – All things public domain  * [awesome-webcomics](https://github.com/dhamaniasad/awesome-webcomics)  * [baby-sleep](https://github.com/simple10/baby-sleep) – Baby sleep guides curated from the best of the Internet.  * [bailfunds.github.io](https://github.com/bailfunds/bailfunds.github.io) – Bail Funds for Protestors across the USA.    - https://bailfunds.github.io/  * [boardgames](https://gitlab.com/gamearians/boardgames) – Boardgames and boardgame-related projects that can be found on GitHub.  * [chinese-poetry](https://github.com/chinese-poetry/chinese-poetry) _In Chinese_ – The most comprehensive database of Chinese poetry    - http://shici.store  * [cocktails](https://github.com/balevine/cocktails) – Cocktail Recipes  * [corporate-logos](https://github.com/marketreef/corporate-logos) – Curated repo of publicly listed co. logos, identified by ticker. *Almost 1500 logos*  * [creative-commons-media](https://github.com/shime/creative-commons-media) – Audio, graphics and other resources that provide media licensed under Creative Commons licenses.  * [discord-listings](https://github.com/angrymouse/discord-listings) – Places where to promote Discord servers.  * [dissertation-tips](https://github.com/katychuang/dissertation-tips) – Resources to help PhD students complete their dissertation successfully.  * [diversity-index](https://github.com/svaksha/diversity-index) – Grants, scholarships and FA that encourages diversity in STEM fields aimed at half the world's population, Women!    - http://svaksha.github.io/diversity-index  * [diversity-twitter](https://github.com/gregorycoleman/diversity-twitter) – Twitter feeds of interesting people to follow for Diversity & Inclusion  * [food](https://notabug.org/themusicgod1/food)  * [food-recipes](https://github.com/obfuscurity/food-recipes) – Honest-to-goodness ""real food"" recipes  * [frequent-transit-maps](https://github.com/wwcline/list-of-frequent-transit-maps) – Transit maps highlighting frequent all-day service  * [global-reports](https://github.com/andressoop/global-reports) – Major global reports published by international organisations  * [guitarspecs](https://github.com/gitfrage/guitarspecs) – Electric guitar's parts specs    - https://gitfrage.github.io/guitarspecs/  * [isaacs/reading-list](https://github.com/isaacs/reading-list) – [isaac](https://github.com/isaacs)'s reading list.  * [lawrence-veggie](https://github.com/codysoyland/lawrence-veggie) – Vegetarian/vegan restaurants in Lawrence, KS.  * [lawyersongithub](https://github.com/dpp/lawyersongithub) – A club full of lawyers who also have GitHub accounts.  * [low-resource-languages](https://github.com/RichardLitt/low-resource-languages) – Conservation, development, and documentation of endangered, minority, and low or under-resourced human languages.  * [Mind-Expanding-Books](https://github.com/hackerkid/Mind-Expanding-Books) – :books: Books that will blow your mind    - http://books.vishnuks.com  * [mining-resources](https://github.com/Mining-Resources/mining-resources) – Natural resources mining.  * [no-free-basics](https://github.com/net-neutrality/no-free-basics) – Those who have spoken up against Facebook's “Free Basics”    - https://net-neutrality.github.io/no-free-basics/  * [open-sustainable-technology](https://github.com/protontypes/open-sustainable-technology) – Worldwide open technology projects preserving a stable climate, energy supply and vital natural resources.  * [plastic-free](https://github.com/IrosTheBeggar/plastic-free) – Plastic-free products.  * [ProjectSoundtracks](https://github.com/sarthology/ProjectSoundtracks) – Soundtracks to boost your Productivity and Focus.  * [PublicMedia](https://github.com/melodykramer/PublicMedia) – Everything about public (broadcast) media.    - Also [an introduction to working with GitHub](https://melodykramer.github.io/2015/04/06/learning-github-without-one-line-of-code) for non-programmers.  * [recipes](https://github.com/bzimmerman/recipes) by @bzimmerman – This repository contains tasty open-source recipes.  * [recipes](https://github.com/csswizardry/recipes) by @csswizardy – Collection of things I like cooking  * [recipes](https://github.com/LarryMad/recipes) by @LarryMad  * [recipes](https://github.com/nofunsir/recipes) by @nofunsir  * [recipes](https://github.com/schacon/recipes) by @schacon  * [recipes](https://github.com/silizuo/recipes) _In Chinese and English_ by @silizuo  * [sf-vegetarian-restaurants](https://github.com/mojombo/sf-vegetarian-restaurants) – Awesome vegetarian-friendly restaurants in SF  * [shelfies](https://github.com/kyro/shelfies) – Bookshelves of awesome people, community-transcribed.  * [SiliconValleyThingsToDo](https://github.com/cjbarber/SiliconValleyThingsToDo) – Things to do and activities within Silicon Valley.  * [stayinghomeclub](https://github.com/phildini/stayinghomeclub) – All the companies working from home or events changed because of covid-19.    - https://stayinghome.club  * [Sustainable-Earth](https://github.com/bizz84/Sustainable-Earth) – All things sustainable  * [tacofancy](https://github.com/sinker/tacofancy) – community-driven taco repo. stars stars stars.  * [teesites](https://github.com/elder-cb/teesites) – Great sites to buy awesome t-shirts and other cool stuff.      ## Technical    * [101](https://github.com/ojas/101) – Resources on running a software biz.  * [10PL](https://github.com/nuprl/10PL) – 10 papers that all PhD students in programming languages ought to know, for some value of 10.  * [1on1-questions](https://github.com/VGraupera/1on1-questions) – 1 on 1 meeting questions.  * [30-seconds-of-code](https://github.com/30-seconds/30-seconds-of-code) – JavaScript snippets you can understand in 30 seconds or less.    - https://30secondsofcode.org/  * [30-seconds-of-interviews](https://github.com/30-seconds/30-seconds-of-interviews) – Common interview questions to help you prepare for your next interview.  * [a11yproject.com](https://github.com/a11yproject/a11yproject.com) – A community–driven effort to make web accessibility easier.    - https://a11yproject.com  * [addinslist](https://github.com/daattali/addinslist) – Useful [RStudio](https://www.rstudio.com/) addins  * [admesh-projects](https://github.com/admesh/admesh-projects) – Projects using [ADMesh](https://github.com/admesh/admesh) (a triangulated solid meshes processor).  * [AI-reading-list](https://github.com/m0nologuer/AI-reading-list) – Papers about Artificial Intelligence.  * [alexandria](https://github.com/alxgcrz/alexandria) _In English and Spanish_ – Various resources by [@alxgcrz](https://github.com/alxgcrz)  * [algovis](https://github.com/enjalot/algovis) – Algorithm Visualization.  * [alternative-front-ends](https://github.com/mendel5/alternative-front-ends) – Alternative open source front-ends for popular internet platforms (e.g. YouTube, Twitter, etc.).  * [alternative-internet](https://github.com/redecentralize/alternative-internet) – A collection of interesting new networks and tech aiming at decentralisation (in some form).  * [amazing-deployment](https://github.com/delirehberi/amazing-deployment)  * [android-awesome-libraries](https://github.com/kaiinui/android-awesome-libraries) – Useful Android development libraries with usage examples.  * [android-dev-readme](https://github.com/anirudh24seven/android-dev-readme) – Links for every Android developer.  * [AndroidDevTools](https://github.com/inferjay/AndroidDevTools) _In Chinese_ – SDK, development tools, libraries, and resources.    - http://www.androiddevtools.cn/  * [android-jobs](https://github.com/android-cn/android-jobs) _In Chinese_ – Android positions in China.  * [Android-Learning-Resources](https://github.com/zhujun2730/Android-Learning-Resources) _In Chinese_ – Learning resources for Android.  * [android-open-project](https://github.com/Trinea/android-open-project) _In Chinese_ – Collect and classify android open source projects.  * [android-security-awesome](https://github.com/ashishb/android-security-awesome) – “A lot of work is happening in academia and industry on tools to perform dynamic analysis, static analysis and reverse engineering of android apps.”  * [android-tech-frontier](https://github.com/hehonghui/android-tech-frontier) _In Chinese_ – Translation of articles about Android development.  * [angular-education](https://github.com/timjacobi/angular-education) – Helpful material to develop using Angular  * [AngularJS-Learning](https://github.com/jmcunningham/AngularJS-Learning)  * [ansible-gentoo-roles](https://github.com/jirutka/ansible-gentoo-roles) – Ansible roles for Gentoo Linux.  * [apis-list](https://github.com/apis-list/apis-list) – Community maintained, human and machine readable list of Public APIs  * [app-ideas](https://github.com/florinpop17/app-ideas) – Application ideas which can be used to improve your coding skills.  * [app-launch-guide](https://github.com/adamwulf/app-launch-guide) – Indie dev's definitive guide to building and launching your app, including pre-launch, marketing, building, QA, buzz building, and launch.  * [applied-ml](https://github.com/eugeneyan/applied-ml) – Data science & machine learning in production.  * [APTnotes](https://github.com/kbandla/APTnotes) – Various public documents, whitepapers and articles about APT [Advanced persistent threat] campaigns.  * [architect-awesome](https://github.com/xingshaocheng/architect-awesome) _In Chinese_ – 后端架构师技术图谱  * [asynchronous-php](https://github.com/elazar/asynchronous-php) – Asynchronous programming in PHP.  * [Automated-SPA-Testing](https://github.com/webpro/Automated-SPA-Testing) – Automated unit & functional testing for web applications [JavaScript et al.].  * [awful-ai](https://github.com/daviddao/awful-ai) – Current scary usages of AI, hoping to raise awareness to its misuses in society.  * [awmy](https://github.com/potch/awmy) – Are We Meta Yet?    - http://arewemetayet.com/  * [b1fipl](https://github.com/marcpaq/b1fipl) – A Bestiary of Single-File Implementations of Programming Languages.  * [Backpack](https://github.com/sevab/Backpack) – Various learning resources, organized by technology/topic.  * [badass-dev-resources](https://github.com/sodevious/badass-dev-resources) – #bada55 front-end developer resources.  * [Badges4-README.md-Profile](https://github.com/alexandresanlim/Badges4-README.md-Profile) – Badges for GitHub profiles.  * [bangalore-startups](https://github.com/hemanth/bangalore-startups) – Startups in Bangalore.  * [beautiful-docs](https://github.com/PharkMillups/beautiful-docs) – Pointers to useful, well-written, and otherwise beautiful documentation.  * [BEM-resources](https://github.com/sturobson/BEM-resources)  * [Best-App](https://github.com/hzlzh/Best-App) _In Chinese_ – Recommendations for best desktop and mobile apps.  * [best-of-awesomeness-and-usefulness-for-webdev](https://github.com/Pestov/best-of-awesomeness-and-usefulness-for-webdev) – Digest of the most useful tools and resources for the last year.    - [Russian version](https://github.com/Pestov/best-of-awesomeness-and-usefulness-for-webdev/tree/master/ru)  * [best-practices-checklist](https://github.com/palash25/best-practices-checklist) – Language-specific resources to look up the best practices followed by that particular language's community.  * [Best-websites-a-programmer-should-visit](https://github.com/sdmg15/Best-websites-a-programmer-should-visit) – Some useful websites for programmers.  * [Best-websites-a-programmer-should-visit-zh](https://github.com/tuteng/Best-websites-a-programmer-should-visit-zh) _In Chinese_ – 程序员应该访问的最佳网站中文版  * [bigdata-ecosystem](https://github.com/zenkay/bigdata-ecosystem) – Big-data related projects packed into a JSON dataset.    - http://bigdata.andreamostosi.name/  * [Big-List-of-ActivityPub](https://github.com/shleeable/Big-List-of-ActivityPub) – ActivityPub Projects  * [big-list-of-naughty-strings](https://github.com/minimaxir/big-list-of-naughty-strings) – Strings which have a high probability of causing issues when used as user-input data.  * [bioinformatics-compbio-tools](https://github.com/lancelafontaine/bioinformatics-compbio-tools) – Bioinformatics and computational biology tools.  * [bitcoin-reading-list](https://github.com/jashmenn/bitcoin-reading-list) – Learn to program Bitcoin transactions.  * [BNN-ANN-papers](https://github.com/takyamamoto/BNN-ANN-papers) – Papers about Biological and Artificial Neural Networks related to (Computational) Neuroscience  * [bookmarklets](https://github.com/RadLikeWhoa/bookmarklets) – Bookmarklets that are useful on the web    - https://sacha.me/bookmarklets/  * [bookshelf](https://github.com/OpenTechSchool/bookshelf) – Reading lists for learners.  * [bots](https://github.com/hackerkid/bots) – Tools for building bots  * [breakfast-repo](https://github.com/ashleygwilliams/breakfast-repo) – Videos, recordings, and podcasts to accompany our morning coffee.  * [browser-resources](https://github.com/azu/browser-resources) – Latest JavaScript information by browser.  * [build-your-own-x](https://github.com/danistefanovic/build-your-own-x) – Build your own (insert technology here)  * [channels](https://github.com/andrew--r/channels) _In Russian_ – YouTube channels for web developers.  * [citizen-science](https://github.com/dylanrees/citizen-science) – Scientific tools to empower communities and/or practice various forms of non-institutional science  * [classics](https://github.com/eyy/classics) – Classical studies (Latin and Ancient Greek) resources: software, code and raw data.  * [Clone-Wars](https://github.com/GorvGoyl/Clone-Wars) – Open-source clones of popular sites.  * [cloud-conferences](https://github.com/stefan-kolb/cloud-conferences) – A collection of scientific and industry conferences focused on cloud computing.    - http://stefan-kolb.github.io/cloud-conferences/  * [code-canon](https://github.com/darius/code-canon) – Code worth reading.  * [codeface](https://github.com/chrissimpkins/codeface) – Typefaces for source code / text editors.  * [Colorful](https://github.com/Siddharth11/Colorful) – Choose your next color scheme  * [CompilerJobs](https://github.com/mgaudet/CompilerJobs) – Compiler, language, and runtime teams for people looking for jobs in this area.  * [compilers-targeting-c](https://github.com/dbohdan/compilers-targeting-c) – Compilers that can generate C code.  * [computer-science](https://github.com/ossu/computer-science) – Path to a free self-taught graduation in Computer Science.  * [content-management-systems](https://github.com/ahadb/content-management-systems) – Open source & proprietary content management systems.  * [critical-path-css-tools](https://github.com/addyosmani/critical-path-css-tools) – Tools to help prioritize above-the-fold CSS.  * [CryptoList](https://github.com/coinpride/CryptoList) – Blockchain & cryptocurrency resources.  * [crypto-might-not-suck](https://github.com/sweis/crypto-might-not-suck) – Crypto Projects that Might not Suck.  * [cscs](https://github.com/SalGnt/cscs) – Coding Style Conventions and Standards.  * [css-in-js](https://github.com/MicheleBertoli/css-in-js) – CSS in JS techniques comparison for React et al.  * [css-protips](https://github.com/AllThingsSmitty/css-protips) – Take your CSS skills pro  * [cto](https://github.com/92bondstreet/cto) – Chief Technology Officers resources.  * [curated-list-espresso-sugar-plugins](https://github.com/GioSensation/curated-list-espresso-sugar-plugins) – Sugar plugins for Espresso, the code editor by MacRabbit.  * [curated-programming-resources](https://github.com/Michael0x2a/curated-programming-resources) – Resources for learning programming and computer science.  * [curatedseotools](https://github.com/sneg55/curatedseotools) – Best SEO Tools Stash    - https://curatedseotools.com  * [cycle-ecosystem](https://github.com/Widdershin/cycle-ecosystem) – What are the most popular and trending libraries for [Cycle.js](http://cycle.js.org/)?  * [dad-jokes](https://github.com/wesbos/dad-jokes) – Dad style programming jokes.  * [datajournalists-toolbox](https://github.com/basilesimon/datajournalists-toolbox) – Tools for datajournalists, with examples and gists.  * [datascience](https://github.com/r0f1/datascience) – Python resources for data science.  * [data-science-blogs](https://github.com/rushter/data-science-blogs)  * [data-science-must-watch](https://github.com/kmonsoor/data-must-watch)  * [datasciencemasters](https://github.com/datasciencemasters/go) – The Curriculum for learning Data Science, Open Source and at your fingertips.    - http://datasciencemasters.org/  * [datascience-pizza](https://github.com/PizzaDeDados/datascience-pizza) _In Portugese_ – Materiais de estudo em análise de dados e áreas afins, empresas que trabalham com dados e dicionário de conceitos.  * [DataSciencePython](https://github.com/ujjwalkarn/DataSciencePython) – Python tutorials for Data Science, NLP and Machine Learning  * [debugging-stories](https://github.com/danluu/debugging-stories) – Collection of links to various debugging stories.  * [Deep-NLP-Resources](https://github.com/pawangeek/Deep-NLP-Resources) – Deep Natural Language Processing  * [degoogle](https://github.com/tycrek/degoogle) – Alternatives to Google's products.  * [Developer-Conferences](https://github.com/MurtzaM/Developer-Conferences) – Upcoming developer conferences.  * [dev-movies](https://github.com/aryaminus/dev-movies) – Recommended movies for people working in the Software and IT Industry.  * [devopsbookmarks.com](https://github.com/devopsbookmarks/devopsbookmarks.com) – To discover tools in the devops landscape.    - http://www.devopsbookmarks.com/  * [devops_resources](https://github.com/dustinmm80/devops_resources)  * [DevopsWiki](https://github.com/Leo-G/DevopsWiki) – Devops Tools, Tutorials and Scripts.  * [dev-resource](https://github.com/Ibrahim-Islam/dev-resource) – Resources for devs online and offline.  * [digital-gardeners](https://github.com/MaggieAppleton/digital-gardeners) – Resources for gardeners tending their digital notes on the public interwebs.  * [discord-resources](https://github.com/DTinker/discord-resources) – Discord modding resources.  * [discount-for-student-dev](https://github.com/AchoArnold/discount-for-student-dev) – Discounts on software (SaaS, PaaS, IaaS, etc.) and other offerings for developers who are students  * [dive-into-machine-learning](https://github.com/hangtwenty/dive-into-machine-learning) – Dive into Machine Learning with Python Jupyter notebook and scikit-learn    - http://hangtwenty.github.io/dive-into-machine-learning/  * [django-must-watch](https://gitlab.com/rosarior/django-must-watch) – Must-watch videos bout Django web framework + Python.  * [DL4NLP](https://github.com/andrewt3000/DL4NLP) – Deep Learning for Natural Language Processing resources.  * [dumb-password-rules](https://github.com/dumb-password-rules/dumb-password-rules) – Shaming sites with dumb password rules.  * [easy-application](https://github.com/j-delaney/easy-application) – Software engineering companies that are easy to apply to.  * [effects-bibliography](https://github.com/yallop/effects-bibliography) – A collaborative bibliography of work related to the theory and practice of computational effects  * [ElixirBooks](https://github.com/sger/ElixirBooks) – Elixir programming language books  * [elm-companies](https://github.com/jah2488/elm-companies) – Companies using Elm  * [embedded-scripting-languages](https://github.com/dbohdan/embedded-scripting-languages)  * [ember-links/list](https://github.com/ember-links/list) – Ember.js web framework  * [empathy-in-engineering](https://github.com/KimberlyMunoz/empathy-in-engineering) – Building and promoting more compassionate engineering cultures  * [engineering-blogs](https://github.com/kilimchoi/engineering-blogs)  * [engine.so](https://github.com/pmwkaa/engine.so) – Tracking, Benchmarking and Sharing Information about an open source embedded data storage engines, internals, architectures, data storage and transaction processing.  * [erlang-bookmarks](https://github.com/0xAX/erlang-bookmarks) – All about erlang programming language.  * [erlang-watchlist](https://github.com/gabrielelana/erlang-watchlist) – Where to find good code to master Erlang idioms  * [ES6-Learning](https://github.com/ericdouglas/ES6-Learning) – Resources to learn ECMAScript 6!  * [es6-tools](https://github.com/addyosmani/es6-tools) – An aggregation of tooling for ES6  * [Essential-JavaScript-Links](https://github.com/starandtina/Essential-JavaScript-Links)    - http://starandtina.github.io/Essential-JavaScript-Links/  * [every-programmer-should-know](https://github.com/mtdvio/every-programmer-should-know) – (Mostly) technical things every software developer should know.  * [Facets](https://github.com/O-I/Facets) – One-liners in Ruby  * [fks](https://github.com/JacksonTian/fks) _In Chinese_ – Frontend Knowledge Structure.  * [flat-file-cms](https://github.com/ahadb/flat-file-cms) – Stictly flat-file cms systems.  * [FOSS-for-Dev](https://github.com/tvvocold/FOSS-for-Dev) – Free and open-source software for developers  * [freeCodeCamp](https://github.com/freeCodeCamp/freeCodeCamp) – Open Source, Free Full Stack Training with hours of coding challenges, projects, and certifications.    - https://www.freecodecamp.org/  * [free-for-dev](https://github.com/ripienaar/free-for-dev) – Software, SaaS, PaaS etc offerings that have free tiers for devs.    - https://free-for.dev/  * [free-programming-books](https://github.com/EbookFoundation/free-programming-books)    - http://resrc.io/list/10/list-of-free-programming-books/  * [free-programming-books-zh_CN](https://github.com/justjavac/free-programming-books-zh_CN) _In Chinese_  * [frontdesk](https://github.com/miripiruni/frontdesk) – Useful things for Front End Developers  * [Front-end-Developer-Interview-Questions](https://github.com/h5bp/Front-end-Developer-Interview-Questions) – Helpful front-end related questions you can use to interview potential candidates, test yourself or completely ignore.    - Available in [various translations](https://github.com/darcyclarke/Front-end-Developer-Interview-Questions/tree/master/Translations)  * [Front-end-Web-Development-Interview-Question](https://github.com/paddingme/Front-end-Web-Development-Interview-Question) _In Chinese_  * [Front-End-Web-Development-Resources](https://github.com/RitikPatni/Front-End-Web-Development-Resources)    - https://resources.ritikpatni.me/  * [frontend-challenges](https://github.com/felipefialho/frontend-challenges) – Playful challenges for job applicants to test your knowledge.  * [frontend-dev-bookmarks](https://github.com/dypsilon/frontend-dev-bookmarks) – Frontend development resources I collected over time.  * [frontend-dev-resources](https://github.com/dmytroyarmak/frontend-dev-resources) – Frontend resources [conferences].  * [frontend-developer-resources](https://github.com/mrcodedev/frontend-developer-resources) _In Spanish._ – El camino del Frontend Developer.  * [frontend-development](https://github.com/mojpm/frontend-development)  * [frontend-resources](https://github.com/JonathanZWhite/frontend-resources) by @JonathanZWhite  * [frontend-resources](https://github.com/zedix/frontend-resources) by @zedix  * [frontend-stuff](https://github.com/moklick/frontend-stuff) – Framework/libraries/tools to use when building things on the web. Mostly Javascript stuff.  * [frontend-tools](https://github.com/codylindley/frontend-tools) – Tools for frontend (i.e. html, js, css) desktop/laptop (i.e. does not include tablet or phone yet) web development  * [fsharp-companies](https://github.com/Kavignon/fsharp-companies) – Companies that use F#  * [game-datasets](https://github.com/leomaurodesenv/game-datasets) – Game datasets, tools for artificial intelligence in games  * [Game-Networking-Resources](https://github.com/MFatihMAR/Game-Networking-Resources) – Game Network Programming  * [games](https://github.com/leereilly/games) – Popular/awesome videos games, add-on, maps, etc. hosted on GitHub.  * [generated-awesomeness](https://github.com/orsinium-labs/generated-awesomeness) – Awesome list autogenerated from GitHub API.  * [git-cheat-sheet](https://github.com/arslanbilal/git-cheat-sheet) – git and git flow cheat sheet    - http://bilalarslan.me/git-cheat-sheet/  * [github-cheat-sheet](https://github.com/tiimgreen/github-cheat-sheet) – Cool features of Git and GitHub.  * [github-drama](https://github.com/nikolas/github-drama)  * [github-hall-of-fame](https://github.com/mehulkar/github-hall-of-fame) – Hall of Fame for spectacular things on Github.  * [GoBooks](https://github.com/dariubs/GoBooks) – Golang books.  * [go-is-not-good](https://github.com/ksimka/go-is-not-good) – Articles that complain about Golang's imperfection.  * [go-must-watch](https://github.com/sauravtom/go-must-watch) – Must-watch videos about Golang.  * [go-patterns](https://github.com/tmrts/go-patterns) – Go design patterns, recipes and idioms    - http://tmrts.com/go-patterns  * [graph-adversarial-learning-literature](https://github.com/YingtongDou/graph-adversarial-learning-literature) – Adversarial learning papers on graph-structured data.  * [graphics-resources](https://github.com/mattdesl/graphics-resources) – Game development and realtime graphics programming.  * [guides](https://github.com/NARKOZ/guides) by @NARKOZ – Design and development guides  * [guides](https://github.com/taniarascia/guides) by @taniarascia – Web Development Guides, Tutorials and Snippets.  * [Hackathon-Resources](https://github.com/xasos/Hackathon-Resources) by @xasos – Hackathon Resources for organizers.  * [hack-chat/3rd-party-software-list](https://github.com/hack-chat/3rd-party-software-list) – Bots, clients, and other software people have made for [hack.chat](https://hack.chat).  * [hacker-laws](https://github.com/dwmkerr/hacker-laws) – Laws, Theories, Principles and Patterns that developers will find useful.  * [hacktoberfest-swag](https://github.com/benbarth/hacktoberfest-swag) – Looking for [Hacktoberfest](https://hacktoberfest.digitalocean.com/) swag? You've come to the right place.  * [hacktoberfest-swag-list](https://github.com/crweiner/hacktoberfest-swag-list) – Companies giving out swag for participation in [Hacktoberfest](https://hacktoberfest.digitalocean.com/).    - https://hacktoberfestswaglist.com  * [HarmonyOS](https://github.com/Awesome-HarmonyOS/HarmonyOS) – [HarmonyOS](https://www.harmonyos.com/en/) by Huawei  * [haskell-companies](https://github.com/erkmos/haskell-companies) – Companies using Haskel  * [haskell-must-watch](https://github.com/hzlmn/haskell-must-watch)  * [HeadlessBrowsers](https://github.com/dhamaniasad/HeadlessBrowsers)  * [hipchat-alternatives](https://github.com/cjbarber/hipchat-alternatives)  * [hiring-without-whiteboards](https://github.com/poteto/hiring-without-whiteboards) – Companies that don't have a broken hiring process.  * [htaccess](https://github.com/phanan/htaccess) – Useful .htaccess snippets.  * [hyperawesome](https://github.com/jorgebucaran/hyperawesome) – Hyperapp JavaScript framework  * [idaplugins-list](https://github.com/onethawt/idaplugins-list) – Plugins for [IDA disassembler](https://www.hex-rays.com/products/ida/).  * [ideas](https://github.com/samsquire/ideas) – One Hundred Ideas for Computing  * [InfoSec-Black-Friday](https://github.com/0x90n/InfoSec-Black-Friday) – Deals for InfoSec related software/tools this Black Friday  * [Inspire](https://github.com/NoahBuscher/Inspire) – Links to assist you in web design and development  * [interviews](https://github.com/kdn251/interviews) – Your personal guide to Software Engineering technical interviews.  * [InterviewThis](https://github.com/Twipped/InterviewThis) – Developer questions to ask prospective employers  * [ios-awesome-libraries](https://github.com/kaiinui/ios-awesome-libraries) – Useful iOS development libraries with usage examples.  * [iOS-Developer-and-Designer-Interview-Questions](https://github.com/9magnets/iOS-Developer-and-Designer-Interview-Questions)  * [iOSDevResource](https://github.com/objcc/iOSDevResource)  * [iptv](https://github.com/iptv-org/iptv) – 5000+ publicly available IPTV channels from all over the world.  * [javacard-curated-list](https://github.com/EnigmaBridge/javacard-curated-list) – Java Card applets and related applications for cryptographic smartcards.  * [javascript-dev-bookmarks](https://github.com/didicodes/javascript-dev-bookmarks) – Articles that will help you get better at JavaScript.  * [javascript-patterns](https://github.com/shichuan/javascript-patterns) – JavaScript Patterns    - http://shichuan.github.io/javascript-patterns/  * [javascript-resources](https://github.com/ztsu/javascript-resources)  * [javascript-sdk-design](https://github.com/hueitan/javascript-sdk-design)  * [jquery-tips-everyone-should-know](https://github.com/AllThingsSmitty/jquery-tips-everyone-should-know)  * [jsemu](https://github.com/fcambus/jsemu) – Emulators written in JavaScript.  * [jshomes/learning-resources](https://github.com/jshomes/learning-resources) – Web Platform/SaaS Learning Resources.  * [jslibs](https://github.com/esamattis/jslibs) – My picks of promising/useful Javascript libraries.    - *See also [JSwiki](http://jswiki.org/)*  * [js-must-watch](https://github.com/bolshchikov/js-must-watch) – Must-watch videos about javascript.  * [jsonauts](https://github.com/jsonauts/jsonauts.github.com) – The ultimate reference for JSON tooling and specs.    - http://jsonauts.github.io/  * [jstips](https://github.com/loverajoel/jstips) – JavaScript tips    - http://jstips.co  * [jstools](https://github.com/codefellows/jstools) – Foundational JavaScript Tools  * [js-type-master](https://github.com/yumyo/js-type-master) – JavaScript resources about web typography.    - https://www.codefellows.org/blog/a-list-of-foundational-javascript-tools  * [Julia.jl](https://github.com/svaksha/Julia.jl) – Curated decibans of Julia language.    - https://github.com/svaksha/Julia.jl  * [killer-talks](https://github.com/PharkMillups/killer-talks) – Talks that are worth watching.  * [kubernetes-failure-stories](https://github.com/hjacobs/kubernetes-failure-stories) – Public failure/horror stories related to Kubernetes    - https://k8s.af  * [langs-in-rust](https://github.com/alilleybrinker/langs-in-rust) – Programming languages implemented in Rust.  * [language-list](https://github.com/thomasfoster96/language-list) – Programming languages being developed on GitHub.  * [Laravel-Resources](https://github.com/abhimanyu003/Laravel-Resources) – Laravel Framework Resources and Blogs.  * [learn-drupal](https://github.com/rocketeerbkw/learn-drupal) – Stuff to help you learn Drupal.  * [learn-for-free](https://github.com/aviaryan/learn-for-free) – Free learning resources for all topics you can think of.  * [learnhaskell](https://github.com/bitemyapp/learnhaskell) – A curated guide for learning Haskell.  * [learning-code-through-github-repos](https://github.com/muchirijane/learning-code-through-github-repos) – Github repositories that you can use in your coding journey.  * [learn-python](https://github.com/adrianmoisey/learn-python) by @adrianmoisey – Links that teach Python.  * [learn-python](https://github.com/trekhleb/learn-python) by @trekhleb – Python scripts that are split by topics and contain code examples with explanations.  * [learn-to-program](https://github.com/karlhorky/learn-to-program) – Foundation in Web Development.  * [learn-tt](https://github.com/jozefg/learn-tt) – Resources for learning type theory.  * [learnxinyminutes-docs](https://github.com/adambard/learnxinyminutes-docs) – Code documentation written as code!    - https://learnxinyminutes.com/  * [libertr](https://github.com/gaapt/libertr) – Resources for liberty seekers.  * [lifeofjs](https://github.com/abhijeetkpawar/lifeofjs) – Curated source for all types of awesome resources available for JavaScript.  * [Linux_websites](https://github.com/hduffddybz/Linux_websites) _In Chinese_ – Websites related to Linux kernel development.  * [lua-languages](https://github.com/hengestone/lua-languages) – Languages that compile to Lua.  * [machine-learning-algorithms](https://github.com/Sahith02/machine-learning-algorithms) – Conceptual understanding of all machine learning algorithms.  * [Machine-Learning-Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials) – Machine Learning and Deep Learning Tutorials  * [machine-learning-with-ruby](https://github.com/arbox/machine-learning-with-ruby) – Machine learning in Ruby  * [macos-apps](https://github.com/learn-anything/macos-apps)  * [magictools](https://github.com/ellisonleao/magictools) – Game Development resources to make magic happen.  * [maintenance-modules](https://github.com/maxogden/maintenance-modules) – NPM / Node.js modules useful for maintaining or developing modules  * [manong](https://github.com/nemoTyrant/manong) _In Chinese_ – Weekly digest of technology  * [markdown-resources](https://github.com/rhythmus/markdown-resources) – Markdown resources: apps, dialects, parsers, people, …  * [Marketing-for-Engineers](https://github.com/goabstract/Marketing-for-Engineers) – Marketing articles & tools to grow your product.  * [mind-bicycles](https://github.com/pel-daniel/mind-bicycles) – Future of programming projects  * [motion-ui-design](https://github.com/fliptheweb/motion-ui-design) – Motion UI design, animations and transitions.  * [movies-for-hackers](https://github.com/k4m4/movies-for-hackers)    - https://hackermovie.club/  * [must-watch-css](https://github.com/AllThingsSmitty/must-watch-css) – Must-watch videos about CSS.  * [must-watch-javascript](https://github.com/AllThingsSmitty/must-watch-javascript) – Must-watch videos about JavaScript.  * [my_tech_resources](https://github.com/JamesLavin/my_tech_resources) by @JamesLavin  * [nashville-lispers/resources](https://github.com/nashville-lispers/resources) – Lisp Resources: exercises, great books, videos, etc.  * [net-libraries-that-make-your-life-easier](https://github.com/tallesl/net-libraries-that-make-your-life-easier) – Open Source .NET libraries that make your life easier.  * [neural-network-papers](https://github.com/robertsdionne/neural-network-papers)  * [nginx-resources](https://github.com/fcambus/nginx-resources) – Nginx web server (+ Lua), OpenResty and Tengine.  * [nlp_thai_resources](https://github.com/kobkrit/nlp_thai_resources) – Natural Language Processing for Thai  * [nlp-with-ruby](https://github.com/arbox/nlp-with-ruby) – Practical Natural Language Processing done in Ruby    - http://rubynlp.org  * [node-daily](https://github.com/dailyNode/node-daily) _In Chinese_ – Daily article about Node.js.  * [node-frameworks](https://github.com/pillarjs/node-frameworks) – Comparison of server-side Node frameworks.  * [nodejs-conference-cfps](https://github.com/rosskukulinski/nodejs-conference-cfps) – NodeJS and Javascript Conference Call for Presentations.  * [NodeJS-Learning](https://github.com/sergtitov/NodeJS-Learning) – Resources to help you learn Node.js and keep up to date.  * [NotesIndex](https://github.com/Wilbeibi/NotesIndex)  * [not-yet-awesome-rust](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust) – Rust code and resources that do NOT exist yet, but would be beneficial to the Rust community.  * [offline-first](https://github.com/pazguille/offline-first) – Everything you need to know to create offline-first web apps.  * [open-source-android-apps](https://github.com/pcqpcq/open-source-android-apps) – Collection of Android Apps which are open source.  * [open-source-ios-apps](https://github.com/dkhamsing/open-source-ios-apps) – Open-source iOS apps.  * [open-source-mac-os-apps](https://github.com/serhii-londar/open-source-mac-os-apps) – macOS open source applications.  * [open-source-meetup-alternatives](https://github.com/coderbyheart/open-source-meetup-alternatives)  * [opensource-discordbots](https://github.com/gillesheinesch/opensource-discordbots) – Open-source bots for Discord.  * [ops-books](https://github.com/stack72/ops-books) – Book recommendations related to Continuous Delivery, DevOps, Operations and Systems Thinking.  * [osx-and-ios-security-awesome](https://github.com/ashishb/osx-and-ios-security-awesome) – OSX and iOS related security tools  * [papers](https://github.com/NicolasT/papers) – A collection of papers found across the web.  * [papers-we-love](https://github.com/papers-we-love/papers-we-love) – Papers from the computer science community to read and discuss. (Contains actual papers)  * [ParseAlternatives](https://github.com/relatedcode/ParseAlternatives) – Alternative backend service providers ala [Parse](http://parse.com/).  * [pattern_classification](https://github.com/rasbt/pattern_classification) – A collection of tutorials and examples for solving and understanding machine learning and pattern classification tasks.  * [PayloadsAllTheThings](https://github.com/swisskyrepo/PayloadsAllTheThings) – Useful payloads and bypasses for Web Application Security and Pentest/CTF  * [personal-security-checklist](https://github.com/Lissy93/personal-security-checklist) – 100+ tips for protecting digital security and privacy  * [php-must-watch](https://github.com/phptodayorg/php-must-watch) – Must-watch videos about PHP.  * [phpvietnam/bookmarks](https://github.com/phpvietnam/bookmarks) – PHP resources for Vietnamese.  * [PlacesToPostYourStartup](https://github.com/mmccaff/PlacesToPostYourStartup) – “Where can I post my startup to get beta users?”  * [planetruby/calendar](https://github.com/planetruby/calendar) – Ruby events (meetups, conferences, camps, etc.) from around the world.    - https://planetruby.github.io/calendar/  * [post-mortems](https://github.com/danluu/post-mortems)  * [programmers-proverbs](https://github.com/AntJanus/programmers-proverbs) – Proverbs from the programmer  * [programming-talks](https://github.com/hellerve/programming-talks) – Awesome & Interesting Talks concerning Programming  * [progressive-enhancement-resources](https://github.com/jbmoelker/progressive-enhancement-resources) – (code) examples.  * [project-based-learning](https://github.com/tuvtran/project-based-learning) – Programming tutorials to build an application from scratch.  * [Projects](https://github.com/karan/Projects) – Practical projects that anyone can solve in any programming language.  * [public-apis](https://github.com/public-apis/public-apis) – JSON APIs for use in web development.  * [purescript-companies](https://github.com/ajnsit/purescript-companies) – Companies that use Purescript  * [pycrumbs](https://github.com/kirang89/pycrumbs) – Bits and Bytes of Python from the Internet.  * [py-must-watch](https://github.com/s16h/py-must-watch) by @s16h – Must-watch videos about Python.  * [python-github-projects](https://github.com/checkcheckzz/python-github-projects) – Collect and classify python projects on Github.    - http://itgeekworkhard.com/python-github-projects/  * [pythonidae](https://github.com/svaksha/pythonidae) – Curated decibans of Python scientific programming resources.    - http://svaksha.github.io/pythonidae/  * [python-must-watch](https://github.com/primalpop/python-must-watch) by @primalpop – Must-watch videos about Python.  * [python_reference](https://github.com/rasbt/python_reference) – Useful functions, tutorials, and other Python-related things.  * [Qix](https://github.com/ty4z2008/Qix) _In Chinese_ – Node, Golang, Machine Learning, PostgreSQL.  * [queues.io](https://github.com/lukaszx0/queues.io) – Job queues, message queues and other queues.    - http://queues.io/  * [quick-look-plugins](https://github.com/sindresorhus/quick-look-plugins) – macOS Quick Look plugins for developers  * [rails-must-watch](https://github.com/gerricchaplin/rails-must-watch) – Must-watch videos about Ruby on Rails.  * [rbooks](https://github.com/RomanTsegelskyi/rbooks) – R programming language books  * [remote-in-japan](https://github.com/remote-jp/remote-in-japan) – Tech companies in Japan that hire remote workers.  * [remote-jobs](https://github.com/remoteintech/remote-jobs) – Semi to fully remote-friendly companies in tech.  * [remote-jobs-brazil](https://github.com/lerrua/remote-jobs-brazil) – Remote-friendly Brazilian companies.  * [resource-list](https://github.com/kyasui/resource-list) – Design & Development Resources.  * [resources](https://github.com/jbranchaud/resources) by @jbranchaud – Free, online resources for various technologies, languages, and tools.  * [Resources](https://github.com/tevko/Resources) by @tevko – Tools for front end devs.  * [Resources-for-Beginner-Bug-Bounty-Hunters](https://github.com/nahamsec/Resources-for-Beginner-Bug-Bounty-Hunters) – Getting started with bug bounties.  * [Resources-for-Writing-Shaders-in-Unity](https://github.com/VoxelBoy/Resources-for-Writing-Shaders-in-Unity)  * [retter](https://github.com/MaciejCzyzewski/retter) – Hash functions, ciphers, tools, libraries, and materials related to cryptography & security.  * [reverse-interview](https://github.com/viraptor/reverse-interview) – Questions to ask the company during your interview  * [Rich-Hickey-fanclub](https://github.com/tallesl/Rich-Hickey-fanclub) – Rich Hickey's works on the internet.  * [rss-readers-list](https://github.com/smithbr/rss-readers-list) – Reader replacements megalist    - http://smithbr.github.io/rss-readers-list  * [rubybib.org](https://github.com/rubybib/rubybib.org) – The Ruby Bibliography    - http://rubybib.org/  * [ruby-bookmarks](https://github.com/dreikanter/ruby-bookmarks) – Ruby and Ruby on Rails bookmarks collection.  * [ruby-dev-bookmarks](https://github.com/saberma/ruby-dev-bookmarks) – Ruby development resources I've collected.  * [ruby-nlp](https://github.com/diasks2/ruby-nlp) – Ruby Natural Language Processing (NLP) libraries, tools and software.  * [rust-lang-resources](https://github.com/dschenkelman/rust-lang-resources) – Links related to the Rust programming language.  * [rxjs-ecosystem](https://github.com/Widdershin/rxjs-ecosystem) – What are the most popular libraries in the RxJS ecosystem?  * [rx-react-flux](https://github.com/christianramsey/rx-react-flux) – RxJS + React/Flux implementations.  * [scalable-css-reading-list](https://github.com/davidtheclark/scalable-css-reading-list) – Collected dispatches from The Quest for Scalable CSS.  * [search-engine-optimization](https://github.com/marcobiedermann/search-engine-optimization) – Checklist / collection of Search Engine Optimization (SEO) tips and technics.  * [SecLists](https://github.com/danielmiessler/SecLists) – Lists used during security assessments: usernames, passwords, URLs, sensitive data patterns, fuzzing payloads, web shells, etc.  * [secure-email](https://github.com/OpenTechFund/secure-email) – Overview of projects working on next-generation secure email.  * [Security_list](https://github.com/zbetcheckin/Security_list)  * [selfhosted-music-overview](https://github.com/basings/selfhosted-music-overview) – Software network services which can be hosted on your own servers.  * [services-engineering](https://github.com/mmcgrana/services-engineering) – A reading list for services engineering, with a focus on cloud infrastructure services.  * [shareable-links](https://github.com/vinkla/shareable-links) – URLs for sharing on social media.  * [shellshocker-pocs](https://github.com/mubix/shellshocker-pocs) – Proof of concepts and potential targets for Shellshock.  * [slack-groups](https://github.com/learn-anything/slack-groups) – Public Slack communities.  * [spark-joy](https://github.com/sw-yx/spark-joy) – Add design flair, user delight, and whimsy to your product.  * [spawnedshelter](https://github.com/unbalancedparentheses/spawnedshelter) – Erlang Spawned Shelter – the best articles, videos and presentations related to Erlang.  * [speech-language-processing](https://github.com/edobashira/speech-language-processing)  * [stack-on-a-budget](https://github.com/255kb/stack-on-a-budget) – Services with great free tiers for developers on a budget  * [startup-must-watch](https://github.com/gerricchaplin/startup-must-watch) – Must-watch videos devoted to Entrepreneurship and Startups.  * [startupreadings](https://github.com/dennybritz/startupreadings) – Reading list for all things startup-related.  * [startup-resources](https://github.com/JonathanZWhite/startup-resources)  * [state-machines](https://github.com/achou11/state-machines)  * [static-analysis](https://github.com/analysis-tools-dev/static-analysis) – Static analysis tools, linters and code quality checkers  * [Static-Site-Generators](https://github.com/pinceladasdaweb/Static-Site-Generators)  * [staticsitegenerators-list](https://github.com/bevry/staticsitegenerators-list)    - https://staticsitegenerators.net/  * [streaming-papers](https://github.com/sorenmacbeth/streaming-papers) – Papers on streaming algorithms.  * [structured-text-tools](https://github.com/dbohdan/structured-text-tools) – Command line tools for manipulating structured text data  * [styleguide-generators](https://github.com/davidhund/styleguide-generators) – Automatic living styleguide generators.  * [sublime](https://github.com/JaredCubilla/sublime) – Some of the best Sublime Text packages, themes, and goodies.  * [sublime-bookmarks](https://github.com/dreikanter/sublime-bookmarks) – Sublime Text essential plugins and resources.  * [svelte/integrations](https://github.com/sveltejs/integrations) – Ways to incorporate [Svelte](https://svelte.dev/) framework into your stack  * [SwiftInFlux](https://github.com/ksm/SwiftInFlux) – An attempt to gather all that is in flux in Swift.  * [tech-weekly](https://github.com/adrianmoisey/tech-weekly) – Weekly technical newsletters.  * [terminals-are-sexy](https://github.com/k4m4/terminals-are-sexy) – Terminal frameworks, plugins & resources for CLI lovers.    - https://terminalsare.sexy/  * [the-book-of-secret-knowledge](https://github.com/trimstray/the-book-of-secret-knowledge) – Inspiring lists, manuals, cheatsheets, blogs, hacks, one-liners, cli/web tools and more.  * [The-Documentation-Compendium](https://github.com/kylelobo/The-Documentation-Compendium) – Templates & tips on writing high-quality documentation  * [think-awesome](https://github.com/thinkjs/think-awesome) – [ThinkJS](https://thinkjs.org/) Node.js framework  * [til](https://github.com/jbranchaud/til) – Today I Learned.  * [tips](https://github.com/git-tips/tips) – Most commonly used git tips and tricks.    - http://git.io/git-tips  * [Toolbox](https://github.com/Dillion/Toolbox) – Open source iOS stuff.  * [tool_lists](https://github.com/johnyf/tool_lists) – Links to tools by theme. *Verification, synthesis, and static analysis.*  * [tools](https://github.com/lvwzhen/tools) – Tools for web.  * [toolsforactivism](https://github.com/drewrwilson/toolsforactivism) – Digital tools for activism  * [tools-list](https://github.com/everestpipkin/tools-list) – Open source, experimental, and tiny tools for building game/website/interactive project.    - https://tinytools.directory/  * [ToolsOfTheTrade](https://github.com/cjbarber/ToolsOfTheTrade) – Tools of The Trade, from Hacker News.  * [top-starred-devs-and-repos-to-follow](https://github.com/StijnMiroslav/top-starred-devs-and-repos-to-follow) – Top-Starred Python GitHub Devs, Orgs, and Repos to Follow (All-Time and Trending).  * [trending-repositories](https://github.com/Semigradsky/trending-repositories) – Repositories that were trending for a day.  * [trip-to-iOS](https://github.com/Aufree/trip-to-iOS) _In Chinese_ – Delightful iOS resources.  * [twofactorauth](https://github.com/2factorauth/twofactorauth) – Sites with two factor auth support which includes SMS, email, phone calls, hardware, and software.    - https://twofactorauth.org/  * [type-findings](https://github.com/charliewilco/type-findings) – Posts about web typography.  * [typography](https://github.com/deanhume/typography) – Web typography    - https://deanhume.github.io/typography/  * [ui-styleguides](https://github.com/kevinwuhoo/ui-styleguides)    - http://kevinformatics.com/ui-styleguides/  * [universities-on-github](https://github.com/filler/universities-on-github) – Universities which have a public organization on GitHub.  * [upcoming-conferences](https://github.com/svenanders/upcoming-conferences) – Upcoming web developer conferences.  * [vertx-awesome](https://github.com/vert-x3/vertx-awesome) – [Vert.x](http://vertx.io/) toolkit  * [vim-galore](https://github.com/mhinz/vim-galore) – All things Vim!  * [visual-programming-codex](https://github.com/ivanreese/visual-programming-codex) – Resources and references for the past and future of visual programming.  * [we-are-twtxt](https://github.com/mdom/we-are-twtxt) – [twtxt](https://twtxt.readthedocs.io/) users and bots  * [web-audio-resources](https://github.com/alemangui/web-audio-resources) – A list of curated resources related to the Web audio API.  * [WebComponents-Polymer-Resources](https://github.com/matthiasn/WebComponents-Polymer-Resources)  * [webcomponents-the-right-way](https://github.com/mateusortiz/webcomponents-the-right-way) – Introduction to Web Components.  * [web-dev-resources](https://github.com/ericandrewlewis/web-dev-resources) – A table of contents for web developer resources across the internet.  * [web-development-resources](https://github.com/MasonONeal/web-development-resources)  * [webdev-jokes](https://github.com/jerstew/webdev-jokes) – Web development jokes.  * [webdevresourcecuration](https://github.com/lwakefield/webdevresourcecuration)  * [weekly](https://github.com/zenany/weekly) _In Chinese_ – Weekly summary of articles and resources.  * [what-next](https://github.com/messa/what-next) _In Czech_ – Co dělat, když se chci naučit programovat ještě víc.  * [Women-Made-It](https://github.com/LisaDziuba/Women-Made-It) – Design & development tools, books, podcasts, and blogs made by women.  * [Worth-Reading-the-Android-technical-articles](https://github.com/zmywly8866/Worth-Reading-the-Android-technical-articles) _In Chinese_  * [You-Dont-Need](https://github.com/you-dont-need/You-Dont-Need) – People choose popular projects, often not because it applies to their problems.      ### awesome-*    * [awesome-2048-and-beyond](https://github.com/cstrap/awesome-2048-and-beyond) – Waste and lose at least 8 hours of your life… then **multiply** it…  * [awesome4girls](https://github.com/cristianoliveira/awesome4girls) – Inclusive events/projects/initiatives for women in the tech area.  * [awesome-a11y](https://github.com/brunopulis/awesome-a11y) – Accesibility tools, articles and resources.  * [awesome-accessibility](https://github.com/GonzagaAccess/awesome-accessibility) – Utilities for accessibility-based web development  * [awesome-acf](https://github.com/navidkashani/awesome-acf) – Add-ons for the Advanced Custom Field plugin for WordPress.  * [awesome-actions](https://github.com/sdras/awesome-actions) – [GitHub Actions](https://github.com/features/actions)  * [awesome-actionscript3](https://github.com/robinrodricks/awesome-actionscript3) – ActionScript 3 and Adobe AIR.  * [awesome-activeadmin](https://github.com/serradura/awesome-activeadmin) – Active Admin resources, extensions, posts and utilities. *For Rails.*  * [awesome-activitypub](https://github.com/BasixKOR/awesome-activitypub) – ActivityPub based projects  * [awesome-ad-free](https://github.com/johnjago/awesome-ad-free) – Ad-free alternatives to popular services on the web  * [awesome-ada](https://github.com/ohenley/awesome-ada) – Ada and SPARK programming language  * [awesome-adafruitio](https://github.com/adafruit/awesome-adafruitio) – [Adafruit IO](https://io.adafruit.com/) Internet of Things platform  * [awesome-advent-of-code](https://github.com/Bogdanp/awesome-advent-of-code) – [Advent of Code](https://adventofcode.com/)  * [awesome-agile](https://github.com/lorabv/awesome-agile) – Agile Software Development.    - https://lorabv.github.io/awesome-agile  * [awesome-agriculture](https://github.com/brycejohnston/awesome-agriculture) – Open source technology for agriculture, farming, and gardening  * [awesome-alfred-workflows](https://github.com/alfred-workflows/awesome-alfred-workflows) – [Alfred](https://www.alfredapp.com/) macOS app workflows  * [awesome-algolia](https://github.com/algolia/awesome-algolia) – [Algolia](https://www.algolia.com/) web search service  * [awesome-algorithms](https://github.com/tayllan/awesome-algorithms) – Places to learn and/or practice algorithms.  * [awesome-algorithms-education](https://github.com/gaerae/awesome-algorithms-education) – Learning and practicing algorithms    - https://gaerae.com/awesome-algorithms  * [awesome-alternatives](https://gitlab.com/linuxcafefederation/awesome-alternatives) – Mostly free and open source alternatives to proprietary software and services.  * [awesome-ama-answers](https://github.com/stoeffel/awesome-ama-answers) – @stoeffel's AMA answers  * [awesome-amazon-alexa](https://github.com/miguelmota/awesome-amazon-alexa) – Resources for the Amazon Alexa platform.  * [awesome-amazon-seller](https://github.com/ScaleLeap/awesome-amazon-seller) – Tools and resources for Amazon sellers.  * [awesome-analytics](https://github.com/onurakpolat/awesome-analytics) – Analytics services, frameworks, software and other tools.  * [awesome-android](https://github.com/Jackgris/awesome-android) _In Spanish._ by @Jackgris  * [awesome-android](https://github.com/JStumpp/awesome-android) by @JStumpp  * [awesome-android](https://github.com/snowdream/awesome-android) _Partially in Chinese_ by @snowdream  * [awesome-android-awesomeness](https://github.com/yongjhih/awesome-android-awesomeness)  * [awesome-android-kotlin-apps](https://github.com/androiddevnotes/awesome-android-kotlin-apps) – Open-source Android apps written in Kotlin with particular tech stack and libraries.  * [awesome-android-learner](https://github.com/MakinGiants/awesome-android-learner) – A “study guide” for mobile development.  * [awesome-android-learning-resources](https://github.com/androiddevnotes/awesome-android-learning-resources)  * [awesome-android-libraries](https://github.com/wasabeef/awesome-android-libraries) – General Android libraries.  * [awesome-android-performance](https://github.com/Juude/awesome-android-performance) – Performance optimization on Android.  * [awesome-android-release-notes](https://github.com/pedronveloso/awesome-android-release-notes) – Keep up-to-date with all the things related with Android software development.  * [awesome-android-tips](https://github.com/jiang111/awesome-android-tips) _In Chinese_  * [awesome-android-ui](https://github.com/wasabeef/awesome-android-ui) – UI/UX libraries for Android.  * [awesome-androidstudio-plugins](https://github.com/jiang111/awesome-androidstudio-plugins) _In Chinese_  * [awesome-angular](https://github.com/hugoleodev/awesome-angular) by @hugoleodev  * [awesome-angular](https://github.com/PatrickJS/awesome-angular) by @PatrickJS  * [awesome-angularjs](https://github.com/gianarb/awesome-angularjs) by @gianarb  * [awesome-animation](https://github.com/Animatious/awesome-animation) – Open-source UI animations by Animatious Group.  * [awesome-ansible](https://github.com/jdauphant/awesome-ansible) – [Ansible](https://www.ansible.com/) configuration management  * [awesome-answers](https://github.com/cyberglot/awesome-answers) – Inspiring and thoughtful answers given at stackoverflow, quora, etc.  * [awesome-ant-design](https://github.com/websemantics/awesome-ant-design) – [Ant Design](https://ant.design/) system  * [awesome-api](https://github.com/Kikobeats/awesome-api) – Design and implement RESTful API's  * [awesome-app-ideas](https://github.com/tastejs/awesome-app-ideas) – Ideas for apps to demonstrate how framework or library approach specific problems.  * [awesome-appium](https://github.com/SrinivasanTarget/awesome-appium) – [Appium](http://appium.io/) test automation frmework  * [awesome-apple](https://github.com/joeljfischer/awesome-apple) – 3rd party libraries and tools for Apple platforms development.  * [awesome-appsec](https://github.com/paragonie/awesome-appsec) – Resources for developers to learn application security.  * [awesome-arabic](https://github.com/OthmanAba/awesome-arabic) – Arabic supporting tools, fonts, and development resources.  * [Awesome-arduino](https://github.com/Lembed/Awesome-arduino) – Arduino hardwares, libraries and softwares with update script  * [awesome-arm-exploitation](https://github.com/HenryHoggard/awesome-arm-exploitation) – ARM processors security and exploitation.  * [awesome-artificial-intelligence](https://github.com/owainlewis/awesome-artificial-intelligence)  * [awesome-asciidoc](https://github.com/bodiam/awesome-asciidoc) – Collection of AsciiDoc tools, guides, tutorials and examples of usage.  * [awesome-asciidoctor](https://github.com/dongwq/awesome-asciidoctor) – Collection of asciidoctor’s intros, examples and usages.  * [awesome-ast](https://github.com/chadbrewbaker/awesome-ast) by @chadbrewbaker – Tools for Abstract Syntax Tree processing.  * [awesome-ast](https://github.com/cowchimp/awesome-ast) by @cowchimp – Abstract Syntax Trees.  * [awesome-asyncio](https://github.com/timofurrer/awesome-asyncio) – [asyncio](https://docs.python.org/3/library/asyncio.html) Python library  * [awesome-asyncio-cn](https://github.com/chenjiandongx/awesome-asyncio-cn) _In Chinese_ – [asyncio](https://docs.python.org/3/library/asyncio.html) Python library    - https://awesome-asyncio-cn.chenjiandongx.com/  * [awesome-atom](https://github.com/mehcode/awesome-atom) – [Atom](https://atom.io/) text editor  * [awesome-audio-visualization](https://github.com/willianjusten/awesome-audio-visualization)  * [awesome-aurelia](https://github.com/aurelia-contrib/awesome-aurelia) – [Aurelia](https://aurelia.io/) JavaScript framework  * [awesome-authentication](https://github.com/gitcommitshow/awesome-authentication)  * [awesome-AutoHotkey](https://github.com/ahkscript/awesome-AutoHotkey) – AutoHotkey libraries, library distributions, scripts, tools and resources.  * [awesome-AutoIt](https://github.com/J2TeaM/awesome-AutoIt) – UDFs, example scripts, tools and useful resources for AutoIt.    - https://j2team.github.io/awesome-AutoIt/  * [awesome-automotive](https://github.com/Marcin214/awesome-automotive) – Automotive engineering.  * [awesome-ava](https://github.com/avajs/awesome-ava) – [AVA](https://github.com/avajs/ava) JavaScript test runner.  * [awesome-avr](https://github.com/fffaraz/awesome-avr)  * [awesome-aws](https://github.com/donnemartin/awesome-aws) – Amazon Web Services (AWS)  * [awesome-backbone](https://github.com/sadcitizen/awesome-backbone) – Resources for [Backbone.js](http://backbonejs.org/)  * [awesome-bash](https://github.com/awesome-lists/awesome-bash)  * [awesome-bci](https://github.com/NeuroTechX/awesome-bci) – Brain-Computer Interface.  * [awesome-beacon](https://github.com/rabschi/awesome-beacon) – Bluetooth beacon (iBeacon, Eddystone)  * [awesome-beancount](https://github.com/wzyboy/awesome-beancount) – [Beancount](http://furius.ca/beancount/), a double-entry bookkeeping with text files.  * [awesome-bem](https://github.com/getbem/awesome-bem) – Tools, sites, articles about BEM (frontend development method).  * [awesome-big-o](https://github.com/okulbilisim/awesome-big-o) – Big O notation  * [awesome-bigdata](https://github.com/onurakpolat/awesome-bigdata) – Big data frameworks, resources and other awesomeness.  * [Awesome-Bioinformatics](https://github.com/danielecook/Awesome-Bioinformatics) – Open-source bioinformatics software and libraries.  * [awesome-bitcoin](https://github.com/igorbarinov/awesome-bitcoin) – Bitcoin services and tools for software developers.  * [awesome-bitcoin-payment-processors](https://github.com/alexk111/awesome-bitcoin-payment-processors) – Bitcoin payment processors and stories from merchants using them.  * [awesome-blazor](https://github.com/AdrienTorris/awesome-blazor) – [Blazor](https://blazor.net/), a .NET web framework using C#/Razor and HTML that runs in the browser with WebAssembly.  * [awesome-blender](https://github.com/agmmnn/awesome-blender) – [Blender](https://www.blender.org/) add-ons, tools, tutorials and 3D resources.  * [awesome-blockchain](https://github.com/0xtokens/awesome-blockchain) by @0xtokens – Blockchain and Crytocurrency Resources  * [awesome-blockchain](https://github.com/coderplex-org/awesome-blockchain) by @coderplex-org – Blockchain, Bitcoin and Ethereum related resources  * [awesome-blockchain](https://github.com/cyberFund/awesome-blockchain) _In Russian_ by @cyberFund – Digest of knowledge about crypto networks (including cryptocurrencies).  * [awesome-blockchain](https://github.com/hitripod/awesome-blockchain) by @hitripod  * [awesome-blockchain](https://github.com/igorbarinov/awesome-blockchain) by @igorbarinov – The bitcoin blockchain services  * [awesome-blockchain](https://github.com/imbaniac/awesome-blockchain) by @imbaniac – Blockchain services and exchanges  * [awesome-blockchain](https://github.com/iNiKe/awesome-blockchain) by @iNiKe – Blockchain, ICO, ₿itcoin, Cryptocurrencies  * [awesome-blockchain](https://github.com/oiwn/awesome-blockchain) by @oiwn – Projects and services based on blockchain technology  * [awesome-blockchain-ai](https://github.com/steven2358/awesome-blockchain-ai) – Blockchain projects for Artificial Intelligence and Machine Learning  * [awesome-blockchains](https://github.com/openblockchains/awesome-blockchains) – Blockchains - open distributed databases w/ crypto hashes incl. git  * [awesome-blockstack](https://github.com/jackzampolin/awesome-blockstack) – [Blockstack](https://blockstack.org/) decentralized computing platform  * [awesome-book-authoring](https://github.com/TalAter/awesome-book-authoring) – Resources for technical book authors  * [awesome-bootstrap](https://github.com/therebelrobot/awesome-bootstrap) – Free Bootstrap themes I think are cool.  * [awesome-bpm](https://github.com/ungerts/awesome-bpm) – Business Process Management (BPM) awesomeness.  * [awesome-broadcasting](https://github.com/ebu/awesome-broadcasting) – Open source resources related to broadcast technologies    - http://ebu.io/opensource  * [awesome-browser-extensions-for-github](https://github.com/stefanbuck/awesome-browser-extensions-for-github) – Browser extensions for GitHub.  * [awesome-browserify](https://github.com/browserify/awesome-browserify) – [Browserify](http://browserify.org/) bundler  * [awesome-btcdev](https://github.com/btcbrdev/awesome-btcdev) – Bitcoin development  * [awesome-bugs](https://github.com/criswell/awesome-bugs) – Funny and interesting bugs  * [awesome-building-blocks-for-web-apps](https://github.com/componently-com/awesome-building-blocks-for-web-apps) – Standalone features (services, components, libraries) to be integrated into web applications.    - https://www.componently.com/  * [awesome-c](https://github.com/aleksandar-todorovic/awesome-c) by @aleksandar-todorovic – Continuing the development of awesome-c on GitHub  * [awesome-c](https://github.com/kozross/awesome-c) by @kozross – C frameworks, libraries, resources etc.    - [mirror](https://notabug.org/koz.ross/awesome-c)  * [awesome-cakephp](https://github.com/FriendsOfCake/awesome-cakephp) – [CakePHP](https://cakephp.org/) web framework  * [awesome-calculators](https://github.com/xxczaki/awesome-calculators)  * [awesome-canvas](https://github.com/raphamorim/awesome-canvas) – HTML5 Canvas  * [awesome-captcha](https://github.com/ZYSzys/awesome-captcha) – Captcha libraries and crack tools.    - http://zyszys.github.io/awesome-captcha/  * [awesome-cassandra](https://github.com/yikebocai/awesome-cassandra)  * [awesome-ccxt](https://github.com/suenot/awesome-ccxt) – [CryptoCurrency eXchange Trading Library](https://github.com/ccxt/ccxt)  * [awesome_challenge_list](https://github.com/AwesomeRubyist/awesome_challenge_list) – Sites with challenges to improve your programming skills.  * [awesome-challenges](https://github.com/mauriciovieira/awesome-challenges) – Algorithmic challenges  * [awesome-charting](https://github.com/zingchart/awesome-charting) – Charts and dataviz.  * [awesome-chatops](https://github.com/exAspArk/awesome-chatops) – ChatOps – managing operations through a chat  * [awesome-chef](https://github.com/obazoud/awesome-chef) – Cookbooks, handlers, add-ons and other resources for Chef, a configuration management tool.  * [awesome-cheminformatics](https://github.com/hsiaoyi0504/awesome-cheminformatics) – Chemical informatics  * [awesome-chess](https://github.com/hkirat/awesome-chess) – Chess software, libraries, and resources  * [awesome-choo](https://github.com/choojs/awesome-choo) – [choo](https://choo.io/) web framework  * [awesome-chrome-devtools](https://github.com/ChromeDevTools/awesome-chrome-devtools) – Chrome DevTools ecosystem tooling and resources.  * [awesome-ci](https://github.com/ligurio/awesome-ci) by @ligurio – Comparison of cloud based CI services.  * [awesome-ci](https://github.com/pditommaso/awesome-ci) by @pditommaso – Continuous integation services.  * [awesome-ciandcd](https://github.com/cicdops/awesome-ciandcd) – Continuous Integration and Continuous Delivery    - http://www.ciandcd.com/  * [awesome-circuitpython](https://github.com/adafruit/awesome-circuitpython) – [CircuitPython](https://circuitpython.org/) microcontrollers programming language  * [awesome-cl](https://github.com/CodyReichert/awesome-cl) – Common Lisp  * [awesome-cl-software](https://github.com/azzamsa/awesome-cl-software) – Applications built with Common Lisp  * [awesome-cli-apps](https://github.com/agarrharr/awesome-cli-apps) – Command line apps  * [awesome-clojure](https://github.com/mbuczko/awesome-clojure) by @mbuczko – Useful links for clojurians  * [awesome-clojure](https://github.com/razum2um/awesome-clojure) by @razum2um  * [awesome-clojurescript](https://github.com/hantuzun/awesome-clojurescript)  * [awesome-cloud](https://github.com/JStumpp/awesome-cloud) – Delightful cloud services.  * [awesome-cloud-certifications](https://gitlab.com/edzob/awesome-cloud-certifications) – Certifications for cloud platforms  * [awesome-cloudflare](https://github.com/irazasyed/awesome-cloudflare) – [Cloudflare](https://www.cloudflare.com/) tools and recipes.  * [awesome-cmake](https://github.com/onqtam/awesome-cmake) – CMake  * [awesome-cms](https://github.com/postlight/awesome-cms) – Open and closed source Content Management Systems (CMS)  * [Awesome-CobaltStrike-Defence](https://github.com/MichaelKoczwara/Awesome-CobaltStrike-Defence) – Defences against [Cobalt Strike](https://www.cobaltstrike.com/), Adversary Simulations and Red Team Operations software.  * [awesome-cobol](https://github.com/mickaelandrieu/awesome-cobol) – COBOL programming language  * [awesome-cocoa](https://github.com/v-braun/awesome-cocoa) – Cocoa controls for iOS, watchOS and macOS    - http://cocoa.rocks  * [awesome-code-formatters](https://github.com/rishirdua/awesome-code-formatters)  * [awesome-code-review](https://github.com/joho/awesome-code-review)  * [awesome-codepoints](https://github.com/Codepoints/awesome-codepoints) – Interesting Unicode characters  * [awesome-coins](https://github.com/Zheaoli/awesome-coins) – Guide to cryto-currencies and their algos.  * [awesome-cold-showers](https://github.com/hwayne/awesome-cold-showers) – For when people get too hyped up about things.  * [awesome-coldfusion](https://github.com/seancoyne/awesome-coldfusion)  * [awesome-common-lisp-learning](https://github.com/GustavBertram/awesome-common-lisp-learning)  * [awesome-community](https://github.com/phpearth/awesome-community) – development, support and discussion channels, groups and communities.  * [awesome-community-building](https://github.com/CrowdDevHQ/awesome-community-building) – Building developer communities.  * [awesome-community-detection](https://github.com/benedekrozemberczki/awesome-community-detection) – Community detection papers with implementations.  * [awesome-comparisons](https://github.com/dhamaniasad/awesome-comparisons) – Framework and code comparison projects, like TodoMVC and Notejam.  * [awesome-competitive-programming](https://github.com/lnishan/awesome-competitive-programming) – Competitive Programming, Algorithm and Data Structure resources    - http://codeforces.com/blog/entry/23054  * [awesome-composer](https://github.com/jakoch/awesome-composer) – Composer, Packagist, Satis PHP ecosystem  * [awesome-computational-neuroscience](https://github.com/eselkin/awesome-computational-neuroscience) – Schools and researchers in computational neuroscience  * [awesome-computer-history](https://github.com/watson/awesome-computer-history) – Computer history videos, documentaries and related folklore.  * [awesome-computer-vision](https://github.com/AGV-IIT-KGP/awesome-computer-vision) by @AGV-IIT-KGP  * [awesome-computer-vision](https://github.com/jbhuang0604/awesome-computer-vision) by @jbhuang0604  * [awesome-computer-vision-models](https://github.com/nerox8664/awesome-computer-vision-models) – Popular deep learning models related to classification and segmentation task  * [awesome-conference-playlists](https://github.com/chentsulin/awesome-conference-playlists) – Video playlists for conferences.  * [awesome-conferences](https://github.com/RichardLitt/awesome-conferences)  * [awesome-connectivity-info](https://github.com/stevesong/awesome-connectivity-info) – Connectivity indexes and reports to help you better under who has access to communication infrastructure and on what terms.  * [awesome-conservation-tech](https://github.com/anselmbradford/awesome-conservation-tech) – Intersection of tech and environmental conservation.  * [awesome-console-services](https://github.com/chubin/awesome-console-services) – Console services (reachable via HTTP, HTTPS and other network protocols).  * [awesome-construct](https://github.com/WebCreationClub/awesome-construct) – [Construct](https://www.construct.net/) game development toolkit  * [awesome-container](https://github.com/tcnksm/awesome-container) – Container technologies and services.  * [awesome-conversational](https://github.com/mortenjust/awesome-conversational) – Conversational UI  * [awesome-cordova](https://github.com/busterc/awesome-cordova) _Apache Cordova / PhoneGap_  * [Awesome-CoreML-Models](https://github.com/likedan/Awesome-CoreML-Models) – Models for Core ML (for iOS 11+)  * [awesome-coronavirus](https://github.com/soroushchehresa/awesome-coronavirus) – Projects and resources related to SARS-CoV-2 and COVID-19.  * [awesome-couchdb](https://github.com/quangv/awesome-couchdb) – CouchDB resource list.  * [awesome-courses](https://github.com/fffaraz/awesome-courses) by @fffaraz – Online programming/CS courses.  * [awesome-courses](https://github.com/prakhar1989/awesome-courses) by @prakhar1989 – University Computer Science courses across the web.  * [awesome-cpp](https://github.com/fffaraz/awesome-cpp) – C/C++  * [awesome-crdt](https://github.com/alangibson/awesome-crdt) – Conflict-free replicated data types  * [awesome-creative-coding](https://github.com/terkelg/awesome-creative-coding) – Creative Coding: Generative Art, Data visualization, Interaction Design  * [awesome-critical-tech-reading-list](https://github.com/chobeat/awesome-critical-tech-reading-list) – Reading list for the modern critical programmer.  * [Awesome-Cross-Platform-Apps](https://github.com/Juude/Awesome-Cross-Platform-Apps) – Solutions for building cross-platform apps.  * [awesome-cross-platform-nodejs](https://github.com/bcoe/awesome-cross-platform-nodejs) – Tools for writing cross-platform Node.js code.  * [awesome-crypto-papers](https://github.com/pFarb/awesome-crypto-papers) – Cryptography papers, articles, tutorials and howtos.  * [awesome-cryptocurrencies](https://github.com/kasketis/awesome-cryptocurrencies)  * [awesome-cryptography](https://github.com/sobolevn/awesome-cryptography) – Cryptography and encryption resources.  * [awesome-crystal](https://github.com/veelenga/awesome-crystal) – Crystal Language  * [awesome-css](https://github.com/awesome-css-group/awesome-css) by @awesome-css-group  * [awesome-css](https://github.com/bring2dip/awesome-css) by @deepakbhattarai  * [awesome-css-frameworks](https://github.com/troxler/awesome-css-frameworks) – CSS frameworks  * [awesome-css-learning](https://github.com/micromata/awesome-css-learning) – A tiny list limited to the best CSS Learning Resources  * [awesomeCSV](https://github.com/secretGeek/awesomeCSV) – CSV, Comma Separated Values format  * [awesome-ctf](https://github.com/apsdehal/awesome-ctf) – [Capture the Flag](https://en.wikipedia.org/wiki/Capture_the_flag#Computer_security)    - https://apsdehal.in/awesome-ctf/  * [awesome-cto](https://github.com/kuchin/awesome-cto) – Resources for Chief Technology Officers, with the emphasis on startups  * [awesome-cto-resources](https://github.com/mateusz-brainhub/awesome-cto-resources) – Grow as a Chief Technology Officer.  * [awesome-cybersecurity-blueteam](https://github.com/fabacab/awesome-cybersecurity-blueteam) – [Cybersecurity blue teams](https://en.wikipedia.org/wiki/Blue_team_(computer_security)) resources  * [awesome-cyclejs](https://github.com/cyclejs-community/awesome-cyclejs) – Cycle.js framework  * [awesome-d](https://github.com/zhaopuming/awesome-d) – D programming language.  * [awesome-d3](https://github.com/wbkd/awesome-d3) – [D3js](http://d3js.org/) libraries, plugins and utilities.  * [awesome-dart](https://github.com/yissachar/awesome-dart)  * [awesome-dash](https://github.com/ucg8j/awesome-dash) – [Dash (plotly)](https://plot.ly/dash/) framework for analytical web applications  * [awesome-dashboard](https://github.com/obazoud/awesome-dashboard) – Dashboards/visualization resources.  * [awesome-data-engineering](https://github.com/igorbarinov/awesome-data-engineering) – Data engineering tools for software developers.  * [awesome-datascience](https://github.com/academic/awesome-datascience) – An open source DataScience repository to learn and apply for real world problems.  * [awesome-datasets](https://github.com/viisar/awesome-datasets) – Datasets for papers/experiments/validation.  * [awesome-dataviz](https://github.com/fasouto/awesome-dataviz) – Data visualizations frameworks, libraries and software.  * [awesome-db](https://github.com/numetriclabz/awesome-db) – Database libraries and resources.  * [awesome-ddd](https://github.com/heynickc/awesome-ddd) by @heynickc – Domain-Driven Design (DDD), Command Query Responsibility Segregation (CQRS), Event Sourcing, and Event Storming  * [awesome-ddd](https://github.com/wkjagt/awesome-ddd) by @wkjagt – Domain-Driven Design  * [awesome-decentralized-web](https://github.com/gdamdam/awesome-decentralized-web) – Decentralized services and technologies  * [awesome-decision-tree-papers](https://github.com/benedekrozemberczki/awesome-decision-tree-papers) – Decision Tree Research Papers  * [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning) – Deep Learning tutorials, projects and communities.  * [awesome-deep-learning-papers](https://github.com/terryum/awesome-deep-learning-papers) – The most cited deep learning papers  * [awesome-deep-learning-resources](https://github.com/guillaume-chevalier/awesome-deep-learning-resources) – Rough list of resources about deep learning.  * [awesome-deep-rl](https://github.com/tigerneil/awesome-deep-rl) – Deep Reinforcement Learning  * [awesome-deep-vision](https://github.com/kjw0612/awesome-deep-vision) – Computer vision / deep learning.  * [awesome-deku](https://github.com/lambtron/awesome-deku) – Resources for the Deku library.  * [awesome-delphi](https://github.com/Fr0sT-Brutal/awesome-delphi)  * [awesome-deno](https://github.com/denolib/awesome-deno) – [Deno](https://deno.land/), a secure runtime for JavaScript and TypeScript.  * [awesome-derby](https://github.com/russll/awesome-derby) – Components for DerbyJS.  * [awesome-design](https://github.com/troyericg/awesome-design) – Resources for digital designers.  * [awesome-design-patterns](https://github.com/DovAmir/awesome-design-patterns) – Resources on software design patterns.  * [awesome-design-principles](https://github.com/robinstickel/awesome-design-principles)  * [awesome-design-systems](https://github.com/alexpate/awesome-design-systems)  * [Awesome-Design-Tools](https://github.com/goabstract/Awesome-Design-Tools)    - https://flawlessapp.io/designtools  * [awesome-desktop-js](https://github.com/styfle/awesome-desktop-js) – Implementing desktop apps with JavaScript  * [awesome-dev-discord](https://github.com/ljosberinn/awesome-dev-discord) – Official, development-related Discord servers.    - https://dev-discords.now.sh/  * [awesome-dev-fun](https://github.com/mislavcimpersak/awesome-dev-fun) – Fun libs/packages/languages that have no real purpose but to make a developer chuckle.  * [awesome-developer-blogs](https://github.com/endymion1818/awesome-developer-blogs)  * [awesome-developer-experience](https://github.com/prokopsimek/awesome-developer-experience)  * [awesome-devenv](https://github.com/jondot/awesome-devenv) – Tools, resources and workflow tips making an awesome development environment.  * [awesome-devops](https://github.com/joubertredrat/awesome-devops)  * [awesome-devrel](https://github.com/devrelcollective/awesome-devrel) – Developer Relations  * [awesome-devtools](https://github.com/moimikey/awesome-devtools) – In-browser bookmarklets, tools, and resources for front-end devs.  * [awesome-digital-nomads](https://github.com/cbovis/awesome-digital-nomads) – Resources for Digital Nomads.  * [awesome-digitalocean](https://github.com/jonleibowitz/awesome-digitalocean) – DigitalOcean cloud infrastructure provider  * [awesome-discord](https://github.com/alfg/awesome-discord) by @alfg  * [awesome-discord](https://github.com/jacc/awesome-discord) by @jacc – Discord chat and VoIP application.  * [awesome-discord-communities](https://github.com/mhxion/awesome-discord-communities) – Discord communities for programmers.  * [awesome-diversity](https://github.com/folkswhocode/awesome-diversity) – Diversity in technology.  * [awesome-django](https://github.com/wsvincent/awesome-django) – [Django](https://www.djangoproject.com/) Python web framework  * [awesome-django-cms](https://github.com/mishbahr/awesome-django-cms) – django CMS add-ons.  * [awesome-docker](https://github.com/veggiemonk/awesome-docker) by @veggiemonk  * [awesome-docsify](https://github.com/docsifyjs/awesome-docsify) – [docsify](https://docsify.js.org/) documentation site generator.  * [awesome-doctrine](https://github.com/biberlabs/awesome-doctrine) – Doctrine ORM libraries and resources.  * [awesome-document-understanding](https://github.com/tstanislawek/awesome-document-understanding) – Automated data extraction from documents.  * [awesome-dojo](https://github.com/petk/awesome-dojo) – Dojo JavaScript Toolkit resources and libraries.  * [awesome-dot-dev](https://github.com/orbit-love/awesome-dot-dev) – Developer resources on the .dev TLD.  * [awesome-dotfiles](https://github.com/webpro/awesome-dotfiles)  * [awesome-dotnet](https://github.com/quozd/awesome-dotnet) – .NET libraries, tools, frameworks and software.  * [awesome-dotnet-architecture](https://github.com/mehdihadeli/awesome-dotnet-architecture) – Software architecture, patterns, and principles in .NET platform.  * [awesome-dotnet-async](https://github.com/mehdihadeli/awesome-dotnet-async) – Async, threading, and channels in .NET platform,  * [awesome-dotnet-core](https://github.com/thangchung/awesome-dotnet-core) – .NET core libraries, tools, frameworks and software  * [awesome-dotnet-core-education](https://github.com/mehdihadeli/awesome-dotnet-core-education) – .NET Core education resources.  * [awesome-draft-js](https://github.com/nikgraf/awesome-draft-js) – [Draft.js](https://draftjs.org/) text editor framework  * [awesome-dropwizard](https://github.com/stve/awesome-dropwizard) – [Dropwizard](https://www.dropwizard.io/) Java web framework  * [awesome-drupal](https://github.com/emincansumer/awesome-drupal) by @emincansumer  * [awesome-drupal](https://github.com/mrsinguyen/awesome-drupal) by @mrsinguyen  * [awesome-drupal](https://github.com/nirgn975/awesome-drupal) by @nirgn975 – Useful resources for Drupal CMS :droplet:  * [awesome-dtrace](https://github.com/xen0l/awesome-dtrace) – DTrace books, articles, videos, tools and resources.    - https://awesome-dtrace.com  * [awesome-ebpf](https://github.com/zoidbergwill/awesome-ebpf) – eBPF Linux packet filter  * [awesome-economics](https://github.com/antontarasenko/awesome-economics) – Economics related projects, software, people  * [awesome-ecs](https://github.com/nathanpeck/awesome-ecs) – AWS Elastic Container Service and Fargate.  * [awesome-edtech-tools](https://github.com/hkalant/awesome-edtech-tools) – Tools and resources for educators and virtual teachers.  * [awesome-educate](https://github.com/mercer/awesome-educate) – Education resources online.  * [awesome-educational-games](https://github.com/yrgo/awesome-educational-games) – Educational games to learn editors, languages, programming  * [awesome-ejabberd](https://github.com/shantanu-deshmukh/awesome-ejabberd) – All awesome stuff of the ejabberd ecosystem.    - https://ejabberd.shantanudeshmukh.com  * [awesome-electron](https://github.com/sindresorhus/awesome-electron) – Resources for creating apps with [Electron](http://electron.atom.io/) (formerly atom-shell).  * [awesome-electronics](https://github.com/kitspace/awesome-electronics) – Electronic engineering  * [awesome-elixir](https://github.com/h4cc/awesome-elixir)  * [awesome-elm](https://github.com/sporto/awesome-elm) – [Elm](https://elm-lang.org/), a functional reactive language  * [awesome-emacs](https://github.com/emacs-tw/awesome-emacs) by @emacs-tw  * [awesome-emacs](https://github.com/sefakilic/awesome-emacs) by @sefakilic  * [awesome-emacs](https://github.com/tacticiankerala/awesome-emacs) by @tacticiankerala  * [awesome-emails](https://github.com/jonathandion/awesome-emails) – Build better emails.  * [awesome-embedded-rust](https://github.com/rust-embedded/awesome-embedded-rust) – Embedded and Low-level development in the Rust programming language  * [awesome-ember](https://github.com/ember-community-russia/awesome-ember) by @ember-community-russia – [Ember.js](https://emberjs.com/) JavaScript framework  * [awesome-ember](https://github.com/nmec/awesome-ember) by @nmec – Ember.js things.  * [awesome-endless-codeforall-list](https://github.com/RobTranquillo/awesome-endless-codeforall-list) – Every tool that civic hackers worldwide use to work.  * [awesome-engineer-onboarding](https://github.com/posquit0/awesome-engineer-onboarding)  * [awesome-engineering-ladders](https://github.com/posquit0/awesome-engineering-ladders)  * [awesome-engineering-team-principles](https://github.com/posquit0/awesome-engineering-team-principles)  * [awesome-eosio](https://github.com/DanailMinchev/awesome-eosio) – [EOS.IO](https://eos.io/) blockchain protocol  * [awesome-erlang](https://github.com/drobakowski/awesome-erlang)  * [awesome-eslint](https://github.com/dustinspecker/awesome-eslint) – [ESLint](https://eslint.org/) JavaScript linter  * [awesome-esolangs](https://github.com/angrykoala/awesome-esolangs) – Esoteric languages  * [awesome-eta](https://github.com/sfischer13/awesome-eta) – [Eta](https://eta-lang.org/) programming language  * [awesome-ethereum](https://github.com/bekatom/awesome-ethereum) by @bekatom – [Ethereum](https://ethereum.org/) decentralized software platform & Dapps.  * [Awesome-Ethereum](https://github.com/ttumiel/Awesome-Ethereum) by @ttumiel  * [awesome-ethereum](https://github.com/vinsgo/awesome-ethereum) by @vinsgo    - http://awesome-ethereum.com/  * [awesome-ethereum-virtual-machine](https://github.com/pirapira/awesome-ethereum-virtual-machine)  * [awesome-falsehood](https://github.com/kdeldycke/awesome-falsehood) – Falsehoods programmers believe in.  * [awesome-fantasy](https://github.com/r7kamura/awesome-fantasy) – FinalFantasy-ish metaphors in software.  * [awesome-fast-check](https://github.com/dubzzz/awesome-fast-check) – [fast-check](https://github.com/dubzzz/fast-check/) property based testing framework for JavaScript/TypeScript  * [awesome-fastapi](https://github.com/mjhea0/awesome-fastapi) – [FastAPI](https://fastapi.tiangolo.com/) Python web framework  * [awesome-feathersjs](https://github.com/feathersjs/awesome-feathersjs) – [Feathers](https://feathersjs.com/) Node.js framework for real-time applications REST APIs.  * [awesome-fediverse](https://github.com/emilebosch/awesome-fediverse) – [Fediverse](https://en.wikipedia.org/wiki/Fediverse) resources.  * [awesome-ffmpeg](https://github.com/transitive-bullshit/awesome-ffmpeg) – FFmpeg resources.  * [awesome-firebase](https://github.com/jthegedus/awesome-firebase) – Firebase mobile development platform  * [awesome.fish](https://github.com/jorgebucaran/awesome.fish) – Fish shell    - https://git.io/awesome-fish  * [awesome-flask](https://github.com/humiaozuzu/awesome-flask) – Flask Python web framework resources and plugins.  * [awesome-flexbox](https://github.com/afonsopacifer/awesome-flexbox) – CSS Flexible Box Layout Module.  * [awesome-fluidapp](https://github.com/lborgav/awesome-fluidapp) – Icons, Userstyles and Userscripts for Fluid Apps  * [awesome-flutter](https://github.com/Solido/awesome-flutter) – An awesome list that curates the best Flutter libraries, tools, tutorials, articles and more.  * [awesome-fonts](https://github.com/brabadu/awesome-fonts) – Fonts and everything  * [awesome-food](https://github.com/jzarca01/awesome-food) – Food related software projects  * [awesome-for-beginners](https://github.com/MunGell/awesome-for-beginners) – Beginner-friendly projects to start contributing.  * [awesome-fortran](https://github.com/rabbiabram/awesome-fortran)  * [awesome-foss-apps](https://github.com/DataDaoDe/awesome-foss-apps) – Production grade free and open source software  * [awesome-fp-js](https://github.com/stoeffel/awesome-fp-js) – Functional programming stuff in JavaScript.  * [awesome-framer](https://github.com/podo/awesome-framer) – Framer prototyping tool  * [awesome-fraud-detection-papers](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers) – Fraud detection research papers.  * [awesome-frc](https://github.com/andrewda/awesome-frc) – First Robotics Competition  * [awesome-free-software](https://github.com/johnjago/awesome-free-software) – Free as in freedom software  * [awesome-frege](https://github.com/sfischer13/awesome-frege) – [Frege](https://github.com/Frege/frege) programming language  * [awesome-fsharp](https://github.com/fsprojects/awesome-fsharp) – F# programming language  * [awesome-fsm](https://github.com/leonardomso/awesome-fsm) by @leonardomso – Finite State Machines and Statecharts  * [awesome-fsm](https://github.com/soixantecircuits/awesome-fsm) by @soixantecircuits – Finite State Machines  * [awesome-functional-programming](https://github.com/lucasviola/awesome-functional-programming) by @lucasviola  * [awesome-functional-programming](https://github.com/xgrommx/awesome-functional-programming) by @xgrommx  * [awesome-funny-markov](https://github.com/sublimino/awesome-funny-markov) – Delightfully amusing and facetious Markov chain output.  * [awesome-fuse](https://github.com/fuse-compound/awesome-fuse) – [Fuse](https://fuseopen.com/) mobile development framework  * [awesome-fuzzing](https://github.com/cpuu/awesome-fuzzing) – Fuzzing (or Fuzz Testing) for software security  * [awesome-gametalks](https://github.com/hzoo/awesome-gametalks) – Gaming talks (development, design, etc)  * [awesome-gbdev](https://github.com/gbdev/awesome-gbdev) – Game Boy development resources such as tools, docs, emulators, related projects and open-source ROMs    - https://gbdev.github.io/list  * [awesome-geek-podcasts](https://github.com/ayr-ton/awesome-geek-podcasts) – Podcasts we like to listen to.    - http://ayr-ton.github.io/awesome-geek-podcasts  * [awesome-gemini](https://github.com/kr1sp1n/awesome-gemini) – [Gemini protocol](https://gemini.circumlunar.space/)  * [awesome-geojson](https://github.com/tmcw/awesome-geojson) – GeoJSON  * [awesome-ggplot2](https://github.com/erikgahner/awesome-ggplot2) – [ggplot2](https://ggplot2.tidyverse.org/) data visualization for R.  * [awesome-gideros](https://github.com/stetso/awesome-gideros) – [Gideros](http://giderosmobile.com/) game development framework  * [awesome-gif](https://github.com/davisonio/awesome-gif) – GIF software resources    - https://davison.io/awesome-gif  * [awesome-gists](https://github.com/vsouza/awesome-gists) – Amazing gists  * [awesome-git](https://github.com/dictcp/awesome-git) – Git tools, resources and shiny things.  * [awesome-git-addons](https://github.com/stevemao/awesome-git-addons) – Add-ons that extend/enhance the git CLI.  * [awesome-git-hooks](https://github.com/CompSciLauren/awesome-git-hooks) – Easy-to-use git hooks for automating tasks during git workflows.  * [awesome-github](https://github.com/AntBranch/awesome-github) _In Chinese_ by @AntBranch – GitHub guides, articles, sites, tools, projects and resources.  收集这个列表，只是为了更好地使用亲爱的GitHub,欢迎提交pr和issue。    - https://github.com/AntBranch/awesome-github  * [awesome-github](https://github.com/fffaraz/awesome-github) by @fffaraz – Git and GitHub references.  * [awesome-github](https://github.com/Kikobeats/awesome-github) by @Kikobeats – GitHub secrets and goodies.  * [awesome-github](https://github.com/phillipadsmith/awesome-github) by @phillipadsmith – GitHub's awesomeness  * [awesome-github-repo](https://github.com/flyhigher139/awesome-github-repo) – GitHub repositories; various topics like study materials, Raspberry Pi etc.  * [awesome-gnome](https://github.com/Kazhnuz/awesome-gnome) – Gnome Desktop Environment.  * [awesome-go](https://github.com/avelino/awesome-go) by @avelino – Golang    - http://awesome-go.com/  * [awesome-go-books](https://github.com/heatroom/awesome-go-books) – Online and free golang books.  * [awesome-godot](https://github.com/godotengine/awesome-godot) – [Godot](https://godotengine.org/) game engine  * [awesome-gradient-boosting-papers](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers) – Gradient boosting research papers with implementations.  * [awesome-grails](https://github.com/hitenpratap/awesome-grails)  * [awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification) – Graph embedding papers with implementations.  * [awesome-graphql](https://github.com/chentsulin/awesome-graphql) – GraphQL & Relay Resources.  * [awesome-groovy](https://github.com/kdabir/awesome-groovy)  * [awesome-growth-hacking](https://github.com/bekatom/awesome-growth-hacking)  * [awesome-gulp](https://github.com/alferov/awesome-gulp) – [Gulp](http://gulpjs.com/) build system resources and plugins.  * [awesome-gyazo](https://github.com/gyazo/awesome-gyazo) – Tools for [Gyazo](https://gyazo.com/) screen capture application.  * [awesome-h2o](https://github.com/h2oai/awesome-h2o) – H2O Machine Learning  * [awesome-hacking](https://github.com/carpedm20/awesome-hacking)  * [awesome-hacktoberfest-2020](https://github.com/Piyushhbhutoria/awesome-hacktoberfest-2020) – [Hacktoberfest](https://hacktoberfest.digitalocean.com/)-friendly repositories and resources.  * [awesome-hadoop](https://github.com/youngwookim/awesome-hadoop) – Hadoop and Hadoop ecosystem resources.  * [awesome-haskell](https://github.com/krispo/awesome-haskell)  * [awesome-hasura](https://github.com/aaronhayes/awesome-hasura) – [Hasura](https://hasura.io/) is an instant realtime GraphQL engine for PostgreSQL.  * [awesome-haxe-gamedev](https://github.com/dvergar/awesome-haxe-gamedev) – Game development in [Haxe](https://haxe.org/) cross-platform programming language  * [awesome-hbase](https://github.com/rayokota/awesome-hbase) – Apache HBase  * [awesome-hdl](https://github.com/drom/awesome-hdl) – Hardware Description Languages  * [awesome-healthcare](https://github.com/kakoni/awesome-healthcare) – Open source healthcare software, libraries, tools and resources.  * [awesome-heroku](https://github.com/ianstormtaylor/awesome-heroku) – Heroku resources.  * [awesome_hierarchical_matrices](https://github.com/gchavez2/awesome_hierarchical_matrices) – Hierarchical matrices frameworks, libraries, and software.  * [awesome-home-assistant](https://github.com/frenck/awesome-home-assistant) – [Home Assistant](https://www.home-assistant.io/) home automation    - https://awesome-ha.com  * [awesome-homematic](https://github.com/homematic-community/awesome-homematic) – [HomeMatic](https://www.homematic.com/) home automation  * [awesome-honeypots](https://github.com/paralax/awesome-honeypots) – Honeypot resources  * [awesome-html5](https://github.com/diegocard/awesome-html5)  * [awesome-humane-tech](https://github.com/humanetech-community/awesome-humane-tech) – Promoting Solutions that Improve Wellbeing, Freedom and Society  * [awesome-hyper](https://github.com/bnb/awesome-hyper) – [Hyper](https://hyper.is/) terminal  * [awesome-ibmcloud](https://github.com/victorshinya/awesome-ibmcloud) – IBM Cloud    - https://awesome-ibmcloud.mybluemix.net  * [awesome-icons](https://github.com/notlmn/awesome-icons) – Downloadable SVG/PNG/Font icon projects  * [awesome-idris](https://github.com/joaomilho/awesome-idris) – 𝛌 [Idris](https://www.idris-lang.org/), functional programming language with dependent types  * [awesome-incident-response](https://github.com/meirwah/awesome-incident-response) – Resources useful for incident responders.  * [awesome-indie](https://github.com/mezod/awesome-indie) – Resources for independent developers to make money  * [awesome-infinidash](https://github.com/joenash/awesome-infinidash)  * [awesome-influxdb](https://github.com/mark-rushakoff/awesome-influxdb) – Resources for the time series database InfluxDB  * [awesome-information-retrieval](https://github.com/harpribot/awesome-information-retrieval) – Information retrieval resources  * [awesome-inspectit](https://github.com/inspectit-labs/awesome-inspectit) – InspectIT documentations and resources.  * [awesome-interview-questions](https://github.com/MaximAbramchuck/awesome-interview-questions) – Interview questions.  * [awesome-ionic](https://github.com/candelibas/awesome-ionic) – [Ionic](https://ionicframework.com/) mobile development framework  * [awesome-ios](https://github.com/vsouza/awesome-ios)  * [awesome-ios-cn](https://github.com/jobbole/awesome-ios-cn) _In Chinese_ – iOS 资源大全中文版，内容包括：框架、组件、测试、Apple Store、SDK、XCode、网站、书籍等  * [awesome-ios-ui](https://github.com/cjwirth/awesome-ios-ui) – UI/UX libraries for iOS.  * [awesome-IoT](https://github.com/dharmeshkakadia/awesome-IoT) by @dharmeshkakadia – Internet of Things  * [awesome-iot](https://github.com/HQarroum/awesome-iot) by @HQarroum – Internet of Things  * [awesome-IoT-hybrid](https://github.com/weblancaster/awesome-IoT-hybrid) – Internet of Things and Hybrid Applications  * [awesome-ipfs](https://github.com/ipfs/awesome-ipfs) – [IPFS](https://ipfs.io/) distributed web    - https://awesome.ipfs.io/  * [awesome-irc](https://github.com/davisonio/awesome-irc) – Internet Relay Chat protocol.  * [awesome-it-quotes](https://github.com/victorlaerte/awesome-it-quotes) – Collect all relevant quotes said over the history of IT  * [awesome-jamstack](https://github.com/automata/awesome-jamstack) – [JAMstack](https://jamstack.org) (JavaScript, APIs, Markup)  * [awesome-java](https://github.com/akullpp/awesome-java)  * [awesome-javascript](https://github.com/sorrycc/awesome-javascript)  * [awesome-javascript-books](https://github.com/heatroom/awesome-javascript-books) – Online and free JavaScript books.  * [awesome-javascript-learning](https://github.com/micromata/awesome-javascript-learning) – Tiny list limited to the best JavaScript Learning Resources  * [awesome-jitsi](https://github.com/easyjitsi/awesome-jitsi) – [Jitsi](https://jitsi.org/) open-source video conferencing.  * [awesome-jmeter](https://github.com/aliesbelik/awesome-jmeter) – Apache JMeter load testing  * [awesome-job-boards](https://github.com/emredurukn/awesome-job-boards) by @emredurukn  * [awesome-job-boards](https://github.com/tramcar/awesome-job-boards) by @tramcar  * [awesome-jquery](https://github.com/petk/awesome-jquery)  * [awesome-js-drama](https://github.com/scottcorgan/awesome-js-drama) – JavaScript topics the just might spark the next revolt!  * [awesome-json](https://github.com/burningtree/awesome-json)  * [awesome-jsonschema](https://github.com/jviotti/awesome-jsonschema) – [JSON Schema](http://json-schema.org/).  * [awesome-json-datasets](https://github.com/jdorfman/awesome-json-datasets) – JSON datasets that don't require authentication  * [awesome-json-next](https://github.com/json-next/awesome-json-next) – What's Next for JSON for Structured (Meta) Data in Text.  * [awesome-julia](https://github.com/melvin0008/awesome-julia)  * [awesome-jupyter](https://github.com/markusschanta/awesome-jupyter) – [Jupyter](https://jupyter.org/)  * [awesome-jvm](https://github.com/deephacks/awesome-jvm)  * [awesome-kafka](https://github.com/monksy/awesome-kafka) – [Apache Kafka](http://kafka.apache.org/), distributed streaming platform  * [awesome-katas](https://github.com/gamontal/awesome-katas) – Code katas  * [awesome-kde](https://github.com/francoism90/awesome-kde) – KDE Desktop Environment.  * [awesome-knockout](https://github.com/dnbard/awesome-knockout) – Plugins for Knockout MVVM framework.  * [awesome-koa](https://github.com/ellerbrock/awesome-koa) – [Koa.js](https://koajs.com/) Web Framework    - https://ellerbrock.github.io/awesome-koa  * [awesome-koans](https://github.com/ahmdrefat/awesome-koans) – Programming kōans in various languages.  * [awesome-kotlin](https://github.com/KotlinBy/awesome-kotlin) – [Kotlin](https://kotlinlang.org/) programming language    - https://kotlin.link/  * [awesome-kotlin-native](https://github.com/bipinvaylu/awesome-kotlin-native) – Kotlin Multiplatform libraries & resources.  * [awesome-kr-foss](https://github.com/darjeeling/awesome-kr-foss) – Korean open source projects.  * [awesome-kubernetes](https://github.com/ramitsurana/awesome-kubernetes)    - https://ramitsurana.github.io/awesome-kubernetes  * [awesome-landing-page](https://github.com/nordicgiant2/awesome-landing-page) – Landing pages templates  * [awesome-languages](https://github.com/perfaram/awesome-languages) – Open-source programming languages.  * [awesome-laravel](https://github.com/chiraggude/awesome-laravel) by @chiraggude  * [awesome-laravel](https://github.com/TimothyDJones/awesome-laravel) by @TimothyDJones  * [Awesome-Laravel-Education](https://github.com/fukuball/Awesome-Laravel-Education) _In English and Chinese_ – Laravel PHP framework learning resources.  * [awesome-latam](https://github.com/gophers-latam/awesome-latam) _In Spanish_ – Recursos en Español para desarrolladores de Golang.    - https://gophers-latam.github.io/  * [awesome-LaTeX](https://github.com/egeerardyn/awesome-LaTeX)  * [awesome-ld-preload](https://github.com/gaul/awesome-ld-preload) – LD_PRELOAD, a mechanism for changing application behavior at run-time.  * [awesome-leading-and-managing](https://github.com/LappleApple/awesome-leading-and-managing) – Leading people and being a manager. Geared toward tech, but potentially useful to anyone.  * [awesome-learn-datascience](https://github.com/siboehm/awesome-learn-datascience) – Resources to help you get started with Data Science  * [awesome-ledger](https://github.com/sfischer13/awesome-ledger) – Ledger command-line accounting system  * [awesome-legacy-code](https://github.com/legacycoderocks/awesome-legacy-code) – Legacy systems with publicly available source code  * [awesome-less](https://github.com/LucasBassetti/awesome-less) – Less CSS preprocessor  * [awesome-lesscode](https://github.com/dream2023/awesome-lesscode) _In Chinese_ – Low code / no code projects  * [awesome-libgdx](https://github.com/rafaskb/awesome-libgdx) – [libGDX](https://libgdx.badlogicgames.com/) cross-platform games development framework  * [awesome-libgen](https://github.com/freereadorg/awesome-libgen) – Library Genesis, the world's largest free library.  * [awesome-libra](https://github.com/learndapp/awesome-libra) by @learndapp – [Libra](https://libra.org/) cryptocurrency by Facebook  * [awesome-libra](https://github.com/reed-hong/awesome-libra) by @reed-hong – [Facebook Diem](https://www.diem.com/) (née Libra) digital currency.  * [awesome-librehosters](https://github.com/libresh/awesome-librehosters) – Nice hosting providers  * [awesome-linguistics](https://github.com/theimpossibleastronaut/awesome-linguistics) – Tools, theory and platforms for linguistics.  * [awesome-links](https://github.com/rbk/awesome-links) – Web Development Links by @richardbenjamin.  * [awesome-linters](https://github.com/caramelomartins/awesome-linters) – Resources for a more literate programming.  * [awesome-linux](https://github.com/aleksandar-todorovic/awesome-linux) – Linux software.  * [awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers) – Linux Containers frameworks, libraries and software  * [awesome-linux-resources](https://github.com/itech001/awesome-linux-resources)    - http://www.linux6.com  * [Awesome-Linux-Software](https://github.com/luong-komorebi/Awesome-Linux-Software) – Linux applications for all users and developers.  * [awesome-linuxaudio](https://github.com/nodiscc/awesome-linuxaudio) – Professional audio/video/live events production on Linux.  * [awesome-lit-html](https://github.com/web-padawan/awesome-lit-html) – [lit-html](https://lit-html.polymer-project.org/) HTML templating library  * [awesome-livecoding](https://github.com/toplap/awesome-livecoding) – All things Livecoding.  * [awesome-logging](https://github.com/roundrobin/awesome-logging)  * [awesome-loginless](https://github.com/fiatjaf/awesome-loginless) – Internet services that don't require logins or registrations.  * [awesome-love2d](https://github.com/love2d-community/awesome-love2d) – [LÖVE](http://love2d.org/) Lua game framework  * [awesome-lowcode](https://github.com/taowen/awesome-lowcode) _In Chinese_ – Chinese low code platforms.  * [awesome-lua](https://github.com/forhappy/awesome-lua) by @forhappy  * [awesome-lua](https://github.com/LewisJEllis/awesome-lua) by @LewisJEllis  * [awesome-lumen](https://github.com/unicodeveloper/awesome-lumen) – [Lumen](https://lumen.laravel.com/), PHP Microframework by Laravel  * [awesome-luvit](https://github.com/luvit/awesome-luvit) – [Luvit](https://luvit.io/), asynchronous I/O for Lua  * [awesome-mac](https://github.com/jaywcjlove/awesome-mac) by @jaywcjlove – Premium macOS software in various categories    - https://git.io/macx  * [awesome-mac](https://github.com/xyNNN/awesome-mac) by @xyNNN – macOS tools, applications and games.  * [awesome-mac-apps](https://github.com/justin-j/awesome-mac-apps) – macOS apps  * [awesome-machine-learning](https://github.com/josephmisiti/awesome-machine-learning)  * [awesome-macOS](https://github.com/iCHAIT/awesome-macOS) – OS X applications, tools and communities.  * [awesome-macos-command-line](https://github.com/herrbischoff/awesome-macos-command-line) – Shell commands and tools specific to OS X.  * [awesome-macos-screensavers](https://github.com/agarrharr/awesome-macos-screensavers) – Screensavers for Mac OS X  * [awesome-mad-science](https://github.com/feross/awesome-mad-science) – npm packages that make you say ""wow, didn't know that was possible!""  * [awesome-magento2](https://github.com/DavidLambauer/awesome-magento2) – [Magento 2](https://magento.com/) PHP eCommerce platform    - https://davidlambauer.github.io/awesome-magento2/  * [awesome-maintainers](https://github.com/nayafia/awesome-maintainers) – Talks, blog posts, and interviews about the experience of being an open source maintainer  * [awesome-malware-analysis](https://github.com/rshipp/awesome-malware-analysis)  * [awesome-manifestos](https://github.com/imsky/awesome-manifestos) – Interesting software manifestos and principles  * [awesome-marionette](https://github.com/sadcitizen/awesome-marionette) – [marionette.js](https://marionettejs.com/) framework  * [awesome-markdown](https://github.com/BubuAnabelas/awesome-markdown)  * [awesome-markdown-alternatives](https://github.com/mundimark/awesome-markdown-alternatives) – Light-weight markup markdown alternatives.  * [awesome-masonite](https://github.com/vaibhavmule/awesome-masonite) – [Masonite](https://docs.masoniteproject.com/) Python web framework  * [awesome-mastodon](https://github.com/tleb/awesome-mastodon) – [Mastodon](https://joinmastodon.org/) decentralized microblogging network  * [awesome-material](https://github.com/sachin1092/awesome-material) – Google's material design  * [Awesome-MaterialDesign](https://github.com/lightSky/Awesome-MaterialDesign) _In Chinese_ – Resources and libraries for [Material Design](http://www.google.com/design/spec/material-design/introduction.html).  * [awesome-math](https://github.com/rossant/awesome-math) – Mathematics  * [awesome-MATLAB](https://github.com/mikecroucher/awesome-MATLAB)  * [awesome-mechanical-keyboard](https://github.com/BenRoe/awesome-mechanical-keyboard) – Mechanical Keyboards    - https://keebfol.io  * [awesome-mesos](https://github.com/dharmeshkakadia/awesome-mesos) by @dharmeshkakadia  * [awesome-mesos](https://github.com/parolkar/awesome-mesos) by @parolkar  * [awesome-meteor](https://github.com/Urigo/awesome-meteor)  * [awesome-meteor-developers](https://github.com/harryadel/awesome-meteor-developers) – Ways to support Meteor developers and packages.  * [awesome-mews](https://github.com/MewsSystems/awesome-mews) – Resources Mews developers like and aligns with their vision.  * [awesome-micro-npm-packages](https://github.com/parro-it/awesome-micro-npm-packages) – Small, focused npm packages.  * [awesome-microbit](https://github.com/carlosperate/awesome-microbit) – BBC micro:bit  * [awesome-microfrontends](https://github.com/ChristianUlbrich/awesome-microfrontends)  * [awesome-microservices](https://github.com/mfornos/awesome-microservices) – Microservice Architecture related principles and technologies.  * [awesome-minecraft](https://github.com/bs-community/awesome-minecraft)  * [awesome-minimalist](https://github.com/neiesc/awesome-minimalist) – Minimalist frameworks (simple and lightweight).  * [awesome-mobile](https://github.com/alec-c4/awesome-mobile) – Instruments for mobile marketing and development  * [awesome-mobile-web-development](https://github.com/myshov/awesome-mobile-web-development) – All that you need to create a great mobile web experience  * [awesome-mongodb](https://github.com/ramnes/awesome-mongodb)  * [awesome-monitoring](https://github.com/crazy-canux/awesome-monitoring) – INFRASTRUCTURE、OPERATION SYSTEM and APPLICATION monitoring tools for Operations.    - http://canuxcheng.com/awesome-monitoring/  * [awesome-monte-carlo-tree-search-papers](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers) – Monte Carlo tree search, a heuristic search algorithm frequently used in games.  * [awesome-motion-design-web](https://github.com/lucasmaiaesilva/awesome-motion-design-web)  * [awesome-motion-planning](https://github.com/AGV-IIT-KGP/awesome-motion-planning) – Papers, books and tools for motion planning.  * [awesome-mqtt](https://github.com/hobbyquaker/awesome-mqtt) – MQTT related stuff.  * [awesome-msr](https://github.com/dspinellis/awesome-msr) – Empirical Software Engineering: evidence-based, data-driven research on software systems  * [awesome-music](https://github.com/ciconia/awesome-music) – Music, audio, MIDI  * [awesome-mysql](https://github.com/shlomi-noach/awesome-mysql) – MySQL software, libraries, tools and resources  * [awesome-naming](https://github.com/gruhn/awesome-naming) – When naming things is done right.  * [awesome-neo4j](https://github.com/neueda/awesome-neo4j) – Neo4j graph database  * [awesome-netherlands-events](https://github.com/awkward/awesome-netherlands-events) – Dutch (tech related) events  * [awesome-network-analysis](https://github.com/briatte/awesome-network-analysis)    - http://f.briatte.org/r/awesome-network-analysis-list  * [awesome-network-embedding](https://github.com/chihming/awesome-network-embedding) – Papers on node embedding techniques.  * [awesome-network-js](https://github.com/Kikobeats/awesome-network-js) – Network layer resources in pure JavaScript  * [Awesome-Networking](https://github.com/clowwindy/Awesome-Networking)  * [awesome-neuroscience](https://github.com/analyticalmonk/awesome-neuroscience) – Neuroscience libraries, software and resources    - http://akashtandon.com/awesome-neuroscience/  * [awesome-newsletters](https://github.com/mpron/awesome-newsletters) by @mpron – Developer newsletters  * [awesome-newsletters](https://github.com/webpro/awesome-newsletters) by @webpro – The best (weekly) newsletters  * [awesome-newsletters](https://github.com/zudochkin/awesome-newsletters) by @zudochkin  * [awesome-nextjs](https://github.com/unicodeveloper/awesome-nextjs) – [Next.js](https://nextjs.org/) React-based JavaScript framework  * [awesome-nim](https://github.com/VPashkov/awesome-nim) – [Nim](https://nim-lang.org/) programming language  * [awesome-nlp](https://github.com/keon/awesome-nlp) – Natural Language Processing.  * [Awesome-no-code-tools](https://github.com/ElijT/Awesome-no-code-tools)  * [awesome-no-login-web-apps](https://github.com/aviaryan/awesome-no-login-web-apps) – Web apps that work without login  * [awesome-nocode](https://github.com/nslindtner/awesome-nocode)  * [awesome-node-esm](https://github.com/talentlessguy/awesome-node-esm) – ES modules for Node.js  * [awesome-nodejs](https://github.com/sindresorhus/awesome-nodejs) by @sindresorhus  * [awesome-non-financial-blockchain](https://github.com/machinomy/awesome-non-financial-blockchain) – Non-financial applications of blockchain  * [awesome-nosql-guides](https://github.com/erictleung/awesome-nosql-guides) – NoSQL databases    - https://erictleung.com/awesome-nosql-guides/  * [awesome-npm](https://github.com/sindresorhus/awesome-npm)  * [awesome-npm-scripts](https://github.com/RyanZim/awesome-npm-scripts) – using npm as a build tool  * [awesome-ntnu](https://github.com/michaelmcmillan/awesome-ntnu) – Projects by NTNU students.  * [awesome-nuxt](https://github.com/nuxt-community/awesome-nuxt) – Resources for [Nuxt.js](https://nuxtjs.org/), framework for universal Vue.js applications.  * [awesome-objc-frameworks](https://github.com/follyxing/awesome-objc-frameworks)  * [awesome-observables](https://github.com/sindresorhus/awesome-observables) – An Observable is a collection that arrives over time.  * [awesome-obsidian](https://github.com/kmaasrud/awesome-obsidian) – [Obsidian](https://obsidian.md/) knowledge base app.  * [awesome-ocaml](https://github.com/ocaml-community/awesome-ocaml)  * [awesome-ocap](https://github.com/dckc/awesome-ocap) – Capability-based security enables the concise composition of powerful patterns of cooperation without vulnerability.  * [awesome-okr](https://github.com/domenicosolazzo/awesome-okr) – Objective - Key Results, the best practice of setting and communicating company, team and employee objectives and measuring their progress based on achieved results  * [awesome-online-ide](https://github.com/styfle/awesome-online-ide) – Online development environments    - https://ide.ceriously.com  * [awesome-online-machine-learning](https://github.com/MaxHalford/awesome-online-machine-learning) – [Online machine learning](https://en.wikipedia.org/wiki/Online_machine_learning)  * [awesome-open-company](https://github.com/opencompany/awesome-open-company) – Open companies: Share as much as possible, charge as little as possible.  * [awesome-open-science](https://github.com/silky/awesome-open-science)  * [awesome-open-source-supporters](https://github.com/zachflower/awesome-open-source-supporters) – Companies that offer their services for free to Open Source projects  * [awesome-opengl](https://github.com/eug/awesome-opengl) – OpenGL libraries, debuggers and resources.  * [awesome-opensource-documents](https://github.com/44bits/awesome-opensource-documents) – Open source or open source licensed documents, guides, books.  * [awesome-OpenSourcePhotography](https://github.com/ibaaj/awesome-OpenSourcePhotography) – Free open source software & libraries for photography. Also tools for video.  * [awesome-osc](https://github.com/amir-arad/awesome-osc) – [Open Sound Control](http://opensoundcontrol.org/)  * [awesome-oss-alternatives](https://github.com/RunaCapital/awesome-oss-alternatives) – Open-source alternatives to established SaaS products.  * [awesome-pascal](https://github.com/Fr0sT-Brutal/awesome-pascal) – Delphi/FreePascal/(any)Pascal frameworks, libraries, resources, and shiny things.  * [awesome-pcaptools](https://github.com/caesar0301/awesome-pcaptools) – Tools to process network traces.  * [awesome-pentest](https://github.com/enaqx/awesome-pentest) – Penetration testing resources and tools.  * [awesome-pentest-cheat-sheets](https://github.com/coreb1t/awesome-pentest-cheat-sheets) – Penetration testing  * [Awesome-People-in-Computer-Vision](https://github.com/solarlee/Awesome-People-in-Computer-Vision)  * [awesome-perfocards](https://github.com/Wolg/awesome-perfocards) _See [perfokaart](https://et.wikipedia.org/wiki/Perfokaart)._  * [awesome-perl](https://github.com/hachiojipm/awesome-perl)  * [awesome-persian](https://github.com/fffaraz/awesome-persian) – Persian/Farsi supporting tools, fonts, and development resources.  * [awesome-phalcon](https://github.com/phalcon/awesome-phalcon) – [Phalcon](https://phalconphp.com/en/) PHP framework libraries and resources.  * [awesome-pharo](https://github.com/pharo-open-documentation/awesome-pharo) – [Pharo](https://pharo.org/) Smalltalk  * [awesome-pharo-ml](https://github.com/pharo-ai/awesome-pharo-ml) – Machine learning, AI, data science in Pharo.  * [awesome-php](https://github.com/ziadoz/awesome-php)  * [awesome-PICO-8](https://github.com/pico-8/awesome-PICO-8) – [PICO-8](https://www.lexaloffle.com/pico-8.php) fantasy console for making, sharing and playing tiny games    - https://pico-8.github.io/awesome-PICO-8/  * [awesome-pinned-gists](https://github.com/matchai/awesome-pinned-gists) – Dynamic pinned gists for GitHub.  * [awesome-pipeline](https://github.com/pditommaso/awesome-pipeline) – Pipeline toolkits.  * [awesome-piracy](https://github.com/Igglybuff/awesome-piracy) – Warez and piracy links  * [awesome-pixel-art](https://github.com/Siilwyn/awesome-pixel-art)  * [awesome-play1](https://github.com/PerfectCarl/awesome-play1) – Play Framework 1.x modules, tools, and resources.  * [awesome-plotters](https://github.com/beardicus/awesome-plotters) – Computer-controlled drawing machines and other visual art robots.  * [awesome-podcasts](https://github.com/Ghosh/awesome-podcasts) by @Ghosh – Podcasts for designers, developers, product managers, entrepreneurs and hustlers    - http://podcasts.surge.sh/  * [awesome-podcasts](https://github.com/rShetty/awesome-podcasts) by @rShetty – Important Podcasts for software engineers.  * [awesome-pokemon](https://github.com/tobiasbueschel/awesome-pokemon) – Pokémon & Pokémon Go  * [awesome-polymer](https://github.com/Granze/awesome-polymer) – [Polymer Project](https://www.polymer-project.org/)  * [awesome-postcss](https://github.com/jdrgomes/awesome-postcss) – [PostCSS](https://postcss.org/) CSS processor  * [awesome-postgres](https://github.com/dhamaniasad/awesome-postgres)  * [awesome-power-mode](https://github.com/codeinthedark/awesome-power-mode)  * [awesome-powershell](https://github.com/janikvonrotz/awesome-powershell)  * [awesome-preact](https://github.com/preactjs/awesome-preact) – [Preact](https://github.com/preactjs/preact) JavaScript framework  * [awesome-prisma](https://github.com/catalinmiron/awesome-prisma) – [Prisma](https://www.prisma.io/) GraphQL library  * [awesome-privacy](https://github.com/pluja/awesome-privacy) – Services and alternatives that respect your privacy because PRIVACY MATTERS.  * [awesome-product-design](https://github.com/teoga/awesome-product-design) by @teoga – Bookmarks, resources, articles for product designers.  * [awesome-product-design](https://github.com/ttt30ga/awesome-product-design) by @ttt30ga – Resources for product designers.  * [awesome-product-management](https://github.com/dend/awesome-product-management) – Resources for product/program managers to learn and grow.  * [awesome-productivity](https://github.com/jyguyomarch/awesome-productivity) – Delightful productivity resources.  * [awesome-ProductManager](https://github.com/hugo53/awesome-ProductManager) – Books and tools for Product Managers.  * [awesome-programming-for-kids](https://github.com/HollyAdele/awesome-programming-for-kids) – Teaching kids programming  * [awesome-progressive-web-apps](https://github.com/TalAter/awesome-progressive-web-apps) – Progressive Web Apps (PWA)  * [awesome-projects-boilerplates](https://github.com/melvin0008/awesome-projects-boilerplates)  * [awesome-prolog](https://github.com/klaussinani/awesome-prolog) – Prolog logic programming language  * [awesome-prometheus](https://github.com/roaldnefs/awesome-prometheus) – [Prometheus](https://prometheus.io/) monitoring system  * [awesome-prometheus-alerts](https://github.com/samber/awesome-prometheus-alerts) – Prometheus alerting rules    - https://awesome-prometheus-alerts.grep.to  * [awesome-promises](https://github.com/wbinnssmith/awesome-promises) – JavaScript Promises.  * [awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets) by @awesomedata – (Large-scale) public datasets on the Internet.    - [source data](https://github.com/awesomedata/apd-core)  * [awesome-puppet](https://github.com/rnelson0/awesome-puppet)  * [awesome-pure-css-no-javascript](https://github.com/Zhangjd/awesome-pure-css-no-javascript) _In Chinese_  * [awesome-purescript](https://github.com/passy/awesome-purescript)  * [awesome-pyramid](https://github.com/uralbash/awesome-pyramid) – Resources for Pyramid Python web framework.  * [awesome-python](https://github.com/kevmo/awesome-python) by @kevmo  * [awesome-python](https://github.com/vinta/awesome-python) by @vinta  * [awesome-python-cn](https://github.com/jobbole/awesome-python-cn) _In Chinese_  * [awesome-python-data-science](https://github.com/krzjoa/awesome-python-data-science)  * [awesome-python-in-education](https://github.com/quobit/awesome-python-in-education)  * [awesome-python-models](https://github.com/grundic/awesome-python-models) – List of ORMs, models, schemas, serializers, etc. libraries  for python.  * [awesome-python-scientific-audio](https://github.com/faroit/awesome-python-scientific-audio) – Python software and packages related to scientific research in audio  * [awesome-python-talks](https://github.com/jhermann/awesome-python-talks) – Videos related to Python, with a focus on training and gaining hands-on experience.  * [awesome-python-typing](https://github.com/typeddjango/awesome-python-typing) – Python types, stubs, plugins, and tools to work with them.  * [Awesome-pytorch-list](https://github.com/bharathgs/Awesome-pytorch-list) – [PyTorch](https://pytorch.org/) Python machine learning framework.  * [awesome-qa](https://github.com/seriousran/awesome-qa) – [Question Answering](https://en.wikipedia.org/wiki/Question_answering) systems automatically answer questions asked in a natural language  * [awesome-qsharp](https://github.com/ebraminio/awesome-qsharp) – [Q#](https://docs.microsoft.com/en-us/quantum/) quantum programming language  * [awesome-qt](https://github.com/JesseTG/awesome-qt) by @JesseTG – Qt framework  * [awesome-qt](https://github.com/skhaz/awesome-qt) by @skhaz – Qt framework  * [awesome-quantified-self](https://github.com/woop/awesome-quantified-self) – Devices, Wearables, Applications, and Platforms for Self Tracking  * [awesome-quantum-computing](https://github.com/desireevl/awesome-quantum-computing) – Quantum computing learning and developing resources.  * [awesome-R](https://github.com/qinwf/awesome-R)  * [awesome-radio](https://github.com/kyleterry/awesome-radio) – Radio and citizens band (CB) radio resources.  * [awesome-rails](https://github.com/dpaluy/awesome-rails) by @dpaluy  * [awesome-rails](https://github.com/gramantin/awesome-rails) by @gramantin – Projects and sites made with Rails.  * [awesome-rails](https://github.com/ruby-vietnam/awesome-rails) by @ruby-vietnam – Rails libraries/app examples/ebooks/tutorials/screencasts/magazines/news.  * [awesome-rails-gem](https://github.com/hothero/awesome-rails-gem) – Ruby Gems for Rails development.  * [awesome-random-forest](https://github.com/kjw0612/awesome-random-forest) – Decision forest, tree-based methods, including random forest, bagging, and boosting.  * [awesome-raspberry-pi](https://github.com/blackout314/awesome-raspberry-pi) by @blackout314    - http://blackout314.github.io/awesome-raspberry-pi/  * [awesome-raspberry-pi](https://github.com/thibmaek/awesome-raspberry-pi) by @thibmaek – Raspberry Pi tools, projects, images and resources  * [awesome-react](https://github.com/enaqx/awesome-react) – ReactJS tools, resources, videos.  * [awesome-react-components](https://github.com/brillout/awesome-react-components) – React Components & Libraries.  * [awesome-react-hooks](https://github.com/glauberfc/awesome-react-hooks) – React Hooks  * [awesome-react-native](https://github.com/jondot/awesome-react-native)    - http://www.awesome-react-native.com  * [awesome-react-state-management](https://github.com/olegrjumin/awesome-react-state-management)  * [awesome-react-state-management-tools](https://github.com/cs01/awesome-react-state-management-tools)  * [awesome-readme](https://github.com/matiassingers/awesome-readme) – READMEs examples and best practices  * [awesome-reasonml](https://github.com/vramana/awesome-reasonml) – [ReasonML](https://reasonml.github.io/), [BuckleScript](https://bucklescript.github.io/) and [OCaml](https://ocaml.org/) programming languages.  * [awesome-recommender-system](https://github.com/Geek4IT/awesome-recommender-system) – Recommender System frameworks, libraries and software.  * [awesome-recursion-schemes](https://github.com/passy/awesome-recursion-schemes)  * [awesome-redux](https://github.com/brillout/awesome-redux) by @brillout – Redux Libraries & Learning Material    - https://devarchy.com/redux  * [awesome-redux](https://github.com/xgrommx/awesome-redux) by @xgrommx – [Redux](https://github.com/rackt/redux) web application state container  * [awesome-refinerycms](https://github.com/refinerycms-contrib/awesome-refinerycms) – [Refinery](https://www.refinerycms.com/) Ruby on Rails CMS  * [awesome-regex](https://github.com/aloisdg/awesome-regex) – Regular expressions  * [awesome-regression-testing](https://github.com/mojoaxel/awesome-regression-testing) – Visual regression testing  * [awesome-relay](https://github.com/expede/awesome-relay) – [Relay](https://relay.dev/) JavaScript framework for React and GraphQL  * [awesome-reMarkable](https://github.com/reHackable/awesome-reMarkable) – [reMarkable](https://remarkable.com/) e-ink tablet.  * [awesome-remote-job](https://github.com/lukasz-madon/awesome-remote-job) – Remote companies and other resources.  * [awesome-RemoteWork](https://github.com/hugo53/awesome-RemoteWork) – Books and links about and for remote work.  * [awesome-research](https://github.com/emptymalei/awesome-research) – Tools to help you with research/life    - http://openmetric.org/tool/  * [awesome-rest](https://github.com/marmelab/awesome-rest) – Great resources about RESTful API architecture, development, test, and performance  * [awesome-rethinkdb](https://github.com/d3viant0ne/awesome-rethinkdb) – [RethinkDB](https://rethinkdb.com/) realtime database  * [awesome-retrospectives](https://github.com/josephearl/awesome-retrospectives) – Facilitating and learning about retrospectives.  * [awesome-ripple](https://github.com/vhpoet/awesome-ripple) – [Ripple](https://ripple.com/) cryptocurrency  * [awesome-rl](https://github.com/aikorea/awesome-rl) – Reinforcement Learning.  * [awesome-rl-for-cybersecurity](https://github.com/Limmen/awesome-rl-for-cybersecurity) – Reinforcement learning applied to cyber security.  * [awesome-rnn](https://github.com/kjw0612/awesome-rnn) – Recurrent Neural Networks.  * [awesome-roadmaps](https://github.com/liuchong/awesome-roadmaps) – Skills roadmaps for software development  * [awesome-roam](https://github.com/roam-unofficial/awesome-roam) – Roam Research networked note-taking  * [awesome-robotics](https://github.com/Kiloreux/awesome-robotics)  * [awesome-ros2](https://github.com/fkromer/awesome-ros2) – [Robot Operating System](http://www.ros.org/)    - https://fkromer.github.io/awesome-ros2  * [awesome-roslyn](https://github.com/ironcev/awesome-roslyn) – Roslyn .NET Compiler Platform  * [awesome-rshiny](https://github.com/grabear/awesome-rshiny) – A curated list of resources for the R shiny package.    - https://grabear.github.io/awesome-rshiny/  * [awesome-ruby](https://github.com/markets/awesome-ruby) by @markets    - http://awesome-ruby.com/  * [awesome-ruby](https://github.com/Sdogruyol/awesome-ruby) by @Sdogruyol  * [awesome-ruby-ast](https://github.com/rajasegar/awesome-ruby-ast) – Abstract Syntax Trees (AST) in Ruby  * [AwesomeRubyist/awesome_podcast_list](https://github.com/AwesomeRubyist/awesome_podcast_list) – Podcasts about Ruby and development, also in Russian.  * [AwesomeRubyist/awesome_reading_list](https://github.com/AwesomeRubyist/awesome_reading_list) – Books about Ruby and Rails.  * [AwesomeRubyist/awesome_resource_list](https://github.com/AwesomeRubyist/awesome_resource_list) – Resources for Ruby and Rails.  * [awesome-rubymotion](https://github.com/motion-open-source/awesome-rubymotion) – [RubyMotion](http://www.rubymotion.com/), cross-platform development in Ruby    - http://motion-open-source.github.io/awesome-rubymotion/  * [awesome-rust](https://github.com/rust-unofficial/awesome-rust)  * [awesome-rxjava](https://github.com/eleventigers/awesome-rxjava) – RxJava, reactive programming library  * [awesome-salesforce](https://github.com/mailtoharshit/awesome-salesforce) – Salesforce Platform Resources  * [awesome-saltstack](https://github.com/hbokh/awesome-saltstack) – [SaltStack](https://www.saltstack.com/) configuration management  * [awesome-sarl](https://github.com/sarl/awesome-sarl) – Resources for [SARL](http://www.sarl.io/) Agent-Oriented Programming Language.  * [awesome-SAS](https://github.com/huyingjie/awesome-SAS) – [SAS](https://www.sas.com/) analysis system  * [awesome-sass](https://github.com/Famolus/awesome-sass) by @Famolus – Sass and SCSS CSS preprocessor  * [awesome-sass](https://github.com/HugoGiraudel/awesome-sass) by @HugoGiraudel – Sass and SCSS CSS preprocessor  * [awesome-satellite-imagery-datasets](https://github.com/chrieke/awesome-satellite-imagery-datasets) – Satellite imagery datasets with annotations for computer vision and deep learning.  * [awesome-scala](https://github.com/lauris/awesome-scala) – Scala programming language  * [awesome-scala-native](https://github.com/tindzk/awesome-scala-native) – [Scala Native](http://www.scala-native.org) compiler  * [awesome-scalability](https://github.com/binhnguyennus/awesome-scalability) – The Patterns of Scalable, Reliable, and Performant Large-Scale Systems  * [awesome-scientific-computing](https://github.com/nschloe/awesome-scientific-computing) – Software for numerical analysis  * [awesome-scientific-writing](https://github.com/writing-resources/awesome-scientific-writing) – Tools, demos and resources to go beyond LaTeX.  * [awesome-sdn](https://github.com/sdnds-tw/awesome-sdn) – Software Defined Network (SDN)  * [awesome-sec-talks](https://github.com/PaulSec/awesome-sec-talks) – Security talks.  * [awesome-security](https://github.com/sbilly/awesome-security) – Software, libraries, documents, books, resources and cool stuff about security.  * [awesome-selenium](https://github.com/christian-bromann/awesome-selenium)  * [awesome-selfhosted](https://github.com/awesome-selfhosted/awesome-selfhosted) – Network services and web applications which can be hosted locally.  * [awesome-semantic-web](https://github.com/semantalytics/awesome-semantic-web) – Semantic web and linked data  * [awesome-seo](https://github.com/teles/awesome-seo) – SEO (Search Engine Optimization) links.    - http://jotateles.com.br/awesome-seo/  * [awesome-serverless](https://github.com/anaibol/awesome-serverless) by @anaibol – Services, solutions and resources for serverless / nobackend applications.  * [awesome-serverless](https://github.com/pmuens/awesome-serverless) by @pmuens – Resources related to serverless computing and serverless architectures.  * [awesome-serverless-security](https://github.com/puresec/awesome-serverless-security) – Serverless security resources  * [awesome-service-workers](https://github.com/TalAter/awesome-service-workers) – Service Workers for Progressive Web Applications  * [awesome-servicefabric](https://github.com/lawrencegripper/awesome-servicefabric) – Azure [Service Fabric](https://docs.microsoft.com/en-us/azure/service-fabric/) distributed services platform  * [awesome-services](https://github.com/indrasantosa/awesome-services) – Services that make a painful programmer's life easier.  * [awesome-sharepoint](https://github.com/BSUG/awesome-sharepoint) by @BSUG  * [awesome-SharePoint](https://github.com/siaf/awesome-SharePoint) by @siaf  * [awesome-sheet-music](https://github.com/ad-si/awesome-sheet-music) – Sheet music software, libraries and resources.  * [awesome-shell](https://github.com/alebcay/awesome-shell) – Command-line frameworks, toolkits, guides and gizmos.  * [awesome-sites](https://github.com/Gherciu/awesome-sites) – Various websites with resources for development, graphics, and learning  * [awesome-sketch](https://github.com/diessica/awesome-sketch) – Guides, articles, videos about [Sketch 3](http://www.sketchapp.com/).  * [awesome-slack](https://github.com/filipelinhares/awesome-slack) by @filipelinhares – Communities powered by Slack.  * [awesome-slack](https://github.com/matiassingers/awesome-slack) by @matiassingers  * [awesome-slack-communities](https://github.com/radermacher/awesome-slack-communities) – Public Slack Communities.  * [awesome-smart-tv](https://github.com/vitalets/awesome-smart-tv) – Smart TV apps  * [awesome-software-architecture](https://github.com/mehdihadeli/awesome-software-architecture) by @mehdihadeli – Software architecture, patterns, and principles.  * [awesome-software-architecture](https://github.com/simskij/awesome-software-architecture) by @simskij – Design, reason around and build software using architectural patterns and methods  * [awesome-software-craftsmanship](https://github.com/benas/awesome-software-craftsmanship) – [Software craftsmanship](http://manifesto.softwarecraftsmanship.org/) resources to help learn the craft.  * [awesome-software-patreons](https://github.com/uraimo/awesome-software-patreons) – Programmers and software-related Patreon accounts.  * [awesome-software-quality](https://github.com/ligurio/awesome-software-quality) – Free software testing books.  * [awesome-solid](https://github.com/kustomzone/awesome-solid) – [Solid](https://solidproject.org/) (social linked data) project.  * [awesome-sound](https://github.com/hwclass/awesome-sound) – Sound & audio libraries and resources.  * [awesome-space](https://github.com/elburz/awesome-space) – Outer Space  * [awesome-space-books](https://github.com/Hunter-Github/awesome-space-books) – Space exploration related book  * [awesome-spanish-nlp](https://github.com/dav009/awesome-spanish-nlp) – Linguistic Resources for doing NLP & CL on Spanish  * [awesome-spark](https://github.com/awesome-spark/awesome-spark) – Apache Spark packages and resources.  * [awesome-speakers](https://github.com/karlhorky/awesome-speakers) – Speakers in the programming and design communities  * [awesome-sphinxdoc](https://github.com/yoloseem/awesome-sphinxdoc) – Tools for Sphinx Python Documentation Generator.  * [awesome-split-keyboards](https://github.com/diimdeep/awesome-split-keyboards) – Ergonomic split keyboards.  * [awesome-sqlalchemy](https://github.com/dahlia/awesome-sqlalchemy) – Extra libraries for SQLAlchemy, a Python ORM.  * [awesome-sre](https://github.com/dastergon/awesome-sre) – Site Reliability and Production Engineering    - https://sre.xyz  * [awesome-ssh](https://github.com/moul/awesome-ssh)    - https://manfred.life/awesome-ssh  * [awesome-stacks](https://github.com/stackshareio/awesome-stacks) – Tech stacks for building different applications & features    - https://awesomestacks.dev  * [awesome-standard](https://github.com/standard/awesome-standard) – Documenting the explosion of packages in the [standard](http://standardjs.com/) (JavaScript code style) ecosystem.  * [awesome-stars](https://github.com/lichunqiang/awesome-stars) _In Chinese_ – Useful libraries with personal remarks.  * [awesome-startup](https://github.com/KrishMunot/awesome-startup) – Resources to build your own startup  * [awesome-static-generators](https://github.com/myles/awesome-static-generators) – Static web site generators.  * [awesome-static-website-services](https://github.com/agarrharr/awesome-static-website-services)  * [awesome-steam](https://github.com/scholtzm/awesome-steam) – Steam video games distribution platform development  * [awesome-storybook](https://github.com/lauthieb/awesome-storybook) – [Storybook](https://storybook.js.org/) UI web development  * [awesome-streaming](https://github.com/manuzhang/awesome-streaming) – Streaming frameworks, applications, etc  * [awesome-styleguides](https://github.com/RichardLitt/awesome-styleguides)  * [awesome-stylelint](https://github.com/stylelint/awesome-stylelint) – [Stylelint](https://stylelint.io/) CSS linter.  * [awesome-sustainable-technology](https://github.com/protontypes/awesome-sustainable-technology) – Open technology projects sustaining stable climate, energy supply and vital natural resources.    - https://opensustain.tech/  * [awesome-svelte](https://github.com/CalvinWalzel/awesome-svelte) – [Svelte](https://svelte.dev/) framework  * [awesome-svelte-resources](https://github.com/ryanatkn/awesome-svelte-resources) – [Svelte](https://svelte.dev/) framework  * [awesome-svg](https://github.com/willianjusten/awesome-svg)  * [awesome-swedish-opensource](https://github.com/gurre/awesome-swedish-opensource) – Open-source projects from Swedes  * [awesome-swift](https://github.com/matteocrippa/awesome-swift) by @matteocrippa  * [awesome-swift](https://github.com/Wolg/awesome-swift) by @Wolg  * [awesome-swift-and-tutorial-resources](https://github.com/MaxChen/awesome-swift-and-tutorial-resources) – Swift programming language  * [Awesome-Swift-Education](https://github.com/hsavit1/Awesome-Swift-Education) – Learn some Swift  * [Awesome-Swift-Playgrounds](https://github.com/uraimo/Awesome-Swift-Playgrounds) – Swift Playgrounds  * [awesome-symfony](https://github.com/sitepoint-editors/awesome-symfony) – [Symfony PHP framework](http://symfony.com/) bundles, utilities and resources.  * [awesome-symfony-education](https://github.com/pehapkari/awesome-symfony-education) – Symfony PHP framework learning resources  * [awesome-sysadmin](https://github.com/kahun/awesome-sysadmin) by @kahun – Open source sysadmin resources.  * [awesome-sysadmin](https://github.com/n1trux/awesome-sysadmin) by @n1trux – Open source sysadmin resources.  * [awesome-system-design](https://github.com/madd86/awesome-system-design) – Distributed systems design  * [awesome-system-fonts](https://github.com/mrmrs/awesome-system-fonts) – Websites that use system fonts.  * [awesome-taglines](https://github.com/miketheman/awesome-taglines) – Software taglines  * [awesome-tailwindcss](https://github.com/aniftyco/awesome-tailwindcss) – [Tailwind CSS](https://tailwindcss.com/)    - https://git.io/awesome-tailwindcss  * [awesome-talks](https://github.com/JanVanRyswyck/awesome-talks)  * [awesome-tap](https://github.com/sindresorhus/awesome-tap) – Test Anything Protocol  * [awesome-tech-blogs](https://github.com/markodenic/awesome-tech-blogs) – Technical blogs    - https://tech-blogs.dev/  * [awesome-tech-conferences](https://github.com/trstringer/awesome-tech-conferences) – Upcoming technical conferences.  * [awesome-tech-videos](https://github.com/lucasviola/awesome-tech-videos) – Tech conferences from youtube, vimeo, etc for us to get inspired  * [awesome-technical-writing](https://github.com/BolajiAyodeji/awesome-technical-writing)  * [awesome-telegram](https://github.com/ebertti/awesome-telegram) – Telegram messaging service  * [awesome-tensorflow](https://github.com/jtoy/awesome-tensorflow) – [TensorFlow](https://www.tensorflow.org/) machine intelligence library.  * [awesome-terraform](https://github.com/shuaibiyy/awesome-terraform) – HashiCorp Terraform  * [awesome-test-automation](https://github.com/atinfo/awesome-test-automation)    - http://automated-testing.info  * [awesome-testing](https://github.com/TheJambo/awesome-testing) – Testing resources    - https://git.io/v1hSm  * [awesome-text-editing](https://github.com/dok/awesome-text-editing) – Text editing resources and libraries for the web  * [awesome-textpattern](https://github.com/drmonkeyninja/awesome-textpattern) – Textpattern plugins and resources  * [awesome-themes](https://github.com/AdrienTorris/awesome-themes) – Web themes and templates  * [awesome-tikz](https://github.com/xiaohanyu/awesome-tikz) – [TikZ](https://pgf-tikz.github.io/) graph drawing package for TeX/LaTeX/ConTeXt  * [awesome-tinkerpop](https://github.com/mohataher/awesome-tinkerpop) – [Apache TinkerPop](http://tinkerpop.apache.org/) graph computing framework  * [awesome-token-sale](https://github.com/holographicio/awesome-token-sale) – Token sale / ICO resources  * [awesome-torch](https://github.com/carpedm20/awesome-torch) – Tutorials, projects and communities for [Torch](http://torch.ch/), a scientific computing framework for LuaJIT.  * [awesome-transit](https://github.com/CUTR-at-USF/awesome-transit) – Transit APIs, apps, datasets, research, and software  * [awesome-tunneling](https://github.com/anderspitman/awesome-tunneling) – [Ngrok](https://ngrok.com/) alternatives and other ngrok-like tunneling software and services. Focus on self-hosting.  * [awesome-twilio](https://github.com/Twilio-org/awesome-twilio) – Curated repository of useful and generally awesome Twilio tools and technologies  * [AwesomeTwitterAccounts](https://github.com/yask123/AwesomeTwitterAccounts) – Twitter accounts, organised by programming communities.  * [awesome-typescript](https://github.com/dzharii/awesome-typescript) by @dzharii – TypeScript programming language  * [awesome-typescript](https://github.com/ellerbrock/awesome-typescript) by @ellerbrock    - https://ellerbrock.github.io/awesome-typescript  * [awesome-typescript-projects](https://github.com/brookshi/awesome-typescript-projects) – TypeScript open-source projects  * [awesome-typography](https://github.com/Jolg42/awesome-typography) – Resources on OpenType & TrueType.  * [awesome-ui-component-library](https://github.com/anubhavsrivastava/awesome-ui-component-library) – Framework component libraries for UI styles/toolkit    - https://anubhavsrivastava.github.io/awesome-ui-component-library/  * [awesome-umbraco](https://github.com/umbraco-community/awesome-umbraco) – Resources for Umbraco 7, a .NET CMS.  * [Awesome-Unicode](https://github.com/Wisdom/Awesome-Unicode) – Unicode tidbits, packages and resources.    - https://git.io/Awesome-Unicode  * [awesome-unity](https://github.com/RyanNielson/awesome-unity) – Assets and resources for [Unity](http://unity3d.com/) game engine.  * [awesome-unix](https://github.com/sirredbeard/Awesome-UNIX)  * [awesome-userscripts](https://github.com/brunocvcunha/awesome-userscripts)  * [awesome-uses](https://github.com/wesbos/awesome-uses) – `/uses` pages detailing developer setups, gear, software and configs.    - https://uses.tech  * [awesome-uxn](https://github.com/hundredrabbits/awesome-uxn) – The [Uxn](https://100r.co/site/uxn.html) ecosystem is a personal computing playground, created to host small tools and games, programmable in its own unique assembly language.  * [awesome-v](https://github.com/vlang/awesome-v) – [V](https://vlang.io/) programming language  * [awesome-vagrant](https://github.com/iJackUA/awesome-vagrant)  * [awesome-vanilla-js](https://github.com/davidhund/awesome-vanilla-js) – Plain—‘Vanilla’—JavaScript  * [awesome-vapor](https://github.com/Cellane/awesome-vapor) – [Vapor](https://vapor.codes/) Swift web framework  * [awesome-vector-tiles](https://github.com/mapbox/awesome-vector-tiles) – Implementations of the [Mapbox Vector Tile](https://www.mapbox.com/developers/vector-tiles/) specification.  * [awesome-vehicle-security](https://github.com/jaredthecoder/awesome-vehicle-security) – Vehicle security and car hacking  * [awesome-vhdl](https://github.com/VHDL/awesome-vhdl) – VHDL hardware description language  * [awesome-vim](https://github.com/akrawchyk/awesome-vim) by @akrawchyk  * [awesome-vim](https://github.com/matteocrippa/awesome-vim) by @matteocrippa  * [awesome-vite](https://github.com/vitejs/awesome-vite) – [Vite](https://vitejs.dev/) front-end build tooling.  * [awesome-vlc](https://github.com/mfkl/awesome-vlc) – [VideoLAN VLC](https://www.videolan.org/) multimedia player and framework.  * [awesome-volt](https://github.com/heri/awesome-volt) – [Volt](http://voltframework.com/) Ruby web framework.  * [awesome-vorpal](https://github.com/vorpaljs/awesome-vorpal) – [Vorpal](http://vorpal.js.org/) Node.js interactive CLI framework  * [awesome-vscode](https://github.com/viatsko/awesome-vscode) – Visual Studio Code    - https://viatsko.github.io/awesome-vscode/  * [awesome-vue](https://github.com/vuejs/awesome-vue) – Resources for [Vue.js](http://vuejs.org/) JavaScript UI library.  * [awesome-vulkan](https://github.com/vinjn/awesome-vulkan) – [3D graphics and compute API](https://www.khronos.org/vulkan/)  * [awesome-wagtail](https://github.com/springload/awesome-wagtail) – [Wagtail](https://wagtail.io/) Python CMS  * [awesome-wasm](https://github.com/mbasso/awesome-wasm) – WebAssembly  * [awesome-watchos](https://github.com/yenchenlin/awesome-watchos) – Apple watchOS  * [awesome-web-animation](https://github.com/sergey-pimenov/awesome-web-animation) – Web animation libraries, books, apps etc.    - https://awesome-web-animation.netlify.com  * [awesome-web-archiving](https://github.com/iipc/awesome-web-archiving) – Getting started with web archiving  * [awesome-web-design](https://github.com/nicolesaidy/awesome-web-design) – Resources for digital designers.  * [awesome-web-effect](https://github.com/lindelof/awesome-web-effect) – Exquisite and compact web page effects.  * [awesome-web-scraping](https://github.com/lorien/awesome-web-scraping) – tools and programming libraries related to web scraping and data processing  * [awesome-web-security](https://github.com/qazbnm456/awesome-web-security)    - https://awesomelists.top/#/repos/qazbnm456/awesome-web-security  * [awesome-webaudio](https://github.com/notthetup/awesome-webaudio) – WebAudio packages and resources.  * [awesome-webauthn](https://github.com/herrjemand/awesome-webauthn) – WebAuthn/FIDO2  * [awesome-webcomponents](https://github.com/obetomuniz/awesome-webcomponents)  * [Awesome-WebExtensions](https://github.com/fregante/Awesome-WebExtensions) – WebExtensions development.  * [awesome-webgl](https://github.com/sjfricke/awesome-webgl) – WebGL libraries, resources and much more  * [awesome-webpack](https://github.com/webpack-contrib/awesome-webpack) – Webpack resources, libraries and tools  * [awesome-webpack-perf](https://github.com/iamakulov/awesome-webpack-perf) – Webpack tools for web performance  * [awesome-webservice](https://github.com/wapmorgan/awesome-webservice) – Web and cloud services, SaaS.  * [awesome-websockets](https://github.com/facundofarias/awesome-websockets) – Websocket libraries and resources.  * [awesome-webvis](https://github.com/rajsite/awesome-webvis) – [WebVI](http://www.webvi.io/) examples made using [LabVIEW](http://www.ni.com/en-us/support/software-technology-preview.html) systems engineering software.  * [awesome-weekly](https://github.com/jondot/awesome-weekly) – Quality weekly subscription newsletters from the software world.  * [awesome-wicket](https://github.com/PhantomYdn/awesome-wicket) – [Apache Wicket](http://wicket.apache.org/) Java web framework  * [awesome-wikipedia](https://github.com/emijrp/awesome-wikipedia) – Wikipedia-related frameworks, libraries, software, datasets and references.  * [Awesome-Windows/Awesome](https://github.com/Awesome-Windows/Awesome) – Applications and tools for Windows.  * [awesome-wordpress](https://github.com/dropndot/awesome-wordpress) by @dropndot  * [awesome-wordpress](https://github.com/endel/awesome-wordpress) by @endel  * [awesome-wordpress](https://github.com/miziomon/awesome-wordpress) by @miziomon  * [awesome-workflow-engines](https://github.com/meirwah/awesome-workflow-engines) – Open source workflow engines  * [awesome-workshopper](https://github.com/therebelrobot/awesome-workshopper)  * [awesome-wpo](https://github.com/davidsonfellipe/awesome-wpo) – Web Performance Optimization  * [awesome-xamarin](https://github.com/XamSome/awesome-xamarin) by @XamSome – [Xamarin](https://visualstudio.microsoft.com/xamarin/) mobile application framework  * [awesome-xamarin](https://github.com/XamSome/awesome-xamarin) by @XamSome – Interesting libraries/tools for Xamarin mobile projects  * [awesome-xcode-plugin](https://github.com/aashishtamsya/awesome-xcode-scripts) – XCode IDE scripts  * [awesome-xmpp](https://github.com/bluszcz/awesome-xmpp) – Curated list of awesome XMPP protocol resources.  * [awesome-yamada](https://github.com/supermomonga/awesome-yamada) – Dancing yamada  * [awesome-yii](https://github.com/iJackUA/awesome-yii) – Yii PHP framework extensions, tutorials and other nice things.  * [awesome-zig](https://github.com/nrdmn/awesome-zig) – [Zig](https://ziglang.org/) programming language.  * [awesome-zsh-plugins](https://github.com/unixorn/awesome-zsh-plugins)  * [awesomo](https://github.com/lk-geimfari/awesomo) – Open source projects in various languages.  * [craftcms/awesome](https://github.com/craftcms/awesome) – [Craft CMS](https://craftcms.com/)  * [not-awesome-es6-classes](https://github.com/petsel/not-awesome-es6-classes) – Why ES6 (aka ES2015) classes are NOT awesome    - https://matthias-endler.de/awesome-static-analysis/      ## Lists of lists    * [awesome](https://github.com/sindresorhus/awesome) – A curated list of awesome lists.  * [awesome-all](https://github.com/bradoyler/awesome-all) – A curated list of awesome lists of awesome frameworks, libraries and software  * [awesome-android-awesomeness](https://github.com/yongjhih/awesome-android-awesomeness#awesomeness)  * [awesome-awesome](https://github.com/aligoren/awesome-awesome) by @aligoren – List of GitHub Lists  * [awesome-awesome](https://github.com/emijrp/awesome-awesome) by @emijrp – A curated list of awesome curated lists of many topics.  * [awesome-awesome](https://github.com/erichs/awesome-awesome) by @erichs – A curated list of awesome curated lists! Inspired by inspiration.  * [awesome-awesome](https://github.com/oyvindrobertsen/awesome-awesome) by @oyvindrobertsen – A curated list of curated lists of libraries, resources and shiny things for various languages.  * [awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness) – A curated list of awesome awesomeness  * [awesome-awesomeness-zh_CN](https://github.com/justjavac/awesome-awesomeness-zh_CN) _In Chinese_ – 中文版awesome list 系列文章  * [awesome-awesomes](https://github.com/fleveque/awesome-awesomes) – Awesome collection of awesome lists of libraries, tools, frameworks and software for any programming language  * [awesome-collection](https://github.com/flyhigher139/awesome-collection) – A list of awesome repos.  * [Awesome-Hacking](https://github.com/Hack-with-Github/Awesome-Hacking) – Lists for hackers, pentesters and security researchers.  * [awesome-lists](https://github.com/pshah123/awesome-lists) – A curated list for your curated lists, including other curated lists of curated lists that may or may not contain other curated lists.  * [curated-lists](https://github.com/learn-anything/curated-lists)  * [delightful](https://codeberg.org/teaserbot-labs/delightful) – Home of delightful curated lists of free software, open science and information sources.  * [getAwesomeness](https://github.com/panzhangwang/getAwesomeness) – Explorer designed for curated awesome list hosted on Github    - https://getawesomeness.herokuapp.com/  * [list-of-lists](https://github.com/cyrusstoller/list-of-lists) – A meta list of lists of useful open source projects and developer tools.  * [ListOfGithubLists](https://github.com/asciimoo/ListOfGithubLists) – List of github lists  * [must-watch-list](https://github.com/adrianmoisey/must-watch-list) – List of must-watch lists.  * [this one](https://github.com/jnv/lists)  * [wiki](https://github.com/huguangju/wiki) _In Chinese_ – A curated list of awesome lists.      ### Lists of lists of lists    * [awesome-awesome-awesome](https://github.com/geekan/awesome-awesome-awesome) by @geekan – An awesome-awesome list.  * [awesome-awesome-awesome](https://github.com/t3chnoboy/awesome-awesome-awesome) by @t3chnoboy – A a curated list of curated lists of awesome lists.  * [awesomecubed](https://github.com/hunterboerner/awesomecubed) – A curated list of awesome awesomeness awesomenesses.  * [lologl](https://github.com/yaph/lologl) – List of Lists of Github Lists.  * [meta-awesome](https://github.com/PatrickMcDonald/meta-awesome)  * [the one above](#lists-of-lists)      #### Lists of lists of lists of lists    * [awesome-awesome-awesome-awesome](https://github.com/sindresorhus/awesome-awesome-awesome-awesome)  * [the one above](#lists-of-lists-of-lists)      ##### Lists of lists of lists of lists of lists    * [awesome-power-of-5](https://github.com/therebelbeta/awesome-power-of-5)  * [the one above](#lists-of-lists-of-lists-of-lists)      ###### Lists of lists of lists of lists of lists of lists    * [awesome-awesome-awesome-awesome-awesome-awesome](https://github.com/enedil/awesome-awesome-awesome-awesome-awesome-awesome)  * [the one above](#lists-of-lists-of-lists-of-lists-of-lists)      ###### Lists of lists of lists of lists of lists of lists of lists    * [awesome-awesome-awesome-awesome-awesome-awesome-awesome](https://github.com/sparanoid/awesome-awesome-awesome-awesome-awesome-awesome-awesome)  * [the one above](#lists-of-lists-of-lists-of-lists-of-lists-of-lists)    <!-- lists-end -->    ## License    [![CC0 Public Domain](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)    Social preview photo by [Eli Francis](https://unsplash.com/@elifrancis?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/books-clutter?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText). """
Big data;https://github.com/benedekrozemberczki/pytorch_geometric_temporal;"""[pypi-image]: https://badge.fury.io/py/torch-geometric-temporal.svg  [pypi-url]: https://pypi.python.org/pypi/torch-geometric-temporal  [size-image]: https://img.shields.io/github/repo-size/benedekrozemberczki/pytorch_geometric_temporal.svg  [size-url]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/archive/master.zip  [build-image]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/workflows/CI/badge.svg  [build-url]: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/actions?query=workflow%3ACI  [docs-image]: https://readthedocs.org/projects/pytorch-geometric-temporal/badge/?version=latest  [docs-url]: https://pytorch-geometric-temporal.readthedocs.io/en/latest/?badge=latest  [coverage-image]: https://codecov.io/gh/benedekrozemberczki/pytorch_geometric_temporal/branch/master/graph/badge.svg  [coverage-url]: https://codecov.io/github/benedekrozemberczki/pytorch_geometric_temporal?branch=master        <p align=""center"">    <img width=""90%"" src=""https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/docs/source/_static/img/text_logo.jpg?sanitize=true"" />  </p>    -----------------------------------------------------    [![PyPI Version][pypi-image]][pypi-url]  [![Docs Status][docs-image]][docs-url]  [![Code Coverage][coverage-image]][coverage-url]  [![Build Status][build-image]][build-url]  [![Arxiv](https://img.shields.io/badge/ArXiv-2104.07788-orange.svg)](https://arxiv.org/abs/2104.07788)  [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)⠀    **[Documentation](https://pytorch-geometric-temporal.readthedocs.io)** | **[External Resources](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/resources.html)** | **[Datasets](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#discrete-time-datasets)**    *PyTorch Geometric Temporal* is a temporal (dynamic) extension library for [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric).    <p align=""justify"">The library consists of various dynamic and temporal geometric deep learning, embedding, and spatio-temporal regression methods from a variety of published research papers. Moreover, it comes with an easy-to-use dataset loader, train-test splitter and temporal snaphot iterator for dynamic and temporal graphs. The framework naturally provides GPU support. It also comes with a number of benchmark datasets from the epidemological forecasting, sharing economy, energy production and web traffic management domains. Finally, you can also create your own datasets.</p>    The package interfaces well with [Pytorch Lightning](https://pytorch-lightning.readthedocs.io) which allows training on CPUs, single and multiple GPUs out-of-the-box. Take a look at this [introductory example](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/examples/recurrent/lightning_example.py) of using PyTorch Geometric Temporal with Pytorch Lighning.    We also provide detailed examples for each of the [recurrent](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/tree/master/examples/recurrent) models and [notebooks](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/tree/master/notebooks) for the attention based ones.      --------------------------------------------------------------------------------    **Case Study Tutorials**    We provide in-depth case study tutorials in the [Documentation](https://pytorch-geometric-temporal.readthedocs.io/en/latest/), each covers an aspect of PyTorch Geometric Temporal’s functionality.    **Incremental Training**: [Epidemiological Forecasting Case Study](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#epidemiological-forecasting)    **Cumulative Training**: [Web Traffic Management Case Study](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#web-traffic-prediction)    --------------------------------------------------------------------------------    **Citing**      If you find *PyTorch Geometric Temporal* and the new datasets useful in your research, please consider adding the following citation:    ```bibtex  @inproceedings{rozemberczki2021pytorch,                 author = {Benedek Rozemberczki and Paul Scherer and Yixuan He and George Panagopoulos and Alexander Riedel and Maria Astefanoaei and Oliver Kiss and Ferenc Beres and Guzman Lopez and Nicolas Collignon and Rik Sarkar},                 title = {{PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models}},                 year = {2021},                 booktitle={Proceedings of the 30th ACM International Conference on Information and Knowledge Management},                 pages = {4564–4573},  }  ```    --------------------------------------------------------------------------------    **A simple example**    PyTorch Geometric Temporal makes implementing Dynamic and Temporal Graph Neural Networks quite easy - see the accompanying [tutorial](https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/introduction.html#applications). For example, this is all it takes to implement a recurrent graph convolutional network with two consecutive [graph convolutional GRU](https://arxiv.org/abs/1612.07659) cells and a linear layer:    ```python  import torch  import torch.nn.functional as F  from torch_geometric_temporal.nn.recurrent import GConvGRU    class RecurrentGCN(torch.nn.Module):        def __init__(self, node_features, num_classes):          super(RecurrentGCN, self).__init__()          self.recurrent_1 = GConvGRU(node_features, 32, 5)          self.recurrent_2 = GConvGRU(32, 16, 5)          self.linear = torch.nn.Linear(16, num_classes)        def forward(self, x, edge_index, edge_weight):          x = self.recurrent_1(x, edge_index, edge_weight)          x = F.relu(x)          x = F.dropout(x, training=self.training)          x = self.recurrent_2(x, edge_index, edge_weight)          x = F.relu(x)          x = F.dropout(x, training=self.training)          x = self.linear(x)          return F.log_softmax(x, dim=1)  ```  --------------------------------------------------------------------------------    **Methods Included**    In detail, the following temporal graph neural networks were implemented.      **Recurrent Graph Convolutions**    * **[DCRNN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dcrnn.DCRNN)** from Li *et al.*: [Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting](https://arxiv.org/abs/1707.01926) (ICLR 2018)    * **[GConvGRU](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gconv_gru.GConvGRU)** from Seo *et al.*: [Structured Sequence Modeling with Graph  Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659) (ICONIP 2018)    * **[GConvLSTM](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gconv_lstm.GConvLSTM)** from Seo *et al.*: [Structured Sequence Modeling with Graph  Convolutional Recurrent Networks](https://arxiv.org/abs/1612.07659) (ICONIP 2018)    * **[GC-LSTM](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.gc_lstm.GCLSTM)** from Chen *et al.*: [GC-LSTM: Graph Convolution Embedded LSTM for Dynamic Link Prediction](https://arxiv.org/abs/1812.04206) (CoRR 2018)    * **[LRGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.lrgcn.LRGCN)** from Li *et al.*: [Predicting Path Failure In Time-Evolving Graphs](https://arxiv.org/abs/1905.03994) (KDD 2019)    * **[DyGrEncoder](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dygrae.DyGrEncoder)** from Taheri *et al.*: [Learning to Represent the Evolution of Dynamic Graphs with Recurrent Models](https://dl.acm.org/doi/10.1145/3308560.3316581)    * **[EvolveGCNH](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.evolvegcnh.EvolveGCNH)** from Pareja *et al.*: [EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs](https://arxiv.org/abs/1902.10191)    * **[EvolveGCNO](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.evolvegcno.EvolveGCNO)** from Pareja *et al.*: [EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs](https://arxiv.org/abs/1902.10191)    * **[T-GCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.temporalgcn.TGCN)** from Zhao *et al.*: [T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction](https://arxiv.org/abs/1811.05320)    * **[A3T-GCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.attentiontemporalgcn.A3TGCN)** from Zhu *et al.*: [A3T-GCN: Attention Temporal Graph Convolutional Network for Traffic Forecasting](https://arxiv.org/abs/2006.11583)     * **[AGCRN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.agcrn.AGCRN)** from Bai *et al.*: [Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting](https://arxiv.org/abs/2007.02842) (NeurIPS 2020)    * **[MPNN LSTM](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.mpnn_lstm.MPNNLSTM)** from Panagopoulos *et al.*: [Transfer Graph Neural Networks for Pandemic Forecasting](https://arxiv.org/abs/2009.08388) (AAAI 2021)      **Attention Aggregated Temporal Graph Convolutions**    * **[STGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.stgcn.STConv)** from Yu *et al.*: [Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting](https://arxiv.org/abs/1709.04875) (IJCAI 2018)    * **[ASTGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.astgcn.ASTGCN)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)    * **[MSTGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.mstgcn.MSTGCN)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)    * **[GMAN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.gman.GMAN)** from Zheng *et al.*: [GMAN: A Graph Multi-Attention Network for Traffic Prediction](https://arxiv.org/pdf/1911.08415.pdf) (AAAI 2020)    * **[MTGNN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.mtgnn.MTGNN)** from Wu *et al.*: [Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks](https://arxiv.org/abs/2005.11650) (KDD 2020)    * **[2S-AGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.tsagcn.AAGCN)** from Shi *et al.*: [Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition](https://arxiv.org/abs/1805.07694) (CVPR 2019)    * **[DNNTSP](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.dnntsp.DNNTSP)** from Yu *et al.*: [Predicting Temporal Sets with Deep Neural Networks](https://dl.acm.org/doi/abs/10.1145/3394486.3403152) (KDD 2020)    **Auxiliary Graph Convolutions**    * **[TemporalConv](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.stgcn.TemporalConv)** from Yu *et al.*: [Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting](https://arxiv.org/abs/1709.04875) (IJCAI 2018)    * **[DConv](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.dcrnn.DConv)** from Li *et al.*: [Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting](https://arxiv.org/abs/1707.01926) (ICLR 2018)    * **[ChebConvAttention](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.attention.astgcn.ChebConvAttention)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)    * **[AVWGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.recurrent.agcrn.AVWGCN)** from Bai *et al.*: [Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting](https://arxiv.org/abs/2007.02842) (NeurIPS 2020)      --------------------------------------------------------------------------------      Head over to our [documentation](https://pytorch-geometric-temporal.readthedocs.io) to find out more about installation, creation of datasets and a full list of implemented methods and available datasets.  For a quick start, check out the [examples](https://pytorch-geometric-temporal.readthedocs.io) in the `examples/` directory.    If you notice anything unexpected, please open an [issue](https://benedekrozemberczki/pytorch_geometric_temporal/issues). If you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/issues).      --------------------------------------------------------------------------------    **Installation**    Binaries are provided for Python version <= 3.9.    **PyTorch 1.10.0**    To install the binaries for PyTorch 1.10.0, simply run    ```sh  pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+${CUDA}.html  pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+${CUDA}.html  pip install torch-geometric  pip install torch-geometric-temporal  ```    where `${CUDA}` should be replaced by either `cpu`, `cu102`, or `cu113` depending on your PyTorch installation.    |             | `cpu` | `cu102` | `cu113` |  |-------------|-------|---------|---------|  | **Linux**   | ✅    | ✅      | ✅      |  | **Windows** | ✅    | ✅      | ✅      |  | **macOS**   | ✅    |         |         |    --------------------------------------------------------------------------------    **Running tests**    ```  $ python setup.py test  ```  --------------------------------------------------------------------------------    **License**    - [MIT License](https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/LICENSE) """
Big data;https://github.com/johnsonc/lambdo;"""[![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/conceptoriented/Lobby)  [![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://github.com/asavinov/lambdo/blob/master/LICENSE)  [![Python 3.6](https://img.shields.io/badge/python-3.6-brightgreen.svg)](https://www.python.org/downloads/release/python-360/)    # Feature engineering and machine learning: together at last!    Lambdo is a workflow engine which significantly simplifies the analysis process by *unifying* feature engineering and machine learning operations. Lambdo data analysis workflow does not distinguish between them and any node can be treated either as a feature or as prediction, and both of them can be trained.    Such a unification is possible because of the underlying *column-oriented* data processing paradigm which treats columns as first-class elements of the data processing pipeline having the same rights as tables. In Lambdo, a workflow consist of *table population* operations which process sets of records and *column evaluation* operations which produce new columns (features) from existing columns. This radically changes the way data is processed. The same approach is used also in Bistro: <https://github.com/asavinov/bistro>    Here are some unique distinguishing features of Lambdo:    * **No difference between features and models.** Lambdo unifies feature engineering and machine learning so that a workflow involves many feature definitions and many machine learning algorithms. It is especially important for deep learning where abstract intermediate features have to be learned.  * **One workflow for both prediction and training.** Lambdo nodes combine applying a transformation with training its model so that nodes of a workflow can be re-trained when required. This also guarantees that the same features will be used for both learning phase and prediction phase.  * **Columns first.**] Lambdo workflow use column operations along with table operations which makes many operations much simpler.  * **User-defined functions for extensibility.** Lambdo relies on user-defined functions which can be as simple as format conversion and as complex as deep learning networks.  * **Analysis of time-series and forecasting made easy.** Lambdo makes time series analysis much simpler by providing many using mechanisms like column families (for example, several moving averages with different window sizes), window-awareness (generation of windows is a built-in function), pre-defined functions for extracting goals.  * **As flexible as programming and as easy as IDE.** Lambdo is positioned between (Python) programming and interactive environments (like KNIME)    # Contents    * [Why Lambdo?](#why-lambdo)    * [Why feature engineering?](#why-feature-engineering)    * [Uniting feature engineering with data mining](#uniting-feature-engineering-with-data-mining)    * [Any transformation model has an automatic training procedure](#any-transformation-model-has-an-automatic-training-procedure)    * [Columns first: Column-orientation as a basis for feature engineering](#columns-first-column-orientation-as-a-basis-for-feature-engineering)    * [Time series first: Time series analysis and forecasting made easy](#time-series-first-time-series-analysis-and-forecasting-made-easy)    * [Getting started with Lambdo](#getting-started-with-lambdo)    * [Workflow definition](#workflow-definition)      * [Workflow structure](#workflow-structure)      * [Imports](#imports)    * [Table definition](#table-definition)      * [Table population function](#table-population-function)      * [Column filter](#column-filter)      * [Row filter](#row-filter)    * [Column definition](#column-definition)      * [Column evaluation function](#column-evaluation-function)      * [Function scopes](#function-scopes)      * [Training a model](#training-a-model)    * [Examples and analysis templates](#examples-and-analysis-templates)    * [Example 1: Input and output](#example-1-input-and-output)    * [Example 2: Record-based features](#example-2-record-based-features)    * [Example 3: User-defined record-based features](#example-3-user-defined-record-based-features)    * [Example 4: Table-based features](#example-4-table-based-features)    * [How to install](#how-to-install)    * [Install from source code](#install-from-source-code)    * [Install from package](#install-from-package)    * [How to test](#how-to-test)    * [How to use](#how-to-use)    # Why Lambdo?    ## Why feature engineering?    In many cases, defining good features is more important than choosing and tuning a machine learning algorithm to be applied to the data. Hence, the quality of the data analysis result depends more on the quality of the generated features than on the machine learning model.    Such high importance of having good features is explained by the following factors:    *	It is a quite rare situation when you have enough data and even if you have it then it then probably you do not have enough computing resources to process it. In this situation, manually defined or automatically mined features compensate this lack of data or computing resources to process it. Essentially, we combine expert systems with data mining.    *	Feature engineering is a mechanism of creating new levels of abstraction in knowledge representation because each (non-trivial) feature extract and makes explicit some piece of knowledge hidden in the data. It is almost precisely what deep learning is intended for. In this sense, feature engineering does what hidden layers of a neural network do or what the convolutional layer of a neural network does.    ## Uniting feature engineering with data mining    Let us assume that we want to compute moving average for a stock price. For each record, we compute an average value of this and some previous prices. This operation is interpreted as a transformation which generates a new feature stored as a column. Its result is defined by one parameter: window size (the number of days to be averaged including this day), and this number is essentially our transformation model.    Now let us assume that we want to add a second column which stores prices for tomorrow predicted using some algorithm from this and some previous values. We could develop a rather simple model which extrapolates price using previous values. This forecasting model, when applied, will also generate a new column with some values. Its result could depend on how many previous values are used by the extrapolation algorithm and this number is essentially our forecasting model.    An important observation here is that there is no difference between generating a new feature using some transformation model and generating a prediction using some forecast model. Technically, these are simply some transformations using some parameters, and these parameters are referred to as a model. Although there exist some exceptions where this analogy does not work, Lambdo assumes that it is so and follows the principle that    > Both generating a feature and applying a machine learning algorithm are data transformations parameterized by their specific models    Lambdo simply does not distinguish between them by assuming that any transformation that needs to be done is described by its (Python) function name and a (Python) model object. Lambdo will execute this function but it is unaware of its purpose. It can be a procedure for extracting dates from a string or it can be a prediction using a deep neural network model. In all these cases, the function will add new (derived) column to the existing table.    ## Any transformation model has an automatic training procedure    One difference between feature generation and machine learning is that machine learning models cannot exist without an algorithm for their training - the whole idea of machine learning is that models are learned automatically from data rather than defined manually. On the hand, features can be defined either manually or learned from data. Since Lambdo is intended for unifying feature engineering and machine learning, it makes the following assumption:    > Any transformation has an accompanying function for generating its models    This means that first we define some transformation which is supposed to be applied to the data and produce a new feature or analysis result. However, the model for this transformation can be generated automatically (if necessary) before applying it. For example, even for computing moving averages an important question is what window size to choose, and instead of guessing we can delegate this question to the training procedure (which could use auto-regression to choose the best window size). Importantly, both procedures – applying a transformation and training its model – are part of the same workflow where they can be intermediate nodes and not only the final predicting node.    ## Columns first: Column-orientation as a basis for feature engineering    Assume that we compute a moving average for a time series. The result of this operation is a new column. Now assume that we apply a clustering algorithm to records from a data set. In this case the result is again a new column. In fact, generating new columns is opposed to generating a new table and we can see these two operations in many data processing and data analysis approaches. Formally, we distinguish between set operations and operations with functions. Lambdo uses the following principle:    > Lambdo workflow consists of a graph of table definitions and each tables consists of a number of column definitions    A table definition describes how some set is populated using data in already existing tables. A typical table definition is a read/write operation or resampling time series or pivoting. A column definition describes how new values are evaluated using data in other columns in this and other tables.    This approach relies on the principles of the concept-oriented data model which also underlies [Bistro](https://github.com/asavinov/bistro) – an open source system for batch and stream data processing.    ## Time series first: Time series analysis and forecasting made easy    Most existing machine learning algorithms are not time-aware and they cannot be directly applied to time series. Although Lambdo is a general purpose workflow engine which can be applied to any data, its functionality was developed with time series analysis and forecasting in mind. Therefore, it includes many mechanisms which make feature engineering much easier when working with time series:    *	Easily defining a family of columns which are features with only minor changes in their definitions. A typical but wide spread example is a family of features which use different window sizes.    *	Predefined column definitions for typical goal functions to be predicted or used for training intermediate features. Note that time series analysis is almost always supervised learning but there are different formulations for what we want to forecast.    *	Lambdo is window-aware workflow engine and for any transformation it is necessary to define its scope which means the number of rows the function will be applied to. This parameter is essentially the length of the history (number of previous records to be processed by the function).    *	Lambdo is going to be also object-aware which means it can partition the whole data set according to the value of the selected column interpreted as an object. This allows us to analyze data coming from multiple objects like devices, sensors, stock symbols etc.    *	Easy control of when to train which nodes. The problem here is that frequently a workflow has to be re-trained periodically but we do not want to re-train all nodes. This mechanism allows us to specify criteria for re-training its models.    # Getting started with Lambdo    Lambdo implements a novel *column-oriented* approach to data analysis by unifying various kinds of data processing: feature engineering, data transformations and data mining.    ## Workflow definition    ### Workflow structure    Lambdo is intended for processing data by using two types of transformations:    * Table transformations produce a new table given one or more input tables.  * Column transformations produce a new column in the table given one or more input columns.    A workflow is a number of table definitions each having a number of column definitions. These tables and columns compose a graph where edges are dependencies. If an element (table or column) in this graph has another element as its inputs then this dependency is an edge. If an element (table or column) does not have inputs then it is a data source. If an element does not have dependents then it is a data sink.    This data processing logic of Lambdo is represented in JSON format and stored in a workflow file and having the following structure:     ```javascript  {    ""tables"": [      ""table"": { ""function"": ""my_table_func_1"", ""columns"": [...] }      ""table"": {        ""function"": ""my_table_func_2"",        ""columns"": {          ""column"": { ""function"": ""my_column_func_1"", ... }          ""column"": { ""function"": ""my_column_func_2"", ... }        }      ""table"": { ""function"": ""my_table_func_3"", ""columns"": [...] }    ]  }  ```    Each table and column definition has to specify a (Python) function name which will actually do data processing. Table definition will use functions for data population. Column definitions will use functions for evaluating new columns. When Lambo executes a workflow, it populates tables according to their definitions and evaluates columns (within tables) according to their definitions. Here it is important to understand that tables are used for set operations while columns are used for operations with mathematical functions.    ### Imports    Data processing in Lambdo relies on Python functions which do real data processing. Before these functions can be executed, they have to be imported. The location of functions to be imported is specified in a special field. For example, if the functions we want to use for data processing are in the `examples.example1` module and `datetime` module then we specify them as follows:    ```json  {    ""id"": ""Example 1"",    ""imports"": [""examples.example1"", ""datetime""],  }  ```    Now we can use functions from these modules in the workflow table and column definitions.    ## Table definition    ### Table population function    A table definition has to provide some Python function which will *populate* this table. This function can be standard (built-in) Python function or it could be part of an imported module like `scale` function from the `sklearn.preprocessing` module or `BaseLibSVM.predict` function from the `sklearn.svm.base` module. Functions can be also defined for this specific workflow if they encode some domain-specific feature definition.    For example, if we want to read data then such a table could be defined as follows:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""inputs"": [],    ""model"": {      ""filepath_or_buffer"": ""my_file.csv"",      ""nrows"": 100    }  }  ```    Here we used a standard function from `pandas` but it could be any other function which returns a `DataFrame`.    Any function takes parameters which are referred to as a *model* and passed to the function. In the above example, we passed input file name and maximum umber of records to be read.    ### Column filter    Frequently, it is necessary to generate some intermediate features (columns) which are not needed for the final analysis. Such features should be removed from the table. This can be done by specifying a *column filter* and this selection of necessary columns is performed always when all features within this table have been generated.    We can specify a list of columns, which have to be selected and passed to the next nodes in the graph:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""column_filter"": [""Open"", ""Close""]  },  ```    Alternatively, we can specify columns, which have to be excluded from the selected features to be passed to the next nodes:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""column_filter"": {""exclude"": [""Date""]}  },  ```    The next table will then receive a table with all columns generated in this table excepting the `Date` column (which contains time stamps not needed for analysis).    ### Row filter    Not all records in the table need to be analyzed and such records can be excluded before the table is passed to the next node for processing. Records to be removed are specified in the row filter which provides several methods for removal.    Many analysis algorithms cannot deal with `NaN` values and the simplest way to solve this problem is to remove all records which have at least one `NaN`:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""row_filter"": {""dropna"": true}  },  ```    The `dropna` can also specify a list of columns and then only the values of these columns will be checked.    Another way to filter rows is to specify columns with boolean values and then the result table will retain only rows with `True` in these columns:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""row_filter"": {""predicate"": [""Selection""]}  },  ```    A column with binary values can be defined precisely as any other derived column using a function, which knows which records are needed for analysis. (This column can be then removed by using a column filter.)    It is also possible to reandomly shuffle records by specifying the portion we want to keep in the table. This filter will keep only 80% of randomly selected records:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""row_filter"": {""sample"": {""frac"": 0.8}  },  ```    You can specify `""sample"":true` if all records have to be shuffled.    The records can be also selected by specifying their integer position: start, end (exclusive) and step. The following filter will select every second record:    ```json  {    ""id"": ""My table"",    ""function"": ""pandas:read_csv"",    ""row_filter"": {""slice"": {""start"": 0, ""step"": 2}  },  ```    ## Column definition    ### Column evaluation function    A column definition specifies how its values are computed from the values stored in other columns. The way these values are computed is implemented by some Python function which can be either a standard Python function, a function from some existing module or a user-defined function. Lambdo simply gets the name of this function from the workflow and then calls it to generate this column values.    A function is specified as a pair of its module and function name separated by a colon:    ```javascript  ""function"": ""my_module:my_function""  ```    It is assumed that the first argument of the function is data to be processed and the second argument is a model which parameterizes this transformation. Note however that some function can take other parameters and also the type of these arguments can vary.    ### Function scopes    What data a transformation function receives in its first argument? There are different options starting from a single value and ending with a whole input table. This is determined by the column definition parameter called `scope` which takes the following values:    * Scope `one` or `1` means that Lambdo will apply this function to every row of the input table and the function is expected to return a single value stored as the column value for this record. Type of data passed to the function depends on how many columns the `input` has.    * If `input` has only one column then the function will receive a single value.    * If `input` has more than 1 columns then the function will receive a `Series` object with their field values.  * Scope `all` means that the function will be applied to all rows of the table, that is, there will be one call and the whole table will be passed as a parameter. Type of the argument is `DataFrame`.  * Otherwise the system assume that the function has to be applied to all subsets of the table rows, called windows. Size of the window (number of records in one group) is scope value. For example, `scope: 5` means that each window will consists of 5 records. Type of this group depends on the number of columns in the `input`:     * If `input` has only one column then the function will receive a `Series` of values.    * If `input` has more than 1 columns then the function will receive a `DataFrame` object with the records from the group.    ### Training a model    A new feature is treated as a transformation, which results in a new column with the values derived from the data in other columns. This transformation is performed using some *model*, which is simply a set of parameters. A model can be specified explicitly by-value if we know these parameters. However, model parameters can be derived from the data using a separate procedure, called *training*. The transformation is then applied *after* the training.    How a model is trained is specified in a block within a column definition:    ```json  {    ""id"": ""Prediction"",    ""function"": ""examples.example1:gb_predict"",    ""scope"": ""all"",    ""inputs"": {""exclude"": [""Labels""]},    ""train"": {      ""function"": ""examples.example1:gb_fit"",      ""model"": {""n_estimators"": 500, ""max_depth"": 4, ""min_samples_split"": 2, ""learning_rate"": 0.01, ""loss"": ""ls""},      ""outputs"": [""Labels""]    }  }  ```    Here we need to specify a function which will perform such a training: `""function"": ""examples.example1:gb_fit""`. The training function also needs its own hyper-parameters, for example: `""model"": {""max_depth"": 4}`. Finally, the training procedure (in the case of supervised learning) needs labels: `""outputs"": [""Labels""]`. Note also that excluded the `Labels` from the input so that they are not used as features for training.    Lambdo will first train a model by using the input data and then use this model for prediction.    # Examples and analysis templates    ## Example 1: Input and output    Assume that the data is stored in a CSV file and we want to use this data to produce new features or for data analysis. Loading data is a table population operation which is defined in some table node of the workflow. How the table is populated depends on the `function` of this definition. In our example, we want to re-use a standard `pandas` for loading CSV files. Such a table node is defined as follows:    ```json  {    ""id"": ""Source table"",    ""function"": ""pandas:read_csv"",    ""inputs"": [],    ""model"": {      ""filepath_or_buffer"": ""my_file.csv"",      ""nrows"": 100    }  }  ```    After executing this node, it will store the data from this file. We could also use any other function for loading or generating data. For example, it could a function which produces random data.    Data output can also be performed by using a standard `pandas` function:    ```json  {    ""id"": ""Source table"",    ""function"": ""pandas:DataFrame.to_csv"",    ""inputs"": ""Source table"",    ""model"": {      ""path_or_buf"": ""my_output.csv"",      ""index"": false    }  }  ```    Note that the `inputs` fields points to the table which needs to be processed. The result of its execution will be a new CSV file.    Run this example from command line by executing:    ```console  $ lambdo examples/example1.json  ```    Another useful standard function for storing a table is `to_json` with a possible model like `{""path_or_buf"": ""my_file.json.gz"", ""orient""=""records"", ""lines""=True, ""compression""=""gzip""}` (the file will be compressed). To read a JSON file into a table, use the function `read_json`.    ## Example 2: Record-based features    The table definition where we load data has no column definitions. However, we can easily add them. A typical use case is where we want to change the format or data type of some columns. For example, if the source file has a text field with a time stamp then we might want to convert it the `datetime` object which is done by defining a new column:    ```json  {      ""id"": ""Source table"",      ""function"": ""pandas:read_csv"",      ""inputs"": [],      ""model"": {          ""filepath_or_buffer"": ""my_file.csv""      },      ""columns"": [        {            ""id"": ""Datetime"",            ""function"": ""pandas.core.tools.datetimes:to_datetime"",            ""scope"": ""one"",            ""inputs"": ""Date""        }      ]  }  ```    The most important parameter in this column definition is `scope`. If it is `one` (or `1`) then the function will be applied to each row of the table. In other words, this function will get *one* row as its first argument. After evaluating this column definition, the table will get a new column `Datetime` storing time stamp objects (which are more convenient for further data transformations).     If we do not need the source (string) column then the new column may get the same name `""id"": ""Date""` and it will overwrite the already existing column. Also, if the source column has a non-standard format then it can be specified in the model `""model"": {""format"": ""%Y-%m-%d""}` which will be passed to the function.    ```json  {      ""id"": ""Datetime"",      ""function"": ""pandas.core.tools.datetimes:to_datetime"",      ""scope"": ""one"",      ""inputs"": ""Date"",      ""model"": {""format"": ""%Y-%m-%d""}  }  ```    If some source or intermediate columns are not needed for later analysis then they can be excluded by adding a column filter to the table definition where we can specify columns to retain as a list like `""column_filter"": [""Open"", ""High"", ""Low"", ""Close"", ""Volume""]` or to exclude like `""column_filter"": {""exclude"": [""Adj Close""]}`.    Execute this workflow as follows:    ```console  $ lambdo examples/example2.json  ```    ## Example 3: User-defined record-based features    Let us now assume that we want to analyze the difference between high and low daily prices and hence we need to derive such a column from two input columns `High` and `Low`. There is no such a standard function and hence we need to define our own domain-specific function which will return the derived value given some input values. This user-defined function is defined in a Python source file:    ```python  def diff_fn(X):      """"""      Difference between first and second fields of the input Series.      """"""      if len(X) < 2: return None      if not X[0] or not X[1]: return None      if pd.isna(X[0]) or pd.isna(X[1]): return None      return X[0] - X[1]  ```    This function will get a Series object for each row of the input table. For each pair of numbers it will return their difference.    In order for the workflow to load this function definition, we need to specify its location in the workflow:    ```json  {    ""id"": ""Example 3"",    ""imports"": [""examples.example3""],  }  ```    The column definition, which uses this function is defined as follows:    ```json  {    ""id"": ""diff_high_low"",    ""function"": ""examples.example3:diff_fn"",    ""inputs"": [""High"", ""Low""]  }  ```    We specified two columns which have to be passed as parameters to the user-defined functions: `""inputs"": [""High"", ""Low""]`. The same function could be also applied to other column where we want to find difference. This function will be called for each row of the table and its return values will be stored in the new column.    Each function including this one can accept additional arguments via its `model` (similar to how we passed data format in the previous example).    ## Example 4: Table-based features    A record-based function with scope 1 will be applied to each row of the table and get this row fields in arguments. There will be as many calls as there are rows in the table. If `scope` is equal to `all` then the function will be called only one time and it will get all rows it has to process. Earlier, we described how string dates can be converted to datetime object by applying the transformation function to each row. The same result can be obtained if we pass the whole column to the transformation function. The only field that has to be changed in this definition is `scope`, which is now equals `all`:    ```json  {    ""id"": ""Datetime"",    ""function"": ""pandas.core.tools.datetimes:to_datetime"",    ""scope"": ""all"",    ""inputs"": ""Date"",    ""model"": {""format"": ""%Y-%m-%d""}  }  ```    The result will be the same but this column will be evaluated faster.    Such functions which get all rows have to know how to iterate over the rows and they return one column rather than a single value. Such functions can apply any kind of computations because they have the whole data set. Therefore, such functions are used for more complex transformations including forecasts using some model.    Another example of applying a function to all rows is shifting a column. For example, if our goal is forecasting the closing price tomorrow then we need shift this column one step backwards:    ```json  {    ""id"": ""Close_Tomorrow"",    ""function"": ""pandas.core.series:Series.shift"",    ""scope"": ""all"",    ""inputs"": [""Close""],    ""model"": {""periods"": -1}  }  ```    Values of this new column will be equal to the value of the specified input column taken from the next record.    ## Example 5: Window-based rolling aggregation    Lambdo is focused on time series analysis where important pieces of behavior (features) are hidden in sequences of events. Therefore, one of the main goals of feature engineering is making such features explicitly as attribute values by extracting data from the history. Normally it is done by using rolling aggregation where a function is applied to some historic window of recent events and returns one value, which characterizes the behavior. In Lambdo, it is possible to specify an arbitrary (Python) function, which encodes some domain-specific logic specific for this feature.    For window-based columns, the most important parameter is `scope` which is an integer specifying the number of events to be passed to the function as its first argument. For example, if `""scope"": 10` then the Python function will always get 10 elements of the time series: this element and 9 previous elements. It can be a series of 10 values or a sub-table with 10 rows depending on other parameters. The function then analyzes these 10 events and returns one single value, which is stored as a value of this derived column.    Assume that we want to find running average volume for 10 days. This can be done as follows:    ```json  {    ""id"": ""mean_Volume"",    ""function"": ""numpy.core.fromnumeric:mean"",    ""scope"": 10,    ""inputs"": [""Volume""]  }  ```    Each value of the derived column `mean_Volume` will store average volume for the last 10 days.     Note that instead of `mean` we could use arbitrary Python function including user-defined functions. Such a function will be called for each row in the table and it will get 10 values of the volume for the last 10 days (including this one). For example, we could write a function which counts the number of peaks (local maximums) in volume or we could find some more complex pattern. Also, if `inputs` has more columns then the functions will get a data frame as input with the columns specified in `inputs`.    Typically in time series analysis we use several running aggregations with different window sizes. Such columns can can be defined independently but their definitions will differ only in one parameter: `scope`. In order to simplify such definitions Lambdo allows for defining a base definition and extensions. For example, if we want to define average volumes with windows 10, 5 and 2 then this can be done by definition scopes in the extensions:    ```json  {    ""id"": ""mean_Volume"",    ""function"": ""numpy.core.fromnumeric:mean"",    ""inputs"": [""Volume""],    ""extensions"": [      {""scope"": ""10""},      {""scope"": ""5""},      {""scope"": ""2""}    ]  }  ```    The number of columns defined is equal to the number of extensions, that is, three columns in this examples. The names of the columns by default will be `id` of this family definition and the suffix `_N` where `N` is an index of the extension. In our example, three columns will be added after evaluating this definition: `mean_Volume_0`, `mean_Volume_1` and `mean_Volume_2`.    Moving averages can produce empty values, which we want to exclude from analysis, for example, because other analysis algorithms are not able to process them. Each table definition allows for filtering its records at the end before the table data is passed to the next node. In order to exclude all rows with empty values we add this block to the end of the table definition:    ```json  ""row_filter"": {""dropna"": true}  ```    Run this example and check out its result which will contain three new columns with moving averages of the volume:    ```console  $ lambdo examples/example5.json  ```    ## Example 6: Training a model    All previous examples assumed that a column definition is treated as a data transformation performed via a Python function which also takes parameters of this transformation, which is called a model. The model describes how specifically the transformation has to be performed. One of the main features of Lambdo is that it treats such transformations as applying a data mining model. In other words, the result of applying a data mining model is a new column. For example, this column could store the cluster number this row belongs to or likelihood this object (row) is some object. What is specific to data mining is that its models are not specified explicitly but rather are trained from the data. This possibility to train a model (as opposed to providing an explicit model) is provided by Lambdo for any kind of column definition. If a model is absent and the training function is provided, then the model will be trained before it is applied to the data.    How a model has to be train is specified in a workflow using the `train` block of a column definition:    ```json  ""columns"": [    {      ""id"": ""My Column"",      ""function"": ""my_transform_function"",        ""train"": {        ""function"": ""my_train_function"",        ""model"": {""hyper_param"": 123}      }    }  ]  ```    This column definition does not have a model specified but it does specify a function for generating (training) such a model. It also provides a hyper-model for this training function which specifies how to do training. The training function gets the data and the hyper-model in its arguments and returns a trained model which is then used for generating the column data.    Here is an example of a column definition which trains and applies a gradient boosting data mining model:    ```json  {    ""id"": ""Close_Tomorrow_Predict"",    ""function"": ""examples.example6:gb_predict"",    ""scope"": ""all"",    ""data_type"": ""ndarray"",    ""inputs"": {""exclude"": [""Date"", ""Close_Tomorrow""]},    ""train"": {      ""function"": ""examples.example6:gb_fit"",      ""row_filter"": {""slice"": {""end"": 900}},      ""model"": {""n_estimators"": 500, ""max_depth"": 4, ""min_samples_split"": 2, ""learning_rate"": 0.01, ""loss"": ""ls""},      ""outputs"": [""Close_Tomorrow""]    }  }  ```    This definition has the following new elements:    * `data_type` field indicates that the functions accepts `ndarray` and not `DataFrame`.  * `inputs` field allows us to select columns we want to use. We want to exclude the `Date` column because its data type is not supported and `Close_Tomorrow` column which is a goal  * `row_filter` is used to limit the number of records for training  * `model` in the training section provides hyper-parameters for the training function  * `outputs` field specifies labels for the training procedure    Thus in this definition we want to use 900 records and all columns except for `Date` for training by using `Close_Tomorrow` as labels. The resulted model is then applied to *all* the data and the predictions are stored as the `Close_Tomorrow_Predict` column.    Run this example and check out its results in the last column with predictions:    ```console  $ lambdo examples/example6.json  ```    ## Example 7: Reading and writing models    If a column (feature definition) model is not specified then Lambdo will try to generate it by using the train function. This trained model will be then applied to the input data. However, after finishing executing the workflow, this model will be lost. In many scenarios we would like to retain some trained models. In particular, it is necessary if we explicitly separate two phases: training and predicting. The goal of the training phase is to generate a prediction model by using some (typically large amount of) input data. The model resulted from the training phase can be then used for prediction (by this same workflow because we want to have the same features). Therefore, we do not want to train it again but rather load it from the location where it was stored during training.    The mechanism of storing and loading models is implemented by Lambdo as one of its main features. The idea is that workflow field values can specified either by-value or by-reference. Specifying a value by-value means that we simply provide a JSON object for the corresponding JSON field. However, we can also point to values by providing a reference which will be used by the system for reading or writing it.    The general rule is that if a JSON field value is a string which starts from `$` sign then it is a reference. If we want to store some model in a file (and not directly in the workflow) then the location is specified as follows:    ```json  ""model"": ""$file:my_model.pkl""  ```    Now Lambdo will try to load this model from the file. If it succeeds then the model will be used for transformation (no training needed). If it fails, for example, the file does not exist, then Lambdo will generate this model by using the training function, store the model in the file and then use it for generating the column as usual.    Example 7 has one small modification with respect to Example 6: its trained model is stored in a file. As a result, we can apply this workflow to a large data set for training, and then this same workflow with the present model can be applied to smaller data sets for prediction.    ## Example 8: Joining input tables    TBD    # How to install    ## Install from source code    Check out the source code and execute this command in the project directory (where `setup.py` is located):    ```console  $ pip install .  ```    Or alternatively:    ```console  $ python setup.py install  ```    ## Install from package    Create wheel package:    ```console  $ python setup.py bdist_wheel  ```    The `whl` package will be stored in the `dist` folder and can then be installed using `pip`.    Execute this command by specifying the `whl` file as a parameter:    ```console  $ pip install dist\lambdo-0.1.0-py3-none-any.whl  ```    # How to test    Run tests:    ```console  $ python -m unittest discover -s ./tests  ```    or    ```console  $ python setup.py test  ```    # How to use    If you execute `lambdo` without any options then it will return a short help by describing its usage.    A workflow file is needed to analyze data. Very simple workflows for test purposes can be found in the `tests` directory. More complex workflows can be found in the `examples` directory. To execute a workflow start `lambdo` with this workflow file name as a parameter:    ```console  $ lambdo examples/example1.json  ```    The workflow reads a CSV file, computes some features from the time series data, trains a model by applying it them to the data and finally writes the result to an output CSV file. """
Big data;https://github.com/UnderstandLingBV/Tuktu;"""# Tuktu - Big Data Science Swiss Army Knife    [![Join the chat at https://gitter.im/UnderstandLingBV/Tuktu](https://badges.gitter.im/UnderstandLingBV/Tuktu.svg)](https://gitter.im/UnderstandLingBV/Tuktu?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Build Status](https://travis-ci.org/UnderstandLingBV/Tuktu.svg?branch=master)](https://travis-ci.org/UnderstandLingBV/Tuktu) [![Release](https://img.shields.io/github/release/UnderstandLingBV/Tuktu.svg)](https://github.com/UnderstandLingBV/Tuktu/releases/latest) [![Docker Automated build](https://img.shields.io/docker/automated/understandling/tuktu.svg?maxAge=2592000)](https://hub.docker.com/r/understandling/tuktu)    Tuktu is created and officially maintained by [UnderstandLing Intellect](http://www.understandling.com).    ![UnderstandLing Logo](images/ul.png)    # Documentation    For documentation on Tuktu, please see [http://www.tuktu.io](http://www.tuktu.io).    # Docker   Start the container (have [docker](https://www.docker.com/) installed) by running:    - docker pull understandling/tuktu  - docker run -p 9000:9000 -p 2552:2552 --name tuktu -d understandling/tuktu"""
Big data;https://github.com/rakam-io/rakam;"""[![Build Status](https://travis-ci.org/rakam-io/rakam.svg?branch=master)](https://travis-ci.org/rakam-io/rakam)  [<img alt=""Deploy"" src=""https://www.herokucdn.com/deploy/button.png"" height=""21"">](https://dashboard.heroku.com/new?button-url=https%3A%2F%2Fgithub.com%2Frakam-io%2Frakam&template=https%3A%2F%2Fgithub.com%2Frakam-io%2Frakam)       Rakam  =======    Rakam is an analytics platform that allows you to create your analytics services.    Features / Goals  ------------  Rakam is a modular analytics platform that gives you a set of features to create your own analytics service.    Typical workflow of using Rakam:  * Collect data from multiple sources with **[trackers, client libraries, webhooks, tasks etc.](https://docs.rakam.io/docs/collect-data)**  * Enrich and sanitize your event data with **[event mappers](https://docs.rakam.io/docs/event-enrichment)**  * Store data in a data warehouse to analyze it later. (Postgresql, Snowflake, S3 etc.)  * Analyze your event data with your SQL queries and integrated rich analytics APIs with Rakam Cloud (**[funnel, retention, segmentation reports](https://docs.rakam.io/docs/core-cencept)**  * **[Develop your own modules](https://docs.rakam.io/docs/developing-modules)** for Rakam to customize it for your needs.    We also provide user interface for Rakam as a separate product called [Rakam UI](https://app.rakam.io). You can create custom reports with SQL, dashboards, funnel and retention reports via [Rakam UI](https://beta.rakam.io).    All these features come with a single box, you just need to specify which modules you want to use using a configuration file (config.properties) and Rakam will do the rest for you.  We also provide cloud deployment tools for scaling your Rakam cluster easily.    Deployment  ----------    If your event data-set can fit in a single server, we recommend using Postgresql backend. Rakam will collect all your events in row-oriented format in a Postgresql node. All the features provided by Rakam are supported in Postgresql deployment type. Please note that we support Postgresql 11 because we're using new features such as partitioning and BRIN indexes for performance.    However Rakam is designed to be highly scalable in order to provide a solution for high work-loads. You can configure Rakam to send events to a distributed commit-log such as Apache Kafka or Amazon Kinesis in serialized Apache Avro format and process data in PrestoDB workers and store them in a distributed filesystem in a columnar format.    ### Heroku    You can deploy Rakam to Heroku using Heroku button, it uses Heroku Postgresql add-on for your app and uses Postgresql deployment type.    [![Deploy](https://www.herokucdn.com/deploy/button.png)](https://dashboard.heroku.com/new?button-url=https%3A%2F%2Fgithub.com%2Frakam-io%2Frakam&template=https%3A%2F%2Fgithub.com%2Frakam-io%2Frakam)    ### Docker    Run the following command to start a Postgresql server in docker container and Rakam API in your local environment.        docker run -d --name rakam-db -e POSTGRES_PASSWORD=dummy -e POSTGRES_USER=rakam postgres:10.1 && docker run --link rakam-db --name rakam -p 9999:9999 -e RAKAM_CONFIG_LOCK__KEY=mylockKey -e RAKAM_CONFIG_STORE_ADAPTER_POSTGRESQL_URL=postgres://rakam:dummy@rakam-db:5432/rakam buremba/rakam    After docker container is started, visit [http://127.0.0.1:9999](http://127.0.0.1:9999) and follow the instructions. You can also register your local Rakam API to Rakam BI at  [http://app.rakam.io](http://app.rakam.io)  or directly use Rakam API. You may also consult to [API documentation](https://api.rakam.io) for details of the API.    We also provide a docker-compose definition for a Postgresql backend. Create a `docker-compose.yml` with this definition and run the command `docker-compose -f docker-compose.yml up -d`.        version: '2.1'      services:        rakam-db:          image: postgres:11.4          environment:            - POSTGRES_PASSWORD=dummy            - POSTGRES_USER=rakam          healthcheck:            test: [""CMD-SHELL"", ""pg_isready""]            interval: 5s            timeout: 5s            retries: 3        rakam-api:          image: buremba/rakam          environment:            - RAKAM_CONFIG_STORE_ADAPTER_POSTGRESQL_URL=postgres://rakam:dummy@rakam-db:5432/rakam            - RAKAM_CONFIG_LOCK__KEY=mylockKey          ports:            - ""9999:9999""          depends_on:            rakam-db:              condition: service_healthy    You can set config variables for Rakam instance using environment variables. All properties in config.properties file can be set via environment variable `RAKAM_CONFIG_property_name_dots_replaced_by_underscore`.  For example, if you want to set `store.adapter=postgresql` you need to set environment variable `RAKAM_CONFIG_STORE_ADAPTER=postgresql`. Also the dash `-` is replaced by double underscore character `__`.   Therefore the environment variable `RAKAM_CONFIG_LOCK__KEY` corresponds to `lock-key` config property.     Dockerfile will generate `config.properties` file from environment variables in docker container that start with `RAKAM_CONFIG` prefix.    In order to set environment variables for container, you may use `-e` flag for for `docker run` but we advice you to set all environment variables in a file and use  `--env-file` flag when starting your container.    Then you can share same file among the Rakam containers. If Dockerfile can't find any environment variable starts with `RAKAM_CONFIG`, it tries to connect Postgresql instance created with docker-compose.    ### AWS (Terraform)    See [https://github.com/rakam-io/rakam-api-terraform-aws](https://github.com/rakam-io/rakam-api-terraform-aws).    Terraform installer is the recommended way to deploy Rakam in production because it automatically handles most of the complexity like fail over and load-balancing.    ### Custom  - Download Java 1.8 for your operating system.  - Download latest version from [Bintray](https://dl.bintray.com/buremba/maven/org/rakam/rakam) ([VERSION]/rakam-[VERSION]-.bundle.tar.gz) extract package.  - Modify `etc/config.properties` [(sample for Postgresql deployment type)](https://gist.github.com/buremba/ada247b0ce837cfd3a81a92a98629f1d) file and run `bin/launcher start`.  - The launcher script can take the following arguments: `start|restart|stop|status|run`.   `bin/launcher run` will start Rakam in foreground.    ### Building Rakam  You can try the master branch by pulling the source code from Github and building Rakam using Maven:    ##### Requirements  - Java 8  - Maven 3.2.3+ (for building)    ```sh  git clone https://github.com/rakam-io/rakam.git  cd rakam  mvn clean install package -DskipTests  ```    ##### Running the application locally  ```sh  rakam/target/rakam-*-bundle/rakam-*/bin/launcher.py run --config rakam/target/rakam-*-bundle/rakam-*/etc/config.properties  ```    Note that you need to modify `config.properties` file in order to be able to start Rakam. [(sample for Postgresql deployment type)](https://gist.github.com/buremba/ada247b0ce837cfd3a81a92a98629f1d)    ##### Running Rakam in your IDE    Since we already use Maven, you can import Rakam to your IDE using the root pom.xml file. We recommend using Intellij IDEA since the core team uses it when developing Rakam. Here is a sample configuration for executing Rakam in your IDE:    ```  Main Class: org.rakam.ServiceStarter  VM Options: -ea -Xmx2G -Dconfig=YOUR_CONFIG_DIRECTORY/config.properties  Working directory: $MODULE_DIR$  Use classpath of module: rakam  ```    ### Managed    We're also working for managed Rakam cluster, we will deploy Rakam to our AWS accounts and manage it for you so that you don't need to worry about scaling, managing and software updates. We will do it for you.  Please shoot us an email to `emre@rakam.io` if you want to test our managed Rakam service.    Web application  ------------  This repository contains Rakam API server that allows you to interact with Rakam using a REST interface. If you already have a frontend and developed a custom analytics service based on Rakam, it's all you need.    However, we also developed Rakam Web Application that allows you to analyze your user and event data-set but performing SQL queries, visualising your data in various charts, creating (real-time) dashboards and custom reports. You can turn Rakam into a analytics web service similar to [Mixpanel](https://mixpanel.com), [Kissmetrics](https://kissmetrics.com) and [Localytics](https://localytics.com) using the web application. Otherwise, Rakam server is similar to [Keen.io](https://keen.io) with SQL as query language and some extra features.    Another nice property of Rakam web application is being BI `(Business Intelligence)` tool. If you can disable collect APIs and connect Rakam to your SQL database with JDBC adapter and use Rakam application to query your data in your database. Rakam Web Application has various charting formats, supports parameterized SQL queries, custom pages that allows you to design pages with internal components.    Contribution  ------------  Currently I'm actively working on Rakam. If you want to contribute the project or suggest an idea feel free to fork it or create a ticket for your suggestions. I promise to respond you ASAP.  The purpose of Rakam is being generic data analysis tool which can be a solution for many use cases. Rakam still needs too much work and will be evolved based on people's needs so your thoughts are important.    Acknowledgment  --------------  [![YourKit](https://www.yourkit.com/images/yklogo.png)](https://www.yourkit.com/java/profiler/index.jsp)    We use YourKit Java Profiler in order to monitor the JVM instances for identifing the bugs and potential bottlenecks. Kudos to YourKit for supporting Rakam with your full-featured Java Profile! """
Big data;https://github.com/plotly/plotly.js;"""<a href=""https://plotly.com/javascript/""><img src=""https://images.plot.ly/logo/plotlyjs-logo@2x.png"" height=""70""></a>    [![npm version](https://badge.fury.io/js/plotly.js.svg)](https://badge.fury.io/js/plotly.js)  [![circle ci](https://circleci.com/gh/plotly/plotly.js.png?&style=shield&circle-token=1f42a03b242bd969756fc3e53ede204af9b507c0)](https://circleci.com/gh/plotly/plotly.js)  [![MIT License](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://github.com/plotly/plotly.js/blob/master/LICENSE)    [Plotly.js](https://plotly.com/javascript) is a standalone Javascript data visualization library, and it also powers the Python and R modules named `plotly` in those respective ecosystems (referred to as [Plotly.py](https://plotly.com/python) and [Plotly.R](http://plotly.com/r)).    Plotly.js can be used to produce dozens of chart types and visualizations, including statistical charts, 3D graphs, scientific charts, SVG and tile maps, financial charts and more.    <p align=""center"">      <a href=""https://plotly.com/javascript/"" target=""_blank"">          <img src=""https://raw.githubusercontent.com/cldougl/plot_images/add_r_img/plotly_2017.png"">      </a>  </p>    [Contact us](https://plotly.com/products/consulting-and-oem/) for Plotly.js consulting, dashboard development, application integration, and feature additions.    ## Table of contents    * [Load as a node module](#load-as-a-node-module)  * [Load via script tag](#load-via-script-tag)  * [Bundles](#bundles)  * [Alternative ways to load and build plotly.js](#alternative-ways-to-load-and-build-plotlyjs)  * [Documentation](#documentation)  * [Bugs and feature requests](#bugs-and-feature-requests)  * [Contributing](#contributing)  * [Notable contributors](#notable-contributors)  * [Copyright and license](#copyright-and-license)  * [Community](#community)    ---  ## Load as a node module  Install [a ready-to-use distributed bundle](https://github.com/plotly/plotly.js/blob/master/dist/README.md)  ```sh  npm i --save plotly.js-dist-min  ```    and use import or require in node.js  ```js  // ES6 module  import Plotly from 'plotly.js-dist-min'    // CommonJS  var Plotly = require('plotly.js-dist-min')  ```    You may also consider using [`plotly.js-dist`](https://www.npmjs.com/package/plotly.js-dist) if you prefer using an unminified package.    ---  ## Load via script tag    ### The script HTML element  > In the examples below `Plotly` object is added to the window scope by `script`. The `newPlot` method is then used to draw an interactive figure as described by `data` and `layout` into the desired `div` here named `gd`. As demonstrated in the example above basic knowledge of `html` and [JSON](https://en.wikipedia.org/wiki/JSON) syntax is enough to get started i.e. with/without JavaScript! To learn and build more with plotly.js please visit [plotly.js documentation](https://plotly.com/javascript).    ```html  <head>      <script src=""https://cdn.plot.ly/plotly-2.11.1.min.js""></script>  </head>  <body>      <div id=""gd""></div>        <script>          Plotly.newPlot(""gd"", /* JSON object */ {              ""data"": [{ ""y"": [1, 2, 3] }],              ""layout"": { ""width"": 600, ""height"": 400}          })      </script>  </body>  ```    Alternatively you may consider using [native ES6 import](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules) in the script tag.  ```html  <script type=""module"">      import ""https://cdn.plot.ly/plotly-2.11.1.min.js""      Plotly.newPlot(""gd"", [{ y: [1, 2, 3] }])  </script>  ```    Fastly supports Plotly.js with free CDN service. Read more at <https://www.fastly.com/open-source>.    ### Un-minified versions are also available on CDN  While non-minified source files may contain characters outside UTF-8, it is recommended that you specify the `charset` when loading those bundles.  ```html  <script src=""https://cdn.plot.ly/plotly-2.11.1.js"" charset=""utf-8""></script>  ```    > Please note that as of v2 the ""plotly-latest"" outputs (e.g. https://cdn.plot.ly/plotly-latest.min.js) will no longer be updated on the CDN, and will stay at the last v1 patch v1.58.5. Therefore, to use the CDN with plotly.js v2 and higher, you must specify an exact plotly.js version.    To support MathJax, you could load either version two or version three of MathJax files, for example:  ```html  <script src=""https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG.js""></script>  ```    ```html  <script src=""https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-svg.js""></script>  ```    > When using MathJax version 3, it is also possible to use `chtml` output on the other parts of the page in addition to `svg` output for the plotly graph.  Please refer to `devtools/test_dashboard/index-mathjax3chtml.html` to see an example.      ## Bundles  There are two kinds of plotly.js bundles:  1. Complete and partial official bundles that are distributed to `npm` and the `CDN`, described in [the dist README](https://github.com/plotly/plotly.js/blob/master/dist/README.md).  2. Custom bundles you can create yourself to optimize the size of bundle depending on your needs. Please visit [CUSTOM_BUNDLE](https://github.com/plotly/plotly.js/blob/master/CUSTOM_BUNDLE.md) for more information.    ---  ## Alternative ways to load and build plotly.js  If your library needs to bundle or directly load [plotly.js/lib/index.js](https://github.com/plotly/plotly.js/blob/master/lib/index.js) or parts of its modules similar to [index-basic](https://github.com/plotly/plotly.js/blob/master/lib/index-basic.js) in some other way than via an official or a custom bundle, or in case you want to tweak the default build configurations of `browserify` or `webpack`, etc. then please visit [`BUILDING.md`](https://github.com/plotly/plotly.js/blob/master/BUILDING.md).    ---  ## Documentation    Official plotly.js documentation is hosted at [https://plotly.com/javascript](https://plotly.com/javascript).    These pages are generated by the Plotly [graphing-library-docs repo](https://github.com/plotly/graphing-library-docs) built with [Jekyll](https://jekyllrb.com/) and publicly hosted on GitHub Pages.  For more info about contributing to Plotly documentation, please read through [contributing guidelines](https://github.com/plotly/graphing-library-docs/blob/master/README.md).    ---  ## Bugs and feature requests    Have a bug or a feature request? Please [open a Github issue](https://github.com/plotly/plotly.js/issues/new) keeping in mind the [issue guidelines](https://github.com/plotly/plotly.js/blob/master/.github/ISSUE_TEMPLATE.md). You may also want to read about [how changes get made to Plotly.js](https://github.com/plotly/plotly.js/blob/master/CONTRIBUTING.md)    ---  ## Contributing    Please read through our [contributing guidelines](https://github.com/plotly/plotly.js/blob/master/CONTRIBUTING.md). Included are directions for opening issues, using plotly.js in your project and notes on development.    ---  ## Notable contributors    Plotly.js is at the core of a large and dynamic ecosystem with many contributors who file issues, reproduce bugs, suggest improvements, write code in this repo (and other upstream or downstream ones) and help users in the Plotly community forum. The following people deserve special recognition for their outsized contributions to this ecosystem:    |   | GitHub | Twitter | Status |  |---|--------|---------|--------|  |**Alex C. Johnson**| [@alexcjohnson](https://github.com/alexcjohnson) | | Active, Maintainer |  |**Mojtaba Samimi** | [@archmoj](https://github.com/archmoj) | [@solarchvision](https://twitter.com/solarchvision) | Active, Maintainer |  |**Antoine Roy-Gobeil** | [@antoinerg](https://github.com/antoinerg) | | Active, Maintainer |  |**Nicolas Kruchten** | [@nicolaskruchten](https://github.com/nicolaskruchten) | [@nicolaskruchten](https://twitter.com/nicolaskruchten) | Active, Maintainer |  |**Jon Mease** | [@jonmmease](https://github.com/jonmmease) | [@jonmmease](https://twitter.com/jonmmease) | Active |  |**Étienne Tétreault-Pinard**| [@etpinard](https://github.com/etpinard) | [@etpinard](https://twitter.com/etpinard) | Hall of Fame |  |**Mikola Lysenko**| [@mikolalysenko](https://github.com/mikolalysenko) | [@MikolaLysenko](https://twitter.com/MikolaLysenko) | Hall of Fame |  |**Ricky Reusser**| [@rreusser](https://github.com/rreusser) | [@rickyreusser](https://twitter.com/rickyreusser) | Hall of Fame |  |**Dmitry Yv.** | [@dy](https://github.com/dy) | [@DimaYv](https://twitter.com/dimayv)| Hall of Fame |  |**Robert Monfera**| [@monfera](https://github.com/monfera) | [@monfera](https://twitter.com/monfera) | Hall of Fame |  |**Robert Möstl** | [@rmoestl](https://github.com/rmoestl) | [@rmoestl](https://twitter.com/rmoestl) | Hall of Fame |  |**Nicolas Riesco**| [@n-riesco](https://github.com/n-riesco) | | Hall of Fame |  |**Miklós Tusz**| [@mdtusz](https://github.com/mdtusz) | [@mdtusz](https://twitter.com/mdtusz)| Hall of Fame |  |**Chelsea Douglas**| [@cldougl](https://github.com/cldougl) | | Hall of Fame |  |**Ben Postlethwaite**| [@bpostlethwaite](https://github.com/bpostlethwaite) | | Hall of Fame |  |**Chris Parmer**| [@chriddyp](https://github.com/chriddyp) | | Hall of Fame |  |**Alex Vados**| [@alexander-daniel](https://github.com/alexander-daniel) | | Hall of Fame |    ---  ## Copyright and license    Code and documentation copyright 2021 Plotly, Inc.    Code released under the [MIT license](https://github.com/plotly/plotly.js/blob/master/LICENSE).    ### Versioning    This project is maintained under the [Semantic Versioning guidelines](https://semver.org/).    See the [Releases section](https://github.com/plotly/plotly.js/releases) of our GitHub project for changelogs for each release version of plotly.js.    ---  ## Community    * Follow [@plotlygraphs](https://twitter.com/plotlygraphs) on Twitter for the latest Plotly news.  * Implementation help may be found on community.plot.com (tagged [`plotly-js`](https://community.plotly.com/c/plotly-js)) or    on Stack Overflow (tagged [`plotly`](https://stackoverflow.com/questions/tagged/plotly)).  * Developers should use the keyword `plotly` on packages which modify or add to the functionality of plotly.js when distributing through [npm](https://www.npmjs.com/browse/keyword/plotly). """
Big data;https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers;"""# Awesome Gradient Boosting Research Papers.  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-gradient-boosting-papers.svg?color=blue) [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-gradient-boosting-papers.svg)](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers/archive/master.zip) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)  <p align=""center"">    <img width=""450"" src=""boosting.gif"">  </p>    ---------------------------------------------    A curated list of gradient and adaptive boosting papers with implementations from the following conferences:    - Machine learning     * [NeurIPS](https://nips.cc/)      * [ICML](https://icml.cc/)      * [ICLR](https://iclr.cc/)  - Computer vision     * [CVPR](http://cvpr2019.thecvf.com/)     * [ICCV](http://iccv2019.thecvf.com/)     * [ECCV](https://eccv2018.org/)  - Natural language processing     * [ACL](http://www.acl2019.org/EN/index.xhtml)     * [NAACL](https://naacl2019.org/)     * [EMNLP](https://www.emnlp-ijcnlp2019.org/)   - Data     * [KDD](https://www.kdd.org/)     * [CIKM](http://www.cikmconference.org/)        * [ICDM](http://icdm2019.bigke.org/)     * [SDM](https://www.siam.org/Conferences/CM/Conference/sdm19)        * [PAKDD](http://pakdd2019.medmeeting.org)     * [PKDD/ECML](http://ecmlpkdd2019.org)     * [RECSYS](https://recsys.acm.org/)     * [SIGIR](https://sigir.org/)     * [WWW](https://www2019.thewebconf.org/)     * [WSDM](www.wsdm-conference.org)   - Artificial intelligence     * [AAAI](https://www.aaai.org/)     * [AISTATS](https://www.aistats.org/)     * [ICANN](https://e-nns.org/icann2019/)        * [IJCAI](https://www.ijcai.org/)     * [UAI](http://www.auai.org/)    Similar collections about [graph classification](https://github.com/benedekrozemberczki/awesome-graph-classification), [classification/regression tree](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [fraud detection](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers), [Monte Carlo tree search](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers), and [community detection](https://github.com/benedekrozemberczki/awesome-community-detection) papers with implementations.    ## 2021    - **Precision-based Boosting (AAAI 2021)**    - Mohammad Hossein Nikravan, Marjan Movahedan, Sandra Zilles    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17105)    - **BNN: Boosting Neural Network Framework Utilizing Limited Amount of Data (CIKM 2021)**    - Amit Livne, Roy Dor, Bracha Shapira, Lior Rokach    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3482414)    - **Unsupervised Domain Adaptation for Static Malware Detection based on Gradient Boosting Trees (CIKM 2021)**    - Panpan Qi, Wei Wang, Lei Zhu, See-Kiong Ng    - [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3459637.3482400)    - **Individually Fair Gradient Boosting (ICLR 2021)**    - Alexander Vargo, Fan Zhang, Mikhail Yurochkin, Yuekai Sun    - [[Paper]](https://arxiv.org/abs/2103.16785)    - **Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees (ICLR 2021)**    - Zhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky, Marc Najork    - [[Paper]](https://iclr.cc/virtual/2021/spotlight/3536)    - **AdaGCN: Adaboosting Graph Convolutional Networks into Deep Models (ICLR 2021)**    - Ke Sun, Zhanxing Zhu, Zhouchen Lin    - [[Paper]](https://arxiv.org/abs/1908.05081)    - [[Code]](https://github.com/datake/AdaGCN)    - **Uncertainty in Gradient Boosting via Ensembles (ICLR 2021)**    - Andrey Malinin, Liudmila Prokhorenkova, Aleksei Ustimenko    - [[Paper]](https://arxiv.org/abs/2006.10562)    -   - **Boost then Convolve: Gradient Boosting Meets Graph Neural Networks (ICLR 2021)**    - Sergei Ivanov, Liudmila Prokhorenkova    - [[Paper]](https://arxiv.org/abs/2101.08543)    - **GBHT: Gradient Boosting Histogram Transform for Density Estimation (ICML 2021)**    - Jingyi Cui, Hanyuan Hang, Yisen Wang, Zhouchen Lin    - [[Paper]](https://arxiv.org/abs/2106.05738)    - **Boosting for Online Convex Optimization (ICML 2021)**    - Elad Hazan, Karan Singh    - [[Paper]](https://arxiv.org/abs/2102.09305)    - **Accuracy, Interpretability, and Differential Privacy via Explainable Boosting (ICML 2021)**    - Harsha Nori, Rich Caruana, Zhiqi Bu, Judy Hanwen Shen, Janardhan Kulkarni    - [[Paper]](https://arxiv.org/abs/2106.09680)    - **SGLB: Stochastic Gradient Langevin Boosting (ICML 2021)**    - Aleksei Ustimenko, Liudmila Prokhorenkova    - [[Paper]](https://arxiv.org/abs/2001.07248)    - **Self-boosting for Feature Distillation (IJCAI 2021)**    - Yulong Pei, Yanyun Qu, Junping Zhang    - [[Paper]](https://www.ijcai.org/proceedings/2021/131)    - **Boosting Variational Inference With Locally Adaptive Step-Sizes (IJCAI 2021)**    - Gideon Dresdner, Saurav Shekhar, Fabian Pedregosa, Francesco Locatello, Gunnar Rätsch    - [[Paper]](https://arxiv.org/abs/2105.09240)    - **Probabilistic Gradient Boosting Machines for Large-Scale Probabilistic Regression (KDD 2021)**    - Olivier Sprangers, Sebastian Schelter, Maarten de Rijke    - [[Paper]](https://arxiv.org/abs/2106.01682)    - **Task-wise Split Gradient Boosting Trees for Multi-center Diabetes Prediction (KDD 2021)**    - Mingcheng Chen, Zhenghui Wang, Zhiyun Zhao, Weinan Zhang, Xiawei Guo, Jian Shen, Yanru Qu, Jieli Lu, Min Xu, Yu Xu, Tiange Wang, Mian Li, Weiwei Tu, Yong Yu, Yufang Bi, Weiqing Wang, Guang Ning    - [[Paper]](https://arxiv.org/abs/2108.07107)    - **Better Short than Greedy: Interpretable Models through Optimal Rule Boosting (SDM 2021)**    - Mario Boley, Simon Teshuva, Pierre Le Bodic, Geoffrey I. Webb    - [[Paper]](https://arxiv.org/abs/2101.08380)    ## 2020    - **A Unified Framework for Knowledge Intensive Gradient Boosting: Leveraging Human Experts for Noisy Sparse Domains (AAAI 2020)**    - Harsha Kokel, Phillip Odom, Shuo Yang, Sriraam Natarajan    - [[Paper]](https://personal.utdallas.edu/~sriraam.natarajan/Papers/Kokel_AAAI20.pdf)    - [[Code]](https://github.com/harshakokel/KiGB)    - **Practical Federated Gradient Boosting Decision Trees (AAAI 2020)**    - Qinbin Li, Zeyi Wen, Bingsheng He    - [[Paper]](https://arxiv.org/abs/1911.04206)    - **Privacy-Preserving Gradient Boosting Decision Trees (AAAI 2020)**    - Qinbin Li, Zhaomin Wu, Zeyi Wen, Bingsheng He    - [[Paper]](https://arxiv.org/abs/1911.04209)      - **Accelerating Gradient Boosting Machines (AISTATS 2020)**    - Haihao Lu, Sai Praneeth Karimireddy, Natalia Ponomareva, Vahab S. Mirrokni    - [[Paper]](https://arxiv.org/abs/1903.08708)    - **Scalable Feature Selection for Multitask Gradient Boosted Trees (AISTATS 2020)**    - Cuize Han, Nikhil Rao, Daria Sorokina, Karthik Subbian    - [[Paper]](http://proceedings.mlr.press/v108/han20a.html)    - **Functional Gradient Boosting for Learning Residual-like Networks with Statistical Guarantees (AISTATS 2020)**    - Atsushi Nitanda, Taiji Suzuki    - [[Paper]](http://proceedings.mlr.press/v108/nitanda20a.html)      - **Learning Optimal Decision Trees with MaxSAT and its Integration in AdaBoost (IJCAI 2020)**    - Hao Hu, Mohamed Siala, Emmanuel Hebrard, Marie-José Huguet    - [[Paper]](https://www.ijcai.org/Proceedings/2020/163)    - **MixBoost: Synthetic Oversampling using Boosted Mixup for Handling Extreme Imbalance (ICDM 2020)**    - Anubha Kabra, Ayush Chopra, Nikaash Puri, Pinkesh Badjatiya, Sukriti Verma, Piyush Gupta, Balaji Krishnamurthy    - [[Paper]](https://arxiv.org/abs/2009.01571)    - **Boosting for Control of Dynamical Systems (ICML 2020)**    - Naman Agarwal, Nataly Brukhim, Elad Hazan, Zhou Lu    - [[Paper]](https://arxiv.org/abs/1906.08720)    - **Quantum Boosting (ICML 2020)**    - Srinivasan Arunachalam, Reevu Maity    - [[Paper]](https://arxiv.org/abs/2002.05056)    - **Boosted Histogram Transform for Regression (ICML 2020)**    - Yuchao Cai, Hanyuan Hang, Hanfang Yang, Zhouchen Lin    - [[Paper]](https://proceedings.icml.cc/static/paper_files/icml/2020/2360-Paper.pdf)    - **Boosting Frank-Wolfe by Chasing Gradients (ICML 2020)**    - Cyrille W. Combettes, Sebastian Pokutta    - [[Paper]](https://arxiv.org/abs/2003.06369)    - **NGBoost: Natural Gradient Boosting for Probabilistic Prediction (ICML 2020)**    - Tony Duan, Avati Anand, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu, Andrew Y. Ng, Alejandro Schuler    - [[Paper]](https://arxiv.org/abs/1910.03225)    - [[Code]](https://github.com/stanfordmlgroup/ngboost)      - **Online Agnostic Boosting via Regret Minimization (NeurIPS 2020)**    - Nataly Brukhim, Xinyi Chen, Elad Hazan, Shay Moran    - [[Paper]](https://arxiv.org/abs/2003.01150)      - **Boosting First-Order Methods by Shifting Objective: New Schemes with Faster Worst Case Rates (NeurIPS 2020)**    - Kaiwen Zhou, Anthony Man-Cho So, James Cheng    - [[Paper]](https://arxiv.org/abs/2005.12061)    - **Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks (NeurIPS 2020)**    - Kenta Oono, Taiji Suzuki    - [[Paper]](https://arxiv.org/abs/2006.08550)    - [[Code]](https://github.com/delta2323/GB-GNN)      - **Gradient Boosted Normalizing Flows (NeurIPS 2020)**    - Robert Giaquinto, Arindam Banerjee    - [[Paper]](https://arxiv.org/abs/2002.11896)    - [[Code]](https://github.com/robert-giaquinto/gradient-boosted-normalizing-flows)    - **HyperML: A Boosting Metric Learning Approach in Hyperbolic Space for Recommender Systems (WSDM 2020)**    - Lucas Vinh Tran, Yi Tay, Shuai Zhang, Gao Cong, Xiaoli Li    - [[Paper]](https://arxiv.org/abs/1809.01703)    ## 2019    - **Induction of Non-Monotonic Logic Programs to Explain Boosted Tree Models Using LIME (AAAI 2019)**    - Farhad Shakerin, Gopal Gupta    - [[Paper]](https://arxiv.org/abs/1808.00629)    - **Verifying Robustness of Gradient Boosted Models (AAAI 2019)**    - Gil Einziger, Maayan Goldstein, Yaniv Sa'ar, Itai Segall    - [[Paper]](https://arxiv.org/pdf/1906.10991.pdf)    - **Online Multiclass Boosting with Bandit Feedback (AISTATS 2019)**    - Daniel T. Zhang, Young Hun Jung, Ambuj Tewari    - [[Paper]](https://arxiv.org/abs/1810.05290)      - **AdaFair: Cumulative Fairness Adaptive Boosting (CIKM 2019)**    - Vasileios Iosifidis, Eirini Ntoutsi    - [[Paper]](https://arxiv.org/abs/1909.08982)    - **Interpretable MTL from Heterogeneous Domains using Boosted Tree (CIKM 2019)**    - Ya-Lin Zhang, Longfei Li    - [[Paper]](https://dl.acm.org/citation.cfm?id=3357384.3358072)    - **Adversarial Training of Gradient-Boosted Decision Trees (CIKM 2019)**    - Stefano Calzavara, Claudio Lucchese, Gabriele Tolomei    - [[Paper]](https://www.dais.unive.it/~calzavara/papers/cikm19.pdf)      - **Fair Adversarial Gradient Tree Boosting (ICDM 2019)**    - Vincent Grari, Boris Ruf, Sylvain Lamprier, Marcin Detyniecki    - [[Paper]](https://arxiv.org/abs/1911.05369)    - **Boosted Density Estimation Remastered (ICML 2019)**    - Zac Cranko, Richard Nock    - [[Paper]](https://arxiv.org/abs/1803.08178)    - **Lossless or Quantized Boosting with Integer Arithmetic (ICML 2019)**    - Richard Nock, Robert C. Williamson    - [[Paper]](http://proceedings.mlr.press/v97/nock19a.html)    - **Optimal Minimal Margin Maximization with Boosting (ICML 2019)**    - Alexander Mathiasen, Kasper Green Larsen, Allan Grønlund    - [[Paper]](https://arxiv.org/abs/1901.10789)    - **Katalyst: Boosting Convex Katayusha for Non-Convex Problems with a Large Condition Number (ICML 2019)**    - Zaiyi Chen, Yi Xu, Haoyuan Hu, Tianbao Yang    - [[Paper]](https://arxiv.org/abs/1809.06754)      - **Boosting for Comparison-Based Learning (IJCAI 2019)**    - Michaël Perrot, Ulrike von Luxburg    - [[Paper]](https://arxiv.org/abs/1810.13333)    - **AugBoost: Gradient Boosting Enhanced with Step-Wise Feature Augmentation (IJCAI 2019)**    - Philip Tannor, Lior Rokach    - [[Paper]](https://www.ijcai.org/proceedings/2019/0493.pdf)    - **Gradient Boosting with Piece-Wise Linear Regression Trees (IJCAI 2019)**    - Yu Shi, Jian Li, Zhize Li    - [[Paper]](https://arxiv.org/abs/1802.05640)    - [[Code]](https://github.com/GBDT-PL/GBDT-PL)      - **SpiderBoost and Momentum: Faster Variance Reduction Algorithms (NeurIPS 2019)**    - Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, Vahid Tarokh    - [[Paper]](http://papers.nips.cc/paper/8511-spiderboost-and-momentum-faster-variance-reduction-algorithms)    - **Faster Boosting with Smaller Memory (NeurIPS 2019)**    - Julaiti Alafate, Yoav Freund    - [[Paper]](https://arxiv.org/abs/1901.09047)    - **Regularized Gradient Boosting (NeurIPS 2019)**    - Corinna Cortes, Mehryar Mohri, Dmitry Storcheus    - [[Paper]](https://papers.nips.cc/paper/8784-regularized-gradient-boosting)    - **Margin-Based Generalization Lower Bounds for Boosted Classifiers (NeurIPS 2019)**    - Allan Grønlund, Lior Kamma, Kasper Green Larsen, Alexander Mathiasen, Jelani Nelson    - [[Paper]](https://arxiv.org/abs/1909.12518)    - **Minimal Variance Sampling in Stochastic Gradient Boosting (NeurIPS 2019)**    - Bulat Ibragimov, Gleb Gusev    - [[Paper]](https://papers.nips.cc/paper/9645-minimal-variance-sampling-in-stochastic-gradient-boosting)    - **Universal Boosting Variational Inference (NeurIPS 2019)**    - Trevor Campbell, Xinglong Li    - [[Paper]](https://arxiv.org/abs/1906.01235)      - **Provably Robust Boosted Decision Stumps and Trees against Adversarial Attacks (NeurIPS 2019)**    - Maksym Andriushchenko, Matthias Hein    - [[Paper]](https://arxiv.org/abs/1906.03526)    - [[Code]](https://github.com/max-andr/provably-robust-boosting)      - **Block-distributed Gradient Boosted Trees (SIGIR 2019)**    - Theodore Vasiloudis, Hyunsu Cho, Henrik Boström    - [[Paper]](https://arxiv.org/abs/1904.10522)      - **Learning to Rank in Theory and Practice: From Gradient Boosting to Neural Networks and Unbiased Learning (SIGIR 2019)**    - Claudio Lucchese, Franco Maria Nardini, Rama Kumar Pasumarthi, Sebastian Bruch, Michael Bendersky, Xuanhui Wang, Harrie Oosterhuis, Rolf Jagerman, Maarten de Rijke    - [[Paper]](https://www.researchgate.net/publication/334579610_Learning_to_Rank_in_Theory_and_Practice_From_Gradient_Boosting_to_Neural_Networks_and_Unbiased_Learning)    ## 2018  - **Boosted Generative Models (AAAI 2018)**    - Aditya Grover, Stefano Ermon    - [[Paper]](https://arxiv.org/pdf/1702.08484.pdf)    - [[Code]](https://github.com/ermongroup/bgm)    - **Boosting Variational Inference: an Optimization Perspective (AISTATS 2018)**    - Francesco Locatello, Rajiv Khanna, Joydeep Ghosh, Gunnar Rätsch    - [[Paper]](https://arxiv.org/abs/1708.01733)    - [[Code]](https://github.com/ratschlab/boosting-bbvi)    - **Online Boosting Algorithms for Multi-label Ranking (AISTATS 2018)**    - Young Hun Jung, Ambuj Tewari    - [[Paper]](https://arxiv.org/abs/1710.08079)    - [[Code]](https://github.com/yhjung88/OnlineMLRBoostingWithVFDT)    - **DualBoost: Handling Missing Values with Feature Weights and Weak Classifiers that Abstain (CIKM 2018)**    - Weihong Wang, Jie Xu, Yang Wang, Chen Cai, Fang Chen    - [[Paper]](http://delivery.acm.org/10.1145/3270000/3269319/p1543-wang.pdf?ip=129.215.164.203&id=3269319&acc=ACTIVE%20SERVICE&key=C2D842D97AC95F7A%2EEB9E991028F4E1F1%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1558633895_f01b39fd47b943fd01eade763a397e04)    - **Functional Gradient Boosting based on Residual Network Perception (ICML 2018)**    - Atsushi Nitanda, Taiji Suzuki    - [[Paper]](https://arxiv.org/abs/1802.09031)    - [[Code]](https://github.com/anitan0925/ResFGB)    - **Finding Influential Training Samples for Gradient Boosted Decision Trees (ICML 2018)**    - Boris Sharchilev, Yury Ustinovskiy, Pavel Serdyukov, Maarten de Rijke    - [[Paper]](https://arxiv.org/abs/1802.06640)    - **Learning Deep ResNet Blocks Sequentially using Boosting Theory (ICML 2018)**    - Furong Huang, Jordan T. Ash, John Langford, Robert E. Schapire    - [[Paper]](https://arxiv.org/abs/1706.04964)    - [[Code]](https://github.com/JordanAsh/boostresnet)    - **UCBoost: A Boosting Approach to Tame Complexity and Optimality for Stochastic Bandits (IJCAI 2018)**    - Fang Liu, Sinong Wang, Swapna Buccapatnam, Ness B. Shroff    - [[Paper]](https://www.ijcai.org/proceedings/2018/0338.pdf)    - [[Code]](https://smpybandits.github.io/docs/Policies.UCBoost.html)    - **Adaboost with Auto-Evaluation for Conversational Models (IJCAI 2018)**    - Juncen Li, Ping Luo, Ganbin Zhou, Fen Lin, Cheng Niu    - [[Paper]](https://www.ijcai.org/proceedings/2018/0580.pdf)    - **Ensemble Neural Relation Extraction with Adaptive Boosting (IJCAI 2018)**    - Dongdong Yang, Senzhang Wang, Zhoujun Li    - [[Paper]](https://www.ijcai.org/proceedings/2018/0630.pdf)    - **CatBoost: Unbiased Boosting with Categorical Features (NIPS 2018)**    - Liudmila Ostroumova Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, Andrey Gulin    - [[Paper]](https://papers.nips.cc/paper/7898-catboost-unbiased-boosting-with-categorical-features.pdf)    - [[Code]](https://github.com/catboost/catboost)    - **Multitask Boosting for Survival Analysis with Competing Risks (NIPS 2018)**    - Alexis Bellot, Mihaela van der Schaar    - [[Paper]](https://papers.nips.cc/paper/7413-multitask-boosting-for-survival-analysis-with-competing-risks)    - **Multi-Layered Gradient Boosting Decision Trees (NIPS 2018)**    - Ji Feng, Yang Yu, Zhi-Hua Zhou    - [[Paper]](https://papers.nips.cc/paper/7614-multi-layered-gradient-boosting-decision-trees.pdf)    - [[Code]](https://github.com/kingfengji/mGBDT)    - **Boosted Sparse and Low-Rank Tensor Regression (NIPS 2018)**    - Lifang He, Kun Chen, Wanwan Xu, Jiayu Zhou, Fei Wang    - [[Paper]](https://arxiv.org/abs/1811.01158)    - [[Code]](https://github.com/LifangHe/NeurIPS18_SURF)      - **Selective Gradient Boosting for Effective Learning to Rank (SIGIR 2018)**    - Claudio Lucchese, Franco Maria Nardini, Raffaele Perego, Salvatore Orlando, Salvatore Trani    - [[Paper]](http://quickrank.isti.cnr.it/selective-data/selective-SIGIR2018.pdf)    - [[Code]](https://github.com/hpclab/quickrank/blob/master/documentation/selective.md)    ## 2017  - **Boosting for Real-Time Multivariate Time Series Classification (AAAI 2017)**    - Haishuai Wang, Jun Wu    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14852/14241)    - **Cross-Domain Sentiment Classification via Topic-Related TrAdaBoost (AAAI 2017)**    - Xingchang Huang, Yanghui Rao, Haoran Xie, Tak-Lam Wong, Fu Lee Wang    - [[Paper]](https://pdfs.semanticscholar.org/826c/c83d98a5c4c7dcc02be1f4dd9c27e2b99670.pdf)    - [[Code]](https://github.com/xchhuang/cross-domain-sentiment-classification)    - **Extreme Gradient Boosting and Behavioral Biometrics (AAAI 2017)**    - Benjamin Manning    - [[Paper]](https://pdfs.semanticscholar.org/8c6e/6c887d6d47dda3f0c73297fd4da516fef1ee.pdf)    - **FeaBoost: Joint Feature and Label Refinement for Semantic Segmentation (AAAI 2017)**    - Yulei Niu, Zhiwu Lu, Songfang Huang, Xin Gao, Ji-Rong Wen    - [[Paper]](https://pdfs.semanticscholar.org/d566/73be998b3ed38ccbb53551e38758ae8cfc9d.pdf)    - **Boosting Complementary Hash Tables for Fast Nearest Neighbor Search (AAAI 2017)**    - Xianglong Liu, Cheng Deng, Yadong Mu, Zhujin Li    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14336)    - **Gradient Boosting on Stochastic Data Streams (AISTATS 2017)**    - Hanzhang Hu, Wen Sun, Arun Venkatraman, Martial Hebert, J. Andrew Bagnell    - [[Paper]](https://arxiv.org/abs/1703.00377)    - **BoostVHT: Boosting Distributed Streaming Decision Trees (CIKM 2017)**    - Theodore Vasiloudis, Foteini Beligianni, Gianmarco De Francisci Morales    - [[Paper]](https://melmeric.files.wordpress.com/2010/05/boostvht-boosting-distributed-streaming-decision-trees.pdf)    - **Fast Boosting Based Detection Using Scale Invariant Multimodal Multiresolution Filtered Features (CVPR 2017)**    - Arthur Daniel Costea, Robert Varga, Sergiu Nedevschi    - [[Paper]](http://openaccess.thecvf.com/content_cvpr_2017/papers/Costea_Fast_Boosting_Based_CVPR_2017_paper.pdf)    - **BIER - Boosting Independent Embeddings Robustly (ICCV 2017)**    - Michael Opitz, Georg Waltner, Horst Possegger, Horst Bischof    - [[Paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Opitz_BIER_-_Boosting_ICCV_2017_paper.pdf)    - [[Code]](https://github.com/mop/bier)    - **An Analysis of Boosted Linear Classifiers on Noisy Data with Applications to Multiple-Instance Learning (ICDM 2017)**    - Rui Liu, Soumya Ray    - [[Paper]](https://ieeexplore.ieee.org/document/8215501)    - **Variational Boosting: Iteratively Refining Posterior Approximations (ICML 2017)**    - Andrew C. Miller, Nicholas J. Foti, Ryan P. Adams    - [[Paper]](https://arxiv.org/abs/1611.06585)    - [[Code]](https://github.com/andymiller/vboost)    - **Boosted Fitted Q-Iteration (ICML 2017)**    - Samuele Tosatto, Matteo Pirotta, Carlo D'Eramo, Marcello Restelli    - [[Paper]](http://proceedings.mlr.press/v70/tosatto17a.html)    - **A Simple Multi-Class Boosting Framework with Theoretical Guarantees and Empirical Proficiency (ICML 2017)**    - Ron Appel, Pietro Perona    - [[Paper]](http://proceedings.mlr.press/v70/appel17a.html)    - [[Code]](https://github.com/GuillaumeCollin/A-Simple-Multi-Class-Boosting-Framework-with-Theoretical-Guarantees-and-Empirical-Proficiency)    - **Gradient Boosted Decision Trees for High Dimensional Sparse Output (ICML 2017)**    - Si Si, Huan Zhang, S. Sathiya Keerthi, Dhruv Mahajan, Inderjit S. Dhillon, Cho-Jui Hsieh    - [[Paper]](http://proceedings.mlr.press/v70/si17a.html)    - [[Code]](https://github.com/springdaisy/GBDT)    - **Local Topic Discovery via Boosted Ensemble of Nonnegative Matrix Factorization (IJCAI 2017)**    - Sangho Suh, Jaegul Choo, Joonseok Lee, Chandan K. Reddy    - [[Paper]](http://dmkd.cs.vt.edu/papers/IJCAI17.pdf)    - [[Code]](https://github.com/benedekrozemberczki/BoostedFactorization)    - **Boosted Zero-Shot Learning with Semantic Correlation Regularization (IJCAI 2017)**    - Te Pi, Xi Li, Zhongfei (Mark) Zhang    - [[Paper]](https://arxiv.org/abs/1707.08008)    - **BDT: Gradient Boosted Decision Tables for High Accuracy and Scoring Efficiency (KDD 2017)**    - Yin Lou, Mikhail Obukhov    - [[Paper]](https://yinlou.github.io/papers/lou-kdd17.pdf)      - **CatBoost: Gradient Boosting with Categorical Features Support (NIPS 2017)**    - Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin    - [[Paper]](https://arxiv.org/abs/1810.11363)    - [[Code]](https://catboost.ai/)    - **Cost Efficient Gradient Boosting (NIPS 2017)**    - Sven Peter, Ferran Diego, Fred A. Hamprecht, Boaz Nadler    - [[Paper]](https://papers.nips.cc/paper/6753-cost-efficient-gradient-boosting)    - [[Code]](https://github.com/svenpeter42/LightGBM-CEGB)    - **AdaGAN: Boosting Generative Models (NIPS 2017)**    - Ilya O. Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, Bernhard Schölkopf    - [[Paper]](https://arxiv.org/abs/1701.02386)    - [[Code]](https://github.com/tolstikhin/adagan)    - **LightGBM: A Highly Efficient Gradient Boosting Decision Tree (NIPS 2017)**    - Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu    - [[Paper]](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)    - [[Code]](https://lightgbm.readthedocs.io/en/latest/)    - **Early Stopping for Kernel Boosting Algorithms: A General Analysis with Localized Complexities (NIPS 2017)**    - Yuting Wei, Fanny Yang, Martin J. Wainwright    - [[Paper]](https://arxiv.org/abs/1707.01543)    - [[Code]](https://github.com/fanny-yang/EarlyStoppingRKHS)    - **Online Multiclass Boosting (NIPS 2017)**    - Young Hun Jung, Jack Goetz, Ambuj Tewari    - [[Paper]](https://papers.nips.cc/paper/6693-online-multiclass-boosting.pdf)    - **Stacking Bagged and Boosted Forests for Effective Automated Classification (SIGIR 2017)**    - Raphael R. Campos, Sérgio D. Canuto, Thiago Salles, Clebson C. A. de Sá, Marcos André Gonçalves    - [[Paper]](https://homepages.dcc.ufmg.br/~rcampos/papers/sigir2017/appendix.pdf)    - [[Code]](https://github.com/raphaelcampos/stacking-bagged-boosted-forests)    - **GB-CENT: Gradient Boosted Categorical Embedding and Numerical Trees (WWW 2017)**    - Qian Zhao, Yue Shi, Liangjie Hong    - [[Paper]](http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p1311.pdf)    - [[Code]](https://github.com/grouplens/samantha)    ## 2016  - **Group Cost-Sensitive Boosting for Multi-Resolution Pedestrian Detection (AAAI 2016)**    - Chao Zhu, Yuxin Peng    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/11898/12146)    - [[Code]](https://github.com/nnikolaou/Cost-sensitive-Boosting-Tutorial)    - **Communication Efficient Distributed Agnostic Boosting (AISTATS 2016)**    - Shang-Tse Chen, Maria-Florina Balcan, Duen Horng Chau    - [[Paper]](https://arxiv.org/abs/1506.06318)    - **Logistic Boosting Regression for Label Distribution Learning (CVPR 2016)**    - Chao Xing, Xin Geng, Hui Xue    - [[Paper]](https://zpascal.net/cvpr2016/Xing_Logistic_Boosting_Regression_CVPR_2016_paper.pdf)    - **Structured Regression Gradient Boosting (CVPR 2016)**    - Ferran Diego, Fred A. Hamprecht    - [[Paper]](https://hci.iwr.uni-heidelberg.de/sites/default/files/publications/files/1037872734/diego_16_structured.pdf)      - **L-EnsNMF: Boosted Local Topic Discovery via Ensemble of Nonnegative Matrix Factorization (ICDM 2016)**    - Sangho Suh, Jaegul Choo, Joonseok Lee, Chandan K. Reddy    - [[Paper]](https://ieeexplore.ieee.org/document/7837872)    - [[Code]](https://github.com/benedekrozemberczki/BoostedFactorization)    - **Meta-Gradient Boosted Decision Tree Model for Weight and Target Learning (ICML 2016)**    - Yury Ustinovskiy, Valentina Fedorova, Gleb Gusev, Pavel Serdyukov    - [[Paper]](http://proceedings.mlr.press/v48/ustinovskiy16.html)    - **Generalized Dictionary for Multitask Learning with Boosting (IJCAI 2016)**    - Boyu Wang, Joelle Pineau    - [[Paper]](https://www.ijcai.org/Proceedings/16/Papers/299.pdf)    - **Self-Paced Boost Learning for Classification (IJCAI 2016)**    - Te Pi, Xi Li, Zhongfei Zhang, Deyu Meng, Fei Wu, Jun Xiao, Yueting Zhuang    - [[Paper]](https://pdfs.semanticscholar.org/31b6/ab4a0771d5b7405cacdd12c398b1c832729d.pdf)    - **Interactive Martingale Boosting (IJCAI 2016)**    - Ashish Kulkarni, Pushpak Burange, Ganesh Ramakrishnan    - [[Paper]](https://www.ijcai.org/Proceedings/16/Papers/124.pdf)    - **Optimal and Adaptive Algorithms for Online Boosting (IJCAI 2016)**    - Alina Beygelzimer, Satyen Kale, Haipeng Luo    - [[Paper]](https://www.ijcai.org/Proceedings/16/Papers/614.pdf)    - [[Code]](https://github.com/VowpalWabbit/vowpal_wabbit/blob/master/vowpalwabbit/boosting.cc)    - **Rating-Boosted Latent Topics: Understanding Users and Items with Ratings and Reviews (IJCAI 2016)**    - Yunzhi Tan, Min Zhang, Yiqun Liu, Shaoping Ma    - [[Paper]](https://pdfs.semanticscholar.org/db63/89e0ca49ec0e4686e40604e7489cb4c0729d.pdf)    - **XGBoost: A Scalable Tree Boosting System (KDD 2016)**    - Tianqi Chen, Carlos Guestrin    - [[Paper]](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)    - [[Code]](https://github.com/dmlc/xgboost)    - **Boosted Decision Tree Regression Adjustment for Variance Reduction in Online Controlled Experiments (KDD 2016)**    - Alexey Poyarkov, Alexey Drutsa, Andrey Khalyavin, Gleb Gusev, Pavel Serdyukov    - [[Paper]](https://www.kdd.org/kdd2016/papers/files/adf0653-poyarkovA.pdf)    - **Boosting with Abstention (NIPS 2016)**    - Corinna Cortes, Giulia DeSalvo, Mehryar Mohri    - [[Paper]](https://papers.nips.cc/paper/6336-boosting-with-abstention)    - **SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques (NIPS 2016)**    - Elad Richardson, Rom Herskovitz, Boris Ginsburg, Michael Zibulevsky    - [[Paper]](https://papers.nips.cc/paper/6109-seboost-boosting-stochastic-learning-using-subspace-optimization-techniques.pdf)    - [[Code]](https://github.com/eladrich/seboost)    - **Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition (NIPS 2016)**    - Shizhong Han, Zibo Meng, Ahmed-Shehab Khan, Yan Tong    - [[Paper]](https://arxiv.org/abs/1707.05395)    - [[Code]](https://github.com/sjsingh91/IB-CNN)      - **Generalized BROOF-L2R: A General Framework for Learning to Rank Based on Boosting and Random Forests (SIGIR 2016)**    - Clebson C. A. de Sá, Marcos André Gonçalves, Daniel Xavier de Sousa, Thiago Salles    - [[Paper]](https://dl.acm.org/citation.cfm?id=2911540)    ## 2015    - **Online Boosting Algorithms for Anytime Transfer and Multitask Learning (AAAI 2015)**    - Boyu Wang, Joelle Pineau    - [[Paper]](https://www.cs.mcgill.ca/~jpineau/files/bwang-aaai15.pdf)    - **A Boosted Multi-Task Model for Pedestrian Detection with Occlusion Handling (AAAI 2015)**    - Chao Zhu, Yuxin Peng    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewFile/9879/9825)    - **Efficient Second-Order Gradient Boosting for Conditional Random Fields (AISTATS 2015)**    - Tianqi Chen, Sameer Singh, Ben Taskar, Carlos Guestrin    - [[Paper]](http://proceedings.mlr.press/v38/chen15b.html)    - **Tumblr Blog Recommendation with Boosted Inductive Matrix Completion (CIKM 2015)**    - Donghyuk Shin, Suleyman Cetintas, Kuang-Chih Lee, Inderjit S. Dhillon    - [[Paper]](https://dl.acm.org/citation.cfm?id=2806578)    - **Basis mapping based boosting for object detection (CVPR 2015)**    - Haoyu Ren, Ze-Nian Li    - [[Paper]](https://ieeexplore.ieee.org/document/7298766)    - **Tracking-by-Segmentation with Online Gradient Boosting Decision Tree (ICCV 2015)**    - Jeany Son, Ilchae Jung, Kayoung Park, Bohyung Han    - [[Paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Son_Tracking-by-Segmentation_With_Online_ICCV_2015_paper.pdf)    - [[Code]](http://cvlab.postech.ac.kr/research/ogbdt_track/)    - **Learning to Boost Filamentary Structure Segmentation (ICCV 2015)**    - Lin Gu, Li Cheng    - [[Paper]](https://isg.nist.gov/BII_2015/webPages/pages/2015_BII_program/PDFs/Day_3/Session_9/Abstract_Gu_Lin.pdf)    - **Optimal and Adaptive Algorithms for Online Boosting (ICML 2015)**    - Alina Beygelzimer, Satyen Kale, Haipeng Luo    - [[Paper]](http://proceedings.mlr.press/v37/beygelzimer15.pdf)    - [[Code]](https://github.com/VowpalWabbit/vowpal_wabbit/blob/master/vowpalwabbit/boosting.cc)    - **Rademacher Observations, Private Data, and Boosting (ICML 2015)**    - Richard Nock, Giorgio Patrini, Arik Friedman    - [[Paper]](https://arxiv.org/abs/1502.02322)    - **Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions (ICML 2015)**    - Taehoon Lee, Sungroh Yoon    - [[Paper]](https://pdfs.semanticscholar.org/d0ad/beef3053e98dd88ff74f42744417bc65a729.pdf)    - **A Direct Boosting Approach for Semi-supervised Classification (IJCAI 2015)**    - Shaodan Zhai, Tian Xia, Zhongliang Li, Shaojun Wang    - [[Paper]](https://www.ijcai.org/Proceedings/15/Papers/565.pdf)    - **A Boosting Algorithm for Item Recommendation with Implicit Feedback (IJCAI 2015)**    - Yong Liu, Peilin Zhao, Aixin Sun, Chunyan Miao    - [[Paper]](https://www.ijcai.org/Proceedings/15/Papers/255.pdf)    - [[Code]](https://github.com/microsoft/recommenders)    - **Training-Time Optimization of a Budgeted Booster (IJCAI 2015)**    - Yi Huang, Brian Powers, Lev Reyzin    - [[Paper]](https://www.ijcai.org/Proceedings/15/Papers/504.pdf)    - **Optimal Action Extraction for Random Forests and Boosted Trees (KDD 2015)**    - Zhicheng Cui, Wenlin Chen, Yujie He, Yixin Chen    - [[Paper]](https://www.cse.wustl.edu/~ychen/public/OAE.pdf)    - **Online Gradient Boosting (NIPS 2015)**    - Alina Beygelzimer, Elad Hazan, Satyen Kale, Haipeng Luo    - [[Paper]](https://arxiv.org/abs/1506.04820)    - [[Code]](https://github.com/crm416/online_boosting)      - **BROOF: Exploiting Out-of-Bag Errors Boosting and Random Forests for Effective Automated Classification (SIGIR 2015)**    - Thiago Salles, Marcos André Gonçalves, Victor Rodrigues, Leonardo C. da Rocha    - [[Paper]](https://homepages.dcc.ufmg.br/~tsalles/broof/appendix.pdf)    - **Boosting Search with Deep Understanding of Contents and Users (WSDM 2015)**    - Kaihua Zhu    - [[Paper]](https://www.researchgate.net/publication/282482189_Boosting_Search_with_Deep_Understanding_of_Contents_and_Users)    ## 2014  - **On Boosting Sparse Parities (AAAI 2014)**    - Lev Reyzin    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8587)    - **Joint Coupled-Feature Representation and Coupled Boosting for AD Diagnosis (CVPR 2014)**    - Yinghuan Shi, Heung-Il Suk, Yang Gao, Dinggang Shen    - [[Paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Shi_Joint_Coupled-Feature_Representation_2014_CVPR_paper.pdf)    - **From Categories to Individuals in Real Time - A Unified Boosting Approach (CVPR 2014)**    - David Hall, Pietro Perona    - [[Paper]](https://ieeexplore.ieee.org/document/6909424)    - [[Code]](http://www.vision.caltech.edu/~dhall/projects/CategoriesToIndividuals/)    - **Efficient Boosted Exemplar-Based Face Detection (CVPR 2014)**    - Haoxiang Li, Zhe Lin, Jonathan Brandt, Xiaohui Shen, Gang Hua    - [[Paper]](http://users.eecs.northwestern.edu/~xsh835/assets/cvpr14_exemplarfacedetection.pdf)    - **Facial Expression Recognition via a Boosted Deep Belief Network (CVPR 2014)**    - Ping Liu, Shizhong Han, Zibo Meng, Yan Tong    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6909629)    - **Confidence-Rated Multiple Instance Boosting for Object Detection (CVPR 2014)**    - Karim Ali, Kate Saenko    - [[Paper]](https://ieeexplore.ieee.org/document/6909708)    - **The Return of AdaBoost.MH: Multi-Class Hamming Trees (ICLR 2014)**    - Balázs Kégl    - [[Paper]](https://arxiv.org/pdf/1312.6086.pdf)    - [[Code]](https://github.com/aciditeam/acidano/blob/master/acidano/utils/cost.py)    - **Deep Boosting (ICML 2014)**    - Corinna Cortes, Mehryar Mohri, Umar Syed    - [[Paper]](http://proceedings.mlr.press/v32/cortesb14.pdf)    - [[Code]](https://github.com/google/deepboost)    - **A Convergence Rate Analysis for LogitBoost, MART and Their Variant (ICML 2014)**    - Peng Sun, Tong Zhang, Jie Zhou    - [[Paper]](http://proceedings.mlr.press/v32/sunc14.pdf)    - **Boosting with Online Binary Learners for the Multiclass Bandit Problem (ICML 2014)**    - Shang-Tse Chen, Hsuan-Tien Lin, Chi-Jen Lu    - [[Paper]](https://www.cc.gatech.edu/~schen351/paper/icml14boost.pdf)    - **Boosting Multi-Step Autoregressive Forecasts (ICML 2014)**    - Souhaib Ben Taieb, Rob J. Hyndman    - [[Paper]](http://proceedings.mlr.press/v32/taieb14.pdf)    - **Dynamic Programming Boosting for Discriminative Macro-Action Discovery (ICML 2014)**    - Leonidas Lefakis, François Fleuret    - [[Paper]](http://proceedings.mlr.press/v32/lefakis14.html)    - **Guess-Averse Loss Functions For Cost-Sensitive Multiclass Boosting (ICML 2014)**    - Oscar Beijbom, Mohammad J. Saberian, David J. Kriegman, Nuno Vasconcelos    - [[Paper]](http://proceedings.mlr.press/v32/beijbom14.pdf)    - **A Multi-Class Boosting Method with Direct Optimization (KDD 2014)**    - Shaodan Zhai, Tian Xia, Shaojun Wang    - [[Paper]](https://dl.acm.org/citation.cfm?id=2623689)    - **Gradient Boosted Feature Selection (KDD 2014)**    - Zhixiang Eddie Xu, Gao Huang, Kilian Q. Weinberger, Alice X. Zheng    - [[Paper]](https://arxiv.org/abs/1901.04055)    - [[Code]](https://github.com/dmlc/xgboost)    - **Multi-Class Deep Boosting (NIPS 2014)**    - Vitaly Kuznetsov, Mehryar Mohri, Umar Syed    - [[Paper]](https://papers.nips.cc/paper/5514-multi-class-deep-boosting)    - **Deconvolution of High Dimensional Mixtures via Boosting with Application to Diffusion-Weighted MRI of Human Brain (NIPS 2014)**    - Charles Y. Zheng, Franco Pestilli, Ariel Rokem    - [[Paper]](https://papers.nips.cc/paper/5506-deconvolution-of-high-dimensional-mixtures-via-boosting-with-application-to-diffusion-weighted-mri-of-human-brain)    - **A Drifting-Games Analysis for Online Learning and Applications to Boosting (NIPS 2014)**    - Haipeng Luo, Robert E. Schapire    - [[Paper]](https://arxiv.org/abs/1406.1856)    - **A Boosting Framework on Grounds of Online Learning (NIPS 2014)**    - Tofigh Naghibi Mohamadpoor, Beat Pfister    - [[Paper]](https://papers.nips.cc/paper/5512-a-boosting-framework-on-grounds-of-online-learning.pdf)      - **Gradient Boosting Factorization Machines (RECSYS 2014)**    - Chen Cheng, Fen Xia, Tong Zhang, Irwin King, Michael R. Lyu    - [[Paper]](http://tongzhang-ml.org/papers/recsys14-fm.pdf)    ## 2013    - **Boosting Binary Keypoint Descriptors (CVPR 2013)**    - Tomasz Trzcinski, C. Mario Christoudias, Pascal Fua, Vincent Lepetit    - [[Paper]](https://cvlab.epfl.ch/research/page-90554-en-html/research-detect-binboost/)    - [[Code]](https://github.com/biotrump/cvlab-BINBOOST)    - **PerturBoost: Practical Confidential Classifier Learning in the Cloud (ICDM 2013)**    - Keke Chen, Shumin Guo    - [[Paper]](https://ieeexplore.ieee.org/document/6729587)    - **Multiclass Semi-Supervised Boosting Using Similarity Learning (ICDM 2013)**    - Jafar Tanha, Mohammad Javad Saberian, Maarten van Someren    - [[Paper]](https://www.cse.msu.edu/~rongjin/publications/MultiClass-08.pdf)    - **Saving Evaluation Time for the Decision Function in Boosting: Representation and Reordering Base Learner (ICML 2013)**    - Peng Sun, Jie Zhou    - [[Paper]](http://proceedings.mlr.press/v28/sun13.pdf)    - **General Functional Matrix Factorization Using Gradient Boosting (ICML 2013)**    - Tianqi Chen, Hang Li, Qiang Yang, Yong Yu    - [[Paper]](http://w.hangli-hl.com/uploads/3/1/6/8/3168008/icml_2013.pdf)    - **Margins, Shrinkage, and Boosting (ICML 2013)**    - Matus Telgarsky    - [[Paper]](https://arxiv.org/abs/1303.4172)    - **Quickly Boosting Decision Trees - Pruning Underachieving Features Early (ICML 2013)**    - Ron Appel, Thomas J. Fuchs, Piotr Dollár, Pietro Perona    - [[Paper]](http://proceedings.mlr.press/v28/appel13.pdf)    - [[Code]](https://github.com/pdollar/toolbox/blob/master/classify/adaBoostTrain.m)    - **Human Boosting (ICML 2013)**    - Harsh H. Pareek, Pradeep Ravikumar    - [[Paper]](https://www.cs.cmu.edu/~pradeepr/paperz/humanboosting.pdf)    - **Collaborative Boosting for Activity Classification in Microblogs (KDD 2013)**    - Yangqiu Song, Zhengdong Lu, Cane Wing-ki Leung, Qiang Yang    - [[Paper]](http://chbrown.github.io/kdd-2013-usb/kdd/p482.pdf)    - **Direct 0-1 Loss Minimization and Margin Maximization with Boosting (NIPS 2013)**    - Shaodan Zhai, Tian Xia, Ming Tan, Shaojun Wang    - [[Paper]](https://papers.nips.cc/paper/5214-direct-0-1-loss-minimization-and-margin-maximization-with-boosting)    - **Reservoir Boosting : Between Online and Offline Ensemble Learning (NIPS 2013)**    - Leonidas Lefakis, François Fleuret    - [[Paper]](https://papers.nips.cc/paper/5215-reservoir-boosting-between-online-and-offline-ensemble-learning)    - **Non-Linear Domain Adaptation with Boosting (NIPS 2013)**    - Carlos J. Becker, C. Mario Christoudias, Pascal Fua    - [[Paper]](https://papers.nips.cc/paper/5200-non-linear-domain-adaptation-with-boosting)    - **Boosting in the Presence of Label Noise (UAI 2013)**    - Jakramate Bootkrajang, Ata Kabán    - [[Paper]](https://arxiv.org/abs/1309.6818)    ## 2012  - **Contextual Boost for Pedestrian Detection (CVPR 2012)**    - Yuanyuan Ding, Jing Xiao    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.5611&rep=rep1&type=pdf)    - **Shrink Boost for Selecting Multi-LBP Histogram Features in Object Detection (CVPR 2012)**    - Cher Keng Heng, Sumio Yokomitsu, Yuichi Matsumoto, Hajime Tamura    - [[Paper]](https://ieeexplore.ieee.org/document/6248061)    - **Boosting Bottom-Up and Top-Down Visual Features for Saliency Estimation (CVPR 2012)**    - Ali Borji    - [[Paper]](http://ilab.usc.edu/borji/papers/cvpr-2012-BUModel-v4.pdf)    - **Boosting Algorithms for Simultaneous Feature Extraction and Selection (CVPR 2012)**    - Mohammad J. Saberian, Nuno Vasconcelos    - [[Paper]](http://svcl.ucsd.edu/publications/conference/2012/cvpr/SOPBoost.pdf)    - **Sharing Features in Multi-class Boosting via Group Sparsity (CVPR 2012)**    - Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel    - [[Paper]](https://cs.adelaide.edu.au/~paulp/publications/pubs/sharing_cvpr2012.pdf)    - **Feature Weighting and Selection Using Hypothesis Margin of Boosting (ICDM 2012)**    - Malak Alshawabkeh, Javed A. Aslam, Jennifer G. Dy, David R. Kaeli    - [[Paper]](http://www.ece.neu.edu/fac-ece/jdy/papers/alshawabkeh-ICDM2012.pdf)    - **An AdaBoost Algorithm for Multiclass Semi-supervised Learning (ICDM 2012)**    - Jafar Tanha, Maarten van Someren, Hamideh Afsarmanesh    - [[Paper]]https://ieeexplore.ieee.org/document/6413799)      - **AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem (ICML 2012)**    - Peng Sun, Mark D. Reid, Jie Zhou    - [[Paper]](AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem)    - [[Code]](https://github.com/pengsun/AOSOLogitBoost)    - **An Online Boosting Algorithm with Theoretical Justifications (ICML 2012)**    - Shang-Tse Chen, Hsuan-Tien Lin, Chi-Jen Lu    - [[Paper]](https://arxiv.org/abs/1206.6422)    - **Learning Image Descriptors with the Boosting-Trick (NIPS 2012)**    - Tomasz Trzcinski, C. Mario Christoudias, Vincent Lepetit, Pascal Fua    - [[Paper]](https://papers.nips.cc/paper/4848-learning-image-descriptors-with-the-boosting-trick.pdf)    - [[Code]](https://github.com/biotrump/cvlab-BINBOOST)    - **Accelerated Training for Matrix-norm Regularization: A Boosting Approach (NIPS 2012)**    - Xinhua Zhang, Yaoliang Yu, Dale Schuurmans    - [[Paper]](https://papers.nips.cc/paper/4663-accelerated-training-for-matrix-norm-regularization-a-boosting-approach)      - **Learning from Heterogeneous Sources via Gradient Boosting Consensus (SDM 2012)**    - Xiaoxiao Shi, Jean-François Paiement, David Grangier, Philip S. Yu    - [[Paper]](http://david.grangier.info/papers/2012/shi_sdm_2012.pdf)    - [[Code]](https://github.com/PriyeshV/GBDT-CC)    ## 2011  - **Selective Transfer Between Learning Tasks Using Task-Based Boosting (AAAI 2011)**    - Eric Eaton, Marie desJardins    - [[Paper]](http://www.cis.upenn.edu/~eeaton/papers/Eaton2011Selective.pdf)    - **Incorporating Boosted Regression Trees into Ecological Latent Variable Models (AAAI 2011)**    - Rebecca A. Hutchinson, Li-Ping Liu, Thomas G. Dietterich    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewFile/3711/4086)    - **FlowBoost - Appearance Learning from Sparsely Annotated Video (CVPR 2011)**    - Karim Ali, David Hasler, François Fleuret    - [[Paper]](http://www.karimali.org/publications/AHF_CVPR11.pdf)    - **AdaBoost on Low-Rank PSD Matrices for Metric Learning (CVPR 2011)**    - Jinbo Bi, Dijia Wu, Le Lu, Meizhu Liu, Yimo Tao, Matthias Wolf    - [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5995363)    - **Boosted Local Structured HOG-LBP for Object Localization (CVPR 2011)**    - Junge Zhang, Kaiqi Huang, Yinan Yu, Tieniu Tan    - [[Paper]](http://www.cbsr.ia.ac.cn/users/ynyu/1682.pdf)    - **A Direct Formulation for Totally-Corrective Multi-Class Boosting (CVPR 2011)**    - Chunhua Shen, Zhihui Hao    - [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995554)    - **Gated Classifiers: Boosting Under High Intra-class Variation (CVPR 2011)**    - Oscar M. Danielsson, Babak Rasolzadeh, Stefan Carlsson    - [[Paper]](http://www.nada.kth.se/cvap/cvg/papers/danielssonCVPR11.pdf)    - **TaylorBoost: First and Second-order Boosting Algorithms with Explicit Margin Control (CVPR 2011)**    - Mohammad J. Saberian, Hamed Masnadi-Shirazi, Nuno Vasconcelos    - [[Paper]](https://ieeexplore.ieee.org/document/5995605)    - [[Code]](https://pythonhosted.org/bob.learn.boosting/)    - **Robust and Efficient Regularized Boosting Using Total Bregman Divergence (CVPR 2011)**    - Meizhu Liu, Baba C. Vemuri    - [[Paper]](https://ieeexplore.ieee.org/document/5995686)    - **Treat Samples differently: Object Tracking with Semi-Supervised Online CovBoost (ICCV 2011)**    - Guorong Li, Lei Qin, Qingming Huang, Junbiao Pang, Shuqiang Jiang    - [[Paper]](https://ieeexplore.ieee.org/document/6126297)    - **LinkBoost: A Novel Cost-Sensitive Boosting Framework for Community-Level Network Link Prediction (ICDM 2011)**    - Prakash Mandayam Comar, Pang-Ning Tan, Anil K. Jain    - [[Paper]](http://www.cse.msu.edu/~ptan/papers/icdm2011.pdf)    - **Learning Markov Logic Networks via Functional Gradient Boosting (ICDM 2011)**    - Tushar Khot, Sriraam Natarajan, Kristian Kersting, Jude W. Shavlik    - [[Paper]](https://github.com/starling-lab/BoostSRL)    - [[Code]](https://ieeexplore.ieee.org/document/6137236)    - **Boosting on a Budget: Sampling for Feature-Efficient Prediction (ICML 2011)**    - Lev Reyzin    - [[Paper]](http://www.icml-2011.org/papers/348_icmlpaper.pdf)    - **Multiclass Boosting with Hinge Loss based on Output Coding (ICML 2011)**    - Tianshi Gao, Daphne Koller    - [[Paper]](http://ai.stanford.edu/~tianshig/papers/multiclassHingeBoost-ICML2011.pdf)    - [[Code]](https://github.com/memect/hao/blob/master/awesome/multiclass-boosting.md)    - **Generalized Boosting Algorithms for Convex Optimization (ICML 2011)**    - Alexander Grubb, Drew Bagnell    - [[Paper]](https://arxiv.org/pdf/1105.2054.pdf)    - **Imitation Learning in Relational Domains: A Functional-Gradient Boosting Approach (IJCAI 2011)**    - Sriraam Natarajan, Saket Joshi, Prasad Tadepalli, Kristian Kersting, Jude W. Shavlik    - [[Paper]](http://ftp.cs.wisc.edu/machine-learning/shavlik-group/natarajan.ijcai11.pdf)    - **Boosting with Maximum Adaptive Sampling (NIPS 2011)**    - Charles Dubout, François Fleuret    - [[Paper]](https://papers.nips.cc/paper/4310-boosting-with-maximum-adaptive-sampling)    - **The Fast Convergence of Boosting (NIPS 2011)**    - Matus Telgarsky    - [[Paper]](https://papers.nips.cc/paper/4343-the-fast-convergence-of-boosting)    - **ShareBoost: Efficient Multiclass Learning with Feature Sharing (NIPS 2011)**    - Shai Shalev-Shwartz, Yonatan Wexler, Amnon Shashua    - [[Paper]](https://papers.nips.cc/paper/4213-shareboost-efficient-multiclass-learning-with-feature-sharing)    - **Multiclass Boosting: Theory and Algorithms (NIPS 2011)**    - Mohammad J. Saberian, Nuno Vasconcelos    - [[Paper]](https://papers.nips.cc/paper/4450-multiclass-boosting-theory-and-algorithms.pdf)    - **Variance Penalizing AdaBoost (NIPS 2011)**    - Pannagadatta K. Shivaswamy, Tony Jebara    - [[Paper]](https://papers.nips.cc/paper/4207-variance-penalizing-adaboost.pdf)    - **MKBoost: A Framework of Multiple Kernel Boosting (SDM 2011)**    - Hao Xia, Steven C. H. Hoi    - [[Paper]](https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=3280&context=sis_research)    - **A Boosting Approach to Improving Pseudo-Relevance Feedback (SIGIR 2011)**    - Yuanhua Lv, ChengXiang Zhai, Wan Chen    - [[Paper]](http://www.tyr.unlu.edu.ar/tallerIR/2012/papers/pseudorelevance.pdf)    - **Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models (SIGIR 2011)**    - Yasser Ganjisaffar, Rich Caruana, Cristina Videira Lopes    - [[Paper]](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/materials/bagging_lmbamart_jforests.pdf)    - **Boosting as a Product of Experts (UAI 2011)**    - Narayanan Unny Edakunni, Gary Brown, Tim Kovacs    - [[Paper]](https://arxiv.org/abs/1202.3716)    - **Parallel Boosted Regression Trees for Web Search Ranking (WWW 2011)**    - Stephen Tyree, Kilian Q. Weinberger, Kunal Agrawal, Jennifer Paykin    - [[Paper]](http://www.cs.cornell.edu/~kilian/papers/fr819-tyreeA.pdf)    - [[Code]](https://github.com/YS-L/pgbm)    ## 2010  - **The Boosting Effect of Exploratory Behaviors (AAAI 2010)**    - Jivko Sinapov, Alexander Stoytchev    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/download/1777/2265)    - **Boosting-Based System Combination for Machine Translation (ACL 2010)**    - Tong Xiao, Jingbo Zhu, Muhua Zhu, Huizhen Wang    - [[Paper]](https://www.aclweb.org/anthology/P10-1076)    - **BagBoo: A Scalable Hybrid Bagging-the-Boosting Model (CIKM 2010)**    - Dmitry Yurievich Pavlov, Alexey Gorodilov, Cliff A. Brunk    - [[Paper]](http://cache-default03h.cdn.yandex.net/download.yandex.ru/company/a_scalable_hybrid_bagging_the_boosting_model.pdf)    - [[Code]](https://github.com/arogozhnikov/infiniteboost)    - **Automatic Detection of Craters in Planetary Images: an Embedded Framework Using Feature Selection and Boosting (CIKM 2010)**    - Wei Ding, Tomasz F. Stepinski, Lourenço P. C. Bandeira, Ricardo Vilalta, Youxi Wu, Zhenyu Lu, Tianyu Cao    - [[Paper]](https://www.uh.edu/~rvilalta/papers/2010/cikm10.pdf)    - **Facial Point Detection Using Boosted Regression and Graph Models (CVPR 2010)**    - Michel François Valstar, Brais Martínez, Xavier Binefa, Maja Pantic    - [[Paper]](https://ibug.doc.ic.ac.uk/media/uploads/documents/CVPR-2010-ValstarEtAl-CAMERA.pdf)    - **Boosting for Transfer Learning with Multiple Sources (CVPR 2010)**    - Yi Yao, Gianfranco Doretto    - [[Paper]](https://ieeexplore.ieee.org/document/5539857)    - **Efficient Rotation Invariant Object Detection Using Boosted Random Ferns (CVPR 2010)**    - Michael Villamizar, Francesc Moreno-Noguer, Juan Andrade-Cetto, Alberto Sanfeliu    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.307.4002&rep=rep1&type=pdf)      - **Implicit Hierarchical Boosting for Multi-view Object Detection (CVPR 2010)**    - Xavier Perrotton, Marc Sturzel, Michel Roux    - [[Paper]](https://ieeexplore.ieee.org/document/5540115)    - **On-Line Semi-Supervised Multiple-Instance Boosting (CVPR 2010)**    - Bernhard Zeisl, Christian Leistner, Amir Saffari, Horst Bischof    - [[Paper]](https://ieeexplore.ieee.org/document/5539860)    - **Online Multi-Class LPBoost (CVPR 2010)**    - Amir Saffari, Martin Godec, Thomas Pock, Christian Leistner, Horst Bischof    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.8939&rep=rep1&type=pdf)    - [[Code]](https://github.com/amirsaffari/online-multiclass-lpboost)    - **Homotopy Regularization for Boosting (ICDM 2010)**    - Zheng Wang, Yangqiu Song, Changshui Zhang    - [[Paper]](https://ieeexplore.ieee.org/document/5694094)    - **Exploiting Local Data Uncertainty to Boost Global Outlier Detection (ICDM 2010)**    - Bo Liu, Jie Yin, Yanshan Xiao, Longbing Cao, Philip S. Yu    - [[Paper]](https://ieeexplore.ieee.org/document/5693984)    - **Boosting Classifiers with Tightened L0-Relaxation Penalties (ICML 2010)**    - Noam Goldberg, Jonathan Eckstein    - [[Paper]](https://pdfs.semanticscholar.org/11df/aed4ec2a2f72878789fa3a54d588d693bdda.pdf)    - **Boosting for Regression Transfer (ICML 2010)**    - David Pardoe, Peter Stone    - [[Paper]](https://www.cs.utexas.edu/~dpardoe/papers/ICML10.pdf)    - [[Code]](https://github.com/jay15summer/Two-stage-TrAdaboost.R2)    - **Boosted Backpropagation Learning for Training Deep Modular Networks (ICML 2010)**    - Alexander Grubb, J. Andrew Bagnell    - [[Paper]](https://icml.cc/Conferences/2010/papers/451.pdf)    - **Fast Boosting Using Adversarial Bandits (ICML 2010)**    - Róbert Busa-Fekete, Balázs Kégl    - [[Paper]](https://www.lri.fr/~kegl/research/PDFs/BuKe10.pdf)    - **Boosting with Structure Information in the Functional Space: an Application to Graph Classification (KDD 2010)**    - Hongliang Fei, Jun Huan    - [[Paper]](https://dl.acm.org/citation.cfm?id=1835804.1835886)    - **Multi-task Learning for Boosting with Application to Web Search Ranking (KDD 2010)**    - Olivier Chapelle, Pannagadatta K. Shivaswamy, Srinivas Vadrevu, Kilian Q. Weinberger, Ya Zhang, Belle L. Tseng    - [[Paper]](https://dl.acm.org/citation.cfm?id=1835953)    - **A Theory of Multiclass Boosting (NIPS 2010)**    - Indraneel Mukherjee, Robert E. Schapire    - [[Paper]](http://rob.schapire.net/papers/multiboost-journal.pdf)    - **Boosting Classifier Cascades (NIPS 2010)**    - Mohammad J. Saberian, Nuno Vasconcelos    - [[Paper]](https://papers.nips.cc/paper/4033-boosting-classifier-cascades.pdf)    - **Joint Cascade Optimization Using A Product Of Boosted Classifiers (NIPS 2010)**    - Leonidas Lefakis, François Fleuret    - [[Paper]](https://papers.nips.cc/paper/4148-joint-cascade-optimization-using-a-product-of-boosted-classifiers)    - **Robust LogitBoost and Adaptive Base Class (ABC) LogitBoost (UAI 2010)**    - Ping Li    - [[Paper]](https://arxiv.org/abs/1203.3491)    - [[Code]](https://github.com/pengsun/AOSOLogitBoost)    ## 2009    - **Feature Selection for Ranking Using Boosted Trees (CIKM 2009)**    - Feng Pan, Tim Converse, David Ahn, Franco Salvetti, Gianluca Donato    - [[Paper]](http://www.francosalvetti.com/cikm09_camera2.pdf)    - **Boosting KNN Text Classification Accuracy by Using Supervised Term Weighting Schemes (CIKM 2009)**    - Iyad Batal, Milos Hauskrecht    - [[Paper]](https://people.cs.pitt.edu/~milos/research/CIKM_2009_boosting_KNN.pdf)      - **Stochastic Gradient Boosted Distributed Decision Trees (CIKM 2009)**    - Jerry Ye, Jyh-Herng Chow, Jiang Chen, Zhaohui Zheng    - [[Paper]](http://cse.iitrpr.ac.in/ckn/courses/f2012/thomas.pdf)    - **A General Magnitude-Preserving Boosting Algorithm for Search Ranking (CIKM 2009)**    - Chenguang Zhu, Weizhu Chen, Zeyuan Allen Zhu, Gang Wang, Dong Wang, Zheng Chen    - [[Paper]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cikm2009-1.pdf)      - **Reducing Joint Boost-Based Multiclass Classification to Proximity Search (CVPR 2009)**    - Alexandra Stefan, Vassilis Athitsos, Quan Yuan, Stan Sclaroff    - [[Paper]](https://www.semanticscholar.org/paper/Reducing-JointBoost-based-multiclass-classification-Stefan-Athitsos/08ba1a7d91ce9b4ac26869bfe4bb7c955b0d1a24)    - **Imbalanced RankBoost for Efficiently Ranking Large-Scale Image-Video Collections (CVPR 2009)**    - Michele Merler, Rong Yan, John R. Smith    - [[Paper]](https://www.semanticscholar.org/paper/Imbalanced-RankBoost-for-efficiently-ranking-Merler-Yan/031ba6bf0d6df8bd3aa686ce85791b7d74f0b6d5)    - **Regularized Multi-Class Semi-Supervised Boosting (CVPR 2009)**    - Amir Saffari, Christian Leistner, Horst Bischof    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/5206715)    - **Learning to Associate: HybridBoosted Multi-Target Tracker for Crowded Scene (CVPR 2009)**    - Yuan Li, Chang Huang, Ram Nevatia    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.8335&rep=rep1&type=pdf)    - **Boosted Multi-task Learning for Face Verification with Applications to Web Image and Video Search (CVPR 2009)**    - Xiaogang Wang, Cha Zhang, Zhengyou Zhang    - [[Paper]](http://www.ee.cuhk.edu.hk/~xgwang/webface.pdf)    - **LidarBoost: Depth Superresolution for ToF 3D Shape Scanning (CVPR 2009)**    - Sebastian Schuon, Christian Theobalt, James E. Davis, Sebastian Thrun    - [[Paper]](http://ai.stanford.edu/~schuon/sr/cvpr09_poster_lidarboost.pdf)    - **Model Adaptation via Model Interpolation and Boosting for Web Search Ranking (EMNLP 2009)**    - Jianfeng Gao, Qiang Wu, Chris Burges, Krysta Marie Svore, Yi Su, Nazan Khan, Shalin Shah, Hongyan Zhou    - [[Paper]](https://pdfs.semanticscholar.org/7a82/66335d0b44596574588eabb090bfeae4ab35.pdf)    - **Finding Shareable Informative Patterns and Optimal Coding Matrix for Multiclass Boosting (ICCV 2009)**    - Bang Zhang, Getian Ye, Yang Wang, Jie Xu, Gunawan Herman    - [[Paper]](https://ieeexplore.ieee.org/document/5459146)    - **RankBoost with L1 Regularization for Facial Expression Recognition and Intensity Estimation (ICCV 2009)**    - Peng Yang, Qingshan Liu, Dimitris N. Metaxas    - [[Paper]](https://ieeexplore.ieee.org/document/5459371)    - **A Robust Boosting Tracker with Minimum Error Bound in a Co-Training Framework (ICCV 2009)**    - Rong Liu, Jian Cheng, Hanqing Lu    - [[Paper]](http://nlpr-web.ia.ac.cn/2009papers/gjhy/gh1.pdf)    - **Tutorial Summary: Survey of Boosting from an Optimization Perspective (ICML 2009)**    - Manfred K. Warmuth, S. V. N. Vishwanathan    - [[Paper]](http://www.stat.purdue.edu/~vishy/erlpboost/manfred.pdf)    - **Boosting Products of Base Classifiers (ICML 2009)**    - Balázs Kégl, Róbert Busa-Fekete    - [[Paper]](https://users.lal.in2p3.fr/kegl/research/PDFs/keglBusafekete09.pdf)    - **ABC-boost: Adaptive Base Class Boost for Multi-Class Classification (ICML 2009)**    - Ping Li    - [[Paper]](https://icml.cc/Conferences/2009/papers/417.pdf)    - **Boosting with Structural Sparsity (ICML 2009)**    - John C. Duchi, Yoram Singer    - [[Paper]](https://web.stanford.edu/~jduchi/projects/DuchiSi09a.pdf)    - **Boosting Constrained Mutual Subspace Method for Robust Image-Set Based Object Recognition (IJCAI 2009)**    - Xi Li, Kazuhiro Fukui, Nanning Zheng    - [[Paper]](https://www.researchgate.net/publication/220812439_Boosting_Constrained_Mutual_Subspace_Method_for_Robust_Image-Set_Based_Object_Recognition)    - **Information Theoretic Regularization for Semi-supervised Boosting (KDD 2009)**    - Lei Zheng, Shaojun Wang, Yan Liu, Chi-Hoon Lee    - [[Paper]](https://pdfs.semanticscholar.org/5255/242d50851ce56354e10ae8fdcee6f47591c9.pdf)    - **Potential-Based Agnostic Boosting (NIPS 2009)**    - Adam Kalai, Varun Kanade    - [[Paper]](https://papers.nips.cc/paper/3676-potential-based-agnostic-boosting)    - **Positive Semidefinite Metric Learning with Boosting (NIPS 2009)**    - Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel    - [[Paper]](https://papers.nips.cc/paper/3658-positive-semidefinite-metric-learning-with-boosting)    - **Boosting with Spatial Regularization (NIPS 2009)**    - Zhen James Xiang, Yongxin Taylor Xi, Uri Hasson, Peter J. Ramadge    - [[Paper]](https://papers.nips.cc/paper/3696-boosting-with-spatial-regularization)      - **Effective Boosting of Na%C3%AFve Bayesian Classifiers by Local Accuracy Estimation (PAKDD 2009)**    - Zhipeng Xie    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-642-01307-2_88)    - **Multi-resolution Boosting for Classification and Regression Problems (PAKDD 2009)**    - Chandan K. Reddy, Jin Hyeong Park    - [[Paper]](http://dmkd.cs.vt.edu/papers/PAKDD09.pdf)    - **Efficient Active Learning with Boosting (SDM 2009)**    - Zheng Wang, Yangqiu Song, Changshui Zhang    - [[Paper]](https://pdfs.semanticscholar.org/c8be/b70c37e4b4c4ad77e46b39060c977779d201.pdf)    ## 2008  - **Group-Based Learning: A Boosting Approach (CIKM 2008)**    - Weijian Ni, Jun Xu, Hang Li, Yalou Huang    - [[Paper]](http://www.bigdatalab.ac.cn/~junxu/publications/CIKM2008_GroupLearn.pdf)    - **Semi-Supervised Boosting Using Visual Similarity Learning (CVPR 2008)**    - Christian Leistner, Helmut Grabner, Horst Bischof    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.7914&rep=rep1&type=pdf)    - **Mining Compositional Features for Boosting (CVPR 2008)**    - Junsong Yuan, Jiebo Luo, Ying Wu    - [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587347)    - **Boosted Deformable Model for Human Body Alignment (CVPR 2008)**    - Xiaoming Liu, Ting Yu, Thomas Sebastian, Peter H. Tu    - [[Paper]](https://www.cse.msu.edu/~liuxm/publication/Liu_Yu_Sebastian_Tu_cvpr08.pdf)    - **Discriminative Modeling by Boosting on Multilevel Aggregates (CVPR 2008)**    - Jason J. Corso    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.409.3166&rep=rep1&type=pdf)    - **Face Alignment via Boosted Ranking Model (CVPR 2008)**    - Hao Wu, Xiaoming Liu, Gianfranco Doretto    - [[Paper]](https://ieeexplore.ieee.org/document/4587753)      - **Boosting Adaptive Linear Weak Classifiers for Online Learning and Tracking (CVPR 2008)**    - Toufiq Parag, Fatih Porikli, Ahmed M. Elgammal    - [[Paper]](https://www.merl.com/publications/docs/TR2008-065.pdf)    - **Detection with Multi-Exit Asymmetric Boosting (CVPR 2008)**    - Minh-Tri Pham, V-D. D. Hoang, Tat-Jen Cham    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.330.6364&rep=rep1&type=pdf)    - **Boosting Ordinal Features for Accurate and Fast Iris Recognition (CVPR 2008)**    - Zhaofeng He, Zhenan Sun, Tieniu Tan, Xianchao Qiu, Cheng Zhong, Wenbo Dong    - [[Paper]](https://www.researchgate.net/publication/224323296_Boosting_ordinal_features_for_accurate_and_fast_iris_recognition)    - **Adaptive and Compact Shape Descriptor by Progressive Feature Combination and Selection with Boosting (CVPR 2008)**    - Cheng Chen, Yueting Zhuang, Jun Xiao, Fei Wu    - [[Paper]](https://ieeexplore.ieee.org/document/4587613)      - **Boosting Relational Sequence Alignments (ICDM 2008)**    - Andreas Karwath, Kristian Kersting, Niels Landwehr    - [[Paper]](https://www.cs.uni-potsdam.de/~landwehr/ICDM08boosting.pdf)    - **Boosting with Incomplete Information (ICML 2008)**    - Gholamreza Haffari, Yang Wang, Shaojun Wang, Greg Mori, Feng Jiao    - [[Paper]](http://users.monash.edu.au/~gholamrh/publications/boosting_icml08_slides.pdf)      - **ManifoldBoost: Stagewise Function Approximation for Fully-, Semi- and Un-supervised Learning (ICML 2008)**    - Nicolas Loeff, David A. Forsyth, Deepak Ramachandran    - [[Paper]](http://reason.cs.uiuc.edu/deepak/manifoldboost.pdf)    - **Random Classification Noise Defeats All Convex Potential Boosters (ICML 2008)**    - Philip M. Long, Rocco A. Servedio    - [[Paper]](http://phillong.info/publications/LS09_potential.pdf)    - **Multi-class Cost-Sensitive Boosting with P-norm Loss Functions (KDD 2008)**    - Aurelie C. Lozano, Naoki Abe    - [[Paper]](https://dl.acm.org/citation.cfm?id=1401953)    - **MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features (NIPS 2008)**    - Tae-Kyun Kim, Roberto Cipolla    - [[Paper]](https://papers.nips.cc/paper/3483-mcboost-multiple-classifier-boosting-for-perceptual-co-clustering-of-images-and-visual-features)    - **PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning (NIPS 2008)**    - Chunhua Shen, Alan Welsh, Lei Wang    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.879.7750&rep=rep1&type=pdf)    - **On the Design of Loss Functions for Classification: Theory, Robustness to Outliers, and SavageBoost (NIPS 2008)**    - Hamed Masnadi-Shirazi, Nuno Vasconcelos    - [[Paper]](https://papers.nips.cc/paper/3591-on-the-design-of-loss-functions-for-classification-theory-robustness-to-outliers-and-savageboost)    - **Adaptive Martingale Boosting (NIPS 2008)**    - Philip M. Long, Rocco A. Servedio    - [[Paper]](http://phillong.info/publications/LS08_adaptive_martingale_boosting.pdf)      - **A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data (SIGIR 2008)**    - Massih-Reza Amini, Tuong-Vinh Truong, Cyril Goutte    - [[Paper]](http://ama.liglab.fr/~amini/Publis/SemiSupRanking_sigir08.pdf)    ## 2007    - **Using Error-Correcting Output Codes with Model-Refinement to Boost Centroid Text Classifier (ACL 2007)**    - Songbo Tan    - [[Paper]](https://dl.acm.org/citation.cfm?id=1557794)    - **Fast Human Pose Estimation using Appearance and Motion via Multi-Dimensional Boosting Regression (CVPR 2007)**    - Alessandro Bissacco, Ming-Hsuan Yang, Stefano Soatto    - [[Paper]](http://vision.ucla.edu/papers/bissaccoYS07.pdf)    - **Generic Face Alignment using Boosted Appearance Model (CVPR 2007)**    - Xiaoming Liu    - [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270290)    - **Eigenboosting: Combining Discriminative and Generative Information (CVPR 2007)**    - Helmut Grabner, Peter M. Roth, Horst Bischof    - [[Paper]](https://www.tugraz.at/fileadmin/user_upload/Institute/ICG/Documents/lrs/pubs/grabner_cvpr_07.pdf)    - **Online Learning Asymmetric Boosted Classifiers for Object Detection (CVPR 2007)**    - Minh-Tri Pham, Tat-Jen Cham    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/4270108)    - **Improving Part based Object Detection by Unsupervised Online Boosting (CVPR 2007)**    - Bo Wu, Ram Nevatia    - [[Paper]](https://ieeexplore.ieee.org/document/4270173)    - **A Specialized Processor Suitable for AdaBoost-Based Detection with Haar-like Features (CVPR 2007)**    - Masayuki Hiromoto, Kentaro Nakahara, Hiroki Sugano, Yukihiro Nakamura, Ryusuke Miyamoto    - [[Paper]](https://ieeexplore.ieee.org/document/4270413)    - **Simultaneous Object Detection and Segmentation by Boosting Local Shape Feature based Classifier (CVPR 2007)**    - Bo Wu, Ram Nevatia    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.9795&rep=rep1&type=pdf)    - **Compositional Boosting for Computing Hierarchical Image Structures (CVPR 2007)**    - Tianfu Wu, Gui-Song Xia, Song Chun Zhu    - [[Paper]](https://ieeexplore.ieee.org/document/4270059)    - **Boosting Coded Dynamic Features for Facial Action Units and Facial Expression Recognition (CVPR 2007)**    - Peng Yang, Qingshan Liu, Dimitris N. Metaxas    - [[Paper]](https://ieeexplore.ieee.org/document/4270084)    - **Object Classification in Visual Surveillance Using Adaboost (CVPR 2007)**    - John-Paul Renno, Dimitrios Makris, Graeme A. Jones    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/4270512)    - **A Boosting Regression Approach to Medical Anatomy Detection (CVPR 2007)**    - Shaohua Kevin Zhou, Jinghao Zhou, Dorin Comaniciu    - [[Paper]](http://ww.w.comaniciu.net/Papers/BoostingRegression_CVPR07.pdf)    - **Joint Real-time Object Detection and Pose Estimation Using Probabilistic Boosting Network (CVPR 2007)**    - Jingdan Zhang, Shaohua Kevin Zhou, Leonard McMillan, Dorin Comaniciu    - [[Paper]](http://csbio.unc.edu/mcmillan/pubs/CVPR07_Zhang.pdf)    - **Kernel Sharing With Joint Boosting For Multi-Class Concept Detection (CVPR 2007)**    - Wei Jiang, Shih-Fu Chang, Alexander C. Loui    - [[Paper]](http://www.ee.columbia.edu/~wjiang/references/jiangcvprws07.pdf)    - **Scale-Space Based Weak Regressors for Boosting (ECML 2007)**    - Jin Hyeong Park, Chandan K. Reddy    - [[Paper]](http://www.cs.wayne.edu/~reddy/Papers/ECML07.pdf)    - **Avoiding Boosting Overfitting by Removing Confusing Samples (ECML 2007)**    - Alexander Vezhnevets, Olga Barinova    - [[Paper]](http://groups.inf.ed.ac.uk/calvin/hp_avezhnev/Pubs/AvoidingBoostingOverfitting.pdf)    - **DynamicBoost: Boosting Time Series Generated by Dynamical Systems (ICCV 2007)**    - René Vidal, Paolo Favaro    - [[Paper]](http://vision.jhu.edu/assets/VidalICCV07.pdf)    - **Incremental Learning of Boosted Face Detector (ICCV 2007)**    - Chang Huang, Haizhou Ai, Takayoshi Yamashita, Shihong Lao, Masato Kawade    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.9012&rep=rep1&type=pdf)    - **Gradient Feature Selection for Online Boosting (ICCV 2007)**    - Xiaoming Liu, Ting Yu    - [[Paper]](https://www.cse.msu.edu/~liuxm/publication/Liu_Yu_ICCV2007.pdf)    - **Fast Training and Selection of Haar Features Using Statistics in Boosting-based Face Detection (ICCV 2007)**    - Minh-Tri Pham, Tat-Jen Cham    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.212.6173&rep=rep1&type=pdf)    - **Cluster Boosted Tree Classifier for Multi-View - Multi-Pose Object Detection (ICCV 2007)**    - Bo Wu, Ramakant Nevatia    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.9885&rep=rep1&type=pdf)    - **Asymmetric Boosting (ICML 2007)**    - Hamed Masnadi-Shirazi, Nuno Vasconcelos    - [[Paper]](http://www.svcl.ucsd.edu/publications/conference/2007/icml07/AsymmetricBoosting.pdf)    - **Boosting for Transfer Learning (ICML 2007)**    - Wenyuan Dai, Qiang Yang, Gui-Rong Xue, Yong Yu    - [[Paper]](http://www.cs.ust.hk/~qyang/Docs/2007/tradaboost.pdf)      - **Gradient Boosting for Kernelized Output Spaces (ICML 2007)**    - Pierre Geurts, Louis Wehenkel, Florence d'Alché-Buc    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.435.3970&rep=rep1&type=pdf)    - **Boosting a Complete Technique to Find MSS and MUS Thanks to a Local Search Oracle (IJCAI 2007)**    - Éric Grégoire, Bertrand Mazure, Cédric Piette    - [[Paper]](http://www.cril.univ-artois.fr/~piette/IJCAI07_HYCAM.pdf)    - **Training Conditional Random Fields Using Virtual Evidence Boosting (IJCAI 2007)**    - Lin Liao, Tanzeem Choudhury, Dieter Fox, Henry A. Kautz    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/407.pdf)    - **Simple Training of Dependency Parsers via Structured Boosting (IJCAI 2007)**    - Qin Iris Wang, Dekang Lin, Dale Schuurmans    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/284.pdf)    - **Real Boosting a la Carte with an Application to Boosting Oblique Decision Tree (IJCAI 2007)**    - Claudia Henry, Richard Nock, Frank Nielsen    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/135.pdf)    - **Managing Domain Knowledge and Multiple Models with Boosting (IJCAI 2007)**    - Peng Zang, Charles Lee Isbell Jr.    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/185.pdf)    - **Model-Shared Subspace Boosting for Multi-label Classification (KDD 2007)**    - Rong Yan, Jelena Tesic, John R. Smith    - [[Paper]](http://rogerioferis.com/VisualRecognitionAndSearch2014/material/papers/IMARSKDD2007.pdf)    - **Regularized Boost for Semi-Supervised Learning (NIPS 2007)**    - Ke Chen, Shihai Wang    - [[Paper]](https://papers.nips.cc/paper/3167-regularized-boost-for-semi-supervised-learning.pdf)    - **Boosting Algorithms for Maximizing the Soft Margin (NIPS 2007)**    - Manfred K. Warmuth, Karen A. Glocer, Gunnar Rätsch    - [[Paper]](https://papers.nips.cc/paper/3374-boosting-algorithms-for-maximizing-the-soft-margin.pdf)    - **McRank: Learning to Rank Using Multiple Classification and Gradient Boosting (NIPS 2007)**    - Ping Li, Christopher J. C. Burges, Qiang Wu    - [[Paper]](https://papers.nips.cc/paper/3270-mcrank-learning-to-rank-using-multiple-classification-and-gradient-boosting.pdf)    - **One-Pass Boosting (NIPS 2007)**    - Zafer Barutçuoglu, Philip M. Long, Rocco A. Servedio    - [[Paper]](http://phillong.info/publications/BLS07_one_pass.pdf)    - **Boosting the Area under the ROC Curve (NIPS 2007)**    - Philip M. Long, Rocco A. Servedio    - [[Paper]](https://papers.nips.cc/paper/3247-boosting-the-area-under-the-roc-curve.pdf)    - **FilterBoost: Regression and Classification on Large Datasets (NIPS 2007)**    - Joseph K. Bradley, Robert E. Schapire    - [[Paper]](http://rob.schapire.net/papers/FilterBoost_paper.pdf)    - **A General Boosting Method and its Application to Learning Ranking Functions for Web Search (NIPS 2007)**    - Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier Chapelle, Keke Chen, Gordon Sun    - [[Paper]](https://pdfs.semanticscholar.org/8f8d/874a3f0217289ba317b1f6175ac3b6f73d70.pdf)    - **Efficient Multiclass Boosting Classification with Active Learning (SDM 2007)**    - Jian Huang, Seyda Ertekin, Yang Song, Hongyuan Zha, C. Lee Giles    - [[Paper]](https://epubs.siam.org/doi/abs/10.1137/1.9781611972771.27)    - **AdaRank: a Boosting Algorithm for Information Retrieval (SIGIR 2007)**    - Jun Xu, Hang Li    - [[Paper]](http://www.bigdatalab.ac.cn/~junxu/publications/SIGIR2007_AdaRank.pdf)    ## 2006    - **Gradient Boosting for Sequence Alignment (AAAI 2006)**    - Charles Parker, Alan Fern, Prasad Tadepalli    - [[Paper]](http://web.engr.oregonstate.edu/~afern/papers/aaai06-align.pdf)    - **Boosting Kernel Models for Regression (ICDM 2006)**    - Ping Sun, Xin Yao    - [[Paper]](https://www.cs.bham.ac.uk/~xin/papers/icdm06SunYao.pdf)    - **Boosting for Learning Multiple Classes with Imbalanced Class Distribution (ICDM 2006)**    - Yanmin Sun, Mohamed S. Kamel, Yang Wang    - [[Paper]](http://people.ee.duke.edu/~lcarin/ImbalancedClassDistribution.pdf)    - **Boosting the Feature Space: Text Classification for Unstructured Data on the Web (ICDM 2006)**    - Yang Song, Ding Zhou, Jian Huang, Isaac G. Councill, Hongyuan Zha, C. Lee Giles    - [[Paper]](http://sonyis.me/paperpdf/icdm06_song.pdf)    - **Totally Corrective Boosting Algorithms that Maximize the Margin (ICML 2006)**    - Manfred K. Warmuth, Jun Liao, Gunnar Rätsch    - [[Paper]](https://users.soe.ucsc.edu/~manfred/pubs/C75.pdf)      - **How Boosting the Margin Can Also Boost Classifier Complexity (ICML 2006)**    - Lev Reyzin, Robert E. Schapire    - [[Paper]](http://rob.schapire.net/papers/boost_complexity.pdf)    - **Multiclass Boosting with Repartitioning (ICML 2006)**    - Ling Li    - [[Paper]](https://authors.library.caltech.edu/72259/1/p569-li.pdf)    - **AdaBoost is Consistent (NIPS 2006)**    - Peter L. Bartlett, Mikhail Traskin    - [[Paper]](http://jmlr.csail.mit.edu/papers/volume8/bartlett07b/bartlett07b.pdf)    - **Boosting Structured Prediction for Imitation Learning (NIPS 2006)**    - Nathan D. Ratliff, David M. Bradley, J. Andrew Bagnell, Joel E. Chestnutt    - [[Paper]](https://papers.nips.cc/paper/3154-boosting-structured-prediction-for-imitation-learning.pdf)    - **Chained Boosting (NIPS 2006)**    - Christian R. Shelton, Wesley Huie, Kin Fai Kan    - [[Paper]](https://papers.nips.cc/paper/2981-chained-boosting)      - **When Efficient Model Averaging Out-Performs Boosting and Bagging (PKDD 2006)**    - Ian Davidson, Wei Fan    - [[Paper]](https://link.springer.com/chapter/10.1007/11871637_46)    ## 2005  - **Semantic Place Classification of Indoor Environments with Mobile Robots Using Boosting (AAAI 2005)**    - Axel Rottmann, Óscar Martínez Mozos, Cyrill Stachniss, Wolfram Burgard    - [[Paper]](http://www2.informatik.uni-freiburg.de/~stachnis/pdf/rottmann05aaai.pdf)      - **Boosting-based Parse Reranking with Subtree Features (ACL 2005)**    - Taku Kudo, Jun Suzuki, Hideki Isozaki    - [[Paper]](http://chasen.org/~taku/publications/acl2005.pdf)    - **Using RankBoost to Compare Retrieval Systems (CIKM 2005)**    - Huyen-Trang Vu, Patrick Gallinari    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9470&rep=rep1&type=pdf)    - **Classifier Fusion Using Shared Sampling Distribution for Boosting (ICDM 2005)**    - Costin Barbu, Raja Tanveer Iqbal, Jing Peng    - [[Paper]](https://ieeexplore.ieee.org/document/1565659)    - **Semi-Supervised Mixture of Kernels via LPBoost Methods (ICDM 2005)**    - Jinbo Bi, Glenn Fung, Murat Dundar, R. Bharat Rao    - [[Paper]](https://ieeexplore.ieee.org/document/1565728)    - **Efficient Discriminative Learning of Bayesian Network Classifier via Boosted Augmented Naive Bayes (ICML 2005)**    - Yushi Jing, Vladimir Pavlovic, James M. Rehg    - [[Paper]](http://mrl.isr.uc.pt/pub/bscw.cgi/d27355/Jing05Efficient.pdf)    - **Unifying the Error-Correcting and Output-Code AdaBoost within the Margin Framework (ICML 2005)**    - Yijun Sun, Sinisa Todorovic, Jian Li, Dapeng Wu    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.138.4246&rep=rep1&type=pdf)    - **A Smoothed Boosting Algorithm Using Probabilistic Output Codes (ICML 2005)**    - Rong Jin, Jian Zhang    - [[Paper]](http://www.stat.purdue.edu/~jianzhan/papers/icml05jin.pdf)    - **Robust Boosting and its Relation to Bagging (KDD 2005)**    - Saharon Rosset    - [[Paper]](https://www.tau.ac.il/~saharon/papers/bagboost.pdf)    - **Efficient Computations via Scalable Sparse Kernel Partial Least Squares and Boosted Latent Features (KDD 2005)**    - Michinari Momma    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.387.2078&rep=rep1&type=pdf)    - **Multiple Instance Boosting for Object Detection (NIPS 2005)**    - Paul A. Viola, John C. Platt, Cha Zhang    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.138.8312&rep=rep1&type=pdf)    - **Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations (NIPS 2005)**    - Aurelie C. Lozano, Sanjeev R. Kulkarni, Robert E. Schapire    - [[Paper]](https://www.cs.princeton.edu/~schapire/papers/betamix.pdf)      - **Boosted decision trees for word recognition in handwritten document retrieval (SIGIR 2005)**    - Nicholas R. Howe, Toni M. Rath, R. Manmatha    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.1551&rep=rep1&type=pdf)      - **Obtaining Calibrated Probabilities from Boosting (UAI 2005)**    - Alexandru Niculescu-Mizil, Rich Caruana    - [[Paper]](https://www.cs.cornell.edu/~caruana/niculescu.scldbst.crc.rev4.pdf)    ## 2004    - **Online Parallel Boosting (AAAI 2004)**    - Jesse A. Reichler, Harlan D. Harris, Michael A. Savchenko    - [[Paper]](https://www.aaai.org/Papers/AAAI/2004/AAAI04-059.pdf)    - **A Boosting Approach to Multiple Instance Learning (ECML 2004)**    - Peter Auer, Ronald Ortner    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-30115-8_9)    - **A Boosting Algorithm for Classification of Semi-Structured Text (EMNLP 2004)**    - Taku Kudo, Yuji Matsumoto    - [[Paper]](https://www.aclweb.org/anthology/W04-3239)    - **Text Classification by Boosting Weak Learners based on Terms and Concepts (ICDM 2004)**    - Stephan Bloehdorn, Andreas Hotho    - [[Paper]](https://ieeexplore.ieee.org/document/1410303)    - **Boosting Grammatical Inference with Confidence Oracles (ICML 2004)**    - Jean-Christophe Janodet, Richard Nock, Marc Sebban, Henri-Maxime Suchier    - [[Paper]](http://www1.univ-ag.fr/~rnock/Articles/Drafts/icml04-jnss.pdf)    - **Surrogate Maximization/Minimization Algorithms for AdaBoost and the Logistic Regression Model (ICML 2004)**    - Zhihua Zhang, James T. Kwok, Dit-Yan Yeung    - [[Paper]](https://icml.cc/Conferences/2004/proceedings/papers/77.pdf)    - **Training Conditional Random Fields via Gradient Tree Boosting (ICML 2004)**    - Thomas G. Dietterich, Adam Ashenfelter, Yaroslav Bulatov    - [[Paper]](http://web.engr.oregonstate.edu/~tgd/publications/ml2004-treecrf.pdf)    - **Boosting Margin Based Distance Functions for Clustering (ICML 2004)**    - Tomer Hertz, Aharon Bar-Hillel, Daphna Weinshall    - [[Paper]](http://www.cs.huji.ac.il/~daphna/papers/distboost-icml.pdf)    - **Column-Generation Boosting Methods for Mixture of Kernels (KDD 2004)**    - Jinbo Bi, Tong Zhang, Kristin P. Bennett    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.6359&rep=rep1&type=pdf)    - **Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging (NIPS 2004)**    - Vladimir Koltchinskii, Manel Martínez-Ramón, Stefan Posse    - [[Paper]](https://papers.nips.cc/paper/2699-optimal-aggregation-of-classifiers-and-boosting-maps-in-functional-magnetic-resonance-imaging.pdf)    - **Boosting on Manifolds: Adaptive Regularization of Base Classifiers (NIPS 2004)**    - Balázs Kégl, Ligen Wang    - [[Paper]](https://papers.nips.cc/paper/2613-boosting-on-manifolds-adaptive-regularization-of-base-classifiers)    - **Contextual Models for Object Detection Using Boosted Random Fields (NIPS 2004)**    - Antonio Torralba, Kevin P. Murphy, William T. Freeman    - [[Paper]](https://www.cs.ubc.ca/~murphyk/Papers/BRF-nips04-camera.pdf)    - **Generalization Error and Algorithmic Convergence of Median Boosting (NIPS 2004)**    - Balázs Kégl    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.8990&rep=rep1&type=pdf)    - **An Application of Boosting to Graph Classification (NIPS 2004)**    - Taku Kudo, Eisaku Maeda, Yuji Matsumoto    - [[Paper]](https://papers.nips.cc/paper/2739-an-application-of-boosting-to-graph-classification)    - **Logistic Regression and Boosting for Labeled Bags of Instances (PAKDD 2004)**    - Xin Xu, Eibe Frank    - [[Paper]](https://www.cs.waikato.ac.nz/~ml/publications/2004/xu-frank.pdf)    - **Fast and Light Boosting for Adaptive Mining of Data Streams (PAKDD 2004)**    - Fang Chu, Carlo Zaniolo    - [[Paper]](http://web.cs.ucla.edu/~zaniolo/papers/NBCAJMW77MW0J8CP.pdf)    ## 2003  - **On Boosting and the Exponential Loss (AISTATS 2003)**    - Abraham J. Wyner    - [[Paper]](http://www-stat.wharton.upenn.edu/~ajw/exploss.ps)    - **Boosting Support Vector Machines for Text Classification through Parameter-Free Threshold Relaxation (CIKM 2003)**    - James G. Shanahan, Norbert Roma    - [[Paper]](https://dl.acm.org/citation.cfm?id=956911)    - **Learning Cross-Document Structural Relationships Using Boosting (CIKM 2003)**    - Zhu Zhang, Jahna Otterbacher, Dragomir R. Radev    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.7712&rep=rep1&type=pdf)      - **On Boosting Improvement: Error Reduction and Convergence Speed-Up (ECML 2003)**    - Marc Sebban, Henri-Maxime Suchier    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-39857-8_32)    - **Boosting Lazy Decision Trees (ICML 2003)**    - Xiaoli Zhang Fern, Carla E. Brodley    - [[Paper]](https://www.aaai.org/Papers/ICML/2003/ICML03-026.pdf)    - **On the Convergence of Boosting Procedures (ICML 2003)**    - Tong Zhang, Bin Yu    - [[Paper]](https://pdfs.semanticscholar.org/dd3f/901b232280533fbdb9e57f144f44723617cf.pdf)    - **Linear Programming Boosting for Uneven Datasets (ICML 2003)**    - Jure Leskovec, John Shawe-Taylor    - [[Paper]](https://cs.stanford.edu/people/jure/pubs/textbooster-icml03.pdf)    - **Monte Carlo Theory as an Explanation of Bagging and Boosting (IJCAI 2003)**    - Roberto Esposito, Lorenza Saitta    - [[Paper]](https://dl.acm.org/citation.cfm?id=1630733)    - **On the Dynamics of Boosting (NIPS 2003)**    - Cynthia Rudin, Ingrid Daubechies, Robert E. Schapire    - [[Paper]](https://papers.nips.cc/paper/2535-on-the-dynamics-of-boosting)    - **Mutual Boosting for Contextual Inference (NIPS 2003)**    - Michael Fink, Pietro Perona    - [[Paper]](https://papers.nips.cc/paper/2520-mutual-boosting-for-contextual-inference)    - **Boosting Versus Covering (NIPS 2003)**    - Kohei Hatano, Manfred K. Warmuth    - [[Paper]](https://papers.nips.cc/paper/2532-boosting-versus-covering)    - **Multiple-Instance Learning via Disjunctive Programming Boosting (NIPS 2003)**    - Stuart Andrews, Thomas Hofmann    - [[Paper]](https://papers.nips.cc/paper/2478-multiple-instance-learning-via-disjunctive-programming-boosting)    - **Averaged Boosting: A Noise-Robust Ensemble Method (PAKDD 2003)**    - Yongdai Kim    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-36175-8_38)    - **SMOTEBoost: Improving Prediction of the Minority Class in Boosting (PKDD 2003)**    - Nitesh V. Chawla, Aleksandar Lazarevic, Lawrence O. Hall, Kevin W. Bowyer    - [[Paper]](https://www3.nd.edu/~nchawla/papers/ECML03.pdf)    ## 2002    - **Minimum Majority Classification and Boosting (AAAI 2002)**    - Philip M. Long    - [[Paper]](http://phillong.info/publications/minmaj.pdf)    - **Ranking Algorithms for Named Entity Extraction: Boosting and the Voted Perceptron (ACL 2002)**    - Michael Collins    - [[Paper]](https://www.aclweb.org/anthology/P02-1062)    - **Boosting to Correct Inductive Bias in Text Classification (CIKM 2002)**    - Yan Liu, Yiming Yang, Jaime G. Carbonell    - [[Paper]](https://dl.acm.org/citation.cfm?id=584792.584850)      - **How to Make AdaBoost.M1 Work for Weak Base Classifiers by Changing Only One Line of the Code (ECML 2002)**    - Günther Eibl, Karl Peter Pfeiffer    - [[Paper]](https://dl.acm.org/citation.cfm?id=650068)    - **Scaling Boosting by Margin-Based Inclusionof Features and Relations (ECML 2002)**    - Susanne Hoche, Stefan Wrobel    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-36755-1_13)    - **A Robust Boosting Algorithm (ECML 2002)**    - Richard Nock, Patrice Lefaucheur    - [[Paper]](https://dl.acm.org/citation.cfm?id=650081)    - **iBoost: Boosting Using an instance-Based Exponential Weighting Scheme (ECML 2002)**    - Stephen Kwek, Chau Nguyen    - [[Paper]](https://www.researchgate.net/publication/220516082_iBoost_Boosting_using_an_instance-based_exponential_weighting_scheme)    - **Boosting Density Function Estimators (ECML 2002)**    - Franck Thollard, Marc Sebban, Philippe Ézéquel    - [[Paper]](https://link.springer.com/chapter/10.1007%2F3-540-36755-1_36)      - **Statistical Behavior and Consistency of Support Vector Machines, Boosting, and Beyond (ICML 2002)**    - Tong Zhang    - [[Paper]](https://www.researchgate.net/publication/221344927_Statistical_Behavior_and_Consistency_of_Support_Vector_Machines_Boosting_and_Beyond)    - **A Boosted Maximum Entropy Model for Learning Text Chunking (ICML 2002)**    - Seong-Bae Park, Byoung-Tak Zhang    - [[Paper]](https://www.researchgate.net/publication/221345636_A_Boosted_Maximum_Entropy_Model_for_Learning_Text_Chunking)    - **Towards Large Margin Speech Recognizers by Boosting and Discriminative Training (ICML 2002)**    - Carsten Meyer, Peter Beyerlein    - [[Paper]](https://www.semanticscholar.org/paper/Towards-Large-Margin-Speech-Recognizers-by-Boosting-Meyer-Beyerlein/8408479e36da812cdbf6bc15f7849c3e76a1016d)    - **Incorporating Prior Knowledge into Boosting (ICML 2002)**    - Robert E. Schapire, Marie Rochery, Mazin G. Rahim, Narendra K. Gupta    - [[Paper]](http://rob.schapire.net/papers/boostknowledge.pdf)    - **Modeling Auction Price Uncertainty Using Boosting-based Conditional Density Estimation (ICML 2002)**    - Robert E. Schapire, Peter Stone, David A. McAllester, Michael L. Littman, János A. Csirik    - [[Paper]](http://www.cs.utexas.edu/~ai-lab/pubs/ICML02-tac.pdf)    - **MARK: A Boosting Algorithm for Heterogeneous Kernel Models (KDD 2002)**    - Kristin P. Bennett, Michinari Momma, Mark J. Embrechts    - [[Paper]](http://homepages.rpiscrews.us/~bennek/papers/kdd2.pdf)    - **Predicting rare classes: can boosting make any weak learner strong (KDD 2002)**    - Mahesh V. Joshi, Ramesh C. Agarwal, Vipin Kumar    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.1159&rep=rep1&type=pdf)    - **Kernel Design Using Boosting (NIPS 2002)**    - Koby Crammer, Joseph Keshet, Yoram Singer    - [[Paper]](https://pdfs.semanticscholar.org/ff79/344807e972fdd7e5e1c3ed5c539dd1aeecbe.pdf)    - **FloatBoost Learning for Classification (NIPS 2002)**    - Stan Z. Li, ZhenQiu Zhang, Heung-Yeung Shum, HongJiang Zhang    - [[Paper]](https://pdfs.semanticscholar.org/8ccc/5ef87eab96a4cae226750eba8322b30606ea.pdf)    - **Discriminative Learning for Label Sequences via Boosting (NIPS 2002)**    - Yasemin Altun, Thomas Hofmann, Mark Johnson    - [[Paper]](http://web.science.mq.edu.au/~mjohnson/papers/nips02.pdf)    - **Boosting Density Estimation (NIPS 2002)**    - Saharon Rosset, Eran Segal    - [[Paper]](https://papers.nips.cc/paper/2298-boosting-density-estimation.pdf)    - **Self Supervised Boosting (NIPS 2002)**    - Max Welling, Richard S. Zemel, Geoffrey E. Hinton    - [[Paper]](https://pdfs.semanticscholar.org/6a2a/f112a803e70c23b7055de2e73007cf42c301.pdf)    - **Boosted Dyadic Kernel Discriminants (NIPS 2002)**    - Baback Moghaddam, Gregory Shakhnarovich    - [[Paper]](http://www.merl.com/publications/docs/TR2002-55.pdf)      - **A Method to Boost Support Vector Machines (PAKDD 2002)**    - Lili Diao, Keyun Hu, Yuchang Lu, Chunyi Shi    - [[Paper]](https://elkingarcia.github.io/Papers/MLDM07.pdf)    - **A Method to Boost Naive Bayesian Classifiers (PAKDD 2002)**    - Lili Diao, Keyun Hu, Yuchang Lu, Chunyi Shi    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-47887-6_11)    - **Predicting Rare Classes: Comparing Two-Phase Rule Induction to Cost-Sensitive Boosting (PKDD 2002)**    - Mahesh V. Joshi, Ramesh C. Agarwal, Vipin Kumar    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45681-3_20)    - **Iterative Data Squashing for Boosting Based on a Distribution-Sensitive Distance (PKDD 2002)**    - Yuta Choki, Einoshin Suzuki    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45681-3_8)    - **Staged Mixture Modelling and Boosting (UAI 2002)**    - Christopher Meek, Bo Thiesson, David Heckerman    - [[Paper]](https://arxiv.org/abs/1301.0586)    - **Advances in Boosting (UAI 2002)**    - Robert E. Schapire    - [[Paper]](http://rob.schapire.net/papers/uai02.pdf)    ## 2001  - **Is Regularization Unnecessary for Boosting? (AISTATS 2001)**    - Wenxin Jiang    - [[Paper]](https://www.researchgate.net/publication/2439718_Is_Regularization_Unnecessary_for_Boosting)    - **Online Bagging and Boosting (AISTATS 2001)**    - Nikunj C. Oza, Stuart J. Russell    - [[Paper]](https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf)      - **Text Categorization Using Transductive Boosting (ECML 2001)**    - Hirotoshi Taira, Masahiko Haruno    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-44795-4_39)    - **Improving Term Extraction by System Combination Using Boosting (ECML 2001)**    - Jordi Vivaldi, Lluís Màrquez, Horacio Rodríguez    - [[Paper]](https://dl.acm.org/citation.cfm?id=3108351)    - **Analysis of the Performance of AdaBoost.M2 for the Simulated Digit-Recognition-Example (ECML 2001)**    - Günther Eibl, Karl Peter Pfeiffer    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-44795-4_10)    - **On the Practice of Branching Program Boosting (ECML 2001)**    - Tapio Elomaa, Matti Kääriäinen    - [[Paper]](https://www.researchgate.net/publication/221112522_On_the_Practice_of_Branching_Program_Boosting)    - **Boosting Mixture Models for Semi-supervised Learning (ICANN 2001)**    - Yves Grandvalet, Florence d'Alché-Buc, Christophe Ambroise    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-44668-0_7    - **A Comparison of Stacking with Meta Decision Trees to Bagging, Boosting, and Stacking with other Methods (ICDM 2001)**    - Bernard Zenko, Ljupco Todorovski, Saso Dzeroski    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.3118&rep=rep1&type=pdf)    - **Using Boosting to Simplify Classification Models (ICDM 2001)**    - Virginia Wheway    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/989565)    - **Evaluating Boosting Algorithms to Classify Rare Classes: Comparison and Improvements (ICDM 2001)**    - Mahesh V. Joshi, Vipin Kumar, Ramesh C. Agarwal    - [[Paper]](https://pdfs.semanticscholar.org/b829/fe743e4beeeed65d32d2d7931354df7a2f60.pdf)    - [[Code]]( )    - **Boosting Neighborhood-Based Classifiers (ICML 2001)**    - Marc Sebban, Richard Nock, Stéphane Lallich    - [[Paper]](https://www.semanticscholar.org/paper/Boosting-Neighborhood-Based-Classifiers-Sebban-Nock/ee88e3bbe8a7e81cae7ee53da2c824de7c82f882)    - **Boosting Noisy Data (ICML 2001)**    - Abba Krieger, Chuan Long, Abraham J. Wyner    - [[Paper]](https://www.researchgate.net/profile/Abba_Krieger/publication/221345435_Boosting_Noisy_Data/links/00463528a1ba641692000000.pdf)    - **Some Theoretical Aspects of Boosting in the Presence of Noisy Data (ICML 2001)**    - Wenxin Jiang    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=2494A2C06ACA22FA971AC1C29B53FF62?doi=10.1.1.27.7231&rep=rep1&type=pdf)    - **Filters, Wrappers and a Boosting-Based Hybrid for Feature Selection (ICML 2001)**    - Sanmay Das    - [[Paper]](https://pdfs.semanticscholar.org/93b6/25a0e35b59fa6a3e7dc1cbdb31268d62d69f.pdf)    - **The Distributed Boosting Algorithm (KDD 2001)**    - Aleksandar Lazarevic, Zoran Obradovic    - [[Paper]](https://www.researchgate.net/publication/2488971_The_Distributed_Boosting_Algorithm)    - **Experimental Comparisons of Online and Batch Versions of Bagging and Boosting (KDD 2001)**    - Nikunj C. Oza, Stuart J. Russell    - [[Paper]](https://people.eecs.berkeley.edu/~russell/papers/kdd01-online.pdf)    - **Semi-supervised MarginBoost (NIPS 2001)**    - Florence d'Alché-Buc, Yves Grandvalet, Christophe Ambroise    - [[Paper]](https://pdfs.semanticscholar.org/2197/f1c2d55827b6928cc80030922569acce2d6c.pdf)    - **Boosting and Maximum Likelihood for Exponential Models (NIPS 2001)**    - Guy Lebanon, John D. Lafferty    - [[Paper]](https://papers.nips.cc/paper/2042-boosting-and-maximum-likelihood-for-exponential-models.pdf)    - **Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade (NIPS 2001)**    - Paul A. Viola, Michael J. Jones    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.4306&rep=rep1&type=pdf)      - **Boosting Localized Classifiers in Heterogeneous Databases (SDM 2001)**    - Aleksandar Lazarevic, Zoran Obradovic    - [[Paper]](https://epubs.siam.org/doi/abs/10.1137/1.9781611972719.14)    ## 2000  - **Boosted Wrapper Induction (AAAI 2000)**    - Dayne Freitag, Nicholas Kushmerick    - [[Paper]](https://pdfs.semanticscholar.org/d009/a2bd48a9d1971fbc0d99f6df00539a62048a.pdf)    - **An Improved Boosting Algorithm and its Application to Text Categorization (CIKM 2000)**    - Fabrizio Sebastiani, Alessandro Sperduti, Nicola Valdambrini    - [[Paper]](http://nmis.isti.cnr.it/sebastiani/Publications/CIKM00.pdf)    - **Boosting for Document Routing (CIKM 2000)**    - Raj D. Iyer, David D. Lewis, Robert E. Schapire, Yoram Singer, Amit Singhal    - [[Paper]](http://singhal.info/cikm-2000.pdf)    - **On the Boosting Pruning Problem (ECML 2000)**    - Christino Tamon, Jie Xiang    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45164-1_41)    - **Boosting Applied to Word Sense Disambiguation (ECML 2000)**    - Gerard Escudero, Lluís Màrquez, German Rigau    - [[Paper]](https://dl.acm.org/citation.cfm?id=649539)    - **An Empirical Study of MetaCost Using Boosting Algorithms (ECML 2000)**    - Kai Ming Ting    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.1624&rep=rep1&type=pdf)    - **FeatureBoost: A Meta-Learning Algorithm that Improves Model Robustness (ICML 2000)**    - Joseph O'Sullivan, John Langford, Rich Caruana, Avrim Blum    - [[Paper]](https://www.researchgate.net/publication/221345746_FeatureBoost_A_Meta-Learning_Algorithm_that_Improves_Model_Robustness)    - **Comparing the Minimum Description Length Principle and Boosting in the Automatic Analysis of Discourse (ICML 2000)**    - Tadashi Nomoto, Yuji Matsumoto    - [[Paper]](https://www.researchgate.net/publication/221344998_Comparing_the_Minimum_Description_Length_Principle_and_Boosting_in_the_Automatic_Analysis_of_Discourse)    - **A Boosting Approach to Topic Spotting on Subdialogues (ICML 2000)**    - Kary Myers, Michael J. Kearns, Satinder P. Singh, Marilyn A. Walker    - [[Paper]](https://www.cis.upenn.edu/~mkearns/papers/topicspot.pdf)    - **A Comparative Study of Cost-Sensitive Boosting Algorithms (ICML 2000)**    - Kai Ming Ting    - [[Paper]](https://dl.acm.org/citation.cfm?id=657944)    - **Boosting a Positive-Data-Only Learner (ICML 2000)**    - Andrew R. Mitchell    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.3669)    - **A Column Generation Algorithm For Boosting (ICML 2000)**    - Kristin P. Bennett, Ayhan Demiriz, John Shawe-Taylor    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1828D5853F656BD6892E9C2C446ECC68?doi=10.1.1.16.9612&rep=rep1&type=pdf)    - **A Gradient-Based Boosting Algorithm for Regression Problems (NIPS 2000)**    - Richard S. Zemel, Toniann Pitassi    - [[Paper]](https://pdfs.semanticscholar.org/c41a/9417f5605b55bdd216d119e47669a92f5c50.pdf)    - **Weak Learners and Improved Rates of Convergence in Boosting (NIPS 2000)**    - Shie Mannor, Ron Meir    - [[Paper]](https://papers.nips.cc/paper/1906-weak-learners-and-improved-rates-of-convergence-in-boosting.pdf)    - **Adaptive Boosting for Spatial Functions with Unstable Driving Attributes (PAKDD 2000)**    - Aleksandar Lazarevic, Tim Fiez, Zoran Obradovic    - [[Paper]](http://www.dabi.temple.edu/~zoran/papers/lazarevic01j.pdf)    - **Scaling Up a Boosting-Based Learner via Adaptive Sampling (PAKDD 2000)**    - Carlos Domingo, Osamu Watanabe    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45571-X_37)    - **Learning First Order Logic Time Series Classifiers: Rules and Boosting (PKDD 2000)**    - Juan J. Rodríguez Diez, Carlos Alonso González, Henrik Boström    - [[Paper]](https://people.dsv.su.se/~henke/papers/rodriguez00b.pdf)    - **Bagging and Boosting with Dynamic Integration of Classifiers (PKDD 2000)**    - Alexey Tsymbal, Seppo Puuronen    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45372-5_12)    - **Text Filtering by Boosting Naive Bayes Classifiers (SIGIR 2000)**    - Yu-Hwan Kim, Shang-Yoon Hahn, Byoung-Tak Zhang    - [[Paper]](https://www.researchgate.net/publication/221299823_Text_filtering_by_boosting_Naive_Bayes_classifiers)    ## 1999  - **Boosting Methodology for Regression Problems (AISTATS 1999)**    - Greg Ridgeway, David Madigan, Thomas Richardson    - [[Paper]](https://pdfs.semanticscholar.org/5f19/6a8baa281b2190c4519305bec8f5c91c8e5a.pdf)    - **Boosting Applied to Tagging and PP Attachment (EMNLP 1999)**    - Steven Abney, Robert E. Schapire, Yoram Singer    - [[Paper]](https://www.aclweb.org/anthology/W99-0606)    - **Lazy Bayesian Rules: A Lazy Semi-Naive Bayesian Learning Technique Competitive to Boosting Decision Trees (ICML 1999)**    - Zijian Zheng, Geoffrey I. Webb, Kai Ming Ting    - [[Paper]](https://pdfs.semanticscholar.org/067e/86836ddbcb5e2844e955c16e058366a18c77.pdf)    - **AdaCost: Misclassification Cost-Sensitive Boosting (ICML 1999)**    - Wei Fan, Salvatore J. Stolfo, Junxin Zhang, Philip K. Chan    - [[Paper]](https://pdfs.semanticscholar.org/9ddf/bc2cc5c1b13b80a1a487b9caa57e80edd863.pdf)    - **Boosting a Strong Learner: Evidence Against the Minimum Margin (ICML 1999)**    - Michael Bonnell Harries    - [[Paper]](https://dl.acm.org/citation.cfm?id=657480)    - **Boosting Algorithms as Gradient Descent (NIPS 1999)**    - Llew Mason, Jonathan Baxter, Peter L. Bartlett, Marcus R. Frean    - [[Paper]](https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf)    - **Boosting with Multi-Way Branching in Decision Trees (NIPS 1999)**    - Yishay Mansour, David A. McAllester    - [[Paper]](https://papers.nips.cc/paper/1659-boosting-with-multi-way-branching-in-decision-trees.pdf)    - **Potential Boosters (NIPS 1999)**    - Nigel Duffy, David P. Helmbold    - [[Paper]](https://pdfs.semanticscholar.org/4884/c765b6ceab7bdfb6703489810c8a386fd2a8.pdf)    ## 1998  - **An Efficient Boosting Algorithm for Combining Preferences (ICML 1998)**    - Yoav Freund, Raj D. Iyer, Robert E. Schapire, Yoram Singer    - [[Paper]](http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf)    - **Query Learning Strategies Using Boosting and Bagging (ICML 1998)**    - Naoki Abe, Hiroshi Mamitsuka    - [[Paper]](https://www.bic.kyoto-u.ac.jp/pathway/mami/pubs/Files/icml98.pdf)    - **Regularizing AdaBoost (NIPS 1998)**    - Gunnar Rätsch, Takashi Onoda, Klaus-Robert Müller    - [[Paper]](https://pdfs.semanticscholar.org/0afc/9de245547c675d40ad29240e2788c0416f91.pdf)    ## 1997  - **Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods (ICML 1997)**    - Robert E. Schapire, Yoav Freund, Peter Barlett, Wee Sun Lee    - [[Paper]](https://www.cc.gatech.edu/~isbell/tutorials/boostingmargins.pdf)    - **Using Output Codes to Boost Multiclass Learning Problems (ICML 1997)**    - Robert E. Schapire    - [[Paper]](http://rob.schapire.net/papers/Schapire97.pdf)    - **Improving Regressors Using Boosting Techniques (ICML 1997)**    - Harris Drucker    - [[Paper]](https://pdfs.semanticscholar.org/8d49/e2dedb817f2c3330e74b63c5fc86d2399ce3.pdf)    - **Pruning Adaptive Boosting (ICML 1997)**    - Dragos D. Margineantu, Thomas G. Dietterich    - [[Paper]](https://pdfs.semanticscholar.org/b25f/615fc139fbdeccc3bcf4462f908d7f8e37f9.pdf)    - **Training Methods for Adaptive Boosting of Neural Networks (NIPS 1997)**    - Holger Schwenk, Yoshua Bengio    - [[Paper]](https://papers.nips.cc/paper/1335-training-methods-for-adaptive-boosting-of-neural-networks.pdf)    ## 1996  - **Experiments with a New Boosting Algorithm (ICML 1996)**    - Yoav Freund, Robert E. Schapire    - [[Paper]](https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf)    ## 1995  - **Boosting Decision Trees (NIPS 1995)**    - Harris Drucker, Corinna Cortes    - [[Paper]](https://papers.nips.cc/paper/1059-boosting-decision-trees.pdf)    ## 1994  - **Boosting and Other Machine Learning Algorithms (ICML 1994)**    - Harris Drucker, Corinna Cortes, Lawrence D. Jackel, Yann LeCun, Vladimir Vapnik    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500155)    --------------------------------------------------------------------------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers/blob/master/LICENSE) """
Big data;https://github.com/facebook/bistro;"""# Bistro: A fast, flexible toolkit for scheduling and running distributed tasks    [![Build Status](https://travis-ci.org/facebook/bistro.svg?branch=master)](https://travis-ci.org/facebook/bistro)    This README is a very abbreviated introduction to Bistro. Visit  http://facebook.github.io/bistro for a more structured introduction, and for the docs.    Bistro is a toolkit for making distributed computation systems. It can  schedule and run distributed tasks, including data-parallel jobs.  It  enforces resource constraints for worker hosts and data-access bottlenecks.  It supports remote worker pools, low-latency batch scheduling, dynamic  shards, and a variety of other possibilities.  It has command-line and web  UIs.    Some of the diverse problems that Bistro solved at Facebook:   - Safely run map-only ETL tasks against live production databases (MySQL,     HBase, Postgres).   - Provide a resource-aware job queue for batch CPU/GPU compute jobs.   - Replace Hadoop for a periodic online data compression task on HBase,     improving time-to-completion and reliability by over 10x.    You can run Bistro ""out of the box"" to suit a variety of different  applications, but even so, it is a tool for engineers.  You should be able  to get started just by reading the documentation, but when in doubt, look at  the code --- it was written to be read.    Some applications of Bistro may involve writing small plugins to make it fit  your needs.  The code is built to be extensible.  Ask for tips, and we'll do  our best to [help](https://www.facebook.com/groups/bistro.scheduler).  In  return, we hope that you will send a pull request to allow us to share your  work with the community.    ## Early release    Although Bistro has been in production at Facebook for over 3 years, the  present public release is partial, including just the server components.    ## Install the dependencies and build    Bistro needs a 64-bit Linux, Folly, FBThrift, Proxygen, boost, and  libsqlite3.  You need 2-3GB of RAM to build, as well as GCC 4.9 or above.    `build/README.md` documents the usage of Docker-based scripts that build  Bistro on Ubuntu 14.04, 16.04, and Debian 8.6.  You should be able to follow  very similar steps on most modern Linux distributions.    If you run into dependency problems, look at `bistro/cmake/setup.cmake` for  a full list of Bistro's external dependencies (direct and indirect).  We  gratefully accept patches that improve Bistro's builds, or add support for  various flavors of Linux and Mac OS.    The binaries will be in `bistro/cmake/{Debug,Release}`.  Available build  targets are explained here:     http://cmake.org/Wiki/CMake_Useful_Variables#Compilers_and_Tools  You can start Bistro's unit tests by running `ctest` in those directories.    ## Your first Bistro run    This is just one simple demo, but Bistro is a very flexible tool. Refer to  http://facebook.github.io/bistro/ for more in-depth information.    We are going to start a single Bistro scheduler talking to one 'remote'  worker.    Aside: The scheduler tracks jobs, and data shards on which to execute them.  It also makes sure only to start new tasks when the required resources are  available.  The remote worker is a module for executing centrally scheduled  work on many machines.  The UI can aggregate many schedulers at once, so  using remote workers is optional --- a share-nothing, many-scheduler system  is sometimes preferable.    Let's make a task to execute:    ```  cat <<EOF > ~/demo_bistro_task.sh  #!/bin/bash  echo ""I got these arguments: \$@""  echo ""stderr is also logged"" 1>&2  echo ""done"" > ""\$2""  # Report the task status to Bistro via a named pipe  EOF  chmod u+x ~/demo_bistro_task.sh  ```    Open two terminals, one for the scheduler, and one for the worker.    ```  # In both terminals  cd bistro/bistro  # Start the scheduler in one terminal  ./cmake/Debug/server/bistro_scheduler \    --server_port=6789 --http_server_port=6790 \    --config_file=scripts/test_configs/simple --clean_statuses \    --CAUTION_startup_wait_for_workers=1 --instance_node_name=scheduler  # Start the worker in another  mkdir /tmp/bistro_worker  ./cmake/Debug/worker/bistro_worker --server_port=27182 --scheduler_host=:: \    --scheduler_port=6789 --worker_command=""$HOME/demo_bistro_task.sh"" \    --data_dir=/tmp/bistro_worker  ```    You should be seeing some lively log activity on both terminals. In several  seconds, the worker-scheduler negotiation should complete, and you should  see messages like ""Task ...  quit with status"" and ""Got status"".    Since we passed `--clean_statuses`, the scheduler will not persist any task  completions that happened during this run.  The worker, on the other hand,  will keep a record of the task logs in `/tmp/bistro_worker/task_logs.sql3`.    If you want task completions to persist across runs, tell Bistro where to  put the SQLite database, via `--data_dir=/tmp/bistro_scheduler` and  `--status_table=task_statuses`    ```  mkdir /tmp/bistro_scheduler  ./cmake/Debug/server/bistro_scheduler \    --server_port=6789 --http_server_port=6790 \    --config_file=scripts/test_configs/simple \    --data_dir=/tmp/bistro_scheduler --status_table=task_statuses \    --CAUTION_startup_wait_for_workers=1 --instance_node_name=scheduler  ```    You can query the running scheduler via its REST API:    ```  curl -d '{""a"":{""handler"":""jobs""},""b"":{""handler"":""running_tasks""}}' :::6790  curl -d '{""my subquery"":{""handler"":""task_logs"",""log_type"":""stdout""}}' :::6790  ```    **Pro-tip:** For ease of reading, pipe the output through either `jq` or  `json_pp` (from a Perl package). For longer outputs, try `| jq -C . | less -R`.    You should also take a look at the scheduler configuration to see how its  jobs, nodes, and resources were specified.    ```  less scripts/test_configs/simple  ```    For debugging, we typically invoke the binaries like this:    ```  gdb cmake/Debug/worker/bistro_worker -ex ""r ..."" 2>&1 | tee WORKER.txt  ```    When configuring a real deployment, be sure to carefully review the `--help`  of the scheduler & worker binaries, as well as the documentation on  http://facebook.github.io/bistro.  And don't hesitate to ask for help in the group:  https://www.facebook.com/groups/bistro.scheduler    ## License    See [LICENSE](LICENSE). """
Big data;https://github.com/gchq/Gaffer;"""Copyright 2016-2020 Crown Copyright    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.    <img src=""logos/logoWithText.png"" width=""300"">    Gaffer  ======    Gaffer is a graph database framework. It allows the storage of very large graphs containing rich properties on the nodes and edges. Several storage options are available, including Accumulo, Hbase and Parquet.    It is designed to be as flexible, scalable and extensible as possible, allowing for rapid prototyping and transition to production systems.    Gaffer offers:     - Rapid query across very large numbers of nodes and edges;   - Continual ingest of data at very high data rates, and batch bulk ingest of data via MapReduce or Spark;   - Storage of arbitrary Java objects on the nodes and edges;   - Automatic, user-configurable in-database aggregation of rich statistical properties (e.g. counts, histograms, sketches) on the nodes and edges;   - Versatile query-time summarisation, filtering and transformation of data;   - Fine grained data access controls;   - Hooks to apply policy and compliance rules to queries;   - Automated, rule-based removal of data (typically used to age-off old data);   - Retrieval of graph data into Apache Spark for fast and flexible analysis;   - A fully-featured REST API.    To get going with Gaffer, visit our [getting started pages](https://gchq.github.io/gaffer-doc/summaries/getting-started.html).    Gaffer is under active development. Version 1.0 of Gaffer was released in October 2017.    License  -------    Gaffer is licensed under the Apache 2 license and is covered by [Crown Copyright](https://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/copyright-and-re-use/crown-copyright/).    Getting Started  ---------------    ### Try it out    We have a demo available to try that is based around a small uk road use dataset. See the example/road-traffic [README](https://github.com/gchq/Gaffer/blob/master/example/road-traffic/README.md) to try it out.    ### Building and Deploying    To build Gaffer run `mvn clean install -Pquick` in the top-level directory. This will build all of Gaffer's core libraries and some examples of how to load and query data.    See our [Store](https://gchq.github.io/gaffer-doc/summaries/stores.html) documentation page for a list of available Gaffer Stores to chose from and the relevant documentation for each.    ### Inclusion in other projects    Gaffer is hosted on [Maven Central](https://mvnrepository.com/search?q=uk.gov.gchq.gaffer) and can easily be incorporated into your own maven projects.    To use Gaffer from the Java API the only required dependencies are the Gaffer graph module and a store module for the specific database technology used to store the data, e.g. for the Accumulo store:    ```  <dependency>      <groupId>uk.gov.gchq.gaffer</groupId>      <artifactId>graph</artifactId>      <version>${gaffer.version}</version>  </dependency>  <dependency>      <groupId>uk.gov.gchq.gaffer</groupId>      <artifactId>accumulo-store</artifactId>      <version>${gaffer.version}</version>  </dependency>  ```    This will include all other mandatory dependencies. Other (optional) components can be added to your project as required.    ### Documentation    Our Javadoc can be found [here](http://gchq.github.io/Gaffer/).    We have some user guides in our [docs](https://gchq.github.io/gaffer-doc/getting-started/user-guide/contents.html).    Related repositories  --------------------    The [gaffer-tools](https://github.com/gchq/gaffer-tools) repository contains useful tools to help work with Gaffer. These include:    - `jar-shader` - Used to shade the version of Jackson to avoid incompatibility problems on CDH clusters;  - `mini-accumulo-cluster` - Allows a mini Accumulo cluster to be spun up for testing purposes;  - `performance-testing` - Methods of testing the performance of ingest and query operations against a graph;  - `python-shell` - Allows operations against a graph to be executed from a Python shell;  - `random-element-generation` - Code to generate large volumes of random graph data;  - `schema-builder` - A (beta) visual tool for writing schemas for a graph;  - `slider` - Code to deploy a Gaffer cluster to a YARN cluster using [Apache Slider](https://slider.incubator.apache.org/), including the ability to easily run Slider on an [AWS EMR cluster](https://aws.amazon.com/emr/);  - `ui` - A basic graph visualisation tool.    Contributing  ------------    We welcome contributions to the project. Detailed information on our ways of working can be found [here](https://gchq.github.io/gaffer-doc/other/ways-of-working.html). In brief:    - Sign the [GCHQ Contributor Licence Agreement](https://cla-assistant.io/gchq/Gaffer);  - Push your changes to a fork;  - Submit a pull request. """
Big data;https://github.com/ZEPL/zeppelin;"""# Zeppelin has moved to Apache.    Zeppelin's has moved to Apache incubator.   This github repository is not going to be synced to the ASF's one after 20/Mar/2015.  Please consider using ASF repo for source code access and JIRA for issues.  <br />  <br />  ###Please use new github repo to create pull-request.    Github : [https://github.com/apache/incubator-zeppelin](https://github.com/apache/incubator-zeppelin)      ###Please create issue on JIRA.  JIRA : [https://issues.apache.org/jira/browse/ZEPPELIN](https://issues.apache.org/jira/browse/ZEPPELIN) """
Big data;https://github.com/Codecademy/EventHub;"""# EventHub  EventHub enables companies to do cross device event tracking. Events are joined by their associated user on EventHub and can be visualized by the built-in dashboard to answer the following common business questions  * what is my funnel conversion rate?  * what is my cohorted KPI retention?  * which variant in my A/B test has a higher conversion rate?    Most important of all, EventHub is free and open source.    **Table of Contents**  - [Quick Start](#quick-start)  - [Server](#server)  - [Dashboard](#dashboard)  - [Javascript Library](#javascript-library)  - [Ruby Library](#ruby-library)    ## Quick Start  ### Playground  A [demo server](http://codecademy:codecademy@floating-mesa-9408.herokuapp.com/) is available on Heroku and the username/password to access the dashboard is `codecademy/codecademy`.    - [Example funnel query](http://codecademy:codecademy@54.193.159.140/?start_date=20130101&end_date=20130107&num_days_to_complete_funnel=7&funnel_steps%5B%5D=receive_email&funnel_steps%5B%5D=view_track_page&funnel_steps%5B%5D=finish_course&type=funnel)  - [Example cohort query](http://codecademy:codecademy@54.193.159.140/?start_date=20130101&end_date=20130107&row_event_type=receive_email&column_event_type=start_track&num_days_per_row=1&num_columns=11&type=cohort)    ### Screenshots  ![Funnel screenshot](https://raw.githubusercontent.com/Codecademy/EventHub/master/funnel-screenshot.png)  ![Cohort screenshot](https://raw.githubusercontent.com/Codecademy/EventHub/master/cohort-screenshot.png)    ### Deploy with Heroku  Developers who want to try EventHub can quickly set the server up on Heroku with the following commands. However, please be aware that Heroku's file system is ephemeral and your data will be wiped after the instance is closed.  ```bash  git clone https://github.com/Codecademy/EventHub.git    cd EventHub  heroku create  git push heroku master    heroku open  ```    ### Required dependencies  * [java sdk7](http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html)  * [maven](http://maven.apache.org)    ### Compile and run  ```bash  # set up proper JAVA_HOME for mac  export JAVA_HOME=$(/usr/libexec/java_home)    git clone https://github.com/Codecademy/EventHub.git  cd EventHub  export EVENT_HUB_DIR=`pwd`  mvn -am -pl web clean package  java -jar web/target/web-1.0-SNAPSHOT.jar  ```    ### How to run all the tests  #### Unit/Integration/Functional testing  ```bash  mvn -am -pl web clean test  ```    #### Manual testing with curl  Comprehensive examples can be found in `script.sh`.  ```bash  cd ${EVENT_HUB_DIR}; ./script.sh  ```    Test all event related endpoints  * Add new event      ```bash      curl -X POST http://localhost:8080/events/track --data ""event_type=signup&external_user_id=foobar&event_property_1=1""      ```    * Batch add new event      ```bash      curl -X POST http://localhost:8080/events/batch_track --data ""events=[{event_type: signup, external_user_id: foobar, date: 20130101, event_property_1: 1}]""      ```    * Show all event types      ```bash      curl http://localhost:8080/events/types      ```    * Show events for a given user      ```bash      curl http://localhost:8080/users/timeline\?external_user_id\=chengtao@codecademy.com\&offset\=0\&num_records\=1      ```    * Show all property keys for the given event type      ```bash      curl 'http://localhost:8080/events/keys?event_type=signup'      ```    * Show all property values for the given event type and property key      ```bash      curl 'http://localhost:8080/events/values?event_type=signup&event_key=treatment'      ```    * Show all property values for the given event type, property key and value prefix      ```bash      curl 'http://localhost:8080/events/values?event_type=signup&event_key=treatment&prefix=fa'      ```    * Show server stats      ```bash      curl http://localhost:8080/varz      ```    * Funnel query      ```bash      today=`date +'%Y%m%d'`      end_date=`(date -d '+7day' +'%Y%m%d' || date -v '+7d' +'%Y%m%d') 2> /dev/null`        curl -X POST ""http://localhost:8080/events/funnel"" --data ""start_date=${today}&end_date=${end_date}&funnel_steps[]=signup&funnel_steps[]=view_shopping_cart&funnel_steps[]=checkout&num_days_to_complete_funnel=7&eck=event_property_1&ecv=1""      ```    * Retention query      ```bash      today=`date +'%Y%m%d'`      end_date=`(date -d '+7day' +'%Y%m%d' || date -v '+7d' +'%Y%m%d') 2> /dev/null`        curl -X POST ""http://localhost:8080/events/cohort"" --data ""start_date=${today}&end_date=${end_date}&row_event_type=signup&column_event_type=view_shopping_cart&num_days_per_row=1&num_columns=2""      ```    Test all user related endpoints  * show paginated events for a given user      ```bash      curl http://localhost:8080/users/timeline\?external_user_id\=chengtao@codecademy.com\&offset\=0\&num_records\=5      ```    * show information of users who have matched property keys & values      ```bash      curl -X POST http://localhost:8080/users/find --data ""ufk[]=external_user_id&ufv[]=chengtao1@codecademy.com""      ```    * add or update user information      ```bash      curl -X POST http://localhost:8080/users/add_or_update --data ""external_user_id=chengtao@codecademy.com&foo=bar&hello=world""      ```    * Show all property keys for users      ```bash      curl 'http://localhost:8080/users/keys      ```    * Show all property values for users given property key and (optional) value prefix      ```bash      curl 'http://localhost:8080/users/values?user_key=hello&prefix=w'      ```    #### Load testing with Jmeter  We use [Apache Jmeter](http://jmeter.apache.org) for load testing, and the load testing script can be found in `${EVENT_HUB_DIR}/jmeter.jmx`.  ```bash  export JMETER_DIR=~/Downloads/apache-jmeter-2.11/  java -jar ${JMETER_DIR}/bin/ApacheJMeter.jar -JnumThreads=1 -n -t jmeter.jmx -p jmeter.properties  java -jar ${JMETER_DIR}/bin/ApacheJMeter.jar -JnumThreads=5 -n -t jmeter.jmx -p jmeter.properties  java -jar ${JMETER_DIR}/bin/ApacheJMeter.jar -JnumThreads=10 -n -t jmeter.jmx -p jmeter.properties    # generate graph (require matplotlib)  ./plot_jmeter_performance.py 1-jmeter-performance.csv 5-jmeter-performance.csv 10-jmeter-performance.csv    # open ""Track Event.png""  ```    ## Server    ### Key observations & design decisions  Our goal is to build something usable on a single machine with a reasonably large SSD drive. Let's say, hypothetically, the server receives 100M events monthly (might cost you few thousand dollars per month to use SAAS provider), and each event is 500 bytes without compression. In this situation, storing all the events likely only takes you few hundreds GB with compression, and chances are, only the data in recent months are of interest.    Also, to efficiently run basic funnel and cohort queries without filtering, only two forward indices are needed, event index sharded by event types and event index sharded by users. Therefore, our strategy is to make those two indices as small as possible to fit in memory, and if the client wants to do filtering for events, we build a bloomfilter to reject most of the non exact-match. Imagine we are running another hypothetical query while assuming both indices and the bloomfilters can be fit in memory. Say there are 1M events that cannot be rejected and need to hit the disk, assuming each SSD disk read is 16 microseconds, we are talking about sub-minute query time, while assuming none of the data are in memory. In practice, this situation is likely much better as we cache all the recently hit records, and most of the queries likely only care the most recent data.    To simplify the design of the server and store indices compactly so that they fit in memory, we made the following two assumptions.    1. Times are associated to events when the server receives the an event  2. Date is the finest level of granularity    With the above two assumptions, we can rely on the server generated monotonically increasing id to maintain the total order for the events. In addition, as long as we track the id of the first event in any given date, we do not need to store the time information in the indices (which greatly reduces the size of the indices). The direct implication for those assumptions are, first, if the client chose to cache some events locally and sent them later, the timing for those events will be recorded as the server receives them, not when the user made those actions; second, though the server maintains the total ordering of all events, it cannot answer questions like what is the conversion rate for the given funnel between 2pm and 3pm on a given date.    Lastly, for both indices, since they are sharded by event types or users, we can expect the size of the indices to reduce significantly with proper compression.    ### Architecture  At the highest level, `com.codecademy.evenhub.web.EventHubHandler` is the main entry point. It runs a [Jetty](http://www.eclipse.org/jetty) server, reflectively collects supported commands under `com.codecademy.evenhub.web.commands`, handles JSONP request transparently, handles requests to static resources like the dashboard, and most importantly, act as a proxy which translates http request and respones to and from method calls to `com.codecademy.evenhub.EventHub`.    `com.codecademy.evenhub.EventHub` can be thought of as a facade to the key components of `UserStorage`, `EventStorage`, `ShardedEventIndex`, `DatedEventIndex`, `UserEventIndex` and `PropertiesIndex`.    For `UserStorage` and `EventStorage`, at the lowest level, we implemented `Journal{User,Event}Storage` backed by [HawtJournal](https://github.com/fusesource/hawtjournal/) to store underlying records reliably. In addition, when clients are quering records which cannot be filtered by the supported indices, the server will loop through all the potential hits, look up the properties from the `Journal` and then filter accordingly. For better performance, there are also decorators for each storage like `Cached{User,Event}Storage` to support caching and `BloomFiltered{User,Event}Storage` to support fast rejection for filters like `ExactMatch`. Please also beware that each `Storage` maintains a monotonically increasing counter as the internal id generator for each event and user received.    To make the funnel and cohort queries fast, `EventHub` also maintains three indices, `ShardedEventIndex`, `UserEventIndex`, and `DatedEventIndex` behind the scene. `DatedEventIndex` simply tracks the mapping from a given date, the id of the first event received in that day. `ShardedEventIndex` can be thought of as sorted event ids sharded by event type. `UserEventIndex` can be thought of as sorted event ids sharded by users.    Lastly, `EventHub` maintains a `PropertiesIndex` backed by [LevelDB Jni](https://github.com/fusesource/leveldbjni) to track what properties keys are available for a given event type and what properties values are available for a given event type and a property key.    ### Horizontal scalabiltiy  While EventHub does not need any information from different users, with a broker in front of EventHub servers, EventHub can be easily sharded by users and scale horizontally.    ### Performance  In the following three experiments, the spec of the computer used can be found in the following table    | Component      | Spec                                    |  |----------------|-----------------------------------------|  | Computer Model | Mac Book Pro, Retina 15-inch, Late 2013 |  | Processor      | 2GHz Intel Core i7                      |  | Memory         | 8GB 1600 MHz DDR3                       |  | Software       | OS X 10.9.2                             |  | Jvm            | Oracle JDK 1.7                          |    #### Write performance  The following graph is generated as described in [Load testing with Jmeter](#load-testing-with-jmeter). The graph shows both the throughput and latency of adding the first one million events (without batching) with different number of threads (1, 5, 10, 15).  ![Throughput and latency by threads](http://i60.tinypic.com/16ad66b.png)    #### Query performance  While it is difficult to come up with a generic benchmark, we would rather show something rather than show nothing. After generating about one million events with the load testing script as described in [Load testing with Jmeter](#load-testing-with-jmeter), we ran the four types of queries twice, once after the server starts cleanly and another time while the cache is still warm.    | Query                   | 1st execution | 2nd execution | command |  |-------------------------|---------------|---------------|---------|  | Funnel without filters  | 1.15s         | 0.19s         | curl -X POST ""http://localhost:8080/events/funnel"" --data ""start_date=20130101&end_date=20130130&funnel_steps[]=receive_email&funnel_steps[]=view_track_page&funnel_steps[]=start_track&num_days_to_complete_funnel=30"" |  | Funnel with filters     | 1.31s         | 0.43s         | curl -X POST ""http://localhost:8080/events/funnel"" --data ""start_date=20130101&end_date=20130130&funnel_steps[]=receive_email&funnel_steps[]=view_track_page&funnel_steps[]=start_track&num_days_to_complete_funnel=30&efk0[]=event_property_1&efv0[]=1"" |  | Cohort without filters  | 0.63s         | 0.13s         | curl -X POST ""http://localhost:8080/events/cohort"" --data ""start_date=20130101&end_date=20130130&row_event_type=receive_email&column_event_type=start_track&num_days_per_row=1&num_columns=7"" |  | Cohort with filters     | 1.20s         | 0.32s         | curl -X POST ""http://localhost:8080/events/cohort"" --data ""start_date=20130101&end_date=20130130&row_event_type=receive_email&column_event_type=start_track&num_days_per_row=1&num_columns=7&refk[]=event_property_1&refv[]=1"" |    #### Memory footprint  In the experiment, the server was bootstrapped differently. Instead of using the load testing script, we used subset of data from Codecademy, which has around 53M events and 2.4M users. Please be aware that the current storage format on disk is fairly inefficient and has serious internal fragmentation. However, when the data are loaded to memory, it will be much more efficient as we would never load those ""hole"" pages into memory.    | Key Component             | Size in memory  | Note |  |---------------------------|-----------------|------|  | ShardedEventIndex         | 424Mb           | (data size) + (index size) <br>= (event id size * number of events) + negligible<br>= (8 * 53M) |  | UserEventIndex            | 722Mb           | (data size) + (index size) <br>= (event id size * number of events) + (index entry size * number of users)<br>= (8 * 53M) + ((numPointersPerIndexEntry * 2 + 1) * 8 + 4) * 2.4M)<br>= (8 * 53M) + (124 * 2.4M) |  | BloomFilteredEventStorage | 848Mb           | (bloomfilter size) * (number of events) <br>= 16 * 53M |    ## Dashboard  The server comes with a built-in dashboard which is simply some static resources stored in `/web/src/main/resources/frontend` and gets compiled into the server jar file. After running the server, the dashboard can be accessed at [http://localhost:8080](http://localhost:8080). Through the dashboard, you can access the server for your funnel and cohort analysis.    #### Password protection  The dashboard comes with insecure basic authentication which send unencrypted information without SSL. Please use it at your own discretion. The default username/password is codecademy/codecademy and you can change it by modifying your web.properties file or use the following command to start your server  ```bash  USERNAME=foo  PASSWORD=bar  java -Deventhubhandler.username=${USERNAME} -Deventhubhandler.password=${PASSWORD} -jar web/target/web-1.0-SNAPSHOT.jar  ```    ## Javascript Library  The project comes with a javascript library which can be integrated with your website as a way to send events to your EventHub server.     ### How to run JS tests  #### install [karma](http://karma-runner.github.io/0.12/index.html)  ```bash  cd ${EVENT_HUB_DIR}    npm install -g karma  npm install -g karma-jasmine@2_0  npm install -g karma-chrome-launcher    karma start karma.conf.js  ```    ### API  The javascript library is extremely simple and heavily inspired by mixpanel. There are only five methods that a developer needs to understand. Beware that behind the scenes, the library maintains a queue backed by localStorage, buffers the events in the queue, and has a timer reguarly clear the queue. If the browser doesn't support localStorage, a in-memory queue will be created as EventHub is created. Also, our implementation relies on the server to track the timestamp of each event. Therefore, in the case of a browser session disconnected before all the events are sent, the remaining events will be sent in the next browser session and thus have the timestamp recorded as the next session starts.    #### window.newEventHub()  The method will create an EventHub and start the timer which clears out the event queue in every second (default)  ```javascript  var name = ""EventHub"";  var options = {    url: 'http://example.com',    flushInterval: 10 /* in seconds */  };  var eventHub = window.newEventHub(name, options);  ```    #### eventHub.track()  This method enqueues the given event which will be cleared in batch at every flushInterval. Beware that if there is no identify method called before the track method is called, the library will automatically generate an user id which remain the same for the entire session (clears after the browser tab is closed), and send the generated user id along with the queued event. On the other hand, if `eventhub.identify()` is called before the track method is called, the user information passed along with the identify method call will be merged to the queued event.  ```javascript  eventHub.track(""signup"", {    property_1: 'value1',    property_2: 'value2'  });  ```    #### eventHub.alias()  This method links the given user to the automatically generated user. Typically, you only want to call this method once -- right after the user successfully signs up.  ```javascript  eventHub.alias('chengtao@codecademy.com');  ```    #### eventHub.identify()  This method tells the library instead of using the automatically generated user information, use the given information instead.  ```javascript  eventHub.identify('chengtao@codecademy.com', {    user_property_1: 'value1',    user_property_2: 'value2'  });  ```    #### eventHub.register()  This method allows the developer to add additional information to the generated user.  ```javascript  eventHub.register({    user_property_1: 'value1',    user_property_2: 'value2'  });  ```    ### Scenario and Receipes  #### Link the events sent before and after an user sign up  The following code  ```javascript  var eventHub = window.newEventHub('EventHub', { url: 'http://example.com' });  eventHub.track('pageview', { page: 'home' });  eventHub.register({    ip: '10.0.0.1'  });    // after user signup  eventHub.alias('chengtao@codecademy.com');  eventHub.identify('chengtao@codecademy.com', {    gender: 'male'  });  eventHub.track('pageview', { page: 'learn' });  ```   will result in a funnel like  ```javascript  {    user: 'something generated',    event: 'pageview',    page: 'home',    ip: '10.0.0.1'  }  link 'chengtao@codecademy.com' to 'something generated'  {    user: 'chengtao@codecademy.com',    event: 'pageview',    page: 'learn',    gender: 'male'  }  ```    #### A/B testing  The following code  ```javascript  var eventHub = window.newEventHub('EventHub', { url: 'http://example.com' });  eventHub.identify('chengtao@codecademy.com', {});  eventHub.track('pageview', {    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'new'  });  eventHub.track('submit', {    page: 'javascript exercise 1'  });  ```  and  ```javascript  var eventHub = window.newEventHub('EventHub', { url: 'http://example.com' });  eventHub.identify('bob@codecademy.com', {});  eventHub.track('pageview', {    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'control'  });  eventHub.track('skip', {    page: 'javascript exercise 1'  });  ```  will result in two funnels like  ```javascript  {    user: 'chengtao@codecademy.com',    event: 'pageview',    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'new'  }  {    user: 'chengtao@codecademy.com',    event: 'submit',    page: 'javascript exercise 1'  }  ```  and  ```javascript  {    user: 'bob@codecademy.com',    event: 'pageview',    page: 'javascript exercise 1',    experiment: 'fancy feature',    treatment: 'control'  }  {    user: 'bob@codecademy.com',    event: 'skip',    page: 'javascript exercise 1'  }  ```    ## Ruby Library  Separate ruby gem is also available at [https://github.com/Codecademy/EventHubClient](https://github.com/Codecademy/EventHubClient)    ## License  MIT License.    Copyright (c) 2014 Ryzac, Inc.   """
Big data;https://github.com/twitter/fatcache;"""# fatcache     [![status: retired](https://opensource.twitter.dev/status/retired.svg)](https://opensource.twitter.dev/status/#retired)  [![Build Status](https://travis-ci.org/twitter/fatcache.png?branch=master)](https://travis-ci.org/twitter/fatcache)    fatcache is no longer actively maintained.  See [twitter/pelikan](https://github.com/twitter/pelikan) for our latest caching work.    **fatcache** is memcache on SSD. Think of fatcache as a cache for your big data.    ## Overview    There are two ways to think of SSDs in system design. One is to think of SSD as an extension of disk, where it plays the role of making disks fast and the other is to think of them as an extension of memory, where it plays the role of making memory fat. The latter makes sense when persistence (non-volatility) is unnecessary and data is accessed over the network. Even though memory is thousand times faster than SSD, network connected SSD-backed memory makes sense, if we design the system in a way that network latencies dominate over the SSD latencies by a large factor.    To understand why network connected SSD makes sense, it is important to understand the role distributed memory plays in large-scale web architecture. In recent years, terabyte-scale, distributed, in-memory caches have become a fundamental building block of any web architecture. In-memory indexes, hash tables, key-value stores and caches are increasingly incorporated for scaling throughput and reducing latency of persistent storage systems. However, power consumption, operational complexity and single node DRAM cost make horizontally scaling this architecture challenging. The current cost of DRAM per server increases dramatically beyond approximately 150 GB, and power cost scales similarly as DRAM density increases.    Fatcache extends a volatile, in-memory cache by incorporating SSD-backed storage.    SSD-backed memory presents a viable alternative for applications with large workloads that need to maintain high hit rate for high performance. SSDs have higher capacity per dollar and lower power consumption per byte, without degrading random read latency beyond network latency.    Fatcache achieves performance comparable to an in-memory cache by focusing on two design criteria:    - Minimize disk reads on cache hit  - Eliminate small, random disk writes    The latter is important due to SSDs' unique write characteristics. Writes and in-place updates to SSDs degrade performance due to an erase-and-rewrite penalty and garbage collection of dead blocks. Fatcache batches small writes to obtain consistent performance and increased disk lifetime.    SSD reads happen at a page-size granularity, usually 4 KB. Single page read access times are approximately 50 to 70 usec and a single [commodity SSD](http://ark.intel.com/products/56569/Intel-SSD-320-Series-600GB-2_5in-SATA-3Gbs-25nm-ML) can sustain nearly 40K read IOPS at a 4 KB page size. 70 usec read latency dictates that disk latency will overtake typical network latency after a small number of reads. Fatcache reduces disk reads by maintaining an in-memory index for all on-disk data.    ## Batched Writes    There have been attempts to use an SSD as a swap layer to implement SSD-backed memory. This method degrades write performance and SSD lifetime with many small, random writes. Similar issues occur when an SSD is simply mmaped.    To minimize the number of small, random writes, fatcache treats the SSD as a log-structured object store. All writes are aggregated in memory and written to the end of the circular log in batches - usually multiples of 1 MB.    By managing an SSD as a log-structured store, no disk updates are in-place and objects won't have a fixed address on disk. To locate an object, fatcache maintains an in-memory index. An on-disk object without an index entry is a candidate for garbage collection, which occurs during capacity-triggered eviction.    ## In-memory index    Fatcache maintains an in-memory index for all data stored on disk. An in-memory index serves two purposes:    - Cheap object existence checks  - On-disk object address storage    An in-memory index is preferable to an on-disk index to minimize disk lookups to locate and read an object. Furthermore, in-place index updates become complicated by an SSD's unique write characteristics. An in-memory index avoids these shortcomings and ensures there are no disk accesses on cache miss and only a single disk access on cache hit.    Maintaining an in-memory index of all on-disk data requires a compact representation of the index. The fatcache index has the following format:    ```c  struct itemx {    STAILQ_ENTRY(itemx) tqe;    /* link in index / free q */    uint8_t             md[20]; /* sha1 message digest */    uint32_t            sid;    /* owner slab id */    uint32_t            offset; /* item offset from owner slab base */    rel_time_t          expiry; /* expiry in secs */    uint64_t            cas;    /* cas */  } __attribute__ ((__packed__));  ```    Each index entry contains both object-specific information (key name, &c.) and disk-related information (disk address, &c.). The entries are stored in a chained hash table. To avoid long hash bin traversals, the number of hash bins is fixed to the expected number of index entries.    To further reduce the memory consumed by the index, we store the SHA-1 hash of the key in each index entry, instead of the key itself. The SHA-1 hash acts as the unique identifier for each object. The on-disk object format contains the complete object key and value. False positives from SHA-1 hash collisions are detected after object retrieval from the disk by comparison with the requested key. If there are collisions on the write path, new objects with the same hash key simply overwrite previous objects.    The index entry (struct itemx) on a 64-bit system is 48 bytes in size. It is possible to further reduce index entry size to 32 bytes, if CAS is unsupported, MD5 hashing is used, and the next pointer is reduced to 4 bytes.    At this point, it is instructive to consider the relative size of fatcache's index and the on-disk data. With a 44 byte index entry, an index consuming 48 MB of memory can address 1M objects. If the average object size is 1 KB, then a 48 MB index can address 1 GB of on-disk storage - a 23x memory overcommit. If the average object size is 500 bytes, then a 48 MB index can address 500 MB of SSD - a 11x memory overcommit. Index size and object size relate in this way to determine the addressable capacity of the SSD.    ## Build    To build fatcache from a [distribution tarball](http://code.google.com/p/fatcache/downloads/list):        $ ./configure      $ make      $ sudo make install    To build fatcache from a [distribution tarball](http://code.google.com/p/fatcache/downloads/list) in _debug mode_:        $ CFLAGS=""-ggdb3 -O0"" ./configure --enable-debug=full      $ make      $ sudo make install    To build fatcache from source with _debug logs enabled_ and _assertions disabled_:        $ git clone git@github.com:twitter/fatcache.git      $ cd fatcache      $ autoreconf -fvi      $ ./configure --enable-debug=log      $ make      $ src/fatcache -h    ## Help        Usage: fatcache [-?hVdS] [-o output file] [-v verbosity level]                 [-p port] [-a addr] [-e hash power]                 [-f factor] [-n min item chunk size] [-I slab size]                 [-i max index memory[ [-m max slab memory]                 [-z slab profile] [-D ssd device] [-s server id]        Options:        -h, --help                  : this help        -V, --version               : show version and exit        -d, --daemonize             : run as a daemon        -S, --show-sizes            : print slab, item and index sizes and exit        -o, --output=S              : set the logging file (default: stderr)        -v, --verbosity=N           : set the logging level (default: 6, min: 0, max: 11)        -p, --port=N                : set the port to listen on (default: 11211)        -a, --addr=S                : set the address to listen on (default: 0.0.0.0)        -e, --hash-power=N          : set the item index hash table size as a power of two (default: 20)        -f, --factor=D              : set the growth factor of slab item sizes (default: 1.25)        -n, --min-item-chunk-size=N : set the minimum item chunk size in bytes (default: 84 bytes)        -I, --slab-size=N           : set slab size in bytes (default: 1048576 bytes)        -i, --max-index-memory=N    : set the maximum memory to use for item indexes in MB (default: 64 MB)        -m, --max-slab-memory=N     : set the maximum memory to use for slabs in MB (default: 64 MB)        -z, --slab-profile=S        : set the profile of slab item chunk sizes (default: n/a)        -D, --ssd-device=S          : set the path to the ssd device file (default: n/a)        -s, --server-id=I/N         : set fatcache instance to be I out of total N instances (default: 0/1)    ## Performance    - Initial performance results are available [here](https://github.com/twitter/fatcache/blob/master/notes/performance.md).    ## Future Work    - fatcache deals with two kinds of IOs - disk IO and network IO. Network IO in fatcache is async, but disk IO is sync. It is recommended to run multiple instances of fatcache on a single machine to exploit CPU and SSD parallelism. However, by making disk IO async (using libaio, perhaps), it would be possible for a single instance to completely exploit all available SSD device parallelism.  - observability in fatcache through stats    ## Issues and Support    Have a bug or question? Please create an issue here on GitHub!    https://github.com/twitter/fatcache/issues    ## Contributors    * Manju Rajashekhar ([@manju](https://twitter.com/manju))  * Yao Yue ([@thinkingfish](https://twitter.com/thinkingfish))    ## License    Copyright 2013 Twitter, Inc.    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/tensorflow/tensorflow;"""<div align=""center"">    <img src=""https://www.tensorflow.org/images/tf_logo_horizontal.png"">  </div>    [![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)  [![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg)](https://doi.org/10.5281/zenodo.4724125)    **`Documentation`** |  ------------------- |  [![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |    [TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform  for machine learning. It has a comprehensive, flexible ecosystem of  [tools](https://www.tensorflow.org/resources/tools),  [libraries](https://www.tensorflow.org/resources/libraries-extensions), and  [community](https://www.tensorflow.org/community) resources that lets  researchers push the state-of-the-art in ML and developers easily build and  deploy ML-powered applications.    TensorFlow was originally developed by researchers and engineers working on the  Google Brain team within Google's Machine Intelligence Research organization to  conduct machine learning and deep neural networks research. The system is  general enough to be applicable in a wide variety of other domains, as well.    TensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)  and [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as  non-guaranteed backward compatible API for  [other languages](https://www.tensorflow.org/api_docs).    Keep up-to-date with release announcements and security updates by subscribing  to  [announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).  See all the [mailing lists](https://www.tensorflow.org/community/forums).    ## Install    See the [TensorFlow install guide](https://www.tensorflow.org/install) for the  [pip package](https://www.tensorflow.org/install/pip), to  [enable GPU support](https://www.tensorflow.org/install/gpu), use a  [Docker container](https://www.tensorflow.org/install/docker), and  [build from source](https://www.tensorflow.org/install/source).    To install the current release, which includes support for  [CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and  Windows)*:    ```  $ pip install tensorflow  ```    A smaller CPU-only package is also available:    ```  $ pip install tensorflow-cpu  ```    To update TensorFlow to the latest version, add `--upgrade` flag to the above  commands.    *Nightly binaries are available for testing using the  [tf-nightly](https://pypi.python.org/pypi/tf-nightly) and  [tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*    #### *Try your first TensorFlow program*    ```shell  $ python  ```    ```python  >>> import tensorflow as tf  >>> tf.add(1, 2).numpy()  3  >>> hello = tf.constant('Hello, TensorFlow!')  >>> hello.numpy()  b'Hello, TensorFlow!'  ```    For more examples, see the  [TensorFlow tutorials](https://www.tensorflow.org/tutorials/).    ## Contribution guidelines    **If you want to contribute to TensorFlow, be sure to review the  [contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's  [code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to  uphold this code.**    **We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for  tracking requests and bugs, please see  [TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)  for general questions and discussion, and please direct specific questions to  [Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**    The TensorFlow project strives to abide by generally accepted best practices in  open-source software development:    [![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)  [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)  [![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)    ## Continuous build status    You can find more community-supported platforms and configurations in the  [TensorFlow SIG Build community builds table](https://github.com/tensorflow/build#community-supported-tensorflow-builds).    ### Official Builds    Build Type                    | Status                                                                                                                                                                           | Artifacts  ----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------  **Linux CPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html)           | [PyPI](https://pypi.org/project/tf-nightly/)  **Linux GPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [PyPI](https://pypi.org/project/tf-nightly-gpu/)  **Linux XLA**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html)         | TBA  **macOS**                     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html)     | [PyPI](https://pypi.org/project/tf-nightly/)  **Windows CPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html)       | [PyPI](https://pypi.org/project/tf-nightly/)  **Windows GPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html)       | [PyPI](https://pypi.org/project/tf-nightly-gpu/)  **Android**                   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html)               | [Download](https://bintray.com/google/tensorflow/tensorflow/_latestVersion)  **Raspberry Pi 0 and 1**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)  **Raspberry Pi 2 and 3**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)  **Libtensorflow MacOS CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)  **Libtensorflow Linux CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)  **Libtensorflow Linux GPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)  **Libtensorflow Windows CPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)  **Libtensorflow Windows GPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)    ## Resources    *   [TensorFlow.org](https://www.tensorflow.org)  *   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)  *   [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)  *   [TensorFlow Examples](https://github.com/tensorflow/examples)  *   [DeepLearning.AI TensorFlow Developer Professional Certificate](https://www.coursera.org/specializations/tensorflow-in-practice)  *   [TensorFlow: Data and Deployment from Coursera](https://www.coursera.org/specializations/tensorflow-data-and-deployment)  *   [Getting Started with TensorFlow 2 from Coursera](https://www.coursera.org/learn/getting-started-with-tensor-flow2)  *   [TensorFlow: Advanced Techniques from Coursera](https://www.coursera.org/specializations/tensorflow-advanced-techniques)  *   [TensorFlow 2 for Deep Learning Specialization from Coursera](https://www.coursera.org/specializations/tensorflow2-deeplearning)  *   [Intro to TensorFlow for A.I, M.L, and D.L from Coursera](https://www.coursera.org/learn/introduction-tensorflow)  *   [Intro to TensorFlow for Deep Learning from Udacity](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187)  *   [Introduction to TensorFlow Lite from Udacity](https://www.udacity.com/course/intro-to-tensorflow-lite--ud190)  *   [Machine Learning with TensorFlow on GCP](https://www.coursera.org/specializations/machine-learning-tensorflow-gcp)  *   [TensorFlow Codelabs](https://codelabs.developers.google.com/?cat=TensorFlow)  *   [TensorFlow Blog](https://blog.tensorflow.org)  *   [Learn ML with TensorFlow](https://www.tensorflow.org/resources/learn-ml)  *   [TensorFlow Twitter](https://twitter.com/tensorflow)  *   [TensorFlow YouTube](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)  *   [TensorFlow model optimization roadmap](https://www.tensorflow.org/model_optimization/guide/roadmap)  *   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)  *   [TensorBoard Visualization Toolkit](https://github.com/tensorflow/tensorboard)    Learn more about the  [TensorFlow community](https://www.tensorflow.org/community) and how to  [contribute](https://www.tensorflow.org/community/contribute).    ## License    [Apache License 2.0](LICENSE) """
Big data;https://github.com/rudderlabs/rudder-server;"""<p align=""center"">    <a href=""https://rudderstack.com/"">      <img src=""resources/logo.png"">    </a>  </p>    <p align=""center""><b>The Customer Data Platform for Developers</b></p>    <p align=""center"">    <a href=""https://rudderstack.com/"">      <img src=""https://codebuild.us-east-1.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiT01EQkVPc0NBbDJLV2txTURidkRTMTNmWFRZWUY2dEtia3FRVmFXdXhWeUwzaC9aV3dsWWNNT0NwaVZKd1hKTFVMazB2cDQ5UHlaZTgvbFRER3R5SXRvPSIsIml2UGFyYW1ldGVyU3BlYyI6IktJQVMveHIzQnExZVE5b0YiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=master"">    </a>    <a href=""https://github.com/rudderlabs/rudder-server/releases"">      <img src=""https://img.shields.io/github/v/release/rudderlabs/rudder-server?color=blue&sort=semver"">    </a>    <a href=""https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/docker/"">      <img src=""https://img.shields.io/docker/pulls/rudderlabs/rudder-server"">    </a>    <a href=""https://github.com/rudderlabs/rudder-server/blob/master/LICENSE"">      <img src=""https://img.shields.io/github/license/rudderlabs/rudder-server"">    </a>  </p>    <p align=""center"">    <b>      <a href=""https://rudderstack.com"">Website</a>      ·      <a href=""https://rudderstack.com/docs"">Documentation</a>      ·      <a href=""https://rudderstack.com/blog"">Blog</a>      ·      <a href=""https://rudderstack.com/join-rudderstack-slack-community"">Slack</a>      ·      <a href=""https://twitter.com/rudderstack"">Twitter</a>    </b>  </p>    ---    As the leading open source Customer Data Platform (CDP), [**RudderStack**](https://rudderstack.com/) provides data pipelines that make it easy to collect data from every application, website and SaaS platform, then activate it in your warehouse and business tools.    With RudderStack, you can build customer data pipelines that connect your whole customer data stack and then make them smarter by triggering enrichment and activation in customer tools based on analysis in your data warehouse. It's easy-to-use SDKs and event source integrations, Cloud Extract integrations, transformations, and expansive library of destination and warehouse integrations makes building customer data pipelines for both event streaming and cloud-to-warehouse ELT simple.    <p align=""center"">    <a href=""https://rudderstack.com"">      <img src=""https://user-images.githubusercontent.com/59817155/121468374-4ef91e00-c9d8-11eb-8611-28bea18f609d.gif"" alt=""RudderStack"">    </a>  </p>    | Try **RudderStack Cloud Free** - a free tier of [**RudderStack Cloud**](https://rudderstack.com/cloud). Click [**here**](https://app.rudderlabs.com/signup?type=freetrial) to start building a smarter customer data pipeline today, with RudderStack Cloud. |  |:------|    ## Key features    - **Warehouse-first**: RudderStack treats your data warehouse as a first class citizen among destinations, with advanced features and configurable, near real-time sync.    - **Developer-focused**: RudderStack is built API-first. It integrates seamlessly with the tools that the developers already use and love.    - **High Availability**: RudderStack comes with at least 99.99% uptime. We have built a sophisticated error handling and retry system that ensures that your data will be delivered even in the event of network partitions or destinations downtime.    - **Privacy and Security**: You can collect and store your customer data without sending everything to a third-party vendor. With RudderStack, you get fine-grained control over what data to forward to which analytical tool.     - **Unlimited Events**: Event volume-based pricing of most of the commercial systems is broken. With RudderStack, you are be able to collect as much data as possible without worrying about overrunning your event budgets.    - **Segment API-compatible**: RudderStack is fully compatible with the Segment API. So you don't need to change your app if you are using Segment; just integrate the RudderStack SDKs into your app and your events will keep flowing to the destinations (including data warehouses) as before.    - **Production-ready**: Companies like Mattermost, IFTTT, Torpedo, Grofers, 1mg, Nana, OnceHub, and dozens of large companies use RudderStack for collecting their events.    - **Seamless Integration**: RudderStack currently supports integration with over 90 popular [**tool**](https://rudderstack.com/docs/destinations/) and [**warehouse**](https://rudderstack.com/docs/data-warehouse-integrations/) destinations.    - **User-specified Transformation**: RudderStack offers a powerful JavaScript-based event transformation framework which lets you enhance or transform your event data by combining it with your other internal data. Furthermore, as RudderStack runs inside your cloud or on-premise environment, you can easily access your production data to join with the event data.    ## Get started    The easiest way to experience RudderStack is to [**sign up**](https://app.rudderlabs.com/signup?type=freetrial) for **RudderStack Cloud Free** - a completely free tier of [**RudderStack Cloud**](https://rudderstack.com/cloud).    You can also set up RudderStack on your platform of choice with these two easy steps:    ### Step 1: Set up RudderStack    - [**Docker**](https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/docker/)  - [**Kubernetes**](https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/kubernetes/)  - [**Developer machine setup**](https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/developer-machine-setup/)    > **Note**: If you are planning to use RudderStack in production, we STRONGLY recommend using our Kubernetes Helm charts. We update our Docker images with bug fixes much more frequently than our GitHub repo.    ### Step 2: Verify the installation    Once you have installed RudderStack, [**send test events**](https://rudderstack.com/docs/get-started/installing-and-setting-up-rudderstack/sending-test-events/) to verify the setup.    ## Architecture    RudderStack is an independent, stand-alone system with a dependency only on the database (PostgreSQL). Its backend is written in **Go** with a rich UI written in **React.js**.    A high-level view of RudderStack’s architecture is shown below:    ![Architecture](resources/rudder-server-architecture.png)    For more details on the various architectural components, refer to our [**documentation**](https://rudderstack.com/docs/get-started/rudderstack-architecture/).    ## Contribute    We would love to see you contribute to RudderStack. Get more information on how to contribute [**here**](https://github.com/rudderlabs/rudder-server/blob/master/CONTRIBUTING.md).    ## License    RudderStack server is released under the [**AGPLv3 License**](https://github.com/rudderlabs/rudder-server/blob/master/LICENSE).    Read [**our blog**](https://rudderstack.com/blog/rudderstacks-licensing-explained) to know more about how our software is licensed. """
Big data;https://github.com/pivotalsoftware/PivotalR;"""PivotalR  =======    PivotalR is a package that enables users of R, the most popular open source statistical programming language  and environment, to interact with [Greenplum Database](https://greenplum.org/)  and the [PostgreSQL](https://www.postgresql.org/) for big data  analytics. It does so by providing an interface to the operations on tables/views in the database. These  operations are almost the same as those of data.frame. Minimal amount of data is transfered between R and  the database. Thus the users of R do not need to learn SQL when they  operate on the objects in the database. PivotalR also lets the user to run the functions of the open source  machine learning package [Apache MADlib](https://madlib.apache.org/) directly from R.    1. An Introduction to PivotalR            vignette(""pivotalr"") # execute in R console to view the PDF file  2. To install PivotalR:      * Get the latest stable version from CRAN by running `install.packages(""PivotalR"")`      * Or try out the latest development version from github by running the following code (need R >= 3.0.2):            ```          ## install.packages(""devtools"") # 'devtools' package is only available for R >= 3.0.2          devtools::install_github(""PivotalR"", ""greenplum-db"")          ```      * Or download the source tarball directly from [**here**](https://github.com/greenplum-db/PivotalR/tarball/master), and then install the tarball            ```          install.packages(""greenplum-db-PivotalR-xxxx.tar.gz"", repos = NULL, type = ""source"")          ```      where ""greenplum-db-PivotalR-xxxx.tar.gz"" is the name of the package that you have downloaded.  3. To get started:      * [Read the wiki](https://github.com/greenplum-db/PivotalR/wiki)      * [Look at some demo code](https://github.com/greenplum-db/PivotalR/wiki/Example)      * [Watch a training video](https://www.youtube.com/watch?v=6cmyRCMY6j0)      * [Read the quick-start guide](https://github.com/wjjung317/gp-r/blob/master/docs/PivotalR-quick-start%20v2.pdf) """
Big data;https://github.com/plotly/dash;"""# Dash    [![CircleCI](https://img.shields.io/circleci/project/github/plotly/dash/master.svg)](https://circleci.com/gh/plotly/dash)  [![GitHub](https://img.shields.io/github/license/plotly/dash.svg?color=dark-green)](https://github.com/plotly/dash/blob/master/LICENSE)  [![PyPI](https://img.shields.io/pypi/v/dash.svg?color=dark-green)](https://pypi.org/project/dash/)  [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/dash.svg?color=dark-green)](https://pypi.org/project/dash/)  [![GitHub commit activity](https://img.shields.io/github/commit-activity/y/plotly/dash.svg?color=dark-green)](https://github.com/plotly/dash/graphs/contributors)  [![LGTM Alerts](https://img.shields.io/lgtm/alerts/g/plotly/dash.svg)](https://lgtm.com/projects/g/plotly/dash/alerts)  [![LGTM Grade](https://img.shields.io/lgtm/grade/python/g/plotly/dash.svg)](https://lgtm.com/projects/g/plotly/dash/context:python)    #### *Dash is the most downloaded, trusted Python framework for building ML & data science web apps*.    Built on top of [Plotly.js](https://github.com/plotly/plotly.js), [React](https://reactjs.org/) and [Flask](https://palletsprojects.com/p/flask/), Dash ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical Python code. Read [our tutorial](https://dash.plotly.com/getting-started) (proudly crafted ❤️ with Dash itself).    - [Docs](https://dash.plotly.com/getting-started): Create your first Dash app in under 5 minutes    - [dash.gallery](https://dash.gallery): Dash app gallery with Python & R code    ### Dash App Examples    | Dash App | Description |  |--- | :---: |  |![Sample Dash App](https://user-images.githubusercontent.com/1280389/30086128-9bb4a28e-9267-11e7-8fe4-bbac7d53f2b0.gif) | Here’s a simple example of a Dash App that ties a Dropdown to a Plotly Graph. As the user selects a value in the Dropdown, the application code dynamically exports data from Google Finance into a Pandas DataFrame. This app was written in just **43** lines of code ([view the source](https://gist.github.com/chriddyp/3d2454905d8f01886d651f207e2419f0)). |  |![Crossfiltering Dash App](https://user-images.githubusercontent.com/1280389/30086123-97c58bde-9267-11e7-98a0-7f626de5199a.gif)|Dash app code is declarative and reactive, which makes it easy to build complex apps that contain many interactive elements. Here’s an example with 5 inputs, 3 outputs, and cross filtering. This app was composed in just 160 lines of code, all of which were Python.|  |![Dash App with Mapbox map showing walmart store openings](https://user-images.githubusercontent.com/1280389/30086299-768509d0-9268-11e7-8e6b-626ac9ca512c.gif)| Dash uses [Plotly.js](https://github.com/plotly/plotly.js) for charting. About 50 chart types are supported, including maps. |  |![Financial report](https://github.com/plotly/dash-docs/blob/516f80c417051406210b94ea23a6d3b6cd84d146/assets/images/gallery/dash-financial-report.gif)| Dash isn't just for dashboards. You have full control over the look and feel of your applications. Here's a Dash App that's styled to look like a PDF report. |    To learn more about Dash, read the [extensive announcement letter](https://medium.com/@plotlygraphs/introducing-dash-5ecf7191b503) or [jump in with the user guide](https://plotly.com/dash).    ### Dash OSS & Dash Enterprise    With Dash Open Source, Dash apps run on your local laptop or workstation, but cannot be easily accessed by others in your organization.    Scale up with Dash Enterprise when your Dash app is ready for department or company-wide consumption. Or, launch your initiative with Dash Enterprise from the start to unlock developer productivity gains and hands-on acceleration from Plotly's team.    ML Ops Features: A one-stop shop for ML Ops: Horizontally scalable hosting, deployment, and authentication for your Dash apps. No IT or DevOps required.  - [**App manager**](https://plotly.com/dash/app-manager/) Deploy & manage Dash apps without needing IT or a DevOps team. App Manager gives you point & click control over all aspects of your Dash deployments.  - [**Kubernetes scaling**](https://plotly.com/dash/kubernetes/) Ensure high availability of Dash apps and scale horizontally with Dash Enterprise’s Kubernetes architecture. No IT or Helm required.  - [**No code auth**](https://plotly.com/dash/authentication/) Control Dash app access in a few clicks. Dash Enterprise supports LDAP, AD, PKI, Okta, SAML, OpenID Connect, OAuth, SSO, and simple email authentication.  - [**Job Queue**](https://plotly.com/dash/job-queue/) The Job Queue is the key to building scalable Dash apps. Move heavy computation from synchronous Dash callbacks to the Job Queue for asynchronous background processing.    Low-Code Features: Low-code Dash app capabilities that supercharge developer productivity.  - [**Design Kit**](https://plotly.com/dash/design-kit/) Design like a pro without writing a line of CSS. Easily arrange, style, brand, and customize your Dash apps.  - [**Snapshot Engine**](https://plotly.com/dash/snapshot-engine/) Save & share Dash app views as links or PDFs. Or, run a Python job through Dash and have Snapshot Engine email a report when the job is done.  - [**Dashboard Toolkit**](https://plotly.com/dash/toolkit/) Drag & drop layouts, chart editing, and crossfilter for your Dash apps.  - [**Embedding**](https://plotly.com/dash/embedding/) Natively embed Dash apps in an existing web application or website without the use of IFrames.    Enterprise AI Features: Everything that your data science team needs to rapidly deliver AI/ML research and business initiatives.  - [**AI App Marketplace**](https://plotly.com/dash/ai-and-ml-templates/) Dash Enterprise ships with dozens of Dash app templates for business problems where AI/ML is having the greatest impact.  - [**Big Data for Pything**](https://plotly.com/dash/big-data-for-python/) Connect to Python's most popular big data back ends: Dask, Databricks, NVIDIA RAPIDS, Snowflake, Postgres, Vaex, and more.  - [**GPU & Dask Acceleration**](https://plotly.com/dash/gpu-dask-acceleration/) Dash Enterprise puts Python’s most popular HPC stack for GPU and parallel CPU computing in the hands of business users.  - [**Data Science Workspaces**](https://plotly.com/dash/workspaces/) Be productive from Day 1. Write and execute Python, R, & Julia code from Dash Enterprise's onboard code editor.    See [https://plotly.com/contact-us/](https://plotly.com/contact-us/) to get in touch.    ![image](https://images.prismic.io/plotly-marketing-website/493eec39-8467-4610-b9d0-d6ad3ea61423_Dash+Open+source%2BDash+enterprise2-01.jpg?auto=compress,format) """
Big data;https://github.com/bayandin/awesome-awesomeness;"""# Awesome Awesomeness    A curated list of amazingly awesome awesomeness.  - Programming Languages Package Manager      - [Package-Manager](https://github.com/damon-kwok/awesome-package-manager)    - Programming Languages  	- [Ada(Spark)](https://github.com/ohenley/awesome-ada)	  	- [Ansible](https://github.com/jdauphant/awesome-ansible)  	- [AutoHotkey](https://github.com/ahkscript/awesome-AutoHotkey)  	- [AutoIt](https://github.com/J2TeaM/awesome-AutoIt)  	- [C](https://notabug.org/koz.ross/awesome-c)  	- [C/C++](https://github.com/fffaraz/awesome-cpp)  	- [CMake](https://github.com/onqtam/awesome-cmake)  	- Clojure  		- [by @mbuczko](https://github.com/mbuczko/awesome-clojure)  		- [by @razum2um](https://github.com/razum2um/awesome-clojure)  	- [ColdFusion](https://github.com/seancoyne/awesome-coldfusion)  	- Common Lisp  		- [Common Lisp Libraries](https://github.com/CodyReichert/awesome-cl)  		- [Learning Common Lisp](https://github.com/GustavBertram/awesome-common-lisp-learning-list)  	- [Coronavirus](https://github.com/soroushchehresa/awesome-coronavirus)  	- [Crystal](https://github.com/veelenga/awesome-crystal)  	- [D](https://github.com/zhaopuming/awesome-d)  	- [Delphi](https://github.com/Fr0sT-Brutal/awesome-delphi)  	- [Elixir](https://github.com/h4cc/awesome-elixir)  	- [Elm](https://github.com/isRuslan/awesome-elm)  	- Erlang  		- [by @0xAX](https://github.com/0xAX/erlang-bookmarks)  		- [by @drobakowski](https://github.com/drobakowski/awesome-erlang)  		- [by @unbalancedparentheses](https://github.com/unbalancedparentheses/spawnedshelter)  	- [F#](https://github.com/fsprojects/awesome-fsharp)  	- [Fortran](https://github.com/rabbiabram/awesome-fortran)  	- [Go](https://github.com/avelino/awesome-go)  	- [Go Patterns](https://github.com/tmrts/go-patterns)  	- [Groovy](https://github.com/kdabir/awesome-groovy)  	- [Haskell](https://github.com/krispo/awesome-haskell)  	- [Idris](https://github.com/joaomilho/awesome-idris)  	- [Java](https://github.com/akullpp/awesome-java)  	- [JavaScript](https://github.com/sorrycc/awesome-javascript)  		- [Angular 2](https://github.com/AngularClass/awesome-angular)  		- [Ember.js](https://github.com/nmec/awesome-ember)  		- [JavaScript Learning Resources](https://github.com/micromata/awesome-javascript-learning)  		- [Koa](https://github.com/ellerbrock/awesome-koa)  		- [Node.js](https://github.com/sindresorhus/awesome-nodejs)  			- [Cross-platform Node.js](https://github.com/bcoe/awesome-cross-platform-nodejs)  			- [Node ESM](https://github.com/talentlessguy/awesome-node-esm)  		- [React](https://github.com/enaqx/awesome-react)  		- [Svelte](https://github.com/flagello/awesome-sveltejs)  		- [VueJS](https://github.com/vuejs/awesome-vue)  	- [Julia](https://github.com/svaksha/Julia.jl)  	- [Kotlin](https://github.com/KotlinBy/awesome-kotlin)  	- [Kotlin/Native](https://github.com/bipinvaylu/awesome-kotlin-native)  	- Lua  		- [by @forhappy](https://github.com/forhappy/awesome-lua)  		- [by @lewisjellis](https://github.com/LewisJEllis/awesome-lua)  	- [MongoDB](https://github.com/ramnes/awesome-mongodb)  	- [MySQL](https://github.com/shlomi-noach/awesome-mysql)  	- .NET  	        - [by @mehdihadeli](https://github.com/mehdihadeli/awesome-dotnet-core-education)  		- [by @quozd](https://github.com/quozd/awesome-dotnet)  		- [by @tallesl](https://github.com/tallesl/net-libraries-that-make-your-life-easier)  		- [by @thangchung](https://github.com/thangchung/awesome-dotnet-core)  	- [Nim](https://github.com/VPashkov/awesome-nim)  	- [OCaml](https://github.com/ocaml-community/awesome-ocaml)  	- [Perl](https://github.com/hachiojipm/awesome-perl)  	- [PHP](https://github.com/ziadoz/awesome-php)  		- [CakePHP](https://github.com/FriendsOfCake/awesome-cakephp)  	- [Postgres](https://github.com/dhamaniasad/awesome-postgres)  	- Python  		- [by @kirang89](https://github.com/kirang89/pycrumbs)  		- [by @svaksha](https://github.com/svaksha/pythonidae)  		- [by @trekhleb](https://github.com/trekhleb/learn-python)  		- [by @vinta](https://github.com/vinta/awesome-python)  		- [awesome-python-in-education](https://github.com/quobit/awesome-python-in-education)  	- [R](https://github.com/qinwf/awesome-R)  	- Ruby  		- [by @dreikanter](https://github.com/dreikanter/ruby-bookmarks)  		- [by @markets](https://github.com/markets/awesome-ruby)  		- [by @Sdogruyol](https://github.com/Sdogruyol/awesome-ruby)  		- [by @asyraffff](https://github.com/asyraffff/Open-Source-Ruby-and-Rails-Apps)  	- [Rust](https://github.com/rust-unofficial/awesome-rust)  	- [SAS](https://github.com/huyingjie/awesome-SAS)  	- [Scala](https://github.com/lauris/awesome-scala)  	- [Shell](https://github.com/alebcay/awesome-shell)  	- Swift  		- [by @matteocrippa](https://github.com/matteocrippa/awesome-swift)  		- [by @MaxChen](https://github.com/MaxChen/awesome-swift-and-tutorial-resources)  		- [by @Wolg](https://github.com/Wolg/awesome-swift)  		- [from ZEEF by @Edubits](https://swift.zeef.com/robin.eggenkamp)  	- TypeScript  		- [by @brookshi](https://github.com/brookshi/awesome-typescript-projects)  		- [by @dzharii](https://github.com/dzharii/awesome-typescript)  		- [by @ellerbrock](https://github.com/ellerbrock/awesome-typescript)  	- [V](https://github.com/vlang/awesome-v)    - General  	- [.htaccess](https://github.com/phanan/htaccess)  	- Accessibility  		- [by @a11yproject](https://github.com/a11yproject/a11yproject.com)  		- [by @brunopulis](https://github.com/brunopulis/awesome-a11y)  	- [Agile](https://github.com/lorabv/awesome-agile)  	- [Algolia](https://github.com/algolia/awesome-algolia)  	- [Algorithms](https://github.com/tayllan/awesome-algorithms)  		- [Algorithms Visualisation](https://github.com/enjalot/algovis)  		- [Big O Notation](https://github.com/okulbilisim/awesome-big-o)  	- [Amazon Web Services](https://github.com/donnemartin/awesome-aws)  	- [Analytics](https://github.com/onurakpolat/awesome-analytics)  	- [Android](https://github.com/JStumpp/awesome-android)  		- [Android Apps](https://github.com/LinuxCafeFederation/awesome-android)  		- [Android Release Notes](https://github.com/pedronveloso/awesome-android-release-notes)  		- [Android Security](https://github.com/ashishb/android-security-awesome)  		- [Android UI](https://github.com/wasabeef/awesome-android-ui)  	- [ARM Exploitation](https://github.com/HenryHoggard/awesome-arm-exploitation)  	- [Software Architecture](https://github.com/simskij/awesome-software-architecture)  	- [Arduino](https://github.com/Lembed/Awesome-arduino)  	- [Artificial intelligence](https://github.com/owainlewis/awesome-artificial-intelligence)  	- API  		- [by @Kikobeats](https://github.com/Kikobeats/awesome-api)  		- [by @toddmotto](https://github.com/toddmotto/public-apis)  	- [Apple](https://github.com/joeljfischer/awesome-apple)  		- [OS X](https://github.com/iCHAIT/awesome-macOS)  		- [OS X and iOS Security](https://github.com/ashishb/osx-and-ios-security-awesome)  	- [Beacons](https://github.com/beaconinside/awesome-beacon)  	- Big data  		- [by @onurakpolat](https://github.com/onurakpolat/awesome-bigdata)  		- [by @zenkay](https://github.com/zenkay/bigdata-ecosystem)  		- [Hadoop](https://github.com/youngwookim/awesome-hadoop)  	- [Blazor](https://github.com/AdrienTorris/awesome-blazor)  	- Blockchain  		- [by @0xtokens](https://github.com/0xtokens/awesome-blockchain)  		- [by @imbaniac](https://github.com/imbaniac/awesome-blockchain)  		- [by @coderplex](https://github.com/coderplex/awesome-blockchain)  		- [by @hitripod](https://github.com/hitripod/awesome-blockchain)  		- [by @iNiKe](https://github.com/iNiKe/awesome-blockchain)  		- [by @igorbarinov](https://github.com/igorbarinov/awesome-blockchain)  		- [by @istinspring](https://github.com/istinspring/awesome-blockchain)  		- [by @openblockchains](https://github.com/openblockchains/awesome-blockchains)  		- [by @kennethreitz](https://github.com/kennethreitz/awesome-coins)  		- [awesome-token-sale](https://github.com/holographicio/awesome-token-sale)  		- Bitcoin  			- [by @btcbrdev](https://github.com/btcbrdev/awesome-btcdev)  			- [by @igorbarinov](https://github.com/igorbarinov/awesome-bitcoin)  			- [Bitcoin Payment Processors](https://github.com/alexk111/awesome-bitcoin-payment-processors)  		- Ethereum  			- [by @vinsgo](https://github.com/vinsgo/awesome-ethereum)  			- [awesome-ethereum-virtual-machine](https://github.com/pirapira/awesome-ethereum-virtual-machine)  			- [by @Tom2718](https://github.com/Tom2718/Awesome-Ethereum)  		- [Ripple](https://github.com/vhpoet/awesome-ripple)  	- [Boilerplates](https://github.com/melvin0008/awesome-projects-boilerplates)  	- Books  		- [Free Programming Books](https://github.com/EbookFoundation/free-programming-books)  		- [Free Software Testing Books](https://github.com/ligurio/free-software-testing-books)  		- [Mind Expanding Books](https://github.com/hackerkid/Mind-Expanding-Books)  	- [Bootstrap](https://github.com/therebelrobot/awesome-bootstrap)  	- [BSD Software](https://github.com/SaintFenix/Awesome-BSD-Ports-Programs-And-Projects)  	- [Building Blocks for Web Apps](https://github.com/componently-com/awesome-building-blocks-for-web-apps)  	- [Web Effect](https://github.com/lindelof/awesome-web-effect)  	- [Landing Page](https://github.com/nordicgiant2/awesome-landing-page)  	- [Captcha](https://github.com/ZYSzys/awesome-captcha)  	- [Challenges](https://github.com/mauriciovieira/awesome-challenges)  	- [Code Formatters](https://github.com/rishirdua/awesome-code-formatters)  	- [Community Detection](https://github.com/benedekrozemberczki/awesome-community-detection)  	- [Competitive Programming](https://github.com/lnishan/awesome-competitive-programming)  	- [Computer Vision](https://github.com/jbhuang0604/awesome-computer-vision)  	- [Conferences](https://github.com/RichardLitt/awesome-conferences)  	- [Continuous Delivery](https://github.com/ciandcd/awesome-ciandcd)  	- [Conversational UI](https://github.com/mortenjust/awesome-conversational/)  	- [Cordova](https://github.com/busterc/awesome-cordova)  	- [Courses](https://github.com/prakhar1989/awesome-courses)  	- [Creative Commons Media](https://github.com/shime/creative-commons-media)  	- Cryptography  		- [by @MaciejCzyzewski](https://github.com/MaciejCzyzewski/retter)  		- [by @sobolevn](https://github.com/sobolevn/awesome-cryptography)  		- [by @coinpride](https://github.com/coinpride/CryptoList)  	- [Crypto Papers](https://github.com/pFarb/awesome-crypto-papers)  	- [CSS](https://github.com/sotayamashita/awesome-css)  		- [CSS Frameworks](https://github.com/troxler/awesome-css-frameworks)  	- [Data Science](https://github.com/bulutyazilim/awesome-datascience)  		- Data Science with Python  		  	- [by @r0f1](https://github.com/r0f1/datascience)  			- [by @krzjoa](https://github.com/krzjoa/awesome-python-data-science)  	- [Data Visualization](https://github.com/fasouto/awesome-dataviz)  	- [Database](https://github.com/numetriclabz/awesome-db)  		- [SQLAlchemy](https://github.com/dahlia/awesome-sqlalchemy)  	- Datasets  		- [by @caesar0301](https://github.com/caesar0301/awesome-public-datasets)  		- [by @leomaurodesenv](https://github.com/leomaurodesenv/game-datasets)  	- Deep Learning  		- [by @ChristosChristofidis](https://github.com/ChristosChristofidis/awesome-deep-learning)  		- [by @guillaume-chevalier](https://github.com/guillaume-chevalier/awesome-deep-learning-resources)  		- [by @tigerneil](https://github.com/tigerneil/awesome-deep-rl)  		- [by @nerox8664](https://github.com/nerox8664/awesome-computer-vision-models)  	- [Decision Tree Papers](https://github.com/benedekrozemberczki/awesome-decision-tree-papers)  	- [Design Patterns](https://github.com/DovAmir/awesome-design-patterns)  	- [Design Tools](https://github.com/LisaDziuba/Awesome-Design-Tools)  	- [Design](https://github.com/gztchan/awesome-design)  	- [Dev Env](https://github.com/jondot/awesome-devenv)  	- [DevOps](https://github.com/joubertredrat/awesome-devops)  	- [DevSecOps](https://github.com/TaptuIT/awesome-devsecops)  	- [Django](https://github.com/wsvincent/awesome-django)  	- [Docker](https://github.com/veggiemonk/awesome-docker)  	- [Documentation](https://github.com/PharkMillups/beautiful-docs)  	- [Dotfiles](https://github.com/webpro/awesome-dotfiles)  	- [Electron](https://github.com/sindresorhus/awesome-electron)  	- [Emacs](https://github.com/emacs-tw/awesome-emacs)  	- [Embedded](https://github.com/nhivp/Awesome-Embedded)  	- [Ethics](https://github.com/HussainAther/awesome-ethics)  	- [Falsehood](https://github.com/kdeldycke/awesome-falsehood)  	- [FastAPI](https://github.com/mjhea0/awesome-fastapi)  	- [FIRST Robotics Competition](https://github.com/andrewda/awesome-frc)  	- [Flask](https://github.com/mjhea0/awesome-flask)  	- [FluidApp Resources](https://github.com/lborgav/awesome-fluidapp)  	- [Flutter](https://github.com/Solido/awesome-flutter)  	- [Fonts](https://github.com/brabadu/awesome-fonts)  	- [Free Open Source Software (FOSS)](https://github.com/ishanvyas22/awesome-open-source-systems)  	- [Fraud Detection Papers](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers)  	- [Free Services](https://github.com/ripienaar/free-for-dev)  	- Frontend  		- [by @dypsilon](https://github.com/dypsilon/frontend-dev-bookmarks)  		- [by @moklick](https://github.com/moklick/frontend-stuff)  	- [Game Development](https://github.com/ellisonleao/magictools)  	- [Games](https://github.com/leereilly/games)  	- GIF  		- [by @Kikobeats](https://github.com/Kikobeats/awesome-gif)  	- [Gists](https://github.com/vsouza/awesome-gists)  	- [Git](https://github.com/dictcp/awesome-git)  	- [GitHub](https://github.com/Kikobeats/awesome-github)  		- [Browser extensions for GitHub](https://github.com/stefanbuck/awesome-browser-extensions-for-github)  		- [GitHub - Chinese](https://github.com/AntBranch/awesome-github)  	- [Gradient Boosting Papers](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers)  	- [Graph Classification](https://github.com/benedekrozemberczki/awesome-graph-classification)  	- [GraphQL](https://github.com/chentsulin/awesome-graphql)  	- [Growth Hacking](https://github.com/btomashvili/awesome-growth-hacking)  	- Guides  		- [by @narkoz](https://github.com/narkoz/guides)  		- [by @RichardLitt](https://github.com/RichardLitt/awesome-styleguides)  	- Hacking  		- [by @carpedm20](https://github.com/carpedm20/awesome-hacking)  		- [by @Hack-with-Github](https://github.com/Hack-with-Github/Awesome-Hacking)  	- [HTML5](https://github.com/diegocard/awesome-html5)  	- [Honeypots](https://github.com/paralax/awesome-honeypots)  	- [Hyper](https://github.com/bnb/awesome-hyper)  	- [Incident Response](https://github.com/meirwah/awesome-incident-response)  	- [Images](https://github.com/heyalexej/awesome-images)  	- [Image coloring](https://github.com/oskar-j/awesome-image-coloring)  	- [Internationalization](https://github.com/jpomykala/awesome-i18n)  	- [Internet of Things (IOT)](https://github.com/HQarroum/awesome-iot)  	- [iOS](https://github.com/vsouza/awesome-ios)  		- [Cocoa Controls](https://github.com/v-braun/awesome-cocoa)  		- [Open Source Apps](https://github.com/dkhamsing/open-source-ios-apps)  		- [UI](https://github.com/cjwirth/awesome-ios-ui)  	- [JSON](https://github.com/burningtree/awesome-json)  	- [Jupyter](https://github.com/markusschanta/awesome-jupyter)  	- [JVM](https://github.com/deephacks/awesome-jvm)  	- [Kafka](https://github.com/monksy/awesome-kafka)  	- [Koans](https://github.com/ahmdrefat/awesome-koans)  	- [Laravel](https://github.com/chiraggude/awesome-laravel)  	- [Leadership and Management](https://github.com/LappleApple/awesome-leading-and-managing)  	- [Lego](https://github.com/adius/awesome-lego)  	- [Linux Containers](https://github.com/Friz-zy/awesome-linux-containers)  	- [Linux resources](https://github.com/itech001/awesome-linux-resources)  	- Lists  		- [by @bayandin](https://github.com/bayandin/awesome-awesomeness)  		- [by @jnv](https://github.com/jnv/lists)  		- [by @sindresorhus](https://github.com/sindresorhus/awesome)  	- [Mac]  		- [by @xyNNN](https://github.com/xyNNN/awesome-mac)  		- [by @justin-j](https://github.com/justin-j/awesome-mac-apps)  	- [Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)  	- [Malware Analysis](https://github.com/rshipp/awesome-malware-analysis)  	- [Material Design](https://github.com/sachin1092/awesome-material)  	- [Math](https://github.com/rossant/awesome-math)  	- [Matlab](https://github.com/mikecroucher/awesome-MATLAB)  	- [Mental Health](https://github.com/dreamingechoes/awesome-mental-health)  	- [micro:bit](https://github.com/carlosperate/awesome-microbit)  	- [MLOps](https://github.com/kelvins/awesome-mlops)  	- [Mobile marketing and development](https://github.com/alec-c4/awesome-mobile)  	- [Mobile Web Development](https://github.com/myshov/awesome-mobile-web-development)  	- [Monitoring](https://github.com/crazy-canux/awesome-monitoring)  		- [Prometheus](https://github.com/roaldnefs/awesome-prometheus)  		- [Prometheus alerting rules](https://github.com/samber/awesome-prometheus-alerts)          - [Monte Carlo Tree Search Papers](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers)  	- [Motion Design for Web](https://github.com/lucasmaiaesilva/awesome-motion-design-web)  	- [Nginx](https://github.com/fcambus/nginx-resources)  	- Newsletters  		- [by @vredniy](https://github.com/vredniy/awesome-newsletters)  		- [by @webpro](https://github.com/webpro/awesome-newsletters)  		- [by @mpron](https://github.com/mpron/awesome-newsletters)  	- [No Login Web Apps](https://github.com/aviaryan/awesome-no-login-web-apps)  	- [Open Science](https://github.com/silky/awesome-open-science)  	- [Open Source Photography](https://github.com/ibaaj/awesome-OpenSourcePhotography)  	- [Papers](https://github.com/papers-we-love/papers-we-love)  	- [Podcasts](https://github.com/Ghosh/awesome-podcasts)  	- [Philosophy](https://github.com/HussainAther/awesome-philosophy)  	- [Pipelines](https://github.com/pditommaso/awesome-pipeline)  	- [Product Manager](https://github.com/hugo53/awesome-ProductManager)  	- Protocols  		- [OSC](https://github.com/amir-arad/awesome-osc) (open sound control)  	- [Pentest Cheat Sheets](https://github.com/coreb1t/awesome-pentest-cheat-sheets)  	- [Quick Look Plugins](https://github.com/sindresorhus/quick-look-plugins)  	- [Random-Forest](https://github.com/kjw0612/awesome-random-forest)  	- Raspberry Pi  		- [by @blackout314](https://github.com/blackout314/awesome-raspberry-pi)  		- [by @thibmaek](https://github.com/thibmaek/awesome-raspberry-pi)  	- [React Native](https://github.com/jondot/awesome-react-native)  	- [README](https://github.com/matiassingers/awesome-readme)  	- [Regex](https://github.com/aloisdg/awesome-regex)  	- [Remote Job](https://github.com/lukasz-madon/awesome-remote-job)  	- [Remote Work](https://github.com/hugo53/awesome-RemoteWork)  	- [REST](https://github.com/marmelab/awesome-rest)  	- [Robotics](https://github.com/Kiloreux/awesome-robotics)  	- [Robotic Tooling](https://github.com/protontypes/awesome-robotic-tooling)  	- [RNN](https://github.com/kjw0612/awesome-rnn)  	- [Scalability](https://github.com/binhnguyennus/awesome-scalability)  	- [Science Fiction](https://github.com/sindresorhus/awesome-scifi)  	- Search Engine Optimization (SEO)  		- [by @marcobiedermann](https://github.com/marcobiedermann/search-engine-optimization)  		- [by @sneg55](https://github.com/sneg55/curatedseotools)  		- [by @teles](https://github.com/teles/awesome-seo)  	- [Security](https://github.com/sbilly/awesome-security)  	- [Selfhosted](https://github.com/Kickball/awesome-selfhosted)  	- [Serverless](https://github.com/anaibol/awesome-serverless)  	- [Serverless Security](https://github.com/puresec/awesome-serverless-security/)  	- [Service Fabric](https://github.com/lawrencegripper/awesome-servicefabric)  	- [Services Engineering](https://github.com/mmcgrana/services-engineering)  	- [Sheet Music](https://github.com/adius/awesome-sheet-music)  	- [Slack](https://github.com/matiassingers/awesome-slack)  	- [Sound](https://github.com/hwclass/awesome-sound)  	- [Space](https://github.com/elburz/awesome-space)  		- [Books and manuals](https://github.com/Hunter-Github/awesome-space-books)  	- [Speech and Natural Language Processing](https://github.com/edobashira/speech-language-processing)  		- [NLP with Ruby](https://github.com/arbox/nlp-with-ruby)  	- [Sphinx Documentation](https://github.com/yoloseem/awesome-sphinxdoc)  	- [Startup](https://github.com/KrishMunot/awesome-startup)  	- [Static Analysis](https://github.com/mre/awesome-static-analysis/)  	- [Styleguides](https://github.com/RichardLitt/awesome-styleguides)  	- [Sublime Text](https://github.com/dreikanter/sublime-bookmarks)  	- [Sustainable Technology](https://github.com/protontypes/awesome-sustainable-technology)  	- [SVG](https://github.com/willianjusten/awesome-svg)  	- [Swedish](https://github.com/gurre/awesome-swedish-opensource)  	- [Sysadmin](https://github.com/kahun/awesome-sysadmin)  	- [Taglines](https://github.com/miketheman/awesome-taglines)  	- [Tailwind CSS](https://github.com/aniftyco/awesome-tailwindcss)  	- [Talks](https://github.com/JanVanRyswyck/awesome-talks)  		- [Gaming](https://github.com/hzoo/awesome-gametalks)  	- [Telegram](https://github.com/ebertti/awesome-telegram)  	- [Terminals Are Sexy](https://github.com/k4m4/terminals-are-sexy)  	- [Test Automation](https://github.com/atinfo/awesome-test-automation)  	- [Testing](https://github.com/TheJambo/awesome-testing)  		- [JMeter](https://github.com/aliesbelik/awesome-jmeter)  	- [Threat Intelligence](https://github.com/hslatman/awesome-threat-intelligence)  	- [Tools](https://github.com/cjbarber/ToolsOfTheTrade)  	- [Twilio](https://github.com/Twilio-org/awesome-twilio)  	- [Unity](https://github.com/RyanNielson/awesome-unity)  	- [UI Styleguide](https://github.com/kevinwuhoo/ui-styleguides)  		- [UI Components for Styleguide](https://github.com/anubhavsrivastava/awesome-ui-component-library)  	- [UNIX](https://github.com/sirredbeard/Awesome-UNIX)  	- [Vagrant](https://github.com/iJackUA/awesome-vagrant)  	- [Vehicle Security](https://github.com/jaredthecoder/awesome-vehicle-security)  	- Vim  		- [by @akrawchyk](https://github.com/akrawchyk/awesome-vim)  		- [by @matteocrippa](https://github.com/matteocrippa/awesome-vim)  	- [Vulkan](https://github.com/vinjn/awesome-vulkan)  	- [Web Performance Optimization](https://github.com/davidsonfellipe/awesome-wpo)  	- [WebComponents](https://github.com/mateusortiz/webcomponents-the-right-way)  	- [Wordpress](https://github.com/miziomon/awesome-wordpress)  	- [Workshops](https://github.com/therebelrobot/awesome-workshopper)  	- [Xamarin](https://github.com/benoitjadinon/awesome-xamarin)  	- XMPP  		- [Ejabberd](https://github.com/shantanu-deshmukh/awesome-ejabberd)  	- [Typography](https://github.com/Jolg42/awesome-typography)    ## License    [![Creative Commons License](http://i.creativecommons.org/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/)    This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/). """
Big data;https://github.com/linkedin/kamikaze;"""What is Kamikaze  ===============    Kamikaze is a utility package wrapping set implementations on document lists.     It also implements the PForDelta compression algorithm for sorted integer segments to enable Inverted List compression for search engines like Lucene (http://lucene.apache.org/core/4_5_1/core/org/apache/lucene/util/PForDeltaDocIdSet.html).     Kamikaze is based on the PForDelta algorithm proposed in the following paper:  Inverted Index Compression and Query Processing with Optimized Document Ordering   Hao Yan, S.Ding and T.Suel.  The 18th International World Wide Web Conference (WWW'09), Madrid, Spain, April 2009    Kamikaze is open sourced by LinkedIn Corp : http://data.linkedin.com/opensource/kamikaze.    The principal committer of Kamikaze is Hao Yan. If you have any questions regarding Kamikaze, please email him at hyan@linkedin.com.    ------------------------------------    ### Wiki    Wiki is available [HERE](http://snaprojects.jira.com/wiki/display/KAMI/Home)    ### Issues    Issues are tracked [HERE](http://snaprojects.jira.com/browse/KAMI) """
Big data;https://github.com/semi-technologies/weaviate;"""<h1>Weaviate <img alt='Weaviate logo' src='https://raw.githubusercontent.com/semi-technologies/weaviate/19de0956c69b66c5552447e84d016f4fe29d12c9/docs/assets/weaviate-logo.png' width='124' align='right' /></h1>    ## The ML-first vector search engine    [![Build Status](https://api.travis-ci.org/semi-technologies/weaviate.svg?branch=master)](https://travis-ci.org/semi-technologies/weaviate/branches)  [![Go Report Card](https://goreportcard.com/badge/github.com/semi-technologies/weaviate)](https://goreportcard.com/report/github.com/semi-technologies/weaviate)  [![Coverage Status](https://codecov.io/gh/semi-technologies/weaviate/branch/master/graph/badge.svg)](https://codecov.io/gh/semi-technologies/weaviate)  [![Slack](https://img.shields.io/badge/slack--channel-blue?logo=slack)](https://join.slack.com/t/weaviate/shared_invite/zt-goaoifjr-o8FuVz9b1HLzhlUfyfddhw)  [![Newsletter](https://img.shields.io/badge/newsletter-blue?logo=revue)](http://weaviate-newsletter.semi.technology/)    ## Description    **Weaviate in a nutshell**: Weaviate is a vector search engine and vector database. Weaviate uses machine learning to vectorize and store data, and to find answers to natural language queries. With Weaviate you can also bring your custom ML models to production scale.    **Weaviate in detail**: Weaviate is a low-latency vector search engine with out-of-the-box support for different media types (text, images, etc.). It offers Semantic Search, Question-Answer-Extraction, Classification, Customizable Models (PyTorch/TensorFlow/Keras), and more. Built from scratch in Go, Weaviate stores both objects and vectors, allowing for combining vector search with structured filtering with the fault-tolerance of a cloud-native database, all accessible through GraphQL, REST, and various language clients.    ## Weaviate helps ...    1. **Software Engineers** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate as an ML-first database for your applications.       * Out-of-the-box modules for: NLP/semantic search, automatic classification and image similarity search.      * Easy to integrate in your current architecture, with full CRUD support like you're used to from other OSS databases.      * Cloud-native, distributed, runs well on Kubernetes and scales with your workloads.    2. **Data Engineers** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate as a vector database that is built up from the ground with ANN at its core, and with the same UX they love from Lucene-based search engines.      * Weaviate has a modular setup that allows to use your own ML models inside Weaviate, but you can also use out-of-the-box ML models (e.g., SBERT, ResNet, fasttext, etc).      * Weaviate takes care of the scalability, so that you don't have to.      * Deploy and maintain ML models in production reliably and efficiently.    3. **Data Scientists** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate for a seamless handover of their Machine Learning models to MLOps.      * Deploy and maintain your ML models in production reliably and efficiently.      * Weaviate's modular design allows you to easily package any custom trained model you want.      * Smooth and accelerated handover of your Machine Learning models to engineers.    ## GraphQL interface demo    <a href=""https://weaviate.io/developers/weaviate/current/"" target=""_blank""><img src=""https://weaviate.io/img/weaviate-demo.gif?i=8"" alt=""Demo of Weaviate"" width=""100%""></a>    <sup>Weaviate GraphQL demo on news article dataset containing: Transformers module, GraphQL usage, semantic search, _additional{} features, Q&A, and Aggregate{} function. You can the demo on this dataset in the GUI here: <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Get%20%7B%0A%20%20%20%20Article(%0A%20%20%20%20%20%20nearText%3A%20%7B%0A%20%20%20%20%20%20%20%20concepts%3A%20%5B%22Housing%20prices%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20where%3A%20%7B%0A%20%20%20%20%20%20%20%20operator%3A%20Equal%0A%20%20%20%20%20%20%20%20path%3A%20%5B%22inPublication%22%2C%20%22Publication%22%2C%20%22name%22%5D%0A%20%20%20%20%20%20%20%20valueString%3A%20%22The%20Economist%22%0A%20%20%20%20%20%20%7D%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inPublication%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Publication%20%7B%0A%20%20%20%20%20%20%20%20%20%20name%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20certainty%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">semantic search</a>, <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Get%7B%0A%20%20%20%20Article(%0A%20%20%20%20%20%20ask%3A%20%7B%0A%20%20%20%20%20%20%20%20question%3A%20%22What%20did%20Jemina%20Packington%20predict%3F%22%0A%20%20%20%20%20%20%20%20properties%3A%20%5B%22summary%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%201%0A%20%20%20%20)%7B%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inPublication%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Publication%20%7B%0A%20%20%20%20%20%20%20%20%20%20name%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20answer%20%7B%0A%20%20%20%20%20%20%20%20%20%20endPosition%0A%20%20%20%20%20%20%20%20%20%20property%0A%20%20%20%20%20%20%20%20%20%20result%0A%20%20%20%20%20%20%20%20%20%20startPosition%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">Q&A</a>, <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Aggregate%20%7B%0A%20%20%20%20Article%20%7B%0A%20%20%20%20%20%20meta%20%7B%0A%20%20%20%20%20%20%20%20count%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">Aggregate</a>.</sup>    ## Features    Weaviate makes it easy to use state-of-the-art ML models while giving you the scalability, ease of use, safety and cost-effectiveness of a purpose-built vector database. Most notably:    * **Fast queries**<br>     Weaviate typically performs a 10-NN neighbor search out of millions of objects in considerably less than 100ms.     <br><sub></sub>    * **Any media type with Weaviate Modules**<br>    Use State-of-the-Art ML model inference (e.g. Transformers) for Text, Images, etc. at search and query time to let Weaviate manage the process of vectorizing your data for your - or import your own vectors.    * **Combine vector and scalar search**<br>    Weaviate allows for efficient combined vector and scalar searches, e.g “articles related to the COVID 19 pandemic published within the past 7 days”. Weaviate stores both your objects and the vectors and make sure the retrieval of both is always efficient. There is no need for a third party object storage.     * **Real-time and persistent**<br>  Weaviate let’s you search through your data even if it’s currently being imported or updated. In addition, every write is written to a Write-Ahead-Log (WAL) for immediately persisted writes - even when a crash occurs.    * **Horizontal Scalability**<br>    Scale Weaviate for your exact needs, e.g. High-Availability, maximum ingestion, largest possible dataset size, maximum queries per second, etc. (Multi-Node sharding since `v1.8.0`, Replication under development)     * **Cost-Effectiveness**<br>    Very large datasets do not need to be kept entirely in memory in Weaviate. At the same time available memory can be used to increase the speed of queries. This allows for a conscious speed/cost trade-off to suit every use case.    * **Graph-like connections between objects**<br>    Make arbitrary connections between your objects in a graph-like fashion to resemble real-life connections between your data points. Traverse those connections using GraphQL.    ## Documentation    You can find detailed documentation in the [developers section of our website](https://weaviate.io/developers/weaviate/current/) or directly go to one of the docs using the links in the list below.    ## Additional material    ### Video    - [Weaviate introduction video](https://www.youtube.com/watch?v=IExopg1r4fw)    ### Reading    - [Weaviate is an open-source search engine powered by ML, vectors, graphs, and GraphQL (ZDNet)](https://www.zdnet.com/article/weaviate-an-open-source-search-engine-powered-by-machine-learning-vectors-graphs-and-graphql/)  - [Weaviate, an ANN Database with CRUD support (DB-Engines.com)](https://db-engines.com/en/blog_post/87)  - [A sub-50ms neural search with DistilBERT and Weaviate (Towards Datascience)](https://towardsdatascience.com/a-sub-50ms-neural-search-with-distilbert-and-weaviate-4857ae390154)  - [Getting Started with Weaviate Python Library (Towards Datascience)](https://towardsdatascience.com/getting-started-with-weaviate-python-client-e85d14f19e4f)    ## Examples    You can find [code examples here](https://github.com/semi-technologies/weaviate-examples)    ## Support    - [Stackoverflow for questions](https://stackoverflow.com/questions/tagged/weaviate)  - [Github for issues](https://github.com/semi-technologies/weaviate/issues)  - [Slack channel to connect](https://join.slack.com/t/weaviate/shared_invite/zt-goaoifjr-o8FuVz9b1HLzhlUfyfddhw)  - [Newsletter to stay in the know](http://weaviate-newsletter.semi.technology/)    ## Contributing    - [How to Contribute](https://weaviate.io/developers/contributor-guide/current/) """
Big data;https://github.com/dgraph-io/dgraph;"""![](/logo.png)    **The Only Native GraphQL Database With A Graph Backend.**    [![Wiki](https://img.shields.io/badge/res-wiki-blue.svg)](https://dgraph.io/docs/)  [![Build Status](https://teamcity.dgraph.io/guestAuth/app/rest/builds/buildType:(id:Dgraph_Ci)/statusIcon.svg)](https://teamcity.dgraph.io/viewLog.html?buildTypeId=Dgraph_Ci&buildId=lastFinished&guest=1)  [![Coverage Status](https://coveralls.io/repos/github/dgraph-io/dgraph/badge.svg?branch=master)](https://coveralls.io/github/dgraph-io/dgraph?branch=master)  [![Go Report Card](https://goreportcard.com/badge/github.com/dgraph-io/dgraph)](https://goreportcard.com/report/github.com/dgraph-io/dgraph)    Dgraph is a horizontally scalable and distributed GraphQL database with a graph backend. It provides ACID transactions, consistent replication, and linearizable reads. It's built from the ground up to perform for  a rich set of queries. Being a native GraphQL database, it tightly controls how the  data is arranged on disk to optimize for query performance and throughput,  reducing disk seeks and network calls in a cluster.      Dgraph's goal is to provide [Google](https://www.google.com) production level scale and throughput,  with low enough latency to be serving real-time user queries, over terabytes of structured data.  Dgraph supports [GraphQL query syntax](https://dgraph.io/docs/master/query-language/), and responds in [JSON](http://www.json.org/) and [Protocol Buffers](https://developers.google.com/protocol-buffers/) over [GRPC](http://www.grpc.io/) and HTTP.    **Use [Discuss Issues](https://discuss.dgraph.io/c/issues/dgraph/38) for reporting issues about this repository.**    ## Status    Dgraph is [at version v21.03.0][rel] and is production-ready. Apart from the vast open source community, it is being used in  production at multiple Fortune 500 companies, and by  [Intuit Katlas](https://github.com/intuit/katlas) and [VMware Purser](https://github.com/vmware/purser).    [rel]: https://github.com/dgraph-io/dgraph/releases/tag/v21.03.0    ## Quick Install    The quickest way to install Dgraph is to run this command on Linux or Mac.    ```bash  curl https://get.dgraph.io -sSf | bash  ```    ## Install with Docker    If you're using Docker, you can use the [official Dgraph image](https://hub.docker.com/r/dgraph/dgraph/).    ```bash  docker pull dgraph/dgraph:latest  ```    ## Install from Source    If you want to install from source, install Go 1.13+ or later and the following dependencies:    ### Ubuntu    ```bash  sudo apt-get update  sudo apt-get install gcc make  ```    ### Build and Install    Then clone the Dgraph repository and use `make install` to install the Dgraph binary to `$GOPATH/bin`.    ```bash  git clone https://github.com/dgraph-io/dgraph.git  cd ./dgraph  make install  ```    ## Get Started  **To get started with Dgraph, follow:**    - Installation to queries in 3 steps via [dgraph.io/docs/](https://dgraph.io/docs/get-started/).  - A longer interactive tutorial via [dgraph.io/tour/](https://dgraph.io/tour/).  - Tutorial and  presentation videos on [YouTube channel](https://www.youtube.com/channel/UCghE41LR8nkKFlR3IFTRO4w/featured).    ## Is Dgraph the right choice for me?    - Do you have more than 10 SQL tables connected via foreign keys?  - Do you have sparse data, which doesn't elegantly fit into SQL tables?  - Do you want a simple and flexible schema, which is readable and maintainable    over time?  - Do you care about speed and performance at scale?    If the answers to the above are YES, then Dgraph would be a great fit for your  application. Dgraph provides NoSQL like scalability while providing SQL like  transactions and the ability to select, filter, and aggregate data points. It  combines that with distributed joins, traversals, and graph operations, which  makes it easy to build applications with it.    ## Dgraph compared to other graph DBs    | Features | Dgraph | Neo4j | Janus Graph |  | -------- | ------ | ----- | ----------- |  | Architecture | Sharded and Distributed | Single server (+ replicas in enterprise) | Layer on top of other distributed DBs |  | Replication | Consistent | None in community edition (only available in enterprise) | Via underlying DB |  | Data movement for shard rebalancing | Automatic | Not applicable (all data lies on each server) | Via underlying DB |  | Language | GraphQL inspired | Cypher, Gremlin | Gremlin |  | Protocols | Grpc / HTTP + JSON / RDF | Bolt + Cypher | Websocket / HTTP |  | Transactions | Distributed ACID transactions | Single server ACID transactions | Not typically ACID  | Full-Text Search | Native support | Native support | Via External Indexing System |  | Regular Expressions | Native support | Native support | Via External Indexing System |  | Geo Search | Native support | External support only | Via External Indexing System |  | License | Apache 2.0 | GPL v3 | Apache 2.0 |    ## Users  - **Dgraph official documentation is present at [dgraph.io/docs/](https://dgraph.io/docs/).**  - For feature requests or questions, visit    [https://discuss.dgraph.io](https://discuss.dgraph.io).  - Check out [the demo at dgraph.io](http://dgraph.io) and [the visualization at    play.dgraph.io](http://play.dgraph.io/).  - Please see [releases tab](https://github.com/dgraph-io/dgraph/releases) to    find the latest release and corresponding release notes.  - [See the Roadmap](https://discuss.dgraph.io/t/dgraph-product-roadmap-2021/12284) for a list of    working and planned features.  - Read about the latest updates from the Dgraph team [on our    blog](https://open.dgraph.io/).  - Watch tech talks on our [YouTube    channel](https://www.youtube.com/channel/UCghE41LR8nkKFlR3IFTRO4w/featured).    ## Developers  - See a list of issues [that we need help with](https://github.com/dgraph-io/dgraph/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22).  - Please see [Contributing to Dgraph](https://github.com/dgraph-io/dgraph/blob/master/CONTRIBUTING.md) for guidelines on contributions.    ## Client Libraries  The Dgraph team maintains several [officially supported client libraries](https://dgraph.io/docs/clients/). There are also libraries contributed by the community [unofficial client libraries](https://dgraph.io/docs/clients#unofficial-dgraph-clients).    ## Contact  - Please use [discuss.dgraph.io](https://discuss.dgraph.io) for documentation, questions, feature requests and discussions.  - Please use [discuss.dgraph.io](https://discuss.dgraph.io/c/issues/dgraph/38) for filing bugs or feature requests.  - Follow us on Twitter [@dgraphlabs](https://twitter.com/dgraphlabs). """
Big data;https://github.com/cbd/edis;"""An *Erlang* version of [Redis](http://redis.io), with the goal of similar algorithmic performance but support for multiple master nodes and larger-than-RAM datasets. For [More info](http://inakanetworks.com/assets/pdf//Edis_Implementing_Redis_In_Erlang.pdf), see this PDF of a Talk at Erlang Factory 2012.    ## Contact Us  For **questions** or **general comments** regarding the use of this library, please use our public  [hipchat room](http://inaka.net/hipchat).    If you find any **bugs** or have a **problem** while using this library, please [open an issue](https://github.com/inaka/edis/issues/new) in this repo (or a pull request :)).    And you can check all of our open-source projects at [inaka.github.io](http://inaka.github.io)    ## Usage  Just run `$ make run` and open connections with your favourite redis client.    ## Differences with Redis  ### Different Behaviour  * _SAVE_, _BGSAVE_ and _LASTSAVE_ are database dependent. The original Redis saves all databases at once, edis saves just the one you _SELECT_'ed.  * _INFO_ provides much less information and no statistics (so, _CONFIG RESETSTAT_ does nothing at all)  * _MULTI_ doesn't support:    - cross-db commands (i.e. _FLUSHALL_, _SELECT_, _MOVE_)    - non-db commands (i.e _AUTH_, _CONFIG *_, _SHUTDOWN_, _MONITOR_)    - pub/sub commands (i.e. _PUBLISH_, _SUBSCRIBE_, _UNSUBSCRIBE_, _PSUBSCRIBE_, _PUNSUBSCRIBE_)  * _(P)UNSUBSCRIBE_ commands are not allowed outside _PUBSUB_ mode  * _PUBLISH_ response is not precise: it's the amount of all clients subscribed to any channel and/or pattern, not just those that will handle the message. On the other hand it runs in _O(1)_ because it's asynchronous, it just dispatches the message.    ### Missing Features  * Dynamic node configuration (i.e. the _SLAVEOF_ command is not implemented)  * Encoding optimization (i.e. all objects are encoded as binary representations of erlang terms, so for instance ""123"" will never be stored as an int)  * _OBJECT REFCOUNT_ allways returns 1 for existing keys and (nil) otherwise    ### Unsupported Commands  _SYNC_, _SLOWLOG_, _SLAVEOF_, _DEBUG *_    ### License  edis is licensed by Electronic Inaka, LLC under the Apache 2.0 license; see the LICENSE file in this repository.    ### TODO    * Backends    * HanoiDB      * Make use of the efficient range searchs with start/end for searching ranges      * Make use of time-based key expiry      * Finish the TODO items   """
Big data;https://github.com/linkedin/gobblin;"""# Apache Gobblin   [![Build Status](https://github.com/apache/gobblin/actions/workflows/build_and_test.yaml/badge.svg?branch=master)](https://travis-ci.org/apache/gobblin)  [![Documentation Status](https://readthedocs.org/projects/gobblin/badge/?version=latest)](https://gobblin.readthedocs.org/en/latest/?badge=latest)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.gobblin/gobblin-api/badge.svg)](https://search.maven.org/search?q=g:org.apache.gobblin)  [![Stack Overflow](http://img.shields.io/:stack%20overflow-gobblin-brightgreen.svg)](http://stackoverflow.com/questions/tagged/gobblin)  [![Join us on Slack](https://img.shields.io/badge/slack-apache--gobblin-brightgreen.svg)]( https://join.slack.com/t/apache-gobblin/shared_invite/zt-vqgdztup-UUq8S6gGJqE6L5~9~JelNg)  [![codecov.io](https://codecov.io/github/apache/gobblin/branch/master/graph/badge.svg)](https://codecov.io/github/apache/gobblin)    Apache Gobblin is a highly scalable data management solution for structured and byte-oriented data in heterogeneous data ecosystems.     ### Capabilities  - Ingestion and export of data from a variety of sources and sinks into and out of the data lake. Gobblin is optimized and designed for ELT patterns with inline transformations on ingest (small t).  - Data Organization within the lake (e.g. compaction, partitioning, deduplication)  - Lifecycle Management of data within the lake (e.g. data retention)  - Compliance Management of data across the ecosystem (e.g. fine-grain data deletions)    ### Highlights  - Battle tested at scale: Runs in production at petabyte-scale at companies like LinkedIn, PayPal, Verizon etc.  - Feature rich: Supports task partitioning, state management for incremental processing, atomic data publishing, data quality checking, job scheduling, fault tolerance etc.  - Supports stream and batch execution modes   - Control Plane (Gobblin-as-a-service) supports programmatic triggering and orchestration of data plane operations.     ### Common Patterns used in production  - Stream / Batch ingestion of Kafka to Data Lake (HDFS, S3, ADLS)  - Bulk-loading serving stores from the Data Lake (e.g. HDFS -> Couchbase)  - Support for data sync across Federated Data Lake (HDFS <-> HDFS, HDFS <-> S3, S3 <-> ADLS)  - Integrate external vendor API-s (e.g. Salesforce, Dynamics etc.) with data store (HDFS, Couchbase etc)  - Enforcing Data retention policies and GDPR deletion on HDFS / ADLS      ### Apache Gobblin is NOT  - A general purpose data transformation engine like Spark or Flink. Gobblin can delegate complex-data processing tasks to Spark, Hive etc.   - A data storage system like Apache Kafka or HDFS. Gobblin integrates with these systems as sources or sinks.   - A general-purpose workflow execution system like Airflow, Azkaban, Dagster, Luigi.       # Requirements  * Java >= 1.8    If building the distribution with tests turned on:  * Maven version 3.5.3     # Instructions to run Apache RAT (Release Audit Tool)  1. Extract the archive file to your local directory.  2. Run `./gradlew rat`. Report will be generated under build/rat/rat-report.html    # Instructions to build the distribution  1. Extract the archive file to your local directory.  2. Skip tests and build the distribution:   Run `./gradlew build -x findbugsMain -x test -x rat -x checkstyleMain`   The distribution will be created in build/gobblin-distribution/distributions directory.  (or)  3. Run tests and build the distribution (requires Maven):   Run `./gradlew build`   The distribution will be created in build/gobblin-distribution/distributions directory.    # Quick Links      * [Gobblin documentation](https://gobblin.apache.org/docs/)      * [Running Gobblin on Docker from your laptop](https://github.com/apache/gobblin/blob/master/gobblin-docs/user-guide/Docker-Integration.md)      * [Getting started guide](https://gobblin.apache.org/docs/Getting-Started/)      * [Gobblin architecture](https://gobblin.apache.org/docs/Gobblin-Architecture/)    * Community Slack: [Get your invite]( https://join.slack.com/t/apache-gobblin/shared_invite/zt-vqgdztup-UUq8S6gGJqE6L5~9~JelNg)    * [List of companies known to use Gobblin](https://gobblin.apache.org/docs/Powered-By/)     * [Sample project](https://github.com/apache/gobblin/tree/master/gobblin-example)    * [How to build Gobblin from source code](https://gobblin.apache.org/docs/user-guide/Building-Gobblin/)    * [Issue tracker - Apache Jira](https://issues.apache.org/jira/projects/GOBBLIN/issues/) """
Big data;https://github.com/etsy/411;"""![411](/docs/imgs/logo.png?raw=true)      What is 411?  ============    [![Join the chat at https://gitter.im/411/Lobby](https://badges.gitter.im/411/Lobby.svg)](https://gitter.im/411/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  [![Build Status](https://travis-ci.org/etsy/411.svg?branch=master)](https://travis-ci.org/etsy/411)  [![Code Climate](https://codeclimate.com/github/etsy/411/badges/gpa.svg)](https://codeclimate.com/github/etsy/411)  [![Test Coverage](https://codeclimate.com/github/etsy/411/badges/coverage.svg)](https://codeclimate.com/github/etsy/411/coverage)    Search scheduling  -----------------    Configure Searches to periodically run against a variety of data sources. You can define a custom pipeline of Filters to manipulate any generated Alerts and forward them to multiple Targets.      Alert management  ----------------    Review and manage Alerts through the web interface. You can apply Renderers to alerts to enrich them with additional metadata.      Use cases  =========    - You want to detect when certain log lines show up in ES.  - You want to detect when a Graphite metric changes.  - You want to detect when a server stops responding  - You want to manage alerts through a simple workflow.    And much more!      Setup  =====    - [Setup instructions](/docs/Setup.md)  - [Docker instructions](/docs/Docker.md)      Help  ====    See the [Table of Contents](/docs/README.md) for a list of documentation pages.    If you've any questions, feel free to ask on Gitter. You can also contact us on Twitter at [@sixhundredns](https://twitter.com/sixhundredns) and [@kennysan](https://twitter.com/Kennysan).      Links  =====    - [CaC post](https://codeascraft.com/2016/09/15/introducing-411-a-new-open-source-framework-for-handling-alerting/)  - [Defcon presentation](https://www.youtube.com/watch?v=LQyqhrDl7f8)  - [Slides](https://speakerdeck.com/kennysan/building-effective-security-alerting)  - [Demo](https://demo.fouroneone.io) (User: user, Pass: user)      Contribute  ==========    Check out the contribution [guidelines](/CONTRIBUTING.md).      License  =======    411 is released under the [MIT License](/LICENSE). """
Big data;https://github.com/harthur/brain;"""*This project has reached the end of its development as a simple neural network library. Feel free to browse the code, but please use other JavaScript neural network libraries in development like [brain.js](https://github.com/BrainJS/brain.js) and [convnetjs](https://github.com/karpathy/convnetjs).*    # brain    `brain` is a JavaScript [neural network](http://neuralnetworksanddeeplearning.com/) library. Here's an example of using it to approximate the XOR function:    ```javascript  var net = new brain.NeuralNetwork();    net.train([{input: [0, 0], output: [0]},             {input: [0, 1], output: [1]},             {input: [1, 0], output: [1]},             {input: [1, 1], output: [0]}]);    var output = net.run([1, 0]);  // [0.987]  ```    There's no reason to use a neural network to figure out XOR however (-: so here's a more involved, realistic example:  [Demo: training a neural network to recognize color contrast](http://harthur.github.com/brain/)    ## Using in node  If you have [node](http://nodejs.org/) you can install with [npm](http://npmjs.org):    ```  npm install brain  ```    ## Using in the browser  Download the latest [brain.js](https://github.com/harthur/brain/tree/gh-pages). Training is computationally expensive, so you should try to train the network offline (or on a Worker) and use the `toFunction()` or `toJSON()` options to plug the pre-trained network in to your website.    ## Training  Use `train()` to train the network with an array of training data. The network has to be trained with all the data in bulk in one call to `train()`. The more training patterns, the longer it will probably take to train, but the better the network will be at classifiying new patterns.    #### Data format  Each training pattern should have an `input` and an `output`, both of which can be either an array of numbers from `0` to `1` or a hash of numbers from `0` to `1`. For the [color constrast demo](http://harthur.github.com/brain/) it looks something like this:    ```javascript  var net = new brain.NeuralNetwork();    net.train([{input: { r: 0.03, g: 0.7, b: 0.5 }, output: { black: 1 }},             {input: { r: 0.16, g: 0.09, b: 0.2 }, output: { white: 1 }},             {input: { r: 0.5, g: 0.5, b: 1.0 }, output: { white: 1 }}]);    var output = net.run({ r: 1, g: 0.4, b: 0 });  // { white: 0.99, black: 0.002 }  ```    #### Options  `train()` takes a hash of options as its second argument:    ```javascript  net.train(data, {    errorThresh: 0.005,  // error threshold to reach    iterations: 20000,   // maximum training iterations    log: true,           // console.log() progress periodically    logPeriod: 10,       // number of iterations between logging    learningRate: 0.3    // learning rate  })  ```    The network will train until the training error has gone below the threshold (default `0.005`) or the max number of iterations (default `20000`) has been reached, whichever comes first.    By default training won't let you know how its doing until the end, but set `log` to `true` to get periodic updates on the current training error of the network. The training error should decrease every time. The updates will be printed to console. If you set `log` to a function, this function will be called with the updates instead of printing to the console.    The learning rate is a parameter that influences how quickly the network trains. It's a number from `0` to `1`. If the learning rate is close to `0` it will take longer to train. If the learning rate is closer to `1` it will train faster but it's in danger of training to a local minimum and performing badly on new data. The default learning rate is `0.3`.    #### Output  The output of `train()` is a hash of information about how the training went:    ```javascript  {    error: 0.0039139985510105032,  // training error    iterations: 406                // training iterations  }  ```    #### Failing  If the network failed to train, the error will be above the error threshold. This could happen because the training data is too noisy (most likely), the network doesn't have enough hidden layers or nodes to handle the complexity of the data, or it hasn't trained for enough iterations.    If the training error is still something huge like `0.4` after 20000 iterations, it's a good sign that the network can't make sense of the data you're giving it.    ## JSON  Serialize or load in the state of a trained network with JSON:    ```javascript  var json = net.toJSON();    net.fromJSON(json);  ```    You can also get a custom standalone function from a trained network that acts just like `run()`:    ```javascript  var run = net.toFunction();    var output = run({ r: 1, g: 0.4, b: 0 });    console.log(run.toString()); // copy and paste! no need to import brain.js  ```    ## Options  `NeuralNetwork()` takes a hash of options:    ```javascript  var net = new brain.NeuralNetwork({    hiddenLayers: [4],    learningRate: 0.6 // global learning rate, useful when training using streams  });  ```    #### hiddenLayers  Specify the number of hidden layers in the network and the size of each layer. For example, if you want two hidden layers - the first with 3 nodes and the second with 4 nodes, you'd give:    ```  hiddenLayers: [3, 4]  ```    By default `brain` uses one hidden layer with size proportionate to the size of the input array.    ## Streams  The network now has a [WriteStream](http://nodejs.org/api/stream.html#stream_class_stream_writable). You can train the network by using `pipe()` to send the training data to the network.    #### Example  Refer to `stream-example.js` for an example on how to train the network with a stream.    #### Initialization  To train the network using a stream you must first create the stream by calling `net.createTrainStream()` which takes the following options:    * `floodCallback()` - the callback function to re-populate the stream. This gets called on every training iteration.  * `doneTrainingCallback(info)` - the callback function to execute when the network is done training. The `info` param will contain a hash of information about how the training went:    ```javascript  {    error: 0.0039139985510105032,  // training error    iterations: 406                // training iterations  }  ```    #### Transform  Use a [Transform](http://nodejs.org/api/stream.html#stream_class_stream_transform) to coerce the data into the correct format. You might also use a Transform stream to normalize your data on the fly. """
Big data;https://github.com/VCNC/haeinsa;"""# Haeinsa    [![Build Status](https://travis-ci.org/VCNC/haeinsa.svg?branch=master)](https://travis-ci.org/VCNC/haeinsa)    Haeinsa is linearly scalable multi-row, multi-table transaction library for HBase.  Haeinsa uses two-phase locking and optimistic concurrency control for implementing transaction.  The isolation level of transaction is serializable.    ## Features    Please see Haeinsa [Wiki] for further information.    - **ACID**: Provides multi-row, multi-table transaction with full ACID semantics.  - **[Linearly scalable]**: Can linearly scale out throughput of transaction as scale out your HBase cluster.  - **[Serializability]**: Provide isolation level of serializability.  - **[Low overhead]**: Relatively low overhead compared to other comparable libraries.  - **Fault-tolerant**: Haeinsa is fault-tolerant against both client and HBase failures.  - **[Easy migration]**: Add transaction feature to your own HBase cluster without any change in HBase cluster except adding lock column family.  - **[Used in practice]**: Haeinsa is used in real service.    ## Usage    APIs of Haeinsa is really similar to APIs of HBase. Please see [How to Use] and [API Usage] document for further information.    	HaeinsaTransactionManager tm = new HaeinsaTransactionManager(tablePool);  	HaeinsaTableIface table = tablePool.getTable(""test"");  	byte[] family = Bytes.toBytes(""data"");  	byte[] qualifier = Bytes.toBytes(""status"");    	HaeinsaTransaction tx = tm.begin(); // start transaction    	HaeinsaPut put1 = new HaeinsaPut(Bytes.toBytes(""user1""));  	put1.add(family, qualifier, Bytes.toBytes(""Hello World!""));  	table.put(tx, put1);    	HaeinsaPut put2 = new HaeinsaPut(Bytes.toBytes(""user2""));  	put2.add(family, qualifier, Bytes.toBytes(""Linearly Scalable!""));  	table.put(tx, put2);    	tx.commit(); // commit transaction to HBase    ## Resources    - [Haeinsa Overview Presentation]: Introducing how Haeina works.  - [Announcing Haeinsa]: Blog post of VCNC Engineering Blog (Korean)  - [Haeinsa: Hbase Transaction Library]: Presentation for Deview Conference (Korean)    ## License    	Copyright (C) 2013-2015 VCNC Inc.  	  	Licensed under the Apache License, Version 2.0 (the ""License"");  	you may not use this file except in compliance with the License.  	You may obtain a copy of the License at  	  	        http://www.apache.org/licenses/LICENSE-2.0  	  	Unless required by applicable law or agreed to in writing, software  	distributed under the License is distributed on an ""AS IS"" BASIS,  	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  	See the License for the specific language governing permissions and  	limitations under the License.    [Wiki]: https://github.com/vcnc/haeinsa/wiki  [How to Use]: https://github.com/vcnc/haeinsa/wiki/How-to-Use  [API Usage]: https://github.com/vcnc/haeinsa/wiki/API-Usage  [HBase]: http://hbase.apache.org/  [Serializability]: http://en.wikipedia.org/wiki/Serializability  [Percolator]: http://research.google.com/pubs/pub36726.html  [Haeinsa]: http://en.wikipedia.org/wiki/Haeinsa  [Tripitaka Koreana, or Palman Daejanggyeong]: http://en.wikipedia.org/wiki/Tripitaka_Koreana  [Haeinsa Overview Presentation]: https://speakerdeck.com/vcnc/haeinsa-overview-hbase-transaction-library  [Announcing Haeinsa]: http://engineering.vcnc.co.kr/2013/10/announcing-haeinsa/  [Linearly scalable]: https://github.com/vcnc/haeinsa/wiki/Performance  [Low overhead]: https://github.com/vcnc/haeinsa/wiki/Performance  [Easy Migration]: https://github.com/vcnc/haeinsa/wiki/Migration-from-HBase  [Used in practice]: https://github.com/vcnc/haeinsa/wiki/Use-Case  [Haeinsa: Hbase Transaction Library]: https://speakerdeck.com/vcnc/haeinsa-hbase-transaction-library """
Big data;https://github.com/tidwall/summitdb;"""<p align=""center"">  <img       src=""resources/logo.png""       width=""350"" height=""85"" border=""0"" alt=""SummitDB"">  </p>    SummitDB is an in-memory, [NoSQL](https://en.wikipedia.org/wiki/NoSQL) key/value database. It persists to disk, uses the [Raft](https://raft.github.io/) consensus algorithm, is [ACID](https://en.wikipedia.org/wiki/ACID) compliant, and built on a transactional and strongly-consistent model. It supports [custom indexes](https://github.com/tidwall/summitdb/wiki/SETINDEX), [geospatial data](https://github.com/tidwall/summitdb/wiki/SETINDEX#spatial), [JSON documents](#json-documents), and [user-defined JS scripting](https://github.com/tidwall/summitdb/wiki/EVAL).    Under the hood it utilizes [Finn](https://github.com/tidwall/finn), [Redcon](https://github.com/tidwall/redcon), [BuntDB](https://github.com/tidwall/buntdb), [GJSON](https://github.com/tidwall/gjson), and [Otto](https://github.com/robertkrimen/otto).    Features  --------  - [In-memory with disk persistence](#in-memory-disk-persistence)  - [Strong-consistency and durability](#consistency-and-durability)  - [High-availability](#consistency-and-durability)  - [Ordered key space](#differences-between-summitdb-and-redis)  - [Hot backups](#hot-backups)  - [Simplified Redis-style APIs](#commands)  - [Indexing on values](https://github.com/tidwall/summitdb/wiki/SETINDEX)  - [JSON documents](#json-documents)  - [Spatial indexing](https://github.com/tidwall/summitdb/wiki/SETINDEX#spatial)  - [Fencing tokens](#fencing-tokens)    Getting started  ---------------    ### Getting SummitDB    The easiest way to get SummitDB is to use one of the pre-built release binaries which are available for OSX, Linux, and Windows.   Instructions for using these binaries are on the GitHub [releases page](https://github.com/tidwall/summitdb/releases).    If you want to try the latest version, you can build SummitDB from the master branch.    ### Building SummitDB    SummitDB can be compiled and used on Linux, OSX, Windows, FreeBSD, ARM (Raspberry PI) and probably others since the codebase is 100% Go. We support both 32 bit and 64 bit systems. Go must be installed on the build machine.    To build simply:    ```  $ make  ```    It's a good idea to install the [redis-cli](http://redis.io/topics/rediscli).    ```  $ make redis-cli  ```    To run tests:    ```  $ make test  ```    ## Docker    Check out the SummitDB images in [Docker Hub](https://hub.docker.com/search?q=summitdb&type=image).    ### Running    First start a single-member cluster:  ```  $ ./summitdb-server  ```    This will start the server listening on port 7481 for client and server-to-server communication.    Next, let's set a single key, and then retrieve it:    ```  $ ./redis-cli -p 7481 SET mykey ""my value""  OK  $ ./redis-cli -p 7481 GET mykey  ""my value""  ```    Adding members:  ```  $ ./summitdb-server -p 7482 -dir data2 -join localhost:7481  $ ./summitdb-server -p 7483 -dir data3 -join localhost:7481  ```    That's it. Now if node1 goes down, node2 and node3 will continue to operate.    ## Differences between SummitDB and Redis    It may be worth noting that while SummitDB supports many Redis features, it is not a strict Redis clone. Redis has a lot of commands and data types that are not available in SummitDB such as Sets, Hashes, Sorted Sets, and PubSub. SummitDB also has many features that are not available in Redis such as:    - **Ordered key space** - SummitDB provides one key space that is a large B-tree. An ordered key space allows for stable paging through keys using the [KEYS](https://github.com/tidwall/summitdb/wiki/KEYS) command. Redis uses an unordered dictionary structure and provides a specialized [SCAN](http://redis.io/commands/scan) command for iterating through keys.  - **Everything a string** - SummitDB stores only strings which are exact binary representations of what the user stores. Redis has many [internal data types](http://redis.io/topics/data-types-intro), such as strings, hashes, floats, sets, etc.   - **Raft clusters** - SummitDB uses the Raft consensus algorithm to provide high-availablity. Redis provides [Master/Slave replication](http://redis.io/topics/replication).   - **Javascript** - SummitDB uses Javascript for user-defined scripts. Redis uses Lua.  - **Indexes** - SummitDB provides an API for indexing the key space. Indexes allow for quickly querying and iterating on values. Redis has specialized data types like Sorted Sets and Hashes which can provide [secondary indexing](http://redis.io/topics/indexes).  - **Spatial indexes** - SummitDB provides the ability to create spatial indexes. A spatial index uses an R-tree under the hood, and each index can be up to 20 dimensions. This is useful for geospatial, statistical, time, and range data. Redis has the [GEO API](http://redis.io/commands/geoadd) which allows for using storing and querying geospatial data using the [Geohashes](https://en.wikipedia.org/wiki/Geohash).  - **JSON documents** - SummitDB allows for storing JSON documents and indexing fields directly. Redis has Hashes and a JSON parser via Lua.    <a name=""in-memory-disk-persistence""></a>  ## In-memory with disk persistence  SummitDB store all data in memory. Yet each writable command is appended to a file that is used to rebuild the database if the database needs to be restarted.     This is similar to [Redis AOF persistence](http://redis.io/topics/persistence).    ## JSON Documents    SummitDB provides the commands  [JSET](https://github.com/tidwall/summitdb/wiki/JSET),  [JGET](https://github.com/tidwall/summitdb/wiki/JGET),  [JDEL](https://github.com/tidwall/summitdb/wiki/JDEL)  for working with json documents.    `JSET` and `JDEL` uses the   [sjson path syntax](https://github.com/tidwall/sjson#path-syntax)   and `JGET` uses the   [gjson path syntax](https://github.com/tidwall/gjson#path-syntax).    Here are some examples:    ```  > JSET user:101 name Tom  OK  > JSET user:101 age 46  OK  > GET user:101  ""{\""age\"":46,\""name\"":\""Tom\""}""  > JGET user:101 age  ""46""  > JSET user:101 name.first Tom  OK  > JSET user:101 name.last Anderson  OK  > GET user:101  ""{\""age\"":46,\""name\"":{\""last\"":\""Anderson\"",\""first\"":\""Tom\""}}""  > JDEL user:101 name.last  (integer) 1  > GET user:101  ""{\""age\"":46,\""name\"":{\""first\"":\""Tom\""}}""  > JSET user:101 friends.0 Carol  OK  > JSET user:101 friends.1 Andy  OK  > JSET user:101 friends.3 Frank  OK  > GET user:101  ""{\""friends\"":[\""Carol\"",\""Andy\"",null,\""Frank\""],\""age\"":46,\""name\"":{\""first\"":\""Tom\""}}""  > JGET user:101 friends.1  ""Andy""  ```    ## JSON Indexes    Indexes can be created on individual fields inside JSON documents.    For example, let's say you have the following documents:    ```json  {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}  {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}  {""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}  {""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}  ```    Create an index:    ```  > SETINDEX last_name user:* JSON name.last  ```    Then add some JSON:  ```  > SET user:1 '{""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}'  > SET user:2 '{""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}'  > SET user:3 '{""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}'  > SET user:4 '{""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}'  ```    Query with the ITER command:    ```  > ITER last_name  1) ""user:3""  2) ""{\""name\"":{\""first\"":\""Carol\"",\""last\"":\""Anderson\""},\""age\"":52}""  3) ""user:4""  4) ""{\""name\"":{\""first\"":\""Alan\"",\""last\"":\""Cooper\""},\""age\"":28}""  5) ""user:1""  6) ""{\""name\"":{\""first\"":\""Tom\"",\""last\"":\""Johnson\""},\""age\"":38}""  7) ""user:2""  8) ""{\""name\"":{\""first\"":\""Janet\"",\""last\"":\""Prichard\""},\""age\"":47}""  ```    Or perhaps you want to index on age:    ```  > SETINDEX age user:* JSON age  > ITER age  1) ""user:4""  2) ""{\""name\"":{\""first\"":\""Alan\"",\""last\"":\""Cooper\""},\""age\"":28}""  3) ""user:1""  4) ""{\""name\"":{\""first\"":\""Tom\"",\""last\"":\""Johnson\""},\""age\"":38}""  5) ""user:2""  6) ""{\""name\"":{\""first\"":\""Janet\"",\""last\"":\""Prichard\""},\""age\"":47}""  7) ""user:3""  8) ""{\""name\"":{\""first\"":\""Carol\"",\""last\"":\""Anderson\""},\""age\"":52}""  ```    It's also possible to multi-index on two fields:    ```  > SETINDEX last_name_age user:* JSON name.last JSON age  ```    For full JSON indexing syntax check out the [SETINDEX](https://github.com/tidwall/summitdb/wiki/SETINDEX#json) and [ITER](https://github.com/tidwall/summitdb/wiki/ITER) commands.    Fencing Tokens  --------------  A fencing token is simply a number that increases.   It's guaranteed to be consistent across the cluster and can never be deleted or decreased.   The value is a 64-bit unsigned integer. The first FENCE call will return ""1"".  This can be useful in applications that need things like distributed locking and preventing race conditions. FENCEGET will read the token without incrementing it.    ```  > FENCE mytoken  ""1""  > FENCE mytoken  ""2""  > FENCE mytoken  ""3""  > FENCEGET mytoken  ""3""  > FENCE mytoken  ""4""  ```      <a href=""raft-commands""></a>  Built-in Raft Commands  ----------------------  Here are a few commands for monitoring and managing the cluster:    - **RAFTADDPEER addr**    Adds a new member to the Raft cluster  - **RAFTREMOVEPEER addr**    Removes an existing member  - **RAFTPEERS**    Lists known peers and their status  - **RAFTLEADER**    Returns the Raft leader, if known  - **RAFTSNAPSHOT**    Triggers a snapshot operation  - **RAFTSTATE**    Returns the state of the node  - **RAFTSTATS**    Returns information and statistics for the node and cluster    Consistency and Durability  --------------------------    SummitDB is tuned by design for strong consistency and durability. A server shutdown, power event, or `kill -9` will not corrupt the state of the cluster or lose data.     All data persists to disk. SummitDB uses an append-only file format that stores for each command in exact order of execution.   Each command consists of a one write and one fsync. This provides excellent durability.    ### Read Consistency    The `--consistency` param has the following options:    - `low` - all nodes accept reads, small risk of [stale](http://stackoverflow.com/questions/1563319/what-is-stale-state) data  - `medium` - only the leader accepts reads, itty-bitty risk of stale data during a leadership change  - `high` - only the leader accepts reads, the raft log index is incremented to guarantee no stale data. **this is the default**    For example, setting the following options:    ```  $ summitdb --consistency high  ```    Provides the highest level of consistency. The default is **high**.      Leadership Changes  ------------------    In a Raft cluster only the leader can apply commands. If a command is attempted on a follower you will be presented with the response:    ```  > SET x y  -TRY 127.0.0.1:7481  ```    This means you should try the same command at the specified address.      Hot Backups  -----------    SummitDB supports hot-backing up a node.   You can retrieve and restore a snapshot of the database to a file using the [BACKUP](https://github.com/tidwall/summitdb/wiki/BACKUP) command.    ```  > BACKUP  BULK REPLY OF DATA  ```    Or using an HTTP connection like such:    ```  curl localhost:7481/backup -o backup.db  ```    The backup file format is a series of commands which are stored as [RESP Arrays](http://redis.io/topics/protocol#resp-arrays).  The command:  ```  SET mykey 123  ```  Is stored on disk as:  ```go  ""*3\r\n$3\r\nSET\r\n$5\r\nmykey\r\n$3\r\n123\r\n""  ```      To restore a system from a backup, issue each command to the leader. For example, using the `nc` command you could execute:  ```sh  $ cat backup.db | nc localhost 7481  ```    Commands  --------    Below is the complete list of commands.    **Keys and values**    [APPEND](https://github.com/tidwall/summitdb/wiki/APPEND),   [BITCOUNT](https://github.com/tidwall/summitdb/wiki/BITCOUNT),   [BITOP](https://github.com/tidwall/summitdb/wiki/BITOP),   [BITPOS](https://github.com/tidwall/summitdb/wiki/BITPOS),   [DBSIZE](https://github.com/tidwall/summitdb/wiki/DBSIZE),  [DECR](https://github.com/tidwall/summitdb/wiki/DECR),   [DECRBY](https://github.com/tidwall/summitdb/wiki/DECRBY),   [DEL](https://github.com/tidwall/summitdb/wiki/DEL),  [EXISTS](https://github.com/tidwall/summitdb/wiki/EXISTS),  [EXPIRE](https://github.com/tidwall/summitdb/wiki/EXPIRE),  [EXPIREAT](https://github.com/tidwall/summitdb/wiki/EXPIREAT),  [FENCE](https://github.com/tidwall/summitdb/wiki/FENCE),  [FENCEGET](https://github.com/tidwall/summitdb/wiki/FENCEGET),  [FLUSHDB](https://github.com/tidwall/summitdb/wiki/FLUSHDB),  [GET](https://github.com/tidwall/summitdb/wiki/GET),   [GETBIT](https://github.com/tidwall/summitdb/wiki/GETBIT),   [GETRANGE](https://github.com/tidwall/summitdb/wiki/GETRANGE),   [GETSET](https://github.com/tidwall/summitdb/wiki/GETSET),   [INCR](https://github.com/tidwall/summitdb/wiki/INCR),   [INCRBY](https://github.com/tidwall/summitdb/wiki/INCRBY),   [INCRBYFLOAT](https://github.com/tidwall/summitdb/wiki/INCRBYFLOAT),   [KEYS](https://github.com/tidwall/summitdb/wiki/KEYS),  [MGET](https://github.com/tidwall/summitdb/wiki/MGET),   [MSET](https://github.com/tidwall/summitdb/wiki/MSET),   [MSETNX](https://github.com/tidwall/summitdb/wiki/MSETNX),   [PDEL](https://github.com/tidwall/summitdb/wiki/PDEL),  [PERSIST](https://github.com/tidwall/summitdb/wiki/PERSIST),  [PEXPIRE](https://github.com/tidwall/summitdb/wiki/PEXPIRE),  [PEXPIREAT](https://github.com/tidwall/summitdb/wiki/PEXPIREAT),  [PTTL](https://github.com/tidwall/summitdb/wiki/PTTL),  [RENAME](https://github.com/tidwall/summitdb/wiki/RENAME),  [RENAMENX](https://github.com/tidwall/summitdb/wiki/RENAMENX),  [SET](https://github.com/tidwall/summitdb/wiki/SET),   [SETBIT](https://github.com/tidwall/summitdb/wiki/SETBIT),   [SETRANGE](https://github.com/tidwall/summitdb/wiki/SETRANGE),   [STRLEN](https://github.com/tidwall/summitdb/wiki/STRLEN),  [TTL](https://github.com/tidwall/summitdb/wiki/TTL)    **JSON**  [JSET](https://github.com/tidwall/summitdb/wiki/JSET),  [JGET](https://github.com/tidwall/summitdb/wiki/JGET),  [JDEL](https://github.com/tidwall/summitdb/wiki/JDEL)    **Indexes and iteration**    [DELINDEX](https://github.com/tidwall/summitdb/wiki/DELINDEX),  [INDEXES](https://github.com/tidwall/summitdb/wiki/INDEXES),  [ITER](https://github.com/tidwall/summitdb/wiki/ITER),  [RECT](https://github.com/tidwall/summitdb/wiki/RECT),  [SETINDEX](https://github.com/tidwall/summitdb/wiki/SETINDEX)    **Transactions**    [MULTI](https://github.com/tidwall/summitdb/wiki/MULTI),  [EXEC](https://github.com/tidwall/summitdb/wiki/EXEC),  [DISCARD](https://github.com/tidwall/summitdb/wiki/DISCARD)    **Scripts**    [EVAL](https://github.com/tidwall/summitdb/wiki/EVAL),  [EVALRO](https://github.com/tidwall/summitdb/wiki/EVALRO),  [EVALSHA](https://github.com/tidwall/summitdb/wiki/EVALSHA),  [EVALSHARO](https://github.com/tidwall/summitdb/wiki/EVALSHARO),  [SCRIPT LOAD](https://github.com/tidwall/summitdb/wiki/SCRIPT-LOAD),  [SCRIPT FLUSH](https://github.com/tidwall/summitdb/wiki/SCRIPT-FLUSH)    **Raft management**    [RAFTADDPEER](https://github.com/tidwall/summitdb/wiki/RAFTADDPEER),  [RAFTREMOVEPEER](https://github.com/tidwall/summitdb/wiki/RAFTREMOVEPEER),  [RAFTLEADER](https://github.com/tidwall/summitdb/wiki/RAFTLEADER),  [RAFTSNAPSHOT](https://github.com/tidwall/summitdb/wiki/RAFTSNAPSHOT),  [RAFTSTATE](https://github.com/tidwall/summitdb/wiki/RAFTSTATE),  [RAFTSTATS](https://github.com/tidwall/summitdb/wiki/RAFTSTATS)    **Server**    [BACKUP](https://github.com/tidwall/summitdb/wiki/BACKUP)    ## Contact  Josh Baker [@tidwall](http://twitter.com/tidwall)    ## License    SummitDB source code is available under the MIT [License](/LICENSE).       """
Big data;https://github.com/kairosdb/kairosdb;"""![KairosDB](webroot/img/kairosdb.png)  [![Build Status](https://travis-ci.org/kairosdb/kairosdb.svg?branch=develop)](https://travis-ci.org/kairosdb/kairosdb)    KairosDB is a fast distributed scalable time series database written on top of Cassandra.    ## Documentation    Documentation is found [here](http://kairosdb.github.io/website/).    [Frequently Asked Questions](https://github.com/kairosdb/kairosdb/wiki/Frequently-Asked-Questions)    ## Installing    Download the latest [KairosDB release](https://github.com/kairosdb/kairosdb/releases).    Installation instructions are found [here](http://kairosdb.github.io/docs/build/html/GettingStarted.html)    If you want to test KairosDB in Kubernetes please follow the instructions from [KairosDB Helm chart](deployment/helm/README.md).    ## Getting Involved    Join the [KairosDB discussion group](https://groups.google.com/forum/#!forum/kairosdb-group).    ## Contributing to KairosDB    Contributions to KairosDB are **very welcome**. KairosDB is mainly developed in Java, but there's a lot of tasks for non-Java programmers too, so don't feel shy and join us!    What you can do for KairosDB:    - [KairosDB Core](https://github.com/kairosdb/kairosdb): join the development of core features of KairosDB.  - [Website](https://github.com/kairosdb/kairosdb.github.io): improve the KairosDB website.  - [Documentation](https://github.com/kairosdb/kairosdb/wiki/Contribute:-Documentation): improve our documentation, it's a very important task.    If you have any questions about how to contribute to KairosDB, [join our discussion group](https://groups.google.com/forum/#!forum/kairosdb-group) and tell us your issue.    ## License  The license is the [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0) """
Big data;https://github.com/improbable-eng/thanos;"""<p align=""center""><img src=""docs/img/Thanos-logo_fullmedium.png"" alt=""Thanos Logo""></p>    [![Latest Release](https://img.shields.io/github/release/thanos-io/thanos.svg?style=flat-square)](https://github.com/thanos-io/thanos/releases/latest) [![Go Report Card](https://goreportcard.com/badge/github.com/thanos-io/thanos)](https://goreportcard.com/report/github.com/thanos-io/thanos) [![Go Code reference](https://img.shields.io/badge/code%20reference-go.dev-darkblue.svg)](https://pkg.go.dev/github.com/thanos-io/thanos?tab=subdirectories) [![Slack](https://img.shields.io/badge/join%20slack-%23thanos-brightgreen.svg)](https://slack.cncf.io/) [![Netlify Status](https://api.netlify.com/api/v1/badges/664a5091-934c-4b0e-a7b6-bc12f822a590/deploy-status)](https://app.netlify.com/sites/thanos-io/deploys) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3048/badge)](https://bestpractices.coreinfrastructure.org/projects/3048)    [![CI](https://github.com/thanos-io/thanos/workflows/CI/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3ACI) [![CI](https://circleci.com/gh/thanos-io/thanos.svg?style=svg)](https://circleci.com/gh/thanos-io/thanos) [![go](https://github.com/thanos-io/thanos/workflows/go/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3Ago) [![react](https://github.com/thanos-io/thanos/workflows/react/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3Areact) [![docs](https://github.com/thanos-io/thanos/workflows/docs/badge.svg)](https://github.com/thanos-io/thanos/actions?query=workflow%3Adocs) [![Gitpod ready-to-code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/thanos-io/thanos)    ## Overview    Thanos is a set of components that can be composed into a highly available metric system with unlimited storage capacity, which can be added seamlessly on top of existing Prometheus deployments.    Thanos is a [CNCF](https://www.cncf.io/) Incubating project.    Thanos leverages the Prometheus 2.0 storage format to cost-efficiently store historical metric data in any object storage while retaining fast query latencies. Additionally, it provides a global query view across all Prometheus installations and can merge data from Prometheus HA pairs on the fly.    Concretely the aims of the project are:    1. Global query view of metrics.  2. Unlimited retention of metrics.  3. High availability of components, including Prometheus.    ## Getting Started    * **[Getting Started](https://thanos.io/tip/thanos/getting-started.md/)**  * [Design](https://thanos.io/tip/thanos/design.md/)  * [Blog posts](docs/getting-started.md#blog-posts)  * [Talks](docs/getting-started.md#talks)  * [Proposals](docs/proposals-done)  * [Integrations](docs/integrations.md)    ## Features    * Global querying view across all connected Prometheus servers  * Deduplication and merging of metrics collected from Prometheus HA pairs  * Seamless integration with existing Prometheus setups  * Any object storage as its only, optional dependency  * Downsampling historical data for massive query speedup  * Cross-cluster federation  * Fault-tolerant query routing  * Simple gRPC ""Store API"" for unified data access across all metric data  * Easy integration points for custom metric providers    ## Architecture Overview    Deployment with Sidecar:    ![Sidecar](https://docs.google.com/drawings/d/e/2PACX-1vTBFKKgf8YDInJyRakPE8eZZg9phTlOsBB2ogNkFvhNGbZ8YDvz_cGMbxWZBG1G6hpsQfSX145FpYcv/pub?w=960&h=720)    Deployment with Receive:    ![Receive](https://docs.google.com/drawings/d/e/2PACX-1vTfko27YB_3ab7ZL8ODNG5uCcrpqKxhmqaz3lW-yhGN3_oNxkTrqXmwwlcZjaWf3cGgAJIM4CMwwkEV/pub?w=960&h=720)    ## Thanos Philosophy    The philosophy of Thanos and our community is borrowing much from UNIX philosophy and the golang programming language.    * Each subcommand should do one thing and do it well    * eg. thanos query proxies incoming calls to known store API endpoints merging the result  * Write components that work together    * e.g. blocks should be stored in native prometheus format  * Make it easy to read, write, and, run components    * e.g. reduce complexity in system design and implementation    ## Releases    Main branch should be stable and usable. Every commit to main builds docker image named `main-<date>-<sha>` in [quay.io/thanos/thanos](https://quay.io/repository/thanos/thanos) and [thanosio/thanos dockerhub (mirror)](https://hub.docker.com/r/thanosio/thanos)    We also perform minor releases every 6 weeks.    During that, we build tarballs for major platforms and release docker images.    See [release process docs](docs/release-process.md) for details.    ## Contributing    Contributions are very welcome! See our [CONTRIBUTING.md](CONTRIBUTING.md) for more information.    ## Community    Thanos is an open source project and we value and welcome new contributors and members of the community. Here are ways to get in touch with the community:    * Slack: [#thanos](https://slack.cncf.io/)  * Issue Tracker: [GitHub Issues](https://github.com/thanos-io/thanos/issues)    ## Adopters    See [`Adopters List`](website/data/adopters.yml).    ## Maintainers    See [MAINTAINERS.md](MAINTAINERS.md) """
Big data;https://github.com/snowplow/snowplow;"""# Snowplow    [![Release][release-badge]][release]  [![License][license-image]][license]  [![Discourse posts][discourse-image]][discourse]    [![Snowplow logo][logo-image]][website]    ## Overview    Snowplow is an enterprise-strength marketing and product analytics platform. It does three things:    1. Identifies your users, and tracks the way they engage with your website or application  2. Stores your users' behavioral data in a scalable ""event data warehouse"" you control: Amazon Redshift, Google BigQuery, Snowflake or Elasticsearch  3. Lets you leverage the biggest range of tools to analyze that data, including big data tools (e.g. Spark) via EMR or more traditional tools e.g. Looker, Mode, Superset, Re:dash to analyze that behavioral data    **To find out more, please check out the [Snowplow website][website] and the [docs website][docs].**    ### Version Compatibility Matrix    For compatibility assurance, the version compatibility matrix offers clarity on our recommended stack. It is strongly recommended when setting up a Snowplow pipeline to use the versions listed in the version compatibility matrix which can be found [within our docs][version-compatibility].    ### Public Roadmap    This repository also contains the [Snowplow Public Roadmap][roadmap]. The Public Roadmap lets you stay up to date and find out what's happening on the Snowplow Platform. Help us prioritize our cards: open the issue and leave a 👍 to vote for your favorites. Want us to build a feature or function? Tell us by heading to our [Discourse forum][discourse] 💬.    ### Try Snowplow    Setting up a full open-source Snowplow pipeline requires a non-trivial amount of engineering expertise and time investment.  You might be interested in finding out what Snowplow can do first, by setting up [Try Snowplow][try-snowplow].    ### Open Source Quick Start    The [Open Source Quick Start][open-source-quick-start] will help you get up and running with a Snowplow open source pipeline. Snowplow publishes a [set of terraform modules][terraform-modules], which automate the setting up & deployment of the required infrastructure & applications for an operational Snowplow open source pipeline, with just a handful of input variables required on your side.    ### Join the Snowplow Research Panel and help shape the future of open source    As part of our ongoing efforts to improve the Snowplow Open Source experience, we're looking for users of our open-source software and  members of our community to take part in research studies. [Join here][research-survey].    ### Our Commercial Offering    If you wish to get everything setup and managed for you, you can consider [Snowplow BDP][snowplow-bdp]. You can also [request a demo][request-a-demo].    ## Snowplow technology 101    [![Snowplow architecture][architecture-image]][architecture]    The repository structure follows the conceptual architecture of Snowplow, which consists of six loosely-coupled sub-systems connected by five standardized data protocols/formats.    To briefly explain these six sub-systems:    * **[Trackers][trackers]** fire Snowplow events. Currently we have 15 trackers, covering web, mobile, desktop, server and IoT  * **[Collector][collector]** receives Snowplow events from trackers. Currently we have one official collector implementation with different sinks: Amazon Kinesis, Google PubSub, Amazon SQS, Apache Kafka and NSQ  * **[Enrich][enrich]** cleans up the raw Snowplow events, enriches them and puts them into storage. Currently we have several implementations, built for different environments (GCP, AWS, Apache Kafka) and one core library  * **[Storage][storage]** is where the Snowplow events live. Currently we store the Snowplow events in a flat file structure on S3, and in the Redshift, Postgres, Snowflake and BigQuery databases  * **[Data modeling][data-modeling]** is where event-level data is joined with other data sets and aggregated into smaller data sets, and business logic is applied. This produces a clean set of tables which make it easier to perform analysis on the data. We officially support data models for Redshift, Snowflake and BigQuery.  * **[Analytics][analytics-sdks]** are performed on the Snowplow events or on the aggregate tables.    **For more information on the current Snowplow architecture, please see the [Technical architecture][architecture]**.    ## About this repository    This repository is an umbrella repository for all loosely-coupled Snowplow components and is updated on each component release.    Since June 2020, all components have been extracted into their dedicated repositories (more info [here][split-blogpost])  and this repository serves as an entry point for Snowplow users, the home of our public roadmap and as a historical artifact.    Components that have been extracted to their own repository are still here as [git submodules][submodules].    ### Trackers    |                Web               |           Mobile           |         Gaming         |          TV          |       Desktop & Server        |  |:--------------------------------:|:--------------------------:|:----------------------:|:--------------------:|:-----------------------------:|  | [JavaScript][javascript-tracker] | [Android][android-tracker] | [Unity][unity-tracker] | [Roku][roku-tracker] | [Command line][tracking-cli]  |  | [AMP][amp-tracker]               | [iOS][ios-tracker]         |                        |                      | [.NET][dotnet-tracker]        |  |                                  | [React Native][rn-tracker] |                        |                      | [Go][golang-tracker]          |  |                                  | [Flutter][flutter-tracker] |                        |                      | [Java][java-tracker]          |  |                                  |                            |                        |                      | [Node.js][javascript-tracker] |  |                                  |                            |                        |                      | [PHP][php-tracker]            |  |                                  |                            |                        |                      | [Python][python-tracker]      |  |                                  |                            |                        |                      | [Ruby][ruby-tracker]          |  |                                  |                            |                        |                      | [Scala][scala-tracker]        |    ### [Collector](https://github.com/snowplow/stream-collector)    ### [Enrich](https://github.com/snowplow/enrich)    ### Loaders    * [BigQuery (streaming)](https://github.com/snowplow-incubator/snowplow-bigquery-loader)  * [Redshift (batch)](https://github.com/snowplow/snowplow-rdb-loader)  * [Snowflake (batch)](https://github.com/snowplow-incubator/snowplow-snowflake-loader)  * [Google Cloud Storage (streaming)](https://github.com/snowplow-incubator/snowplow-google-cloud-storage-loader)  * [Amazon S3 (streaming)](https://github.com/snowplow/snowplow-s3-loader)  * [Postgres (streaming)](https://github.com/snowplow-incubator/snowplow-postgres-loader)  * [Elasticsearch (streaming)](https://github.com/snowplow/snowplow-elasticsearch-loader)    ### Iglu    * [Iglu Server](https://github.com/snowplow-incubator/iglu-server/)  * [igluctl](https://github.com/snowplow-incubator/igluctl/)  * [Iglu Central](https://github.com/snowplow/iglu-central/)    ### Data modeling    #### Web    * [Web model: SQL-Runner version](https://github.com/snowplow/data-models/tree/master/web/v1)  * [Web model: dbt version](https://github.com/snowplow/dbt-snowplow-web)    #### Mobile    * [Mobile model: SQL-Runner version](https://github.com/snowplow/data-models/tree/master/mobile/v1)    ### Testing    * [Mini](https://github.com/snowplow/snowplow-mini)  * [Micro](https://github.com/snowplow-incubator/snowplow-micro)    ### Parsing enriched event    * [Analytics SDK Scala](https://github.com/snowplow/snowplow-scala-analytics-sdk)  * [Analytics SDK Python](https://github.com/snowplow/snowplow-python-analytics-sdk)  * [Analytics SDK .NET](https://github.com/snowplow/snowplow-dotnet-analytics-sdk)  * [Analytics SDK Javascript](https://github.com/snowplow-incubator/snowplow-js-analytics-sdk/)  * [Analytics SDK Golang](https://github.com/snowplow/snowplow-golang-analytics-sdk)    ### [Bad rows](https://github.com/snowplow-incubator/snowplow-badrows)    ### [Terraform Modules][terraform-modules]    ## Need help?    We want to make it super-easy for Snowplow users and contributors to talk to us and connect with each other, to share ideas, solve problems and help make Snowplow awesome. Here are the main channels we're running currently, we'd love to hear from you on one of them:    ### [Discourse][discourse]    This is for all Snowplow users: engineers setting up Snowplow, data modelers structuring the data and data consumers building insights. You can find guides, recipes, questions and answers from Snowplow users including the Snowplow team.    We welcome all questions and contributions!    ### Twitter    [@SnowplowData][snowplow-twitter] for official news or [@SnowplowLabs][snowplow-labs-twitter] for engineering-heavy conversations and release updates.    ### GitHub    If you spot a bug, then please raise an issue in the GitHub repository of the component in question.  Likewise if you have developed a cool new feature or an improvement, please open a pull request,  we'll be glad to integrate it in the codebase!    If you want to brainstorm a potential new feature, then [Discourse][discourse] is the best place to start.    ### Email    [community@snowplowanalytics.com][community-email]    If you want to talk directly to us (e.g. about a commercially sensitive issue), email is the easiest way.    ## Copyright and license    Snowplow is copyright 2012-2022 Snowplow Analytics Ltd.    Licensed under the **[Apache License, Version 2.0][license]** (the ""License"");  you may not use this software except in compliance with the License.    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.    [license-image]: https://img.shields.io/badge/license-Apache--2-blue.svg?style=flat  [license]: https://www.apache.org/licenses/LICENSE-2.0    [logo-image]: media/snowplow_logo.png  [website]: https://snowplowanalytics.com  [docs]: https://docs.snowplowanalytics.com/open-source-docs/    [snowplow-bdp]: https://snowplowanalytics.com/products/snowplow-bdp/  [version-compatibility]: https://docs.snowplowanalytics.com/docs/pipeline-components-and-applications/version-compatibility-matrix/  [try-snowplow]: https://try.snowplowanalytics.com/?utm_source=github&utm_medium=post&utm_campaign=try-snowplow  [request-a-demo]: https://go.snowplowanalytics.com/l/571483/2021-05-04/3sv1pg8  [roadmap]: https://github.com/snowplow/snowplow/projects  [open-source-quick-start]: https://docs.snowplowanalytics.com/docs/open-source-quick-start/  [terraform-modules]: https://registry.terraform.io/modules/snowplow-devops  [research-survey]: https://forms.gle/pCtYx8naum7A8vvw5    [architecture-image]: media/snowplow_architecture.png  [architecture]: ./ARCHITECTURE.md    [trackers]: https://github.com/snowplow/snowplow/tree/master/1-trackers  [collector]: https://github.com/snowplow/snowplow/tree/master/2-collectors  [enrich]: https://github.com/snowplow/snowplow/tree/master/3-enrich  [storage]: https://github.com/snowplow/snowplow/tree/master/4-storage  [data-modeling]: https://github.com/snowplow/snowplow/tree/master/5-data-modeling  [analytics-sdks]: https://docs.snowplowanalytics.com/docs/modeling-your-data/analytics-sdk/    [split-blogpost]: https://snowplowanalytics.com/blog/2020/07/16/changing-releasing/  [submodules]: https://git-scm.com/book/en/v2/Git-Tools-Submodules    [discourse-image]: https://img.shields.io/discourse/posts?server=https%3A%2F%2Fdiscourse.snowplowanalytics.com%2F  [discourse]: http://discourse.snowplowanalytics.com/  [snowplow-twitter]: https://twitter.com/SnowplowData  [snowplow-labs-twitter]: https://twitter.com/SnowplowLabs  [community-email]: mailto:community@snowplowanalytics.com    [release]: https://github.com/snowplow/snowplow/releases/tag/22.01  [release-badge]: https://img.shields.io/badge/Snowplow-22.01%20Western%20Ghats-6638b8    [javascript-tracker]: https://github.com/snowplow/snowplow-javascript-tracker  [amp-tracker]: https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/google-amp-tracker/  [android-tracker]: https://github.com/snowplow/snowplow-android-tracker  [ios-tracker]: https://github.com/snowplow/snowplow-objc-tracker  [rn-tracker]: https://github.com/snowplow-incubator/snowplow-react-native-tracker  [roku-tracker]: https://github.com/snowplow-incubator/snowplow-roku-tracker  [flutter-tracker]: https://github.com/snowplow-incubator/snowplow-flutter-tracker  [tracking-cli]: https://github.com/snowplow/snowplow-tracking-cli  [dotnet-tracker]: https://github.com/snowplow/snowplow-dotnet-tracker  [golang-tracker]: https://github.com/snowplow/snowplow-golang-tracker  [java-tracker]: https://github.com/snowplow/snowplow-java-tracker  [php-tracker]: https://github.com/snowplow/snowplow-php-tracker  [python-tracker]: https://github.com/snowplow/snowplow-python-tracker  [ruby-tracker]: https://github.com/snowplow/snowplow-ruby-tracker  [scala-tracker]: https://github.com/snowplow/snowplow-scala-tracker  [unity-tracker]: https://github.com/snowplow/snowplow-unity-tracker """
Big data;https://github.com/Netflix/atlas;"""# Atlas    Backend for managing dimensional time series data.    ## Links    * [Documentation](https://github.com/Netflix/atlas/wiki)  * [Mailing List](https://groups.google.com/forum/#!forum/netflix-atlas)  * [Issues](https://github.com/Netflix/atlas/issues)  * [Releases](https://github.com/Netflix/atlas/releases)    ## License    Copyright 2014-2021 Netflix, Inc.    Licensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. """
Big data;https://github.com/baidu/bfs;"""[The Baidu File System](http://github.com/baidu/bfs)  =======     [![Build Status](https://travis-ci.org/baidu/bfs.svg?branch=master)](https://travis-ci.org/baidu/bfs)  [![Build Status](https://scan.coverity.com/projects/8135/badge.svg)](https://scan.coverity.com/projects/myawan-bfs-1/)     The Baidu File System (BFS) is a distributed file system designed to support real-time applications. Like many other distributed file systems, BFS is highly fault-tolerant. But different from others, BFS provides low read/write latency while maintaining high throughput rates. Together with [Galaxy](https://github.com/baidu/galaxy) and [Tera](http://github.com/baidu/tera), BFS supports many real-time products in Baidu, including Baidu webpage database, Baidu incremental indexing system, Baidu user behavior analysis system, etc.    ## Features  1. Continuous availability   	* Nameserver is implemented as a `raft group`, no single point failure.  2. High throughput  	* High performance data engine to maximize IO utils.  3. Low latency  	* Global load balance and slow node detection.  4. Linear scalability  	* Support multi data center deployment and up to 10,000 data nodes.    ## Architecture  ![架构图](resources/images/bfs-arch2-mini.png)    ## Quick Start  #### Build        ./build.sh  #### Standalone BFS      cd sandbox      ./deploy.sh      ./start_bfs.sh    ## How to Contribute  1. Please read the [RoadMap](docs/en/roadmap.md) or source code.    2. Find something you are interested in and start working on it.  3. Test your code by simply running `make test` and `make check`.  4. Make a pull request.  5. Once your code has passed the code-review and merged, it will be run on thousands of servers :)      ## Contact us  opensearch@baidu.com    ====    [百度文件系统](http://github.com/baidu/bfs)  ====    百度的核心业务和数据库系统都依赖分布式文件系统作为底层存储，文件系统的可用性和性能对上层搜索业务的稳定性与效果有着至关重要的影响。现有的分布式文件系统（如HDFS等）是为离线批处理设计的，无法在保证高吞吐的情况下做到低延迟和持续可用，所以我们从搜索的业务特点出发，设计了百度文件系统。    ## 核心特点  1. 持续可用    	* 数据多机房、多地域冗余，元数据通过Raft维护一致性，单个机房宕机，不影响整体可用性。    2. 高吞吐    	* 通过高性能的单机引擎，最大化存储介质IO吞吐；    3. 低延时    	* 全局负载均衡、慢节点自动规避    4. 水平扩展    	* 设计支持两地三机房，1万+台机器管理。      ## 架构  ![架构图](resources/images/bfs-arch2-mini.png)    ## 快速试用  #### 构建      ./build.sh  #### 单机版BFS      cd sandbox      ./deploy.sh      ./start_bfs.sh    ## 如何参与开发  1. 阅读[RoadMap](docs/cn/roadmap.md)文件或者源代码，了解我们当前的开发方向  2. 找到自己感兴趣开发的的功能或模块  3. 进行开发，开发完成后自测功能是否正确，并运行make test及make check检查是否可以通过已有的测试case  4. 发起pull request  5. 在code-review通过后，你的代码便有机会运行在百度的数万台服务器上~      ## 联系我们  邮件：opensearch@baidu.com    QQ群：188471131   """
Big data;https://github.com/onurakpolat/awesome-analytics;"""# Awesome Analytics [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)    A curated list of awesome analytics platforms, resources and other awesomeness. Inspired by [awesome-bigdata](https://github.com/onurakpolat/awesome-bigdata).     Your feedback and contributions are always welcome! Maintained by [@onurakpolat](https://github.com/onurakpolat) & [@koconder](https://github.com/koconder)    - [Awesome Analytics](#awesome-analytics)      - [General analytics](#general-analytics)      - [Real-time](#real-time)      - [Website analytics](#website-analytics)      - [Endpoints](#endpoints)      - [SEO](#seo)      - [Privacy focused analytics](#privacy-focused-analytics)      - [Heatmap analytics](#heatmap-analytics)      - [Analytics layers](#analytics-layers)      - [Mobile analytics](#mobile-analytics)      - [App store analytics](#app-store-analytics)      - [Attribution tracking](#attribution-tracking)      - [Social media analytics](#social-media-analytics)      - [Analytics dashboards](#analytics-dashboards)      - [Developer analytics](#developer-analytics)        - [Other Awesome Lists](#other-awesome-lists)    ## General analytics  * [userTrack](https://www.usertrack.net/) - Self-hosted web analytics with heatmaps, session-recordings, A/B tests and more. `©` `Self-Hosted` `PHP`  * [Panelbear](https://panelbear.com/) - free real-time website analytics. Supports custom event tracking, email digests, and site speed metrics. `©` `SaaS`  * [PostHog](https://posthog.com) - Open-source product analytics to track users, events, funnels and trends. Alternative to Mixpanel/Amplitude/Heap. ([Source Code](https://github.com/posthog/posthog)) `MIT` `Python`  * [Hotjar](https://www.hotjar.com/) - new and easy way to truly understand your web and mobile site visitors. `©` `SaaS`  * [Matomo](https://matomo.org/) - Leading open-source analytics platform that gives you more than just powerful analytics, formerly known as Piwik. ([Source Code](https://github.com/matomo-org/)) `GPL-3.0` `PHP`  * [Heap](https://heap.io) - tracks your app users, clicks, form submissions, and anything else. `©` `SaaS`  * [Opentracker](http://www.opentracker.net/) - real time reporting, geo-location user tracking. `©` `SaaS`  * [FoxMetrics](http://foxmetrics.com/) - analytics to track your user’s actions and activities. `©` `SaaS`  * [Adobe Analytics](https://www.adobe.com/analytics/web-analytics.html) - web data into insights that everyone can act on. `©` `SaaS`  * [Google Analytics](https://www.google.com/analytics/) - de facto standard for analytics in the web analytics space. `©` `SaaS`  * [Screpy](https://screpy.com) - Screpy is a web analyzer and monitoring tool. Its powered by Google Lighthouse. `©` `SaaS`  * [Clicktale](https://www.clicktale.com) - record and watch exactly how a visitor used your website. `©` `SaaS`  * [GoSquared](https://www.gosquared.com/) - analytics with visitor tagging to help you dig deeper into one user’s visit. `©` `SaaS`  * [Clicky](http://clicky.com/) - track visits and conversions, you can also track your video and audio analytics. `©` `SaaS`  * [Woopra](https://www.woopra.com/) - track where your users are coming from. `©` `SaaS`  * [Mint](https://haveamint.com/) - self-hosted analytics solution (no longer on sale).  `©` `SaaS`  * [Going Up](https://www.goingup.com/) - manage SEO analytics and web app analytics with one tool. `©` `SaaS`  * [Chartbeat](https://chartbeat.com/) - beautiful, real-time app analytics tool for web apps. `©` `SaaS`  * [Gauges](http://get.gaug.es/) - real-time web analytics tool. `©` `SaaS`  * [Indicative](https://www.indicative.com/) - Web & mobile analytics tool, with heavy emphasis on segmentation and funnel visualization. `©` `SaaS`  * [Open Web Analytics](http://www.openwebanalytics.com/) - Google Analytics and Matomo alternative. ([Source Code](https://github.com/padams/Open-Web-Analytics)) `GPL-2.0` `PHP`  * [Statcounter](https://statcounter.com/) - one of the ORIGINAL web analytics tools available. `©` `SaaS`  * [Adobe Digital Analytics](http://www.adobe.com/data-analytics-cloud/analytics/capabilities.html) - standard analytics tools plus some that large organizations can use. `©` `SaaS`  * [Hitslink.com](https://www.hitslink.com/) - real-time analytics, social media traffic reporting, and real-time dynamic segmentation. `©` `SaaS`  * [parse.ly](https://www.parse.ly) - real-time web analytics tool with a focus on tracking content. `©` `SaaS`  * [Loggr](http://loggr.net/) -  track your user events and monitor your web app. `©` `SaaS`  * [Kissmetrics](https://www.kissmetrics.com/) - real-time standard cohort analysis tool. `©` `SaaS`  * [Sitemeter](http://sitemeter.com/) - old analytics tool. `©` `SaaS`  * [Crawl Track](http://www.crawltrack.net/) - another old analytics tool. `©` `SaaS`  * [Sitespect](https://www.sitespect.com/) - full-suite web app analytics tool including A/B testing. `©` `SaaS`  * [Rakam](https://rakam.io/) - Custom analytics platform that lets you to create your own analytics service. Integrate with any data source (web, mobile, IoT etc.), analyze data with SQL and create dashboards. ([Source Code](https://github.com/rakam-io/rakam)) `Apache-2.0` `Java`  * [Metabase](https://www.metabase.com) - opensource analytics/BI tool  `©` `SaaS`  * [LiveSession](https://livesession.io) - session replay user analytics. `©` `SaaS`  * [Glassbox](https://glassboxdigital.com/) - customer experince and session recording analytics. `©` `SaaS`  * [Redash](https://redash.io/) - open source analytics/BI tool `©` `SaaS`  * [AWStats](http://www.awstats.org/) - Generates web, streaming, ftp or mail server statistics graphically. ([Source Code](https://github.com/eldy/awstats)) `GPL-3.0` `Perl`  * [Countly](https://count.ly) - Real time mobile and web analytics, crash reporting and push notifications platform. ([Source Code](https://github.com/countly)) `AGPL-3.0` `Javascript`  * [Druid](http://druid.io/) - Distributed, column-oriented, real-time analytics data store. ([Source Code](https://github.com/druid-io/druid)) `Apache-2.0` `Java`  * [Hastic](https://hastic.io) - Hackable time series pattern recognition tool with UI for Grafana. ([Source Code](https://github.com/hastic)) `Apache-2.0` `Python/Nodejs`  * [EDA](https://eda.jortilles.com/en/jortilles-english/) - Open source analytics/BI tool.  ([Source Code](https://github.com/jortilles/EDA)) `Apache-2.0` `Angular/Nodejs`  * [Count](https://count.co/) - notebook-based analytics platform, use SQL or drag-and-drop to build queries. `©` `SaaS`    ## Real-time    * [GoAccess](http://goaccess.io/) - Real-time web log analyzer and interactive viewer that runs in a terminal. ([Source Code](https://github.com/allinurl/goaccess)) `GPL-2.0` `C`    ## Website analytics    * [KISSS](https://kis3.dev) - Very minimalistic (KISS) website statistics tool. ([Source Code](https://github.com/kis3/kis3)) `MIT` `Go`    ## Endpoints  * [Census](https://getcensus.com/) - The easiest way to sync your customer data from your cloud data warehouse to SaaS applications like Salesforce, Marketo, HubSpot, Zendesk, etc. Census is the operational analytics platform that syncs your data warehouse with all your favorite apps. Get your customer success, sales & marketing teams on the same page by keeping customer data in sync. No engineering favors required—just SQL. `SaaS`  * [RudderStack](https://rudderstack.com/) - The warehouse-first customer data platform (CDP) that builds your CDP on your data warehouse for you. RudderStack makes it easy to collect, unify, transform, and store your customer data as well as route it securely to a wide range of common, popular marketing, sales, and product tools (open-source alternative to Segment et al.). ([Source Code](https://github.com/rudderlabs/rudder-server/)) `AGPL-3.0` `Go`  * [Snowplow](http://snowplowanalytics.com/) - Analytics tool for web apps with a lot of data. Have every single event, from your websites, mobile apps, desktop applications and server-side systems, stored in your own data warehouse and available to action in real-time. ([Source Code](https://github.com/snowplow/)) `Apache-2.0` `Scala` `real-time`    ## SEO  * [Serposcope](https://serposcope.serphacker.com/) - Serposcope is a free and open-source rank tracker to monitor websites ranking in Google and improve your SEO performances. ([Source Code](https://github.com/serphacker/serposcope)) `MIT` `Java`    ## Privacy focused analytics    * [Fathom](https://usefathom.com/) - Fathom Analytics provides simple, useful websites stats without tracking or storing personal data of your users `©` `SaaS`  * [Plausible Analytics](https://plausible.io/) - Lightweight and [open source](https://github.com/plausible-insights/plausible) web analytics. Doesn’t use cookies and doesn't track personal data. A privacy-friendly alternative to Google Analytics. ([Source Code](https://github.com/plausible/analytics/)) `MIT` `Elixir`  * [GoatCounter](https://www.goatcounter.com) - Easy web statistics without tracking of personal data; `SaaS` `Self-Hosted` ([Source Code](https://github.com/zgoat/goatcounter)) `EUPL-1.2` `Go`  * [Simple Analytics](https://simpleanalytics.io/) - Simple, clean, and friendly analytics for developers `©` `SaaS`  * [Nibspace](https://nibspace.com/) - Affordable, lightweight, privacy-friendly website analytics `©` `SaaS`  * [Metrical](https://metrical.xyz/) - A privacy-first web analytics tool for everyone. `©` `SaaS`  * [Shynet](https://github.com/milesmcc/shynet) - Modern, privacy-friendly, and detailed web analytics that works without cookies or JS. Designed for self-hosting. `Apache-2.0` `Python`  * [Umami](https://umami.is/) - Umami is a simple, easy to use, self-hosted web analytics solution. The goal is to provide you with a friendlier, privacy-focused alternative to Google Analytics and a free, open-sourced alternative to paid solutions. ([Demo](https://app.umami.is/share/ISgW2qz8/flightphp.com), [Source Code](https://github.com/mikecao/umami)) `MIT` `Nodejs`  * [Koko Analytics](https://www.kokoanalytics.com/) - Privacy-friendly and open source analytics plugin for WordPress. ([Source Code](https://github.com/ibericode/koko-analytics/)) `GPL-3.0` `PHP`  * [Offen](https://www.offen.dev/) - Offen is a fair and open web analytics tool. Gain insights while your users have full access to their data. Lightweight, self hosted and free. ([Demo](https://www.offen.dev/try-demo/), [Source Code](https://github.com/offen/offen)) `Apache-2.0` `Go/Docker`  * [Freshlytics](https://github.com/sheshbabu/freshlytics) - Privacy respecting, cookie free and low resource usage analytics platform. `MIT` `Docker/Nodejs`  * [Kindmetrics](https://kindmetrics.io/) - Clean privacy-focused website analytics. ([Source Code](https://github.com/kindmetrics/kindmetrics)) `MIT` `Crystal`  * [Ackee](https://ackee.electerious.com) - Self-hosted analytics tool for those who care about privacy. ([Demo](http://demo.ackee.electerious.com), [Source Code](https://github.com/electerious/Ackee)) `MIT` `Nodejs`  * [piratepx](https://www.piratepx.com/) - Just a little analytics insight for your personal or indie project. 100% free and open source. ([Demo](https://app.piratepx.com/shared/bGQbUJ-YADC_xIGZaYmyqp-J_PD6O1pkCdHmYdIjUvs53ExsImlzFeou4MCuZRbH), [Source](https://github.com/piratepx/app)) `MIT` `Nodejs`  * [Piwik PRO](https://piwik.pro/) - A privacy-friendly alternative to Google Analytics with built-in consent management. Hosted in EU, in your private cloud or on-premises. `©` `SaaS` `self-hosted`    ## Heatmap analytics    * [Crazyegg](http://www.crazyegg.com/) - a heatmaps only analytics tool. `©` `SaaS`  * [Inspeclet](https://www.inspectlet.com/) - another web app heatmaps tool. `©` `SaaS`  * [Mouseflow](http://mouseflow.com/) - live analytics and heatmaps. `©` `SaaS`  * [Session Cam](http://www.sessioncam.com/) - heatmaps analytics tool. `©` `SaaS`    ## Analytics layers    * [Keen.io](http://keen.io/) - custom-analytics API. `©` `SaaS`  * [Popcorn Metrics](http://www.popcornmetrics.com/) - visual editor to capture events and send to other platforms. `©` `SaaS`  * [Segment](https://segment.com/) - helps you integrate multiple app analytics tool with one piece of code. `©` `SaaS`  * [Iteratively](https://iterative.ly/) - capture clean product analytics consistently across teams & platforms. `©` `SaaS`  * [Analytics npm package](https://getanalytics.io/) - A lightweight, extendable analytics library designed to work with any third-party analytics provider to track page views, custom events, & identify users. Works in browsers & node.js. `©` `SaaS`    ## Mobile analytics    The tools listed here are not necessarily mobile analytics tools only. However they show a strong mobile focus.    * [Upsight](http://www.upsight.com/) - mobile app analytics tool for developers. `©` `SaaS`  * [Appsflyer](http://www.appsflyer.com/) - all-in-one marketing tool with analytics. `©` `SaaS`  * [Amazon Pinpoint](https://aws.amazon.com/pinpoint/) - Amazons multi-platform, basic mobile analytics tool. `©` `SaaS`  * [Tapstream](https://tapstream.com/) - user lifecycle analytics. `©` `SaaS`  * [Honeytracks](https://honeytracks.com/) - mobile app analytics for games. `©` `SaaS`  * [Apsalar](https://apsalar.com/) - analytics tool for larger app shops. `©` `SaaS`  * [Roambi](http://www.roambi.com/) - 3-in-1 analytics tool that helps you track analytics, handle mobile app business intelligence, and app reporting. `©` `SaaS`  * [Appcelerator](http://www.appcelerator.com/platform/appcelerator-analytics/) - entire mobile app marketing suite. `©` `SaaS`  * [Flurry](http://www.flurry.com/) - pretty much the “industry standard” for mobile app analytics. `©` `SaaS`  * [Countly](http://count.ly/) - open source mobile & web application analytics tool. `©` `SaaS`  * [Playtomatic](http://playtomic.org/) - mobile app open source analytics tool for games. `©` `SaaS`  * [Capptain](http://www.capptain.com/) - real-time analytics tool with segmentation and push. `©` `SaaS`  * [Amplitude](https://amplitude.com/) - real-time mobile analytics with all data provided in redshift. `©` `SaaS`  * [Appsee](http://www.appsee.com/) - mobile app analytics platform automatically tracks all users' interactions in your app `©` `SaaS`  * [Mixpanel](https://mixpanel.com/) - fully featured mobile analytics platform with segmentation and push. `©` `SaaS`  * [Localytics](http://www.localytics.com/) - fast and beautiful real-time mobile analytics platform with in-app and push. `©` `SaaS`  * [GameAnalytics](http://www.gameanalytics.com/) - leading game analytics platform. `©` `SaaS`  * [Swrve](https://swrve.com) - mobile analytics with segmentation, push, A/B testing and rich messaging `©` `SaaS`  * [Firebase](https://firebase.google.com/features/) - a free and unlimited analytics solution for android and iOS `©` `SaaS`  * [Liquid](https://onliquid.com/) - real-time mobile analytics, personalization, multivariate testing, audience segmentation and push. `©` `SaaS`    ## App store analytics    * [Appfigures](http://appfigures.com/) - app store analytics to track sales, reviews and rankings with an API. `©` `SaaS`  * [Appannie](http://www.appannie.com/) - track your app data from iTunes, Google Play & Amazon. `©` `SaaS`  * [Distimo](http://www.distimo.com/) - free app store analytics (acquired by [Appannie](http://www.appannie.com/)). `©` `SaaS`  * [Priori Data](https://prioridata.com/) - track and benchmark the performance of apps on Apple- and Play store. `©` `SaaS`  * [Asking Point](http://www.askingpoint.com/mobile-app-rating-widget) - track your mobile app user ratings. `©` `SaaS`  * [Apptrace](http://www.apptrace.com/) - fast and free app store analytics platform. `©` `SaaS`    ## Attribution tracking    * [Adjust](http://adjust.com/) - open-source SDK with sophisticated analysis and campaign tracking. `©` `SaaS`  * [Clickmeter](https://clickmeter.com) - analytics tool that helps you track marketing campaigns. `©` `SaaS`  * [HasOffers Mobile app tracking](http://www.mobileapptracking.com/) - attribution analytics platform. `©` `SaaS`    ## Social media analytics    Often there is no clear differentiation between social media management and analytics as most the tools provide analytics.    * [Brandwatch](http://www.brandwatch.com/) - Social media monitoring and analytics. `©` `SaaS`  * [Falconsocial](http://www.falconsocial.com/) - communications platform built on social media with analytics. `©` `SaaS`  * [Quintly](https://www.quintly.com/) - web-based tool to help you track, benchmark and optimize your social media performance. `©` `SaaS`  * [Kred](http://kred.com/) - Klout-like social media analytics platform. `©` `SaaS`  * [Buffer](https://bufferapp.com/) - Social media publishing and analytics platform. `©` `SaaS`  * [Topsy](http://topsy.com/) - Social analytics tool with search. `©` `SaaS`  * [SocialBlade](http://socialblade.com/) - premiere YouTube statistics tracking. `©` `SaaS`  * [Hootsuite](https://hootsuite.com/) - Social media management dashbaord. `©` `SaaS`  * [Sproutsocial](http://sproutsocial.com/) - Social media management and analytics platform. `©` `SaaS`    ## Developer analytics    * [GitSpo](https://gitspo.com/) - Analytics for Open-Source. `©` `SaaS`  * [Pull Panda](https://pullpanda.com/analytics) - Metrics and insights for engineering teams `©` `SaaS`  * [Screenful](https://screenful.com/) - Visualise and share your project progress `©` `SaaS`  * [Haystack](https://usehaystack.io) - Metrics and insights for engineering teams `©` `SaaS`  * [GitSpo](https://gitspo.com/) - Analytics for Open-Source. `©` `SaaS`  * [Pull Panda](https://pullpanda.com/analytics) - Metrics and insights for engineering teams `©` `SaaS`  * [Plandek](https://plandek.com) - Metrics and insights for software delivery `©` `SaaS`  * [Screenful](https://screenful.com/) - Visualise and share your project progress `©` `SaaS`  * [Moiva.io](https://moiva.io/) - A dashboard with charts and graphs to evaluate and compare any npm package. `©` `SaaS`    ## Analytics dashboards    * [Freeboard](https://github.com/Freeboard/freeboard) - open source real-time dashboard builder for IOT and other web mashups. `©` `SaaS`  * [Geckboard](https://www.geckoboard.com/) - dashboard for key metrics in one place. `©` `SaaS`  * [Klipfolio](https://www.klipfolio.com/) - Klipfolio is an online dashboard platform for building powerful real-time business dashboards for your team or your clients. `©` `SaaS`  * [Vizia](https://www.brandwatch.com/products/vizia/) - Visual command center dashboarding solution `©` `SaaS`  * [Metabase](https://metabase.com/) - Metabase is the easy, open source way for everyone in your company to ask questions and learn from data. Simple Dashboarding and GUI Query tool, Nightly Emails and Slack Integration w/ PostgreSQL, MySQL, Redshift and other DBs. ([Source Code](https://github.com/metabase/metabase)) `AGPL-3.0` `Java`  * [Chartbrew](https://chartbrew.com) - Chartbrew allows you to query your databases and APIs to create live charts and visualize your data. You can share your charts with anyone or embed them on your own sites, blogs, Notion, etc. ([Demo](https://app.chartbrew.com/live-demo), [Source Code](https://github.com/chartbrew/chartbrew)) `MIT` `NodeJS`  * [Redash](http://redash.io) - connect to over 18 types of databases (SQL and ""NoSQL""), query your data, visualize it and create dashboards. Everything has a URL that can be shared. Slack and HipChat integration. ([Demo](https://demo.redash.io), [Source Code](https://github.com/getredash/redash)) `BSD-2-Clause` `Python`  * [Superset](http://superset.apache.org/) - Modern, enterprise-ready business intelligence web application. ([Source Code](https://github.com/apache/incubator-superset)) `Apache-2.0` `Python`  * [Socioboard](https://socioboard.org/) - `⚠` Social media management, analytics, and reporting platform supporting nine social media networks out-of-the-box. ([Source Code](https://github.com/socioboard/Socioboard-4.0)) `GPL-3.0` `C#/JavaScript`  * [EDA](https://eda.jortilles.com/en/jortilles-english/) - EDA is an user friendly Analtical Tool specially designed for busines users.  ([Source Code](https://github.com/jortilles/EDA)) `Apache-2.0` `Angular/Nodejs`    # Other Awesome Lists  - Other awesome lists [awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness).  - Even more lists [awesome](https://github.com/sindresorhus/awesome).  - Another list? [list](https://github.com/jnv/lists).  - WTF! [awesome-awesome-awesome](https://github.com/t3chnoboy/awesome-awesome-awesome).  - Analytics [awesome-bigdata](https://github.com/onurakpolat/awesome-bigdata). """
Big data;https://github.com/biokoda/actordb;"""### ActorDB is a distributed SQL database...    with the scalability of a KV store, while keeping the query capabilities of a relational database.    ActorDB is ideal as a server side database for [apps](http://www.actordb.com/docs-examples.html#example_filesync). Think of running a large mail service, dropbox, evernote, etc. They all require server side storage for user data, but the vast majority of queries is within a specific user. With many users, the server side database can get very large. Using ActorDB you can keep a full relational database for every user and not be forced into painful scaling strategies that require you to throw away everything that makes relational databases good.    ActorDB is a database that does not hide sharding from you. It makes it explicit, so you can keep fully relational chunks (i.e. actors) for the 99% of your database queries.    Even if your data model is not easily partitioned, ActorDB has a powerful KV data type that you can use instead. An [ActorDB KV](http://www.actordb.com/docs-kvstore.html#about_kv_store) type is an sql table that is partitioned across all servers. That table can have sub tables linked to it using foreign keys.    You can run queries or transactions on a single actor or across any number of actors. ActorDB can run on a single server or many servers. Writing to one actor is completely independent of writes to another actor, unless they are participating in the same transaction.    Servers can be added and schema can be updated at any time while the database is running.    Homepage: http://www.actordb.com/    For any questions you can use: https://gitter.im/actordb/    ActorDB is:    *   A distributed relational SQL database.  *   Consistent (not eventually consistent).  *   Distributed.  *   Redundant.  *   Massively concurrent.  *   No single point of failure.  *   ACID.  *   Connectable over MySQL protocol and [Thrift](https://github.com/biokoda/actordb/blob/master/adbt.thrift).  *   Replicated safely using the Raft distributed consensus algorithm.    Advantages    *   Complete horizontal scalability. All nodes are equivalent and you can have as many nodes as you need.  *   Full featured ACID database.  *   Suitable for very large datasets over many actors and servers.  *   No special drivers needed. Use the mysql driver of your language of choice.  *   Easy to configure and administer.  *   No global locks. Only the actors (one or many) involved in a transaction are locked during a write. All other actors are unaffected.  *   Uses stable reliable SQL and storage engines: SQLite on top of LMDB.  *   Inherits SQLite features like JSON support and common table expressions.    ### Would you like to contribute?    What we would most like to see is more client libraries on top of Thrift. Thrift generated code can be a bit verbose. Generally it is much nicer to implement an interface to it that hides some boilerplate code and uses nicer types.    Also if you have any ideas, thoughts on possible improvements or bugs to report, contact us using github issues.    So if you're interested in contributing. Use your language of choice. Generate a thrift interface using our [adbt.thrift](https://github.com/biokoda/actordb/blob/master/adbt.thrift), then write a clean interface to it.    We will list any outside contributions here.    ### Learn more    Documentation: http://www.actordb.com/docs-about.html    Story: http://blog.biokoda.com/post/112206754025/why-we-built-actordb    How SQLite runs on top of LMDB: http://blog.biokoda.com/post/133121776825/actordb-how-and-why-we-run-sqlite-on-top-of-lmdb    How to configure and run: http://www.actordb.com/docs-configuration.html    Change log: https://github.com/biokoda/actordb/blob/master/CHANGES.md    ### Client libs    Erlang: https://github.com/biokoda/actordb_client    .NET 2.0: https://github.com/hq-io/actordb-net    ### Builds    **ubuntu/debian package (64bit)**    https://dzbscw1ubdtyw.cloudfront.net/actordb_0.10.29-1_amd64.deb    **osx package (64bit):**    https://dzbscw1ubdtyw.cloudfront.net/actordb-0.10.29-OSX-x86_64.tar.gz    **red hat/centos package (64bit):**    Centos 7: https://dzbscw1ubdtyw.cloudfront.net/actordb-0.10.29-1.el7.x86_64.rpm    **general linux build (64bit)**    https://dzbscw1ubdtyw.cloudfront.net/actordb-0.10.29-linux.tar.gz    **windows package (64bit):**    https://dzbscw1ubdtyw.cloudfront.net/actordb-0.10.25-win-x86_64.zip """
Big data;https://github.com/shunfei/indexr;"""# IndexR    ![IndexR Logo](images/indexr-logo-150x150.png)    **IndexR** is a super fast columnar data format on HDFS, which focus on fast analytic, both for massive static(historical) data and rapidly ingesting realtime data. IndexR is designed for OLAP. IndexR is greatly suitable for building data warehouse based on Hadoop ecosystem.    * Super fast, 2~4x read speed of Parquet.  * 3 levels indices supported. Say goodbye to full scan.  * Support realtime ingestion. No more wait, analyse anything right after they happen.  * Hardware efficiency, anyone can use.  * Features like realtime and offline pre-aggregation, online schema update, 100% accurate, etc.  * Deep integration with Hadoop ecosystem. Adapted with popular query engines like Apache Drill, Apache Hive, etc.    #### Getting started    * Installation    * First [Compile from source](https://github.com/shunfei/indexr/wiki/Compilation) or download a pre-compiled package directly from [release page](https://github.com/shunfei/indexr/releases).    * Then [Set up a cluster](https://github.com/shunfei/indexr/wiki/Deployment).  * User manual - Check [here](https://github.com/shunfei/indexr/wiki/User-Guide).  * Any problems? - Found an [issue](https://github.com/shunfei/indexr/issues).    #### Documentation    [https://github.com/shunfei/indexr/wiki](https://github.com/shunfei/indexr/wiki)    #### Useful Links    * [IndexR 技术白皮书](https://github.com/shunfei/sfmind/blob/master/indexr_white_paper/indexr_white_paper.md)  * [IndexR introduction](https://github.com/shunfei/sfmind/blob/master/indexr.about.en.md)    Please feel free to file any issues.    ## Contact    * WeChat: xilyflow  * QQ Group: 606666586 (IndexR讨论组)    ## License        Copyright 2016 Sunteng Tech.        Licensed under the Apache License, Version 2.0 (the ""License""); you may not      use this file except in compliance with the License. You may obtain a copy of      the License at        http://www.apache.org/licenses/LICENSE-2.0        Unless required by applicable law or agreed to in writing, software      distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT      WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the      License for the specific language governing permissions and limitations under      the License. """
Big data;https://github.com/pingcap/tikv;"""<img src=""images/tikv-logo.png"" alt=""tikv_logo"" width=""300""/>    ## [Website](https://tikv.org) | [Documentation](https://tikv.org/docs/latest/concepts/overview/) | [Community Chat](https://tikv.org/chat)    [![Build Status](https://ci.pingcap.net/buildStatus/icon?job=tikv_ghpr_build_master)](https://ci.pingcap.net/blue/organizations/jenkins/tikv_ghpr_build_master/activity)  [![Coverage Status](https://codecov.io/gh/tikv/tikv/branch/master/graph/badge.svg)](https://codecov.io/gh/tikv/tikv)  [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2574/badge)](https://bestpractices.coreinfrastructure.org/projects/2574)    TiKV is an open-source, distributed, and transactional key-value database. Unlike other traditional NoSQL systems, TiKV not only provides classical key-value APIs, but also transactional APIs with ACID compliance. Built in Rust and powered by Raft, TiKV was originally created to complement [TiDB](https://github.com/pingcap/tidb), a distributed HTAP database compatible with the MySQL protocol.    The design of TiKV ('Ti' stands for titanium) is inspired by some great distributed systems from Google, such as BigTable, Spanner, and Percolator, and some of the latest achievements in academia in recent years, such as the Raft consensus algorithm.    If you're interested in contributing to TiKV, or want to build it from source, see [CONTRIBUTING.md](./CONTRIBUTING.md).    ![cncf_logo](images/cncf.png#gh-light-mode-only)  ![cncf_logo](images/cncf-white.png#gh-dark-mode-only)    TiKV is a graduated project of the [Cloud Native Computing Foundation](https://cncf.io/) (CNCF). If you are an organization that wants to help shape the evolution of technologies that are container-packaged, dynamically-scheduled and microservices-oriented, consider joining the CNCF. For details about who's involved and how TiKV plays a role, read the CNCF [announcement](https://www.cncf.io/announcements/2020/09/02/cloud-native-computing-foundation-announces-tikv-graduation/).    ---    With the implementation of the Raft consensus algorithm in Rust and consensus state stored in RocksDB, TiKV guarantees data consistency. [Placement Driver (PD)](https://github.com/pingcap/pd/), which is introduced to implement auto-sharding, enables automatic data migration. The transaction model is similar to Google's Percolator with some performance improvements. TiKV also provides snapshot isolation (SI), snapshot isolation with lock (SQL: `SELECT ... FOR UPDATE`), and externally consistent reads and writes in distributed transactions.    TiKV has the following key features:    - **Geo-Replication**        TiKV uses [Raft](http://raft.github.io/) and the Placement Driver to support Geo-Replication.    - **Horizontal scalability**        With PD and carefully designed Raft groups, TiKV excels in horizontal scalability and can easily scale to 100+ TBs of data.    - **Consistent distributed transactions**        Similar to Google's Spanner, TiKV supports externally-consistent distributed transactions.    - **Coprocessor support**        Similar to HBase, TiKV implements a coprocessor framework to support distributed computing.    - **Cooperates with [TiDB](https://github.com/pingcap/tidb)**        Thanks to the internal optimization, TiKV and TiDB can work together to be a compelling database solution with high horizontal scalability, externally-consistent transactions, support for RDBMS, and NoSQL design patterns.    ## Governance    See [Governance](https://github.com/tikv/community/blob/master/GOVERNANCE.md).    ## Documentation    For instructions on deployment, configuration, and maintenance of TiKV,see TiKV documentation on our [website](https://tikv.org/docs/4.0/tasks/introduction/). For more details on concepts and designs behind TiKV, see [Deep Dive TiKV](https://tikv.org/deep-dive/introduction/).    > **Note:**  >  > We have migrated our documentation from the [TiKV's wiki page](https://github.com/tikv/tikv/wiki/) to the [official website](https://tikv.org/docs). The original Wiki page is discontinued. If you have any suggestions or issues regarding documentation, offer your feedback [here](https://github.com/tikv/website).    ## TiKV adopters    You can view the list of [TiKV Adopters](https://tikv.org/adopters/).    ## TiKV software stack    ![The TiKV software stack](images/tikv_stack.png)    - **Placement Driver:** PD is the cluster manager of TiKV, which periodically checks replication constraints to balance load and data automatically.  - **Store:** There is a RocksDB within each Store and it stores data into the local disk.  - **Region:** Region is the basic unit of Key-Value data movement. Each Region is replicated to multiple Nodes. These multiple replicas form a Raft group.  - **Node:** A physical node in the cluster. Within each node, there are one or more Stores. Within each Store, there are many Regions.    When a node starts, the metadata of the Node, Store and Region are recorded into PD. The status of each Region and Store is reported to PD regularly.    ## Quick start    ### Deploy a playground with TiUP    The most quickest to try out TiKV with TiDB is using TiUP, a component manager for TiDB.    You can see [this page](https://docs.pingcap.com/tidb/stable/quick-start-with-tidb#deploy-a-local-test-environment-using-tiup-playground) for a step by step tutorial.    ### Deploy a playground with binary    TiKV is able to run separately with PD, which is the minimal deployment required.    1. Download and extract binaries.    ```bash  $ export TIKV_VERSION=v4.0.12  $ export GOOS=darwin  # only {darwin, linux} are supported  $ export GOARCH=amd64 # only {amd64, arm64} are supported  $ curl -O  https://tiup-mirrors.pingcap.com/tikv-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz  $ curl -O  https://tiup-mirrors.pingcap.com/pd-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz  $ tar -xzf tikv-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz  $ tar -xzf pd-$TIKV_VERSION-$GOOS-$GOARCH.tar.gz  ```    2. Start PD instance.    ```bash  $ ./pd-server --name=pd --data-dir=/tmp/pd/data --client-urls=""http://127.0.0.1:2379"" --peer-urls=""http://127.0.0.1:2380"" --initial-cluster=""pd=http://127.0.0.1:2380"" --log-file=/tmp/pd/log/pd.log  ```    3. Start TiKV instance.    ```bash  $ ./tikv-server --pd-endpoints=""127.0.0.1:2379"" --addr=""127.0.0.1:20160"" --data-dir=/tmp/tikv/data --log-file=/tmp/tikv/log/tikv.log  ```    4. Install TiKV Client(Python) and verify the deployment, required Python 3.5+.    ```bash  $ pip3 install -i https://test.pypi.org/simple/ tikv-client  ```    ```python  from tikv_client import RawClient    client = RawClient.connect(""127.0.0.1:2379"")    client.put(b'foo', b'bar')  print(client.get(b'foo')) # b'bar'    client.put(b'foo', b'baz')  print(client.get(b'foo')) # b'baz'  ```    ### Deploy a cluster with TiUP    You can see [this manual](./doc/deploy.md) of production-like cluster deployment presented by @c4pt0r.    ### Build from source    See [CONTRIBUTING.md](./CONTRIBUTING.md).    ## Client drivers    Currently, the interfaces to TiKV are the [TiDB Go client](https://github.com/pingcap/tidb/tree/master/store/tikv) and the [TiSpark Java client](https://github.com/pingcap/tispark/tree/master/tikv-client/src/main/java/com/pingcap/tikv).    These are the clients for TiKV:    - [Go](https://github.com/tikv/client-go) (The most stable and widely used)  - [Java](https://github.com/tikv/client-java)  - [Rust](https://github.com/tikv/client-rust)  - [C](https://github.com/tikv/client-c)    If you want to try the Go client, see [Go Client](https://tikv.org/docs/4.0/reference/clients/go/).    ## Security    ### Security audit    A third-party security auditing was performed by Cure53. See the full report [here](./security/Security-Audit.pdf).    ### Reporting Security Vulnerabilities    To report a security vulnerability, please send an email to [TiKV-security](mailto:tikv-security@lists.cncf.io) group.    See [Security](./security/SECURITY.md) for the process and policy followed by the TiKV project.    ## Communication    Communication within the TiKV community abides by [TiKV Code of Conduct](./CODE_OF_CONDUCT.md). Here is an excerpt:    > In the interest of fostering an open and welcoming environment, we as  contributors and maintainers pledge to making participation in our project and  our community a harassment-free experience for everyone, regardless of age, body  size, disability, ethnicity, sex characteristics, gender identity and expression,  level of experience, education, socio-economic status, nationality, personal  appearance, race, religion, or sexual identity and orientation.    ### Social Media    - [Twitter](https://twitter.com/tikvproject)  - [Blog](https://tikv.org/blog/)  - [Reddit](https://www.reddit.com/r/TiKV)  - Post questions or help answer them on [Stack Overflow](https://stackoverflow.com/questions/tagged/tikv)    ### Slack    Join the TiKV community on [Slack](https://slack.tidb.io/invite?team=tikv-wg&channel=general) - Sign up and join channels on TiKV topics that interest you.    ## License    TiKV is under the Apache 2.0 license. See the [LICENSE](./LICENSE) file for details.    ## Acknowledgments    - Thanks [etcd](https://github.com/coreos/etcd) for providing some great open source tools.  - Thanks [RocksDB](https://github.com/facebook/rocksdb) for their powerful storage engines.  - Thanks [rust-clippy](https://github.com/rust-lang/rust-clippy). We do love the great project. """
Big data;https://github.com/salesforce/Argus;"""Argus  [![Build Status](https://travis-ci.org/salesforce/Argus.svg?branch=master)](https://travis-ci.org/salesforce/Argus)  [![Static Analysis](https://scan.coverity.com/projects/8155/badge.svg)](https://scan.coverity.com/projects/salesforceeng-argus)  [![argus-sdk - Maven Central](https://maven-badges.herokuapp.com/maven-central/com.salesforce.argus/argus-sdk/badge.svg?maxAge=3600)](https://maven-badges.herokuapp.com/maven-central/com.salesforce.argus/argus-sdk)  =====    Argus is a time-series monitoring and alerting platform. It consists of discrete services to configure alerts, ingest and transform metrics & events, send notifications, create namespaces, and to both establish and enforce policies and quotas for usage.    Its architecture allows any and all of these services to be retargeted to new technology as it becomes available, with little to no impact on the users.    To find out more [see the wiki](https://github.com/salesforce/Argus/wiki) and check out the [release notes](https://github.com/salesforce/Argus/releases).    ![Argus UI](ArgusUIScreenshot.png ""Viewing metric with default UI"")    ## Building Argus    ### Installing The Resource Filters    Argus uses the `argus-build.properties` file as a resource filter for the build and all the module builds.  After you clone the project for the first time, or after you change this file, you must create and install the dependency jars which will contain these filters.  Those dependency jars are then pulled in by the modules, expanded and have their values applied to the module specific builds.  Luckily, it's a straightforward operation.  Just execute the following command from within the parent project, after you first clone the project or after you update the `argus-build.properties` file.    ```  mvn -DskipTests=true -DskipDockerBuild --non-recursive install  ```    ### Running The Unit Tests    Once the resource filters are installed, you can run unit tests.  Running the unit tests doesn't require any changes to the argus-build.properties file.  Just install the resource filters and execute the `test` goal.    ```  mvn test  ```    **Only the unit tests are run by `codecov.io` and as such, the coverage reported by it is significantly less than the coverage obtained by running the full test suite.**    ### Running The Integration Tests    The integration tests for Argus use the `LDAPAuthService` implementation of the `AuthService` interface and the `DefaultTSDBService` implementation of the `TSDBService` interface (which targets OpenTSDB).  Additionally it uses the `RedisCacheService` implementation of the `CacheService` interface to facilitate integration testing of the `BatchService`.  In order to run the integration tests you must update the `argus-build.properties` file to correctly setup the external LDAP you'll be testing against and the OpenTSDB endpoints to use as well as the Redis cluster.  The snippet below shows the specific properties that should be modified in the `argus-build.properties` file.  Of course, after you make these updates, you must re-install the resource filter dependencies as described above and execute the `clean` goal, before running the integration tests.    ```  # The LDAP endpoint to use  service.property.auth.ldap.endpoint=ldaps://ldaps.yourdomain.com:636  # A list of comma separated search paths used to query the DN of users attempting to authenticate.  # This example lists two separate search bases.  One for users and one for service accounts.  service.property.auth.ldap.searchbase=OU=active,OU=user,DC=yourdomain,DC=com:OU=active,OU=robot,DC=yourdomain,DC=com  # This specifies of the DN for the privileged user that is used to bind and subsequently execute the search for user DN's  service.property.auth.ldap.searchdn=CN=argus_admin,OU=active,OU=user,DC=yourdomain,DC=com  # The password for the privileged user above.  service.property.auth.ldap.searchpwd=Argu5R0cks!  # The LDAP field with which the username provided during a login attempt, will be matched.  # This is used so Argus can obtain the DN for the user attempting to login, and subsequently attempt to bind as that user.  service.property.auth.ldap.usernamefield=sAMAccountName  # The TSDB read endpoint  service.property.tsdb.endpoint.read=http://readtsdb.yourdomain.com:4466  # The TSDB write endpoint  service.property.tsdb.endpoint.write=http://writetsdb.yourdomain.com:4477  # The Redis cache cluster information  service.property.cache.redis.cluster=redis0.mycompany.com:6379,redis1.mycompany.com:6389  ```    Once the modifications have been made and the resource filters re-installed, you're ready to run the complete suite of tests, including the integration tests.    ```  mvn verify  ```    ### Generating Coverage Reports    Coverage is calculated everytime tests are run for all modules with the exception of ArgusWeb.  In order to generate a coverage report for a module, just `cd` into the module subdirectory and run the report generation target.    ```  mvn jacoco:report  ```    Coverage reports are generated in the `target/site/jacoco` directory.    ### Deploying & Running Argus    Please [see the wiki](https://github.com/salesforce/Argus/wiki) for information on how to deploy, configure and run Argus. """
Big data;https://github.com/gionkunz/chartist-js;"""# Big welcome by the Chartist Guy    [![Join the chat at https://gitter.im/gionkunz/chartist-js](https://badges.gitter.im/gionkunz/chartist-js.svg)](https://gitter.im/gionkunz/chartist-js?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  [![npm version](http://img.shields.io/npm/v/chartist.svg)](https://npmjs.org/package/chartist) [![build status](http://img.shields.io/travis/gionkunz/chartist-js.svg)](https://travis-ci.org/gionkunz/chartist-js) [![Inline docs](http://inch-ci.org/github/gionkunz/chartist-js.svg?branch=develop)](http://inch-ci.org/github/gionkunz/chartist-js)    ![The Chartist Guy](https://raw.github.com/gionkunz/chartist-js/develop/site/images/chartist-guy.gif ""The Chartist Guy"")    *Checkout the documentation site at http://gionkunz.github.io/chartist-js/*    *Checkout this lightning talk that gives you an overview of Chartist in 5 minutes https://www.youtube.com/watch?v=WdYzPhOB_c8*    *Guest talk of the Chartist.js Guy at the Treehouse Show https://www.youtube.com/watch?v=h9oH0iDaZDQ&t=2m40s*    Chartist.js is a simple responsive charting library built with SVG. There are hundreds of nice charting libraries already  out there, but they are either:    * not responsive  * use the wrong technologies for illustration (canvas)  * are not flexible enough while keeping the configuration simple  * are not friendly to your own code  * are not friendly to designers  * have unnecessary dependencies to monolithic libraries   * more annoying things    That's why we started Chartist.js and our goal is to solve all of the above issues.    ## What is it made for?    Chartist's goal is to provide a simple, lightweight and unintrusive library to responsively craft charts on your website.   It's important to understand that one of the main intentions of Chartist.js is to rely on standards rather than providing   it's own solution to a problem which is already solved by those standards. We need to leverage the power of browsers   today and say good bye to the idea of solving all problems ourselves.    Chartist works with inline-SVG and therefore leverages the power of the DOM to provide parts of its functionality. This   also means that Chartist does not provide it's own event handling, labels, behaviors or anything else that can just be   done with plain HTML, JavaScript and CSS. The single and only responsibility of Chartist is to help you drawing ""Simple   responsive Charts"" using inline-SVG in the DOM, CSS to style and JavaScript to provide an API for configuring your charts.    ## Example site    You can visit this Site http://gionkunz.github.io/chartist-js/ which is in fact a build of the current project.  We are still developing and constantly add features but you can already use Chartist.js in your projects as we have   reached a stable and reliable state already.    ## Version notes    We are currently still heavily developing in order to make Chartist.js better. Your help is needed! Please contribute  to the project if you like the idea and the concept and help us to bring nice looking responsive open-source charts  to the masses.    ### Important missing stuff    1. Jasmine Tests!  2. Documentation: JSDoc, Getting started documentation and landing page  3. Better accessibility using ARIA and other optimizations  4. Better interfaces to the library (i.e. jQuery with data-* attributes for configuration), Angular.js directive etc.  5. Richer Sass / CSS framework  6. Other charts types (spider etc.)    ## Plugins    Some features aren't right for the core product  but there is a great set of plugins available  which add features like:    * [Axis labels](http://gionkunz.github.io/chartist-js/plugins.html#axis-title-plugin)  * [Tooltips at data points](https://gionkunz.github.io/chartist-js/plugins.html#tooltip-plugin)  * [Coloring above/below a threshold](https://gionkunz.github.io/chartist-js/plugins.html#threshold-plugin)    and more.    See all the plugins [here](https://gionkunz.github.io/chartist-js/plugins.html).    ## Contribution    We are looking for people who share the idea of having a simple, flexible charting library that is responsive and uses  modern and future-proof technologies. The goal of this project is to create a responsive charting library where developers  have their joy in using it and designers love it because of the designing flexibility they have.    Contribute if you like the Chartist Guy! """
Big data;https://github.com/apache/pulsar;"""<!--        Licensed to the Apache Software Foundation (ASF) under one      or more contributor license agreements.  See the NOTICE file      distributed with this work for additional information      regarding copyright ownership.  The ASF licenses this file      to you under the Apache License, Version 2.0 (the      ""License""); you may not use this file except in compliance      with the License.  You may obtain a copy of the License at          http://www.apache.org/licenses/LICENSE-2.0        Unless required by applicable law or agreed to in writing,      software distributed under the License is distributed on an      ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY      KIND, either express or implied.  See the License for the      specific language governing permissions and limitations      under the License.    -->    ![logo](site2/website/static/img/pulsar.svg)    Pulsar is a distributed pub-sub messaging platform with a very  flexible messaging model and an intuitive client API.    Learn more about Pulsar at https://pulsar.apache.org    ## Main features    * Horizontally scalable (Millions of independent topics and millions    of messages published per second)  * Strong ordering and consistency guarantees  * Low latency durable storage  * Topic and queue semantics  * Load balancer  * Designed for being deployed as a hosted service:    * Multi-tenant    * Authentication    * Authorization    * Quotas    * Support mixing very different workloads    * Optional hardware isolation  * Keeps track of consumer cursor position  * REST API for provisioning, admin and stats  * Geo replication  * Transparent handling of partitioned topics  * Transparent batching of messages    ## Repositories    This repository is the main repository of Apache Pulsar. Pulsar PMC also maintains other repositories for  components in the Pulsar ecosystem, including connectors, adapters, and other language clients.    - [Pulsar Core](https://github.com/apache/pulsar)    ### Helm Chart    - [Pulsar Helm Chart](https://github.com/apache/pulsar-helm-chart)    ### Ecosystem    - [Pulsar Adapters](https://github.com/apache/pulsar-adapters)  - [Pulsar Connectors](https://github.com/apache/pulsar-connectors)  - [Pulsar SQL (Pulsar Presto Connector)](https://github.com/apache/pulsar-presto)    ### Clients    - [.NET/C# Client](https://github.com/apache/pulsar-dotpulsar)  - [Go Client](https://github.com/apache/pulsar-client-go)  - [NodeJS Client](https://github.com/apache/pulsar-client-node)  - [Ruby Client](https://github.com/apache/pulsar-client-ruby)    ### Dashboard & Management Tools    - [Pulsar Manager](https://github.com/apache/pulsar-manager)    ### Documentation    - [Pulsar Translation](https://github.com/apache/pulsar-translation)    ### CI/CD    - [Pulsar CI](https://github.com/apache/pulsar-test-infra)    ## Build Pulsar    Requirements:   * Java [JDK 11](https://adoptium.net/?variant=openjdk11) or [JDK 8](https://adoptium.net/?variant=openjdk8)   * Maven 3.6.1+   * zip    Compile and install:    ```bash  $ mvn install -DskipTests  ```    Compile and install individual module  ```bash  $ mvn -pl module-name (e.g: pulsar-broker) install -DskipTests  ```    ## Minimal build (This skips most of external connectors and tiered storage handlers)  ```  mvn install -Pcore-modules,-main -DskipTests  ```    Run Unit Tests:    ```bash  $ mvn test  ```    Run Individual Unit Test:    ```bash  $ mvn -pl module-name (e.g: pulsar-client) test -Dtest=unit-test-name (e.g: ConsumerBuilderImplTest)  ```    Run Selected Test packages:    ```bash  $ mvn test -pl module-name (for example, pulsar-broker) -Dinclude=org/apache/pulsar/**/*.java  ```    Start standalone Pulsar service:    ```bash  $ bin/pulsar standalone  ```    Check https://pulsar.apache.org for documentation and examples.    ## Build custom docker images    Docker images must be built with Java 8 for `branch-2.7` or previous branches because of  [issue 8445](https://github.com/apache/pulsar/issues/8445).  Java 11 is the recommended JDK version in `master`/`branch-2.8`.    This builds the docker images `apachepulsar/pulsar-all:latest` and `apachepulsar/pulsar:latest`.    ```bash  mvn clean install -DskipTests  mvn package -Pdocker,-main -am -pl docker/pulsar-all -DskipTests  ```    After the images are built, they can be tagged and pushed to your custom repository.   Here's an example of a bash script that tags the docker images with the current version and git revision and   pushes them to `localhost:32000/apachepulsar`.    ```bash  image_repo_and_project=localhost:32000/apachepulsar  pulsar_version=$(mvn initialize help:evaluate -Dexpression=project.version -pl . -q -DforceStdout)  gitrev=$(git rev-parse HEAD | colrm 10)  tag=""${pulsar_version}-${gitrev}""  echo ""Using tag $tag""  docker tag apachepulsar/pulsar-all:latest ${image_repo_and_project}/pulsar-all:$tag  docker push ${image_repo_and_project}/pulsar-all:$tag  docker tag apachepulsar/pulsar:latest ${image_repo_and_project}/pulsar:$tag  docker push ${image_repo_and_project}/pulsar:$tag  ```    ## Setting up your IDE    Apache Pulsar is using [lombok](https://projectlombok.org/) so you have to ensure your IDE setup with  required plugins.    ### Intellij    #### Configure Project JDK to Java 11 JDK    1. Open **Project Settings**.         Click **File** -> **Project Structure** -> **Project Settings** -> **Project**.       2. Select the JDK version.            From the JDK version drop-down list, select **Download JDK...** or choose an existing recent Java 11 JDK version.    3. In the download dialog, select version **11**. You can pick a version from many vendors. Unless you have a specific preference, choose **Eclipse Temurin (AdoptOpenJDK (Hotspot))**.       #### Configure Java version for Maven in IntelliJ    1. Open Maven Importing Settings dialog by going to      **Settings** -> **Build, Execution, Deployment** -> **Build Tools** -> **Maven** -> **Importing**.    2. Choose **Use Project JDK** for **JDK for Importer** setting. This uses the Java 11 JDK for running Maven      when importing the project to IntelliJ. Some of the configuration in the Maven build is conditional based on      the JDK version. Incorrect configuration gets chosen when the ""JDK for Importer"" isn't the same as the ""Project JDK"".    3. Validate that the JRE setting in **Maven** -> **Runner** dialog is set to **Use Project JDK**.    #### Configure annotation processing in IntelliJ    1. Open Annotation Processors Settings dialog box by going to     **Settings** -> **Build, Execution, Deployment** -> **Compiler** -> **Annotation Processors**.    2. Select the following buttons:     1. **Enable annotation processing**     2. **Obtain processors from project classpath**     3. Store generated sources relative to: **Module content root**    3. Set the generated source directories to be equal to the Maven directories:     1. Set ""Production sources directory:"" to ""target/generated-sources/annotations"".     2. Set ""Test sources directory:"" to ""target/generated-test-sources/test-annotations"".    4. Click **OK**.    5. Install the lombok plugin in intellij.    #### Configure code style    1. Open Code Style Settings dialog box by going to **Settings** -> **Editor** -> **Code Style**.    2. Click on the :gear: symbol -> **Import scheme** -> **Intellij IDEA code style XML**    3. Pick the file `${pulsar_dir}/src/idea-code-style.xml`    4. On the dialog box that opens, click **OK**.    5. Ensure the scheme you just created is selected in **Scheme** dropdown then click **OK**.    #### Configure Checkstyle    1. Install the Checkstyle-IDEA plugin.    2. Open Checkstyle Settings dialog box by going to **Settings** -> **Tools** -> **Checkstyle**.    3. Set **Checkstyle version** to **8.37**.    4. Set **Scan scope** to **Only Java sources (including tests)**.    5. Click **+** button in the **Configuration** section to open a dialog to choose the checkfile file.     1. Enter a **Description**. For example, Pulsar.     2. Select **Use a local checkstyle file**.     3. Set **File** to **buildtools/src/main/resources/pulsar/checkstyle.xml**.     4. Select **Store relative to project location**.     5. Click **Next** -> **Next** -> **Finish**.    6. Activate the configuration you just added by toggling the corresponding box.    7. Click **OK**.    #### Further configuration in IntelliJ     * When working on the Pulsar core modules in IntelliJ, reduce the number of active projects in IntelliJ to speed up IDE actions and reduce unrelated IDE warnings.    * In IntelliJ's Maven UI's tree view under ""Profiles""      * Activate ""core-modules"" Maven profile      * De-activate ""main"" Maven profile      * Run the ""Reload All Maven Projects"" action from the Maven UI toolbar. You can also find the action by the name in the IntelliJ ""Search Everywhere"" window that gets activated by pressing the **Shift** key twice.    * Run the ""Generate Sources and Update Folders For All Projects"" action from the Maven UI toolbar. You can also find the action by the name in the IntelliJ ""Search Everywhere"" window that gets activated by pressing the **Shift** key twice. Running the action takes about 10 minutes for all projects. This is faster when the ""core-modules"" profile is the only active profile.      #### IntelliJ usage tips    * In the case of compilation errors with missing Protobuf classes, ensure to run the ""Generate Sources and Update Folders For All Projects"" action.    * All of the Pulsar source code doesn't compile properly in IntelliJ and there are compilation errors.    * Use the ""core-modules"" profile if working on the Pulsar core modules since the source code for those modules can be compiled in IntelliJ.    * Sometimes it might help to mark a specific project ignored in IntelliJ Maven UI by right-clicking the project name and select **Ignore Projects** from the menu.    * Currently, it is not always possible to run unit tests directly from the IDE because of the compilation issues. As a workaround, individual test classes can be run by using the `mvn test -Dtest=TestClassName` command.        * The above steps have all been performed, but a test still won't run.    * In this case, try the following steps:      1. Close IntelliJ.      2. Run `mvn clean install -DskipTests` on the command line.      3. Reopen IntelliJ.    * If that still doesn't work:      1. Verify Maven is using a supported version. Currently, the supported version of Maven is specified in the          <requireMavenVersion> section of the main pom.xml file.      2. Try ""restart and clear caches"" in IntelliJ and repeat the above steps to reload projects and generate sources.    ### Eclipse    Follow the instructions [here](https://howtodoinjava.com/automation/lombok-eclipse-installation-examples/)  to configure your Eclipse setup.    ## Build Pulsar docs    Refer to the docs [README](site2/README.md).    ## Contact    ##### Mailing lists    | Name                                                                          | Scope                           |                                                                 |                                                                     |                                                                              |  |:------------------------------------------------------------------------------|:--------------------------------|:----------------------------------------------------------------|:--------------------------------------------------------------------|:-----------------------------------------------------------------------------|  | [users@pulsar.apache.org](mailto:users@pulsar.apache.org) | User-related discussions        | [Subscribe](mailto:users-subscribe@pulsar.apache.org) | [Unsubscribe](mailto:users-unsubscribe@pulsar.apache.org) | [Archives](http://mail-archives.apache.org/mod_mbox/pulsar-users/) |  | [dev@pulsar.apache.org](mailto:dev@pulsar.apache.org)     | Development-related discussions | [Subscribe](mailto:dev-subscribe@pulsar.apache.org)   | [Unsubscribe](mailto:dev-unsubscribe@pulsar.apache.org)   | [Archives](http://mail-archives.apache.org/mod_mbox/pulsar-dev/)   |    ##### Slack    Pulsar slack channel at https://apache-pulsar.slack.com/    You can self-register at https://apache-pulsar.herokuapp.com/    ##### Report a security vulnerability    To report a vulnerability for Pulsar, contact the [Apache Security Team](https://www.apache.org/security/). When reporting a vulnerability to [security@apache.org](mailto:security@apache.org), you can copy your email to [private@pulsar.apache.org](mailto:private@pulsar.apache.org) to send your report to the Apache Pulsar Project Management Committee. This is a private mailing list.    ## License    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0    ## Crypto Notice    This distribution includes cryptographic software. The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to another country, of encryption software. BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted. See <http://www.wassenaar.org/> for more information.    The U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing cryptographic functions with asymmetric algorithms. The form and manner of this Apache Software Foundation distribution makes it eligible for export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for both object code and source code.    The following provides more details on the included cryptographic software: Pulsar uses the SSL library from Bouncy Castle written by http://www.bouncycastle.org. """
Big data;https://github.com/bloomberg/comdb2;"""## Overview    Comdb2 is a clustered RDBMS built on Optimistic Concurrency Control techniques.   It provides multiple isolation levels, including Snapshot and Serializable Isolation.   Read/Write transactions run on any node, with the client library transparently negotiating connections to lowest cost (latency) node which is available.  The client library provides transparent reconnect.    Work on Comdb2 was started at Bloomberg LP in 2004 and it has been under heavy development since.  More information about the architecture of the project can be found in our [VLDB 2016 paper](http://www.vldb.org/pvldb/vol9/p1377-scotti.pdf) and for more information on usage please look in the [Docs](https://bloomberg.github.io/comdb2/overview_home.html).    ## Documentation    [Comdb2 documentation](http://bloomberg.github.io/comdb2) is included in the `docs` directory.   It can be hosted locally with jekyll by running `jekyll serve` from the `docs` directory.    ## Contributing    Please refer to our [contribution guide](https://bloomberg.github.io/comdb2/contrib.html) for instructions.  We welcome code and idea contributions.    ## Quick Start    On every machine in the cluster:    1. Make sure all machines in the cluster can talk to each other via ssh.       Copy keys around if needed.      2. Install prerequisites:           **Debian/Ubuntu**               ```     sudo apt-get install -y  \         bison                \         build-essential      \         cmake                \         flex                 \         libevent-dev         \         liblz4-dev           \         libprotobuf-c-dev    \         libreadline-dev      \         libsqlite3-dev       \         libssl-dev           \         libunwind-dev        \         ncurses-dev          \         protobuf-c-compiler  \         tcl                  \         uuid-dev             \         zlib1g-dev     ```       **CentOS 7/8**       On CentOS 8, enable the PowerTools repository first:       ```     dnf config-manager --set-enabled powertools     ```       ```     yum install -y       \         byacc            \         cmake            \         epel-release     \         flex             \         gawk             \         gcc              \         gcc-c++          \         libevent-devel   \         libunwind        \         libunwind-devel  \         libuuid          \         libuuid-devel    \         lz4              \         lz4-devel        \         make             \         openssl          \         openssl-devel    \         openssl-libs     \         protobuf-c       \         protobuf-c-devel \         readline-devel   \         rpm-build        \         sqlite           \         sqlite-devel     \         tcl              \         which            \         zlib             \         zlib-devel     ```       **macOS High Sierra (experimental)**       Install Xcode and Homebrew. Then install required libraries:       ```     brew install cmake lz4 openssl protobuf-c readline libevent     ```       To run tests, install following:       ```     brew install coreutils bash gawk jq     export PATH=""/usr/local/opt/coreutils/libexec/gnubin:$PATH""     ```       It is recommended to increase the open file limits, at least in the sessions which start pmux and server.      3. Build and Install Comdb2:       ```     mkdir build && cd build && cmake .. && make && sudo make install     ```    4. Add */opt/bb/bin* to your PATH       ```     export PATH=$PATH:/opt/bb/bin     ```    5. Start pmux:       ```     pmux -n     ```    6. _(optional)_ Comdb2 nodes identify each other by their hostnames.  If the hostname      of each node isn't resolvable from other nodes, we should tell Comdb2 the full      domain name to use for the current node.  Most setups won't have this issue.       Tell comdb2 our FQDN.     ```bash     vi /opt/bb/etc/cdb2/config/comdb2.d/hostname.lrl     add current machine's name, e.g.     hostname machine-1.comdb2.example.com     ```    7. On one machine (say machine-1), create a database - this example creates a database      called _testdb_ stored in _~/db_.       ```     comdb2 --create --dir ~/db testdb     ```          Note: the `--dir PATH` parameter is optional, and if it is omitted comdb2 uses a default root of */opt/bb/var/cdb2/* for creating a database directory to contain the database files, which is named as per the database name parameter; hence in this case  */opt/bb/var/cdb2/testdb*.       The default root will have to be created explicitly with the desired permissions before invoking `comdb2 --create` for a database.       In this quick start, we use the home directory to avoid obfuscating the key steps of the process.             8. Configure the nodes in the cluster:     ```     vi ~/db/testdb.lrl     add     cluster nodes machine-1.comdb2.example.com machine-2.comdb2.example.com     ```       9. On other nodes, copy the database over:     ```     copycomdb2 mptest-1.comdb2.example.com:${HOME}/db/testdb.lrl     ```       0. On all nodes, start the database.     ```     comdb2 --lrl ~/db/testdb.lrl testdb     ```     All nodes will say 'I AM READY.' when ready.          Note: the log dir comdb2 uses by default is */opt/bb/var/log/cdb2/*      If this directory does not have permissions allowing the user to create file, there will be diagnostics output such as:       > [ERROR] error opening '/opt/bb/var/log/cdb2/testdb.longreqs' for logging: 13 Permission denied            This condition will not impact operation of the database for the purposes of this quick start.           1. On any node, start using the database.  You don't have any tables yet.  You can add them with *cdb2sql*      Example -     ```sql     cdb2sql testdb local 'CREATE TABLE t1 {          schema {              int a          }     }'     ```       Database can be queried/updated with cdb2sql:     ```sql     cdb2sql testdb local 'insert into t1(a) values(1)'     (rows inserted=1)     cdb2sql testdb local 'select * from t1'     (a=1)     ```    ## Comdb2 Directory Contents    | Directory | Description |  | --- | --- |  | bbinc/        | Header & Generic include files |  | bdb/          | Table layer |  | berkdb/       | Btrees layer |  | cdb2api/      | Client code |  | cdb2jdbc/     | JDBC driver |  | cmake/        | cmake configuration files |  | comdb2rle/    | Run length encoding |  | contrib/      | Misc useful programs that aren't part of core Comdb2 |  | crc32c/       | Checksum component |  | csc2/         | csc2 processing |  | csc2files/    | csc2 config files |  | cson/         | JSON library |  | datetime/     | Datetime component |  | db/           | Types layer and overall glue |  | dfp/          | Decimal number component |  | dlmalloc/     | Local malloc version |  | docs/         | Documentation |  | lua/          | All things pertaining to lua VM used for stored procedures |  | mem/          | Memory accounting subsystem |  | net/          | Network component |  | pkg/          | deb and rpm packaging rules |  | plugin/       | Plugin subsystem |  | protobuf/     | API to communicate with the server |  | schemachange/ | Code for table create/alter/truncate/etc |  | sockpool/     | sockpool related files  |  | sqlite/       | Sqlite VM SQL engine  |  | tcl/          | Tcl language bindings |  | tests/        | Comdb2 test suite |  | tools/        | Tools that are part of Comdb2 core |  | util/         | Useful generic modules |   """
Big data;https://github.com/cayleygraph/cayley;"""<div align=""center"">    <a href=""https://github.com/cayleygraph/cayley"">      <img width=""200"" src=""https://github.com/cayleygraph/branding/raw/master/cayley_bottom.png"" alt=""Cayley"">    </a>  </div>    [![Build Status](https://travis-ci.com/cayleygraph/cayley.svg?branch=master)](https://travis-ci.com/cayleygraph/cayley)  [![Container Repository](https://img.shields.io/docker/cloud/build/cayleygraph/cayley ""Container Repository"")](https://hub.docker.com/r/cayleygraph/cayley)    Cayley is an open-source database for [Linked Data](https://www.w3.org/standards/semanticweb/data). It is inspired by the graph database behind Google's [Knowledge Graph](https://en.wikipedia.org/wiki/Knowledge_Graph) (formerly [Freebase](https://en.wikipedia.org/wiki/Freebase_(database))).    [![Get it from the Snap Store](https://snapcraft.io/static/images/badges/en/snap-store-white.svg)](https://snapcraft.io/cayley)    ## [Documentation](https://cayley.gitbook.io/cayley/)    ## Features    - Built-in query editor, visualizer and REPL  - Multiple query languages:    - [Gizmo](./docs/gizmoapi.md): query language inspired by [Gremlin](http://gremlindocs.com/)    - [GraphQL](./docs/graphql.md)-inspired query language    - [MQL](./docs/mql.md): simplified version for [Freebase](https://en.wikipedia.org/wiki/Freebase_(database)) fans  - Modular: easy to connect to your favorite programming languages and back-end stores  - Production ready: well tested and used by various companies for their production workloads  - Fast: optimized specifically for usage in applications    ### Performance    Rough performance testing shows that, on 2014 consumer hardware and an average disk, 134m quads in LevelDB is no problem and a multi-hop intersection query -- films starring X and Y -- takes ~150ms.    ## Community    - Website: [cayley.io](https://cayley.io)  - Slack: [cayleygraph.slack.com](https://cayleygraph.slack.com) -- Invite [here](https://cayley-slackin.herokuapp.com/)  - Discourse list: [discourse.cayley.io](https://discourse.cayley.io) (Also acts as mailing list, enable mailing list mode)  - Twitter: [@cayleygraph](https://twitter.com/cayleygraph)  - Involvement: [Contribute](./docs/contributing.md) """
Big data;https://github.com/cswinter/LocustDB;"""# LocustDB    [![Build Status][bi]][bl] [![Crates.io][ci]][cl] [![Gitter][gi]][gl]    [bi]: https://github.com/cswinter/LocustDB/workflows/Test/badge.svg  [bl]: https://github.com/cswinter/LocustDB/actions    [ci]: https://img.shields.io/crates/v/locustdb.svg  [cl]: https://crates.io/crates/locustdb/    [gi]: https://badges.gitter.im/LocustDB/Lobby.svg  [gl]: https://gitter.im/LocustDB/Lobby    An experimental analytics database aiming to set a new standard for query performance and storage efficiency on commodity hardware.  See [How to Analyze Billions of Records per Second on a Single Desktop PC][blogpost] and [How to Read 100s of Millions of Records per Second from a Single Disk][blogpost-2] for an overview of current capabilities.    ## Usage    Download the [latest binary release][latest-release], which can be run from the command line on most x64 Linux systems, including Windows Subsystem for Linux. For example, to load the file `test_data/nyc-taxi.csv.gz` in this repository and start the repl run:    ```Bash  ./locustdb --load test_data/nyc-taxi.csv.gz --trips  ```    When loading `.csv` or `.csv.gz` files with `--load`, the first line of each file is assumed to be a header containing the names for all columns. The type of each column will be derived automatically, but this might break for columns that contain a mixture of numbers/strings/empty entries.    To persist data to disk in LocustDB's internal storage format (which allows fast queries from disk after the initial load), specify the storage location with `--db-path`  When creating/opening a persistent database, LocustDB will open a lot of files and might crash if the limit on the number of open files is too low.  On Linux, you can check the current limit with `ulimit -n` and set a new limit with e.g. `ulimit -n 4096`.    The `--trips` flag will configure the ingestion schema for loading the 1.46 billion taxi ride dataset which can be downloaded [here][nyc-taxi-trips].    For additional usage info, invoke with `--help`:    ```Bash  $ ./locustdb --help  LocustDB 0.2.1  Clemens Winter <clemenswinter1@gmail.com>  Massively parallel, high performance analytics database that will rapidly devour all of your data.    USAGE:      locustdb [FLAGS] [OPTIONS]    FLAGS:      -h, --help             Prints help information          --mem-lz4          Keep data cached in memory lz4 encoded. Decreases memory usage and query speeds.          --reduced-trips    Set ingestion schema for select set of columns from nyc taxi ride dataset          --seq-disk-read    Improves performance on HDD, can hurt performance on SSD.          --trips            Set ingestion schema for nyc taxi ride dataset      -V, --version          Prints version information    OPTIONS:          --db-path <PATH>           Path to data directory          --load <FILES>             Load .csv or .csv.gz files into the database          --mem-limit-tables <GB>    Limit for in-memory size of tables in GiB [default: 8]          --partition-size <ROWS>    Number of rows per partition when loading new data [default: 65536]          --readahead <MB>           How much data to load at a time when reading from disk during queries in MiB                                     [default: 256]          --schema <SCHEMA>          Comma separated list specifying the types and (optionally) names of all columns in                                     files specified by `--load` option.                                     Valid types: `s`, `string`, `i`, `integer`, `ns` (nullable string), `ni` (nullable                                     integer)                                     Example schema without column names: `int,string,string,string,int`                                     Example schema with column names: `name:s,age:i,country:s`          --table <NAME>             Name for the table populated with --load [default: default]          --threads <INTEGER>        Number of worker threads. [default: number of cores (12)]  ```    ## Goals  A vision for LocustDB.    ### Fast  Query performance for analytics workloads is best-in-class on commodity hardware, both for data cached in memory and for data read from disk.    ### Cost-efficient  LocustDB automatically achieves spectacular compression ratios, has minimal indexing overhead, and requires less machines to store the same amount of data than any other system. The trade-off between performance and storage efficiency is configurable.    ### Low latency  New data is available for queries within seconds.    ### Scalable  LocustDB scales seamlessly from a single machine to large clusters.    ### Flexible and easy to use  LocustDB should be usable with minimal configuration or schema-setup as:  - a highly available distributed analytics system continuously ingesting data and executing queries  - a commandline tool/repl for loading and analysing data from CSV files  - an embedded database/query engine included in other Rust programs via cargo      ## Non-goals  Until LocustDB is production ready these are distractions at best, if not wholly incompatible with the main goals.    ### Strong consistency and durability guarantees  - small amounts of data may be lost during ingestion  - when a node is unavailable, queries may return incomplete results  - results returned by queries may not represent a consistent snapshot    ### High QPS  LocustDB does not efficiently execute queries inserting or operating on small amounts of data.    ### Full SQL support  - All data is append only and can only be deleted/expired in bulk.  - LocustDB does not support queries that cannot be evaluated independently by each node (large joins, complex subqueries, precise set sizes, precise top n).    ### Support for cost-inefficient or specialised hardware  LocustDB does not run on GPUs.      ## Compiling from source    1. Install Rust: [rustup.rs][rustup]  2. Clone the repository    ```Bash  git clone https://github.com/cswinter/LocustDB.git  cd LocustDB  ```    3. Compile with `--release` for optimal performance:    ```Bash  cargo run --release --bin repl -- --load test_data/nyc-taxi.csv.gz --reduced-trips  ```    ### Running tests or benchmarks    `cargo test`    `cargo bench`    ### Storage backend  LocustDB has support for persisting data to disk and running queries on data stored on disk.  This feature is disabled by default, and has to be enabled explicitly by passing `--features ""enable_rocksdb""` to cargo during compilation.  The database backend uses RocksDB, which is a somewhat complex C++ dependency that has to be compiled from source and requires gcc and various libraries to be available.  You will have to manually install those on your system, instructions can be found [here][rocksdb-dependencies].  You may also have to install various other random tools until compilation succeeds.    ### LZ4    Compile with `--features ""enable_lz4""` to enable an additional lz4 compression pass which can significantly reduce data size both on disk and in-memory, at the cost of slightly slower in-memory queries.      [nyc-taxi-trips]: https://www.dropbox.com/sh/4xm5vf1stnf7a0h/AADRRVLsqqzUNWEPzcKnGN_Pa?dl=0  [blogpost]: https://clemenswinter.com/2018/07/09/how-to-analyze-billions-of-records-per-second-on-a-single-desktop-pc/  [blogpost-2]: https://clemenswinter.com/2018/08/13/how-read-100s-of-millions-of-records-per-second-from-a-single-disk/  [rustup]: https://rustup.rs/  [rocksdb-dependencies]: https://github.com/facebook/rocksdb/blob/master/INSTALL.md#dependencies  [latest-release]: https://github.com/cswinter/LocustDB/releases/download/v0.1.0-alpha/locustdb-0.1.0-alpha-x64-linux.0-alpha """
Big data;https://github.com/WeBankFinTech/Linkis;"""Linkis  ==========    [![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)    [English](README.md) | [中文](README_CN.md)    # Introduction     Linkis builds a layer of computation middleware between upper applications and underlying engines. By using standard interfaces such as REST/WS/JDBC provided by Linkis, the upper applications can easily access the underlying engines such as MySQL/Spark/Hive/Presto/Flink, etc., and achieve the intercommunication of user resources like unified variables, scripts, UDFs, functions and resource files at the same time.    As a computation middleware, Linkis provides powerful connectivity, reuse, orchestration, expansion, and governance capabilities. By decoupling the application layer and the engine layer, it simplifies the complex network call relationship, and thus reduces the overall complexity and saves the development and maintenance costs as well.    Since the first release of Linkis in 2019, it has accumulated more than **700** trial companies and **1000+** sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on. Lots of companies have already used Linkis as a unified entrance for the underlying computation and storage engines of the big data platform.      ![linkis-intro-01](https://user-images.githubusercontent.com/7869972/148767375-aeb11b93-16ca-46d7-a30e-92fbefe2bd5e.png)    ![linkis-intro-03](https://user-images.githubusercontent.com/7869972/148767380-c34f44b2-9320-4633-9ec8-662701f41d15.png)    # Features    - **Support for diverse underlying computation storage engines**.        Currently supported computation/storage engines: Spark, Hive, Python, Presto, ElasticSearch, MLSQL, TiSpark, JDBC, Shell, etc;            Computation/storage engines to be supported: Flink(Supported in version >=1.0.2), Impala, etc;            Supported scripting languages: SparkSQL, HiveQL, Python, Shell, Pyspark, R, Scala and JDBC, etc.        - **Powerful task/request governance capabilities**. With services such as Orchestrator, Label Manager and customized Spring Cloud Gateway, Linkis is able to provide multi-level labels based, cross-cluster/cross-IDC fine-grained routing, load balance, multi-tenancy, traffic control, resource control, and orchestration strategies like dual-active, active-standby, etc.      - **Support full stack computation/storage engine**. As a computation middleware, it will receive, execute and manage tasks and requests for various computation storage engines, including batch tasks, interactive query tasks, real-time streaming tasks and storage tasks;    - **Resource management capabilities**.  ResourceManager is not only capable of managing resources for Yarn and Linkis EngineManger as in Linkis 0.X, but also able to provide label-based multi-level resource allocation and recycling, allowing itself to have powerful resource management capabilities across mutiple Yarn clusters and mutiple computation resource types;    - **Unified Context Service**. Generate Context ID for each task/request,  associate and manage user and system resource files (JAR, ZIP, Properties, etc.), result set, parameter variable, function, etc., across user, system, and computing engine. Set in one place, automatic reference everywhere;    - **Unified materials**. System and user-level unified material management, which can be shared and transferred across users and systems.    # Supported engine types    | **Engine** | **Supported Version** | **Linkis 0.X version requirement**| **Linkis 1.X version requirement** | **Description** |  |:---- |:---- |:---- |:---- |:---- |  |Flink |1.12.2|\>=dev-0.12.0, PR #703 not merged yet.|>=1.0.2|	Flink EngineConn. Supports FlinkSQL code, and also supports Flink Jar to Linkis Manager to start a new Yarn application.|  |Impala|\>=3.2.0, CDH >=6.3.0""|\>=dev-0.12.0, PR #703 not merged yet.|ongoing|Impala EngineConn. Supports Impala SQL.|  |Presto|\>= 0.180|\>=0.11.0|ongoing|Presto EngineConn. Supports Presto SQL.|  |ElasticSearch|\>=6.0|\>=0.11.0|ongoing|ElasticSearch EngineConn. Supports SQL and DSL code.|  |Shell|Bash >=2.0|\>=0.9.3|\>=1.0.0_rc1|Shell EngineConn. Supports shell code.|  |MLSQL|\>=1.1.0|\>=0.9.1|ongoing|MLSQL EngineConn. Supports MLSQL code.|  |JDBC|MySQL >=5.0, Hive >=1.2.1|\>=0.9.0|\>=1.0.0_rc1|JDBC EngineConn. Supports MySQL and HiveQL code.|  |Spark|Apache 2.0.0~2.4.7, CDH >=5.4.0|\>=0.5.0|\>=1.0.0_rc1|Spark EngineConn. Supports SQL, Scala, Pyspark and R code.|  |Hive|Apache >=1.0.0, CDH >=5.4.0|\>=0.5.0|\>=1.0.0_rc1|Hive EngineConn. Supports HiveQL code.|  |Hadoop|Apache >=2.6.0, CDH >=5.4.0|\>=0.5.0|ongoing|Hadoop EngineConn. Supports Hadoop MR/YARN application.|  |Python|\>=2.6|\>=0.5.0|\>=1.0.0_rc1|Python EngineConn. Supports python code.|  |TiSpark|1.1|\>=0.5.0|ongoing|TiSpark EngineConn. Support querying TiDB data by SparkSQL.|    # Ecosystem    | Component | Description | Linkis 0.x(recommend 0.11.0) Compatible | Linkis 1.x(recommend 1.0.3) Compatible |  | --------------- | -------------------------------------------------------------------- | --------- | --------- |  | [**DataSphereStudio**](https://github.com/WeBankFinTech/DataSphereStudio/blob/master/README.md) | DataSphere Studio (DSS for short) is WeDataSphere, a one-stop data application development management portal. | DSS 0.9.1[released] | **DSS 1.0.1[developing]** |  | [**Scriptis**](https://github.com/WeBankFinTech/Scriptis) | Support online script writing such as SQL, Pyspark, HiveQL, etc., submit to [Linkis](https://github.com/apache/incubator-linkis) to perform data analysis web tools. | Scriptis merged in DSS（DSS 0.9.1[released]） | **In DSS 1.0.1[developing]** |  | [**Schedulis**](https://github.com/WeBankFinTech/Schedulis) | Workflow task scheduling system based on Azkaban secondary development, with financial-grade features such as high performance, high availability and multi-tenant resource isolation. | Schedulis 0.6.1[released] |  **Schedulis0.6.2 [developing]** |  | [**Qualitis**](https://github.com/WeBankFinTech/Qualitis) | Data quality verification tool, providing data verification capabilities such as data integrity and correctness  | Qualitis 0.8.0[released] | **Qualitis 0.9.0 [developing]** |  | [**Streamis**](https://github.com/WeBankFinTech/Streamis) | Streaming application development management tool. It supports the release of Flink Jar and Flink SQL, and provides the development, debugging and production management capabilities of streaming applications, such as: start-stop, status monitoring, checkpoint, etc. | **No support** | **Streamis 0.1.0 [developing]** |  | [**Exchangis**](https://github.com/WeBankFinTech/Exchangis) | A data exchange platform that supports data transmission between structured and unstructured heterogeneous data sources, the upcoming Exchangis1. 0, will be connected with DSS workflow | **No support** | **Exchangis 1.0.0 [developing]**|  | [**Visualis**](https://github.com/WeBankFinTech/Visualis) | A data visualization BI tool based on the second development of Davinci, an open source project of CreditEase, provides users with financial-level data visualization capabilities in terms of data security. | Visualis 0.5.0[released]| **Visualis 1.0.0[developing]**|  | [**Prophecis**](https://github.com/WeBankFinTech/Prophecis) | A one-stop machine learning platform that integrates multiple open source machine learning frameworks. Prophecis' MLFlow can be connected to DSS workflow through AppConn. | Prophecis 0.2.2[released] | **Prophecis 0.3.0 [developing]** |    # Download    Please go to the [Linkis Releases Page](https://github.com/apache/incubator-linkis/releases) to download a compiled distribution or a source code package of Linkis.    # Compile and deploy  Please follow [Compile Guide](https://linkis.apache.org/docs/latest/development/linkis_compile_and_package) to compile Linkis from source code.    Please refer to [Deployment Documents](https://linkis.apache.org/docs/latest/deployment/quick_deploy) to do the deployment.      # Examples and Guidance  You can find examples and guidance for how to use and manage Linkis in [User Manual](https://linkis.apache.org/docs/latest/user_guide/overview), [Engine Usage Documents](https://linkis.apache.org/docs/latest/engine_usage/overview) and [API Documents](https://linkis.apache.org/docs/latest/api/overview).    # Documentation    The documentation of linkis is in [Linkis-Website Git Repository](https://github.com/apache/incubator-linkis-website).    # Architecture  Linkis services could be divided into three categories: computation governance services, public enhancement services and microservice governance services.  - The computation governance services, support the 3 major stages of processing a task/request: submission -> preparation -> execution;  - The public enhancement services, including the material library service, context service, and data source service;  - The microservice governance services, including Spring Cloud Gateway, Eureka and Open Feign.    Below is the Linkis architecture diagram. You can find more detailed architecture docs in [Linkis-Doc/Architecture](https://linkis.apache.org/docs/latest/architecture/overview).  ![architecture](https://user-images.githubusercontent.com/7869972/148767383-f87e84ba-5baa-4125-8b6e-d0aa4f7d3a66.png)    Based on Linkis the computation middleware, we've built a lot of applications and tools on top of it in the big data platform suite [WeDataSphere](https://github.com/WeBankFinTech/WeDataSphere). Below are the currently available open-source projects. More projects upcoming, please stay tuned.    ![wedatasphere_stack_Linkis](https://user-images.githubusercontent.com/7869972/148767389-049361df-3609-4c2f-a4e2-c904c273300e.png)    # Contributing    Contributions are always welcomed, we need more contributors to build Linkis together. either code, or doc, or other supports that could help the community.    For code and documentation contributions, please follow the [contribution guide](https://linkis.apache.org/community/how-to-contribute).    # Contact Us    Any questions or suggestions please kindly submit an issue.    You can scan the QR code below to join our WeChat and QQ group to get more immediate response.    ![introduction05](https://user-images.githubusercontent.com/7869972/148767386-0663f833-547d-4c30-8876-081bb966ffb8.png)    Meetup videos on [Bilibili](https://space.bilibili.com/598542776?from=search&seid=14344213924133040656).    # Who is Using Linkis    We opened [an issue](https://github.com/apache/incubator-linkis/issues/23) for users to feedback and record who is using Linkis.    Since the first release of Linkis in 2019, it has accumulated more than **700** trial companies and **1000+** sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on. """
Big data;https://github.com/chrislusf/seaweedfs;"""# SeaweedFS      [![Slack](https://img.shields.io/badge/slack-purple)](https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY)  [![Twitter](https://img.shields.io/twitter/follow/seaweedfs.svg?style=social&label=Follow)](https://twitter.com/intent/follow?screen_name=seaweedfs)  [![Build Status](https://img.shields.io/github/workflow/status/chrislusf/seaweedfs/Go)](https://github.com/chrislusf/seaweedfs/actions/workflows/go.yml)  [![GoDoc](https://godoc.org/github.com/chrislusf/seaweedfs/weed?status.svg)](https://godoc.org/github.com/chrislusf/seaweedfs/weed)  [![Wiki](https://img.shields.io/badge/docs-wiki-blue.svg)](https://github.com/chrislusf/seaweedfs/wiki)  [![Docker Pulls](https://img.shields.io/docker/pulls/chrislusf/seaweedfs?maxAge=4800)](https://hub.docker.com/r/chrislusf/seaweedfs/)  [![SeaweedFS on Maven Central](https://img.shields.io/maven-central/v/com.github.chrislusf/seaweedfs-client)](https://search.maven.org/search?q=g:com.github.chrislusf)      ![SeaweedFS Logo](https://raw.githubusercontent.com/chrislusf/seaweedfs/master/note/seaweedfs.png)    <h2 align=""center""><a href=""https://www.patreon.com/seaweedfs"">Sponsor SeaweedFS via Patreon</a></h2>    SeaweedFS is an independent Apache-licensed open source project with its ongoing development made  possible entirely thanks to the support of these awesome [backers](https://github.com/chrislusf/seaweedfs/blob/master/backers.md).  If you'd like to grow SeaweedFS even stronger, please consider joining our  <a href=""https://www.patreon.com/seaweedfs"">sponsors on Patreon</a>.    Your support will be really appreciated by me and other supporters!    <!--  <h4 align=""center"">Platinum</h4>    <p align=""center"">    <a href="""" target=""_blank"">      Add your name or icon here    </a>  </p>  -->      ### Gold Sponsors  - [![nodion](https://www.nodion.com/img/logo.svg)](https://www.nodion.com)  - ![shuguang](https://raw.githubusercontent.com/chrislusf/seaweedfs/master/note/shuguang.png)    ---      - [Download Binaries for different platforms](https://github.com/chrislusf/seaweedfs/releases/latest)  - [SeaweedFS on Slack](https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY)  - [SeaweedFS on Twitter](https://twitter.com/SeaweedFS)  - [SeaweedFS on Telegram](https://t.me/Seaweedfs)   - [SeaweedFS Mailing List](https://groups.google.com/d/forum/seaweedfs)  - [Wiki Documentation](https://github.com/chrislusf/seaweedfs/wiki)  - [SeaweedFS White Paper](https://github.com/chrislusf/seaweedfs/wiki/SeaweedFS_Architecture.pdf)  - [SeaweedFS Introduction Slides 2021.5](https://docs.google.com/presentation/d/1DcxKWlINc-HNCjhYeERkpGXXm6nTCES8mi2W5G0Z4Ts/edit?usp=sharing)  - [SeaweedFS Introduction Slides 2019.3](https://www.slideshare.net/chrislusf/seaweedfs-introduction)    Table of Contents  =================    * [Quick Start](#quick-start)      * [Quick Start for S3 API on Docker](#quick-start-for-s3-api-on-docker)      * [Quick Start with Single Binary](#quick-start-with-single-binary)  * [Introduction](#introduction)  * [Features](#features)      * [Additional Features](#additional-features)      * [Filer Features](#filer-features)  * [Example: Using Seaweed Object Store](#example-Using-Seaweed-Object-Store)  * [Architecture](#architecture)  * [Compared to Other File Systems](#compared-to-other-file-systems)      * [Compared to HDFS](#compared-to-hdfs)      * [Compared to GlusterFS, Ceph](#compared-to-glusterfs-ceph)      * [Compared to GlusterFS](#compared-to-glusterfs)      * [Compared to Ceph](#compared-to-ceph)  * [Dev Plan](#dev-plan)  * [Installation Guide](#installation-guide)  * [Disk Related Topics](#disk-related-topics)  * [Benchmark](#Benchmark)  * [License](#license)      ## Quick Start for S3 API on Docker ##    `docker run -p 8333:8333 chrislusf/seaweedfs server -s3`    ## Quick Start with Single Binary ##  * Download the latest binary from https://github.com/chrislusf/seaweedfs/releases and unzip a single binary file `weed` or `weed.exe`  * Run `weed server -dir=/some/data/dir -s3` to start one master, one volume server, one filer, and one S3 gateway.    Also, to increase capacity, just add more volume servers by running `weed volume -dir=""/some/data/dir2"" -mserver=""<master_host>:9333"" -port=8081` locally, or on a different machine, or on thousands of machines. That is it!    ## Introduction ##    SeaweedFS is a simple and highly scalable distributed file system. There are two objectives:    1. to store billions of files!  2. to serve the files fast!    SeaweedFS started as an Object Store to handle small files efficiently.   Instead of managing all file metadata in a central master,   the central master only manages volumes on volume servers,   and these volume servers manage files and their metadata.   This relieves concurrency pressure from the central master and spreads file metadata into volume servers,   allowing faster file access (O(1), usually just one disk read operation).    There is only 40 bytes of disk storage overhead for each file's metadata.   It is so simple with O(1) disk reads that you are welcome to challenge the performance with your actual use cases.    SeaweedFS started by implementing [Facebook's Haystack design paper](http://www.usenix.org/event/osdi10/tech/full_papers/Beaver.pdf).   Also, SeaweedFS implements erasure coding with ideas from   [f4: Facebook’s Warm BLOB Storage System](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-muralidhar.pdf), and has a lot of similarities with [Facebook’s Tectonic Filesystem](https://www.usenix.org/system/files/fast21-pan.pdf)    On top of the object store, optional [Filer] can support directories and POSIX attributes.   Filer is a separate linearly-scalable stateless server with customizable metadata stores,   e.g., MySql, Postgres, Redis, Cassandra, HBase, Mongodb, Elastic Search, LevelDB, RocksDB, Sqlite, MemSql, TiDB, Etcd, CockroachDB, etc.    For any distributed key value stores, the large values can be offloaded to SeaweedFS.   With the fast access speed and linearly scalable capacity,   SeaweedFS can work as a distributed [Key-Large-Value store][KeyLargeValueStore].    SeaweedFS can transparently integrate with the cloud.   With hot data on local cluster, and warm data on the cloud with O(1) access time,   SeaweedFS can achieve both fast local access time and elastic cloud storage capacity.  What's more, the cloud storage access API cost is minimized.   Faster and Cheaper than direct cloud storage!    [Back to TOC](#table-of-contents)    ## Additional Features ##  * Can choose no replication or different replication levels, rack and data center aware.  * Automatic master servers failover - no single point of failure (SPOF).  * Automatic Gzip compression depending on file mime type.  * Automatic compaction to reclaim disk space after deletion or update.  * [Automatic entry TTL expiration][VolumeServerTTL].  * Any server with some disk spaces can add to the total storage space.  * Adding/Removing servers does **not** cause any data re-balancing unless triggered by admin commands.  * Optional picture resizing.  * Support ETag, Accept-Range, Last-Modified, etc.  * Support in-memory/leveldb/readonly mode tuning for memory/performance balance.  * Support rebalancing the writable and readonly volumes.  * [Customizable Multiple Storage Tiers][TieredStorage]: Customizable storage disk types to balance performance and cost.  * [Transparent cloud integration][CloudTier]: unlimited capacity via tiered cloud storage for warm data.  * [Erasure Coding for warm storage][ErasureCoding]  Rack-Aware 10.4 erasure coding reduces storage cost and increases availability.    [Back to TOC](#table-of-contents)    ## Filer Features ##  * [Filer server][Filer] provides ""normal"" directories and files via http.  * [File TTL][FilerTTL] automatically expires file metadata and actual file data.  * [Mount filer][Mount] reads and writes files directly as a local directory via FUSE.  * [Filer Store Replication][FilerStoreReplication] enables HA for filer meta data stores.  * [Active-Active Replication][ActiveActiveAsyncReplication] enables asynchronous one-way or two-way cross cluster continuous replication.  * [Amazon S3 compatible API][AmazonS3API] accesses files with S3 tooling.  * [Hadoop Compatible File System][Hadoop] accesses files from Hadoop/Spark/Flink/etc or even runs HBase.  * [Async Replication To Cloud][BackupToCloud] has extremely fast local access and backups to Amazon S3, Google Cloud Storage, Azure, BackBlaze.  * [WebDAV] accesses as a mapped drive on Mac and Windows, or from mobile devices.  * [AES256-GCM Encrypted Storage][FilerDataEncryption] safely stores the encrypted data.  * [Super Large Files][SuperLargeFiles] stores large or super large files in tens of TB.  * [Cloud Drive][CloudDrive] mounts cloud storage to local cluster, cached for fast read and write with asynchronous write back.  * [Gateway to Remote Object Store][GatewayToRemoteObjectStore] mirrors bucket operations to remote object storage, in addition to [Cloud Drive][CloudDrive]    ## Kubernetes ##  * [Kubernetes CSI Driver][SeaweedFsCsiDriver] A Container Storage Interface (CSI) Driver. [![Docker Pulls](https://img.shields.io/docker/pulls/chrislusf/seaweedfs-csi-driver.svg?maxAge=4800)](https://hub.docker.com/r/chrislusf/seaweedfs-csi-driver/)  * [SeaweedFS Operator](https://github.com/seaweedfs/seaweedfs-operator)    [Filer]: https://github.com/chrislusf/seaweedfs/wiki/Directories-and-Files  [SuperLargeFiles]: https://github.com/chrislusf/seaweedfs/wiki/Data-Structure-for-Large-Files  [Mount]: https://github.com/chrislusf/seaweedfs/wiki/FUSE-Mount  [AmazonS3API]: https://github.com/chrislusf/seaweedfs/wiki/Amazon-S3-API  [BackupToCloud]: https://github.com/chrislusf/seaweedfs/wiki/Async-Replication-to-Cloud  [Hadoop]: https://github.com/chrislusf/seaweedfs/wiki/Hadoop-Compatible-File-System  [WebDAV]: https://github.com/chrislusf/seaweedfs/wiki/WebDAV  [ErasureCoding]: https://github.com/chrislusf/seaweedfs/wiki/Erasure-coding-for-warm-storage  [TieredStorage]: https://github.com/chrislusf/seaweedfs/wiki/Tiered-Storage  [CloudTier]: https://github.com/chrislusf/seaweedfs/wiki/Cloud-Tier  [FilerDataEncryption]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Data-Encryption  [FilerTTL]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Stores  [VolumeServerTTL]: https://github.com/chrislusf/seaweedfs/wiki/Store-file-with-a-Time-To-Live  [SeaweedFsCsiDriver]: https://github.com/seaweedfs/seaweedfs-csi-driver  [ActiveActiveAsyncReplication]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Active-Active-cross-cluster-continuous-synchronization  [FilerStoreReplication]: https://github.com/chrislusf/seaweedfs/wiki/Filer-Store-Replication  [KeyLargeValueStore]: https://github.com/chrislusf/seaweedfs/wiki/Filer-as-a-Key-Large-Value-Store  [CloudDrive]: https://github.com/chrislusf/seaweedfs/wiki/Cloud-Drive-Architecture  [GatewayToRemoteObjectStore]: https://github.com/chrislusf/seaweedfs/wiki/Gateway-to-Remote-Object-Storage      [Back to TOC](#table-of-contents)    ## Example: Using Seaweed Object Store ##    By default, the master node runs on port 9333, and the volume nodes run on port 8080.  Let's start one master node, and two volume nodes on port 8080 and 8081. Ideally, they should be started from different machines. We'll use localhost as an example.    SeaweedFS uses HTTP REST operations to read, write, and delete. The responses are in JSON or JSONP format.    ### Start Master Server ###    ```  > ./weed master  ```    ### Start Volume Servers ###    ```  > weed volume -dir=""/tmp/data1"" -max=5  -mserver=""localhost:9333"" -port=8080 &  > weed volume -dir=""/tmp/data2"" -max=10 -mserver=""localhost:9333"" -port=8081 &  ```    ### Write File ###    To upload a file: first, send a HTTP POST, PUT, or GET request to `/dir/assign` to get an `fid` and a volume server url:    ```  > curl http://localhost:9333/dir/assign  {""count"":1,""fid"":""3,01637037d6"",""url"":""127.0.0.1:8080"",""publicUrl"":""localhost:8080""}  ```    Second, to store the file content, send a HTTP multi-part POST request to `url + '/' + fid` from the response:    ```  > curl -F file=@/home/chris/myphoto.jpg http://127.0.0.1:8080/3,01637037d6  {""name"":""myphoto.jpg"",""size"":43234,""eTag"":""1cc0118e""}  ```    To update, send another POST request with updated file content.    For deletion, send an HTTP DELETE request to the same `url + '/' + fid` URL:    ```  > curl -X DELETE http://127.0.0.1:8080/3,01637037d6  ```    ### Save File Id ###    Now, you can save the `fid`, 3,01637037d6 in this case, to a database field.    The number 3 at the start represents a volume id. After the comma, it's one file key, 01, and a file cookie, 637037d6.    The volume id is an unsigned 32-bit integer. The file key is an unsigned 64-bit integer. The file cookie is an unsigned 32-bit integer, used to prevent URL guessing.    The file key and file cookie are both coded in hex. You can store the <volume id, file key, file cookie> tuple in your own format, or simply store the `fid` as a string.    If stored as a string, in theory, you would need 8+1+16+8=33 bytes. A char(33) would be enough, if not more than enough, since most uses will not need 2^32 volumes.    If space is really a concern, you can store the file id in your own format. You would need one 4-byte integer for volume id, 8-byte long number for file key, and a 4-byte integer for the file cookie. So 16 bytes are more than enough.    ### Read File ###    Here is an example of how to render the URL.    First look up the volume server's URLs by the file's volumeId:    ```  > curl http://localhost:9333/dir/lookup?volumeId=3  {""volumeId"":""3"",""locations"":[{""publicUrl"":""localhost:8080"",""url"":""localhost:8080""}]}  ```    Since (usually) there are not too many volume servers, and volumes don't move often, you can cache the results most of the time. Depending on the replication type, one volume can have multiple replica locations. Just randomly pick one location to read.    Now you can take the public url, render the url or directly read from the volume server via url:    ```   http://localhost:8080/3,01637037d6.jpg  ```    Notice we add a file extension "".jpg"" here. It's optional and just one way for the client to specify the file content type.    If you want a nicer URL, you can use one of these alternative URL formats:    ```   http://localhost:8080/3/01637037d6/my_preferred_name.jpg   http://localhost:8080/3/01637037d6.jpg   http://localhost:8080/3,01637037d6.jpg   http://localhost:8080/3/01637037d6   http://localhost:8080/3,01637037d6  ```    If you want to get a scaled version of an image, you can add some params:    ```  http://localhost:8080/3/01637037d6.jpg?height=200&width=200  http://localhost:8080/3/01637037d6.jpg?height=200&width=200&mode=fit  http://localhost:8080/3/01637037d6.jpg?height=200&width=200&mode=fill  ```    ### Rack-Aware and Data Center-Aware Replication ###    SeaweedFS applies the replication strategy at a volume level. So, when you are getting a file id, you can specify the replication strategy. For example:    ```  curl http://localhost:9333/dir/assign?replication=001  ```    The replication parameter options are:    ```  000: no replication  001: replicate once on the same rack  010: replicate once on a different rack, but same data center  100: replicate once on a different data center  200: replicate twice on two different data center  110: replicate once on a different rack, and once on a different data center  ```    More details about replication can be found [on the wiki][Replication].    [Replication]: https://github.com/chrislusf/seaweedfs/wiki/Replication    You can also set the default replication strategy when starting the master server.    ### Allocate File Key on Specific Data Center ###    Volume servers can be started with a specific data center name:    ```   weed volume -dir=/tmp/1 -port=8080 -dataCenter=dc1   weed volume -dir=/tmp/2 -port=8081 -dataCenter=dc2  ```    When requesting a file key, an optional ""dataCenter"" parameter can limit the assigned volume to the specific data center. For example, this specifies that the assigned volume should be limited to 'dc1':    ```   http://localhost:9333/dir/assign?dataCenter=dc1  ```    ### Other Features ###    * [No Single Point of Failure][feat-1]    * [Insert with your own keys][feat-2]    * [Chunking large files][feat-3]    * [Collection as a Simple Name Space][feat-4]    [feat-1]: https://github.com/chrislusf/seaweedfs/wiki/Failover-Master-Server  [feat-2]: https://github.com/chrislusf/seaweedfs/wiki/Optimization#insert-with-your-own-keys  [feat-3]: https://github.com/chrislusf/seaweedfs/wiki/Optimization#upload-large-files  [feat-4]: https://github.com/chrislusf/seaweedfs/wiki/Optimization#collection-as-a-simple-name-space    [Back to TOC](#table-of-contents)    ## Object Store Architecture ##    Usually distributed file systems split each file into chunks, a central master keeps a mapping of filenames, chunk indices to chunk handles, and also which chunks each chunk server has.    The main drawback is that the central master can't handle many small files efficiently, and since all read requests need to go through the chunk master, so it might not scale well for many concurrent users.    Instead of managing chunks, SeaweedFS manages data volumes in the master server. Each data volume is 32GB in size, and can hold a lot of files. And each storage node can have many data volumes. So the master node only needs to store the metadata about the volumes, which is a fairly small amount of data and is generally stable.    The actual file metadata is stored in each volume on volume servers. Since each volume server only manages metadata of files on its own disk, with only 16 bytes for each file, all file access can read file metadata just from memory and only needs one disk operation to actually read file data.    For comparison, consider that an xfs inode structure in Linux is 536 bytes.    ### Master Server and Volume Server ###    The architecture is fairly simple. The actual data is stored in volumes on storage nodes. One volume server can have multiple volumes, and can both support read and write access with basic authentication.    All volumes are managed by a master server. The master server contains the volume id to volume server mapping. This is fairly static information, and can be easily cached.    On each write request, the master server also generates a file key, which is a growing 64-bit unsigned integer. Since write requests are not generally as frequent as read requests, one master server should be able to handle the concurrency well.    ### Write and Read files ###    When a client sends a write request, the master server returns (volume id, file key, file cookie, volume node url) for the file. The client then contacts the volume node and POSTs the file content.    When a client needs to read a file based on (volume id, file key, file cookie), it asks the master server by the volume id for the (volume node url, volume node public url), or retrieves this from a cache. Then the client can GET the content, or just render the URL on web pages and let browsers fetch the content.    Please see the example for details on the write-read process.    ### Storage Size ###    In the current implementation, each volume can hold 32 gibibytes (32GiB or 8x2^32 bytes). This is because we align content to 8 bytes. We can easily increase this to 64GiB, or 128GiB, or more, by changing 2 lines of code, at the cost of some wasted padding space due to alignment.    There can be 4 gibibytes (4GiB or 2^32 bytes) of volumes. So the total system size is 8 x 4GiB x 4GiB which is 128 exbibytes (128EiB or 2^67 bytes).    Each individual file size is limited to the volume size.    ### Saving memory ###    All file meta information stored on an volume server is readable from memory without disk access. Each file takes just a 16-byte map entry of <64bit key, 32bit offset, 32bit size>. Of course, each map entry has its own space cost for the map. But usually the disk space runs out before the memory does.    ### Tiered Storage to the cloud ###    The local volume servers are much faster, while cloud storages have elastic capacity and are actually more cost-efficient if not accessed often (usually free to upload, but relatively costly to access). With the append-only structure and O(1) access time, SeaweedFS can take advantage of both local and cloud storage by offloading the warm data to the cloud.    Usually hot data are fresh and warm data are old. SeaweedFS puts the newly created volumes on local servers, and optionally upload the older volumes on the cloud. If the older data are accessed less often, this literally gives you unlimited capacity with limited local servers, and still fast for new data.     With the O(1) access time, the network latency cost is kept at minimum.     If the hot/warm data is split as 20/80, with 20 servers, you can achieve storage capacity of 100 servers. That's a cost saving of 80%! Or you can repurpose the 80 servers to store new data also, and get 5X storage throughput.    [Back to TOC](#table-of-contents)    ## Compared to Other File Systems ##    Most other distributed file systems seem more complicated than necessary.    SeaweedFS is meant to be fast and simple, in both setup and operation. If you do not understand how it works when you reach here, we've failed! Please raise an issue with any questions or update this file with clarifications.    SeaweedFS is constantly moving forward. Same with other systems. These comparisons can be outdated quickly. Please help to keep them updated.    [Back to TOC](#table-of-contents)    ### Compared to HDFS ###    HDFS uses the chunk approach for each file, and is ideal for storing large files.    SeaweedFS is ideal for serving relatively smaller files quickly and concurrently.    SeaweedFS can also store extra large files by splitting them into manageable data chunks, and store the file ids of the data chunks into a meta chunk. This is managed by ""weed upload/download"" tool, and the weed master or volume servers are agnostic about it.    [Back to TOC](#table-of-contents)    ### Compared to GlusterFS, Ceph ###    The architectures are mostly the same. SeaweedFS aims to store and read files fast, with a simple and flat architecture. The main differences are    * SeaweedFS optimizes for small files, ensuring O(1) disk seek operation, and can also handle large files.  * SeaweedFS statically assigns a volume id for a file. Locating file content becomes just a lookup of the volume id, which can be easily cached.  * SeaweedFS Filer metadata store can be any well-known and proven data stores, e.g., Redis, Cassandra, HBase, Mongodb, Elastic Search, MySql, Postgres, Sqlite, MemSql, TiDB, CockroachDB, Etcd etc, and is easy to customized.  * SeaweedFS Volume server also communicates directly with clients via HTTP, supporting range queries, direct uploads, etc.    | System         | File Metadata                   | File Content Read| POSIX  | REST API | Optimized for large number of small files |  | -------------  | ------------------------------- | ---------------- | ------ | -------- | ------------------------- |  | SeaweedFS      | lookup volume id, cacheable     | O(1) disk seek   |        | Yes      | Yes                       |  | SeaweedFS Filer| Linearly Scalable, Customizable | O(1) disk seek   | FUSE   | Yes      | Yes                       |  | GlusterFS      | hashing          |                  | FUSE, NFS          |          |                           |  | Ceph           | hashing + rules  |                  | FUSE               | Yes      |                           |  | MooseFS        | in memory        |                  | FUSE               |       | No                          |  | MinIO          | separate meta file for each file  |                  |         | Yes   | No                          |    [Back to TOC](#table-of-contents)    ### Compared to GlusterFS ###    GlusterFS stores files, both directories and content, in configurable volumes called ""bricks"".    GlusterFS hashes the path and filename into ids, and assigned to virtual volumes, and then mapped to ""bricks"".    [Back to TOC](#table-of-contents)    ### Compared to MooseFS ###    MooseFS chooses to neglect small file issue. From moosefs 3.0 manual, ""even a small file will occupy 64KiB plus additionally 4KiB of checksums and 1KiB for the header"", because it ""was initially designed for keeping large amounts (like several thousands) of very big files""    MooseFS Master Server keeps all meta data in memory. Same issue as HDFS namenode.     [Back to TOC](#table-of-contents)    ### Compared to Ceph ###    Ceph can be setup similar to SeaweedFS as a key->blob store. It is much more complicated, with the need to support layers on top of it. [Here is a more detailed comparison](https://github.com/chrislusf/seaweedfs/issues/120)    SeaweedFS has a centralized master group to look up free volumes, while Ceph uses hashing and metadata servers to locate its objects. Having a centralized master makes it easy to code and manage.    Same as SeaweedFS, Ceph is also based on the object store RADOS. Ceph is rather complicated with mixed reviews.    Ceph uses CRUSH hashing to automatically manage the data placement, which is efficient to locate the data. But the data has to be placed according to the CRUSH algorithm. Any wrong configuration would cause data loss. Topology changes, such as adding new servers to increase capacity, will cause data migration with high IO cost to fit the CRUSH algorithm. SeaweedFS places data by assigning them to any writable volumes. If writes to one volume failed, just pick another volume to write. Adding more volumes are also as simple as it can be.    SeaweedFS is optimized for small files. Small files are stored as one continuous block of content, with at most 8 unused bytes between files. Small file access is O(1) disk read.    SeaweedFS Filer uses off-the-shelf stores, such as MySql, Postgres, Sqlite, Mongodb, Redis, Elastic Search, Cassandra, HBase, MemSql, TiDB, CockroachCB, Etcd, to manage file directories. These stores are proven, scalable, and easier to manage.    | SeaweedFS         | comparable to Ceph | advantage |  | -------------  | ------------- | ---------------- |  | Master  | MDS | simpler |  | Volume  | OSD | optimized for small files |  | Filer  | Ceph FS | linearly scalable, Customizable, O(1) or O(logN) |    [Back to TOC](#table-of-contents)    ### Compared to MinIO ###    MinIO follows AWS S3 closely and is ideal for testing for S3 API. It has good UI, policies, versionings, etc. SeaweedFS is trying to catch up here. It is also possible to put MinIO as a gateway in front of SeaweedFS later.    MinIO metadata are in simple files. Each file write will incur extra writes to corresponding meta file.    MinIO does not have optimization for lots of small files. The files are simply stored as is to local disks.  Plus the extra meta file and shards for erasure coding, it only amplifies the LOSF problem.    MinIO has multiple disk IO to read one file. SeaweedFS has O(1) disk reads, even for erasure coded files.    MinIO has full-time erasure coding. SeaweedFS uses replication on hot data for faster speed and optionally applies erasure coding on warm data.    MinIO does not have POSIX-like API support.    MinIO has specific requirements on storage layout. It is not flexible to adjust capacity. In SeaweedFS, just start one volume server pointing to the master. That's all.    ## Dev Plan ##    * More tools and documentation, on how to manage and scale the system.  * Read and write stream data.  * Support structured data.    This is a super exciting project! And we need helpers and [support](https://www.patreon.com/seaweedfs)!    [Back to TOC](#table-of-contents)    ## Installation Guide ##    > Installation guide for users who are not familiar with golang    Step 1: install go on your machine and setup the environment by following the instructions at:    https://golang.org/doc/install    make sure you set up your $GOPATH      Step 2: checkout this repo:  ```bash  git clone https://github.com/chrislusf/seaweedfs.git  ```  Step 3: download, compile, and install the project by executing the following command    ```bash  cd seaweedfs/weed && make install  ```    Once this is done, you will find the executable ""weed"" in your `$GOPATH/bin` directory    [Back to TOC](#table-of-contents)    ## Disk Related Topics ##    ### Hard Drive Performance ###    When testing read performance on SeaweedFS, it basically becomes a performance test of your hard drive's random read speed. Hard drives usually get 100MB/s~200MB/s.    ### Solid State Disk ###    To modify or delete small files, SSD must delete a whole block at a time, and move content in existing blocks to a new block. SSD is fast when brand new, but will get fragmented over time and you have to garbage collect, compacting blocks. SeaweedFS is friendly to SSD since it is append-only. Deletion and compaction are done on volume level in the background, not slowing reading and not causing fragmentation.    [Back to TOC](#table-of-contents)    ## Benchmark ##    My Own Unscientific Single Machine Results on Mac Book with Solid State Disk, CPU: 1 Intel Core i7 2.6GHz.    Write 1 million 1KB file:  ```  Concurrency Level:      16  Time taken for tests:   66.753 seconds  Complete requests:      1048576  Failed requests:        0  Total transferred:      1106789009 bytes  Requests per second:    15708.23 [#/sec]  Transfer rate:          16191.69 [Kbytes/sec]    Connection Times (ms)                min      avg        max      std  Total:        0.3      1.0       84.3      0.9    Percentage of the requests served within a certain time (ms)     50%      0.8 ms     66%      1.0 ms     75%      1.1 ms     80%      1.2 ms     90%      1.4 ms     95%      1.7 ms     98%      2.1 ms     99%      2.6 ms    100%     84.3 ms  ```    Randomly read 1 million files:  ```  Concurrency Level:      16  Time taken for tests:   22.301 seconds  Complete requests:      1048576  Failed requests:        0  Total transferred:      1106812873 bytes  Requests per second:    47019.38 [#/sec]  Transfer rate:          48467.57 [Kbytes/sec]    Connection Times (ms)                min      avg        max      std  Total:        0.0      0.3       54.1      0.2    Percentage of the requests served within a certain time (ms)     50%      0.3 ms     90%      0.4 ms     98%      0.6 ms     99%      0.7 ms    100%     54.1 ms  ```    [Back to TOC](#table-of-contents)    ## License ##    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at        http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.    The text of this page is available for modification and reuse under the terms of the Creative Commons Attribution-Sharealike 3.0 Unported License and the GNU Free Documentation License (unversioned, with no invariant sections, front-cover texts, or back-cover texts).    [Back to TOC](#table-of-contents)    ## Stargazers over time    [![Stargazers over time](https://starchart.cc/chrislusf/seaweedfs.svg)](https://starchart.cc/chrislusf/seaweedfs)   """
Big data;https://github.com/papertrail/kestrel;"""Kestrel  =======    [![Project status](https://img.shields.io/badge/status-active-green.svg)](#status)    Kestrel is based on Blaine Cook's ""starling"" simple, distributed message  queue, with added features and bulletproofing, as well as the scalability  offered by actors and the JVM.    Each server handles a set of reliable, ordered message queues. When you put a  cluster of these servers together, *with no cross communication*, and pick a  server at random whenever you do a `set` or `get`, you end up with a reliable,  *loosely ordered* message queue.    In many situations, loose ordering is sufficient. Dropping the requirement on  cross communication makes it horizontally scale to infinity and beyond: no  multicast, no clustering, no ""elections"", no coordination at all. No talking!  Shhh!    For more information about what it is and how to use it, check out  the included [guide](https://github.com/robey/kestrel/blob/master/docs/guide.md).    Kestrel has a mailing list here:  [kestrel-talk@googlegroups.com](http://groups.google.com/group/kestrel-talk)    Author's address: Robey Pointer \<robeypointer@gmail.com>      Status  ------    We ([Papertrail](https://papertrailapp.com/)) use Kestrel extensively and are taking on maintaining a fork to fold in bug fixes and small improvements that we find while operating it.      Twitter's Status Statement  --------------------------    We've deprecated Kestrel because internally we've shifted our attention to an alternative project based on DistributedLog, and we no longer have the resources to contribute fixes or accept pull requests. While Kestrel is a great solution up to a certain point (simple, fast, durable, and easy to deploy), it hasn't been able to cope with Twitter's massive scale (in terms of number of tenants, QPS, operability, diversity of workloads etc.) or operating environment (an Aurora cluster without persistent storage).    Features  --------    Kestrel is:    - fast        It runs on the JVM so it can take advantage of the hard work people have      put into java performance.    - small        Currently about 2500 lines of scala, because it relies on Netty (a rough      equivalent of Danger's ziggurat or Ruby's EventMachine) -- and because      Scala is extremely expressive.    - durable        Queues are stored in memory for speed, but logged into a journal on disk      so that servers can be shutdown or moved without losing any data.    - reliable        A client can ask to ""tentatively"" fetch an item from a queue, and if that      client disconnects from kestrel before confirming ownership of the item,      the item is handed to another client. In this way, crashing clients don't      cause lost messages.      Anti-Features  -------------    Kestrel is not:    - strongly ordered        While each queue is strongly ordered on each machine, a cluster will      appear ""loosely ordered"" because clients pick a machine at random for      each operation. The end result should be ""mostly fair"".    - transactional        This is not a database. Item ownership is transferred with acknowledgement,      but kestrel does not support grouping multiple operations into an atomic      unit.      Downloading it  --------------    The latest release is always on the homepage here:    - [http://robey.github.io/kestrel/](http://robey.github.io/kestrel/)    Or the latest development versions & branches are on github:    - [http://gitub.com/robey/kestrel](https://github.com/twitter/kestrel)      Building it  -----------    Kestrel requires java 6 and sbt 0.11.2. Presently some sbt plugins used by kestrel  depend on that exact version of sbt. On OS X 10.5, you may have to hard-code  an annoying `JAVA_HOME` to use java 6:        $ export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home    Building from source is easy:        $ sbt clean update package-dist    Scala libraries and dependencies will be downloaded from maven repositories  the first time you do a build. The finished distribution will be in `dist`.      Running it  ----------    You can run kestrel by hand, in development mode, via:        $ ./dist/kestrel-VERSION/scripts/devel.sh    Like all ostrich-based servers, it uses the ""stage"" property to determine  which config file to load, so `devel.sh` sets `-Dstage=development`.    When running it as a server, a startup script is provided in  `dist/kestrel-VERSION/scripts/kestrel.sh`. The script assumes you have  `daemon`, a standard daemonizer for Linux, but also available  [here](http://libslack.org/daemon/) for all common unix platforms.    The created archive `kestrel-VERSION.zip` can be expanded into a place  like `/usr/local` (or wherever you like) and executed within its own folder as  a self-contained package. All dependent jars are included. The current startup  script, however, assumes that kestrel has been deployed to  `/usr/local/kestrel/current` (e.g., as if by capistrano), and the startup  script loads kestrel from that path.    The default configuration puts logfiles into `/var/log/kestrel/` and queue  journal files into `/var/spool/kestrel/`.    The startup script logs extensive GC information to a file named `stdout` in  the log folder. If kestrel has problems starting up (before it can initialize  logging), it will usually appear in `error` in the same folder.      Configuration  -------------    Queue configuration is described in detail in `docs/guide.md` (an operational  guide). Scala docs for the config variables are  [here](http://robey.github.io/kestrel/api/main/api/net/lag/kestrel/config/KestrelConfig.html).      Performance  -----------    Several performance tests are included. To run them, first start up a kestrel instance  locally.        $ sbt clean update package-dist      $ ./dist/kestrel-VERSION/scripts/devel.sh    ## Put-many    This test just spams a kestrel server with ""put"" operations, to see how  quickly it can absorb and journal them.    A sample run on a 2010 MacBook Pro:        $ ./dist/kestrel/scripts/load/put-many -n 100000      Put 100000 items of 1024 bytes to localhost:22133 in 1 queues named spam        using 100 clients.      Finished in 6137 msec (61.4 usec/put throughput).      Transactions: min=71.00; max=472279.00 472160.00 469075.00;        median=3355.00; average=5494.69 usec      Transactions distribution: 5.00%=485.00 10.00%=1123.00 25.00%=2358.00        50.00%=3355.00 75.00%=4921.00 90.00%=7291.00 95.00%=9729.00        99.00%=50929.00 99.90%=384638.00 99.99%=467899.00    ## Many-clients    This test has one producer that trickles out one item at a time, and a pile of  consumers fighting for each item. It usually takes exactly as long as the  number of items times the delay, but is useful as a validation test to make  sure kestrel works as advertised without blowing up.    A sample run on a 2010 MacBook Pro:        $ ./dist/kestrel/scripts/load/many-clients      many-clients: 100 items to localhost using 100 clients, kill rate 0%,        at 100 msec/item      Received 100 items in 11046 msec.    This test always takes about 11 seconds -- it's a load test instead of a  speed test.    ## Flood    This test starts up one producer and one consumer, and just floods items  through kestrel as fast as it can.    A sample run on a 2010 MacBook Pro:        $ ./dist/kestrel/scripts/load/flood      flood: 1 threads each sending 10000 items of 1kB through spam      Finished in 1563 msec (156.3 usec/put throughput).      Consumer(s) spun 0 times in misses.    ## Packing    This test starts up one producer and one consumer, seeds the queue with a  bunch of items to cause it to fall behind, then does cycles of flooding items  through the queue, separated by pauses. It's meant to test kestrel's behavior  with a queue that's fallen behind and *stays* behind indefinitely, to make  sure the journal files are packed periodically without affecting performance  too badly.    A sample run on a 2010 MacBook Pro:        $ ./dist/kestrel/scripts/load/packing -c 10 -q small      packing: 25000 items of 1kB with 1 second pauses      Wrote 25000 items starting at 0.      cycle: 1      Wrote 25000 items starting at 25000.      Read 25000 items in 5279 msec. Consumer spun 0 times in misses.      cycle: 2      Wrote 25000 items starting at 50000.      Read 25000 items in 4931 msec. Consumer spun 0 times in misses.      ...      cycle: 10      Wrote 25000 items starting at 250000.      Read 25000 items in 5304 msec. Consumer spun 0 times in misses.      Read 25000 items in 3370 msec. Consumer spun 0 times in misses.    You can see the journals being packed in the kestrel log. Like  ""many-clients"", this test is a load test instead of a speed test.    ## Leaky-reader    This test starts a producer and several consumers, with the consumers  occasionally ""forgetting"" to acknowledge an item that they've read. It  verifies that the un-acknowledged items are eventually handed off to another  consmer.    A sample run:        $ ./dist/kestrel/scripts/load/leaky-reader -n 100000 -t 10      leaky-reader: 10 threads each sending 100000 items through spam      Flushing queues first.      1000      2000      100000      Finished in 40220 msec (40.2 usec/put throughput).      Completed all reads    Like ""many-clients"", it's just a load test. """
Big data;https://github.com/WeBankFinTech/DataSphereStudio;"""![DSS](images/en_US/readme/DSS_logo.png)  ====    [![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)    English | [中文](README-ZH.md)    ## Introduction     &nbsp; &nbsp; &nbsp; &nbsp;DataSphere Studio (DSS for short) is WeDataSphere, a one-stop data application development management portal developed by WeBank.     &nbsp; &nbsp; &nbsp; &nbsp;With the pluggable integrated framework design and the Linkis, a computing middleware, DSS can easily integrate  various upper-layer data application systems, making data development simple and easy to use.     &nbsp; &nbsp; &nbsp; &nbsp;DataSphere Studio is positioned as a data application development portal, and the closed loop covers the entire process of data application development. With a unified UI, the workflow-like graphical drag-and-drop development experience meets the entire lifecycle of data application development from data import, desensitization cleaning, data analysis, data mining, quality inspection, visualization, scheduling to data output applications, etc.     &nbsp; &nbsp; &nbsp; &nbsp;With the connection, reusability, and simplification capabilities of Linkis, DSS is born with financial-grade capabilities of high concurrency, high availability, multi-tenant isolation, and resource management.    ## UI preview     &nbsp; &nbsp; &nbsp; &nbsp;Please be patient, it will take some time to load gif.    ![DSS-V1.0 GIF](images/en_US/readme/DSS_gif.gif)    ## Core features    ### 1. One-stop, full-process application development management UI     &nbsp; &nbsp; &nbsp; &nbsp;DSS is highly integrated. Currently integrated components include(**DSS version compatibility for the above components, please visit: [Compatibility list of integrated components](README.md#4-integrated-data-application-components)**):     &nbsp; &nbsp; &nbsp; &nbsp;1. Data Development IDE Tool - [Scriptis](https://github.com/WeBankFinTech/Scriptis)     &nbsp; &nbsp; &nbsp; &nbsp;2. Data Visualization Tool - [Visualis](https://github.com/WeBankFinTech/Visualis) (Based on the open source project [Davinci](https://github.com/edp963/davinci ) contributed by CreditEase)     &nbsp; &nbsp; &nbsp; &nbsp;3. Data Quality Management Tool - [Qualitis](https://github.com/WeBankFinTech/Qualitis)     &nbsp; &nbsp; &nbsp; &nbsp;4. Workflow scheduling tool - [Schedulis](https://github.com/WeBankFinTech/Schedulis)     &nbsp; &nbsp; &nbsp; &nbsp;5. Data Exchange Tool - [Exchangis](https://github.com/WeBankFinTech/Exchangis) (**The upcoming Exchangis1.0 will be integrated with the DSS workflow**)     &nbsp; &nbsp; &nbsp; &nbsp;6. Data Api Service - [DataApiService](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Using_Document/DataApiService_Usage_Documentation.md)     &nbsp; &nbsp; &nbsp; &nbsp;7. Streaming Application Development Management Tool - [Streamis](https://github.com/WeBankFinTech/Streamis)     &nbsp; &nbsp; &nbsp; &nbsp;8. One-stop machine Learning Platform - [Prophecis](https://github.com/WeBankFinTech/Prophecis) (**Integrated version will be released soon**)     &nbsp; &nbsp; &nbsp; &nbsp;9. Workflow Task Scheduling Tool - DolphinScheduler (**In Code Merging**)      &nbsp; &nbsp; &nbsp; &nbsp;10. Help documentation and beginner's guide - UserGuide (**In Code Merging**)     &nbsp; &nbsp; &nbsp; &nbsp;11. Data Model Center - DataModelCenter (**In development**)      &nbsp; &nbsp; &nbsp; &nbsp;**DSS version compatibility for the above components, please visit: [Compatibility list of integrated components](README.md#4-integrated-data-application-components)**.     &nbsp; &nbsp; &nbsp; &nbsp;With a pluggable framework architecture, DSS is designed to allow users to quickly integrate new data application tools, or replace various tools that DSS has integrated. For example, replace Scriptis with Zeppelin, and replace Schedulis with DolphinScheduler...    ![DSS one-stop video](images/en_US/readme/onestop.gif)     ### 2. AppConn, based on Linkis，defines a unique design concept     &nbsp; &nbsp; &nbsp; &nbsp;AppConn is the core concept that enables DSS to easily and quickly integrate various upper-layer web systems.     &nbsp; &nbsp; &nbsp; &nbsp;AppConn, an application connector, defines a set of unified front-end and back-end three-level integration protocols, allowing external data application systems to easily and quickly becoming a part of DSS data application development.      &nbsp; &nbsp; &nbsp; &nbsp;The three-level specifications of AppConn are: the first-level SSO specification, the second-level organizational structure specification, and the third-level development process specification.     &nbsp; &nbsp; &nbsp; &nbsp;DSS arranges multiple AppConns in series to form a workflow that supports real-time execution and scheduled execution. Users can complete the entire process development of data applications with simple drag and drop operations.     &nbsp; &nbsp; &nbsp; &nbsp;Since AppConn is integrated with Linkis, the external data application system shares the capabilities of resource management, concurrent limiting, and high performance. AppConn also allows sharable context across system level and thus makes external data application completely gets away from application silos.    ### 3. Workspace, as the management unit     &nbsp; &nbsp; &nbsp; &nbsp;With Workspace as the management unit, it organizes and manages business applications of various data application systems, defines a set of common standards for collaborative development of workspaces across data application systems, and provides user role management capabilities.    ### 4. Integrated data application components     &nbsp; &nbsp; &nbsp; &nbsp;DSS has integrated a variety of upper-layer data application systems by implementing multiple AppConns, which can basically meet the data development needs of users.     &nbsp; &nbsp; &nbsp; &nbsp;**If desired, new data application systems can also be easily integrated to replace or enrich DSS's data application development process.** [Click me to learn how to quickly integrate new application systems](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Development_Documentation/Third-party_System_Access_Development_Guide.md)    | Component | Description | DSS0.X Compatibility(recommend DSS0.9.1) | DSS1.0 Compatibility(recommend DSS1.0.1) |  | --------------- | -------------------------------------------------------------------- | --------- | ---------- |  | [**Linkis**](https://github.com/apache/incubator-linkis) | Apache Linkis, builds a layer of computation middleware, by using standard interfaces such as REST/WS/JDBC provided by Linkis, the upper applications can easily access the underlying engines such as MySQL/Spark/Hive/Presto/Flink, etc.  | recommend Linkis0.11.0 (**Released**) | recommend Linkis1.0.3 (**Released**)|  | [**DataApiService**](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Using_Document/DataApiService_Usage_Documentation.md) | (Third-party applications built into DSS) Data API service. The SQL script can be quickly published as a Restful interface, providing Rest access capability to the outside world. | Not supported | recommend DSS1.0.1 (**Released**) |  | [**Scriptis**](https://github.com/WeBankFinTech/DataSphereStudio) | (Third-party applications built into DSS) Support online script writing such as SQL, Pyspark, HiveQL, etc., submit to [Linkis](https://github.com/WeBankFinTech/Linkis ) to perform data analysis web tools. | recommend DSS0.9.1 (**Released**) | recommend DSS1.0.1 (**Released**) |  | [**Schedulis**](https://github.com/WeBankFinTech/Schedulis) | Workflow task scheduling system based on Azkaban secondary development, with financial-grade features such as high performance, high availability and multi-tenant resource isolation. | recommend Schedulis0.6.1 (**Released**) | >= Schedulis0.6.2 (**Developing**) |  | **EventCheck** | (Third-party applications built into DSS) Provides cross-business, cross-engineering, and cross-workflow signaling capabilities. | recommend DSS0.9.1 (**Released**) | recommend DSS1.0.1 (**Released**) |  | **SendEmail** | (Third-party applications built into DSS) Provides the ability to send data, all the result sets of other workflow nodes can be sent by email | recommend 0.9.1 (**Released**) | recommend 1.0.1 (**Released**) |  | [**Qualitis**](https://github.com/WeBankFinTech/Qualitis) | Data quality verification tool, providing data verification capabilities such as data integrity and correctness | recommend Qualitis0.8.0 (**Released**) | >= Qualitis0.9.0 (**Released**) |  | [**Streamis**](https://github.com/WeBankFinTech/Streamis) | Streaming application development management tool. It supports the release of Flink Jar and Flink SQL, and provides the development, debugging and production management capabilities of streaming applications, such as: start-stop, status monitoring, checkpoint, etc. | Not supported | >= Streamis0.1.0 (**Released**) |  | [**Exchangis**](https://github.com/WeBankFinTech/Exchangis) | A data exchange platform that supports data transmission between structured and unstructured heterogeneous data sources, the upcoming Exchangis1. 0, will be connected with DSS workflow | not supported | >= Exchangis1.0.0 (**Developing**) |  | [**Visualis**](https://github.com/WeBankFinTech/Visualis) | A data visualization BI tool based on the second development of Davinci, an open source project of CreditEase, provides users with financial-level data visualization capabilities in terms of data security. | recommend Visualis0.5.0 (**Released**) | >= Visualis1.0.0 (**Developing**) |  | [**Prophecis**](https://github.com/WeBankFinTech/Prophecis) | A one-stop machine learning platform that integrates multiple open source machine learning frameworks. Prophecis' MLFlow can be connected to DSS workflow through AppConn. | Not supported | >= Prophecis 0.3.0 (**Developing**) |  | **UserManager** | (Third-party applications built into DSS) Automatically initialize all user environments necessary for a new DSS user, including: creating Linux users, various user paths, directory authorization, etc. | recommend DSS0.9.1 (**Released**) | Planned in DSS1.0.2 (**Developing**) |  | [**DolphinScheduler**](https://github.com/apache/dolphinscheduler) | Apache DolphinScheduler, a distributed and scalable visual workflow task scheduling platform, supports one-click publishing of DSS workflows to DolphinScheduler. | Not supported | >= DolphinScheduler1.3.6, planned in DSS1.1.0 (**Developing**) |  | **UserGuide**     | (Third-party applications to be built into DSS) It mainly provides help documentation, beginner's guide, Dark mode skinning, etc.      | Not supported | Planning in DSS1.1.0 (**Developing**) |  | **DataModelCenter** | (Third-party applications to be built into DSS) It mainly provides the capabilities of data warehouse planning, data model development and data asset management. Data warehouse planning includes subject domains, data warehouse layers, modifiers, etc.; data model development includes indicators, dimensions, metrics, wizard-based table building, etc.; data assets are connected to Apache Atlas to provide data lineage capabilities. | Not supported | Planning in DSS1.2.0 (**Developing**) |  | [**Airflow**](https://github.com/apache/airflow) | Supports publishing DSS workflows to Apache Airflow for scheduling. | recommend DSS0.9.1, not yet merged | Not supported |      ## Demo Trial environment     &nbsp; &nbsp; &nbsp; &nbsp;The function of DataSphere Studio supporting script execution has high security risks, and the isolation of the WeDataSphere Demo environment has not been completed. Considering that many users are inquiring about the Demo environment, we decided to first issue invitation codes to the community and accept trial applications from enterprises and organizations.     &nbsp; &nbsp; &nbsp; &nbsp;If you want to try out the Demo environment, please join the DataSphere Studio community user group (**Please refer to the end of the document**), and contact **WeDataSphere Group Robot** to get an invitation code.     &nbsp; &nbsp; &nbsp; &nbsp;DataSphereStudio Demo environment user registration page: [click me to enter](https://dss-open.wedatasphere.com/#/register)     &nbsp; &nbsp; &nbsp; &nbsp;DataSphereStudio Demo environment login page: [click me to enter](https://dss-open.wedatasphere.com/#/login)    ##  Download     &nbsp; &nbsp; &nbsp; &nbsp;Please go to the [DSS Releases Page](https://github.com/WeBankFinTech/DataSphereStudio/releases) to download a compiled version or a source code package of DSS.    ## Compile and deploy     &nbsp; &nbsp; &nbsp; &nbsp;Please follow [Compile Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Development_Documentation/Compilation_Documentation.md) to compile DSS from source code.     &nbsp; &nbsp; &nbsp; &nbsp;Please refer to [Deployment Documents](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/DSS_Single-Server_Deployment_Documentation.md) to do the deployment.    ## Examples and Guidance     &nbsp; &nbsp; &nbsp; &nbsp;You can find examples and guidance for how to use DSS in [User Manual](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Using_Document/DSS_User_Manual.md).      ## Documents     &nbsp; &nbsp; &nbsp; &nbsp;For a complete list of documents for DSS1.0, see [DSS-Doc](https://github.com/WeBankFinTech/DataSphereStudio-Doc)     &nbsp; &nbsp; &nbsp; &nbsp;The following is the installation guide for DSS-related AppConn plugins:    - [Visualis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/VisualisAppConn_Plugin_Installation_Documentation.md)    - [Schedulis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/SchedulisAppConn_Plugin_Installation_Documentation.md)    - [Qualitis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/QualitisAppConn_Plugin_Installation_Documentation.md)    - [Exchangis AppConn Plugin Installation Guide](https://github.com/WeBankFinTech/DataSphereStudio-Doc/blob/main/en_US/Installation_and_Deployment/ExchangisAppConn_Plugin_Installation_Documentation.md)    ## Architecture    ![DSS Architecture](images/en_US/readme/architecture.png)    ## Usage Scenarios     &nbsp; &nbsp;&nbsp; &nbsp;DataSphere Studio is suitable for the following scenarios:     &nbsp; &nbsp;&nbsp; &nbsp;1. Scenarios in which big data platform capability is being prepared or initialized but no data application tools are available.     &nbsp; &nbsp;&nbsp; &nbsp;2. Scenarios in which users already have big data foundation platform capabilities but with only a few data application tools.     &nbsp; &nbsp;&nbsp; &nbsp;3. Scenarios in which users have the ability of big data foundation platform and comprehensive data application tools, but suffers strong isolation and and high learning costs because those tools have not been integrated together.     &nbsp; &nbsp;&nbsp; &nbsp;4. Scenarios in which users have the capabilities of big data foundation platform and comprehensive data application tools. but lacks unified and standardized specifications, while a part of these tools have been integrated.    ## Contributing     &nbsp; &nbsp; &nbsp; &nbsp;Contributions are always welcomed, we need more contributors to build DSS together. either code, or doc, or other supports that could help the community.     &nbsp; &nbsp; &nbsp; &nbsp;For code and documentation contributions, please follow the contribution guide.    ## Communication     &nbsp; &nbsp; &nbsp; &nbsp;For any questions or suggestions, please kindly submit an issue.     &nbsp; &nbsp; &nbsp; &nbsp;You can scan the QR code below to join our WeChat and QQ group to get more immediate response.    ![communication](images/en_US/readme/communication.png)    ## Who is using DSS     &nbsp; &nbsp; &nbsp; &nbsp;We opened an issue for users to feedback and record who is using DSS.     &nbsp; &nbsp; &nbsp; &nbsp;Since the first release of DSS in 2019, it has accumulated more than 700 trial companies and 1000+ sandbox trial users, which involving diverse industries, from finance, banking, tele-communication, to manufactory, internet companies and so on.    ## License     &nbsp; &nbsp; &nbsp; &nbsp;DSS is under the Apache 2.0 license. See the [License](LICENSE) file for details. """
Big data;https://github.com/ottogroup/schedoscope;"""*Schedoscope is no longer under development by OttoGroup. Feel free to fork!*    # ![Schedoscope](https://raw.githubusercontent.com/wiki/ottogroup/schedoscope/images/schedoscope_logo.jpg)    ## Introduction    Schedoscope is a scheduling framework for painfree agile development, testing, (re)loading, and monitoring of your datahub, datalake, or whatever you choose to call your Hadoop data warehouse these days.    Schedoscope makes the headache go away you are certainly going to get when having to frequently rollout and retroactively apply changes to computation logic and data structures in your datahub with traditional ETL job schedulers such as Oozie.    With Schedoscope,  * you never have to create DDL and schema migration scripts;  * you do not have to manually determine which data must be deleted and recomputed in face of retroactive changes to logic or data structures;  * you specify Hive table structures (called ""views""), partitioning schemes, storage formats, dependent views, as well as transformation logic in a concise Scala DSL;  * you have a wide range of options for expressing data transformations - from file operations and MapReduce jobs to Pig scripts, Hive queries, Spark jobs, and Oozie workflows;  * you benefit from Scala's static type system and your IDE's code completion to make less typos that hit you late during deployment or runtime;  * you can easily write unit tests for your transformation logic in [ScalaTest](http://www.scalatest.org/) and run them quickly right out of your IDE;  * you schedule jobs by expressing the views you need - Schedoscope takes care that all required dependencies - and only those-  are computed as well;  * you can easily  export view data in parallel to external systems such as Redis caches, JDBC, or Kafka topics;  * you have Metascope - a nice metadata management and data lineage tracing tool - at your disposal;  * you achieve a higher utilization of your YARN cluster's resources because job launchers are not YARN applications themselves that consume cluster capacitity.     ## Getting Started    Get a glance at     - [Schedoscope's features](https://github.com/ottogroup/schedoscope/wiki/Schedoscope-at-a-Glance)    Build it:         [~]$ git clone https://github.com/ottogroup/schedoscope.git       [~]$ cd schedoscope       [~/schedoscope]$  MAVEN_OPTS='-XX:MaxPermSize=512m -Xmx1G' mvn clean install         Follow the Open Street Map tutorial to install and run Schedoscope in a standard Hadoop distribution image:    - [Open Street Map Tutorial](https://github.com/ottogroup/schedoscope/wiki/Open%20Street%20Map%20Tutorial)    Take a look at the View DSL Primer to get more information about the capabilities of the Schedoscope DSL:    - [Schedoscope View DSL Primer](https://github.com/ottogroup/schedoscope/wiki/Schedoscope%20View%20DSL%20Primer)    Read more about how Schedoscope actually performs its scheduling work:    - [Schedoscope Scheduling](https://github.com/ottogroup/schedoscope/wiki/Scheduling)    More documentation can be found here:    - [Schedoscope Wiki](https://github.com/ottogroup/schedoscope/wiki)    Check out Metascope! It's an add-on to Schedoscope for collaborative metadata management, data discovery, exploration, and data lineage tracing:    - [Metascope Primer](https://github.com/ottogroup/schedoscope/wiki/Metascope%20Primer)    ![Metascope](https://raw.githubusercontent.com/wiki/ottogroup/schedoscope/images/view-lineage.png)    ## When is Schedoscope not for you?    Schedoscope is based on the following assumptions:  * data are largely relational and meaningfully representable as Hive tables;  * there is enough cluster time and capacity to actually allow for retroactive recomputation of data;  * it is acceptable to compile table structures, dependencies, and transformation logic into what is effectively a project-specific scheduler;  * it is acceptable that your scheduler creates and possibly drops Hive tables and databases it manages.    Should any of those assumptions not hold in your context, you should probably look for a different scheduler.    ## Origins    Schedoscope was conceived at the Business Intelligence department of [Otto Group](http://www.ottogroup.com/en/die-otto-group/)    ## Contributions    The following people have contributed to the various parts of Schedoscope so far:     [Utz Westermann](https://github.com/utzwestermann) (maintainer), [Kassem Tohme](https://github.com/ktohme), [Alexander Kolb](https://github.com/lofifnc), [Christian Richter](https://github.com/christianrichter), [Jan Hicken](https://github.com/janhicken), [Diogo Aurelio](https://github.com/diogoaurelio), [Hans-Peter Zorn](https://github.com/hpzorn), [Dominik Benz](https://github.com/dominikbenz), [Annika Seidler](https://github.com/aleveringhaus), [Martin Sänger](https://github.com/martinsaenger), [Julian Keppel](https://github.com/juliankeppel).    We would love to get contributions from you as well. We haven't got a formalized submission process yet. If you have an idea for a contribution or even coded one already, get in touch with Utz or just send us your pull request. We will work it out from there.    Please help making Schedoscope better!    ## News    ###### 05/08/2018 - Release 0.10.2    We have released Version 0.10.2 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    We have changed the materialization logic of `materializeOnce` views such that they no longer ask their child views to materialize if the `materializeOnce` views have been materialized already. This improves performance.    ###### 04/24/2018 - Release 0.10.1    We have released Version 0.10.1 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This is a bugfix release correcting the order of the TBLPROPERTIES and LOCATION clauses in the Hive DDL generated for views. **Please do note that if you use the `tblProperties` clause in some views, this change affects the DDL checksum making Schedoscope drop and recreate the respective tables.** Hence the version bump to 0.10.1.    Thanks to Julian Keppel for reporting the issue and providing the fix.    ###### 04/06/2018 - Release 0.9.13    We have released Version 0.9.13 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Removed derelict indirect CDH5.12.0 dependencies incurred by Cloudera's Spark 2.2.0-Cloudera2 dependency.    ###### 03/15/2018 - Release 0.9.11    We have released Version 0.9.11 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Added [configuration parameter](https://github.com/ottogroup/schedoscope/wiki/Configuring-Schedoscope) `schedoscope.export.disableAll` to globally disable all view exports. Useful in test environments.    ###### 03/09/2018 - Release 0.9.10    We have released Version 0.9.10 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Upgraded Cloudera dependencies to CDH 5.14.0.    ###### 01/25/2018 - Release 0.9.9    We have released Version 0.9.9 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Export your views to Google Cloud Platform's BigQuery via a simple [`exportAs()`](https://github.com/ottogroup/schedoscope/wiki/Google-BigQuery-Export) statement.    BigQuery export now compresses view data before sending it off to Google Cloud Storage.    ###### 01/24/2018 - Release 0.9.7    We have released Version 0.9.7 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Optimized performance of BigQuery export by moving more work to the map phase of the export job.     ###### 01/23/2018 - Release 0.9.6    We have released Version 0.9.6 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Corrected a problem with command line argument construction within BigQuery `exportAs()` clauses in a Kerberized cluster.    ###### 01/22/2018 - Release 0.9.5    We have released Version 0.9.5 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Export your views to Google Cloud Platform's BigQuery via a simple [`exportAs()`](https://github.com/ottogroup/schedoscope/wiki/Google-BigQuery-Export) statement.    ###### 10/12/2017 - Release 0.9.4    We have released Version 0.9.4 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Emergency bug fix for Schedoscope crashing upon exports. Do not use 0.9.3!    ###### 10/11/2017 - Release 0.9.3    We have released Version 0.9.3 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Minor bug fix. Show view name in resource manager also for transformations of views that have exportAs statements.    ###### 09/21/2017 - Release 0.9.2    We have released Version 0.9.2 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Minor bug fixes. Improved Metascope performance by optionally circumventing the Hive Metastore API and accessing the Metastore DB directly.    ###### 08/17/2017 - Release 0.9.1    We have released Version 0.9.1 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    We fixed a bug in the Spark driver that could lead to incomplete consumption of the error stream of the Spark submit subprocess resulting in transformation freezes.      ###### 08/11/2017 - Release 0.9.0    We have released Version 0.9.0 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This release upgrades Spark transformations from Spark version 1.6.0 to Spark version 2.2.0 based on Cloudera's CDH 5.12 Spark 2.2 beta parcel. As a consequence, Schedoscope has been lifted to Scala 2.11 and JDK8 as well.     This is an incompatible change likely requiring adaptation of Spark jobs, dependencies, and build pipelines of existing Schedoscope projects - hence the incrememtation of the minor release number.    ###### 08/04/2017 - Release 0.8.9    We have released Version 0.8.9 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This release contains the following enhancements and changes:  * Cloudera client libraries updated to CDH-5.12.0;  * a [DistCp transformation](https://github.com/ottogroup/schedoscope/wiki/DistCp-Transformations) for view materialization by parallel, cross-cluser file copying;  * a new [development mode](https://github.com/ottogroup/schedoscope/wiki/Development-Mode) setup that helps developers to easily copy data from a production environment to the direct dependencies of the view they are developing;  * shell transformations had to be moved back into `schedoscope-core` to facilitate development mode;  * a versioning issue with the Scala Maven compiler plugin with regard to Scala 2.10 was fixed so that finally Schedoscope compiles and runs under JDK8 as well.    ###### 07/04/2017 - Release 0.8.7    We have released Version 0.8.7 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version contains a critical Metascope bugfix introduced with the last version preventing startup. Also, finally Metascope field lineage documentation has been provided in the [View DSL Primer](https://github.com/ottogroup/schedoscope/wiki/Schedoscope-View-DSL-Primer) and the [Metascope Primer](https://github.com/ottogroup/schedoscope/wiki/Metascope-Primer).    ###### 06/23/2017 - Release 0.8.6    We have released Version 0.8.6 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version includes support for field level data lineage - automatically inferred from Hive transformations, declaratively specifyable for other transformations - in Metascope. Also, Metascope lineage graph rendering has been reworked. Extensive documentation to come.    Schedoscope now fails immediately if a driver specified in schedoscope.conf cannot be found on the classpath.    ###### 05/26/2017 - Release 0.8.5    We have released Version 0.8.5 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version adds support for float view fields to JDBC exports    ###### 05/24/2017 - Release 0.8.4    We have released Version 0.8.4 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version removes a race condition the file system driver initialization that seems to have been introduced with CDH-5.10. Also, we have changed the way how we delete and recreate output folders for Map/Reduce transformations to avoid Hive partitions pointing to temporarily non-existing folders.    ###### 04/24/2017 - Release 0.8.3    We have released Version 0.8.3 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version has been built against Cloudera's CDH 5.10.1 client libraries. The test framework no longer artificially sets the storage formats of views under test to text, making testing of Spark jobs writing Parquet files simpler. The robustness of the Schedoscope HTTP service has been improved in face of invalid view parameters.    ###### 03/24/2017 - Release 0.8.2    We have released Version 0.8.2 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This version provides significant performance improvements when initializing the scheduling state for a large number of views.    ###### 03/18/2017 - Release 0.8.1    We have released Version 0.8.1 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    This fixes a critical bug that could result in applying commands to all views in a table and not just the ones addressed. *Do not use Release 0.8.0*    ###### 03/17/2017 - Release 0.8.0      We have released Version 0.8.0 as a Maven artifact to our Bintray repository (see [Setting Up A Schedoscope Project](https://github.com/ottogroup/schedoscope/wiki/Setting-up-a-Schedoscope-Project) for an example pom).    Schedoscope 0.8.0 includes, among other things:    * significant rework of Schedoscope's actor system that supports testing and uses significantly fewer actors reducing stress for poor Akka;  * support for a lot more [Hive storage formats](https://github.com/ottogroup/schedoscope/wiki/Storage-Formats);  * definition of arbitrary [Hive table properties / SerDes](https://github.com/ottogroup/schedoscope/wiki/Storage-Formats);  * stability, performance, and UI improvements to Metascope;  * the names of views being transformed appear as the job name in the Hadoop resource manager.    Please note that Metascope's database schema has changed with this release, so back up your database before deploying.    ## Community / Forums    - Google Groups: [Schedoscope Users](https://groups.google.com/forum/#!forum/schedoscope-users)  - Twitter: @Schedoscope    ## Build Status    [![Build Status](https://travis-ci.org/ottogroup/schedoscope.svg?branch=master)](https://travis-ci.org/ottogroup/schedoscope)    ## License  Licensed under the [Apache License 2.0](https://github.com/ottogroup/schedoscope/blob/master/LICENSE) """
Big data;https://github.com/benedekrozemberczki/karateclub;"""   ![Version](https://badge.fury.io/py/karateclub.svg?style=plastic)   ![License](https://img.shields.io/github/license/benedekrozemberczki/karateclub.svg)  [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/karateclub.svg)](https://github.com/benedekrozemberczki/karateclub/archive/master.zip)   [![Arxiv](https://img.shields.io/badge/ArXiv-2003.04819-orange.svg)](https://arxiv.org/abs/2003.04819)  [![build badge](https://github.com/benedekrozemberczki/karateclub/workflows/CI/badge.svg)](https://github.com/benedekrozemberczki/karateclub/actions?query=workflow%3ACI)   [![coverage badge](https://codecov.io/gh/benedekrozemberczki/karateclub/branch/master/graph/badge.svg)](https://codecov.io/github/benedekrozemberczki/karateclub?branch=master)  <p align=""center"">    <img width=""90%"" src=""https://github.com/benedekrozemberczki/karateclub/blob/master/karatelogo.jpg?sanitize=true"" />  </p>    ----------------------------------------------------------      **Karate Club** is an unsupervised machine learning extension library for [NetworkX](https://networkx.github.io/).      Please look at the **[Documentation](https://karateclub.readthedocs.io/)**, relevant **[Paper](https://arxiv.org/abs/2003.04819)**, **[Promo Video](https://www.youtube.com/watch?v=t212-ntxu2U)**, and **[External Resources](https://karateclub.readthedocs.io/en/latest/notes/resources.html)**.    *Karate Club* consists of state-of-the-art methods to do unsupervised learning on graph structured data. To put it simply it is a Swiss Army knife for small-scale graph mining research. First, it provides network embedding techniques at the node and graph level. Second, it includes a variety of overlapping and non-overlapping community detection methods. Implemented methods cover a wide range of network science ([NetSci](https://netscisociety.net/home), [Complenet](https://complenet.weebly.com/)), data mining ([ICDM](http://icdm2019.bigke.org/), [CIKM](http://www.cikm2019.net/), [KDD](https://www.kdd.org/kdd2020/)), artificial intelligence ([AAAI](http://www.aaai.org/Conferences/conferences.php), [IJCAI](https://www.ijcai.org/)) and machine learning ([NeurIPS](https://nips.cc/), [ICML](https://icml.cc/), [ICLR](https://iclr.cc/)) conferences, workshops, and pieces from prominent journals.    The newly introduced graph classification datasets are available at [SNAP](https://snap.stanford.edu/data/#disjointgraphs), [TUD Graph Kernel Datasets](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets), and [GraphLearning.io](https://chrsmrrs.github.io/datasets/).    --------------------------------------------------------------    **Citing**    If you find *Karate Club* and the new datasets useful in your research, please consider citing the following paper:    ```bibtex  @inproceedings{karateclub,         title = {{Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs}},         author = {Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},         year = {2020},         pages = {3125–3132},         booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},         organization = {ACM},  }  ```  ----------------------------------------------------------------    **A simple example**    *Karate Club* makes the use of modern community detection techniques quite easy (see [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial). For example, this is all it takes to use on a Watts-Strogatz graph [Ego-splitting](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf):    ```python  import networkx as nx  from karateclub import EgoNetSplitter    g = nx.newman_watts_strogatz_graph(1000, 20, 0.05)    splitter = EgoNetSplitter(1.0)    splitter.fit(g)    print(splitter.get_memberships())  ```    ----------------------------------------------------------------    **Models included**    In detail, the following community detection and embedding methods were implemented.    **Overlapping Community Detection**    * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)    * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)    * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)    * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)    * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)    * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)    **Non-Overlapping Community Detection**    * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)    * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)    * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)    * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)      **Proximity Preserving Node Embedding**    * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?id=2806512) (CIKM 2015)    * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)    * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)    * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)    * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)    * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)    * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)    * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)    * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)    * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)    * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip! Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)    * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)    * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and Févotte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)    * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)     **Structural Node Level Embedding**    * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)    * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)    **Attributed Node Level Embedding**    * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)    * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)    * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)    * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)     * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)    * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)    * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)    * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)    * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)     **Meta Node Embedding**    * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)    **Graph Level Embedding**    * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)    * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)    * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)    * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)    * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)    * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)    * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)    * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)    * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)     * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)    Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets. For a quick start, check out our [examples](https://github.com/benedekrozemberczki/karateclub/tree/master/examples.py).    If you notice anything unexpected, please open an [issue](https://github.com/benedekrozemberczki/karateclub/issues) and let us know. If you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/karateclub/issues).  We are motivated to constantly make Karate Club even better.      --------------------------------------------------------------------------------    **Installation**    Karate Club can be installed with the following pip command.    ```sh  $ pip install karateclub  ```    As we create new releases frequently, upgrading the package casually might be beneficial.    ```sh  $ pip install karateclub --upgrade  ```    --------------------------------------------------------------------------------    **Running examples**    As part of the documentation we provide a number of use cases to show how the clusterings and embeddings can be utilized for downstream learning. These can accessed [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) with detailed line-by-line explanations.      Besides the case studies we provide synthetic examples for each model. These can be tried out by running the example scripts. In order to run one of the examples, the Graph2Vec snippet:    ```sh  $ cd examples/whole_graph_embedding/  $ python graph2vec_example.py  ```    --------------------------------------------------------------------------------    **Running tests**    ```sh  $ python setup.py test  ```    --------------------------------------------------------------------------------    **License**    - [GNU General Public License v3.0](https://github.com/benedekrozemberczki/karateclub/blob/master/LICENSE) """
Big data;https://github.com/amplab/velox-modelserver;"""velox-modelserver  =================    Serving machine learning models """
Big data;https://github.com/twitter/twemproxy;"""# twemproxy (nutcracker) [![Build Status](https://github.com/twitter/twemproxy/actions/workflows/main.yml/badge.svg?branch=master)](https://github.com/twitter/twemproxy/actions/workflows/main.yml?query=branch%3Amaster)    **twemproxy** (pronounced ""two-em-proxy""), aka **nutcracker** is a fast and lightweight proxy for [memcached](http://www.memcached.org/) and [redis](http://redis.io/) protocol. It was built primarily to reduce the number of connections to the caching servers on the backend. This, together with protocol pipelining and sharding enables you to horizontally scale your distributed caching architecture.    ## Build    To build twemproxy 0.5.0+ from [distribution tarball](https://github.com/twitter/twemproxy/releases):        $ ./configure      $ make      $ sudo make install    To build twemproxy 0.5.0+ from [distribution tarball](https://github.com/twitter/twemproxy/releases) in _debug mode_:        $ CFLAGS=""-ggdb3 -O0"" ./configure --enable-debug=full      $ make      $ sudo make install    To build twemproxy from source with _debug logs enabled_ and _assertions enabled_:        $ git clone git@github.com:twitter/twemproxy.git      $ cd twemproxy      $ autoreconf -fvi      $ ./configure --enable-debug=full      $ make      $ src/nutcracker -h    A quick checklist:    + Use newer version of gcc (older version of gcc has problems)  + Use CFLAGS=""-O1"" ./configure && make  + Use CFLAGS=""-O3 -fno-strict-aliasing"" ./configure && make  + `autoreconf -fvi && ./configure` needs `automake` and `libtool` to be installed    `make check` will run unit tests.    ### Older Releases    Distribution tarballs for older twemproxy releases (<= 0.4.1) can be found on [Google Drive](https://drive.google.com/open?id=0B6pVMMV5F5dfMUdJV25abllhUWM&authuser=0).  The build steps are the same (`./configure; make; sudo make install`).    ## Features    + Fast.  + Lightweight.  + Maintains persistent server connections.  + Keeps connection count on the backend caching servers low.  + Enables pipelining of requests and responses.  + Supports proxying to multiple servers.  + Supports multiple server pools simultaneously.  + Shard data automatically across multiple servers.  + Implements the complete [memcached ascii](notes/memcache.md) and [redis](notes/redis.md) protocol.  + Easy configuration of server pools through a YAML file.  + Supports multiple hashing modes including consistent hashing and distribution.  + Can be configured to disable nodes on failures.  + Observability via stats exposed on the stats monitoring port.  + Works with Linux, *BSD, OS X and SmartOS (Solaris)    ## Help        Usage: nutcracker [-?hVdDt] [-v verbosity level] [-o output file]                        [-c conf file] [-s stats port] [-a stats addr]                        [-i stats interval] [-p pid file] [-m mbuf size]        Options:        -h, --help             : this help        -V, --version          : show version and exit        -t, --test-conf        : test configuration for syntax errors and exit        -d, --daemonize        : run as a daemon        -D, --describe-stats   : print stats description and exit        -v, --verbose=N        : set logging level (default: 5, min: 0, max: 11)        -o, --output=S         : set logging file (default: stderr)        -c, --conf-file=S      : set configuration file (default: conf/nutcracker.yml)        -s, --stats-port=N     : set stats monitoring port (default: 22222)        -a, --stats-addr=S     : set stats monitoring ip (default: 0.0.0.0)        -i, --stats-interval=N : set stats aggregation interval in msec (default: 30000 msec)        -p, --pid-file=S       : set pid file (default: off)        -m, --mbuf-size=N      : set size of mbuf chunk in bytes (default: 16384 bytes)    ## Zero Copy    In twemproxy, all the memory for incoming requests and outgoing responses is allocated in mbuf. Mbuf enables zero-copy because the same buffer on which a request was received from the client is used for forwarding it to the server. Similarly the same mbuf on which a response was received from the server is used for forwarding it to the client.    Furthermore, memory for mbufs is managed using a reuse pool. This means that once mbuf is allocated, it is not deallocated, but just put back into the reuse pool. By default each mbuf chunk is set to 16K bytes in size. There is a trade-off between the mbuf size and number of concurrent connections twemproxy can support. A large mbuf size reduces the number of read syscalls made by twemproxy when reading requests or responses. However, with a large mbuf size, every active connection would use up 16K bytes of buffer which might be an issue when twemproxy is handling large number of concurrent connections from clients. When twemproxy is meant to handle a large number of concurrent client connections, you should set chunk size to a small value like 512 bytes using the -m or --mbuf-size=N argument.    ## Configuration    Twemproxy can be configured through a YAML file specified by the -c or --conf-file command-line argument on process start. The configuration file is used to specify the server pools and the servers within each pool that twemproxy manages. The configuration files parses and understands the following keys:    + **listen**: The listening address and port (name:port or ip:port) or an absolute path to sock file (e.g. /var/run/nutcracker.sock) for this server pool.  + **client_connections**: The maximum number of connections allowed from redis clients. Unlimited by default, though OS-imposed limitations will still apply.  + **hash**: The name of the hash function. Possible values are:    + one_at_a_time    + md5    + crc16    + crc32 (crc32 implementation compatible with [libmemcached](http://libmemcached.org/))    + crc32a (correct crc32 implementation as per the spec)    + fnv1_64    + fnv1a_64 (default)    + fnv1_32    + fnv1a_32    + hsieh    + murmur    + jenkins  + **hash_tag**: A two character string that specifies the part of the key used for hashing. Eg ""{}"" or ""$$"". [Hash tag](notes/recommendation.md#hash-tags) enable mapping different keys to the same server as long as the part of the key within the tag is the same.  + **distribution**: The key distribution mode for choosing backend servers based on the computed hash value. Possible values are:    + ketama (default, recommended. An implementation of https://en.wikipedia.org/wiki/Consistent_hashing)    + modula (use hash modulo number of servers to choose the backend)    + random (choose a random backend for each key of each request)  + **timeout**: The timeout value in msec that we wait for to establish a connection to the server or receive a response from a server. By default, we wait indefinitely.  + **backlog**: The TCP backlog argument. Defaults to 512.  + **tcpkeepalive**: A boolean value that controls if tcp keepalive is enabled for connections to servers. Defaults to false.  + **preconnect**: A boolean value that controls if twemproxy should preconnect to all the servers in this pool on process start. Defaults to false.  + **redis**: A boolean value that controls if a server pool speaks redis or memcached protocol. Defaults to false.  + **redis_auth**: Authenticate to the Redis server on connect.  + **redis_db**: The DB number to use on the pool servers. Defaults to 0. Note: Twemproxy will always present itself to clients as DB 0.  + **server_connections**: The maximum number of connections that can be opened to each server. By default, we open at most 1 server connection.  + **auto_eject_hosts**: A boolean value that controls if server should be ejected temporarily when it fails consecutively server_failure_limit times. See [liveness recommendations](notes/recommendation.md#liveness) for information. Defaults to false.  + **server_retry_timeout**: The timeout value in msec to wait for before retrying on a temporarily ejected server, when auto_eject_hosts is set to true. Defaults to 30000 msec.  + **server_failure_limit**: The number of consecutive failures on a server that would lead to it being temporarily ejected when auto_eject_hosts is set to true. Defaults to 2.  + **servers**: A list of server address, port and weight (name:port:weight or ip:port:weight) for this server pool.      For example, the configuration file in [conf/nutcracker.yml](conf/nutcracker.yml), also shown below, configures 5 server pools with names - _alpha_, _beta_, _gamma_, _delta_ and omega. Clients that intend to send requests to one of the 10 servers in pool delta connect to port 22124 on 127.0.0.1. Clients that intend to send request to one of 2 servers in pool omega connect to unix path /tmp/gamma. Requests sent to pool alpha and omega have no timeout and might require timeout functionality to be implemented on the client side. On the other hand, requests sent to pool beta, gamma and delta timeout after 400 msec, 400 msec and 100 msec respectively when no response is received from the server. Of the 5 server pools, only pools alpha, gamma and delta are configured to use server ejection and hence are resilient to server failures. All the 5 server pools use ketama consistent hashing for key distribution with the key hasher for pools alpha, beta, gamma and delta set to fnv1a_64 while that for pool omega set to hsieh. Also only pool beta uses [nodes names](notes/recommendation.md#node-names-for-consistent-hashing) for consistent hashing, while pool alpha, gamma, delta and omega use 'host:port:weight' for consistent hashing. Finally, only pool alpha and beta can speak the redis protocol, while pool gamma, delta and omega speak memcached protocol.        alpha:        listen: 127.0.0.1:22121        hash: fnv1a_64        distribution: ketama        auto_eject_hosts: true        redis: true        server_retry_timeout: 2000        server_failure_limit: 1        servers:         - 127.0.0.1:6379:1        beta:        listen: 127.0.0.1:22122        hash: fnv1a_64        hash_tag: ""{}""        distribution: ketama        auto_eject_hosts: false        timeout: 400        redis: true        servers:         - 127.0.0.1:6380:1 server1         - 127.0.0.1:6381:1 server2         - 127.0.0.1:6382:1 server3         - 127.0.0.1:6383:1 server4        gamma:        listen: 127.0.0.1:22123        hash: fnv1a_64        distribution: ketama        timeout: 400        backlog: 1024        preconnect: true        auto_eject_hosts: true        server_retry_timeout: 2000        server_failure_limit: 3        servers:         - 127.0.0.1:11212:1         - 127.0.0.1:11213:1        delta:        listen: 127.0.0.1:22124        hash: fnv1a_64        distribution: ketama        timeout: 100        auto_eject_hosts: true        server_retry_timeout: 2000        server_failure_limit: 1        servers:         - 127.0.0.1:11214:1         - 127.0.0.1:11215:1         - 127.0.0.1:11216:1         - 127.0.0.1:11217:1         - 127.0.0.1:11218:1         - 127.0.0.1:11219:1         - 127.0.0.1:11220:1         - 127.0.0.1:11221:1         - 127.0.0.1:11222:1         - 127.0.0.1:11223:1        omega:        listen: /tmp/gamma 0666        hash: hsieh        distribution: ketama        auto_eject_hosts: false        servers:         - 127.0.0.1:11214:100000         - 127.0.0.1:11215:1    Finally, to make writing a syntactically correct configuration file easier, twemproxy provides a command-line argument `-t` or `--test-conf` that can be used to test the YAML configuration file for any syntax error.    ## Observability    Observability in twemproxy is through logs and stats.    Twemproxy exposes stats at the granularity of server pool and servers per pool through the stats monitoring port by responding with the raw data over TCP. The stats are essentially JSON formatted key-value pairs, with the keys corresponding to counter names. By default stats are exposed on port 22222 and aggregated every 30 seconds. Both these values can be configured on program start using the `-c` or `--conf-file` and `-i` or `--stats-interval` command-line arguments respectively. You can print the description of all stats exported by  using the `-D` or `--describe-stats` command-line argument.        $ nutcracker --describe-stats        pool stats:        client_eof          ""# eof on client connections""        client_err          ""# errors on client connections""        client_connections  ""# active client connections""        server_ejects       ""# times backend server was ejected""        forward_error       ""# times we encountered a forwarding error""        fragments           ""# fragments created from a multi-vector request""        server stats:        server_eof          ""# eof on server connections""        server_err          ""# errors on server connections""        server_timedout     ""# timeouts on server connections""        server_connections  ""# active server connections""        requests            ""# requests""        request_bytes       ""total request bytes""        responses           ""# responses""        response_bytes      ""total response bytes""        in_queue            ""# requests in incoming queue""        in_queue_bytes      ""current request bytes in incoming queue""        out_queue           ""# requests in outgoing queue""        out_queue_bytes     ""current request bytes in outgoing queue""    See [`notes/debug.txt`](notes/debug.txt) for examples of how to read the stats from the stats port.    Logging in twemproxy is only available when twemproxy is built with logging enabled. By default logs are written to stderr. Twemproxy can also be configured to write logs to a specific file through the `-o` or `--output` command-line argument. On a running twemproxy, we can turn log levels up and down by sending it SIGTTIN and SIGTTOU signals respectively and reopen log files by sending it SIGHUP signal.    ## Pipelining    Twemproxy enables proxying multiple client connections onto one or few server connections. This architectural setup makes it ideal for pipelining requests and responses and hence saving on the round trip time.    For example, if twemproxy is proxying three client connections onto a single server and we get requests - `get key\r\n`, `set key 0 0 3\r\nval\r\n` and `delete key\r\n` on these three connections respectively, twemproxy would try to batch these requests and send them as a single message onto the server connection as `get key\r\nset key 0 0 3\r\nval\r\ndelete key\r\n`.    Pipelining is the reason why twemproxy ends up doing better in terms of throughput even though it introduces an extra hop between the client and server.    ## Deployment    If you are deploying twemproxy in production, you might consider reading through the [recommendation document](notes/recommendation.md) to understand the parameters you could tune in twemproxy to run it efficiently in the production environment.    ## Utils  + [collectd-plugin](https://github.com/bewie/collectd-twemproxy)  + [munin-plugin](https://github.com/eveiga/contrib/tree/nutcracker/plugins/nutcracker)  + [twemproxy-ganglia-module](https://github.com/ganglia/gmond_python_modules/tree/master/twemproxy)  + [nagios checks](https://github.com/wanelo/nagios-checks/blob/master/check_twemproxy)  + [circonus](https://github.com/wanelo-chef/nad-checks/blob/master/recipes/twemproxy.rb)  + [puppet module](https://github.com/wuakitv/puppet-twemproxy)  + [nutcracker-web](https://github.com/kontera-technologies/nutcracker-web)  + [redis-twemproxy agent](https://github.com/Stono/redis-twemproxy-agent)  + [sensu-metrics](https://github.com/sensu-plugins/sensu-plugins-twemproxy/blob/master/bin/metrics-twemproxy.rb)  + [redis-mgr](https://github.com/idning/redis-mgr)  + [smitty for twemproxy failover](https://github.com/areina/smitty)  + [Beholder, a Python agent for twemproxy failover](https://github.com/Serekh/beholder)  + [chef cookbook](https://supermarket.getchef.com/cookbooks/twemproxy)  + [twemsentinel](https://github.com/yak0/twemsentinel)    ## Companies using Twemproxy in Production  + [Twitter](https://twitter.com/)  + [Wikimedia](https://www.wikimedia.org/)  + [Pinterest](http://pinterest.com/)  + [Snapchat](http://www.snapchat.com/)  + [Flickr](https://www.flickr.com)  + [Yahoo!](https://www.yahoo.com)  + [Tumblr](https://www.tumblr.com/)  + [Vine](http://vine.co/)  + [Wayfair](http://www.wayfair.com/)  + [Kiip](http://www.kiip.me/)  + [Wuaki.tv](https://wuaki.tv/)  + [Wanelo](http://wanelo.com/)  + [Kontera](http://kontera.com/)  + [Bright](http://www.bright.com/)  + [56.com](http://www.56.com/)  + [Digg](http://digg.com/)  + [Gawkermedia](http://advertising.gawker.com/)  + [3scale.net](http://3scale.net)  + [Ooyala](http://www.ooyala.com)  + [Twitch](http://twitch.tv)  + [Socrata](http://www.socrata.com/)  + [Hootsuite](http://hootsuite.com/)  + [Trivago](http://www.trivago.com/)  + [Machinezone](http://www.machinezone.com)  + [Path](https://path.com)  + [AOL](http://engineering.aol.com/)  + [Soysuper](https://soysuper.com/)  + [Vinted](http://vinted.com/)  + [Poshmark](https://poshmark.com/)  + [FanDuel](https://www.fanduel.com/)  + [Bloomreach](http://bloomreach.com/)  + [Hootsuite](https://hootsuite.com)  + [Tradesy](https://www.tradesy.com/)  + [Uber](http://uber.com) ([details](http://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html))  + [Greta](https://greta.io/)    ## Issues and Support    Have a bug or a question? Please create an issue here on GitHub!    https://github.com/twitter/twemproxy/issues    ## Committers    * Manju Rajashekhar ([@manju](https://twitter.com/manju))  * Lin Yang ([@idning](https://github.com/idning))  * Tyson Andre ([@TysonAndre](https://github.com/TysonAndre))    Thank you to all of our [contributors](https://github.com/twitter/twemproxy/graphs/contributors)!    ## License    Copyright 2012 Twitter, Inc.    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/Freeboard/freeboard;"""freeboard  ==========    **free·board** (noun) *\ˈfrē-ˌbȯrd\*    1. the distance between the waterline and the main deck or weather deck of a ship or between the level of the water and the upper edge of the side of a small boat.  2. the act of freeing data from below the ""waterline"" and exposing it to the world.  3. a damn-sexy, open source real-time dashboard builder/viewer for IOT and other web mashups.    ### Demo  http://freeboard.github.io/freeboard    https://freeboard.io    ### Screenshots  ![Weather](https://raw.github.com/Freeboard/branding/master/screenshots/freeboard-screenshot-1.jpg)    ### What is It?    Freeboard is a turn-key HTML-based ""engine"" for dashboards. Besides a nice looking layout engine, it provides a plugin architecture for creating datasources (which fetch data) and widgets (which display data)— freeboard then does all the work to connect the two together. Another feature of freeboard is its ability to run entirely in the browser as a single-page static web app without the need for a server. The feature makes it extremely attractive as a front-end for embedded devices which may have limited ability to serve complex and dynamic web pages.    The code here is the client-side portion of what you see when you visit a freeboard at http://freeboard.io. It does not include any of the server-side code for user management, saving to a database or public/private functionality— this is left up to you to implement should you want to use freeboard as an online service.    ### How to Use    Freeboard can be run entirely from a local hard drive. Simply download/clone the repository and open index.html. When using Chrome, you may run into issues with CORS when accessing JSON based APIs if you load from your local hard-drive— in this case you can switch to using JSONP or load index.html and run from a local or remote web server.    1. git clone https://github.com/Freeboard/freeboard.git  2. cd freeboard  3. npm install  4. grunt    Then run a index.html or index-dev.html through a webserver.    ### API    While freeboard runs as a stand-alone app out of the box, you can augment and control it from javascript with a simple API. All API calls are made on the `freeboard` singleton object.    -------    **freeboard.initialize(allowEdit, [callback])**    Must be called first to initialize freeboard.    > **allowEdit** (boolean) - Sets the initial state of freeboard to allow or disallow editing.    > **callback** (function) - Function that will be called back when freeboard has finished initializing.    -------    **freeboard.newDashboard()**    Clear the contents of the freeboard and initialize a new dashboard.    -------    **freeboard.serialize()**    Serializes the current dashboard and returns a javascript object.    -------    **freeboard.loadDashboard(configuration, [callback])**    Load the dashboard from a serialized dashboard object.    > **configuration** (object) - A javascript object containing the configuration of a dashboard. Normally this will be an object that has been created and saved via the `freeboard.serialize()` function.    > **callback** (function) - Function that will be called back when the dashboard has finished loading.    -------    **freeboard.setEditing(editing, animate)**    Programatically control the editing state of the of dashboard.    > **editing** (bool) - Set to true or false to modify the view-only or editing state of the board.    > **animate** (function) - Set to true or false to animate the modification of the editing state. This animates the top-tab dropdown (the part where you can edit datasources and such).    -------    **freeboard.isEditing()**    Returns boolean depending on whether the dashboard is in in the view-only or edit state.    -------    **freeboard.loadDatasourcePlugin(plugin)**    Register a datasource plugin. See http://freeboard.github.io/freeboard/docs/plugin_example.html for information on creating plugins.    > **plugin** (object) - A plugin definition object as defined at http://freeboard.github.io/freeboard/docs/plugin_example.html    -------    **freeboard.loadWidgetPlugin(plugin)**    Register a widget plugin. See http://freeboard.github.io/freeboard/docs/plugin_example.html for information on creating plugins.    > **plugin** (object) - A plugin definition object as defined at http://freeboard.github.io/freeboard/docs/plugin_example.html    -------    **freeboard.showLoadingIndicator(show)**    Show/hide the loading indicator. The loading indicator will display an indicator over the entire board that can be useful when you have some code that takes a while and you want to give a visual indication and to prevent the user from modifying the board.    > **show** (boolean) - Set to true or false to show or hide the loading indicator.    -------    **freeboard.showDialog(contentElement, title, okButtonTitle, cancelButtonTitle, okCallback)**    Show a styled dialog box with custom content.    > **contentElement** (DOM or jquery element) - The DOM or jquery element to display within the content of the dialog box.    > **title** (string) - The title of the dialog box displayed on the top left.    > **okButtonTitle** (string) - The string to display in the button that will be used as the OK button. A null or undefined value will result in no button being displayed.    > **cancelButtonTitle** (string) - The string to display in the button that will be used as the Cancel button. A null or undefined value will result in no button being displayed.    > **okCallback** (function) - A function that will be called if the user presses the OK button.    -------    **freeboard.getDatasourceSettings(datasourceName)**    Returns an object with the current settings for a datasource or null if no datasource with the given name is found.    > **datasourceName** (string) - The name of a datasource in the dashboard.    -------    **freeboard.setDatasourceSettings(datasourceName, settings)**    Updates settings on a datasource.    > **datasourceName** (string) - The name of a datasource in the dashboard.    > **settings** (object) - An object of key-value pairs for the settings of the datasource. The values specified here will be combined with the current settings, so you do not need specify every setting if you only want to update one. To get a list of possible settings for a datasource, consult the datasource documentation or code, or call the freeboard.getDatasourceSettings function.    -------    **freeboard.on(eventName, callback)**    Attach to a global freeboard event.    > **eventName** (string) - The name of a global event. The following events are supported:    > **""dashboard_loaded""** - Occurs after a dashboard has been loaded.    > **""initialized""** - Occurs after freeboard has first been initialized.    > **callback** (function) - The callback function to be called when the event occurs.    -------    ### Building Plugins    See http://freeboard.github.io/freeboard/docs/plugin_example.html for info on how to build plugins for freeboard.    ### Testing Plugins    Just edit index.html and add a link to your javascript file near the end of the head.js script loader, like:    ```javascript  ...  ""path/to/my/plugin/file.js"",  $(function()  { //DOM Ready      freeboard.initialize(true);  });  ```    ### Copyright     Copyright © 2013 Jim Heising (https://github.com/jheising)<br/>Copyright © 2013 Bug Labs, Inc. (http://buglabs.net)<br/>Licensed under the **MIT** license.    --- """
Big data;https://github.com/ml-tooling/ml-workspace;"""<h1 align=""center"">      <a href=""https://github.com/ml-tooling/ml-workspace"" title=""ML Workspace Home"">      <img width=50% alt="""" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/ml-workspace-logo.png""> </a>      <br>  </h1>    <p align=""center"">      <strong>All-in-one web-based development environment for machine learning</strong>  </p>    <p align=""center"">      <a href=""https://hub.docker.com/r/mltooling/ml-workspace"" title=""Docker Image Version""><img src=""https://img.shields.io/docker/v/mltooling/ml-workspace?color=blue&sort=semver""></a>      <a href=""https://hub.docker.com/r/mltooling/ml-workspace"" title=""Docker Pulls""><img src=""https://img.shields.io/docker/pulls/mltooling/ml-workspace.svg?color=blue""></a>      <a href=""https://hub.docker.com/r/mltooling/ml-workspace"" title=""Docker Image Size""><img src=""https://img.shields.io/docker/image-size/mltooling/ml-workspace?color=blue&sort=semver""></a>      <a href=""https://gitter.im/ml-tooling/ml-workspace"" title=""Chat on Gitter""><img src=""https://badges.gitter.im/ml-tooling/ml-workspace.svg""></a>      <a href=""https://mltooling.substack.com/subscribe"" title=""Subscribe to newsletter""><img src=""http://bit.ly/2Md9rxM""></a>      <a href=""https://twitter.com/mltooling"" title=""Follow on Twitter""><img src=""https://img.shields.io/twitter/follow/mltooling.svg?style=social&label=Follow""></a>  </p>    <p align=""center"">    <a href=""#getting-started"">Getting Started</a> •    <a href=""#features"">Features & Screenshots</a> •    <a href=""#support"">Support</a> •    <a href=""https://github.com/ml-tooling/ml-workspace/issues/new?labels=bug&template=01_bug-report.md"">Report a Bug</a> •    <a href=""#faq"">FAQ</a> •    <a href=""#known-issues"">Known Issues</a> •    <a href=""#contribution"">Contribution</a>  </p>    The ML workspace is an all-in-one web-based IDE specialized for machine learning and data science. It is simple to deploy and gets you started within minutes to productively built ML solutions on your own machines. This workspace is the ultimate tool for developers preloaded with a variety of popular data science libraries (e.g., Tensorflow, PyTorch, Keras, Sklearn) and dev tools (e.g., Jupyter, VS Code, Tensorboard) perfectly configured, optimized, and integrated.    ## Highlights    - 💫&nbsp; Jupyter, JupyterLab, and Visual Studio Code web-based IDEs.  - 🗃&nbsp; Pre-installed with many popular data science libraries & tools.  - 🖥&nbsp; Full Linux desktop GUI accessible via web browser.  - 🔀&nbsp; Seamless Git integration optimized for notebooks.  - 📈&nbsp; Integrated hardware & training monitoring via Tensorboard & Netdata.  - 🚪&nbsp; Access from anywhere via Web, SSH, or VNC under a single port.  - 🎛&nbsp; Usable as remote kernel (Jupyter) or remote machine (VS Code) via SSH.  - 🐳&nbsp; Easy to deploy on Mac, Linux, and Windows via Docker.    <br>    ## Getting Started    <p>  <a href=""https://labs.play-with-docker.com/?stack=https://raw.githubusercontent.com/ml-tooling/ml-workspace/main/deployment/play-with-docker/docker-compose.yml"" title=""Docker Image Metadata"" target=""_blank""><img src=""https://cdn.rawgit.com/play-with-docker/stacks/cff22438/assets/images/button.png"" alt=""Try in PWD"" width=""100px""></a>  </p>    ### Prerequisites    The workspace requires **Docker** to be installed on your machine ([📖 Installation Guide](https://docs.docker.com/install/#supported-platforms)).    ### Start single instance    Deploying a single workspace instance is as simple as:    ```bash  docker run -p 8080:8080 mltooling/ml-workspace:0.13.2  ```    Voilà, that was easy! Now, Docker will pull the latest workspace image to your machine. This may take a few minutes, depending on your internet speed. Once the workspace is started, you can access it via http://localhost:8080.    > _If started on another machine or with a different port, make sure to use the machine's IP/DNS and/or the exposed port._    To deploy a single instance for productive usage, we recommend to apply at least the following options:    ```bash  docker run -d \      -p 8080:8080 \      --name ""ml-workspace"" \      -v ""${PWD}:/workspace"" \      --env AUTHENTICATE_VIA_JUPYTER=""mytoken"" \      --shm-size 512m \      --restart always \      mltooling/ml-workspace:0.13.2  ```    This command runs the container in background (`-d`), mounts your current working directory into the `/workspace` folder (`-v`), secures the workspace via a provided token (`--env AUTHENTICATE_VIA_JUPYTER`), provides 512MB of shared memory (`--shm-size`) to prevent unexpected crashes (see [known issues section](#known-issues)), and keeps the container running even on system restarts (`--restart always`). You can find additional options for docker run [here](https://docs.docker.com/engine/reference/commandline/run/) and workspace configuration options in [the section below](#Configuration).    ### Configuration Options    The workspace provides a variety of configuration options that can be used by setting environment variables (via docker run option: `--env`).    <details>  <summary>Configuration options (click to expand...)</summary>    <table>      <tr>          <th>Variable</th>          <th>Description</th>          <th>Default</th>      </tr>      <tr>          <td>WORKSPACE_BASE_URL</td>          <td>The base URL under which Jupyter and all other tools will be reachable from.</td>          <td>/</td>      </tr>      <tr>          <td>WORKSPACE_SSL_ENABLED</td>          <td>Enable or disable SSL. When set to true, either certificate (cert.crt) must be mounted to <code>/resources/ssl</code> or, if not, the container generates self-signed certificate.</td>          <td>false</td>      </tr>      <tr>          <td>WORKSPACE_AUTH_USER</td>          <td>Basic auth user name. To enable basic auth, both the user and password need to be set. We recommend to use the <code>AUTHENTICATE_VIA_JUPYTER</code> for securing the workspace.</td>          <td></td>      </tr>      <tr>          <td>WORKSPACE_AUTH_PASSWORD</td>          <td>Basic auth user password. To enable basic auth, both the user and password need to be set. We recommend to use the <code>AUTHENTICATE_VIA_JUPYTER</code> for securing the workspace.</td>          <td></td>      </tr>      <tr>          <td>WORKSPACE_PORT</td>          <td>Configures the main container-internal port of the workspace proxy. For most scenarios, this configuration should not be changed, and the port configuration via Docker should be used instead of the workspace should be accessible from a different port.</td>          <td>8080</td>      </tr>      <tr>          <td>CONFIG_BACKUP_ENABLED</td>          <td>Automatically backup and restore user configuration to the persisted <code>/workspace</code> folder, such as the .ssh, .jupyter, or .gitconfig from the users home directory.</td>          <td>true</td>      </tr>      <tr>          <td>SHARED_LINKS_ENABLED</td>          <td>Enable or disable the capability to share resources via external links. This is used to enable file sharing, access to workspace-internal ports, and easy command-based SSH setup. All shared links are protected via a token. However, there are certain risks since the token cannot be easily invalidated after sharing and does not expire.</td>          <td>true</td>      </tr>      <tr>          <td>INCLUDE_TUTORIALS</td>          <td>If <code>true</code>, a selection of tutorial and introduction notebooks are added to the <code>/workspace</code> folder at container startup, but only if the folder is empty.</td>          <td>true</td>      </tr>      <tr>          <td>MAX_NUM_THREADS</td>          <td>The number of threads used for computations when using various common libraries (MKL, OPENBLAS, OMP, NUMBA, ...). You can also use <code>auto</code> to let the workspace dynamically determine the number of threads based on available CPU resources. This configuration can be overwritten by the user from within the workspace. Generally, it is good to set it at or below the number of CPUs available to the workspace.</td>          <td>auto</td>      </tr>      <tr>          <td colspan=""3""><b>Jupyter Configuration:</b></td>      </tr>      <tr>          <td>SHUTDOWN_INACTIVE_KERNELS</td>          <td>Automatically shutdown inactive kernels after a given timeout (to clean up memory or GPU resources). Value can be either a timeout in seconds or set to <code>true</code> with a default value of 48h.</td>          <td>false</td>      </tr>      <tr>          <td>AUTHENTICATE_VIA_JUPYTER</td>          <td>If <code>true</code>, all HTTP requests will be authenticated against the Jupyter server, meaning that the authentication method configured with Jupyter will be used for all other tools as well. This can be deactivated with <code>false</code>. Any other value will activate this authentication and are applied as token via NotebookApp.token configuration of Jupyter.</td>          <td>false</td>      </tr>      <tr>          <td>NOTEBOOK_ARGS</td>          <td>Add and overwrite Jupyter configuration options via command line args. Refer to <a href=""https://jupyter-notebook.readthedocs.io/en/stable/config.html"">this overview</a> for all options.</td>          <td></td>      </tr>  </table>    </details>    ### Persist Data    To persist the data, you need to mount a volume into `/workspace` (via docker run option: `-v`).    <details>  <summary>Details (click to expand...)</summary>    The default work directory within the container is `/workspace`, which is also the root directory of the Jupyter instance. The `/workspace` directory is intended to be used for all the important work artifacts. Data within other directories of the server (e.g., `/root`) might get lost at container restarts.  </details>    ### Enable Authentication    We strongly recommend enabling authentication via one of the following two options. For both options, the user will be required to authenticate for accessing any of the pre-installed tools.    > _The authentication only works for all tools accessed through the main workspace port (default: `8080`). This works for all preinstalled tools and the [Access Ports](#access-ports) feature. If you expose another port of the container, please make sure to secure it with authentication as well!_    <details>  <summary>Details (click to expand...)</summary>    #### Token-based Authentication via Jupyter (recommended)    Activate the token-based authentication based on the authentication implementation of Jupyter via the `AUTHENTICATE_VIA_JUPYTER` variable:    ```bash  docker run -p 8080:8080 --env AUTHENTICATE_VIA_JUPYTER=""mytoken"" mltooling/ml-workspace:0.13.2  ```    You can also use `<generated>` to let Jupyter generate a random token that is printed out on the container logs. A value of `true` will not set any token but activate that every request to any tool in the workspace will be checked with the Jupyter instance if the user is authenticated. This is used for tools like JupyterHub, which configures its own way of authentication.    #### Basic Authentication via Nginx    Activate the basic authentication via the `WORKSPACE_AUTH_USER` and `WORKSPACE_AUTH_PASSWORD` variable:    ```bash  docker run -p 8080:8080 --env WORKSPACE_AUTH_USER=""user"" --env WORKSPACE_AUTH_PASSWORD=""pwd"" mltooling/ml-workspace:0.13.2  ```    The basic authentication is configured via the nginx proxy and might be more performant compared to the other option since with `AUTHENTICATE_VIA_JUPYTER` every request to any tool in the workspace will check via the Jupyter instance if the user (based on the request cookies) is authenticated.    </details>    ### Enable SSL/HTTPS    We recommend enabling SSL so that the workspace is accessible via HTTPS (encrypted communication). SSL encryption can be activated via the `WORKSPACE_SSL_ENABLED` variable.     <details>  <summary>Details (click to expand...)</summary>    When set to `true`, either the `cert.crt` and `cert.key` file must be mounted to `/resources/ssl` or, if the certificate files do not exist, the container generates self-signed certificates. For example, if the `/path/with/certificate/files` on the local system contains a valid certificate for the host domain (`cert.crt` and `cert.key` file), it can be used from the workspace as shown below:    ```bash  docker run \      -p 8080:8080 \      --env WORKSPACE_SSL_ENABLED=""true"" \      -v /path/with/certificate/files:/resources/ssl:ro \      mltooling/ml-workspace:0.13.2  ```    If you want to host the workspace on a public domain, we recommend to use [Let's encrypt](https://letsencrypt.org/getting-started/) to get a trusted certificate for your domain.  To use the generated certificate (e.g., via [certbot](https://certbot.eff.org/) tool) for the workspace, the `privkey.pem` corresponds to the `cert.key` file and the `fullchain.pem` to the `cert.crt` file.    > _When you enable SSL support, you must access the workspace over `https://`, not over plain `http://`._    </details>    ### Limit Memory & CPU    By default, the workspace container has no resource constraints and can use as much of a given resource as the host’s kernel scheduler allows. Docker provides ways to control how much memory, or CPU a container can use, by setting runtime configuration flags of the docker run command.    > _The workspace requires atleast 2 CPUs and 500MB to run stable and be usable._    <details>  <summary>Details (click to expand...)</summary>    For example, the following command restricts the workspace to only use a maximum of 8 CPUs, 16 GB of memory, and 1 GB of shared memory (see [Known Issues](#known-issues)):    ```bash  docker run -p 8080:8080 --cpus=8 --memory=16g --shm-size=1G mltooling/ml-workspace:0.13.2  ```    > 📖 _For more options and documentation on resource constraints, please refer to the [official docker guide](https://docs.docker.com/config/containers/resource_constraints/)._    </details>    ### Enable Proxy    If a proxy is required, you can pass the proxy configuration via the `HTTP_PROXY`, `HTTPS_PROXY`, and `NO_PROXY` environment variables.    ### Workspace Flavors    In addition to the main workspace image (`mltooling/ml-workspace`), we provide other image flavors that extend the features or minimize the image size to support a variety of use cases.    #### Minimal Flavor    <p>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace"" title=""Docker Image Version""><img src=""https://img.shields.io/docker/v/mltooling/ml-workspace?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-minimal"" title=""Docker Image Size""><img src=""https://img.shields.io/docker/image-size/mltooling/ml-workspace-minimal?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-minimal"" title=""Docker Pulls""><img src=""https://img.shields.io/docker/pulls/mltooling/ml-workspace-minimal.svg""></a>  </p>    <details>  <summary>Details (click to expand...)</summary>    The minimal flavor (`mltooling/ml-workspace-minimal`) is our smallest image that contains most of the tools and features described in the [features section](#features) without most of the python libraries that are pre-installed in our main image. Any Python library or excluded tool can be installed manually during runtime by the user.    ```bash  docker run -p 8080:8080 mltooling/ml-workspace-minimal:0.13.2  ```  </details>    #### R Flavor    <p>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-r"" title=""Docker Image Version""><img src=""https://img.shields.io/docker/v/mltooling/ml-workspace-r?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-r"" title=""Docker Image Size""><img src=""https://img.shields.io/docker/image-size/mltooling/ml-workspace-r?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-r"" title=""Docker Pulls""><img src=""https://img.shields.io/docker/pulls/mltooling/ml-workspace-r.svg""></a>  </p>    <details>  <summary>Details (click to expand...)</summary>    The R flavor (`mltooling/ml-workspace-r`) is based on our default workspace image and extends it with the R-interpreter, R-Jupyter kernel, RStudio server (access via `Open Tool -> RStudio`), and a variety of popular packages from the R ecosystem.    ```bash  docker run -p 8080:8080 mltooling/ml-workspace-r:0.12.1  ```  </details>    #### Spark Flavor    <p>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-spark"" title=""Docker Image Version""><img src=""https://img.shields.io/docker/v/mltooling/ml-workspace-spark?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-spark"" title=""Docker Image Size""><img src=""https://img.shields.io/docker/image-size/mltooling/ml-workspace-spark?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-spark"" title=""Docker Pulls""><img src=""https://img.shields.io/docker/pulls/mltooling/ml-workspace-spark.svg""></a>  </p>    <details>  <summary>Details (click to expand...)</summary>    The Spark flavor (`mltooling/ml-workspace-spark`) is based on our R-flavor workspace image and extends it with the Spark runtime, Spark-Jupyter kernel, Zeppelin Notebook (access via `Open Tool -> Zeppelin`), PySpark, Hadoop, Java Kernel, and a few additional libraries & Jupyter extensions.    ```bash  docker run -p 8080:8080 mltooling/ml-workspace-spark:0.12.1  ```    </details>    #### GPU Flavor    <p>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-gpu"" title=""Docker Image Version""><img src=""https://img.shields.io/docker/v/mltooling/ml-workspace-gpu?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-gpu"" ttitle=""Docker Image Size""><img src=""https://img.shields.io/docker/image-size/mltooling/ml-workspace-gpu?color=blue&sort=semver""></a>  <a href=""https://hub.docker.com/r/mltooling/ml-workspace-gpu"" title=""Docker Pulls""><img src=""https://img.shields.io/docker/pulls/mltooling/ml-workspace-gpu.svg""></a>  </p>    <details>  <summary>Details (click to expand...)</summary>    > _Currently, the GPU-flavor only supports CUDA 11.2. Support for other CUDA versions might be added in the future._    The GPU flavor (`mltooling/ml-workspace-gpu`) is based on our default workspace image and extends it with CUDA 10.1 and GPU-ready versions of various machine learning libraries (e.g., tensorflow, pytorch, cntk, jax). This GPU image has the following additional requirements for the system:    - Nvidia Drivers for the GPUs. Drivers need to be CUDA 11.2 compatible, version `>=460.32.03` ([📖 Instructions](https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#how-do-i-install-the-nvidia-driver)).  - (Docker >= 19.03) Nvidia Container Toolkit ([📖 Instructions](https://github.com/NVIDIA/nvidia-docker/wiki/Installation-(Native-GPU-Support))).    ```bash  docker run -p 8080:8080 --gpus all mltooling/ml-workspace-gpu:0.13.2  ```    - (Docker < 19.03) Nvidia Docker 2.0 ([📖 Instructions](https://github.com/NVIDIA/nvidia-docker/wiki/Installation-(version-2.0))).    ```bash  docker run -p 8080:8080 --runtime nvidia --env NVIDIA_VISIBLE_DEVICES=""all"" mltooling/ml-workspace-gpu:0.13.2  ```    The GPU flavor also comes with a few additional configuration options, as explained below:    <table>      <tr>          <th>Variable</th>          <th>Description</th>          <th>Default</th>      </tr>      <tr>          <td>NVIDIA_VISIBLE_DEVICES</td>          <td>Controls which GPUs will be accessible inside the workspace. By default, all GPUs from the host are accessible within the workspace. You can either use <code>all</code>, <code>none</code>, or specify a comma-separated list of device IDs (e.g., <code>0,1</code>). You can find out the list of available device IDs by running <code>nvidia-smi</code> on the host machine.</td>          <td>all</td>      </tr>      <tr>          <td>CUDA_VISIBLE_DEVICES</td>          <td>Controls which GPUs CUDA applications running inside the workspace will see. By default, all GPUs that the workspace has access to will be visible. To restrict applications, provide a comma-separated list of internal device IDs (e.g., <code>0,2</code>) based on the available devices within the workspace (run <code>nvidia-smi</code>). In comparison to <code>NVIDIA_VISIBLE_DEVICES</code>, the workspace user will be still able to access other GPUs by overwriting this configuration from within the workspace.</td>          <td></td>      </tr>      <tr>          <td>TF_FORCE_GPU_ALLOW_GROWTH</td>          <td>By default, the majority of GPU memory will be allocated by the first execution of a TensorFlow graph. While this behavior can be desirable for production pipelines, it is less desirable for interactive use. Use <code>true</code> to enable dynamic GPU Memory allocation or <code>false</code> to instruct TensorFlow to allocate all memory at execution.</td>          <td>true</td>      </tr>  </table>  </details>    ### Multi-user setup    The workspace is designed as a single-user development environment. For a multi-user setup, we recommend deploying [🧰 ML Hub](https://github.com/ml-tooling/ml-hub). ML Hub is based on JupyterHub with the task to spawn, manage, and proxy workspace instances for multiple users.    <details>  <summary>Deployment (click to expand...)</summary>    ML Hub makes it easy to set up a multi-user environment on a single server (via Docker) or a cluster (via Kubernetes) and supports a variety of usage scenarios & authentication providers. You can try out ML Hub via:    ```bash  docker run -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock mltooling/ml-hub:latest  ```    For more information and documentation about ML Hub, please take a look at the [Github Site](https://github.com/ml-tooling/ml-hub).    </details>    ---    <br>    ## Support    This project is maintained by [Benjamin Räthlein](https://twitter.com/raethlein), [Lukas Masuch](https://twitter.com/LukasMasuch), and [Jan Kalkan](https://www.linkedin.com/in/jan-kalkan-b5390284/). Please understand that we won't be able to provide individual support via email. We also believe that help is much more valuable if it's shared publicly so that more people can benefit from it.    | Type                     | Channel                                              |  | ------------------------ | ------------------------------------------------------ |  | 🚨&nbsp; **Bug Reports**       | <a href=""https://github.com/ml-tooling/ml-workspace/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3Abug+sort%3Areactions-%2B1-desc+"" title=""Open Bug Report""><img src=""https://img.shields.io/github/issues/ml-tooling/ml-workspace/bug.svg""></a>                                  |  | 🎁&nbsp; **Feature Requests**  | <a href=""https://github.com/ml-tooling/ml-workspace/issues?q=is%3Aopen+is%3Aissue+label%3Afeature+sort%3Areactions-%2B1-desc"" title=""Open Feature Request""><img src=""https://img.shields.io/github/issues/ml-tooling/ml-workspace/feature.svg?label=feature%20request""></a>                                 |  | 👩‍💻&nbsp; **Usage Questions**   |  <a href=""https://github.com/ml-tooling/ml-workspace/issues?q=is%3Aopen+is%3Aissue+label%3Asupport+sort%3Areactions-%2B1-desc"" title=""Open Support Request""> <img src=""https://img.shields.io/github/issues/ml-tooling/ml-workspace/support.svg?label=support%20request""></a> <a href=""https://stackoverflow.com/questions/tagged/ml-tooling"" title=""Open Question on Stackoverflow""> <img src=""https://img.shields.io/badge/stackoverflow-ml--tooling-orange.svg""></a> <a href=""https://gitter.im/ml-tooling/ml-workspace"" title=""Chat on Gitter""><img src=""https://badges.gitter.im/ml-tooling/ml-workspace.svg""></a> |  | 📢&nbsp; **Announcements** | <a href=""https://gitter.im/ml-tooling/ml-workspace"" title=""Chat on Gitter""><img src=""https://badges.gitter.im/ml-tooling/ml-workspace.svg""></a> <a href=""https://mltooling.substack.com/subscribe"" title=""Subscribe for updates""><img src=""http://bit.ly/2Md9rxM""></a> <a href=""https://twitter.com/mltooling"" title=""ML Tooling on Twitter""><img src=""https://img.shields.io/twitter/follow/mltooling.svg?style=social&label=Follow""> |  | ❓&nbsp; **Other Requests** | <a href=""mailto:team@mltooling.org"" title=""Email ML Tooling Team""><img src=""https://img.shields.io/badge/email-ML Tooling-green?logo=mail.ru&logoColor=white""></a> |    ---    <br>    ## Features    <p align=""center"">    <a href=""#jupyter"">Jupyter</a> •    <a href=""#desktop-gui"">Desktop GUI</a> •    <a href=""#visual-studio-code"">VS Code</a> •    <a href=""#jupyterlab"">JupyterLab</a> •    <a href=""#git-integration"">Git Integration</a> •    <a href=""#file-sharing"">File Sharing</a> •    <a href=""#access-ports"">Access Ports</a> •    <a href=""#tensorboard"">Tensorboard</a> •    <a href=""#extensibility"">Extensibility</a> •    <a href=""#hardware-monitoring"">Hardware Monitoring</a> •    <a href=""#ssh-access"">SSH Access</a> •    <a href=""#remote-development"">Remote Development</a> •    <a href=""#run-as-a-job"">Job Execution</a>  </p>    The workspace is equipped with a selection of best-in-class open-source development tools to help with the machine learning workflow. Many of these tools can be started from the `Open Tool` menu from Jupyter (the main application of the workspace):    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/open-tools.png""/>    > _Within your workspace you have **full root & sudo privileges** to install any library or tool you need via terminal (e.g., `pip`, `apt-get`, `conda`, or `npm`). You can find more ways to extend the workspace within the [Extensibility](#extensibility) section_    ### Jupyter    [Jupyter Notebook](https://jupyter.org/) is a web-based interactive environment for writing and running code. The main building blocks of Jupyter are the file-browser, the notebook editor, and kernels. The file-browser provides an interactive file manager for all notebooks, files, and folders in the `/workspace` directory.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/jupyter-tree.png""/>    A new notebook can be created by clicking on the `New` drop-down button at the top of the list and selecting the desired language kernel.    > _You can spawn interactive **terminal** instances as well by selecting `New -> Terminal` in the file-browser._    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/jupyter-notebook.png""/>    The notebook editor enables users to author documents that include live code, markdown text, shell commands, LaTeX equations, interactive widgets, plots, and images. These notebook documents provide a complete and self-contained record of a computation that can be converted to various formats and shared with others.    > _This workspace has a variety of **third-party Jupyter extensions** activated. You can configure these extensions in the nbextensions configurator: `nbextensions` tab on the file browser_    The Notebook allows code to be run in a range of different programming languages. For each notebook document that a user opens, the web application starts a **kernel** that runs the code for that notebook and returns output. This workspace has a Python 3 kernel pre-installed. Additional Kernels can be installed to get access to other languages (e.g., R, Scala, Go) or additional computing resources (e.g., GPUs, CPUs, Memory).    > _**Python 2** is deprected and we do not recommend to use it. However, you can still install a Python 2.7 kernel via this command: `/bin/bash /resources/tools/python-27.sh`_    ### Desktop GUI    This workspace provides an HTTP-based VNC access to the workspace via [noVNC](https://github.com/novnc/noVNC). Thereby, you can access and work within the workspace with a fully-featured desktop GUI. To access this desktop GUI, go to `Open Tool`, select `VNC`, and click the `Connect` button. In the case you are asked for a password, use `vncpassword`.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/desktop-vnc.png""/>    Once you are connected, you will see a desktop GUI that allows you to install and use full-fledged web-browsers or any other tool that is available for Ubuntu. Within the `Tools` folder on the desktop, you will find a collection of install scripts that makes it straightforward to install some of the most commonly used development tools, such as Atom, PyCharm, R-Runtime, R-Studio, or Postman (just double-click on the script).    **Clipboard:** If you want to share the clipboard between your machine and the workspace, you can use the copy-paste functionality as described below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/desktop-vnc-clipboard.png""/>    > 💡 _**Long-running tasks:** Use the desktop GUI for long-running Jupyter executions. By running notebooks from the browser of your workspace desktop GUI, all output will be synchronized to the notebook even if you have disconnected your browser from the notebook._    ### Visual Studio Code    [Visual Studio Code](https://github.com/microsoft/vscode) (`Open Tool -> VS Code`) is an open-source lightweight but powerful code editor with built-in support for a variety of languages and a rich ecosystem of extensions. It combines the simplicity of a source code editor with powerful developer tooling, like IntelliSense code completion and debugging. The workspace integrates VS Code as a web-based application accessible through the browser-based on the awesome [code-server](https://github.com/cdr/code-server) project. It allows you to customize every feature to your liking and install any number of third-party extensions.    <p align=""center""><img src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/vs-code.png""/></p>    The workspace also provides a VS Code integration into Jupyter allowing you to open a VS Code instance for any selected folder, as shown below:    <p align=""center""><img src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/vs-code-open.png""/></p>    ### JupyterLab    [JupyterLab](https://github.com/jupyterlab/jupyterlab) (`Open Tool -> JupyterLab`) is the next-generation user interface for Project Jupyter. It offers all the familiar building blocks of the classic Jupyter Notebook (notebook, terminal, text editor, file browser, rich outputs, etc.) in a flexible and powerful user interface. This JupyterLab instance comes pre-installed with a few helpful extensions such as a the [jupyterlab-toc](https://github.com/jupyterlab/jupyterlab-toc), [jupyterlab-git](https://github.com/jupyterlab/jupyterlab-git), and [juptyterlab-tensorboard](https://github.com/chaoleili/jupyterlab_tensorboard).    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/jupyterlab.png""/>    ### Git Integration    Version control is a crucial aspect of productive collaboration. To make this process as smooth as possible, we have integrated a custom-made Jupyter extension specialized on pushing single notebooks, a full-fledged web-based Git client ([ungit](https://github.com/FredrikNoren/ungit)), a tool to open and edit plain text documents (e.g., `.py`, `.md`) as notebooks ([jupytext](https://github.com/mwouts/jupytext)), as well as a notebook merging tool ([nbdime](https://github.com/jupyter/nbdime)). Additionally, JupyterLab and VS Code also provide GUI-based Git clients.    #### Clone Repository    For cloning repositories via `https`, we recommend to navigate to the desired root folder and to click on the `git` button as shown below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/git-open.png""/>    This might ask for some required settings and, subsequently, opens [ungit](https://github.com/FredrikNoren/ungit), a web-based Git client with a clean and intuitive UI that makes it convenient to sync your code artifacts. Within ungit, you can clone any repository. If authentication is required, you will get asked for your credentials.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/git-ungit-credentials.png""/>    #### Push, Pull, Merge, and Other Git Actions    To commit and push a single notebook to a remote Git repository, we recommend to use the Git plugin integrated into Jupyter, as shown below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/git-push-notebook.png""/>    For more advanced Git operations, we recommend to use [ungit](https://github.com/FredrikNoren/ungit). With ungit, you can do most of the common git actions such as push, pull, merge, branch, tag, checkout, and many more.    #### Diffing and Merging Notebooks    Jupyter notebooks are great, but they often are huge files, with a very specific JSON file format. To enable seamless diffing and merging via Git this workspace is pre-installed with [nbdime](https://github.com/jupyter/nbdime). Nbdime understands the structure of notebook documents and, therefore, automatically makes intelligent decisions when diffing and merging notebooks. In the case you have merge conflicts, nbdime will make sure that the notebook is still readable by Jupyter, as shown below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/git-nbdime-merging.png""/>    Furthermore, the workspace comes pre-installed with [jupytext](https://github.com/mwouts/jupytext), a Jupyter plugin that reads and writes notebooks as plain text files. This allows you to open, edit, and run scripts or markdown files (e.g., `.py`, `.md`) as notebooks within Jupyter. In the following screenshot, we have opened a markdown file via Jupyter:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/git-jupytext.png""/>    In combination with Git, jupytext enables a clear diff history and easy merging of version conflicts. With both of those tools, collaborating on Jupyter notebooks with Git becomes straightforward.    ### File Sharing    The workspace has a feature to share any file or folder with anyone via a token-protected link. To share data via a link, select any file or folder from the Jupyter directory tree and click on the share button as shown in the following screenshot:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/file-sharing-open.png""/>    This will generate a unique link protected via a token that gives anyone with the link access to view and download the selected data via the [Filebrowser](https://github.com/filebrowser/filebrowser) UI:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/file-sharing-filebrowser.png""/>    To deactivate or manage (e.g., provide edit permissions) shared links, open the Filebrowser via `Open Tool -> Filebrowser` and select `Settings->User Management`.    ### Access Ports    It is possible to securely access any workspace internal port by selecting `Open Tool -> Access Port`. With this feature, you are able to access a REST API or web application running inside the workspace directly with your browser. The feature enables developers  to build, run, test, and debug REST APIs or web applications directly from the workspace.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/access-port.png""/>    If you want to use an HTTP client or share access to a given port, you can select the `Get shareable link` option. This generates a token-secured link that anyone with access to the link can use to access the specified port.    > _The HTTP app requires to be resolved from a relative URL path or configure a base path (`/tools/PORT/`). Tools made accessible this way are secured by the workspace's authentication system! If you decide to publish any other port of the container yourself instead of using this feature to make a tool accessible, please make sure to secure it via an authentication mechanism!_    <details>    <summary>Example (click to expand...)</summary>    1. Start an HTTP server on port `1234` by running this command in a terminal within the workspace: `python -m http.server 1234`  2. Select `Open Tool -> Access Port`, input port `1234`, and select the `Get shareable link` option.  3. Click `Access`, and you will see the content provided by Python's `http.server`.  4. The opened link can also be shared to other people or called from external applications (e.g., try with Incognito Mode in Chrome).    </details>    ### SSH Access    SSH provides a powerful set of features that enables you to be more productive with your development tasks. You can easily set up a secure and passwordless SSH connection to a workspace by selecting `Open Tool -> SSH`. This will generate a secure setup command that can be run on any Linux or Mac machine to configure a passwordless & secure SSH connection to the workspace. Alternatively, you can also download the setup script and run it (instead of using the command).    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/ssh-access.png""/>    > _The setup script only runs on Mac and Linux. Windows is currently not supported._    Just run the setup command or script on the machine from where you want to setup a connection to the workspace and input a name for the connection (e.g., `my-workspace`). You might also get asked for some additional input during the process, e.g. to install a remote kernel if `remote_ikernel` is installed. Once the passwordless SSH connection is successfully setup and tested, you can securely connect to the workspace by simply executing `ssh my-workspace`.    Besides the ability to execute commands on a remote machine, SSH also provides a variety of other features that can improve your development workflow as described in the following sections.    <details>  <summary><b>Tunnel Ports</b> (click to expand...)</summary>    An SSH connection can be used for tunneling application ports from the remote machine to the local machine, or vice versa. For example, you can expose the workspace internal port `5901` (VNC Server) to the local machine on port `5000` by executing:    ```bash  ssh -nNT -L 5000:localhost:5901 my-workspace  ```    > _To expose an application port from your local machine to a workspace, use the `-R` option (instead of `-L`)._    After the tunnel is established, you can use your favorite VNC viewer on your local machine and connect to `vnc://localhost:5000` (default password: `vncpassword`). To make the tunnel connection more resistant and reliable, we recommend to use [autossh](https://www.harding.motd.ca/autossh/) to automatically restart SSH tunnels in the case that the connection dies:    ```bash  autossh -M 0 -f -nNT -L 5000:localhost:5901 my-workspace  ```    Port tunneling is quite useful when you have started any server-based tool within the workspace that you like to make accessible for another machine. In its default setting, the workspace has a variety of tools already running on different ports, such as:    - `8080`: Main workspace port with access to all integrated tools.  - `8090`: Jupyter server.  - `8054`: VS Code server.  - `5901`: VNC server.  - `22`: SSH server.    You can find port information on all the tools in the [supervisor configuration](https://github.com/ml-tooling/ml-workspace/blob/main/resources/supervisor/supervisord.conf).    > 📖 _For more information about port tunneling/forwarding, we recommend [this guide](https://www.everythingcli.org/ssh-tunnelling-for-fun-and-profit-local-vs-remote/)._    </details>    <details>  <summary><b>Copy Data via SCP</b> (click to expand...)</summary>    [SCP](https://linux.die.net/man/1/scp) allows files and directories to be securely copied to, from, or between different machines via SSH connections. For example, to copy a local file (`./local-file.txt`) into the `/workspace` folder inside the workspace, execute:    ```bash  scp ./local-file.txt my-workspace:/workspace  ```    To copy the `/workspace` directory from `my-workspace` to the working directory of the local machine, execute:    ```bash  scp -r my-workspace:/workspace .  ```    > 📖 _For more information about scp, we recommend [this guide](https://www.garron.me/en/articles/scp.html)._  </details>    <details>  <summary><b>Sync Data via Rsync</b> (click to expand...)</summary>    [Rsync](https://linux.die.net/man/1/rsync) is a utility for efficiently transferring and synchronizing files between different machines (e.g., via SSH connections) by comparing the modification times and sizes of files. The rsync command will determine which files need to be updated each time it is run, which is far more efficient and convenient than using something like scp or sftp. For example, to sync all content of a local folder (`./local-project-folder/`) into the `/workspace/remote-project-folder/` folder inside the workspace, execute:    ```bash  rsync -rlptzvP --delete --exclude="".git"" ""./local-project-folder/"" ""my-workspace:/workspace/remote-project-folder/""  ```    If you have some changes inside the folder on the workspace, you can sync those changes back to the local folder by changing the source and destination arguments:    ```bash  rsync -rlptzvP --delete --exclude="".git"" ""my-workspace:/workspace/remote-project-folder/"" ""./local-project-folder/""  ```    You can rerun these commands each time you want to synchronize the latest copy of your files. Rsync will make sure that only updates will be transferred.    > 📖 _You can find more information about rsync on [this man page](https://linux.die.net/man/1/rsync)._  </details>    <details>  <summary><b>Mount Folders via SSHFS</b> (click to expand...)</summary>    Besides copying and syncing data, an SSH connection can also be used to mount directories from a remote machine into the local filesystem via [SSHFS](https://github.com/libfuse/sshfs).   For example, to mount the `/workspace` directory of `my-workspace` into a local path (e.g. `/local/folder/path`), execute:    ```bash  sshfs -o reconnect my-workspace:/workspace /local/folder/path  ```    Once the remote directory is mounted, you can interact with the remote file system the same way as with any local directory and file.    > 📖 _For more information about sshfs, we recommend [this guide](https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh)._  </details>    ### Remote Development    The workspace can be integrated and used as a remote runtime (also known as remote kernel/machine/interpreter) for a variety of popular development tools and IDEs, such as Jupyter, VS Code, PyCharm, Colab, or Atom Hydrogen. Thereby, you can connect your favorite development tool running on your local machine to a remote machine for code execution. This enables a local-quality development experience with remote-hosted compute resources.    These integrations usually require a passwordless SSH connection from the local machine to the workspace. To set up an SSH connection, please follow the steps explained in the [SSH Access](#ssh-access) section.    <details>  <summary><b>Jupyter - Remote Kernel</b> (click to expand...)</summary>    The workspace can be added to a Jupyter instance as a remote kernel by using the [remote_ikernel](https://bitbucket.org/tdaff/remote_ikernel/) tool. If you have installed remote_ikernel (`pip install remote_ikernel`) on your local machine, the SSH setup script of the workspace will automatically offer you the option to setup a remote kernel connection.    > _When running kernels on remote machines, the notebooks themselves will be saved onto the local filesystem, but the kernel will only have access to the filesystem of the remote machine running the kernel. If you need to sync data, you can make use of rsync, scp, or sshfs as explained in the [SSH Access](#ssh-access) section._    In case you want to manually setup and manage remote kernels, use the [remote_ikernel](https://bitbucket.org/tdaff/remote_ikernel/src/default/README.rst) command-line tool, as shown below:    ```bash  # Change my-workspace with the name of a workspace SSH connection  remote_ikernel manage --add \      --interface=ssh \      --kernel_cmd=""ipython kernel -f {connection_file}"" \      --name=""ml-server (Python)"" \      --host=""my-workspace""  ```    You can use the remote_ikernel command line functionality to list (`remote_ikernel manage --show`) or delete (`remote_ikernel manage --delete <REMOTE_KERNEL_NAME>`) remote kernel connections.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/remote-dev-jupyter-kernel.png""/>    </details>    <details>  <summary><b>VS Code - Remote Machine</b> (click to expand...)</summary>    The Visual Studio Code [Remote - SSH](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh) extension allows you to open a remote folder on any remote machine with SSH access and work with it just as you would if the folder were on your own machine. Once connected to a remote machine, you can interact with files and folders anywhere on the remote filesystem and take full advantage of VS Code's feature set (IntelliSense, debugging, and extension support). The discovers and works out-of-the-box with passwordless SSH connections as configured by the workspace SSH setup script. To enable your local VS Code application to connect to a workspace:    1. Install [Remote - SSH](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh) extension inside your local VS Code.  2. Run the SSH setup script of a selected workspace as explained in the [SSH Access](#ssh-access) section.  3. Open the Remote-SSH panel in your local VS Code. All configured SSH connections should be automatically discovered. Just select any configured workspace connection you like to connect to as shown below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/remote-dev-vscode.gif""/>    > 📖 _You can find additional features and information about the Remote SSH extension in [this guide](https://code.visualstudio.com/docs/remote/ssh)._    </details>    ### Tensorboard    [Tensorboard](https://www.tensorflow.org/tensorboard) provides a suite of visualization tools to make it easier to understand, debug, and optimize your experiment runs. It includes logging features for scalar, histogram, model structure, embeddings, and text & image visualization. The workspace comes pre-installed with [jupyter_tensorboard extension](https://github.com/lspvic/jupyter_tensorboard) that integrates Tensorboard into the Jupyter interface with functionalities to start, manage, and stop instances. You can open a new instance for a valid logs directory, as shown below:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/tensorboard-open.png"" />    If you have opened a Tensorboard instance in a valid log directory, you will see the visualizations of your logged data:    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/tensorboard-dashboard.png"" />    > _Tensorboard can be used in combination with many other ML frameworks besides Tensorflow. By using the [tensorboardX](https://github.com/lanpa/tensorboardX) library you can log basically from any python based library. Also, PyTorch has a direct Tensorboard integration as described [here](https://pytorch.org/docs/stable/tensorboard.html)._    If you prefer to see the tensorboard directly within your notebook, you can make use of following **Jupyter magic**:    ```  %load_ext tensorboard  %tensorboard --logdir /workspace/path/to/logs  ```    ### Hardware Monitoring    The workspace provides two pre-installed web-based tools to help developers during model training and other experimentation tasks to get insights into everything happening on the system and figure out performance bottlenecks.    [Netdata](https://github.com/netdata/netdata) (`Open Tool -> Netdata`) is a real-time hardware and performance monitoring dashboard that visualize the processes and services on your Linux systems. It monitors metrics about CPU, GPU, memory, disks, networks, processes, and more.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/hardware-monitoring-netdata.png"" />    [Glances](https://github.com/nicolargo/glances) (`Open Tool -> Glances`) is a web-based hardware monitoring dashboard as well and can be used as an alternative to Netdata.    <img style=""width: 100%"" src=""https://github.com/ml-tooling/ml-workspace/raw/main/docs/images/features/hardware-monitoring-glances.png""/>    > _Netdata and Glances will show you the hardware statistics for the entire machine on which the workspace container is running._    ### Run as a job    > _A job is defined as any computational task that runs for a certain time to completion, such as a model training or a data pipeline._    The workspace image can also be used to execute arbitrary Python code without starting any of the pre-installed tools. This provides a seamless way to productize your ML projects since the code that has been developed interactively within the workspace will have the same environment and configuration when run as a job via the same workspace image.    <details>  <summary><b>Run Python code as a job via the workspace image</b> (click to expand...)</summary>    To run Python code as a job, you need to provide a path or URL to a code directory (or script) via `EXECUTE_CODE`. The code can be either already mounted into the workspace container or downloaded from a version control system (e.g., git or svn) as described in the following sections. The selected code path needs to be python executable. In case the selected code is a directory (e.g., whenever you download the code from a VCS) you need to put a `__main__.py` file at the root of this directory. The `__main__.py` needs to contain the code that starts your job.    #### Run code from version control system    You can execute code directly from Git, Mercurial, Subversion, or Bazaar by using the pip-vcs format as described in [this guide](https://pip.pypa.io/en/stable/reference/pip_install/#vcs-support). For example, to execute code from a [subdirectory](https://github.com/ml-tooling/ml-workspace/tree/main/resources/tests/ml-job) of a git repository, just run:    ```bash  docker run --env EXECUTE_CODE=""git+https://github.com/ml-tooling/ml-workspace.git#subdirectory=resources/tests/ml-job"" mltooling/ml-workspace:0.13.2  ```    > 📖 _For additional information on how to specify branches, commits, or tags please refer to [this guide](https://pip.pypa.io/en/stable/reference/pip_install/#vcs-support)._    #### Run code mounted into the workspace    In the following example, we mount and execute the current working directory (expected to contain our code) into the `/workspace/ml-job/` directory of the workspace:    ```bash  docker run -v ""${PWD}:/workspace/ml-job/"" --env EXECUTE_CODE=""/workspace/ml-job/"" mltooling/ml-workspace:0.13.2  ```    #### Install Dependencies    In the case that the pre-installed workspace libraries are not compatible with your code, you can install or change dependencies by just adding one or multiple of the following files to your code directory:    - `requirements.txt`: [pip requirements format](https://pip.pypa.io/en/stable/user_guide/#requirements-files) for pip-installable dependencies.  - `environment.yml`: [conda environment file](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html?highlight=environment.yml#creating-an-environment-file-manually) to create a separate Python environment.  - `setup.sh`: A shell script executed via `/bin/bash`.    The execution order is 1. `environment.yml` -> 2. `setup.sh` -> 3. `requirements.txt`    #### Test job in interactive mode    You can test your job code within the workspace (started normally with interactive tools) by executing the following python script:    ```bash  python /resources/scripts/execute_code.py /path/to/your/job  ```    #### Build a custom job image    It is also possible to embed your code directly into a custom job image, as shown below:    ```dockerfile  FROM mltooling/ml-workspace:0.13.2    # Add job code to image  COPY ml-job /workspace/ml-job  ENV EXECUTE_CODE=/workspace/ml-job    # Install requirements only  RUN python /resources/scripts/execute_code.py --requirements-only    # Execute only the code at container startup  CMD [""python"", ""/resources/docker-entrypoint.py"", ""--code-only""]  ```    </details>    ### Pre-installed Libraries and Interpreters    The workspace is pre-installed with many popular interpreters, data science libraries, and ubuntu packages:    - **Interpreter:** Python 3.8 (Miniconda 3), NodeJS 14, Scala, Perl 5  - **Python libraries:** Tensorflow, Keras, Pytorch, Sklearn, XGBoost, MXNet, Theano, and [many more](https://github.com/ml-tooling/ml-workspace/tree/main/resources/libraries)  - **Package Manager:** `conda`, `pip`, `apt-get`, `npm`, `yarn`, `sdk`, `poetry`, `gdebi`...      The full list of installed tools can be found within the [Dockerfile](https://github.com/ml-tooling/ml-workspace/blob/main/Dockerfile).    > _For every minor version release, we run vulnerability, virus, and security checks within the workspace using [safety](https://pyup.io/safety/), [clamav](https://www.clamav.net/), [trivy](https://github.com/aquasecurity/trivy), and [snyk via docker scan](https://docs.docker.com/engine/scan/) to make sure that the workspace environment is as secure as possible. We are committed to fix and prevent all high- or critical-severity vulnerabilities. You can find some up-to-date reports [here](https://github.com/ml-tooling/ml-workspace/tree/main/resources/reports)._    ### Extensibility    The workspace provides a high degree of extensibility. Within the workspace, you have **full root & sudo privileges** to install any library or tool you need via terminal (e.g., `pip`, `apt-get`, `conda`, or `npm`). You can open a terminal by one of the following ways:    - **Jupyter:** `New -> Terminal`  - **Desktop VNC:** `Applications -> Terminal Emulator`  - **JupyterLab:** `File -> New -> Terminal`  - **VS Code:** `Terminal -> New Terminal`    Additionally, pre-installed tools such as Jupyter, JupyterLab, and Visual Studio Code each provide their own rich ecosystem of extensions. The workspace also contains a [collection of installer scripts](https://github.com/ml-tooling/ml-workspace/tree/main/resources/tools) for many commonly used development tools or libraries (e.g., `PyCharm`, `Zeppelin`, `RStudio`, `Starspace`). You can find and execute all tool installers via `Open Tool -> Install Tool`. Those scripts can be also executed from the Desktop VNC (double-click on the script within the `Tools` folder on the Desktop VNC).    <details>  <summary>Example (click to expand...)</summary>    For example, to install the [Apache Zeppelin](https://zeppelin.apache.org/) notebook server, simply execute:    ```bash  /resources/tools/zeppelin.sh --port=1234  ```    After installation, refresh the Jupyter website and the Zeppelin tool will be available under `Open Tool -> Zeppelin`. Other tools might only be available within the Desktop VNC (e.g., `atom` or `pycharm`) or do not provide any UI (e.g., `starspace`, `docker-client`).  </details>    As an alternative to extending the workspace at runtime, you can also customize the workspace Docker image to create your own flavor as explained in the [FAQ](#faq) section.    ---    <br>    ## FAQ    <details>  <summary><b>How to customize the workspace image (create your own flavor)?</b> (click to expand...)</summary>    The workspace can be extended in many ways at runtime, as explained [here](#extensibility). However, if you like to customize the workspace image with your own software or configuration, you can do that via a Dockerfile as shown below:    ```dockerfile  # Extend from any of the workspace versions/flavors  FROM mltooling/ml-workspace:0.13.2    # Run you customizations, e.g.  RUN \      # Install r-runtime, r-kernel, and r-studio web server from provided install scripts      /bin/bash $RESOURCES_PATH/tools/r-runtime.sh --install && \      /bin/bash $RESOURCES_PATH/tools/r-studio-server.sh --install && \      # Cleanup Layer - removes unneccessary cache files      clean-layer.sh  ```    Finally, use [docker build](https://docs.docker.com/engine/reference/commandline/build/) to build your customized Docker image.    > 📖 _For a more comprehensive Dockerfile example, take a look at the [Dockerfile of the R-flavor](https://github.com/ml-tooling/ml-workspace/blob/main/r-flavor/Dockerfile)._    </details>    <details>  <summary><b>How to update a running workspace container?</b> (click to expand...)</summary>    To update a running workspace instance to a more recent version, the running Docker container needs to be replaced with a new container based on the updated workspace image.    All data within the workspace that is not persisted to a mounted volume will be lost during this update process. As mentioned in the [persist data](#Persist-Data) section, a volume is expected to be mounted into the `/workspace` folder. All tools within the workspace are configured to make use of the `/workspace` folder as the root directory for all source code and data artifacts. During an update, data within other directories will be removed, including installed/updated libraries or certain machine configurations. We have integrated a backup and restore feature (`CONFIG_BACKUP_ENABLED`) for various selected configuration files/folders, such as the user's Jupyter/VS-Code configuration, `~/.gitconfig`, and `~/.ssh`.    <details>    <summary>Update Example (click to expand...)</summary>    If the workspace is deployed via Docker (Kubernetes will have a different update process), you need to remove the existing container (via `docker rm`) and start a new one (via `docker run`) with the newer workspace image. Make sure to use the same configuration, volume, name, and port. For example, a workspace (image version `0.8.7`) was started with this command:  ```  docker run -d \      -p 8080:8080 \      --name ""ml-workspace"" \      -v ""/path/on/host:/workspace"" \      --env AUTHENTICATE_VIA_JUPYTER=""mytoken"" \      --restart always \      mltooling/ml-workspace:0.8.7  ```  and needs to be updated to version `0.9.1`, you need to:    1. Stop and remove the running workspace container: `docker stop ""ml-workspace"" && docker rm ""ml-workspace""`  2. Start a new workspace container with the newer image and same configuration: `docker run -d -p 8080:8080 --name ""ml-workspace"" -v ""/path/on/host:/workspace"" --env AUTHENTICATE_VIA_JUPYTER=""mytoken"" --restart always mltooling/ml-workspace:0.9.1`    </details>    </details>    <details>  <summary><b>How to configure the VNC server?</b> (click to expand...)</summary>    If you want to directly connect to the workspace via a VNC client (not using the [noVNC webapp](#desktop-gui)), you might be interested in changing certain VNC server configurations. To configure the VNC server, you can provide/overwrite the following environment variables at container start (via docker run option: `--env`):    <table>      <tr>          <th>Variable</th>          <th>Description</th>          <th>Default</th>      </tr>      <tr>          <td>VNC_PW</td>          <td>Password of VNC connection. This password only needs to be secure if the VNC server is directly exposed. If it is used via noVNC, it is already protected based on the configured authentication mechanism.</td>          <td>vncpassword</td>      </tr>      <tr>          <td>VNC_RESOLUTION</td>          <td>Default desktop resolution of VNC connection. When using noVNC, the resolution will be dynamically adapted to the window size.</td>          <td>1600x900</td>      </tr>      <tr>          <td>VNC_COL_DEPTH</td>          <td>Default color depth of VNC connection.</td>          <td>24</td>      </tr>  </table>    </details>    <details>  <summary><b>How to use a non-root user within the workspace?</b> (click to expand...)</summary>    Unfortunately, we currently do not support using a non-root user within the workspace. We plan to provide this capability and already started with some refactoring to allow this configuration. However, this still requires a lot more work, refactoring, and testing from our side.    Using root-user (or users with sudo permission) within containers is generally not recommended since, in case of system/kernel vulnerabilities, a user might be able to break out of the container and be able to access the host system. Since it is not very common to have such problematic kernel vulnerabilities, the risk of a severe attack is quite minimal. As explained in the [official Docker documentation](https://docs.docker.com/engine/security/security/#linux-kernel-capabilities), containers (even with root users) are generally quite secure in preventing a breakout to the host. And compared to many other container use-cases, we actually want to provide the flexibility to the user to have control and system-level installation permissions within the workspace container.    </details>    <details>  <summary><b>How to create and use a virtual environment?</b> (click to expand...)</summary>    The workspace comes preinstalled with various common tools to create isolated Python environments (virtual environments). The following sections provide a quick-intro on how to use these tools within the workspace. You can find information on when to use which tool [here](https://stackoverflow.com/a/41573588). Please refer to the documentation of the given tool for additional usage information.    **venv** (recommended):    To create a virtual environment via [venv](https://docs.python.org/3/tutorial/venv.html), execute the following commands:    ```bash  # Create environment in the working directory  python -m venv my-venv  # Activate environment in shell  source ./my-venv/bin/activate  # Optional: Create Jupyter kernel for this environment  pip install ipykernel  python -m ipykernel install --user --name=my-venv --display-name=""my-venv ($(python --version))""  # Optional: Close enviornment session  deactivate  ```    **pipenv** (recommended):    To create a virtual environment via [pipenv](https://pipenv.pypa.io/en/latest/), execute the following commands:    ```bash  # Create environment in the working directory  pipenv install  # Activate environment session in shell  pipenv shell  # Optional: Create Jupyter kernel for this environment  pipenv install ipykernel  python -m ipykernel install --user --name=my-pipenv --display-name=""my-pipenv ($(python --version))""  # Optional: Close environment session  exit  ```    **virtualenv**:    To create a virtual environment via [virtualenv](https://virtualenv.pypa.io/en/latest/), execute the following commands:    ```bash  # Create environment in the working directory  virtualenv my-virtualenv  # Activate environment session in shell  source ./my-virtualenv/bin/activate  # Optional: Create Jupyter kernel for this environment  pip install ipykernel  python -m ipykernel install --user --name=my-virtualenv --display-name=""my-virtualenv ($(python --version))""  # Optional: Close environment session  deactivate  ```    **conda**:    To create a virtual environment via [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html), execute the following commands:    ```bash  # Create environment (globally)  conda create -n my-conda-env  # Activate environment session in shell  conda activate my-conda-env  # Optional: Create Jupyter kernel for this environment  python -m ipykernel install --user --name=my-conda-env --display-name=""my-conda-env ($(python --version))""  # Optional: Close environment session  conda deactivate  ```    **Tip: Shell Commands in Jupyter Notebooks:**    If you install and use a virtual environment via a dedicated Jupyter Kernel and use shell commands within Jupyter (e.g. `!pip install matplotlib`), the wrong python/pip version will be used. To use the python/pip version of the selected kernel, do the following instead:    ```python  import sys  !{sys.executable} -m pip install matplotlib  ```    </details>    <details>  <summary><b>How to install a different Python version?</b> (click to expand...)</summary>    The workspace provides three easy options to install different Python versions alongside the main Python instance: [pyenv](https://github.com/pyenv/pyenv), [pipenv](https://pipenv.pypa.io/en/latest/cli/) (recommended), [conda](https://github.com/pyenv/pyenv).    **pipenv** (recommended):    To install a different python version (e.g. `3.7.8`) within the workspace via [pipenv](https://pipenv.pypa.io/en/latest/cli/), execute the following commands:    ```bash  # Install python vers  pipenv install --python=3.7.8  # Activate environment session in shell  pipenv shell  # Check python installation  python --version  # Optional: Create Jupyter kernel for this environment  pipenv install ipykernel  python -m ipykernel install --user --name=my-pipenv --display-name=""my-pipenv ($(python --version))""  # Optional: Close environment session  exit  ```    **pyenv**:    To install a different python version (e.g. `3.7.8`) within the workspace via [pyenv](https://github.com/pyenv/pyenv), execute the following commands:    ```bash  # Install python version  pyenv install 3.7.8  # Make globally accessible  pyenv global 3.7.8  # Activate python version in shell  pyenv shell 3.7.8  # Check python installation  python3.7 --version  # Optional: Create Jupyter kernel for this python version  python3.7 -m pip install ipykernel  python3.7 -m ipykernel install --user --name=my-pyenv-3.7.8 --display-name=""my-pyenv (Python 3.7.8)""  ```    **conda**:    To install a different python version (e.g. `3.7.8`) within the workspace via [conda](https://github.com/pyenv/pyenv), execute the following commands:    ```bash  # Create environment with python version  conda create -n my-conda-3.7 python=3.7.8  # Activate environment session in shell  conda activate my-conda-3.7  # Check python installation  python --version  # Optional: Create Jupyter kernel for this python version  pip install ipykernel  python -m ipykernel install --user --name=my-conda-3.7 --display-name=""my-conda ($(python --version))""  # Optional: Close environment session  conda deactivate  ```    **Tip: Shell Commands in Jupyter Notebooks:**    If you install and use another Python version via a dedicated Jupyter Kernel and use shell commands within Jupyter (e.g. `!pip install matplotlib`), the wrong python/pip version will be used. To use the python/pip version of the selected kernel, do the following instead:    ```python  import sys  !{sys.executable} -m pip install matplotlib  ```    </details>    <details>  <summary><b>Can I publish any other than the default port to access a tool inside the container?</b> (click to expand...)</summary>  You can do this, but please be aware that this port is <b>not</b> protected by the workspace's authentication mechanism then! For security reasons, we therefore highly recommend to use the <a href=""#access-ports"">Access Ports</a> functionality of the workspace.  </details>    <details>  <summary><b>System and Tool Translations</b> (click to expand...)</summary>  If you want to configure another language than English in your workspace and some tools are not translated properly, have a look <a href=""https://github.com/ml-tooling/ml-workspace/issues/70#issuecomment-841863145"">at this issue</a>. Try to comment out the 'exclude translations' line in `/etc/dpkg/dpkg.cfg.d/excludes` and re-install / configure the package.  </details>    ---    <br>    ## Known Issues    <details>    <summary><b>Too small shared memory might crash tools or scripts</b> (click to expand...)</summary>    Certain desktop tools (e.g., recent versions of [Firefox](https://github.com/jlesage/docker-firefox#increasing-shared-memory-size)) or libraries (e.g., Pytorch - see Issues: [1](https://github.com/pytorch/pytorch/issues/2244), [2](https://github.com/pytorch/pytorch/issues/1355)) might crash if the shared memory size (`/dev/shm`) is too small. The default shared memory size of Docker is 64MB, which might not be enough for a few tools. You can provide a higher shared memory size via the `shm-size` docker run option:    ```bash  docker run --shm-size=2G mltooling/ml-workspace:0.13.2  ```    </details>    <details>    <summary><b>Multiprocessing code is unexpectedly slow </b> (click to expand...)</summary>    In general, the performance of running code within Docker is [nearly identical](https://stackoverflow.com/questions/21889053/what-is-the-runtime-performance-cost-of-a-docker-container) compared to running it directly on the machine. However, in case you have limited the container's CPU quota (as explained in [this section](#limit-memory--cpu)), the container can still see the full count of CPU cores available on the machine and there is no technical way to prevent this. Many libraries and tools will use the full CPU count (e.g., via `os.cpu_count()`) to set the number of threads used for multiprocessing/-threading. This might cause the program to start more threads/processes than it can efficiently handle with the available CPU quota, which can tremendously slow down the overall performance. Therefore, it is important to set the available CPU count or the maximum number of threads explicitly to the configured CPU quota. The workspace provides capabilities to detect the number of available CPUs automatically, which are used to configure a variety of common libraries via environment variables such as `OMP_NUM_THREADS` or `MKL_NUM_THREADS`. It is also possible to explicitly set the number of available CPUs at container startup via the `MAX_NUM_THREADS` environment variable (see [configuration section](https://github.com/ml-tooling/ml-workspace#configuration-options)). The same environment variable can also be used to get the number of available CPUs at runtime.    Even though the automatic configuration capabilities of the workspace will fix a variety of inefficiencies, we still recommend configuring the number of available CPUs with all libraries explicitly. For example:    ```python  import os  MAX_NUM_THREADS = int(os.getenv(""MAX_NUM_THREADS""))    # Set in pytorch  import torch  torch.set_num_threads(MAX_NUM_THREADS)    # Set in tensorflow  import tensorflow as tf  config = tf.ConfigProto(      device_count={""CPU"": MAX_NUM_THREADS},      inter_op_parallelism_threads=MAX_NUM_THREADS,      intra_op_parallelism_threads=MAX_NUM_THREADS,  )  tf_session = tf.Session(config=config)    # Set session for keras  import keras.backend as K  K.set_session(tf_session)    # Set in sklearn estimator  from sklearn.linear_model import LogisticRegression  LogisticRegression(n_jobs=MAX_NUM_THREADS).fit(X, y)    # Set for multiprocessing pool  from multiprocessing import Pool    with Pool(MAX_NUM_THREADS) as pool:      results = pool.map(lst)  ```    </details>    <details>    <summary><b>Nginx terminates with SIGILL core dumped error</b> (click to expand...)</summary>    If you encounter the following error within the container logs when starting the workspace, it will most likely not be possible to run the workspace on your hardware:    ```  exited: nginx (terminated by SIGILL (core dumped); not expected)  ```    The OpenResty/Nginx binary package used within the workspace requires to run on a CPU with `SSE4.2` support (see [this issue](https://github.com/openresty/openresty/issues/267#issuecomment-309296900)). Unfortunately, some older CPUs do not have support for `SSE4.2` and, therefore, will not be able to run the workspace container. On Linux, you can check if your CPU supports `SSE4.2` when looking into the `cat /proc/cpuinfo` flags section. If you encounter this problem, feel free to notify us by commenting on the following issue: [#30](https://github.com/ml-tooling/ml-workspace/issues/30).    </details>    ---    <br>    ## Contribution    - Pull requests are encouraged and always welcome. Read our [contribution guidelines](https://github.com/ml-tooling/ml-workspace/tree/main/CONTRIBUTING.md) and check out [help-wanted](https://github.com/ml-tooling/ml-workspace/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3A""help+wanted""+sort%3Areactions-%2B1-desc+) issues.  - Submit Github issues for any [feature request and enhancement](https://github.com/ml-tooling/ml-workspace/issues/new?assignees=&labels=feature&template=02_feature-request.md&title=), [bugs](https://github.com/ml-tooling/ml-workspace/issues/new?assignees=&labels=bug&template=01_bug-report.md&title=), or [documentation](https://github.com/ml-tooling/ml-workspace/issues/new?assignees=&labels=documentation&template=03_documentation.md&title=) problems.  - By participating in this project, you agree to abide by its [Code of Conduct](https://github.com/ml-tooling/ml-workspace/blob/main/.github/CODE_OF_CONDUCT.md).  - The [development section](#development) below contains information on how to build and test the project after you have implemented some changes.    ## Development    > _**Requirements**: [Docker](https://docs.docker.com/get-docker/) and [Act](https://github.com/nektos/act#installation) are required to be installed on your machine to execute the build process._    To simplify the process of building this project from scratch, we provide build-scripts - based on [universal-build](https://github.com/ml-tooling/universal-build) - that run all necessary steps (build, test, and release) within a containerized environment. To build and test your changes, execute the following command in the project root folder:    ```bash  act -b -j build  ```    Under the hood it uses the build.py files in this repo based on the [universal-build library](https://github.com/ml-tooling/universal-build). So, if you want to build it locally, you can also execute this command in the project root folder to build the docker container:    ```bash  python build.py --make  ```    For additional script options:    ```bash  python build.py --help  ```    Refer to our [contribution guides](https://github.com/ml-tooling/ml-workspace/blob/main/CONTRIBUTING.md#development-instructions) for more detailed information on our build scripts and development process.    ---    Licensed **Apache 2.0**. Created and maintained with ❤️&nbsp; by developers from Berlin. """
Big data;https://github.com/metabase/metabase;"""# Metabase    [Metabase](https://www.metabase.com/) is the easy, open-source way for everyone in your company to ask questions and learn from data.    ![Metabase Product Screenshot](docs/metabase-product-screenshot.png)    [![Latest Release](https://img.shields.io/github/release/metabase/metabase.svg?label=latest%20release)](https://github.com/metabase/metabase/releases)  [![Circle CI](https://circleci.com/gh/metabase/metabase.svg?style=svg&circle-token=3ccf0aa841028af027f2ac9e8df17ce603e90ef9)](https://circleci.com/gh/metabase/metabase)  [![codecov](https://codecov.io/gh/metabase/metabase/branch/master/graph/badge.svg)](https://codecov.io/gh/metabase/metabase)    ## Features    - [Set up in five minutes](https://metabase.com/docs/latest/setting-up-metabase.html) (we're not kidding).  - Let anyone on your team [ask questions](https://metabase.com/docs/latest/users-guide/04-asking-questions.html) without knowing SQL.  - Use the [SQL editor](https://www.metabase.com/docs/latest/users-guide/writing-sql.html) for more complex queries.   - Build handsome, interactive [dashboards](https://metabase.com/docs/latest/users-guide/06-sharing-answers.html) with filters, auto-refresh, fullscreen, and custom click behavior.  - Create [models](https://www.metabase.com/learn/getting-started/models) that clean up, annotate, and/or combine raw tables.  - Define canonical [segments and metrics](https://metabase.com/docs/latest/administration-guide/07-segments-and-metrics.html) for your team to use.  - Send data to Slack or email on a schedule with [dashboard subscriptions](https://www.metabase.com/docs/latest/users-guide/dashboard-subscriptions.html).  - Set up [alerts](https://www.metabase.com/docs/latest/users-guide/15-alerts.html) to have Metabase notify you when your data changes.  - [Embed charts and dashboards](https://www.metabase.com/docs/latest/administration-guide/13-embedding.html) in your app, or even [your entire Metabase](https://www.metabase.com/docs/latest/enterprise-guide/full-app-embedding.html).    Take a [tour of Metabase](https://www.metabase.com/learn/getting-started/tour-of-metabase).    ## Supported databases    - [Officially supported databases](https://www.metabase.com/docs/latest/administration-guide/01-managing-databases.html#officially-supported-databases).  - [Community-supported drivers](https://www.metabase.com/docs/latest/developers-guide-drivers.html#how-to-use-a-community-built-driver).    ## Installation    Metabase can be run just about anywhere. Check out our [Installation Guides](https://www.metabase.com/docs/latest/operations-guide/installing-metabase.html).    ## Contributing    To get started with a development installation of the Metabase, check out our [Developers Guide](https://www.metabase.com/docs/latest/developers-guide/start).    ## Internationalization    We want Metabase to be available in as many languages as possible. See which translations are available and help contribute to internationalization using our project over at [POEditor](https://poeditor.com/join/project/ynjQmwSsGh). You can also check out our [policies on translations](https://www.metabase.com/docs/latest/administration-guide/localization.html).    ## Extending Metabase    Metabase also allows you to hit our Query API directly from Javascript to integrate the simple analytics we provide with your own application or third party services to do things like:    - Build moderation interfaces.  - Export subsets of your users to third party marketing automation software.  - Provide a specialized customer lookup application for the people in your company.    Check out our guide, [Working with the Metabase API](https://www.metabase.com/learn/administration/metabase-api).    ## Security Disclosure    See [SECURITY.md](./SECURITY.md) for details.    ## License    This repository contains the source code for both the Open Source edition of Metabase, released under the AGPL, as well as the [commercial editions of Metabase](https://www.metabase.com/pricing/), which are released under the Metabase Commercial Software License.     See [LICENSE.txt](./LICENSE.txt) for details.    Unless otherwise noted, all files © 2022 Metabase, Inc. """
Big data;https://github.com/tarantool/tarantool;"""# Tarantool    [![Actions Status][actions-badge]][actions-url]  [![Code Coverage][coverage-badge]][coverage-url]  [![Telegram][telegram-badge]][telegram-url]  [![GitHub Discussions][discussions-badge]][discussions-url]  [![Stack Overflow][stackoverflow-badge]][stackoverflow-url]    [Tarantool][tarantool-url] is an in-memory computing platform consisting of a  database and an application server.    It is distributed under [BSD 2-Clause][license] terms.    Key features of the application server:    * Heavily optimized Lua interpreter with incredibly fast tracing JIT compiler,    based on LuaJIT 2.1.  * Cooperative multitasking, non-blocking IO.  * [Persistent queues][queue].  * [Sharding][vshard].  * [Cluster and application management framework][cartridge].  * Access to external databases such as [MySQL][mysql] and [PostgreSQL][pg].  * A rich set of built-in and standalone [modules][modules].    Key features of the database:    * MessagePack data format and MessagePack based client-server protocol.  * Two data engines: 100% in-memory with complete WAL-based persistence and an    own implementation of LSM-tree, to use with large data sets.  * Multiple index types: HASH, TREE, RTREE, BITSET.  * Document oriented JSON path indexes.  * Asynchronous master-master replication.  * Synchronous quorum-based replication.  * RAFT-based automatic leader election for the single-leader configuration.  * Authentication and access control.  * ANSI SQL, including views, joins, referential and check constraints.  * [Connectors][connectors] for many programming languages.  * The database is a C extension of the application server and can be turned    off.    Supported platforms are Linux (x86_64, aarch64), Mac OS X (x86_64, M1), FreeBSD  (x86_64).    Tarantool is ideal for data-enriched components of scalable Web architecture:  queue servers, caches, stateful Web applications.    To download and install Tarantool as a binary package for your OS or using  Docker, please see the [download instructions][download].    To build Tarantool from source, see detailed [instructions][building] in the  Tarantool documentation.    To find modules, connectors and tools for Tarantool, check out our [Awesome  Tarantool][awesome-list] list.    Please report bugs to our [issue tracker][issue-tracker]. We also warmly  welcome your feedback on the [discussions][discussions-url] page and questions  on [Stack Overflow][stackoverflow-url].    We accept contributions via pull requests. Check out our [How to get  involved][get-involved] guide.    Thank you for your interest in Tarantool!    [actions-badge]: https://github.com/tarantool/tarantool/workflows/release/badge.svg  [actions-url]: https://github.com/tarantool/tarantool/actions  [coverage-badge]: https://coveralls.io/repos/github/tarantool/tarantool/badge.svg?branch=master  [coverage-url]: https://coveralls.io/github/tarantool/tarantool?branch=master  [telegram-badge]: https://img.shields.io/badge/Telegram-join%20chat-blue.svg  [telegram-url]: http://telegram.me/tarantool  [discussions-badge]: https://img.shields.io/github/discussions/tarantool/tarantool  [discussions-url]: https://github.com/tarantool/tarantool/discussions  [stackoverflow-badge]: https://img.shields.io/badge/stackoverflow-tarantool-orange.svg  [stackoverflow-url]: https://stackoverflow.com/questions/tagged/tarantool  [tarantool-url]: https://www.tarantool.io/en/  [license]: LICENSE  [modules]: https://www.tarantool.io/en/download/rocks  [queue]: https://github.com/tarantool/queue  [vshard]: https://github.com/tarantool/vshard  [cartridge]: https://github.com/tarantool/cartridge  [mysql]: https://github.com/tarantool/mysql  [pg]: https://github.com/tarantool/pg  [connectors]: https://www.tarantool.io/en/download/connectors  [download]: https://www.tarantool.io/en/download/  [building]: https://www.tarantool.io/en/doc/latest/dev_guide/building_from_source/  [issue-tracker]: https://github.com/tarantool/tarantool/issues  [get-involved]: https://www.tarantool.io/en/doc/latest/contributing/contributing/  [awesome-list]: https://github.com/tarantool/awesome-tarantool/ """
Big data;https://github.com/square/cubism;"""# Cubism.js    Cubism.js is a [D3](http://d3js.org) plugin for visualizing time series. Use Cubism to construct better realtime dashboards, pulling data from [Graphite](https://github.com/square/cubism/wiki/Graphite), [Cube](https://github.com/square/cubism/wiki/Cube) and other sources. Cubism is available under the [Apache License](LICENSE).    Want to learn more? [See the wiki.](https://github.com/square/cubism/wiki) """
Big data;https://github.com/spring-projects/spring-xd;"""Spring XD  =========    *Spring XD* makes it easy to solve common big data problems such as data ingestion and export, real-time analytics, and batch workflow orchestration.  By building on mature, open source projects such as Spring Integration, Data and Batch, Spring XD will simplify the process of creating real-word big data solutions.  XD stands for 'eXtreme Data' or 'x' as in y=mx+b :)    While it is possible today to build such solutions using Spring (see the [Spring Data Book][] for details and examples), Spring XD will move well beyond the framework API level by providing an out-of-the-box executable server, a pluggable module system, a high level configuration DSL, a simple model for distributing data processing instances on or off the Hadoop cluster, and more.    You can fork the repository and/or monitor JIRA to see what is going on. As always, we consider the feedback from our broad and passionate community to be one of our greatest assets.    ## Documentation    Look for it on the [XD wiki](https://github.com/springsource/spring-xd/wiki). [API Documentation](http://static.springsource.org/spring-xd/docs/current-SNAPSHOT/api/) (JavaDoc) is available as well. Please also visit the SpringSource.org [project website](http://www.springsource.org/spring-xd) for more information.    ## How to build     Check the documentation on how to build Spring XD [here](http://docs.spring.io/spring-xd/docs/current-SNAPSHOT/reference/html/#building-spring-xd).    ## Getting Help    * Get involved with the community on StackOverflow using the tag spring-xd.    ## License    *Spring XD* is released under version 2.0 of the [Apache License][].    ## Contributing to Spring XD    Here are some ways for you to get involved     * Create [JIRA](https://jira.springsource.org/browse/XD) tickets for bugs and new features and comment and vote on the ones that you are interested in.  * Follow the flow of developing on the [work board](https://jira.springsource.org/secure/RapidBoard.jspa?rapidView=6).  * Github is for social coding: if you want to write code, we encourage contributions through pull requests from [forks of this repository](http://help.github.com/forking/).  If you want to contribute code this way, please familiarize yourself with the process outlined for contributing to Spring projects here: [Contributor Guidelines](https://github.com/SpringSource/spring-integration/wiki/Contributor-Guidelines).    Before we accept a non-trivial patch or pull request we will need you to sign the [contributor's agreement](https://support.springsource.com/spring_committer_signup).  Signing the contributor's agreement does not grant anyone commit rights to the main repository, but it does mean that we can accept your contributions, and you will get an author credit if we do.  Active contributors might be asked to join the core team, and given the ability to merge pull requests.    ## Issue Tracking    Report issues via the [Spring XD JIRA][].    ## Continuous Integration    * **Master**: https://build.spring.io/browse/XD-MASTER  * **Sonar**: https://build.spring.io/browse/XD-SONAR    ## Metrics    Source Metrics are available via Sonar at:    * https://sonar.springsource.org/dashboard/index/org.springframework.xd:spring-xd      [Spring XD JIRA]: https://jira.springsource.org/browse/XD  [Apache License]: http://www.apache.org/licenses/LICENSE-2.0  [Spring Data Book]: http://bit.ly/sd-book  """
Big data;https://github.com/Yelp/elastalert;"""Recent changes: As of Elastalert 0.2.0, you must use Python 3.6. Python 2 will not longer be supported.    [![Build Status](https://travis-ci.org/Yelp/elastalert.svg)](https://travis-ci.org/Yelp/elastalert)  [![Join the chat at https://gitter.im/Yelp/elastalert](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/Yelp/elastalert?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    ## ElastAlert - [Read the Docs](http://elastalert.readthedocs.org).  ### Easy & Flexible Alerting With Elasticsearch    ElastAlert is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch.    ElastAlert works with all versions of Elasticsearch.    At Yelp, we use Elasticsearch, Logstash and Kibana for managing our ever increasing amount of data and logs.  Kibana is great for visualizing and querying data, but we quickly realized that it needed a companion tool for alerting  on inconsistencies in our data. Out of this need, ElastAlert was created.    If you have data being written into Elasticsearch in near real time and want to be alerted when that data matches certain patterns, ElastAlert is the tool for you. If you can see it in Kibana, ElastAlert can alert on it.    ## Overview    We designed ElastAlert to be reliable, highly modular, and easy to set up and configure.    It works by combining Elasticsearch with two types of components, rule types and alerts.  Elasticsearch is periodically queried and the data is passed to the rule type, which determines when  a match is found. When a match occurs, it is given to one or more alerts, which take action based on the match.    This is configured by a set of rules, each of which defines a query, a rule type, and a set of alerts.    Several rule types with common monitoring paradigms are included with ElastAlert:    - Match where there are at least X events in Y time"" (``frequency`` type)  - Match when the rate of events increases or decreases"" (``spike`` type)  - Match when there are less than X events in Y time"" (``flatline`` type)  - Match when a certain field matches a blacklist/whitelist"" (``blacklist`` and ``whitelist`` type)  - Match on any event matching a given filter"" (``any`` type)  - Match when a field has two different values within some time"" (``change`` type)  - Match when a never before seen term appears in a field"" (``new_term`` type)  - Match when the number of unique values for a field is above or below a threshold (``cardinality`` type)    Currently, we have built-in support for the following alert types:    - Email  - JIRA  - OpsGenie  - Commands  - HipChat  - MS Teams  - Slack  - Telegram  - GoogleChat  - AWS SNS  - VictorOps  - PagerDuty  - PagerTree  - Exotel  - Twilio  - Gitter  - Line Notify  - Zabbix    Additional rule types and alerts can be easily imported or written.    In addition to this basic usage, there are many other features that make alerts more useful:    - Alerts link to Kibana dashboards  - Aggregate counts for arbitrary fields  - Combine alerts into periodic reports  - Separate alerts by using a unique key field  - Intercept and enhance match data    To get started, check out `Running ElastAlert For The First Time` in the [documentation](http://elastalert.readthedocs.org).    ## Running ElastAlert  You can either install the latest released version of ElastAlert using pip:    ```pip install elastalert```    or you can clone the ElastAlert repository for the most recent changes:    ```git clone https://github.com/Yelp/elastalert.git```    Install the module:    ```pip install ""setuptools>=11.3""```    ```python setup.py install```    The following invocation can be used to run ElastAlert after installing    ``$ elastalert [--debug] [--verbose] [--start <timestamp>] [--end <timestamp>] [--rule <filename.yaml>] [--config <filename.yaml>]``    ``--debug`` will print additional information to the screen as well as suppresses alerts and instead prints the alert body. Not compatible with `--verbose`.    ``--verbose`` will print additional information without suppressing alerts. Not compatible with `--debug.`    ``--start`` will begin querying at the given timestamp. By default, ElastAlert will begin querying from the present.  Timestamp format is ``YYYY-MM-DDTHH-MM-SS[-/+HH:MM]`` (Note the T between date and hour).  Eg: ``--start 2014-09-26T12:00:00`` (UTC) or ``--start 2014-10-01T07:30:00-05:00``    ``--end`` will cause ElastAlert to stop querying at the given timestamp. By default, ElastAlert will continue  to query indefinitely.    ``--rule`` will allow you to run only one rule. It must still be in the rules folder.  Eg: ``--rule this_rule.yaml``    ``--config`` allows you to specify the location of the configuration. By default, it is will look for config.yaml in the current directory.    ## Third Party Tools And Extras  ### Kibana plugin  ![img](https://raw.githubusercontent.com/bitsensor/elastalert-kibana-plugin/master/showcase.gif)  Available at the [ElastAlert Kibana plugin repository](https://github.com/bitsensor/elastalert-kibana-plugin).    ### Docker  A [Dockerized version](https://github.com/bitsensor/elastalert) of ElastAlert including a REST api is build from `master` to `bitsensor/elastalert:latest`.    ```bash  git clone https://github.com/bitsensor/elastalert.git; cd elastalert  docker run -d -p 3030:3030 \      -v `pwd`/config/elastalert.yaml:/opt/elastalert/config.yaml \      -v `pwd`/config/config.json:/opt/elastalert-server/config/config.json \      -v `pwd`/rules:/opt/elastalert/rules \      -v `pwd`/rule_templates:/opt/elastalert/rule_templates \      --net=""host"" \      --name elastalert bitsensor/elastalert:latest  ```    ## Documentation    Read the documentation at [Read the Docs](http://elastalert.readthedocs.org).    To build a html version of the docs locally    ```  pip install sphinx_rtd_theme sphinx  cd docs  make html  ```    View in browser at build/html/index.html    ## Configuration    See config.yaml.example for details on configuration.    ## Example rules    Examples of different types of rules can be found in example_rules/.    - ``example_spike.yaml`` is an example of the ""spike"" rule type, which allows you to alert when the rate of events, averaged over a time period,  increases by a given factor. This example will send an email alert when there are 3 times more events matching a filter occurring within the  last 2 hours than the number of events in the previous 2 hours.    - ``example_frequency.yaml`` is an example of the ""frequency"" rule type, which will alert when there are a given number of events occuring  within a time period. This example will send an email when 50 documents matching a given filter occur within a 4 hour timeframe.    - ``example_change.yaml`` is an example of the ""change"" rule type, which will alert when a certain field in two documents changes. In this example,  the alert email is sent when two documents with the same 'username' field but a different value of the 'country_name' field occur within 24 hours  of each other.    - ``example_new_term.yaml`` is an example of the ""new term"" rule type, which alerts when a new value appears in a field or fields. In this example,  an email is sent when a new value of (""username"", ""computer"") is encountered in example login logs.    ## Frequently Asked Questions    ### My rule is not getting any hits?    So you've managed to set up ElastAlert, write a rule, and run it, but nothing happens, or it says ``0 query hits``. First of all, we recommend using the command ``elastalert-test-rule rule.yaml`` to debug. It will show you how many documents match your filters for the last 24 hours (or more, see ``--help``), and then shows you if any alerts would have fired. If you have a filter in your rule, remove it and try again. This will show you if the index is correct and that you have at least some documents. If you have a filter in Kibana and want to recreate it in ElastAlert, you probably want to use a query string. Your filter will look like    ```  filter:  - query:      query_string:        query: ""foo: bar AND baz: abc*""  ```  If you receive an error that Elasticsearch is unable to parse it, it's likely the YAML is not spaced correctly, and the filter is not in the right format. If you are using other types of filters, like ``term``, a common pitfall is not realizing that you may need to use the analyzed token. This is the default if you are using Logstash. For example,    ```  filter:  - term:      foo: ""Test Document""  ```    will not match even if the original value for ``foo`` was exactly ""Test Document"". Instead, you want to use ``foo.raw``. If you are still having trouble troubleshooting why your documents do not match, try running ElastAlert with ``--es_debug_trace /path/to/file.log``. This will log the queries made to Elasticsearch in full so that you can see exactly what is happening.    ### I got hits, why didn't I get an alert?    If you got logs that had ``X query hits, 0 matches, 0 alerts sent``, it depends on the ``type`` why you didn't get any alerts. If ``type: any``, a match will occur for every hit. If you are using ``type: frequency``, ``num_events`` must occur within ``timeframe`` of each other for a match to occur. Different rules apply for different rule types.    If you see ``X matches, 0 alerts sent``, this may occur for several reasons. If you set ``aggregation``, the alert will not be sent until after that time has elapsed. If you have gotten an alert for this same rule before, that rule may be silenced for a period of time. The default is one minute between alerts. If a rule is silenced, you will see ``Ignoring match for silenced rule`` in the logs.    If you see ``X alerts sent`` but didn't get any alert, it's probably related to the alert configuration. If you are using the ``--debug`` flag, you will not receive any alerts. Instead, the alert text will be written to the console. Use ``--verbose`` to achieve the same affects without preventing alerts. If you are using email alert, make sure you have it configured for an SMTP server. By default, it will connect to localhost on port 25. It will also use the word ""elastalert"" as the ""From:"" address. Some SMTP servers will reject this because it does not have a domain while others will add their own domain automatically. See the email section in the documentation for how to configure this.    ### Why did I only get one alert when I expected to get several?    There is a setting called ``realert`` which is the minimum time between two alerts for the same rule. Any alert that occurs within this time will simply be dropped. The default value for this is one minute. If you want to receive an alert for every single match, even if they occur right after each other, use    ```  realert:    minutes: 0  ```    You can of course set it higher as well.    ### How can I prevent duplicate alerts?    By setting ``realert``, you will prevent the same rule from alerting twice in an amount of time.    ```  realert:    days: 1  ```    You can also prevent duplicates based on a certain field by using ``query_key``. For example, to prevent multiple alerts for the same user, you might use    ```  realert:    hours: 8  query_key: user  ```    Note that this will also affect the way many rule types work. If you are using ``type: frequency`` for example, ``num_events`` for a single value of ``query_key`` must occur before an alert will be sent. You can also use a compound of multiple fields for this key. For example, if you only wanted to receieve an alert once for a specific error and hostname, you could use    ```  query_key: [error, hostname]  ```    Internally, this works by creating a new field for each document called ``field1,field2`` with a value of ``value1,value2`` and using that as the ``query_key``.    The data for when an alert will fire again is stored in Elasticsearch in the ``elastalert_status`` index, with a ``_type`` of ``silence`` and also cached in memory.    ### How can I change what's in the alert?    You can use the field ``alert_text`` to add custom text to an alert. By setting ``alert_text_type: alert_text_only``, it will be the entirety of the alert. You can also add different fields from the alert by using Python style string formatting and ``alert_text_args``. For example    ```  alert_text: ""Something happened with {0} at {1}""  alert_text_type: alert_text_only  alert_text_args: [""username"", ""@timestamp""]  ```    You can also limit the alert to only containing certain fields from the document by using ``include``.    ```  include: [""ip_address"", ""hostname"", ""status""]  ```    ### My alert only contains data for one event, how can I see more?    If you are using ``type: frequency``, you can set the option ``attach_related: true`` and every document will be included in the alert. An alternative, which works for every type, is ``top_count_keys``. This will show the top counts for each value for certain fields. For example, if you have    ```  top_count_keys: [""ip_address"", ""status""]  ```    and 10 documents matched your alert, it may contain something like    ```  ip_address:  127.0.0.1: 7  10.0.0.1: 2  192.168.0.1: 1    status:  200: 9  500: 1  ```    ### How can I make the alert come at a certain time?    The ``aggregation`` feature will take every alert that has occured over a period of time and send them together in one alert. You can use cron style syntax to send all alerts that have occured since the last once by using    ```  aggregation:    schedule: '2 4 * * mon,fri'  ```    ### I have lots of documents and it's really slow, how can I speed it up?    There are several ways to potentially speed up queries. If you are using ``index: logstash-*``, Elasticsearch will query all shards, even if they do not possibly contain data with the correct timestamp. Instead, you can use Python time format strings and set ``use_strftime_index``    ```  index: logstash-%Y.%m  use_strftime_index: true  ```    Another thing you could change is ``buffer_time``. By default, ElastAlert will query large overlapping windows in order to ensure that it does not miss any events, even if they are indexed in real time. In config.yaml, you can adjust ``buffer_time`` to a smaller number to only query the most recent few minutes.    ```  buffer_time:    minutes: 5  ```    By default, ElastAlert will download every document in full before processing them. Instead, you can have ElastAlert simply get a count of the number of documents that have occured in between each query. To do this, set ``use_count_query: true``. This cannot be used if you use ``query_key``, because ElastAlert will not know the contents of each documents, just the total number of them. This also reduces the precision of alerts, because all events that occur between each query will be rounded to a single timestamp.    If you are using ``query_key`` (a single key, not multiple keys) you can use ``use_terms_query``. This will make ElastAlert perform a terms aggregation to get the counts for each value of a certain field. Both ``use_terms_query`` and ``use_count_query`` also require ``doc_type`` to be set to the ``_type`` of the documents. They may not be compatible with all rule types.    ### Can I perform aggregations?    The only aggregation supported currently is a terms aggregation, by setting ``use_terms_query``.    ### I'm not using @timestamp, what do I do?    You can use ``timestamp_field`` to change which field ElastAlert will use as the timestamp. You can use ``timestamp_type`` to change it between ISO 8601 and unix timestamps. You must have some kind of timestamp for ElastAlert to work. If your events are not in real time, you can use ``query_delay`` and ``buffer_time`` to adjust when ElastAlert will look for documents.    ### I'm using flatline but I don't see any alerts    When using ``type: flatline``, ElastAlert must see at least one document before it will alert you that it has stopped seeing them.    ### How can I get a ""resolve"" event?    ElastAlert does not currently support stateful alerts or resolve events.    ### Can I set a warning threshold?    Currently, the only way to set a warning threshold is by creating a second rule with a lower threshold.    ## License    ElastAlert is licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0    ### Read the documentation at [Read the Docs](http://elastalert.readthedocs.org).    ### Questions? Drop by #elastalert on Freenode IRC. """
Big data;https://github.com/jhuckaby/Cronicle;"""# Important Upgrade Note!    For those upgrading multi-server clusters from Cronicle v0.8 to v0.9, you must upgrade **all** of your servers for them to be able to communicate. In v0.9 we have upgraded to socket.io v4.4, which is incompatible with socket.io v1.7.3 that previous Cronicle versions used. We had to upgrade this dependency due to high severity vulnerabilities. Since this is a breaking API change for them, you cannot run ""mixed"" server clusters with some servers on Cronicle v0.8 and others on v0.9. They all have to be on v0.8 or they all have to be on v0.9.    So, it is recommended that you first disable the main scheduler on your master server (checkbox in top-right corner of the UI), wait for all jobs to complete, then shut down all servers and upgrade them all together. Then start them all up again, and finally re-enable the scheduler.    # Overview    **Cronicle** is a multi-server task scheduler and runner, with a web based front-end UI.  It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer.  It's basically a fancy [Cron](https://en.wikipedia.org/wiki/Cron) replacement written in [Node.js](https://nodejs.org/).  You can give it simple shell commands, or write Plugins in virtually any language.    ![Main Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/job-details-complete.png)    ## Features at a Glance    * Single or multi-server setup.  * Automated failover to backup servers.  * Auto-discovery of nearby servers.  * Real-time job status with live log viewer.  * Plugins can be written in any language.  * Schedule events in multiple timezones.  * Optionally queue up long-running events.  * Track CPU and memory usage for each job.  * Historical stats with performance graphs.  * Simple JSON messaging system for Plugins.  * Web hooks for external notification systems.  * Simple REST API for scheduling and running events.  * API Keys for authenticating remote apps.    ## Table of Contents    <details><summary>Table of Contents</summary>    <!-- toc -->  * [Glossary](#glossary)  - [Installation](#installation)  - [Setup](#setup)  	* [Single Server](#single-server)  	* [Single Primary with Workers](#single-primary-with-workers)  	* [Multi-Server Cluster](#multi-server-cluster)  		+ [Load Balancers](#load-balancers)  		+ [Ops Notes](#ops-notes)  - [Configuration](#configuration)  	* [Basics](#basics)  		+ [base_app_url](#base_app_url)  		+ [email_from](#email_from)  		+ [smtp_hostname](#smtp_hostname)  		+ [smtp_port](#smtp_port)  		+ [mail_options](#mail_options)  		+ [secret_key](#secret_key)  		+ [log_dir](#log_dir)  		+ [log_filename](#log_filename)  		+ [log_columns](#log_columns)  		+ [log_archive_path](#log_archive_path)  		+ [copy_job_logs_to](#copy_job_logs_to)  		+ [queue_dir](#queue_dir)  		+ [pid_file](#pid_file)  		+ [debug_level](#debug_level)  		+ [maintenance](#maintenance)  		+ [list_row_max](#list_row_max)  		+ [job_data_expire_days](#job_data_expire_days)  		+ [child_kill_timeout](#child_kill_timeout)  		+ [dead_job_timeout](#dead_job_timeout)  		+ [master_ping_freq](#master_ping_freq)  		+ [master_ping_timeout](#master_ping_timeout)  		+ [udp_broadcast_port](#udp_broadcast_port)  		+ [scheduler_startup_grace](#scheduler_startup_grace)  		+ [universal_web_hook](#universal_web_hook)  		+ [web_hook_custom_data](#web_hook_custom_data)  		+ [web_hook_custom_opts](#web_hook_custom_opts)  		+ [web_hook_text_templates](#web_hook_text_templates)  		+ [ssl_cert_bypass](#ssl_cert_bypass)  		+ [job_memory_max](#job_memory_max)  		+ [job_memory_sustain](#job_memory_sustain)  		+ [job_cpu_max](#job_cpu_max)  		+ [job_cpu_sustain](#job_cpu_sustain)  		+ [job_log_max_size](#job_log_max_size)  		+ [job_env](#job_env)  		+ [server_comm_use_hostnames](#server_comm_use_hostnames)  		+ [web_direct_connect](#web_direct_connect)  		+ [web_socket_use_hostnames](#web_socket_use_hostnames)  		+ [socket_io_transports](#socket_io_transports)  	* [Storage Configuration](#storage-configuration)  		+ [Filesystem](#filesystem)  		+ [Couchbase](#couchbase)  		+ [Amazon S3](#amazon-s3)  	* [Web Server Configuration](#web-server-configuration)  	* [User Configuration](#user-configuration)  	* [Email Configuration](#email-configuration)  - [Web UI](#web-ui)  	* [Home Tab](#home-tab)  		+ [General Stats](#general-stats)  		+ [Active Jobs](#active-jobs)  		+ [Upcoming Events](#upcoming-events)  	* [Schedule Tab](#schedule-tab)  		+ [Edit Event Tab](#edit-event-tab)  			- [Event ID](#event-id)  			- [Event Name](#event-name)  			- [Event Enabled](#event-enabled)  			- [Event Category](#event-category)  			- [Event Target](#event-target)  				* [Algorithm](#algorithm)  				* [Multiplexing](#multiplexing)  			- [Event Plugin](#event-plugin)  			- [Event Timing](#event-timing)  			- [Event Concurrency](#event-concurrency)  			- [Event Timeout](#event-timeout)  			- [Event Retries](#event-retries)  			- [Event Options](#event-options)  				* [Run All Mode](#run-all-mode)  				* [Detached Mode](#detached-mode)  				* [Allow Queued Jobs](#allow-queued-jobs)  				* [Chain Reaction](#chain-reaction)  			- [Event Time Machine](#event-time-machine)  			- [Event Notification](#event-notification)  				* [Event Web Hook](#event-web-hook)  			- [Event Resource Limits](#event-resource-limits)  			- [Event Notes](#event-notes)  			- [Run Now](#run-now)  	* [Completed Jobs Tab](#completed-jobs-tab)  		+ [Event History Tab](#event-history-tab)  		+ [Event Stats Tab](#event-stats-tab)  	* [Job Details Tab](#job-details-tab)  	* [My Account Tab](#my-account-tab)  	* [Administration Tab](#administration-tab)  		+ [Activity Log Tab](#activity-log-tab)  		+ [API Keys Tab](#api-keys-tab)  		+ [Categories Tab](#categories-tab)  		+ [Plugins Tab](#plugins-tab)  			- [Plugin Parameters](#plugin-parameters)  			- [Advanced Plugin Options](#advanced-plugin-options)  		+ [Servers Tab](#servers-tab)  			- [Server Groups](#server-groups)  		+ [Users Tab](#users-tab)  - [Plugins](#plugins)  	* [Writing Plugins](#writing-plugins)  		+ [JSON Input](#json-input)  		+ [JSON Output](#json-output)  			- [Reporting Progress](#reporting-progress)  			- [Performance Metrics](#performance-metrics)  				* [Nested Metrics](#nested-metrics)  			- [Changing Notification Settings](#changing-notification-settings)  			- [Chain Reaction Control](#chain-reaction-control)  				* [Chain Data](#chain-data)  			- [Custom Data Tables](#custom-data-tables)  			- [Custom HTML Content](#custom-html-content)  			- [Updating The Event](#updating-the-event)  		+ [Job Environment Variables](#job-environment-variables)  	* [Sample Node Plugin](#sample-node-plugin)  	* [Sample Perl Plugin](#sample-perl-plugin)  	* [Sample PHP Plugin](#sample-php-plugin)  	* [Built-in Shell Plugin](#built-in-shell-plugin)  	* [Built-in HTTP Request Plugin](#built-in-http-request-plugin)  		+ [HTTP Request Chaining](#http-request-chaining)  - [Command Line](#command-line)  	* [Starting and Stopping](#starting-and-stopping)  	* [Environment Variables](#environment-variables)  	* [Storage Maintenance](#storage-maintenance)  	* [Recover Admin Access](#recover-admin-access)  	* [Server Startup](#server-startup)  	* [Upgrading Cronicle](#upgrading-cronicle)  	* [Data Import and Export](#data-import-and-export)  	* [Storage Migration Tool](#storage-migration-tool)  - [Inner Workings](#inner-workings)  	* [Cron Noncompliance](#cron-noncompliance)  	* [Storage](#storage)  	* [Logs](#logs)  	* [Keeping Time](#keeping-time)  	* [Primary Server Failover](#primary-server-failover)  		+ [Unclean Shutdown](#unclean-shutdown)  - [API Reference](#api-reference)  	* [JSON REST API](#json-rest-api)  		+ [Redirects](#redirects)  	* [API Keys](#api-keys)  	* [Standard Response Format](#standard-response-format)  	* [API Calls](#api-calls)  		+ [get_schedule](#get_schedule)  		+ [get_event](#get_event)  		+ [create_event](#create_event)  		+ [update_event](#update_event)  		+ [delete_event](#delete_event)  		+ [run_event](#run_event)  		+ [get_job_status](#get_job_status)  		+ [get_active_jobs](#get_active_jobs)  		+ [update_job](#update_job)  		+ [abort_job](#abort_job)  	* [Event Data Format](#event-data-format)  		+ [Event Timing Object](#event-timing-object)  - [Development](#development)  	* [Installing Dev Tools](#installing-dev-tools)  	* [Manual Installation](#manual-installation)  	* [Starting in Debug Mode](#starting-in-debug-mode)  	* [Running Unit Tests](#running-unit-tests)  - [Companies Using Cronicle](#companies-using-cronicle)  - [Colophon](#colophon)  - [License](#license)    </details>    ## Glossary    A quick introduction to some common terms used in Cronicle:    | Term | Description |  |------|-------------|  | **Primary Server** | The primary server which keeps time and runs the scheduler, assigning jobs to other servers, and/or itself. |  | **Backup Server** | A worker server which will automatically become primary and take over duties if the current primary dies. |  | **Worker Server** | A server which sits idle until it is assigned jobs by the primary server. |  | **Server Group** | A named group of servers which can be targeted by events, and tagged as ""primary eligible"", or ""worker only"". |  | **API Key** | A special key that can be used by external apps to send API requests into Cronicle.  Remotely trigger jobs, etc. |  | **User** | A human user account, which has a username and a password.  Passwords are salted and hashed with [bcrypt](https://en.wikipedia.org/wiki/Bcrypt). |  | **Plugin** | Any executable script in any language, which runs a job and reads/writes JSON to communicate with Cronicle. |  | **Schedule** | The list of events, which are scheduled to run at particular times, on particular servers. |  | **Category** | Events can be assigned to categories which define defaults and optionally a color highlight in the UI. |  | **Event** | An entry in the schedule, which may run once or many times at any interval.  Each event points to a Plugin, and a server or group to run it. |  | **Job** | A running instance of an event.  If an event is set to run hourly, then a new job will be created every hour. |    # Installation    Please note that Cronicle currently only works on POSIX-compliant operating systems, which basically means Unix/Linux and OS X.  You'll also need to have [Node.js LTS](https://nodejs.org/en/download/) pre-installed on your server.  Please note that we **only support the Active LTS versions of Node.js**.  Cronicle may not work on the ""current"" release channel.  See [Node.js Releases](https://nodejs.org/en/about/releases/) for details.    Once you have Node.js LTS installed, type this as root:    ```  curl -s https://raw.githubusercontent.com/jhuckaby/Cronicle/master/bin/install.js | node  ```    This will install the latest stable release of Cronicle and all of its [dependencies](#colophon) under: `/opt/cronicle/`    If you'd rather install it manually (or something went wrong with the auto-installer), here are the raw commands:    ```  mkdir -p /opt/cronicle  cd /opt/cronicle  curl -L https://github.com/jhuckaby/Cronicle/archive/v1.0.0.tar.gz | tar zxvf - --strip-components 1  npm install  node bin/build.js dist  ```    Replace `v1.0.0` with the desired Cronicle version from the [release list](https://github.com/jhuckaby/Cronicle/releases), or `master` for the head revision (unstable).    # Setup    If this is your first time installing, please read the [Configuration](#configuration) section first.  You'll likely want to customize a few configuration parameters in the `/opt/cronicle/conf/config.json` file before proceeding.  At the very least, you should set these properties:    | Key | Description |  |-----|-------------|  | `base_app_url` | A fully-qualified URL to Cronicle on your server, including the `http_port` if non-standard.  This is used in e-mails to create self-referencing URLs. |  | `email_from` | The e-mail address to use as the ""From"" address when sending out notifications. |  | `smtp_hostname` | The hostname of your SMTP server, for sending mail.  This can be `127.0.0.1` or `localhost` if you have [sendmail](https://en.wikipedia.org/wiki/Sendmail) running locally. |  | `secret_key` | For multi-server setups (see below) all your servers must share the same secret key.  Any randomly generated string is fine. |  | `job_memory_max` | The default maximum memory limit for each job (can also be customized per event and per category). |  | `http_port` | The web server port number for the user interface.  Defaults to 3012. |    Now then, the only other decision you have to make is what to use as a storage back-end.  Cronicle can use local disk (easiest setup), [Couchbase](http://www.couchbase.com/nosql-databases/couchbase-server) or [Amazon S3](https://aws.amazon.com/s3/).  For single server installations, or even single primary with multiple workers, local disk is probably just fine, and this is the default setting.  But if you want to run a true multi-server cluster with automatic primary failover, please see [Multi-Server Cluster](#multi-server-cluster) for details.    With that out of the way, run the following script to initialize the storage system.  You only need to do this once, *and only on the primary server*.  Do not run this on any worker servers:    ```  /opt/cronicle/bin/control.sh setup  ```    Among other things, this creates an administrator user account you can use to login right away.  The username is `admin` and the password is `admin`.  It is recommended you change the password as soon as possible, for security purposes (or just create your own administrator account and delete `admin`).    At this point you should be able to start the service and access the web UI.  Enter this command:    ```  /opt/cronicle/bin/control.sh start  ```    Give it a minute to decide to become primary, then send your browser to the server on the correct port:    ```  http://YOUR_SERVER_HOSTNAME:3012/  ```    You only need to include the port number in the URL if you are using a non-standard HTTP port (see [Web Server Configuration](#web-server-configuration)).    See the [Web UI](#web-ui) section below for instructions on using the Cronicle web interface.    ## Single Server    For a single server installation, there is nothing more you need to do.  After installing the package, running the `bin/control.sh setup` script and starting the service, Cronicle should be 100% ready to go.  You can always add more servers later (see below).    ## Single Primary with Workers    The easiest multi-server Cronicle setup is a single ""primary"" server with one or more workers.  This means that one server is the scheduler, so it keeps track of time, and assigns jobs for execution.  Jobs may be assigned to any number of worker servers, and even the primary itself.  Worker servers simply sit idle and wait for jobs to be assigned by the primary server.  Workers never take over primary scheduling duties, even if the primary server goes down.    This is the simplest multi-server setup because the primary server can use local disk for all its storage.  Workers do not need access to the file storage.  This is the default configuration, so you don't have to change anything at all.  What it means is, all the scheduling data, event categories, user accounts, sessions, plugins, job logs and other data is stored as plain JSON files on local disk.  Cronicle can also be configured to use a NoSQL database such as [Couchbase](http://www.couchbase.com/nosql-databases/couchbase-server) or [Amazon S3](https://aws.amazon.com/s3/), but this is not required.    So by default, when you run the setup script above, the current server is placed into a ""Primary Group"", meaning it is the only server that is eligible to become primary.  If you then install Cronicle on additional servers, they will become workers only.  You can change all this from the UI, but please read the next section before running multiple primary backup servers.    When installing Cronicle onto worker servers, please do not run the `bin/control.sh setup` script.  Instead, simply copy over your `conf/config.json` file, and then start the service.    ## Multi-Server Cluster    Cronicle also has the ability to run with one or more ""backup"" servers, which can become primary if need be.  Failover is automatic, and the cluster negotiates who should be primary at any given time.  But in order for this to work, all the primary eligible servers need access to the same storage data.  This can be achieved in one of three ways:    * Use a shared filesystem such as [NFS](https://en.wikipedia.org/wiki/Network_File_System).  * Use a [Couchbase](http://www.couchbase.com/nosql-databases/couchbase-server) server.  * Use an [Amazon S3](https://aws.amazon.com/s3/) bucket.    See the [Storage Configuration](#storage-configuration) section below for details on these.    The other thing you'll need to do is make sure all your primary backup servers are in the appropriate server group.  By default, a single ""Primary Group"" is created which only contains your primary primary server.  Using the UI, you can simply change the hostname regular expression so it encompasses all your eligible servers, or you can just add additional groups that match each backup server.  More details can be found in the [Servers Tab](#servers-tab) section below.    ### Load Balancers    You can run Cronicle behind a load balancer, as long as you ensure that only the primary server and eligible backup servers are in the load balancer pool.  Do not include any worker-only servers, as they typically do not have access to the back-end storage system, and cannot serve up the UI.    You can then set the [base_app_url](#base_app_url) configuration parameter to point to the load balancer, instead of an individual server, and also use that hostname when loading the UI in your browser.    Note that Web UI needs to make API calls and open [WebSocket](https://en.wikipedia.org/wiki/WebSocket) connections to the primary server directly, so it needs to also be accessible directly via its hostname.    You must set the [web_direct_connect](#web_direct_connect) configuration property to `true`.  This ensures that the Web UI will make API and WebSocket connections directly to the primary server, rather than via the load balancer hostname.    ### Ops Notes    For teams setting up multi-server clusters, here are some operational concerns to keep in mind:    * All servers should have the same exact configuration file (`/opt/cronicle/conf/config.json`).  * All servers need to have correct clocks (timezones do not matter, clock sync does).  * Server auto-discovery happens via UDP broadcast on port 3014 (by default).  This is not required.  * The primary server will also open TCP [WebSocket](https://en.wikipedia.org/wiki/WebSocket) connections to each worker on the web server port.  * Each server in the cluster needs to have a fully-qualified hostname that resolves in DNS.  * The server hostnames determine priority of which server becomes primary (alphabetical sort).  * All servers need to have unique hostnames (very bad things will happen otherwise).  * All servers need to have at least one active IPv4 interface.  * For the ""live log"" feature in the UI to work, the user needs a network route to the server running the job, via its hostname.  * If you have to change any server IP addresses, they'll have to be removed and re-added to the cluster.  * See the [Cron Noncompliance](#cron-noncompliance) section for differences in how Cronicle schedules events, versus the Unix Cron standard.    # Configuration    The main Cronicle configuration file is in JSON format, and can be found here:    ```  /opt/cronicle/conf/config.json  ```    Please edit this file directly.  It will not be touched by any upgrades.  A pristine copy of the default configuration can always be found here: `/opt/cronicle/sample_conf/config.json`.    ## Basics    Here are descriptions of the top-level configuration parameters:    ### base_app_url    This should be set to a fully-qualified URL, pointing to your Cronicle server, including the HTTP port number if non-standard.  Do not include a trailing slash.  This is used in e-mails to create self-referencing URLs.  Example:    ```  http://local.cronicle.com:3012  ```    If you are running Cronicle behind a load balancer, this should be set to the load balanced virtual hostname.    ### email_from    The e-mail address to use as the ""From"" address when sending out notifications.  Most SMTP servers require this to be a valid address to accept mail.    ### smtp_hostname    The hostname of your SMTP server, for sending mail.  This can be set to `127.0.0.1` or `localhost` if you have [sendmail](https://en.wikipedia.org/wiki/Sendmail) running locally.    ### smtp_port    The port number to use when communicating with the SMTP server.  The default is `25`.    ### mail_options    Set specific mailer options, such as SMTP SSL and authentication, passed directly to [pixl-mail](https://www.npmjs.com/package/pixl-mail#options) (and then to [nodemailer](https://nodemailer.com/)).  Example:    ```js  ""mail_options"": {  	""secure"": true,  	""auth"": { ""user"": ""fsmith"", ""pass"": ""12345"" },  	""connectionTimeout"": 10000,  	""greetingTimeout"": 10000,  	""socketTimeout"": 10000  }  ```    The `connectionTimeout`, `greetingTimeout` and `socketTimeout` properties are all expressed in milliseconds.    You can also use `mail_options` to use local [sendmail](https://nodemailer.com/transports/sendmail/), if you have that configured on your server.  To do this, set the following properties, and tune as needed:    ```js  ""mail_options"": {  	""sendmail"": true,  	""newline"": ""unix"",  	""path"": ""/usr/sbin/sendmail""  }  ```    You can omit `smtp_hostname` and `smtp_port` if you are using sendmail.    ### secret_key    For multi-server setups, all your Cronicle servers need to share the same secret key.  Any randomly generated string is fine.    The install script will automatically set to this to a random string for you, but you are free to change it to anything you want.  Just make sure all your servers have the same shared secret key.  This is used to authenticate internal server-to-server API requests.    ### log_dir    The directory where logs will be written, before they are archived.  This can be a partial path, relative to the Cronicle base directory (`/opt/cronicle`) or a full path to a custom location.  It defaults to `logs` (i.e. `/opt/cronicle/logs`).    ### log_filename    The filename to use when writing logs.  You have three options here: a single combined log file for all logs, multiple log files for each component, or multiple log files for each category (debug, transaction, error).  See the [Logs](#logs) section below for details.    ### log_columns    This is an array of column IDs to log.  You are free to reorder or remove some of these, but do not change the names.  They are specific IDs that match up to log function calls in the code.  See the [Logs](#logs) section below for details.    ### log_archive_path    Every night at midnight (local server time), the logs can be archived (gzipped) to a separate location.  This parameter specifies the path, and the directory naming / filenaming convention of the archive files.  It can utilize date placeholders including `[yyyy]`, `[mm]` and `[dd]`.    This can be a partial path, relative to the Cronicle base directory (`/opt/cronicle`) or a full path to a custom location.  It defaults to `logs/archives/[yyyy]/[mm]/[dd]/[filename]-[yyyy]-[mm]-[dd].log.gz`.    ### copy_job_logs_to    Your job logs (i.e. the output from Plugins) are stored separately from the main Cronicle log files.  They are actually written to the Cronicle storage system, and made available in the UI.  This parameter allows you to copy them to a separate directory on disk, presumably for some kind of log processing system (such as [Splunk](http://www.splunk.com/)) to then pick them up, index them, etc.  It is optional, and defaults to disabled.    This can be a partial path, relative to the Cronicle base directory (`/opt/cronicle`) or a full path to a custom location.    ### queue_dir    The queue directory is used internally for misc. background tasks, such as handling detached jobs sending messages back to the daemon.  You shouldn't ever have to deal with this directly, and the directory is auto-created on install.      This can be a partial path, relative to the Cronicle base directory (`/opt/cronicle`) or a full path to a custom location.    ### pid_file    The PID file is simply a text file containing the Process ID of the main Cronicle daemon.  It is used by the `control.sh` script to stop the daemon, and detect if it is running.  You should never have to deal with this file directly, and it defaults to living in the `logs` directory which is auto-created.      This can be a partial path, relative to the Cronicle base directory (`/opt/cronicle`) or a full path to a custom location.  However, it should probably not be changed, as the `control.sh` script expects it to live in `logs/cronicled.pid`.    ### debug_level    The level of verbosity in the debug logs.  It ranges from `1` (very quiet) to `10` (extremely loud).  The default value is `9`.    ### maintenance    Cronicle needs to run storage maintenance once per day, which generally involves deleting expired records and trimming lists which have grown too large.  This only runs on the primary server, and typically only takes a few seconds, depending on the number of events you have.  The application is still usable during this time, but UI performance may be slightly impacted.    By default the maintenance is set to run at 4:00 AM (local server time).  Feel free to change this to a more convenient time for your server environment.  The format of the parameter is `HH:MM`.    ### list_row_max    This parameter controls how many items are kept in historical lists such as the [Activity Log](#activity-log-tab), [Completed Jobs](#completed-jobs-tab), and [Event History](#event-history-tab).  When this limit is exceeded, the oldest entries are removed during the nightly maintenance run.  The default limit is `10000` items.    This has no real effect on performance -- only space on disk (or Couchbase / S3).    ### job_data_expire_days    Completed job data is only kept around for this number of days.  This includes job logs and the metadata for each completed job (stats, etc.).  The default value is `180` days, but feel free to tune this for your own environment.    This has no real effect on performance -- only space on disk (or Couchbase / S3).    ### child_kill_timeout    This is the number of seconds to allow child processes to exit after sending a TERM signal (for aborted jobs, server shutdown, etc.).  If they do not exit within the specified timeout, a KILL signal is sent.  The default value is `10` seconds.    ### dead_job_timeout    When the primary server loses connectivity with a worker that had running jobs on it, they go into a ""limbo"" state for a period of time, before they are finally considered lost.  The `dead_job_timeout` parameter specifies the amount of time before these wayward jobs are aborted (and possibly retried, depending on the event settings).  The default value is `120` seconds.    This parameter exists because certain networks may have unreliable connections between servers, and it is possible a server may drop for a few seconds, then come right back.  If a short hiccup like that occurs, you probably don't want to abort all the running jobs right away.  Also, when you are upgrading Cronicle itself, you don't want detached jobs to be interrupted.    The worst case scenario is that a remote server with running jobs goes MIA for longer than the `dead_job_timeout`, the primary server aborts all the jobs, then the server reappears and finishes the jobs.  This creates a bit of a mess, because the jobs are reported as both errors and successes.  The latter success prevails in the end, but the errors stay in the logs and event history.    ### master_ping_freq    For multi-server clusters, this specifies how often the primary server should send out pings to worker servers, to let them know who is the boss.  The default is `20` seconds.    ### master_ping_timeout    For multi-server clusters, this specifies how long to wait after receiving a ping, before a backup server considers the primary server to be dead.  At this point a new primary server will be chosen.  The default value is `60` seconds.    ### udp_broadcast_port    For auto-discovery of nearby servers, this specifies the UDP port to use for broadcasting.  Do not worry if your network doesn't support UDP broadcast, as this is just an optional feature where nearby servers will show up in the UI.  The default port is `3014`.    ### scheduler_startup_grace    When the scheduler first starts up on the primary server, it waits for a few seconds before actually assigning jobs.  This is to allow all the servers in the cluster to check in and register themselves with the primary server.  The default value is `10` seconds, which should be plenty of time.    Once a server becomes primary, it should immediately attempt to connect to all remote servers right away.  So in theory this grace period could be as short as 1 second or less, but a longer delay allows for any random network connectivity errors to work themselves out.    ### universal_web_hook    While you can specify a web hook in the UI per each category and/or per each event, this parameter allows you to define a universal one, which is *always* fired for *every* job regardless of UI settings.  It should be a fully-qualified URL to an API endpoint that accepts an HTTP POST containing JSON data.    Web hooks are fired at the start and the end of each job (success or fail).  A JSON record is sent in the HTTP POST body, which contains all the relevant information about the job, including an `action` property, which will be set to `job_start` at the start and `job_complete` at the end of the job.  See the [Web Hooks](#event-web-hook) section below for more on the data format.    To include custom HTTP request headers with your web hook, append them onto the end of the URL using this format: `[header: My-Header: My-Value]`.  Make sure to include a space before the opening bracket.  Example URL:    ```  https://myserver.com/api/chat.postMessage [header: My-Header: My-Value]  ```    ### web_hook_custom_data    If you need to include custom JSON data with the web hook HTTP POST, you can do so by specifying a `web_hook_custom_data` property, and any keys/values will be merged in with the event data as it is sent to the web hook URL.  Example:    ```js  ""web_hook_custom_data"": {  	""my_custom_key1"": ""My custom value 1"",  	""my_custom_key2"": ""My custom value 2""  }  ```    In this example `my_custom_key1` and `my_custom_key2` will be merged in with the event data that usually accompanies the web hook post data.  See the [Web Hooks](#event-web-hook) section below for more on the data format.    ### web_hook_custom_opts    If you need to customize the low-level properties sent to the Node.js [http.request](https://nodejs.org/api/http.html#http_http_request_options_callback) method for making outbound web hook requests, use the `web_hook_custom_opts` property.  Using this you can set things like a proxy host and port.  Example use:    ```js  ""web_hook_custom_opts"": {  	""host"": ""my-corp-proxy.com"",  	""port"": 8080  }  ```    See the [http.request](https://nodejs.org/api/http.html#http_http_request_options_callback) docs for all the possible properties you can set here.    ### web_hook_text_templates    The web hook JSON POST data includes a `text` property which is a simple summary of the action taking place, which is compatible with [Slack Webhook Integrations](https://api.slack.com/incoming-webhooks).  These text strings are generated based on the action, and use the following templates:    ```js  ""web_hook_text_templates"": {  	""job_start"": ""Job started on [hostname]: [event_title] [job_details_url]"",  	""job_complete"": ""Job completed successfully on [hostname]: [event_title] [job_details_url]"",  	""job_failure"": ""Job failed on [hostname]: [event_title]: Error [code]: [description] [job_details_url]"",  	""job_launch_failure"": ""Failed to launch scheduled event: [event_title]: [description] [edit_event_url]""  }  ```    You can customize these text strings by including a `web_hook_text_templates` object in your configuration, and setting each of the action properties within.  Also, you can use this to *disable* any of the web hook actions, by simply removing certain action keys.  For example, if you don't want to fire a web hook for starting a job, remove the `job_start` key.  If you only want web hooks to fire for errors, remove both the `job_start` and `job_complete` keys.    The text string templates can use any data values from the web hook JSON data by inserting `[square_bracket]` placeholders.  See the [Web Hooks](#event-web-hook) section below for more on the data format, and which values are available.    ### ssl_cert_bypass    If you are having trouble getting HTTPS web hooks or SSL SMTP e-mails to work, you might need to set `ssl_cert_bypass` to true.  This causes Node.js to blindly accept all SSL connections, even when it cannot validate the SSL certificate.  This effectively sets the following environment variable at startup:    ```js  process.env.NODE_TLS_REJECT_UNAUTHORIZED = ""0"";  ```    Please only do this if you understand the security ramifications, and *completely trust* the host(s) you are connecting to, and the network you are on.  Skipping the certificate validation step should really only be done in special circumstances, such as trying to hit one of your own internal servers with a self-signed cert.    For legacy compatibility, the old `web_hook_ssl_cert_bypass` property is still accepted, and has the same effect as `ssl_cert_bypass`.    ### job_memory_max    This parameter allows you to set a default memory usage limit for jobs, specified in bytes.  This is measured as the total usage of the job process *and any sub-processes spawned or forked by the main process*.  If the memory limit is exceeded, the job is aborted.  The default value is `1073741824` (1 GB).  To disable set it to `0`.    Memory limits can also be customized in the UI per each category and/or per each event (see [Event Resource Limits](#event-resource-limits) below).  Doing either overrides the primary default.    ### job_memory_sustain    When using the [job_memory_max](#job_memory_max) feature, you can optionally specify how long a job is allowed exceed the maximum memory limit until it is aborted.  For example, you may want to allow jobs to spike over 1 GB of RAM, but not use it sustained for more a certain amount of time.  That is what the `job_memory_sustain` property allows, and it accepts a value in seconds.  It defaults to `0` (abort instantly when exceeded).    Memory limits can also be customized in the UI per each category and/or per each event (see [Event Resource Limits](#event-resource-limits) below).  Doing either overrides the default.    ### job_cpu_max    This parameter allows you to set a default CPU usage limit for jobs, specified in percentage of one CPU core.  This is measured as the total CPU usage of the job process *and any sub-processes spawned or forked by the main process*.  If the CPU limit is exceeded, the job is aborted.  The default value is `0` (disabled).  For example, to allow jobs to use up to 2 CPU cores, specify `200` as the limit.    CPU limits can also be customized in the UI per each category and/or per each event (see [Event Resource Limits](#event-resource-limits) below).  Doing either overrides the default.    ### job_cpu_sustain    When using the [job_cpu_max](#job_cpu_max) feature, you can optionally specify how long a job is allowed exceed the maximum CPU limit until it is aborted.  For example, you may want to allow jobs to use up to 2 CPU cores, but not use them sustained for more a certain amount of time.  That is what the `job_cpu_sustain` property allows, and it accepts a value in seconds.  It defaults to `0` (abort instantly when exceeded).    CPU limits can also be customized in the UI per each category and/or per each event (see [Event Resource Limits](#event-resource-limits) below).  Doing either overrides the default.    ### job_log_max_size    This parameter allows you to set a default log file size limit for jobs, specified in bytes.  If the file size limit is exceeded, the job is aborted.  The default value is `0` (disabled).    Job log file size limits can also be customized in the UI per each category and/or per each event (see [Event Resource Limits](#event-resource-limits) below).  Doing either overrides the default.    ### job_env    Place any key/value pairs you want into the `job_env` object, and they will become environment variables passed to all job processes, as they are spawned.  Note that these can be overridden by event parameters with the same names.  The `job_env` can be thought of as a way to specify universal default environment variables for all your jobs.  Example:    ```js  ""job_env"": {  	""TZ"": ""America/Los_Angeles"",  	""LANG"": ""en_US.UTF-8""  },  ```    ### server_comm_use_hostnames    Setting this parameter to `true` will force the Cronicle servers to connect to each other using hostnames rather than LAN IP addresses.  This is mainly for special situations where your local server IP addresses may change, and you would prefer to rely on DNS instead.  The default is `false` (disabled), meaning connect using IP addresses.    ### web_direct_connect    When this property is set to `false` (which is the default), the Cronicle Web UI will connect to whatever hostname/port is on the URL.  It is expected that this hostname/port will always resolve to your primary server.  This is useful for single server setups, situations when your users do not have direct access to your Cronicle servers via their IPs or hostnames, or if you are running behind some kind of reverse proxy.    If you set this parameter to `true`, then the Cronicle web application will connect *directly* to your individual Cronicle servers.  This is more for multi-server configurations, especially when running behind a [load balancer](#load-balancers) with multiple backup servers.  The Web UI must always connect to the primary server, so if you have multiple backup servers, it needs a direct connection.    Note that the ability to watch live logs for active jobs requires a direct web socket connection to the server running the job.  For that feature, this setting has no effect (it always attempts to connect directly).    ### web_socket_use_hostnames    Setting this parameter to `true` will force Cronicle's Web UI to connect to the back-end servers using their hostnames rather than IP addresses.  This includes both AJAX API calls and Websocket streams.  You should only need to enable this in special situations where your users cannot access your servers via their LAN IPs, and you need to proxy them through a hostname (DNS) instead.  The default is `false` (disabled), meaning connect using IP addresses.    This property only takes effect if [web_direct_connect](#web_direct_connect) is also set to `true`.    ### socket_io_transports    This is an advanced configuration property that you will probably never need to worry about.  This allows you to customize the [socket.io transports](https://socket.io/docs/client-api/) used to connect to the server for real-time updates.  By default, this property is set internally to an array containing the `websocket` transport only, e.g.    ```js  ""socket_io_transports"": [""websocket""]  ```    However, if you are trying to run Cronicle in an environment where WebSockets are not allowed (perhaps an ancient firewall or proxy), you can change this array to contain the `polling` transport first, e.g.    ```js  ""socket_io_transports"": [""polling"", ""websocket""]  ```    However, please only do this if you know exactly what you are doing, and why.    ### max_jobs    You can optionally set a global maximum number of concurrent jobs to allow.  This is across all servers and categories, and is designed as an ""emergency brake"" for runaway events.  The property is called `max_jobs`.  The default is `0` (no limit).  Example:    ```js  ""max_jobs"": 256  ```    ## Storage Configuration    The `Storage` object contains settings for the Cronicle storage system.  This is built on the [pixl-server-storage](https://www.npmjs.com/package/pixl-server-storage) module, which can write everything to local disk (the default), [Couchbase](http://www.couchbase.com/nosql-databases/couchbase-server) or [Amazon S3](https://aws.amazon.com/s3/).    To select a storage engine, place one of the following values into the `engine` property:    ### Filesystem    The default storage method is to use local disk (can also be an NFS mount, for multi-server setups with failover support).  For this, set the `engine` property to `Filesystem`, and declare a sub-object with the same name, with a couple more properties:    ```js  {  	""Storage"": {  		""engine"": ""Filesystem"",  		""Filesystem"": {  			""base_dir"": ""data"",  			""key_namespaces"": 1  		}  	}  }  ```    The `base_dir` is the base directory to store everything under.  It can be a fully-qualified filesystem path, or a relative path to the Cronicle base directory (e.g. `/opt/cronicle`).  In this case it will be `/opt/cronicle/data`.    For more details on using the Filesystem as a backing store, please read the [Local Filesystem section in the pixl-server-storage docs](https://www.npmjs.com/package/pixl-server-storage#local-filesystem).    ### Couchbase    To use Couchbase as a backing store for Cronicle, please read the [Couchbase section in the pixl-server-storage docs](https://www.npmjs.com/package/pixl-server-storage#couchbase).  It has complete details for how to setup the storage object.  Example configuration:    ```js  {  	""Storage"": {  		""engine"": ""Couchbase"",  		""Couchbase"": {  			""connectString"": ""couchbase://127.0.0.1"",  			""bucket"": ""default"",  			""username"": """",  			""password"": """",  			""serialize"": false,  			""keyPrefix"": ""cronicle""  		}  	}  }  ```    If you are sharing a bucket with other applications, use the `keyPrefix` property to keep the Cronicle data separate, in its own ""directory"".  For example, set `keyPrefix` to `""cronicle""` to keep all the Cronicle-related records in a top-level ""cronicle"" directory in the bucket.    Note that for Couchbase Server v5.0+ (Couchbase Node SDK 2.5+), you will have to supply both a `username` and `password` for a valid user created in the Couchbase UI.  Prior to v5+ you could omit the `username` and only specify a `password`, or no password at all if your bucket has no authentication.    You'll also need to install the npm [couchbase](https://www.npmjs.com/package/couchbase) module:    ```  cd /opt/cronicle  npm install couchbase  ```    After configuring Couchbase, you'll need to run the Cronicle setup script manually, to recreate all the base storage records needed to bootstrap the system:    ```  /opt/cronicle/bin/control.sh setup  ```    ### Amazon S3    To use Amazon S3 as a backing store for Cronicle, please read the [Amazon S3 section in the pixl-server-storage docs](https://www.npmjs.com/package/pixl-server-storage#amazon-s3).  It has complete details for how to setup the storage object.  Example configuration:    ```js  {  	""Storage"": {  		""engine"": ""S3"",  		""AWS"": {  			""accessKeyId"": ""YOUR_AMAZON_ACCESS_KEY"",   			""secretAccessKey"": ""YOUR_AMAZON_SECRET_KEY"",   			""region"": ""us-west-1"",  			""correctClockSkew"": true,  			""maxRetries"": 5,  			""httpOptions"": {  				""connectTimeout"": 5000,  				""timeout"": 5000  			}  		},  		""S3"": {  			""keyPrefix"": ""cronicle"",  			""fileExtensions"": true,  			""params"": {  				""Bucket"": ""YOUR_S3_BUCKET_ID""  			}  		}  	}  }  ```    If you are sharing a bucket with other applications, use the `keyPrefix` property to keep the Cronicle data separate, in its own ""directory"".  For example, set `keyPrefix` to `""cronicle""` to keep all the Cronicle-related records in a top-level ""cronicle"" directory in the bucket.  A trailing slash will be automatically added to the prefix if missing.    It is recommended that you always set the S3 `fileExtensions` property to `true` for new installs.  This makes the Cronicle S3 records play nice with sync / copy tools such as [Rclone](https://rclone.org/).  See [Issue #60](https://github.com/jhuckaby/Cronicle/issues/60) for more details.  Do not change this property on existing installs -- use the [Storage Migration Tool](#storage-migration-tool).    To use S3 you'll also need to install the npm [aws-sdk](https://www.npmjs.com/package/aws-sdk) module:    ```  cd /opt/cronicle  npm install aws-sdk  ```    After configuring S3, you'll need to run the Cronicle setup script manually, to recreate all the base storage records needed to bootstrap the system:    ```  /opt/cronicle/bin/control.sh setup  ```    If you're worried about Amazon S3 costs, you probably needn't.  With a typical setup running ~30 events per hour (about ~25,000 events per month), this translates to approximately 350,000 S3 PUTs plus 250,000 S3 GETs, or about $2 USD per month.  Add in 100GB of data storage and it's another $3.    ## Web Server Configuration    Cronicle has an embedded web server which handles serving up the user interface, as well as some server-to-server communication that takes place between the primary and workers.  This is configured in the `WebServer` object, and there are only a handful of parameters you should ever need to configure:    ```js  {  	""WebServer"": {  		""http_port"": 3012,  		  		""https"": 0,  		""https_port"": 3013,  		""https_cert_file"": ""conf/ssl.crt"",  		""https_key_file"": ""conf/ssl.key""  	}  }  ```    Changing the `http_port` is probably the most common thing you will want to customize.  For example, if you don't have anything else running on port 80, you will probably want to change it to that, so you can access the UI without entering a port number.    This is also where you can enable HTTPS, if you want the UI to be SSL encrypted.  Set the `https` property to `1` to enable, and configure the `https_port` as you see fit (the standard HTTPS port is `443`).  You will have to supply your own SSL certificate files (sample self-signed certs are provided for testing, but they will generate browser warnings).    For more details on the web server component, please see the [pixl-server-web](https://www.npmjs.com/package/pixl-server-web#configuration) module documentation.    ## User Configuration    Cronicle has a simple user login and management system, which is built on the [pixl-server-user](https://www.npmjs.com/package/pixl-server-user) module.  It handles creating new users, assigning permissions, and login / session management.  It is configured in the `User` object, and there are only a couple of parameters you should ever need to configure:    ```js  {  	""User"": {  		""free_accounts"": 0,  		  		""default_privileges"": {  			""admin"": 0,  			""create_events"": 1,  			""edit_events"": 1,  			""delete_events"": 1,  			""run_events"": 0,  			""abort_events"": 0,  			""state_update"": 0  		}  	}  }  ```    The `free_accounts` property specifies whether guests visiting the UI can create their own accounts, or not.  This defaults to `0` (disabled), but you can set it to `1` to enable.  This feature should only be used when your install of Cronicle is running on a private network, and you trust all your employees.    The `default_privileges` object specifies which privileges new accounts will receive by default.  Here is a list of all the possible privileges and what they mean:    | Privilege ID | Description |  |--------------|-------------|  | `admin` | User is a full administrator.  **This automatically grants ALL privileges, current and future.** |  | `create_events` | User is allowed to create new events and add them to the schedule. |  | `edit_events` | User is allowed to edit and save events -- even those created by others. |  | `delete_events` | User is allowed to delete events -- event those created by others. |  | `run_events` | User is allowed to run events on-demand by clicking the ""Run"" button in the UI. |  | `abort_events` | User is allowed to abort jobs in progress, even those for events created by others. |  | `state_update` | User is allowed to enable or disable the primary scheduler. |    By default new users have the `create_events`, `edit_events` and `delete_events` privileges, and nothing else.  Note that when an administrator creates new accounts via the UI, (s)he can customize the privileges at that point.  The configuration only sets the defaults.    For more details on the user manager component, please see the [pixl-server-user](https://www.npmjs.com/package/pixl-server-user#configuration) module documentation.    ## Email Configuration    Cronicle will send a number of different types of e-mails in response to certain events.  These are mostly confirmations of actions, or just simple notifications.  Most of these can be disabled in the UI if desired.  The e-mail content is also configurable, including the `From` and `Subject` headers, and is based on plain text e-mail template files located on disk:    | Action | Email Template | Description |  |--------|----------------|-------------|  | **New User Account** | `conf/emails/welcome_new_user.txt` | Sent when a new user account is created. |  | **Changed Password** | `conf/emails/changed_password.txt` | Sent when a user changes their password. |  | **Recover Password** | `conf/emails/recover_password.txt` | Sent when a user requests password recovery. |  | **Job Succeeded** | `conf/emails/job_success.txt` | Conditionally sent when a job completes successfully (depends on event configuration). |  | **Job Failed** | `conf/emails/job_fail.txt` | Conditionally sent when a job fails (depends on event configuration). |  | **Event Error** | `conf/emails/event_error.txt` | Sent when a job fails to launch (depends on event configuration). |    Feel free to edit these files to your liking.  Note that any text in `[/square_brackets]` is a placeholder which gets swapped out with live data relevant to the event which fired off the e-mail.    Here is an example e-mail template file:    ```  To: [/notify_success]  From: [/config/email_from]  Subject: Cronicle Job Completed Successfully: [/event_title]    Date/Time: [/nice_date_time]  Event Title: [/event_title]  Category: [/category_title]  Server Target: [/nice_target]  Plugin: [/plugin_title]    Job ID: [/id]  Hostname: [/hostname]  PID: [/pid]  Elapsed Time: [/nice_elapsed]  Performance Metrics: [/perf]  Avg. Memory Usage: [/nice_mem]  Avg. CPU Usage: [/nice_cpu]    Job Details:  [/job_details_url]    Job Debug Log ([/nice_log_size]):  [/job_log_url]    Edit Event:  [/edit_event_url]    Description:  [/description]    Event Notes:  [/notes]    Regards,  The Cronicle Team  ```    The stock e-mail templates shipped with Cronicle are plain text, but you can provide your own rich HTML e-mail templates if you want.  Simply start the e-mail body content (what comes after the Subject line) with an HTML open tag, e.g. `<div>`, and the e-mails will be sent as HTML instead of text.    You can include any property from the main `conf/config.json` file by using the syntax `[/config/KEY]`.  Also, to include environment variables, use the syntax `[/env/ENV_KEY]`, for example `[/env/NODE_ENV]`.    # Web UI    This section describes the Cronicle web user interface.  It has been tested extensively in Safari, Chrome and Firefox.  Recent versions of IE should also work (11 and Edge).    ## Home Tab    ![Home Tab Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/home.png)    The **Home** tab, also known as the dashboard, is the default page shown after you log in.  It displays basic information about the application, currently active (running) jobs, and a list of upcoming jobs in the next 24 hours.  The page is split vertically into three main sections:    ### General Stats    This section contains a summary of various Cronicle stats.  Some are current totals, and others are daily totals.  Here is a list of everything that is displayed:    | Statistic | Description |  |-----------|-------------|  | **Total Events** | Current number of active events in the schedule. |  | **Total Categories** | Current number of event categories in the system. |  | **Total Plugins** | Current number of registered Plugins in the system. |  | **Jobs Completed Today** | Number of jobs completed today (resets at midnight, local server time). |  | **Jobs Failed Today** | Number of jobs that failed today (resets at midnight, local server time). |  | **Job Success Rate** | Percentage of completed jobs that succeeded today (resets at midnight, local server time). |  | **Total Servers** | Current total number of servers in the Cronicle cluster. |  | **Total CPU in Use** | Current total CPU in use (all servers, all jobs, all processes). |  | **Total RAM in Use** | Current total RAM in use (all servers, all jobs, all processes). |  | **Primary Server Uptime** | Elapsed time since Cronicle on the primary server was restarted. |  | **Average Job Duration** | The average elapsed time for all completed jobs today (resets at midnight, local server time). |  | **Average Job Log Size** | The average job log file size for all completed jobs today (resets at midnight, local server time). |    ### Active Jobs    This table lists all the currently active (running) jobs, and various information about them.  The table columns are:    | Column | Description |  |--------|-------------|  | **Job ID** | A unique ID assigned to the job.  Click this to see live job progress (see [Job Details Tab](#job-details-tab) below). |  | **Event Name** | The name of the scheduled event which started the job. |  | **Category** | The category to which the event is assigned. |  | **Hostname** | The server hostname which is running the job. |  | **Elapsed** | The current elapsed time since the job started. |  | **Progress** | A visual representation of the job's progress, if available. |  | **Remaining** | The estimated remaining time, if available. |  | **Actions** | Click *Abort* to cancel the job. |    ### Upcoming Events    This table lists all the upcoming scheduled events in the next 24 hours, and various information about them.  The table columns are:    | Column | Description |  |--------|-------------|  | **Event Name** | The name of the scheduled event.  Click this to edit the event (see [Edit Event Tab](#edit-event-tab) below). |  | **Category** | The category to which the event is assigned. |  | **Plugin** | The Plugin which will be loaded to run the event. |  | **Target** | The server target (server group or individual server hostname) which will run the event. |  | **Scheduled Time** | When the event is scheduled to run (in your local timezone unless otherwise specified). |  | **Countdown** | How much time remains until the event runs. |  | **Actions** | Click *Edit Event* to edit the event (see [Edit Event Tab](#edit-event-tab) below). |    ## Schedule Tab    ![Schedule Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/schedule.png)    This tab displays the list of all events currently in the schedule, including both active and disabled events.  From here you can add new events, edit existing events, run events on-demand, and jump to locations such as [Event History](#event-history-tab) and [Event Stats](#event-stats-tab).  The schedule table has the following columns:    | Column | Description |  |--------|-------------|  | **Active** | This checkbox indicates whether the event is active or disabled.  Click it to toggle the state. |  | **Event Name** | The name of the scheduled event.  Click this to edit the event (see [Edit Event Tab](#edit-event-tab) below). |  | **Timing** | A summary of the event's timing settings (daily, hourly, etc.). |  | **Category** | The category to which the event is assigned. |  | **Plugin** | The Plugin which will be loaded to run the event. |  | **Target** | The server target (server group or individual server hostname) which will run the event. |  | **Status** | Current status of the event (idle or number of running jobs). |  | **Actions** | A list of actions to run on the event.  See below. |    Here are the actions you can run on each event from the Schedule tab:    | Action | Description |  |--------|-------------|  | **Run** | Immediately runs the event (starts an on-demand job), regardless of the event's timing settings. |  | **Edit** | This jumps over to the [Edit Event Tab](#edit-event-tab) to edit the event. |  | **Stats** | This jumps over to the [Event Stats Tab](#event-stats-tab) to see statistics about the event and past jobs. |  | **History** | This jumps over to the [Event History Tab](#event-history-tab) to see the event's history of completed jobs. |    ### Edit Event Tab    ![Edit Event Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/edit-event.png)    The Edit Event Tab displays a form for editing scheduled events, and creating new ones.  Here are all the form fields and what they mean:    #### Event ID    Each event has a unique ID which is used when making API calls, and can be ignored otherwise.  This is only displayed when editing events.    #### Event Name    Each event has a name, which can be anything you like.  It is displayed on the Schedule tab, and in reports, e-mails, etc.    #### Event Enabled    This checkbox specifies whether the event is enabled (active) in the scheduler, and will fire off jobs according to the [Event Timing](#event-timing), or disabled.  If disabled, you can still run on-demand jobs by clicking the ""Run Now"" button.    #### Event Category    All events are assigned to a particular category.  If you don't want to create categories, just assign your events to the provided ""General"" category.  Categories can define limits such as max concurrent jobs, max RAM per job and max CPU per job.  See the [Categories Tab](#categories-tab) below for more details on creating categories.    #### Event Target    In a multi-server cluster, events can be targeted to run on individual servers, or server groups.  Both are listed in the drop-down menu.  If a server group is targeted, one of the group's servers is chosen each time the event runs a job.  You can decide which algorithm to use for picking servers from the group (see below).  Also, see the [Servers Tab](#servers-tab) for more details on creating server groups.    ##### Algorithm    When you target a server group for your event, a supplementary menu appears to select an ""algorithm"".  This is simply the method by which Cronicle picks a server in the group to run your job.  The default is ""Random"" (i.e. select a random server from the group for each job), but there are several others as well:    | Algorithm ID | Algorithm Name | Description |  |--------------|----------------|-------------|  | `random` | **Random** | Pick a random server from the group. |  | `round_robin` | **Round Robin** | Pick each server in sequence (alphabetically sorted). |  | `least_cpu` | **Least CPU Usage** | Pick the server with the least amount of CPU usage in the group. |  | `least_mem` | **Least Memory Usage** | Pick the server with the least amount of memory usage in the group. |  | `prefer_first` | **Prefer First** | Prefer the first server in the group (alphabetically sorted), only picking alternatives if the first server is unavailable. |  | `prefer_last` | **Prefer Last** | Prefer the last server in the group (alphabetically sorted), only picking alternatives if the last server is unavailable. |  | `multiplex` | **Multiplex** | Run the event on **all** servers in the group simultaneously (see below). |    ##### Multiplexing    If the event targets a server group, you have the option of ""multiplexing"" it.  That is, the event will run jobs on *all* the servers in the group at once, rather than picking one server at random.    When this feature is enabled, an optional ""Stagger"" text field will appear.  This allows you to stagger (progressively delay) the launch of jobs across the servers, so they don't all start at the same exact time.    #### Event Plugin    Whenever Cronicle runs an event, a ""Plugin"" is loaded to handle the job.  This is basically a shell command which runs as its own process, reads JSON from STDIN to receive metadata about the job, and writes JSON to STDOUT to report progress and completion.  Plugins can be written in virtually any language.  See the [Plugins](#plugins) section below for more details.    If the Plugin defines any custom parameters, they are editable per event.  This feature allows the Plugin to define a set of UI elements (text fields, checkboxes, drop-down menus, etc.) which the event editor can provide values for.  Then, when jobs are started, the Plugin is provided a JSON document containing all the custom keys and values set by the UI for the event.    #### Event Timing    ![Event Timing Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/edit-event-timing.png)    Events are scheduled to run at various dates and times using a visual multi-selector widget, as shown above.  This allows you to multi-select any combination of years, months, days, weekdays, hours and/or minutes, for an event to run on.  It will also repeat on a recurring basis, each time the server clock matches your selections.  This is very similar to the [Unix Cron](https://en.wikipedia.org/wiki/Cron) format, but with a more visual user interface.    If you leave all the boxes unchecked in a particular time scale, it means the same as ""all"" (similar to the Cron asterisk `*` operator).  So if you leave everything blank in all the categories and select only the "":00"" minute, it means: every year, every month, every day, every weekday, and every hour on the "":00"" minute.  Or in other words, hourly.    If you click the ""Import..."" link, you can import date/time settings from a Crontab, i.e. the famous `* * * * *` syntax.  This saves you the hassle of having to translate your existing Crontabs over to the Cronicle UI by hand.    By default, events are scheduled in your current local timezone, but you can customize this using the menu at the top-right.  The menu is pre-populated with all the [IANA standard timezones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).  Also, the timezone selection is saved with each event, so Cronicle always knows exactly when the they should all run, regardless of the server timezone.    The Cronicle scheduling system is versatile, but it can't do *everything*.  For example, you cannot schedule an event to run at two different times on two different days, such as 2:30 PM on Monday and 8:30 AM on Tuesday.  For that, you'd have to create separate events.    #### Event Concurrency    The Event Concurrency selector allows you to specify how many simultaneous jobs are allowed to run for the event.  For example, multiple jobs may need to run if your event is scheduled to run hourly, but it takes longer than a hour to complete a job.  Or, a job may be already running and someone clicks the ""Run Now"" link on the Schedule tab.  This selector specifies the maximum allowed jobs to run concurrently for each event.    If an event cannot start a job due to the concurrency limit, an error is logged (see the [Activity Log Tab](#activity-log-tab) below).  What happens next depends on the [Event Options](#event-options).  If the event has **Run All (Catch-Up)** mode enabled, then the scheduler will keep trying to run every scheduled job indefinitely.  Otherwise, it will simply wait until the next scheduled run.    #### Event Timeout    You can optionally specify an event timeout, which is a maximum run time for event jobs.  If a job takes longer than the specified timeout period, it is aborted, and logged as a failed job.  To disable the timeout and allow jobs to run indefinitely, set this field to `0`.    #### Event Retries    If a job throws an internal error (meaning, it returns a non-zero `code` in the JSON response, or a shell command exits with a non-zero exit code), you can have Cronicle automatically retry it up to 32 times.  Aborting a job (either manually or by a timeout) does not trigger a retry.    If the retries is set to a non-zero amount, a ""Retry Delay"" text field will appear.  This allows you to have Cronicle wait a certain amount of time between retries, if you want.  The idea is to reduce bashing on services that may be overloaded.    Note that the [Event Timeout](#event-timeout) applies to the total run time of the job, *which includes all retries*.  For example, if you set the timeout to 10 minutes, and the job takes 9 minutes and fails, any retries will then only have 1 minute to complete the job.  So please set the timeout accordingly.    #### Event Options    This section allows you to select options on how jobs are handled for the event:    ##### Run All Mode    When Run All (Catch-Up) Mode mode is enabled on an event, the scheduler will do its best to ensure that *every* scheduled job will run, even if they have to run late.  This is useful for time-sensitive events such as generating reports.  So for example, if you have an event scheduled to run hourly, but something prevents it from starting or completing (see below), the scheduler will *keep trying indefinitely* until each separate hourly job runs.  If the event cannot run for multiple hours, the jobs will simply queue up, and the scheduler will run them all in order, as quickly as its rules allow.    If any of the following situations occur, and the event has Run All (Catch-Up) mode enabled, the scheduler will queue up and re-run all missed jobs:    * Job could not run due to concurrency limits.  * Job could not run due to the target server being unavailable.  * Job could not run due to the event category or Plugin being disabled.  * Server running the job was shut down.  * Server running the job crashed.  * Job was aborted due to exceeding a timeout limit.  * Job was aborted due to exceeding a resource limit (RAM or CPU).    The only time a Catch-Up job is *not* re-run is when one of the following actions occur:    * Job is manually aborted via the Web UI or API.  * Job fails due to error thrown from inside the Plugin (user code generated error).    You can see all queued jobs on the [Home Tab](#home-tab).  They will be listed in the [Upcoming Events](#upcoming-events) table, and have their ""Countdown"" column set to ""Now"".  To jump over the queue and reset an event that has fallen behind, use the [Event Time Machine](#event-time-machine) feature.    When Run All (Catch-Up) mode is disabled, and a job cannot run or fails due to any of the reasons listed above, the scheduler simply logs an error, and resumes normal operations.  The event will not run until the next scheduled time, if any.  This is more suitable for events that are not time-sensitive, such as log rotation.    ##### Detached Mode    When Uninterruptible (Detached) mode is enabled on an event, jobs are spawned as standalone background processes, which are not interrupted for things like the Cronicle daemon restarting.  This is designed mainly for critical operations that *cannot* be stopped in the middle for whatever reason.    Please use this mode with caution, and only when truly needed, as there are downsides.  First of all, since the process runs detached and standalone, there are no real-time updates.  Meaning, the progress bar and time remaining displays are delayed by up to a minute.  Also, when your job completes, there is a delay of up to a minute before Cronicle realizes and marks the job as complete.    It is much better to design your jobs to be interrupted, if at all possible.  Note that Cronicle will re-run interrupted jobs if they have [Run All Mode](#run-all-mode) set.  So Detached Mode should only be needed in very special circumstances.    ##### Allow Queued Jobs    By default, when jobs cannot run due to concurrency settings, or other issues like an unavailable target server, an error is generated.  That is, unless you enable the event queue.  With queuing enabled, jobs that can't run immediately are queued up, and executed on a first come, first serve basis, as quickly as conditions allow.    When the queue is enabled on an event, a new ""Queue Limit"" section will appear in the form, allowing you to set the maximum queue length per event.  If this limit is reached, no additional jobs can be queued, and an error will be generated.    You can track the progress of your event queues on the [Home Tab](#home-tab).  Queued events and counts appear in a table between the [Active Jobs](#active-jobs) and [Upcoming Events](#upcoming-events) sections.  From there you can also ""flush"" an event queue (i.e. delete all queued jobs), in case one grows out of control.    ##### Chain Reaction    Chain Reaction mode allows you to select an event which will be launched automatically each time the current event completes a job.  You are essentially ""chaining"" two events together, so one always runs at the completion of the other.  This chain can be any number of events long, and the events can all run on different servers.    You can optionally select different events to run if the current job succeeds or fails.  For example, you may have a special error handling / notification event, which needs to run upon specific event failures.    You can have more control over this process by using the JSON API in your Plugins.  See [Chain Reaction Control](#chain-reaction-control) below for details.    #### Event Time Machine    When editing an existing event that has Run All (Catch-Up) mode enabled, the **Event Time Machine** will appear.  This is a way to reset the internal ""clock"" for an event, allowing you to re-run past jobs, or skip over a queue of stuck jobs.    For each event in the schedule, Cronicle keeps an internal clock called a ""cursor"".  If you imagine time running along a straight line, the event cursors are points along that line.  When the primary server ticks a new minute, it shifts all the event cursors forward up to the current minute, running any scheduled events along the way.    So for example, if you needed to re-run a daily 4 AM report event, you can just edit the cursor clock and set it back to 3:59 AM.  The cursor will catch up to the current time as quickly as it can, stopping only to run any scheduled events along the way.  You can also use this feature to ""jump"" over a queue, if jobs have stacked up for an event.  Just set the cursor clock to the current time, and the scheduler will resume jobs from that point onward.    The event clock for the Time Machine is displayed and interpreted in the event's currently selected timezone.    #### Event Notification    Cronicle can optionally send out an e-mail notification to a custom list of recipients for each job's completion (for multiple, separate addresses by commas).  You can also specify different e-mail addresses for when job succeeds, vs. when it fails.  The e-mails will contain a plethora of information about the event, the job, and the error if applicable.  Example email contents:    ```  To: ops@local.cronicle.com  From: notify@local.cronicle.com  Subject: Cronicle Job Failed: Rebuild Indexes    Date/Time: 2015/10/24 19:47:50 (GMT-7)  Event Title: Rebuild Indexes  Category: Database  Server Target: db01.prod  Plugin: DB Indexer    Job ID: jig5wyx9801  Hostname: db01.prod  PID: 4796  Elapsed Time: 1 minute, 31 seconds  Performance Metrics: scale=1&total=30.333&db_query=1.699&db_connect=1.941&log_read=2.931&gzip_data=3.773  Memory Usage: 274.5 MB Avg, 275.1 MB Peak  CPU Usage: 31.85% Avg, 36.7% Peak  Error Code: 999    Error Description:  Failed to write to file: /backup/db/schema.sql: Out of disk space    Job Details:  http://local.syncronic.com:3012/#JobDetails?id=jig5wyx9801    Job Debug Log (18.2 K):  http://local.syncronic.com:3012/api/app/get_job_log?id=jig5wyx9801    Edit Event:  http://local.syncronic.com:3012/#Schedule?sub=edit_event&id=3c182051    Event Notes:  This event handles reindexing our primary databases nightly.  Contact Daniel in Ops for details.    Regards,  The Cronicle Team  ```    You have control over much of the content of these e-mails.  The **Error Code** and **Description** are entirely generated by your own Plugins, and can be as custom and verbose as you want.  The **Performance Metrics** are also generated by your Plugins (if applicable), and the **Event Notes** are taken straight from the UI for the event (see [Event Notes](#event-notes) below).  Finally, the entire e-mail template can be customized to including additional information or to fit your company's brand.  HTML formatted e-mails are supported as well.    See the [Email Configuration](#email-configuration) section for more details on customization.    ##### Event Web Hook    Another optional notification method for events is a ""web hook"".  This means Cronicle will send an HTTP POST to a custom URL that you specify, both at the start and the end of each job, and include full details in JSON format.  Your own API endpoint will receive the JSON POST from Cronicle, and then your code can fire off its own custom notification.    You can determine if the request represents a start or the end of a job by looking at the `action` property.  It will be set to `job_start` or `job_complete` respectively.  Here is a list of all the JSON properties that will be included in the web hook, and what they mean:    | JSON Property | Description |  |---------------|-------------|  | `action` | Specifies whether the web hook signifies the start (`job_start`) or end (`job_complete`) of a job. |  | `base_app_url` | The [base_app_url](#base_app_url) configuration property. |  | `category` | The Category ID to which the event is assigned. |  | `category_title` | The title of the Category to which the event is assigned. |  | `code` | The response code as specified by your Plugin (only applicable for `job_complete` hooks). |  | `cpu` | An object representing the min, max, average and latest CPU usage for the job (only applicable for `job_complete` hooks). |  | `description` | A custom text string populated by your Plugin, typically contains the error message on failure. |  | `edit_event_url` | A fully-qualified URL to edit the event in the Cronicle UI. |  | `elapsed` | The total elapsed time for the job, in seconds (only applicable for `job_complete` hooks). |  | `event` | The ID of the event which spawned the job. |  | `event_title` | The title of the event which spawned the job. |  | `hostname` | The hostname of the server which ran (or is about to run) the event. |  | `id` | An auto-assigned unique ID for the job, which can be used in API calls to query for status. |  | `job_details_url` | A fully-qualified URL to view the job details in the Cronicle UI. |  | `log_file_size` | The size of the job's log file in bytes (only applicable for `job_complete` hooks). |  | `mem` | An object representing the min, max, average and latest memory usage for the job (only applicable for `job_complete` hooks). |  | `nice_target` | Will be set to the title of the target server group, or exact server hostname, depending on how the event is configured. |  | `params` | An object containing all the UI selections for the Plugin custom parameters, from the event. |  | `perf` | An object or string containing performance metrics, as reported by your Plugin (only applicable for `job_complete` hooks). |  | `pid` | The Process ID (PID) of the main job process which ran your Plugin code (only applicable for `job_complete` hooks). |  | `plugin` | The ID of the Plugin assigned to the event. |  | `plugin_title` | The title of the Plugin assigned to the event. |  | `source` | A string describing who or what started the job (user or API).  Will be blank if launched normally by the scheduler. |  | `text` | A simple text string describing the action that took place.  Useful for [Slack Webhook Integrations](https://api.slack.com/incoming-webhooks). |  | `time_end` | The Epoch timestamp of when the job ended (only applicable for `job_complete` hooks). |  | `time_start` | The Epoch timestamp of when the job started. |    Here is an example web hook JSON record (`job_complete` version shown):    ```js  {  	""action"": ""job_complete"",  	""base_app_url"": ""http://localhost:3012"",  	""category"": ""general"",  	""category_title"": ""General"",  	""code"": 0,  	""cpu"": {  		""min"": 23.4,  		""max"": 23.4,  		""total"": 23.4,  		""count"": 1,  		""current"": 23.4  	},  	""description"": ""Success!"",  	""edit_event_url"": ""http://localhost:3012/#Schedule?sub=edit_event&id=3c182051"",  	""elapsed"": 90.414,  	""event"": ""3c182051"",  	""event_title"": ""Test Event 2"",  	""hostname"": ""joeretina.local"",  	""id"": ""jihuyalli01"",  	""job_details_url"": ""http://localhost:3012/#JobDetails?id=jihuyalli01"",  	""log_file_size"": 25119,  	""mem"": {  		""min"": 190459904,  		""max"": 190459904,  		""total"": 190459904,  		""count"": 1,  		""current"": 190459904  	},  	""nice_target"": ""joeretina.local"",  	""params"": {  		""db_host"": ""idb01.mycompany.com"",  		""verbose"": 1,  		""cust"": ""Marketing""  	},  	""perf"": ""scale=1&total=90.103&db_query=0.237&db_connect=6.888&log_read=9.781&gzip_data=12.305&http_post=14.867"",  	""pid"": 72589,  	""plugin"": ""test"",  	""plugin_title"": ""Test Plugin"",  	""source"": ""Manual (admin)"",  	""text"": ""Job completed successfully on joeretina.local: Test Event 2 http://localhost:3012/#JobDetails?id=jihuyalli01"",  	""time_end"": 1449431930.628,  	""time_start"": 1449431840.214  }  ```    In addition to `job_start` and `job_complete`, there is one other special hook action that may be sent, and that is `job_launch_failure`.  This happens if a scheduled event completely fails to start a job, due to an unrecoverable error (such as an unavailable target server or group).  In this case the `code` property will be non-zero, and the `description` property will contain a summary of the error.    Only a small subset of the properties shown above will be included with a `job_launch_failure`, as a job object was never successfully created, so there will be no `hostname`, `pid`, `elapsed`, `log_file_size`, etc.    To include custom HTTP request headers with your web hook, append them onto the end of the URL using this format: `[header: My-Header: My-Value]`.  Make sure to include a space before the opening bracket.  Example URL:    ```  https://myserver.com/api/chat.postMessage [header: My-Header: My-Value]  ```    #### Event Resource Limits    ![Resource Limits Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/edit-event-res-limits-new.png)    Cronicle can automatically limit the server resource consumption of your jobs, by monitoring their CPU, memory and/or log file size, and aborting them if your limits are exceeded.  You can also specify ""sustain"" times for CPU and memory, so no action is taken until the limits are exceeded for a certain amount of time.    CPU and RAM usage are measured every 10 seconds, by looking at the process spawned for the job, *and any child processes that may have also been spawned by your code*.  So if you fork your own child subprocess, or shell out to a command-line utility, all the memory is totaled up, and compared against the resource limits for the job.    #### Event Notes    Event notes are for your own internal use.  They are displayed to users when editing an event, and in all e-mails regarding successful or failed jobs.  For example, you could use this to describe the event to members of your team who may not be familiar, and possibly provide a link to other documentation.  There is no character limit, so knock yourself out.    #### Run Now    To run an event immediately, click the **Run Now** button.  This will run the current event regardless of its timing settings, and whether the event is enabled or disabled in the schedule.  This is simply an on-demand job which is created and executed right away.    If you need to customize the internal clock for the job, hold Shift which clicking the Run Now button.  This will bring up a dialog allowing you to set the date and time which the Plugin will see as the ""current time"", for your on-demand job only.  It will not affect any other jobs or the event itself.  This is useful for re-running past jobs with a Plugin that honors the `now` timestamp in the job data.    ## Completed Jobs Tab    ![Completed Jobs Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/completed-jobs.png)    This tab shows you all recently completed jobs, for all events, and whether they succeeded or failed.  Cronicle will keep up to [list_row_max](#list_row_max) job completions in storage (default is 10,000).  The jobs are sorted by completion date/time, with the latest at the top.  Use the pagination controls on the top right to jump further back in time.  The table columns are:    | Column | Description |  |--------|-------------|  | **Job ID** | A unique ID assigned to the job.  Click this to see details (see [Job Details Tab](#job-details-tab) below). |  | **Event Name** | The name of the scheduled event for the job.  Click this to see the event history (see [Event History Tab](#event-history-tab) below). |  | **Category** | The category to which the event is assigned. |  | **Plugin** | The Plugin which was used to run the job. |  | **Hostname** | The hostname of the server which ran the job. |  | **Result** | This shows whether the job completed successfully, or returned an error. |  | **Start Date/Time** | The date/time when the job first started. |  | **Elapsed Time** | The total elapsed time of the job (including any retries). |    ### Event History Tab    ![Event History Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/event-history.png)    This tab shows you all recently completed jobs for one specific event.  Cronicle will keep up to [list_row_max](#list_row_max) job completions in storage (default is 10,000).  The jobs are sorted by completion date/time, with the latest at the top.  Use the pagination controls on the top right to jump further back in time.  The table columns are:    | Column | Description |  |--------|-------------|  | **Job ID** | A unique ID assigned to the job.  Click this to see details (see [Job Details Tab](#job-details-tab) below). |  | **Hostname** | The hostname of the server which ran the job. |  | **Result** | This shows whether the job completed successfully, or returned an error. |  | **Start Date/Time** | The date/time when the job first started. |  | **Elapsed Time** | The total elapsed time of the job (including any retries). |  | **Avg CPU** | The average CPU percentage used by the job process (including any subprocesses), where 100% equals one CPU core. |  | **Avg Mem** | The average memory used by the job process (including any subprocesses). |    ### Event Stats Tab    ![Event Stats Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/event-stats.png)    This tab contains statistics about a specific event, including basic information and performance graphs.  The data is calculated from the last 50 completed jobs.  Here is a list of all the stats that are displayed at the top of the screen:    | Statistic | Description |  |-----------|-------------|  | **Event Name** | The name of the event being displayed. |  | **Category Name** | The category to which the event is assigned. |  | **Event Timing** | The timing settings for the event (daily, hourly, etc.). |  | **Username** | The username of the user who first created the event. |  | **Plugin Name** | The Plugin selected to run jobs for the event. |  | **Event Target** | The server group or individual server selected to run jobs for the event. |  | **Avg. Elapsed** | The average elapsed time of jobs for the event. |  | **Avg. CPU** | The average CPU used by jobs for the event. |  | **Avg. Memory** | The average RAM used by jobs for the event. |  | **Success Rate** | The success percentage rate of jobs for the event. |  | **Last Result** | Shows the result of the latest job completion (success or fail). |  | **Avg. Log Size** | The average log file size of jobs for the event. |    Below the stats are a number of graphs:    ![Graphs Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/event-stats-graphs.png)    The first graph shows performance of your Plugin's metrics over time.  The different categories shown are entirely driven by your custom code.  You can choose to provide performance metrics or not, and add as many custom categories as you like.  For details, see the [Writing Plugins](#writing-plugins) and [Performance Metrics](#performance-metrics) sections below.    Below the performance history graph are the **CPU Usage History** and **Memory Usage History** graphs.  These display your event's server resource usage over time.  Each dot on the graph is a particular job run, and the history goes back for 50 runs, if available.    ## Job Details Tab    ![Job In Progress Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/job-live-progress.png)    The Job Details Tab is used to view jobs currently in progress, and to see details about completed jobs.  The display is slightly different for each case.  For jobs in progress, you'll see some statistics about the job which are updated live (see below), and three large donut charts: The current job progress (if provided by the Plugin), and the current CPU and memory usage of the job.  Below the donuts you'll find the live job log file, updated in real-time.  The stats consist of:    | Statistic | Description |  |-----------|-------------|  | **Job ID** | A unique ID assigned to the job. |  | **Event Name** | The name of the event being displayed. |  | **Event Timing** | The timing settings for the event (daily, hourly, etc.). |  | **Category Name** | The category to which the event is assigned. |  | **Plugin Name** | The Plugin selected to run jobs for the event. |  | **Event Target** | The server group or individual server selected to run jobs for the event. |  | **Job Source** | The source of the job (who started it, the scheduler or manually by hand). |  | **Server Hostname** | The hostname of the server which is running the job. |  | **Process ID** | The ID of the process currently running the job. |  | **Job Started** | The date/time of when the job first started. |  | **Elapsed Time** | The current elapsed time of the job. |  | **Remaining Time** | The estimated remaining time (if available). |    The CPU donut chart visually indicates how close your job is to using a full CPU core.  If it uses more than that (i.e. multiple threads or sub-processes), the chart simply shows ""full"" (solid color).  The Memory donut visually indicates your job's memory vs. the ""maximum"" configured resource limit, or 1 GB if resource limits are not in effect for the job.  The colors are green if the donut is under 50% filled, yellow if between 50% and 75%, and red if over 75%.    When the job completes, or when viewing details about a previously completed job, the display looks slightly different:    ![Job Success Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/job-details-complete.png)    The only difference in the statistics table is that the right-hand column contains the **Job Completed** date/time instead of the remaining time.  Below that, the left-hand donut graph now shows performance metrics from your Plugin (if provided).  You can choose to provide performance metrics or not, and add as many custom categories as you like.  For details, see the [Writing Plugins](#writing-plugins) and [Performance Metrics](#performance-metrics) sections below.    The CPU and Memory donut charts now show the average values over the course of your job run, rather than the ""current"" values while the job is still running.  The same visual maximums and color rules apply (see above).    Here is how the Job Details tab looks when a job fails:    ![Job Failed Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/job-details-error.png)    The error message is displayed in a banner along the top of the screen.  The banner will be yellow if the job was aborted, or red if an error was returned from your Plugin.    In this case the job was aborted, so the Plugin had no chance to report performance metrics, hence the left-hand pie chart is blank.    ## My Account Tab    ![My Account Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/my-account.png)    On this tab you can edit your own account profile, i.e. change your name, e-mail address, and/or password.  You also have the option of completely deleting your account via the ""Delete Account"" button.  For security purposes, in order to save any changes you must enter your existing account password.    Your user avatar image is automatically pulled from the free [Gravatar.com](https://en.gravatar.com/) service, using your e-mail address.  To customize your image, please [login or create a Gravatar account](https://en.gravatar.com/connect/) using the same e-mail address as the one in your Cronicle account.    ## Administration Tab    This tab is only visible and accessible to administrator level users.  It allows you to view the global activity log, manage API keys, categories, Plugins, servers and users.  To access all the various functions, use the tabs along the left side of the page.    ### Activity Log Tab    ![Activity Log Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-activity-log.png)    All activity in Cronicle is logged, and viewable on the **Activity Log** tab.  The log is presented as a paginated table, sorted by descending date/time (newest activity at the top).  Pagination controls are located in the top-right corner.  The table columns are as follows:    | Column | Description |  |--------|-------------|  | **Date/Time** | The date and time of the activity (adjusted for your local timezone). |  | **Type** | The type of activity (error, event, category, server, etc.). |  | **Description** | A description of the activity (varies based on the type). |  | **Username** | The username or API Key associated with the activity, if applicable. |  | **IP Address** | The IP address associated with the activity, if applicable. |  | **Actions** | A set of actions to take on the activity (usually links to more info). |    There are several different types of activity that can appear in the activity log:    | Activity Type | Description |  |---------------|-------------|  | **Error** | Error messages, such as failing to send e-mail or server clocks out of sync. |  | **Warning** | Warning messages, such as a failure to launch a scheduled event. |  | **API Key** | API Key related activity, such as a user creating, modifying or deleting keys. |  | **Category** | Category related activity, such as a user creating, modifying or deleting categories. |  | **Event** | Event related activity, such as a user creating, modifying or deleting events. |  | **Group** | Server Group related activity, such as a user creating, modifying or deleting groups. |  | **Job** | Job related activity, including every job completion (success or fail). |  | **Plugin** | Plugin related activity, such as a user creating, modifying or deleting Plugins. |  | **Server** | Multi-Server related activity, such as servers being added or removed from the cluster. |  | **Scheduler** | Scheduler related activity, such as the scheduler being enabled or disabled. |  | **User** | User related activity, such as a user being created, modified or deleted. |    Cronicle will keep the latest [list_row_max](#list_row_max) activity log entries in storage (the default is 10,000).    ### API Keys Tab    ![API Keys Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-api-keys.png)    [API Keys](#api-keys) allow you to register external applications or services to use the REST API.  This tab lists all the API Keys registered in the system, and allows you to edit, delete and add new keys.  The table columns include:    | Column | Description |  |--------|-------------|  | **App Title** | The application title associated with the key. |  | **API Key** | The API Key itself, which is a 32-character hexadecimal string. |  | **Status** | The status of the key, which can be active or disabled. |  | **Author** | The username who originally created the key. |  | **Created** | The date when the key was created. |  | **Actions** | A list of actions to take (edit and delete). |    When you create or edit a key, you will see this screen:    ![Editing API Key Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-api-keys-edit-2.png)    The API Key form contains the following elements:    - The **API Key** itself, which is an automatically generated 32-character hexadecimal string.  You can manually customize this if desired, or click **Generate Random** to generate a new random key.  - The **Status** which is either `Active` or `Disabled`.  Disable an API Key if you have a misbehaving app or an exposed key, and all API calls will be rejected.  Only active keys are allowed to make any calls.  - The **App Title** which is the name of your app that will be using the key.  This is displayed in various places including the activity log.  - An **App Description** which is an optional verbose description of your app, just so other users can understand what the purpose of each API key is.  - A set of **Privileges**, which grant the API Key access to specific features of Cronicle.  This is the same system used to grant user level permissions.    For more details on how to use the API Key system in your apps, see the [API Keys](#api-keys) section below.    ### Categories Tab    Events can be assigned to custom categories that you define.  This is a great way to enable/disable groups of events at once, set a maximum concurrent job limit for the entire category, set default notification and resource limits, and highlight events with a specific color.    ![Categories Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-categories.png)    On this screen is the list of all event categories.  The table columns are as follows:    | Column | Description |  |--------|-------------|  | **Title** | The title of the category, used for display purposes. |  | **Description** | An optional description of the category. |  | **Assigned Events** | The number of events assigned to the category. |  | **Max Concurrent** | The maximum number of concurrent jobs to allow. |  | **Actions** | A list of actions to take (edit and delete). |    When you create or edit a category, you are taken to this screen:    ![Edit Category Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-category-edit.png)    The category edit form contains the following elements:    - The **Category ID** which is an auto-generated alphanumeric ID used only by the API.  This cannot be changed.  - The **Category Title** which is used for display purposes.  - The **Status** which controls whether the category is active or disabled.  This also affects all events assigned to the category, meaning if the category is disabled, all the events assigned to it will also be effectively disabled as well (the scheduler won't start jobs for them).  - An optional **Description** of the category, for your own use.  - A **Max Concurrent** selector, which allows you to set the maximum allowed jobs to run concurrently across all events in the category.  This is not a default setting that is applied per event, but rather a total overall limit for all assigned events.  - An optional **Highlight Color** which is displayed as the background color on the main [Schedule Tab](#schedule-tab) for all events assigned to the category.  - A set of **Default Notification Options**, which include separate e-mail lists for successful and failed jobs, and a default [Web Hook](#event-web-hook) URL.  These settings may be overridden per each event.  - A set of **Default Resource Limits**, which include separate CPU and Memory limits.  Note that these are measured per job, and not as a category total.  These settings may be overridden per each event (see [Event Resource Limits](#event-resource-limits) for more details).    ### Plugins Tab    [Plugins](#plugins) are used to run jobs, and can be written in virtually any language.  They are spawned as sub-processes, launched via a custom command-line script that you provide.  Communication is achieved via reading and writing JSON to STDIN / STDOUT.  For more details, see the [Plugins](#plugins) section below.    ![Plugins Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugins.png)    On this screen you'll find a list of all the Plugins registered in the system.  The table columns are as follows:    | Column | Description |  |--------|-------------|  | **Plugin Name** | The name of the Plugin, used for display purposes. |  | **Author** | The username of the user who originally created the Plugin. |  | **Number of Events** | The number of events using the Plugin. |  | **Created** | The date when the Plugin was first created. |  | **Modified** | The date when the Plugin was last modified. |  | **Actions** | A list of actions to take (edit and delete). |    When you create or edit a Plugin, you are taken to this screen:    ![Edit Plugin Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit.png)    The Plugin edit form contains the following elements:    - The **Plugin ID** which is an auto-generated alphanumeric ID used only by the API.  This cannot be changed.  - The **Plugin Name** which is used for display purposes.  - The **Status** which controls whether the Plugin is active or disabled.  This also affects all events assigned to the Plugin, meaning if the Plugin is disabled, all the events assigned to it will also be effectively disabled as well (the scheduler won't start jobs for them).  - The **Executable** path, which is executed in a sub-process to run jobs.  You may include command-line arguments here if your Plugin requires them, but no shell redirects or pipes (see the [Shell Plugin](#built-in-shell-plugin) if you need those).  - A set of custom **Parameters** which you can define here, and then edit later for each event.  These parameters are then passed to your Plugin when jobs run. See below for details.  - A set of **Advanced Options**, including customizing the working directory, and user/group (all optional).  See below for details.    #### Plugin Parameters    The Parameter system allows you to define a set of UI controls for your Plugin (text fields, text boxes, checkboxes, drop-down menus, etc.) which are then presented to the user when editing events.  This can be useful if your Plugin has configurable behavior which you want to expose to people creating events in the schedule.  Each Parameter has an ID which identifies the user's selection, and is included in the JSON data sent to your Plugin for each job.    When adding or editing Plugin Parameters, you will be presented with this dialog:    ![Edit Plugin Param Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit-param.png)    Here you will need to provide:    - A **Parameter ID** which uniquely identifies the parameter in the JSON data sent to your Plugin.  - A **Label** for display purposes (used in the UI only).  - A **Control Type** selection, which can be a text field, a text box, a checkbox, a drop-down menu, or a hidden variable.    Depending on the control type selected, you may need to provide different pieces of information to render the control in the UI, such as a comma-separated list of values for a menu, a default text field value and size, etc.    Parameter values for each event are passed to your Plugin when jobs are launched.  These arrive as a JSON object (located in `params`), as well as upper-case [environment variables](https://en.wikipedia.org/wiki/Environment_variable).  So for example, you can use Plugin Parameters to set custom environment variables for your jobs, or override existing ones such as `PATH`.  Example of this:    ![Edit Plugin PATH Example](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit-path.png)    For more details on how to use these parameters in your Plugin code, see the [Plugins](#plugins) section below.    #### Advanced Plugin Options    The **Advanced** options pane expands to the following:    ![Edit Plugin Advanced Panel](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit-advanced-uid.png)    Here you can customize the following settings:    | Option | Description |  |--------|-------------|  | **Working Directory** | This is the directory path to set as the [current working directory](https://en.wikipedia.org/wiki/Working_directory) when your jobs are launched.  It defaults to the Cronicle base directory (`/opt/cronicle`). |  | **Run as User** | You can optionally run your jobs as another user by entering their UID here.  A username string is also acceptable (and recommended, as UIDs may differ between servers). |    ### Servers Tab    ![Servers Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-servers.png)    When Cronicle is configured to run in a multi-server environment, this tab allows you to manage the cluster.  At the top of the page you'll see a list of all the servers currently registered, their status, and controls to restart or shut them down.  The table columns include:    | Column | Description |  |--------|-------------|  | **Hostname** | The hostname of the server. |  | **IP Address** | The IP address of the server. |  | **Groups** | A list of the groups to which the server belongs. |  | **Status** | The current server status (primary, backup or worker). |  | **Active Jobs** | The number of active jobs currently running on the server. |  | **Uptime** | The elapsed time since Cronicle was restarted on the server. |  | **CPU** | The current CPU usage on the server (all jobs). |  | **Memory** | The current memory usage on the server (all jobs). |  | **Actions** | A list of actions to take (restart and shutdown). |    There is also an **Add Server** button, but note that servers on the same LAN should be automatically discovered and added to the cluster.  You will only need to manually add a server if it lives in a remote location, outside of local UDP broadcast range.    #### Server Groups    Below the server cluster you'll find a list of server groups.  These serve two purposes.  First, you can define groups in order to target events at them.  For example, an event can target the group of servers instead of an individual server, and one of the servers will be picked for each job (or, if [Multiplex](#multiplexing) is enabled, all the servers at once).  Second, you can use server groups to define which of your servers are eligible to become the primary server, if the current primary is shut down.    When Cronicle is first installed, two server groups are created by default.  A ""Primary Group"" which contains only the current (primary) server, and an ""All Servers"" group, which contains all the servers (current and future).  Groups automatically add servers by a hostname-based regular expression match.  Therefore, when additional servers join the cluster, they will be assigned to groups automatically via their hostname.    The list of server groups contains the following columns:    | Column | Description |  |--------|-------------|  | **Title** | The title of the server group. |  | **Hostname Match** | A hostname regular expression to automatically add servers to the group. |  | **Number of Servers** | The number of servers currently in the group. |  | **Number of Events** | The number of events currently targeting the group. |  | **Class** | The server group classification (whether it supports becoming primary or not). |  | **Actions** | A list of actions to take (edit and delete). |    When adding or editing a server group, you will be presented with this dialog:    ![Edit Server Group Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-servers-group-edit.png)    Here you will need to provide:    - A **Group Title** which is used for display purposes.  - A **Hostname Match** which is a regular expression applied to every server hostname (used to automatically add servers to the group).  - A **Server Class** which sets the servers in your group as ""Primary Eligible"" or ""Worker Only"".    Note that ""Primary Eligible"" servers all need to be properly configured and have access to your storage back-end.  Meaning, if you opted to use the filesystem, you'll need to make sure it is mounted (via NFS or similar mechanism) on all the servers who could become primary.  Or, if you opted to use a NoSQL DB such as Couchbase or S3, they need all the proper settings and/or credentials to connect.  For more details, see the [Multi-Server Cluster](#multi-server-cluster) section.    ### Users Tab    ![Users Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-users.png)    This tab shows you all the user accounts registered in the system, and allows you to edit, delete and add new ones.  The table columns are as follows:    | Column | Description |  |--------|-------------|  | **Username** | The account username. |  | **Full Name** | The user's full name. |  | **Email Address** | The user's e-mail address. |  | **Status** | The account status (active or disabled). |  | **Type** | The account type (standard or administrator). |  | **Created** | The date when the user was first created. |  | **Actions** | A list of actions to take (edit and delete). |    When you create or edit a user, you are taken to this screen:    ![Edit User Screenshot](https://pixlcore.com/software/cronicle/screenshots-new/admin-users-edit.png)    The user edit form contains the following elements:    - The **Username**, which cannot be edited after the user is created.  Usernames may contain alphanumeric characters, periods and dashes.  - The **Account Status** which is either `Active` or `Disabled`.  Disabled accounts cannot login.  - The user's **Full Name**, which is used for display purposes.  - The user's **Email Address**, which is used for event and error notifications.  - The account **Password**, which can be reset here, as well as randomized.  - A set of **Privileges**, which grant the user access to specific features of Cronicle.    # Plugins    Plugins handle running your events, and reporting status back to the Cronicle daemon.  They can be written in virtually any language, as they are really just command-line executables.  Cronicle spawns a sub-process for each job, executes a command-line you specify, and then uses [pipes](https://en.wikipedia.org/wiki/Pipeline_%28Unix%29) to pass in job information and retrieve status, all in JSON format.    So you can write a Plugin in your language of choice, as long as it can read and write JSON.  Also, Cronicle ships with a built-in Plugin for handling shell scripts, which makes things even easier if you just have some simple shell commands to run, and don't want to have to deal with JSON at all.  See [Shell Plugin](#built-in-shell-plugin) below for more on this.    ## Writing Plugins    To write your own Plugin, all you need is to provide a command-line executable, and have it read and write JSON over [STDIN and STDOUT](https://en.wikipedia.org/wiki/Standard_streams).  Information about the current job is passed as a JSON document to your STDIN, and you can send back status updates and completion events simply by writing JSON to your STDOUT.    Please note that regardless of your choice of programming language, your Plugin needs to be a real command-line executable.  So it needs to have execute file permissions (usually `0755`), and a [shebang](https://en.wikipedia.org/wiki/Shebang_%28Unix%29) line at the top of the file, indicating which interpreter to use.  For example, if you write a Node.js Plugin, you need something like this at the very top of your `.js` file:    ```  	#!/usr/bin/node  ```    The location of the `node` binary may vary on your servers.    ### JSON Input    As soon as your Plugin is launched as a sub-process, a JSON document is piped to its STDIN stream, describing the job.  This will be compacted onto a single line followed by an EOL, so you can simply read a line, and not have to worry about locating the start and end of the JSON.  Here is an example JSON document (pretty-printed here for display purposes):    ```js  {  	""id"": ""jihuxvagi01"",  	""hostname"": ""joeretina.local"",  	""command"": ""/usr/local/bin/my-plugin.js"",  	""event"": ""3c182051"",  	""now"": 1449431125,  	""log_file"": ""/opt/cronicle/logs/jobs/jihuxvagi01.log"",  	""params"": {  		""myparam1"": ""90"",  		""myparam2"": ""Value""  	}  }  ```    There may be other properties in the JSON (many are copied over from the event), but these are the relevant ones:    | Property Name | Description |  |---------------|-------------|  | `id` | A unique alphanumeric ID assigned to the job. |  | `hostname` | The hostname of the server currently processing the job. |  | `command` | The exact command that was executed to start your Plugin. |  | `event` | The Event ID of the event that fired off the job. |  | `now` | A Unix timestamp representing the ""current"" time for the job (very important -- see below). |  | `log_file` | A fully-qualified filesystem path to a unique log file specifically for the job, which you can use. |  | `params` | An object containing your Plugin's custom parameters, filled out with values from the Event Editor. |    The `now` property is special.  It may or may not be the ""current"" time, depending on how and why the job started.  Meaning, if the associated event is set to [Run All Mode](#run-all-mode) it is possible that a previous job couldn't run for some reason, and Cronicle is now ""catching up"" by running missed jobs.  In this situation, the `now` property will be set to the time when the event *should have* run.  For example, if you have a Plugin that generates daily reports, you'll need to know *which day* to run the report for.  Normally this will be the current date, but if a day was missed for some reason, and Cronicle is catching up by running yesterday's job, you should use the `now` time to determine which day's data to pull for your report.    The `log_file` is designed for your Plugin's own use.  It is a unique file created just for the job, and its contents are displayed in real-time via the Cronicle UI on the [Job Details Tab](#job-details-tab).  You can use any log format you want, just make sure you open the file in ""append"" mode (it contains a small header written by the daemon).  Note that you can also just print to STDOUT or STDERR, and these are automatically appended to the log file for you.    The `params` object will contain all the custom parameter keys defined when you created the Plugin (see the [Plugins Tab](#plugins-tab) section), and values populated from the event editor, when the Plugin was selected for the event.  The keys should match the parameter IDs you defined.    ### JSON Output    Your Plugin is expected to write JSON to STDOUT in order to report status back to the Cronicle daemon.  At the very least, you need to notify Cronicle that the job was completed.  This is done by printing a JSON object with a `complete` property set to `1` (or any true value).  You need to make sure the JSON is compacted onto a single line, and ends with a single EOL character (`\n` on Unix).  Example:    ```js  { ""complete"": 1 }  ```    This tells Cronicle that the job was completed, and your process is about to exit.  By default, the job is considered a success.  However, if the job failed and you need to report an error, you must include a `code` property set to any non-zero error code you want, and a `description` property set to a custom error message.  Include these along with the `complete` property in the JSON.  Example:    ```js  { ""complete"": 1, ""code"": 999, ""description"": ""Failed to connect to database."" }  ```    Your error code and description will be displayed on the [Job Details Tab](#job-details-tab), and in any e-mail notifications and/or web hooks sent out for the event completion.    If your Plugin writes anything other than JSON to STDOUT (or STDERR), it is automatically appended to your log file.  This is so you don't have to worry about using existing code or utilities that may emit some kind of output.  Cronicle is very forgiving in this regard.    #### Reporting Progress    In addition to reporting success or failure at the end of a job, you can also optionally report progress at custom intervals while your job is running.  This is how Cronicle can display its visual progress meter in the UI, as well as calculate the estimated time remaining.  To update the progress of a job, simply print a JSON document with a `progress` property, set to a number between `0.0` and `1.0`.  Example:    ```js  { ""progress"": 0.5 }  ```    This would show progress at 50% completion, and automatically calculate the estimated time remaining based on the duration and progress so far.  You can repeat this as often as you like, with as granular progress as you can provide.    #### Performance Metrics    You can optionally include performance metrics at the end of a job, which are displayed as a pie chart on the [Job Details Tab](#job-details-tab).  These metrics can consist of any categories you like, and the JSON format is a simple `perf` object where the values represent the amount of time spent in seconds.  Example:    ```js  { ""perf"": { ""db"": 18.51, ""http"": 3.22, ""gzip"": 0.84 } }  ```    The perf keys can be anything you want.  They are just arbitrary categories you can make up, which represent how your Plugin spent its time during the job.    Cronicle accepts a number of different formats for the perf metrics, to accommodate various performance tracking libraries.  For example, you can provide the metrics in query string format, like this:    ```js  { ""perf"": ""db=18.51&http=3.22&gzip=0.84"" }  ```    If your metrics include a `total` (or `t`) in addition to other metrics, this is assumed to represent the total time, and will automatically be excluded from the pie chart (but included in the performance history graph).    If you track metrics in units other than seconds, you can provide the `scale`.  For example, if your metrics are all in milliseconds, just set the `scale` property to `1000`.  Example:    ```js  { ""perf"": { ""scale"": 1000, ""db"": 1851, ""http"": 3220, ""gzip"": 840 } }  ```    The slightly more complex format produced by our own [pixl-perf](https://www.npmjs.com/package/pixl-perf) library is also supported.    ##### Nested Metrics    In order for the pie chart to be accurate, your perf metrics must not overlap each other.  Each metric should represent a separate period of time.  Put another way, if all the metrics were added together, they should equal the total time.  To illustrate this point, consider the following ""bad"" example:    ```js  { ""perf"": { ""database"": 18.51, ""run_sql_query"": 3.22, ""connect_to_db"": 0.84 } }  ```    In this case the Plugin is tracking three different metrics, but the `database` metric encompasses *all* database related activities, including the `run_sql_query` and `connect_to_db`.  So the `database` metric overlaps the others.  Cronicle has no way of knowing this, so the pie chart would be quite inaccurate, because the three metrics do not add up to the total time.    However, if you want to track nested metrics as well as a parent metric, just make sure you prefix your perf keys properly.  In the above example, all you would need to do is rename the keys like this:    ```js  { ""perf"": { ""db"": 18.51, ""db_run_sql"": 3.22, ""db_connect"": 0.84 } }  ```    Cronicle will automatically detect that the `db` key is used as a prefix in the other two keys, and it will be omitted from the pie chart.  Only the nested `db_run_sql` and `db_connect` keys will become slices of the pie, as they should add up to the total in this case.    Note that *all* the metrics are included in the performance history graph, as that is a line graph, not a pie chart, so it doesn't matter that everything add up to the total time.    #### Changing Notification Settings    Notification settings for the job are configured in the UI at the event level, and handled automatically after your Plugin exits.  E-mail addresses may be entered for both successful and failure results.  However, your Plugin running the job can alter these settings on-the-fly.    For example, if you only want to send a successful e-mail in certain cases, and want to disable it based on some outcome from inside the Plugin, just print some JSON to STDOUT like this:    ```js  { ""notify_success"": """" }  ```    This will disable the e-mail that is normally sent upon success.  Similarly, if you want to disable the failure e-mail, print this to STDOUT:    ```js  { ""notify_fail"": """" }  ```    Another potential use of this feature is to change who gets e-mailed, based on a decision made inside your Plugin.  For example, you may have multiple error severity levels, and want to e-mail a different set of people for the really severe ones.  To do that, just specify a new set of e-mail addresses in the `notify_fail` property:    ```js  { ""notify_fail"": ""emergency-ops-pager@mycompany.com"" }  ```    These JSON updates can be sent as standalone records as shown here, at any time during your job run, or you can batch everything together at the very end:    ```js  { ""complete"": 1, ""code"": 999, ""description"": ""Failed to connect to database."", ""perf"": { ""db"": 18.51, ""db_run_sql"": 3.22, ""db_connect"": 0.84 }, ""notify_fail"": ""emergency-ops-pager@mycompany.com"" }  ```    #### Chain Reaction Control    You can enable or disable [Chain Reaction](#chain-reaction) mode on the fly, by setting the `chain` property in your JSON output.  This allows you to designate another event to launch as soon as the current job completes, or to clear the property (set it to false or a blank string) to disable Chain Reaction mode if it was enabled in the UI.    To enable a chain reaction, you need to know the Event ID of the event you want to trigger.  You can determine this by editing the event in the UI and copy the Event ID from the top of the form, just above the title.  Then you can specify the ID in your jobs by printing some JSON to STDOUT like this:    ```js  { ""chain"": ""e29bf12db"" }  ```    Remember that your job must complete successfully in order to trigger the chain reaction, and fire off the next event.  However, if you want to run a event only on job failure, set the `chain_error` property instead:    ```js  { ""chain_error"": ""e29bf12db"" }  ```    You set both the `chain` and `chain_error` properties, to run different events on success / failure.    To disable chain reaction mode, set the `chain` and `chain_error` properties to false or empty strings:    ```js  { ""chain"": """", ""chain_error"": """" }  ```    ##### Chain Data    When a chained event runs, some additional information is included in the initial JSON job object sent to STDIN:    | Property Name | Description |  |---------------|-------------|  | `source_event` | The ID of the original event that started the chain reaction. |  | `chain_code` | The error code from the original job, or `0` for success. |  | `chain_description` | The error description from the original job, if applicable. |  | `chain_data` | Custom user data, if applicable (see below). |    You can pass custom JSON data to the next event in the chain, when using a [Chain Reaction](#chain-reaction) event.  Simply specify a JSON property called `chain_data` in your JSON output, and pass in anything you want (can be a complex object / array tree), and the next event will receive it.  Example:    ```js  { ""chain"": ""e29bf12db"", ""chain_data"": { ""custom_key"": ""foobar"", ""value"": 42 } }  ```    So in this case when the event `e29bf12db` runs, it will be passed your `chain_data` object as part of the JSON sent to it when the job starts.  The Plugin code running the chained event can access the data by parsing the JSON and grabbing the `chain_data` property.    #### Custom Data Tables    If your Plugin produces statistics or other tabular data at the end of a run, you can have Cronicle render this into a table on the Job Details page.  Simply print a JSON object with a property named `table`, containing the following keys:    | Property Name | Description |  |---------------|-------------|  | `title` | Optional title displayed above the table, defaults to ""Job Stats"". |  | `header` | Optional array of header columns, displayed in shaded bold above the main data rows. |  | `rows` | **Required** array of rows, with each one being its own inner array of column values. |  | `caption` | Optional caption to show under the table (centered, small gray text). |    Here is an example data table.  Note that this has been expanded for documentation purposes, but in practice your JSON needs to be compacted onto a single line when printed to STDOUT.    ```js  {  	""table"": {  		""title"": ""Sample Job Stats"",  		""header"": [  			""IP Address"", ""DNS Lookup"", ""Flag"", ""Count"", ""Percentage""  		],  		""rows"": [  			[""62.121.210.2"", ""directing.com"", ""MaxEvents-ImpsUserHour-DMZ"", 138, ""0.0032%"" ],  			[""97.247.105.50"", ""hsd2.nm.comcast.net"", ""MaxEvents-ImpsUserHour-ILUA"", 84, ""0.0019%"" ],  			[""21.153.110.51"", ""grandnetworks.net"", ""InvalidIP-Basic"", 20, ""0.00046%"" ],  			[""95.224.240.69"", ""hsd6.mi.comcast.net"", ""MaxEvents-ImpsUserHour-NM"", 19, ""0.00044%"" ],  			[""72.129.60.245"", ""hsd6.nm.comcast.net"", ""InvalidCat-Domestic"", 17, ""0.00039%"" ],  			[""21.239.78.116"", ""cable.mindsprung.com"", ""InvalidDog-Exotic"", 15, ""0.00037%"" ],  			[""172.24.147.27"", ""cliento.mchsi.com"", ""MaxEvents-ClicksPer"", 14, ""0.00035%"" ],  			[""60.203.211.33"", ""rgv.res.com"", ""InvalidFrog-Croak"", 14, ""0.00030%"" ],  			[""24.8.8.129"", ""dsl.att.com"", ""Pizza-Hawaiian"", 12, ""0.00025%"" ],  			[""255.255.1.1"", ""favoriteisp.com"", ""Random-Data"", 10, ""0%"" ]  		],  		""caption"": ""This is an example stats table you can generate from within your Plugin code.""  	}  }  ```    This would produce a table like the following:    ![Custom Stats Table Example](https://pixlcore.com/software/cronicle/screenshots-new/job-details-custom-stats.png)    #### Custom HTML Content    If you would prefer to generate your own HTML content from your Plugin code, and just have it rendered into the Job Details page, you can do that as well.  Simply print a JSON object with a property named `html`, containing the following keys:    | Property Name | Description |  |---------------|-------------|  | `title` | Optional title displayed above the section, defaults to ""Job Report"". |  | `content` | **Required** Raw HTML content to render into the page. |  | `caption` | Optional caption to show under your HTML (centered, small gray text). |    Here is an example HTML report.  Note that this has been expanded for documentation purposes, but in practice your JSON needs to be compacted onto a single line when printed to STDOUT.    ```js  {  	""html"": {  		title: ""Sample Job Report"",  		content: ""This is <b>HTML</b> so you can use <i>styling</i> and such."",  		caption: ""This is a caption displayed under your HTML content.""  	}  }  ```    If your Plugin generates plain text instead of HTML, you can just wrap it in a `<pre>` block, which will preserve formatting such as whitespace.    #### Updating The Event    Your job can optionally trigger an event update when it completes.  This can be used to do things such as disable the event (remove it from the schedule) in response to a catastrophic error, or change the event's timing, change the server or group target, and more.    To update the event for a job, simply include an `update_event` object in your Plugin's JSON output, containing any properties from the [Event Data Format](#event-data-format).  Example:    ```js  {  	""update_event"": {  		""enabled"": 0  	}  }  ```    This would cause the event to be disabled, so the schedule would no longer launch it.  Note that you can only update the event once, and it happens at the completion of your job.    ### Job Environment Variables    When processes are spawned to run jobs, your Plugin executable is provided with a copy of the current environment, along with the following custom environment variables:    | Variable | Description |  |----------|-------------|  | `$CRONICLE` | The current Cronicle version, e.g. `1.0.0`. |  | `$JOB_ALGO` | Specifies the algorithm that was used for picking the server from the target group. See [Algorithm](#algorithm). |  | `$JOB_CATCH_UP` | Will be set to `1` if the event has [Run All Mode](#run-all-mode) mode enabled, `0` otherwise. |  | `$JOB_CATEGORY_TITLE` | The Category Title to which the event is assigned.  See [Categories Tab](#categories-tab). |  | `$JOB_CATEGORY` | The Category ID to which the event is assigned.  See [Categories Tab](#categories-tab). |  | `$JOB_CHAIN` | The chain reaction event ID to launch if job completes successfully.  See [Chain Reaction](#chain-reaction). |  | `$JOB_CHAIN_ERROR` | The chain reaction event ID to launch if job fails.  See [Chain Reaction](#chain-reaction). |  | `$JOB_COMMAND` | The command-line executable that was launched for the current Plugin. |  | `$JOB_CPU_LIMIT` | Limits the CPU to the specified percentage (100 = 1 core), abort if exceeded. See [Event Resource Limits](#event-resource-limits). |  | `$JOB_CPU_SUSTAIN` | Only abort if the CPU limit is exceeded for this many seconds. See [Event Resource Limits](#event-resource-limits). |  | `$JOB_DETACHED` | Specifies whether [Detached Mode](#detached-mode) is enabled or not. |  | `$JOB_EVENT_TITLE` | A display name for the event, shown on the [Schedule Tab](#schedule-tab) as well as in reports and e-mails. |  | `$JOB_EVENT` | The ID of the event that launched the job. |  | `$JOB_HOSTNAME` | The hostname of the server chosen to run the current job. |  | `$JOB_ID` | The unique alphanumeric ID assigned to the job. |  | `$JOB_LOG` | The filesystem path to the job log file, which you can append to if you want.  However, STDOUT or STDERR are both piped to the log already. |  | `$JOB_MEMORY_LIMIT` | Limits the memory usage to the specified amount, in bytes. See [Event Resource Limits](#event-resource-limits). |  | `$JOB_MEMORY_SUSTAIN` | Only abort if the memory limit is exceeded for this many seconds. See [Event Resource Limits](#event-resource-limits). |  | `$JOB_MULTIPLEX` | Will be set to `1` if the event has [Multiplexing](#multiplexing) enabled, `0` otherwise. |  | `$JOB_NOTES` | Text notes saved with the event, included in e-mail notifications. See [Event Notes](#event-notes). |  | `$JOB_NOTIFY_FAIL` | List of e-mail recipients to notify upon job failure (CSV). See [Event Notification](#event-notification). |  | `$JOB_NOTIFY_SUCCESS` | List of e-mail recipients to notify upon job success (CSV). See [Event Notification](#event-notification). |  | `$JOB_NOW` | The Epoch timestamp of the job time (may be in the past if re-running a missed job). |  | `$JOB_PLUGIN_TITLE` | The Plugin Title for the associated event. |  | `$JOB_PLUGIN` | The Plugin ID for the associated event. |  | `$JOB_RETRIES` | The number of retries to allow before reporting an error. See [Event Retries](#event-retries). |  | `$JOB_RETRY_DELAY` | Optional delay between retries, in seconds. See [Event Retries](#event-retries). |  | `$JOB_SOURCE` | String representing who launched the job, will be `Scheduler` or `Manual (USERNAME)`. |  | `$JOB_STAGGER` | If [Multiplexing](#multiplexing) is enabled, this specifies the number of seconds to wait between job launches. |  | `$JOB_TIME_START` | The starting time of the job, in Epoch seconds. |  | `$JOB_TIMEOUT` | The event timeout (max run time) in seconds, or `0` if no timeout is set. |  | `$JOB_TIMEZONE` | The timezone for interpreting the event timing settings. Needs to be an [IANA timezone string](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).  See [Event Timing](#event-timing). |  | `$JOB_WEB_HOOK` | An optional URL to hit for the start and end of each job. See [Event Web Hook](#event-web-hook). |    In addition, any [Plugin Parameters](#plugin-parameters) are also passed as environment variables.  The keys are converted to upper-case, as that seems to be the standard.  So for example, you can customize the `PATH` by declaring it as a Plugin Parameter:    ![Edit Plugin PATH Example](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit-path.png)    You can also include inline variables in the parameter value itself, using the syntax `$VARNAME`.  So for example, if you wanted to *append* to the current `PATH` instead of having to set it from scratch, you could:    ![Edit Plugin PATH Inline Example](https://pixlcore.com/software/cronicle/screenshots-new/admin-plugin-edit-path-inline.png)    Please note that if you do overwrite the entire path, you must include the location of the Node.js `node` binary (typically in `/usr/bin` or `/usr/local/bin`).  Otherwise, things will not work well.    ## Sample Node Plugin    Here is a sample Plugin written in [Node.js](https://nodejs.org/):    ```js  #!/usr/bin/env node    var rl = require('readline').createInterface({ input: process.stdin });    rl.on('line', function(line) {  	// got line from stdin, parse JSON  	var job = JSON.parse(line);  	console.log(""Running job: "" + job.id + "": "" + JSON.stringify(job) );  	  	// Update progress at 50%  	setTimeout( function() {  		console.log(""Halfway there!"");  		process.stdout.write( JSON.stringify({ progress: 0.5 }) + ""\n"" );  	}, 5 * 1000 );  	  	// Write completion to stdout  	setTimeout( function() {  		console.log(""Job complete, exiting."");  		  		process.stdout.write( JSON.stringify({  			complete: 1,  			code: 0  		}) + ""\n"" );  		  	}, 10 * 1000 );  	  	// close readline interface  	rl.close();  });  ```    ## Sample Perl Plugin    Here is a sample Plugin written in [Perl](https://www.perl.org/), using the [JSON](http://search.cpan.org/perldoc?JSON) module:    ```perl  #!/usr/bin/env perl    use strict;  use JSON;    # set output autoflush  $| = 1;    # read line from stdin -- it should be our job JSON  my $line = <STDIN>;  my $job = decode_json($line);    print ""Running job: "" . $job->{id} . "": "" . encode_json($job) . ""\n"";    # report progress at 50%  sleep 5;  print ""Halfway there!\n"";  print encode_json({ progress => 0.5 }) . ""\n"";    sleep 5;  print ""Job complete, exiting.\n"";    # All done, send completion via JSON  print encode_json({  	complete => 1,  	code => 0  }) . ""\n"";    exit(0);    1;  ```    ## Sample PHP Plugin    Here is a sample Plugin written in [PHP](https://php.net/):    ```php  #!/usr/bin/env php  <?php    // make sure php flushes after every print  ob_implicit_flush();    // read line from stdin -- it should be our job JSON  $line = fgets( STDIN );  $job = json_decode($line, true);    print( ""Running job: "" . $job['id'] . "": "" . json_encode($job) . ""\n"" );    // report progress at 50%  sleep(5);  print( ""Halfway there!\n"" );  print( json_encode(array( 'progress' => 0.5 )) . ""\n"" );    sleep(5);  print( ""Job complete, exiting.\n"" );    // All done, send completion via JSON  print( json_encode(array(  	'complete' => 1,  	'code' => 0  )) . ""\n"" );    exit(0);  ?>  ```    ## Built-in Shell Plugin    Cronicle ships with a built-in ""Shell Plugin"", which you can use to execute arbitrary shell scripts.  Simply select the Shell Plugin from the [Edit Event Tab](#edit-event-tab), and enter your script.  This is an easy way to get up and running quickly, because you don't have to worry about reading or writing JSON.    The Shell Plugin determines success or failure based on the [exit code](https://en.wikipedia.org/wiki/Exit_status) of your script.  This defaults to `0` representing success.  Meaning, if you want to trigger an error, exit with a non-zero status code, and make sure you print your error message to STDOUT or STDERR (both will be appended to your job's log file).  Example:    ```sh  #!/bin/bash    # Perform tasks or die trying...  /usr/local/bin/my-task-1.bin || exit 1  /usr/local/bin/my-task-2.bin || exit 1  /usr/local/bin/my-task-3.bin || exit 1  ```    You can still report intermediate progress with the Shell Plugin.  It can accept JSON in the [standard output format](#json-output) if enabled, but there is also a shorthand.  You can echo a single number on its own line, from 0 to 100, with a `%` suffix, and that will be interpreted as the current progress.  Example:    ```sh  #!/bin/bash    # Perform some long-running task...  /usr/local/bin/my-task-1.bin || exit 1  echo ""25%""    # And another...  /usr/local/bin/my-task-2.bin || exit 1  echo ""50%""    # And another...  /usr/local/bin/my-task-3.bin || exit 1  echo ""75%""    # And the final task...  /usr/local/bin/my-task-4.bin || exit 1  ```    This would allow Cronicle to show a graphical progress bar on the [Home](#home-tab) and [Job Details](#job-details-tab) tabs, and estimate the time remaining based on the elapsed time and current progress.    **Pro-Tip:** The Shell Plugin actually supports any interpreted scripting language, including Node.js, PHP, Perl, Python, and more.  Basically, any language that supports a [Shebang](https://en.wikipedia.org/wiki/Shebang_%28Unix%29) line will work in the Shell Plugin.  Just change the `#!/bin/sh` to point to your interpreter binary of choice.    ## Built-in HTTP Request Plugin    Cronicle ships with a built-in ""HTTP Request"" Plugin, which you can use to send simple GET, HEAD or POST requests to any URL, and log the response.  You can specify custom HTTP request headers, and also supply regular expressions to match a successful response based on the content.  Here is the user interface when selected:    ![HTTP Request Plugin](https://pixlcore.com/software/cronicle/screenshots-new/http-request-plugin.png)    Here are descriptions of the parameters:    | Plugin Parameter | Description |  |------------------|-------------|  | **Method** | Select the HTTP request method, either GET, HEAD or POST. |  | **URL** | Enter your fully-qualified URL here, which must begin with either `http://` or `https://`. |  | **Headers** | Optionally include any custom request headers here, one per line. |  | **POST Data** | If you are sending a HTTP POST, enter the raw POST data here. |  | **Timeout** | Enter the timeout in seconds, which is measured as the time to first byte in the response. |  | **Follow Redirects** | Check this box to automatically follow HTTP redirect responses (up to 32 of them). |  | **SSL Cert Bypass** | Check this box if you need to make HTTPS requests to servers with invalid SSL certificates (self-signed or other). |  | **Success Match** | Optionally enter a regular expression here, which is matched against the response body.  If specified, this must match to consider the job a success. |  | **Error Match** | Optionally enter a regular expression here, which is matched against the response body.  If this matches the response body, then the job is aborted with an error. |    ### HTTP Request Chaining    The HTTP Request Plugin supports Cronicle's [Chain Reaction](#chain-reaction) system in two ways.  First, information about the HTTP response is passed into the [Chain Data](#chain-data) object, so downstream chained events can read and act on it.  Specifically, all the HTTP response headers, and possibly even the content body itself (if formatted as JSON and smaller than 1 MB) are included.  Example:    ```js  ""chain_data"": {  	""headers"": {  		""date"": ""Sat, 14 Jul 2018 20:14:01 GMT"",  		""server"": ""Apache/2.4.28 (Unix) LibreSSL/2.2.7 PHP/5.6.30"",  		""last-modified"": ""Sat, 14 Jul 2018 20:13:54 GMT"",  		""etag"": ""\""2b-570fb3c47e480\"""",  		""accept-ranges"": ""bytes"",  		""content-length"": ""43"",  		""connection"": ""close"",  		""content-type"": ""application/json"",  		""x-uuid"": ""7617a494-823f-4566-8f8b-f479c2a6e707""  	},  	""json"": {  		""key1"": ""value1"",  		""key2"": 12345  	}  }  ```    In this example an HTTP request was made that returned those specific response headers (the header names are converted to lower-case), and the body was also formatted as JSON, so the JSON data itself is parsed and included in a property named `json`.  Downstream events that are chain-linked to the HTTP Request event can read these properties and act on them.    Secondly, you can chain an HTTP Request into *another* HTTP Request, and use the chained data values from the previous response in the next request.  To do this, you need to utilize a special `[/bracket/slash]` placeholder syntax in the second request, to lookup values in the `chain_data` object from the first one.  You can use these placeholders in the **URL**, **Request Headers** and **POST Data** text fields.  Example:    ![HTTP Request Chain Data](https://pixlcore.com/software/cronicle/screenshots-new/http-request-chained.png)    Here you can see we are using two placeholders, one in the URL and another in the HTTP request headers.  These are looking up values from a *previous* HTTP Request event, and passing them into the next request.  Specifically, we are using:    | Placeholder | Description |  |-------------|-------------|  | `[/chain_data/json/key1]` | This placeholder is looking up the `key` value from the JSON data (body content) of the previous HTTP response.  Using our example response shown above, this would resolve to `value1`. |  | `[/chain_data/headers/x-uuid]` | This placeholder is looking up the `X-UUID` response header from the previous HTTP response.  Using our example response shown above, this would resolve to `7617a494-823f-4566-8f8b-f479c2a6e707`. |    So once the second request is sent off, after placeholder expansion the URL would actually resolve to:    ```  http://myserver.com/test.json?key=value1  ```    And the headers would expand to:    ```  User-Agent: Mozilla/5.0  X-UUID: 7617a494-823f-4566-8f8b-f479c2a6e707  ```    You can chain as many requests together as you like, but note that each request can only see and act on chain data from the *previous* request (the one that directly chained to it).    # Command Line    Here are all the Cronicle services available to you on the command line.  Most of these are accessed via the following shell script:    ```  /opt/cronicle/bin/control.sh [COMMAND]  ```    Here are all the accepted commands:    | Command | Description |  |---------|-------------|  | `start` | Starts Cronicle in daemon mode. See [Starting and Stopping](#starting-and-stopping). |  | `stop` | Stops the Cronicle daemon and waits for exit. See [Starting and Stopping](#starting-and-stopping). |  | `restart` | Calls `stop`, then `start`, in sequence. See [Starting and Stopping](#starting-and-stopping).  |  | `status` | Checks whether Cronicle is currently running. See [Starting and Stopping](#starting-and-stopping).  |  | `setup` | Runs initial storage setup (for first time install). See [Setup](#setup). |  | `maint` | Runs daily storage maintenance routine. See [Storage Maintenance](#storage-maintenance). |  | `admin` | Creates new emergency admin account (specify user / pass). See [Recover Admin Access](#recover-admin-access). |  | `export` | Exports data to specified file. See [Data Import and Export](#data-import-and-export). |  | `import` | Imports data from specified file. See [Data Import and Export](#data-import-and-export). |  | `upgrade` | Upgrades Cronicle to the latest stable (or specify version). See [Upgrading Cronicle](#upgrading-cronicle). |  | `version` | Outputs the current Cronicle package version and exits. |  | `help` | Displays a list of available commands and exits. |    ## Starting and Stopping    To start the service, use the `start` command:    ```  /opt/cronicle/bin/control.sh start  ```    And to stop it, the `stop` command:    ```  /opt/cronicle/bin/control.sh stop  ```    You can also issue a quick stop + start with the `restart` command:    ```  /opt/cronicle/bin/control.sh restart  ```    The `status` command will tell you if the service is running or not:    ```  /opt/cronicle/bin/control.sh status  ```    ## Environment Variables    Cronicle supports a special environment variable syntax, which can specify command-line options as well as override any configuration settings.  The variable name syntax is `CRONICLE_key` where `key` is one of several command-line options (see table below) or a JSON configuration property path.  These can come in handy for automating installations, and using container systems.      For overriding configuration properties by environment variable, you can specify any top-level JSON key from `config.json`, or a *path* to a nested property using double-underscore (`__`) as a path separator.  For boolean properties, you can specify `1` for true and `0` for false.  Here is an example of some of the possibilities available:    | Variable | Sample Value | Description |  |----------|--------------|-------------|  | `CRONICLE_foreground` | `1` | Run Cronicle in the foreground (no background daemon fork). |  | `CRONICLE_echo` | `1` | Echo the event log to the console (STDOUT), use in conjunction with `CRONICLE_foreground`. |  | `CRONICLE_color` | `1` | Echo the event log with color-coded columns, use in conjunction with `CRONICLE_echo`. |  | `CRONICLE_base_app_url` | `http://cronicle.mycompany.com` | Override the [base_app_url](#base_app_url) configuration property. |  | `CRONICLE_email_from` | `cronicle@mycompany.com` | Override the [email_from](#email_from) configuration property. |  | `CRONICLE_smtp_hostname` | `mail.mycompany.com` | Override the [smtp_hostname](#smtp_hostname) configuration property. |  | `CRONICLE_secret_key` | `CorrectHorseBatteryStaple` | Override the [secret_key](#secret_key) configuration property. |  | `CRONICLE_web_socket_use_hostnames` | `1` | Override the [web_socket_use_hostnames](#web_socket_use_hostnames) configuration property. |  | `CRONICLE_server_comm_use_hostnames` | `1` | Override the [server_comm_use_hostnames](#server_comm_use_hostnames) configuration property. |  | `CRONICLE_WebServer__http_port` | `80` | Override the `http_port` property *inside* the [WebServer](#web-server-configuration) object. |  | `CRONICLE_WebServer__https_port` | `443` | Override the `https_port` property *inside* the [WebServer](#web-server-configuration) object. |  | `CRONICLE_Storage__Filesystem__base_dir` | `/data/cronicle` | Override the `base_dir` property *inside* the [Filesystem](#filesystem) object *inside* the [Storage](#storage-configuration) object. |    Almost every [configuration property](#configuration) can be overridden using this environment variable syntax.  The only exceptions are things like arrays, e.g. [log_columns](#log_columns) and [socket_io_transports](#socket_io_transports).    ## Storage Maintenance    Storage maintenance automatically runs every morning at 4 AM local server time (this is [configurable](#maintenance) if you want to change it).  The operation is mainly for deleting expired records, and pruning lists that have grown too large.  However, if the Cronicle service was stopped and you missed a day or two, you can force it to run at any time.  Just execute this command on your primary server:    ```  /opt/cronicle/bin/control.sh maint  ```    This will run maintenance for the current day.  However, if the service was down for more than one day, please run the command for each missed day, providing the date in `YYYY-MM-DD` format:    ```  /opt/cronicle/bin/control.sh maint 2015-10-29  /opt/cronicle/bin/control.sh maint 2015-10-30  ```    ## Recover Admin Access    Lost access to your admin account?  You can create a new temporary administrator account on the command-line.  Just execute this command on your primary server:    ```  /opt/cronicle/bin/control.sh admin USERNAME PASSWORD  ```    Replace `USERNAME` with the desired username, and `PASSWORD` with the desired password for the new account.  Note that the new user will not show up in the main list of users in the UI.  But you will be able to login using the provided credentials.  This is more of an emergency operation, just to allow you to get back into the system.  *This is not a good way to create permanent users*.  Once you are logged back in, you should consider creating another account from the UI, then deleting the emergency admin account.    ## Server Startup    Here are the instructions for making Cronicle automatically start on server boot (Linux only).  Type these commands as root:    ```  cp /opt/cronicle/bin/cronicled.init /etc/init.d/cronicled  chmod 775 /etc/init.d/cronicled  ```    Then, if you have a RedHat-style Linux (i.e. Fedora, CentOS), type this:    ```  chkconfig cronicled on  ```    Or, if you have Debian-style Linux (i.e. Ubuntu), type this:    ```  update-rc.d cronicled defaults  ```    For multi-server clusters, you'll need to repeat these steps on each server.    **Important Note:** When Cronicle starts on server boot, it typically does not have a proper user environment, namely a `PATH` environment variable.  So if your scripts rely on binary executables in alternate locations, e.g. `/usr/local/bin`, you may have to restore the `PATH` and other variables inside your scripts by redeclaring them.    ## Upgrading Cronicle    To upgrade Cronicle, you can use the built-in `upgrade` command:    ```  /opt/cronicle/bin/control.sh upgrade  ```    This will upgrade the app and all dependencies to the latest stable release, if a new one is available.  It will not affect your data storage, users, or configuration settings.  All those will be preserved and imported to the new version.  For multi-server clusters, you'll need to repeat this command on each server.    Alternately, you can specify the exact version you want to upgrade (or downgrade) to:    ```  /opt/cronicle/bin/control.sh upgrade 1.0.4  ```    If you upgrade to the `HEAD` version, this will grab the very latest from GitHub.  Note that this is primarily for developers or beta-testers, and is likely going to contain bugs.  Use at your own risk:    ```  /opt/cronicle/bin/control.sh upgrade HEAD  ```    ## Data Import and Export    Cronicle can import and export data via the command-line, to/from a plain text file.  This data includes all the ""vital"" storage records such as Users, Plugins, Categories, Servers, Server Groups, API Keys and all Scheduled Events.  It *excludes* things like user sessions, job completions and job logs.    To export your Cronicle data, issue this command on your primary server:    ```  /opt/cronicle/bin/control.sh export /path/to/cronicle-data-backup.txt --verbose  ```    The `--verbose` flag makes it emit some extra information to the console.  Omit that if you want it to run silently.  Omit the filename if you want it to export the data to STDOUT instead of a file.    To import data back into the system, **first make sure Cronicle is stopped on all servers**, and then run this command:    ```  /opt/cronicle/bin/control.sh import /path/to/cronicle-data-backup.txt  ```    If you want daily backups of the data which auto-expire after a year, a simple shell script can do it for ya:    ```sh  #!/bin/bash  DATE_STAMP=`date ""+%Y-%m-%d""`  BACKUP_DIR=""/backup/cronicle/data""  BACKUP_FILE=""$BACKUP_DIR/backup-$DATE_STAMP.txt""    mkdir -p $BACKUP_DIR  /opt/cronicle/bin/control.sh export $BACKUP_FILE --verbose  find $BACKUP_DIR -mtime +365 -type f -exec rm -v {} \;  ```    ## Storage Migration Tool    If you need to migrate your Cronicle storage data to a new location or even a new engine, a simple built-in migration tool is provided.  This tool reads *all* Cronicle storage records and writes them back out, using two different storage configurations (old and new).    To use the tool, first edit your Cronicle's `conf/config.json` file on your primary server, and locate the `Storage` object.  This should point to your *current* storage configuration, i.e. where we are migrating *from*.  Then, add a new object right next to it, and name it `NewStorage`.  This should point to your *new* storage location and/or storage engine, i.e. where we are migrating *to*.    The contents of the `NewStorage` object should match whatever you'd typically put into `Storage`, if setting up a new install.  See the [Storage Configuration](#storage-configuration) section for details.  It can point to any of the supported engines.  Here is an example that would migrate from the local filesystem to Amazon S3:    ```js  {  	""Storage"": {  		""engine"": ""Filesystem"",  		""Filesystem"": {  			""base_dir"": ""data"",  			""key_namespaces"": 1  		}  	},  	  	""NewStorage"": {  		""engine"": ""S3"",  		""AWS"": {  			""accessKeyId"": ""YOUR_AMAZON_ACCESS_KEY"",   			""secretAccessKey"": ""YOUR_AMAZON_SECRET_KEY"",   			""region"": ""us-west-1"",  			""correctClockSkew"": true,  			""maxRetries"": 5,  			""httpOptions"": {  				""connectTimeout"": 5000,  				""timeout"": 5000  			}  		},  		""S3"": {  			""keyPrefix"": ""cronicle"",  			""fileExtensions"": true,  			""params"": {  				""Bucket"": ""YOUR_S3_BUCKET_ID""  			}  		}  	}  }  ```    You could also use this to migrate between two AWS regions, S3 buckets or key prefixes on S3.  Just point `Storage` and `NewStorage` to the same engine, e.g. `S3`, and change only the region, bucket or prefix in the `NewStorage` object.    When you are ready to proceed, make sure you **shut down Cronicle** on all your servers.  You should not migrate storage while Cronicle is running, as it can result in corrupted data.    All good?  Okay then, on your Cronicle primary server as root (superuser), issue this command:    ```  /opt/cronicle/bin/control.sh migrate  ```    The following command-line arguments are supported:    | Argument | Description |  |----------|-------------|  | `--debug` | Echo all debug log messages to the console.  This also disables the progress bar. |  | `--verbose` | Print the key of each record as it is migrated.  This also disables the progress bar. |  | `--dryrun` | Do not write any changes to new storage (except for a single test record, which is then deleted).  Used for debugging and troubleshooting. |    It is recommended that you first run the migrate command with `--dryrun` to make sure that it can read and write to the two storage locations.  The script also logs all debug messages and transactions to `logs/StorageMigration.log`.    Once the migration is complete and you have verified that your data is where you expect, edit the `conf/config.json` file one last time, and remove the old `Storage` object, and then rename `NewStorage` to `Storage`, effectively replacing it.  Cronicle will now access your storage data from the new location.  Make a backup of the file in case you ever need to roll back.    If you have multiple Cronicle servers, make sure you sync your `conf/config.json` between all servers!  They all need to be identical.    Finally, restart Cronicle, and all should be well.    # Inner Workings    This section contains details on some of the inner workings of Cronicle.    ## Cron Noncompliance    Cronicle has a custom-built scheduling system that is *loosely* based on Unix [Cron](https://en.wikipedia.org/wiki/Cron).  It does not, however, conform to the [specification](https://linux.die.net/man/5/crontab) written by [Paul Vixie](https://en.wikipedia.org/wiki/Paul_Vixie).  Namely, it differs in the following ways:    - Month days and weekdays are intersected when both are present  	- If you specify both month days and weekdays, *both must match* for Cronicle to fire an event.  Vixie Cron behaves differently, in that it will fire if *either* matches.  This was a deliberate design decision to enable more flexibility in scheduling.    When importing Crontab syntax:    - Cronicle does not support the concept of running jobs on reboot, so the `@reboot` macro is disallowed.  - If a 6th column is specified, it is assumed to be years.  - Weekdays `0` and `7` are both considered to be Sunday.    For more details on Cronicle's scheduler implementation, see the [Event Timing Object](#event-timing-object).    ## Storage    The storage system in Cronicle is built on the [pixl-server-storage](https://www.npmjs.com/package/pixl-server-storage) module, which is basically a key/value store.  It can write everything to local disk (the default), [Couchbase](http://www.couchbase.com/nosql-databases/couchbase-server) or [Amazon S3](https://aws.amazon.com/s3/).    Writing to local disk should work just fine for most installations, even with multi-server setups, as long as there is only one primary server.  However, if you want redundant backups with auto-failover, and/or redundant data storage, then you either have to use a shared filesystem such as NFS, or switch to Couchbase or S3.    With an NFS shared filesystem, your primary and backup servers can have access to the same Cronicle data storage.  Only the primary server performs data writes, so there should never be any data corruption.  The easiest way to set up a shared filesystem is to configure Cronicle to point at your NFS mount in the `conf/config.json` file, then run the setup script.  The filesystem storage location is in the `base_dir` property, which is found in the `Storage` / `Filesystem` objects:    ```js  {  	""Storage"": {  		""engine"": ""Filesystem"",  		""list_page_size"": 50,  		""concurrency"": 4,  		  		""Filesystem"": {  			""base_dir"": ""/PATH/TO/YOUR/NFS/MOUNT"",  			""key_namespaces"": 1  		}  	}  }  ```    Then run the setup script as instructed in the [Setup](#setup) section.  Make sure all your backup servers have the NFS filesystem mounted in the same location, and then copy the `conf/config.json` file to all the servers.  **Do not run the setup script more than once.**    Setting up Couchbase or S3 is handled in much the same way.  Edit the `conf/config.json` file to point to the service of your choice, then run the setup script to create the initial storage records.  See the [Couchbase](#couchbase) or [Amazon S3](#amazon-s3) configuration sections for more details.    ## Logs    Cronicle writes its logs in a plain text, square-bracket delimited column format, which looks like this:    ```  [1450993152.554][2015/12/24 13:39:12][joeretina.local][Cronicle][debug][3][Cronicle starting up][]  [1450993152.565][2015/12/24 13:39:12][joeretina.local][Cronicle][debug][4][Server is eligible to become primary (Main Group)][]  [1450993152.566][2015/12/24 13:39:12][joeretina.local][Cronicle][debug][3][We are becoming the primary server][]  [1450993152.576][2015/12/24 13:39:12][joeretina.local][Cronicle][debug][2][Startup complete, entering main loop][]  ```    The log columns are defined as follows, from left to right:    | Log Column | Description |  |------------|-------------|  | `hires_epoch` | A date/time stamp in high-resolution [Epoch time](https://en.wikipedia.org/wiki/Unix_time). |  | `date` | A human-readable date/time stamp in the format: `YYYY/MM/DD HH:MI:SS` (local server time) |  | `hostname` | The hostname of the server that wrote the log entry (useful for multi-server setups if you merge your logs together). |  | `component` | The component name which generated the log entry.  See below for a list of all the components. |  | `category` | The category of the log entry, which will be one of `debug`, `transaction` or `error`. |  | `code` | Debug level (1 to 10), transaction or error code. |  | `msg` | Debug, transaction or error message text. |  | `data` | Additional JSON data, may or may not present. |    The columns are configurable via the [log_columns](#log_columns) property in the `conf/config.json` file:    ```js  {  	""log_columns"": [""hires_epoch"", ""date"", ""hostname"", ""component"", ""category"", ""code"", ""msg"", ""data""]  }  ```    Feel free to reorder or remove columns, but don't rename any.  The IDs are special, and match up to keywords in the source code.    By default, logging consists of several different files, each for a specific component of the system.  After starting up Cronicle, you will find these log files in the [log_dir](#log_dir) directory:    | Log Filename | Description |  |--------------|-------------|  | `Cronicle.log` | The main component will contain most of the app logic (scheduler, jobs, startup, shutdown, etc.). |  | `Error.log` | The error log will contain all errors, including job failures, server disconnects, etc. |  | `Transaction.log` | The transaction log will contain all transactions, including API actions, job completions, etc. |  | `API.log` | The API component log will contain information about incoming HTTP API calls. |  | `Storage.log` | The storage component log will contain information about data reads and writes. |  | `Filesystem.log` | Only applicable if you use the local filesystem storage back-end. |  | `Couchbase.log` | Only applicable if you use the [Couchbase](#couchbase) storage back-end. |  | `S3.log` | Only applicable if you use the [Amazon S3](#amazon-s3) storage back-end. |  | `User.log` | The user component log will contain user related information such as logins and logouts. |  | `WebServer.log` | The web server component log will contain information about HTTP requests and connections. |  | `crash.log` | If Cronicle crashed for any reason, you should find a date/time and stack trace in this log. |  | `install.log` | Contains detailed installation notes from npm, and the build script. |    The [log_filename](#log_filename) configuration property controls this, and by default it is set to the following:    ```js  {  	""log_filename"": ""[component].log"",  }  ```    This causes the value of the `component` column to dictate the actual log filename.  If you would prefer that everything be logged to a single combo file instead, just change this to a normal string without brackets, such as:    ```js  {  	""log_filename"": ""event.log"",  }  ```    ## Keeping Time    Cronicle manages job scheduling for its events by using a ""cursor"" system, as opposed to a classic queue.  Each event has its own internal cursor (pointer) which contains a timestamp.  This data is stored in RAM but is also persisted to disk for failover.  When the clock advances a minute, the scheduler iterates over all the active events, and moves their cursors forward, always trying to keep them current.  It launches any necessary jobs along the way.    For events that have [Run All Mode](#run-all-mode) set, a cursor may pause or even move backwards, depending on circumstances.  If a job fails to launch, the cursor stays back in the previous minute so it can try again.  In this way jobs virtually ""queue up"" as time advances.  When the blocking issue is resolved (resource constraint or other), the event cursor will be moved forward as quickly as resources and settings allow, so it can ""catch up"" to current time.    Also, you have the option of manually resetting an event's cursor using the [Time Machine](#event-time-machine) feature.  This way you can manually have it re-run past jobs, or hop over a ""queue"" that has built up.    ## Primary Server Failover    In a [Multi-Server Cluster](#multi-server-cluster), you can designate a number of servers as primary backups, using the [Server Groups](#server-groups) feature.  These backup servers will automatically take over as primary if something happens to the current primary server (shutdown or crash).  Your servers will automatically negotiate who should become primary, both at startup and at failover, based on an alphabetical sort of their hostnames.  Servers which sort higher will become primary before servers that sort lower.    Upon startup there is a ~60 second delay before a primary server is chosen.  This allows time for all the servers in the cluster to auto-discover each other.    ### Unclean Shutdown    Cronicle is designed to handle server failures.  If a worker server goes down for any reason, the cluster will automatically adjust.  Any active jobs on the dead server will be failed and possibly retried after a short period of time (see [dead_job_timeout](#dead_job_timeout)), and new jobs will be reassigned to other servers as needed.    If a primary server goes down, one of the backups will take over within 60 seconds (see [master_ping_timeout](#master_ping_timeout)).  The same rules apply for any jobs that were running on the primary server.  They'll be failed and retried as needed by the new primary server, with one exception: unclean shutdown.    When a primary server experiences a catastrophic failure such as a daemon crash, kernel panic or power loss, it has no time to do anything, so any active jobs on the server are instantly dead.  The jobs will eventually be logged as failures, and the logs recovered when the server comes back online.  However, if the events had [Run All Mode](#run-all-mode) enabled, they won't be auto-retried when the new primary takes over, because it has no way of knowing a job was even running.  And by the time the old primary server is brought back online, days or weeks may have passed, so it would be wrong to blindly rewind the event clock to before the event ran.    So in summary, the only time human intervention may be required is if a primary server dies unexpectedly due to an unclean shutdown, and it had active jobs running on it, and those jobs had [Run All Mode](#run-all-mode) set.  In that case, you may want to use the [Time Machine](#event-time-machine) feature to reset the event clock, to re-run any missed jobs.    # API Reference    ## JSON REST API    All API calls expect JSON as input (unless they are simple HTTP GETs), and will return JSON as output.  The main API endpoint is:    ```  /api/app/NAME/v1  ```    Replace `NAME` with the specific API function you are calling (see below for list).  All requests should be HTTP GET or HTTP POST as the API dictates, and should be directed at the Cronicle primary server on the correct TCP port (the default is `3012` but is often reconfigured to be `80`).  Example URL:    ```  http://myserver.com:3012/api/app/get_schedule/v1  ```    For web browser access, [JSONP](https://en.wikipedia.org/wiki/JSONP) response style is supported for all API calls, by including a `callback` query parameter.  However, all responses include a `Access-Control-Allow-Origin: *` header, so cross-domain [XHR](https://en.wikipedia.org/wiki/XMLHttpRequest) requests will work as well.    ### Redirects    If you are running a multi-server Cronicle cluster with multiple primary backup servers behind a load balancer, you may receive a `HTTP 302` response if you hit a non-primary server for an API request.  In this case, the `Location` response header will contain the proper primary server hostname.  Please repeat your request pointed at the correct server.  Most HTTP request libraries have an option to automatically follow redirects, so you can make this process automatic.    It is recommended (although not required) that you cache the primary server hostname if you receive a 302 redirect response, so you can make subsequent calls to the primary server directly, without requiring a round trip.      ## API Keys    API Keys allow you to register external applications or services to use the REST API.  These can be thought of as special user accounts specifically for applications.  API calls include running jobs on-demand, monitoring job status, and managing the schedule (creating, editing and/or deleting events).  Each API key can be granted a specific set of privileges.    To create an API Key, you must first be an administrator level user.  Login to the Cronicle UI, proceed to the [API Keys Tab](#api-keys-tab), and click the ""Add API Key..."" button.  Fill out the form and click the ""Create Key"" button at the bottom of the page.    API Keys are randomly generated hexadecimal strings, and are 32 characters in length.  Example:    ```  0095f5b664b93304d5f8b1a61df605fb  ```    You must include a valid API Key with every API request.  There are three ways to do this: include a `X-API-Key` HTTP request header, an `api_key` query string parameter, or an `api_key` JSON property.    Here is a raw HTTP request showing all three methods of passing the API Key (only one of these is required):    ```  GET /api/app/get_schedule/v1?api_key=0095f5b664b93304d5f8b1a61df605fb HTTP/1.1  Host: myserver.com  X-API-Key: 0095f5b664b93304d5f8b1a61df605fb  Content-Type: application/json    {""offset"": 0, ""limit"": 50, ""api_key"": ""0095f5b664b93304d5f8b1a61df605fb""}  ```    ## Standard Response Format    Regardless of the specific API call you requested, all responses will be in JSON format, and include at the very least a `code` property.  This will be set to `0` upon success, or any other value if an error occurred.  In the event of an error, a `description` property will also be included, containing the error message itself.  Individual API calls may include additional properties, but these two are standard fare in all cases.  Example successful response:    ```js  { ""code"": 0 }  ```    Example error response:    ```js  {""code"": ""session"", ""description"": ""No Session ID or API Key could be found""}  ```    ## API Calls    Here is the list of supported API calls:    ### get_schedule    ```  /api/app/get_schedule/v1  ```    This fetches scheduled events and returns details about them.  It supports pagination to fetch chunks, with the default being the first 50 events.  Both HTTP GET (query string) or HTTP POST (JSON data) are acceptable.  Parameters:    | Parameter Name | Description |  |----------------|-------------|  | `offset` | (Optional) The offset into the data to start returning records, defaults to 0. |  | `limit` | (Optional) The number of records to return, defaults to 50. |    Example request:    ```js  {  	""offset"": 0,  	""limit"": 1000  }  ```    Example response:    ```js  {  	""code"": 0,  	""rows"": [  		{  			""enabled"": 1,  			""params"": {  				""script"": ""#!/bin/sh\n\n/usr/local/bin/db-reindex.pl\n""  			},  			""timing"": {  				""minutes"": [ 10 ]  			},  			""max_children"": 1,  			""timeout"": 3600,  			""catch_up"": false,  			""plugin"": ""shellplug"",  			""title"": ""Rebuild Indexes"",  			""category"": ""general"",  			""target"": ""c33ff006"",  			""multiplex"": 0,  			""retries"": 0,  			""detached"": 0,  			""notify_success"": """",  			""notify_fail"": """",  			""web_hook"": """",  			""notes"": """",  			""id"": ""29bf12db"",  			""modified"": 1445233242,  			""created"": 1445233022,  			""username"": ""admin"",  			""timezone"": ""America/Los_Angeles""  		}  	],  	""list"": {  		""page_size"": 50,  		""first_page"": 0,  		""last_page"": 0,  		""length"": 12,  		""type"": ""list""  	}  }  ```    In addition to the [Standard Response Format](#standard-response-format), this API will include the following:    The `rows` array will contain an element for every matched event in the requested set.  It will contain up to `limit` elements.  See the [Event Data Format](#event-data-format) section below for details on the event object properties themselves.    The `list` object contains internal metadata about the list structure in storage.  You can probably ignore this, except perhaps the `list.length` property, which will contain the total number of events in the schedule, regardless if your `offset` and `limit` parameters.  This can be useful for building pagination systems.    ### get_event    ```  /api/app/get_event/v1  ```    This fetches details about a single event, given its ID or exact title.  Both HTTP GET (query string) or HTTP POST (JSON data) are acceptable.  Parameters:    | Parameter Name | Description |  |----------------|-------------|  | `id` | The ID of the event you wish to fetch details on. |  | `title` | The exact title of the event you wish to fetch details on (case-sensitive). |    Example request:    ```js  {  	""id"": ""540cf457""  }  ```    Example response:    ```js  {  	""code"": 0,  	""event"": {  		""enabled"": 0,  		""params"": {  			""script"": ""#!/bin/sh\n\n/usr/local/bin/s3-backup-logs.pl\n""  		},  		""timing"": {  			""minutes"": [ 5 ]  		},  		""max_children"": 1,  		""timeout"": 3600,  		""catch_up"": false,  		""plugin"": ""shellplug"",  		""title"": ""Backup Logs to S3"",  		""category"": ""ad8190ff"",  		""target"": ""all"",  		""multiplex"": 0,  		""retries"": 0,  		""detached"": 0,  		""notify_success"": """",  		""notify_fail"": """",  		""web_hook"": """",  		""notes"": """",  		""id"": ""540cf457"",  		""modified"": 1449941100,  		""created"": 1445232960,  		""username"": ""admin"",  		""retry_delay"": 0,  		""cpu_limit"": 0,  		""cpu_sustain"": 0,  		""memory_limit"": 0,  		""memory_sustain"": 0,  		""log_max_size"": 0,  		""timezone"": ""America/Los_Angeles""  	}  }  ```    In addition to the [Standard Response Format](#standard-response-format), this API will include the following:    The `event` object will contain the details for the requested event.  See the [Event Data Format](#event-data-format) section below for details on the event object properties themselves.    If [Allow Queued Jobs](#allow-queued-jobs) is enabled on the event, the API response will also include a `queue` property, which will be set to the number of jobs currently queued up.    If there are any active jobs currently running for the event, they will also be included in the response, in a `jobs` array.  Each job object will contain detailed information about the running job.  See [get_job_status](#get_job_status) below for more details.    ### create_event    ```  /api/app/create_event/v1  ```    This creates a new event and adds it to the schedule.  API Keys require the `create_events` privilege to use this API.  Only HTTP POST (JSON data) is acceptable.  The required parameters are as follows:    | Parameter Name | Description |  |----------------|-------------|  | `title` | **(Required)** A display name for the event, shown on the [Schedule Tab](#schedule-tab) as well as in reports and e-mails. |  | `enabled` | **(Required)** Specifies whether the event is enabled (active in the scheduler) or not.  Should be set to 1 or 0. |  | `category` | **(Required)** The Category ID to which the event will be assigned.  See [Categories Tab](#categories-tab). |  | `plugin` | **(Required)** The ID of the Plugin which will run jobs for the event. See [Plugins Tab](#plugins-tab). |  | `target` | **(Required)** Events can target a [Server Group](#server-groups) (Group ID), or an individual server (hostname). |    In addition to the required parameters, almost anything in the [Event Data Object](#event-data-format) can also be included here.  Example request:    ```js  {  	""catch_up"": 1,  	""category"": ""43f8c57e"",  	""cpu_limit"": 100,  	""cpu_sustain"": 0,  	""detached"": 0,  	""enabled"": 1,  	""log_max_size"": 0,  	""max_children"": 1,  	""memory_limit"": 0,  	""memory_sustain"": 0,  	""modified"": 1451185588,  	""multiplex"": 0,  	""notes"": ""This event handles database maintenance."",  	""notify_fail"": """",  	""notify_success"": """",  	""params"": {  		""db_host"": ""idb01.mycompany.com"",  		""verbose"": 1,  		""cust"": ""Sales""  	},  	""plugin"": ""test"",  	""retries"": 0,  	""retry_delay"": 30,  	""target"": ""db1.int.myserver.com"",  	""timeout"": 3600,  	""timezone"": ""America/New_York"",  	""timing"": {  		""hours"": [ 21 ],  		""minutes"": [ 20, 40 ]  	},  	""title"": ""DB Reindex"",  	""web_hook"": ""http://myserver.com/notify-chronos.php""  }  ```    Example response:    ```js  {  	""code"": 0,  	""id"": ""540cf457""  }  ```    In addition to the [Standard Response Format](#standard-response-format), the ID of the new event will be returned in the `id` property.    ### update_event    ```  /api/app/update_event/v1  ```    This updates an existing event given its ID, replacing any properties you specify.  API Keys require the `edit_events` privilege to use this API.  Only HTTP POST (JSON data) is acceptable.  The parameters are as follows:    | Parameter Name | Description |  |----------------|-------------|  | `id` | **(Required)** The ID of the event you wish to update. |  | `reset_cursor` | (Optional) Reset the event clock to the given Epoch timestamp (see [Event Time Machine](#event-time-machine)). |  | `abort_jobs` | (Optional) If you are disabling the event by setting `enabled` to 0, you may also abort any running jobs if you want. |    Include anything from the [Event Data Object](#event-data-format) to update (i.e. replace) the values.  Anything omitted is preserved.  Example request:    ```js  {  	""id"": ""3c182051"",  	""enabled"": 0,  }  ```    Example request with everything updated:    ```js  {  	""id"": ""3c182051"",  	""reset_cursor"": 1451185588,  	""abort_jobs"": 0,  	""catch_up"": 1,  	""category"": ""43f8c57e"",  	""cpu_limit"": 100,  	""cpu_sustain"": 0,  	""detached"": 0,  	""enabled"": 1,  	""log_max_size"": 0,  	""max_children"": 1,  	""memory_limit"": 0,  	""memory_sustain"": 0,  	""multiplex"": 0,  	""notes"": ""This event handles database maintenance."",  	""notify_fail"": """",  	""notify_success"": """",  	""params"": {  		""db_host"": ""idb01.mycompany.com"",  		""verbose"": 1,  		""cust"": ""Marketing""  	},  	""plugin"": ""test"",  	""retries"": 0,  	""retry_delay"": 30,  	""target"": ""db1.int.myserver.com"",  	""timeout"": 3600,  	""timezone"": ""America/New_York"",  	""timing"": {  		""hours"": [ 21 ],  		""minutes"": [ 20, 40 ]  	},  	""title"": ""DB Reindex"",  	""username"": ""admin"",  	""web_hook"": ""http://myserver.com/notify-chronos.php""  }  ```    Example response:    ```js  {  	""code"": 0  }  ```    See the [Standard Response Format](#standard-response-format) for details.    ### delete_event    ```  /api/app/delete_event/v1  ```    This deletes an existing event given its ID.  Note that the event must not have any active jobs still running (or else an error will be returned).  API Keys require the `delete_events` privilege to use this API.  Only HTTP POST (JSON data) is acceptable.  The parameters are as follows:    | Parameter Name | Description |  |----------------|-------------|  | `id` | **(Required)** The ID of the event you wish to delete. |    Example request:    ```js  {  	""id"": ""3c182051""  }  ```    Example response:    ```js  {  	""code"": 0  }  ```    See the [Standard Response Format](#standard-response-format) for details.    ### run_event    ```  /api/app/run_event/v1  ```    This immediately starts an on-demand job for an event, regardless of the schedule.  This is effectively the same as a user clicking the ""Run Now"" button in the UI.  API Keys require the `run_events` privilege to use this API.  Both HTTP GET (query string) or HTTP POST (JSON data) are acceptable.  You can specify the target event by its ID or exact title:    | Parameter Name | Description |  |----------------|-------------|  | `id` | The ID of the event you wish to run a job for. |  | `title` | The exact title of the event you wish to run a job for (case-sensitive). |    You can also include almost anything from the [Event Data Object](#event-data-format) to customize the settings for the job.  Anything omitted is pulled from the event object.  Example request:    ```js  {  	""id"": ""3c182051""  }  ```    Example request with everything customized:    ```js  {  	""id"": ""3c182051"",  	""category"": ""43f8c57e"",  	""cpu_limit"": 100,  	""cpu_sustain"": 0,  	""detached"": 0,  	""log_max_size"": 0,  	""max_children"": 1,  	""memory_limit"": 0,  	""memory_sustain"": 0,  	""multiplex"": 0,  	""notify_fail"": """",  	""notify_success"": """",  	""params"": {  		""db_host"": ""idb01.mycompany.com"",  		""verbose"": 1,  		""cust"": ""Marketing""  	},  	""plugin"": ""test"",  	""retries"": 0,  	""retry_delay"": 30,  	""target"": ""db1.internal.myserver.com"",  	""timeout"": 3600,  	""title"": ""DB Reindex"",  	""username"": ""admin"",  	""web_hook"": ""http://myserver.com/notify-chronos.php""  }  ```    Note that the `params` object can be omitted entirely, or sparsely populated, and any missing properties that are defined in the event are automatically merged in.  This allows your API client to only specify the `params` it needs to (including arbitrary new ones).    Example response:    ```js  {  	""code"": 0,  	""ids"": [""23f5c37f"", ""f8ac3082""]  }  ```    In addition to the [Standard Response Format](#standard-response-format), the IDs of all the launched jobs will be returned in the `ids` array.  Typically only a single job is launched, but it may be multiple if the event has [Multiplexing](#multiplexing) enabled and targets a group with multiple servers.    If [Allow Queued Jobs](#allow-queued-jobs) is enabled on the event, the API response will also include a `queue` property, which will be set to the number of jobs currently queued up.    **Advanced Tip:** If you do not wish to merge the POST data into the `params` (for example if you are using a webhook that provides other data as JSON), then you can add `&post_data=1` to the query string. If you do this, then the POST data will be available in the `post_data` key of the `params` object.    ### get_job_status    ```  /api/app/get_job_status/v1  ```    This fetches status for a job currently in progress, or one already completed.  Both HTTP GET (query string) or HTTP POST (JSON data) are acceptable.  Parameters:    | Parameter Name | Description |  |----------------|-------------|  | `id` | **(Required)** The ID of the job you wish to fetch status on. |    Example request:    ```js  {  	""id"": ""jiinxhh5203""  }  ```    Example response:    ```js  {  	""code"": 0,  	""job"": {  		""params"": {  			""db_host"": ""idb01.mycompany.com"",  			""verbose"": 1,  			""cust"": ""Marketing""  		},  		""timeout"": 3600,  		""catch_up"": 1,  		""plugin"": ""test"",  		""category"": ""43f8c57e"",  		""retries"": 0,  		""detached"": 0,  		""notify_success"": ""jhuckaby@test.com"",  		""notify_fail"": ""jhuckaby@test.com"",  		""web_hook"": ""http://myserver.com/notify-chronos.php"",  		""notes"": ""Joe testing."",  		""multiplex"": 0,  		""memory_limit"": 0,  		""memory_sustain"": 0,  		""cpu_limit"": 0,  		""cpu_sustain"": 0,  		""log_max_size"": 0,  		""retry_delay"": 30,  		""timezone"": ""America/New_York"",  		""source"": ""Manual (admin)"",  		""id"": ""jiiqjexr701"",  		""time_start"": 1451341765.987,  		""hostname"": ""joeretina.local"",  		""command"": ""bin/test-plugin.js"",  		""event"": ""3c182051"",  		""now"": 1451341765,  		""event_title"": ""Test Event 2"",  		""plugin_title"": ""Test Plugin"",  		""category_title"": ""Test Cat"",  		""nice_target"": ""joeretina.local"",  		""log_file"": ""/opt/cronicle/logs/jobs/jiiqjexr701.log"",  		""pid"": 11743,  		""progress"": 1,  		""cpu"": {  			""min"": 19,  			""max"": 19,  			""total"": 19,  			""count"": 1,  			""current"": 19  		},  		""mem"": {  			""min"": 214564864,  			""max"": 214564864,  			""total"": 214564864,  			""count"": 1,  			""current"": 214564864  		},  		""complete"": 1,  		""code"": 0,  		""description"": ""Success!"",  		""perf"": ""scale=1&total=90.319&db_query=3.065&db_connect=5.096&log_read=7.425&gzip_data=11.094&http_post=17.72"",  		""log_file_size"": 25110,  		""time_end"": 1451341856.61,  		""elapsed"": 90.62299990653992  	}  }  ```    In addition to the [Standard Response Format](#standard-response-format), the job details can be found in the `job` object.    In the `job` object you'll find all the standard [Event Data Object](#event-data-format) properties, as well as the following properties unique to this API:    | Property Name | Description |  |---------------|-------------|  | `hostname` | The hostname of the server currently running, or the server who ran the job. |  | `source` | If the job was started manually via user or API, this will contain a text string identifying who it was. |  | `log_file` | A local filesystem path to the job's log file (only applicable if job is in progress). |  | `pid` | The main PID of the job process that was spawned. |  | `progress` | Current progress of the job, from `0.0` to `1.0`, as reported by the Plugin (optional). |  | `complete` | Will be set to `1` when the job is complete, omitted if still in progress. |  | `code` | A code representing job success (`0`) or failure (any other value).  Only applicable for completed jobs. |  | `description` | If the job failed, this will contain the error message.  Only applicable for completed jobs. |  | `perf` | [Performance metrics](#performance-metrics) for the job, if reported by the Plugin (optional). Only applicable for completed jobs. |  | `time_start` | A Unix Epoch timestamp of when the job started. |  | `time_end` | A Unix Epoch timestamp of when the job completed. Only applicable for completed jobs. |  | `elapsed` | The elapsed time of the job, in seconds. |  | `cpu` | An object representing the CPU use of the job.  See below. |  | `mem` | An object representing the memory use of the job.  See below. |    Throughout the course of a job, its process CPU and memory usage are measured periodically, and tracked in these objects:    ```js  {  	""cpu"": {  		""min"": 19,  		""max"": 19,  		""total"": 19,  		""count"": 1,  		""current"": 19  	},  	""mem"": {  		""min"": 214564864,  		""max"": 214564864,  		""total"": 214564864,  		""count"": 1,  		""current"": 214564864  	}  }  ```    The CPU is measured as percentage of one CPU core, so 100 means that a full CPU core is in use.  It may also go above 100, if multiple threads or sub-processes are in use.  The current value can be found in `current`, and the minimum (`min`) and maximum (`max`) readings are also tracked.  To compute the average, divide the `total` value by the `count`.    The memory usage is measured in bytes.  The current value can be found in `current`, and the minimum (`min`) and maximum (`max`) readings are also tracked.  To compute the average, divide the `total` value by the `count`.    ### get_active_jobs    ```  /api/app/get_active_jobs/v1  ```    This fetches status for **all active** jobs, and returns them all at once.  It takes no parameters (except an [API Key](#api-keys) of course).  The response format is as follows:    ```js  {  	""code"": 0,  	""jobs"": {  		""jk6lmar4c01"": {  			...  		},  		""jk6lmar4d04"": {  			...  		}  	}  }  ```    In addition to the [Standard Response Format](#standard-response-format), the response object will contain a `jobs` object.  This object will have zero or more nested objects, each representing one active job.  The inner property names are the Job IDs, and the contents are the status, progress, and other information about the active job.  For details on the job objects, see the [get_job_status](#get_job_status) API call above, as the parameters of each job will be the same as that API.    ### update_job    ```  /api/app/update_job/v1  ```    This updates a job that is already in progress.  Only certain job properties may be changed when the job is running, and those are listed below.  This is typically used to adjust timeouts, resource limits, or user notification settings.  API Keys require the `edit_events` privilege to use this API.  Only HTTP POST (JSON data) is acceptable.  The parameters are as follows:    | Parameter Name | Description |  |----------------|-------------|  | `id` | **(Required)** The ID of the job you wish to update. |  | `timeout` | (Optional) The total run time in seconds to allow, before the job is aborted. |  | `retries` | (Optional) The number of retries before the job is reported a failure. |  | `retry_delay` | (Optional) The number of seconds between retries. |  | `chain` | (Optional) Launch another event when the job completes successfully (see [Chain Reaction](#chain-reaction)). |  | `chain_error` | (Optional) Launch another event when the job fails (see [Chain Reaction](#chain-reaction)). |  | `notify_success` | (Optional) A comma-separated list of e-mail addresses to notify on job success. |  | `notify_fail` | (Optional) A comma-separated list of e-mail addresses to notify on job failure. |  | `web_hook` | (Optional) A fully-qualified URL to ping when the job completes. |  | `cpu_limit` | (Optional) The maximum allowed CPU before the job is aborted (100 = 1 CPU core). |  | `cpu_sustain` | (Optional) The number of seconds to allow the max CPU to be exceeded. |  | `memory_limit` | (Optional) The maximum allowed memory usage (in bytes) before the job is aborted. |  | `memory_sustain` | (Optional) The number of seconds to allow the max memory to be exceeded. |  | `log_max_size` | (Optional) The maximum allowed job log file size (in bytes) before the job is aborted. |    As shown above, you can include *some* of the properties from the [Event Data Object](#event-data-format) to customize the job in progress.  Example request:    ```js  {  	""id"": ""j3c182051"",  	""timeout"": 300,  	""notify_success"": ""email@server.com""  }  ```    Example response:    ```js  {  	""code"": 0  }  ```    See the [Standard Response Format](#standard-response-format) for details.    ### abort_job    ```  /api/app/abort_job/v1  ```    This aborts a running job given its ID.  API Keys require the `abort_events` privilege to use this API.  Only HTTP POST (JSON data) is acceptable.  The parameters are as follows:    | Parameter Name | Description |  |----------------|-------------|  | `id` | **(Required)** The ID of the job you wish to abort. |    Example request:    ```js  {  	""id"": ""jiinxhh5203""  }  ```    Example response:    ```js  {  	""code"": 0  }  ```    See the [Standard Response Format](#standard-response-format) for details.    ## Event Data Format    Here are descriptions of all the properties in the event object, which is common in many API calls:    | Event Property | Format | Description |  |----------------|--------|-------------|  | `algo` | String | Specifies the algorithm to use for picking a server from the target group. See [Algorithm](#algorithm). |  | `api_key` | String | The API Key of the application that originally created the event (if created via API). |  | `catch_up` | Boolean | Specifies whether the event has [Run All Mode](#run-all-mode) enabled or not. |  | `category` | String | The Category ID to which the event is assigned.  See [Categories Tab](#categories-tab). |  | `chain` | String | The chain reaction event ID to launch when jobs complete successfully.  See [Chain Reaction](#chain-reaction). |  | `chain_error` | String | The chain reaction event ID to launch when jobs fail.  See [Chain Reaction](#chain-reaction). |  | `cpu_limit` | Number | Limit the CPU to the specified percentage (100 = 1 core), abort if exceeded. See [Event Resource Limits](#event-resource-limits). |  | `cpu_sustain` | Number | Only abort if the CPU limit is exceeded for this many seconds. See [Event Resource Limits](#event-resource-limits). |  | `created` | Number | The date/time of the event's initial creation, in Epoch seconds. |  | `detached` | Boolean | Specifies whether [Detached Mode](#detached-mode) is enabled or not. |  | `enabled` | Boolean | Specifies whether the event is enabled (active in the scheduler) or not. |  | `id` | String | A unique ID assigned to the event when it was first created. |  | `log_max_size` | Number | Limit the job log file size to the specified amount, in bytes.  See [Event Resource Limits](#event-resource-limits). |  | `max_children` | Number | The total amount of concurrent jobs allowed to run. See [Event Concurrency](#event-concurrency). |  | `memory_limit` | Number | Limit the memory usage to the specified amount, in bytes. See [Event Resource Limits](#event-resource-limits). |  | `memory_sustain` | Number | Only abort if the memory limit is exceeded for this many seconds. See [Event Resource Limits](#event-resource-limits). |  | `modified` | Number | The date/time of the event's last modification, in Epoch seconds. |  | `multiplex` | Boolean | Specifies whether the event has [Multiplexing](#multiplexing) mode is enabled or not. |  | `notes` | String | Text notes saved with the event, included in e-mail notifications. See [Event Notes](#event-notes). |  | `notify_fail` | String | List of e-mail recipients to notify upon job failure (CSV). See [Event Notification](#event-notification). |  | `notify_success` | String | List of e-mail recipients to notify upon job success (CSV). See [Event Notification](#event-notification). |  | `params` | Object | An object containing the Plugin's custom parameters, filled out with values from the Event Editor. See [Plugins Tab](#plugins-tab). |  | `plugin` | String | The ID of the Plugin which will run jobs for the event. See [Plugins Tab](#plugins-tab). |  | `queue` | Boolean | Allow jobs to be queued up when they can't run immediately. See [Allow Queued Jobs](#allow-queued-jobs). |  | `queue_max` | Number | Maximum queue length, when `queue` is enabled. See [Allow Queued Jobs](#allow-queued-jobs). |  | `retries` | Number | The number of retries to allow before reporting an error. See [Event Retries](#event-retries). |  | `retry_delay` | Number | Optional delay between retries, in seconds. See [Event Retries](#event-retries). |  | `stagger` | Number | If [Multiplexing](#multiplexing) is enabled, this specifies the number of seconds to wait between job launches. |  | `target` | String | Events can target a [Server Group](#server-groups) (Group ID), or an individual server (hostname). |  | `timeout` | Number | The maximum allowed run time for jobs, specified in seconds. See [Event Timeout](#event-timeout). |  | `timezone` | String | The timezone for interpreting the event timing settings. Needs to be an [IANA timezone string](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).  See [Event Timing](#event-timing). |  | `timing` | Object | An object describing when to run scheduled jobs.  See [Event Timing Object](#event-timing-object) below for details. |  | `title` | String | A display name for the event, shown on the [Schedule Tab](#schedule-tab) as well as in reports and e-mails. |  | `username` | String | The username of the user who originally created the event (if created in the UI). |  | `web_hook` | String | An optional URL to hit for the start and end of each job. See [Event Web Hook](#event-web-hook). |    ### Event Timing Object    The `timing` object describes the event's timing settings (when and how frequent it should run jobs).  It works similarly to the [Unix Cron](https://en.wikipedia.org/wiki/Cron) system, with selections of years, months, days, weekdays, hours and/or minutes.  Each property should be an array of numerical values.  If omitted, it means the same as ""all"" in that category (i.e. asterisk `*` in Cron syntax).    For example, an event with this timing object would run once per hour, on the hour:    ```js  {  	""minutes"": [0]  }  ```    It essentially means every year, every month, every day, every hour, but only on the ""0"" minute.  The scheduler ticks only once a minute, so this only results in running one job for each matching minute.    For another example, this would run twice daily, at 4:30 AM and 4:30 PM:    ```js  {  	""hours"": [4, 16],  	""minutes"": [30]  }  ```    For a more complex example, this would run only in year 2015, from March to May, on the 1st and 15th of the month (but only if also weekdays), at 6AM to 10AM, and on the :15 and :45 of those hours:    ```js  {  	""years"": [2015],  	""months"": [3, 4, 5],  	""days"": [1, 15],  	""weekdays"": [1, 2, 3, 4, 5],  	""hours"": [6, 7, 8, 9, 10],  	""minutes"": [15, 45]  }  ```    Here is a list of all the timing object properties and their descriptions:    | Timing Property | Range | Description |  |-----------------|-------|-------------|  | `years` | ∞ | One or more years in YYYY format. |  | `months` | 1 - 12 | One or more months, where January is 1 and December is 12. |  | `days` | 1 - 31 | One or more month days, from 1 to 31. |  | `weekdays` | 0 - 6 | One or more weekdays, where Sunday is 0, and Saturday is 6 |  | `hours` | 0 - 23 | One or more hours in 24-hour time, from 0 to 23. |  | `minutes` | 0 - 59 | One or more minutes, from 0 to 59. |    # Development    Cronicle runs as a component in the [pixl-server](https://www.npmjs.com/package/pixl-server) framework.  It is highly recommended to read and understand that module and its component system before attempting to develop Cronicle.  The following server components are also used:    | Module Name | Description | License |  |-------------|-------------|---------|  | [pixl-server-api](https://www.npmjs.com/package/pixl-server-api) | A JSON API component for the pixl-server framework. | MIT |  | [pixl-server-storage](https://www.npmjs.com/package/pixl-server-storage) | A key/value/list storage component for the pixl-server framework. | MIT |  | [pixl-server-user](https://www.npmjs.com/package/pixl-server-user) | A basic user login system for the pixl-server framework. | MIT |  | [pixl-server-web](https://www.npmjs.com/package/pixl-server-web) | A web server component for the pixl-server framework. | MIT |    In addition, Cronicle uses the following server-side PixlCore utility modules:    | Module Name | Description | License |  |-------------|-------------|---------|  | [pixl-args](https://www.npmjs.com/package/pixl-args) | A simple module for parsing command line arguments. | MIT |  | [pixl-class](https://www.npmjs.com/package/pixl-class) | A simple module for creating classes, with inheritance and mixins. | MIT |  | [pixl-config](https://www.npmjs.com/package/pixl-config) | A simple JSON configuration loader. | MIT |  | [pixl-json-stream](https://www.npmjs.com/package/pixl-json-stream) | Provides an easy API for sending and receiving JSON records over standard streams (pipes or sockets). | MIT |  | [pixl-logger](https://www.npmjs.com/package/pixl-logger) | A simple logging class which generates bracket delimited log columns. | MIT |  | [pixl-mail](https://www.npmjs.com/package/pixl-mail) | A very simple class for sending e-mail via SMTP. | MIT |  | [pixl-perf](https://www.npmjs.com/package/pixl-perf) | A simple, high precision performance tracking system. | MIT |  | [pixl-request](https://www.npmjs.com/package/pixl-request) | A very simple module for making HTTP requests. | MIT |  | [pixl-tools](https://www.npmjs.com/package/pixl-tools) | A set of miscellaneous utility functions for Node.js. | MIT |  | [pixl-unit](https://www.npmjs.com/package/pixl-unit) | A very simple unit test runner for Node.js. | MIT |    For the client-side, the Cronicle web application is built on the [pixl-webapp](https://www.npmjs.com/package/pixl-webapp) HTML5/CSS/JavaScript framework:    | Module Name | Description | License |  |-------------|-------------|---------|  | [pixl-webapp](https://www.npmjs.com/package/pixl-webapp) | A client-side JavaScript framework, designed to be a base for web applications. | MIT |    ## Installing Dev Tools    For Debian (Ubuntu) OSes:    ```  apt-get install build-essential  ```    For RedHat (Fedora / CentOS):    ```  yum install gcc-c++ make  ```    For Mac OS X, download [Apple's Xcode](https://developer.apple.com/xcode/download/), and then install the [command-line tools](https://developer.apple.com/downloads/).    ## Manual Installation    Here is how you can download the very latest Cronicle dev build and install it manually (may contain bugs!):    ```  git clone https://github.com/jhuckaby/Cronicle.git  cd Cronicle  npm install  node bin/build.js dev  ```    This will keep all JavaScript and CSS unobfuscated (original source served as separate files).    I highly recommend placing the following `.gitignore` file at the base of the project, if you plan on committing changes and sending pull requests:    ```  .gitignore  /node_modules  /work  /logs  /queue  /data  /conf  htdocs/index.html  htdocs/js/common  htdocs/js/external/*  htdocs/fonts/*  htdocs/css/base.css  htdocs/css/c3*  htdocs/css/font*  htdocs/css/mat*  ```    ## Starting in Debug Mode    To start Cronicle in debug mode, issue the following command:    ```  ./bin/debug.sh  ```    This will launch the service without forking a daemon process, and echo the entire debug log contents to the console.  This is great for debugging server-side issues.  Beware of file permissions if you run as a non-root user.  Hit Ctrl-C to shut down the service when in this mode.    Also, you can force it to become the primary server right away, so there is no delay before you can use the web app:    ```  ./bin/debug.sh --master  ```    Do not use the `--master` switch on multiple servers in a cluster.  For multi-server setups, it is much better to wait for Cronicle to decide who should become primary (~60 seconds after startup).    Please note that when starting Cronicle in debug mode, all existing events with [Run All Mode](#run-all-mode) set will instantly be ""caught up"" to the current time, and not run any previous jobs.  Also, some features are not available in debug mode, namely the ""Restart"" and ""Shut Down"" links in the UI.    ## Running Unit Tests    Cronicle comes with a full unit test suite, which runs via the [pixl-unit](https://www.npmjs.com/package/pixl-unit) module (which should be installed automatically).  To run the unit tests, make sure Cronicle isn't already running, and type this:    ```  npm test  ```    If any tests fail, please open a [GitHub issue](https://github.com/jhuckaby/Cronicle/issues) and include the full unit test log, which can be found here:    ```  /opt/cronicle/logs/unit.log  ```    # Companies Using Cronicle  Cronicle is known to be in use by the following companies:  - [Agnes & Dora](https://agnesanddora.com)  - [Sling TV](https://sling.com)  # Colophon    We stand on the shoulders of giants.  Cronicle was inspired by a PHP application called **Ubercron**, which was designed and programmed by [Larry Azlin](http://azlin.com/).  Cheers Larry!    Cronicle was built using these awesome Node modules:    | Module Name | Description | License |  |-------------|-------------|---------|  | [async](https://www.npmjs.com/package/async) | Higher-order functions and common patterns for asynchronous code. | MIT |  | [bcrypt-node](https://www.npmjs.com/package/bcrypt-node) | Native JS implementation of BCrypt for Node. | BSD 3-Clause |  | [chart.js](https://www.npmjs.com/package/chart.js) | Simple HTML5 charts using the canvas element. | MIT |  | [daemon](https://www.npmjs.com/package/daemon) | Add-on for creating \*nix daemons. | MIT |  | [errno](https://www.npmjs.com/package/errno) | Node.js libuv errno details exposed. | MIT |  | [font-awesome](https://www.npmjs.com/package/font-awesome) | The iconic font and CSS framework. | OFL-1.1 and MIT |  | [form-data](https://www.npmjs.com/package/form-data) | A library to create readable ""multipart/form-data"" streams. Can be used to submit forms and file uploads to other web applications. | MIT |  | [formidable](https://www.npmjs.com/package/formidable) | A Node.js module for parsing form data, especially file uploads. | MIT |  | [glob](https://www.npmjs.com/package/glob) | Filesystem globber (`*.js`). | ISC |  | [jstimezonedetect](https://www.npmjs.com/package/jstimezonedetect) | Automatically detects the client or server timezone. | MIT |  | [jquery](https://www.npmjs.com/package/jquery) | JavaScript library for DOM operations. | MIT |  | [mdi](https://www.npmjs.com/package/mdi) | Material Design Webfont. This includes the Stock and Community icons in a single webfont collection. | OFL-1.1 and MIT |  | [mkdirp](https://www.npmjs.com/package/mkdirp) | Recursively mkdir, like `mkdir -p`. | MIT |  | [moment](https://www.npmjs.com/package/moment) | Parse, validate, manipulate, and display dates. | MIT |  | [moment-timezone](https://www.npmjs.com/package/moment-timezone) | Parse and display moments in any timezone. | MIT |  | [netmask](https://www.npmjs.com/package/netmask) | Parses and understands IPv4 CIDR blocks so they can be explored and compared. | MIT |  | [node-static](https://www.npmjs.com/package/node-static) | A simple, compliant file streaming module for node. | MIT |  | [nodemailer](https://www.npmjs.com/package/nodemailer) | Easy as cake e-mail sending from your Node.js applications. | MIT |  | [shell-quote](https://www.npmjs.com/package/shell-quote) | Quote and parse shell commands. | MIT |  | [socket.io](https://www.npmjs.com/package/socket.io) | Node.js real-time framework server (Websockets). | MIT |  | [socket.io-client](https://www.npmjs.com/package/socket.io-client) | Client library for server-to-server socket.io connections. | MIT |  | [uglify-js](https://www.npmjs.com/package/uglify-js) | JavaScript parser, mangler/compressor and beautifier toolkit. | BSD-2-Clause |  | [zxcvbn](https://www.npmjs.com/package/zxcvbn) | Realistic password strength estimation, from Dropbox. | MIT |    # License    The MIT License (MIT)    Copyright (c) 2015 - 2022 Joseph Huckaby    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. """
Big data;https://github.com/boltdb/bolt;"""Bolt [![Coverage Status](https://coveralls.io/repos/boltdb/bolt/badge.svg?branch=master)](https://coveralls.io/r/boltdb/bolt?branch=master) [![GoDoc](https://godoc.org/github.com/boltdb/bolt?status.svg)](https://godoc.org/github.com/boltdb/bolt) ![Version](https://img.shields.io/badge/version-1.2.1-green.svg)  ====    Bolt is a pure Go key/value store inspired by [Howard Chu's][hyc_symas]  [LMDB project][lmdb]. The goal of the project is to provide a simple,  fast, and reliable database for projects that don't require a full database  server such as Postgres or MySQL.    Since Bolt is meant to be used as such a low-level piece of functionality,  simplicity is key. The API will be small and only focus on getting values  and setting values. That's it.    [hyc_symas]: https://twitter.com/hyc_symas  [lmdb]: http://symas.com/mdb/    ## Project Status    Bolt is stable, the API is fixed, and the file format is fixed. Full unit  test coverage and randomized black box testing are used to ensure database  consistency and thread safety. Bolt is currently used in high-load production  environments serving databases as large as 1TB. Many companies such as  Shopify and Heroku use Bolt-backed services every day.    ## A message from the author    > The original goal of Bolt was to provide a simple pure Go key/value store and to  > not bloat the code with extraneous features. To that end, the project has been  > a success. However, this limited scope also means that the project is complete.  >   > Maintaining an open source database requires an immense amount of time and energy.  > Changes to the code can have unintended and sometimes catastrophic effects so  > even simple changes require hours and hours of careful testing and validation.  >  > Unfortunately I no longer have the time or energy to continue this work. Bolt is  > in a stable state and has years of successful production use. As such, I feel that  > leaving it in its current state is the most prudent course of action.  >  > If you are interested in using a more featureful version of Bolt, I suggest that  > you look at the CoreOS fork called [bbolt](https://github.com/coreos/bbolt).    - Ben Johnson ([@benbjohnson](https://twitter.com/benbjohnson))    ## Table of Contents    - [Getting Started](#getting-started)    - [Installing](#installing)    - [Opening a database](#opening-a-database)    - [Transactions](#transactions)      - [Read-write transactions](#read-write-transactions)      - [Read-only transactions](#read-only-transactions)      - [Batch read-write transactions](#batch-read-write-transactions)      - [Managing transactions manually](#managing-transactions-manually)    - [Using buckets](#using-buckets)    - [Using key/value pairs](#using-keyvalue-pairs)    - [Autoincrementing integer for the bucket](#autoincrementing-integer-for-the-bucket)    - [Iterating over keys](#iterating-over-keys)      - [Prefix scans](#prefix-scans)      - [Range scans](#range-scans)      - [ForEach()](#foreach)    - [Nested buckets](#nested-buckets)    - [Database backups](#database-backups)    - [Statistics](#statistics)    - [Read-Only Mode](#read-only-mode)    - [Mobile Use (iOS/Android)](#mobile-use-iosandroid)  - [Resources](#resources)  - [Comparison with other databases](#comparison-with-other-databases)    - [Postgres, MySQL, & other relational databases](#postgres-mysql--other-relational-databases)    - [LevelDB, RocksDB](#leveldb-rocksdb)    - [LMDB](#lmdb)  - [Caveats & Limitations](#caveats--limitations)  - [Reading the Source](#reading-the-source)  - [Other Projects Using Bolt](#other-projects-using-bolt)    ## Getting Started    ### Installing    To start using Bolt, install Go and run `go get`:    ```sh  $ go get github.com/boltdb/bolt/...  ```    This will retrieve the library and install the `bolt` command line utility into  your `$GOBIN` path.      ### Opening a database    The top-level object in Bolt is a `DB`. It is represented as a single file on  your disk and represents a consistent snapshot of your data.    To open your database, simply use the `bolt.Open()` function:    ```go  package main    import (  	""log""    	""github.com/boltdb/bolt""  )    func main() {  	// Open the my.db data file in your current directory.  	// It will be created if it doesn't exist.  	db, err := bolt.Open(""my.db"", 0600, nil)  	if err != nil {  		log.Fatal(err)  	}  	defer db.Close()    	...  }  ```    Please note that Bolt obtains a file lock on the data file so multiple processes  cannot open the same database at the same time. Opening an already open Bolt  database will cause it to hang until the other process closes it. To prevent  an indefinite wait you can pass a timeout option to the `Open()` function:    ```go  db, err := bolt.Open(""my.db"", 0600, &bolt.Options{Timeout: 1 * time.Second})  ```      ### Transactions    Bolt allows only one read-write transaction at a time but allows as many  read-only transactions as you want at a time. Each transaction has a consistent  view of the data as it existed when the transaction started.    Individual transactions and all objects created from them (e.g. buckets, keys)  are not thread safe. To work with data in multiple goroutines you must start  a transaction for each one or use locking to ensure only one goroutine accesses  a transaction at a time. Creating transaction from the `DB` is thread safe.    Read-only transactions and read-write transactions should not depend on one  another and generally shouldn't be opened simultaneously in the same goroutine.  This can cause a deadlock as the read-write transaction needs to periodically  re-map the data file but it cannot do so while a read-only transaction is open.      #### Read-write transactions    To start a read-write transaction, you can use the `DB.Update()` function:    ```go  err := db.Update(func(tx *bolt.Tx) error {  	...  	return nil  })  ```    Inside the closure, you have a consistent view of the database. You commit the  transaction by returning `nil` at the end. You can also rollback the transaction  at any point by returning an error. All database operations are allowed inside  a read-write transaction.    Always check the return error as it will report any disk failures that can cause  your transaction to not complete. If you return an error within your closure  it will be passed through.      #### Read-only transactions    To start a read-only transaction, you can use the `DB.View()` function:    ```go  err := db.View(func(tx *bolt.Tx) error {  	...  	return nil  })  ```    You also get a consistent view of the database within this closure, however,  no mutating operations are allowed within a read-only transaction. You can only  retrieve buckets, retrieve values, and copy the database within a read-only  transaction.      #### Batch read-write transactions    Each `DB.Update()` waits for disk to commit the writes. This overhead  can be minimized by combining multiple updates with the `DB.Batch()`  function:    ```go  err := db.Batch(func(tx *bolt.Tx) error {  	...  	return nil  })  ```    Concurrent Batch calls are opportunistically combined into larger  transactions. Batch is only useful when there are multiple goroutines  calling it.    The trade-off is that `Batch` can call the given  function multiple times, if parts of the transaction fail. The  function must be idempotent and side effects must take effect only  after a successful return from `DB.Batch()`.    For example: don't display messages from inside the function, instead  set variables in the enclosing scope:    ```go  var id uint64  err := db.Batch(func(tx *bolt.Tx) error {  	// Find last key in bucket, decode as bigendian uint64, increment  	// by one, encode back to []byte, and add new key.  	...  	id = newValue  	return nil  })  if err != nil {  	return ...  }  fmt.Println(""Allocated ID %d"", id)  ```      #### Managing transactions manually    The `DB.View()` and `DB.Update()` functions are wrappers around the `DB.Begin()`  function. These helper functions will start the transaction, execute a function,  and then safely close your transaction if an error is returned. This is the  recommended way to use Bolt transactions.    However, sometimes you may want to manually start and end your transactions.  You can use the `DB.Begin()` function directly but **please** be sure to close  the transaction.    ```go  // Start a writable transaction.  tx, err := db.Begin(true)  if err != nil {      return err  }  defer tx.Rollback()    // Use the transaction...  _, err := tx.CreateBucket([]byte(""MyBucket""))  if err != nil {      return err  }    // Commit the transaction and check for error.  if err := tx.Commit(); err != nil {      return err  }  ```    The first argument to `DB.Begin()` is a boolean stating if the transaction  should be writable.      ### Using buckets    Buckets are collections of key/value pairs within the database. All keys in a  bucket must be unique. You can create a bucket using the `DB.CreateBucket()`  function:    ```go  db.Update(func(tx *bolt.Tx) error {  	b, err := tx.CreateBucket([]byte(""MyBucket""))  	if err != nil {  		return fmt.Errorf(""create bucket: %s"", err)  	}  	return nil  })  ```    You can also create a bucket only if it doesn't exist by using the  `Tx.CreateBucketIfNotExists()` function. It's a common pattern to call this  function for all your top-level buckets after you open your database so you can  guarantee that they exist for future transactions.    To delete a bucket, simply call the `Tx.DeleteBucket()` function.      ### Using key/value pairs    To save a key/value pair to a bucket, use the `Bucket.Put()` function:    ```go  db.Update(func(tx *bolt.Tx) error {  	b := tx.Bucket([]byte(""MyBucket""))  	err := b.Put([]byte(""answer""), []byte(""42""))  	return err  })  ```    This will set the value of the `""answer""` key to `""42""` in the `MyBucket`  bucket. To retrieve this value, we can use the `Bucket.Get()` function:    ```go  db.View(func(tx *bolt.Tx) error {  	b := tx.Bucket([]byte(""MyBucket""))  	v := b.Get([]byte(""answer""))  	fmt.Printf(""The answer is: %s\n"", v)  	return nil  })  ```    The `Get()` function does not return an error because its operation is  guaranteed to work (unless there is some kind of system failure). If the key  exists then it will return its byte slice value. If it doesn't exist then it  will return `nil`. It's important to note that you can have a zero-length value  set to a key which is different than the key not existing.    Use the `Bucket.Delete()` function to delete a key from the bucket.    Please note that values returned from `Get()` are only valid while the  transaction is open. If you need to use a value outside of the transaction  then you must use `copy()` to copy it to another byte slice.      ### Autoincrementing integer for the bucket  By using the `NextSequence()` function, you can let Bolt determine a sequence  which can be used as the unique identifier for your key/value pairs. See the  example below.    ```go  // CreateUser saves u to the store. The new user ID is set on u once the data is persisted.  func (s *Store) CreateUser(u *User) error {      return s.db.Update(func(tx *bolt.Tx) error {          // Retrieve the users bucket.          // This should be created when the DB is first opened.          b := tx.Bucket([]byte(""users""))            // Generate ID for the user.          // This returns an error only if the Tx is closed or not writeable.          // That can't happen in an Update() call so I ignore the error check.          id, _ := b.NextSequence()          u.ID = int(id)            // Marshal user data into bytes.          buf, err := json.Marshal(u)          if err != nil {              return err          }            // Persist bytes to users bucket.          return b.Put(itob(u.ID), buf)      })  }    // itob returns an 8-byte big endian representation of v.  func itob(v int) []byte {      b := make([]byte, 8)      binary.BigEndian.PutUint64(b, uint64(v))      return b  }    type User struct {      ID int      ...  }  ```    ### Iterating over keys    Bolt stores its keys in byte-sorted order within a bucket. This makes sequential  iteration over these keys extremely fast. To iterate over keys we'll use a  `Cursor`:    ```go  db.View(func(tx *bolt.Tx) error {  	// Assume bucket exists and has keys  	b := tx.Bucket([]byte(""MyBucket""))    	c := b.Cursor()    	for k, v := c.First(); k != nil; k, v = c.Next() {  		fmt.Printf(""key=%s, value=%s\n"", k, v)  	}    	return nil  })  ```    The cursor allows you to move to a specific point in the list of keys and move  forward or backward through the keys one at a time.    The following functions are available on the cursor:    ```  First()  Move to the first key.  Last()   Move to the last key.  Seek()   Move to a specific key.  Next()   Move to the next key.  Prev()   Move to the previous key.  ```    Each of those functions has a return signature of `(key []byte, value []byte)`.  When you have iterated to the end of the cursor then `Next()` will return a  `nil` key.  You must seek to a position using `First()`, `Last()`, or `Seek()`  before calling `Next()` or `Prev()`. If you do not seek to a position then  these functions will return a `nil` key.    During iteration, if the key is non-`nil` but the value is `nil`, that means  the key refers to a bucket rather than a value.  Use `Bucket.Bucket()` to  access the sub-bucket.      #### Prefix scans    To iterate over a key prefix, you can combine `Seek()` and `bytes.HasPrefix()`:    ```go  db.View(func(tx *bolt.Tx) error {  	// Assume bucket exists and has keys  	c := tx.Bucket([]byte(""MyBucket"")).Cursor()    	prefix := []byte(""1234"")  	for k, v := c.Seek(prefix); k != nil && bytes.HasPrefix(k, prefix); k, v = c.Next() {  		fmt.Printf(""key=%s, value=%s\n"", k, v)  	}    	return nil  })  ```    #### Range scans    Another common use case is scanning over a range such as a time range. If you  use a sortable time encoding such as RFC3339 then you can query a specific  date range like this:    ```go  db.View(func(tx *bolt.Tx) error {  	// Assume our events bucket exists and has RFC3339 encoded time keys.  	c := tx.Bucket([]byte(""Events"")).Cursor()    	// Our time range spans the 90's decade.  	min := []byte(""1990-01-01T00:00:00Z"")  	max := []byte(""2000-01-01T00:00:00Z"")    	// Iterate over the 90's.  	for k, v := c.Seek(min); k != nil && bytes.Compare(k, max) <= 0; k, v = c.Next() {  		fmt.Printf(""%s: %s\n"", k, v)  	}    	return nil  })  ```    Note that, while RFC3339 is sortable, the Golang implementation of RFC3339Nano does not use a fixed number of digits after the decimal point and is therefore not sortable.      #### ForEach()    You can also use the function `ForEach()` if you know you'll be iterating over  all the keys in a bucket:    ```go  db.View(func(tx *bolt.Tx) error {  	// Assume bucket exists and has keys  	b := tx.Bucket([]byte(""MyBucket""))    	b.ForEach(func(k, v []byte) error {  		fmt.Printf(""key=%s, value=%s\n"", k, v)  		return nil  	})  	return nil  })  ```    Please note that keys and values in `ForEach()` are only valid while  the transaction is open. If you need to use a key or value outside of  the transaction, you must use `copy()` to copy it to another byte  slice.    ### Nested buckets    You can also store a bucket in a key to create nested buckets. The API is the  same as the bucket management API on the `DB` object:    ```go  func (*Bucket) CreateBucket(key []byte) (*Bucket, error)  func (*Bucket) CreateBucketIfNotExists(key []byte) (*Bucket, error)  func (*Bucket) DeleteBucket(key []byte) error  ```    Say you had a multi-tenant application where the root level bucket was the account bucket. Inside of this bucket was a sequence of accounts which themselves are buckets. And inside the sequence bucket you could have many buckets pertaining to the Account itself (Users, Notes, etc) isolating the information into logical groupings.    ```go    // createUser creates a new user in the given account.  func createUser(accountID int, u *User) error {      // Start the transaction.      tx, err := db.Begin(true)      if err != nil {          return err      }      defer tx.Rollback()        // Retrieve the root bucket for the account.      // Assume this has already been created when the account was set up.      root := tx.Bucket([]byte(strconv.FormatUint(accountID, 10)))        // Setup the users bucket.      bkt, err := root.CreateBucketIfNotExists([]byte(""USERS""))      if err != nil {          return err      }        // Generate an ID for the new user.      userID, err := bkt.NextSequence()      if err != nil {          return err      }      u.ID = userID        // Marshal and save the encoded user.      if buf, err := json.Marshal(u); err != nil {          return err      } else if err := bkt.Put([]byte(strconv.FormatUint(u.ID, 10)), buf); err != nil {          return err      }        // Commit the transaction.      if err := tx.Commit(); err != nil {          return err      }        return nil  }    ```          ### Database backups    Bolt is a single file so it's easy to backup. You can use the `Tx.WriteTo()`  function to write a consistent view of the database to a writer. If you call  this from a read-only transaction, it will perform a hot backup and not block  your other database reads and writes.    By default, it will use a regular file handle which will utilize the operating  system's page cache. See the [`Tx`](https://godoc.org/github.com/boltdb/bolt#Tx)  documentation for information about optimizing for larger-than-RAM datasets.    One common use case is to backup over HTTP so you can use tools like `cURL` to  do database backups:    ```go  func BackupHandleFunc(w http.ResponseWriter, req *http.Request) {  	err := db.View(func(tx *bolt.Tx) error {  		w.Header().Set(""Content-Type"", ""application/octet-stream"")  		w.Header().Set(""Content-Disposition"", `attachment; filename=""my.db""`)  		w.Header().Set(""Content-Length"", strconv.Itoa(int(tx.Size())))  		_, err := tx.WriteTo(w)  		return err  	})  	if err != nil {  		http.Error(w, err.Error(), http.StatusInternalServerError)  	}  }  ```    Then you can backup using this command:    ```sh  $ curl http://localhost/backup > my.db  ```    Or you can open your browser to `http://localhost/backup` and it will download  automatically.    If you want to backup to another file you can use the `Tx.CopyFile()` helper  function.      ### Statistics    The database keeps a running count of many of the internal operations it  performs so you can better understand what's going on. By grabbing a snapshot  of these stats at two points in time we can see what operations were performed  in that time range.    For example, we could start a goroutine to log stats every 10 seconds:    ```go  go func() {  	// Grab the initial stats.  	prev := db.Stats()    	for {  		// Wait for 10s.  		time.Sleep(10 * time.Second)    		// Grab the current stats and diff them.  		stats := db.Stats()  		diff := stats.Sub(&prev)    		// Encode stats to JSON and print to STDERR.  		json.NewEncoder(os.Stderr).Encode(diff)    		// Save stats for the next loop.  		prev = stats  	}  }()  ```    It's also useful to pipe these stats to a service such as statsd for monitoring  or to provide an HTTP endpoint that will perform a fixed-length sample.      ### Read-Only Mode    Sometimes it is useful to create a shared, read-only Bolt database. To this,  set the `Options.ReadOnly` flag when opening your database. Read-only mode  uses a shared lock to allow multiple processes to read from the database but  it will block any processes from opening the database in read-write mode.    ```go  db, err := bolt.Open(""my.db"", 0666, &bolt.Options{ReadOnly: true})  if err != nil {  	log.Fatal(err)  }  ```    ### Mobile Use (iOS/Android)    Bolt is able to run on mobile devices by leveraging the binding feature of the  [gomobile](https://github.com/golang/mobile) tool. Create a struct that will  contain your database logic and a reference to a `*bolt.DB` with a initializing  constructor that takes in a filepath where the database file will be stored.  Neither Android nor iOS require extra permissions or cleanup from using this method.    ```go  func NewBoltDB(filepath string) *BoltDB {  	db, err := bolt.Open(filepath+""/demo.db"", 0600, nil)  	if err != nil {  		log.Fatal(err)  	}    	return &BoltDB{db}  }    type BoltDB struct {  	db *bolt.DB  	...  }    func (b *BoltDB) Path() string {  	return b.db.Path()  }    func (b *BoltDB) Close() {  	b.db.Close()  }  ```    Database logic should be defined as methods on this wrapper struct.    To initialize this struct from the native language (both platforms now sync  their local storage to the cloud. These snippets disable that functionality for the  database file):    #### Android    ```java  String path;  if (android.os.Build.VERSION.SDK_INT >=android.os.Build.VERSION_CODES.LOLLIPOP){      path = getNoBackupFilesDir().getAbsolutePath();  } else{      path = getFilesDir().getAbsolutePath();  }  Boltmobiledemo.BoltDB boltDB = Boltmobiledemo.NewBoltDB(path)  ```    #### iOS    ```objc  - (void)demo {      NSString* path = [NSSearchPathForDirectoriesInDomains(NSLibraryDirectory,                                                            NSUserDomainMask,                                                            YES) objectAtIndex:0];  	GoBoltmobiledemoBoltDB * demo = GoBoltmobiledemoNewBoltDB(path);  	[self addSkipBackupAttributeToItemAtPath:demo.path];  	//Some DB Logic would go here  	[demo close];  }    - (BOOL)addSkipBackupAttributeToItemAtPath:(NSString *) filePathString  {      NSURL* URL= [NSURL fileURLWithPath: filePathString];      assert([[NSFileManager defaultManager] fileExistsAtPath: [URL path]]);        NSError *error = nil;      BOOL success = [URL setResourceValue: [NSNumber numberWithBool: YES]                                    forKey: NSURLIsExcludedFromBackupKey error: &error];      if(!success){          NSLog(@""Error excluding %@ from backup %@"", [URL lastPathComponent], error);      }      return success;  }    ```    ## Resources    For more information on getting started with Bolt, check out the following articles:    * [Intro to BoltDB: Painless Performant Persistence](http://npf.io/2014/07/intro-to-boltdb-painless-performant-persistence/) by [Nate Finch](https://github.com/natefinch).  * [Bolt -- an embedded key/value database for Go](https://www.progville.com/go/bolt-embedded-db-golang/) by Progville      ## Comparison with other databases    ### Postgres, MySQL, & other relational databases    Relational databases structure data into rows and are only accessible through  the use of SQL. This approach provides flexibility in how you store and query  your data but also incurs overhead in parsing and planning SQL statements. Bolt  accesses all data by a byte slice key. This makes Bolt fast to read and write  data by key but provides no built-in support for joining values together.    Most relational databases (with the exception of SQLite) are standalone servers  that run separately from your application. This gives your systems  flexibility to connect multiple application servers to a single database  server but also adds overhead in serializing and transporting data over the  network. Bolt runs as a library included in your application so all data access  has to go through your application's process. This brings data closer to your  application but limits multi-process access to the data.      ### LevelDB, RocksDB    LevelDB and its derivatives (RocksDB, HyperLevelDB) are similar to Bolt in that  they are libraries bundled into the application, however, their underlying  structure is a log-structured merge-tree (LSM tree). An LSM tree optimizes  random writes by using a write ahead log and multi-tiered, sorted files called  SSTables. Bolt uses a B+tree internally and only a single file. Both approaches  have trade-offs.    If you require a high random write throughput (>10,000 w/sec) or you need to use  spinning disks then LevelDB could be a good choice. If your application is  read-heavy or does a lot of range scans then Bolt could be a good choice.    One other important consideration is that LevelDB does not have transactions.  It supports batch writing of key/values pairs and it supports read snapshots  but it will not give you the ability to do a compare-and-swap operation safely.  Bolt supports fully serializable ACID transactions.      ### LMDB    Bolt was originally a port of LMDB so it is architecturally similar. Both use  a B+tree, have ACID semantics with fully serializable transactions, and support  lock-free MVCC using a single writer and multiple readers.    The two projects have somewhat diverged. LMDB heavily focuses on raw performance  while Bolt has focused on simplicity and ease of use. For example, LMDB allows  several unsafe actions such as direct writes for the sake of performance. Bolt  opts to disallow actions which can leave the database in a corrupted state. The  only exception to this in Bolt is `DB.NoSync`.    There are also a few differences in API. LMDB requires a maximum mmap size when  opening an `mdb_env` whereas Bolt will handle incremental mmap resizing  automatically. LMDB overloads the getter and setter functions with multiple  flags whereas Bolt splits these specialized cases into their own functions.      ## Caveats & Limitations    It's important to pick the right tool for the job and Bolt is no exception.  Here are a few things to note when evaluating and using Bolt:    * Bolt is good for read intensive workloads. Sequential write performance is    also fast but random writes can be slow. You can use `DB.Batch()` or add a    write-ahead log to help mitigate this issue.    * Bolt uses a B+tree internally so there can be a lot of random page access.    SSDs provide a significant performance boost over spinning disks.    * Try to avoid long running read transactions. Bolt uses copy-on-write so    old pages cannot be reclaimed while an old transaction is using them.    * Byte slices returned from Bolt are only valid during a transaction. Once the    transaction has been committed or rolled back then the memory they point to    can be reused by a new page or can be unmapped from virtual memory and you'll    see an `unexpected fault address` panic when accessing it.    * Bolt uses an exclusive write lock on the database file so it cannot be    shared by multiple processes.    * Be careful when using `Bucket.FillPercent`. Setting a high fill percent for    buckets that have random inserts will cause your database to have very poor    page utilization.    * Use larger buckets in general. Smaller buckets causes poor page utilization    once they become larger than the page size (typically 4KB).    * Bulk loading a lot of random writes into a new bucket can be slow as the    page will not split until the transaction is committed. Randomly inserting    more than 100,000 key/value pairs into a single new bucket in a single    transaction is not advised.    * Bolt uses a memory-mapped file so the underlying operating system handles the    caching of the data. Typically, the OS will cache as much of the file as it    can in memory and will release memory as needed to other processes. This means    that Bolt can show very high memory usage when working with large databases.    However, this is expected and the OS will release memory as needed. Bolt can    handle databases much larger than the available physical RAM, provided its    memory-map fits in the process virtual address space. It may be problematic    on 32-bits systems.    * The data structures in the Bolt database are memory mapped so the data file    will be endian specific. This means that you cannot copy a Bolt file from a    little endian machine to a big endian machine and have it work. For most    users this is not a concern since most modern CPUs are little endian.    * Because of the way pages are laid out on disk, Bolt cannot truncate data files    and return free pages back to the disk. Instead, Bolt maintains a free list    of unused pages within its data file. These free pages can be reused by later    transactions. This works well for many use cases as databases generally tend    to grow. However, it's important to note that deleting large chunks of data    will not allow you to reclaim that space on disk.      For more information on page allocation, [see this comment][page-allocation].    [page-allocation]: https://github.com/boltdb/bolt/issues/308#issuecomment-74811638      ## Reading the Source    Bolt is a relatively small code base (<3KLOC) for an embedded, serializable,  transactional key/value database so it can be a good starting point for people  interested in how databases work.    The best places to start are the main entry points into Bolt:    - `Open()` - Initializes the reference to the database. It's responsible for    creating the database if it doesn't exist, obtaining an exclusive lock on the    file, reading the meta pages, & memory-mapping the file.    - `DB.Begin()` - Starts a read-only or read-write transaction depending on the    value of the `writable` argument. This requires briefly obtaining the ""meta""    lock to keep track of open transactions. Only one read-write transaction can    exist at a time so the ""rwlock"" is acquired during the life of a read-write    transaction.    - `Bucket.Put()` - Writes a key/value pair into a bucket. After validating the    arguments, a cursor is used to traverse the B+tree to the page and position    where they key & value will be written. Once the position is found, the bucket    materializes the underlying page and the page's parent pages into memory as    ""nodes"". These nodes are where mutations occur during read-write transactions.    These changes get flushed to disk during commit.    - `Bucket.Get()` - Retrieves a key/value pair from a bucket. This uses a cursor    to move to the page & position of a key/value pair. During a read-only    transaction, the key and value data is returned as a direct reference to the    underlying mmap file so there's no allocation overhead. For read-write    transactions, this data may reference the mmap file or one of the in-memory    node values.    - `Cursor` - This object is simply for traversing the B+tree of on-disk pages    or in-memory nodes. It can seek to a specific key, move to the first or last    value, or it can move forward or backward. The cursor handles the movement up    and down the B+tree transparently to the end user.    - `Tx.Commit()` - Converts the in-memory dirty nodes and the list of free pages    into pages to be written to disk. Writing to disk then occurs in two phases.    First, the dirty pages are written to disk and an `fsync()` occurs. Second, a    new meta page with an incremented transaction ID is written and another    `fsync()` occurs. This two phase write ensures that partially written data    pages are ignored in the event of a crash since the meta page pointing to them    is never written. Partially written meta pages are invalidated because they    are written with a checksum.    If you have additional notes that could be helpful for others, please submit  them via pull request.      ## Other Projects Using Bolt    Below is a list of public, open source projects that use Bolt:    * [BoltDbWeb](https://github.com/evnix/boltdbweb) - A web based GUI for BoltDB files.  * [Operation Go: A Routine Mission](http://gocode.io) - An online programming game for Golang using Bolt for user accounts and a leaderboard.  * [Bazil](https://bazil.org/) - A file system that lets your data reside where it is most convenient for it to reside.  * [DVID](https://github.com/janelia-flyem/dvid) - Added Bolt as optional storage engine and testing it against Basho-tuned leveldb.  * [Skybox Analytics](https://github.com/skybox/skybox) - A standalone funnel analysis tool for web analytics.  * [Scuttlebutt](https://github.com/benbjohnson/scuttlebutt) - Uses Bolt to store and process all Twitter mentions of GitHub projects.  * [Wiki](https://github.com/peterhellberg/wiki) - A tiny wiki using Goji, BoltDB and Blackfriday.  * [ChainStore](https://github.com/pressly/chainstore) - Simple key-value interface to a variety of storage engines organized as a chain of operations.  * [MetricBase](https://github.com/msiebuhr/MetricBase) - Single-binary version of Graphite.  * [Gitchain](https://github.com/gitchain/gitchain) - Decentralized, peer-to-peer Git repositories aka ""Git meets Bitcoin"".  * [event-shuttle](https://github.com/sclasen/event-shuttle) - A Unix system service to collect and reliably deliver messages to Kafka.  * [ipxed](https://github.com/kelseyhightower/ipxed) - Web interface and api for ipxed.  * [BoltStore](https://github.com/yosssi/boltstore) - Session store using Bolt.  * [photosite/session](https://godoc.org/bitbucket.org/kardianos/photosite/session) - Sessions for a photo viewing site.  * [LedisDB](https://github.com/siddontang/ledisdb) - A high performance NoSQL, using Bolt as optional storage.  * [ipLocator](https://github.com/AndreasBriese/ipLocator) - A fast ip-geo-location-server using bolt with bloom filters.  * [cayley](https://github.com/google/cayley) - Cayley is an open-source graph database using Bolt as optional backend.  * [bleve](http://www.blevesearch.com/) - A pure Go search engine similar to ElasticSearch that uses Bolt as the default storage backend.  * [tentacool](https://github.com/optiflows/tentacool) - REST api server to manage system stuff (IP, DNS, Gateway...) on a linux server.  * [Seaweed File System](https://github.com/chrislusf/seaweedfs) - Highly scalable distributed key~file system with O(1) disk read.  * [InfluxDB](https://influxdata.com) - Scalable datastore for metrics, events, and real-time analytics.  * [Freehold](http://tshannon.bitbucket.org/freehold/) - An open, secure, and lightweight platform for your files and data.  * [Prometheus Annotation Server](https://github.com/oliver006/prom_annotation_server) - Annotation server for PromDash & Prometheus service monitoring system.  * [Consul](https://github.com/hashicorp/consul) - Consul is service discovery and configuration made easy. Distributed, highly available, and datacenter-aware.  * [Kala](https://github.com/ajvb/kala) - Kala is a modern job scheduler optimized to run on a single node. It is persistent, JSON over HTTP API, ISO 8601 duration notation, and dependent jobs.  * [drive](https://github.com/odeke-em/drive) - drive is an unofficial Google Drive command line client for \*NIX operating systems.  * [stow](https://github.com/djherbis/stow) -  a persistence manager for objects    backed by boltdb.  * [buckets](https://github.com/joyrexus/buckets) - a bolt wrapper streamlining    simple tx and key scans.  * [mbuckets](https://github.com/abhigupta912/mbuckets) - A Bolt wrapper that allows easy operations on multi level (nested) buckets.  * [Request Baskets](https://github.com/darklynx/request-baskets) - A web service to collect arbitrary HTTP requests and inspect them via REST API or simple web UI, similar to [RequestBin](http://requestb.in/) service  * [Go Report Card](https://goreportcard.com/) - Go code quality report cards as a (free and open source) service.  * [Boltdb Boilerplate](https://github.com/bobintornado/boltdb-boilerplate) - Boilerplate wrapper around bolt aiming to make simple calls one-liners.  * [lru](https://github.com/crowdriff/lru) - Easy to use Bolt-backed Least-Recently-Used (LRU) read-through cache with chainable remote stores.  * [Storm](https://github.com/asdine/storm) - Simple and powerful ORM for BoltDB.  * [GoWebApp](https://github.com/josephspurrier/gowebapp) - A basic MVC web application in Go using BoltDB.  * [SimpleBolt](https://github.com/xyproto/simplebolt) - A simple way to use BoltDB. Deals mainly with strings.  * [Algernon](https://github.com/xyproto/algernon) - A HTTP/2 web server with built-in support for Lua. Uses BoltDB as the default database backend.  * [MuLiFS](https://github.com/dankomiocevic/mulifs) - Music Library Filesystem creates a filesystem to organise your music files.  * [GoShort](https://github.com/pankajkhairnar/goShort) - GoShort is a URL shortener written in Golang and BoltDB for persistent key/value storage and for routing it's using high performent HTTPRouter.  * [torrent](https://github.com/anacrolix/torrent) - Full-featured BitTorrent client package and utilities in Go. BoltDB is a storage backend in development.  * [gopherpit](https://github.com/gopherpit/gopherpit) - A web service to manage Go remote import paths with custom domains  * [bolter](https://github.com/hasit/bolter) - Command-line app for viewing BoltDB file in your terminal.  * [btcwallet](https://github.com/btcsuite/btcwallet) - A bitcoin wallet.  * [dcrwallet](https://github.com/decred/dcrwallet) - A wallet for the Decred cryptocurrency.  * [Ironsmith](https://github.com/timshannon/ironsmith) - A simple, script-driven continuous integration (build - > test -> release) tool, with no external dependencies  * [BoltHold](https://github.com/timshannon/bolthold) - An embeddable NoSQL store for Go types built on BoltDB  * [Ponzu CMS](https://ponzu-cms.org) - Headless CMS + automatic JSON API with auto-HTTPS, HTTP/2 Server Push, and flexible server framework.    If you are using Bolt in a project please send a pull request to add it to the list. """
Big data;https://github.com/linkedin/white-elephant;"""# White Elephant    White Elephant is a Hadoop log aggregator and dashboard which enables   visualization of Hadoop cluster utilization across users.    ![Screenshot](https://s3.amazonaws.com/snaprojects/blog/whitel/whitel_ex2.png)    ## Quick Start    To try out the server with some test data:        cd server      ant      ./startup.sh    Then visit [http://localhost:3000](http://localhost:3000).  It may take a minute for the test data  to load.    ## Hadoop Version Compatibility    White Elephant is compiled and tested against Hadoop 1.0.3 and should work with any 1.0.x version.  Hadoop 2.0 is not yet supported.    ## Server    The server is a JRuby web application.  In a production environment it can be deployed to tomcat  and reads aggregated usage data directly from Hadoop.  This data is stored in an in-memory database  provided by [HyperSQL](http://hsqldb.org/).  Charting is provided by   [Rickshaw](http://code.shutterstock.com/rickshaw/).    ### Getting started    To get started using the server, first set up the environment:        cd server      ant    The default target does several things, among them:    * Installs JRuby to a local directory under .rbenv  * Installs Ruby gems to the above directory  * Downloads JARs  * Creates test data under data/usage    At this point you should be able to start the server:        ./startup.sh    You can now visit [http://localhost:3000](http://localhost:3000).  It may take a minute for the test data  to load.    This uses [trinidad](https://github.com/trinidad/trinidad) to run the JRuby web app in development mode.  Since it is in development mode the app assumes local data should be used,  which it looks for in the directory specified in `config.yml`.    ### Configuration    The server configuration is contained in `config.yml`.  You can see a sample in `sample_config.yml`.    When run in development mode using `./startup.sh`, `sample_config.yml` is used and it follows the  settings specified under `local`.  The only configurable parameter here is `file_pattern`, which specifies   where to load the usage data from on local disk.    When packaged as a WAR it runs in production mode and uses configuration specified under `hadoop`,   the assumption being that the aggregated usage data will be available there.  The following   parameter must be specified:    * **file_pattern**: Glob pattern to load usage files from Hadoop  * **libs**: Directories containing Hadoop JARs (to be added to the classpath)  * **conf_dir**: Directory containing Hadoop configuration (to be added to the classpath)  * **principal**: User name used to access secure Hadoop  * **keytab**: Path to keytab file for user to access secure Hadoop    White Elephant does not assume a specific version of Hadoop, so the JARs are not packaged in the WAR.  Therefore the path to the Hadoop JARs must be specified in the configuration.    ### Deploying    To build a WAR which can be deployed to tomcat:        ant war -Dconfig.path=<path-to-config>    The config file you specify will be packaged as `config.yml` within the WAR.  See `sample_config.yml`  as an example for how to write the config file.    ## Hadoop Log Uploading    The script `hadoop/scripts/statsupload.pl` can be used to upload the Hadoop logs to HDFS  so they can be processed.  Check its documentation for details.    ## Hadoop Jobs    There are three Hadoop jobs, all managed by a job executor which keeps track of what  work needs to be done.    The first two jobs parse and convert raw job configurations and logging into an easier-to-work-with Avro  format. Together, these two datasets can serve as the base data for a variety of usage analytics workflows.    The third and final job reads the Avro-fied log data and aggregates it per hour, writing the data  out in Avro format.  It essentially builds a data cube which can be easily loaded by the  web application into the DB and queried against.    ### Configuration    Some sample configuration files can be found under `hadoop/config/jobs`:    * **base.properties**: Contains most of the configuration  * **white-elephant-full-usage.job**: Job file used when processing all logs.  * **white-elephant-incremental-usage.job**: Job file used when incrementally processing logs.    The `base.properties` file consists of configuration specific to White Elephant and configuration  specifically for Hadoop.  All Hadoop configuration parameter begin with `hadoop-conf`.  The two  job files just have a single settings `incremental` and only differ in the value they use for it.    ### Hadoop Logs    Within `base.properties` is a parameter `logs.root`.  This is the root path where the Hadoop logs  are found which are to be parsed.  The parsing job assumes the logs are stored in Hadoop under daily  directories using the following directory structure:        <logs.root>/<cluster-name>/daily/<yyyy>/<MMdd>    For example, logs on January 23rd, 2013 for the production cluster may be stored in a directory  such as:        /data/hadoop/logs/prod/daily/2013/0123    ### Packaging    To create a zip package containing all files necessary to run the jobs simply run:        ant zip -Djob.config.dir=<path-to-job-config-dir>    The `job.config.dir` should be the directory containing the `.properties` and `.job` files you would  like to include in the package.    If you happen to be using [Azkaban](http://data.linkedin.com/opensource/azkaban) as your job scheduler  of choice then this zip file will work with it as long as you add the Azkaban specific configuration   to `base.properties`.    ### Running    After unzipping the zip package you can run using the `run.sh` script.  This requires a couple environment   variables to be set:    * **HADOOP_CONF_DIR**: Hadoop configuration directory  * **HADOOP_LIB_DIR**: Hadoop JARs directory    To run the full job:        ./run.sh white-elephant-full-usage.job    To run the incremental job:        ./run.sh white-elephant-incremental-usage.job    The incremental job is more efficient as it only processes new data.  The full job reprocesses  *everything*.    ## Contributing    White Elephant is open source and freely available under the Apache 2 license.  As always, we  welcome contributors, so send us your pull requests.    For help please see the [discussion group](http://groups.google.com/group/linkedin-white-elephant).    ## Thanks    White Elephant is built using a suite of great open source projects.  Just to name a few:    * [JRuby](http://jruby.org/)  * [HyperSQL](http://hsqldb.org/)  * [Bootstrap](http://twitter.github.com/bootstrap/)  * [Rickshaw](http://code.shutterstock.com/rickshaw/)  * [Ember.js](http://emberjs.com/)  * [D3](http://d3js.org/)  * [Moment.js](http://momentjs.com/)  * [jQuery](http://jquery.com/)  * [jQuery UI](http://jqueryui.com/)    ## License    Copyright 2012 LinkedIn, Inc    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. """
Big data;https://github.com/etsy/Conjecture;"""# Conjecture [![Build Status](https://travis-ci.org/etsy/Conjecture.svg?branch=master)](https://travis-ci.org/etsy/Conjecture)    Conjecture is a framework for building machine learning models in Hadoop using the Scalding DSL.  The goal of this project is to enable the development of statistical models as viable components  in a wide range of product settings. Applications include classification and categorization,  recommender systems, ranking, filtering, and regression (predicting real-valued numbers).  Conjecture has been designed with a primary emphasis on flexibility and can handle a wide variety of inputs.  Integration with Hadoop and scalding enable seamless handling of extremely large data volumes,  and integration with established ETL processes. Predicted labels can either be consumed directly  by the web stack using the dataset loader, or models can be deployed and consumed by live web code.  Currently, binary classification (assigning one of two possible labels to input data points)  is the most mature component of the Conjecture package.    # Tutorial  There are a few stages involved in training a machine learning model using Conjecture.    ## Create Training Data  We represent the training data as ""feature vectors"" which are just mappings of feature names to real values.  In this case we represent them as a java map of strings to doubles  (although we have a class StringKeyedVector which provides convenience methods for feature vector construction).  We also need the true label of each instance, which we represent as 0 and 1  (the mapping of these binary labels to e.g., ""male"" and ""female"" is up to the user).  We construct BinaryLabeledInstances, which are just wrappers for a feature vector and a label.        val bl = new BinaryLabeledInstance(0.0)      bl.addTerm(""bias"", 1.0)      bl.addTerm(""some_feature"", 0.5)    ## Training a Classifier  Classifiers are essentially trained by presenting the labeled instances to them.  There are several kinds   of linear classifiers we implement, among them:    * Logistic regression,  * Perceptron,  * MIRA (a large margin perceptron model),  * Passive aggressive.    These models all have several options, such as learning rate, regularization parameters and so on.  We supply  reasonable defaults for these parameters although they can be changed readily.  To train a linear model  simply call the update function with the labeled instance:        val p = new LogisticRegression()      p.update(bl)    In order to make this procedure tractable for large datasets, we provided scalding wrappers for the training.  These operate by training several small models on mappers, then aggregating them into a final complete model  on the reducers.  This wrapper is called like so:        new BinaryModelTrainer(args)        .train(instances, 'instance, 'model)        .write(SequenceFile(""model""))        .map('model -> 'model){ x : UpdateableBinaryModel => new com.google.gson.Gson.toJson(x) }        .write(Tsv(""model_json""))    This code segment will train a model using a pipe called instances which has a field called instance which contains  the BinaryLabeledInstance objects.  It produces a pipe with a single field containing the completed model, which can  then be written to disk.    This class uses the command line args object from scalding, in order to let you set some options on the command line.  Some useful options are:    | Argument                            | Possible values                               | Default            | Meaning                                          |  |-------------------------------------|-----------------------------------------------|--------------------|--------------------------------------------------|  | --model                             | mira, logistic_regression, passive_aggressive | passive_aggressive | The type of model to use.                        |  | --iters                             | 1, 2, 3...                                    | 1                  | The number of iterations of training to perform. |  | --zero_class_prob, --one_class_prob | [0, 1]                                        | 1                  |                                                  |    To see all the command line options, see the BinaryModelTrainer class.    ## Evaluating a Classifier  It is important to get a sense of the performance you can expect out of your classifier on unseen data.  In order to do this we recommend to use cross validation.  In essence, your input set of instances is split up into testing and training portions (multiple different ways),  then a classifier is trained on each training portion, and evaluated (against the true labels which are present)  using the testing portion.  This is all wrapped up in a class called BinaryCrossValidator, it is used like so:        new BinaryCrossValidator(args, 5)        .crossValidate(instances, 'instance)        .write(Tsv(""model_xval""))    This class also takes the command line arguments, which it passes to a model trainer for each fold.  This allows the specification of options to the cross validated models on the command line.  The output contains statistics about the performance of the model as well as the confusion matrices  for each fold.    A script is included which cross validates a logistic regression model on the iris dataset.       """
Big data;https://github.com/mara/data-integration;"""# Mara Pipelines    [![Build Status](https://travis-ci.org/mara/mara-pipelines.svg?branch=master)](https://travis-ci.org/mara/mara-pipelines)  [![PyPI - License](https://img.shields.io/pypi/l/mara-pipelines.svg)](https://github.com/mara/mara-pipelines/blob/master/LICENSE)  [![PyPI version](https://badge.fury.io/py/mara-pipelines.svg)](https://badge.fury.io/py/mara-pipelines)  [![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://communityinviter.com/apps/mara-users/public-invite)        This package contains a lightweight data transformation framework with a focus on transparency and complexity reduction. It has a number of baked-in assumptions/ principles:    - Data integration pipelines as code: pipelines, tasks and commands are created using declarative Python code.    - PostgreSQL as a data processing engine.    - Extensive web ui. The web browser as the main tool for inspecting, running and debugging pipelines.    - GNU make semantics. Nodes depend on the completion of upstream nodes. No data dependencies or data flows.    - No in-app data processing: command line tools as the main tool for interacting with databases and data.    - Single machine pipeline execution based on Python's [multiprocessing](https://docs.python.org/3.6/library/multiprocessing.html). No need for distributed task queues. Easy debugging and output logging.    - Cost based priority queues: nodes with higher cost (based on recorded run times) are run first.    &nbsp;    ## Installation    To use the library directly, use pip:    ```  pip install mara-pipelines  ```    or     ```  pip install git+https://github.com/mara/mara-pipelines.git  ```    For an example of an integration into a flask application, have a look at the [mara example project 1](https://github.com/mara/mara-example-project-1) and [mara example project 2](https://github.com/mara/mara-example-project-2).    Due to the heavy use of forking, Mara Pipelines does not run natively on Windows. If you want to run it on Windows, then please use Docker or the [Windows Subsystem for Linux](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux).     &nbsp;    ## Example    Here is a pipeline ""demo"" consisting of three nodes that depend on each other: the task `ping_localhost`, the pipeline `sub_pipeline` and the task `sleep`:    ```python  from mara_pipelines.commands.bash import RunBash  from mara_pipelines.pipelines import Pipeline, Task  from mara_pipelines.ui.cli import run_pipeline, run_interactively    pipeline = Pipeline(      id='demo',      description='A small pipeline that demonstrates the interplay between pipelines, tasks and commands')    pipeline.add(Task(id='ping_localhost', description='Pings localhost',                    commands=[RunBash('ping -c 3 localhost')]))    sub_pipeline = Pipeline(id='sub_pipeline', description='Pings a number of hosts')    for host in ['google', 'amazon', 'facebook']:      sub_pipeline.add(Task(id=f'ping_{host}', description=f'Pings {host}',                            commands=[RunBash(f'ping -c 3 {host}.com')]))    sub_pipeline.add_dependency('ping_amazon', 'ping_facebook')  sub_pipeline.add(Task(id='ping_foo', description='Pings foo',                        commands=[RunBash('ping foo')]), ['ping_amazon'])    pipeline.add(sub_pipeline, ['ping_localhost'])    pipeline.add(Task(id='sleep', description='Sleeps for 2 seconds',                    commands=[RunBash('sleep 2')]), ['sub_pipeline'])  ```    Tasks contain lists of commands, which do the actual work (in this case running bash commands that ping various hosts).     &nbsp;    In order to run the pipeline, a PostgreSQL database needs to be configured for storing run-time information, run output and status of incremental processing:     ```python  import mara_db.auto_migration  import mara_db.config  import mara_db.dbs    mara_db.config.databases \      = lambda: {'mara': mara_db.dbs.PostgreSQLDB(host='localhost', user='root', database='example_etl_mara')}    mara_db.auto_migration.auto_discover_models_and_migrate()  ```    Given that PostgresSQL is running and the credentials work, the output looks like this (a database with a number of tables is created):    ```  Created database ""postgresql+psycopg2://root@localhost/example_etl_mara""    CREATE TABLE data_integration_file_dependency (      node_path TEXT[] NOT NULL,       dependency_type VARCHAR NOT NULL,       hash VARCHAR,       timestamp TIMESTAMP WITHOUT TIME ZONE,       PRIMARY KEY (node_path, dependency_type)  );    .. more tables  ```    ### CLI UI    This runs a pipeline with output to stdout:    ```python  from mara_pipelines.ui.cli import run_pipeline    run_pipeline(pipeline)  ```    ![Example run cli 1](docs/example-run-cli-1.gif)    &nbsp;    And this runs a single node of pipeline `sub_pipeline` together with all the nodes that it depends on:    ```python  run_pipeline(sub_pipeline, nodes=[sub_pipeline.nodes['ping_amazon']], with_upstreams=True)  ```    ![Example run cli 2](docs/example-run-cli-2.gif)    &nbsp;      And finally, there is some sort of menu based on [pythondialog](http://pythondialog.sourceforge.net/) that allows to navigate and run pipelines like this:    ```python  from mara_pipelines.ui.cli import run_interactively    run_interactively()  ```    ![Example run cli 3](docs/example-run-cli-3.gif)        ### Web UI    More importantly, this package provides an extensive web interface. It can be easily integrated into any [Flask](http://flask.pocoo.org/) based app and the [mara example project](https://github.com/mara/mara-example-project) demonstrates how to do this using [mara-app](https://github.com/mara/mara-app).    For each pipeline, there is a page that shows    - a graph of all child nodes and the dependencies between them  - a chart of the overal run time of the pipeline and it's most expensive nodes over the last 30 days (configurable)  - a table of all the pipeline's nodes with their average run times and the resulting queuing priority  - output and timeline for the last runs of the pipeline      ![Mara pipelines web ui 1](docs/mara-pipelines-web-ui-1.png)    For each task, there is a page showing     - the upstreams and downstreams of the task in the pipeline  - the run times of the task in the last 30 days  - all commands of the task  - output of the last runs of the task    ![Mara pipelines web ui 2](docs/mara-pipelines-web-ui-2.png)      Pipelines and tasks can be run from the web ui directly, which is probably one of the main features of this package:     ![Example run web ui](docs/example-run-web-ui.gif)    &nbsp;    # Getting started    Documentation is currently work in progress. Please use the [mara example project 1](https://github.com/mara/mara-example-project-1) and [mara example project 2](https://github.com/mara/mara-example-project-2) as a reference for getting started.      """
Big data;https://github.com/Hydrospheredata/mist;"""[![Build Status](https://ci.hydrosphere.io/buildStatus/icon?job=hydrosphere.io/mist/master)](https://ci.hydrosphere.io/job/hydrosphere.io/job/mist/job/master/)  [![Build Status](https://travis-ci.org/Hydrospheredata/mist.svg?branch=master)](https://travis-ci.org/Hydrospheredata)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/io.hydrosphere/mist-lib_2.11/badge.svg)](https://maven-badges.herokuapp.com/maven-central/io.hydrosphere/mist-lib_2.11/)  [![Docker Hub Pulls](https://img.shields.io/docker/pulls/hydrosphere/mist.svg)](https://img.shields.io/docker/pulls/hydrosphere/mist.svg)  # Hydrosphere Mist    [![Join the chat at https://gitter.im/Hydrospheredata/mist](https://badges.gitter.im/Hydrospheredata/mist.svg)](https://gitter.im/Hydrospheredata/mist?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    [Hydrosphere](http://hydrosphere.io) Mist is a serverless proxy for Spark cluster.  Mist provides a new functional programming framework and deployment model for Spark applications.     Please see our [quick start guide](https://hydrosphere.io/mist-docs/quick_start.html) and [documentation](https://hydrosphere.io/mist-docs/)    Features:  * **Spark Function as a Service**. Deploy Spark functions rather than notebooks or scripts.  * Spark Cluster and Session management. Fully managed Spark sessions backed by on-demand EMR, Hortonworks, Cloudera, DC/OS and vanilla Spark clusters.  * **Typesafe** programming framework that clearly defines inputs and outputs of every Spark job.  * **REST** HTTP & Messaging (MQTT, Kafka) API for Scala & Python Spark jobs.  * Multi-cluster mode: Seamless Spark cluster on-demand provisioning, autoscaling and termination(**pending**)  ![Cluster of Spark Clusters](http://dv9c7babquml0.cloudfront.net/docs-images/mist-cluster-of-spark-clusters.gif)    It creates a unified API layer for building enterprise solutions and microservices on top of a Spark functions.    ![Mist use cases](http://dv9c7babquml0.cloudfront.net/docs-images/mist-use-case.png)    ## High Level Architecture    ![High Level Architecture](http://dv9c7babquml0.cloudfront.net/docs-images/mist-highlevel-architecture.png)    ## Contact    Please report bugs/problems to:   <https://github.com/Hydrospheredata/mist/issues>.    <http://hydrosphere.io/>    [LinkedIn](https://www.linkedin.com/company/hydrospherebigdata)    [Facebook](https://www.facebook.com/hydrosphere.io/)    [Twitter](https://twitter.com/hydrospheredata) """
Big data;https://github.com/allegro/hermes;"""Hermes  ======    [![Build Status](https://github.com/allegro/hermes/actions/workflows/ci.yml/badge.svg?branch=master)](https://github.com/allegro/hermes/actions/workflows/ci.yml?query=branch%3Amaster)  [![Documentation Status](https://readthedocs.org/projects/hermes-pubsub/badge/?version=latest)](https://readthedocs.org/projects/hermes-pubsub/?badge=latest)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/pl.allegro.tech.hermes/hermes-client/badge.svg)](https://maven-badges.herokuapp.com/maven-central/pl.allegro.tech.hermes/hermes-client)  [![Join the chat](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/allegro/hermes?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    Hermes is an asynchronous message broker built on top of [Kafka](http://kafka.apache.org/).  We provide reliable, fault tolerant REST interface for message publishing and adaptive push  mechanisms for message sending.    See our 10-minute getting started guide with Vagrant: [Getting started](http://hermes-pubsub.readthedocs.org/en/latest/quickstart/)    Visit our page: [hermes.allegro.tech](http://hermes.allegro.tech)    See our full documentation: [http://hermes-pubsub.readthedocs.org/en/latest/](http://hermes-pubsub.readthedocs.org/en/latest/)    Questions? We are on [gitter](https://gitter.im/allegro/hermes).    ## License    **hermes** is published under [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0). """
Big data;https://github.com/transceptor-technology/siridb-server;"""[![CI](https://github.com/SiriDB/siridb-server/workflows/CI/badge.svg)](https://github.com/SiriDB/siridb-server/actions)  [![Release Version](https://img.shields.io/github/release/SiriDB/siridb-server)](https://github.com/SiriDB/siridb-server/releases)    SiriDB Server  =============  SiriDB is a highly-scalable, robust and super fast time series database.    ---------------------------------------    * [Installation](#installation)      * [Ubuntu](#ubuntu)      * [Compile from source](#compile-from-source)        * [Linux](#linux)        * [OSX](#osx)        * [Docker](#docker)        * [Configuration](#configuration)      * [Build Debian package](#build-debian-package)      * [Run integration tests](#run-integration-tests)    * [Create or expand a database](#create-or-expand-a-database)    * [Using SiriDB](#using-siridb)      * [SiriDB Connectors](#siridb-connectors)      * [SiriDB HTTP](#siridb-http)      * [SiriDB Prompt](#siridb-prompt)      * [Grafana](#grafana)    * [API/Query language](#query-language)    ---------------------------------------    ## Installation  ### Ubuntu  For Ubuntu we have a deb package available which can be downloaded [here](https://github.com/SiriDB/siridb-server/releases/latest).    Note: SiriDB requires *libexpat1*, *libuv1*, *libpcre2-8-0* and *libcleri0* these libraries can be easily installed using apt:  ```  apt install libexpat1 libuv1 libpcre2-8-0 libcleri0  ```    >Library `libcleri0` is available from Ubuntu 18.04, for older versions a deb package can be found here:  >https://github.com/cesbit/libcleri/releases/latest    The .deb package installs a configuration file at `/etc/siridb/siridb.conf`. You might want to view or change this file before starting SiriDB.    ### Compile from source  >From version 2.0.19 libcleri is not included as part of this source anymore  >and needs to be installed separately. libcleri can be found here:  >[https://github.com/cesbit/libcleri](https://github.com/cesbit/libcleri)  >or can be installed using `apt`.    #### Linux  Install the following requirements: (Ubuntu 18.04)  ```  sudo apt install libcleri-dev  sudo apt install libpcre2-dev  sudo apt install libuv1-dev  sudo apt install libyajl-dev  sudo apt install uuid-dev  ```    Compile (replace Release with Debug for a debug build):  ```  cd ./Release  make clean  make test  make  ```    Install  ```  sudo make install  ```    #### OSX  >Make sure [libcleri](https://github.com/cesbit/libcleri) is installed!    Install the following requirements:  ```  brew install pcre2  brew install libuv  brew install yajl  brew install ossp-uuid  ```  Compile (replace Release with Debug for a debug build):  ```  cd ./Release  export CFLAGS=""-I/usr/local/include""  export LDFLAGS=""-L/usr/local/lib""  make clean  make test  make  ```    Install  ```  sudo make install  ```    #### Docker    ```bash  docker run \      -d \      -p 9000:9000 \      -p 9080:9080 \      -p 8080:8080 \      -v ~/siridb-data:/var/lib/siridb \      ghcr.io/siridb/siridb-server:latest      ```    #### Configuration  SiriDB accepts a configuration file or environment variable as configuration. By default SiriDB will search for the configuration file in `/etc/siridb/siridb.conf` but alternatively you can specify a custom path by using the `-c/--config` argument or use environment variable.    An example configuration file can be found here:  [https://github.com/SiriDB/siridb-server/blob/master/siridb.conf](https://github.com/SiriDB/siridb-server/blob/master/siridb.conf)    ### Build Debian package:    Install required packages (*autopkgtest is required for running the tests*)  ```  apt-get install devscripts lintian help2man autopkgtest  ```    Create archive  ```  git archive -o ../siridb-server_2.0.31.orig.tar.gz master  ```    Run tests  ```  autopkgtest -B -- null  ```    Build package  ```  debuild -us -uc  ```    ## Run integration tests  The simplest way to run the integration tests is to use [docker](https://docs.docker.com/install/).    Build integration test image  ```  docker build -t siridb/itest -f itest/Dockerfile .  ```    Run integration tests  ```  docker run siridb/itest:latest  ```    ## Create or expand a database  [SiriDB Admin](https://github.com/SiriDB/siridb-admin) can be used for creating a new database or expanding an existing database with a new server. Documentation on how to install and use the admin tool can be found at the [siridb-admin](https://github.com/SiriDB/siridb-admin#readme) github project. Binaries are available for most platforms and can be downloaded from [here](https://github.com/SiriDB/siridb-admin/releases/latest). As an alternative it is possible to use a simple [HTTP API](https://docs.siridb.net/connect/http_api/) for creating or expanding a SiriDB database.    ## Using SiriDB  SiriDB has several tools available to connect to a SiriDB database.    ### SiriDB Connectors  The following native connectors are available:   - [C/C++](https://github.com/SiriDB/libsiridb#readme)   - [Python](https://github.com/SiriDB/siridb-connector#readme)   - [Go](https://github.com/SiriDB/go-siridb-connector#readme)   - [Node.js](https://github.com/SiriDB/siridb-nodejs-addon#readme)    When not using one of the above, you can still communicate to SiriDB using [SiriDB HTTP](#siridb-http).    ### SiriDB HTTP  [SiriDB HTTP](https://github.com/SiriDB/siridb-http#readme) provides a HTTP API for SiriDB and has support for JSON, MsgPack, Qpack, CSV and Socket.io. SiriDB HTTP also has an optional web interface and SSL support.    ### SiriDB Prompt  [SiriDB Prompt](https://github.com/SiriDB/siridb-prompt#readme) can be used as a command line SiriDB client with auto-completion support and can be used to load json or csv data into a SiriDB database. Click [here](https://github.com/SiriDB/siridb-prompt/blob/master/README.md) for more information about SiriDB Prompt.    ### Grafana  [SiriDB Grafana Datasource](https://github.com/SiriDB/grafana-siridb-http-datasource#readme) is a plugin for Grafana. See the following blog article on how to configure and use this plugin: https://github.com/SiriDB/grafana-siridb-http-example.    ## Query language  Documentation about the query language can be found at https://siridb.net/documentation. """
Big data;https://github.com/twitter/heron;"""<!--      Licensed to the Apache Software Foundation (ASF) under one      or more contributor license agreements.  See the NOTICE file      distributed with this work for additional information      regarding copyright ownership.  The ASF licenses this file      to you under the Apache License, Version 2.0 (the      ""License""); you may not use this file except in compliance      with the License.  You may obtain a copy of the License at          http://www.apache.org/licenses/LICENSE-2.0        Unless required by applicable law or agreed to in writing,      software distributed under the License is distributed on an      ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY      KIND, either express or implied.  See the License for the      specific language governing permissions and limitations      under the License.  -->  [![Build Status](https://travis-ci.com/apache/incubator-heron.svg?&branch=master)](https://travis-ci.com/apache/incubator-heron)    ![logo](website2/docs/assets/HeronTextLogo.png)    Heron is a realtime analytics platform developed by Twitter.  It has a wide array of architectural improvements over it's predecessor.    [Heron in Apache Incubation](http://incubator.apache.org/projects/heron.html)    ### Documentation    [https://heron.incubator.apache.org/](https://heron.apache.org/)  </br>  Confluence: https://cwiki.apache.org/confluence/display/HERON    #### Heron Requirements:   * Java 11   * Python 3.6   * Bazel 4.2.2    ## Contact    #### Mailing lists    | Name                                                                      | Scope                           |                                                                |                                                                    |                                                                           |  |:--------------------------------------------------------------------------|:--------------------------------|:---------------------------------------------------------------|:-------------------------------------------------------------------|:--------------------------------------------------------------------------|  | [user@heron.incubator.apache.org](mailto:user@heron.incubator.apache.org) | User-related discussions        | [Subscribe](mailto:user-subscribe@heron.incubator.apache.org)  | [Unsubscribe](mailto:user-unsubscribe@heron.incubator.apache.org)  | [Archives](http://mail-archives.apache.org/mod_mbox/incubator-heron-user/)|  | [dev@heron.incubator.apache.org](mailto:dev@heron.incubator.apache.org)   | Development-related discussions | [Subscribe](mailto:dev-subscribe@heron.incubator.apache.org)   | [Unsubscribe](mailto:dev-unsubscribe@heron.incubator.apache.org)   | [Archives](http://mail-archives.apache.org/mod_mbox/incubator-heron-dev/) |    #### Slack    [Self-Register](http://heronstreaming.herokuapp.com/) to our [Heron Slack Workspace](https://heronstreaming.slack.com/)    #### Meetup Group    [Bay Area Heron Meetup](https://www.meetup.com/Apache-Heron-Bay-Area), *We meet on Third Monday of Every Month in Palo Alto.*    ## For more information:    * Official Heron documentation located at [https://heron.apache.org/](https://heron.apache.org/)  * Official Heron resources, including Conference & Journal Papers, Videos, Blog Posts and selected Press located at [Heron Resources](https://heron.apache.org/resources)  * [Twitter Heron: Stream Processing at Scale](http://dl.acm.org/citation.cfm?id=2742788) (academic paper)  * [Twitter Heron: Stream Processing at Scale](https://www.youtube.com/watch?v=pUaFOuGgmco) (YouTube video)  * [Flying Faster with Twitter Heron](https://blog.twitter.com/2015/flying-faster-with-twitter-heron) (blog post)    ## License    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/benedekrozemberczki/awesome-fraud-detection-papers;"""# Awesome Fraud Detection Research Papers.  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-fraud-detection-papers.svg)](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers/archive/master.zip)  ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-fraud-detection-papers.svg?color=blue) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)  <p align=""center"">    <img width=""450"" src=""fraud.png"">  </p>  A curated list of fraud detection papers from the following conferences:    - Network Science     * [ASONAM](http://asonam.cpsc.ucalgary.ca/2019/)     * [COMPLEX NETWORKS](https://www.complexnetworks.org/)  - Data Science     * [DSAA](http://dsaa2019.dsaa.co/)  - Natural Language Processing     * [ACL](http://www.acl2019.org/EN/index.xhtml)  - Data Mining     * [KDD](https://www.kdd.org/)     * [ICDM](http://icdm2019.bigke.org/)     * [SIGIR](https://sigir.org/)     * [SDM](https://www.siam.org/conferences/cm/conference/sdm20)     * [WWW](https://www2019.thewebconf.org/)     * [CIKM](http://www.cikmconference.org/)  - Artificial Intelligence     * [AAAI](https://www.aaai.org/)     * [AISTATS](http://www.auai.org/)     * [IJCAI](https://www.ijcai.org/)     * [UAI](http://www.auai.org/)  - Databases     * [VLDB](http://www.vldb.org/)    Similar collections about [graph classification](https://github.com/benedekrozemberczki/awesome-graph-classification), [classification/regression tree](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers), [Monte Carlo tree search](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers), and [community detection](https://github.com/benedekrozemberczki/awesome-community-detection) papers with implementations.      ## 2022  - **Active Learning for Human-in-the-loop Customs Inspection (TKDE 2022)**    - Sundong Kim, Tung-Duong Mai, Thi Nguyen Duc Khanh, Sungwon Han, Sungwon Park, Karandeep Singh, Meeyoung Cha    - [[Paper]](https://ieeexplore.ieee.org/document/9695316/)    - [[Code]](https://github.com/Seondong/Customs-Fraud-Detection)    - **Knowledge Sharing via Domain Adaptation in Customs Fraud Detection (AAAI 2022)**    - Sungwon Park, Sundong Kim, Meeyoung Cha    - [[Paper]](https://arxiv.org/abs/2201.06759)      ## 2021  - **Towards Consumer Loan Fraud Detection: Graph Neural Networks with Role-Constrained Conditional Random Field (AAAI 2021)**    - Bingbing Xu, Huawei Shen, Bing-Jie Sun, Rong An, Qi Cao, Xueqi Cheng    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/16582)    - **Modeling the Field Value Variations and Field Interactions Simultaneously for Fraud Detection (AAAI 2021)**    - Dongbo Xi, Bowen Song, Fuzhen Zhuang, Yongchun Zhu, Shuai Chen, Tianyi Zhang, Yuan Qi, Qing He    - [[Paper]](https://arxiv.org/abs/2008.05600)    - **IFDDS: An Anti-fraud Outbound Robot (AAAI 2021)**    - Zihao Wang, Minghui Yang, Chunxiang Jin, Jia Liu, Zujie Wen, Saishuai Liu, Zhe Zhang    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/18030)    - **Modeling Heterogeneous Graph Network on Fraud Detection: A Community-based Framework with Attention Mechanism (CIKM 2021)**    - Li Wang, Peipei Li, Kai Xiong, Jiashu Zhao, Rui Lin    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3482277)    - **Fraud Detection under Multi-Sourced Extremely Noisy Annotations (CIKM 2021)**    - Chuang Zhang, Qizhou Wang, Tengfei Liu, Xun Lu, Jin Hong, Bo Han, Chen Gong    - [[Paper]](https://gcatnjust.github.io/ChenGong/paper/zhang_cikm21.pdf)    - **Adversarial Reprogramming of Pretrained Neural Networks for Fraud Detection (CIKM 2021)**    - Lingwei Chen, Yujie Fan, Yanfang Ye    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3482053)    - **Fine-Grained Element Identification in Complaint Text of Internet Fraud (CIKM 2021)**    - Tong Liu, Siyuan Wang, Jingchao Fu, Lei Chen, Zhongyu Wei, Yaqi Liu, Heng Ye, Liaosa Xu, Weiqiang Wang, Xuanjing Huang    - [[Paper]](https://arxiv.org/abs/2108.08676)    - **Could You Describe the Reason for the Transfer: A Reinforcement Learning Based Voice-Enabled Bot Protecting Customers from Financial Frauds (CIKM 2021)**    - Zihao Wang, Fudong Wang, Haipeng Zhang, Minghui Yang, Shaosheng Cao, Zujie Wen, Zhe Zhang    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3481906)    - **Online Credit Payment Fraud Detection via Structure-Aware Hierarchical Recurrent Neural Network (IJCAI 2021)**    - Wangli Lin, Li Sun, Qiwei Zhong, Can Liu, Jinghua Feng, Xiang Ao, Hao Yang    - [[Paper]](https://www.ijcai.org/proceedings/2021/505)    - **Intention-aware Heterogeneous Graph Attention Networks for Fraud Transactions Detection (KDD 2021)**    - Can Liu, Li Sun, Xiang Ao, Jinghua Feng, Qing He, Hao Yang    - [[Paper]](https://dl.acm.org/doi/10.1145/3447548.3467142)    - **Live-Streaming Fraud Detection: A Heterogeneous Graph Neural Network Approach (KDD 2021)**    - Haishuai Wang, Zhao Li, Peng Zhang, Jiaming Huang, Pengrui Hui, Jian Liao, Ji Zhang, Jiajun Bu    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3447548.3467065)    - **Customs Fraud Detection in the Presence of Concept Drift (IncrLearn@ICDM 2021)**    - Tung-Duong Mai, Kien Hoang, Aitolkyn Baigutanova, Gaukhartas Alina, Sundong Kim    - [[Paper]](https://arxiv.org/abs/2109.14155)      - **Pick and Choose: A GNN-based Imbalanced Learning Approach for Fraud Detection (WWW 2021)**    - Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, Qing He    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3442381.3449989)    ## 2020      - **Spatio-Temporal Attention-Based Neural Network for Credit Card Fraud Detection (AAAI 2020)**    - Dawei Cheng, Sheng Xiang, Chencheng Shang, Yiyi Zhang, Fangzhou Yang, Liqing Zhang    - [[Paper]](https://aaai.org/Papers/AAAI/2020GB/AISI-ChengD.87.pdf)    - **FlowScope: Spotting Money Laundering Based on Graphs (AAAI 2020)**    - Xiangfeng Li, Shenghua Liu, Zifeng Li, Xiaotian Han, Chuan Shi, Bryan Hooi, He Huang, Xueqi Cheng    - [[Paper]](https://shenghua-liu.github.io/papers/aaai2020cr-flowscope.pdf)    - [[Code]](https://github.com/aplaceof/FlowScope)    - **Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters (CIKM 2020)**    - Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, Philip S. Yu    - [[Paper]](https://arxiv.org/abs/2008.08692)    - [[Code]](https://github.com/YingtongDou/CARE-GNN)    - **Loan Default Analysis with Multiplex Graph Learning (CIKM 2020)**    - Binbin Hu, Zhiqiang Zhang, Jun Zhou, Jingli Fang, Quanhui Jia, Yanming Fang, Quan Yu, Yuan Qi    - [[Paper]](https://www.researchgate.net/publication/343626706_Loan_Default_Analysis_with_Multiplex_Graph_Learning)    - **Error-Bounded Graph Anomaly Loss for GNNs (CIKM 2020)**    - Tong Zhao, Chuchen Deng, Kaifeng Yu, Tianwen Jiang, Daheng Wang, Meng Jiang    - [[Paper]](http://www.meng-jiang.com/pubs/gal-cikm20/gal-cikm20-paper.pdf)    - [[Code]](https://github.com/zhao-tong/Graph-Anomaly-Loss)      - **BotSpot: A Hybrid Learning Framework to Uncover Bot Install Fraud in Mobile Advertising (CIKM 2020)**    - Tianjun Yao, Qing Li, Shangsong Liang, Yadong Zhu    - [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3340531.3412690)    - [[Code]](https://github.com/akakeigo2020/CIKM-Applied_Research-2150)    - **Early Fraud Detection with Augmented Graph Learning (DLG@KDD 2020)**    - Tong Zhao, Bo Ni, Wenhao Yu, Meng Jiang    - [[Paper]](http://www.meng-jiang.com/pubs/earlyfraud-dlg20/earlyfraud-dlg20-paper.pdf)    - **NAG: Neural Feature Aggregation Framework for Credit Card Fraud Detection (ICDM 2020)**    - Kanishka Ghosh Dastidar, Johannes Jurgovsky, Wissam Siblini, Liyun He-Guelton, Michael Granitzer    - [[Paper]](https://www.computer.org/csdl/proceedings-article/icdm/2020/831600a092/1r54A3Sb2yk)    - **Heterogeneous Mini-Graph Neural Network and Its Application to Fraud Invitation Detection (ICDM 2020)**    - Yong-Nan Zhu, Xiaotian Luo, Yu-Feng Li, Bin Bu, Kaibo Zhou, Wenbin Zhang, Mingfan Lu    - [[Paper]](https://cs.nju.edu.cn/liyf/paper/icdm20-hmgnn.pdf)      - **Collaboration Based Multi-Label Propagation for Fraud Detection (IJCAI 2020)**    - Haobo Wang, Zhao Li, Jiaming Huang, Pengrui Hui, Weiwei Liu, Tianlei Hu, Gang Chen    - [[Paper]](https://www.ijcai.org/Proceedings/2020/343)    - **The Behavioral Sign of Account Theft: Realizing Online Payment Fraud Alert (IJCAI 2020)**    - Cheng Wang    - [[Paper]](https://www.ijcai.org/Proceedings/2020/0636.pdf)    - **Federated Meta-Learning for Fraudulent Credit Card Detection (IJCAI 2020)**    - Wenbo Zheng, Lan Yan, Chao Gou, Fei-Yue Wang    - [[Paper]](https://www.ijcai.org/Proceedings/2020/642)    - **Robust Spammer Detection by Nash Reinforcement Learning (KDD 2020)**    - Yingtong Dou, Guixiang Ma, Philip S. Yu, Sihong Xie    - [[Paper]](https://arxiv.org/abs/2006.06069)    - [[Code]](https://github.com/YingtongDou/Nash-Detect)      - **DATE: Dual Attentive Tree-aware Embedding for Customs Fraud Detection (KDD 2020)**    - Sundong Kim, Yu-Che Tsai, Karandeep Singh, Yeonsoo Choi, Etim Ibok, Cheng-Te Li, Meeyoung Cha    - [[Paper]](https://seondong.github.io/assets/papers/2020_KDD_DATE.pdf)    - [[Code]](https://github.com/Roytsai27/Dual-Attentive-Tree-aware-Embedding)    - **Fraud Transactions Detection via Behavior Tree with Local Intention Calibration (KDD 2020)**    - Can Liu, Qiwei Zhong, Xiang Ao, Li Sun, Wangli Lin, Jinghua Feng, Qing He, Jiayu Tang    - [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3394486.3403354)    - **Interleaved Sequence RNNs for Fraud Detection (KDD 2020)**    - Bernardo Branco, Pedro Abreu, Ana Sofia Gomes, Mariana S. C. Almeida, João Tiago Ascensão, Pedro Bizarro    - [[Paper]](https://arxiv.org/abs/2002.05988)    - **GCN-Based User Representation Learning for Unifying Robust Recommendation and Fraudster Detection (SIGIR 2020)**    - Shijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, Lizhen Cui    - [[Paper]](https://arxiv.org/abs/2005.10150)     - **Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection (SIGIR 2020)**    - Zhiwei Liu, Yingtong Dou, Philip S. Yu, Yutong Deng, Hao Peng    - [[Paper]](https://arxiv.org/abs/2005.00625)    - [[Code]](https://github.com/safe-graph/DGFraud)      - **Friend or Faux: Graph-Based Early Detection of Fake Accounts on Social Networks (WWW 2020)**    - Adam Breuer, Roee Eilat, Udi Weinsberg    - [[Paper]](https://arxiv.org/abs/2004.04834)    - **Financial Defaulter Detection on Online Credit Payment via Multi-view Attributed Heterogeneous Information Network (WWW 2020)**    - Qiwei Zhong, Yang Liu, Xiang Ao, Binbin Hu, Jinghua Feng, Jiayu Tang, Qing He    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3366423.3380159)    - **ASA: Adversary Situation Awareness via Heterogeneous Graph Convolutional Networks (WWW 2020)**    - Rui Wen, Jianyu Wang, Chunming Wu, Jian Xiong    - [[Paper]](https://dl.acm.org/doi/10.1145/3366424.3391266)      - **Modeling Users' Behavior Sequences with Hierarchical Explainable Network for Cross-domain Fraud Detection (WWW 2020)**    - Yongchun Zhu, Dongbo Xi, Bowen Song, Fuzhen Zhuang, Shuai Chen, Xi Gu, Qing He    - [[Paper]](https://dl.acm.org/doi/fullHtml/10.1145/3366423.3380172)      ## 2019  - **SliceNDice: Mining Suspicious Multi-attribute Entity Groups with Multi-view Graphs (DSAA 2019)**    - Hamed Nilforoshan, Neil Shah    - [[Paper]](https://arxiv.org/abs/1908.07087)    - [[Code]](https://github.com/hamedn/SliceNDice)    - **FARE: Schema-Agnostic Anomaly Detection in Social Event Logs (DSAA 2019)**    - Neil Shah    - [[Paper]](http://nshah.net/publications/FARE.DSAA.19.pdf)      - **Cash-Out User Detection Based on Attributed Heterogeneous Information Network with a Hierarchical Attention Mechanism (AAAI 2019)**    - Binbin Hu, Zhiqiang Zhang, Chuan Shi, Jun Zhou, Xiaolong Li, Yuan Qi    - [[Paper]](https://aaai.org/ojs/index.php/AAAI/article/view/3884)    - [[Code]](https://github.com/safe-graph/DGFraud)      - **GeniePath: Graph Neural Networks with Adaptive Receptive Paths (AAAI 2019)**    - Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, Yuan Qi    - [[Paper]](https://arxiv.org/abs/1802.00910)    - [[Code]](https://github.com/safe-graph/DGFraud)      - **SAFE: A Neural Survival Analysis Model for Fraud Early Detection (AAAI 2019)**    - Panpan Zheng, Shuhan Yuan, Xintao Wu    - [[Paper]](https://arxiv.org/abs/1809.04683v2)    - [[Code]](https://github.com/PanpanZheng/SAFE)    - **One-Class Adversarial Nets for Fraud Detection (AAAI 2019)**    - Panpan Zheng, Shuhan Yuan, Xintao Wu, Jun Li, Aidong Lu    - [[Paper]](https://arxiv.org/abs/1803.01798)    - [[Code]](https://github.com/ILoveAI2019/OCAN)      - **Uncovering Download Fraud Activities in Mobile App Markets (ASONAM 2019)**    - Yingtong Dou, Weijian Li, Zhirong Liu, Zhenhua Dong, Jiebo Luo, Philip S. Yu    - [[Paper]](https://arxiv.org/pdf/1907.03048.pdf)    - **Spam Review Detection with Graph Convolutional Networks (CIKM 2019)**    - Ao Li, Zhou Qin, Runshi Liu, Yiqun Yang, Dong Li    - [[Paper]](https://arxiv.org/abs/1908.10679)    - [[Code]](https://github.com/safe-graph/DGFraud)    - **Key Player Identification in Underground Forums Over Attributed Heterogeneous Information Network Embedding Framework (CIKM 2019)**     - Yiming Zhang, Yujie Fan, Yanfang Ye, Liang Zhao, Chuan Shi     - [[Paper]](http://mason.gmu.edu/~lzhao9/materials/papers/lp0110-zhangA.pdf)     - [[Code]](https://github.com/safe-graph/DGFraud)       - **CatchCore: Catching Hierarchical Dense Subtensor (ECML-PKDD 2019)**    -  Wenjie Feng, Shenghua Liu, Huawei Shen, and Xueqi Cheng    - [[Paper]](https://shenghua-liu.github.io/papers/pkdd2019-catchcore.pdf)    - [[Code]](https://github.com/wenchieh/catchcore)      - **Spotting Collective Behaviour of Online Frauds in Customer Reviews (IJCAI 2019)**    - Sarthika Dhawan, Siva Charan Reddy Gangireddy, Shiv Kumar, Tanmoy Chakraborty    - [[Paper]](https://arxiv.org/abs/1905.13649)    - [[Code]](https://github.com/LCS2-IIITD/DeFrauder)      - **A Semi-Supervised Graph Attentive Network for Fraud Detection (ICDM 2019)**    - Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang, Quan Yu, Jun Zhou, Shuang Yang, and Qi Yuan     - [[Paper]](https://arxiv.org/abs/2003.01171)    - [[Code]](https://github.com/safe-graph/DGFraud)     - **EigenPulse: Detecting Surges in Large Streaming Graphs with Row Augmentation (PAKDD 2019)**    - Jiabao Zhang, Shenghua Liu, Wenjian Yu, Wenjie Feng, Xueqi Cheng    - [[Paper]](https://shenghua-liu.github.io/papers/pakdd2019-eigenpulse.pdf)    - **Uncovering Insurance Fraud Conspiracy with Network Learning (SIGIR 2019)**    - Chen Liang, Ziqi Liu, Bin Liu, Jun Zhou, Xiaolong Li, Shuang Yang, Yuan Qi    - [[Paper]](https://dl.acm.org/citation.cfm?id=3331372)      - **A Contrast Metric for Fraud Detection in Rich Graphs (TKDE 2019)**    - Shenghua Liu, Bryan Hooi, Christos Faloutsos    - [[Paper]](https://shenghua-liu.github.io/papers/tkde2019-constrastsusp_holoscope.pdf)    - **Think Outside the Dataset: Finding Fraudulent Reviews using Cross-Dataset Analysis (WWW 2019)**    - Shirin Nilizadeh, Hojjat Aghakhani, Eric Gustafson, Christopher Kruegel, Giovanni Vigna    - [[Paper]](https://www.researchgate.net/publication/333060486_Think_Outside_the_Dataset_Finding_Fraudulent_Reviews_using_Cross-Dataset_Analysis)    - **Securing the Deep Fraud Detector in Large-Scale E-Commerce Platform via Adversarial Machine Learning Approach (WWW 2019)**    - Qingyu Guo, Zhao Li, Bo An, Pengrui Hui, Jiaming Huang, Long Zhang, Mengchen Zhao    - [[Paper]](https://www.ntu.edu.sg/home/boan/papers/WWW19.pdf)    - **No Place to Hide: Catching Fraudulent Entities in Tensors (WWW 2019)**    - Yikun Ban, Xin Liu, Ling Huang, Yitao Duan, Xue Liu, Wei Xu    - [[Paper]](https://arxiv.org/pdf/1810.06230.pdf)      - **FdGars: Fraudster Detection via Graph Convolutional Networks in Online App Review System (WWW 2019)**    - Rui Wen, Jianyu Wang and Yu Huang    - [[Paper]](https://dl.acm.org/citation.cfm?id=3316586)    - [[Code]](https://github.com/safe-graph/DGFraud)    ## 2018    - **Heterogeneous Graph Neural Networks for Malicious Account Detection (CIKM 2018)**    - Ziqi Liu, Chaochao Chen, Xinxing Yang, Jun Zhou, Xiaolong Li, and Le Song    - [[Paper]](https://dl.acm.org/doi/10.1145/3269206.3272010)    - [[Code]](https://github.com/safe-graph/DGFraud)      - **Reinforcement Mechanism Design for Fraudulent Behaviour in e-Commerce (AAAI 2018)**    - Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, Yiwei Zhang    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16650)      - **Adapting to Concept Drift in Credit Card Transaction Data Streams Using Contextual Bandits and Decision Trees (AAAI 2018)**    - Dennis J. N. J. Soemers, Tim Brys, Kurt Driessens, Mark H. M. Winands, Ann Nowé    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16183/16394)    - **Nextgen AML: Distributed Deep Learning Based Language Technologies to Augment Anti Money Laundering Investigation(ACL 2018)**    - Jingguang Han, Utsab Barman, Jeremiah Hayes, Jinhua Du, Edward Burgin, Dadong Wan    - [[Paper]](https://www.aclweb.org/anthology/P18-4007)    - **Preserving Privacy of Fraud Detection Rule Sharing Using Intel's SGX (CIKM 2018)**    - Daniel Deutch, Yehonatan Ginzberg, Tova Milo    - [[Paper]](https://www.researchgate.net/publication/328439345_Preserving_Privacy_of_Fraud_Detection_Rule_Sharing_Using_Intel%27s_SGX)    - **Deep Structure Learning for Fraud Detection (ICDM 2018)**    - Haibo Wang, Chuan Zhou, Jia Wu, Weizhen Dang, Xingquan Zhu, Jilong Wang    - [[Paper]](https://www.researchgate.net/publication/330030140_Deep_Structure_Learning_for_Fraud_Detection)    - **Learning Sequential Behavior Representations for Fraud Detection (ICDM 2018)**    - Jia Guo, Guannan Liu, Yuan Zuo, Junjie Wu    - [[Paper]](https://www.researchgate.net/publication/330028902_Learning_Sequential_Behavior_Representations_for_Fraud_Detection)    - **Impression Allocation for Combating Fraud in E-commerce Via Deep Reinforcement Learning with Action Norm Penalty (IJCAI 2018)**    - Mengchen Zhao, Zhao Li, Bo An, Haifeng Lu, Yifan Yang, Chen Chu    - [[Paper]](https://www.ijcai.org/proceedings/2018/0548.pdf)    - **Tax Fraud Detection for Under-Reporting Declarations Using an Unsupervised Machine Learning Approach (KDD 2018)**    - Daniel de Roux, Boris Perez, Andrés Moreno, María-Del-Pilar Villamil, César Figueroa    - [[Paper]](https://www.kdd.org/kdd2018/accepted-papers/view/tax-fraud-detection-for-under-reporting-declarations-using-an-unsupervised-)      - **Collective Fraud Detection Capturing Inter-Transaction Dependency (KDD 2018)**    - Bokai Cao, Mia Mao, Siim Viidu, Philip Yu    - [[Paper]](http://proceedings.mlr.press/v71/cao18a.html)      - **Fraud Detection with Density Estimation Trees (KDD 2018)**    - Fraud Detection with Density Estimation Trees    - [[Paper]](http://proceedings.mlr.press/v71/ram18a/ram18a.pdf)    - **Real-time Constrained Cycle Detection in Large Dynamic Graphs (VLDB 2018)**    - Xiafei Qiu, Wubin Cen, Zhengping Qian, You Peng, Ying Zhang, Xuemin Lin, Jingren Zhou    - [[Paper]](http://www.vldb.org/pvldb/vol11/p1876-qiu.pdf)      - **REV2: Fraudulent User Prediction in Rating Platforms (WSDM 2018)**    - Srijan Kumar, Bryan Hooi, Disha Makhija, Mohit Kumar, Christos Faloutsos, V. S. Subrahmanian    - [[Paper]](https://cs.stanford.edu/~srijan/pubs/rev2-wsdm18.pdf)    - [[Code]](https://cs.stanford.edu/~srijan/rev2/)      - **Exposing Search and Advertisement Abuse Tactics and Infrastructure of Technical Support Scammers (WWW 2018)**    - Bharat Srinivasan, Athanasios Kountouras, Najmeh Miramirkhani, Monjur Alam, Nick Nikiforakis, Manos Antonakakis, Mustaque Ahamad    - [[Paper]](https://www.securitee.org/files/tss_www2018.pdf)    ## 2017  - **ZooBP: Belief Propagation for Heterogeneous Networks (VLDB 2017)**    - Dhivya Eswaran, Stephan Gunnemann, Christos Faloutsos, Disha Makhija, Mohit Kumar    - [[Paper]](http://www.vldb.org/pvldb/vol10/p625-eswaran.pdf)    - [[Code]](https://github.com/safe-graph/UGFraud)    - **Behavioral Analysis of Review Fraud: Linking Malicious Crowdsourcing to Amazon and Beyond (AAAI 2017)**    - Parisa Kaghazgaran, James Caverlee, Majid Alfifi    - [[Paper]](https://aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15659)      - **Detection of Money Laundering Groups: Supervised Learning on Small Networks (AAAI 2017)**    - David Savage, Qingmai Wang, Xiuzhen Zhang, Pauline Chou, Xinghuo Yu    - [[Paper]](https://arxiv.org/pdf/1608.00708.pdf)      - **Spectrum-based Deep Neural Networks for Fraud Detection (CIKM 2017)**    - Shuhan Yuan, Xintao Wu, Jun Li, Aidong Lu    - [[Paper]](https://arxiv.org/abs/1706.00891)    - **HoloScope: Topology-and-Spike Aware Fraud Detection (CIKM 2017)**    - Shenghua Liu, Bryan Hooi, Christos Faloutsos    - [[Paper]](https://arxiv.org/abs/1705.02505)    - **The Many Faces of Link Fraud (ICDM 2017)**    - Neil Shah, Hemank Lamba, Alex Beutel, Christos Faloutsos    - [[Paper]](https://arxiv.org/abs/1704.01420)    - **HitFraud: A Broad Learning Approach for Collective Fraud Detection in Heterogeneous Information Networks (ICDM 2017)**    - Bokai Cao, Mia Mao, Siim Viidu, Philip S. Yu    - [[Paper]](https://arxiv.org/abs/1709.04129)      - **GANG: Detecting Fraudulent Users in Online Social Networks via Guilt-by-Association on Directed Graphs (ICDM 2017)**    - Binghui Wang, Neil Zhenqiang Gong, Hao Fu    - [[Paper]](https://ieeexplore.ieee.org/document/8215519)    - [[Code]](https://github.com/safe-graph/UGFraud)      - **Improving Card Fraud Detection Through Suspicious Pattern Discovery (IEA/AIE 2017)**    - Fabian Braun, Olivier Caelen, Evgueni N. Smirnov, Steven Kelk, Bertrand Lebichot:    - [[Paper]](http://www.oliviercaelen.be/doc/GBSSCCFDS.pdf)    - **Online Reputation Fraud Campaign Detection in User Ratings (IJCAI 2017)**    - Chang Xu, Jie Zhang, Zhu Sun    - [[Paper]](https://www.ijcai.org/proceedings/2017/0541.pdf)      - **Uncovering Unknown Unknowns in Financial Services Big Data by Unsupervised Methodologies: Present and Future trends (KDD 2017)**    - Gil Shabat, David Segev, Amir Averbuch    - [[Paper]](http://proceedings.mlr.press/v71/shabat18a.html)      - **PD-FDS: Purchase Density based Online Credit Card Fraud Detection System (KDD 2017)**    - Youngjoon Ki, Ji Won Yoon     - [[Paper]](http://proceedings.mlr.press/v71/ki18a/ki18a.pdf)      - **HiDDen: Hierarchical Dense Subgraph Detection with Application to Financial Fraud Detection (SDM 2017)**    - Si Zhang, Dawei Zhou, Mehmet Yigit Yildirim, Scott Alcorn, Jingrui He, Hasan Davulcu, Hanghang Tong    - [[Paper]](http://www.public.asu.edu/~hdavulcu/SDM17.pdf)    ## 2016  - **A Fraud Resilient Medical Insurance Claim System (AAAI 2016)**    - Yuliang Shi, Chenfei Sun, Qingzhong Li, Lizhen Cui, Han Yu, Chunyan Miao    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11813)    - **A Graph-Based, Semi-Supervised, Credit Card Fraud Detection System (COMPLEX NETWORKS 2016)**    - Bertrand Lebichot, Fabian Braun, Olivier Caelen, Marco Saerens    - [[Paper]](http://www.oliviercaelen.be/doc/IEAAIE_2017_Finalversion-PDF_39.pdf)    - **FRAUDAR: Bounding Graph Fraud in the Face of Camouflage (KDD 2016)**    - Bryan Hooi, Hyun Ah Song, Alex Beutel, Neil Shah, Kijung Shin, Christos Faloutsos    - [[Paper]](https://www.andrew.cmu.edu/user/bhooi/papers/fraudar_kdd16.pdf)    - [[Code]](https://github.com/safe-graph/UGFraud)      - **Identifying Anomalies in Graph Streams Using Change Detection (KDD 2016)**    - William Eberle and Lawrence Holde    - [[Paper]](http://www.mlgworkshop.org/2016/paper/MLG2016_paper_12.pdf)    - **FairPlay: Fraud and Malware Detection in Google Play (SDM 2016)**    - Mahmudur Rahman, Mizanur Rahman, Bogdan Carbunar, Duen Horng Chau    - [[Paper]](https://arxiv.org/abs/1703.02002)    - **BIRDNEST: Bayesian Inference for Ratings-Fraud Detection (SDM 2016)**    - Bryan Hooi, Neil Shah, Alex Beutel, Stephan Günnemann, Leman Akoglu, Mohit Kumar, Disha Makhija, Christos Faloutsos    - [[Paper]](https://www.andrew.cmu.edu/user/bhooi/papers/birdnest_sdm16.pdf)    - **Understanding the Detection of View Fraud in Video Content Portals (WWW 2016)**    - Miriam Marciel, Rubén Cuevas, Albert Banchs, Roberto Gonzalez, Stefano Traverso, Mohamed Ahmed, Arturo Azcorra    - [[Paper]](https://dl.acm.org/citation.cfm?id=2882980)    ## 2015  - **Toward An Intelligent Agent for Fraud Detection — The CFE Agent (AAAI 2015)**    - Joe Johnson    - [[Paper]](https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/download/11664/11485)      - **Graph Analysis for Detecting Fraud, Waste, and Abuse in Healthcare Data (AAAI 2015)**    - Juan Liu, Eric Bier, Aaron Wilson, Tomonori Honda, Kumar Sricharan, Leilani Gilpin, John Alexis Guerra Gómez, Daniel Davies    - [[Paper]](https://pdfs.semanticscholar.org/1ea7/125b789ef938bffe10c7588e6b071c4ff73c.pdf)    - **Robust System for Identifying Procurement Fraud (AAAI 2015)**    - Amit Dhurandhar, Rajesh Kumar Ravi, Bruce Graves, Gopikrishnan Maniachari, Markus Ettl    - [[Paper]](https://pdfs.semanticscholar.org/27af/c9ec453ae0cf9e55f4032ff688cb70c2a61e.pdf)    - **Fraud Transaction Recognition: A Money Flow Network Approach (CIKM 2015)**    - Renxin Mao, Zhao Li, Jinhua Fu    - [[Paper]](https://dl.acm.org/citation.cfm?id=2806647)    - **Towards Collusive Fraud Detection in Online Reviews (ICDM 2015)**    - Chang Xu, Jie Zhang    - [[Paper]](https://ieeexplore.ieee.org/document/7373434)    - **Catch the Black Sheep: Unified Framework for Shilling Attack Detection Based on Fraudulent Action Propagation (IJCAI 2015)**    - Yongfeng Zhang, Yunzhi Tan, Min Zhang, Yiqun Liu, Tat-Seng Chua, Shaoping Ma    - [[Paper]](https://www.ijcai.org/Proceedings/15/Papers/341.pdf)    - **Collective Opinion Spam Detection: Bridging Review Networks and Metadata (KDD 2015)**    - Shebuti Rayana, Leman Akoglu    - [[Paper]](https://www.andrew.cmu.edu/user/lakoglu/pubs/15-kdd-collectiveopinionspam.pdf)    - [[Code]](https://github.com/safe-graph/UGFraud)    - **Graph-Based User Behavior Modeling: From Prediction to Fraud Detection (KDD 2015)**    - Alex Beutel, Leman Akoglu, Christos Faloutsos    - [[Paper]](https://www.cs.cmu.edu/~abeutel/kdd2015_tutorial/tutorial.pdf)    - **FrauDetector: A Graph-Mining-based Framework for Fraudulent Phone Call Detection (KDD 2015)**    - Vincent S. Tseng, Jia-Ching Ying, Che-Wei Huang, Yimin Kao, Kuan-Ta Chen    - [[Paper]](http://repository.ncku.edu.tw/bitstream/987654321/166322/1/4010204000-000004_1.pdf)      - **A Framework for Intrusion Detection Based on Frequent Subgraph Mining (SDM 2015)**    - Vitali Herrera-Semenets, Niusvel Acosta-Mendoza, Andres Gago-Alonso    - [[Paper]](https://www.researchgate.net/publication/271839253_A_Framework_for_Intrusion_Detection_based_on_Frequent_Subgraph_Mining)    - **Crowd Fraud Detection in Internet Advertising (WWW 2015)**    - Tian Tian, Jun Zhu, Fen Xia, Xin Zhuang, Tong Zhang    - [[Paper]](http://www.www2015.it/documents/proceedings/proceedings/p1100.pdf)    ## 2014  - **Spotting Suspicious Link Behavior with fBox: An Adversarial Perspective (ICDM 2014)**    - Neil Shah, Alex Beutel, Brian Gallagher, Christos Faloutsos    - [[Paper]](https://arxiv.org/pdf/1410.3915.pdf)    - [[Code]](https://github.com/safe-graph/UGFraud)    - **Fraudulent Support Telephone Number Identification Based on Co-Occurrence Information on the Web (AAAI 2014)**    - Xin Li, Yiqun Liu, Min Zhang, Shaoping Ma    - [[Paper]](https://pdfs.semanticscholar.org/2733/1f48c87736ea12b9edec062e384d3bd58f88.pdf)    - **Corporate Residence Fraud Detection (KDD 2014)**    - Enric Junqué de Fortuny, Marija Stankova, Julie Moeyersoms, Bart Minnaert, Foster J. Provost, David Martens    - [[Paper]](http://delivery.acm.org/10.1145/2630000/2623333/p1650-fortuny.pdf?ip=129.215.164.203&id=2623333&acc=ACTIVE%20SERVICE&key=C2D842D97AC95F7A%2EEB9E991028F4E1F1%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1559048806_f1a6f763ef7088a4fb4b1a4ff94856f8)    - **Graphical Models for Identifying Fraud and Waste in Healthcare Claims (SDM 2014)**    - Peder A. Olsen, Ramesh Natarajan, Sholom M. Weiss    - [[Paper]](https://epubs.siam.org/doi/abs/10.1137/1.9781611973440.66)    - **Improving Credit Card Fraud Detection with Calibrated Probabilities (SDM 2014)**    - Alejandro Correa Bahnsen, Aleksandar Stojanovic, Djamila Aouada, Björn E. Ottersten    - [[Paper]](https://pdfs.semanticscholar.org/9241/ef2a2f6638eafeffd0056736c0f46f9aa083.pdf)    - **Large Graph Mining: Patterns, Cascades, Fraud Detection, and Algorithms (WWW 2014)**    - Christos Faloutsos    - [[Paper]](http://wwwconference.org/proceedings/www2014/proceedings/p1.pdf)      ## 2013    - **Opinion Fraud Detection in Online Reviews by Network Effects (AAAI 2013)**    - Leman Akoglu, Rishi Chandy, Christos Faloutsos    - [[Paper]](https://www.researchgate.net/publication/279905898_Opinion_fraud_detection_in_online_reviews_by_network_effects)      - **Using Social Network Knowledge for Detecting Spider Constructions in Social Security Fraud (ASONAM 2013)**    - Véronique Van Vlasselaer, Jan Meskens, Dries Van Dromme, Bart Baesens     - [[Paper]](https://ieeexplore.ieee.org/document/6785796)      - **Ranking Fraud Detection for Mobile Apps: a Holistic View (CIKM 2013)**    - Hengshu Zhu, Hui Xiong, Yong Ge, Enhong Chen    - [[Paper]](http://dm.ustc.edu.cn/zhu-cikm13.pdf)    - **Using Co-Visitation Networks for Detecting Large Scale Online Display Advertising Exchange Fraud (KDD 2013)**    - Ori Stitelman, Claudia Perlich, Brian Dalessandro, Rod Hook, Troy Raeder, Foster J. Provost    - [[Paper]](http://chbrown.github.io/kdd-2013-usb/kdd/p1240.pdf)    - **Adaptive Adversaries: Building Systems to Fight Fraud and Cyber Intruders (KDD 2013)**    - Ari Gesher    - [[Paper]](https://dl.acm.org/citation.cfm?id=2491134)    - **Anomaly, Event, and Fraud Detection in Large Network Datasets (WSDM 2013)**    - Leman Akoglu, Christos Faloutsos    - [[Paper]](https://www.andrew.cmu.edu/user/lakoglu/wsdm13/13-wsdm-tutorial.pdf)    ## 2012    - **Fraud Detection: Methods of Analysis for Hypergraph Data (ASONAM 2012)**    - Anna Leontjeva, Konstantin Tretyakov, Jaak Vilo, and Taavi Tamkivi    - [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6425618)    - **Online Modeling of Proactive Moderation System for Auction Fraud Detection (WWW 2012)**    - Liang Zhang, Jie Yang, Belle L. Tseng    - [[Paper]](http://www.chennaisunday.com/Java%202012%20Base%20Paper/Online%20Modeling%20of%20Proactive%20Moderation%20System%20for%20Auction%20Fraud%20Detection.pdf)    ## 2011  - **A Machine-Learned Proactive Moderation System for Auction Fraud Detection (CIKM 2011)**    - Liang Zhang, Jie Yang, Wei Chu, Belle L. Tseng    - [[Paper]](http://www.gatsby.ucl.ac.uk/~chuwei/paper/p2501-zhang.pdf)    - **A Taxi Driving Fraud Detection System (ICDM 2011)**    - Yong Ge, Hui Xiong, Chuanren Liu, Zhi-Hua Zhou    - [[Paper]](https://ieeexplore.ieee.org/document/6137222)    - **Utility-Based Fraud Detection (IJCAI 2011)**    - Luís Torgo, Elsa Lopes    - [[Paper]](https://www.ijcai.org/Proceedings/11/Papers/255.pdf)    - **A Pattern Discovery Approach to Retail Fraud Detection (KDD 2011)**    - Prasad Gabbur, Sharath Pankanti, Quanfu Fan, Hoang Trinh    - [[Paper]](http://www2.engr.arizona.edu/~pgsangam/gabbur_kdd_11.pdf)    ## 2010    - **Hunting for the Black Swan: Risk Mining from Text (ACL 2010)**    - JL Leidner, F Schilder    - [[Paper]](https://www.aclweb.org/anthology/P10-4010)      - **Fraud Detection by Generating Positive Samples for Classification from Unlabeled Data (ACL 2010)**    - Levente Kocsis, Andras George    - [[Paper]](http://www.szit.bme.hu/~gya/publications/KocsisGyorgy.pdf)      ## 2009  - **SVM-based Credit Card Fraud Detection with Reject Cost and Class-Dependent Error Cost (PAKDD 2009)**    - En-hui Zheng,Chao Zou,Jian Sun, Le Chen    - [[Paper]](https://www.semanticscholar.org/paper/SVM-Based-Cost-sensitive-Classification-Algorithm-Zheng-Zou/bcae06626ccd453925ef040a1edb5cbb10b862ef)      - **An Approach for Automatic Fraud Detection in the Insurance Domain (AAAI 2009)**    - Alexander Widder, Rainer v. Ammon, Gerit Hagemann, Dirk Schönfeld    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.325.3231&rep=rep1&type=pdf)      ## 2007  - **Relational Data Pre-Processing Techniques for Improved Securities Fraud Detection (KDD 2007)**    - Andrew S. Fast, Lisa Friedland, Marc E. Maier, Brian J. Taylor, David D. Jensen, Henry G. Goldberg, John Komoroske    - [[Paper]](https://dl.acm.org/citation.cfm?id=1281192.1281293)    - **Uncovering Fraud in Direct Marketing Data with a Fraud Auditing Case Builder (PKDD 2007)**    - Fletcher Lu    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-74976-9_56)    - **Netprobe: A Fast and Scalable System for Fraud Detection in Online Auction Networks (WWW 2007)**    - Shashank Pandit, Duen Horng Chau, Samuel Wang, Christos Faloutsos    - [[Paper]](http://www.cs.cmu.edu/~christos/PUBLICATIONS/netprobe-www07.pdf)    ## 2006    - **Data Mining Approaches to Criminal Career Analysis (ICDM 2006)**    - Jeroen S. De Bruin, Tim K. Cocx, Walter A. Kosters, Jeroen F. J. Laros, Joost N. Kok     - [[Paper]](https://ieeexplore.ieee.org/document/4053045)      - **Large Scale Detection of Irregularities in Accounting Data (ICDM 2006)**    - Stephen Bay, Krishna Kumaraswamy, Markus G. Anderle, Rohit Kumar, David M. Steier     - [[Paper]](https://ieeexplore.ieee.org/document/4053036)      - **Camouflaged Fraud Detection in Domains with Complex Relationships (KDD 2006)**    - Sankar Virdhagriswaran, Gordon Dakin    - [[Paper]](https://dl.acm.org/citation.cfm?id=1150532)    - **Detecting Fraudulent Personalities in Networks of Online Auctioneers (PKDD 2006)**    - Duen Horng Chau, Shashank Pandit, Christos Faloutsos    - [[Paper]](http://www.cs.cmu.edu/~dchau/papers/auction_fraud_pkdd06.pdf)    ## 2005    - **Technologies to Defeat Fraudulent Schemes Related to Email Requests (AAAI 2005)**    - Edoardo Airoldi, Bradley Malin, and Latanya Sweeney    - [[Paper]](http://www.aaai.org/Library/Symposia/Spring/2005/ss05-01-023.php)      - **AI Technologies to Defeat Identity Theft Vulnerabilities (AAAI 2005)**    - Latanya Sweeney    - [[Paper]](https://dataprivacylab.org/dataprivacy/projects/idangel/paper1.pdf)      - **Detecting Fraud in Health Insurance Data: Learning to Model Incomplete Benford's Law Distributions (ECML 2005)**    - Fletcher Lu, J. Efrim Boritz    - [[Paper]](https://faculty.uoit.ca/fletcherlu/LuECML05.pdf)    - **Using Relational Knowledge Discovery to Prevent Securities Fraud (KDD 2005)**    - Jennifer Neville, Özgür Simsek, David D. Jensen, John Komoroske, Kelly Palmer, Henry G. Goldberg    - [[Paper]](https://www.cs.purdue.edu/homes/neville/papers/neville-et-al-kdd2005.pdf)    ## 2003  - **Applying Data Mining in Investigating Money Laundering Crimes (KDD 2003)**    - Zhongfei (Mark) Zhang, John J. Salerno, Philip S. Yu    - [[Paper]](https://pdfs.semanticscholar.org/9124/b61d48b7e52008c7fd5fac1b7eac38474581.pdf)    ## 2000  - **Document Classification and Visualisation to Support the Investigation of Suspected Fraud (PKDD 2000)**    - Johan Hagman, Domenico Perrotta, Ralf Steinberger, and Aristi de Varfis    - [[Paper]](https://pdfs.semanticscholar.org/9124/b61d48b7e52008c7fd5fac1b7eac38474581.pdf)      ## 1999  - **Statistical Challenges to Inductive Inference in Linked Data. (AISTATS 1999)**    - David Jensen    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.589.1445&rep=rep1&type=pdf)    ## 1998    - **Toward Scalable Learning with Non-Uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection (KDD 1998)**    - Phillip K Chan, Salvatore J Stolfo     - [[Paper]](https://pdfs.semanticscholar.org/6e19/3366945bf3bd72d5ba906e3982ac4d8ae874.pdf)      - **Call-Based Fraud Detection in Mobile Communication Networks Using a Hierarchical Regime-Switching Model (NIPS 1998)**    - Jaakko Hollmén, Volker Tresp    - [[Paper]](https://papers.nips.cc/paper/1505-call-based-fraud-detection-in-mobile-communication-networks-using-a-hierarchical-regime-switching-model.pdf)    ## 1997    - **Detection of Mobile Phone Fraud Using Supervised Neural Networks: A First Prototype (ICANN 1997)**    - Yves Moreau, Herman Verrelst, Joos Vandewalle    - [[Paper]](https://link.springer.com/content/pdf/10.1007%2FBFb0020294.pdf)      - **Prospective Assessment of AI Technologies for Fraud Detection: A Case Study (AAAI 1997)**    - David Jensen    - [[Paper]](https://pdfs.semanticscholar.org/0efe/8a145cc4d52e8769bb1d13142326a154624f.pdf)      - **Credit Card Fraud Detection Using Meta-Learning: Issues and Initial Results (AAAI 1997)**    - Salvatore J. Stolfo, David W. Fan, Wenke Lee and Andreas L. Prodromidis    - [[Paper]](https://pdfs.semanticscholar.org/29b3/e330e0045e5da71cc1d333bed24b7a4670f8.pdf)    ## 1995  - **Fraud: Uncollectible Debt Detection Using a Bayesian Network Based Learning System: A Rare Binary Outcome with Mixed Data Structures (UAI 1995)**    - Kazuo J. Ezawa, Til Schuermann    - [[Paper]](https://arxiv.org/abs/1302.4945)      --------------------------------------------------------------------------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers/blob/master/LICENSE)   """
Big data;https://github.com/influxdata/kapacitor;"""# Kapacitor [![Circle CI](https://circleci.com/gh/influxdata/kapacitor/tree/master.svg?style=svg&circle-token=78c97422cf89526309e502a290c230e8a463229f)](https://circleci.com/gh/influxdata/kapacitor/tree/master) [![Docker pulls](https://img.shields.io/docker/pulls/library/kapacitor.svg)](https://hub.docker.com/_/kapacitor/)  Open source framework for processing, monitoring, and alerting on time series data    # Installation    Kapacitor has two binaries:    * kapacitor – a CLI program for calling the Kapacitor API.  * kapacitord – the Kapacitor server daemon.    You can either download the binaries directly from the [downloads](https://influxdata.com/downloads/#kapacitor) page or go get them:    ```sh  go get github.com/influxdata/kapacitor/cmd/kapacitor  go get github.com/influxdata/kapacitor/cmd/kapacitord  ```    # Configuration  An example configuration file can be found [here](https://github.com/influxdata/kapacitor/blob/master/etc/kapacitor/kapacitor.conf)    Kapacitor can also provide an example config for you using this command:    ```sh  kapacitord config  ```      # Getting Started    This README gives you a high level overview of what Kapacitor is and what its like to use it. As well as some details of how it works.  To get started using Kapacitor see [this guide](https://docs.influxdata.com/kapacitor/latest/introduction/getting-started/). After you finish the getting started exercise you can check out the [TICKscripts](https://github.com/influxdata/kapacitor/tree/master/examples/telegraf) for different Telegraf plugins.    # Basic Example    Kapacitor uses a DSL named [TICKscript](https://docs.influxdata.com/kapacitor/latest/tick/) to define tasks.    A simple TICKscript that alerts on high cpu usage looks like this:    ```javascript  stream      |from()          .measurement('cpu_usage_idle')          .groupBy('host')      |window()          .period(1m)          .every(1m)      |mean('value')      |eval(lambda: 100.0 - ""mean"")          .as('used')      |alert()          .message('{{ .Level}}: {{ .Name }}/{{ index .Tags ""host"" }} has high cpu usage: {{ index .Fields ""used"" }}')          .warn(lambda: ""used"" > 70.0)          .crit(lambda: ""used"" > 85.0)            // Send alert to hander of choice.            // Slack          .slack()          .channel('#alerts')            // VictorOps          .victorOps()          .routingKey('team_rocket')            // PagerDuty          .pagerDuty()  ```    Place the above script into a file `cpu_alert.tick` then run these commands to start the task:    ```sh  # Define the task (assumes cpu data is in db 'telegraf')  kapacitor define \      cpu_alert \      -type stream \      -dbrp telegraf.default \      -tick ./cpu_alert.tick  # Start the task  kapacitor enable cpu_alert  ``` """
Big data;https://github.com/zrosenbauer/awesome-bigtable;"""<div align=""center"">  	<div>  		<img width=""180"" src=""/awesome-logo.png"" alt=""Awesome Bigtable"">  	</div>      <br />  	<p>  		:zap: Delightful list of <a href=""https://cloud.google.com/bigtable/"">Google Bigtable</a> resources, packages and interesting finds.  	</p>  	<br>  	<img src=""https://awesome.re/badge.svg"" alt=""Awesome List"">  </div>    ---    # Awesome Bigtable    [Bigtable](https://cloud.google.com/bigtable) is a fully managed, scalable NoSQL database service for large analytical and operational workloads, built and managed by Google.    ## Contents    - [Tools](#tools)    - [Official Client Libraries](#official-client-libraries)    - [Other Client Libraries](#other-client-libraries)    - [Command-line](#command-line)    - [Emulators](#emulators)    - [Databases](#databases)  - [Resources](#resources)    - [Articles & Blogs](#articles--blogs)    - [Tutorials](#tutorials)  - [Cool Stuff](#cool-stuff)    - [Inspired by Bigtable](#inspired-by-bigtable)    - [Interesting Projects](#interesting-projects)    ---    If you are new to Bigtable I'd recommend checking out the [Bigtable Documentation](https://cloud.google.com/bigtable/docs/). The docs are a great place to start, as you can view a full list of integrations, tutorials and other treats. This list is meant to be a curated list of awesome Bigtable ""things"" to supplement any official documentation.    ## Tools  A curated list of tools that will help you when working with or building on-top of Bigtable.    ### Official Client Libraries  - [C++](https://github.com/GoogleCloudPlatform/google-cloud-cpp/tree/master/google/cloud/bigtable) - Official implementation of the Google Cloud Bigtable C++ client.  - [C#](https://github.com/googleapis/google-cloud-dotnet) - Official implementation of the Google Cloud Bigtable .NET client.  - [Node.js](https://github.com/googleapis/nodejs-bigtable) - Official implementation of the Google Cloud Bigtable Node.js client.  - [Python](https://github.com/googleapis/python-bigtable) - Official implementation of the Google Cloud Bigtable python client.  - [HappyBase](https://github.com/googleapis/google-cloud-python-happybase) - Official client which uses a HappyBase emulation layer which uses Bigtable as the underlying storage layer.  - [Java](https://github.com/googleapis/java-bigtable) - Official implementation of the Google Cloud Bigtable Java client.  - [HBase Java](https://github.com/GoogleCloudPlatform/cloud-bigtable-client) - Official Java libraries and HBase client extensions for accessing Google Cloud Bigtable.  - [Go](https://github.com/googleapis/google-cloud-go/tree/master/bigtable) - Official implementation of the Google Cloud Bigtable Go client.  - [PHP](https://github.com/googleapis/google-cloud-php-bigtable) - Official implementation of the Google Cloud Bigtable PHP client.    ### Other Client Libraries  - [Simple Bigtable](https://github.com/spotify/simple-bigtable) - Java based client built and maintained by Spotify.  - [Rust Bigtable](https://github.com/durch/rust-bigtable) - Rust library for working with Google Bigtable Data API.  - [AsyncBigtable](https://github.com/OpenTSDB/asyncbigtable) - Implementation of AsyncHBase but on top of Google's Cloud Bigtable service.    ### Command-line  - [cbt](https://cloud.google.com/bigtable/docs/cbt-overview) - Official command-line interface for performing several different operations on Cloud Bigtable.   - [btcli](https://github.com/takashabe/btcli) - CLI client for the Bigtable with auto-completion.    ### Emulators  - [Google Emulator](https://cloud.google.com/bigtable/docs/emulator) - Official in-memory emulator for Cloud Bigtable, included with the Google Cloud SDK.  - [Spotify Docker Bigtable](https://github.com/spotify/docker-bigtable) - Docker container with an in memory implementation of Google Cloud Bigtable.  - [Shopify Bigtable Emulator](https://github.com/Shopify/bigtable-emulator) - In memory Go implementation of Bigtable.  - [LittleTable](https://github.com/steveniemitz/littletable) - In-memory JVM-based emulator for Bigtable.    ### Databases  - [Heroic](https://github.com/spotify/heroic) - Scalable time series database based on Bigtable, Cassandra, and Elasticsearch.  - [Janusgraph](https://github.com/JanusGraph/janusgraph) - Open-source, distributed graph database that can use Bigtable as its storage layer.  - [GeoMesa](https://github.com/locationtech/geomesa) - Suite of tools for working with big geo-spatial data in a distributed fashion, that can leverage Bigtable as its backend.  - [GeoWave](https://github.com/locationtech/geowave) - Tool that provides geospatial and temporal indexing on top of Accumulo, HBase, Bigtable, Cassandra, and DynamoDB.  - [HGraphDB](https://github.com/rayokota/hgraphdb) - Client layer for using HBase (Bigtable) as a graph database.  - [OpenTSDB](https://github.com/GoogleCloudPlatform/opentsdb-bigtable) - An Open Source Time Series Data Base that can levearge Bigtable as its storage layer.  - [Cattle DB](https://github.com/wuttem/cattledb) - Timeseries store built on top of Bigtable.  - [YildizDB](https://github.com/yildizdb/yildiz) - Graph database layer on top of Bigtable.    ## Resources  A curated list of resources to help you get off the ground with Bigtable.    ### Articles & Blogs  - [Bigtable: A Distributed Storage System for Structured Data](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf) - Published on 2006.  - [A NoSQL massively parallel table](https://www.cs.rutgers.edu/~pxk/417/notes/content/bigtable.html) - Published on 2011-11.  - [How we moved our Historical Stats from MySQL to Bigtable with zero downtime](https://www.fastly.com/blog/how-we-moved-our-historical-stats-from-mysql-bigtable-zero-downtime) - Published on 2017-07.  - [Medium @duhroach](https://medium.com/@duhroach) - Bigtable centric posts by Colt McAnlis, DA @ Google.    - [Cloud Bigtable Performance 101](https://medium.com/@duhroach/cloud-bigtable-performance-101-8bf884bc1d1c) - Published on 2018-11.    - [The right Cloud Bigtable index makes all the difference.](https://medium.com/@duhroach/the-right-cloud-bigtable-index-makes-all-the-difference-3bcabe9bd65a) - Published on 2019-1.    - [Cloud Bigtable : Getting the geography right](https://medium.com/@duhroach/cloud-bigtable-getting-the-geography-right-645577216516) - Published on 2019-1.    - [Using Cloud Bigtable Monitoring UI](https://medium.com/@duhroach/using-cloud-bigtable-monitoring-ui-40d3f4c726d6) - Published on 2019-1.  - [Bigtable: storing Protobuf bytes in one column vs splitting the content into column families/qualifiers](https://tech.travelaudience.com/bigtable-storing-protobuf-bytes-in-one-column-vs-splitting-the-content-into-column-families-c231bdff8db7) - Published on 2018-1.  - [Using Google Cloud Emulators in Integration Tests](https://medium.com/google-cloud/using-google-cloud-emulators-for-integration-tests-7812890ebe0d) - Published on 2017-6.  - [The Joy and Pain of using Google Bigtable](https://syslog.ravelin.com/the-joy-and-pain-of-using-google-bigtable-4210604c75be) - Published on 2019-1.    ### Tutorials  - [Google Tutorials for Bigtable](https://cloud.google.com/bigtable/docs/tutorials) - List of official tutorials related to Bigtable.   - [Cloud Bigtable Examples](https://github.com/GoogleCloudPlatform/cloud-bigtable-examples) - Repo containing official examples of using Bigtable.  - [Introduction to Google Cloud Bigtable](https://cloudacademy.com/course/introduction-to-google-cloud-bigtable/) - CloudAcademy provided intro tutorial to Bigtable (membership required).    ## Cool Stuff  A list of cool things related to Bigtable.    ### Inspired by Bigtable  - [Apache Cassandra](http://cassandra.apache.org/) - Highly-scalable partitioned row store.  - [Apache HBase](https://hbase.apache.org/) - The Hadoop database, a distributed, scalable, big data store.  - [Apache Accumulo](https://github.com/apache/accumulo) - Sorted, distributed key/value store that provides robust, scalable data storage and retrieval.  - [Tera](https://github.com/baidu/tera) - High performance distributed NoSQL database.  - [obigstore](https://github.com/mfp/obigstore) - Database with Bigtable-like data model atop LevelDB.    ### Interesting Projects  - [YildizDB Bigtable](https://github.com/yildizdb/bigtable) - TypeScript Bigtable Client with 🔋🔋 included.  - [Bigtable Autoscaler](https://github.com/spotify/bigtable-autoscaler) - Service that autoscales Bigtable clusters based on CPU load.    <!--lint ignore no-emphasis-as-heading-->  **Awesome mentioned badge**    If your package or repository is mentioned in this list feel free to add the Awesome mentioned badge to your README.md.    ```md  [![Mentioned in Awesome Bigtable](https://awesome.re/mentioned-badge-flat.svg)](https://github.com/zrosenbauer/awesome-bigtable)  ```    ---    **Logo Source:** https://logomakr.com/4gLK5l """
Big data;https://github.com/vinta/awesome-python;"""# Awesome Python [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)    A curated list of awesome Python frameworks, libraries, software and resources.    Inspired by [awesome-php](https://github.com/ziadoz/awesome-php).    - [Awesome Python](#awesome-python)      - [Admin Panels](#admin-panels)      - [Algorithms and Design Patterns](#algorithms-and-design-patterns)      - [ASGI Servers](#asgi-servers)      - [Asynchronous Programming](#asynchronous-programming)      - [Audio](#audio)      - [Authentication](#authentication)      - [Build Tools](#build-tools)      - [Built-in Classes Enhancement](#built-in-classes-enhancement)      - [Caching](#caching)      - [ChatOps Tools](#chatops-tools)      - [CMS](#cms)      - [Code Analysis](#code-analysis)      - [Command-line Interface Development](#command-line-interface-development)      - [Command-line Tools](#command-line-tools)      - [Compatibility](#compatibility)      - [Computer Vision](#computer-vision)      - [Concurrency and Parallelism](#concurrency-and-parallelism)      - [Configuration](#configuration)      - [Cryptography](#cryptography)      - [Data Analysis](#data-analysis)      - [Data Validation](#data-validation)      - [Data Visualization](#data-visualization)      - [Database Drivers](#database-drivers)      - [Database](#database)      - [Date and Time](#date-and-time)      - [Debugging Tools](#debugging-tools)      - [Deep Learning](#deep-learning)      - [DevOps Tools](#devops-tools)      - [Distributed Computing](#distributed-computing)      - [Distribution](#distribution)      - [Documentation](#documentation)      - [Downloader](#downloader)      - [E-commerce](#e-commerce)      - [Editor Plugins and IDEs](#editor-plugins-and-ides)      - [Email](#email)      - [Enterprise Application Integrations](#enterprise-application-integrations)      - [Environment Management](#environment-management)      - [Files](#files)      - [Foreign Function Interface](#foreign-function-interface)      - [Forms](#forms)      - [Functional Programming](#functional-programming)      - [Game Development](#game-development)      - [Geolocation](#geolocation)      - [GUI Development](#gui-development)      - [Hardware](#hardware)      - [HTML Manipulation](#html-manipulation)      - [HTTP Clients](#http-clients)      - [Image Processing](#image-processing)      - [Implementations](#implementations)      - [Interactive Interpreter](#interactive-interpreter)      - [Internationalization](#internationalization)      - [Job Scheduler](#job-scheduler)      - [Logging](#logging)      - [Machine Learning](#machine-learning)      - [Miscellaneous](#miscellaneous)      - [Natural Language Processing](#natural-language-processing)      - [Network Virtualization](#network-virtualization)      - [News Feed](#news-feed)      - [ORM](#orm)      - [Package Management](#package-management)      - [Package Repositories](#package-repositories)      - [Penetration testing](#penetration-testing)      - [Permissions](#permissions)      - [Processes](#processes)      - [Recommender Systems](#recommender-systems)      - [Refactoring](#refactoring)      - [RESTful API](#restful-api)      - [Robotics](#robotics)      - [RPC Servers](#rpc-servers)      - [Science](#science)      - [Search](#search)      - [Serialization](#serialization)      - [Serverless Frameworks](#serverless-frameworks)      - [Shell](#shell)      - [Specific Formats Processing](#specific-formats-processing)      - [Static Site Generator](#static-site-generator)      - [Tagging](#tagging)      - [Task Queues](#task-queues)      - [Template Engine](#template-engine)      - [Testing](#testing)      - [Text Processing](#text-processing)      - [Third-party APIs](#third-party-apis)      - [URL Manipulation](#url-manipulation)      - [Video](#video)      - [Web Asset Management](#web-asset-management)      - [Web Content Extracting](#web-content-extracting)      - [Web Crawling](#web-crawling)      - [Web Frameworks](#web-frameworks)      - [WebSocket](#websocket)      - [WSGI Servers](#wsgi-servers)  - [Resources](#resources)      - [Books](#books)      - [Newsletters](#newsletters)      - [Podcasts](#podcasts)      - [Websites](#websites)  - [Contributing](#contributing)    ---    ## Admin Panels    *Libraries for administrative interfaces.*    * [ajenti](https://github.com/ajenti/ajenti) - The admin panel your servers deserve.  * [django-grappelli](https://grappelliproject.com/) - A jazzy skin for the Django Admin-Interface.  * [django-jet](https://github.com/geex-arts/django-jet) - Modern responsive template for the Django admin interface with improved functionality.  * [django-suit](https://djangosuit.com/) - Alternative Django Admin-Interface (free only for Non-commercial use).  * [django-xadmin](https://github.com/sshwsfc/xadmin) - Drop-in replacement of Django admin comes with lots of goodies.  * [flask-admin](https://github.com/flask-admin/flask-admin) - Simple and extensible administrative interface framework for Flask.  * [flower](https://github.com/mher/flower) - Real-time monitor and web admin for Celery.  * [jet-bridge](https://github.com/jet-admin/jet-bridge) - Admin panel framework for any application with nice UI (ex Jet Django).  * [wooey](https://github.com/wooey/wooey) - A Django app which creates automatic web UIs for Python scripts.    ## Algorithms and Design Patterns    *Python implementation of data structures, algorithms and design patterns. Also see [awesome-algorithms](https://github.com/tayllan/awesome-algorithms).*    * Algorithms      * [algorithms](https://github.com/keon/algorithms) - Minimal examples of data structures and algorithms.      * [python-ds](https://github.com/prabhupant/python-ds) - A collection of data structure and algorithms for coding interviews.      * [sortedcontainers](https://github.com/grantjenks/python-sortedcontainers) - Fast and pure-Python implementation of sorted collections.      * [TheAlgorithms](https://github.com/TheAlgorithms/Python) - All Algorithms implemented in Python.  * Design Patterns      * [PyPattyrn](https://github.com/tylerlaberge/PyPattyrn) - A simple yet effective library for implementing common design patterns.      * [python-patterns](https://github.com/faif/python-patterns) - A collection of design patterns in Python.      * [transitions](https://github.com/pytransitions/transitions) - A lightweight, object-oriented finite state machine implementation.    ## ASGI Servers    *[ASGI](https://asgi.readthedocs.io/en/latest/)-compatible web servers.*    * [daphne](https://github.com/django/daphne) - A HTTP, HTTP2 and WebSocket protocol server for ASGI and ASGI-HTTP.  * [uvicorn](https://github.com/encode/uvicorn) - A lightning-fast ASGI server implementation, using uvloop and httptools.    ## Asynchronous Programming    * [asyncio](https://docs.python.org/3/library/asyncio.html) - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks.      - [awesome-asyncio](https://github.com/timofurrer/awesome-asyncio)  * [trio](https://github.com/python-trio/trio) - A friendly library for async concurrency and I/O.  * [Twisted](https://twistedmatrix.com/trac/) - An event-driven networking engine.  * [uvloop](https://github.com/MagicStack/uvloop) - Ultra fast asyncio event loop.    ## Audio    *Libraries for manipulating audio and its metadata.*    * Audio      * [audioread](https://github.com/beetbox/audioread) - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.      * [dejavu](https://github.com/worldveil/dejavu) - Audio fingerprinting and recognition.      * [kapre](https://github.com/keunwoochoi/kapre) - Keras Audio Preprocessors.      * [librosa](https://github.com/librosa/librosa) - Python library for audio and music analysis.      * [matchering](https://github.com/sergree/matchering) - A library for automated reference audio mastering.      * [mingus](http://bspaans.github.io/python-mingus/) - An advanced music theory and notation package with MIDI file and playback support.      * [pyAudioAnalysis](https://github.com/tyiannak/pyAudioAnalysis) - Audio feature extraction, classification, segmentation and applications.      * [pydub](https://github.com/jiaaro/pydub) - Manipulate audio with a simple and easy high level interface.      * [TimeSide](https://github.com/Parisson/TimeSide) - Open web audio processing framework.  * Metadata      * [beets](https://github.com/beetbox/beets) - A music library manager and [MusicBrainz](https://musicbrainz.org/) tagger.      * [eyeD3](https://github.com/nicfit/eyeD3) - A tool for working with audio files, specifically MP3 files containing ID3 metadata.      * [mutagen](https://github.com/quodlibet/mutagen) - A Python module to handle audio metadata.      * [tinytag](https://github.com/devsnd/tinytag) - A library for reading music meta data of MP3, OGG, FLAC and Wave files.    ## Authentication    *Libraries for implementing authentications schemes.*    * OAuth      * [authlib](https://github.com/lepture/authlib) - JavaScript Object Signing and Encryption draft implementation.      * [django-allauth](https://github.com/pennersr/django-allauth) - Authentication app for Django that ""just works.""      * [django-oauth-toolkit](https://github.com/evonove/django-oauth-toolkit) - OAuth 2 goodies for Django.      * [oauthlib](https://github.com/idan/oauthlib) - A generic and thorough implementation of the OAuth request-signing logic.      * [python-oauth2](https://github.com/joestump/python-oauth2) - A fully tested, abstract interface to creating OAuth clients and servers.      * [python-social-auth](https://github.com/omab/python-social-auth) - An easy-to-setup social authentication mechanism.  * JWT      * [pyjwt](https://github.com/jpadilla/pyjwt) - JSON Web Token implementation in Python.      * [python-jose](https://github.com/mpdavis/python-jose/) - A JOSE implementation in Python.      * [python-jwt](https://github.com/davedoesdev/python-jwt) - A module for generating and verifying JSON Web Tokens.    ## Build Tools    *Compile software from source code.*    * [BitBake](http://www.yoctoproject.org/docs/1.6/bitbake-user-manual/bitbake-user-manual.html) - A make-like build tool for embedded Linux.  * [buildout](http://www.buildout.org/en/latest/) - A build system for creating, assembling and deploying applications from multiple parts.  * [PlatformIO](https://github.com/platformio/platformio-core) - A console tool to build code with different development platforms.  * [pybuilder](https://github.com/pybuilder/pybuilder) - A continuous build tool written in pure Python.  * [SCons](http://www.scons.org/) - A software construction tool.    ## Built-in Classes Enhancement    *Libraries for enhancing Python built-in classes.*    * [attrs](https://github.com/python-attrs/attrs) - Replacement for `__init__`, `__eq__`, `__repr__`, etc. boilerplate in class definitions.  * [bidict](https://github.com/jab/bidict) - Efficient, Pythonic bidirectional map data structures and related functionality..  * [Box](https://github.com/cdgriffith/Box) - Python dictionaries with advanced dot notation access.  * [dataclasses](https://docs.python.org/3/library/dataclasses.html) - (Python standard library) Data classes.  * [DottedDict](https://github.com/carlosescri/DottedDict) - A library that provides a method of accessing lists and dicts with a dotted path notation.    ## CMS    *Content Management Systems.*    * [django-cms](https://www.django-cms.org/en/) - An Open source enterprise CMS based on the Django.  * [feincms](https://github.com/feincms/feincms) - One of the most advanced Content Management Systems built on Django.  * [indico](https://github.com/indico/indico) - A feature-rich event management system, made @ [CERN](https://en.wikipedia.org/wiki/CERN).  * [Kotti](https://github.com/Kotti/Kotti) - A high-level, Pythonic web application framework built on Pyramid.  * [mezzanine](https://github.com/stephenmcd/mezzanine) - A powerful, consistent, and flexible content management platform.  * [plone](https://plone.org/) - A CMS built on top of the open source application server Zope.  * [quokka](https://github.com/rochacbruno/quokka) - Flexible, extensible, small CMS powered by Flask and MongoDB.  * [wagtail](https://wagtail.io/) - A Django content management system.    ## Caching    *Libraries for caching data.*    * [beaker](https://github.com/bbangert/beaker) - A WSGI middleware for sessions and caching.  * [django-cache-machine](https://github.com/django-cache-machine/django-cache-machine) - Automatic caching and invalidation for Django models.  * [django-cacheops](https://github.com/Suor/django-cacheops) - A slick ORM cache with automatic granular event-driven invalidation.  * [dogpile.cache](http://dogpilecache.readthedocs.io/en/latest/) - dogpile.cache is next generation replacement for Beaker made by same authors.  * [HermesCache](https://pypi.org/project/HermesCache/) - Python caching library with tag-based invalidation and dogpile effect prevention.  * [pylibmc](https://github.com/lericson/pylibmc) - A Python wrapper around the [libmemcached](https://libmemcached.org/libMemcached.html) interface.  * [python-diskcache](http://www.grantjenks.com/docs/diskcache/) - SQLite and file backed cache backend with faster lookups than memcached and redis.    ## ChatOps Tools    *Libraries for chatbot development.*    * [errbot](https://github.com/errbotio/errbot/) - The easiest and most popular chatbot to implement ChatOps.    ## Code Analysis    *Tools of static analysis, linters and code quality checkers. Also see [awesome-static-analysis](https://github.com/mre/awesome-static-analysis).*    * Code Analysis      * [coala](https://github.com/coala/coala/) - Language independent and easily extendable code analysis application.      * [code2flow](https://github.com/scottrogowski/code2flow) - Turn your Python and JavaScript code into DOT flowcharts.      * [prospector](https://github.com/PyCQA/prospector) - A tool to analyse Python code.      * [pycallgraph](https://github.com/gak/pycallgraph) - A library that visualises the flow (call graph) of your Python application.      * [vulture](https://github.com/jendrikseipp/vulture) - A tool for finding and analysing dead Python code.  * Code Linters      * [flake8](https://pypi.org/project/flake8/) - A wrapper around `pycodestyle`, `pyflakes` and McCabe.          * [awesome-flake8-extensions](https://github.com/DmytroLitvinov/awesome-flake8-extensions)      * [pylama](https://github.com/klen/pylama) - A code audit tool for Python and JavaScript.      * [pylint](https://www.pylint.org/) - A fully customizable source code analyzer.      * [wemake-python-styleguide](https://github.com/wemake-services/wemake-python-styleguide) - The strictest and most opinionated python linter ever.  * Code Formatters      * [black](https://github.com/python/black) - The uncompromising Python code formatter.      * [isort](https://github.com/timothycrosley/isort) - A Python utility / library to sort imports.      * [yapf](https://github.com/google/yapf) - Yet another Python code formatter from Google.  * Static Type Checkers, also see [awesome-python-typing](https://github.com/typeddjango/awesome-python-typing)      * [mypy](http://mypy-lang.org/) - Check variable types during compile time.      * [pyre-check](https://github.com/facebook/pyre-check) - Performant type checking.      * [typeshed](https://github.com/python/typeshed) - Collection of library stubs for Python, with static types.  * Static Type Annotations Generators      * [MonkeyType](https://github.com/Instagram/MonkeyType) - A system for Python that generates static type annotations by collecting runtime types.      * [pyannotate](https://github.com/dropbox/pyannotate) - Auto-generate PEP-484 annotations.      * [pytype](https://github.com/google/pytype) - Pytype checks and infers types for Python code - without requiring type annotations.    ## Command-line Interface Development    *Libraries for building command-line applications.*    * Command-line Application Development      * [cement](http://builtoncement.com/) - CLI Application Framework for Python.      * [click](http://click.pocoo.org/dev/) - A package for creating beautiful command line interfaces in a composable way.      * [cliff](https://docs.openstack.org/developer/cliff/) - A framework for creating command-line programs with multi-level commands.      * [docopt](http://docopt.org/) - Pythonic command line arguments parser.      * [python-fire](https://github.com/google/python-fire) - A library for creating command line interfaces from absolutely any Python object.      * [python-prompt-toolkit](https://github.com/jonathanslenders/python-prompt-toolkit) - A library for building powerful interactive command lines.  * Terminal Rendering      * [alive-progress](https://github.com/rsalmei/alive-progress) - A new kind of Progress Bar, with real-time throughput, eta and very cool animations.      * [asciimatics](https://github.com/peterbrittain/asciimatics) - A package to create full-screen text UIs (from interactive forms to ASCII animations).      * [bashplotlib](https://github.com/glamp/bashplotlib) - Making basic plots in the terminal.      * [colorama](https://pypi.org/project/colorama/) - Cross-platform colored terminal text.      * [rich](https://github.com/willmcgugan/rich) - Python library for rich text and beautiful formatting in the terminal. Also provides a great `RichHandler` log handler.      * [tqdm](https://github.com/tqdm/tqdm) - Fast, extensible progress bar for loops and CLI.    ## Command-line Tools    *Useful CLI-based tools for productivity.*    * Productivity Tools      * [copier](https://github.com/pykong/copier) - A library and command-line utility for rendering projects templates.      * [cookiecutter](https://github.com/audreyr/cookiecutter) - A command-line utility that creates projects from cookiecutters (project templates).      * [doitlive](https://github.com/sloria/doitlive) - A tool for live presentations in the terminal.      * [howdoi](https://github.com/gleitz/howdoi) - Instant coding answers via the command line.      * [Invoke](https://github.com/pyinvoke/invoke#readme) - A tool for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks.      * [PathPicker](https://github.com/facebook/PathPicker) - Select files out of bash output.      * [percol](https://github.com/mooz/percol) - Adds flavor of interactive selection to the traditional pipe concept on UNIX.      * [thefuck](https://github.com/nvbn/thefuck) - Correcting your previous console command.      * [tmuxp](https://github.com/tony/tmuxp) - A [tmux](https://github.com/tmux/tmux) session manager.      * [try](https://github.com/timofurrer/try) - A dead simple CLI to try out python packages - it's never been easier.  * CLI Enhancements      * [httpie](https://github.com/jakubroztocil/httpie) - A command line HTTP client, a user-friendly cURL replacement.      * [iredis](https://github.com/laixintao/iredis) - Redis CLI with autocompletion and syntax highlighting.      * [kube-shell](https://github.com/cloudnativelabs/kube-shell) - An integrated shell for working with the Kubernetes CLI.      * [litecli](https://github.com/dbcli/litecli) - SQLite CLI with autocompletion and syntax highlighting.      * [mycli](https://github.com/dbcli/mycli) - MySQL CLI with autocompletion and syntax highlighting.      * [pgcli](https://github.com/dbcli/pgcli) - PostgreSQL CLI with autocompletion and syntax highlighting.      * [saws](https://github.com/donnemartin/saws) - A Supercharged [aws-cli](https://github.com/aws/aws-cli).    ## Compatibility    *Libraries for migrating from Python 2 to 3.*    * [python-future](http://python-future.org/index.html) - The missing compatibility layer between Python 2 and Python 3.  * [modernize](https://github.com/PyCQA/modernize) - Modernizes Python code for eventual Python 3 migration.  * [six](https://pypi.org/project/six/) - Python 2 and 3 compatibility utilities.    ## Computer Vision    *Libraries for Computer Vision.*    * [EasyOCR](https://github.com/JaidedAI/EasyOCR) - Ready-to-use OCR with 40+ languages supported.  * [Face Recognition](https://github.com/ageitgey/face_recognition) - Simple facial recognition library.  * [Kornia](https://github.com/kornia/kornia/) - Open Source Differentiable Computer Vision Library for PyTorch.  * [OpenCV](https://opencv.org/) - Open Source Computer Vision Library.  * [pytesseract](https://github.com/madmaze/pytesseract) - A wrapper for [Google Tesseract OCR](https://github.com/tesseract-ocr).  * [SimpleCV](https://github.com/sightmachine/SimpleCV) - An open source framework for building computer vision applications.  * [tesserocr](https://github.com/sirfz/tesserocr) - Another simple, Pillow-friendly, wrapper around the `tesseract-ocr` API for OCR.    ## Concurrency and Parallelism    *Libraries for concurrent and parallel execution. Also see [awesome-asyncio](https://github.com/timofurrer/awesome-asyncio).*    * [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) - (Python standard library) A high-level interface for asynchronously executing callables.  * [eventlet](http://eventlet.net/) - Asynchronous framework with WSGI support.  * [gevent](http://www.gevent.org/) - A coroutine-based Python networking library that uses [greenlet](https://github.com/python-greenlet/greenlet).  * [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) - (Python standard library) Process-based parallelism.  * [scoop](https://github.com/soravux/scoop) - Scalable Concurrent Operations in Python.  * [uvloop](https://github.com/MagicStack/uvloop) - Ultra fast implementation of `asyncio` event loop on top of `libuv`.    ## Configuration    *Libraries for storing and parsing configuration options.*    * [configobj](https://github.com/DiffSK/configobj) - INI file parser with validation.  * [configparser](https://docs.python.org/3/library/configparser.html) - (Python standard library) INI file parser.  * [hydra](https://github.com/facebookresearch/hydra) - Hydra is a framework for elegantly configuring complex applications.  * [profig](https://profig.readthedocs.io/en/latest/) - Config from multiple formats with value conversion.  * [python-decouple](https://github.com/henriquebastos/python-decouple) - Strict separation of settings from code.    ## Cryptography    * [cryptography](https://cryptography.io/en/latest/) - A package designed to expose cryptographic primitives and recipes to Python developers.  * [paramiko](https://github.com/paramiko/paramiko) - The leading native Python SSHv2 protocol library.  * [passlib](https://passlib.readthedocs.io/en/stable/) - Secure password storage/hashing library, very high level.  * [pynacl](https://github.com/pyca/pynacl) - Python binding to the Networking and Cryptography (NaCl) library.    ## Data Analysis    *Libraries for data analyzing.*    * [AWS Data Wrangler](https://github.com/awslabs/aws-data-wrangler) - Pandas on AWS.  * [Blaze](https://github.com/blaze/blaze) - NumPy and Pandas interface to Big Data.  * [Open Mining](https://github.com/mining/mining) - Business Intelligence (BI) in Pandas interface.  * [Optimus](https://github.com/ironmussa/Optimus) - Agile Data Science Workflows made easy with PySpark.  * [Orange](https://orange.biolab.si/) - Data mining, data visualization, analysis and machine learning through visual programming or scripts.  * [Pandas](http://pandas.pydata.org/) - A library providing high-performance, easy-to-use data structures and data analysis tools.    ## Data Validation    *Libraries for validating data. Used for forms in many cases.*    * [Cerberus](https://github.com/pyeve/cerberus) - A lightweight and extensible data validation library.  * [colander](https://docs.pylonsproject.org/projects/colander/en/latest/) - Validating and deserializing data obtained via XML, JSON, an HTML form post.  * [jsonschema](https://github.com/Julian/jsonschema) - An implementation of [JSON Schema](http://json-schema.org/) for Python.  * [schema](https://github.com/keleshev/schema) - A library for validating Python data structures.  * [Schematics](https://github.com/schematics/schematics) - Data Structure Validation.  * [valideer](https://github.com/podio/valideer) - Lightweight extensible data validation and adaptation library.  * [voluptuous](https://github.com/alecthomas/voluptuous) - A Python data validation library.    ## Data Visualization    *Libraries for visualizing data. Also see [awesome-javascript](https://github.com/sorrycc/awesome-javascript#data-visualization).*    * [Altair](https://github.com/altair-viz/altair) - Declarative statistical visualization library for Python.  * [Bokeh](https://github.com/bokeh/bokeh) - Interactive Web Plotting for Python.  * [bqplot](https://github.com/bloomberg/bqplot) - Interactive Plotting Library for the Jupyter Notebook.  * [Cartopy](https://github.com/SciTools/cartopy) - A cartographic python library with matplotlib support.  * [Dash](https://plot.ly/products/dash/) - Built on top of Flask, React and Plotly aimed at analytical web applications.      * [awesome-dash](https://github.com/Acrotrend/awesome-dash)  * [diagrams](https://github.com/mingrammer/diagrams) - Diagram as Code.  * [Matplotlib](http://matplotlib.org/) - A Python 2D plotting library.  * [plotnine](https://github.com/has2k1/plotnine) - A grammar of graphics for Python based on ggplot2.  * [Pygal](http://www.pygal.org/en/latest/) - A Python SVG Charts Creator.  * [PyGraphviz](https://pypi.org/project/pygraphviz/) - Python interface to [Graphviz](http://www.graphviz.org/).  * [PyQtGraph](http://www.pyqtgraph.org/) - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets.  * [Seaborn](https://github.com/mwaskom/seaborn) - Statistical data visualization using Matplotlib.  * [VisPy](https://github.com/vispy/vispy) - High-performance scientific visualization based on OpenGL.    ## Database    *Databases implemented in Python.*    * [pickleDB](https://github.com/patx/pickledb) - A simple and lightweight key-value store for Python.  * [tinydb](https://github.com/msiemens/tinydb) - A tiny, document-oriented database.  * [ZODB](https://github.com/zopefoundation/ZODB) - A native object database for Python. A key-value and object graph database.    ## Database Drivers    *Libraries for connecting and operating databases.*    * MySQL - [awesome-mysql](http://shlomi-noach.github.io/awesome-mysql/)      * [mysqlclient](https://github.com/PyMySQL/mysqlclient-python) - MySQL connector with Python 3 support ([mysql-python](https://sourceforge.net/projects/mysql-python/) fork).      * [PyMySQL](https://github.com/PyMySQL/PyMySQL) - A pure Python MySQL driver compatible to mysql-python.  * PostgreSQL - [awesome-postgres](https://github.com/dhamaniasad/awesome-postgres)      * [psycopg2](http://initd.org/psycopg/) - The most popular PostgreSQL adapter for Python.      * [queries](https://github.com/gmr/queries) - A wrapper of the psycopg2 library for interacting with PostgreSQL.  * SQlite - [awesome-sqlite](https://github.com/planetopendata/awesome-sqlite)      * [sqlite3](https://docs.python.org/3/library/sqlite3.html) - (Python standard library) SQlite interface compliant with DB-API 2.0      * [SuperSQLite](https://github.com/plasticityai/supersqlite) - A supercharged SQLite library built on top of [apsw](https://github.com/rogerbinns/apsw).  * Other Relational Databases      * [pymssql](https://pymssql.readthedocs.io/en/latest/) - A simple database interface to Microsoft SQL Server.      * [clickhouse-driver](https://github.com/mymarilyn/clickhouse-driver) - Python driver with native interface for ClickHouse.  * NoSQL Databases      * [cassandra-driver](https://github.com/datastax/python-driver) - The Python Driver for Apache Cassandra.      * [happybase](https://github.com/wbolster/happybase) - A developer-friendly library for Apache HBase.      * [kafka-python](https://github.com/dpkp/kafka-python) - The Python client for Apache Kafka.      * [py2neo](https://py2neo.org/) - A client library and toolkit for working with Neo4j.      * [pymongo](https://github.com/mongodb/mongo-python-driver) - The official Python client for MongoDB.      * [redis-py](https://github.com/andymccurdy/redis-py) - The Python client for Redis.  * Asynchronous Clients      * [motor](https://github.com/mongodb/motor) - The async Python driver for MongoDB.    ## Date and Time    *Libraries for working with dates and times.*    * [Arrow](https://arrow.readthedocs.io/en/latest/) - A Python library that offers a sensible and human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps.  * [Chronyk](https://github.com/KoffeinFlummi/Chronyk) - A Python 3 library for parsing human-written times and dates.  * [dateutil](https://github.com/dateutil/dateutil) - Extensions to the standard Python [datetime](https://docs.python.org/3/library/datetime.html) module.  * [delorean](https://github.com/myusuf3/delorean/) - A library for clearing up the inconvenient truths that arise dealing with datetimes.  * [maya](https://github.com/timofurrer/maya) - Datetimes for Humans.  * [moment](https://github.com/zachwill/moment) - A Python library for dealing with dates/times. Inspired by [Moment.js](http://momentjs.com/).  * [Pendulum](https://github.com/sdispater/pendulum) - Python datetimes made easy.  * [PyTime](https://github.com/shinux/PyTime) - An easy-to-use Python module which aims to operate date/time/datetime by string.  * [pytz](https://launchpad.net/pytz) - World timezone definitions, modern and historical. Brings the [tz database](https://en.wikipedia.org/wiki/Tz_database) into Python.  * [when.py](https://github.com/dirn/When.py) - Providing user-friendly functions to help perform common date and time actions.    ## Debugging Tools    *Libraries for debugging code.*    * pdb-like Debugger      * [ipdb](https://github.com/gotcha/ipdb) - IPython-enabled [pdb](https://docs.python.org/3/library/pdb.html).      * [pdb++](https://github.com/antocuni/pdb) - Another drop-in replacement for pdb.      * [pudb](https://github.com/inducer/pudb) - A full-screen, console-based Python debugger.      * [wdb](https://github.com/Kozea/wdb) - An improbable web debugger through WebSockets.  * Tracing      * [lptrace](https://github.com/khamidou/lptrace) - [strace](http://man7.org/linux/man-pages/man1/strace.1.html) for Python programs.      * [manhole](https://github.com/ionelmc/python-manhole) - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt.      * [pyringe](https://github.com/google/pyringe) - Debugger capable of attaching to and injecting code into Python processes.      * [python-hunter](https://github.com/ionelmc/python-hunter) - A flexible code tracing toolkit.  * Profiler      * [line_profiler](https://github.com/rkern/line_profiler) - Line-by-line profiling.      * [memory_profiler](https://github.com/fabianp/memory_profiler) - Monitor Memory usage of Python code.      * [py-spy](https://github.com/benfred/py-spy) - A sampling profiler for Python programs. Written in Rust.      * [pyflame](https://github.com/uber/pyflame) - A ptracing profiler For Python.      * [vprof](https://github.com/nvdv/vprof) - Visual Python profiler.  * Others      * [django-debug-toolbar](https://github.com/jazzband/django-debug-toolbar) - Display various debug information for Django.      * [django-devserver](https://github.com/dcramer/django-devserver) - A drop-in replacement for Django's runserver.      * [flask-debugtoolbar](https://github.com/mgood/flask-debugtoolbar) - A port of the django-debug-toolbar to flask.      * [icecream](https://github.com/gruns/icecream) - Inspect variables, expressions, and program execution with a single, simple function call.      * [pyelftools](https://github.com/eliben/pyelftools) - Parsing and analyzing ELF files and DWARF debugging information.    ## Deep Learning    *Frameworks for Neural Networks and Deep Learning. Also see [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning).*    * [caffe](https://github.com/BVLC/caffe) - A fast open framework for deep learning..  * [keras](https://github.com/keras-team/keras) - A high-level neural networks library and capable of running on top of either TensorFlow or Theano.  * [mxnet](https://github.com/dmlc/mxnet) - A deep learning framework designed for both efficiency and flexibility.  * [pytorch](https://github.com/pytorch/pytorch) - Tensors and Dynamic neural networks in Python with strong GPU acceleration.  * [SerpentAI](https://github.com/SerpentAI/SerpentAI) - Game agent framework. Use any video game as a deep learning sandbox.  * [tensorflow](https://github.com/tensorflow/tensorflow) - The most popular Deep Learning framework created by Google.  * [Theano](https://github.com/Theano/Theano) - A library for fast numerical computation.    ## DevOps Tools    *Software and libraries for DevOps.*    * Configuration Management      * [ansible](https://github.com/ansible/ansible) - A radically simple IT automation platform.      * [cloudinit](https://cloudinit.readthedocs.io/en/latest/) - A multi-distribution package that handles early initialization of a cloud instance.      * [OpenStack](https://www.openstack.org/) - Open source software for building private and public clouds.      * [pyinfra](https://github.com/Fizzadar/pyinfra) - A versatile CLI tools and python libraries to automate infrastructure.      * [saltstack](https://github.com/saltstack/salt) - Infrastructure automation and management system.  * SSH-style Deployment      * [cuisine](https://github.com/sebastien/cuisine) - Chef-like functionality for Fabric.      * [fabric](https://github.com/fabric/fabric) - A simple, Pythonic tool for remote execution and deployment.      * [fabtools](https://github.com/fabtools/fabtools) - Tools for writing awesome Fabric files.  * Process Management      * [honcho](https://github.com/nickstenning/honcho) - A Python clone of [Foreman](https://github.com/ddollar/foreman), for managing Procfile-based applications.      * [supervisor](https://github.com/Supervisor/supervisor) - Supervisor process control system for UNIX.  * Monitoring      * [psutil](https://github.com/giampaolo/psutil) - A cross-platform process and system utilities module.  * Backup      * [BorgBackup](https://www.borgbackup.org/) - A deduplicating archiver with compression and encryption.  * Others      * [docker-compose](https://docs.docker.com/compose/) - Fast, isolated development environments using [Docker](https://www.docker.com/).    ## Distributed Computing    *Frameworks and libraries for Distributed Computing.*    * Batch Processing      * [dask](https://github.com/dask/dask) - A flexible parallel computing library for analytic computing.      * [luigi](https://github.com/spotify/luigi) - A module that helps you build complex pipelines of batch jobs.      * [mrjob](https://github.com/Yelp/mrjob) - Run MapReduce jobs on Hadoop or Amazon Web Services.      * [PySpark](https://pypi.org/project/pyspark/) - [Apache Spark](https://spark.apache.org/) Python API.      * [Ray](https://github.com/ray-project/ray/) - A system for parallel and distributed Python that unifies the machine learning ecosystem.  * Stream Processing      * [faust](https://github.com/robinhood/faust) - A stream processing library, porting the ideas from [Kafka Streams](https://kafka.apache.org/documentation/streams/) to Python.      * [streamparse](https://github.com/Parsely/streamparse) - Run Python code against real-time streams of data via [Apache Storm](http://storm.apache.org/).    ## Distribution    *Libraries to create packaged executables for release distribution.*    * [dh-virtualenv](https://github.com/spotify/dh-virtualenv) - Build and distribute a virtualenv as a Debian package.  * [Nuitka](http://nuitka.net/) - Compile scripts, modules, packages to an executable or extension module.  * [py2app](http://pythonhosted.org/py2app/) - Freezes Python scripts (Mac OS X).  * [py2exe](http://www.py2exe.org/) - Freezes Python scripts (Windows).  * [pyarmor](https://github.com/dashingsoft/pyarmor) - A tool used to obfuscate python scripts, bind obfuscated scripts to fixed machine or expire obfuscated scripts.  * [PyInstaller](https://github.com/pyinstaller/pyinstaller) - Converts Python programs into stand-alone executables (cross-platform).  * [pynsist](http://pynsist.readthedocs.io/en/latest/) - A tool to build Windows installers, installers bundle Python itself.  * [shiv](https://github.com/linkedin/shiv) - A command line utility for building fully self-contained zipapps (PEP 441), but with all their dependencies included.    ## Documentation    *Libraries for generating project documentation.*    * [sphinx](https://github.com/sphinx-doc/sphinx/) - Python Documentation generator.      * [awesome-sphinxdoc](https://github.com/yoloseem/awesome-sphinxdoc)  * [pdoc](https://github.com/mitmproxy/pdoc) - Epydoc replacement to auto generate API documentation for Python libraries.  * [pycco](https://github.com/pycco-docs/pycco) - The literate-programming-style documentation generator.    ## Downloader    *Libraries for downloading.*    * [akshare](https://github.com/jindaxiang/akshare) - A financial data interface library, built for human beings!  * [s3cmd](https://github.com/s3tools/s3cmd) - A command line tool for managing Amazon S3 and CloudFront.  * [s4cmd](https://github.com/bloomreach/s4cmd) - Super S3 command line tool, good for higher performance.  * [you-get](https://you-get.org/) - A YouTube/Youku/Niconico video downloader written in Python 3.  * [youtube-dl](https://rg3.github.io/youtube-dl/) - A small command-line program to download videos from YouTube.    ## E-commerce    *Frameworks and libraries for e-commerce and payments.*    * [alipay](https://github.com/lxneng/alipay) - Unofficial Alipay API for Python.  * [Cartridge](https://github.com/stephenmcd/cartridge) - A shopping cart app built using the Mezzanine.  * [django-oscar](http://oscarcommerce.com/) - An open-source e-commerce framework for Django.  * [django-shop](https://github.com/awesto/django-shop) - A Django based shop system.  * [forex-python](https://github.com/MicroPyramid/forex-python) - Foreign exchange rates, Bitcoin price index and currency conversion.  * [merchant](https://github.com/agiliq/merchant) - A Django app to accept payments from various payment processors.  * [money](https://github.com/carlospalol/money) - `Money` class with optional CLDR-backed locale-aware formatting and an extensible currency exchange.  * [python-currencies](https://github.com/Alir3z4/python-currencies) - Display money format and its filthy currencies.  * [saleor](http://getsaleor.com/) - An e-commerce storefront for Django.  * [shoop](https://www.shuup.com/en/) - An open source E-Commerce platform based on Django.    ## Editor Plugins and IDEs    * Emacs      * [elpy](https://github.com/jorgenschaefer/elpy) - Emacs Python Development Environment.  * Sublime Text      * [anaconda](https://github.com/DamnWidget/anaconda) - Anaconda turns your Sublime Text 3 in a full featured Python development IDE.      * [SublimeJEDI](https://github.com/srusskih/SublimeJEDI) - A Sublime Text plugin to the awesome auto-complete library Jedi.  * Vim      * [jedi-vim](https://github.com/davidhalter/jedi-vim) - Vim bindings for the Jedi auto-completion library for Python.      * [python-mode](https://github.com/python-mode/python-mode) - An all in one plugin for turning Vim into a Python IDE.      * [YouCompleteMe](https://github.com/Valloric/YouCompleteMe) - Includes [Jedi](https://github.com/davidhalter/jedi)-based completion engine for Python.  * Visual Studio      * [PTVS](https://github.com/Microsoft/PTVS) - Python Tools for Visual Studio.  * Visual Studio Code      * [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) - The official VSCode extension with rich support for Python.  * IDE      * [PyCharm](https://www.jetbrains.com/pycharm/) - Commercial Python IDE by JetBrains. Has free community edition available.      * [spyder](https://github.com/spyder-ide/spyder) - Open Source Python IDE.    ## Email    *Libraries for sending and parsing email.*    * Mail Servers      * [modoboa](https://github.com/modoboa/modoboa) - A mail hosting and management platform including a modern Web UI.      * [salmon](https://github.com/moggers87/salmon) - A Python Mail Server.  * Clients      * [imbox](https://github.com/martinrusev/imbox) - Python IMAP for Humans.      * [yagmail](https://github.com/kootenpv/yagmail) - Yet another Gmail/SMTP client.  * Others      * [flanker](https://github.com/mailgun/flanker) - An email address and Mime parsing library.      * [mailer](https://github.com/marrow/mailer) - High-performance extensible mail delivery framework.    ## Enterprise Application Integrations    *Platforms and tools for systems integrations in enterprise environments*    * [Zato](https://zato.io) - ESB, SOA, REST, APIs and Cloud Integrations in Python.    ## Environment Management    *Libraries for Python version and virtual environment management.*    * [pyenv](https://github.com/pyenv/pyenv) - Simple Python version management.  * [virtualenv](https://github.com/pypa/virtualenv) - A tool to create isolated Python environments.    ## Files    *Libraries for file manipulation and MIME type detection.*    * [mimetypes](https://docs.python.org/3/library/mimetypes.html) - (Python standard library) Map filenames to MIME types.  * [path.py](https://github.com/jaraco/path.py) - A module wrapper for [os.path](https://docs.python.org/3/library/os.path.html).  * [pathlib](https://docs.python.org/3/library/pathlib.html) - (Python standard library) An cross-platform, object-oriented path library.  * [PyFilesystem2](https://github.com/pyfilesystem/pyfilesystem2) - Python's filesystem abstraction layer.  * [python-magic](https://github.com/ahupp/python-magic) - A Python interface to the libmagic file type identification library.  * [Unipath](https://github.com/mikeorr/Unipath) - An object-oriented approach to file/directory operations.  * [watchdog](https://github.com/gorakhargosh/watchdog) - API and shell utilities to monitor file system events.    ## Foreign Function Interface    *Libraries for providing foreign function interface.*    * [cffi](https://pypi.org/project/cffi/) - Foreign Function Interface for Python calling C code.  * [ctypes](https://docs.python.org/3/library/ctypes.html) - (Python standard library) Foreign Function Interface for Python calling C code.  * [PyCUDA](https://mathema.tician.de/software/pycuda/) - A Python wrapper for Nvidia's CUDA API.  * [SWIG](http://www.swig.org/Doc1.3/Python.html) - Simplified Wrapper and Interface Generator.    ## Forms    *Libraries for working with forms.*    * [Deform](https://github.com/Pylons/deform) - Python HTML form generation library influenced by the formish form generation library.  * [django-bootstrap3](https://github.com/dyve/django-bootstrap3) - Bootstrap 3 integration with Django.  * [django-bootstrap4](https://github.com/zostera/django-bootstrap4) - Bootstrap 4 integration with Django.  * [django-crispy-forms](https://github.com/django-crispy-forms/django-crispy-forms) - A Django app which lets you create beautiful forms in a very elegant and DRY way.  * [django-remote-forms](https://github.com/WiserTogether/django-remote-forms) - A platform independent Django form serializer.  * [WTForms](https://github.com/wtforms/wtforms) - A flexible forms validation and rendering library.    ## Functional Programming    *Functional Programming with Python.*    * [Coconut](https://github.com/evhub/coconut) - A variant of Python built for simple, elegant, Pythonic functional programming.  * [CyToolz](https://github.com/pytoolz/cytoolz/) - Cython implementation of `Toolz`: High performance functional utilities.  * [fn.py](https://github.com/kachayev/fn.py) - Functional programming in Python: implementation of missing features to enjoy FP.  * [funcy](https://github.com/Suor/funcy) - A fancy and practical functional tools.  * [more-itertools](https://github.com/erikrose/more-itertools) - More routines for operating on iterables, beyond `itertools`.  * [returns](https://github.com/dry-python/returns) - A set of type-safe monads, transformers, and composition utilities.  * [Toolz](https://github.com/pytoolz/toolz) - A collection of functional utilities for iterators, functions, and dictionaries.    ## GUI Development    *Libraries for working with graphical user interface applications.*    * [curses](https://docs.python.org/3/library/curses.html) - Built-in wrapper for [ncurses](http://www.gnu.org/software/ncurses/) used to create terminal GUI applications.  * [Eel](https://github.com/ChrisKnott/Eel) - A library for making simple Electron-like offline HTML/JS GUI apps.  * [enaml](https://github.com/nucleic/enaml) - Creating beautiful user-interfaces with Declarative Syntax like QML.  * [Flexx](https://github.com/zoofIO/flexx) - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering.  * [Gooey](https://github.com/chriskiehl/Gooey) - Turn command line programs into a full GUI application with one line.  * [kivy](https://kivy.org/) - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS.  * [pyglet](https://github.com/pyglet/pyglet) - A cross-platform windowing and multimedia library for Python.  * [PyGObject](https://wiki.gnome.org/Projects/PyGObject) - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3).  * [PyQt](https://riverbankcomputing.com/software/pyqt/intro) - Python bindings for the [Qt](https://www.qt.io/) cross-platform application and UI framework.  * [PySimpleGUI](https://github.com/PySimpleGUI/PySimpleGUI) - Wrapper for tkinter, Qt, WxPython and Remi.  * [pywebview](https://github.com/r0x0r/pywebview/) - A lightweight cross-platform native wrapper around a webview component.  * [Tkinter](https://wiki.python.org/moin/TkInter) - Tkinter is Python's de-facto standard GUI package.  * [Toga](https://github.com/pybee/toga) - A Python native, OS native GUI toolkit.  * [urwid](http://urwid.org/) - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc.  * [wxPython](https://wxpython.org/) - A blending of the wxWidgets C++ class library with the Python.  * [DearPyGui](https://github.com/RaylockLLC/DearPyGui/) - A Simple GPU accelerated Python GUI framework    ## GraphQL    *Libraries for working with GraphQL.*    * [graphene](https://github.com/graphql-python/graphene/) - GraphQL framework for Python.  * [tartiflette-aiohttp](https://github.com/tartiflette/tartiflette-aiohttp/) - An `aiohttp`-based wrapper for Tartiflette to expose GraphQL APIs over HTTP.  * [tartiflette-asgi](https://github.com/tartiflette/tartiflette-asgi/) - ASGI support for the Tartiflette GraphQL engine.  * [tartiflette](https://tartiflette.io) - SDL-first GraphQL engine implementation for Python 3.6+ and asyncio.    ## Game Development    *Awesome game development libraries.*    * [Arcade](https://api.arcade.academy/en/latest/) - Arcade is a modern Python framework for crafting games with compelling graphics and sound.  * [Cocos2d](http://cocos2d.org/) - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications.  * [Harfang3D](http://www.harfang3d.com) - Python framework for 3D, VR and game development.  * [Panda3D](https://www.panda3d.org/) - 3D game engine developed by Disney.  * [Pygame](http://www.pygame.org/news.html) - Pygame is a set of Python modules designed for writing games.  * [PyOgre](http://www.ogre3d.org/tikiwiki/PyOgre) - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D.  * [PyOpenGL](http://pyopengl.sourceforge.net/) - Python ctypes bindings for OpenGL and it's related APIs.  * [PySDL2](https://pysdl2.readthedocs.io) - A ctypes based wrapper for the SDL2 library.  * [RenPy](https://www.renpy.org/) - A Visual Novel engine.    ## Geolocation    *Libraries for geocoding addresses and working with latitudes and longitudes.*    * [django-countries](https://github.com/SmileyChris/django-countries) - A Django app that provides a country field for models and forms.  * [GeoDjango](https://docs.djangoproject.com/en/dev/ref/contrib/gis/) - A world-class geographic web framework.  * [GeoIP](https://github.com/maxmind/geoip-api-python) - Python API for MaxMind GeoIP Legacy Database.  * [geojson](https://github.com/frewsxcv/python-geojson) - Python bindings and utilities for GeoJSON.  * [geopy](https://github.com/geopy/geopy) - Python Geocoding Toolbox.    ## HTML Manipulation    *Libraries for working with HTML and XML.*    * [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML.  * [bleach](https://github.com/mozilla/bleach) - A whitelist-based HTML sanitization and text linkification library.  * [cssutils](https://pypi.org/project/cssutils/) - A CSS library for Python.  * [html5lib](https://github.com/html5lib/html5lib-python) - A standards-compliant library for parsing and serializing HTML documents and fragments.  * [lxml](http://lxml.de/) - A very fast, easy-to-use and versatile library for handling HTML and XML.  * [MarkupSafe](https://github.com/pallets/markupsafe) - Implements a XML/HTML/XHTML Markup safe string for Python.  * [pyquery](https://github.com/gawel/pyquery) - A jQuery-like library for parsing HTML.  * [untangle](https://github.com/stchris/untangle) - Converts XML documents to Python objects for easy access.  * [WeasyPrint](http://weasyprint.org) - A visual rendering engine for HTML and CSS that can export to PDF.  * [xmldataset](https://xmldataset.readthedocs.io/en/latest/) - Simple XML Parsing.  * [xmltodict](https://github.com/martinblech/xmltodict) - Working with XML feel like you are working with JSON.    ## HTTP Clients    *Libraries for working with HTTP.*    * [grequests](https://github.com/spyoungtech/grequests) - requests + gevent for asynchronous HTTP requests.  * [httplib2](https://github.com/httplib2/httplib2) - Comprehensive HTTP client library.  * [httpx](https://github.com/encode/httpx) - A next generation HTTP client for Python.  * [requests](https://github.com/psf/requests) - HTTP Requests for Humans.  * [treq](https://github.com/twisted/treq) - Python requests like API built on top of Twisted's HTTP client.  * [urllib3](https://github.com/shazow/urllib3) - A HTTP library with thread-safe connection pooling, file post support, sanity friendly.    ## Hardware    *Libraries for programming with hardware.*    * [ino](http://inotool.org/) - Command line toolkit for working with [Arduino](https://www.arduino.cc/).  * [keyboard](https://github.com/boppreh/keyboard) - Hook and simulate global keyboard events on Windows and Linux.  * [mouse](https://github.com/boppreh/mouse) - Hook and simulate global mouse events on Windows and Linux.  * [Pingo](http://www.pingo.io/) - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc.  * [PyUserInput](https://github.com/SavinaRoja/PyUserInput) - A module for cross-platform control of the mouse and keyboard.  * [scapy](https://github.com/secdev/scapy) - A brilliant packet manipulation library.    ## Image Processing    *Libraries for manipulating images.*    * [hmap](https://github.com/rossgoodwin/hmap) - Image histogram remapping.  * [imgSeek](https://sourceforge.net/projects/imgseek/) - A project for searching a collection of images using visual similarity.  * [nude.py](https://github.com/hhatto/nude.py) - Nudity detection.  * [pagan](https://github.com/daboth/pagan) - Retro identicon (Avatar) generation based on input string and hash.  * [pillow](https://github.com/python-pillow/Pillow) - Pillow is the friendly [PIL](http://www.pythonware.com/products/pil/) fork.  * [python-barcode](https://github.com/WhyNotHugo/python-barcode) - Create barcodes in Python with no extra dependencies.  * [pygram](https://github.com/ajkumar25/pygram) - Instagram-like image filters.  * [PyMatting](http://github.com/pymatting/pymatting) - A library for alpha matting.  * [python-qrcode](https://github.com/lincolnloop/python-qrcode) - A pure Python QR Code generator.  * [pywal](https://github.com/dylanaraps/pywal) - A tool that generates color schemes from images.  * [pyvips](https://github.com/libvips/pyvips) - A fast image processing library with low memory needs.  * [Quads](https://github.com/fogleman/Quads) - Computer art based on quadtrees.  * [scikit-image](http://scikit-image.org/) - A Python library for (scientific) image processing.  * [thumbor](https://github.com/thumbor/thumbor) - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images.  * [wand](https://github.com/dahlia/wand) - Python bindings for [MagickWand](http://www.imagemagick.org/script/magick-wand.php), C API for ImageMagick.    ## Implementations    *Implementations of Python.*    * [CLPython](https://github.com/metawilm/cl-python) - Implementation of the Python programming language written in Common Lisp.  * [CPython](https://github.com/python/cpython) - **Default, most widely used implementation of the Python programming language written in C.**  * [Cython](http://cython.org/) - Optimizing Static Compiler for Python.  * [Grumpy](https://github.com/google/grumpy) - More compiler than interpreter as more powerful CPython2.7 replacement (alpha).  * [IronPython](https://github.com/IronLanguages/ironpython3) - Implementation of the Python programming language written in C#.  * [Jython](https://hg.python.org/jython) - Implementation of Python programming language written in Java for the JVM.  * [MicroPython](https://github.com/micropython/micropython) - A lean and efficient Python programming language implementation.  * [Numba](http://numba.pydata.org/) - Python JIT compiler to LLVM aimed at scientific Python.  * [PeachPy](https://github.com/Maratyszcza/PeachPy) - x86-64 assembler embedded in Python.  * [Pyjion](https://github.com/Microsoft/Pyjion) - A JIT for Python based upon CoreCLR.  * [PyPy](https://foss.heptapod.net/pypy/pypy) - A very fast and compliant implementation of the Python language.  * [Pyston](https://github.com/dropbox/pyston) - A Python implementation using JIT techniques.  * [Stackless Python](https://github.com/stackless-dev/stackless) - An enhanced version of the Python programming language.    ## Interactive Interpreter    *Interactive Python interpreters (REPL).*    * [bpython](https://github.com/bpython/bpython) - A fancy interface to the Python interpreter.  * [Jupyter Notebook (IPython)](https://jupyter.org) - A rich toolkit to help you make the most out of using Python interactively.      * [awesome-jupyter](https://github.com/markusschanta/awesome-jupyter)  * [ptpython](https://github.com/jonathanslenders/ptpython) - Advanced Python REPL built on top of the [python-prompt-toolkit](https://github.com/jonathanslenders/python-prompt-toolkit).    ## Internationalization    *Libraries for working with i18n.*    * [Babel](http://babel.pocoo.org/en/latest/) - An internationalization library for Python.  * [PyICU](https://github.com/ovalhub/pyicu) - A wrapper of International Components for Unicode C++ library ([ICU](http://site.icu-project.org/)).    ## Job Scheduler    *Libraries for scheduling jobs.*    * [Airflow](https://airflow.apache.org/) - Airflow is a platform to programmatically author, schedule and monitor workflows.  * [APScheduler](http://apscheduler.readthedocs.io/en/latest/) - A light but powerful in-process task scheduler that lets you schedule functions.  * [django-schedule](https://github.com/thauber/django-schedule) - A calendaring app for Django.  * [doit](http://pydoit.org/) - A task runner and build tool.  * [gunnery](https://github.com/gunnery/gunnery) - Multipurpose task execution tool for distributed systems with web-based interface.  * [Joblib](https://joblib.readthedocs.io/) - A set of tools to provide lightweight pipelining in Python.  * [Plan](https://github.com/fengsp/plan) - Writing crontab file in Python like a charm.  * [Prefect](https://github.com/PrefectHQ/prefect) - A modern workflow orchestration framework that makes it easy to build, schedule and monitor robust data pipelines.  * [schedule](https://github.com/dbader/schedule) - Python job scheduling for humans.  * [Spiff](https://github.com/knipknap/SpiffWorkflow) - A powerful workflow engine implemented in pure Python.  * [TaskFlow](https://docs.openstack.org/developer/taskflow/) - A Python library that helps to make task execution easy, consistent and reliable.    ## Logging    *Libraries for generating and working with logs.*    * [logbook](http://logbook.readthedocs.io/en/stable/) - Logging replacement for Python.  * [logging](https://docs.python.org/3/library/logging.html) - (Python standard library) Logging facility for Python.  * [loguru](https://github.com/Delgan/loguru) - Library which aims to bring enjoyable logging in Python.  * [sentry-python](https://github.com/getsentry/sentry-python) - Sentry SDK for Python.  * [structlog](https://www.structlog.org/en/stable/) - Structured logging made easy.    ## Machine Learning    *Libraries for Machine Learning. Also see [awesome-machine-learning](https://github.com/josephmisiti/awesome-machine-learning#python).*    * [gym](https://github.com/openai/gym) - A toolkit for developing and comparing reinforcement learning algorithms.  * [H2O](https://github.com/h2oai/h2o-3) - Open Source Fast Scalable Machine Learning Platform.  * [Metrics](https://github.com/benhamner/Metrics) - Machine learning evaluation metrics.  * [NuPIC](https://github.com/numenta/nupic) - Numenta Platform for Intelligent Computing.  * [scikit-learn](http://scikit-learn.org/) - The most popular Python library for Machine Learning.  * [Spark ML](http://spark.apache.org/docs/latest/ml-guide.html) - [Apache Spark](http://spark.apache.org/)'s scalable Machine Learning library.  * [vowpal_porpoise](https://github.com/josephreisinger/vowpal_porpoise) - A lightweight Python wrapper for [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/).  * [xgboost](https://github.com/dmlc/xgboost) - A scalable, portable, and distributed gradient boosting library.  * [MindsDB](https://github.com/mindsdb/mindsdb) - MindsDB is an open source AI layer for existing databases that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models using standard queries.    ## Microsoft Windows    *Python programming on Microsoft Windows.*    * [Python(x,y)](http://python-xy.github.io/) - Scientific-applications-oriented Python Distribution based on Qt and Spyder.  * [pythonlibs](http://www.lfd.uci.edu/~gohlke/pythonlibs/) - Unofficial Windows binaries for Python extension packages.  * [PythonNet](https://github.com/pythonnet/pythonnet) - Python Integration with the .NET Common Language Runtime (CLR).  * [PyWin32](https://github.com/mhammond/pywin32) - Python Extensions for Windows.  * [WinPython](https://winpython.github.io/) - Portable development environment for Windows 7/8.    ## Miscellaneous    *Useful libraries or tools that don't fit in the categories above.*    * [blinker](https://github.com/jek/blinker) - A fast Python in-process signal/event dispatching system.  * [boltons](https://github.com/mahmoud/boltons) - A set of pure-Python utilities.  * [itsdangerous](https://github.com/pallets/itsdangerous) - Various helpers to pass trusted data to untrusted environments.  * [magenta](https://github.com/magenta/magenta) - A tool to generate music and art using artificial intelligence.  * [pluginbase](https://github.com/mitsuhiko/pluginbase) - A simple but flexible plugin system for Python.  * [tryton](http://www.tryton.org/) - A general purpose business framework.    ## Natural Language Processing    *Libraries for working with human languages.*    - General      * [gensim](https://github.com/RaRe-Technologies/gensim) - Topic Modeling for Humans.      * [langid.py](https://github.com/saffsd/langid.py) - Stand-alone language identification system.      * [nltk](http://www.nltk.org/) - A leading platform for building Python programs to work with human language data.      * [pattern](https://github.com/clips/pattern) - A web mining module.      * [polyglot](https://github.com/aboSamoor/polyglot) - Natural language pipeline supporting hundreds of languages.      * [pytext](https://github.com/facebookresearch/pytext) - A natural language modeling framework based on PyTorch.      * [PyTorch-NLP](https://github.com/PetrochukM/PyTorch-NLP) - A toolkit enabling rapid deep learning NLP prototyping for research.      * [spacy](https://spacy.io/) - A library for industrial-strength natural language processing in Python and Cython.      * [Stanza](https://github.com/stanfordnlp/stanza) - The Stanford NLP Group's official Python library, supporting 60+ languages.  - Chinese      * [funNLP](https://github.com/fighting41love/funNLP) - A collection of tools and datasets for Chinese NLP.      * [jieba](https://github.com/fxsjy/jieba) - The most popular Chinese text segmentation library.      * [pkuseg-python](https://github.com/lancopku/pkuseg-python) - A toolkit for Chinese word segmentation in various domains.      * [snownlp](https://github.com/isnowfy/snownlp) - A library for processing Chinese text.    ## Network Virtualization    *Tools and libraries for Virtual Networking and SDN (Software Defined Networking).*    * [mininet](https://github.com/mininet/mininet) - A popular network emulator and API written in Python.  * [napalm](https://github.com/napalm-automation/napalm) - Cross-vendor API to manipulate network devices.  * [pox](https://github.com/noxrepo/pox) - A Python-based SDN control applications, such as OpenFlow SDN controllers.    ## News Feed    *Libraries for building user's activities.*    * [django-activity-stream](https://github.com/justquick/django-activity-stream) - Generating generic activity streams from the actions on your site.  * [Stream Framework](https://github.com/tschellenbach/Stream-Framework) - Building news feed and notification systems using Cassandra and Redis.    ## ORM    *Libraries that implement Object-Relational Mapping or data mapping techniques.*    * Relational Databases      * [Django Models](https://docs.djangoproject.com/en/dev/topics/db/models/) - The Django ORM.      * [SQLAlchemy](https://www.sqlalchemy.org/) - The Python SQL Toolkit and Object Relational Mapper.          * [awesome-sqlalchemy](https://github.com/dahlia/awesome-sqlalchemy)      * [dataset](https://github.com/pudo/dataset) - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL.      * [orator](https://github.com/sdispater/orator) -  The Orator ORM provides a simple yet beautiful ActiveRecord implementation.      * [orm](https://github.com/encode/orm) - An async ORM.      * [peewee](https://github.com/coleifer/peewee) - A small, expressive ORM.      * [pony](https://github.com/ponyorm/pony/) - ORM that provides a generator-oriented interface to SQL.      * [pydal](https://github.com/web2py/pydal/) - A pure Python Database Abstraction Layer.  * NoSQL Databases      * [hot-redis](https://github.com/stephenmcd/hot-redis) - Rich Python data types for Redis.      * [mongoengine](https://github.com/MongoEngine/mongoengine) - A Python Object-Document-Mapper for working with MongoDB.      * [PynamoDB](https://github.com/pynamodb/PynamoDB) - A Pythonic interface for [Amazon DynamoDB](https://aws.amazon.com/dynamodb/).      * [redisco](https://github.com/kiddouk/redisco) - A Python Library for Simple Models and Containers Persisted in Redis.    ## Package Management    *Libraries for package and dependency management.*    * [pip](https://pip.pypa.io/en/stable/) - The package installer for Python.      * [pip-tools](https://github.com/jazzband/pip-tools) - A set of tools to keep your pinned Python dependencies fresh.      * [PyPI](https://pypi.org/)  * [conda](https://github.com/conda/conda/) - Cross-platform, Python-agnostic binary package manager.  * [poetry](https://github.com/sdispater/poetry) - Python dependency management and packaging made easy.    ## Package Repositories    *Local PyPI repository server and proxies.*    * [bandersnatch](https://github.com/pypa/bandersnatch/) - PyPI mirroring tool provided by Python Packaging Authority (PyPA).  * [devpi](https://github.com/devpi/devpi) - PyPI server and packaging/testing/release tool.  * [localshop](https://github.com/jazzband/localshop) - Local PyPI server (custom packages and auto-mirroring of pypi).  * [warehouse](https://github.com/pypa/warehouse) - Next generation Python Package Repository (PyPI).    ## Penetration Testing    *Frameworks and tools for penetration testing.*    * [fsociety](https://github.com/Manisso/fsociety) - A Penetration testing framework.  * [setoolkit](https://github.com/trustedsec/social-engineer-toolkit) - A toolkit for social engineering.  * [sqlmap](https://github.com/sqlmapproject/sqlmap) - Automatic SQL injection and database takeover tool.    ## Permissions    *Libraries that allow or deny users access to data or functionality.*    * [django-guardian](https://github.com/django-guardian/django-guardian) - Implementation of per object permissions for Django 1.2+  * [django-rules](https://github.com/dfunckt/django-rules) - A tiny but powerful app providing object-level permissions to Django, without requiring a database.    ## Processes    *Libraries for starting and communicating with OS processes.*    * [delegator.py](https://github.com/amitt001/delegator.py) - [Subprocesses](https://docs.python.org/3/library/subprocess.html) for Humans 2.0.  * [sarge](https://sarge.readthedocs.io/en/latest/) - Yet another wrapper for subprocess.  * [sh](https://github.com/amoffat/sh) - A full-fledged subprocess replacement for Python.    ## Recommender Systems    *Libraries for building recommender systems.*    * [annoy](https://github.com/spotify/annoy) - Approximate Nearest Neighbors in C++/Python optimized for memory usage.  * [fastFM](https://github.com/ibayer/fastFM) - A library for Factorization Machines.  * [implicit](https://github.com/benfred/implicit) - A fast Python implementation of collaborative filtering for implicit datasets.  * [libffm](https://github.com/guestwalk/libffm) - A library for Field-aware Factorization Machine (FFM).  * [lightfm](https://github.com/lyst/lightfm) - A Python implementation of a number of popular recommendation algorithms.  * [spotlight](https://github.com/maciejkula/spotlight) - Deep recommender models using PyTorch.  * [Surprise](https://github.com/NicolasHug/Surprise) - A scikit for building and analyzing recommender systems.  * [tensorrec](https://github.com/jfkirk/tensorrec) - A Recommendation Engine Framework in TensorFlow.    ## Refactoring    *Refactoring tools and libraries for Python*     * [Bicycle Repair Man](http://bicyclerepair.sourceforge.net/) - Bicycle Repair Man, a refactoring tool for Python.   * [Bowler](https://pybowler.io/) - Safe code refactoring for modern Python.   * [Rope](https://github.com/python-rope/rope) -  Rope is a python refactoring library.    ## RESTful API    *Libraries for building RESTful APIs.*    * Django      * [django-rest-framework](http://www.django-rest-framework.org/) - A powerful and flexible toolkit to build web APIs.      * [django-tastypie](http://tastypieapi.org/) - Creating delicious APIs for Django apps.  * Flask      * [eve](https://github.com/pyeve/eve) - REST API framework powered by Flask, MongoDB and good intentions.      * [flask-api](https://github.com/flask-api/flask-api) - Browsable Web APIs for Flask.      * [flask-restful](https://github.com/flask-restful/flask-restful) - Quickly building REST APIs for Flask.  * Pyramid      * [cornice](https://github.com/Cornices/cornice) - A RESTful framework for Pyramid.  * Framework agnostic      * [apistar](https://github.com/encode/apistar) - A smart Web API framework, designed for Python 3.      * [falcon](https://github.com/falconry/falcon) - A high-performance framework for building cloud APIs and web app backends.      * [fastapi](https://github.com/tiangolo/fastapi) - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints.      * [hug](https://github.com/hugapi/hug) - A Python 3 framework for cleanly exposing APIs.      * [sandman2](https://github.com/jeffknupp/sandman2) - Automated REST APIs for existing database-driven systems.      * [sanic](https://github.com/huge-success/sanic) - A Python 3.6+ web server and web framework that's written to go fast.      * [vibora](https://vibora.io/) - Fast, efficient and asynchronous Web framework inspired by Flask.    ## Robotics    *Libraries for robotics.*    * [PythonRobotics](https://github.com/AtsushiSakai/PythonRobotics) - This is a compilation of various robotics algorithms with visualizations.  * [rospy](http://wiki.ros.org/rospy) - This is a library for ROS (Robot Operating System).    ## RPC Servers    *RPC-compatible servers.*    * [RPyC](https://github.com/tomerfiliba/rpyc) (Remote Python Call) - A transparent and symmetric RPC library for Python  * [zeroRPC](https://github.com/0rpc/zerorpc-python) - zerorpc is a flexible RPC implementation based on [ZeroMQ](http://zeromq.org/) and [MessagePack](http://msgpack.org/).    ## Science    *Libraries for scientific computing. Also see [Python-for-Scientists](https://github.com/TomNicholas/Python-for-Scientists).*    * [astropy](http://www.astropy.org/) - A community Python library for Astronomy.  * [bcbio-nextgen](https://github.com/chapmanb/bcbio-nextgen) - Providing best-practice pipelines for fully automated high throughput sequencing analysis.  * [bccb](https://github.com/chapmanb/bcbb) - Collection of useful code related to biological analysis.  * [Biopython](http://biopython.org/wiki/Main_Page) - Biopython is a set of freely available tools for biological computation.  * [cclib](http://cclib.github.io/) - A library for parsing and interpreting the results of computational chemistry packages.  * [Colour](http://colour-science.org/) - Implementing a comprehensive number of colour theory transformations and algorithms.  * [Karate Club](https://github.com/benedekrozemberczki/karateclub) - Unsupervised machine learning toolbox for graph structured data.  * [NetworkX](https://networkx.github.io/) - A high-productivity software for complex networks.  * [NIPY](http://nipy.org) - A collection of neuroimaging toolkits.  * [NumPy](http://www.numpy.org/) - A fundamental package for scientific computing with Python.  * [ObsPy](https://github.com/obspy/obspy/wiki/) - A Python toolbox for seismology.  * [Open Babel](http://openbabel.org/wiki/Main_Page) - A chemical toolbox designed to speak the many languages of chemical data.  * [PyDy](http://www.pydy.org/) - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion.  * [PyMC](https://github.com/pymc-devs/pymc3) - Markov Chain Monte Carlo sampling toolkit.  * [QuTiP](http://qutip.org/) - Quantum Toolbox in Python.  * [RDKit](http://www.rdkit.org/) - Cheminformatics and Machine Learning Software.  * [SciPy](https://www.scipy.org/) - A Python-based ecosystem of open-source software for mathematics, science, and engineering.  * [SimPy](https://gitlab.com/team-simpy/simpy) -  A process-based discrete-event simulation framework.  * [statsmodels](https://github.com/statsmodels/statsmodels) - Statistical modeling and econometrics in Python.  * [SymPy](https://github.com/sympy/sympy) - A Python library for symbolic mathematics.  * [Zipline](https://github.com/quantopian/zipline) - A Pythonic algorithmic trading library.    ## Search    *Libraries and software for indexing and performing search queries on data.*    * [django-haystack](https://github.com/django-haystack/django-haystack) - Modular search for Django.  * [elasticsearch-dsl-py](https://github.com/elastic/elasticsearch-dsl-py) - The official high-level Python client for Elasticsearch.  * [elasticsearch-py](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html) - The official low-level Python client for [Elasticsearch](https://www.elastic.co/products/elasticsearch).  * [pysolr](https://github.com/django-haystack/pysolr) - A lightweight Python wrapper for [Apache Solr](https://lucene.apache.org/solr/).  * [whoosh](http://whoosh.readthedocs.io/en/latest/) - A fast, pure Python search engine library.    ## Serialization    *Libraries for serializing complex data types*    * [marshmallow](https://github.com/marshmallow-code/marshmallow) - A lightweight library for converting complex objects to and from simple Python datatypes.  * [pysimdjson](https://github.com/TkTech/pysimdjson) - A Python bindings for [simdjson](https://github.com/lemire/simdjson).  * [python-rapidjson](https://github.com/python-rapidjson/python-rapidjson) - A Python wrapper around [RapidJSON](https://github.com/Tencent/rapidjson).  * [ultrajson](https://github.com/esnme/ultrajson) - A fast JSON decoder and encoder written in C with Python bindings.    ## Serverless Frameworks    *Frameworks for developing serverless Python code.*    * [python-lambda](https://github.com/nficano/python-lambda) - A toolkit for developing and deploying Python code in AWS Lambda.  * [Zappa](https://github.com/Miserlou/Zappa) - A tool for deploying WSGI applications on AWS Lambda and API Gateway.    ## Shell    *Shells based on Python.*    * [xonsh](https://github.com/xonsh/xonsh/) - A Python-powered, cross-platform, Unix-gazing shell language and command prompt.    ## Specific Formats Processing    *Libraries for parsing and manipulating specific text formats.*    * General      * [tablib](https://github.com/jazzband/tablib) - A module for Tabular Datasets in XLS, CSV, JSON, YAML.  * Office      * [docxtpl](https://github.com/elapouya/python-docx-template) - Editing a docx document by jinja2 template      * [openpyxl](https://openpyxl.readthedocs.io/en/stable/) - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files.      * [pyexcel](https://github.com/pyexcel/pyexcel) - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files.      * [python-docx](https://github.com/python-openxml/python-docx) - Reads, queries and modifies Microsoft Word 2007/2008 docx files.      * [python-pptx](https://github.com/scanny/python-pptx) - Python library for creating and updating PowerPoint (.pptx) files.      * [unoconv](https://github.com/unoconv/unoconv) - Convert between any document format supported by LibreOffice/OpenOffice.      * [XlsxWriter](https://github.com/jmcnamara/XlsxWriter) - A Python module for creating Excel .xlsx files.      * [xlwings](https://github.com/ZoomerAnalytics/xlwings) - A BSD-licensed library that makes it easy to call Python from Excel and vice versa.      * [xlwt](https://github.com/python-excel/xlwt) / [xlrd](https://github.com/python-excel/xlrd) - Writing and reading data and formatting information from Excel files.  * PDF      * [PDFMiner](https://github.com/euske/pdfminer) - A tool for extracting information from PDF documents.      * [PyPDF2](https://github.com/mstamy2/PyPDF2) - A library capable of splitting, merging and transforming PDF pages.      * [ReportLab](https://www.reportlab.com/opensource/) - Allowing Rapid creation of rich PDF documents.  * Markdown      * [Mistune](https://github.com/lepture/mistune) - Fastest and full featured pure Python parsers of Markdown.      * [Python-Markdown](https://github.com/waylan/Python-Markdown) - A Python implementation of John Gruber’s Markdown.  * YAML      * [PyYAML](http://pyyaml.org/) - YAML implementations for Python.  * CSV      * [csvkit](https://github.com/wireservice/csvkit) - Utilities for converting to and working with CSV.  * Archive      * [unp](https://github.com/mitsuhiko/unp) - A command line tool that can unpack archives easily.    ## Static Site Generator    *Static site generator is a software that takes some text + templates as input and produces HTML files on the output.*    * [lektor](https://github.com/lektor/lektor) - An easy to use static CMS and blog engine.  * [mkdocs](https://github.com/mkdocs/mkdocs/) - Markdown friendly documentation generator.  * [makesite](https://github.com/sunainapai/makesite) - Simple, lightweight, and magic-free static site/blog generator (< 130 lines).  * [nikola](https://github.com/getnikola/nikola) - A static website and blog generator.  * [pelican](https://github.com/getpelican/pelican) - Static site generator that supports Markdown and reST syntax.    ## Tagging    *Libraries for tagging items.*    * [django-taggit](https://github.com/jazzband/django-taggit) - Simple tagging for Django.    ## Task Queues    *Libraries for working with task queues.*    * [celery](https://docs.celeryproject.org/en/stable/) - An asynchronous task queue/job queue based on distributed message passing.  * [dramatiq](https://github.com/Bogdanp/dramatiq) - A fast and reliable background task processing library for Python 3.  * [huey](https://github.com/coleifer/huey) - Little multi-threaded task queue.  * [mrq](https://github.com/pricingassistant/mrq) - A distributed worker task queue in Python using Redis & gevent.  * [rq](https://github.com/rq/rq) - Simple job queues for Python.    ## Template Engine    *Libraries and tools for templating and lexing.*    * [Genshi](https://genshi.edgewall.org/) - Python templating toolkit for generation of web-aware output.  * [Jinja2](https://github.com/pallets/jinja) - A modern and designer friendly templating language.  * [Mako](http://www.makotemplates.org/) - Hyperfast and lightweight templating for the Python platform.    ## Testing    *Libraries for testing codebases and generating test data.*    * Testing Frameworks      * [hypothesis](https://github.com/HypothesisWorks/hypothesis) - Hypothesis is an advanced Quickcheck style property based testing library.      * [nose2](https://github.com/nose-devs/nose2) - The successor to `nose`, based on `unittest2.      * [pytest](https://docs.pytest.org/en/latest/) - A mature full-featured Python testing tool.      * [Robot Framework](https://github.com/robotframework/robotframework) - A generic test automation framework.      * [unittest](https://docs.python.org/3/library/unittest.html) - (Python standard library) Unit testing framework.  * Test Runners      * [green](https://github.com/CleanCut/green) - A clean, colorful test runner.      * [mamba](http://nestorsalceda.github.io/mamba/) - The definitive testing tool for Python. Born under the banner of BDD.      * [tox](https://tox.readthedocs.io/en/latest/) - Auto builds and tests distributions in multiple Python versions  * GUI / Web Testing      * [locust](https://github.com/locustio/locust) - Scalable user load testing tool written in Python.      * [PyAutoGUI](https://github.com/asweigart/pyautogui) - PyAutoGUI is a cross-platform GUI automation Python module for human beings.      * [Schemathesis](https://github.com/kiwicom/schemathesis) - A tool for automatic property-based testing of web applications built with Open API / Swagger specifications.      * [Selenium](https://pypi.org/project/selenium/) - Python bindings for [Selenium](http://www.seleniumhq.org/) WebDriver.      * [sixpack](https://github.com/seatgeek/sixpack) - A language-agnostic A/B Testing framework.      * [splinter](https://github.com/cobrateam/splinter) - Open source tool for testing web applications.  * Mock      * [doublex](https://pypi.org/project/doublex/) - Powerful test doubles framework for Python.      * [freezegun](https://github.com/spulec/freezegun) - Travel through time by mocking the datetime module.      * [httmock](https://github.com/patrys/httmock) - A mocking library for requests for Python 2.6+ and 3.2+.      * [httpretty](https://github.com/gabrielfalcao/HTTPretty) - HTTP request mock tool for Python.      * [mock](https://docs.python.org/3/library/unittest.mock.html) - (Python standard library) A mocking and patching library.      * [mocket](https://github.com/mindflayer/python-mocket) - A socket mock framework with gevent/asyncio/SSL support.      * [responses](https://github.com/getsentry/responses) - A utility library for mocking out the requests Python library.      * [VCR.py](https://github.com/kevin1024/vcrpy) - Record and replay HTTP interactions on your tests.  * Object Factories      * [factory_boy](https://github.com/FactoryBoy/factory_boy) - A test fixtures replacement for Python.      * [mixer](https://github.com/klen/mixer) - Another fixtures replacement. Supports Django, Flask, SQLAlchemy, Peewee and etc.      * [model_mommy](https://github.com/vandersonmota/model_mommy) - Creating random fixtures for testing in Django.  * Code Coverage      * [coverage](https://pypi.org/project/coverage/) - Code coverage measurement.  * Fake Data      * [fake2db](https://github.com/emirozer/fake2db) - Fake database generator.      * [faker](https://github.com/joke2k/faker) - A Python package that generates fake data.      * [mimesis](https://github.com/lk-geimfari/mimesis) - is a Python library that help you generate fake data.      * [radar](https://pypi.org/project/radar/) - Generate random datetime / time.    ## Text Processing    *Libraries for parsing and manipulating plain texts.*    * General      * [chardet](https://github.com/chardet/chardet) - Python 2/3 compatible character encoding detector.      * [difflib](https://docs.python.org/3/library/difflib.html) - (Python standard library) Helpers for computing deltas.      * [ftfy](https://github.com/LuminosoInsight/python-ftfy) - Makes Unicode text less broken and more consistent automagically.      * [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy) - Fuzzy String Matching.      * [Levenshtein](https://github.com/ztane/python-Levenshtein/) - Fast computation of Levenshtein distance and string similarity.      * [pangu.py](https://github.com/vinta/pangu.py) - Paranoid text spacing.      * [pyfiglet](https://github.com/pwaller/pyfiglet) - An implementation of figlet written in Python.      * [pypinyin](https://github.com/mozillazg/python-pinyin) - Convert Chinese hanzi (漢字) to pinyin (拼音).      * [textdistance](https://github.com/orsinium/textdistance) - Compute distance between sequences with 30+ algorithms.      * [unidecode](https://pypi.org/project/Unidecode/) - ASCII transliterations of Unicode text.  * Slugify      * [awesome-slugify](https://github.com/dimka665/awesome-slugify) - A Python slugify library that can preserve unicode.      * [python-slugify](https://github.com/un33k/python-slugify) - A Python slugify library that translates unicode to ASCII.      * [unicode-slugify](https://github.com/mozilla/unicode-slugify) - A slugifier that generates unicode slugs with Django as a dependency.  * Unique identifiers      * [hashids](https://github.com/davidaurelio/hashids-python) - Implementation of [hashids](http://hashids.org) in Python.      * [shortuuid](https://github.com/skorokithakis/shortuuid) - A generator library for concise, unambiguous and URL-safe UUIDs.  * Parser      * [ply](https://github.com/dabeaz/ply) - Implementation of lex and yacc parsing tools for Python.      * [pygments](http://pygments.org/) - A generic syntax highlighter.      * [pyparsing](https://github.com/pyparsing/pyparsing) - A general purpose framework for generating parsers.      * [python-nameparser](https://github.com/derek73/python-nameparser) - Parsing human names into their individual components.      * [python-phonenumbers](https://github.com/daviddrysdale/python-phonenumbers) - Parsing, formatting, storing and validating international phone numbers.      * [python-user-agents](https://github.com/selwin/python-user-agents) - Browser user agent parser.      * [sqlparse](https://github.com/andialbrecht/sqlparse) - A non-validating SQL parser.    ## Third-party APIs    *Libraries for accessing third party services APIs. Also see [List of Python API Wrappers and Libraries](https://github.com/realpython/list-of-python-api-wrappers).*    * [apache-libcloud](https://libcloud.apache.org/) - One Python library for all clouds.  * [boto3](https://github.com/boto/boto3) - Python interface to Amazon Web Services.  * [django-wordpress](https://github.com/istrategylabs/django-wordpress) - WordPress models and views for Django.  * [facebook-sdk](https://github.com/mobolic/facebook-sdk) - Facebook Platform Python SDK.  * [google-api-python-client](https://github.com/google/google-api-python-client) - Google APIs Client Library for Python.  * [gspread](https://github.com/burnash/gspread) - Google Spreadsheets Python API.  * [twython](https://github.com/ryanmcgrath/twython) - A Python wrapper for the Twitter API.    ## URL Manipulation    *Libraries for parsing URLs.*    * [furl](https://github.com/gruns/furl) - A small Python library that makes parsing and manipulating URLs easy.  * [purl](https://github.com/codeinthehole/purl) - A simple, immutable URL class with a clean API for interrogation and manipulation.  * [pyshorteners](https://github.com/ellisonleao/pyshorteners) - A pure Python URL shortening lib.  * [webargs](https://github.com/marshmallow-code/webargs) - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks.    ## Video    *Libraries for manipulating video and GIFs.*    * [moviepy](https://zulko.github.io/moviepy/) - A module for script-based movie editing with many formats, including animated GIFs.  * [scikit-video](https://github.com/aizvorski/scikit-video) - Video processing routines for SciPy.  * [vidgear](https://github.com/abhiTronix/vidgear) - Most Powerful multi-threaded Video Processing framework.    ## Web Asset Management    *Tools for managing, compressing and minifying website assets.*    * [django-compressor](https://github.com/django-compressor/django-compressor) - Compresses linked and inline JavaScript or CSS into a single cached file.  * [django-pipeline](https://github.com/jazzband/django-pipeline) - An asset packaging library for Django.  * [django-storages](https://github.com/jschneier/django-storages) - A collection of custom storage back ends for Django.  * [fanstatic](http://www.fanstatic.org/en/latest/) - Packages, optimizes, and serves static file dependencies as Python packages.  * [fileconveyor](http://wimleers.com/fileconveyor) - A daemon to detect and sync files to CDNs, S3 and FTP.  * [flask-assets](https://github.com/miracle2k/flask-assets) - Helps you integrate webassets into your Flask app.  * [webassets](https://github.com/miracle2k/webassets) - Bundles, optimizes, and manages unique cache-busting URLs for static resources.    ## Web Content Extracting    *Libraries for extracting web contents.*    * [html2text](https://github.com/Alir3z4/html2text) - Convert HTML to Markdown-formatted text.  * [lassie](https://github.com/michaelhelmick/lassie) - Web Content Retrieval for Humans.  * [micawber](https://github.com/coleifer/micawber) - A small library for extracting rich content from URLs.  * [newspaper](https://github.com/codelucas/newspaper) - News extraction, article extraction and content curation in Python.  * [python-readability](https://github.com/buriy/python-readability) - Fast Python port of arc90's readability tool.  * [requests-html](https://github.com/psf/requests-html) - Pythonic HTML Parsing for Humans.  * [sumy](https://github.com/miso-belica/sumy) - A module for automatic summarization of text documents and HTML pages.  * [textract](https://github.com/deanmalmgren/textract) - Extract text from any document, Word, PowerPoint, PDFs, etc.  * [toapi](https://github.com/gaojiuli/toapi) - Every web site provides APIs.    ## Web Crawling    *Libraries to automate web scraping.*    * [cola](https://github.com/chineking/cola) - A distributed crawling framework.  * [feedparser](https://pythonhosted.org/feedparser/) - Universal feed parser.  * [grab](https://github.com/lorien/grab) - Site scraping framework.  * [MechanicalSoup](https://github.com/MechanicalSoup/MechanicalSoup) - A Python library for automating interaction with websites.  * [portia](https://github.com/scrapinghub/portia) - Visual scraping for Scrapy.  * [pyspider](https://github.com/binux/pyspider) - A powerful spider system.  * [robobrowser](https://github.com/jmcarp/robobrowser) - A simple, Pythonic library for browsing the web without a standalone web browser.  * [scrapy](https://scrapy.org/) - A fast high-level screen scraping and web crawling framework.    ## Web Frameworks    *Traditional full stack web frameworks. Also see [RESTful API](https://github.com/vinta/awesome-python#restful-api).*    * Synchronous      * [Django](https://www.djangoproject.com/) - The most popular web framework in Python.          * [awesome-django](https://github.com/shahraizali/awesome-django)          * [awesome-django](https://github.com/wsvincent/awesome-django)      * [Flask](http://flask.pocoo.org/) - A microframework for Python.          * [awesome-flask](https://github.com/humiaozuzu/awesome-flask)      * [Pyramid](https://pylonsproject.org/) - A small, fast, down-to-earth, open source Python web framework.          * [awesome-pyramid](https://github.com/uralbash/awesome-pyramid)      * [Masonite](https://github.com/MasoniteFramework/masonite) - The modern and developer centric Python web framework.  * Asynchronous      * [Tornado](http://www.tornadoweb.org/en/latest/) - A web framework and asynchronous networking library.    ## WebSocket    *Libraries for working with WebSocket.*    * [autobahn-python](https://github.com/crossbario/autobahn-python) - WebSocket & WAMP for Python on Twisted and [asyncio](https://docs.python.org/3/library/asyncio.html).  * [channels](https://github.com/django/channels) - Developer-friendly asynchrony for Django.  * [websockets](https://github.com/aaugustin/websockets) - A library for building WebSocket servers and clients with a focus on correctness and simplicity.    ## WSGI Servers    *WSGI-compatible web servers.*    * [bjoern](https://github.com/jonashaag/bjoern) - Asynchronous, very fast and written in C.  * [gunicorn](https://github.com/benoitc/gunicorn) - Pre-forked, ported from Ruby's Unicorn project.  * [uWSGI](https://uwsgi-docs.readthedocs.io/en/latest/) - A project aims at developing a full stack for building hosting services, written in C.  * [waitress](https://github.com/Pylons/waitress) - Multi-threaded, powers Pyramid.  * [werkzeug](https://github.com/pallets/werkzeug) - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects.    # Resources    Where to discover learning resources or new Python libraries.    ## Books    - [Fluent Python](https://www.oreilly.com/library/view/fluent-python/9781491946237/)  - [Think Python](https://greenteapress.com/wp/think-python-2e/)    ## Websites    * Tutorials      * [Full Stack Python](https://www.fullstackpython.com/)      * [Python Cheatsheet](https://www.pythoncheatsheet.org/)      * [Real Python](https://realpython.com)      * [The Hitchhiker’s Guide to Python](https://docs.python-guide.org/)      * [Ultimate Python study guide](https://github.com/huangsam/ultimate-python)  * Libraries      * [Awesome Python @LibHunt](https://python.libhunt.com/)  * Others      * [Python ZEEF](https://python.zeef.com/alan.richmond)      * [Pythonic News](https://news.python.sc/)      * [What the f*ck Python!](https://github.com/satwikkansal/wtfpython)    ## Newsletters    * [Awesome Python Newsletter](http://python.libhunt.com/newsletter)  * [Pycoder's Weekly](http://pycoders.com/)  * [Python Tricks](https://realpython.com/python-tricks/)  * [Python Weekly](http://www.pythonweekly.com/)    ## Podcasts    * [Django Chat](https://djangochat.com/)  * [Podcast.\_\_init__](https://podcastinit.com/)  * [Python Bytes](https://pythonbytes.fm)  * [Running in Production](https://runninginproduction.com/)  * [Talk Python To Me](https://talkpython.fm/)  * [Test and Code](https://testandcode.com/)  * [The Real Python Podcast](https://realpython.com/podcasts/rpp/)    # Contributing    Your contributions are always welcome! Please take a look at the [contribution guidelines](https://github.com/vinta/awesome-python/blob/master/CONTRIBUTING.md) first.    I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could [vote for them](https://github.com/vinta/awesome-python/pulls) by adding :+1: to them. Pull requests will be merged when their votes reach **20**.    - - -    If you have any question about this opinionated list, do not hesitate to contact me [@VintaChen](https://twitter.com/VintaChen) on Twitter or open an issue on GitHub.   """
Big data;https://github.com/baidu/tera;"""# Tera - An Internet-Scale Database    [![Build Status](https://travis-ci.org/baidu/tera.svg?branch=master)](https://travis-ci.org/baidu/tera)  [![Coverity Scan Build Status](https://scan.coverity.com/projects/10959/badge.svg)](https://scan.coverity.com/projects/tera)  [![Documentation Status](https://img.shields.io/badge/中文文档-最新-brightgreen.svg)](readme-cn.md)    Copyright 2015, Baidu, Inc.    Tera is a high performance distributed NoSQL database, which is inspired by google's [BigTable](http://static.googleusercontent.com/media/research.google.com/zh-CN//archive/bigtable-osdi06.pdf) and designed for real-time applications. Tera can easily scale to __petabytes__ of data across __thousands__ of commodity servers. Besides, Tera is widely used in many Baidu products with varied demands，which range from throughput-oriented applications to latency-sensitive service, including web indexing, WebPage DB, LinkBase DB, etc. ([中文](readme-cn.md))    ## Features    * Linear and modular scalability  * Automatic and configurable sharding  * Ranged and hashed sharding strategies  * MVCC  * Column-oriented storage and locality group support  * Strictly consistent  * Automatic failover support  * Online schema change  * Snapshot support  * Support RAMDISK/SSD/DFS tiered cache  * Block cache and Bloom Filters for real-time queries  * Multi-type table support (RAMDISK/SSD/DISK table)  * Easy to use [C++](doc/en/sdk_guide.md)/[Java](doc/en/sdk_guide_java.md)/[Python](doc/en/sdk_guide_python.md)/[REST-ful](doc/en/sdk_guide_http.md) API    ## Data model    Tera is the collection of many sparse, distributed, multidimensional tables. The table is indexed by a row key, column key, and a timestamp; each value in the table is an uninterpreted array of bytes.    * (row:string, (column family+qualifier):string, time:int64) → string    To learn more about the schema, you can refer to [BigTable](http://static.googleusercontent.com/media/research.google.com/zh-CN//archive/bigtable-osdi06.pdf).    ## Architecture    ![架构图](resources/images/arch.png)    Tera has three major components: sdk, master and tablet servers.    - __SDK__: a library that is linked into every application client to access Tera cluster.  - __Master__: master is responsible for managing tablet servers and tablets, automatic load balance and garbage collection of files in filesystem.  - __Tablet Server__: tablet server is the core module in tera, and it uses an __enhance__ [Leveldb](https://github.com/google/leveldb) as a basic storage engine. Tablet server manages a set of tablets, handles read/write/scan requests and schedule tablet split and merge online.    ## Building blocks  Tera is built on several pieces of open source infrastructure.    - __Filesystem__ (required)        Tera uses the distributed file system to store transaction log and data files. So Tera uses an abstract file system interface, called Env, to adapt to different implementations of file systems (e.g., [BFS](https://github.com/baidu/bfs), HDFS, HDFS2, POXIS filesystem).    - __Distributed lock service__ (required)        Tera relies on a highly-available and persistent distributed lock service, which is used for a variety of tasks: to ensure that there is at most one active master at any time; to store meta table's location, to discover new tablet server and finalize tablet server deaths. Tera has an adapter class to adapt to different implementations of lock service (e.g., ZooKeeper, [Nexus](https://github.com/baidu/ins))    - __High performance RPC framework__ (required)        Tera is designed to handle a variety of demanding workloads, which range from throughput-oriented applications to latency-sensitive service. So Tera needs a high performance network programming framework. Now Tera heavily relies on [Sofa-pbrpc](https://github.com/baidu/sofa-pbrpc/) to meet the performance demand.    - __Cluster management system__ (not necessary)        A Tera cluster in Baidu typically operates in a shared pool of machines that runs a wide variety of other distributed applications. So Tera can be deployed in a cluster management system [Galaxy](https://github.com/baidu/galaxy), which uses for scheduling jobs, managing resources on shared machines, dealing with machine failures, and monitoring machine status. Besides, Tera can also be deployed on RAW machine or in Docker container.    ## Documents    * [Developer Doc](doc/en/README.md)    ## Quick start  * __How to build__        Use sh [./build.sh](BUILD) to build Tera.    * __How to deploy__        [Pseudo Distributed Mode](doc/en/onebox.md)        [Build on Docker](example/docker)    * __How to access__        [teracli](doc/en/teracli.md)        [API](doc/en/sdk_guide.md)    ## Contributing to Tera  Contributions are welcomed and greatly appreciated.    Read [Roadmap](doc/en/roadmap.md) to get a general knowledge about our development plan.    See [Contributions](doc/en/contributor.md) for more details.    ## Follow us  To join us, please send resume to tera-user at baidu.com.   """
Big data;https://github.com/benedekrozemberczki/shapley;"""[pypi-image]: https://badge.fury.io/py/shapley.svg  [pypi-url]: https://pypi.python.org/pypi/shapley  [size-image]: https://img.shields.io/github/repo-size/benedekrozemberczki/shapley.svg  [size-url]: https://github.com/benedekrozemberczki/shapley/archive/master.zip  [build-image]: https://github.com/benedekrozemberczki/shapley/workflows/CI/badge.svg  [build-url]: https://github.com/benedekrozemberczki/shapley/actions?query=workflow%3ACI  [docs-image]: https://readthedocs.org/projects/shapley/badge/?version=latest  [docs-url]: https://shapley.readthedocs.io/en/latest/?badge=latest  [coverage-image]: https://codecov.io/gh/benedekrozemberczki/shapley/branch/master/graph/badge.svg  [coverage-url]: https://codecov.io/github/benedekrozemberczki/shapley?branch=master  [arxiv-image]: https://img.shields.io/badge/ArXiv-2101.02153-orange.svg  [arxiv-url]: https://arxiv.org/abs/2101.02153    <p align=""center"">    <img width=""90%"" src=""https://github.com/benedekrozemberczki/shapley/raw/master/shapley.jpg?sanitize=true"" />  </p>    [![PyPI Version][pypi-image]][pypi-url]  [![Docs Status][docs-image]][docs-url]  [![Repo size][size-image]][size-url]  [![Code Coverage][coverage-image]][coverage-url]  [![Build Status][build-image]][build-url]  [![Arxiv][arxiv-image]][arxiv-url]    **[Documentation](https://shapley.readthedocs.io)** | **[External Resources](https://shapley.readthedocs.io/en/latest/notes/resources.html)**  | **[Research Paper](https://arxiv.org/abs/2101.02153)**    *Shapley* is a Python library for evaluating binary classifiers in a machine learning ensemble.    The library consists of various methods to compute (approximate) the Shapley value of players (models) in weighted voting games (ensemble games) - a class of transferable utility cooperative games. We covered the exact enumeration based computation and various widely know approximation methods from economics and computer science research papers. There are also functionalities to identify the heterogeneity of the player pool based on the [Shapley entropy](https://arxiv.org/abs/2101.02153). In addition, the framework comes with a [detailed documentation](https://shapley.readthedocs.io/en/latest/), an intuitive [tutorial](https://shapley.readthedocs.io/en/latest/notes/introduction.html), 100% test coverage, and illustrative toy [examples](https://github.com/benedekrozemberczki/shapley/tree/master/examples).    ----------------------------------------------------------    **Citing**      If you find *Shapley* useful in your research please consider adding the following citation:    ```bibtex  @inproceedings{rozemberczki2021shapley,        title = {{The Shapley Value of Classifiers in Ensemble Games}},         author = {Benedek Rozemberczki and Rik Sarkar},        year = {2021},        booktitle={Proceedings of the 30th ACM International Conference on Information and Knowledge Management},        pages = {1558–1567},  }  ```    --------------------------------------------------------------    **A simple example**    Shapley makes solving voting games quite easy - see the accompanying [tutorial](https://shapley.readthedocs.io/en/latest/notes/introduction.html#applications). For example, this is all it takes to solve a weighted voting game with defined on the fly with permutation sampling:    ```python  import numpy as np  from shapley import PermutationSampler    W = np.random.uniform(0, 1, (1, 7))  W = W/W.sum()  q = 0.5    solver = PermutationSampler()  solver.solve_game(W, q)  shapley_values = solver.get_solution()  ```  ----------------------------------------------------------------------------------    **Methods Included**    In detail, the following methods can be used.      * **[Expected Marginal Contribution Approximation](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.expected_marginal_contributions.ExpectedMarginalContributions)** from Fatima *et al.*: [A Linear Approximation Method for the Shapley Value](https://www.sciencedirect.com/science/article/pii/S0004370208000696)    * **[Multilinear Extension](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.multilinear_extension.MultilinearExtension)** from Owen: [Multilinear Extensions of Games](https://www.jstor.org/stable/2661445?seq=1#metadata_info_tab_contents)    * **[Monte Carlo Permutation Sampling](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.permutation_sampler.PermutationSampler)** from Maleki *et al.*: [Bounding the Estimation Error of Sampling-based Shapley Value Approximation](https://arxiv.org/abs/1306.4265)    * **[Exact Enumeration](https://shapley.readthedocs.io/en/latest/modules/root.html#shapley.solvers.exact_enumeration.ExactEnumeration)** from Shapley: [A Value for N-Person Games](https://www.rand.org/pubs/papers/P0295.html)    --------------------------------------------------------------------------------      Head over to our [documentation](https://shapley.readthedocs.io) to find out more about installation, creation of datasets and a full list of implemented methods and available datasets.  For a quick start, check out the [examples](https://github.com/benedekrozemberczki/shapley/tree/master/examples) in the `examples/` directory.    If you notice anything unexpected, please open an [issue](https://benedekrozemberczki/shapley/issues). If you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/shapley/issues).    --------------------------------------------------------------------------------    **Installation**    ```  $ pip install shapley  ```    **Running tests**    ```  $ python setup.py test  ```  ----------------------------------------------------------------------------------    **Running examples**    ```  $ cd examples  $ python permutation_sampler_example.py  ```    ----------------------------------------------------------------------------------    **License**    - [MIT License](https://github.com/benedekrozemberczki/shapley/blob/master/LICENSE) """
Big data;https://github.com/jakekgrog/GhostDB;"""![GhostDB logo](https://imgur.com/ZEGFVo6.png)    ![Build](https://github.com/GhostDB/GhostDB/workflows/GhostDB%20Node%20Test/badge.svg)  [![Twitter](https://img.shields.io/badge/ghostdb-v2.0.0-green)](http://www.ghostdbcache.com)    [![Discord](https://img.shields.io/badge/chat-Join%20us!-green?style=for-the-badge&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2)](https://discord.gg/ZkT5Sdf)    ## Update - 03/03/2021    ### Where we've been    GhostDB stemmed from a University project. Due to the nature of these projects (time constraints etc.), we feel some corners were cut. For example, we opted for the memcached model of distribution to save on time as it was easier to implement. However, this wasn't the original vision of GhostDB. Myself and Connor also started new jobs and these took up a good chunk of our time. This combined with just finishing a really busy final year in Univeristy, we decided to mothball the project for a while. We're finally returning to it and hopefully transforming it into what we had originally planned.     ### A new roadmap    We are revising our roadmap below and plan to release an updated version soon but before we do here is a brief rundown on what we want    - Transition away from the memcached model and move to a consistent, partition tolerant system (with limited fault tolerance too) by implementing the raft concensus protocol. (This is almost complete)  - Release a CLI to allow users to easily manage their clusters  - Re-build our SDKs from the ground up to allow users to interact with GhostDB with more ease than is currently possible.  - Implement new data types to broaden GhostDBs use cases.  - Local caching to give an even greater performance boost to users.  - Release AWS Amazon Machine Images (AMIs) and Google Compute Engine Images to allow users to easily create GhostDB clusters in the cloud with only a few clicks.  - Updates to the website that include a download centre and documentation improvements.    ### Contributing    Unfortunately, with work and life we simply don't have the time at the moment to manage pull requests from anyone else. However, we are still accepting issues and are encouraging them.    And of course, we also want to continue improving on our performance :)    ## :books: Overview    GhostDB is a distributed, in-memory, general purpose key-value data store that delivers microsecond performance at any scale.    GhostDB is designed to speed up dynamic database or API driven websites by storing data in RAM in order to reduce the number of times an external data source such as a database or API must be read. GhostDB provides a very large hash table that is distributed across multiple machines and stores large numbers of key-value pairs within the hash table.    ## :car: Roadmap    > GhostDB was a university project - it is not fully featured but we're getting there!    This is a high-level roadmap of what we want GhostDB to become by the end of 2020. If you have any feature requests please create one from the [template](https://github.com/jakekgrog/GhostDB/blob/master/docs/FEATURE_REQUEST.md) and label it as `feature request`!    - First hand support for list, set, stack and queue data structures  - Atomic command queues  - Subscribable streams  - Monitoring & administration dashboard  - Enhanced security features  - Transition to TCP sockets as transport protocol  - CLI  - Support for a wide range of programming languages    ## :wrench: Installation    To install GhostDB please consult the [installation guide](https://github.com/jakekgrog/GhostDB/blob/master/docs/INSTALL.md) for a quick walkthrough on setting up the system.    ## :hammer: Cluster Configuration    To configure a GhostDB cluster please follow the instructions in the [configuration guide](https://github.com/jakekgrog/GhostDB/blob/master/docs/INSTALL.md)    ## :pencil2: Authors    **Jake Grogan**    - Email: <jake.kgrogan@gmail.com>  - Github: [@jakekgrog](https://github.com/jakekgrog)    **Connor Mulready**    - Github: [@nohclu](https://github.com/nohclu)    ## :star: Show your support    Give a :star: if this project helped you! """
Big data;https://github.com/uber/AthenaX;"""[![Build Status][ci-img]][ci] [![ReadTheDocs][doc-img]][doc]    # AthenaX: SQL-based streaming analytics platform at scale    AthenaX is a streaming analytics platform that enables users to run production-quality, large scale streaming analytics using Structured Query Language (SQL). AthenaX was released and open sourced by [Uber Technologies][ubeross]. It is capable of scaling across hundreds of machines and processing hundreds of billions of real-time events daily.    See also:      * AthenaX [documentation][doc] for getting started, operational details, and other information.    * Blog post [Introducing AthenaX, Uber Engineering’s Open Source Streaming Analytics Platform](https://eng.uber.com/athenax/).    ## License  [Apache 2.0 License](./LICENSE).    [doc-img]: https://readthedocs.org/projects/athenax/badge/?version=latest  [doc]: http://athenax.readthedocs.org/en/latest/  [ci-img]: https://travis-ci.org/jaegertracing/jaeger.svg?branch=master  [ci]: https://travis-ci.org/uber/AthenaX  [ubeross]: http://uber.github.io """
Big data;https://github.com/strapdata/elassandra;"""# Elassandra [![Build Status](https://travis-ci.org/strapdata/elassandra.svg)](https://travis-ci.org/strapdata/elassandra) [![Documentation Status](https://readthedocs.org/projects/elassandra/badge/?version=latest)](https://elassandra.readthedocs.io/en/latest/?badge=latest) [![GitHub release](https://img.shields.io/github/v/release/strapdata/elassandra.svg)](https://github.com/strapdata/elassandra/releases/latest)  [![Twitter](https://img.shields.io/twitter/follow/strapdataio?style=social)](https://twitter.com/strapdataio)    ![Elassandra Logo](elassandra-logo.png)    ## [http://www.elassandra.io/](http://www.elassandra.io/)    Elassandra is an [Apache Cassandra](http://cassandra.apache.org) distribution including an [Elasticsearch](https://github.com/elastic/elasticsearch) search engine.  Elassandra is a multi-master multi-cloud database and search engine with support for replicating across multiple datacenters in active/active mode.    Elasticsearch code is embedded in Cassanda nodes providing advanced search features on Cassandra tables and Cassandra serves as an Elasticsearch data and configuration store.    ![Elassandra architecture](/docs/elassandra/source/images/elassandra1.jpg)    Elassandra supports Cassandra vnodes and scales horizontally by adding more nodes without the need to reshard indices.    Project documentation is available at [doc.elassandra.io](http://doc.elassandra.io).    ## Benefits of Elassandra    For Cassandra users, elassandra provides Elasticsearch features :  * Cassandra updates are indexed in Elasticsearch.  * Full-text and spatial search on your Cassandra data.  * Real-time aggregation (does not require Spark or Hadoop to GROUP BY)  * Provide search on multiple keyspaces and tables in one query.  * Provide automatic schema creation and support nested documents using [User Defined Types](https://docs.datastax.com/en/cql/3.1/cql/cql_using/cqlUseUDT.html).  * Provide read/write JSON REST access to Cassandra data.  * Numerous Elasticsearch plugins and products like [Kibana](https://www.elastic.co/guide/en/kibana/current/introduction.html).  * Manage concurrent elasticsearch mappings changes and applies batched atomic CQL schema changes.  * Support [Elasticsearch ingest processors](https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html) allowing to transform input data.    For Elasticsearch users, elassandra provides useful features :  * Elassandra is masterless. Cluster state is managed through [cassandra lightweight transactions](http://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0).  * Elassandra is a sharded multi-master database, where Elasticsearch is sharded master-slave. Thus, Elassandra has no Single Point Of Write, helping to achieve high availability.  * Elassandra inherits Cassandra data repair mechanisms (hinted handoff, read repair and nodetool repair) providing support for **cross datacenter replication**.  * When adding a node to an Elassandra cluster, only data pulled from existing nodes are re-indexed in Elasticsearch.  * Cassandra could be your unique datastore for indexed and non-indexed data. It's easier to manage and secure. Source documents are now stored in Cassandra, reducing disk space if you need a NoSQL database and Elasticsearch.  * Write operations are not restricted to one primary shard, but distributed across all Cassandra nodes in a virtual datacenter. The number of shards does not limit your write throughput. Adding elassandra nodes increases both read and write throughput.  * Elasticsearch indices can be replicated among many Cassandra datacenters, allowing write to the closest datacenter and search globally.  * The [cassandra driver](http://www.planetcassandra.org/client-drivers-tools/) is Datacenter and Token aware, providing automatic load-balancing and failover.  * Elassandra efficiently stores Elasticsearch documents in binary SSTables without any JSON overhead.    ## Quick start    * [Quick Start](http://doc.elassandra.io/en/latest/quickstart.html) guide to run a single node Elassandra cluster in docker.  * [Deploy Elassandra by launching a Google Kubernetes Engine](./docs/google-kubernetes-tutorial.md):      [![Open in Cloud Shell](https://gstatic.com/cloudssh/images/open-btn.png)](https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/strapdata/elassandra-google-k8s-marketplace&tutorial=docs/google-kubernetes-tutorial.md)      ## Upgrade Instructions      #### Elassandra 6.8.4.2+    <<<<<<< HEAD  Since version 6.8.4.2, the gossip X1 application state can be compressed using a system property. Enabling this settings allows the creation of a lot of virtual indices.  Before enabling this setting, upgrade all the 6.8.4.x nodes to the 6.8.4.2 (or higher). Once all the nodes are in 6.8.4.2, they are able to decompress the application state even if the settings isn't yet configured locally.    #### Elassandra 6.2.3.25+    Elassandra use the Cassandra GOSSIP protocol to manage the Elasticsearch routing table and Elassandra 6.8.4.2+ add support for compression of  the X1 application state to increase the maxmimum number of Elasticsearch indices. For backward compatibility, the compression is disabled by default,   but once all your nodes are upgraded into version 6.8.4.2+, you should enable the X1 compression by adding **-Des.compress_x1=true** in your **conf/jvm.options** and rolling restart all nodes.  Nodes running version 6.8.4.2+ are able to read compressed and not compressed X1.    #### Elassandra 6.2.3.21+    Before version 6.2.3.21, the Cassandra replication factor for the **elasic_admin** keyspace (and elastic_admin_[datacenter.group]) was automatically adjusted to the   number of nodes of the datacenter. Since version 6.2.3.21 and because it has a performance impact on large clusters, it's now up to your Elassandra administrator to   properly adjust the replication factor for this keyspace. Keep in mind that Elasticsearch mapping updates rely on a PAXOS transaction that requires QUORUM nodes to succeed,   so replication factor should be at least 3 on each datacenter.    #### Elassandra 6.2.3.19+    Elassandra 6.2.3.19 metadata version now relies on the Cassandra table **elastic_admin.metadata_log** (that was **elastic_admin.metadata** from 6.2.3.8 to 6.2.3.18)   to keep the elasticsearch mapping update history and automatically recover from a possible PAXOS write timeout issue.     When upgrading the first node of a cluster, Elassandra automatically copy the current **metadata.version** into the new **elastic_admin.metadata_log** table.  To avoid Elasticsearch mapping inconsistency, you must avoid mapping update while the rolling upgrade is in progress. Once all nodes are upgraded,  the **elastic_admin.metadata** is not more used and can be removed. Then, you can get the mapping update history from the new **elastic_admin.metadata_log** and know  which node has updated the mapping, when and for which reason.    #### Elassandra 6.2.3.8+    Elassandra 6.2.3.8+ now fully manages the elasticsearch mapping in the CQL schema through the use of CQL schema extensions (see *system_schema.tables*, column *extensions*). These table extensions and the CQL schema updates resulting of elasticsearch index creation/modification are updated in batched atomic schema updates to ensure consistency when concurrent updates occurs. Moreover, these extensions are stored in binary and support partial updates to be more efficient. As the result, the elasticsearch mapping is not more stored in the *elastic_admin.metadata* table.     WARNING: During the rolling upgrade, elasticserach mapping changes are not propagated between nodes running the new and the old versions, so don't change your mapping while you're upgrading. Once all your nodes have been upgraded to 6.2.3.8+ and validated, apply the following CQL statements to remove useless elasticsearch metadata:  ```bash  ALTER TABLE elastic_admin.metadata DROP metadata;  ALTER TABLE elastic_admin.metadata WITH comment = '';  ```    WARNING: Due to CQL table extensions used by Elassandra, some old versions of **cqlsh** may lead to the following error message **""'module' object has no attribute 'viewkeys'.""**. This comes from the old python cassandra driver embedded in Cassandra and has been reported in [CASSANDRA-14942](https://issues.apache.org/jira/browse/CASSANDRA-14942). Possible workarounds:  * Use the **cqlsh** embedded with Elassandra  * Install a recent version of the  **cqlsh** utility (*pip install cqlsh*) or run it from a docker image:    ```bash  docker run -it --rm strapdata/cqlsh:0.1 node.example.com  ```    #### Elassandra 6.x changes    * Elasticsearch now supports only one document type per index backed by one Cassandra table. Unless you specify an elasticsearch type name in your mapping, data is stored in a cassandra table named **""_doc""**. If you want to search many cassandra tables, you now need to create and search many indices.  * Elasticsearch 6.x manages shard consistency through several metadata fields (_primary_term, _seq_no, _version) that are not used in elassandra because replication is fully managed by cassandra.    ## Installation    Ensure Java 8 is installed and `JAVA_HOME` points to the correct location.    * [Download](https://github.com/strapdata/elassandra/releases) and extract the distribution tarball  * Define the CASSANDRA_HOME environment variable : `export CASSANDRA_HOME=<extracted_directory>`  * Run `bin/cassandra -e`  * Run `bin/nodetool status`  * Run `curl -XGET localhost:9200/_cluster/state`    #### Example    Try indexing a document on a non-existing index:    ```bash  curl -XPUT 'http://localhost:9200/twitter/_doc/1?pretty' -H 'Content-Type: application/json' -d '{      ""user"": ""Poulpy"",      ""post_date"": ""2017-10-04T13:12:00Z"",      ""message"": ""Elassandra adds dynamic mapping to Cassandra""  }'  ```    Then look-up in Cassandra:    ```bash  bin/cqlsh -e ""SELECT * from twitter.\""_doc\""""  ```    Behind the scenes, Elassandra has created a new Keyspace `twitter` and table `_doc`.    ```CQL  admin@cqlsh>DESC KEYSPACE twitter;    CREATE KEYSPACE twitter WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1': '1'}  AND durable_writes = true;    CREATE TABLE twitter.""_doc"" (      ""_id"" text PRIMARY KEY,      message list<text>,      post_date list<timestamp>,      user list<text>  ) WITH bloom_filter_fp_chance = 0.01      AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}      AND comment = ''      AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}      AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}      AND crc_check_chance = 1.0      AND dclocal_read_repair_chance = 0.1      AND default_time_to_live = 0      AND gc_grace_seconds = 864000      AND max_index_interval = 2048      AND memtable_flush_period_in_ms = 0      AND min_index_interval = 128      AND read_repair_chance = 0.0      AND speculative_retry = '99PERCENTILE';  CREATE CUSTOM INDEX elastic__doc_idx ON twitter.""_doc"" () USING 'org.elassandra.index.ExtendedElasticSecondaryIndex';  ```    By default, multi valued Elasticsearch fields are mapped to Cassandra list.  Now, insert a row with CQL :    ```CQL  INSERT INTO twitter.""_doc"" (""_id"", user, post_date, message)  VALUES ( '2', ['Jimmy'], [dateof(now())], ['New data is indexed automatically']);  SELECT * FROM twitter.""_doc"";     _id | message                                          | post_date                           | user  -----+--------------------------------------------------+-------------------------------------+------------     2 |            ['New data is indexed automatically'] | ['2019-07-04 06:00:21.893000+0000'] |  ['Jimmy']     1 | ['Elassandra adds dynamic mapping to Cassandra'] | ['2017-10-04 13:12:00.000000+0000'] | ['Poulpy']    (2 rows)  ```    Then search for it with the Elasticsearch API:    ```bash  curl ""localhost:9200/twitter/_search?q=user:Jimmy&pretty""  ```    And here is a sample response :    ```JSON  {    ""took"" : 3,    ""timed_out"" : false,    ""_shards"" : {      ""total"" : 1,      ""successful"" : 1,      ""skipped"" : 0,      ""failed"" : 0    },    ""hits"" : {      ""total"" : 1,      ""max_score"" : 0.6931472,      ""hits"" : [        {          ""_index"" : ""twitter"",          ""_type"" : ""_doc"",          ""_id"" : ""2"",          ""_score"" : 0.6931472,          ""_source"" : {            ""post_date"" : ""2019-07-04T06:00:21.893Z"",            ""message"" : ""New data is indexed automatically"",            ""user"" : ""Jimmy""          }        }      ]    }  }  ```    ## Support     * Commercial support is available through [Strapdata](http://www.strapdata.com/).   * Community support available via [elassandra google groups](https://groups.google.com/forum/#!forum/elassandra).   * Post feature requests and bugs on https://github.com/strapdata/elassandra/issues    ## License    ```  This software is licensed under the Apache License, version 2 (""ALv2""), quoted below.    Copyright 2015-2018, Strapdata (contact@strapdata.com).    Licensed under the Apache License, Version 2.0 (the ""License""); you may not  use this file except in compliance with the License. You may obtain a copy of  the License at        http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the  License for the specific language governing permissions and limitations under  the License.  ```    ## Acknowledgments    * Elasticsearch, Logstash, Beats and Kibana are trademarks of Elasticsearch BV, registered in the U.S. and in other countries.  * Apache Cassandra, Apache Lucene, Apache, Lucene and Cassandra are trademarks of the Apache Software Foundation.  * Elassandra is a trademark of Strapdata SAS. """
Big data;https://github.com/okfn/recline;"""Present a (Frictionless) dataset for viewing and exploration.    ## Install    Git clone then:    ```  yarn install  ```    ## Usage    In this directory:    ```bash  export PORTAL_DATASET_PATH=/path/to/my/dataset  yarn dev  ```    And you will get a nice dataset page at `http://localhost:3000`    ![](https://i.imgur.com/KSEtNF1.png) """
Big data;https://github.com/probcomp/BayesDB;"""BayesDB  =======    This repository contained a previous implementation of BayesDB which  has been replaced by the new implementation Bayeslite. For the new  repository, visit https://github.com/probcomp/bayeslite. For  information about BayesDB in general, visit  http://probcomp.csail.mit.edu/bayesdb    BayesDB, a Bayesian database, lets users query the probable  implications of their data as easily as a SQL database lets them query  the data itself. Using the built-in Bayesian Query Language (BQL),  users with no statistics training can solve basic data science  problems, such as detecting predictive relationships between  variables, inferring missing values, simulating probable observations,  and identifying statistically similar database entries.    # License    [Apache License, Version 2.0](https://github.com/probcomp/bayesdb/blob/master/LICENSE) """
Big data;https://github.com/datasalt/pangool;"""Pangool  =======    Pangool is a framework on top of Hadoop that implements Tuple MapReduce.    More info in:    http://datasalt.github.io/pangool """
Big data;https://github.com/rayokota/kareldb;"""# KarelDB - A Relational Database Backed by Apache Kafka    [![Build Status][github-actions-shield]][github-actions-link]  [![Maven][maven-shield]][maven-link]  [![Javadoc][javadoc-shield]][javadoc-link]    [github-actions-shield]: https://github.com/rayokota/kareldb/workflows/build/badge.svg?branch=master  [github-actions-link]: https://github.com/rayokota/kareldb/actions  [maven-shield]: https://img.shields.io/maven-central/v/io.kareldb/kareldb-core.svg  [maven-link]: https://search.maven.org/#search%7Cga%7C1%7Cio.kareldb  [javadoc-shield]: https://javadoc.io/badge/io.kareldb/kareldb-core.svg?color=blue  [javadoc-link]: https://javadoc.io/doc/io.kareldb/kareldb-core    KarelDB is a fully-functional relational database backed by Apache Kafka.    ## Maven    Releases of KarelDB are deployed to Maven Central.    ```xml  <dependency>      <groupId>io.kareldb</groupId>      <artifactId>kareldb-core</artifactId>      <version>1.0.0</version>  </dependency>  ```    ## Server Mode    To run KarelDB, download a [release](https://github.com/rayokota/kareldb/releases), unpack it, and then modify `config/kareldb.properties` to point to an existing Kafka broker.  Then run the following:    ```bash  $ bin/kareldb-start config/kareldb.properties  ```    At a separate terminal, enter the following command to start up `sqlline`, a command-line utility for accessing JDBC databases.    ```  $ bin/sqlline  sqlline version 1.9.0    sqlline> !connect jdbc:avatica:remote:url=http://localhost:8765 admin admin    sqlline> create table books (id int, name varchar, author varchar);  No rows affected (0.114 seconds)    sqlline> insert into books values (1, 'The Trial', 'Franz Kafka');  1 row affected (0.576 seconds)    sqlline> select * from books;  +----+-----------+-------------+  | ID |   NAME    |   AUTHOR    |  +----+-----------+-------------+  | 1  | The Trial | Franz Kafka |  +----+-----------+-------------+  1 row selected (0.133 seconds)  ```    To access a KarelDB server from a remote application, use an Avatica JDBC client.  A list of Avatica JDBC clients can be found [here](https://calcite.apache.org/avatica/docs/).    If multiple KarelDB servers are configured with the same cluster group ID (see [Configuration](#configuration)), then they will form a cluster and one of them will be elected as leader, while the others will become followers (replicas).  If a follower receives a request, it will be forwarded to the leader.  If the leader fails, one of the followers will be elected as the new leader.    ## Embedded Mode    KarelDB can also be used in embedded mode.  Here is an example:    ```java  Properties properties = new Properties();  properties.put(""schemaFactory"", ""io.kareldb.schema.SchemaFactory"");  properties.put(""parserFactory"", ""org.apache.calcite.sql.parser.parserextension.ExtensionSqlParserImpl#FACTORY"");  properties.put(""schema.kind"", ""io.kareldb.kafka.KafkaSchema"");  properties.put(""schema.kafkacache.bootstrap.servers"", bootstrapServers);  properties.put(""schema.kafkacache.data.dir"", ""/tmp"");    try (Connection conn = DriverManager.getConnection(""jdbc:kareldb:"", properties);       Statement s = conn.createStatement()) {          s.execute(""create table books (id int, name varchar, author varchar)"");          s.executeUpdate(""insert into books values(1, 'The Trial', 'Franz Kafka')"");          ResultSet rs = s.executeQuery(""select * from books"");          ...  }  ```    ## ANSI SQL Support    KarelDB supports ANSI SQL, using [Calcite](https://calcite.apache.org/docs/reference.html).      When creating a table, the primary key constraint should be specified after the columns, like so:    ```  CREATE TABLE customers       (id int, name varchar, constraint pk primary key (id));  ```    If no primary key constraint is specified, the first column in the table will be designated as the primary key.    KarelDB extends Calcite's SQL grammar by adding support for ALTER TABLE commands.    ```  alterTableStatement:      ALTER TABLE tableName columnAction [ , columnAction ]*        columnAction:      ( ADD tableElement ) | ( DROP columnName )  ```    KarelDB supports the following SQL types:    - boolean  - integer  - bigint  - real  - double  - varbinary  - varchar  - decimal  - date  - time  - timestamp    ## Basic Configuration    KarelDB has a number of configuration properties that can be specified.  When using KarelDB as an embedded database, these properties should be prefixed with `schema.` before passing them to the JDBC driver.    - `listeners` - List of listener URLs that include the scheme, host, and port.  Defaults to `http://0.0.0.0:8765`.    - `cluster.group.id` - The group ID to be used for leader election.  Defaults to `kareldb`.  - `leader.eligibility` - Whether this node can participate in leader election.  Defaults to true.  - `kafkacache.backing.cache` - The backing cache for KCache, one of `memory` (default), `bdbje`, `lmdb`, `mapdb`, or `rocksdb`.  - `kafkacache.data.dir` - The root directory for backing cache storage.  Defaults to `/tmp`.  - `kafkacache.bootstrap.servers` - A list of host and port pairs to use for establishing the initial connection to Kafka.  - `kafkacache.group.id` - The group ID to use for the internal consumers, which needs to be unique for each node.  Defaults to `kareldb-1`.  - `kafkacache.topic.replication.factor` - The replication factor for the internal topics created by KarelDB.  Defaults to 3.  - `kafkacache.init.timeout.ms` - The timeout for initialization of the Kafka cache, including creation of internal topics.  Defaults to 300 seconds.  - `kafkacache.timeout.ms` - The timeout for an operation on the Kafka cache.  Defaults to 60 seconds.    ## Security    ### HTTPS    To use HTTPS, first configure the `listeners` with an `https` prefix, then specify the following properties with the appropriate values.    ```  ssl.keystore.location=/var/private/ssl/custom.keystore  ssl.keystore.password=changeme  ssl.key.password=changeme  ```    When using the Avatica JDBC client, the `truststore` and `truststore_password` can be passed in the JDBC URL as specified [here](https://calcite.apache.org/avatica/docs/client_reference.html#truststore).    ### HTTP Authentication    KarelDB supports both HTTP Basic Authentication and HTTP Digest Authentication, as shown below:    ```  authentication.method=BASIC  # or DIGEST  authentication.roles=admin,developer,user  authentication.realm=KarelDb-Props  # as specified in JAAS file  ```    In the above example, the JAAS file might look like    ```  KarelDb-Props {    org.eclipse.jetty.jaas.spi.PropertyFileLoginModule required    file=""/path/to/password-file""    debug=""false"";  };  ```    The `ProperyFileLoginModule` can be replaced with other implementations, such as `LdapLoginModule` or `JDBCLoginModule`.    When starting KarelDB, the path to the JAAS file must be set as a system property.    ```bash  $ export KARELDB_OPTS=-Djava.security.auth.login.config=/path/to/the/jaas_config.file  $ bin/kareldb-start config/kareldb-secure.properties  ```    When using the Avatica JDBC client, the `avatica_user` and `avatica_password` can be passed in the JDBC URL as specified [here](https://calcite.apache.org/avatica/docs/client_reference.html#avatica-user).    ### Kafka Authentication    Authentication to a secure Kafka cluster is described [here](https://github.com/rayokota/kcache#security).     ## Implementation Notes    KarelDB stores table data in topics of the form `{tableName}_{generation}`.  A different generation ID is used whenever a table is dropped and re-created.    KarelDB uses three topics to hold metadata:    - `_tables` - A topic that holds the schemas for tables.  - `_commits` - A topic that holds the list of committed transactions.  - `_timestamps` - A topic that stores the maximum timestamp that the transaction manager is allowed to return to clients.    ## Database by Components    KarelDB is an example of a database built mostly by assembling pre-existing components.  In particular, KarelDB uses the following:    - [Apache Kafka](https://kafka.apache.org) - for persistence, using [KCache](https://github.com/rayokota/kcache) as an embedded key-value store  - [Apache Avro](https://avro.apache.org) - for serialization and schema evolution  - [Apache Calcite](https://calcite.apache.org) - for SQL parsing, optimization, and execution  - [Apache Omid](https://omid.incubator.apache.org) - for transaction management and MVCC support  - [Apache Avatica](https://calcite.apache.org/avatica/) - for JDBC functionality    See this [blog](https://yokota.blog/2019/09/23/building-a-relational-database-using-kafka) for more on the design of KarelDB.    ## Future Enhancements     Possible future enhancements include support for secondary indices. """
Big data;https://github.com/gephi/gephi;"""# Gephi - The Open Graph Viz Platform    [![build](https://github.com/gephi/gephi/actions/workflows/build.yml/badge.svg)](https://github.com/gephi/gephi/actions/workflows/build.yml)  [![Downloads](https://img.shields.io/github/downloads/gephi/gephi/v0.9.2/total.svg)](https://github.com/gephi/gephi/releases/tag/v0.9.2)  [![Downloads](https://img.shields.io/github/downloads/gephi/gephi/v0.9.1/total.svg)](https://github.com/gephi/gephi/releases/tag/v0.9.1)  [![Translation progress](https://hosted.weblate.org/widgets/gephi/-/svg-badge.svg)](https://hosted.weblate.org/engage/gephi/?utm_source=widget)    [Gephi](http://gephi.org) is an award-winning open-source platform for visualizing and manipulating large graphs. It runs on Windows, Mac OS X and Linux. Localization is available in English, French, Spanish, Japanese, Russian, Brazilian Portuguese, Chinese, Czech and German.    - **Fast** Powered by a built-in OpenGL engine, Gephi is able to push the envelope with very large networks. Visualize networks up to a million elements. All actions (e.g. layout, filter, drag) run in real-time.    - **Simple** Easy to install and [get started](https://gephi.github.io/users/quick-start). An UI that is centered around the visualization. Like Photoshop™ for graphs.    - **Modular** Extend Gephi with [plug-ins](https://gephi.org/plugins). The architecture is built on top of [Apache Netbeans Platform](https://netbeans.apache.org/tutorials/nbm-quick-start.html) and can be extended or reused easily through well-written APIs.    [Download Gephi](https://gephi.github.io/users/download) for Windows, Mac OS X and Linux and consult the [release notes](https://github.com/gephi/gephi/wiki/Releases). Example datasets can be found on our [wiki](https://github.com/gephi/gephi/wiki/Datasets).    ![Gephi](https://gephi.github.io/images/screenshots/select-tool-mini.png)    ## Install and use Gephi    Download and [Install](https://gephi.github.io/users/install/) Gephi on your computer.     Get started with the [Quick Start](https://gephi.github.io/users/quick-start/) and follow the [Tutorials](https://gephi.github.io/users/). Load a sample [dataset](https://github.com/gephi/gephi/wiki/Datasets) and start to play with the data.    If you run into any trouble or have questions consult our [forum](http://forum-gephi.org/).    ## Latest releases    ### Stable    - Latest stable release on [gephi.org](https://gephi.org/users/download/).    ### Nightly builds    Current version is 0.9.3-SNAPSHOT    - [gephi-0.9.3-SNAPSHOT-windows.exe](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.3-SNAPSHOT&c=windows&p=exe) (Windows)    - [gephi-0.9.3-SNAPSHOT-macos.dmg](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.3-SNAPSHOT&c=macos&p=dmg) (Mac OS X)    - [gephi-0.9.3-SNAPSHOT-linux.tar.gz](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.3-SNAPSHOT&c=linux&p=tar.gz) (Linux)    - [gephi-0.9.3-SNAPSHOT-sources.tar.gz](https://oss.sonatype.org/service/local/artifact/maven/content?r=snapshots&g=org.gephi&a=gephi&v=0.9.3-SNAPSHOT&c=sources&p=tar.gz) (Sources)    ## Developer Introduction    Gephi is developed in Java and uses OpenGL for its visualization engine. Built on the top of Netbeans Platform, it follows a loosely-coupled, modular architecture philosophy. Gephi is split into modules, which depend on other modules through well-written APIs. Plugins can reuse existing APIs, create new services and even replace a default implementation with a new one.    Consult the [**Javadoc**](http://gephi.github.io/gephi/0.9.2/apidocs/index.html) for an overview of the APIs.    ### Requirements    - Java JDK 11 (or later)    - [Apache Maven](http://maven.apache.org/) version 3.6.3 or later    ### Checkout and Build the sources    - Fork the repository and clone            git clone git@github.com:username/gephi.git    - Run the following command or [open the project in an IDE](https://github.com/gephi/gephi/wiki/How-to-build-Gephi)            mvn -T 4 clean install    - Once built, one can test running Gephi    		cd modules/application  		mvn nbm:cluster-app nbm:run-platform    ### Create Plug-ins    Gephi is extensible and lets developers create plug-ins to add new features, or to modify existing features. For example, you can create a new layout algorithm, add a metric, create a filter or a tool, support a new file format or database, or modify the visualization.    - [**Plugins Portal**](https://github.com/gephi/gephi/wiki/Plugins)    - [Plugins Quick Start (5 minutes)](https://github.com/gephi/gephi/wiki/Plugin-Quick-Start)    - Browse the [plugins](https://gephi.org/plugins) created by the community    - We've created a [**Plugins Bootcamp**](https://github.com/gephi/gephi-plugins-bootcamp) to learn by examples.    ## Gephi Toolkit    The Gephi Toolkit project packages essential Gephi modules (Graph, Layout, Filters, IO…) in a standard Java library which any Java project can use for getting things done. It can be used on a server or command-line tool to do the same things Gephi does but automatically.    - [Download](https://gephi.org/toolkit/)    - [GitHub Project](https://github.com/gephi/gephi-toolkit)    - [Toolkit Portal](https://github.com/gephi/gephi/wiki/Toolkit)    ## Localization    We use [Weblate](https://hosted.weblate.org/projects/gephi/) for localization. Follow the guidelines on the [wiki](https://github.com/gephi/gephi/wiki/Localization) for more details how to contribute.    ## License    Gephi main source code is distributed under the dual license [CDDL 1.0](http://www.opensource.org/licenses/CDDL-1.0) and [GNU General Public License v3](http://www.gnu.org/licenses/gpl.html). Read the [Legal FAQs](http://gephi.github.io/legal/faq/)  to learn more.  	  Copyright 2011 Gephi Consortium. All rights reserved.    The contents of this file are subject to the terms of either the GNU  General Public License Version 3 only (""GPL"") or the Common  Development and Distribution License (""CDDL"") (collectively, the  ""License""). You may not use this file except in compliance with the  License. You can obtain a copy of the License at  http://gephi.github.io/developers/license/  or /cddl-1.0.txt and /gpl-3.0.txt. See the License for the  specific language governing permissions and limitations under the  License.  When distributing the software, include this License Header  Notice in each file and include the License files at  /cddl-1.0.txt and /gpl-3.0.txt. If applicable, add the following below the  License Header, with the fields enclosed by brackets [] replaced by  your own identifying information:  ""Portions Copyrighted [year] [name of copyright owner]""    If you wish your version of this file to be governed by only the CDDL  or only the GPL Version 3, indicate your decision by adding  ""[Contributor] elects to include this software in this distribution  under the [CDDL or GPL Version 3] license."" If you do not indicate a  single choice of license, a recipient has the option to distribute  your version of this file under either the CDDL, the GPL Version 3 or  to extend the choice of license to its licensees as provided above.  However, if you add GPL Version 3 code and therefore, elected the GPL  Version 3 license, then the option applies only if the new code is  made subject to such option by the copyright holder. """
Big data;https://github.com/benedekrozemberczki/awesome-community-detection;"""# Awesome Community Detection Research Papers  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-community-detection.svg)](https://github.com/benedekrozemberczki/awesome-community-detection/archive/master.zip)  ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-community-detection.svg?color=blue) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)      A collection of community detection papers.    Similar collections about [graph classification](https://github.com/benedekrozemberczki/awesome-graph-classification), [classification/regression tree](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [fraud detection](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers), and [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers) papers with implementations.    <p align=""center"">    <img width=""460"" src=""coms.png"">  </p>    ## Table of Contents      1. [Matrix Factorization](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/factorization.md)    2. [Deep Learning](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/deep_learning.md)   3. [Label Propagation, Percolation and Random Walks](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/label_propagation.md)   4. [Tensor Decomposition](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/tensor_decomposition.md)  5. [Spectral Methods](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/spectral.md)   6. [Temporal Methods](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/temporal.md)   7. [Cyclic Patterns](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/cyclic.md)  8. [Centrality and Cuts](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/centrality.md)   9. [Physics Inspired](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/physics.md)  10. [Block Models](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/blockmodels.md)  11. [Hypergraphs](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/hyper_graphs.md)   12. [Others](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/others.md)   13. [Libraries](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/chapters/libraries.md)    --------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-community-detection/blob/master/LICENSE) """
Big data;https://github.com/nikolaypavlov/MLPNeuralNet;"""#MLPNeuralNet  [![Build Status](https://travis-ci.org/nikolaypavlov/MLPNeuralNet.svg?branch=master)](https://travis-ci.org/nikolaypavlov/MLPNeuralNet)  [![Join the chat at https://gitter.im/nikolaypavlov/MLPNeuralNet](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/nikolaypavlov/MLPNeuralNet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    `MLPNeuralNet` is a fast [multilayer perceptron](http://en.wikipedia.org/wiki/Multilayer_perceptron) neural network library for iOS and Mac OS X. `MLPNeuralNet` predicts new examples through trained neural networks. It is built on top of Apple's [Accelerate Framework](https://developer.apple.com/library/ios/documentation/Accelerate/Reference/AccelerateFWRef/_index.html) using vectored operations and hardware acceleration (if available).    ![Neural Network](http://nikolaypavlov.github.io/MLPNeuralNet/images/500px-Artificial_neural_network.png)    ##Why would you use it?  Imagine that you have engineered a prediction model using Matlab (Python or R) and would like to use it in an iOS application. If that's the case, `MLPNeuralNet` is exactly what you need. `MLPNeuralNet` is designed to load and run models in [forward propagation](http://en.wikipedia.org/wiki/Backpropagation#Phase_1:_Propagation) mode only.    ###Features  - [Classification](http://en.wikipedia.org/wiki/Binary_classification), [Multi-class classification](http://en.wikipedia.org/wiki/Multiclass_classification) and regression output  - Vectorised implementation  - Works with double precision  - Multiple hidden layers or none (in that case it's same as logistic/linear regression)    ##Quick Example  Let's deploy a model for the AND function  ([conjunction](http://en.wikipedia.org/wiki/Logical_conjunction)) that works as follows: (of course, you do not need to use a neural network for this in the real world)    |X1 |X2 | Y |  |:-:|:-:|:-:|  | 0 | 0 | 0 |  | 1 | 0 | 0 |  | 0 | 1 | 0 |  | 1 | 1 | 1 |    Our model has the following weights and network configuration:    ![AND Model Example](http://nikolaypavlov.github.io/MLPNeuralNet/images/network-arch.png)    ```objective  // Use the designated initialiser to pass the network configuration and weights to the model.  // Note: You do not need to specify the biased units (+1 above) in the configuration.    NSArray *netConfig = @[@2, @1];  double wts[] = {-30, 20, 20};  NSData *weights = [NSData dataWithBytes:wts length:sizeof(wts)];    MLPNeuralNet *model = [[MLPNeuralNet alloc] initWithLayerConfig:netConfig                                                          weights:weights                                                       outputMode:MLPClassification];  // Predict output of the model for new sample  double sample[] = {0, 1};  NSData * vector = [NSData dataWithBytes:sample length:sizeof(sample)];  NSMutableData * prediction = [NSMutableData dataWithLength:sizeof(double)];  [model predictByFeatureVector:vector intoPredictionVector:prediction];    double * assessment = (double *)prediction.bytes;  NSLog(@""Model assessment is %f"", assessment[0]);  ```    ##Extended Example  Let's say you trained a net using pybrain or even your own home brewed implementation.    ![Extended Example](http://i.imgur.com/v2kMTUH.png)    ```objective  // Use the designated initialiser to pass the network configuration and weights to the model.  // Note: You do not need to specify the biased units (+1 above) in the configuration.    NSArray *netConfig = @[@3, @2, @1];  double wts[] = {b1, w1, w2, w3, b2, w4, w5, w6, b3, w7, w8};  NSData *weights = [NSData dataWithBytes:wts length:sizeof(wts)];    MLPNeuralNet *model = [[MLPNeuralNet alloc] initWithLayerConfig:netConfig                                                          weights:weights                                                       outputMode:MLPClassification];  model.hiddenActivationFunction = MLPSigmoid;  model.outputActivationFunction = MLPNone;    // Predict output of the model for new sample  double sample[] = {0, 1, 2};  NSData * vector = [NSData dataWithBytes:sample length:sizeof(sample)];  NSMutableData * prediction = [NSMutableData dataWithLength:sizeof(double)];  [model predictByFeatureVector:vector intoPredictionVector:prediction];    double * assessment = (double *)prediction.bytes;  NSLog(@""Model assessment is %f"", assessment[0]);  ```    ##Getting Started  The following instructions describe how to setup and install `MLPNeuralNet` using [CocoaPods](http://cocoapods.org/). It is written for Xcode 5 and the iOS 7.x(+) SDK. If you are not familiar with CocoaPods, just clone the repository and import `MLPNeuralNet` directly as a subproject.    ##Installing through CocoaPods  Please add the following line to your *Podfile*.    ```  pod 'MLPNeuralNet', '~> 1.0.0'  ```    ##Installing through Carthage  Please add the following line to your *Cartfile*.    ```  github ""nikolaypavlov/MLPNeuralNet"" ""master""  ```    ##Import `MLPNeuralNet.h`  Do not forget to add the following line to the top of your model:  ```objectivec  #import ""MLPNeuralNet.h""  ```    ##How many weights do I need to initialise network X->Y->Z?  Most of the popular libraries (including `MLPNeuralNet`) implicitly add biased units for each of the layers except the last one. Assuming these additional units, the total number of weights are `(X + 1) * Y + (Y + 1) * Z`.    ##Importing weights from other libs.  You can do this for *some* of the neural network packages available.    ###R nnet library:   ```r  #Assuming nnet_model is a trained neural network  nnet_model$wts  ```    ###Python NeuroLab    ```python  #Where net argument is an neurolab.core.Net object  import neurolab as nl  import numpy as np    def getweights(net):       vec = []       for layer in net.layers:           b = layer.np['b']           w = layer.np['w']           newvec = np.ravel(np.concatenate((b, np.ravel(w,order='F'))).reshape((layer.ci+1, layer.cn)), order = 'F')           [vec.append(nv) for nv in newvec]       return np.array(vec)    ```    ###Python neon  ```python  import numpy as np    def layer_names(params):      layer_names = params.keys()      layer_names.remove('epochs_complete')      # Sort layers by their appearance in the model architecture      # Since neon appands the index to the layer name we will use it to sort      layer_names.sort(key=lambda x: int(x.split(""_"")[-1]))      return layer_names    def getweights(file_name):      vec = []      # Load a stored model file from disk (should have extension prm)      params = pkl.load(open(file_name, 'r'))      layers = layer_names(params)            for layer in layers:          # Make sure our model has biases activated, otherwise add zeros here          b = params[layer]['biases']          w = params[layer]['weights']            newvec = np.ravel(np.hstack((b,w)))          [vec.append(nv) for nv in newvec]      return vec    # An example call  getweights(expanduser('~/data/workout-dl/workout-ep100.prm'))  ```    ###Python keras  ```python  import numpy as np    def get_weights_from_keras_model(model):      vec = np.array([])      for i in xrange(0, len(model.get_weights()), 2):          bias = model.get_weights()[i + 1]          weights_matrix = model.get_weights()[i]            newvec = np.ravel(np.concatenate((bias.reshape(-1, 1), weights_matrix.T), axis=1))          vec = np.append(vec, newvec)      return np.array(vec)    ```    ## Performance benchmarks  In this test, the neural network has grown layer by layer from a `1 -> 1` configuration to a `200 -> 200 -> 200 -> 1` configuration. At each step, the output is calculated and benchmarked using random input vectorisation and weights. Total number of weights grow from 2 to 80601 accordingly. I understand that the test is quite synthetic, but I hope it illustrates the performance. I will be happy if you can propose a better one! :)    ![MLPNeuralNet Performance Benchmark](http://nikolaypavlov.github.io/MLPNeuralNet/images/mlp-bench-regression-ios.png)    ##Unit Testing  `MLPNeuralNet` includes a diverse suite of unit tests in the `/MLPNeuralNetTests` subdirectory. You can execute them using the ``MLPNeuralNet`` scheme within Xcode.    ##Acknowledgements  `MLPNeuralNet` was inspired by:    - [Andrew Ng's course on Machine Learning](https://www.coursera.org/course/ml).  - [Jeff Leek course on Data Analysis](https://www.coursera.org/course/dataanalysis).    Credits:    - Neural Network image was taken from [Wikipedia Commons](http://en.wikipedia.org/wiki/File:Artificial_neural_network.svg).    ##Contact Me  Maintainer: [Mykola Pavlov](http://github.com/nikolaypavlov/) (me@nikolaypavlov.com)    **Please let me know on how you use `MLPNeuralNet` for some real world problems.**    ##Licensing  `MLPNeuralNet` is released under the BSD license. See the LICENSE file for more information. """
Big data;https://github.com/benedekrozemberczki/littleballoffur;"""![Version](https://badge.fury.io/py/littleballoffur.svg?style=plastic) [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/littleballoffur.svg)](https://github.com/benedekrozemberczki/littleballoffur/archive/master.zip) [![Arxiv](https://img.shields.io/badge/ArXiv-2006.04311-orange.svg)](https://arxiv.org/abs/2006.04311) [![build badge](https://github.com/benedekrozemberczki/littleballoffur/workflows/CI/badge.svg)](https://github.com/benedekrozemberczki/littleballoffur/actions?query=workflow%3ACI) [![coverage badge](https://codecov.io/gh/benedekrozemberczki/littleballoffur/branch/master/graph/badge.svg)](https://codecov.io/github/benedekrozemberczki/littleballoffur?branch=master) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)    <p align=""center"">    <img width=""90%"" src=""https://github.com/benedekrozemberczki/littleballoffur/blob/master/littleballoffurlogo.jpg?sanitize=true"" />  </p>    ------------------------------------------------------------------------------    **Little Ball of Fur** is a graph sampling extension library for Python.    Please look at the **[Documentation](https://little-ball-of-fur.readthedocs.io/)**, relevant **[Paper](https://arxiv.org/abs/2006.04311)**, **[Promo video](https://youtu.be/5OpjBqlPWME)** and **[External Resources](https://little-ball-of-fur.readthedocs.io/en/latest/notes/resources.html)**.    ------------------------------------------------------------------------------    **Little Ball of Fur** consists of methods that can sample from graph structured data. To put it simply it is a Swiss Army knife for graph sampling tasks. First, it includes a large variety of vertex, edge, and exploration sampling techniques. Second, it provides a unified application public interface which makes the application of sampling algorithms trivial for end-users. Implemented methods cover a wide range of networking ([Networking](https://link.springer.com/conference/networking), [INFOCOM](https://infocom2020.ieee-infocom.org/), [SIGCOMM](http://www.sigcomm.org/)) and data mining ([KDD](https://www.kdd.org/kdd2020/), [TKDD](https://dl.acm.org/journal/tkdd), [ICDE](http://www.wikicfp.com/cfp/program?id=1331&s=ICDE&f=International%20Conference%20on%20Data%20Engineering)) conferences, workshops, and pieces from prominent journals.     ------------------------------------------------------------------------------    **Citing**    If you find **Little Ball of Fur** useful in your research, please consider citing the following paper:    ```bibtex  @inproceedings{littleballoffur,                 title={{Little Ball of Fur: A Python Library for Graph Sampling}},                 author={Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},                 year={2020},                 pages = {3133–3140},                 booktitle={Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},                 organization={ACM},  }  ```  ------------------------------------------------------------------------------    **A simple example**    **Little Ball of Fur** makes using modern graph subsampling techniques quite easy (see [here](https://little-ball-of-fur.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial).  For example, this is all it takes to use [Diffusion Sampling](https://arxiv.org/abs/2001.07463) on a Watts-Strogatz graph:    ```python  import networkx as nx  from littleballoffur import DiffusionSampler    graph = nx.newman_watts_strogatz_graph(1000, 20, 0.05)    sampler = DiffusionSampler()    new_graph = sampler.sample(graph)  ```    --------------------------------------------------------------------------------    **Methods included**    In detail, the following sampling methods were implemented.    **Node Sampling**      * **[Degree Based Node Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/node_sampling.html#littleballoffur.node_sampling.degreebasedsampler.DegreeBasedSampler)** from Adamic *et al.*: [Search In Power-Law Networks](https://arxiv.org/abs/cs/0103016) (Physical Review E 2001)    * **[Random Node Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/node_sampling.html#littleballoffur.node_sampling.randomnodesampler.RandomNodeSampler)** from Stumpf *et al.*: [SubNets of Scale-Free Networks Are Not Scale-Free: Sampling Properties of Networks](https://www.pnas.org/content/102/12/4221) (PNAS 2005)    * **[PageRank Based Node Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/node_sampling.html#littleballoffur.node_sampling.pagerankbasedsampler.PageRankBasedSampler)** from Leskovec *et al.*: [Sampling From Large Graphs](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (KDD 2006)    **Edge Sampling**    * **[Random Edge Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomedgesampler.RandomEdgeSampler)** from Krishnamurthy *et al.*: [Reducing Large Internet Topologies for Faster Simulations](http://www.cs.ucr.edu/~michalis/PAPERS/sampling-networking-05.pdf) (Networking 2005)    * **[Random Node-Edge Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomnodeedgesampler.RandomNodeEdgeSampler)** from Krishnamurthy *et al.*: [Reducing Large Internet Topologies for Faster Simulations](http://www.cs.ucr.edu/~michalis/PAPERS/sampling-networking-05.pdf) (Networking 2005)    * **[Hybrid Node-Edge Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.hybridnodeedgesampler.HybridNodeEdgeSampler)** from Krishnamurthy *et al.*: [Reducing Large Internet Topologies for Faster Simulations](http://www.cs.ucr.edu/~michalis/PAPERS/sampling-networking-05.pdf) (Networking 2005)    * **[Random Edge Sampler with Induction](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomedgesamplerwithinduction.RandomEdgeSamplerWithInduction)** from Ahmed *et al.*: [Network Sampling: From Static to Streaming Graphs](https://dl.acm.org/doi/10.1145/2601438) (TKDD 2013)    * **[Random Edge Sampler with Partial Induction](https://little-ball-of-fur.readthedocs.io/en/latest/modules/edge_sampling.html#littleballoffur.edge_sampling.randomedgesamplerwithpartialinduction.RandomEdgeSamplerWithPartialInduction)** from Ahmed *et al.*: [Network Sampling: From Static to Streaming Graphs](https://dl.acm.org/doi/10.1145/2601438) (TKDD 2013)    **Exploration Based Sampling**    * **[Snowball Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.snowballsampler.SnowBallSampler)** from Goodman: [Snowball Sampling](https://projecteuclid.org/euclid.aoms/1177705148) (The Annals of Mathematical Statistics 1961)    * **[Loop-Erased Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.looperasedrandomwalksampler.LoopErasedRandomWalkSampler)** from Wilson: [Generating Random Spanning Trees More Quickly Than the Cover Time](https://link.springer.com/chapter/10.1007/978-1-4612-2168-5_12) (STOC 1996)    * **[Forest Fire Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.forestfiresampler.ForestFireSampler)** from Leskovec *et al.*: [Graphs over Time: Densification Laws, Shrinking Diameters and Possible Explanations](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (KDD 2005)    <details>  <summary><b>Expand to see all exploration samplers...</b></summary>      * **[Random Node-Neighbor Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomnodeneighborsampler.RandomNodeNeighborSampler)** from Leskovec *et al.*: [Sampling From Large Graphs](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (KDD 2006)    * **[Random Walk With Restart Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomwalkwithrestartsampler.RandomWalkWithRestartSampler)** from Leskovec *et al.*: [Sampling From Large Graphs](https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf) (KDD 2006)    * **[Metropolis Hastings Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.metropolishastingsrandomwalksampler.MetropolisHastingsRandomWalkSampler)** from Hubler *et al.*: [Metropolis Algorithms for Representative Subgraph Sampling](http://mlcb.is.tuebingen.mpg.de/Veroeffentlichungen/papers/HueBorKriGha08.pdf) (ICDM 2008)    * **[Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomwalksampler.RandomWalkSampler)** from Gjoka *et al.*: [Walking in Facebook: A Case Study of Unbiased Sampling of OSNs](https://ieeexplore.ieee.org/document/5462078) (INFOCOM 2010)    * **[Random Walk With Jump Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.randomwalkwithjumpsampler.RandomWalkWithJumpSampler)** from Ribeiro *et al.*: [Estimating and Sampling Graphs with Multidimensional Random Walks](https://arxiv.org/abs/1002.1751) (SIGCOMM 2010)    * **[Frontier Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.frontiersampler.FrontierSampler)** from Ribeiro *et al.*: [Estimating and Sampling Graphs with Multidimensional Random Walks](https://arxiv.org/abs/1002.1751) (SIGCOMM 2010)    * **[Community Structure Expansion Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.communitystructureexpansionsampler.CommunityStructureExpansionSampler)** from Maiya *et al.*: [Sampling Community Structure](http://arun.maiya.net/papers/maiya_etal-sampcomm.pdf) (WWW 2010)    * **[Non-Backtracking Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.nonbacktrackingrandomwalksampler.NonBackTrackingRandomWalkSampler)** from Lee *et al.*: [Beyond Random Walk and Metropolis-Hastings Samplers: Why You Should Not Backtrack for Unbiased Graph Sampling](https://dl.acm.org/doi/10.1145/2318857.2254795) (SIGMETRICS 2012)    * **[Randomized Depth First Search Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.depthfirstsearchsampler.DepthFirstSearchSampler)** from Doerr *et al.*: [Metric Convergence in Social Network Sampling](https://dl.acm.org/doi/10.1145/2491159.2491168) (HotPlanet 2013)    * **[Randomized Breadth First Search Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.breadthfirstsearchsampler.BreadthFirstSearchSampler)** from Doerr *et al.*: [Metric Convergence in Social Network Sampling](https://dl.acm.org/doi/10.1145/2491159.2491168) (HotPlanet 2013)    * **[Rejection Constrained Metropolis Hastings Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.metropolishastingsrandomwalksampler.MetropolisHastingsRandomWalkSampler)** from Li *et al.*: [On Random Walk Based Graph Sampling](https://ieeexplore.ieee.org/document/7113345) (ICDE 2015)    * **[Circulated Neighbors Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.circulatedneighborsrandomwalksampler.CirculatedNeighborsRandomWalkSampler)** from Zhou *et al.*: [Leveraging History for Faster Sampling of Online Social Networks](https://dl.acm.org/doi/10.5555/2794367.2794373) (VLDB 2015)    * **[Shortest Path Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.shortestpathsampler.ShortestPathSampler)** from Rezvanian *et al.*: [Sampling Social Networks Using Shortest Paths](https://www.sciencedirect.com/science/article/pii/S0378437115000321) (Physica A 2015)    * **[Diffusion Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.diffusionsampler.DiffusionSampler)** from Rozemberczki *et al.*: [Fast Sequence-Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (Complex Networks 2018)    * **[Diffusion Tree Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.diffusiontreesampler.DiffusionTreeSampler)** from Rozemberczki *et al.*: [Fast Sequence-Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (Complex Networks 2018)    * **[Common Neighbor Aware Random Walk Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.commonneighborawarerandomwalksampler.CommonNeighborAwareRandomWalkSampler)** from Li *et al.*: [Walking with Perception: Efficient Random Walk Sampling via Common Neighbor Awareness](https://ieeexplore.ieee.org/document/8731555) (ICDE 2019)    * **[Spiky Ball Sampler](https://little-ball-of-fur.readthedocs.io/en/latest/modules/exploration_sampling.html#littleballoffur.exploration_sampling.spikyballsampler.SpikyBallSampler)** from Ricaud *et al.*: [Spikyball Sampling: Exploring Large Networks via an Inhomogeneous Filtered Diffusion](https://www.mdpi.com/1999-4893/13/11/275) (Algorithms 2020)      </details>    Head over to our [documentation](https://little-ball-of-fur.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.  For a quick start, check out our [examples](https://github.com/benedekrozemberczki/littleballoffur/tree/master/examples.py).    If you notice anything unexpected, please open an [issue](https://github.com/benedekrozemberczki/littleballoffur/issues) and let us know.  If you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/littleballoffur/issues).  We are motivated to constantly make **Little Ball of Fur** even better.      --------------------------------------------------------------------------------    **Installation**    **Little Ball of Fur** can be installed with the following pip command.    ```sh  $ pip install littleballoffur  ```    As we create new releases frequently, upgrading the package casually might be beneficial.    ```sh  $ pip install littleballoffur --upgrade  ```    --------------------------------------------------------------------------    **Running examples**    As part of the documentation we provide a number of use cases to show how to use various sampling techniques. These can accessed [here](https://little-ball-of-fur.readthedocs.io/en/latest/notes/introduction.html) with detailed explanations.      Besides the case studies we provide synthetic examples for each model. These can be tried out by running the scripts in the examples folder. You can try out the random walk sampling example by running:    ```sh  $ cd examples  $ python ./exploration_sampling/randomwalk_sampler.py  ```    -------------------------------------------------------------------------      **Running tests**    ```sh  $ python setup.py test  ```    -------------------------------------------------------------------------      **License**    - [GNU General Public License v3.0](https://github.com/benedekrozemberczki/littleballoffur/blob/master/LICENSE) """
Big data;https://github.com/deroproject/graviton;"""  # Graviton Database: ZFS for key-value stores.    Graviton Database is simple, fast, versioned, authenticated, embeddable key-value store database in pure GOLANG.    Graviton Database in short is like ""ZFS for key-value stores"" in which every write is tracked, versioned and authenticated with cryptographic proofs. Additionally it is possible to take snapshots of database. Also it is possible to use simple copy,rsync commands for database backup even during live updates without any possibilities of database corruption.    ![Graviton: ZFS for key-value stores](images/GRAVITON.png?raw=true ""Graviton: ZFS for key-value stores"")    ## Project Status  Graviton is currently alpha software. Almost full unit test coverage and randomized black box testing are used to ensure database consistency and thread safety. The project already has 100% code coverage. A number of decisions such as change,rename APIs, handling errors, hashing algorithms etc. are being evaluated and open for improvements and suggestions.    ## Features  Graviton Database in short is  ""ZFS for key-value stores"".    * Authenticated data store (All keys, values are backed by blake 256 bit checksum).  * Append only data store.  * Support of 2^64 trees (Theoretically) within a single data store. Trees can be named and thus used as buckets.  * Support of values version tracking. All committed changes are versioned with ability to visit them at any point in time.   * Snapshots (Multi tree commits in a single version causing multi bucket sync, each snapshot can be visited, appended and further modified, keys deleted, values modified etc., new keys, values stored.)  * Ability to iterate over all key-value pairs in a tree.  * Ability to diff between 2 trees in linear time and report all changes of Insertions, Deletions, Modifications.)  * Minimal and simplified API.  * Theoretically support Exabyte data store, Multi TeraByte tested internally.  * Decoupled storage layer, allowing use of object stores such as Ceph, AWS etc.  * Ability to generate cryptographic proofs which can prove key existance or non-existance (Cryptographic Proofs are around 1 KB.)  * Superfast proof generation time of around 1000 proofs per second per core.  * Support for disk based filesystem based persistant stores.  * Support for memory based non-persistant stores.  * 100% code coverage        ## Table of Contents  1. [Getting Started](#getting-started)   1. [Installing](#installing)   1. [Opening and Using the Database](#opening-and-using-the-database)   1. [Graviton Tree](#graviton-tree)   1. [Using key,value pairs](#using-keyvalue-pairs)   1. [Iterating over keys](#iterating-over-keys)   1. [Snapshots](#snapshots)   1. [Diffing](#diffing) (Diffing of 2 trees to detect changes between versions or compare 2 arbitrary trees in linear time.)  1. [GravitonDB Backups](#gravitondb-backups)   1. [Stress testing](#stress-testing)   1. [Graviton Internals](#graviton-internals)   1. [Lines of Code](#lines-of-Code)   1. [TODO](#todo)   1. [Comparison with other databases](#comparison-with-other-databases) (Mysql, Postgres, LevelDB, RocksDB, LMDB, Bolt etc.)  1. [License](#license)       GNU General Public License v3.0    ## Getting Started  ### Installing  To start using Graviton DB, install Go and run go get:    ```go get github.com/deroproject/graviton/...```    This will retrieve the library and build the library      ### Opening and Using the Database    The top-level object in Graviton is a Store. It is represented as a directory with multiple files on server's disk and represents a consistent snapshot of your data at all times.    Example code to open database:        package main        import ""fmt""      import ""github.com/deroproject/graviton""        func main() {  	   //store, _ := graviton.NewDiskStore(""/tmp/testdb"")   // create a new testdb in ""/tmp/testdb""          store, _ := graviton.NewMemStore()            // create a new  DB in RAM          ss, _ := store.LoadSnapshot(0)           // load most recent snapshot          tree, _ := ss.GetTree(""root"")            // use or create tree named ""root""          tree.Put([]byte(""key""), []byte(""value"")) // insert a value          graviton.Commit(tree)                  // commit the tree          value, _ := tree.Get([]byte(""key""))          fmt.Printf(""value retrived from DB \""%s\""\n"", string(value))      }        //NOTE: Linux (or other platforms) have open file limit for 1024.       //    Default limits allows upto 2TB of Graviton databases.    ### Graviton Tree  A Tree in Graviton DB acts like a bucket in BoltDB or a ZFS dataset. It is named and can contain upto 128 byte names. Any store can contain infinite trees. Each tree can also contain infinite key-value pairs. However, practically being limited by the server or system storage space.    Each tree can be accessed with its merkle root hash using ""*GetTreeWithRootHash*"" API. Also each tree maintains its own separate version number and any specific version can be used *GetTreeWithVersion*. Note that each tree can also have arbitrary tags and any tagged tree can be accessed using the tag *GetTreeWithTag*. Also, 2 arbitrary trees can diffed in linear time and relevant changes detected.        NOTE: Tree tags or names cannot start with ':' .    ### Using key,value pairs    To save a key/value pair to a tree ( or bucket), use the `tree.Put()` function:    ```go          tree, _ := ss.GetTree(""root"")           tree.Put([]byte(""answer""), []byte(""44"")) // insert a value          graviton.Commit(tree)  // make the tree persistant by storing it in backend disk  ```    This will set the value of the `""answer""` key to `""44""` in the `root`  tree. To retrieve this value, we can use the `tree.Get()` function:    ```go  	tree, _ := ss.GetTree(""root"")   	v,_ := tree.Get([]byte(""answer""))  	fmt.Printf(""The answer is: %s\n"", v)  ```    The `Get()` function returns an error because its operation is guaranteed to work (unless there is some kind of system failure which we try to report). If the key exists then it will return its byte slice value. If it doesn't exist then it  will return  an error.     ### Iterating over keys    Graviton stores its keys in hash byte-sorted order within a tree. This makes sequential  iteration over these keys extremely fast. To iterate over keys GravitonDB uses a  `Cursor`:    ```go  	// Assume ""root"" tree exists and has keys      tree, _ := store.GetTree(""root"")   	c := tree.Cursor()    	for k, v, err := c.First(); err == nil; k, v, err = c.Next() {   		fmt.Printf(""key=%s, value=%s\n"", k, v)  	}  ```    The cursor allows you to move to a specific point in the list of keys and move  forward or backward through the keys one at a time.    The following functions are available on the cursor:    ```  First()  Move to the first key.  Last()   Move to the last key.  Next()   Move to the next key.  Prev()   Move to the previous key.  ```    Each of those functions has a return signature of `(key []byte, value []byte, err error)`.  When you have iterated to the end of the cursor then `Next()` will return an error `ErrNoMoreKeys`.  You must seek to a position using `First()`, `Last()`  before calling `Next()` or `Prev()`. If you do not seek to a position then these functions will return an error.      ### Snapshots  Snapshot refers to collective state of all buckets + data + history. Each commit( tree.Commit() or Commit(tree1, tree2 .....)) creates a new snapshot in the store.Each snapshot is represented by an incremental uint64 number, 0 represents most recent snapshot.  Snapshots can be used to access any arbitrary state of entire database at any point in time.    Example code for snapshots:        package main        import ""fmt""      import ""github.com/deroproject/graviton""        func main() {      	   key := []byte(""key1"")  	   //store, _ := graviton.NewDiskStore(""/tmp/testdb"")   // create a new testdb in ""/tmp/testdb""  	   store, _ := graviton.NewMemStore()          // create a new  DB in RAM  	   ss, _ := store.LoadSnapshot(0)         // load most recent snapshot  	   tree, _ := ss.GetTree(""root"")          // use or create tree named ""root""  	   tree.Put(key, []byte(""commit_value1"")) // insert a value  	   commit1, _ := graviton.Commit(tree)         // commit the tree  	   tree.Put(key, []byte(""commit_value2"")) // overwrite existing value  	   commit2, _ := graviton.Commit(tree)         // commit the tree again    	   // at this point, you have done 2 commits  	   // at first commit or snapshot,  ""root"" tree contains  ""key1 : commit_value1""  	   // at second commit or snapshot,  ""root"" tree contains  ""key1 : commit_value2""    	   // we will traverse now commit1 snapshot  	   ss, _ = store.LoadSnapshot(commit1)  	   tree, _ = ss.GetTree(""root"")  	   value, err := tree.Get(key)  	   fmt.Printf("" snapshot%d  key %s value %s err %s\n"", ss.GetVersion(), string(key), string(value), err)    	   // we will traverse now commit2 snapshot  	   ss, _ = store.LoadSnapshot(commit2)  	   tree, _ = ss.GetTree(""root"")  	   value, err = tree.Get(key)  	   fmt.Printf("" snapshot%d  key %s value %s err %s\n"", ss.GetVersion(), string(key), string(value), err)      }    ### Diffing  #### Diffing of 2 trees to detect changes between versions or compare 2 arbitrary trees in linear time.  Two arbitrary trees can be diffed in linear time to detect changes. Changes are of 3 types insertions, deletions and modifications (Same key but value changed). If the reported changes are applied to base tree, it will be equivalent to the head tree being compared.        func Diff(base_tree, head_tree *Tree, deleted, modified, inserted DiffHandler) (err error)    Diffhandler is a callback function of the following type having k,v as arguments        type DiffHandler func(k, v []byte)    The algorithm is linear time in the number of changes. Eg. a tree with billion KVs can be diffed with parent almost instantaneously.        ### GravitonDB Backups  Use simple commands like cp, copy or rsync to sync a Graviton database even while the database is being updated. However, as the database might be continuously appending, backup will always lag a bit. And note that the database or backups will NEVER get corrupted during copying while commits are being done.    ### Stress Testing  A mini tool to do single thread testing is provided which can be used to perform various tests on memory or disk backend.        go run github.com/deroproject/graviton/cmd/stress    See help using `--help` argument. To use disk backend, use `--memory=false`      ### Graviton Internals  Internally, all trees are stored within a base-2 merkle with collapsing path. This means if tree has 4 billion key-value pairs, it will only be 32 level deep.This leads to tremendous savings in storage space.This also means when you modify an existing key-value, only limited amount of nodes are touched.      ### Lines of Code      ~/tools/gocloc   --by-file  node_inner.go tree.go snapshot.go proof.go node_leaf.go  store.go node.go  hash.go  const.go doc.go  diff_tree.go cursor.go       -----------------------------------------------------------------      File           files          blank        comment           code      -----------------------------------------------------------------      node_inner.go                    76             33            364      store.go                         69             22            250      tree.go                          75             71            250      proof.go                         30             16            171      snapshot.go                      36             18            155      node_leaf.go                     29              3            150      diff_tree.go                     34             33            133      cursor.go                        21             15            106      node.go                           5              3             35      const.go                          4              0             21      hash.go                           7              2             19      doc.go                           16             42              1      -----------------------------------------------------------------      TOTAL             12            402            258           1655      -----------------------------------------------------------------    ## TODO   * Currently it is not optimized for speed and GC (Garbage collection).  * Expose/build metrics.  * Currently, we have error reportingapi to reports rot bits, but nothing about disks corruption, should we discard such error design and make the API simpler (except snapshots, tree loading, commiting, no more errors ). More discussion required on this hard-disk failures,errors etc. required.      ### Comparison with other databases  None of the following databases provides ability to traverse back-in-time for each and every commit. GravitonDB is the only DB which provides back-in-time. Also presently GravitonDB is the only database which can diff between 2 trees in linear time. Let's compare between other features of some databases.    #### Postgres, MySQL, & other relational databases    Relational databases structure data into rows and are only accessible through  the use of SQL. This approach provides flexibility in how you store and query  your data but also incurs overhead in parsing and planning SQL statements. GravitonDB  accesses all data by a byte slice key. This makes GravitonDB fast to read and write  data by key but provides no built-in support for joining values together.    Most relational databases (with the exception of SQLite) are standalone servers  that run separately from the application. This gives systems  flexibility to connect multiple application servers to a single database  server but also adds overhead in serializing and transporting data over the  network. Graviton runs as a library included in your application so all data access  has to go through your application's process. This brings data closer to your  application but limits multi-process access to the data.        #### LevelDB, RocksDB    LevelDB and its derivatives (RocksDB, HyperLevelDB) are similar to Graviton in that  they are libraries bundled into the application, However, their underlying  structure is a log-structured merge-tree (LSM tree). An LSM tree optimizes  random writes by using a write ahead log and multi-tiered, sorted files called  SSTables. Graviton uses a base 2 merkle tree internally. Both approaches  have trade-offs.    If you require a high random write throughput or you need to use  spinning disks then LevelDB could be a good choice unless there are requirements of versioning, authenticated proofs or other features of Graviton database.    #### LMDB, BoltDB    LMDB, Bolt are architecturally similar. Both use a B+ tree, have ACID semantics with fully serializable transactions, and support lock-free MVCC using a single writer and multiple readers.    In-addition LMDB heavily focuses on raw performance while BoltDB focus on simplicity and ease of use. For example, LMDB allows several unsafe actions such as direct writes for the sake of performance. Bolt opts to disallow actions which can leave the database in a corrupted state. The only exception to this in Bolt is `DB.NoSync`.GravitonDB does not leave the database in corrupted state at any point in time.      In-addition LMDB, BoltDB doesn't support versioning, snapshots, linear diffing etc. features only Graviton provides such features for now.      ### License   [GNU General Public License v3.0](https://github.com/deroproject/graviton/blob/master/LICENSE) """
Big data;https://github.com/Treode/store;"""# TreodeDB    TreodeDB is a distributed database that provides multirow atomic writes, and it&#700;s designed for RESTful services.    TreodeDB    - is a key-value store  - offers replication for fault tolerance  - offers sharding for scalability  - offers transactions to provide consistency  - tracks versioned data to [extend transactions through a CDN or cache][cbw]  - can feed an [Apache Spark][apache-spark]&trade; RDD or an [Apache Hadoop][apache-hadoop]&trade; InputFormat for analytics  - can feed an [Apache Spark][apache-spark]&trade; DStream for streaming analytics    ![Architecture][arch]      ## Documentation    - [User Docs][user-docs]  - Presentation: [slides][presentation-slides], [video][presentation-video]      [apache-hadoop]: https://hadoop.apache.org ""Apache Hadoop&trade;""    [apache-spark]: https://spark.apache.org ""Apache Spark&trade;""    [arch]: architecture.png ""Architecture""    [cbw]: http://treode.github.io/cbw/ ""Conditional Batch Write""    [presentation-slides]: http://goo.gl/le0rjT ""Slides, SF Bay Chapter of the ACM, Mar 18 2015""    [presentation-video]: https://www.youtube.com/watch?v=sI8vtAjO7x4&list=PL87GtQd0bfJyd9_TEKLbuTTdLFCedM-yw ""Video, SF Bay Chapter of the ACM, Mar 18 2015""    [user-docs]: http://treode.github.io ""TreodeDB Walkthroughs"" """
Big data;https://github.com/Bobris/BTDB;"""# BTDB    Currently this project these parts:    -   Key Value Database  -   Wrapped Dynamic IL generation with debugging + extensions  -   IOC Container  -   Object Database with Relations  -   Snappy Compression  -   Event Storage    All code written in C# and licensed under very permissive [MIT license](http://www.opensource.org/licenses/mit-license.html). Targeting .Net 6.0, main code has just 1 dependency (Microsoft.Extensions.Primitives). Code is tested using xUnit Framework. Used in production on Windows and Linux, on OSX works as well.  Please is you find it useful or have questions, write me e-mail <boris.letocha@gmail.com> so I know that it is used.  It is available in Nuget <http://www.nuget.org/packages/BTDB>. Source code drops are Github releases.    ---    ## Key Value Database    ### Features:    -   This is Key Value store written in C# with 2 implementation old on managed heap and new on native heap (has also prefix compression).  -   It is easily embeddable.  -   One storage is just one directory.  -   It has [ACID] properties with [MVCC].  -   At one time there could be multiple read only transactions and one read/write transaction.  -   Export/Import to stream - could be used for compaction, snapshotting  -   Automatic compaction  -   Customizable compression  -   Relatively Fast DB Open due to key index file - though it still needs to load all keys to memory  -   Inspired by Bitcask [https://github.com/basho/bitcask/blob/develop/doc/bitcask-intro.pdf]    ### Design limits:    -   All keys data needs to fit in RAM  -   Maximum Key length is limited by 31bits (2GB).  -   Maximum value length is limited by 31bits (2GB).    ### Sample code:        using (var fileCollection = new InMemoryFileCollection())      using (IKeyValueDB db = new KeyValueDB(fileCollection))      {          using (var tr = db.StartTransaction())          {              tr.CreateOrUpdateKeyValue(new byte[] { 1 }, new byte[100000]);              tr.Commit();          }      }    ### Roadmap:    -   Everything is there just use it    ---    ## Wrapped Dynamic IL generation with debugging + extensions    This help you to write fluent code which generates IL code in runtime. It is used in Object Database part.    ### Sample code:        var method = ILBuilder.Instance.NewMethod<Func<Nested>>(""SampleCall"");      var il = method.Generator;      var local = il.DeclareLocal(typeof(Nested), ""n"");      il          .Newobj(() => new Nested())          .Dup()          .Stloc(local)          .Ldstr(""Test"")          .Call(() => ((Nested)null).Fun(""""))          .Ldloc(local)          .Ret();      var action = method.Create();    ### Roadmap:    -   Add support for all IL instructions as needed    ---    ## Object Database    ### Features:    -   Builds on top of Key Value Database and Reflection.Emit extensions.  -   It stores Plain .Net Objects and only their public properties with getters and setters.  -   All [ACID] and [MVCC] properties are preserved of course.  -   Automatic upgrading of model on read with dynamically generated optimal IL code.  -   Automatic versioning of model changes.  -   Enumeration of all objects  -   Each object type could store its ""singleton"" - very useful for root objects  -   Relations - Table with primary key and multiple secondary keys  -   By default objects are stored inline in parent object, use IIndirect for objects with Oid which will load lazily    Documentation: [https://github.com/Bobris/BTDB/blob/master/Doc/ODBDictionary.md]    Relations doc: [https://github.com/Bobris/BTDB/blob/master/Doc/Relations.md]    ### Sample code:        public class Person      {          public string Name { get; set; }          public uint Age { get; set; }      }        using (var tr = _db.StartTransaction())      {          tr.Store(new Person { Name = ""Bobris"", Age = 35 });          tr.Commit();      }      using (var tr = _db.StartTransaction())      {          var p = tr.Enumerate<Person>().First();          Assert.AreEqual(""Bobris"", p.Name);          Assert.AreEqual(35, p.Age);      }    ### Roadmap:    -   Support more types of properties  -   Free text search (far future if ever)    ---    ## Event storage    ### Features:    -   Optimal serialization with metadata  -   Deserialization also to dynamic  -   Storage is transactional  -   As storage could be used Azure Page Blobs  -   EventStorage2 is specialized to be used with Kafka, metadata are stored in separate topic    ---    ## Snappy Compression    ### Features:    -   Ported and inspired mainly by Go version of Snappy Compression [http://code.google.com/p/snappy/]  -   Fully compatible with original  -   Fully managed and safe implementation  -   Compression is aborted when target buffer size is not big enough    ### Roadmap:    -   Some speed optimizations around Spans would help    [acid]: http://en.wikipedia.org/wiki/ACID  [mvcc]: http://en.wikipedia.org/wiki/Multiversion_concurrency_control """
Big data;https://github.com/twitter/storehaus;"""## Storehaus [![Build Status](https://secure.travis-ci.org/twitter/storehaus.png)](http://travis-ci.org/twitter/storehaus)    Storehaus is a library that makes it easy to work with asynchronous key value stores. Storehaus is built on top of Twitter's [Future](https://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Future.scala).    ### Storehaus-Core    Storehaus's core module defines three traits; a read-only `ReadableStore` a write-only `WritableStore` and a read-write `Store`. The traits themselves are tiny:    ```scala  package com.twitter.storehaus    import com.twitter.util.{ Closable, Future, Time }    trait ReadableStore[-K, +V] extends Closeable {    def get(k: K): Future[Option[V]]    def multiGet[K1 <: K](ks: Set[K1]): Map[K1, Future[Option[V]]]    override def close(time: Time) = Future.Unit  }    trait WritableStore[-K, -V] {    def put(kv: (K, V)): Future[Unit] = multiPut(Map(kv)).apply(kv._1)    def multiPut[K1 <: K](kvs: Map[K1, V]): Map[K1, Future[Unit]] =      kvs.map { kv => (kv._1, put(kv)) }    override def close(time: Time) = Future.Unit  }    trait Store[-K, V] extends ReadableStore[K, V] with WritableStore[K, Option[V]]  ```    The `ReadableStore` trait uses the `Future[Option[V]]` return type to communicate one of three states about each value. A value is either    * definitely present,  * definitely missing, or  * unknown due to some error (perhaps a timeout, or a downed host).    The [`ReadableStore`](http://twitter.github.com/storehaus/#com.twitter.storehaus.ReadableStore$) and [`Store`](http://twitter.github.com/storehaus/#com.twitter.storehaus.Store$) companion objects provide a bunch of ways to create new stores. See the linked API documentation for more information.    ### Combinators    Coding with Storehaus's interfaces gives you access to a number of powerful combinators. The easiest way to access these combinators is by wrapping your store in an [`EnrichedReadableStore`](http://twitter.github.com/storehaus/#com.twitter.storehaus.EnrichedReadableStore) or an [`EnrichedStore`](http://twitter.github.com/storehaus/#com.twitter.storehaus.EnrichedStore). Storehaus provides implicit conversions inside of the [`ReadableStore`](http://twitter.github.com/storehaus/#com.twitter.storehaus.ReadableStore$) and [`Store`](http://twitter.github.com/storehaus/#com.twitter.storehaus.Store$) objects.    Here's an example of the `mapValues` combinator, useful for transforming the type of an existing store.    ```scala  import com.twitter.storehaus.ReadableStore  import ReadableStore.enrich    // Create a ReadableStore from Int -> String:  val store = ReadableStore.fromMap(Map[Int, String](1 -> ""some value"", 2 -> ""other value""))    // ""get"" behaves as expected:  store.get(1).get  // res5: Option[String] = Some(some value)    // calling ""mapValues"" with a function from V => NewV returns a new ReadableStore[K, NewV]:  val countStore: ReadableStore[Int, Int] = store.mapValues { s => s.size }    // This new store applies the function to every value on the way out:  countStore.get(1).get  // res6: Option[Int] = Some(10)  ```    ### Storehaus-Algebra    `storehaus-algebra` module adds the `MergeableStore` trait. If you're using key-value stores for aggregations, you're going to love `MergeableStore`.    ```scala  package com.twitter.storehaus.algebra    trait MergeableStore[-K, V] extends Store[K, V] {    def monoid: Monoid[V]    def merge(kv: (K, V)): Future[Option[V]] = multiMerge(Map(kv)).apply(kv._1)    def multiMerge[K1 <: K](kvs: Map[K1, V]): Map[K1, Future[Option[V]]] = kvs.map { kv => (kv._1, merge(kv)) }  }  ```    `MergeableStore`'s `merge` and `multiMerge` are similar to `put` and `multiPut`; the difference is that values added with `merge` are added to the store's existing value and the previous value is returned.  Because the addition is handled with a `Semigroup[V]` or `Monoid[V]` from Twitter's [Algebird](https://github.com/twitter/algebird) project, it's easy to write stores that aggregate [Lists](http://twitter.github.com/algebird/#com.twitter.algebird.ListMonoid), [decayed values](http://twitter.github.com/algebird/#com.twitter.algebird.DecayedValue), even [HyperLogLog](http://twitter.github.com/algebird/#com.twitter.algebird.HyperLogLog$) instances.    The [`MergeableStore`](http://twitter.github.com/storehaus/#com.twitter.storehaus.algebra.MergeableStore$) object provides a number of combinators on these stores. For ease of use, Storehaus provides an implicit conversion to an enrichment on `MergeableStore`. Access this by importing `MergeableStore.enrich`.    ### Other Modules    Storehaus provides a number of modules wrapping existing key-value stores. Enriching these key-value stores with Storehaus's combinators has been hugely helpful to us here at Twitter. Writing your jobs in terms of Storehaus stores makes it easy to test your jobs; use an in-memory `JMapStore` in testing and a `MemcacheStore` in production.      * [Storehaus-memcache](http://twitter.github.com/storehaus/#com.twitter.storehaus.memcache.MemcacheStore) (wraps Twitter's [finagle-memcachedx](https://github.com/twitter/finagle/tree/master/finagle-memcachedx) library)    * [Storehaus-mysql](http://twitter.github.com/storehaus/#com.twitter.storehaus.mysql.MySqlStore) (wraps Twitter's [finagle-mysql](https://github.com/twitter/finagle/tree/master/finagle-mysql) library)    * [Storehaus-redis](http://twitter.github.com/storehaus/#com.twitter.storehaus.redis.RedisStore) (wraps Twitter's [finagle-redis](https://github.com/twitter/finagle/tree/master/finagle-redis) library)    * [Storehaus-hbase](http://twitter.github.com/storehaus/#com.twitter.storehaus.hbase.HBaseStore)    * [Storehaus-dynamodb](https://github.com/twitter/storehaus/tree/develop/storehaus-dynamodb)    * [Storehaus-leveldb](https://github.com/twitter/storehaus/tree/develop/storehaus-leveldb)    #### Planned Modules    Here's a list of modules we plan in implementing, with links to the github issues tracking progress on these modules:    * [storehaus-berkeleydb](https://github.com/twitter/storehaus/issues/52)    ## Community and Documentation    This, and all [github.com/twitter](https://github.com/twitter) projects, are under the [Twitter Open Source Code of Conduct](https://engineering.twitter.com/opensource/code-of-conduct). Additionally, see the [Typelevel Code of Conduct](http://typelevel.org/conduct) for specific examples of harassing behavior that are not tolerated.    To learn more and find links to tutorials and information around the web, check out the [Storehaus Wiki](https://github.com/twitter/storehaus/wiki).    The latest ScalaDocs are hosted on Storehaus's [Github Project Page](http://twitter.github.io/storehaus).    Discussion occurs primarily on the [Storehaus mailing list](https://groups.google.com/forum/#!forum/storehaus). Issues should be reported on the [GitHub issue tracker](https://github.com/twitter/storehaus/issues).    ## Maven    Storehaus modules are available on maven central. The current groupid and version for all modules is, respectively, `""com.twitter""` and  `0.12.0`.    Current published artifacts are    * `storehaus-core_2.11`  * `storehaus-core_2.10`  * `storehaus-algebra_2.11`  * `storehaus-algebra_2.10`  * `storehaus-memcache_2.11`  * `storehaus-memcache_2.10`  * `storehaus-mysql_2.11`  * `storehaus-mysql_2.10`  * `storehaus-hbase_2.11`  * `storehaus-hbase_2.10`  * `storehaus-redis_2.11`  * `storehaus-redis_2.10`  * `storehaus-dynamodb_2.11`  * `storehaus-dynamodb_2.10`  * `storehaus-kafka-08_2.11`  * `storehaus-kafka-08_2.10`  * `storehaus-mongodb_2.11`  * `storehaus-mongodb_2.10`  * `storehaus-elasticsearch_2.11`  * `storehaus-elasticsearch_2.10`  * `storehaus-leveldb_2.11`  * `storehaus-leveldb_2.10`  * `storehaus-http_2.11`  * `storehaus-http_2.10`  * `storehaus-cache_2.11`  * `storehaus-cache_2.10`  * `storehaus-testing_2.11`  * `storehaus-testing_2.10`    The suffix denotes the scala version.    ## Testing notes    We use travis-ci to set up any underlying stores (e.g. MySQL, Redis, Memcached) for the tests. In order for these tests to pass on your local machine, you may need additional setup.    ### MySQL tests    You will need MySQL installed on your local machine.  Once installed, run the `mysql` commands listed in [.travis.yml](https://github.com/twitter/storehaus/blob/develop/.travis.yml) file.    ### Redis tests    You will need [redis](http://redis.io/) installed on your local machine. Redis comes bundled with an executable for spinning up a server called `redis-server`. The Storehaus redis tests expect the factory defaults for connecting to one of these redis server instances, resolvable on `localhost` port `6379`.    ### Memcached    You will need [Memcached](http://memcached.org/) installed on your local machine and running on the default port `11211`.    ## Authors    * Oscar Boykin <https://twitter.com/posco>  * Sam Ritchie <https://twitter.com/sritchie>    ## Contributors    Here are a few that shine among the many:    * Ruban Monu <https://twitter.com/rubanm>, for `storehaus-mysql`  * Doug Tangren <https://twitter.com/softprops>, for `storehaus-redis`  * Ryan Weald <https://twitter.com/rweald>, for `storehaus-dynamodb`    ## License    Copyright 2013 Twitter, Inc.    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/HumbleSoftware/envisionjs;"""# Envision.js  -------------    Fast interactive HTML5 charts.    ![Google Groups](http://groups.google.com/intl/en/images/logos/groups_logo_sm.gif)    http://groups.google.com/group/envisionjs/    ## Features    * Modern Browsers, IE 6+  * Mobile / Touch Support  * Pre-built Templates  * Adaptable to Existing Libraries    ## Dependencies    Envision.js ships with all it's dependencies.  It uses:    * <a href=""http://documentcloud.github.com/underscore/"">underscore.js</a>  * <a href=""https://github.com/fat/bean"">bean</a>  * <a href=""https://github.com/ded/bonzo"">bonzo</a>  * <a href=""http://humblesoftware.com/flotr2/"">Flotr2</a>    ## Usage    To use Envision.js, include `envision.min.js` and `envision.min.css` in your  page. To display a visualization, either use a Template or create a custom  visualization with the Envision.js API.    ### Templates    Templates are pre-built visualizations for common use-cases.    Example:     ```javascript    var      container = document.getElementById('container'),      x = [],      y1 = [],      y2 = [],      data, options, i;      // Data Format:    data = [      [x, y1], // First Series      [x, y2]  // Second Series    ];      // Sample the sine function for data    for (i = 0; i < 4 * Math.PI; i += 0.05) {      x.push(i);      y1.push(Math.sin(i));      y2.push(Math.sin(i + Math.PI));    }      // TimeSeries Template Options    options = {      // Container to render inside of      container : container,      // Data for detail (top chart) and summary (bottom chart)      data : {        detail : data,        summary : data      }    };      // Create the TimeSeries    new envision.templates.TimeSeries(options);  ```    ### Custom    Developers can use the envision APIs to build custom visualizations.  The  existing templates are a good reference for this.    Example:     ```javascript    var      container = document.getElementById('container'),      x = [],      y1 = [],      y2 = [],      data, i,      detail, detailOptions,      summary, summaryOptions,      vis, selection,      // Data Format:    data = [      [x, y1], // First Series      [x, y2]  // Second Series    ];      // Sample the sine function for data    for (i = 0; i < 4 * Math.PI; i += 0.05) {      x.push(i);      y1.push(Math.sin(i));      y2.push(Math.sin(i + Math.PI));    }    x.push(4 * Math.PI)    y1.push(Math.sin(4 * Math.PI));    y2.push(Math.sin(4 * Math.PI));      // Configuration for detail:    detailOptions = {      name : 'detail',      data : data,      height : 150,      flotr : {        yaxis : {          min : -1.1,          max : 1.1        }      }    };      // Configuration for summary:    summaryOptions = {      name : 'summary',      data : data,      height : 150,      flotr : {        yaxis : {          min : -1.1,          max : 1.1        },        selection : {          mode : 'x'        }      }    };      // Building a custom vis:    vis = new envision.Visualization();    detail = new envision.Component(detailOptions);    summary = new envision.Component(summaryOptions);    interaction = new envision.Interaction();      // Render Visualization    vis      .add(detail)      .add(summary)      .render(container);      // Wireup Interaction    interaction      .leader(summary)      .follower(detail)      .add(envision.actions.selection);  ```    ## API    ### Class `envision.Component`  _Defines a visualization component._    Components are the building blocks of a visualization,   representing one typically graphical piece of the vis.  This class manages  the options, DOM and API construction for an adapter which handles the  actual drawing of the visualization piece.    Adapters can take the form of an actual object, a constructor function  or a function returning an object.  Only one of these will be used.  If  none is submitted, the default adapter Flotr2 is used.    #### Configuration:    An object is submitted to the constructor for configuration.    * `name` A name for the component.  * `element` A container element for the component.  * `height` An explicit component height.  * `width` An explicit component width.  * `data` An array of data.  Data may be formatted for   envision or for the adapter itself, in which case skipPreprocess will  also need to be submitted.  * `skipPreprocess` Skip data preprocessing.  This is useful  when using the native data format for an adapter.  * `adapter` An adapter object.  * `adapterConstructor` An adapter constructor to be  instantiated by the component.  * `adapterCallback` An callback invoked by the component  returning an adapter.  * `config` Configuration for the adapter.    #### Methods:    ##### `render ([element])`  Render the component.    If no element is submitted, the component will  render in the element configured in the constructor.    ##### `draw ([data], [options])`  Draw the component.    ##### `trigger ()`  Trigger an event on the component's API.    Arguments are passed through to the API.    ##### `attach ()`  Attach to an event on the component's API.    Arguments are passed through to the API.    ##### `detach ()`  Detach a listener from an event on the component's API.    Arguments are passed through to the API.    ##### `destroy ()`  Destroy the component.    Empties the container and calls the destroy method on the  component's API.    ### Class `envision.Visualization`  _Defines a visualization of componenents._    This class manages the rendering of a visualization.  It provides convenience methods for adding, removing, and reordered  components dynamically as well as convenience methods for working  with a logical group of components.    #### Configuration:    An object is submitted to the constructor for configuration.    * `name` A name for the visualization.  * `element` A container element for the visualization.    #### Methods:    ##### `render ([element])`  Render the visualization.    If no element is submitted, the visualization will  render in the element configured in the constructor.    This method is chainable.    ##### `add (component)`  Add a component to the visualization.    If the visualization has already been rendered,  it will render the new component.    This method is chainable.    ##### `remove ()`  Remove a component from the visualization.    This removes the components from the list of components in the  visualization and removes its container from the DOM.  It does not  destroy the component.    This method is chainable.    ##### `setPosition (component, newIndex)`  Reorders a component.    This method is chainable.    ##### `indexOf (component)`  Gets the position of a component.    ##### `getComponent (component)`  Gets the component at a position.    ##### `isFirst (component)`  Gets whether or not the component is the first component  in the visualization.    ##### `isLast (component)`  Gets whether or not the component is the last component  in the visualization.    ##### `destroy ()`  Destroys the visualization.    This empties the container and destroys all the components which are part  of the visualization.    ### Class `envision.Preprocessor`  _Data preprocessor._    Data can be preprocessed before it is rendered by an adapter.    This has several important performance considerations.  If data will be   rendered repeatedly or on slower browsers, it will be faster after being  optimized.    First, data outside the boundaries does not need to be rendered.  Second,  the resolution of the data only needs to be at most the number of pixels  in the width of the visualization.    Performing these optimizations will limit memory overhead, important  for garbage collection and performance on old browsers, as well as drawing  overhead, important for mobile devices, old browsers and large data sets.    #### Configuration:    An object is submitted to the constructor for configuration.    * `data` The data for processing.    #### Methods:    ##### `getData ()`  Returns data.    ##### `setData ()`  Set the data object.    ##### `length ()`  Returns the length of the data set.    ##### `bound (min, max)`  Bounds the data set at within a range.    ##### `subsampleMinMax (resolution)`  Subsample data using MinMax.    MinMax will display the extrema of the subsample intervals.  This is  slower than regular interval subsampling but necessary for data that   is very non-homogenous.    ##### `subsample (resolution)`  Subsample data at a regular interval for resolution.    This is the fastest subsampling and good for monotonic data and fairly  homogenous data (not a lot of up and down).    ### Class `envision.Interaction`  _Defines an interaction between components._    This class defines interactions in which actions are triggered  by leader components and reacted to by follower components.  These actions  are defined as configurable mappings of trigger events and event consumers.  It is up to the adapter to implement the triggers and consumers.    A component may be both a leader and a follower.  A leader which is a   follower will react to actions triggered by other leaders, but will safely  not react to its own.  This allows for groups of components to perform a  common action.    Optionally, actions may be supplied with a callback executed before the   action is consumed.  This allows for quick custom functionality to be added  and is how advanced data management (ie. live Ajax data) may be implemented.    This class follow an observer mediator pattern.    #### Configuration:    An object is submitted to the constructor for configuration.    * `leader` Component(s) to lead the  interaction    #### Methods:    ##### `leader (component)`  Add a component as an interaction leader.    ##### `follower (component)`  Add a component as an interaction leader.    ##### `group (components)`  Adds an array of components as both followers and leaders.    ##### `add (action, [options])`  Adds an action to the interaction.    The action may be optionally configured with the options argument.  Currently the accepts a callback member, invoked after an action  is triggered and before it is consumed by followers.    ## Development    This project uses [smoosh](https://github.com/fat/smoosh) to build and [jasmine](http://pivotal.github.com/jasmine/)   with [js-imagediff](https://github.com/HumbleSoftware/js-imagediff) to test.  Tests may be executed by   [jasmine-headless-webkit](http://johnbintz.github.com/jasmine-headless-webkit/) with   `cd spec; jasmine-headless-webkit -j jasmine.yml -c` or by a browser by navigating to   `spec/SpecRunner.html`.   """
Big data;https://github.com/apache/incubator-superset;"""<!--  Licensed to the Apache Software Foundation (ASF) under one  or more contributor license agreements.  See the NOTICE file  distributed with this work for additional information  regarding copyright ownership.  The ASF licenses this file  to you under the Apache License, Version 2.0 (the  ""License""); you may not use this file except in compliance  with the License.  You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing,  software distributed under the License is distributed on an  ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY  KIND, either express or implied.  See the License for the  specific language governing permissions and limitations  under the License.  -->    # Superset    [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)  [![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/apache/superset?sort=semver)](https://github.com/apache/superset/tree/latest)  [![Build Status](https://github.com/apache/superset/workflows/Python/badge.svg)](https://github.com/apache/superset/actions)  [![PyPI version](https://badge.fury.io/py/apache-superset.svg)](https://badge.fury.io/py/apache-superset)  [![Coverage Status](https://codecov.io/github/apache/superset/coverage.svg?branch=master)](https://codecov.io/github/apache/superset)  [![PyPI](https://img.shields.io/pypi/pyversions/apache-superset.svg?maxAge=2592000)](https://pypi.python.org/pypi/apache-superset)  [![Get on Slack](https://img.shields.io/badge/slack-join-orange.svg)](https://join.slack.com/t/apache-superset/shared_invite/zt-uxbh5g36-AISUtHbzOXcu0BIj7kgUaw)  [![Documentation](https://img.shields.io/badge/docs-apache.org-blue.svg)](https://superset.apache.org)    <img    src=""https://github.com/apache/superset/raw/master/superset-frontend/src/assets/branding/superset-logo-horiz-apache.png""    alt=""Superset""    width=""500""  />    A modern, enterprise-ready business intelligence web application.    [**Why Superset?**](#why-superset) |  [**Supported Databases**](#supported-databases) |  [**Installation and Configuration**](#installation-and-configuration) |  [**Release Notes**](RELEASING/README.md#release-notes-for-recent-releases) |  [**Get Involved**](#get-involved) |  [**Contributor Guide**](#contributor-guide) |  [**Resources**](#resources) |  [**Organizations Using Superset**](RESOURCES/INTHEWILD.md)    ## Why Superset?    Superset is a modern data exploration and data visualization platform. Superset can replace or augment proprietary business intelligence tools for many teams.    Superset provides:    - A **no-code interface** for building charts quickly  - A powerful, web-based **SQL Editor** for advanced querying  - A **lightweight semantic layer** for quickly defining custom dimensions and metrics  - Out of the box support for **nearly any SQL** database or data engine  - A wide array of **beautiful visualizations** to showcase your data, ranging from simple bar charts to geospatial visualizations  - Lightweight, configurable **caching layer** to help ease database load  - Highly extensible **security roles and authentication** options  - An **API** for programmatic customization  - A **cloud-native architecture** designed from the ground up for scale    ## Screenshots & Gifs    **Large Gallery of Visualizations**    <kbd><a href=""https://superset.apache.org/gallery""><img title=""Gallery"" src=""superset-frontend/src/assets/images/screenshots/gallery.jpg""/></a></kbd><br/>    **Craft Beautiful, Dynamic Dashboards**    <kbd><img title=""View Dashboards"" src=""superset-frontend/src/assets/images/screenshots/slack_dash.jpg""/></kbd><br/>    **No-Code Chart Builder**    <kbd><img title=""Slice & dice your data"" src=""superset-frontend/src/assets/images/screenshots/explore.jpg""/></kbd><br/>    **Powerful SQL Editor**    <kbd><img title=""SQL Lab"" src=""superset-frontend/src/assets/images/screenshots/sql_lab.jpg""/></kbd><br/>    ## Supported Databases    Superset can query data from any SQL-speaking datastore or data engine (Presto, Trino, Athena, [and more](https://superset.apache.org/docs/databases/installing-database-drivers/)) that has a Python DB-API driver and a SQLAlchemy dialect.    Here are some of the major database solutions that are supported:    <p align=""center"">    <img src=""superset-frontend/src/assets/images/redshift.png"" alt=""redshift"" border=""0"" width=""106"" height=""41""/>    <img src=""superset-frontend/src/assets/images/google-biquery.png"" alt=""google-biquery"" border=""0"" width=""114"" height=""43""/>    <img src=""superset-frontend/src/assets/images/snowflake.png"" alt=""snowflake"" border=""0"" width=""152"" height=""46""/>    <img src=""superset-frontend/src/assets/images/trino.png"" alt=""trino"" border=""0"" width=""46"" height=""46""/>    <img src=""superset-frontend/src/assets/images/presto.png"" alt=""presto"" border=""0"" width=""152"" height=""46""/>    <img src=""superset-frontend/src/assets/images/druid.png"" alt=""druid"" border=""0"" width=""135"" height=""37"" />    <img src=""superset-frontend/src/assets/images/firebolt.png"" alt=""firebolt"" border=""0"" width=""133"" height=""21.5"" />    <img src=""superset-frontend/src/assets/images/timescale.png"" alt=""timescale"" border=""0"" width=""102"" height=""26.8"" />      <img src=""superset-frontend/src/assets/images/rockset.png"" alt=""rockset"" border=""0"" width=""125"" height=""51"" />    <img src=""superset-frontend/src/assets/images/postgresql.png"" alt=""postgresql"" border=""0"" width=""132"" height=""81"" />    <img src=""superset-frontend/src/assets/images/mysql.png"" alt=""mysql"" border=""0"" width=""119"" height=""62"" />    <img src=""superset-frontend/src/assets/images/mssql-server.png"" alt=""mssql-server"" border=""0"" width=""93"" height=""74"" />    <img src=""superset-frontend/src/assets/images/db2.png"" alt=""db2"" border=""0"" width=""62"" height=""62"" />    <img src=""superset-frontend/src/assets/images/sqlite.png"" alt=""sqlite"" border=""0"" width=""102"" height=""45"" />    <img src=""superset-frontend/src/assets/images/sybase.png"" alt=""sybase"" border=""0"" width=""128"" height=""47"" />    <img src=""superset-frontend/src/assets/images/mariadb.png"" alt=""mariadb"" border=""0"" width=""83"" height=""63"" />    <img src=""superset-frontend/src/assets/images/vertica.png"" alt=""vertica"" border=""0"" width=""128"" height=""40"" />    <img src=""superset-frontend/src/assets/images/oracle.png"" alt=""oracle"" border=""0"" width=""121"" height=""66"" />    <img src=""superset-frontend/src/assets/images/firebird.png"" alt=""firebird"" border=""0"" width=""86"" height=""56"" />    <img src=""superset-frontend/src/assets/images/greenplum.png"" alt=""greenplum"" border=""0"" width=""140"" height=""45"" />    <img src=""superset-frontend/src/assets/images/clickhouse.png"" alt=""clickhouse"" border=""0"" width=""133"" height=""34"" />    <img src=""superset-frontend/src/assets/images/exasol.png"" alt=""exasol"" border=""0"" width=""106"" height=""59"" />    <img src=""superset-frontend/src/assets/images/monet-db.png"" alt=""monet-db"" border=""0"" width=""106"" height=""46"" />    <img src=""superset-frontend/src/assets/images/apache-kylin.png"" alt=""apache-kylin"" border=""0"" width=""56"" height=""64""/>    <img src=""superset-frontend/src/assets/images/hologres.png"" alt=""hologres"" border=""0"" width=""71"" height=""64""/>    <img src=""superset-frontend/src/assets/images/netezza.png"" alt=""netezza"" border=""0"" width=""64"" height=""64""/>    <img src=""superset-frontend/src/assets/images/pinot.png"" alt=""pinot"" border=""0"" width=""165"" height=""64""/>    <img src=""superset-frontend/src/assets/images/teradata.png"" alt=""teradata"" border=""0"" width=""165"" height=""64""/>    <img src=""superset-frontend/src/assets/images/yugabyte.png"" alt=""yugabyte"" border=""0"" width=""180"" height=""31""/>  </p>    **A more comprehensive list of supported databases** along with the configuration instructions can be found  [here](https://superset.apache.org/docs/databases/installing-database-drivers).    Want to add support for your datastore or data engine? Read more [here](https://superset.apache.org/docs/frequently-asked-questions#does-superset-work-with-insert-database-engine-here) about the technical requirements.    ## Installation and Configuration    [Extended documentation for Superset](https://superset.apache.org/docs/installation/installing-superset-using-docker-compose)    ## Get Involved    - Ask and answer questions on [StackOverflow](https://stackoverflow.com/questions/tagged/apache-superset) using the **apache-superset** tag  - [Join our community's Slack](https://join.slack.com/t/apache-superset/shared_invite/zt-uxbh5g36-AISUtHbzOXcu0BIj7kgUaw)    and please read our [Slack Community Guidelines](https://github.com/apache/superset/blob/master/CODE_OF_CONDUCT.md#slack-community-guidelines)  - [Join our dev@superset.apache.org Mailing list](https://lists.apache.org/list.html?dev@superset.apache.org)    ## Contributor Guide    Interested in contributing? Check out our  [CONTRIBUTING.md](https://github.com/apache/superset/blob/master/CONTRIBUTING.md)  to find resources around contributing along with a detailed guide on  how to set up a development environment.    ## Resources    - Getting Started with Superset    - [Superset in 2 Minutes using Docker Compose](https://superset.apache.org/docs/installation/installing-superset-using-docker-compose#installing-superset-locally-using-docker-compose)    - [Installing Database Drivers](https://superset.apache.org/docs/databases/dockeradddrivers)    - [Building New Database Connectors](https://preset.io/blog/building-database-connector/)    - [Create Your First Dashboard](https://superset.apache.org/docs/creating-charts-dashboards/first-dashboard)    - [Comprehensive Tutorial for Contributing Code to Apache Superset  ](https://preset.io/blog/tutorial-contributing-code-to-apache-superset/)  - [Documentation for Superset End-Users (by Preset)](https://docs.preset.io/docs/terminology)  - Deploying Superset    - [Official Docker image](https://hub.docker.com/r/apache/superset)    - [Helm Chart](https://github.com/apache/superset/tree/master/helm/superset)  - Recordings of Past [Superset Community Events](https://preset.io/events)    - [Live Demo: Interactive Time-series Analysis with Druid and Superset](https://preset.io/events/2021-03-02-interactive-time-series-analysis-with-druid-and-superset/)    - [Live Demo: Visualizing MongoDB and Pinot Data using Trino](https://preset.io/events/2021-04-13-visualizing-mongodb-and-pinot-data-using-trino/)  	- [Superset Contributor Bootcamp](https://preset.io/events/superset-contributor-bootcamp-dec-21/)  	- [Introduction to the Superset API](https://preset.io/events/introduction-to-the-superset-api/)  	- [Apache Superset 1.3 Meetup](https://preset.io/events/apache-superset-1-3/)  	- [Building a Database Connector for Superset](https://preset.io/events/2021-02-16-building-a-database-connector-for-superset/)  - Visualizations    - [Building Custom Viz Plugins](https://superset.apache.org/docs/installation/building-custom-viz-plugins)    - [Managing and Deploying Custom Viz Plugins](https://medium.com/nmc-techblog/apache-superset-manage-custom-viz-plugins-in-production-9fde1a708e55)    - [Why Apache Superset is Betting on Apache ECharts](https://preset.io/blog/2021-4-1-why-echarts/)    - [Superset API](https://superset.apache.org/docs/rest-api) """
Big data;https://github.com/BIDData/BIDMach;"""    BIDMach is a very fast machine learning library. Check the latest <b><a href=""https://github.com/BIDData/BIDMach/wiki/Benchmarks"">benchmarks</a></b>    The github distribution contains source code only. You also need a jdk 8, an installation of NVIDIA CUDA 8.0 (if you want to use a GPU) and CUDNN 5 if you plan to use deep networks. For building you need <a href=""https://maven.apache.org/docs/history.html"">maven 3.X</a>.    After doing <code>git clone</code>, cd to the BIDMach directory, and build and install the jars with <code>mvn install</code>. You can then run bidmach with `./bidmach`. More details on installing and running are available <b><a href=""https://github.com/BIDData/BIDMach/wiki/Installing-and-Running"">here</a></b>.    The main project page is <b><a href=""http://bid2.berkeley.edu/bid-data-project/"">here</a></b>.    Documentation is <b><a href=""https://github.com/BIDData/BIDMach/wiki"">here in the wiki</a></b>    <b>New</b> BIDMach has a <b><a href=""https://groups.google.com/forum/#!forum/bidmach-users-group"">discussion group</a></b> on Google Groups.    BIDMach is a sister project of BIDMat, a matrix library, which is   <b><a href=""https://github.com/BIDData/BIDMat"">also on github</a></b>    BIDData also has a project for deep reinforcement learning. <b><a href=""https://github.com/BIDData/BIDMach_RL"">BIDMach_RL</a></b> contains state-of-the-art implementations of several reinforcement learning algorithms. """
Big data;https://github.com/twitter/scalding;"""# Scalding    Scalding is a Scala library that makes it easy to specify Hadoop MapReduce jobs. Scalding is built on top of [Cascading](http://www.cascading.org/), a Java library that abstracts away low-level Hadoop details. Scalding is comparable to [Pig](http://pig.apache.org/), but offers tight integration with Scala, bringing advantages of Scala to your MapReduce jobs.    ![Scalding Logo](https://raw.github.com/twitter/scalding/develop/logo/scalding.png)    Current version: `0.15.0`    ## Word Count    Hadoop is a distributed system for counting words. Here is how it's done in Scalding.    ```scala  package com.twitter.scalding.examples    import com.twitter.scalding._    class WordCountJob(args: Args) extends Job(args) {    TypedPipe.from(TextLine(args(""input"")))      .flatMap { line => tokenize(line) }      .groupBy { word => word } // use each word for a key      .size // in each group, get the size      .write(TypedTsv[(String, Long)](args(""output"")))      // Split a piece of text into individual words.    def tokenize(text: String): Array[String] = {      // Lowercase each word and remove punctuation.      text.toLowerCase.replaceAll(""[^a-zA-Z0-9\\s]"", """").split(""\\s+"")    }  }  ```    Notice that the `tokenize` function, which is standard Scala, integrates naturally with the rest of the MapReduce job. This is a very powerful feature of Scalding. (Compare it to the use of UDFs in Pig.)    You can find more example code under [examples/](https://github.com/twitter/scalding/tree/master/scalding-core/src/main/scala/com/twitter/scalding/examples). If you're interested in comparing Scalding to other languages, see our [Rosetta Code page](https://github.com/twitter/scalding/wiki/Rosetta-Code), which has several MapReduce tasks in Scalding and other frameworks (e.g., Pig and Hadoop Streaming).    ## Documentation and Getting Started    * [**Getting Started**](https://github.com/twitter/scalding/wiki/Getting-Started) page on the [Scalding Wiki](https://github.com/twitter/scalding/wiki)  * [Scalding Scaladocs](http://twitter.github.com/scalding) provide details beyond the API References. Prefer using this as it's always up to date.  * [**REPL in Wonderland**](https://gist.github.com/johnynek/a47699caa62f4f38a3e2) a hands-on tour of the    scalding REPL requiring only git and java installed.  * [**Runnable tutorials**](https://github.com/twitter/scalding/tree/master/tutorial) in the source.  * The API Reference, including many example Scalding snippets:    * [Type-safe API Reference](https://github.com/twitter/scalding/wiki/Type-safe-api-reference)    * [Fields-based API Reference](https://github.com/twitter/scalding/wiki/Fields-based-API-Reference)  * The Matrix Library provides a way of working with key-attribute-value scalding pipes:    * The [Introduction to Matrix Library](https://github.com/twitter/scalding/wiki/Introduction-to-Matrix-Library) contains an overview and a ""getting started"" example    * The [Matrix API Reference](https://github.com/twitter/scalding/wiki/Matrix-API-Reference) contains the Matrix Library API reference with examples  * [**Introduction to Scalding Execution**](https://github.com/twitter/scalding/wiki/Calling-Scalding-from-inside-your-application) contains general rules and examples of calling Scalding from inside another application.    Please feel free to use the beautiful [Scalding logo](https://drive.google.com/folderview?id=0B3i3pDi3yVgNbm9pMUdDcHFKVEk&usp=sharing) artwork anywhere.    ## Code of Conduct  This, and all github.com/twitter projects, are under the [Twitter Open Source Code of Conduct](https://engineering.twitter.com/opensource/code-of-conduct). Additionally, see the [Typelevel Code of Conduct](http://typelevel.org/conduct) for specific examples of harassing behavior that are not tolerated.    ## Building  There is a script (called sbt) in the root that loads the correct sbt version to build:    1. ```./sbt update``` (takes 2 minutes or more)  2. ```./sbt test```  3. ```./sbt assembly``` (needed to make the jar used by the scald.rb script)    The test suite takes a while to run. When you're in sbt, here's a shortcut to run just one test:    ```> test-only com.twitter.scalding.FileSourceTest```    Please refer to [FAQ page](https://github.com/twitter/scalding/wiki/Frequently-asked-questions#issues-with-sbt) if you encounter problems when using sbt.    We use [Travis CI](http://travis-ci.org/) to verify the build:  [![Build Status](https://travis-ci.org/twitter/scalding.svg?branch=develop)](http://travis-ci.org/twitter/scalding)    We use [Coveralls](https://coveralls.io/r/twitter/scalding) for code coverage results:  [![Coverage Status](https://coveralls.io/repos/twitter/scalding/badge.png?branch=develop)](https://coveralls.io/r/twitter/scalding?branch=develop)    Scalding modules are available from maven central.    The current groupid and version for all modules is, respectively, `""com.twitter""` and  `0.16.0-RC1`.    Current published artifacts are    * `scalding-core_2.10`  * `scalding-args_2.10`  * `scalding-date_2.10`  * `scalding-commons_2.10`  * `scalding-avro_2.10`  * `scalding-parquet_2.10`  * `scalding-repl_2.10`      The suffix denotes the scala version.    ## Adopters    * Ebay  * Etsy  * Sharethrough  * Snowplow Analytics  * Soundcloud  * Twitter    To see a full list of users or to add yourself, see the [wiki](https://github.com/twitter/scalding/wiki/Powered-By)    ## Contact  For user questions or scalding development (internals, extending, release planning):  <https://groups.google.com/forum/#!forum/scalding-dev> (Google search also works as a first step)    In the remote possibility that there exist bugs in this code, please report them to:  <https://github.com/twitter/scalding/issues>    Follow [@Scalding](http://twitter.com/scalding) on Twitter for updates.    Chat: [![Gitter](https://badges.gitter.im/twitter/scalding.svg)](https://gitter.im/twitter/scalding?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)    ## Authors:  * Avi Bryant <http://twitter.com/avibryant>  * Oscar Boykin <http://twitter.com/posco>  * Argyris Zymnis <http://twitter.com/argyris>    Thanks for assistance and contributions:    * Sam Ritchie <http://twitter.com/sritchie>  * Aaron Siegel: <http://twitter.com/asiegel>  * Ian O'Connell <http://twitter.com/0x138>  * Alex Levenson <http://twitter.com/THISWILLWORK>  * Jonathan Coveney <http://twitter.com/jco>  * Kevin Lin <http://twitter.com/reconditesea>  * Brad Greenlee: <http://twitter.com/bgreenlee>  * Edwin Chen <http://twitter.com/edchedch>  * Arkajit Dey: <http://twitter.com/arkajit>  * Krishnan Raman: <http://twitter.com/dxbydt_jasq>  * Flavian Vasile <http://twitter.com/flavianv>  * Chris Wensel <http://twitter.com/cwensel>  * Ning Liang <http://twitter.com/ningliang>  * Dmitriy Ryaboy <http://twitter.com/squarecog>  * Dong Wang <http://twitter.com/dongwang218>  * Josh Attenberg <http://twitter.com/jattenberg>  * Juliet Hougland <https://twitter.com/j_houg>  * Eddie Xie <https://twitter.com/eddiex>    A full list of [contributors](https://github.com/twitter/scalding/graphs/contributors) can be found on GitHub.    ## License  Copyright 2013 Twitter, Inc.    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/Microsoft/GraphEngine;"""# Graph Engine - Open Source    | - | Windows Multi Targeting | Ubuntu 16.04 .NET Core |  |:------:|:------:|:------:|  |Build|[<img src=""https://trinitygraphengine.visualstudio.com/_apis/public/build/definitions/4cfbb293-cd2c-4f49-aa03-06894081c93b/3/badge""/>](https://trinitygraphengine.visualstudio.com/trinity-ci/_build/index?definitionId=3)|[<img src=""https://trinitygraphengine.visualstudio.com/_apis/public/build/definitions/4cfbb293-cd2c-4f49-aa03-06894081c93b/4/badge""/>](https://trinitygraphengine.visualstudio.com/trinity-ci/_build/index?definitionId=4)|  |Tests|_|_|  |Stress|_|_|    This repository contains the source code of [Graph Engine][graph-engine] and its graph  query language -- [Language Integrated Knowledge Query][likq] (LIKQ).    Microsoft Graph Engine is a distributed  in-memory data processing engine, underpinned by a strongly-typed  in-memory key-value store and a general-purpose distributed computation  engine.    [LIKQ][likq-gh]  is a versatile graph query language atop Graph Engine. It  combines the capability of fast graph exploration with the flexibility  of lambda expression. Server-side computations can be expressed in  lambda expressions, embedded in LIKQ, and executed on Graph Engine servers during graph traversal.    ## How to contribute    If you are interested in contributing to Graph Engine, please fork the  repository and submit pull requests to the `master` branch.    Pull requests, issue reports, and suggestions are welcome.    Please submit bugs and feature requests as [GitHub Issues](https://github.com/Microsoft/GraphEngine/issues).    ## Getting started with Graph Engine    ### NuGet packages and Visual Studio extension    NuGet packages [Graph Engine Core][graph-engine-core] and [LIKQ][likq-nuget] are available in the NuGet Gallery.    If you develop Graph Engine applications using [Visual Studio][vs] on Windows, [Graph Engine VSExtension][vs-extension] can be used to facilitate the development work.    ### Building on Windows    Install [Visual Studio 2017 or 2019][vs] with the following components selected:    - .NET desktop development      - .NET Framework 4 -- 4.6 development tools  - Desktop development with C++      - Windows 10 SDK      - Windows 8.1 SDK and UCRT SDK  - Visual Studio extension development  - .NET Core SDK 3.1  - cmake    [.NET Core SDK][dotnet-download] and [cmake][cmake-download] can alternatively be installed using their standalone installers.    The Windows build will generate multi-targeting nuget packages.  Open a powershell window, run `tools/build.ps1` for Visual Studio 2017 or `tools/build.ps1 -VS2019` for Visual Studio 2019.    The Linux native assemblies will also be packaged (pre-built at `lib`) to allow the Windows build to work for Linux `.Net Core` as well.    ### Building on Linux    Install `libunwind8`, `g++`, `cmake` and `libssl-dev`. For example, run `sudo apt install libunwind8 g++ cmake libssl-dev` for Ubuntu.    Install [.NET Core 3.1][dotnet-download] and execute `bash tools/build.sh`.    The Windows native assemblies will also be packaged so that the  Linux build will work for Windows `.Net Core` as well.    **Note:** Because `.Net Framework` is Windows-only, the packages built on Linux only support `.Net Core`. The build script is tested only on `Ubuntu 16.04`, `Ubuntu 18.04`, and `Ubuntu 20.04`.    ### How to use the built Graph Engine packages    Nuget packages will be built as  `build/GraphEngine**._version_.nupkg`. The folder `build/` will be  registered as a local NuGet repository and the local package cache for  `GraphEngine.Core` will be cleared. After the packages are built, run `dotnet restore` to use the newly built package.    ### Run your first Graph Engine app    Go to the `samples/Friends/Friends` folder, execute `dotnet restore` and `dotnet run` to run the sample project.    ## License    Copyright (c) Microsoft Corporation. All rights reserved.    Licensed under the [MIT][license] License.      <!--  Links  -->    [graph-engine]: https://www.graphengine.io/    [likq]: https://www.graphengine.io/video/likq.video.html    [likq-gh]: https://github.com/Microsoft/GraphEngine/tree/master/src/Modules/LIKQ    [academic-graph-search]: https://azure.microsoft.com/en-us/services/cognitive-services/academic-knowledge/    [vs-extension]: https://visualstudiogallery.msdn.microsoft.com/12835dd2-2d0e-4b8e-9e7e-9f505bb909b8    [graph-engine-core]: https://www.nuget.org/packages/GraphEngine.Core/    [likq-nuget]: https://www.nuget.org/packages/GraphEngine.LIKQ/    [vs]: https://www.visualstudio.com/    [dotnet-download]: https://dotnet.microsoft.com/download/    [cmake-download]: https://cmake.org/download/    [license]: LICENSE.md """
Big data;https://github.com/fchollet/keras;"""# Keras: Deep Learning for humans    ![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)    This repository hosts the development of the Keras library.  Read the documentation at [keras.io](https://keras.io/).    ## About Keras    Keras is a deep learning API written in Python,  running on top of the machine learning platform [TensorFlow](https://github.com/tensorflow/tensorflow).  It was developed with a focus on enabling fast experimentation.  *Being able to go from idea to result as fast as possible is key to doing good research.*    Keras is:    -   **Simple** -- but not simplistic. Keras reduces developer *cognitive load*      to free you to focus on the parts of the problem that really matter.  -   **Flexible** -- Keras adopts the principle of *progressive disclosure of      complexity*: simple workflows should be quick and easy, while arbitrarily      advanced workflows should be *possible* via a clear path that builds upon      what you've already learned.  -   **Powerful** -- Keras provides industry-strength performance and      scalability: it is used by organizations and companies including NASA,      YouTube, and Waymo.    ---    ## Keras & TensorFlow 2    [TensorFlow 2](https://www.tensorflow.org/) is an end-to-end, open-source machine learning platform.  You can think of it as an infrastructure layer for  [differentiable programming](https://en.wikipedia.org/wiki/Differentiable_programming).  It combines four key abilities:    - Efficiently executing low-level tensor operations on CPU, GPU, or TPU.  - Computing the gradient of arbitrary differentiable expressions.  - Scaling computation to many devices, such as clusters of hundreds of GPUs.  - Exporting programs (""graphs"") to external runtimes such as servers, browsers, mobile and embedded devices.    Keras is the high-level API of TensorFlow 2: an approachable, highly-productive interface  for solving machine learning problems,  with a focus on modern deep learning. It provides essential abstractions and building blocks for developing  and shipping machine learning solutions with high iteration velocity.    Keras empowers engineers and researchers to take full advantage of the scalability  and cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,  and you can export your Keras models to run in the browser or on a mobile device.    ---    ## First contact with Keras    The core data structures of Keras are __layers__ and __models__.  The simplest type of model is the [`Sequential` model](/guides/sequential_model/), a linear stack of layers.  For more complex architectures, you should use the [Keras functional API](/guides/functional_api/),  which allows to build arbitrary graphs of layers, or [write models entirely from scratch via subclasssing](/guides/making_new_layers_and_models_via_subclassing/).    Here is the `Sequential` model:    ```python  from tensorflow.keras.models import Sequential    model = Sequential()  ```    Stacking layers is as easy as `.add()`:    ```python  from tensorflow.keras.layers import Dense    model.add(Dense(units=64, activation='relu'))  model.add(Dense(units=10, activation='softmax'))  ```    Once your model looks good, configure its learning process with `.compile()`:    ```python  model.compile(loss='categorical_crossentropy',                optimizer='sgd',                metrics=['accuracy'])  ```    If you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,  while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code via subclassing).    ```python  model.compile(loss=tf.keras.losses.categorical_crossentropy,                optimizer=tf.keras.optimizers.SGD(                    learning_rate=0.01, momentum=0.9, nesterov=True))  ```    You can now iterate on your training data in batches:    ```python  # x_train and y_train are Numpy arrays.  model.fit(x_train, y_train, epochs=5, batch_size=32)  ```    Evaluate your test loss and metrics in one line:    ```python  loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)  ```    Or generate predictions on new data:    ```python  classes = model.predict(x_test, batch_size=128)  ```    What you just saw is the most elementary way to use Keras.    However, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.  Keras follows the principle of **progressive disclosure of complexity**: it makes it easy to get started,  yet it makes it possible to handle arbitrarily advanced use cases,  only requiring incremental learning at each step.    In much the same way that you were able to train & evaluate a simple neural network above in a few lines,  you can use Keras to quickly develop new training procedures or exotic model architectures.  Here's a low-level training loop example, combining Keras functionality with the TensorFlow `GradientTape`:    ```python  import tensorflow as tf    # Prepare an optimizer.  optimizer = tf.keras.optimizers.Adam()  # Prepare a loss function.  loss_fn = tf.keras.losses.kl_divergence    # Iterate over the batches of a dataset.  for inputs, targets in dataset:      # Open a GradientTape.      with tf.GradientTape() as tape:          # Forward pass.          predictions = model(inputs)          # Compute the loss value for this batch.          loss_value = loss_fn(targets, predictions)        # Get gradients of loss wrt the weights.      gradients = tape.gradient(loss_value, model.trainable_weights)      # Update the weights of the model.      optimizer.apply_gradients(zip(gradients, model.trainable_weights))  ```    For more in-depth tutorials about Keras, you can check out:    - [Introduction to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)  - [Introduction to Keras for researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)  - [Developer guides](https://keras.io/guides/)    ---    ## Installation    Keras comes packaged with TensorFlow 2 as `tensorflow.keras`.  To start using Keras, simply [install TensorFlow 2](https://www.tensorflow.org/install).    ---    ## Release and compatibility    Keras has **nightly releases** (`keras-nightly` on PyPI)  and **stable releases** (`keras` on PyPI).  The nightly Keras releases are usually compatible with the corresponding version  of the `tf-nightly` releases  (e.g. `keras-nightly==2.7.0.dev2021100607` should be  used with `tf-nightly==2.7.0.dev2021100607`).  We don't maintain backward compatibility for nightly releases.  For stable releases, each Keras  version maps to a specific stable version of TensorFlow.    The table below shows the compatibility version mapping  between TensorFlow versions and Keras versions.    All the release branches can be found on [Github](https://github.com/keras-team/keras/releases).    All the release binaries can be found on [Pypi](https://pypi.org/project/keras/#history).    | Keras release | Note      | Compatible Tensorflow version |  | -----------   | ----------- | -----------        |  | [2.4](https://github.com/keras-team/keras/releases/tag/2.4.0)  | Last stable release of multi-backend Keras | < 2.5  | 2.5-pre| Pre-release (not formal) for standalone Keras repo | >= 2.5 < 2.6  | [2.6](https://github.com/keras-team/keras/releases/tag/v2.6.0)    | First formal release of standalone Keras.  | >= 2.6 < 2.7  | [2.7](https://github.com/keras-team/keras/releases/tag/v2.7.0-rc0)    | (Upcoming release) | >= 2.7 < 2.8  | nightly|                                            | tf-nightly    ---  ## Support    You can ask questions and join the development discussion:    - In the [TensorFlow forum](https://discuss.tensorflow.org/).  - On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).  - On the [Keras Slack channel](https://kerasteam.slack.com). Use [this link](https://keras-slack-autojoin.herokuapp.com/) to request an invitation to the channel.    ---    ## Opening an issue    You can also post **bug reports and feature requests** (only)  in [GitHub issues](https://github.com/keras-team/keras/issues).      ---    ## Opening a PR    We welcome contributions! Before opening a PR, please read  [our contributor guide](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md),  and the [API design guideline](https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md). """
Big data;https://github.com/VictoriaMetrics/VictoriaMetrics;"""# VictoriaMetrics    [![Latest Release](https://img.shields.io/github/release/VictoriaMetrics/VictoriaMetrics.svg?style=flat-square)](https://github.com/VictoriaMetrics/VictoriaMetrics/releases/latest)  [![Docker Pulls](https://img.shields.io/docker/pulls/victoriametrics/victoria-metrics.svg?maxAge=604800)](https://hub.docker.com/r/victoriametrics/victoria-metrics)  [![Slack](https://img.shields.io/badge/join%20slack-%23victoriametrics-brightgreen.svg)](https://slack.victoriametrics.com/)  [![GitHub license](https://img.shields.io/github/license/VictoriaMetrics/VictoriaMetrics.svg)](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/LICENSE)  [![Go Report](https://goreportcard.com/badge/github.com/VictoriaMetrics/VictoriaMetrics)](https://goreportcard.com/report/github.com/VictoriaMetrics/VictoriaMetrics)  [![Build Status](https://github.com/VictoriaMetrics/VictoriaMetrics/workflows/main/badge.svg)](https://github.com/VictoriaMetrics/VictoriaMetrics/actions)  [![codecov](https://codecov.io/gh/VictoriaMetrics/VictoriaMetrics/branch/master/graph/badge.svg)](https://codecov.io/gh/VictoriaMetrics/VictoriaMetrics)    <img src=""logo.png"" width=""300"" alt=""VictoriaMetrics logo"">    VictoriaMetrics is a fast, cost-effective and scalable monitoring solution and time series database.    VictoriaMetrics is available in [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases),  [Docker images](https://hub.docker.com/r/victoriametrics/victoria-metrics/), [Snap packages](https://snapcraft.io/victoriametrics)  and [source code](https://github.com/VictoriaMetrics/VictoriaMetrics). Just download VictoriaMetrics and follow [these instructions](#how-to-start-victoriametrics).  Then read [Prometheus setup](#prometheus-setup) and [Grafana setup](#grafana-setup) docs.    Cluster version of VictoriaMetrics is available [here](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html).    [Contact us](mailto:info@victoriametrics.com) if you need enterprise support for VictoriaMetrics. See [features available in enterprise package](https://victoriametrics.com/products/enterprise/). Enterprise binaries can be downloaded and evaluated for free from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).      ## Prominent features    VictoriaMetrics has the following prominent features:    * It can be used as long-term storage for Prometheus. See [these docs](#prometheus-setup) for details.  * It can be used as drop-in replacement for Prometheus in Grafana, because it supports [Prometheus querying API](#prometheus-querying-api-usage).  * It can be used as drop-in replacement for Graphite in Grafana, because it supports [Graphite API](#graphite-api-usage).  * It features easy setup and operation:    * VictoriaMetrics consists of a single [small executable](https://medium.com/@valyala/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d) without external dependencies.    * All the configuration is done via explicit command-line flags with reasonable defaults.    * All the data is stored in a single directory pointed by `-storageDataPath` command-line flag.    * Easy and fast backups from [instant snapshots](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282) to S3 or GCS can be done with [vmbackup](https://docs.victoriametrics.com/vmbackup.html) / [vmrestore](https://docs.victoriametrics.com/vmrestore.html) tools. See [this article](https://medium.com/@valyala/speeding-up-backups-for-big-time-series-databases-533c1a927883) for more details.  * It implements PromQL-based query language - [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html), which provides improved functionality on top of PromQL.  * It provides global query view. Multiple Prometheus instances or any other data sources may ingest data into VictoriaMetrics. Later this data may be queried via a single query.  * It provides high performance and good vertical and horizontal scalability for both [data ingestion](https://medium.com/@valyala/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b) and [data querying](https://medium.com/@valyala/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4). It [outperforms InfluxDB and TimescaleDB by up to 20x](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae).  * It [uses 10x less RAM than InfluxDB](https://medium.com/@valyala/insert-benchmarks-with-inch-influxdb-vs-victoriametrics-e31a41ae2893) and [up to 7x less RAM than Prometheus, Thanos or Cortex](https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f) when dealing with millions of unique time series (aka [high cardinality](https://docs.victoriametrics.com/FAQ.html#what-is-high-cardinality)).  * It is optimized for time series with [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate).  * It provides high data compression, so [up to 70x more data points](https://medium.com/@valyala/when-size-matters-benchmarking-victoriametrics-vs-timescale-and-influxdb-6035811952d4) may be crammed into limited storage comparing to TimescaleDB and [up to 7x less storage space is required compared to Prometheus, Thanos or Cortex](https://valyala.medium.com/prometheus-vs-victoriametrics-benchmark-on-node-exporter-metrics-4ca29c75590f).  * It is optimized for storage with high-latency IO and low IOPS (HDD and network storage in AWS, Google Cloud, Microsoft Azure, etc). See [disk IO graphs from these benchmarks](https://medium.com/@valyala/high-cardinality-tsdb-benchmarks-victoriametrics-vs-timescaledb-vs-influxdb-13e6ee64dd6b).  * A single-node VictoriaMetrics may substitute moderately sized clusters built with competing solutions such as Thanos, M3DB, Cortex, InfluxDB or TimescaleDB. See [vertical scalability benchmarks](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae), [comparing Thanos to VictoriaMetrics cluster](https://medium.com/@valyala/comparing-thanos-to-victoriametrics-cluster-b193bea1683) and [Remote Write Storage Wars](https://promcon.io/2019-munich/talks/remote-write-storage-wars/) talk from [PromCon 2019](https://promcon.io/2019-munich/talks/remote-write-storage-wars/).  * It protects the storage from data corruption on unclean shutdown (i.e. OOM, hardware reset or `kill -9`) thanks to [the storage architecture](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).  * It supports metrics' scraping, ingestion and [backfilling](#backfilling) via the following protocols:    * [Metrics scraping from Prometheus exporters](#how-to-scrape-prometheus-exporters-such-as-node-exporter).    * [Prometheus remote write API](#prometheus-setup).    * [Prometheus exposition format](#how-to-import-data-in-prometheus-exposition-format).    * [InfluxDB line protocol](#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf) over HTTP, TCP and UDP.    * [Graphite plaintext protocol](#how-to-send-data-from-graphite-compatible-agents-such-as-statsd) with [tags](https://graphite.readthedocs.io/en/latest/tags.html#carbon).    * [OpenTSDB put message](#sending-data-via-telnet-put-protocol).    * [HTTP OpenTSDB /api/put requests](#sending-opentsdb-data-via-http-apiput-requests).    * [JSON line format](#how-to-import-data-in-json-line-format).    * [Arbitrary CSV data](#how-to-import-csv-data).    * [Native binary format](#how-to-import-data-in-native-format).  * It supports metrics' relabeling. See [these docs](#relabeling) for details.  * It can deal with [high cardinality issues](https://docs.victoriametrics.com/FAQ.html#what-is-high-cardinality) and [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate) issues via [series limiter](#cardinality-limiter).  * It ideally works with big amounts of time series data from APM, Kubernetes, IoT sensors, connected cars, industrial telemetry, financial data and various [Enterprise workloads](https://victoriametrics.com/products/enterprise/).  * It has open source [cluster version](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster).    See also [various Articles about VictoriaMetrics](https://docs.victoriametrics.com/Articles.html).      ## Case studies and talks    Case studies:    * [AbiosGaming](https://docs.victoriametrics.com/CaseStudies.html#abiosgaming)  * [adidas](https://docs.victoriametrics.com/CaseStudies.html#adidas)  * [Adsterra](https://docs.victoriametrics.com/CaseStudies.html#adsterra)  * [ARNES](https://docs.victoriametrics.com/CaseStudies.html#arnes)  * [Brandwatch](https://docs.victoriametrics.com/CaseStudies.html#brandwatch)  * [CERN](https://docs.victoriametrics.com/CaseStudies.html#cern)  * [COLOPL](https://docs.victoriametrics.com/CaseStudies.html#colopl)  * [Dreamteam](https://docs.victoriametrics.com/CaseStudies.html#dreamteam)  * [Fly.io](https://docs.victoriametrics.com/CaseStudies.html#flyio)  * [German Research Center for Artificial Intelligence](https://docs.victoriametrics.com/CaseStudies.html#german-research-center-for-artificial-intelligence)  * [Grammarly](https://docs.victoriametrics.com/CaseStudies.html#grammarly)  * [Groove X](https://docs.victoriametrics.com/CaseStudies.html#groove-x)  * [Idealo.de](https://docs.victoriametrics.com/CaseStudies.html#idealode)  * [MHI Vestas Offshore Wind](https://docs.victoriametrics.com/CaseStudies.html#mhi-vestas-offshore-wind)  * [Razorpay](https://docs.victoriametrics.com/CaseStudies.html#razorpay)  * [Percona](https://docs.victoriametrics.com/CaseStudies.html#percona)  * [Sensedia](https://docs.victoriametrics.com/CaseStudies.html#sensedia)  * [Smarkets](https://docs.victoriametrics.com/CaseStudies.html#smarkets)  * [Synthesio](https://docs.victoriametrics.com/CaseStudies.html#synthesio)  * [Wedos.com](https://docs.victoriametrics.com/CaseStudies.html#wedoscom)  * [Wix.com](https://docs.victoriametrics.com/CaseStudies.html#wixcom)  * [Zerodha](https://docs.victoriametrics.com/CaseStudies.html#zerodha)  * [zhihu](https://docs.victoriametrics.com/CaseStudies.html#zhihu)    See also [articles and slides about VictoriaMetrics from our users](https://docs.victoriametrics.com/Articles.html#third-party-articles-and-slides-about-victoriametrics)      ## Operation    ## How to start VictoriaMetrics    Just download [VictoriaMetrics executable](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) or [Docker image](https://hub.docker.com/r/victoriametrics/victoria-metrics/) and start it with the desired command-line flags.    The following command-line flags are used the most:    * `-storageDataPath` - VictoriaMetrics stores all the data in this directory. Default path is `victoria-metrics-data` in the current working directory.  * `-retentionPeriod` - retention for stored data. Older data is automatically deleted. Default retention is 1 month. See [the Retention section](#retention) for more details.    Other flags have good enough default values, so set them only if you really need this. Pass `-help` to see [all the available flags with description and default values](#list-of-command-line-flags).    See how to [ingest data to VictoriaMetrics](#how-to-import-time-series-data), how to [query VictoriaMetrics via Grafana](#grafana-setup), how to [query VictoriaMetrics via Graphite API](#graphite-api-usage) and how to [handle alerts](#alerting).    VictoriaMetrics accepts [Prometheus querying API requests](#prometheus-querying-api-usage) on port `8428` by default.    It is recommended setting up [monitoring](#monitoring) for VictoriaMetrics.      ### Environment variables    Each flag value can be set via environment variables according to these rules:    * The `-envflag.enable` flag must be set.  * Each `.` char in flag name must be substituted with `_` (for example `-insert.maxQueueDuration <duration>` will translate to `insert_maxQueueDuration=<duration>`).  * For repeating flags an alternative syntax can be used by joining the different values into one using `,` char as separator (for example `-storageNode <nodeA> -storageNode <nodeB>` will translate to `storageNode=<nodeA>,<nodeB>`).  * Environment var prefix can be set via `-envflag.prefix` flag. For instance, if `-envflag.prefix=VM_`, then env vars must be prepended with `VM_`.      ### Configuration with snap package      Snap package for VictoriaMetrics is available [here](https://snapcraft.io/victoriametrics).    Command-line flags for Snap package can be set with following command:    ```text  echo 'FLAGS=""-selfScrapeInterval=10s -search.logSlowQueryDuration=20s""' > $SNAP_DATA/var/snap/victoriametrics/current/extra_flags  snap restart victoriametrics  ```    Do not change value for `-storageDataPath` flag, because snap package has limited access to host filesystem.      Changing scrape configuration is possible with text editor:    ```text  vi $SNAP_DATA/var/snap/victoriametrics/current/etc/victoriametrics-scrape-config.yaml  ```    After changes were made, trigger config re-read with the command `curl 127.0.0.1:8248/-/reload`.      ## Prometheus setup    Add the following lines to Prometheus config file (it is usually located at `/etc/prometheus/prometheus.yml`) in order to send data to VictoriaMetrics:    ```yml  remote_write:    - url: http://<victoriametrics-addr>:8428/api/v1/write  ```    Substitute `<victoriametrics-addr>` with hostname or IP address of VictoriaMetrics.  Then apply new config via the following command:    ```bash  kill -HUP `pidof prometheus`  ```    Prometheus writes incoming data to local storage and replicates it to remote storage in parallel.  This means that data remains available in local storage for `--storage.tsdb.retention.time` duration  even if remote storage is unavailable.    If you plan sending data to VictoriaMetrics from multiple Prometheus instances, then add the following lines into `global` section  of [Prometheus config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file):    ```yml  global:    external_labels:      datacenter: dc-123  ```    This instructs Prometheus to add `datacenter=dc-123` label to each sample before sending it to remote storage.  The label name can be arbitrary - `datacenter` is just an example. The label value must be unique  across Prometheus instances, so time series could be filtered and grouped by this label.    For highly loaded Prometheus instances (200k+ samples per second) the following tuning may be applied:    ```yaml  remote_write:    - url: http://<victoriametrics-addr>:8428/api/v1/write      queue_config:        max_samples_per_send: 10000        capacity: 20000        max_shards: 30  ```    Using remote write increases memory usage for Prometheus by up to ~25%. If you are experiencing issues with  too high memory consumption of Prometheus, then try to lower `max_samples_per_send` and `capacity` params. Keep in mind that these two params are tightly connected.  Read more about tuning remote write for Prometheus [here](https://prometheus.io/docs/practices/remote_write).    It is recommended upgrading Prometheus to [v2.12.0](https://github.com/prometheus/prometheus/releases) or newer, since previous versions may have issues with `remote_write`.    Take a look also at [vmagent](https://docs.victoriametrics.com/vmagent.html) and [vmalert](https://docs.victoriametrics.com/vmalert.html),  which can be used as faster and less resource-hungry alternative to Prometheus.      ## Grafana setup    Create [Prometheus datasource](http://docs.grafana.org/features/datasources/prometheus/) in Grafana with the following url:    ```url  http://<victoriametrics-addr>:8428  ```    Substitute `<victoriametrics-addr>` with the hostname or IP address of VictoriaMetrics.    Then build graphs and dashboards for the created datasource using [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/) or [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html).      ## How to upgrade VictoriaMetrics    It is safe upgrading VictoriaMetrics to new versions unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise. It is safe skipping multiple versions during the upgrade unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise. It is recommended performing regular upgrades to the latest version, since it may contain important bug fixes, performance optimizations or new features.    It is also safe downgrading to older versions unless [release notes](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) say otherwise.    The following steps must be performed during the upgrade / downgrade procedure:    * Send `SIGINT` signal to VictoriaMetrics process in order to gracefully stop it.  * Wait until the process stops. This can take a few seconds.  * Start the upgraded VictoriaMetrics.    Prometheus doesn't drop data during VictoriaMetrics restart. See [this article](https://grafana.com/blog/2019/03/25/whats-new-in-prometheus-2.8-wal-based-remote-write/) for details. The same applies also to [vmagent](https://docs.victoriametrics.com/vmagent.html).      ## How to apply new config to VictoriaMetrics    VictoriaMetrics is configured via command-line flags, so it must be restarted when new command-line flags should be applied:    * Send `SIGINT` signal to VictoriaMetrics process in order to gracefully stop it.  * Wait until the process stops. This can take a few seconds.  * Start VictoriaMetrics with the new command-line flags.    Prometheus doesn't drop data during VictoriaMetrics restart. See [this article](https://grafana.com/blog/2019/03/25/whats-new-in-prometheus-2.8-wal-based-remote-write/) for details. The same applies alos to [vmagent](https://docs.victoriametrics.com/vmagent.html).      ## How to scrape Prometheus exporters such as [node-exporter](https://github.com/prometheus/node_exporter)    VictoriaMetrics can be used as drop-in replacement for Prometheus for scraping targets configured in `prometheus.yml` config file according to [the specification](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file). Just set `-promscrape.config` command-line flag to the path to `prometheus.yml` config - and VictoriaMetrics should start scraping the configured targets. Currently the following [scrape_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) types are supported:    * [static_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_config)  * [file_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config)  * [kubernetes_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config)  * [ec2_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config)  * [gce_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#gce_sd_config)  * [consul_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config)  * [dns_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config)  * [openstack_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#openstack_sd_config)  * [docker_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#docker_sd_config)  * [dockerswarm_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config)  * [eureka_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#eureka_sd_config)  * [digitalocean_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#digitalocean_sd_config)  * [http_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#http_sd_config)      File a [feature request](https://github.com/VictoriaMetrics/VictoriaMetrics/issues) if you need support for other `*_sd_config` types.    The file pointed by `-promscrape.config` may contain `%{ENV_VAR}` placeholders, which are substituted by the corresponding `ENV_VAR` environment variable values.    VictoriaMetrics also supports [importing data in Prometheus exposition format](#how-to-import-data-in-prometheus-exposition-format).    See also [vmagent](https://docs.victoriametrics.com/vmagent.html), which can be used as drop-in replacement for Prometheus.      ## How to send data from DataDog agent    VictoriaMetrics accepts data from [DataDog agent](https://docs.datadoghq.com/agent/) or [DogStatsD]() via [""submit metrics"" API](https://docs.datadoghq.com/api/latest/metrics/#submit-metrics) at `/datadog/api/v1/series` path.    Run DataDog agent with `DD_DD_URL=http://victoriametrics-host:8428/datadog` environment variable in order to write data to VictoriaMetrics at `victoriametrics-host` host. Another option is to set `dd_url` param at [DataDog agent configuration file](https://docs.datadoghq.com/agent/guide/agent-configuration-files/) to `http://victoriametrics-host:8428/datadog`.    VictoriaMetrics doesn't check `DD_API_KEY` param, so it can be set to arbitrary value.    Example on how to send data to VictoriaMetrics via DataDog ""submit metrics"" API from command line:    ```bash  echo '  {    ""series"": [      {        ""host"": ""test.example.com"",        ""interval"": 20,        ""metric"": ""system.load.1"",        ""points"": [[          0,          0.5        ]],        ""tags"": [          ""environment:test""        ],        ""type"": ""rate""      }    ]  }  ' | curl -X POST --data-binary @- http://localhost:8428/datadog/api/v1/series  ```    The imported data can be read via [export API](https://docs.victoriametrics.com/#how-to-export-data-in-json-line-format):    ```bash  curl http://localhost:8428/api/v1/export -d 'match[]=system.load.1'  ```    This command should return the following output if everything is OK:    ```  {""metric"":{""__name__"":""system.load.1"",""environment"":""test"",""host"":""test.example.com""},""values"":[0.5],""timestamps"":[1632833641000]}  ```    Extra labels may be added to all the written time series by passing `extra_label=name=value` query args.  For example, `/datadog/api/v1/series?extra_label=foo=bar` would add `{foo=""bar""}` label to all the ingested metrics.      ## How to send data from InfluxDB-compatible agents such as [Telegraf](https://www.influxdata.com/time-series-platform/telegraf/)    Use `http://<victoriametric-addr>:8428` url instead of InfluxDB url in agents' configs.  For instance, put the following lines into `Telegraf` config, so it sends data to VictoriaMetrics instead of InfluxDB:    ```toml  [[outputs.influxdb]]    urls = [""http://<victoriametrics-addr>:8428""]  ```    Another option is to enable TCP and UDP receiver for InfluxDB line protocol via `-influxListenAddr` command-line flag  and stream plain InfluxDB line protocol data to the configured TCP and/or UDP addresses.    VictoriaMetrics performs the following transformations to the ingested InfluxDB data:    * [`db` query arg](https://docs.influxdata.com/influxdb/v1.7/tools/api/#write-http-endpoint) is mapped into `db` label value    unless `db` tag exists in the InfluxDB line. The `db` label name can be overriden via `-influxDBLabel` command-line flag.  * Field names are mapped to time series names prefixed with `{measurement}{separator}` value, where `{separator}` equals to `_` by default. It can be changed with `-influxMeasurementFieldSeparator` command-line flag. See also `-influxSkipSingleField` command-line flag. If `{measurement}` is empty or if `-influxSkipMeasurement` command-line flag is set, then time series names correspond to field names.  * Field values are mapped to time series values.  * Tags are mapped to Prometheus labels as-is.    For example, the following InfluxDB line:    ```raw  foo,tag1=value1,tag2=value2 field1=12,field2=40  ```    is converted into the following Prometheus data points:    ```raw  foo_field1{tag1=""value1"", tag2=""value2""} 12  foo_field2{tag1=""value1"", tag2=""value2""} 40  ```    Example for writing data with [InfluxDB line protocol](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/)  to local VictoriaMetrics using `curl`:    ```bash  curl -d 'measurement,tag1=value1,tag2=value2 field1=123,field2=1.23' -X POST 'http://localhost:8428/write'  ```    An arbitrary number of lines delimited by '\n' (aka newline char) can be sent in a single request.  After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match={__name__=~""measurement_.*""}'  ```    The `/api/v1/export` endpoint should return the following response:    ```jsonl  {""metric"":{""__name__"":""measurement_field1"",""tag1"":""value1"",""tag2"":""value2""},""values"":[123],""timestamps"":[1560272508147]}  {""metric"":{""__name__"":""measurement_field2"",""tag1"":""value1"",""tag2"":""value2""},""values"":[1.23],""timestamps"":[1560272508147]}  ```    Note that InfluxDB line protocol expects [timestamps in *nanoseconds* by default](https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/#timestamp),  while VictoriaMetrics stores them with *milliseconds* precision.    Extra labels may be added to all the written time series by passing `extra_label=name=value` query args.  For example, `/write?extra_label=foo=bar` would add `{foo=""bar""}` label to all the ingested metrics.    Some plugins for Telegraf such as [fluentd](https://github.com/fangli/fluent-plugin-influxdb), [Juniper/open-nti](https://github.com/Juniper/open-nti)  or [Juniper/jitmon](https://github.com/Juniper/jtimon) send `SHOW DATABASES` query to `/query` and expect a particular database name in the response.  Comma-separated list of expected databases can be passed to VictoriaMetrics via `-influx.databaseNames` command-line flag.    ## How to send data from Graphite-compatible agents such as [StatsD](https://github.com/etsy/statsd)    Enable Graphite receiver in VictoriaMetrics by setting `-graphiteListenAddr` command line flag. For instance,  the following command will enable Graphite receiver in VictoriaMetrics on TCP and UDP port `2003`:    ```bash  /path/to/victoria-metrics-prod -graphiteListenAddr=:2003  ```    Use the configured address in Graphite-compatible agents. For instance, set `graphiteHost`  to the VictoriaMetrics host in `StatsD` configs.    Example for writing data with Graphite plaintext protocol to local VictoriaMetrics using `nc`:    ```bash  echo ""foo.bar.baz;tag1=value1;tag2=value2 123 `date +%s`"" | nc -N localhost 2003  ```    VictoriaMetrics sets the current time if the timestamp is omitted.  An arbitrary number of lines delimited by `\n` (aka newline char) can be sent in one go.  After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match=foo.bar.baz'  ```    The `/api/v1/export` endpoint should return the following response:    ```bash  {""metric"":{""__name__"":""foo.bar.baz"",""tag1"":""value1"",""tag2"":""value2""},""values"":[123],""timestamps"":[1560277406000]}  ```    ## Querying Graphite data    Data sent to VictoriaMetrics via `Graphite plaintext protocol` may be read via the following APIs:    * [Graphite API](#graphite-api-usage)  * [Prometheus querying API](#prometheus-querying-api-usage). See also [selecting Graphite metrics](#selecting-graphite-metrics).  * [go-graphite/carbonapi](https://github.com/go-graphite/carbonapi/blob/main/cmd/carbonapi/carbonapi.example.victoriametrics.yaml)    ## Selecting Graphite metrics    VictoriaMetrics supports `__graphite__` pseudo-label for selecting time series with Graphite-compatible filters in [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html). For example, `{__graphite__=""foo.*.bar""}` is equivalent to `{__name__=~""foo[.][^.]*[.]bar""}`, but it works faster and it is easier to use when migrating from Graphite to VictoriaMetrics. See [docs for Graphite paths and wildcards](https://graphite.readthedocs.io/en/latest/render_api.html#paths-and-wildcards). VictoriaMetrics also supports [label_graphite_group](https://docs.victoriametrics.com/MetricsQL.html#label_graphite_group) function for extracting the given groups from Graphite metric name.    The `__graphite__` pseudo-label supports e.g. alternate regexp filters such as `(value1|...|valueN)`. They are transparently converted to `{value1,...,valueN}` syntax [used in Graphite](https://graphite.readthedocs.io/en/latest/render_api.html#paths-and-wildcards). This allows using [multi-value template variables in Grafana](https://grafana.com/docs/grafana/latest/variables/formatting-multi-value-variables/) inside `__graphite__` pseudo-label. For example, Grafana expands `{__graphite__=~""foo.($bar).baz""}` into `{__graphite__=~""foo.(x|y).baz""}` if `$bar` template variable contains `x` and `y` values. In this case the query is automatically converted into `{__graphite__=~""foo.{x,y}.baz""}` before execution.    ## How to send data from OpenTSDB-compatible agents    VictoriaMetrics supports [telnet put protocol](http://opentsdb.net/docs/build/html/api_telnet/put.html)  and [HTTP /api/put requests](http://opentsdb.net/docs/build/html/api_http/put.html) for ingesting OpenTSDB data.  The same protocol is used for [ingesting data in KairosDB](https://kairosdb.github.io/docs/PushingData.html).    ### Sending data via `telnet put` protocol    Enable OpenTSDB receiver in VictoriaMetrics by setting `-opentsdbListenAddr` command line flag. For instance,  the following command enables OpenTSDB receiver in VictoriaMetrics on TCP and UDP port `4242`:    ```bash  /path/to/victoria-metrics-prod -opentsdbListenAddr=:4242  ```    Send data to the given address from OpenTSDB-compatible agents.    Example for writing data with OpenTSDB protocol to local VictoriaMetrics using `nc`:    ```bash  echo ""put foo.bar.baz `date +%s` 123 tag1=value1 tag2=value2"" | nc -N localhost 4242  ```    An arbitrary number of lines delimited by `\n` (aka newline char) can be sent in one go.  After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match=foo.bar.baz'  ```    The `/api/v1/export` endpoint should return the following response:    ```bash  {""metric"":{""__name__"":""foo.bar.baz"",""tag1"":""value1"",""tag2"":""value2""},""values"":[123],""timestamps"":[1560277292000]}  ```    ### Sending OpenTSDB data via HTTP `/api/put` requests    Enable HTTP server for OpenTSDB `/api/put` requests by setting `-opentsdbHTTPListenAddr` command line flag. For instance,  the following command enables OpenTSDB HTTP server on port `4242`:    ```bash  /path/to/victoria-metrics-prod -opentsdbHTTPListenAddr=:4242  ```    Send data to the given address from OpenTSDB-compatible agents.    Example for writing a single data point:    ```bash  curl -H 'Content-Type: application/json' -d '{""metric"":""x.y.z"",""value"":45.34,""tags"":{""t1"":""v1"",""t2"":""v2""}}' http://localhost:4242/api/put  ```    Example for writing multiple data points in a single request:    ```bash  curl -H 'Content-Type: application/json' -d '[{""metric"":""foo"",""value"":45.34},{""metric"":""bar"",""value"":43}]' http://localhost:4242/api/put  ```    After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match[]=x.y.z' -d 'match[]=foo' -d 'match[]=bar'  ```    The `/api/v1/export` endpoint should return the following response:    ```bash  {""metric"":{""__name__"":""foo""},""values"":[45.34],""timestamps"":[1566464846000]}  {""metric"":{""__name__"":""bar""},""values"":[43],""timestamps"":[1566464846000]}  {""metric"":{""__name__"":""x.y.z"",""t1"":""v1"",""t2"":""v2""},""values"":[45.34],""timestamps"":[1566464763000]}  ```    Extra labels may be added to all the imported time series by passing `extra_label=name=value` query args.  For example, `/api/put?extra_label=foo=bar` would add `{foo=""bar""}` label to all the ingested metrics.      ## Prometheus querying API usage    VictoriaMetrics supports the following handlers from [Prometheus querying API](https://prometheus.io/docs/prometheus/latest/querying/api/):    * [/api/v1/query](https://prometheus.io/docs/prometheus/latest/querying/api/#instant-queries)  * [/api/v1/query_range](https://prometheus.io/docs/prometheus/latest/querying/api/#range-queries)  * [/api/v1/series](https://prometheus.io/docs/prometheus/latest/querying/api/#finding-series-by-label-matchers)  * [/api/v1/labels](https://prometheus.io/docs/prometheus/latest/querying/api/#getting-label-names)  * [/api/v1/label/.../values](https://prometheus.io/docs/prometheus/latest/querying/api/#querying-label-values)  * [/api/v1/status/tsdb](https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-stats). See [these docs](#tsdb-stats) for details.  * [/api/v1/targets](https://prometheus.io/docs/prometheus/latest/querying/api/#targets) - see [these docs](#how-to-scrape-prometheus-exporters-such-as-node-exporter) for more details.    These handlers can be queried from Prometheus-compatible clients such as Grafana or curl.  All the Prometheus querying API handlers can be prepended with `/prometheus` prefix. For example, both `/prometheus/api/v1/query` and `/api/v1/query` should work.      ### Prometheus querying API enhancements    VictoriaMetrics accepts optional `extra_label=<label_name>=<label_value>` query arg, which can be used for enforcing additional label filters for queries. For example,  `/api/v1/query_range?extra_label=user_id=123&extra_label=group_id=456&query=<query>` would automatically add `{user_id=""123"",group_id=""456""}` label filters to the given `<query>`. This functionality can be used for limiting the scope of time series visible to the given tenant. It is expected that the `extra_label` query args are automatically set by auth proxy sitting in front of VictoriaMetrics. See [vmauth](https://docs.victoriametrics.com/vmauth.html) and [vmgateway](https://docs.victoriametrics.com/vmgateway.html) as examples of such proxies.    VictoriaMetrics accepts optional `extra_filters[]=series_selector` query arg, which can be used for enforcing arbitrary label filters for queries. For example,  `/api/v1/query_range?extra_filters[]={env=~""prod|staging"",user=""xyz""}&query=<query>` would automatically add `{env=~""prod|staging"",user=""xyz""}` label filters to the given `<query>`. This functionality can be used for limiting the scope of time series visible to the given tenant. It is expected that the `extra_filters[]` query args are automatically set by auth proxy sitting in front of VictoriaMetrics. See [vmauth](https://docs.victoriametrics.com/vmauth.html) and [vmgateway](https://docs.victoriametrics.com/vmgateway.html) as examples of such proxies.    VictoriaMetrics accepts relative times in `time`, `start` and `end` query args additionally to unix timestamps and [RFC3339](https://www.ietf.org/rfc/rfc3339.txt).  For example, the following query would return data for the last 30 minutes: `/api/v1/query_range?start=-30m&query=...`.    VictoriaMetrics accepts `round_digits` query arg for `/api/v1/query` and `/api/v1/query_range` handlers. It can be used for rounding response values to the given number of digits after the decimal point. For example, `/api/v1/query?query=avg_over_time(temperature[1h])&round_digits=2` would round response values to up to two digits after the decimal point.    By default, VictoriaMetrics returns time series for the last 5 minutes from `/api/v1/series`, while the Prometheus API defaults to all time.  Use `start` and `end` to select a different time range.    Additionally VictoriaMetrics provides the following handlers:    * `/vmui` - Basic Web UI. See [these docs](#vmui).  * `/api/v1/series/count` - returns the total number of time series in the database. Some notes:    * the handler scans all the inverted index, so it can be slow if the database contains tens of millions of time series;    * the handler may count [deleted time series](#how-to-delete-time-series) additionally to normal time series due to internal implementation restrictions;  * `/api/v1/labels/count` - returns a list of `label: values_count` entries. It can be used for determining labels with the maximum number of values.  * `/api/v1/status/active_queries` - returns a list of currently running queries.  * `/api/v1/status/top_queries` - returns the following query lists:    * the most frequently executed queries - `topByCount`    * queries with the biggest average execution duration - `topByAvgDuration`    * queries that took the most time for execution - `topBySumDuration`      The number of returned queries can be limited via `topN` query arg. Old queries can be filtered out with `maxLifetime` query arg.    For example, request to `/api/v1/status/top_queries?topN=5&maxLifetime=30s` would return up to 5 queries per list, which were executed during the last 30 seconds.    VictoriaMetrics tracks the last `-search.queryStats.lastQueriesCount` queries with durations at least `-search.queryStats.minQueryDuration`.      ## Graphite API usage    VictoriaMetrics supports the following Graphite APIs, which are needed for [Graphite datasource in Grafana](https://grafana.com/docs/grafana/latest/datasources/graphite/):    * Render API - see [these docs](#graphite-render-api-usage).  * Metrics API - see [these docs](#graphite-metrics-api-usage).  * Tags API - see [these docs](#graphite-tags-api-usage).    All the Graphite handlers can be pre-pended with `/graphite` prefix. For example, both `/graphite/metrics/find` and `/metrics/find` should work.    VictoriaMetrics accepts optional query args: `extra_label=<label_name>=<label_value>` and `extra_filters[]=series_selector` query args for all the Graphite APIs. These args can be used for limiting the scope of time series visible to the given tenant. It is expected that the `extra_label` query arg is automatically set by auth proxy sitting in front of VictoriaMetrics. See [vmauth](https://docs.victoriametrics.com/vmauth.html) and [vmgateway](https://docs.victoriametrics.com/vmgateway.html) as examples of such proxies.    [Contact us](mailto:sales@victoriametrics.com) if you need assistance with such a proxy.    VictoriaMetrics supports `__graphite__` pseudo-label for filtering time series with Graphite-compatible filters in [MetricsQL](https://docs.victoriametrics.com/MetricsQL.html). See [these docs](#selecting-graphite-metrics).      ### Graphite Render API usage    [VictoriaMetrics Enterprise](https://victoriametrics.com/products/enterprise/) supports [Graphite Render API](https://graphite.readthedocs.io/en/stable/render_api.html) subset  at `/render` endpoint, which is used by [Graphite datasource in Grafana](https://grafana.com/docs/grafana/latest/datasources/graphite/).  When configuring Graphite datasource in Grafana, the `Storage-Step` http request header must be set to a step between Graphite data points stored in VictoriaMetrics. For example, `Storage-Step: 10s` would mean 10 seconds distance between Graphite datapoints stored in VictoriaMetrics.  Enterprise binaries can be downloaded and evaluated for free from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).      ### Graphite Metrics API usage    VictoriaMetrics supports the following handlers from [Graphite Metrics API](https://graphite-api.readthedocs.io/en/latest/api.html#the-metrics-api):    * [/metrics/find](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-find)  * [/metrics/expand](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-expand)  * [/metrics/index.json](https://graphite-api.readthedocs.io/en/latest/api.html#metrics-index-json)    VictoriaMetrics accepts the following additional query args at `/metrics/find` and `/metrics/expand`:    * `label` - for selecting arbitrary label values. By default `label=__name__`, i.e. metric names are selected.    * `delimiter` - for using different delimiters in metric name hierachy. For example, `/metrics/find?delimiter=_&query=node_*` would return all the metric name prefixes      that start with `node_`. By default `delimiter=.`.      ### Graphite Tags API usage    VictoriaMetrics supports the following handlers from [Graphite Tags API](https://graphite.readthedocs.io/en/stable/tags.html):    * [/tags/tagSeries](https://graphite.readthedocs.io/en/stable/tags.html#adding-series-to-the-tagdb)  * [/tags/tagMultiSeries](https://graphite.readthedocs.io/en/stable/tags.html#adding-series-to-the-tagdb)  * [/tags](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)  * [/tags/{tag_name}](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)  * [/tags/findSeries](https://graphite.readthedocs.io/en/stable/tags.html#exploring-tags)  * [/tags/autoComplete/tags](https://graphite.readthedocs.io/en/stable/tags.html#auto-complete-support)  * [/tags/autoComplete/values](https://graphite.readthedocs.io/en/stable/tags.html#auto-complete-support)  * [/tags/delSeries](https://graphite.readthedocs.io/en/stable/tags.html#removing-series-from-the-tagdb)      ## vmui    VictoriaMetrics provides UI for query troubleshooting and exploration. The UI is available at `http://victoriametrics:8428/vmui`.  The UI allows exploring query results via graphs and tables. Graphs support scrolling and zooming:    * Drag the graph to the left / right in order to move the displayed time range into the past / future.  * Hold `Ctrl` (or `Cmd` on MacOS) and scroll up / down in order to zoom in / out the graph.    Query history can be navigated by holding `Ctrl` (or `Cmd` on MacOS) and pressing `up` or `down` arrows on the keyboard while the cursor is located in the query input field.    When querying the [backfilled data](https://docs.victoriametrics.com/#backfilling), it may be useful disabling response cache by clicking `Enable cache` checkbox.    VMUI automatically adjusts the interval between datapoints on the graph depending on the horizontal resolution and on the selected time range. The step value can be customized by clickhing `Override step value` checkbox.    VMUI allows investigating correlations between two queries on the same graph. Just click `+Query` button, enter the second query in the newly appeared input field and press `Ctrl+Enter`. Results for both queries should be displayed simultaneously on the same graph. Every query has its own vertical scale, which is displayed on the left and the right side of the graph. Lines for the second query are dashed.    See the [example VMUI at VictoriaMetrics playground](https://play.victoriametrics.com/select/accounting/1/6a716b0f-38bc-4856-90ce-448fd713e3fe/prometheus/graph/?g0.expr=100%20*%20sum(rate(process_cpu_seconds_total))%20by%20(job)&g0.range_input=1d).      ## How to build from sources    We recommend using either [binary releases](https://github.com/VictoriaMetrics/VictoriaMetrics/releases) or  [docker images](https://hub.docker.com/r/victoriametrics/victoria-metrics/) instead of building VictoriaMetrics  from sources. Building from sources is reasonable when developing additional features specific  to your needs or when testing bugfixes.    ### Development build    1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.17.  2. Run `make victoria-metrics` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics` binary and puts it into the `bin` folder.    ### Production build    1. [Install docker](https://docs.docker.com/install/).  2. Run `make victoria-metrics-prod` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-prod` binary and puts it into the `bin` folder.    ### ARM build    ARM build may run on Raspberry Pi or on [energy-efficient ARM servers](https://blog.cloudflare.com/arm-takes-wing/).    ### Development ARM build    1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.17.  2. Run `make victoria-metrics-arm` or `make victoria-metrics-arm64` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-arm` or `victoria-metrics-arm64` binary respectively and puts it into the `bin` folder.    ### Production ARM build    1. [Install docker](https://docs.docker.com/install/).  2. Run `make victoria-metrics-arm-prod` or `make victoria-metrics-arm64-prod` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-arm-prod` or `victoria-metrics-arm64-prod` binary respectively and puts it into the `bin` folder.    ### Pure Go build (CGO_ENABLED=0)    `Pure Go` mode builds only Go code without [cgo](https://golang.org/cmd/cgo/) dependencies.    1. [Install Go](https://golang.org/doc/install). The minimum supported version is Go 1.17.  2. Run `make victoria-metrics-pure` from the root folder of [the repository](https://github.com/VictoriaMetrics/VictoriaMetrics).     It builds `victoria-metrics-pure` binary and puts it into the `bin` folder.    ### Building docker images    Run `make package-victoria-metrics`. It builds `victoriametrics/victoria-metrics:<PKG_TAG>` docker image locally.  `<PKG_TAG>` is auto-generated image tag, which depends on source code in the repository.  The `<PKG_TAG>` may be manually set via `PKG_TAG=foobar make package-victoria-metrics`.    The base docker image is [alpine](https://hub.docker.com/_/alpine) but it is possible to use any other base image  by setting it via `<ROOT_IMAGE>` environment variable.  For example, the following command builds the image on top of [scratch](https://hub.docker.com/_/scratch) image:    ```bash  ROOT_IMAGE=scratch make package-victoria-metrics  ```    ## Start with docker-compose    [Docker-compose](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/docker-compose.yml)  helps to spin up VictoriaMetrics, [vmagent](https://docs.victoriametrics.com/vmagent.html) and Grafana with one command.  More details may be found [here](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/master/deployment/docker#folder-contains-basic-images-and-tools-for-building-and-running-victoria-metrics-in-docker).      ## Setting up service    Read [these instructions](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/43) on how to set up VictoriaMetrics as a service in your OS.  There is also [snap package for Ubuntu](https://snapcraft.io/victoriametrics).      ## How to work with snapshots    VictoriaMetrics can create [instant snapshots](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)  for all the data stored under `-storageDataPath` directory.  Navigate to `http://<victoriametrics-addr>:8428/snapshot/create` in order to create an instant snapshot.  The page will return the following JSON response:    ```json  {""status"":""ok"",""snapshot"":""<snapshot-name>""}  ```    Snapshots are created under `<-storageDataPath>/snapshots` directory, where `<-storageDataPath>`  is the command-line flag value. Snapshots can be archived to backup storage at any time  with [vmbackup](https://docs.victoriametrics.com/vmbackup.html).    The `http://<victoriametrics-addr>:8428/snapshot/list` page contains the list of available snapshots.    Navigate to `http://<victoriametrics-addr>:8428/snapshot/delete?snapshot=<snapshot-name>` in order  to delete `<snapshot-name>` snapshot.    Navigate to `http://<victoriametrics-addr>:8428/snapshot/delete_all` in order to delete all the snapshots.    Steps for restoring from a snapshot:    1. Stop VictoriaMetrics with `kill -INT`.  2. Restore snapshot contents from backup with [vmrestore](https://docs.victoriametrics.com/vmrestore.html)     to the directory pointed by `-storageDataPath`.  3. Start VictoriaMetrics.    ## How to delete time series    Send a request to `http://<victoriametrics-addr>:8428/api/v1/admin/tsdb/delete_series?match[]=<timeseries_selector_for_delete>`,  where `<timeseries_selector_for_delete>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to delete. After that all the time series matching the given selector are deleted. Storage space for  the deleted time series isn't freed instantly - it is freed during subsequent [background merges of data files](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).  Note that background merges may never occur for data from previous months, so storage space won't be freed for historical data.  In this case [forced merge](#forced-merge) may help freeing up storage space.    It is recommended verifying which metrics will be deleted with the call to `http://<victoria-metrics-addr>:8428/api/v1/series?match[]=<timeseries_selector_for_delete>`  before actually deleting the metrics.  By default this query will only scan series in the past 5 minutes, so you may need to  adjust `start` and `end` to a suitable range to achieve match hits.    The `/api/v1/admin/tsdb/delete_series` handler may be protected with `authKey` if `-deleteAuthKey` command-line flag is set.    The delete API is intended mainly for the following cases:    * One-off deleting of accidentally written invalid (or undesired) time series.  * One-off deleting of user data due to [GDPR](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation).    Using the delete API is not recommended in the following cases, since it brings a non-zero overhead:    * Regular cleanups for unneeded data. Just prevent writing unneeded data into VictoriaMetrics.    This can be done with [relabeling](#relabeling).    See [this article](https://www.robustperception.io/relabelling-can-discard-targets-timeseries-and-alerts) for details.  * Reducing disk space usage by deleting unneeded time series. This doesn't work as expected, since the deleted    time series occupy disk space until the next merge operation, which can never occur when deleting too old data.    [Forced merge](#forced-merge) may be used for freeing up disk space occupied by old data.    It's better to use the `-retentionPeriod` command-line flag for efficient pruning of old data.      ## Forced merge    VictoriaMetrics performs [data compactions in background](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)  in order to keep good performance characteristics when accepting new data. These compactions (merges) are performed independently on per-month partitions.  This means that compactions are stopped for per-month partitions if no new data is ingested into these partitions.  Sometimes it is necessary to trigger compactions for old partitions. For instance, in order to free up disk space occupied by [deleted time series](#how-to-delete-time-series).  In this case forced compaction may be initiated on the specified per-month partition by sending request to `/internal/force_merge?partition_prefix=YYYY_MM`,  where `YYYY_MM` is per-month partition name. For example, `http://victoriametrics:8428/internal/force_merge?partition_prefix=2020_08` would initiate forced  merge for August 2020 partition. The call to `/internal/force_merge` returns immediately, while the corresponding forced merge continues running in background.    Forced merges may require additional CPU, disk IO and storage space resources. It is unnecessary to run forced merge under normal conditions,  since VictoriaMetrics automatically performs [optimal merges in background](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282)  when new data is ingested into it.      ## How to export time series    VictoriaMetrics provides the following handlers for exporting data:    * `/api/v1/export` for exporing data in JSON line format. See [these docs](#how-to-export-data-in-json-line-format) for details.  * `/api/v1/export/csv` for exporting data in CSV. See [these docs](#how-to-export-csv-data) for details.  * `/api/v1/export/native` for exporting data in native binary format. This is the most efficient format for data export.    See [these docs](#how-to-export-data-in-native-format) for details.      ### How to export data in JSON line format    Send a request to `http://<victoriametrics-addr>:8428/api/v1/export?match[]=<timeseries_selector_for_export>`,  where `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to export. Use `{__name__!=""""}` selector for fetching all the time series.  The response would contain all the data for the selected time series in [JSON streaming format](https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON).  Each JSON line contains samples for a single time series. An example output:    ```jsonl  {""metric"":{""__name__"":""up"",""job"":""node_exporter"",""instance"":""localhost:9100""},""values"":[0,0,0],""timestamps"":[1549891472010,1549891487724,1549891503438]}  {""metric"":{""__name__"":""up"",""job"":""prometheus"",""instance"":""localhost:9090""},""values"":[1,1,1],""timestamps"":[1549891461511,1549891476511,1549891491511]}  ```    Optional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either  unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.    Optional `max_rows_per_line` arg may be added to the request for limiting the maximum number of rows exported per each JSON line.  Optional `reduce_mem_usage=1` arg may be added to the request for reducing memory usage when exporting big number of time series.  In this case the output may contain multiple lines with samples for the same time series.    Pass `Accept-Encoding: gzip` HTTP header in the request to `/api/v1/export` in order to reduce network bandwidth during exporing big amounts  of time series data. This enables gzip compression for the exported data. Example for exporting gzipped data:    ```bash  curl -H 'Accept-Encoding: gzip' http://localhost:8428/api/v1/export -d 'match[]={__name__!=""""}' > data.jsonl.gz  ```    The maximum duration for each request to `/api/v1/export` is limited by `-search.maxExportDuration` command-line flag.    Exported data can be imported via POST'ing it to [/api/v1/import](#how-to-import-data-in-json-line-format).    The [deduplication](#deduplication) is applied to the data exported via `/api/v1/export` by default. The deduplication  isn't applied if `reduce_mem_usage=1` query arg is passed to the request.      ### How to export CSV data    Send a request to `http://<victoriametrics-addr>:8428/api/v1/export/csv?format=<format>&match=<timeseries_selector_for_export>`,  where:    * `<format>` must contain comma-delimited label names for the exported CSV. The following special label names are supported:    * `__name__` - metric name    * `__value__` - sample value    * `__timestamp__:<ts_format>` - sample timestamp. `<ts_format>` can have the following values:      * `unix_s` - unix seconds      * `unix_ms` - unix milliseconds      * `unix_ns` - unix nanoseconds      * `rfc3339` - [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) time      * `custom:<layout>` - custom layout for time that is supported by [time.Format](https://golang.org/pkg/time/#Time.Format) function from Go.    * `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to export.    Optional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either  unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.    The exported CSV data can be imported to VictoriaMetrics via [/api/v1/import/csv](#how-to-import-csv-data).    The [deduplication](#deduplication) is applied for the data exported in CSV by default. It is possible to export raw data without de-duplication by passing `reduce_mem_usage=1` query arg to `/api/v1/export/csv`.      ### How to export data in native format    Send a request to `http://<victoriametrics-addr>:8428/api/v1/export/native?match[]=<timeseries_selector_for_export>`,  where `<timeseries_selector_for_export>` may contain any [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors)  for metrics to export. Use `{__name__=~"".*""}` selector for fetching all the time series.    On large databases you may experience problems with limit on unique timeseries (default value is 300000). In this case you need to adjust `-search.maxUniqueTimeseries` parameter:    ```bash  # count unique timeseries in database  wget -O- -q 'http://your_victoriametrics_instance:8428/api/v1/series/count' | jq '.data[0]'    # relaunch victoriametrics with search.maxUniqueTimeseries more than value from previous command  ```    Optional `start` and `end` args may be added to the request in order to limit the time frame for the exported data. These args may contain either  unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values.    The exported data can be imported to VictoriaMetrics via [/api/v1/import/native](#how-to-import-data-in-native-format).  The native export format may change in incompatible way between VictoriaMetrics releases, so the data exported from the release X  can fail to be imported into VictoriaMetrics release Y.    The [deduplication](#deduplication) isn't applied for the data exported in native format. It is expected that the de-duplication is performed during data import.      ## How to import time series data    Time series data can be imported into VictoriaMetrics via any supported ingestion protocol:    * [Prometheus remote_write API](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write). See [these docs](#prometheus-setup) for details.  * DataDog `submit metrics` API. See [these docs](#how-to-send-data-from-datadog-agent) for details.  * InfluxDB line protocol. See [these docs](#how-to-send-data-from-influxdb-compatible-agents-such-as-telegraf) for details.  * Graphite plaintext protocol. See [these docs](#how-to-send-data-from-graphite-compatible-agents-such-as-statsd) for details.  * OpenTSDB telnet put protocol. See [these docs](#sending-data-via-telnet-put-protocol) for details.  * OpenTSDB http `/api/put` protocol. See [these docs](#sending-opentsdb-data-via-http-apiput-requests) for details.  * `/api/v1/import` for importing data obtained from [/api/v1/export](#how-to-export-data-in-json-line-format).    See [these docs](#how-to-import-data-in-json-line-format) for details.  * `/api/v1/import/native` for importing data obtained from [/api/v1/export/native](#how-to-export-data-in-native-format).    See [these docs](#how-to-import-data-in-native-format) for details.  * `/api/v1/import/csv` for importing arbitrary CSV data. See [these docs](#how-to-import-csv-data) for details.  * `/api/v1/import/prometheus` for importing data in Prometheus exposition format. See [these docs](#how-to-import-data-in-prometheus-exposition-format) for details.      ### How to import data in JSON line format    Example for importing data obtained via [/api/v1/export](#how-to-export-data-in-json-line-format):    ```bash  # Export the data from <source-victoriametrics>:  curl http://source-victoriametrics:8428/api/v1/export -d 'match={__name__!=""""}' > exported_data.jsonl    # Import the data to <destination-victoriametrics>:  curl -X POST http://destination-victoriametrics:8428/api/v1/import -T exported_data.jsonl  ```    Pass `Content-Encoding: gzip` HTTP request header to `/api/v1/import` for importing gzipped data:    ```bash  # Export gzipped data from <source-victoriametrics>:  curl -H 'Accept-Encoding: gzip' http://source-victoriametrics:8428/api/v1/export -d 'match={__name__!=""""}' > exported_data.jsonl.gz    # Import gzipped data to <destination-victoriametrics>:  curl -X POST -H 'Content-Encoding: gzip' http://destination-victoriametrics:8428/api/v1/import -T exported_data.jsonl.gz  ```    Extra labels may be added to all the imported time series by passing `extra_label=name=value` query args.  For example, `/api/v1/import?extra_label=foo=bar` would add `""foo"":""bar""` label to all the imported time series.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.    VictoriaMetrics parses input JSON lines one-by-one. It loads the whole JSON line in memory, then parses it and then saves the parsed samples into persistent storage. This means that VictoriaMetrics can occupy big amounts of RAM when importing too long JSON lines. The solution is to split too long JSON lines into smaller lines. It is OK if samples for a single time series are split among multiple JSON lines.      ### How to import data in native format    The specification of VictoriaMetrics' native format may yet change and is not formally documented yet. So currently we do not recommend that external clients attempt to pack their own metrics in native format file.    If you have a native format file obtained via [/api/v1/export/native](#how-to-export-data-in-native-format) however this is the most efficient protocol for importing data in.    ```bash  # Export the data from <source-victoriametrics>:  curl http://source-victoriametrics:8428/api/v1/export/native -d 'match={__name__!=""""}' > exported_data.bin    # Import the data to <destination-victoriametrics>:  curl -X POST http://destination-victoriametrics:8428/api/v1/import/native -T exported_data.bin  ```    Extra labels may be added to all the imported time series by passing `extra_label=name=value` query args.  For example, `/api/v1/import/native?extra_label=foo=bar` would add `""foo"":""bar""` label to all the imported time series.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.      ### How to import CSV data    Arbitrary CSV data can be imported via `/api/v1/import/csv`. The CSV data is imported according to the provided `format` query arg.  The `format` query arg must contain comma-separated list of parsing rules for CSV fields. Each rule consists of three parts delimited by a colon:    ```  <column_pos>:<type>:<context>  ```    * `<column_pos>` is the position of the CSV column (field). Column numbering starts from 1. The order of parsing rules may be arbitrary.  * `<type>` describes the column type. Supported types are:    * `metric` - the corresponding CSV column at `<column_pos>` contains metric value, which must be integer or floating-point number.      The metric name is read from the `<context>`. CSV line must have at least a single metric field. Multiple metric fields per CSV line is OK.    * `label` - the corresponding CSV column at `<column_pos>` contains label value. The label name is read from the `<context>`.      CSV line may have arbitrary number of label fields. All these labels are attached to all the configured metrics.    * `time` - the corresponding CSV column at `<column_pos>` contains metric time. CSV line may contain either one or zero columns with time.      If CSV line has no time, then the current time is used. The time is applied to all the configured metrics.      The format of the time is configured via `<context>`. Supported time formats are:      * `unix_s` - unix timestamp in seconds.      * `unix_ms` - unix timestamp in milliseconds.      * `unix_ns` - unix timestamp in nanoseconds. Note that VictoriaMetrics rounds the timestamp to milliseconds.      * `rfc3339` - timestamp in [RFC3339](https://tools.ietf.org/html/rfc3339) format, i.e. `2006-01-02T15:04:05Z`.      * `custom:<layout>` - custom layout for the timestamp. The `<layout>` may contain arbitrary time layout according to [time.Parse rules in Go](https://golang.org/pkg/time/#Parse).    Each request to `/api/v1/import/csv` may contain arbitrary number of CSV lines.    Example for importing CSV data via `/api/v1/import/csv`:    ```bash  curl -d ""GOOG,1.23,4.56,NYSE"" 'http://localhost:8428/api/v1/import/csv?format=2:metric:ask,3:metric:bid,1:label:ticker,4:label:market'  curl -d ""MSFT,3.21,1.67,NASDAQ"" 'http://localhost:8428/api/v1/import/csv?format=2:metric:ask,3:metric:bid,1:label:ticker,4:label:market'  ```    After that the data may be read via [/api/v1/export](#how-to-export-data-in-json-line-format) endpoint:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match[]={ticker!=""""}'  ```    The following response should be returned:  ```bash  {""metric"":{""__name__"":""bid"",""market"":""NASDAQ"",""ticker"":""MSFT""},""values"":[1.67],""timestamps"":[1583865146520]}  {""metric"":{""__name__"":""bid"",""market"":""NYSE"",""ticker"":""GOOG""},""values"":[4.56],""timestamps"":[1583865146495]}  {""metric"":{""__name__"":""ask"",""market"":""NASDAQ"",""ticker"":""MSFT""},""values"":[3.21],""timestamps"":[1583865146520]}  {""metric"":{""__name__"":""ask"",""market"":""NYSE"",""ticker"":""GOOG""},""values"":[1.23],""timestamps"":[1583865146495]}  ```    Extra labels may be added to all the imported lines by passing `extra_label=name=value` query args.  For example, `/api/v1/import/csv?extra_label=foo=bar` would add `""foo"":""bar""` label to all the imported lines.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.      ### How to import data in Prometheus exposition format    VictoriaMetrics accepts data in [Prometheus exposition format](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format)  and in [OpenMetrics format](https://github.com/OpenObservability/OpenMetrics/blob/master/specification/OpenMetrics.md)  via `/api/v1/import/prometheus` path. For example, the following line imports a single line in Prometheus exposition format into VictoriaMetrics:    ```bash  curl -d 'foo{bar=""baz""} 123' -X POST 'http://localhost:8428/api/v1/import/prometheus'  ```    The following command may be used for verifying the imported data:    ```bash  curl -G 'http://localhost:8428/api/v1/export' -d 'match={__name__=~""foo""}'  ```    It should return something like the following:    ```  {""metric"":{""__name__"":""foo"",""bar"":""baz""},""values"":[123],""timestamps"":[1594370496905]}  ```    Pass `Content-Encoding: gzip` HTTP request header to `/api/v1/import/prometheus` for importing gzipped data:    ```bash  # Import gzipped data to <destination-victoriametrics>:  curl -X POST -H 'Content-Encoding: gzip' http://destination-victoriametrics:8428/api/v1/import/prometheus -T prometheus_data.gz  ```    Extra labels may be added to all the imported metrics by passing `extra_label=name=value` query args.  For example, `/api/v1/import/prometheus?extra_label=foo=bar` would add `{foo=""bar""}` label to all the imported metrics.    If timestamp is missing in `<metric> <value> <timestamp>` Prometheus exposition format line, then the current timestamp is used during data ingestion.  It can be overriden by passing unix timestamp in *milliseconds* via `timestamp` query arg. For example, `/api/v1/import/prometheus?timestamp=1594370496905`.    VictoriaMetrics accepts arbitrary number of lines in a single request to `/api/v1/import/prometheus`, i.e. it supports data streaming.    Note that it could be required to flush response cache after importing historical data. See [these docs](#backfilling) for detail.    VictoriaMetrics also may scrape Prometheus targets - see [these docs](#how-to-scrape-prometheus-exporters-such-as-node-exporter).        ## Relabeling    VictoriaMetrics supports Prometheus-compatible relabeling for all the ingested metrics if `-relabelConfig` command-line flag points  to a file containing a list of [relabel_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config) entries.  The `-relabelConfig` also can point to http or https url. For example, `-relabelConfig=https://config-server/relabel_config.yml`.  See [this article with relabeling tips and tricks](https://valyala.medium.com/how-to-use-relabeling-in-prometheus-and-victoriametrics-8b90fc22c4b2).    Example contents for `-relabelConfig` file:  ```yml  # Add {cluster=""dev""} label.  - target_label: cluster    replacement: dev    # Drop the metric (or scrape target) with `{__meta_kubernetes_pod_container_init=""true""}` label.  - action: drop    source_labels: [__meta_kubernetes_pod_container_init]    regex: true  ```    See [these docs](https://docs.victoriametrics.com/vmagent.html#relabeling) for more details about relabeling in VictoriaMetrics.      ## Federation    VictoriaMetrics exports [Prometheus-compatible federation data](https://prometheus.io/docs/prometheus/latest/federation/)  at `http://<victoriametrics-addr>:8428/federate?match[]=<timeseries_selector_for_federation>`.    Optional `start` and `end` args may be added to the request in order to scrape the last point for each selected time series on the `[start ... end]` interval.  `start` and `end` may contain either unix timestamp in seconds or [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) values. By default, the last point  on the interval `[now - max_lookback ... now]` is scraped for each time series. The default value for `max_lookback` is `5m` (5 minutes), but it can be overridden.  For instance, `/federate?match[]=up&max_lookback=1h` would return last points on the `[now - 1h ... now]` interval. This may be useful for time series federation  with scrape intervals exceeding `5m`.      ## Capacity planning    VictoriaMetrics uses lower amounts of CPU, RAM and storage space on production workloads compared to competing solutions (Prometheus, Thanos, Cortex, TimescaleDB, InfluxDB, QuestDB, M3DB) according to [our case studies](https://docs.victoriametrics.com/CaseStudies.html).    VictoriaMetrics capacity scales linearly with the available resources. The needed amounts of CPU and RAM highly depends on the workload - the number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series), series [churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate), query types, query qps, etc. It is recommended setting up a test VictoriaMetrics for your production workload and iteratively scaling CPU and RAM resources until it becomes stable according to [troubleshooting docs](#troubleshooting). A single-node VictoriaMetrics works perfectly with the following production workload according to [our case studies](https://docs.victoriametrics.com/CaseStudies.html):    * Ingestion rate: 1.5+ million samples per second  * Active time series: 50+ million  * Total time series: 5+ billion  * Time series churn rate: 150+ million of new series per day  * Total number of samples: 10+ trillion  * Queries: 200+ qps  * Query latency (99th percentile): 1 second    The needed storage space for the given retention (the retention is set via `-retentionPeriod` command-line flag) can be extrapolated from disk space usage in a test run. For example, if `-storageDataPath` directory size becomes 10GB after a day-long test run on a production workload, then it will need at least `10GB*100=1TB` of disk space for `-retentionPeriod=100d` (100-days retention period).    It is recommended leaving the following amounts of spare resources:    * 50% of free RAM for reducing the probability of OOM (out of memory) crashes and slowdowns during temporary spikes in workload.  * 50% of spare CPU for reducing the probability of slowdowns during temporary spikes in workload.  * At least 30% of free storage space at the directory pointed by `-storageDataPath` command-line flag. See also `-storage.minFreeDiskSpaceBytes` command-line flag description [here](#list-of-command-line-flags).      ## High availability    * Install multiple VictoriaMetrics instances in distinct datacenters (availability zones).  * Pass addresses of these instances to [vmagent](https://docs.victoriametrics.com/vmagent.html) via `-remoteWrite.url` command-line flag:    ```bash  /path/to/vmagent -remoteWrite.url=http://<victoriametrics-addr-1>:8428/api/v1/write -remoteWrite.url=http://<victoriametrics-addr-2>:8428/api/v1/write  ```    Alternatively these addresses may be passed to `remote_write` section in Prometheus config:    ```yml  remote_write:    - url: http://<victoriametrics-addr-1>:8428/api/v1/write      queue_config:        max_samples_per_send: 10000    # ...    - url: http://<victoriametrics-addr-N>:8428/api/v1/write      queue_config:        max_samples_per_send: 10000  ```    * Apply the updated config:    ```bash  kill -HUP `pidof prometheus`  ```    It is recommended to use [vmagent](https://docs.victoriametrics.com/vmagent.html) instead of Prometheus for highly loaded setups.    * Now Prometheus should write data into all the configured `remote_write` urls in parallel.  * Set up [Promxy](https://github.com/jacksontj/promxy) in front of all the VictoriaMetrics replicas.  * Set up Prometheus datasource in Grafana that points to Promxy.    If you have Prometheus HA pairs with replicas `r1` and `r2` in each pair, then configure each `r1`  to write data to `victoriametrics-addr-1`, while each `r2` should write data to `victoriametrics-addr-2`.    Another option is to write data simultaneously from Prometheus HA pair to a pair of VictoriaMetrics instances  with the enabled de-duplication. See [this section](#deduplication) for details.      ## Deduplication    VictoriaMetrics de-duplicates data points if `-dedup.minScrapeInterval` command-line flag is set to positive duration. For example, `-dedup.minScrapeInterval=60s` would de-duplicate data points on the same time series if they fall within the same discrete 60s bucket.  The earliest data point will be kept. In the case of equal timestamps, an arbitrary data point will be kept. See [this comment](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/2112#issuecomment-1032587618) for more details on how downsampling works.    The `-dedup.minScrapeInterval=D` is equivalent to `-downsampling.period=0s:D` if [downsampling](#downsampling) is enabled. It is safe to use deduplication and downsampling simultaneously.    The recommended value for `-dedup.minScrapeInterval` must equal to `scrape_interval` config from Prometheus configs. It is recommended to have a single `scrape_interval` across all the scrape targets. See [this article](https://www.robustperception.io/keep-it-simple-scrape_interval-id) for details.    The de-duplication reduces disk space usage if multiple identically configured [vmagent](https://docs.victoriametrics.com/vmagent.html) or Prometheus instances in HA pair  write data to the same VictoriaMetrics instance. These vmagent or Prometheus instances must have identical  `external_labels` section in their configs, so they write data to the same time series.      ## Storage    VictoriaMetrics stores time series data in [MergeTree](https://en.wikipedia.org/wiki/Log-structured_merge-tree)-like   data structures. On insert, VictoriaMetrics accumulates up to 1s of data and dumps it on disk to  `<-storageDataPath>/data/small/YYYY_MM/` subdirectory forming a `part` with the following   name pattern: `rowsCount_blocksCount_minTimestamp_maxTimestamp`. Each part consists of two ""columns"":  values and timestamps. These are sorted and compressed raw time series values. Additionally, part contains  index files for searching for specific series in the values and timestamps files.    `Parts` are periodically merged into the bigger parts. The resulting `part` is constructed   under `<-storageDataPath>/data/{small,big}/YYYY_MM/tmp` subdirectory. When the resulting `part` is complete, it is atomically moved from the `tmp`   to its own subdirectory, while the source parts are atomically removed. The end result is that the source   parts are substituted by a single resulting bigger `part` in the `<-storageDataPath>/data/{small,big}/YYYY_MM/` directory.  Information about merging process is available in [single-node VictoriaMetrics](https://grafana.com/dashboards/10229)   and [clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11176) Grafana dashboards.   See more details in [monitoring docs](#monitoring).    The `merge` process is usually named ""compaction"", because the resulting `part` size is usually smaller than   the sum of the source `parts`. There are following benefits of doing the merge process:  * it improves query performance, since lower number of `parts` are inspected with each query;  * it reduces the number of data files, since each `part`contains fixed number of files;   * better compression rate for the resulting part.    Newly added `parts` either appear in the storage or fail to appear.   Storage never contains partially created parts. The same applies to merge process — `parts` are either fully   merged into a new `part` or fail to merge. There are no partially merged `parts` in MergeTree.   `Part` contents in MergeTree never change. Parts are immutable. They may be only deleted after the merge   to a bigger `part` or when the `part` contents goes outside the configured `-retentionPeriod`.    See [this article](https://valyala.medium.com/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282) for more details.    See also [how to work with snapshots](#how-to-work-with-snapshots).    ## Retention    Retention is configured with the `-retentionPeriod` command-line flag, which takes a number followed by a time unit character - `h(ours)`, `d(ays)`, `w(eeks)`, `y(ears)`. If the time unit is not specified, a month is assumed. For instance, `-retentionPeriod=3` means that the data will be stored for 3 months and then deleted. The default retention period is one month.    Data is split in per-month partitions inside `<-storageDataPath>/data/{small,big}` folders.  Data partitions outside the configured retention are deleted on the first day of the new month.  Each partition consists of one or more data parts with the following name pattern `rowsCount_blocksCount_minTimestamp_maxTimestamp`.  Data parts outside of the configured retention are eventually deleted during   [background merge](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).    The maximum disk space usage for a given `-retentionPeriod` is going to be (`-retentionPeriod` + 1) months.  For example, if `-retentionPeriod` is set to 1, data for January is deleted on March 1st.    Please note, the time range covered by data part is not limited by retention period unit. Hence, data part may contain data  for multiple days and will be deleted only when fully outside of the configured retention.    It is safe to extend `-retentionPeriod` on existing data. If `-retentionPeriod` is set to a lower  value than before, then data outside the configured period will be eventually deleted.    VictoriaMetrics does not support indefinite retention, but you can specify an arbitrarily high duration, e.g. `-retentionPeriod=100y`.    ## Multiple retentions    A single instance of VictoriaMetrics supports only a single retention, which can be configured via `-retentionPeriod` command-line flag. If you need multiple retentions, then you may start multiple VictoriaMetrics instances with distinct values for the following flags:    * `-retentionPeriod`  * `-storageDataPath`, so the data for each retention period is saved in a separate directory  * `-httpListenAddr`, so clients may reach VictoriaMetrics instance with proper retention    Then set up [vmauth](https://docs.victoriametrics.com/vmauth.html) in front of VictoriaMetrics instances,  so it could route requests from particular user to VictoriaMetrics with the desired retention.  The same scheme could be implemented for multiple tenants in [VictoriaMetrics cluster](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html).  See [these docs](https://docs.victoriametrics.com/guides/guide-vmcluster-multiple-retention-setup.html) for multi-retention setup details.      ## Downsampling    [VictoriaMetrics Enterprise](https://victoriametrics.com/products/enterprise/) supports multi-level downsampling with `-downsampling.period` command-line flag. For example:    * `-downsampling.period=30d:5m` instructs VictoriaMetrics to [deduplicate](#deduplication) samples older than 30 days with 5 minutes interval.    * `-downsampling.period=30d:5m,180d:1h` instructs VictoriaMetrics to deduplicate samples older than 30 days with 5 minutes interval and to deduplicate samples older than 180 days with 1 hour interval.    Downsampling is applied independently per each time series. It can reduce disk space usage and improve query performance if it is applied to time series with big number of samples per each series. The downsampling doesn't improve query performance if the database contains big number of time series with small number of samples per each series (aka [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate)), since downsampling doesn't reduce the number of time series. So the majority of time is spent on searching for the matching time series. It is possible to use recording rules in [vmalert](https://docs.victoriametrics.com/vmalert.html) in order to reduce the number of time series. See [these docs](https://docs.victoriametrics.com/vmalert.html#downsampling-and-aggregation-via-vmalert).    The downsampling can be evaluated for free by downloading and using enterprise binaries from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).      ## Multi-tenancy    Single-node VictoriaMetrics doesn't support multi-tenancy. Use [cluster version](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html#multitenancy) instead.      ## Scalability and cluster version    Though single-node VictoriaMetrics cannot scale to multiple nodes, it is optimized for resource usage - storage size / bandwidth / IOPS, RAM, CPU.  This means that a single-node VictoriaMetrics may scale vertically and substitute a moderately sized cluster built with competing solutions  such as Thanos, Uber M3, InfluxDB or TimescaleDB. See [vertical scalability benchmarks](https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae).    So try single-node VictoriaMetrics at first and then [switch to cluster version](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster) if you still need  horizontally scalable long-term remote storage for really large Prometheus deployments.  [Contact us](mailto:info@victoriametrics.com) for enterprise support.      ## Alerting    It is recommended using [vmalert](https://docs.victoriametrics.com/vmalert.html) for alerting.    Additionally, alerting can be set up with the following tools:    * With Prometheus - see [the corresponding docs](https://prometheus.io/docs/alerting/overview/).  * With Promxy - see [the corresponding docs](https://github.com/jacksontj/promxy/blob/master/README.md#how-do-i-use-alertingrecording-rules-in-promxy).  * With Grafana - see [the corresponding docs](https://grafana.com/docs/alerting/rules/).      ## Security    Do not forget protecting sensitive endpoints in VictoriaMetrics when exposing it to untrusted networks such as the internet.  Consider setting the following command-line flags:    * `-tls`, `-tlsCertFile` and `-tlsKeyFile` for switching from HTTP to HTTPS.  * `-httpAuth.username` and `-httpAuth.password` for protecting all the HTTP endpoints    with [HTTP Basic Authentication](https://en.wikipedia.org/wiki/Basic_access_authentication).  * `-deleteAuthKey` for protecting `/api/v1/admin/tsdb/delete_series` endpoint. See [how to delete time series](#how-to-delete-time-series).  * `-snapshotAuthKey` for protecting `/snapshot*` endpoints. See [how to work with snapshots](#how-to-work-with-snapshots).  * `-forceMergeAuthKey` for protecting `/internal/force_merge` endpoint. See [force merge docs](#forced-merge).  * `-search.resetCacheAuthKey` for protecting `/internal/resetRollupResultCache` endpoint. See [backfilling](#backfilling) for more details.  * `-configAuthKey` for protecting `/config` endpoint, since it may contain sensitive information such as passwords.  - `-pprofAuthKey` for protecting `/debug/pprof/*` endpoints, which can be used for [profiling](#profiling).    Explicitly set internal network interface for TCP and UDP ports for data ingestion with Graphite and OpenTSDB formats.  For example, substitute `-graphiteListenAddr=:2003` with `-graphiteListenAddr=<internal_iface_ip>:2003`.    Prefer authorizing all the incoming requests from untrusted networks with [vmauth](https://docs.victoriametrics.com/vmauth.html)  or similar auth proxy.      ## Tuning    * There is no need for VictoriaMetrics tuning since it uses reasonable defaults for command-line flags,    which are automatically adjusted for the available CPU and RAM resources.  * There is no need for Operating System tuning since VictoriaMetrics is optimized for default OS settings.    The only option is increasing the limit on [the number of open files in the OS](https://medium.com/@muhammadtriwibowo/set-permanently-ulimit-n-open-files-in-ubuntu-4d61064429a).    The recommendation is not specific for VictoriaMetrics only but also for any service which handles many HTTP connections and stores data on disk.  * VictoriaMetrics is a write-heavy application and its performance depends on disk performance. So be careful with other    applications or utilities (like [fstrim](http://manpages.ubuntu.com/manpages/bionic/man8/fstrim.8.html))    which could [exhaust disk resources](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/1521).  * The recommended filesystem is `ext4`, the recommended persistent storage is [persistent HDD-based disk on GCP](https://cloud.google.com/compute/docs/disks/#pdspecs),    since it is protected from hardware failures via internal replication and it can be [resized on the fly](https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd).    If you plan to store more than 1TB of data on `ext4` partition or plan extending it to more than 16TB,    then the following options are recommended to pass to `mkfs.ext4`:    ```bash  mkfs.ext4 ... -O 64bit,huge_file,extent -T huge  ```    ## Monitoring    VictoriaMetrics exports internal metrics in Prometheus format at `/metrics` page.  These metrics may be collected by [vmagent](https://docs.victoriametrics.com/vmagent.html)  or Prometheus by adding the corresponding scrape config to it.  Alternatively they can be self-scraped by setting `-selfScrapeInterval` command-line flag to duration greater than 0.  For example, `-selfScrapeInterval=10s` would enable self-scraping of `/metrics` page with 10 seconds interval.    There are officials Grafana dashboards for [single-node VictoriaMetrics](https://grafana.com/dashboards/10229) and [clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11176). There is also an [alternative dashboard for clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11831).    Graphs on these dashboard contain useful hints - hover the `i` icon at the top left corner of each graph in order to read it.    It is recommended setting up alerts in [vmalert](https://docs.victoriametrics.com/vmalert.html) or in Prometheus from [this config](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/alerts.yml).    The most interesting metrics are:    * `vm_cache_entries{type=""storage/hour_metric_ids""}` - the number of time series with new data points during the last hour    aka [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).  * `increase(vm_new_timeseries_created_total[1h])` - time series [churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate) during the previous hour.  * `sum(vm_rows{type=~""storage/.*""})` - total number of `(timestamp, value)` data points in the database.  * `sum(rate(vm_rows_inserted_total[5m]))` - ingestion rate, i.e. how many samples are inserted int the database per second.  * `vm_free_disk_space_bytes` - free space left at `-storageDataPath`.  * `sum(vm_data_size_bytes)` - the total size of data on disk.  * `increase(vm_slow_row_inserts_total[5m])` - the number of slow inserts during the last 5 minutes.    If this number remains high during extended periods of time, then it is likely more RAM is needed for optimal handling    of the current number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).  * `increase(vm_slow_metric_name_loads_total[5m])` - the number of slow loads of metric names during the last 5 minutes.    If this number remains high during extended periods of time, then it is likely more RAM is needed for optimal handling    of the current number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).    VictoriaMetrics also exposes currently running queries with their execution times at `/api/v1/status/active_queries` page.    See the example of alerting rules for VM components [here](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/deployment/docker/alerts.yml).      ## TSDB stats    VictoriaMetrics returns TSDB stats at `/api/v1/status/tsdb` page in the way similar to Prometheus - see [these Prometheus docs](https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-stats). VictoriaMetrics accepts the following optional query args at `/api/v1/status/tsdb` page:    * `topN=N` where `N` is the number of top entries to return in the response. By default top 10 entries are returned.    * `date=YYYY-MM-DD` where `YYYY-MM-DD` is the date for collecting the stats. By default the stats is collected for the current day.    * `match[]=SELECTOR` where `SELECTOR` is an arbitrary [time series selector](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors) for series to take into account during stats calculation. By default all the series are taken into account.    * `extra_label=LABEL=VALUE`. See [these docs](#prometheus-querying-api-enhancements) for more details.      ## Cardinality limiter    By default VictoriaMetrics doesn't limit the number of stored time series. The limit can be enforced by setting the following command-line flags:    * `-storage.maxHourlySeries` - limits the number of time series that can be added during the last hour. Useful for limiting the number of [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series).  * `-storage.maxDailySeries` - limits the number of time series that can be added during the last day. Useful for limiting daily [churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate).    Both limits can be set simultaneously. If any of these limits is reached, then incoming samples for new time series are dropped. A sample of dropped series is put in the log with `WARNING` level.    The exceeded limits can be [monitored](#monitoring) with the following metrics:    * `vm_hourly_series_limit_rows_dropped_total` - the number of metrics dropped due to exceeded hourly limit on the number of unique time series.  * `vm_daily_series_limit_rows_dropped_total` - the number of metrics dropped due to exceeded daily limit on the number of unique time series.    These limits are approximate, so VictoriaMetrics can underflow/overflow the limit by a small percentage (usually less than 1%).    See also more advanced [cardinality limiter in vmagent](https://docs.victoriametrics.com/vmagent.html#cardinality-limiter).      ## Troubleshooting    * It is recommended to use default command-line flag values (i.e. don't set them explicitly) until the need    of tweaking these flag values arises.    * It is recommended inspecting logs during troubleshooting, since they may contain useful information.    * It is recommended upgrading to the latest available release from [this page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases),    since the encountered issue could be already fixed there.    * It is recommended to have at least 50% of spare resources for CPU, disk IO and RAM, so VictoriaMetrics could handle short spikes in the workload without performance issues.    * VictoriaMetrics requires free disk space for [merging data files to bigger ones](https://medium.com/@valyala/how-victoriametrics-makes-instant-snapshots-for-multi-terabyte-time-series-data-e1f3fb0e0282).    It may slow down when there is no enough free space left. So make sure `-storageDataPath` directory    has at least 20% of free space. The remaining amount of free space    can be [monitored](#monitoring) via `vm_free_disk_space_bytes` metric. The total size of data    stored on the disk can be monitored via sum of `vm_data_size_bytes` metrics.    See also `vm_merge_need_free_disk_space` metrics, which are set to values higher than 0    if background merge cannot be initiated due to free disk space shortage. The value shows the number of per-month partitions,    which would start background merge if they had more free disk space.    * VictoriaMetrics buffers incoming data in memory for up to a few seconds before flushing it to persistent storage.    This may lead to the following ""issues"":    * Data becomes available for querying in a few seconds after inserting. It is possible to flush in-memory buffers to persistent storage      by requesting `/internal/force_flush` http handler. This handler is mostly needed for testing and debugging purposes.    * The last few seconds of inserted data may be lost on unclean shutdown (i.e. OOM, `kill -9` or hardware reset).      See [this article for technical details](https://valyala.medium.com/wal-usage-looks-broken-in-modern-time-series-databases-b62a627ab704).    * If VictoriaMetrics works slowly and eats more than a CPU core per 100K ingested data points per second,    then it is likely you have too many [active time series](https://docs.victoriametrics.com/FAQ.html#what-is-an-active-time-series) for the current amount of RAM.    VictoriaMetrics [exposes](#monitoring) `vm_slow_*` metrics such as `vm_slow_row_inserts_total` and `vm_slow_metric_name_loads_total`, which could be used    as an indicator of low amounts of RAM. It is recommended increasing the amount of RAM on the node with VictoriaMetrics in order to improve    ingestion and query performance in this case.    * If the order of labels for the same metrics can change over time (e.g. if `metric{k1=""v1"",k2=""v2""}` may become `metric{k2=""v2"",k1=""v1""}`),    then it is recommended running VictoriaMetrics with `-sortLabels` command-line flag in order to reduce memory usage and CPU usage.    * VictoriaMetrics prioritizes data ingestion over data querying. So if it has no enough resources for data ingestion,    then data querying may slow down significantly.    * If VictoriaMetrics doesn't work because of certain parts are corrupted due to disk errors,    then just remove directories with broken parts. It is safe removing subdirectories under `<-storageDataPath>/data/{big,small}/YYYY_MM` directories    when VictoriaMetrics isn't running. This recovers VictoriaMetrics at the cost of data loss stored in the deleted broken parts.    In the future, `vmrecover` tool will be created for automatic recovering from such errors.    * If you see gaps on the graphs, try resetting the cache by sending request to `/internal/resetRollupResultCache`.    If this removes gaps on the graphs, then it is likely data with timestamps older than `-search.cacheTimestampOffset`    is ingested into VictoriaMetrics. Make sure that data sources have synchronized time with VictoriaMetrics.      If the gaps are related to irregular intervals between samples, then try adjusting `-search.minStalenessInterval` command-line flag    to value close to the maximum interval between samples.    * If you are switching from InfluxDB or TimescaleDB, then take a look at `-search.maxStalenessInterval` command-line flag.    It may be needed in order to suppress default gap filling algorithm used by VictoriaMetrics - by default it assumes    each time series is continuous instead of discrete, so it fills gaps between real samples with regular intervals.    * Metrics and labels leading to [high cardinality](https://docs.victoriametrics.com/FAQ.html#what-is-high-cardinality) or [high churn rate](https://docs.victoriametrics.com/FAQ.html#what-is-high-churn-rate) can be determined at `/api/v1/status/tsdb` page. See [these docs](#tsdb-stats) for details.    * New time series can be logged if `-logNewSeries` command-line flag is passed to VictoriaMetrics.    * VictoriaMetrics limits the number of labels per each metric with `-maxLabelsPerTimeseries` command-line flag.    This prevents from ingesting metrics with too many labels. It is recommended [monitoring](#monitoring) `vm_metrics_with_dropped_labels_total`    metric in order to determine whether `-maxLabelsPerTimeseries` must be adjusted for your workload.    * If you store Graphite metrics like `foo.bar.baz` in VictoriaMetrics, then `{__graphite__=""foo.*.baz""}` filter can be used for selecting such metrics. See [these docs](#selecting-graphite-metrics) for details.    * VictoriaMetrics ignores `NaN` values during data ingestion.      ## Cache removal    VictoriaMetrics uses various internal caches. These caches are stored to `<-storageDataPath>/cache` directory during graceful shutdown (e.g. when VictoriaMetrics is stopped by sending `SIGINT` signal). The caches are read on the next VictoriaMetrics startup. Sometimes it is needed to remove such caches on the next startup. This can be performed by placing `reset_cache_on_startup` file inside the `<-storageDataPath>/cache` directory before the restart of VictoriaMetrics. See [this issue](https://github.com/VictoriaMetrics/VictoriaMetrics/issues/1447) for details.      ## Cache tuning    VictoriaMetrics uses various in-memory caches for faster data ingestion and query performance.  The following metrics for each type of cache are exported at [`/metrics` page](#monitoring):  - `vm_cache_size_bytes` - the actual cache size  - `vm_cache_size_max_bytes` - cache size limit  - `vm_cache_requests_total` - the number of requests to the cache  - `vm_cache_misses_total` - the number of cache misses  - `vm_cache_entries` - the number of entries in the cache    Both Grafana dashboards for [single-node VictoriaMetrics](https://grafana.com/dashboards/10229)  and [clustered VictoriaMetrics](https://grafana.com/grafana/dashboards/11176)  contain `Caches` section with cache metrics visualized. The panels show the current  memory usage by each type of cache, and also a cache hit rate. If hit rate is close to 100%  then cache efficiency is already very high and does not need any tuning.  The panel `Cache usage %` in `Troubleshooting` section shows the percentage of used cache size  from the allowed size by type. If the percentage is below 100%, then no further tuning needed.    Please note, default cache sizes were carefully adjusted accordingly to the most  practical scenarios and workloads. Change the defaults only if you understand the implications.    To override the default values see command-line flags with `-storage.cacheSize` prefix.  See the full description of flags [here](#list-of-command-line-flags).      ## Data migration    ### From VictoriaMetrics    The simplest way to migrate data from one single-node (source) to another (destination), or from one vmstorage node   to another do the following:  1. Stop the VictoriaMetrics (source) with `kill -INT`;  2. Copy (via [rsync](https://en.wikipedia.org/wiki/Rsync) or any other tool) the entire folder specified   via `-storageDataPath` from the source node to the empty folder at the destination node.  3. Once copy is done, stop the VictoriaMetrics (destination) with `kill -INT` and verify that   its `-storageDataPath` points to the copied folder from p.2;  4. Start the VictoriaMetrics (destination). The copied data should be now available.    Things to consider when copying data:  1. Data formats between single-node and vmstorage node aren't compatible and can't be copied.  2. Copying data folder means complete replacement of the previous data on destination VictoriaMetrics.    For more complex scenarios like single-to-cluster, cluster-to-single, re-sharding or migrating only a fraction  of data - see [vmctl. Migrating data from VictoriaMetrics](https://docs.victoriametrics.com/vmctl.html#migrating-data-from-victoriametrics).      ### From other systems    Use [vmctl](https://docs.victoriametrics.com/vmctl.html) for data migration. It supports the following data migration types:    * From Prometheus to VictoriaMetrics  * From InfluxDB to VictoriaMetrics  * From VictoriaMetrics to VictoriaMetrics  * From OpenTSDB to VictoriaMetrics    See [vmctl docs](https://docs.victoriametrics.com/vmctl.html) for more details.      ## Backfilling    VictoriaMetrics accepts historical data in arbitrary order of time via [any supported ingestion method](#how-to-import-time-series-data).  See [how to backfill data with recording rules in vmalert](https://docs.victoriametrics.com/vmalert.html#rules-backfilling).  Make sure that configured `-retentionPeriod` covers timestamps for the backfilled data.    It is recommended disabling query cache with `-search.disableCache` command-line flag when writing  historical data with timestamps from the past, since the cache assumes that the data is written with  the current timestamps. Query cache can be enabled after the backfilling is complete.    An alternative solution is to query `/internal/resetRollupResultCache` url after backfilling is complete. This will reset  the query cache, which could contain incomplete data cached during the backfilling.    Yet another solution is to increase `-search.cacheTimestampOffset` flag value in order to disable caching  for data with timestamps close to the current time. Single-node VictoriaMetrics automatically resets response  cache when samples with timestamps older than `now - search.cacheTimestampOffset` are ingested to it.      ## Data updates    VictoriaMetrics doesn't support updating already existing sample values to new ones. It stores all the ingested data points  for the same time series with identical timestamps. While it is possible substituting old time series with new time series via  [removal of old time series](#how-to-delete-time-series) and then [writing new time series](#backfilling), this approach  should be used only for one-off updates. It shouldn't be used for frequent updates because of non-zero overhead related to data removal.      ## Replication    Single-node VictoriaMetrics doesn't support application-level replication. Use cluster version instead.  See [these docs](https://docs.victoriametrics.com/Cluster-VictoriaMetrics.html#replication-and-data-safety) for details.    Storage-level replication may be offloaded to durable persistent storage such as [Google Cloud disks](https://cloud.google.com/compute/docs/disks#pdspecs).    See also [high availability docs](#high-availability) and [backup docs](#backups).      ## Backups    VictoriaMetrics supports backups via [vmbackup](https://docs.victoriametrics.com/vmbackup.html)  and [vmrestore](https://docs.victoriametrics.com/vmrestore.html) tools.  We also provide [vmbackupmanager](https://docs.victoriametrics.com/vmbackupmanager.html) tool for enterprise subscribers.  Enterprise binaries can be downloaded and evaluated for free from [the releases page](https://github.com/VictoriaMetrics/VictoriaMetrics/releases).      ## Benchmarks    Note, that vendors (including VictoriaMetrics) are often biased when doing such tests. E.g. they try highlighting   the best parts of their product, while highlighting the worst parts of competing products.   So we encourage users and all independent third parties to conduct their becnhmarks for various products   they are evaluating in production and publish the results.    As a reference, please see [benchmarks](https://docs.victoriametrics.com/Articles.html#benchmarks) conducted by  VictoriaMetrics team. Please also see the [helm chart](https://github.com/VictoriaMetrics/benchmark)   for running ingestion benchmarks based on node_exporter metrics.      ## Profiling    VictoriaMetrics provides handlers for collecting the following [Go profiles](https://blog.golang.org/profiling-go-programs):    * Memory profile. It can be collected with the following command (replace `0.0.0.0` with hostname if needed):    <div class=""with-copy"" markdown=""1"">    ```bash  curl http://0.0.0.0:8428/debug/pprof/heap > mem.pprof  ```    </div>    * CPU profile. It can be collected with the following command (replace `0.0.0.0` with hostname if needed):    <div class=""with-copy"" markdown=""1"">    ```bash  curl http://0.0.0.0:8428/debug/pprof/profile > cpu.pprof  ```    </div>    The command for collecting CPU profile waits for 30 seconds before returning.    The collected profiles may be analyzed with [go tool pprof](https://github.com/google/pprof).      ## Integrations    * [Helm charts for single-node and cluster versions of VictoriaMetrics](https://github.com/VictoriaMetrics/helm-charts).  * [Kubernetes operator for VictoriaMetrics](https://github.com/VictoriaMetrics/operator).  * [netdata](https://github.com/netdata/netdata) can push data into VictoriaMetrics via `Prometheus remote_write API`.    See [these docs](https://github.com/netdata/netdata#integrations).  * [go-graphite/carbonapi](https://github.com/go-graphite/carbonapi) can use VictoriaMetrics as time series backend.    See [this example](https://github.com/go-graphite/carbonapi/blob/main/cmd/carbonapi/carbonapi.example.victoriametrics.yaml).  * [Ansible role for installing single-node VictoriaMetrics](https://github.com/dreamteam-gg/ansible-victoriametrics-role).  * [Ansible role for installing cluster VictoriaMetrics](https://github.com/Slapper/ansible-victoriametrics-cluster-role).  * [Snap package for VictoriaMetrics](https://snapcraft.io/victoriametrics).  * [vmalert-cli](https://github.com/aorfanos/vmalert-cli) - a CLI application for managing [vmalert](https://docs.victoriametrics.com/vmalert.html).      ## Third-party contributions    * [Unofficial yum repository](https://copr.fedorainfracloud.org/coprs/antonpatsev/VictoriaMetrics/) ([source code](https://github.com/patsevanton/victoriametrics-rpm))  * [Prometheus -> VictoriaMetrics exporter #1](https://github.com/ryotarai/prometheus-tsdb-dump)  * [Prometheus -> VictoriaMetrics exporter #2](https://github.com/AnchorFree/tsdb-remote-write)  * [Prometheus Oauth proxy](https://gitlab.com/optima_public/prometheus_oauth_proxy) - see [this article](https://medium.com/@richard.holly/powerful-saas-solution-for-detection-metrics-c67b9208d362) for details.      ## Contacts    Contact us with any questions regarding VictoriaMetrics at [info@victoriametrics.com](mailto:info@victoriametrics.com).      ## Community and contributions    Feel free asking any questions regarding VictoriaMetrics:    * [slack](https://slack.victoriametrics.com/)  * [linkedin](https://www.linkedin.com/company/victoriametrics/)  * [reddit](https://www.reddit.com/r/VictoriaMetrics/)  * [telegram-en](https://t.me/VictoriaMetrics_en)  * [telegram-ru](https://t.me/VictoriaMetrics_ru1)  * [articles and talks about VictoriaMetrics in Russian](https://github.com/denisgolius/victoriametrics-ru-links)  * [google groups](https://groups.google.com/forum/#!forum/victorametrics-users)    If you like VictoriaMetrics and want to contribute, then we need the following:    * Filing issues and feature requests [here](https://github.com/VictoriaMetrics/VictoriaMetrics/issues).  * Spreading a word about VictoriaMetrics: conference talks, articles, comments, experience sharing with colleagues.  * Updating documentation.    We are open to third-party pull requests provided they follow [KISS design principle](https://en.wikipedia.org/wiki/KISS_principle):    * Prefer simple code and architecture.  * Avoid complex abstractions.  * Avoid magic code and fancy algorithms.  * Avoid [big external dependencies](https://medium.com/@valyala/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d).  * Minimize the number of moving parts in the distributed system.  * Avoid automated decisions, which may hurt cluster availability, consistency or performance.    Adhering `KISS` principle simplifies the resulting code and architecture, so it can be reviewed, understood and verified by many people.    ## Reporting bugs    Report bugs and propose new features [here](https://github.com/VictoriaMetrics/VictoriaMetrics/issues).      ## VictoriaMetrics Logo    [Zip](https://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/VM_logo.zip) contains three folders with different image orientations (main color and inverted version).    Files included in each folder:    * 2 JPEG Preview files  * 2 PNG Preview files with transparent background  * 2 EPS Adobe Illustrator EPS10 files    ### Logo Usage Guidelines    #### Font used    * Lato Black  * Lato Regular    #### Color Palette    * HEX [#110f0f](https://www.color-hex.com/color/110f0f)  * HEX [#ffffff](https://www.color-hex.com/color/ffffff)    ### We kindly ask    * Please don't use any other font instead of suggested.  * There should be sufficient clear space around the logo.  * Do not change spacing, alignment, or relative locations of the design elements.  * Do not change the proportions of any of the design elements or the design itself. You    may resize as needed but must retain all proportions.      ## List of command-line flags    Pass `-help` to VictoriaMetrics in order to see the list of supported command-line flags with their description:    ```    -bigMergeConcurrency int      	The maximum number of CPU cores to use for big merges. Default value is used if set to 0    -configAuthKey string      	Authorization key for accessing /config page. It must be passed via authKey query arg    -csvTrimTimestamp duration      	Trim timestamps when importing csv data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)    -datadog.maxInsertRequestSize size      	The maximum size in bytes of a single DataDog POST request to /api/v1/series      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 67108864)    -dedup.minScrapeInterval duration      	Leave only the first sample in every time series per each discrete interval equal to -dedup.minScrapeInterval > 0. See https://docs.victoriametrics.com/#deduplication and https://docs.victoriametrics.com/#downsampling    -deleteAuthKey string      	authKey for metrics' deletion via /api/v1/admin/tsdb/delete_series and /tags/delSeries    -denyQueriesOutsideRetention      	Whether to deny queries outside of the configured -retentionPeriod. When set, then /api/v1/query_range would return '503 Service Unavailable' error for queries with 'from' value outside -retentionPeriod. This may be useful when multiple data sources with distinct retentions are hidden behind query-tee    -downsampling.period array      	Comma-separated downsampling periods in the format 'offset:period'. For example, '30d:10m' instructs to leave a single sample per 10 minutes for samples older than 30 days. See https://docs.victoriametrics.com/#downsampling for details      	Supports an array of values separated by comma or specified via multiple flags.    -dryRun      	Whether to check only -promscrape.config and then exit. Unknown config entries aren't allowed in -promscrape.config by default. This can be changed with -promscrape.config.strictParse=false command-line flag    -enableTCP6      	Whether to enable IPv6 for listening and dialing. By default only IPv4 TCP and UDP is used    -envflag.enable      	Whether to enable reading flags from environment variables additionally to command line. Command line flag values have priority over values from environment vars. Flags are read only from command line if this flag isn't set. See https://docs.victoriametrics.com/#environment-variables for more details    -envflag.prefix string      	Prefix for environment variables if -envflag.enable is set    -eula      	By specifying this flag, you confirm that you have an enterprise license and accept the EULA https://victoriametrics.com/assets/VM_EULA.pdf    -finalMergeDelay duration      	The delay before starting final merge for per-month partition after no new data is ingested into it. Final merge may require additional disk IO and CPU resources. Final merge may increase query speed and reduce disk space usage in some cases. Zero value disables final merge    -forceFlushAuthKey string      	authKey, which must be passed in query string to /internal/force_flush pages    -forceMergeAuthKey string      	authKey, which must be passed in query string to /internal/force_merge pages    -fs.disableMmap      	Whether to use pread() instead of mmap() for reading data files. By default mmap() is used for 64-bit arches and pread() is used for 32-bit arches, since they cannot read data files bigger than 2^32 bytes in memory. mmap() is usually faster for reading small data chunks than pread()    -graphiteListenAddr string      	TCP and UDP address to listen for Graphite plaintext data. Usually :2003 must be set. Doesn't work if empty    -graphiteTrimTimestamp duration      	Trim timestamps for Graphite data to this duration. Minimum practical duration is 1s. Higher duration (i.e. 1m) may be used for reducing disk space usage for timestamp data (default 1s)    -http.connTimeout duration      	Incoming http connections are closed after the configured timeout. This may help to spread the incoming load among a cluster of services behind a load balancer. Please note that the real timeout may be bigger by up to 10% as a protection against the thundering herd problem (default 2m0s)    -http.disableResponseCompression      	Disable compression of HTTP responses to save CPU resources. By default compression is enabled to save network bandwidth    -http.idleConnTimeout duration      	Timeout for incoming idle http connections (default 1m0s)    -http.maxGracefulShutdownDuration duration      	The maximum duration for a graceful shutdown of the HTTP server. A highly loaded server may require increased value for a graceful shutdown (default 7s)    -http.pathPrefix string      	An optional prefix to add to all the paths handled by http server. For example, if '-http.pathPrefix=/foo/bar' is set, then all the http requests will be handled on '/foo/bar/*' paths. This may be useful for proxied requests. See https://www.robustperception.io/using-external-urls-and-proxies-with-prometheus    -http.shutdownDelay duration      	Optional delay before http server shutdown. During this delay, the server returns non-OK responses from /health page, so load balancers can route new requests to other servers    -httpAuth.password string      	Password for HTTP Basic Auth. The authentication is disabled if -httpAuth.username is empty    -httpAuth.username string      	Username for HTTP Basic Auth. The authentication is disabled if empty. See also -httpAuth.password    -httpListenAddr string      	TCP address to listen for http connections (default "":8428"")    -import.maxLineLen size      	The maximum length in bytes of a single line accepted by /api/v1/import; the line length can be limited with 'max_rows_per_line' query arg passed to /api/v1/export      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 104857600)    -influx.databaseNames array      	Comma-separated list of database names to return from /query and /influx/query API. This can be needed for accepting data from Telegraf plugins such as https://github.com/fangli/fluent-plugin-influxdb      	Supports an array of values separated by comma or specified via multiple flags.    -influx.maxLineSize size      	The maximum size in bytes for a single InfluxDB line during parsing      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 262144)    -influxDBLabel string      	Default label for the DB name sent over '?db={db_name}' query parameter (default ""db"")    -influxListenAddr string      	TCP and UDP address to listen for InfluxDB line protocol data. Usually :8189 must be set. Doesn't work if empty. This flag isn't needed when ingesting data over HTTP - just send it to http://<victoriametrics>:8428/write    -influxMeasurementFieldSeparator string      	Separator for '{measurement}{separator}{field_name}' metric name when inserted via InfluxDB line protocol (default ""_"")    -influxSkipMeasurement      	Uses '{field_name}' as a metric name while ignoring '{measurement}' and '-influxMeasurementFieldSeparator'    -influxSkipSingleField      	Uses '{measurement}' instead of '{measurement}{separator}{field_name}' for metic name if InfluxDB line contains only a single field    -influxTrimTimestamp duration      	Trim timestamps for InfluxDB line protocol data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)    -insert.maxQueueDuration duration      	The maximum duration for waiting in the queue for insert requests due to -maxConcurrentInserts (default 1m0s)    -logNewSeries      	Whether to log new series. This option is for debug purposes only. It can lead to performance issues when big number of new series are ingested into VictoriaMetrics    -loggerDisableTimestamps      	Whether to disable writing timestamps in logs    -loggerErrorsPerSecondLimit int      	Per-second limit on the number of ERROR messages. If more than the given number of errors are emitted per second, the remaining errors are suppressed. Zero values disable the rate limit    -loggerFormat string      	Format for logs. Possible values: default, json (default ""default"")    -loggerLevel string      	Minimum level of errors to log. Possible values: INFO, WARN, ERROR, FATAL, PANIC (default ""INFO"")    -loggerOutput string      	Output for the logs. Supported values: stderr, stdout (default ""stderr"")    -loggerTimezone string      	Timezone to use for timestamps in logs. Timezone must be a valid IANA Time Zone. For example: America/New_York, Europe/Berlin, Etc/GMT+3 or Local (default ""UTC"")    -loggerWarnsPerSecondLimit int      	Per-second limit on the number of WARN messages. If more than the given number of warns are emitted per second, then the remaining warns are suppressed. Zero values disable the rate limit    -maxConcurrentInserts int      	The maximum number of concurrent inserts. Default value should work for most cases, since it minimizes the overhead for concurrent inserts. This option is tigthly coupled with -insert.maxQueueDuration (default 16)    -maxInsertRequestSize size      	The maximum size in bytes of a single Prometheus remote_write API request      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 33554432)    -maxLabelValueLen int      	The maximum length of label values in the accepted time series. Longer label values are truncated. In this case the vm_too_long_label_values_total metric at /metrics page is incremented (default 16384)    -maxLabelsPerTimeseries int      	The maximum number of labels accepted per time series. Superfluous labels are dropped. In this case the vm_metrics_with_dropped_labels_total metric at /metrics page is incremented (default 30)    -memory.allowedBytes size      	Allowed size of system memory VictoriaMetrics caches may occupy. This option overrides -memory.allowedPercent if set to a non-zero value. Too low a value may increase the cache miss rate usually resulting in higher CPU and disk IO usage. Too high a value may evict too much data from OS page cache resulting in higher disk IO usage      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -memory.allowedPercent float      	Allowed percent of system memory VictoriaMetrics caches may occupy. See also -memory.allowedBytes. Too low a value may increase cache miss rate usually resulting in higher CPU and disk IO usage. Too high a value may evict too much data from OS page cache which will result in higher disk IO usage (default 60)    -metricsAuthKey string      	Auth key for /metrics. It must be passed via authKey query arg. It overrides httpAuth.* settings    -opentsdbHTTPListenAddr string      	TCP address to listen for OpentTSDB HTTP put requests. Usually :4242 must be set. Doesn't work if empty    -opentsdbListenAddr string      	TCP and UDP address to listen for OpentTSDB metrics. Telnet put messages and HTTP /api/put messages are simultaneously served on TCP port. Usually :4242 must be set. Doesn't work if empty    -opentsdbTrimTimestamp duration      	Trim timestamps for OpenTSDB 'telnet put' data to this duration. Minimum practical duration is 1s. Higher duration (i.e. 1m) may be used for reducing disk space usage for timestamp data (default 1s)    -opentsdbhttp.maxInsertRequestSize size      	The maximum size of OpenTSDB HTTP put request      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 33554432)    -opentsdbhttpTrimTimestamp duration      	Trim timestamps for OpenTSDB HTTP data to this duration. Minimum practical duration is 1ms. Higher duration (i.e. 1s) may be used for reducing disk space usage for timestamp data (default 1ms)    -pprofAuthKey string      	Auth key for /debug/pprof. It must be passed via authKey query arg. It overrides httpAuth.* settings    -precisionBits int      	The number of precision bits to store per each value. Lower precision bits improves data compression at the cost of precision loss (default 64)    -promscrape.cluster.memberNum int      	The number of number in the cluster of scrapers. It must be an unique value in the range 0 ... promscrape.cluster.membersCount-1 across scrapers in the cluster    -promscrape.cluster.membersCount int      	The number of members in a cluster of scrapers. Each member must have an unique -promscrape.cluster.memberNum in the range 0 ... promscrape.cluster.membersCount-1 . Each member then scrapes roughly 1/N of all the targets. By default cluster scraping is disabled, i.e. a single scraper scrapes all the targets    -promscrape.cluster.replicationFactor int      	The number of members in the cluster, which scrape the same targets. If the replication factor is greater than 2, then the deduplication must be enabled at remote storage side. See https://docs.victoriametrics.com/#deduplication (default 1)    -promscrape.config string      	Optional path to Prometheus config file with 'scrape_configs' section containing targets to scrape. The path can point to local file and to http url. See https://docs.victoriametrics.com/#how-to-scrape-prometheus-exporters-such-as-node-exporter for details    -promscrape.config.dryRun      	Checks -promscrape.config file for errors and unsupported fields and then exits. Returns non-zero exit code on parsing errors and emits these errors to stderr. See also -promscrape.config.strictParse command-line flag. Pass -loggerLevel=ERROR if you don't need to see info messages in the output.    -promscrape.config.strictParse      	Whether to deny unsupported fields in -promscrape.config . Set to false in order to silently skip unsupported fields (default true)    -promscrape.configCheckInterval duration      	Interval for checking for changes in '-promscrape.config' file. By default the checking is disabled. Send SIGHUP signal in order to force config check for changes    -promscrape.consul.waitTime duration      	Wait time used by Consul service discovery. Default value is used if not set    -promscrape.consulSDCheckInterval duration      	Interval for checking for changes in Consul. This works only if consul_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config for details (default 30s)    -promscrape.digitaloceanSDCheckInterval duration      	Interval for checking for changes in digital ocean. This works only if digitalocean_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#digitalocean_sd_config for details (default 1m0s)    -promscrape.disableCompression      	Whether to disable sending 'Accept-Encoding: gzip' request headers to all the scrape targets. This may reduce CPU usage on scrape targets at the cost of higher network bandwidth utilization. It is possible to set 'disable_compression: true' individually per each 'scrape_config' section in '-promscrape.config' for fine grained control    -promscrape.disableKeepAlive      	Whether to disable HTTP keep-alive connections when scraping all the targets. This may be useful when targets has no support for HTTP keep-alive connection. It is possible to set 'disable_keepalive: true' individually per each 'scrape_config' section in '-promscrape.config' for fine grained control. Note that disabling HTTP keep-alive may increase load on both vmagent and scrape targets    -promscrape.discovery.concurrency int      	The maximum number of concurrent requests to Prometheus autodiscovery API (Consul, Kubernetes, etc.) (default 100)    -promscrape.discovery.concurrentWaitTime duration      	The maximum duration for waiting to perform API requests if more than -promscrape.discovery.concurrency requests are simultaneously performed (default 1m0s)    -promscrape.dnsSDCheckInterval duration      	Interval for checking for changes in dns. This works only if dns_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config for details (default 30s)    -promscrape.dockerSDCheckInterval duration      	Interval for checking for changes in docker. This works only if docker_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#docker_sd_config for details (default 30s)    -promscrape.dockerswarmSDCheckInterval duration      	Interval for checking for changes in dockerswarm. This works only if dockerswarm_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config for details (default 30s)    -promscrape.dropOriginalLabels      	Whether to drop original labels for scrape targets at /targets and /api/v1/targets pages. This may be needed for reducing memory usage when original labels for big number of scrape targets occupy big amounts of memory. Note that this reduces debuggability for improper per-target relabeling configs    -promscrape.ec2SDCheckInterval duration      	Interval for checking for changes in ec2. This works only if ec2_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config for details (default 1m0s)    -promscrape.eurekaSDCheckInterval duration      	Interval for checking for changes in eureka. This works only if eureka_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#eureka_sd_config for details (default 30s)    -promscrape.fileSDCheckInterval duration      	Interval for checking for changes in 'file_sd_config'. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config for details (default 5m0s)    -promscrape.gceSDCheckInterval duration      	Interval for checking for changes in gce. This works only if gce_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#gce_sd_config for details (default 1m0s)    -promscrape.httpSDCheckInterval duration      	Interval for checking for changes in http endpoint service discovery. This works only if http_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#http_sd_config for details (default 1m0s)    -promscrape.kubernetes.apiServerTimeout duration      	How frequently to reload the full state from Kuberntes API server (default 30m0s)    -promscrape.kubernetesSDCheckInterval duration      	Interval for checking for changes in Kubernetes API server. This works only if kubernetes_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config for details (default 30s)    -promscrape.maxDroppedTargets int      	The maximum number of droppedTargets to show at /api/v1/targets page. Increase this value if your setup drops more scrape targets during relabeling and you need investigating labels for all the dropped targets. Note that the increased number of tracked dropped targets may result in increased memory usage (default 1000)    -promscrape.maxResponseHeadersSize size      	The maximum size of http response headers from Prometheus scrape targets      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 4096)    -promscrape.maxScrapeSize size      	The maximum size of scrape response in bytes to process from Prometheus targets. Bigger responses are rejected      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 16777216)    -promscrape.minResponseSizeForStreamParse size      	The minimum target response size for automatic switching to stream parsing mode, which can reduce memory usage. See https://docs.victoriametrics.com/vmagent.html#stream-parsing-mode      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 1000000)    -promscrape.noStaleMarkers      	Whether to disable sending Prometheus stale markers for metrics when scrape target disappears. This option may reduce memory usage if stale markers aren't needed for your setup. This option also disables populating the scrape_series_added metric. See https://prometheus.io/docs/concepts/jobs_instances/#automatically-generated-labels-and-time-series    -promscrape.openstackSDCheckInterval duration      	Interval for checking for changes in openstack API server. This works only if openstack_sd_configs is configured in '-promscrape.config' file. See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#openstack_sd_config for details (default 30s)    -promscrape.seriesLimitPerTarget int      	Optional limit on the number of unique time series a single scrape target can expose. See https://docs.victoriametrics.com/vmagent.html#cardinality-limiter for more info    -promscrape.streamParse      	Whether to enable stream parsing for metrics obtained from scrape targets. This may be useful for reducing memory usage when millions of metrics are exposed per each scrape target. It is posible to set 'stream_parse: true' individually per each 'scrape_config' section in '-promscrape.config' for fine grained control    -promscrape.suppressDuplicateScrapeTargetErrors      	Whether to suppress 'duplicate scrape target' errors; see https://docs.victoriametrics.com/vmagent.html#troubleshooting for details    -promscrape.suppressScrapeErrors      	Whether to suppress scrape errors logging. The last error for each target is always available at '/targets' page even if scrape errors logging is suppressed    -relabelConfig string      	Optional path to a file with relabeling rules, which are applied to all the ingested metrics. The path can point either to local file or to http url. See https://docs.victoriametrics.com/#relabeling for details. The config is reloaded on SIGHUP signal    -relabelDebug      	Whether to log metrics before and after relabeling with -relabelConfig. If the -relabelDebug is enabled, then the metrics aren't sent to storage. This is useful for debugging the relabeling configs    -retentionPeriod value      	Data with timestamps outside the retentionPeriod is automatically deleted      	The following optional suffixes are supported: h (hour), d (day), w (week), y (year). If suffix isn't set, then the duration is counted in months (default 1)    -search.cacheTimestampOffset duration      	The maximum duration since the current time for response data, which is always queried from the original raw data, without using the response cache. Increase this value if you see gaps in responses due to time synchronization issues between VictoriaMetrics and data sources. See also -search.disableAutoCacheReset (default 5m0s)    -search.disableAutoCacheReset      	Whether to disable automatic response cache reset if a sample with timestamp outside -search.cacheTimestampOffset is inserted into VictoriaMetrics    -search.disableCache      	Whether to disable response caching. This may be useful during data backfilling    -search.graphiteMaxPointsPerSeries int      	The maximum number of points per series Graphite render API can return (default 1000000)    -search.graphiteStorageStep duration      	The interval between datapoints stored in the database. It is used at Graphite Render API handler for normalizing the interval between datapoints in case it isn't normalized. It can be overriden by sending 'storage_step' query arg to /render API or by sending the desired interval via 'Storage-Step' http header during querying /render API (default 10s)    -search.latencyOffset duration      	The time when data points become visible in query results after the collection. Too small value can result in incomplete last points for query results (default 30s)    -search.logSlowQueryDuration duration      	Log queries with execution time exceeding this value. Zero disables slow query logging (default 5s)    -search.maxConcurrentRequests int      	The maximum number of concurrent search requests. It shouldn't be high, since a single request can saturate all the CPU cores. See also -search.maxQueueDuration (default 8)    -search.maxExportDuration duration      	The maximum duration for /api/v1/export call (default 720h0m0s)    -search.maxLookback duration      	Synonym to -search.lookback-delta from Prometheus. The value is dynamically detected from interval between time series datapoints if not set. It can be overridden on per-query basis via max_lookback arg. See also '-search.maxStalenessInterval' flag, which has the same meaining due to historical reasons    -search.maxPointsPerTimeseries int      	The maximum points per a single timeseries returned from /api/v1/query_range. This option doesn't limit the number of scanned raw samples in the database. The main purpose of this option is to limit the number of per-series points returned to graphing UI such as Grafana. There is no sense in setting this limit to values bigger than the horizontal resolution of the graph (default 30000)    -search.maxQueryDuration duration      	The maximum duration for query execution (default 30s)    -search.maxQueryLen size      	The maximum search query length in bytes      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 16384)    -search.maxQueueDuration duration      	The maximum time the request waits for execution when -search.maxConcurrentRequests limit is reached; see also -search.maxQueryDuration (default 10s)    -search.maxSamplesPerQuery int      	The maximum number of raw samples a single query can process across all time series. This protects from heavy queries, which select unexpectedly high number of raw samples. See also -search.maxSamplesPerSeries (default 1000000000)    -search.maxSamplesPerSeries int      	The maximum number of raw samples a single query can scan per each time series. This option allows limiting memory usage (default 30000000)    -search.maxStalenessInterval duration      	The maximum interval for staleness calculations. By default it is automatically calculated from the median interval between samples. This flag could be useful for tuning Prometheus data model closer to Influx-style data model. See https://prometheus.io/docs/prometheus/latest/querying/basics/#staleness for details. See also '-search.maxLookback' flag, which has the same meaning due to historical reasons    -search.maxStatusRequestDuration duration      	The maximum duration for /api/v1/status/* requests (default 5m0s)    -search.maxStepForPointsAdjustment duration      	The maximum step when /api/v1/query_range handler adjusts points with timestamps closer than -search.latencyOffset to the current time. The adjustment is needed because such points may contain incomplete data (default 1m0s)    -search.maxTagKeys int      	The maximum number of tag keys returned from /api/v1/labels (default 100000)    -search.maxTagValueSuffixesPerSearch int      	The maximum number of tag value suffixes returned from /metrics/find (default 100000)    -search.maxTagValues int      	The maximum number of tag values returned from /api/v1/label/<label_name>/values (default 100000)    -search.maxUniqueTimeseries int      	The maximum number of unique time series each search can scan. This option allows limiting memory usage (default 300000)    -search.minStalenessInterval duration      	The minimum interval for staleness calculations. This flag could be useful for removing gaps on graphs generated from time series with irregular intervals between samples. See also '-search.maxStalenessInterval'    -search.noStaleMarkers      	Set this flag to true if the database doesn't contain Prometheus stale markers, so there is no need in spending additional CPU time on its handling. Staleness markers may exist only in data obtained from Prometheus scrape targets    -search.queryStats.lastQueriesCount int      	Query stats for /api/v1/status/top_queries is tracked on this number of last queries. Zero value disables query stats tracking (default 20000)    -search.queryStats.minQueryDuration duration      	The minimum duration for queries to track in query stats at /api/v1/status/top_queries. Queries with lower duration are ignored in query stats (default 1ms)    -search.resetCacheAuthKey string      	Optional authKey for resetting rollup cache via /internal/resetRollupResultCache call    -search.treatDotsAsIsInRegexps      	Whether to treat dots as is in regexp label filters used in queries. For example, foo{bar=~""a.b.c""} will be automatically converted to foo{bar=~""a\\.b\\.c""}, i.e. all the dots in regexp filters will be automatically escaped in order to match only dot char instead of matching any char. Dots in "".+"", "".*"" and "".{n}"" regexps aren't escaped. This option is DEPRECATED in favor of {__graphite__=""a.*.c""} syntax for selecting metrics matching the given Graphite metrics filter    -selfScrapeInstance string      	Value for 'instance' label, which is added to self-scraped metrics (default ""self"")    -selfScrapeInterval duration      	Interval for self-scraping own metrics at /metrics page    -selfScrapeJob string      	Value for 'job' label, which is added to self-scraped metrics (default ""victoria-metrics"")    -smallMergeConcurrency int      	The maximum number of CPU cores to use for small merges. Default value is used if set to 0    -snapshotAuthKey string      	authKey, which must be passed in query string to /snapshot* pages    -sortLabels      	Whether to sort labels for incoming samples before writing them to storage. This may be needed for reducing memory usage at storage when the order of labels in incoming samples is random. For example, if m{k1=""v1"",k2=""v2""} may be sent as m{k2=""v2"",k1=""v1""}. Enabled sorting for labels can slow down ingestion performance a bit    -storage.cacheSizeIndexDBDataBlocks size      	Overrides max size for indexdb/dataBlocks cache. See https://docs.victoriametrics.com/Single-server-VictoriaMetrics.html#cache-tuning      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -storage.cacheSizeIndexDBIndexBlocks size      	Overrides max size for indexdb/indexBlocks cache. See https://docs.victoriametrics.com/Single-server-VictoriaMetrics.html#cache-tuning      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -storage.cacheSizeStorageTSID size      	Overrides max size for storage/tsid cache. See https://docs.victoriametrics.com/Single-server-VictoriaMetrics.html#cache-tuning      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 0)    -storage.maxDailySeries int      	The maximum number of unique series can be added to the storage during the last 24 hours. Excess series are logged and dropped. This can be useful for limiting series churn rate. See also -storage.maxHourlySeries    -storage.maxHourlySeries int      	The maximum number of unique series can be added to the storage during the last hour. Excess series are logged and dropped. This can be useful for limiting series cardinality. See also -storage.maxDailySeries    -storage.minFreeDiskSpaceBytes size      	The minimum free disk space at -storageDataPath after which the storage stops accepting new data      	Supports the following optional suffixes for size values: KB, MB, GB, KiB, MiB, GiB (default 10000000)    -storageDataPath string      	Path to storage data (default ""victoria-metrics-data"")    -tls      	Whether to enable TLS (aka HTTPS) for incoming requests. -tlsCertFile and -tlsKeyFile must be set if -tls is set    -tlsCertFile string      	Path to file with TLS certificate. Used only if -tls is set. Prefer ECDSA certs instead of RSA certs as RSA certs are slower. The provided certificate file is automatically re-read every second, so it can be dynamically updated    -tlsKeyFile string      	Path to file with TLS key. Used only if -tls is set. The provided key file is automatically re-read every second, so it can be dynamically updated    -version      	Show VictoriaMetrics version  ``` """
Big data;https://github.com/akumuli/Akumuli;"""README [![Build Status](https://travis-ci.org/akumuli/Akumuli.svg?branch=master)](https://travis-ci.org/akumuli/Akumuli) [![Coverity Scan Build Status](https://scan.coverity.com/projects/8879/badge.svg)](https://scan.coverity.com/projects/akumuli)   ======    **Akumuli** is a time-series database for modern hardware.   It can be used to capture, store and process time-series data in real-time.   The word ""akumuli"" can be translated from Esperanto as ""accumulate"".    Features  -------    * Column-oriented storage.  * Based on novel [LSM and B+tree hybrid datastructure](http://akumuli.org/akumuli/2017/04/29/nbplustree/) with multiversion concurrency control (no concurrency bugs, parallel writes, optimized for SSD and NVMe).  * Supports both metrics and events.  * Fast and effecient compression algorithm that outperforms 'Gorilla' time-series compression.  * Crash safety and recovery.  * Fast aggregation without pre-configured rollups or materialized views.  * Many queries can be executed without decompressing the data.  * Compressed in-memory storage for recent data.  * Can be used as a server application or embedded library.  * Simple API based on JSON and HTTP.  * Fast range scans and joins, read speed doesn't depend on database cardinality.  * Fast data ingestion:    * 5.4M writes/sec on DigitalOcean droplet with 8-cores 32GB of RAM (using only 6 cores)    * 4.6M writes/sec on DigitalOcean droplet with 8-cores 32GB of RAM (6 cores with enabled WAL)    * 16.1M writes/sec on 32-core Intel Xeon E5-2680 v2 (c3.8xlarge EC2 instance).  * Queries are executed lazily. Query results are produced as long as client reads them.  * Compression algorithm and input parsers are fuzz-tested on every code change.  * Grafana [datasource](https://github.com/akumuli/akumuli-datasource) plugin.  * Fast and compact inverted index for time-series lookup.      Roadmap  ------    |Storage engine features        |Current version|Future versions|  |-------------------------------|---------------|---------------|  |Inserts                        |In order       |Out of order   |  |Updates                        |-              |+              |  |Deletes                        |-              |+              |  |MVCC                           |+              |+              |  |Compression                    |+              |+              |  |Tags                           |+              |+              |  |High-throughput ingestion      |+              |+              |  |High cardinality               |+              |+              |  |Crash recovery                 |+              |+              |  |Incremental backup             |-              |+              |  |Clustering                     |-              |+              |  |Replication                    |-              |+              |  |ARM support                    |+              |+              |  |Windows support                |-              |+              |    |Query language features        |Current version|Future versions|  |-------------------------------|---------------|---------------|  |Range scans                    |+              |+              |  |Merge series                   |+              |+              |  |Aggregate series               |+              |+              |  |Merge & aggregate              |+              |+              |  |Group-aggregate                |+              |+              |  |Group-aggregate & merge        |+              |+              |  |Join                           |+              |+              |  |Join & merge                   |-              |+              |  |Join & group-aggregate         |-              |+              |  |Join & group-aggregate & merge |-              |+              |  |Filter by value                |+              |+              |  |Filter & group-aggregate       |+              |+              |  |Filter & join                  |+              |+              |      Gettings Started  ----------------  * You can find [documentation](https://akumuli.gitbook.io/docs) here  * [Installation & build instructions](https://akumuli.gitbook.io/docs/getting-started)  * [Getting started guide](https://akumuli.gitbook.io/docs/getting-started)  * [Writing data](https://akumuli.gitbook.io/docs/writing-data)    Supported Platforms  -------------------    Akumuli supports 64 and 32-bit Intel processors. It also works on 64 and 32-bit ARM processors but these architectures are not covered by continous integration.    Pre-built [Debian/RPM packages](https://packagecloud.io/Lazin/Akumuli) for the following platforms  are available via packagecloud:    * AMD 64 Ubuntu 14.04  * AMD 64 Ubuntu 16.04  * AMD 64 Ubuntu 18.04  * AMD 64 Debian Jessie  * AMD 64 Debian Stretch  * AMD 64 CentOS 7  * ARM 64 Ubuntu 16.04  * ARM 64 Ubuntu 18.04  * ARM 64 CentOS 7    Docker image is availabe through [Docker Hub](https://hub.docker.com/r/akumuli/akumuli/tags/).    Tools for monitoring  --------------------    Akumuli supports OpenTSDB telnet-style API for writing. This means that many collectors works with it  without any trouble, for instance `netdata`, `collectd`, and `tcollector`. Grafana  [datasource](https://github.com/akumuli/akumuli-datasource) plugin is availabe as well.  Akumuli can be used as a long-term storage for Prometheus using [akumuli-prometheus-adapter](https://github.com/akumuli/akumuli-prometheus-adapter).    [Google group](https://groups.google.com/forum/#!forum/akumuli) """
Big data;https://github.com/adobe-research/spindle;"""# Spindle    **Spindle is [Brandon Amos'](http://github.com/bamos)  2014 summer internship project with Adobe Research  and is not under active development.**    ---    ![](https://github.com/adobe-research/spindle/raw/master/images/architecture.png)    Analytics platforms such as [Adobe Analytics][adobe-analytics]  are growing to process petabytes of data in real-time.  Delivering responsive interfaces querying this amount of data is difficult,  and there are many distributed data processing technologies such  as [Hadoop MapReduce][mapreduce], [Apache Spark][spark],  [Apache Drill][drill], and [Cloudera Impala][impala]  to build low-latency query systems.    Spark is part of the [Apache Software Foundation][apache]  and claims speedups up to 100x faster than Hadoop for in-memory  processing.  Spark is shifting from a research project to a production-ready library,  and academic publications and presentations from  the [2014 Spark Summit][2014-spark-summit]  archives several use cases of Spark and related technology.  For example,  [NBC Universal][nbc] presents their use of Spark to query [HBase][hbase]  tables and analyze an international cable TV video distribution [here][nbc-pres].  Telefonica presents their use of  Spark with [Cassandra][cassandra]  for cyber security analytics [here][telefonica-pres].  [ADAM][adam] is an open source data storage format and processing  pipeline for genomics data built in Spark and [Parquet][parquet].    Even though people are publishing use cases of Spark,  few people have published  experiences of building and tuning production-ready Spark systems.  Thorough knowledge of Spark internals  and libraries that interoperate well with Spark is necessary  to achieve optimal performance from Spark applications.    **Spindle is a prototype Spark-based web analytics query engine designed  around the requirements of production workloads.**  Spindle exposes query requests through a multi-threaded  HTTP interface implemented with [Spray][spray].  Queries are processed by loading data from [Apache Parquet][parquet] columnar  storage format on the  [Hadoop distributed filesystem][hdfs].    This repo contains the Spindle implementation and benchmarking scripts  to observe Spindle's performance while exploring Spark's tuning options.  Spindle's goal is to process petabytes of data on thousands of nodes,  but the current implementation has not yet been tested at this scale.  Our current experimental results use six nodes,  each with 24 cores and 21g of Spark memory, to query 13.1GB of analytics data.  The trends show that further Spark tuning and optimizations should  be investigated before attempting larger scale deployments.    # Demo  We used Spindle to generate static webpages that are hosted  statically [here][demo].  Unfortunately, the demo is only for illustrative purposes and  is not running Spindle in real-time.    ![](https://github.com/adobe-research/spindle/raw/master/images/top-pages-by-browser.png)  ![](https://github.com/adobe-research/spindle/raw/master/images/adhoc.png)    [Grunt][grunt] is used to deploy `demo` to [Github pages][ghp]  in the [gh-pages][ghp] branch with the [grunt-build-control][gbc] plugin.  The [npm][npm] dependencies are managed in [package.json][pjson]  and can be installed with `npm install`.    # Loading Sample Data  The `load-sample-data` directory contains a Scala program  to load the following sample data into [HDFS][hdfs]  modeled after  [adobe-research/spark-parquet-thrift-example][spark-parquet-thrift-example].  See [adobe-research/spark-parquet-thrift-example][spark-parquet-thrift-example]  for more information on running this application  with [adobe-research/spark-cluster-deployment][spark-cluster-deployment].    ### hdfs://hdfs_server_address:8020/spindle-sample-data/2014-08-14  | post_pagename | user_agent | visit_referrer | post_visid_high | post_visid_low | visit_num | hit_time_gmt | post_purchaseid | post_product_list | first_hit_referrer |  |---|---|---|---|---|---|---|---|---|---|  | Page A | Chrome | http://facebook.com | 111 | 111 | 1 | 1408007374 | | | http://google.com  | Page B | Chrome | http://facebook.com | 111 | 111 | 1 | 1408007377 | | | http://google.com  | Page C | Chrome | http://facebook.com | 111 | 111 | 1 | 1408007380 | purchase1 | ;ProductID1;1;40;,;ProductID2;1;20; | http://google.com  | Page B | Chrome | http://google.com | 222 | 222 | 1 | 1408007379 | | | http://google.com  | Page C | Chrome | http://google.com | 222 | 222 | 1 | 1408007381 | | | http://google.com  | Page A | Firefox | http://google.com | 222 | 222 | 1 | 1408007382 | | | http://google.com  | Page A | Safari | http://google.com | 333 | 333 | 1 | 1408007383 | | | http://facebook.com  | Page B | Safari | http://google.com | 333 | 333 | 1 | 1408007386 | | | http://facebook.com    ### hdfs://hdfs_server_address:8020/spindle-sample-data/2014-08-15  | post_pagename | user_agent | visit_referrer | post_visid_high | post_visid_low | visit_num | hit_time_gmt | post_purchaseid | post_product_list | first_hit_referrer |  |---|---|---|---|---|---|---|---|---|---|  | Page A | Chrome | http://facebook.com | 111 | 111 | 1 | 1408097374 | | | http://google.com  | Page B | Chrome | http://facebook.com | 111 | 111 | 1 | 1408097377 | | | http://google.com  | Page C | Chrome | http://facebook.com | 111 | 111 | 1 | 1408097380 | purchase1 | ;ProductID1;1;60;,;ProductID2;1;100; | http://google.com  | Page B | Chrome | http://google.com | 222 | 222 | 1 | 1408097379 | | | http://google.com  | Page A | Safari | http://google.com | 333 | 333 | 1 | 1408097383 | | | http://facebook.com  | Page B | Safari | http://google.com | 333 | 333 | 1 | 1408097386 | | | http://facebook.com    ### hdfs://hdfs_server_address:8020/spindle-sample-data/2014-08-16  | post_pagename | user_agent | visit_referrer | post_visid_high | post_visid_low | visit_num | hit_time_gmt | post_purchaseid | post_product_list | first_hit_referrer |  |---|---|---|---|---|---|---|---|---|---|  | Page A | Chrome | http://facebook.com | 111 | 111 | 1 | 1408187380 | purchase1 | ;ProductID1;1;60;,;ProductID2;1;100; | http://google.com  | Page B | Chrome | http://facebook.com | 111 | 111 | 1 | 1408187380 | purchase1 | ;ProductID1;1;200; | http://google.com  | Page D | Chrome | http://google.com | 222 | 222 | 1 | 1408187379 | | | http://google.com  | Page A | Safari | http://google.com | 333 | 333 | 1 | 1408187383 | | | http://facebook.com  | Page B | Safari | http://google.com | 333 | 333 | 1 | 1408187386 | | | http://facebook.com  | Page C | Safari | http://google.com | 333 | 333 | 1 | 1408187388 | | | http://facebook.com    # Queries.  Spindle includes eight queries that are representative of  the data sets and computations of real queries the  Adobe Marketing Cloud processes.  All collect statements refer to the combined filter and map operation,  not the operation to gather an RDD as a local Scala object.    + *Q0* (**Pageviews**)    is a breakdown of the number of pages viewed    each day in the specified range.  + *Q1* (**Revenue**) is the overall revenue for each day in    the specified range.  + *Q2* (**RevenueFromTopReferringDomains**) obtains the top referring    domains for each visit and breaks down the revenue by day.    The `visit_referrer` field is preprocessed into each record in    the raw data.  + *Q3* (**RevenueFromTopReferringDomainsFirstVisitGoogle**) is    the same as RevenueFromTopReferringDomains, but with the    visitor's absolute first referrer from Google.    The `first_hit_referrer` field is preprocessed into each record in    the raw data.  + *Q4* (**TopPages**) is a breakdown of the top pages for the    entire date range, not per day.  + *Q5* (**TopPagesByBrowser**) is a breakdown of the browsers    used for TopPages.  + *Q6* (**TopPagesByPreviousTopPages**) breaks down the top previous    pages a visitor was at for TopPages.  + *Q7* (**TopReferringDomains**) is the top referring domains for    the entire date range, not per day.    The following table shows the columnar subset  each query utilizes.    ![](https://github.com/adobe-research/spindle/raw/master/images/columns-needed.png)    The following table shows the operations each query performs  and is intended as a summary rather than full description of  the implementations.  The bold text in indicate operations in which the target  partition size is specified, which is further described in the  ""Partitioning"" section below.    ![](https://github.com/adobe-research/spindle/raw/master/images/query-operations.png)    # Spindle Architecture  The query engine provides a request and response interface to  interact with the application layer, and Spindle's goal is to  benchmark a realistic low latency web analytics query engine.    Spindle provides query requests and reports over HTTP with the  [Spray][spray] library, which is multi-threaded and provides  REST/HTTP-based integration layer on Scala for queries and parameters,  as illustrated in the figure below.    ![](https://github.com/adobe-research/spindle/raw/master/images/architecture.png)    When a user request to execute a query over HTTP,  Spray allocates a thread to process the HTTP request and converts  it into a Spray request.  The Spray request follows a route defined in the `QueryService` Actor,  and queries are processed with the `QueryProcessor` singleton object.  The `QueryProcessor` interacts with a global Spark context,  which connects the Scala application to the Spark cluster.    The Spark context supports multi-threading and offers a  `FIFO` and `FAIR` scheduling options for concurrent queries.  Spindle uses Spark's `FAIR` scheduling option to minimize overall latency.    ## Future Work - Utilizing Spark job servers or resource managers.  Spindle's architecture can likely be improved on larger clusters by  utilizing a job server or resource manager to  maintain a pool of Spark contexts for query execution.  [Ooyala's spark-jobserver][spark-jobserver] provides  a RESTful interface for submitting Spark jobs that Spindle could  interface with instead of interfacing with Spark directly.  [YARN][yarn] can also be used to manage Spark's  resources on a cluster, as described in [this article][spark-yarn].    However, allocating resources on the cluster raises additional  questions and engineering work that Spindle can address in future work.  Spindle's current architecture coincides HDFS and Spark workers  on the same nodes, minimizing the network traffic required  to load data.  How much will the performance degrade if the resource manager  allocates some subset of Spark workers that don't  coincide with any of the HDFS data being accessed?    Furthermore, how would a production-ready caching policy  on a pool of Spark Contexts look?  What if many queries are being submitted and executed on  different Spark Contexts that use the same data?  Scheduling the queries on the same Spark Context and  caching the data between query executions would substantially  increase the performance, but how should the scheduler  be informed of this information?    ## Data Format  Adobe Analytics events data have at least 250 columns,  and sometimes significantly more than 250 columns.  Most queries use less than 7 columns, and loading all of the  columns into memory to only use 7 is inefficient.  Spindle stores event data in the [Parquet][parquet] columnar store  on the [Hadoop Distributed File System][hdfs] (HDFS) with  [Kryo][kryo] serialization enabled  to only load the subsets of columns each query requires.    [Cassandra][cassandra] is a NoSQL database that we considered  as an alternate to Parquet.  However, Spindle also utilizes [Spark SQL][spark-sql],  which supports Parquet, but not Cassandra.    Parquet can be used with [Avro][avro] or [Thrift][thrift] schemas.  [Matt Massie's article][spark-parquet-avro] provides an example of  using Parquet with Avro.  [adobe-research/spark-parquet-thrift-example][spark-parquet-thrift-example]  is a complete [Scala][scala]/[sbt][sbt] project  using Thrift for data serialization and shows how to only load the  specified columnar subset.  For a more detailed introduction to Thrift,  see [Thrift: The Missing Guide][thrift-guide].    The entire Adobe Analytics schema cannot be published.  The open source release of Spindle uses  [AnalyticsData.thrift][AnalyticsData.thrift],  which contains 10 non-proprietary fields for web analytics.    Columns postprocessed into the data after collection have the `post_`  prefix along with `visit_referrer` and `first_hit_referrer`.  Visitors are categorized by concatenating the strings  `post_visid_high` and `post_visid_low`.  A visitor has visits which are numbered by `visit_num`,  and a visit has hits that occur at `hit_time_gmt`.  If the hit is a webpage hit from a browser, the `post_pagename` and  `user_agent` fields are used, and the revenue from a hit,  is denoted in `post_purchaseid` and `post_product_list`.    ```Thrift  struct AnalyticsData {    1: string post_pagename;    2: string user_agent;    3: string visit_referrer;    4: string post_visid_high;    5: string post_visid_low;    6: string visit_num;    7: string hit_time_gmt;    8: string post_purchaseid;    9: string post_product_list;    10: string first_hit_referrer;  }  ```    This data is separated by day on disk of format `YYYY-MM-DD`.    ## Caching Data  Spindle provides a caching option that will cache the loaded  Spark data in memory between query requests to show the  maximum speedup caching provides.  Caching introduces a number of interesting questions when dealing  with sparse data.  For example, two queries could be submitted on the same date range  that request overlapping, but not identical, column subsets.  How should these data sets with partially overlapping values be  cached in the application?  What if one of the queries is called substantially more times than  the other? How should the caching policy ensure these columns are  not evicted?  We will explore these questions in future work.    ## Partitioning  Spark affords partitioning data across nodes for operations  such as `distinct`, `reduceByKey`, and `groupByKey` to specify the  minimum number of resulting partitions.    Counting the number of records in an RDD  expensive, and automatically knowing the optimal number of partitions  for operations depends highly on the data and operations.  For optimal partitioning, applications should estimate the  number of records to process and ensure the partitions contain  some minimum value of records.    Spindle puts a target number of records in each partition  by estimating the total number of records to be processed  from Parquet's metadata.  However, most queries filter records before doing operations that  impact the partitioning by approximately 50\% in our data.  For example, an empty `post_pagename` field indicates that the  analytics hit is from an event other than a user visiting a page,  and the first Spark operation in TopPages is to obtain only  the page visit hits by filtering out records with empty `post_pagename`  fields.    # Installing Spark and HDFS on a cluster.  | ![](https://github.com/adobe-research/spark-cluster-deployment/raw/master/images/initial-deployment-2.png) | ![](https://github.com/adobe-research/spark-cluster-deployment/raw/master/images/application-deployment-1.png) |  |---|---|    Spark 1.0.0 can be deployed to traditional cloud and job management services  such as [EC2][spark-ec2], [Mesos][spark-mesos], or  [Yarn][spark-yarn].  Further, Spark's [standalone cluster][spark-standalone] mode enables  Spark to run on other servers without installing other  job management services.    However, configuring and submitting applications to a Spark 1.0.0 standalone  cluster currently requires files to be synchronized across the entire cluster,  including the Spark installation directory.  These problems have motivated our  [adobe-research/spark-cluster-deployment][spark-cluster-deployment] project,  which utilizes [Fabric][fabric] and [Puppet][puppet] to further automate  the Spark standalone cluster.    # Building    Ensure you have the following software on the server.  Spindle has been developed on CentOS 6.5 with  sbt 0.13.5, Spark 1.0.0, Hadoop 2.0.0-cdh4.7.0,  and parquet-thrift 1.5.0.    | Command | Output |  |---|---|  | `cat /etc/centos-release` | CentOS release 6.5 (Final) |  | `sbt --version` | sbt launcher version 0.13.5 |  | `thrift --version` | Thrift version 0.9.1 |  | `hadoop version` | Hadoop 2.0.0-cdh4.7.0 |  | `cat /usr/lib/spark/RELEASE` | Spark 1.0.0 built for Hadoop 2.0.0-cdh4.7.0 |    Spindle uses [sbt][sbt] and the [sbt-assembly][sbt-assembly] plugin  to build Spark into a fat JAR to be deployed to the Spark cluster.  Using [adobe-research/spark-cluster-deployment][spark-cluster-deployment],  modify `config.yaml` to have your server configurations,  and build the application with `ss-a`, send the JAR to your cluster  with `ss-sy`, and start Spindle with `ss-st`.    # Experimental Results  All experiments leverage a homogeneous six node production cluster  of HP ProLiant DL360p Gen8 blades.  Each node has 32GB of DDR3 memory at 1333MHz,  (2) 6 core Intel Xeon 0 processors at 2.30GHz and 1066MHz FSB,  and (10) 15K SAS 146GB, RAID 5 hard disks.  Furthermore, each node has CentOS 6.5, Hadoop 2.0.0-cdh4.7.0,  Spark 1.0.0, sbt 0.13.5, and Thrift 0.9.1.  The Spark workers each utilizes 21g of memory.    These experiments benchmark Spindle's queries  on a week's worth of data consuming 13.1G as serialized Thrift objects  in Parquet.    The YAML formatted results, scripts, and resulting figures  are in the [benchmark-scripts][benchmark-scripts] directory.    ## Scaling HDFS and Spark workers.  Predicting the optimal resource allocation to minimize query latency for  distributed applications is difficult. No production software can accurately  predict the optimal number of Spark and HDFS nodes for a given application.  This experiment observes the execution time of queries as the number of Spark  and HDFS workers is increased. We manually scale and rebalance the HDFS data.    The following figure shows the time to load all columns the queries  use for the week of data as the Spark and HDFS workers are scaled. The data is  loaded by caching the Spark RDD and performing a null operation on them, such  as `rdd.cache.foreach{x =>{}}`. The downward trend of the data load times  indicate that using more Spark or HDFS workers will decrease the time to load  data.    ![](https://raw.githubusercontent.com/adobe-research/spindle/master/benchmark-scripts/scaling/dataLoad.png)    The following table and plot show the execution time of the queries  with cached data when scaling the HDFS and Spark workers.  The bold data indicates where adding a  Spark and HDFS worker hurts performance. The surprising results show that  adding a single Spark or HDFS worker commonly hurts query performance, and  interestingly, no query experiences minimal execution time when using all 6  workers. Our future work is to further experiment by tuning Spark to understand  the performance degradation, which might be caused by network traffic or  imbalanced workloads.    Q2 and Q3 are similar queries and consequently have similar performance as  scaling the Spark and HDFS workers, but has an anomaly when using 3 workers  where Q2 executes in 17.10s and Q3 executes in 55.15s. Q6’s execution time  increases by 10.67 seconds between three and six Spark and HDFS workers.    ![](https://github.com/adobe-research/spindle/raw/master/images/scaling-spark-hdfs.png)  ![](https://raw.githubusercontent.com/adobe-research/spindle/master/benchmark-scripts/scaling/scalingWorkers.png)    ## Intermediate data partitioning.  Spark cannot optimize the number of records in the partitions  because counting the number of records in the initial and  intermediate data sets is expensive, and the  Spark application has to provide the number of partitions  to use for certain computations.  This experiment fully utilizes all six nodes with Spark (144 cores)  and HDFS workers.    Averaging four execution times for each point between  10,000 and 1,500,000 target partition sizes for every query  results in similar performance to the TopPages query (Q4) shown below.    ![](https://github.com/adobe-research/spindle/raw/master/benchmark-scripts/partitions/png/TopPages.png)    Targeting 10,000 records per partition results in poor performance,  which we suspect is due to the Spark overhead of creating an execution  environment for the task, and the performance monotonically decreases  and levels off at a target partition size of 1,500,000.  This experiment fully utilizes all six nodes with Spark (144 cores)  and HDFS workers.    The table below summarizes the results from all queries  by showing the best average execution times for all partitions  and the execution time at a target partition size of 1,500,000.  Q2 and Q3 have nearly identical performance because Q3  only adds a filter to Q2.    | Query | Best Execution Time (s) | Final Execution Time (s) |  |---|---|---|  | TopPages | 3.31 | 3.37 |  | TopPagesByBrowser | 15.41 | 15.58 |  | TopPagesByPreviousTopPages | 34.70 | 36.89 |  | TopReferringDomains | 5.68 | 5.68 |  | RevenueFromTopReferringDomains | 16.66 | 16.661 |  | RevenueFromTopReferringDomainsFirstVisitGoogle | 16.89 | 16.89 |    The remaining experiments use a target partition size of 1,500,000,  and the performance is the best observed for the operations with partitioning.  We expect the support for specifying partitioning for  loading Parquet data from HDFS will yield further performance results.    ## Impact of caching on query execution time.  This experiment shows the ideal speedups from having  all the data in memory as RDD's.  Furthermore, the performances from caching in this experiment  are better than the performances from caching the raw data in memory because  the RDD is cached, and the time to load raw data  into a RDD is non-negligible.    The figure below shows the average execution times from four trials  of every query with and without caching.  Caching the data substantially improves performance, but  reveals that Spindle has further performance bottlenecks inhibiting  subsecond query execution time.  These bottlenecks can be partially overcome by preprocessing the data  and further analyzing Spark internals.  ![](https://github.com/adobe-research/spindle/raw/master/benchmark-scripts/caching/caching.png)    ## Query execution time for concurrent queries.  Spindle's can process concurrent queries with multi-threading, since  many users will use the analytics application concurrently.  Users will request different queries concurrently,  but for simplicity, this experiment shows the performance  degradation as the same query is called with an increasing  number of threads with in-memory caching.    This experiment will spawn a number of threads which continuously  execute the same query.  Each thread remains loaded and continues processing  queries until all threads have processed four queries,  and the average execution time of the first four queries  from every thread will be used as a metric to estimate the  slowdowns.    The performance of the TopPages query below  is indicative of the performance of most queries.  TopPages appears to underutilize the Spark system when  processing in serial, and the Spark schedule is able to process  two queries concurrently and return them as a factor of 1.32 of  the original execution time.    ![](https://github.com/adobe-research/spindle/raw/master/benchmark-scripts/concurrent/png/TopPages.png)    The slowdown factors from serial execution are shown in  the table below for two and eight concurrent queries.    | Query | Serial Time (ms) | 2 Concurrent Slowdown | 8 Concurrent Slowdown |  |---|---|---|---|  | Pageviews | 2.70 | 1.63 | 5.98 |  | TopPages | 3.37 | 1.32 | 5.66 |  | TopPagesByBrowser | 15.93 | 2.02 | 7.58 |  | TopPagesByPreviousTopPages | 37.49 | 1.24 | 4.15 |  | Revenue | 2.74 | 1.53 | 5.82 |  | TopReferringDomains | 5.75 | 1.19 | 4.45 |  | RevenueFromTopReferringDomains | 17.79 | 1.55 | 5.91 |  | RevenueFromTopReferringDomainsFirstVisitGoogle | 16.35 | 1.68 | 7.29 |    This experiment shows the ability of Spark's scheduler at the  small scale of six nodes.  The slowdowns for two concurrent queries indicate further query optimizations  could better balance the work between all Spark workers and  likely result in better query execution time.    # Contributing and Development Status  Spindle is not currently under active development by Adobe.  However, we are happy to review and respond to issues,  questions, and pull requests.    # License  Bundled applications are copyright their respective owners.  [Twitter Bootstrap][bootstrap] and  [dangrossman/bootstrap-daterangepicker][bootstrap-daterangepicker]  are Apache 2.0 licensed  and [rlamana/Terminus][terminus] is MIT licensed.  Diagrams are available in the public domain from  [bamos/beamer-snippets][beamer-snippets].    All other portions are copyright 2014 Adobe Systems Incorporated  under the Apache 2 license, and a copy is provided in `LICENSE`.    [adobe-analytics]: http://www.adobe.com/solutions/digital-analytics.html    [mapreduce]: http://wiki.apache.org/hadoop/MapReduce  [drill]: http://incubator.apache.org/drill/  [impala]: http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html  [spark]: http://spark.apache.org/  [spark-sql]: https://spark.apache.org/sql/  [spark-ec2]: http://spark.apache.org/docs/1.0.0/ec2-scripts.html  [spark-mesos]: http://spark.apache.org/docs/1.0.0/running-on-mesos.html  [spark-yarn]: http://spark.apache.org/docs/1.0.0/running-on-yarn.html  [spark-standalone]: http://spark.apache.org/docs/1.0.0/spark-standalone.html    [apache]: http://www.apache.org/  [hbase]: http://hbase.apache.org/  [cassandra]: http://cassandra.apache.org  [parquet]: http://parquet.io/  [hdfs]: http://hadoop.apache.org/  [thrift]: https://thrift.apache.org/  [thrift-guide]: http://diwakergupta.github.io/thrift-missing-guide/  [avro]: http://avro.apache.org/  [spark-parquet-avro]: http://zenfractal.com/2013/08/21/a-powerful-big-data-trio/  [spray]: http://spray.io  [kryo]: https://github.com/EsotericSoftware/kryo  [fabric]: http://www.fabfile.org/  [puppet]: http://puppetlabs.com/puppet/puppet-open-source    [2014-spark-summit]: http://spark-summit.org/2014  [nbc]: http://www.nbcuni.com/  [nbc-pres]: http://spark-summit.org/wp-content/uploads/2014/06/Using-Spark-to-Generate-Analytics-for-International-Cable-TV-Video-Distribution-Christopher-Burdorf.pdf  [telefonica-pres]: http://spark-summit.org/wp-content/uploads/2014/07/Spark-use-case-at-Telefonica-CBS-Fran-Gomez.pdf  [adam]: https://github.com/bigdatagenomics/adam    [grunt]: http://gruntjs.com/  [ghp]: https://pages.github.com/  [gbc]: https://github.com/robwierzbowski/grunt-build-control  [npm]: https://www.npmjs.org/    [scala]: http://scala-lang.org  [rdd]: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD    [sbt]: http://www.scala-sbt.org/  [sbt-thrift]: https://github.com/bigtoast/sbt-thrift  [sbt-assembly]: https://github.com/sbt/sbt-assembly    [pjson]: https://github.com/adobe-research/spindle/blob/master/package.json  [AnalyticsData.thrift]: https://github.com/adobe-research/spindle/blob/master/src/main/thrift/AnalyticsData.thrift  [benchmark-scipts]: https://github.com/adobe-research/spindle/tree/master/benchmark-scripts    [demo]: http://adobe-research.github.io/spindle/  [spark-parquet-thrift-example]: https://github.com/adobe-research/spark-parquet-thrift-example  [spark-cluster-deployment]: https://github.com/adobe-research/spark-cluster-deployment    [bootstrap]: http://getbootstrap.com/  [terminus]: https://github.com/rlamana/Terminus  [beamer-snippets]: https://github.com/bamos/beamer-snippets  [bootstrap-daterangepicker]: https://github.com/dangrossman/bootstrap-daterangepicker    [spark-jobserver]: https://github.com/ooyala/spark-jobserver  [yarn]: http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html  [spark-yarn]: http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/ """
Big data;https://github.com/apache/incubator-slider;"""<!---     Licensed to the Apache Software Foundation (ASF) under one or more     contributor license agreements.  See the NOTICE file distributed with     this work for additional information regarding copyright ownership.     The ASF licenses this file to You under the Apache License, Version 2.0     (the ""License""); you may not use this file except in compliance with     the License.  You may obtain a copy of the License at           http://www.apache.org/licenses/LICENSE-2.0       Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an ""AS IS"" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.  -->    # Slider      Slider is a YARN application to deploy existing distributed applications on YARN,   monitor them and make them larger or smaller as desired -even while   the cluster is running.    Clusters can be stopped and restarted later; the distribution  of the deployed application across the YARN cluster is persisted -enabling  a best-effort placement close to the previous locations on a cluster start.  Applications which remember the previous placement of data (such as HBase)  can exhibit fast start-up times from this feature.    YARN itself monitors the health of 'YARN containers"" hosting parts of   the deployed application -it notifies the Slider manager application of container  failure. Slider then asks YARN for a new container, into which Slider deploys  a replacement for the failed component. As a result, Slider can keep the  size of managed applications consistent with the specified configuration, even  in the face of failures of servers in the cluster -as well as parts of the  application itself    ## Open-Source Development    Apache Slider is an effort undergoing incubation at The Apache Software  Foundation (ASF), sponsored by Apache Incubator.     Incubation is required of all newly accepted projects until a further review  indicates that the infrastructure, communications, and decision making process  have stabilized in a manner consistent with other successful ASF projects.  While incubation status is not necessarily a reflection of the completeness  or stability of the code, it does indicate that the project has yet  to be fully endorsed by the ASF.    ### Mailing Lists    We have a single mailing list for developers and users of Slider: dev@slider.incubator.apache.org    1. You can subscribe to this by emailing dev-subscribe@slider.incubator.apache.org from the  email account to which you wish to subscribe from -and follow the instructions returned.  1. You can unsubscribe later by emailing dev-unsubscribe@slider.incubator.apache.org    There is a mailing list of every commit to the source code repository, commits@slider.incubator.apache.org.  This is generally only of interest to active developers.      ### Bug reports    Bug reports and other issues can be filed on the [Apache Jira](https://issues.apache.org/jira/) server.  Please use the SLIDER project for filing the issues.    ### Source code access    Read-only:    *  [https://git.apache.org/repos/asf/incubator-slider.git](https://git.apache.org/repos/asf/incubator-slider.git)  *  [https://github.com/apache/incubator-slider.git](https://github.com/apache/incubator-slider.git)    Read-write (for committers):    *  [https://git-wip-us.apache.org/repos/asf/incubator-slider.git](https://git-wip-us.apache.org/repos/asf/incubator-slider.git)        # License          Licensed under the Apache License, Version 2.0 (the ""License"");      you may not use this file except in compliance with the License.      You may obtain a copy of the License at             (http://www.apache.org/licenses/LICENSE-2.0)            Unless required by applicable law or agreed to in writing, software      distributed under the License is distributed on an ""AS IS"" BASIS,      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.      See the License for the specific language governing permissions and      limitations under the License. See accompanying LICENSE file.    # Export Control    This distribution includes cryptographic software. The country in which you  currently reside may have restrictions on the import, possession, use, and/or  re-export to another country, of encryption software. BEFORE using any  encryption software, please check your country's laws, regulations and  policies concerning the import, possession, or use, and re-export of encryption  software, to see if this is permitted. See <http://www.wassenaar.org/> for more  information.    The U.S. Government Department of Commerce, Bureau of Industry and Security  (BIS), has classified this software as Export Commodity Control Number (ECCN)  5D002.C.1, which includes information security software using or performing  cryptographic functions with asymmetric algorithms. The form and manner of this  Apache Software Foundation distribution makes it eligible for export under the  License Exception ENC Technology Software Unrestricted (TSU) exception (see the  BIS Export Administration Regulations, Section 740.13) for both object code and  source code.    The following provides more details on the included cryptographic software:    Apache Slider uses the built-in java cryptography libraries. See Oracle's  information regarding Java cryptographic export regulations for more details:  http://www.oracle.com/us/products/export/export-regulations-345813.html    Apache Slider uses the SSL libraries from the Jetty project distributed by the  Eclipse Foundation (http://eclipse.org/jetty). """
Big data;https://github.com/twitter/twemcache;"""# Twemcache: Twitter Memcached     [![status: retired](https://opensource.twitter.dev/status/retired.svg)](https://opensource.twitter.dev/status/#retired)  [![Build Status](https://secure.travis-ci.org/twitter/twemcache.png)](http://travis-ci.org/twitter/twemcache)    Twemcache is no longer actively maintained.  See [twitter/pelikan](https://github.com/twitter/pelikan) for our latest caching work.    Twemcache (pronounced ""tw-em-cache"") is the Twitter Memcached. Twemcache is based on a fork of [Memcached](http://memcached.org/) v.1.4.4 that has been heavily modified to make to suitable for the large scale production environment at Twitter.    ## Build    To build twemcache from distribution tarball:        $ ./configure      $ make      $ sudo make install    To build twemcache from distribution tarball with a non-standard path to [libevent](http://libevent.org) install:        $ ./configure --with-libevent=<path>      $ make      $ sudo make install    To build twemcache from distribution tarball with a statically linked libevent:        $ ./configure --enable-static=libevent      $ make      $ sudo make install    To build twemcache from distribution tarball in _debug mode_ with _assertion panics enabled_:        $ CFLAGS=""-ggdb3 -O0"" ./configure --enable-debug=full      $ make      $ sudo make install    To build twemcache from source with _debug logs enabled_ and _assertions disabled_:        $ git clone git@github.com:twitter/twemcache.git      $ cd twemcache      $ autoreconf -fvi      $ ./configure --enable-debug=log      $ make V=1      $ src/twemcache -h    ## Help        Usage: twemcache [-?hVCELdkrDS] [-o output file] [-v verbosity level]                 [-A stats aggr interval]                 [-t threads] [-P pid file] [-u user]                 [-x command logging entry] [-X command logging file]                 [-R max requests] [-c max conns] [-b backlog] [-p port] [-U udp port]                 [-l interface] [-s unix path] [-a access mask] [-M eviction strategy]                 [-f factor] [-m max memory] [-n min item chunk size] [-I slab size]                 [-z slab profile]        Options:        -h, --help                  : this help        -V, --version               : show version and exit        -E, --prealloc              : preallocate memory for all slabs        -L, --use-large-pages       : use large pages if available        -k, --lock-pages            : lock all pages and preallocate slab memory        -d, --daemonize             : run as a daemon        -r, --maximize-core-limit   : maximize core file limit        -C, --disable-cas           : disable use of cas        -D, --describe-stats        : print stats description and exit        -S, --show-sizes            : print slab and item struct sizes and exit        -o, --output=S              : set the logging file (default: stderr)        -v, --verbosity=N           : set the logging level (default: 5, min: 0, max: 11)        -A, --stats-aggr-interval=N : set the stats aggregation interval in usec (default: 100000 usec)        -t, --threads=N             : set number of threads to use (default: 4)        -P, --pidfile=S             : set the pid file (default: off)        -u, --user=S                : set user identity when run as root (default: off)        -x, --klog-entry=N          : set the command logging entry number per thread (default: 512)        -X, --klog-file=S           : set the command logging file (default: off)        -R, --max-requests=N        : set the maximum number of requests per event (default: 20)        -c, --max-conns=N           : set the maximum simultaneous connections (default: 1024)        -b, --backlog=N             : set the backlog queue limit (default 1024)        -p, --port=N                : set the tcp port to listen on (default: 11211)        -U, --udp-port=N            : set the udp port to listen on (default: 11211)        -l, --interface=S           : set the interface to listen on (default: all)        -s, --unix-path=S           : set the unix socket path to listen on (default: off)        -a, --access-mask=O         : set the access mask for unix socket in octal (default: 0700)        -M, --eviction-strategy=N   : set the eviction strategy on OOM (default: 2, random)        -f, --factor=D              : set the growth factor of slab item sizes (default: 1.25)        -m, --max-memory=N          : set the maximum memory to use for all items in MB (default: 64 MB)        -n, --min-item-chunk-size=N : set the minimum item chunk size in bytes (default: 72 bytes)        -I, --slab-size=N           : set slab size in bytes (default: 1048576 bytes)        -z, --slab-profile=S        : set the profile of slab item chunk sizes (default: off)    ## Features    * Supports the complete memcached ASCII protocol.  * Supports tcp, udp and unix domain sockets.  * Observability through lock-less stats collection and klogger.  * Pluggable eviction strategies.  * Easy debuggability through assertions and logging.    ## Slabs and Items    Memory in twemcache is organized into fixed sized slabs whose size is configured using the -I or --slab-size=N command-line argument. Every slab is carved into a collection of contiguous, equal size items. All slabs that are carved into items of a given size belong to a given slabclass. The number of slabclasses and the size of items they serve can be configured either from a geometric sequence with the inital item size set using -n or --min-item-chunk-size=N argument and growth ratio set using -f or --factor=D argument, or from a profile string set using -z or --slab-profile=S argument.    ## Eviction    Eviction is triggered when a cache reaches full memory capacity. This happens when all cached items are unexpired and there is no space available to store newer items. Twemcache supports the following eviction strategies, configured using the -M or --eviction-strategy=N command-line argument:    * No eviction (0) - don't evict, respond with server error reply.  * Item LRU eviction (1) - evict only existing items in the same slab class, least recently updated first; essentially a per-slabclass LRU eviction.  * Random eviction (2) - evict all items from a randomly chosen slab.  * Slab LRA eviction (4) - choose the least recently accessed slab, and evict all items from it to reuse the slab.  * Slab LRC eviction (8) - choose the least recently created slab, and evict all items from it to reuse the slab. Eviction ignores freeq & lruq to make sure the eviction follows the timestamp closely. Recommended if cache is updated on the write path.    Eviction strategies can be *stacked*, in the order of higher to lower bit. For example, `-M 5` means that if slab LRA eviciton fails, Twemcache will try item LRU eviction.    ## Observability    ### Stats    Stats are the primary form of observability in twemcache. Stats collection in twemcache is lock-less in a sense that each worker thread only updates its thread-local metrics, and a background aggregator thread collects metrics from all threads periodically, holding only one thread-local lock at a time. Once aggregated, stats polling comes for free. There is a slight trade-off between how up-to-date stats are and how much burden stats collection puts on the system, which can be controlled by the aggregation interval -A or --stats-aggr-interval=N command-line argument. By default, the aggregation interval is set to 100 msec. You can set the aggregation interval at run time using `config aggregate <num>\r\n` command. Stats collection can be disabled at run time by passing a negative aggregation interval or at build time through the --disable-stats configure option.    Metrics exposed by twemcache are of three types - timestamp, counter and gauge and are collected both at the global level and per slab level. You can read about the description of all stats exposed by twemcache using the -D or --describe-stats command-line argument.    The following commands can be used to query stats from a running twemcache  * `stats\r\n`  * `stats settings\r\n`  * `stats slabs\r\n`  * `stats sizes\r\n`  * `stats cachedump <id> <limit>\r\n`    ### Klogger (Command Logger)    Command logger allows users to capture the details of every incoming request. Each line of the command log gives precise information on the client, the time when a request was received, the command header including the command, key, flags and data length, a return code, and reply message length. Few example klog lines look as follows:        172.25.135.205:55438 - [09/Jul/2012:18:15:45 -0700] ""set foo 0 0 3"" 1 6      172.25.135.205:55438 - [09/Jul/2012:18:15:46 -0700] ""get foo"" 0 14      172.25.135.205:55438 - [09/Jul/2012:18:15:57 -0700] ""incr num 1"" 3 9      172.25.135.205:55438 - [09/Jul/2012:18:16:05 -0700] ""set num 0 0 1"" 1 6      172.25.135.205:55438 - [09/Jul/2012:18:16:09 -0700] ""incr num 1"" 0 1      172.25.135.205:55438 - [09/Jul/2012:18:16:13 -0700] ""get num"" 0 12    The command logger supports lockless read/write into ring buffers, whose size can be configured with -x or --klog-entry=N command-line argument. Each worker thread logs to a thread-local buffer as they process incoming queries, and a background thread asynchronously dumps buffer contents to a file configured with -X or --klog-file=S command-line argument.    Since this feature has the capability of generating hundreds of MBs of data per minute, the use must be planned carefully. An enabled klog moduled can be started or stopped by sending `config klog run start\r\n` and `config klog run stop\r\n` respectively. To control the speed of log generation, the command logger also supports sampling. Sample rate can be set over with `config klog sampling <num>\r\n` command, which samples one of num commands.    ### Logging    Logging in twemcache is only available when it is built with logging enabled (--enable-debug=[full|yes|log]). By default logs are written to stderr. Twemcache can also be configured to write logs to a specific file through the -o or --output=S command-line argument.    On a running twemcache, we can turn log levels up and down by sending it SIGTTIN and SIGTTOU signals respectively and reopen log files by sending it SIGHUP signal. Logging levels can be set to a specific value using the `verbosity <num>\r\n` command.    ## Issues and Support    Have a bug? Please create an issue here on GitHub!    https://github.com/twitter/twemcache/issues    ## Versioning    For transparency and insight into our release cycle, releases are be numbered with the [semantic versioning](http://semver.org/) format: `<major>.<minor>.<patch>` and constructed with the following guidelines:    * Breaking backwards compatibility bumps the major  * New additions without breaking backwards compatibility bumps the minor  * Bug fixes and misc changes bump the patch    ## Other Work    * [twemproxy](https://github.com/twitter/twemproxy) - a fast, light-weight proxy for memcached.  * [twemperf](https://github.com/twitter/twemperf) - a tool for measuring memcached server performance.  * [twctop.rb](https://github.com/twitter/twemcache/blob/master/scripts/twctop.rb) - a tool like top for monitoring a cluster of twemcache servers.    ## Contributors    * Manju Rajashekhar ([@manju](https://twitter.com/manju))  * Yao Yue ([@thinkingfish](https://twitter.com/thinkingfish))    ## License    Copyright 2003, Danga Interactive, Inc.    Copyright 2012 Twitter, Inc.    Licensed under the New BSD License, see the LICENSE file. """
Big data;https://github.com/facebookresearch/faiss;"""# Faiss    Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at [Facebook AI Research](https://ai.facebook.com/).    ## News    See [CHANGELOG.md](CHANGELOG.md) for detailed information about latest features.    ## Introduction    Faiss contains several methods for similarity search. It assumes that the instances are represented as vectors and are identified by an integer, and that the vectors can be compared with L2 (Euclidean) distances or dot products. Vectors that are similar to a query vector are those that have the lowest L2 distance or the highest dot product with the query vector. It also supports cosine similarity, since this is a dot product on normalized vectors.    Some of the methods, like those based on binary vectors and compact quantization codes, solely use a compressed representation of the vectors and do not require to keep the original vectors. This generally comes at the cost of a less precise search but these methods can scale to billions of vectors in main memory on a single server. Other methods, like HNSW and NSG add an indexing structure on top of the raw vectors to make searching more efficient.    The GPU implementation can accept input from either CPU or GPU memory. On a server with GPUs, the GPU indexes can be used a drop-in replacement for the CPU indexes (e.g., replace `IndexFlatL2` with `GpuIndexFlatL2`) and copies to/from GPU memory are handled automatically. Results will be faster however if both input and output remain resident on the GPU. Both single and multi-GPU usage is supported.    ## Installing    Faiss comes with precompiled libraries for Anaconda in Python, see [faiss-cpu](https://anaconda.org/pytorch/faiss-cpu) and [faiss-gpu](https://anaconda.org/pytorch/faiss-gpu). The library is mostly implemented in C++, the only dependency is a [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) implementation. Optional GPU support is provided via CUDA, and and the Python interface is also optional. It compiles with cmake. See [INSTALL.md](INSTALL.md) for details.    ## How Faiss works    Faiss is built around an index type that stores a set of vectors, and provides a function to search in them with L2 and/or dot product vector comparison. Some index types are simple baselines, such as exact search. Most of the available indexing structures correspond to various trade-offs with respect to    - search time  - search quality  - memory used per index vector  - training time  - adding time  - need for external data for unsupervised training    The optional GPU implementation provides what is likely (as of March 2017) the fastest exact and approximate (compressed-domain) nearest neighbor search implementation for high-dimensional vectors, fastest Lloyd's k-means, and fastest small k-selection algorithm known. [The implementation is detailed here](https://arxiv.org/abs/1702.08734).    ## Full documentation of Faiss    The following are entry points for documentation:    - the full documentation can be found on the [wiki page](http://github.com/facebookresearch/faiss/wiki), including a [tutorial](https://github.com/facebookresearch/faiss/wiki/Getting-started), a [FAQ](https://github.com/facebookresearch/faiss/wiki/FAQ) and a [troubleshooting section](https://github.com/facebookresearch/faiss/wiki/Troubleshooting)  - the [doxygen documentation](https://faiss.ai/) gives per-class information extracted from code comments  - to reproduce results from our research papers, [Polysemous codes](https://arxiv.org/abs/1609.01882) and [Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734), refer to the [benchmarks README](benchs/README.md). For [  Link and code: Fast indexing with graphs and compact regression codes](https://arxiv.org/abs/1804.09996), see the [link_and_code README](benchs/link_and_code)    ## Authors    The main authors of Faiss are:  - [Hervé Jégou](https://github.com/jegou) initiated the Faiss project and wrote its first implementation  - [Matthijs Douze](https://github.com/mdouze) implemented most of the CPU Faiss  - [Jeff Johnson](https://github.com/wickedfoo) implemented all of the GPU Faiss  - [Lucas Hosseini](https://github.com/beauby) implemented the binary indexes and the build system  - [Chengqi Deng](https://github.com/KinglittleQ) implemented NSG, NNdescent and much of the additive quantization code.    ## Reference    Reference to cite when you use Faiss in a research paper:    ```  @article{johnson2019billion,    title={Billion-scale similarity search with {GPUs}},    author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},    journal={IEEE Transactions on Big Data},    volume={7},    number={3},    pages={535--547},    year={2019},    publisher={IEEE}  }  ```    ## Join the Faiss community    For public discussion of Faiss or for questions, there is a Facebook group at https://www.facebook.com/groups/faissusers/    We monitor the [issues page](http://github.com/facebookresearch/faiss/issues) of the repository.  You can report bugs, ask questions, etc.    ## License    Faiss is MIT-licensed. """
Big data;https://github.com/ecomfe/echarts;"""# Apache ECharts    <a href=""https://echarts.apache.org/"">      <img style=""vertical-align: top;"" src=""./asset/logo.png?raw=true"" alt=""logo"" height=""50px"">  </a>    Apache ECharts is a free, powerful charting and visualization library offering an easy way of adding intuitive, interactive, and highly customizable charts to your commercial products. It is written in pure JavaScript and based on <a href=""https://github.com/ecomfe/zrender"">zrender</a>, which is a whole new lightweight canvas library.    **[中文官网](https://echarts.apache.org/zh/index.html)** | **[ENGLISH HOMEPAGE](https://echarts.apache.org/en/index.html)**    [![License](https://img.shields.io/npm/l/echarts?color=5470c6)](https://github.com/apache/echarts/blob/master/LICENSE) [![Latest npm release](https://img.shields.io/npm/v/echarts?color=91cc75)](https://www.npmjs.com/package/echarts) [![NPM downloads](https://img.shields.io/npm/dm/echarts.svg?label=npm%20downloads&style=flat&color=fac858)](https://www.npmjs.com/package/echarts) [![Contributors](https://img.shields.io/github/contributors/apache/echarts?color=3ba272)](https://github.com/apache/echarts/graphs/contributors)    [![Build Status](https://github.com/apache/echarts/actions/workflows/ci.yml/badge.svg)](https://github.com/apache/echarts/actions/workflows/ci.yml)    ## Get Apache ECharts    You may choose one of the following methods:    + Download from the [official website](https://echarts.apache.org/download.html)  + `npm install echarts --save`  + CDN: [jsDelivr CDN](https://www.jsdelivr.com/package/npm/echarts?path=dist)    ## Docs    + [Get Started](https://echarts.apache.org/handbook)  + [API](https://echarts.apache.org/api.html)  + [Option Manual](https://echarts.apache.org/option.html)  + [Examples](https://echarts.apache.org/examples)    ## Get Help    + [GitHub Issues](https://github.com/apache/echarts/issues) for bug report and feature requests  + Email [dev@echarts.apache.org](mailto:dev@echarts.apache.org) for general questions  + Subscribe to the [mailing list](https://echarts.apache.org/en/maillist.html) to get updated with the project    ## Build    Build echarts source code:    Execute the instructions in the root directory of the echarts:  ([Node.js](https://nodejs.org) is required)    ```shell  # Install the dependencies from NPM:  npm install    # Rebuild source code immediately in watch mode when changing the source code.  npm run dev    # Check correctness of TypeScript code.  npm run checktype    # If intending to build and get all types of the ""production"" files:  npm run release  ```    Then the ""production"" files are generated in the `dist` directory.    More custom build approaches can be checked in this tutorial: [Create Custom Build of ECharts](https://echarts.apache.org/en/tutorial.html#Create%20Custom%20Build%20of%20ECharts) please.    ## Contribution    If you wish to debug locally or make pull requests, please refer to the [contributing](https://github.com/apache/echarts/blob/master/CONTRIBUTING.md) document.    ## Resources    ### Awesome ECharts    [https://github.com/ecomfe/awesome-echarts](https://github.com/ecomfe/awesome-echarts)    ### Extensions    + [ECharts GL](https://github.com/ecomfe/echarts-gl) An extension pack of ECharts, which provides 3D plots, globe visualization, and WebGL acceleration.    + [Liquidfill 水球图](https://github.com/ecomfe/echarts-liquidfill)    + [Wordcloud 字符云](https://github.com/ecomfe/echarts-wordcloud)    + [Extension for Baidu Map 百度地图扩展](https://github.com/apache/echarts/tree/master/extension-src/bmap) An extension provides a wrapper of Baidu Map Service SDK.    + [vue-echarts](https://github.com/ecomfe/vue-echarts) ECharts component for Vue.js    + [echarts-stat](https://github.com/ecomfe/echarts-stat) Statistics tool for ECharts    ## License    ECharts is available under the Apache License V2.    ## Code of Conduct    Please refer to [Apache Code of Conduct](https://www.apache.org/foundation/policies/conduct.html).    ## Paper    Deqing Li, Honghui Mei, Yi Shen, Shuang Su, Wenli Zhang, Junting Wang, Ming Zu, Wei Chen.  [ECharts: A Declarative Framework for Rapid Construction of Web-based Visualization](https://www.sciencedirect.com/science/article/pii/S2468502X18300068).  Visual Informatics, 2018. """
Big data;https://github.com/pinterest/secor;"""# Pinterest Secor    [![Build Status](https://travis-ci.org/pinterest/secor.svg)](https://travis-ci.org/pinterest/secor)    Secor is a service persisting [Kafka] logs to [Amazon S3], [Google Cloud Storage], [Microsoft Azure Blob Storage] and [Openstack Swift].    ## Key features ##    - **strong consistency**: as long as [Kafka] is not dropping messages (e.g., due to aggressive cleanup policy) before Secor is able to read them, it is guaranteed that each message will be saved in exactly one [S3] file. This property is not compromised by the notorious temporal inconsistency of [S3] caused by the [eventual consistency] model,    - **fault tolerance**: any component of Secor is allowed to crash at any given point without compromising data integrity,    - **load distribution**: Secor may be distributed across multiple machines,    - **horizontal scalability**: scaling the system out to handle more load is as easy as starting extra Secor processes. Reducing the resource footprint can be achieved by killing any of the running Secor processes. Neither ramping up nor down has any impact on data consistency,    - **output partitioning**: Secor parses incoming messages and puts them under partitioned s3 paths to enable direct import into systems like [Hive]. day,hour,minute level partitions are supported by secor    - **configurable upload policies**: commit points controlling when data is persisted in S3 are configured through size-based and time-based policies (e.g., upload data when local buffer reaches size of 100MB and at least once per hour),    - **monitoring**: metrics tracking various performance properties are exposed through [Ostrich], [Micrometer] and optionally exported to [OpenTSDB] / [statsD],    - **customizability**: external log message parser may be loaded by updating the configuration,    - **event transformation**: external message level transformation can be done by using customized class.    - **Qubole interface**: Secor connects to [Qubole] to add finalized output partitions to Hive tables.    ## Release Notes    Release Notes for past versions can be found in [RELEASE.md](RELEASE.md).    ## Setup/Configuration Guide    Setup/Configuration instruction is available in [README.setup.md](README.setup.md).    ### Secor configuration for Kubernetes/GKE environment    Extra Setup instruction for Kubernetes/GKE environment is available in [README.kubernetes.md](README.kubernetes.md).    ## Detailed design    Design details are available in [DESIGN.md](DESIGN.md).    ## License    Secor is distributed under [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0.html).    ## Maintainers    * [Pawel Garbacki](https://github.com/pgarbacki)    * [Henry Cai](https://github.com/HenryCaiHaiying)    ## Contributors    * [Andy Kramolisch](https://github.com/andykram)    * [Brenden Matthews](https://github.com/brndnmtthws)    * [Lucas Zago](https://github.com/zago)    * [James Green](https://github.com/jfgreen)    * [Praveen Murugesan](https://github.com/lefthandmagic)    * [Zack Dever](https://github.com/zackdever)    * [Leo Woessner](https://github.com/estezz)    * [Jerome Gagnon](https://github.com/jgagnon1)    * [Taichi Nakashima](https://github.com/tcnksm)    * [Lovenish Goyal](https://github.com/lovenishgoyal)    * [Ahsan Nabi Dar](https://github.com/ahsandar)    * [Ashish Kumar](https://github.com/ashubhumca)    * [Ashwin Sinha](https://github.com/tygrash)    * [Avi Chad-Friedman](https://github.com/achad4)      ## Companies who use Secor      * [Airbnb](https://www.airbnb.com)    * [Appsflyer](https://www.appsflyer.com)    * [Branch](http://branch.io)    * [Coupang](https://www.coupang.com)    * [Credit Karma](https://www.creditkarma.com)    * [GO-JEK](http://gojekengineering.com/)    * [Nextperf](http://www.nextperf.com)    * [PayTM](https://www.paytm.com)    * [Pinterest](https://www.pinterest.com)    * [Rakuten](http://techblog.rakuten.co.jp/)    * [Robinhood](http://www.robinhood.com/)    * [Simplaex](https://www.simplaex.com/)    * [Skyscanner](http://www.skyscanner.net)    * [Strava](https://www.strava.com)    * [TiVo](https://www.tivo.com)    * [VarageSale](http://www.varagesale.com)    * [Viacom](http://www.viacom.com)    * [Wego](https://www.wego.com)    * [Yelp](http://www.yelp.com)    * [Zalando](http://www.zalando.com)    * [Zapier](https://www.zapier.com)    ## Help    If you have any questions or comments, you can reach us at [secor-users@googlegroups.com](https://groups.google.com/forum/#!forum/secor-users)    [Kafka]:http://kafka.apache.org/  [Amazon S3]:http://aws.amazon.com/s3/  [Microsoft Azure Blob Storage]:https://azure.microsoft.com/en-us/services/storage/blobs/  [S3]:http://aws.amazon.com/s3/  [Google Cloud Storage]:https://cloud.google.com/storage/  [eventual consistency]:http://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyMode  [Hive]:http://hive.apache.org/  [Ostrich]: https://github.com/twitter/ostrich  [Micrometer]: https://micrometer.io  [OpenTSDB]: http://opentsdb.net/  [Qubole]: http://www.qubole.com/  [statsD]: https://github.com/etsy/statsd/  [Openstack Swift]: http://swift.openstack.org  [Protocol Buffers]: https://developers.google.com/protocol-buffers/  [Parquet]: https://parquet.apache.org/ """
Big data;https://github.com/skizzehq/skizze;"""<img src=""http://i.imgur.com/9z47NdA.png"" align=""center"" height=""190"" width=""600"">  <br>    [![Build Status](https://travis-ci.org/skizzehq/skizze.svg?branch=master)](https://travis-ci.org/skizzehq/skizze) [![license](http://img.shields.io/badge/license-Apache-blue.svg)](https://raw.githubusercontent.com/skizzehq/skizze/master/LICENSE)    Skizze ([ˈskɪt͡sə]: german for sketch) is a sketch data store to deal with all problems around counting and sketching using probabilistic data-structures.    Unlike a Key-Value store, Skizze does not store values, but rather appends values to defined sketches, allowing one to solve frequency and cardinality queries in near O(1) time, with minimal memory footprint.    <b> Current status ==> Alpha (tagged v0.0.2) </b>    ## Motivation    Statistical analysis and mining of huge multi-terabyte data sets is a common task nowadays, especially in areas like web analytics and Internet advertising. Analysis of such large data sets often requires powerful distributed data stores like Hadoop and heavy data processing with techniques like MapReduce. This approach often leads to heavyweight high-latency analytical processes and poor applicability to realtime use cases. On the other hand, when one is interested only in simple additive metrics like total page views or average price of conversion, it is obvious that raw data can be efficiently summarized, for example, on a daily basis or using simple in-stream counters.  Computation of more advanced metrics like a number of unique visitor or most frequent items is more challenging and requires a lot of resources if implemented straightforwardly.    Skizze is a (fire and forget) service that provides a probabilistic data structures (sketches) storage that allows estimation of these and many other metrics, with a trade off in precision of the estimations for the memory consumption. These data structures can be used both as temporary data accumulators in query processing procedures and, perhaps more important, as a compact – sometimes astonishingly compact – replacement of raw data in stream-based computing.    ## Example use cases (queries)  * How many distinct elements are in the data set (i.e. what is the cardinality of the data set)?  * What are the most frequent elements (the terms “heavy hitters” and “top-k elements” are also used)?  * What are the frequencies of the most frequent elements?  * How many elements belong to the specified range (range query, in SQL it looks like `SELECT count(v) WHERE v >= c1 AND v < c2)`?  * Does the data set contain a particular element (membership query)?    ## How to build and run  ```  make dist  ./bin/skizze  ```    ## Bindings    Two bindings are currently available:     * [Go](https://github.com/skizzehq/goskizze)      * `go get github.com/skizzehq/goskizze/skizze` [Documentation](https://godoc.org/github.com/skizzehq/goskizze/skizze)        * [Node.js](http://github.com/skizzehq/node-skizze)     * `npm install --save skizze` [Documentation](https://github.com/skizzehq/node-skizze#documentation)       ## Example usage:    Skizze comes with a CLI to help test and explore the server. It can be run via    ```  ./bin/skizze-cli  ```    ### Commands  **Create** a new Domain (Collection of Sketches):  ```{r, engine='bash', count_lines}  #CREATE DOM $name $estCardinality $topk  CREATE DOM demostream 10000000 100  ```    **Add** values to the domain:  ```{r, engine='bash', count_lines}  #ADD DOM $name $value1, $value2 ....  ADD DOM demostream zod joker grod zod zod grod  ```    **Get** the *cardinality* of the domain:  ```{r, engine='bash', count_lines}  # GET CARD $name  GET CARD demostream    # returns:  # Cardinality: 9  ```    **Get** the *rankings* of the domain:  ```{r, engine='bash', count_lines}  # GET RANK $name  GET RANK demostream    # returns:  # Rank: 1	  Value: zod	  Hits: 3  # Rank: 2	  Value: grod	  Hits: 2  # Rank: 3	  Value: joker	  Hits: 1  ```    **Get** the *frequencies* of values in the domain:  ```{r, engine='bash', count_lines}  # GET FREQ $name $value1 $value2 ...  GET FREQ demostream zod joker batman grod    # returns  # Value: zod	  Hits: 3  # Value: joker	  Hits: 1  # Value: batman	  Hits: 0  # Value: grod	  Hits: 2  ```    **Get** the *membership* of values in the domain:  ```{r, engine='bash', count_lines}  # GET MEMB $name $value1 $value2 ...  GET MEMB demostream zod joker batman grod    # returns  # Value: zod	  Member: true  # Value: joker	  Member: true  # Value: batman	  Member: false  # Value: grod	  Member: true  ```    **List** all available sketches (created by domains):  ```{r, engine='bash', count_lines}  LIST    # returns  # Name: demostream  Type: CARD  # Name: demostream  Type: FREQ  # Name: demostream  Type: MEMB  # Name: demostream  Type: RANK  ```    **Create** a new sketch of type $type (CARD, MEMB, FREQ or RANK):  ```{r, engine='bash', count_lines}  # CREATE CARD $name  CREATE CARD demosketch  ```    **Add** values to the sketch of type $type (CARD, MEMB, FREQ or RANK):  ```{r, engine='bash', count_lines}  #ADD $type $name $value1, $value2 ....  ADD CARD demostream zod joker grod zod zod grod  ```    ### License  Skizze is available under the Apache License, Version 2.0.      ### Authors  - [Seif Lotfy](https://twitter.com/seiflotfy)  - [Neil Jagdish Patel](https://twitter.com/njpatel) """
Big data;https://github.com/rackerlabs/blueflood;"""<p align=""center"">   <img src=""http://blueflood.io/images/bf-bg-color.png"" width=""220"" height=""232"" align=center>  </p>    # Blueflood    [![Build Status](https://travis-ci.org/rackerlabs/blueflood.svg?branch=master)](https://travis-ci.org/rackerlabs/blueflood)  [![Coveralls](https://coveralls.io/repos/github/rackerlabs/blueflood/badge.svg?branch=master)](https://github.com/rackerlabs/blueflood/releases)  [![Releases](http://img.shields.io/badge/rax-release-v1.0.1956.svg)](https://coveralls.io/github/mmi-cookbooks/metrics-repose?branch=master)  [![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)    [Discuss](https://groups.google.com/forum/#!forum/blueflood-discuss) - [Code](http://github.com/rackerlabs/blueflood) - [Site](http://blueflood.io)    ## Introduction    Blueflood is a multi-tenant, distributed metric processing system. Blueflood is capable of ingesting, rolling up and serving metrics at a massive scale.      ## Getting Started    The latest code will always be here on Github.        git clone https://github.com/rackerlabs/blueflood.git      cd blueflood        You can run the entire suite of tests using Maven:        mvn test integration-test    ### Building    Build an ['uber jar'](http://stackoverflow.com/questions/11947037/what-is-an-uber-jar) using maven:        mvn package -P all-modules    The uber jar will be found in ${BLUEFLOOD_DIR}/blueflood-all/target/blueflood-all-${VERSION}-jar-with-dependencies.jar.  This jar contains all the dependencies necessary to run Blueflood with a very simple classpath.    Build a docker image:        mvn clean package  docker:build -Pall-modules    ### Running    The best place to start is the [10 minute guide](https://github.com/rackerlabs/blueflood/wiki/10-Minute-Guide).  In a nutshell, you must do this:        java -cp /path/to/uber.jar \      -Dblueflood.config=file:///path/to/blueflood.conf \      -Dlog4j.configuration=file:///path/to/log4j.properties \      com.rackspacecloud.blueflood.service.BluefloodServiceStarter        Each configuration option can be found in Configuration.java.  Each of those can be overridden on the command line by  doing:        -DCONFIG_OPTION=NEW_VALUE    ## Additional Tools    The Blueflood team maintains a number of tools that are related to the project, but not essential components of it. These tools are kept in various other repos:    * Performance Tests: Scripts for load testing a blueflood installation using [The Grinder](http://grinder.sourceforge.net/). https://github.com/rackerlabs/raxmetrics-perf-test-scripts  * Carbon Forwarder: a process that receives data from carbon (one of the components of [Graphite](https://graphiteapp.org/)) and sends it to a Blueflood instance. https://github.com/rackerlabs/blueflood-carbon-forwarder  * Blueflood-Finder: a plugin for graphite-web and graphite-api that allows them to using a Blueflood instance as a data backend. https://github.com/rackerlabs/blueflood-graphite-finder  * StatsD plugin: a statsD backend that sends metrics a Blueflood instance. https://github.com/rackerlabs/blueflood-statsd-backend    ## Contributing    First, we welcome bug reports and contributions.  If you would like to contribute code, just fork this project and send us a pull request.  If you would like to contribute documentation, you should get familiar with  [our wiki](https://github.com/rackerlabs/blueflood/wiki)    Also, we have set up a [Google Group](https://groups.google.com/forum/#!forum/blueflood-discuss) to answer questions.    If you prefer IRC, most of the Blueflood developers are in #blueflood on Freenode.     ## License    Copyright 2013-2017 Rackspace    Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at       http://www.apache.org/licenses/LICENSE-2.0     Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.   """
Big data;https://github.com/YugaByte/yugabyte-db;"""<img src=""https://www.yugabyte.com/wp-content/uploads/2021/05/yb_horizontal_alt_color_RGB.png"" align=""center"" alt=""YugabyteDB"" width=""50%""/>    ---------------------------------------    [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)  [![Documentation Status](https://readthedocs.org/projects/ansicolortags/badge/?version=latest)](https://docs.yugabyte.com/)  [![Ask in forum](https://img.shields.io/badge/ask%20us-forum-orange.svg)](https://forum.yugabyte.com/)  [![Slack chat](https://img.shields.io/badge/Slack:-%23yugabyte_db-blueviolet.svg?logo=slack)](https://communityinviter.com/apps/yugabyte-db/register)  [![Analytics](https://yugabyte.appspot.com/UA-104956980-4/home?pixel&useReferer)](https://github.com/yugabyte/ga-beacon)    # What is YugabyteDB?     **YugabyteDB** is a **high-performance, cloud-native distributed SQL database** that aims to support **all PostgreSQL features**. It is best to fit for **cloud-native OLTP (i.e. real-time, business-critical) applications** that need absolute **data correctness** and require at least one of the following: **scalability, high tolerance to failures, or globally-distributed deployments.**    * [Core Features](#core-features)  * [Get Started](#get-started)  * [Build Apps](#build-apps)  * [What's being worked on?](#whats-being-worked-on)  * [Architecture](#architecture)  * [Need Help?](#need-help)  * [Contribute](#contribute)  * [License](#license)  * [Read More](#read-more)    # Core Features     * **Powerful RDBMS capabilities** Yugabyte SQL (*YSQL* for short) reuses the query layer of PostgreSQL (similar to Amazon Aurora PostgreSQL), thereby supporting most of its features (datatypes, queries, expressions, operators and functions, stored procedures, triggers, extensions, etc). Here is a detailed [list of features currently supported by YSQL](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/YSQL-Features-Supported.md).    * **Distributed transactions** The transaction design is based on the Google Spanner architecture. Strong consistency of writes is achieved by using Raft consensus for replication and cluster-wide distributed ACID transactions using *hybrid logical clocks*. *Snapshot*, *serializable* and *read committed* isolation levels are supported. Reads (queries) have strong consistency by default, but can be tuned dynamically to read from followers and read-replicas.    * **Continuous availability** YugabyteDB is extremely resilient to common outages with native failover and repair. YugabyteDB can be configured to tolerate disk, node, zone, region, and cloud failures automatically. For a typical deployment where a YugabyteDB cluster is deployed in one region across multiple zones on a public cloud, the RPO is 0 (meaning no data is lost on failure) and the RTO is 3 seconds (meaning the data being served by the failed node is available in 3 seconds).    * **Horizontal scalability** Scaling a YugabyteDB cluster to achieve more IOPS or data storage is as simple as adding nodes to the cluster.    * **Geo-distributed, multi-cloud** YugabyteDB can be deployed in public clouds and natively inside Kubernetes. It supports deployments that span three or more fault domains, such as multi-zone, multi-region, and multi-cloud deployments. It also supports xCluster asynchronous replication with unidirectional master-slave and bidirectional multi-master configurations that can be leveraged in two-region deployments. To serve (stale) data with low latencies, read replicas are also a supported feature.    * **Multi API design** The query layer of YugabyteDB is built to be extensible. Currently, YugabyteDB supports two distributed SQL APIs: **[Yugabyte SQL (YSQL)](https://docs.yugabyte.com/latest/api/ysql/)**, a fully relational API that re-uses query layer of PostgreSQL, and **[Yugabyte Cloud QL (YCQL)](https://docs.yugabyte.com/latest/api/ycql/)**, a semi-relational SQL-like API with documents/indexing support with Apache Cassandra QL roots.    * **100% open source** YugabyteDB is fully open-source under the [Apache 2.0 license](https://github.com/yugabyte/yugabyte-db/blob/master/LICENSE.md). The open-source version has powerful enterprise features such as distributed backups, encryption of data-at-rest, in-flight TLS encryption, change data capture, read replicas, and more.    Read more about YugabyteDB in our [Docs](https://docs.yugabyte.com/latest/introduction/).    # Get Started    * [Install YugabyteDB](https://docs.yugabyte.com/latest/quick-start/install/)  * [Create a local cluster](https://docs.yugabyte.com/latest/quick-start/create-local-cluster/)  * [Start with Yugabyte Cloud](https://www.yugabyte.com/cloud/)  * [Connect and try out SQL commands](https://docs.yugabyte.com/latest/quick-start/explore-ysql/)  * [Build an app](https://docs.yugabyte.com/latest/quick-start/build-apps/) using a PostgreSQL-compatible driver or ORM.  * Try running a real-world demo application:    * [Microservices-oriented e-commerce app](https://github.com/yugabyte/yugastore-java)    * [Streaming IoT app with Kafka and Spark Streaming](https://docs.yugabyte.com/latest/develop/realworld-apps/iot-spark-kafka-ksql/)    Cannot find what you are looking for? Have a question? Please post your questions or comments on our Community [Slack](https://communityinviter.com/apps/yugabyte-db/register) or [Forum](https://forum.yugabyte.com).    # Build Apps    YugabyteDB supports several languages and client drivers. Below is a brief list.    | Language  | ORM | YSQL Drivers | YCQL Drivers |  | --------- | --- | ------------ | ------------ |  | Java  | [Spring/Hibernate](https://docs.yugabyte.com/latest/quick-start/build-apps/java/ysql-spring-data/) | [PostgreSQL JDBC](https://docs.yugabyte.com/latest/quick-start/build-apps/java/ysql-jdbc/) | [cassandra-driver-core-yb](https://docs.yugabyte.com/latest/quick-start/build-apps/java/ycql/)  | Go  | [Gorm](https://github.com/yugabyte/orm-examples) | [pq](https://docs.yugabyte.com/latest/quick-start/build-apps/go/#ysql) | [gocql](https://docs.yugabyte.com/latest/quick-start/build-apps/go/#ycql)  | NodeJS  | [Sequelize](https://github.com/yugabyte/orm-examples) | [pg](https://docs.yugabyte.com/latest/quick-start/build-apps/nodejs/#ysql) | [cassandra-driver](https://docs.yugabyte.com/latest/quick-start/build-apps/nodejs/#ycql)  | Python  | [SQLAlchemy](https://github.com/yugabyte/orm-examples) | [psycopg2](https://docs.yugabyte.com/latest/quick-start/build-apps/python/#ysql) | [yb-cassandra-driver](https://docs.yugabyte.com/latest/quick-start/build-apps/python/#ycql)  | Ruby  | [ActiveRecord](https://github.com/yugabyte/orm-examples) | [pg](https://docs.yugabyte.com/latest/quick-start/build-apps/ruby/#ysql) | [yugabyte-ycql-driver](https://docs.yugabyte.com/latest/quick-start/build-apps/ruby/#ycql)  | C#  | [EntityFramework](https://github.com/yugabyte/orm-examples) | [npgsql](http://www.npgsql.org/) | [CassandraCSharpDriver](https://docs.yugabyte.com/latest/quick-start/build-apps/csharp/#ycql)  | C++ | Not tested | [libpqxx](https://docs.yugabyte.com/latest/quick-start/build-apps/cpp/#ysql) | [cassandra-cpp-driver](https://docs.yugabyte.com/latest/quick-start/build-apps/cpp/#ycql)  | C   | Not tested | [libpq](https://docs.yugabyte.com/latest/quick-start/build-apps/c/#ysql) | Not tested    # What's being worked on?    > This section was last updated in **March, 2022**.    ## Current roadmap    Here is a list of some of the key features being worked on for the upcoming releases (the YugabyteDB [**v2.13 latest release**](https://docs.yugabyte.com/latest/releases/release-notes/v2.13/) has been released in **March, 2022**, and the [**v2.12 stable release**](https://blog.yugabyte.com/announcing-yugabytedb-2-12/) was released in **Feb 2022**).    | Feature                                         | Status    | Release Target | Progress        |  Comments     |  | ----------------------------------------------- | --------- | -------------- | --------------- | ------------- |  |[Faster Bulk-Data Loading in YugabyteDB](https://github.com/yugabyte/yugabyte-db/issues/11765)| PROGRESS| v2.15 |[Track](https://github.com/yugabyte/yugabyte-db/issues/11765)| Master issue to track improvements to make it easier and faster to get large amounts of data into YugabyteDB.  |[Database-level multi-tenancy with tablegroups](https://github.com/yugabyte/yugabyte-db/issues/11665)| PROGRESS| v2.15 |[Track](https://github.com/yugabyte/yugabyte-db/issues/11665)| Master issue to track Database-level multi-tenancy using tablegroups.  |[Upgrade to PostgreSQL v13](https://github.com/yugabyte/yugabyte-db/issues/9797)| PROGRESS| v2.15 |[Track](https://github.com/yugabyte/yugabyte-db/issues/9797)| For latest features, new PostgreSQL extensions, performance, and community fixes  |Support for  [in-cluster PITR](https://github.com/yugabyte/yugabyte-db/issues/7120)  | PROGRESS| v2.15 |[Track](https://github.com/yugabyte/yugabyte-db/issues/7120)|Point in time recovery of YSQL databases, to a fixed point in time, across DDL and DML changes|  | [Automatic tablet splitting enabled by default](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/docdb-automatic-tablet-splitting.md) | PROGRESS  | v2.15 | [Track](https://github.com/yugabyte/yugabyte-db/issues/1004) |Enables changing the number of tablets (which are splits of data) at runtime.|  | YSQL-table statistics and cost based optimizer(CBO) | PROGRESS  |  v2.15 | [Track](https://github.com/yugabyte/yugabyte-db/issues/5242) | Improve YSQL query performance |  | [YSQL-Feature support - ALTER TABLE](https://github.com/yugabyte/yugabyte-db/issues/1124) | PROGRESS | v2.15 | [Track](https://github.com/yugabyte/yugabyte-db/issues/1124) | Support for various `ALTER TABLE` variants |  | [YSQL-Online schema migration](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/online-schema-migrations.md)  | PROGRESS  | v2.15 | [Track](https://github.com/yugabyte/yugabyte-db/issues/4192) | Schema migrations(includes DDL operations) to be safely run concurrently with foreground operations |  | Pessimistic locking Design | PROGRESS  | v2.15  | [Track](https://github.com/yugabyte/yugabyte-db/issues/5680) |  |  | Make [`COLOCATED` tables](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/ysql-colocated-tables.md) default for YSQL | PLANNING  |  | [Track](https://github.com/yugabyte/yugabyte-db/issues/5239)  |  |  | Support for transactions in async [xCluster replication](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/multi-region-xcluster-async-replication.md) | PLANNING  |    | [Track](https://github.com/yugabyte/yugabyte-db/issues/1808) | Apply transactions atomically on consumer cluster. |  | Support for GiST indexes | PLANNING  |    | [Track](https://github.com/yugabyte/yugabyte-db/issues/1337) |Suppor for GiST (Generalized Search Tree) based index|    ## Recently released features    | Feature                                         | Status    | Release Target | Docs / Enhancements |  Comments     |  | ----------------------------------------------- | --------- | -------------- | ------------------- | ------------- |  |[Change Data Capture](https://github.com/yugabyte/yugabyte-db/issues/9019)|  ✅ *DONE*| v2.13 ||Change data capture (CDC) allows multiple downstream apps and services to consume the continuous and never-ending stream(s) of changes to Yugabyte databases|  |[Support for materalized views](https://github.com/yugabyte/yugabyte-db/issues/10102) |  ✅ *DONE*| v2.13 |[Docs](https://docs.yugabyte.com/latest/explore/ysql-language-features/advanced-features/views/#materialized-views)|A materialized view is a pre-computed data set derived from a query specification and stored for later use|  |[Geo-partitioning support](https://github.com/yugabyte/yugabyte-db/issues/9980) for the transaction status table | ✅ *DONE*| v2.13 |[Docs](https://docs.yugabyte.com/latest/explore/multi-region-deployments/row-level-geo-partitioning/)|Instead of central remote transaction execution metatda, it is now optimized for access from different regions. Since the transaction metadata is also geo partitioned, it eliminates the need for round-trip to remote regions to update transaction statuses.|  | Transparently restart transactions |  ✅ *DONE*| v2.13 | |Decrease the incidence of transaction restart errors seen in various scenarios |  | [Row-level geo-partitioning](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/ysql-row-level-partitioning.md) |  ✅ *DONE*| v2.13 |[Docs](https://docs.yugabyte.com/latest/explore/multi-region-deployments/row-level-geo-partitioning/)|Row-level geo-partitioning allows fine-grained control over pinning data in a user table (at a per-row level) to geographic locations, thereby allowing the data residency to be managed at the table-row level.|  | [YSQL-Support `GIN` indexes](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/ysql-gin-indexes.md) |  ✅ *DONE*  | v2.11 | [Docs](https://docs.yugabyte.com/latest/explore/ysql-language-features/gin/) | Support for generalized inverted indexes for container data types like jsonb, tsvector, and array |  | [YSQL-Collation Support](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/ysql-collation-support.md)  | ✅ *DONE*  | v2.11           |[Docs](https://docs.yugabyte.com/latest/explore/ysql-language-features/collations/) |Allows specifying the sort order and character classification behavior of data per-column, or even per-operation according to language and country-specific rules           |  [YSQL-Savepoint Support](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/savepoints.md)  |  ✅ *DONE*  | v2.11     |[Docs](https://docs.yugabyte.com/latest/explore/ysql-language-features/savepoints/) | Useful for implementing complex error recovery in multi-statement transaction|  | [xCluster replication management through Platform](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/platform-xcluster-replication-management.md) | ✅ *DONE* | v2.11           |   [Docs](https://docs.yugabyte.com/latest/yugabyte-platform/create-deployments/async-replication-platform/)     |     | [Spring Data YugabyteDB module](https://github.com/yugabyte/yugabyte-db/blob/master/architecture/design/spring-data-yugabytedb.md) | ✅ *DONE*  | v2.9 | [Track](https://github.com/yugabyte/yugabyte-db/issues/7956) | Bridges the gap for learning the distributed SQL concepts with familiarity and ease of Spring Data APIs |  | Support Liquibase, Flyway, ORM schema migrations | ✅ *DONE* | v2.9           |           [Docs](https://blog.yugabyte.com/schema-versioning-in-yugabytedb-using-flyway/)      |   | [Support `ALTER TABLE` add primary key](https://github.com/yugabyte/yugabyte-db/issues/1124) | ✅ *DONE* | v2.9 | [Track](https://github.com/yugabyte/yugabyte-db/issues/1124) |  |  | [YCQL-LDAP Support](https://github.com/yugabyte/yugabyte-db/issues/4421) |  ✅ *DONE*  | v2.8           |[Docs](https://docs.yugabyte.com/latest/secure/authentication/ldap-authentication-ycql/#root)  | support LDAP authentication in YCQL API |               | [Platform Alerting and Notification](https://blog.yugabyte.com/yugabytedb-2-8-alerts-and-notifications/) | ✅ *DONE* | v2.8  |  [Docs](https://docs.yugabyte.com/latest/yugabyte-platform/alerts-monitoring/alert/) |  To get notified in real time about database alerts, user defined alert policies notify you when a performance metric rises above or falls below a threshold you set.|        | [Platform API](https://blog.yugabyte.com/yugabytedb-2-8-api-automated-operations/) | ✅ *DONE* | v2.8           |   [Docs](https://api-docs.yugabyte.com/docs/yugabyte-platform/ZG9jOjIwMDY0MTA4-platform-api-overview)              |   Securely Deploy YugabyteDB Clusters Using Infrastructure-as-Code|                # Architecture    <img src=""https://raw.githubusercontent.com/yugabyte/yugabyte-db/master/architecture/images/yb-architecture.jpg"" align=""center"" alt=""YugabyteDB Architecture""/>    Review detailed architecture in our [Docs](https://docs.yugabyte.com/latest/architecture/).    # Need Help?    * You can ask questions, find answers, and help others on our Community [Slack](https://communityinviter.com/apps/yugabyte-db/register), [Forum](https://forum.yugabyte.com), [Stack Overflow](https://stackoverflow.com/questions/tagged/yugabyte-db), as well as Twitter [@Yugabyte](https://twitter.com/yugabyte)    * Please use [GitHub issues](https://github.com/yugabyte/yugabyte-db/issues) to report issues or request new features.     * To Troubleshoot YugabyteDB, cluser/node level isssues, Please refer to [Troubleshooting documentation](https://docs.yugabyte.com/latest/troubleshoot/)    # Contribute    As an an open-source project with a strong focus on the user community, we welcome contributions as GitHub pull requests. See our [Contributor Guides](https://docs.yugabyte.com/latest/contribute/) to get going. Discussions and RFCs for features happen on the design discussions section of [our Forum](https://forum.yugabyte.com).    # License    Source code in this repository is variously licensed under the Apache License 2.0 and the Polyform Free Trial License 1.0.0. A copy of each license can be found in the [licenses](licenses) directory.    The build produces two sets of binaries:    * The entire database with all its features (including the enterprise ones) are licensed under the Apache License 2.0  * The  binaries that contain `-managed` in the artifact and help run a managed service are licensed under the Polyform Free Trial License 1.0.0.    > By default, the build options generate only the Apache License 2.0 binaries.    # Read More    * To see our updates, go to [The Distributed SQL Blog](https://blog.yugabyte.com/).  * For an in-depth design and the YugabyteDB architecture, see our [design specs](https://github.com/yugabyte/yugabyte-db/tree/master/architecture/design).  * Tech Talks and [Videos](https://www.youtube.com/c/YugaByte).  * See how YugabyteDB [compares with other databases](https://docs.yugabyte.com/latest/comparisons/). """
Big data;https://github.com/airbnb/airpal;"""# DEPREACTED - Airpal    Airpal is deprecated, and most functionality and feature work has been moved to SQL Lab within [Apache Superset](https://www.github.com/apache/superset).    ---    Airpal is a web-based, query execution tool which leverages Facebook's [PrestoDB](http://prestodb.io)  to make authoring queries and retrieving results simple for users.  Airpal provides the ability to find tables, see metadata, browse sample rows,  write and edit queries, then submit queries all in a web interface. Once  queries are running, users can track query progress and when finished,  get the results back through the browser as a CSV (download it or share it  with friends). The results of a query can be used to generate a new Hive table  for subsequent analysis, and Airpal maintains a searchable history of all  queries run within the tool.    * [Features](#features)  * [Requirements](#requirements)  * [Launching](#steps-to-launch)  * [Presto Compatibility Chart](#compatibility-chart)    ![Airpal UI](screenshots/demo.gif)    ## Features    * Optional [Access Control](docs/USER_ACCOUNTS.md)  * Syntax highlighting  * Results exported to a CSV for download or a Hive table  * Query history for self and others  * Saved queries  * Table finder to search for appropriate tables  * Table explorer to visualize schema of table and first 1000 rows    ## Requirements    * Java 7 or higher  * MySQL database  * [Presto](http://prestodb.io) 0.77 or higher  * S3 bucket (to store CSVs)  * Gradle 2.2 or higher      ## Steps to launch    1. Build Airpal        We'll be using [Gradle](https://www.gradle.org/) to build the back-end Java code      and a [Node.js](http://nodejs.org/)-based build pipeline ([Browserify](http://browserify.org/)      and [Gulp](http://gulpjs.com/)) to build the front-end Javascript code.        If you have `node` and `npm` installed locally, and wish to use      them, simply run:        ```      ./gradlew clean shadowJar -Dairpal.useLocalNode      ```        Otherwise, `node` and `npm` will be automatically downloaded for you      by running:        ```      ./gradlew clean shadowJar      ```        Specify Presto version by `-Dairpal.prestoVersion`:        ```      ./gradlew -Dairpal.prestoVersion=0.145 clean shadowJar      ```    1. Create a MySQL database for Airpal. We recommend you call it `airpal` and will assume that for future steps.    1. Create a `reference.yml` file to store your configuration options.        Start by copying over the example configuration, `reference.example.yml`.        ```      cp reference.example.yml reference.yml      ```      Then edit it to specify your MySQL credentials, and your S3 credentials if      using S3 as a storage layer (Airpal defaults to local file storage, for      demonstration purposes).    1. Migrate your database.        ```      java -Duser.timezone=UTC \           -cp build/libs/airpal-*-all.jar com.airbnb.airpal.AirpalApplication db migrate reference.yml      ```    1. Run Airpal.        ```      java -server \           -Duser.timezone=UTC \           -cp build/libs/airpal-*-all.jar com.airbnb.airpal.AirpalApplication server reference.yml      ```    1. Visit Airpal.      Assuming you used the default settings in `reference.yml` you can      now open http://localhost:8081 to use Airpal. Note that you might      have to change the host, depending on where you deployed it.    *Note:* To override the configuration specified in `reference.yml`, you may  specify certain settings on the command line in [the traditional Dropwizard  fashion](https://dropwizard.github.io/dropwizard/manual/core.html#configuration),  like so:    ```  java -Ddw.prestoCoordinator=http://presto-coordinator-url.com \       -Ddw.s3AccessKey=$ACCESS_KEY \       -Ddw.s3SecretKey=$SECRET_KEY \       -Ddw.s3Bucket=airpal \       -Ddw.dataSourceFactory.url=jdbc:mysql://127.0.0.1:3306/airpal \       -Ddw.dataSourceFactory.user=airpal \       -Ddw.dataSourceFactory.password=$YOUR_PASSWORD \       -Duser.timezone=UTC \       -cp build/libs/airpal-*-all.jar db migrate reference.yml  ```      ## Compatibility Chart    Airpal Version | Presto Versions Tested  ---------------|-----------------------  0.1            | 0.77, 0.87, 0.145    ## In the Wild  Organizations and projects using `airpal` can list themselves [here](INTHEWILD.md).    ## Contributors    - Andy Kramolisch [@andykram](https://github.com/andykram)  - Harry Shoff [@hshoff](https://github.com/hshoff)  - Josh Perez [@goatslacker](https://github.com/goatslacker)  - Spike Brehm [@spikebrehm](https://github.com/spikebrehm)  - Stefan Vermaas [@stefanvermaas](https://github.com/stefanvermaas) """
Big data;https://github.com/pilosa/pilosa;"""<p>      <a href=""https://www.pilosa.com"">          <img src=""https://www.pilosa.com/img/logo.svg"" width=""50%"">      </a>  </p>    [![CircleCI](https://circleci.com/gh/pilosa/pilosa/tree/master.svg?style=shield)](https://circleci.com/gh/pilosa/pilosa/tree/master)  [![GoDoc](https://godoc.org/github.com/pilosa/pilosa?status.svg)](https://godoc.org/github.com/pilosa/pilosa)  [![Go Report Card](https://goreportcard.com/badge/github.com/pilosa/pilosa)](https://goreportcard.com/report/github.com/pilosa/pilosa)  [![license](https://img.shields.io/github/license/pilosa/pilosa.svg)](https://github.com/pilosa/pilosa/blob/master/LICENSE)  [![CLA Assistant](https://cla-assistant.io/readme/badge/pilosa/pilosa)](https://cla-assistant.io/pilosa/pilosa)  [![GitHub release](https://img.shields.io/github/release/pilosa/pilosa.svg)](https://github.com/pilosa/pilosa/releases)    ## An open source, distributed bitmap index.  - [Docs](#docs)  - [Getting Started](#getting-started)  - [Data Model](#data-model)  - [Query Language](#query-language)  - [Client Libraries](#client-libraries)  - [Get Support](#get-support)  - [Contributing](#contributing)    Want to contribute? One of the easiest ways is to [tell us how you're using (or want to use) Pilosa](https://github.com/pilosa/pilosa/issues/1074). We learn from every discussion!    ## Docs    See our [Documentation](https://www.pilosa.com/docs/) for information about installing and working with Pilosa.      ## Getting Started    1.  [Install Pilosa](https://www.pilosa.com/docs/installation/).    2.  [Start Pilosa](https://www.pilosa.com/docs/getting-started/#starting-pilosa) with the default configuration:        ```shell      pilosa server      ```            and verify that it's running:            ```shell      curl localhost:10101/nodes      ```    3.  Follow along with the [Sample Project](https://www.pilosa.com/docs/getting-started/#sample-project) to get a better understanding of Pilosa's capabilities.      ## Data Model    Check out how the Pilosa [Data Model](https://www.pilosa.com/docs/data-model/) works.      ## Query Language    You can interact with Pilosa directly in the console using the [Pilosa Query Language](https://www.pilosa.com/docs/query-language/) (PQL).      ## Client Libraries    There are supported libraries for the following languages:  - [Go](https://www.pilosa.com/docs/client-libraries/#go)  - [Java](https://www.pilosa.com/docs/client-libraries/#java)  - [Python](https://www.pilosa.com/docs/client-libraries/#python)    ## Licenses    The core Pilosa code base and all default builds (referred to as Pilosa Community Edition) are licensed completely under the Apache License, Version 2.0.  If you build Pilosa with the `enterprise` build tag (Pilosa Enterprise Edition), then that build will include features licensed under the GNU Affero General  Public License (AGPL). Enterprise code is located entirely in the [github.com/pilosa/pilosa/enterprise](https://github.com/pilosa/pilosa/tree/master/enterprise)  directory. See [github.com/pilosa/pilosa/NOTICE](https://github.com/pilosa/pilosa/blob/master/NOTICE) and  [github.com/pilosa/pilosa/LICENSE](https://github.com/pilosa/pilosa/blob/master/LICENSE) for more information about Pilosa licenses.    ## Get Support    There are [several channels](https://www.pilosa.com/community/#support) available for you to reach out to us for support. The Slack channel (#pilosa in the [Golang](https://invite.slack.golangbridge.org/) team) is the most active.    ## Contributing    Pilosa is an open source project. Please see our [Contributing Guide](CONTRIBUTING.md) for information about how to get involved. """
Big data;https://github.com/griddb/griddb_nosql;"""<img src=""https://griddb.org/brand-resources/griddb-logo/png/color.png"" align=""center"" height=""240"" alt=""GridDB""/>    [![Visit Website](https://img.shields.io/badge/website-visit-orange.svg)](https://griddb.net)  ![GitHub All Releases](https://img.shields.io/github/downloads/griddb/griddb_nosql/total.svg)  ![GitHub release](https://img.shields.io/github/release/griddb/griddb_nosql.svg)  ## Overview    GridDB is Database for IoT with both NoSQL interface and SQL Interface.      Please refer to [GridDB Features Reference](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_FeaturesReference.md) for functionality.      This repository includes server and Java client. And [jdbc repository](https://github.com/griddb/jdbc) includes JDBC Driver.    ## Quick start (Using source code)    We have confirmed the operation on CentOS 7.6 (gcc 4.8.5), Ubuntu 18.04 (gcc 4.8.5) and openSUSE Leap 15.1 (gcc 4.8.5).      Note: Please install tcl like ""yum install tcl.x86_64"" in advance.    ### Build a server and client(Java)      $ ./bootstrap.sh      $ ./configure      $ make      Note: When you use maven build for Java client, please run the following command. Then gridstore-X.X.X.jar file is created on target/.          $ cd java_client      $ ./make_source_for_mvn.sh      $ mvn clean      $ mvn install    ### Start a server      $ export GS_HOME=$PWD      $ export GS_LOG=$PWD/log      $ export PATH=${PATH}:$GS_HOME/bin        $ bin/gs_passwd admin        #input your_password      $ vi conf/gs_cluster.json        #    ""clusterName"":""your_clustername"" #<-- input your_clustername        $ bin/gs_startnode      $ bin/gs_joincluster -c your_clustername -u admin/your_password    ### Execute a sample program      $ export CLASSPATH=${CLASSPATH}:$GS_HOME/bin/gridstore.jar      $ mkdir gsSample      $ cp $GS_HOME/docs/sample/program/Sample1.java gsSample/.      $ javac gsSample/Sample1.java      $ java gsSample/Sample1 239.0.0.1 31999 your_clustername admin your_password        --> Person:  name=name02 status=false count=2 lob=[65, 66, 67, 68, 69, 70, 71, 72, 73, 74]    ### Stop a server      $ bin/gs_stopcluster -u admin/your_password      $ bin/gs_stopnode -u admin/your_password    ## Quick start (Using RPM or DEB)      We have confirmed the operation on CentOS 7.8/8.1, Ubuntu 18.04 and openSUSE Leap 15.1.    Note:  - When you install this package, a gsadm OS user are created in the OS.      Execute the operating command as the gsadm user.    - You don't need to set environment vatiable GS_HOME and GS_LOG.  - There is Java client library (gridstore.jar) on /usr/share/java and a sample on /usr/gridb-XXX/docs/sample/programs.  - The packages don't include trigger function.  - Please install Python2 in advance except CentOS7.    ### Install        (CentOS)      $ sudo rpm -ivh griddb-X.X.X-linux.x86_64.rpm        (Ubuntu)      $ sudo dpkg -i griddb_X.X.X_amd64.deb        (openSUSE)      $ sudo rpm -ivh griddb-X.X.X-opensuse.x86_64.rpm        Note: X.X.X is the GridDB version.    ### Start a server      [gsadm]$ gs_passwd admin        #input your_password      [gsadm]$ vi conf/gs_cluster.json        #    ""clusterName"":""your_clustername"" #<-- input your_clustername      [gsadm]$ gs_startnode      [gsadm]$ gs_joincluster -c your_clustername -u admin/your_password    ### Execute a sample program      $ export CLASSPATH=${CLASSPATH}:/usr/share/java/gridstore.jar      $ mkdir gsSample      $ cp /usr/griddb-X.X.X/docs/sample/program/Sample1.java gsSample/.      $ javac gsSample/Sample1.java      $ java gsSample/Sample1 239.0.0.1 31999 your_clustername admin your_password        --> Person:  name=name02 status=false count=2 lob=[65, 66, 67, 68, 69, 70, 71, 72, 73, 74]    ### Stop a server      [gsadm]$ gs_stopcluster -u admin/your_password      [gsadm]$ gs_stopnode -u admin/your_password    If necessary, please refer to [Installation Troubleshooting](docs/TroubleShootingTips.md).    ## Document    Refer to the file below for more detailed information.      - [Features Reference](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_FeaturesReference.md)    - [Quick Start Guide](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_QuickStartGuide.md)    - [Java API Reference](http://griddb.github.io/docs-en/manuals/GridDB_Java_API_Reference.html)    - [C API Reference](http://griddb.github.io/docs-en/manuals/GridDB_C_API_Reference.html)    - [TQL Reference](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_TQL_Reference.md)    - [JDBC Driver UserGuide](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_JDBC_Driver_UserGuide.md)    - [SQL Reference](https://github.com/griddb/docs-en/blob/master/manuals/GridDB_SQL_Reference.md)    - [V3.0 Release Notes](docs/GridDB-3.0.0-CE-RELEASE_NOTES.md)    - [V4.0 Release Notes](docs/GridDB-4.0-CE-RELEASE_NOTES.md)    - [V4.1 Release Notes](docs/GridDB-4.1-CE-RELEASE_NOTES.md)    - [V4.2 Release Notes](docs/GridDB-4.2-CE-RELEASE_NOTES.md)    - [V4.3 Release Notes](docs/GridDB-4.3-CE-RELEASE_NOTES.md)    - [V4.5 Release Notes](docs/GridDB-4.5-CE-RELEASE_NOTES.md)    - [V4.6 Release Notes](docs/GridDB-4.6-CE-RELEASE_NOTES.md)    ## Client and Connector    There are other clients and API for GridDB.        (NoSQL Interface)    * [GridDB C Client](https://github.com/griddb/c_client)    * [GridDB Python Client](https://github.com/griddb/python_client)    * [GridDB Ruby Client](https://github.com/griddb/ruby_client)    * [GridDB Go Client](https://github.com/griddb/go_client)    * [GridDB Node.JS Client (SWIG based)](https://github.com/griddb/nodejs_client)    * [GridDB Node API (node-addon-api based)](https://github.com/griddb/node-api)    * [GridDB PHP Client](https://github.com/griddb/php_client)    * [GridDB Perl Client](https://github.com/griddb/perl_client)        (SQL Interface)    * [GridDB JDBC Driver](https://github.com/griddb/jdbc)        (NoSQL & SQL Interface)    * [GridDB WebAPI](https://github.com/griddb/webapi)    * [GridDB CLI](https://github.com/griddb/cli)      There are some connectors for other OSS.    * [GridDB connector for Apache Hadoop MapReduce](https://github.com/griddb/griddb_hadoop_mapreduce)    * [GridDB connector for YCSB (https://github.com/brianfrankcooper/YCSB/tree/master/griddb)](https://github.com/brianfrankcooper/YCSB/tree/master/griddb)    * [GridDB connector for KairosDB](https://github.com/griddb/griddb_kairosdb)    * [GridDB connector for Apache Spark](https://github.com/griddb/griddb_spark)    * [GridDB Foreign Data Wrapper for PostgreSQL (https://github.com/pgspider/griddb_fdw)](https://github.com/pgspider/griddb_fdw)    * [GridDB Sample Application for Apache Kafka](https://github.com/griddb/griddb_kafka_sample_app)    * [GridDB Data Source for Grafana](https://github.com/griddb/griddb-datasource)    * [GridDB Plugin for Redash](https://github.com/griddb/griddb-redash)    * [GridDB Plugin for Fluentd](https://github.com/griddb/fluent-plugin-griddb)    * [GridDB Plugin for Tableau](https://github.com/griddb/tableau-plugin-griddb)    ## [Packages](docs/Packages.md)    ## Community    * Issues        Use the GitHub issue function if you have any requests, questions, or bug reports.    * PullRequest        Use the GitHub pull request function if you want to contribute code.      You'll need to agree GridDB Contributor License Agreement(CLA_rev1.1.pdf).      By using the GitHub pull request function, you shall be deemed to have agreed to GridDB Contributor License Agreement.    ## License    The server source license is GNU Affero General Public License (AGPL),    while the Java client library license and the operational commands is Apache License, version 2.0.    See 3rd_party/3rd_party.md for the source and license of the third party. """
Big data;https://github.com/benedekrozemberczki/awesome-decision-tree-papers;"""# Awesome Decision Tree Research Papers  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  [![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-decision-tree-papers.svg)](https://github.com/benedekrozemberczki/awesome-decision-tree-papers/archive/master.zip)   ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-decision-tree-papers.svg?color=blue) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)  <p align=""center"">    <img width=""300"" src=""tree.png"">  </p>    A curated list of classification and regression tree research papers with implementations from the following conferences:    - Machine learning     * [NeurIPS](https://nips.cc/)      * [ICML](https://icml.cc/)      * [ICLR](https://iclr.cc/)  - Computer vision     * [CVPR](http://cvpr2019.thecvf.com/)     * [ICCV](http://iccv2019.thecvf.com/)     * [ECCV](https://eccv2018.org/)  - Natural language processing     * [ACL](http://www.acl2019.org/EN/index.xhtml)     * [NAACL](https://naacl2019.org/)     * [EMNLP](https://www.emnlp-ijcnlp2019.org/)   - Data     * [KDD](https://www.kdd.org/)     * [CIKM](http://www.cikmconference.org/)        * [ICDM](http://icdm2019.bigke.org/)     * [SDM](https://www.siam.org/Conferences/CM/Conference/sdm19)        * [PAKDD](http://pakdd2019.medmeeting.org)     * [PKDD/ECML](http://ecmlpkdd2019.org)     * [SIGIR](https://sigir.org/)     * [WWW](https://www2019.thewebconf.org/)     * [WSDM](www.wsdm-conference.org)   - Artificial intelligence     * [AAAI](https://www.aaai.org/)     * [AISTATS](https://www.aistats.org/)     * [ICANN](https://e-nns.org/icann2019/)        * [IJCAI](https://www.ijcai.org/)     * [UAI](http://www.auai.org/)    Similar collections about [graph classification](https://github.com/benedekrozemberczki/awesome-graph-classification), [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers), [fraud detection](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers), [Monte Carlo tree search](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers), and [community detection](https://github.com/benedekrozemberczki/awesome-community-detection) papers with implementations.      ## 2021    - **Online Probabilistic Label Trees (AISTATS 2021)**    - Kalina Jasinska-Kobus, Marek Wydmuch, Devanathan Thiruvenkatachari, Krzysztof Dembczyński    - [[Paper]](https://arxiv.org/abs/2007.04451)    - [[Code]](https://github.com/mwydmuch/napkinXC)    - **Optimal Decision Trees for Nonlinear Metrics (AAAI 2021)**    - Emir Demirovic, Peter J. Stuckey    - [[Paper]](https://arxiv.org/abs/2009.06921)    - **SAT-based Decision Tree Learning for Large Data Sets (AAAI 2021)**    - André Schidler, Stefan Szeider    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/16509)    - **Parameterized Complexity of Small Decision Tree Learning (AAAI 2021)**    - Sebastian Ordyniak, Stefan Szeider    - [[Paper]](https://www.ac.tuwien.ac.at/files/tr/ac-tr-21-002.pdf)    - **Counterfactual Explanations for Oblique Decision Trees: Exact - Efficient Algorithms (AAAI 2021)**    - Miguel Á. Carreira-Perpiñán, Suryabhan Singh Hada    - [[Paper]](https://arxiv.org/abs/2103.01096)    - **Geometric Heuristics for Transfer Learning in Decision Trees (CIKM 2021)**    - Siddhesh Chaubal, Mateusz Rzepecki, Patrick K. Nicholson, Guangyuan Piao, Alessandra Sala    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3482259)    - **Fairness-Aware Training of Decision Trees by Abstract Interpretation (CIKM 2021)**    - Francesco Ranzato, Caterina Urban, Marco Zanella    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3482342)    - **Enabling Efficiency-Precision Trade-offs for Label Trees in Extreme Classification (CIKM 2021)**    - Tavor Z. Baharav, Daniel L. Jiang, Kedarnath Kolluri, Sujay Sanghavi, Inderjit S. Dhillon    - [[Paper]](https://arxiv.org/abs/2106.00730)    - **Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees (ICLR 2021)**    - Zhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky, Marc Najork    - [[Paper]](https://openreview.net/forum?id=Ut1vF_q_vC)    - **NBDT: Neural-Backed Decision Tree (ICLR 2021)**    - Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Suzanne Petryk, Sarah Adel Bargal, Joseph E. Gonzalez    - [[Paper]](https://arxiv.org/abs/2004.00221)    - **Versatile Verification of Tree Ensembles (ICML 2021)**    - Laurens Devos, Wannes Meert, Jesse Davis    - [[Paper]](https://arxiv.org/abs/2010.13880)    - **Connecting Interpretability and Robustness in Decision Trees through Separation (ICML 2021)**    - Michal Moshkovitz, Yao-Yuan Yang, Kamalika Chaudhuri    - [[Paper]](https://arxiv.org/abs/2102.07048)    - **Optimal Counterfactual Explanations in Tree Ensembles (ICML 2021)**    - Axel Parmentier, Thibaut Vidal    - [[Paper]](https://arxiv.org/abs/2106.06631)    - **Efficient Training of Robust Decision Trees Against Adversarial Examples (ICML 2021)**    - Daniël Vos, Sicco Verwer    - [[Paper]](https://arxiv.org/abs/2012.10438)    - **Learning Binary Decision Trees by Argmin Differentiation (ICML 2021)**    - Valentina Zantedeschi, Matt J. Kusner, Vlad Niculae    - [[Paper]](https://arxiv.org/pdf/2010.04627.pdf)     - **BLOCKSET (Block-Aligned Serialized Trees): Reducing Inference Latency for Tree ensemble Deployment (KDD 2021)**    - Meghana Madhyastha, Kunal Lillaney, James Browne, Joshua T. Vogelstein, Randal Burns    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3447548.3467368)    - **ControlBurn: Feature Selection by Sparse Forests (KDD 2021)**    - Brian Liu, Miaolan Xie, Madeleine Udell    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3447548.3467387?sid=SCITRUS)    - **Probabilistic Gradient Boosting Machines for Large-Scale Probabilistic Regression (KDD 2021)**    - Olivier Sprangers, Sebastian Schelter, Maarten de Rijke    - [[Paper]](https://dl.acm.org/doi/10.1145/3447548.3467278)    - **Verifying Tree Ensembles by Reasoning about Potential Instances (SDM 2021)**    - Laurens Devos, Wannes Meert, Jesse Davis    - [[Paper]](https://arxiv.org/abs/2001.11905)    ## 2020    - **DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim Verification (ACL 2020)**    - Lianwei Wu, Yuan Rao, Yongqiang Zhao, Hao Liang, Ambreen Nazir    - [[Paper]](https://arxiv.org/abs/2004.13455)    - **Privacy-Preserving Gradient Boosting Decision Trees (AAAI 2020)**    - Qinbin Li, Zhaomin Wu, Zeyi Wen, Bingsheng He    - [[Paper]](https://arxiv.org/abs/1911.04209)    - **Practical Federated Gradient Boosting Decision Trees (AAAI 2020)**    - Qinbin Li, Zeyi Wen, Bingsheng He    - [[Paper]](https://arxiv.org/abs/1911.04206)    - **Efficient Inference of Optimal Decision Trees (AAAI 2020)**    - Florent Avellaneda    - [[Paper]](http://florent.avellaneda.free.fr/dl/AAAI20.pdf)    - [[Code]](https://github.com/FlorentAvellaneda/InferDT)    - **Learning Optimal Decision Trees Using Caching Branch-and-Bound Search (AAAI 2020)**    - Gael Aglin, Siegfried Nijssen, Pierre Schaus    - [[Paper]](https://dial.uclouvain.be/pr/boreal/fr/object/boreal%3A223390/datastream/PDF_01/view)    - [[Code]](https://pypi.org/project/dl8.5/)    - **Abstract Interpretation of Decision Tree Ensemble Classifiers (AAAI 2020)**    - Francesco Ranzato, Marco Zanella    - [[Paper]](https://www.math.unipd.it/~ranzato/papers/aaai20.pdf)    - [[Code]](https://github.com/abstract-machine-learning/silva)    - **Scalable Feature Selection for (Multitask) Gradient Boosted Trees (AISTATS 2020)**    - Cuize Han, Nikhil Rao, Daria Sorokina, Karthik Subbian    - [[Paper]](http://proceedings.mlr.press/v108/han20a.html)    - **Optimization Methods for Interpretable Differentiable Decision Trees Applied to Reinforcement Learning (AISTATS 2020)**    - Andrew Silva, Matthew C. Gombolay, Taylor W. Killian, Ivan Dario Jimenez Jimenez, Sung-Hyun Son    - [[Paper]](https://arxiv.org/abs/1903.09338)    - **Exploiting Categorical Structure Using Tree-Based Methods (AISTATS 2020)**    - Brian Lucena    - [[Paper]](https://arxiv.org/abs/2004.07383)    - **LdSM: Logarithm-depth Streaming Multi-label Decision Trees (AISTATS 2020)**    - Maryam Majzoubi, Anna Choromanska    - [[Paper]](https://arxiv.org/abs/1905.10428)      - **Oblique Decision Trees from Derivatives of ReLU Networks (ICLR 2020)**    - Guang-He Lee, Tommi S. Jaakkola    - [[Paper]](https://openreview.net/pdf?id=Bke8UR4FPB)    - [[Code]](https://github.com/guanghelee/iclr20-lcn)      - **Provable Guarantees for Decision Tree Induction: the Agnostic Setting (ICML 2020)**    - Guy Blanc, Jane Lange, Li-Yang Tan    - [[Paper]](https://arxiv.org/abs/2006.00743v1)    - **Decision Trees for Decision-Making under the Predict-then-Optimize Framework (ICML 2020)**    - Adam N. Elmachtoub, Jason Cheuk Nam Liang, Ryan McNellis    - [[Paper]](https://arxiv.org/abs/2003.00360)    - **The Tree Ensemble Layer: Differentiability meets Conditional Computation (ICML 2020)**    - Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, Rahul Mazumder    - [[Paper]](https://arxiv.org/abs/2002.07772)    - [[Code]](https://github.com/google-research/google-research/tree/master/tf_trees)    - **Generalized and Scalable Optimal Sparse Decision Trees (ICML 2020)**    - Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, Margo I. Seltzer    - [[Paper]](https://arxiv.org/abs/2006.08690)    - [[Code]](https://github.com/xiyanghu/OSDT)    - **Born-Again Tree Ensembles (ICML 2020)**    - Thibaut Vidal, Maximilian Schiffer    - [[Paper]](https://arxiv.org/abs/2003.11132)    - [[Code]](https://github.com/vidalt/BA-Trees)    - **On Lp-norm Robustness of Ensemble Decision Stumps and Trees (ICML 2020)**    - Yihan Wang, Huan Zhang, Hongge Chen, Duane S. Boning, Cho-Jui Hsieh    - [[Paper]](https://arxiv.org/abs/2008.08755)    - **Smaller, More Accurate Regression Forests Using Tree Alternating Optimization (ICML 2020)**    - Arman Zharmagambetov, Miguel Á. Carreira-Perpinan    - [[Paper]](http://proceedings.mlr.press/v119/zharmagambetov20a.html)      - **Learning Optimal Decision Trees with MaxSAT and its Integration in AdaBoost (IJCAI 2020)**    - Hao Hu, Mohamed Siala, Emmanuel Hebrard, Marie-José Huguet    - [[Paper]](https://www.ijcai.org/Proceedings/2020/163)    - **Speeding up Very Fast Decision Tree with Low Computational Cost (IJCAI 2020)**    - Jian Sun, Hongyu Jia, Bo Hu, Xiao Huang, Hao Zhang, Hai Wan, Xibin Zhao    - [[Paper]](https://www.ijcai.org/Proceedings/2020/0177.pdf)    - **PyDL8.5: a Library for Learning Optimal Decision Trees (IJCAI 2020)**    - Gaël Aglin, Siegfried Nijssen, Pierre Schaus    - [[Paper]](https://www.ijcai.org/Proceedings/2020/0750.pdf)    - [[Code]](https://github.com/aia-uclouvain/pydl8.5)    - **Geodesic Forests (KDD 2020)**    - Meghana Madhyastha, Gongkai Li, Veronika Strnadova-Neeley, James Browne, Joshua T. Vogelstein, Randal Burns    - [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3394486.3403094)    - **A Scalable MIP-based Method for Learning Optimal Multivariate Decision Trees (NeurIPS 2020)**    - Haoran Zhu, Pavankumar Murali, Dzung T. Phan, Lam M. Nguyen, Jayant Kalagnanam    - [[Paper]](https://arxiv.org/abs/2011.03375)    - **Estimating Decision Tree Learnability with Polylogarithmic Sample Complexity (NeurIPS 2020)**    - Guy Blanc, Neha Gupta, Jane Lange, Li-Yang Tan    - [[Paper]](https://arxiv.org/abs/2011.01584)     - **Universal Guarantees for Decision Tree Induction via a Higher-Order Splitting Criterion (NeurIPS 2020)**    - Guy Blanc, Neha Gupta, Jane Lange, Li-Yang Tan    - [[Paper]](https://arxiv.org/abs/2010.08633)    - **Smooth And Consistent Probabilistic Regression Trees (NeurIPS 2020)**    - Sami Alkhoury, Emilie Devijver, Marianne Clausel, Myriam Tami, Éric Gaussier, Georges Oppenheim    - [[Paper]](https://papers.nips.cc/paper/2020/file/8289889263db4a40463e3f358bb7c7a1-Paper.pdf)    - **An Efficient Adversarial Attack for Tree Ensembles (NeurIPS 2020)**    - Chong Zhang, Huan Zhang, Cho-Jui Hsieh    - [[Paper]](https://arxiv.org/abs/2010.11598)    - [[Code]](https://github.com/chong-z/tree-ensemble-attack)    - **Decision Trees as Partitioning Machines to Characterize their Generalization Properties (NeurIPS 2020)**    - Jean-Samuel Leboeuf, Frédéric Leblanc, Mario Marchand    - [[Paper]](https://papers.nips.cc/paper/2020/file/d2a10b0bd670e442b1d3caa3fbf9e695-Paper.pdf)      - **Evidence Weighted Tree Ensembles for Text Classification (SIGIR 2020)**    - Md. Zahidul Islam, Jixue Liu, Jiuyong Li, Lin Liu, Wei Kang    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3397271.3401229)      ## 2019    - **Multi-Level Deep Cascade Trees for Conversion Rate Prediction in Recommendation System (AAAI 2019)**    - Hong Wen, Jing Zhang, Quan Lin, Keping Yang, Pipei Huang    - [[Paper]](https://arxiv.org/pdf/1805.09484.pdf)      - **Induction of Non-Monotonic Logic Programs to Explain Boosted Tree Models Using LIME (AAAI 2019)**    - Farhad Shakerin, Gopal Gupta    - [[Paper]](https://arxiv.org/abs/1808.00629)      - **Learning Optimal and Fair Decision Trees for Non-Discriminative Decision-Making (AAAI 2019)**    - Sina Aghaei, Mohammad Javad Azizi, Phebe Vayanos    - [[Paper]](https://arxiv.org/abs/1903.10598)    - **Desiderata for Interpretability: Explaining Decision Tree Predictions with Counterfactuals (AAAI 2019)**    - Kacper Sokol, Peter A. Flach    - [[Paper]](https://aaai.org/ojs/index.php/AAAI/article/view/5154)      - **Weighted Oblique Decision Trees (AAAI 2019)**    - Bin-Bin Yang, Song-Qing Shen, Wei Gao    - [[Paper]](https://aaai.org/ojs/index.php/AAAI/article/view/4505)    - **Learning Optimal Classification Trees Using a Binary Linear Program Formulation (AAAI 2019)**    - Sicco Verwer, Yingqian Zhang    - [[Paper]](https://yingqianzhang.net/wp-content/uploads/2018/12/VerwerZhangAAAI-final.pdf)    - **Optimization of Hierarchical Regression Model with Application to Optimizing Multi-Response Regression K-ary Trees (AAAI 2019)**    - Pooya Tavallali, Peyman Tavallali, Mukesh Singhal    - [[Paper]](https://aaai.org/ojs/index.php/AAAI/article/view/4447/4325)    - **XBART: Accelerated Bayesian Additive Regression Trees (AISTATS 2019)**    - Jingyu He, Saar Yalov, P. Richard Hahn    - [[Paper]](https://arxiv.org/abs/1810.02215)    - **Interaction Detection with Bayesian Decision Tree Ensembles (AISTATS 2019)**    - Junliang Du, Antonio R. Linero    - [[Paper]](https://arxiv.org/abs/1809.08524)      - **Adversarial Training of Gradient-Boosted Decision Trees (CIKM 2019)**    - Stefano Calzavara, Claudio Lucchese, Gabriele Tolomei    - [[Paper]](https://www.dais.unive.it/~calzavara/papers/cikm19.pdf)    - **Interpretable MTL from Heterogeneous Domains using Boosted Tree (CIKM 2019)**    - Ya-Lin Zhang, Longfei Li    - [[Paper]](https://dl.acm.org/citation.cfm?id=3357384.3358072)    - **Interpreting CNNs via Decision Trees (CVPR 2019)**    - Quanshi Zhang, Yu Yang, Haotian Ma, Ying Nian Wu    - [[Paper]](https://arxiv.org/abs/1802.00121)      - **EDiT: Interpreting Ensemble Models via Compact Soft Decision Trees (ICDM 2019)**    - Jaemin Yoo, Lee Sael    - [[Paper]](https://github.com/leesael/EDiT/blob/master/docs/YooS19.pdf)    - [[Code]](https://github.com/leesael/EDiT)    - **Fair Adversarial Gradient Tree Boosting (ICDM 2019)**    - Vincent Grari, Boris Ruf, Sylvain Lamprier, Marcin Detyniecki    - [[Paper]](https://arxiv.org/abs/1911.05369)    - **Functional Transparency for Structured Data: a Game-Theoretic Approach (ICML 2019)**    - Guang-He Lee, Wengong Jin, David Alvarez-Melis, Tommi S. Jaakkola    - [[Paper]](http://proceedings.mlr.press/v97/lee19b/lee19b.pdf)      - **Incorporating Grouping Information into Bayesian Decision Tree Ensembles (ICML 2019)**    - Junliang Du, Antonio R. Linero    - [[Paper]](http://proceedings.mlr.press/v97/du19d.html)    - **Adaptive Neural Trees (ICML 2019)**    - Ryutaro Tanno, Kai Arulkumaran, Daniel C. Alexander, Antonio Criminisi, Aditya V. Nori    - [[Paper]](https://arxiv.org/abs/1807.06699)    - [[Code]](https://github.com/rtanno21609/AdaptiveNeuralTrees)    - **Robust Decision Trees Against Adversarial Examples (ICML 2019)**    - Hongge Chen, Huan Zhang, Duane S. Boning, Cho-Jui Hsieh    - [[Paper]](https://arxiv.org/abs/1902.10660)    - [[Code]](https://github.com/chenhongge/RobustTrees)      - **Learn Smart with Less: Building Better Online Decision Trees with Fewer Training Examples (IJCAI 2019)**    - Ariyam Das, Jin Wang, Sahil M. Gandhi, Jae Lee, Wei Wang, Carlo Zaniolo    - [[Paper]](https://www.ijcai.org/proceedings/2019/0306.pdf)      - **FAHT: An Adaptive Fairness-aware Decision Tree Classifier (IJCAI 2019)**    - Wenbin Zhang, Eirini Ntoutsi    - [[Paper]](https://arxiv.org/abs/1907.07237)    - [[Code]](https://github.com/vanbanTruong/FAHT)    - **Inter-node Hellinger Distance based Decision Tree (IJCAI 2019)**    - Pritom Saha Akash, Md. Eusha Kadir, Amin Ahsan Ali, Mohammad Shoyaib    - [[Paper]](https://www.ijcai.org/proceedings/2019/0272.pdf)    - [[Matlab Code]](https://github.com/ZDanielsResearch/HellingerTreesMatlab)    - [[R Code]](https://github.com/kaustubhrpatil/HDDT)    - **Gradient Boosting with Piece-Wise Linear Regression Trees (IJCAI 2019)**    - Yu Shi, Jian Li, Zhize Li    - [[Paper]](https://arxiv.org/abs/1802.05640)    - [[Code]](https://github.com/GBDT-PL/GBDT-PL)    - **A Gradient-Based Split Criterion for Highly Accurate and Transparent Model Trees (IJCAI 2019)**    - Klaus Broelemann, Gjergji Kasneci    - [[Paper]](https://arxiv.org/abs/1809.09703)      - **Combining Decision Trees and Neural Networks for Learning-to-Rank in Personal Search (KDD 2019)**    - Pan Li, Zhen Qin, Xuanhui Wang, Donald Metzler    - [[Paper]](https://ai.google/research/pubs/pub48133/)      - **Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers (NeurIPS 2019)**    - Guang-He Lee, Yang Yuan, Shiyu Chang, Tommi S. Jaakkola    - [[Paper]](https://papers.nips.cc/paper/8737-tight-certificates-of-adversarial-robustness-for-randomly-smoothed-classifiers.pdf)    - [[Code]](https://github.com/guanghelee/Randomized_Smoothing)    - **Partitioning Structure Learning for Segmented Linear Regression Trees (NeurIPS 2019)**    - Xiangyu Zheng, Song Xi Chen    - [[Paper]](https://papers.nips.cc/paper/8494-partitioning-structure-learning-for-segmented-linear-regression-trees)      - **Provably Robust Boosted Decision Stumps and Trees against Adversarial Attacks (NeurIPS 2019)**    - Maksym Andriushchenko, Matthias Hein    - [[Paper]](https://arxiv.org/abs/1906.03526)    - [[Code]](https://github.com/max-andr/provably-robust-boosting)    - **Optimal Decision Tree with Noisy Outcomes (NeurIPS 2019)**    - Su Jia, Viswanath Nagarajan, Fatemeh Navidi, R. Ravi    - [[Paper]](https://papers.nips.cc/paper/8592-optimal-decision-tree-with-noisy-outcomes.pdf)    - [[Code]](https://github.com/sjia1/ODT-with-noisy-outcomes)    - **Regularized Gradient Boosting (NeurIPS 2019)**    - Corinna Cortes, Mehryar Mohri, Dmitry Storcheus    - [[Paper]](https://papers.nips.cc/paper/8784-regularized-gradient-boosting.pdf)    - **Optimal Sparse Decision Trees (NeurIPS 2019)**    - Xiyang Hu, Cynthia Rudin, Margo Seltzer    - [[Paper]](https://papers.nips.cc/paper/8947-optimal-sparse-decision-trees.pdf)    - [[Code]](https://github.com/xiyanghu/OSDT)      - **MonoForest framework for tree ensemble analysis (NeurIPS 2019)**    - Igor Kuralenok, Vasilii Ershov, Igor Labutin    - [[Paper]](https://papers.nips.cc/paper/9530-monoforest-framework-for-tree-ensemble-analysis)    - [[Code]](https://github.com/xiyanghu/OSDT)    - **Calibrating Probability Estimation Trees using Venn-Abers Predictors (SDM 2019)**    - Ulf Johansson, Tuwe Löfström, Henrik Boström    - [[Paper]](https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.4)      - **Fast Training for Large-Scale One-versus-All Linear Classifiers using Tree-Structured Initialization (SDM 2019)**    - Huang Fang, Minhao Cheng, Cho-Jui Hsieh, Michael P. Friedlander    - [[Paper]](https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.32)    - **Forest Packing: Fast Parallel, Decision Forests (SDM 2019)**    - James Browne, Disa Mhembere, Tyler M. Tomita, Joshua T. Vogelstein, Randal Burns    - [[Paper]](https://epubs.siam.org/doi/abs/10.1137/1.9781611975673.6)      - **Block-distributed Gradient Boosted Trees (SIGIR 2019)**    - Theodore Vasiloudis, Hyunsu Cho, Henrik Boström    - [[Paper]](https://arxiv.org/abs/1904.10522)      - **Entity Personalized Talent Search Models with Tree Interaction Features (WWW 2019)**    - Cagri Ozcaglar, Sahin Cem Geyik, Brian Schmitz, Prakhar Sharma, Alex Shelkovnykov, Yiming Ma, Erik Buchanan    - [[Paper]](https://arxiv.org/abs/1902.09041)    ## 2018  - **Adapting to Concept Drift in Credit Card Transaction Data Streams Using Contextual Bandits and Decision Trees (AAAI 2018)**    - Dennis J. N. J. Soemers, Tim Brys, Kurt Driessens, Mark H. M. Winands, Ann Nowé    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16183/16394)    - **MERCS: Multi-Directional Ensembles of Regression and Classification Trees (AAAI 2018)**    - Elia Van Wolputte, Evgeniya Korneva, Hendrik Blockeel    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16875/16735)    - [[Code]](https://github.com/eliavw/mercs-v5)    - **Differential Performance Debugging With Discriminant Regression Trees (AAAI 2018)**    - Saeid Tizpaz-Niari, Pavol Cerný, Bor-Yuh Evan Chang, Ashutosh Trivedi    - [[Paper]](https://arxiv.org/abs/1711.04076)    - [[Code]](https://github.com/cuplv/DPDEBUGGER)    - **Estimating the Class Prior in Positive and Unlabeled Data Through Decision Tree Induction (AAAI 2018)**    - Jessa Bekker, Jesse Davis    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16776)    - **MDP-Based Cost Sensitive Classification Using Decision Trees (AAAI 2018)**    - Shlomi Maliah, Guy Shani    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17128)    - **Generative Adversarial Image Synthesis With Decision Tree Latent Controller (CVPR 2018)**    - Takuhiro Kaneko, Kaoru Hiramatsu, Kunio Kashino    - [[Paper]](https://arxiv.org/abs/1805.10603)    - [[Code]](https://github.com/LynnHo/DTLC-GAN-Tensorflow)    - **Enhancing Very Fast Decision Trees with Local Split-Time Predictions (ICDM 2018)**    - Viktor Losing, Heiko Wersing, Barbara Hammer    - [[Paper]](https://www.techfak.uni-bielefeld.de/~hwersing/LosingHammerWersing_ICDM2018.pdf)    - [[Code]](https://github.com/ICDM2018Submission/VFDT-split-time-prediction)      - **Realization of Random Forest for Real-Time Evaluation through Tree Framing (ICDM 2018)**    - Sebastian Buschjäger, Kuan-Hsun Chen, Jian-Jia Chen, Katharina Morik    - [[Paper]](https://sfb876.tu-dortmund.de/PublicPublicationFiles/buschjaeger_2018a.pdf)    - **Finding Influential Training Samples for Gradient Boosted Decision Trees (ICML 2018)**    - Boris Sharchilev, Yury Ustinovskiy, Pavel Serdyukov, Maarten de Rijke    - [[Paper]](https://arxiv.org/abs/1802.06640)    - [[Code]](https://github.com/bsharchilev/influence_boosting)    - **Learning Optimal Decision Trees with SAT (IJCAI 2018)**    - Nina Narodytska, Alexey Ignatiev, Filipe Pereira, João Marques-Silva    - [[Paper]](https://www.ijcai.org/proceedings/2018/0189.pdf)    - **Extremely Fast Decision Tree (KDD 2018)**    - Chaitanya Manapragada, Geoffrey I. Webb, Mahsa Salehi    - [[Paper]](https://arxiv.org/abs/1802.08780)    - [[Code]](https://github.com/doubleplusplus/incremental_decision_tree-CART-Random_Forest_python)      - **RapidScorer: Fast Tree Ensemble Evaluation by Maximizing Compactness in Data Level Parallelization (KDD 2018)**    - Ting Ye, Hucheng Zhou, Will Y. Zou, Bin Gao, Ruofei Zhang    - [[Paper]](http://ai.stanford.edu/~wzou/kdd_rapidscorer.pdf)      - **CatBoost: Unbiased Boosting with Categorical Features (NIPS 2018)**    - Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, Andrey Gulin    - [[Paper]](https://papers.nips.cc/paper/7898-catboost-unbiased-boosting-with-categorical-features.pdf)    - [[Code]](https://catboost.ai/)      - **Active Learning for Non-Parametric Regression Using Purely Random Trees (NIPS 2018)**    - Jack Goetz, Ambuj Tewari, Paul Zimmerman    - [[Paper]](https://papers.nips.cc/paper/7520-active-learning-for-non-parametric-regression-using-purely-random-trees.pdf)    - **Alternating Optimization of Decision Trees with Application to Learning Sparse Oblique Trees (NIPS 2018)**    - Miguel Á. Carreira-Perpiñán, Pooya Tavallali    - [[Paper]](https://papers.nips.cc/paper/7397-alternating-optimization-of-decision-trees-with-application-to-learning-sparse-oblique-trees)    - **Multi-Layered Gradient Boosting Decision Trees (NIPS 2018)**    - Ji Feng, Yang Yu, Zhi-Hua Zhou    - [[Paper]](https://papers.nips.cc/paper/7614-multi-layered-gradient-boosting-decision-trees.pdf)    - [[Code]](https://github.com/kingfengji/mGBDT)      - **Transparent Tree Ensembles (SIGIR 2018)**    - Alexander Moore, Vanessa Murdock, Yaxiong Cai, Kristine Jones    - [[Paper]](http://delivery.acm.org/10.1145/3220000/3210151/p1241-moore.pdf?ip=129.215.164.203&id=3210151&acc=ACTIVE%20SERVICE&key=C2D842D97AC95F7A%2EEB9E991028F4E1F1%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1559054892_a29816c683aa83a0ce0fbb777c68daba)    - **Privacy-aware Ranking with Tree Ensembles on the Cloud (SIGIR 2018)**    - Shiyu Ji, Jinjin Shao, Daniel Agun, Tao Yang    - [[Paper]](https://sites.cs.ucsb.edu/projects/ds/sigir18.pdf)    ## 2017  - **Strategic Sequences of Arguments for Persuasion Using Decision Trees (AAAI 2017)**    - Emmanuel Hadoux, Anthony Hunter    - [[Paper]](http://www0.cs.ucl.ac.uk/staff/a.hunter/papers/aaai17.pdf)    - **BoostVHT: Boosting Distributed Streaming Decision Trees (CIKM 2017)**    - Theodore Vasiloudis, Foteini Beligianni, Gianmarco De Francisci Morales    - [[Paper]](https://melmeric.files.wordpress.com/2010/05/boostvht-boosting-distributed-streaming-decision-trees.pdf)    - **Latency Reduction via Decision Tree Based Query Construction (CIKM 2017)**    - Aman Grover, Dhruv Arya, Ganesh Venkataraman    - [[Paper]](https://dl.acm.org/citation.cfm?id=3132865)    - **Enumerating Distinct Decision Trees (ICML 2017)**    - Salvatore Ruggieri    - [[Paper]](http://proceedings.mlr.press/v70/ruggieri17a/ruggieri17a.pdf)    - **Gradient Boosted Decision Trees for High Dimensional Sparse Output (ICML 2017)**    - Si Si, Huan Zhang, S. Sathiya Keerthi, Dhruv Mahajan, Inderjit S. Dhillon, Cho-Jui Hsieh    - [[Paper]](http://proceedings.mlr.press/v70/si17a.html)    - [[Code]](https://github.com/springdaisy/GBDT)    - **Consistent Feature Attribution for Tree Ensembles (ICML 2017)**    - Scott M. Lundberg, Su-In Lee    - [[Paper]](https://arxiv.org/abs/1706.06060)    - [[Code]](https://github.com/slundberg/shap)    - **Extremely Fast Decision Tree Mining for Evolving Data Streams (KDD 2017)**    - Albert Bifet, Jiajin Zhang, Wei Fan, Cheng He, Jianfeng Zhang, Jianfeng Qian, Geoff Holmes, Bernhard Pfahringer    - [[Paper]](https://core.ac.uk/download/pdf/151040580.pdf)      - **CatBoost: Gradient Boosting with Categorical Features Support (NIPS 2017)**    - Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin    - [[Paper]](https://arxiv.org/abs/1810.11363)    - [[Code]](https://catboost.ai/)    - **LightGBM: A Highly Efficient Gradient Boosting Decision Tree (NIPS 2017)**    - Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu    - [[Paper]](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)    - [[Code]](https://lightgbm.readthedocs.io/en/latest/)    - **Variable Importance Using Decision Trees (NIPS 2017)**    - Jalil Kazemitabar, Arash Amini, Adam Bloniarz, Ameet S. Talwalkar    - [[Paper]](https://papers.nips.cc/paper/6646-variable-importance-using-decision-trees)    - **A Unified Approach to Interpreting Model Predictions (NIPS 2017)**    - Scott M. Lundberg, Su-In Lee    - [[Paper]](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions)    - [[Code]](https://github.com/slundberg/shap)    - **Pruning Decision Trees via Max-Heap Projection (SDM 2017)**    - Zhi Nie, Binbin Lin, Shuai Huang, Naren Ramakrishnan, Wei Fan, Jieping Ye    - [[Paper]](https://www.researchgate.net/publication/317485748_Pruning_Decision_Trees_via_Max-Heap_Projection)    - **A Practical Method for Solving Contextual Bandit Problems Using Decision Trees (UAI 2017)**    - Adam N. Elmachtoub, Ryan McNellis, Sechan Oh, Marek Petrik    - [[Paper]](https://arxiv.org/abs/1706.04687)    - **Complexity of Solving Decision Trees with Skew-Symmetric Bilinear Utility (UAI 2017)**    - Hugo Gilbert, Olivier Spanjaard    - [[Paper]](http://auai.org/uai2017/proceedings/papers/64.pdf)      - **GB-CENT: Gradient Boosted Categorical Embedding and Numerical Trees (WWW 2017)**    - Qian Zhao, Yue Shi, Liangjie Hong    - [[Paper]](http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p1311.pdf)    ## 2016  - **Sparse Perceptron Decision Tree for Millions of Dimensions (AAAI 2016)**    - Weiwei Liu, Ivor W. Tsang    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12111)    - **Learning Online Smooth Predictors for Realtime Camera Planning Using Recurrent Decision Trees (CVPR 2016)**    - Jianhui Chen, Hoang Minh Le, Peter Carr, Yisong Yue, James J. Little    - [[Paper]](http://hoangle.info/papers/cvpr2016_online_smooth_long.pdf)    - **Online Learning with Bayesian Classification Trees (CVPR 2016)**    - Samuel Rota Bulò, Peter Kontschieder    - [[Paper]](http://www.dsi.unive.it/~srotabul/files/publications/CVPR2016.pdf)    - **Accurate Robust and Efficient Error Estimation for Decision Trees (ICML 2016)**    - Lixin Fan    - [[Paper]](http://proceedings.mlr.press/v48/fan16.pdf)    - **Meta-Gradient Boosted Decision Tree Model for Weight and Target Learning (ICML 2016)**    - Yury Ustinovskiy, Valentina Fedorova, Gleb Gusev, Pavel Serdyukov    - [[Paper]](http://proceedings.mlr.press/v48/ustinovskiy16.html)    - **Boosted Decision Tree Regression Adjustment for Variance Reduction in Online Controlled Experiments (KDD 2016)**    - Alexey Poyarkov, Alexey Drutsa, Andrey Khalyavin, Gleb Gusev, Pavel Serdyukov    - [[Paper]](https://www.kdd.org/kdd2016/papers/files/adf0653-poyarkovA.pdf)      - **XGBoost: A Scalable Tree Boosting System (KDD 2016)**    - Tianqi Chen, Carlos Guestrin    - [[Paper]](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)    - [[Code]](https://xgboost.readthedocs.io/en/latest/)    - **Yggdrasil: An Optimized System for Training Deep Decision Trees at Scale (NIPS 2016)**    - Firas Abuzaid, Joseph K. Bradley, Feynman T. Liang, Andrew Feng, Lee Yang, Matei Zaharia, Ameet S. Talwalkar    - [[Paper]](https://papers.nips.cc/paper/6366-yggdrasil-an-optimized-system-for-training-deep-decision-trees-at-scale)    - **A Communication-Efficient Parallel Algorithm for Decision Tree (NIPS 2016)**    - Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhiming Ma, Tie-Yan Liu    - [[Paper]](https://arxiv.org/abs/1611.01276)    - [[Code]](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst)    - **Exploiting CPU SIMD Extensions to Speed-up Document Scoring with Tree Ensembles (SIGIR 2016)**    - Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, Rossano Venturini    - [[Paper]](http://pages.di.unipi.it/rossano/wp-content/uploads/sites/7/2016/07/SIGIR16a.pdf)    - [[Code]](https://github.com/hpclab/vectorized-quickscorer)    - **Post-Learning Optimization of Tree Ensembles for Efficient Ranking (SIGIR 2016)**    - Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Fabrizio Silvestri, Salvatore Trani    - [[Paper]](https://www.researchgate.net/publication/305081572_Post-Learning_Optimization_of_Tree_Ensembles_for_Efficient_Ranking)    - [[Code]](https://github.com/hpclab/quickrank)    ## 2015  - **Particle Gibbs for Bayesian Additive Regression Trees (AISTATS 2015)**    - Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh    - [[Paper]](https://arxiv.org/abs/1502.04622)    - **DART: Dropouts Meet Multiple Additive Regression Trees (AISTATS 2015)**    - Korlakai Vinayak Rashmi, Ran Gilad-Bachrach    - [[Paper]](https://arxiv.org/abs/1505.01866)    - [[Code]](https://xgboost.readthedocs.io/en/latest/)    - **Single Target Tracking Using Adaptive Clustered Decision Trees and Dynamic Multi-level Appearance Models (CVPR 2015)**    - Jingjing Xiao, Rustam Stolkin, Ales Leonardis    - [[Paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/3B_058.pdf)    - **Face Alignment Using Cascade Gaussian Process Regression Trees (CVPR 2015)**    - Donghoon Lee, Hyunsin Park, Chang Dong Yoo    - [[Paper]](https://slsp.kaist.ac.kr/paperdata/Face_Alignment_Using.pdf)    - [[Code]](https://github.com/donghoonlee04/cGPRT)    - **Tracking-by-Segmentation with Online Gradient Boosting Decision Tree (ICCV 2015)**    - Jeany Son, Ilchae Jung, Kayoung Park, Bohyung Han    - [[Paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Son_Tracking-by-Segmentation_With_Online_ICCV_2015_paper.pdf)    - **Entropy Evaluation Based on Confidence Intervals of Frequency Estimates : Application to the Learning of Decision Trees (ICML 2015)**    - Mathieu Serrurier, Henri Prade    - [[Paper]](http://proceedings.mlr.press/v37/serrurier15.pdf)      - **Large-scale Distributed Dependent Nonparametric Trees (ICML 2015)**    - Zhiting Hu, Qirong Ho, Avinava Dubey, Eric P. Xing    - [[Paper]](https://www.cs.cmu.edu/~zhitingh/data/icml15hu.pdf)    - **Optimal Action Extraction for Random Forests and Boosted Trees (KDD 2015)**    - Zhicheng Cui, Wenlin Chen, Yujie He, Yixin Chen    - [[Paper]](https://www.cse.wustl.edu/~ychen/public/OAE.pdf)    - **A Decision Tree Framework for Spatiotemporal Sequence Prediction (KDD 2015)**    - Taehwan Kim, Yisong Yue, Sarah L. Taylor, Iain A. Matthews    - [[Paper]](http://www.yisongyue.com/publications/kdd2015_ssw_dt.pdf)    - **Efficient Non-greedy Optimization of Decision Trees (NIPS 2015)**    - Mohammad Norouzi, Maxwell D. Collins, Matthew Johnson, David J. Fleet, Pushmeet Kohli    - [[Paper]](https://arxiv.org/abs/1511.04056)      - **QuickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees (SIGIR 2015)**    - Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, Rossano Venturini    - [[Paper]](http://pages.di.unipi.it/rossano/wp-content/uploads/sites/7/2015/11/sigir15.pdf)    - [[Code]](https://github.com/hpclab/quickrank)    ## 2014    - **A Mixtures-of-Trees Framework for Multi-Label Classification (CIKM 2014)**    - Charmgil Hong, Iyad Batal, Milos Hauskrecht    - [[Paper]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410801/)      - **On Building Decision Trees from Large-scale Data in Applications of On-line Advertising (CIKM 2014)**    - Shivaram Kalyanakrishnan, Deepthi Singh, Ravi Kant    - [[Paper]](https://www.cse.iitb.ac.in/~shivaram/papers/ksk_cikm_2014.pdf)    - **Fast Supervised Hashing with Decision Trees for High-Dimensional Data (CVPR 2014)**    - Guosheng Lin, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, David Suter    - [[Paper]](https://arxiv.org/abs/1404.1561)    - **One Millisecond Face Alignment with an Ensemble of Regression Trees (CVPR 2014)**    - Vahid Kazemi, Josephine Sullivan    - [[Paper]](https://www.researchgate.net/publication/264419855_One_Millisecond_Face_Alignment_with_an_Ensemble_of_Regression_Trees)      - **The return of AdaBoost.MH: multi-class Hamming trees (ICLR 2014)**    - Balázs Kégl    - [[Paper]](https://arxiv.org/pdf/1312.6086.pdf)    - **Diagnosis Determination: Decision Trees Optimizing Simultaneously Worst and Expected Testing Cost (ICML 2014)**    - Ferdinando Cicalese, Eduardo Sany Laber, Aline Medeiros Saettler    - [[Paper]](https://pdfs.semanticscholar.org/47ae/852f83b76f95b27ab00308d04f6020bdf71f.pdf)      - **Learning Multiple-Question Decision Trees for Cold-Start Recommendation (WSDM 2013)**    - Mingxuan Sun, Fuxin Li, Joonseok Lee, Ke Zhou, Guy Lebanon, Hongyuan Zha    - [[Paper]](http://www.joonseok.net/papers/coldstart.pdf)    ## 2013  - **Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria (ICCV 2013)**    - Christoph N. Straehle, Ullrich Köthe, Fred A. Hamprecht    - [[Paper]](https://ieeexplore.ieee.org/document/6751340)    - **Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees (ICCV 2013)**    - Oisin Mac Aodha, Gabriel J. Brostow    - [[Paper]](https://ieeexplore.ieee.org/document/6751133)    - **Conformal Prediction Using Decision Trees (ICDM 2013)**    - Ulf Johansson, Henrik Boström, Tuve Löfström    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/6729517)    - **Focal-Test-Based Spatial Decision Tree Learning: A Summary of Results (ICDM 2013)**    - Zhe Jiang, Shashi Shekhar, Xun Zhou, Joseph K. Knight, Jennifer Corcoran    - [[Paper]](https://pdfs.semanticscholar.org/f28e/df8d9eed76e4ce97cb6bd4182d590547be5e.pdf)    - **Top-down Particle Filtering for Bayesian Decision Trees (ICML 2013)**    - Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh    - [[Paper]](https://arxiv.org/abs/1303.0561)    - **Quickly Boosting Decision Trees - Pruning Underachieving Features Early (ICML 2013)**    - Ron Appel, Thomas J. Fuchs, Piotr Dollár, Pietro Perona    - [[Paper]](http://proceedings.mlr.press/v28/appel13.pdf)    - **Knowledge Compilation for Model Counting: Affine Decision Trees (IJCAI 2013)**    - Frédéric Koriche, Jean-Marie Lagniez, Pierre Marquis, Samuel Thomas    - [[Paper]](https://www.researchgate.net/publication/262398921_Knowledge_Compilation_for_Model_Counting_Affine_Decision_Trees)      - **Understanding Variable Importances in Forests of Randomized Trees (NIPS 2013)**    - Gilles Louppe, Louis Wehenkel, Antonio Sutera, Pierre Geurts    - [[Paper]](https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees)    - **Regression-tree Tuning in a Streaming Setting (NIPS 2013)**    - Samory Kpotufe, Francesco Orabona    - [[Paper]](https://papers.nips.cc/paper/4898-regression-tree-tuning-in-a-streaming-setting)    - **Learning Max-Margin Tree Predictors (UAI 2013)**    - Ofer Meshi, Elad Eban, Gal Elidan, Amir Globerson    - [[Paper]](https://ttic.uchicago.edu/~meshi/papers/mtreen.pdf)    ## 2012  - **Regression Tree Fields - An Efficient, Non-parametric Approach to Image Labeling Problems (CVPR 2012)**    - Jeremy Jancsary, Sebastian Nowozin, Toby Sharp, Carsten Rother    - [[Paper]](http://www.nowozin.net/sebastian/papers/jancsary2012rtf.pdf)    - **ConfDTree: Improving Decision Trees Using Confidence Intervals (ICDM 2012)**    - Gilad Katz, Asaf Shabtai, Lior Rokach, Nir Ofek    - [[Paper]](https://ieeexplore.ieee.org/document/6413889)    - **Improved Information Gain Estimates for Decision Tree Induction (ICML 2012)**    - Sebastian Nowozin    - [[Paper]](https://arxiv.org/abs/1206.4620)    - **Learning Partially Observable Models Using Temporally Abstract Decision Trees (NIPS 2012)**    - Erik Talvitie    - [[Paper]](https://papers.nips.cc/paper/4662-learning-partially-observable-models-using-temporally-abstract-decision-trees)      - **Subtree Replacement in Decision Tree Simplification (SDM 2012)**    - Salvatore Ruggieri    - [[Paper]](http://pages.di.unipi.it/ruggieri/Papers/sdm2012.pdf)    ## 2011  - **Incorporating Boosted Regression Trees into Ecological Latent Variable Models (AAAI 2011)**    - Rebecca A. Hutchinson, Li-Ping Liu, Thomas G. Dietterich    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewFile/3711/4086)    - **Syntactic Decision Tree LMs: Random Selection or Intelligent Design (EMNLP 2011)**    - Denis Filimonov, Mary P. Harper    - [[Paper]](https://www.aclweb.org/anthology/D11-1064)      - **Decision Tree Fields (ICCV 2011)**    - Sebastian Nowozin, Carsten Rother, Shai Bagon, Toby Sharp, Bangpeng Yao, Pushmeet Kohli    - [[Paper]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/nrbsyk_iccv11.pdf)    - **Confidence in Predictions from Random Tree Ensembles (ICDM 2011)**    - Siddhartha Bhattacharyya    - [[Paper]](https://link.springer.com/article/10.1007/s10115-012-0600-z)    - **Speeding-Up Hoeffding-Based Regression Trees With Options (ICML 2011)**    - Elena Ikonomovska, João Gama, Bernard Zenko, Saso Dzeroski    - [[Paper]](https://icml.cc/Conferences/2011/papers/349_icmlpaper.pdf)      - **Density Estimation Trees (KDD 2011)**    - Parikshit Ram, Alexander G. Gray    - [[Paper]](https://mlpack.org/papers/det.pdf)      - **Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models (SIGIR 2011)**    - Yasser Ganjisaffar, Rich Caruana, Cristina Videira Lopes    - [[Paper]](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/materials/bagging_lmbamart_jforests.pdf)    - **On the Complexity of Decision Making in Possibilistic Decision Trees (UAI 2011)**    - Hélène Fargier, Nahla Ben Amor, Wided Guezguez    - [[Paper]](https://dslpitt.org/uai/papers/11/p203-fargier.pdf)    - **Adaptive Bootstrapping of Recommender Systems Using Decision Trees (WSDM 2011)**    - Nadav Golbandi, Yehuda Koren, Ronny Lempel    - [[Paper]](https://dl.acm.org/citation.cfm?id=1935910)    - **Parallel Boosted Regression Trees for Web Search Ranking (WWW 2011)**    - Stephen Tyree, Kilian Q. Weinberger, Kunal Agrawal, Jennifer Paykin    - [[Paper]](http://www.cs.cornell.edu/~kilian/papers/fr819-tyreeA.pdf)    ## 2010  - **Discrimination Aware Decision Tree Learning (ICDM 2010)**    - Faisal Kamiran, Toon Calders, Mykola Pechenizkiy    - [[Paper]](https://www.win.tue.nl/~mpechen/publications/pubs/KamiranICDM2010.pdf)    - **Decision Trees for Uplift Modeling (ICDM 2010)**    - Piotr Rzepakowski, Szymon Jaroszewicz    - [[Paper]](https://core.ac.uk/download/pdf/81899141.pdf)    - **Learning Markov Network Structure with Decision Trees (ICDM 2010)**    - Daniel Lowd, Jesse Davis    - [[Paper]](https://ix.cs.uoregon.edu/~lowd/icdm10lowd.pdf)    - **Multivariate Dyadic Regression Trees for Sparse Learning Problems (NIPS 2010)**    - Han Liu, Xi Chen    - [[Paper]](https://papers.nips.cc/paper/4178-multivariate-dyadic-regression-trees-for-sparse-learning-problems.pdf)      - **Fast and Accurate Gene Prediction by Decision Tree Classification (SDM 2010)**    - Rong She, Jeffrey Shih-Chieh Chu, Ke Wang, Nansheng Chen    - [[Paper]](http://www.sfu.ca/~chenn/genBlastDT_sdm.pdf)    - **A Robust Decision Tree Algorithm for Imbalanced Data Sets (SDM 2010)**    - Wei Liu, Sanjay Chawla, David A. Cieslak, Nitesh V. Chawla    - [[Paper]](https://www3.nd.edu/~nchawla/papers/SDM10.pdf)    ## 2009  - **Stochastic Gradient Boosted Distributed Decision Trees (CIKM 2009)**    - Jerry Ye, Jyh-Herng Chow, Jiang Chen, Zhaohui Zheng    - [[Paper]](https://dl.acm.org/citation.cfm?id=1646301)      - **Feature Selection for Ranking Using Boosted Trees (CIKM 2009)**    - Feng Pan, Tim Converse, David Ahn, Franco Salvetti, Gianluca Donato    - [[Paper]](http://www.francosalvetti.com/cikm09_camera2.pdf)      - **Thai Word Segmentation with Hidden Markov Model and Decision Tree (PAKDD 2009)**    - Poramin Bheganan, Richi Nayak, Yue Xu    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-642-01307-2_10)    - **Parameter Estimdation in Semi-Random Decision Tree Ensembling on Streaming Data (PAKDD 2009)**    - Pei-Pei Li, Qianhui Liang, Xindong Wu, Xuegang Hu    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-642-01307-2_35)    - **DTU: A Decision Tree for Uncertain Data (PAKDD 2009)**    - Biao Qin, Yuni Xia, Fang Li    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-642-01307-2_4)    ## 2008  - **Predicting Future Decision Trees from Evolving Data (ICDM 2008)**    - Mirko Böttcher, Martin Spott, Rudolf Kruse    - [[Paper]](https://ieeexplore.ieee.org/document/4781098)    - **Bayes Optimal Classification for Decision Trees (ICML 2008)**    - Siegfried Nijssen    - [[Paper]](http://icml2008.cs.helsinki.fi/papers/455.pdf)      - **A New Credit Scoring Method Based on Rough Sets and Decision Tree (PAKDD 2008)**    - XiYue Zhou, Defu Zhang, Yi Jiang    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-68125-0_117)    - **A Comparison of Different Off-Centered Entropies to Deal with Class Imbalance for Decision Trees (PAKDD 2008)**    - Philippe Lenca, Stéphane Lallich, Thanh-Nghi Do, Nguyen-Khang Pham    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-68125-0_59)    - **BOAI: Fast Alternating Decision Tree Induction Based on Bottom-Up Evaluation (PAKDD 2008)**    - Bishan Yang, Tengjiao Wang, Dongqing Yang, Lei Chang    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-68125-0_36)    - **A General Framework for Estimating Similarity of Datasets and Decision Trees: Exploring Semantic Similarity of Decision Trees (SDM 2008)**    - Irene Ntoutsi, Alexandros Kalousis, Yannis Theodoridis    - [[Paper]](https://www.researchgate.net/publication/220907047_A_general_framework_for_estimating_similarity_of_datasets_and_decision_trees_exploring_semantic_similarity_of_decision_trees)    - **ROC-tree: A Novel Decision Tree Induction Algorithm Based on Receiver Operating Characteristics to Classify Gene Expression Data (SDM 2008)**    - M. Maruf Hossain, Md. Rafiul Hassan, James Bailey    - [[Paper]](https://pdfs.semanticscholar.org/bd80/db2f0903169b7611d34b2cc85f60a736375d.pdf)    ## 2007    - **Tree-based Classifiers for Bilayer Video Segmentation (CVPR 2007)**    - Pei Yin, Antonio Criminisi, John M. Winn, Irfan A. Essa    - [[Paper]](https://ieeexplore.ieee.org/document/4270033)    - **Additive Groves of Regression Trees (ECML 2007)**    - Daria Sorokina, Rich Caruana, Mirek Riedewald    - [[Paper]](http://additivegroves.net/papers/groves.pdf)    - **Decision Tree Instability and Active Learning (ECML 2007)**    - Kenneth Dwyer, Robert Holte    - [[Paper]](https://webdocs.cs.ualberta.ca/~holte/Publications/ecml07.pdf)    - **Ensembles of Multi-Objective Decision Trees (ECML 2007)**    - Dragi Kocev, Celine Vens, Jan Struyf, Saso Dzeroski    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-74958-5_61)    - **Seeing the Forest Through the Trees: Learning a Comprehensible Model from an Ensemble (ECML 2007)**    - Anneleen Van Assche, Hendrik Blockeel    - [[Paper]](http://ftp.cs.wisc.edu/machine-learning/shavlik-group/ilp07wip/ilp07_assche.pdf)    - **Sample Compression Bounds for Decision Trees (ICML 2007)**    - Mohak Shah    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.9136&rep=rep1&type=pdf)    - **A Tighter Error Bound for Decision Tree Learning Using PAC Learnability (IJCAI 2007)**    - Chaithanya Pichuka, Raju S. Bapi, Chakravarthy Bhagvati, Arun K. Pujari, Bulusu Lakshmana Deekshatulu    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/163.pdf)    - **Keep the Decision Tree and Estimate the Class Probabilities Using its Decision Boundary (IJCAI 2007)**    - Isabelle Alvarez, Stephan Bernard, Guillaume Deffuant    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/104.pdf)    - **Real Boosting a la Carte with an Application to Boosting Oblique Decision Tree (IJCAI 2007)**    - Claudia Henry, Richard Nock, Frank Nielsen    - [[Paper]](https://www.ijcai.org/Proceedings/07/Papers/135.pdf)    - **Scalable Look-ahead Linear Regression Trees (KDD 2007)**    - David S. Vogel, Ognian Asparouhov, Tobias Scheffer    - [[Paper]](https://www.cs.uni-potsdam.de/ml/publications/kdd2007.pdf)    - **Mining Optimal Decision Trees from Itemset Lattices (KDD 2007)**    - Siegfried Nijssen, Élisa Fromont    - [[Paper]](https://hal.archives-ouvertes.fr/hal-00372011/document)      - **A Hybrid Multi-group Privacy-Preserving Approach for Building Decision Trees (PAKDD 2007)**    - Zhouxuan Teng, Wenliang Du    - [[Paper]](https://link.springer.com/chapter/10.1007/978-3-540-71701-0_30)    ## 2006  - **Decision Tree Methods for Finding Reusable MDP Homomorphisms (AAAI 2006)**    - Alicia P. Wolfe, Andrew G. Barto    - [[Paper]](https://www.aaai.org/Papers/AAAI/2006/AAAI06-085.pdf)    - **A Fast Decision Tree Learning Algorithm (AAAI 2006)**    - Jiang Su, Harry Zhang    - [[Paper]](http://www.cs.unb.ca/~hzhang/publications/AAAI06.pdf)    - **Anytime Induction of Decision Trees: An Iterative Improvement Approach (AAAI 2006)**    - Saher Esmeir, Shaul Markovitch    - [[Paper]](https://www.aaai.org/Papers/AAAI/2006/AAAI06-056.pdf)    - **When a Decision Tree Learner Has Plenty of Time (AAAI 2006)**    - Saher Esmeir, Shaul Markovitch    - [[Paper]](https://www.aaai.org/Papers/AAAI/2006/AAAI06-259.pdf)    - **Decision Trees for Functional Variables (ICDM 2006)**    - Suhrid Balakrishnan, David Madigan    - [[Paper]](http://archive.dimacs.rutgers.edu/Research/MMS/PAPERS/fdt17.pdf)      - **Cost-Sensitive Decision Tree Learning for Forensic Classification (ECML 2006)**    - Jason V. Davis, Jungwoo Ha, Christopher J. Rossbach, Hany E. Ramadan, Emmett Witchel    - [[Paper]](https://www.cs.utexas.edu/users/witchel/pubs/davis-ecml06.pdf)    - **Improving the Ranking Performance of Decision Trees (ECML 2006)**    - Bin Wang, Harry Zhang    - [[Paper]](https://link.springer.com/chapter/10.1007/11871842_44)    - **A General Framework for Accurate and Fast Regression by Data Summarization in Random Decision Trees (KDD 2006)**    - Wei Fan, Joe McCloskey, Philip S. Yu    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.442.2004&rep=rep1&type=pdf)      - **Constructing Decision Trees for Graph-Structured Data by Chunkingless Graph-Based Induction (PAKDD 2006)**    - Phu Chien Nguyen, Kouzou Ohara, Akira Mogi, Hiroshi Motoda, Takashi Washio    - [[Paper]](http://www.ar.sanken.osaka-u.ac.jp/~motoda/papers/pakdd06.pdf)    - **Variable Randomness in Decision Tree Ensembles (PAKDD 2006)**    - Fei Tony Liu, Kai Ming Ting    - [[Paper]](https://link.springer.com/chapter/10.1007/11731139_12)    - **Generalized Conditional Entropy and a Metric Splitting Criterion for Decision Trees (PAKDD 2006)**    - Dan A. Simovici, Szymon Jaroszewicz    - [[Paper]](https://www.researchgate.net/profile/Szymon_Jaroszewicz/publication/220895184_Generalized_Conditional_Entropy_and_a_Metric_Splitting_Criterion_for_Decision_Trees/links/0fcfd50b1267f7b868000000/Generalized-Conditional-Entropy-and-a-Metric-Splitting-Criterion-for-Decision-Trees.pdf)    - **Decision Trees for Hierarchical Multilabel Classification: A Case Study in Functional Genomics (PKDD 2006)**    - Hendrik Blockeel, Leander Schietgat, Jan Struyf, Saso Dzeroski, Amanda Clare    - [[Paper]](https://link.springer.com/chapter/10.1007/11871637_7)    - **k-Anonymous Decision Tree Induction (PKDD 2006)**    - Arik Friedman, Assaf Schuster, Ran Wolff    - [[Paper]](http://www.cs.technion.ac.il/~arikf/online-publications/kADET06.pdf)    ## 2005  - **Representing Conditional Independence Using Decision Trees (AAAI 2005)**    - Jiang Su, Harry Zhang    - [[Paper]](http://www.cs.unb.ca/~hzhang/publications/AAAI051SuJ.pdf)    - **Use of Expert Knowledge for Decision Tree Pruning (AAAI 2005)**    - Jingfeng Cai, John Durkin    - [[Paper]](http://www.aaai.org/Papers/AAAI/2005/SA05-009.pdf)      - **Model Selection in Omnivariate Decision Trees (ECML 2005)**    - Olcay Taner Yildiz, Ethem Alpaydin    - [[Paper]](https://www.cmpe.boun.edu.tr/~ethem/files/papers/yildiz_ecml05.pdf)    - **Combining Bias and Variance Reduction Techniques for Regression Trees (ECML 2005)**    - Yuk Lai Suen, Prem Melville, Raymond J. Mooney    - [[Paper]](http://www.cs.utexas.edu/users/ml/papers/bv-ecml-05.pdf)    - **Simple Test Strategies for Cost-Sensitive Decision Trees (ECML 2005)**    - Shengli Sheng, Charles X. Ling, Qiang Yang    - [[Paper]](https://www.researchgate.net/publication/3297582_Test_strategies_for_cost-sensitive_decision_trees)    - **Effective Estimation of Posterior Probabilities: Explaining the Accuracy of Randomized Decision Tree Approaches (ICDM 2005)**    - Wei Fan, Ed Greengrass, Joe McCloskey, Philip S. Yu, Kevin Drummey    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.9713&rep=rep1&type=pdf)    - **Exploiting Informative Priors for Bayesian Classification and Regression Trees (IJCAI 2005)**    - Nicos Angelopoulos, James Cussens    - [[Paper]](https://www.ijcai.org/Proceedings/05/Papers/1013.pdf)    - **Ranking Cases with Decision Trees: a Geometric Method that Preserves Intelligibility (IJCAI 2005)**    - Isabelle Alvarez, Stephan Bernard    - [[Paper]](https://www.ijcai.org/Proceedings/05/Papers/1502.pdf)      - **Maximizing Tree Diversity by Building Complete-Random Decision Trees (PAKDD 2005)**    - Fei Tony Liu, Kai Ming Ting, Wei Fan    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.7805&rep=rep1&type=pdf)    - **Hybrid Cost-Sensitive Decision Tree (PKDD 2005)**    - Shengli Sheng, Charles X. Ling    - [[Paper]](https://cling.csd.uwo.ca/papers/pkdd05a.pdf)    - **Tree2 - Decision Trees for Tree Structured Data (PKDD 2005)**    - Björn Bringmann, Albrecht Zimmermann    - [[Paper]](https://link.springer.com/chapter/10.1007/11564126_10)    - **Building Decision Trees on Records Linked through Key References (SDM 2005)**    - Ke Wang, Yabo Xu, Philip S. Yu, Rong She    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.215.7181&rep=rep1&type=pdf)    - **Decision Tree Induction in High Dimensional, Hierarchically Distributed Databases (SDM 2005)**    - Amir Bar-Or, Ran Wolff, Assaf Schuster, Daniel Keren    - [[Paper]](https://www.semanticscholar.org/paper/Decision-Tree-Induction-in-High-Dimensional%2C-Bar-Or-Wolff/90235fc35c27dae273681f7847c2b20ff37928a9)    - **Boosted Decision Trees for Word Recognition in Handwritten Document Retrieval (SIGIR 2005)**    - Nicholas R. Howe, Toni M. Rath, R. Manmatha    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.1551&rep=rep1&type=pdf)    ## 2004  - **On the Optimality of Probability Estimation by Random Decision Trees (AAAI 2004)**    - Wei Fan    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.447.2128&rep=rep1&type=pdf)    - **Occam's Razor and a Non-Syntactic Measure of Decision Tree Complexity (AAAI 2004)**    - Goutam Paul    - [[Paper]](https://www.aaai.org/Papers/AAAI/2004/AAAI04-130.pdf)    - **Using Emerging Patterns and Decision Trees in Rare-Class Classification (ICDM 2004)**    - Hamad Alhammady, Kotagiri Ramamohanarao    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1410299)    - **Orthogonal Decision Trees (ICDM 2004)**    - Hillol Kargupta, Haimonti Dutta    - [[Paper]](https://www.csee.umbc.edu/~hillol/PUBS/odtree.pdf)    - **Improving the Reliability of Decision Tree and Naive Bayes Learners (ICDM 2004)**    - David George Lindsay, Siân Cox    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.521.3127&rep=rep1&type=pdf)    - **Communication Efficient Construction of Decision Trees Over Heterogeneously Distributed Data (ICDM 2004)**    - Chris Giannella, Kun Liu, Todd Olsen, Hillol Kargupta    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.7119&rep=rep1&type=pdf)    - **Decision Tree Evolution Using Limited Number of Labeled Data Items from Drifting Data Streams (ICDM 2004)**    - Wei Fan, Yi-an Huang, Philip S. Yu    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.9450&rep=rep1&type=pdf)    - **Lookahead-based Algorithms for Anytime Induction of Decision Trees (ICML 2004)**    - Saher Esmeir, Shaul Markovitch    - [[Paper]](http://www.cs.technion.ac.il/~shaulm/papers/pdf/Esmeir-Markovitch-icml2004.pdf)      - **Decision Trees with Minimal Costs (ICML 2004)**    - Charles X. Ling, Qiang Yang, Jianning Wang, Shichao Zhang    - [[Paper]](https://icml.cc/Conferences/2004/proceedings/papers/136.pdf)    - **Training Conditional Random Fields via Gradient Tree Boosting (ICML 2004)**    - Thomas G. Dietterich, Adam Ashenfelter, Yaroslav Bulatov    - [[Paper]](http://web.engr.oregonstate.edu/~tgd/publications/ml2004-treecrf.pdf)    - **Detecting Structural Metadata with Decision Trees and Transformation-Based Learning (NAACL 2004)**    - Joungbum Kim, Sarah E. Schwarm, Mari Ostendorf    - [[Paper]](https://www.aclweb.org/anthology/N04-1018)    - **On the Adaptive Properties of Decision Trees (NIPS 2004)**    - Clayton D. Scott, Robert D. Nowak    - [[Paper]](https://papers.nips.cc/paper/2625-on-the-adaptive-properties-of-decision-trees.pdf)      - **A Metric Approach to Building Decision Trees Based on Goodman-Kruskal Association Index (PAKDD 2004)**    - Dan A. Simovici, Szymon Jaroszewicz    - [[Paper]](https://www.researchgate.net/publication/2906289_A_Metric_Approach_to_Building_Decision_Trees_Based_on_Goodman-Kruskal_Association_Index)    ## 2003  - **Rademacher Penalization over Decision Tree Prunings (ECML 2003)**    - Matti Kääriäinen, Tapio Elomaa    - [[Paper]](https://www.researchgate.net/publication/221112653_Rademacher_Penalization_over_Decision_Tree_Prunings)      - **Ensembles of Cascading Trees (ICDM 2003)**    - Jinyan Li, Huiqing Liu    - [[Paper]](https://www.researchgate.net/publication/4047523_Ensembles_of_cascading_trees)    - **Postprocessing Decision Trees to Extract Actionable Knowledge (ICDM 2003)**    - Qiang Yang, Jie Yin, Charles X. Ling, Tielin Chen    - [[Paper]](https://pdfs.semanticscholar.org/b2c6/ff54c7aeefc70820ff04a8fc8b804012c504.pdf)    - **K-D Decision Tree: An Accelerated and Memory Efficient Nearest Neighbor Classifier (ICDM 2003)**    - Tomoyuki Shibata, Takekazu Kato, Toshikazu Wada    - [[Paper]](https://ieeexplore.ieee.org/abstract/document/1250997)    - **Identifying Markov Blankets with Decision Tree Induction (ICDM 2003)**    - Lewis J. Frey, Douglas H. Fisher, Ioannis Tsamardinos, Constantin F. Aliferis, Alexander R. Statnikov    - [[Paper]](https://www.semanticscholar.org/paper/Identifying-Markov-Blankets-with-Decision-Tree-Frey-Fisher/1aa0b0ede22f3963c923ea320a8bed91ac5aafbf)    - **Comparing Naive Bayes, Decision Trees, and SVM with AUC and Accuracy (ICDM 2003)**    - Jin Huang, Jingjing Lu, Charles X. Ling    - [[Paper]](https://pdfs.semanticscholar.org/8a73/74b98a9d94b8c01e996e72340f86a4327869.pdf)    - **Boosting Lazy Decision Trees (ICML 2003)**    - Xiaoli Zhang Fern, Carla E. Brodley    - [[Paper]](https://www.aaai.org/Papers/ICML/2003/ICML03-026.pdf)    - **Decision Tree with Better Ranking (ICML 2003)**    - Charles X. Ling, Robert J. Yan    - [[Paper]](https://www.aaai.org/Papers/ICML/2003/ICML03-064.pdf)    - **Skewing: An Efficient Alternative to Lookahead for Decision Tree Induction (IJCAI 2003)**    - David Page, Soumya Ray    - [[Paper]](http://pages.cs.wisc.edu/~dpage/ijcai3.pdf)    - **Efficient Decision Tree Construction on Streaming Data (KDD 2003)**    - Ruoming Jin, Gagan Agrawal    - [[Paper]](http://web.cse.ohio-state.edu/~agrawal.28/p/sigkdd03.pdf)    - **PaintingClass: Interactive Construction Visualization and Exploration of Decision Trees (KDD 2003)**    - Soon Tee Teoh, Kwan-Liu Ma    - [[Paper]](https://www.researchgate.net/publication/220272011_PaintingClass_interactive_construction_visualization_and_exploration_of_decision_trees)    - **Accurate Decision Trees for Mining High-Speed Data Streams (KDD 2003)**    - João Gama, Ricardo Rocha, Pedro Medas    - [[Paper]](http://staff.icar.cnr.it/manco/Teaching/2006/datamining/Esami2006/ArticoliSelezionatiDM/SEMINARI/Mining%20Data%20Streams/kdd03.pdf)    - **Near-Minimax Optimal Classification with Dyadic Classification Trees (NIPS 2003)**    - Clayton D. Scott, Robert D. Nowak    - [[Paper]](http://nowak.ece.wisc.edu/nips03.pdf)      - **Improving Performance of Decision Tree Algorithms with Multi-edited Nearest Neighbor Rule (PAKDD 2003)**    - Chenzhou Ye, Jie Yang, Lixiu Yao, Nian-yi Chen    - [[Paper]](https://www.researchgate.net/publication/220895462_Improving_Performance_of_Decision_Tree_Algorithms_with_Multi-edited_Nearest_Neighbor_Rule)    - **Arbogodai: a New Approach for Decision Trees (PKDD 2003)**    - Djamel A. Zighed, Gilbert Ritschard, Walid Erray, Vasile-Marian Scuturici    - [[Paper]](http://mephisto.unige.ch/pub/publications/gr/zig_rit_arbo_pkdd03.pdf)    - **Communication and Memory Efficient Parallel Decision Tree Construction (SDM 2003)**    - Ruoming Jin, Gagan Agrawal    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.3059&rep=rep1&type=pdf)    - **Decision Tree Classification of Spatial Data Patterns from Videokeratography using Zernicke Polynomials (SDM 2003)**    - Michael D. Twa, Srinivasan Parthasarathy, Thomas W. Raasch, Mark Bullimore    - [[Paper]](https://www.researchgate.net/publication/220907147_Decision_Tree_Classification_of_Spatial_Data_Patterns_From_Videokeratography_Using_Zernike_Polynomials)    ## 2002    - **Multiclass Alternating Decision Trees (ECML 2002)**    - Geoffrey Holmes, Bernhard Pfahringer, Richard Kirkby, Eibe Frank, Mark A. Hall    - [[Paper]](https://www.cs.waikato.ac.nz/~bernhard/papers/ecml2002.pdf)      - **Heterogeneous Forests of Decision Trees (ICANN 2002)**    - Krzysztof Grabczewski, Wlodzislaw Duch    - [[Paper]](https://fizyka.umk.pl/publications/kmk/02forest.pdf)    - **Solving the Fragmentation Problem of Decision Trees by Discovering Boundary Emerging Patterns (ICDM 2002)**    - Jinyan Li, Limsoon Wong    - [[Paper]](https://ieeexplore.ieee.org/document/1184021)    - **Solving the Fragmentation Problem of Decision Trees by Discovering Boundary Emerging Patterns (ICDM 2002)**    - Jinyan Li, Limsoon Wong    - [[Paper]](https://www.comp.nus.edu.sg/~wongls/psZ/decisionTreeandEP-2.ps)    - **Learning Decision Trees Using the Area Under the ROC Curve (ICML 2002)**    - César Ferri, Peter A. Flach, José Hernández-Orallo    - [[Paper]](http://dmip.webs.upv.es/papers/ICML2002.pdf)    - **Finding an Optimal Gain-Ratio Subset-Split Test for a Set-Valued Attribute in Decision Tree Induction (ICML 2002)**    - Fumio Takechi, Einoshin Suzuki    - [[Paper]](https://www.researchgate.net/publication/221346121_Finding_an_Optimal_Gain-Ratio_Subset-Split_Test_for_a_Set-Valued_Attribute_in_Decision_Tree_Induction)    - **Efficiently Mining Frequent Trees in a Forest (KDD 2002)**    - Mohammed Javeed Zaki    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.8511&rep=rep1&type=pdf)    - **SECRET: a Scalable Linear Regression Tree Algorithm (KDD 2002)**    - Alin Dobra, Johannes Gehrke    - [[Paper]](http://www.cs.cornell.edu/people/dobra/papers/secret-extended.pdf)    - **Instability of Decision Tree Classification Algorithms (KDD 2002)**    - Ruey-Hsia Li, Geneva G. Belford    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.12.8094&rep=rep1&type=pdf)    - **Extracting Decision Trees From Trained Neural Networks (KDD 2002)**    - Olcay Boz    - [[Paper]](http://dspace.library.iitb.ac.in/jspui/bitstream/10054/1285/1/5664.pdf)    - **Dyadic Classification Trees via Structural Risk Minimization (NIPS 2002)**    - Clayton D. Scott, Robert D. Nowak    - [[Paper]](https://papers.nips.cc/paper/2198-dyadic-classification-trees-via-structural-risk-minimization.pdf)      - **Approximate Splitting for Ensembles of Trees using Histograms (SDM 2002)**    - Chandrika Kamath, Erick Cantú-Paz, David Littau    - [[Paper]](https://pdfs.semanticscholar.org/0855/0a94993a268e4e3e99c41e7e0ee43eabd993.pdf)    ## 2001  - **Japanese Named Entity Recognition based on a Simple Rule Generator and Decision Tree Learning (ACL 2001)**    - Hideki Isozaki    - [[Paper]](https://www.aclweb.org/anthology/P01-1041)    - **Message Length as an Effective Ockham's Razor in Decision Tree Induction (AISTATS 2001)**    - Scott Needham, David L. Dowe    - [[Paper]](www.gatsby.ucl.ac.uk/aistats/aistats2001/files/needham122.ps)    - **SQL Database Primitives for Decision Tree Classifiers (CIKM 2001)**    - Kai-Uwe Sattler, Oliver Dunemann    - [[Paper]](http://fusion.cs.uni-magdeburg.de/pubs/classprim.pdf)      - **A Unified Framework for Evaluation Metrics in Classification Using Decision Trees (ECML 2001)**    - Ricardo Vilalta, Mark Brodie, Daniel Oblinger, Irina Rish    - [[Paper]](https://scholar.harvard.edu/files/nkc/files/2015_framework_for_benefit_risk_assessment_value_in_health.pdf)    - **Backpropagation in Decision Trees for Regression (ECML 2001)**    - Victor Medina-Chico, Alberto Suárez, James F. Lutsko    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-44795-4_30)    - **Consensus Decision Trees: Using Consensus Hierarchical Clustering for Data Relabelling and Reduction (ECML 2001)**    - Branko Kavsek, Nada Lavrac, Anuska Ferligoj    - [[Paper]](https://link.springer.com/content/pdf/10.1007/3-540-44795-4_22.pdf)    - **Mining Decision Trees from Data Streams in a Mobile Environment (ICDM 2001)**    - Hillol Kargupta, Byung-Hoon Park    - [[Paper]](https://ieeexplore.ieee.org/document/989530)    - **Efficient Determination of Dynamic Split Points in a Decision Tree (ICDM 2001)**    - David Maxwell Chickering, Christopher Meek, Robert Rounthwaite    - [[Paper]](https://pdfs.semanticscholar.org/3587/a245c34ea415b205a903bde3220eb533d1a7.pdf)    - **A Comparison of Stacking with Meta Decision Trees to Bagging, Boosting, and Stacking with other Methods (ICDM 2001)**    - Bernard Zenko, Ljupco Todorovski, Saso Dzeroski    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.3118&rep=rep1&type=pdf)    - **Efficient Algorithms for Decision Tree Cross-Validation (ICML 2001)**    - Hendrik Blockeel, Jan Struyf    - [[Paper]](http://www.jmlr.org/papers/volume3/blockeel02a/blockeel02a.pdf)    - **Bias Correction in Classification Tree Construction (ICML 2001)**    - Alin Dobra, Johannes Gehrke    - [[Paper]](http://www.cs.cornell.edu/people/dobra/papers/icml2001-bias.pdf)    - **Breeding Decision Trees Using Evolutionary Techniques (ICML 2001)**    - Athanassios Papagelis, Dimitrios Kalles    - [[Paper]](http://www.gatree.com/data/BreedinDecisioTreeUsinEvo.pdf)    - **Obtaining Calibrated Probability Estimates from Decision Trees and Naive Bayesian Classifiers (ICML 2001)**    - Bianca Zadrozny, Charles Elkan    - [[Paper]](http://cseweb.ucsd.edu/~elkan/calibrated.pdf)    - **Temporal Decision Trees or the lazy ECU vindicated (IJCAI 2001)**    - Luca Console, Claudia Picardi, Daniele Theseider Dupré    - [[Paper]](https://www.researchgate.net/publication/220815333_Temporal_Decision_Trees_or_the_lazy_ECU_vindicated)      - **Data Mining Criteria for Tree-based Regression and Classification (KDD 2001)**    - Andreas Buja, Yung-Seop Lee    - [[Paper]](https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1406&context=statistics_papers)    - **A Decision Tree of Bigrams is an Accurate Predictor of Word Sense (NAACL 2001)**    - Ted Pedersen    - [[Paper]](https://www.aclweb.org/anthology/N01-1011)      - **Rule Reduction over Numerical Attributes in Decision Tree Using Multilayer Perceptron (PAKDD 2001)**    - DaeEun Kim, Jaeho Lee    - [[Paper]](https://dl.acm.org/citation.cfm?id=693490)    - **A Scalable Algorithm for Rule Post-pruning of Large Decision Trees (PAKDD 2001)**    - Trong Dung Nguyen, Tu Bao Ho, Hiroshi Shimodaira    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45357-1_49)    - **Optimizing the Induction of Alternating Decision Trees (PAKDD 2001)**    - Bernhard Pfahringer, Geoffrey Holmes, Richard Kirkby    - [[Paper]](https://www.researchgate.net/publication/33051701_Optimizing_the_Induction_of_Alternating_Decision_Trees)    - **Interactive Construction of Decision Trees (PAKDD 2001)**    - Jianchao Han, Nick Cercone    - [[Paper]](https://pure.tue.nl/ws/files/3522084/672434611234867.pdf)    - **Bloomy Decision Tree for Multi-objective Classification (PKDD 2001)**    - Einoshin Suzuki, Masafumi Gotoh, Yuta Choki    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-44794-6_36)      - **A Fourier Analysis Based Approach to Learning Decision Trees in a Distributed Environment (SDM 2001)**    - Byung-Hoon Park, Rajeev Ayyagari, Hillol Kargupta    - [[Paper]](https://archive.siam.org/meetings/sdm01/pdf/sdm01_19.pdf)      ## 2000    - **Intuitive Representation of Decision Trees Using General Rules and Exceptions (AAAI 2000)**    - Bing Liu, Minqing Hu, Wynne Hsu    - [[Paper]](https://pdfs.semanticscholar.org/e284/96551e595f1850a53f93affa98919147712f.pdf)    - **Tagging Unknown Proper Names Using Decision Trees (ACL 2000)**    - Frédéric Béchet, Alexis Nasr, Franck Genet    - [[Paper]](https://www.aclweb.org/anthology/P00-1011)    - **Clustering Through Decision Tree Construction (CIKM 2000)**    - Bing Liu, Yiyuan Xia, Philip S. Yu    - [[Paper]](https://dl.acm.org/citation.cfm?id=354775)    - **Handling Continuous-Valued Attributes in Decision Tree with Neural Network Modelling (ECML 2000)**    - DaeEun Kim, Jaeho Lee    - [[Paper]](https://link.springer.com/content/pdf/10.1007/3-540-45164-1_22.pdf)    - **Investigation and Reduction of Discretization Variance in Decision Tree Induction (ECML 2000)**    - Pierre Geurts, Louis Wehenkel    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45164-1_17)    - **Nonparametric Regularization of Decision Trees (ECML 2000)**    - Tobias Scheffer    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45164-1_36)    - **Exploiting the Cost (In)sensitivity of Decision Tree Splitting Criteria (ICML 2000)**    - Chris Drummond, Robert C. Holte    - [[Paper]](https://pdfs.semanticscholar.org/160e/21c3acc925b60dc040cb1705e58bb166b045.pdf)    - **Multi-agent Q-learning and Regression Trees for Automated Pricing Decisions (ICML 2000)**    - Manu Sridharan, Gerald Tesauro    - [[Paper]](https://manu.sridharan.net/files/icml00.pdf)    - **Growing Decision Trees on Support-less Association Rules (KDD 2000)**    - Ke Wang, Senqiang Zhou, Yu He    - [[Paper]](https://www2.cs.sfu.ca/~wangk/pub/kdd002.pdf)    - **Efficient Algorithms for Constructing Decision Trees with Constraints (KDD 2000)**    - Minos N. Garofalakis, Dongjoon Hyun, Rajeev Rastogi, Kyuseok Shim    - [[Paper]](http://www.softnet.tuc.gr/~minos/Papers/kdd00-cam.pdf)    - **Interactive Visualization in Mining Large Decision Trees (PAKDD 2000)**    - Trong Dung Nguyen, Tu Bao Ho, Hiroshi Shimodaira    - [[Paper]](https://link.springer.com/content/pdf/10.1007/3-540-45571-X_40.pdf)    - **VQTree: Vector Quantization for Decision Tree Induction (PAKDD 2000)**    - Shlomo Geva, Lawrence Buckingham    - [[Paper]](https://link.springer.com/chapter/10.1007%2F3-540-45571-X_41)    - **Some Enhencements of Decision Tree Bagging (PKDD 2000)**    - Pierre Geurts    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45372-5_14)    - **Combining Multiple Models with Meta Decision Trees (PKDD 2000)**    - Ljupco Todorovski, Saso Dzeroski    - [[Paper]](http://kt.ijs.si/bernard/mdts/pub01.pdf)    - **Induction of Multivariate Decision Trees by Using Dipolar Criteria (PKDD 2000)**    - Leon Bobrowski, Marek Kretowski    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45372-5_33)    - **Decision Tree Toolkit: A Component-Based Library of Decision Tree Algorithms (PKDD 2000)**    - Nikos Drossos, Athanassios Papagelis, Dimitrios Kalles    - [[Paper]](https://link.springer.com/chapter/10.1007/3-540-45372-5_40)    ## 1999  - **Modeling Decision Tree Performance with the Power Law (AISTATS 1999)**    - Lewis J. Frey, Douglas H. Fisher    - [[Paper]](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/ModelingTree.pdf)    - **Causal Mechanisms and Classification Trees for Predicting Chemical Carcinogens (AISTATS 1999)**    - Louis Anthony Cox Jr.    - [[Paper]](https://pdfs.semanticscholar.org/0d7b/1d55c5abfd024aacf645c66d0c90c283814e.pdf)    - **POS Tags and Decision Trees for Language Modeling (EMNLP 1999)**    - Peter A. Heeman    - [[Paper]](https://www.aclweb.org/anthology/W99-0617)    - **Lazy Bayesian Rules: A Lazy Semi-Naive Bayesian Learning Technique Competitive to Boosting Decision Trees (ICML 1999)**    - Zijian Zheng, Geoffrey I. Webb, Kai Ming Ting    - [[Paper]](https://pdfs.semanticscholar.org/067e/86836ddbcb5e2844e955c16e058366a18c77.pdf)    - **The Alternating Decision Tree Learning Algorithm (ICML 1999)**    - Yoav Freund, Llew Mason    - [[Paper]](https://cseweb.ucsd.edu/~yfreund/papers/atrees.pdf)    - [[Code]](https://github.com/rajanil/mkboost)    - **Boosting with Multi-Way Branching in Decision Trees (NIPS 1999)**    - Yishay Mansour, David A. McAllester    - [[Paper]](https://papers.nips.cc/paper/1659-boosting-with-multi-way-branching-in-decision-trees.pdf)    ## 1998  - **Learning Sorting and Decision Trees with POMDPs (ICML 1998)**    - Blai Bonet, Hector Geffner    - [[Paper]](https://bonetblai.github.io/reports/icml98-learning.pdf)    - **Using a Permutation Test for Attribute Selection in Decision Trees (ICML 1998)**    - Eibe Frank, Ian H. Witten    - [[Paper]](https://pdfs.semanticscholar.org/9aa9/21b0203e06e98b49bf726a33e124f4310ea3.pdf)    - **A Fast and Bottom-Up Decision Tree Pruning Algorithm with Near-Optimal Generalization (ICML 1998)**    - Michael J. Kearns, Yishay Mansour    - [[Paper]](https://www.cis.upenn.edu/~mkearns/papers/pruning.pdf)    ## 1997  - **Pessimistic Decision Tree Pruning Based Continuous-Time (ICML 1997)**    - Yishay Mansour    - [[Paper]](https://pdfs.semanticscholar.org/b6fc/e37612db10a9756b904b5e79e1144ca12574.pdf)    - **PAC Learning with Constant-Partition Classification Noise and Applications to Decision Tree Induction (ICML 1997)**    - Scott E. Decatur    - [[Paper]](https://www.semanticscholar.org/paper/PAC-Learning-with-Constant-Partition-Classification-Decatur/dd205073aeb512ecd1e823b35f556058fdeea5e0)    - **Option Decision Trees with Majority Votes (ICML 1997)**    - Ron Kohavi, Clayton Kunz    - [[Paper]](https://pdfs.semanticscholar.org/383b/381d1ac0bb41ec595e0d1603ed642809eb86.pdf)    - **Integrating Feature Construction with Multiple Classifiers in Decision Tree Induction (ICML 1997)**    - Ricardo Vilalta, Larry A. Rendell    - [[Paper]](https://pdfs.semanticscholar.org/1f73/d9d409a75d16871cfa1182ac72b37c839d86.pdf)    - **Functional Models for Regression Tree Leaves (ICML 1997)**    - Luís Torgo    - [[Paper]](https://pdfs.semanticscholar.org/48e4/b3187ca234308e97e1ac0cab84222c603bdd.pdf)    - **The Effects of Training Set Size on Decision Tree Complexity (ICML 1997)**    - Tim Oates, David D. Jensen    - [[Paper]](https://pdfs.semanticscholar.org/e003/9dbdec3bd4cfbb3273b623fbed2d6b2f0cc9.pdf)    - **Unsupervised On-line Learning of Decision Trees for Hierarchical Data Analysis (NIPS 1997)**    - Marcus Held, Joachim M. Buhmann    - [[Paper]](https://papers.nips.cc/paper/1479-unsupervised-on-line-learning-of-decision-trees-for-hierarchical-data-analysis.pdf)    - **Data-Dependent Structural Risk Minimization for Perceptron Decision Trees (NIPS 1997)**    - John Shawe-Taylor, Nello Cristianini    - [[Paper]](https://papers.nips.cc/paper/1359-data-dependent-structural-risk-minimization-for-perceptron-decision-trees)    - **Generalization in Decision Trees and DNF: Does Size Matter (NIPS 1997)**    - Mostefa Golea, Peter L. Bartlett, Wee Sun Lee, Llew Mason    - [[Paper]](https://papers.nips.cc/paper/1340-generalization-in-decision-trees-and-dnf-does-size-matter.pdf)    ## 1996  - **Second Tier for Decision Trees (ICML 1996)**    - Miroslav Kubat    - [[Paper]](https://pdfs.semanticscholar.org/b619/7c531b1c83dfaa52563449f9b8248cc68c5a.pdf)    - **Non-Linear Decision Trees - NDT (ICML 1996)**    - Andreas Ittner, Michael Schlosser    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.2133&rep=rep1&type=pdf)    - **Learning Relational Concepts with Decision Trees (ICML 1996)**    - Peter Geibel, Fritz Wysotzki    - [[Paper]](https://pdfs.semanticscholar.org/32f1/78d7266fee779257b87ac8f948951db57d1e.pdf)    ## 1995  - **A Hill-Climbing Approach for Optimizing Classification Trees (AISTATS 1995)**    - Xiaorong Sun, Steve Y. Chiu, Louis Anthony Cox Jr.    - [[Paper]](https://link.springer.com/chapter/10.1007%2F978-1-4612-2404-4_11)    - **An Exact Probability Metric for Decision Tree Splitting (AISTATS 1995)**    - J. Kent Martin    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.6378&rep=rep1&type=pdf)    - **On Pruning and Averaging Decision Trees (ICML 1995)**    - Jonathan J. Oliver, David J. Hand    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.6733&rep=rep1&type=pdf)    - **On Handling Tree-Structured Attributed in Decision Tree Learning (ICML 1995)**    - Hussein Almuallim, Yasuhiro Akiba, Shigeo Kaneda    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603776500116)    - **Retrofitting Decision Tree Classifiers Using Kernel Density Estimation (ICML 1995)**    - Padhraic Smyth, Alexander G. Gray, Usama M. Fayyad    - [[Paper]](https://pdfs.semanticscholar.org/3a05/8ab505f096b23962591bb14e495a543aa2a1.pdf)    - **Increasing the Performance and Consistency of Classification Trees by Using the Accuracy Criterion at the Leaves (ICML 1995)**    - David J. Lubinsky    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603776500530)    - **Efficient Algorithms for Finding Multi-way Splits for Decision Trees (ICML 1995)**    - Truxton Fulton, Simon Kasif, Steven Salzberg    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603776500384)    - **Theory and Applications of Agnostic PAC-Learning with Small Decision Trees (ICML 1995)**    - Peter Auer, Robert C. Holte, Wolfgang Maass    - [[Paper]](https://igi-web.tugraz.at/PDF/77.pdf)    - **Boosting Decision Trees (NIPS 1995)**    - Harris Drucker, Corinna Cortes    - [[Paper]](http://papers.nips.cc/paper/1059-boosting-decision-trees.pdf)    - **Using Pairs of Data-Points to Define Splits for Decision Trees (NIPS 1995)**    - Geoffrey E. Hinton, Michael Revow    - [[Paper]](https://www.cs.toronto.edu/~hinton/absps/bcart.pdf)    - **A New Pruning Method for Solving Decision Trees and Game Trees (UAI 1995)**    - Prakash P. Shenoy    - [[Paper]](https://arxiv.org/abs/1302.4981)    ## 1994  - **A Statistical Approach to Decision Tree Modeling (ICML 1994)**    - Michael I. Jordan    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500519)    - **In Defense of C4.5: Notes Learning One-Level Decision Trees (ICML 1994)**    - Tapio Elomaa    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.30.9386)    - **An Improved Algorithm for Incremental Induction of Decision Trees (ICML 1994)**    - Paul E. Utgoff    - [[Paper]](https://www.sciencedirect.com/science/article/pii/B9781558603356500465)      - **Decision Tree Parsing using a Hidden Derivation Model (NAACL 1994)**    - Frederick Jelinek, John D. Lafferty, David M. Magerman, Robert L. Mercer, Adwait Ratnaparkhi, Salim Roukos    - [[Paper]](http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d3/data/pdf/anthology-PDF/H/H94/H94-1052.pdf)    ## 1993  - **Using Decision Trees to Improve Case-Based Learning (ICML 1993)**    - Claire Cardie    - [[Paper]](https://www.cs.cornell.edu/home/cardie/papers/ml-93.ps)      ## 1991  - **Context Dependent Modeling of Phones in Continuous Speech Using Decision Trees (NAACL 1991)**    - Lalit R. Bahl, Peter V. de Souza, P. S. Gopalakrishnan, David Nahamoo, Michael Picheny    - [[Paper]](https://www.aclweb.org/anthology/H91-1051.pdf)    ## 1989  - **Performance Comparisons Between Backpropagation Networks and Classification Trees on Three Real-World Applications (NIPS 1989)**    - Les E. Atlas, Ronald A. Cole, Jerome T. Connor, Mohamed A. El-Sharkawi, Robert J. Marks II, Yeshwant K. Muthusamy, Etienne Barnard    - [[Paper]](https://papers.nips.cc/paper/203-performance-comparisons-between-backpropagation-networks-and-classification-trees-on-three-real-world-applications)    ## 1988  - **Multiple Decision Trees (UAI 1988)**    - Suk Wah Kwok, Chris Carter    - [[Paper]](https://arxiv.org/abs/1304.2363)    ## 1987  - **Decision Tree Induction Systems: A Bayesian Analysis (UAI 1987)**    - Wray L. Buntine    - [[Paper]](https://arxiv.org/abs/1304.2732)      --------------------------------------------------------------------------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-decision-tree-papers/blob/master/LICENSE) """
Big data;https://github.com/twitter/summingbird;"""## Summingbird [![Build Status](https://secure.travis-ci.org/twitter/summingbird.png)](http://travis-ci.org/twitter/summingbird)    Summingbird is a library that lets you write MapReduce programs that look like native Scala or Java collection transformations and execute them on a number of well-known distributed MapReduce platforms, including [Storm](https://github.com/nathanmarz/storm) and [Scalding](https://github.com/twitter/scalding).    ![Summingbird Logo](https://raw.github.com/twitter/summingbird/develop/logo/summingbird_logo.png)    While a word-counting aggregation in pure Scala might look like this:    ```scala    def wordCount(source: Iterable[String], store: MutableMap[String, Long]) =      source.flatMap { sentence =>        toWords(sentence).map(_ -> 1L)      }.foreach { case (k, v) => store.update(k, store.get(k) + v) }  ```    Counting words in Summingbird looks like this:    ```scala    def wordCount[P <: Platform[P]]      (source: Producer[P, String], store: P#Store[String, Long]) =        source.flatMap { sentence =>          toWords(sentence).map(_ -> 1L)        }.sumByKey(store)  ```    The logic is exactly the same, and the code is almost the same. The main difference is that you can execute the Summingbird program in ""batch mode"" (using [Scalding](https://github.com/twitter/scalding)), in ""realtime mode"" (using [Storm](https://github.com/nathanmarz/storm)), or on both Scalding and Storm in a hybrid batch/realtime mode that offers your application very attractive fault-tolerance properties.    Summingbird provides you with the primitives you need to build rock solid production systems.    ## Getting Started: Word Count with Twitter    The `summingbird-example` project allows you to run the wordcount program above on a sample of Twitter data using a local Storm topology and memcache instance. You can find the actual job definition in [ExampleJob.scala](https://github.com/twitter/summingbird/blob/develop/summingbird-example/src/main/scala/com/twitter/summingbird/example/ExampleJob.scala).    First, make sure you have `memcached` installed locally. If not, if you're on OS X, you can get it by installing [Homebrew](http://brew.sh/) and running this command in a shell:    ```bash  brew install memcached  ```    When this is finished, run the `memcached` command in a separate terminal.    Now you'll need to set up access to the Twitter Streaming API. [This blog post](http://tugdualgrall.blogspot.com/2012/11/couchbase-create-large-dataset-using.html) has a great walkthrough, so open that page, head over to https://dev.twitter.com/ and get your various keys and tokens. Once you have these, clone the Summingbird repository:    ```bash  git clone https://github.com/twitter/summingbird.git  cd summingbird  ```    And open [StormRunner.scala](https://github.com/twitter/summingbird/blob/develop/summingbird-example/src/main/scala/com/twitter/summingbird/example/StormRunner.scala) in your editor. Replace the dummy variables under `config` variable with your auth tokens:    ```scala  lazy val config = new ConfigurationBuilder()      .setOAuthConsumerKey(""mykey"")      .setOAuthConsumerSecret(""mysecret"")      .setOAuthAccessToken(""token"")      .setOAuthAccessTokenSecret(""tokensecret"")      .setJSONStoreEnabled(true) // required for JSON serialization      .build  ```    You're all ready to go! Now it's time to unleash Storm on your Twitter stream. Make sure the `memcached` terminal is still open, then start Storm from the `summingbird` directory:    ```bash  ./sbt ""summingbird-example/run --local""  ```    Storm should puke out a bunch of output, then stabilize and hang. This means that Storm is updating your local memcache instance with counts of every word that it sees in each tweet.    To query the aggregate results in Memcached, you'll need to open an SBT repl in a new terminal:    ```bash  ./sbt summingbird-example/console  ```    At the launched repl, run the following:    ```scala  scala> import com.twitter.summingbird.example._  import com.twitter.summingbird.example._    scala> StormRunner.lookup(""i"")  <memcache store loading elided>  res0: Option[Long] = Some(5)    scala> StormRunner.lookup(""i"")  res1: Option[Long] = Some(52)  ```    Boom. Counts for the word `""i""` are growing in realtime.    See the [wiki page](https://github.com/twitter/summingbird/wiki/Getting-started-with-summingbird-example) for a more detailed explanation of the configuration required to get this job up and running and some ideas for where to go next.    ## Community and Documentation    This, and all [github.com/twitter](https://github.com/twitter) projects, are under the [Twitter Open Source Code of Conduct](https://engineering.twitter.com/opensource/code-of-conduct). Additionally, see the [Typelevel Code of Conduct](http://typelevel.org/conduct) for specific examples of harassing behavior that are not tolerated.    To learn more and find links to tutorials and information around the web, check out the [Summingbird Wiki](https://github.com/twitter/summingbird/wiki).    The latest ScalaDocs are hosted on Summingbird's [Github Project Page](http://twitter.github.io/summingbird).    Discussion occurs primarily on the [Summingbird mailing list](https://groups.google.com/forum/#!forum/summingbird). Issues should be reported on the GitHub issue tracker. Simpler issues appropriate for first-time contributors looking to help out are tagged ""newbie"".    IRC: freenode channel #summingbird    Follow [@summingbird](https://twitter.com/summingbird) on Twitter for updates.    Please feel free to use the beautiful [Summingbird logo](https://drive.google.com/folderview?id=0B3i3pDi3yVgNMHV0TXVkTGZteWM&usp=sharing) artwork anywhere.    ## Maven    Summingbird modules are published on maven central. The current groupid and version for all modules is, respectively, `""com.twitter""` and  `0.9.1`.    Current published artifacts are    * `summingbird-core_2.11`  * `summingbird-core_2.10`  * `summingbird-batch_2.11`  * `summingbird-batch_2.10`  * `summingbird-client_2.11`  * `summingbird-client_2.10`  * `summingbird-storm_2.11`  * `summingbird-storm_2.10`  * `summingbird-scalding_2.11`  * `summingbird-scalding_2.10`  * `summingbird-builder_2.11`  * `summingbird-builder_2.10`    The suffix denotes the scala version.    ## Authors (alphabetically)    * Oscar Boykin <https://twitter.com/posco>  * Ian O'Connell <https://twitter.com/0x138>  * Sam Ritchie <https://twitter.com/sritchie>  * Ashutosh Singhal <https://twitter.com/daashu>    ## License    Copyright 2013 Twitter, Inc.    Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 """
Big data;https://github.com/materializeinc/materialize;"""[![Build status](https://badge.buildkite.com/97d6604e015bf633d1c2a12d166bb46f3b43a927d3952c999a.svg?branch=main)](https://buildkite.com/materialize/tests)  [![Doc reference](https://img.shields.io/badge/doc-reference-orange)](https://materialize.com/docs)  [![Chat on Slack](https://img.shields.io/badge/chat-on%20slack-purple)](https://materialize.com/s/chat)    [<img src=""https://materialize.com/wp-content/uploads/2020/01/materialize_logo_primary.png"" width=60%>](https://materialize.com)    Materialize is a streaming database for real-time applications.    ## Get started    Check out [our getting started guide](https://materialize.com/docs/get-started/).    ## About    Materialize lets you ask questions of your live data, which it answers and then maintains for you as your data continue to change. The moment you need a refreshed answer, you can get it in milliseconds. Materialize is designed to help you interactively explore your streaming data, perform data warehousing analytics against live relational data, or just increase the freshness *and* reduce the load of your dashboard and monitoring tasks.    Materialize focuses on providing correct and consistent answers with minimal latency. It does not ask you to accept either approximate answers or eventual consistency. Whenever Materialize answers a query, that answer is the correct result on some specific (and recent) version of your data. Materialize does all of this by recasting your SQL92 queries as *dataflows*, which can react efficiently to changes in your data as they happen. Materialize is powered by [timely dataflow](https://github.com/TimelyDataflow/timely-dataflow), which connects the times at which your inputs change with the times of answers reported back to you.    We support a large fraction of  PostgreSQL, and are actively working on supporting more builtin PostgreSQL functions. Please file an issue if there's something that you expected to work that didn't!    ## Get data in    Materialize reads Avro, Protobuf, JSON, and newline-delimited text. Need something else? [Just ask](https://github.com/MaterializeInc/materialize/issues/new/choose).    Materialize can read data from Kafka topics, Kinesis streams (in preview), or tail local files.    ## Transform, manipulate, and read your data    Once you've got the data in, define views and perform reads via the PostgreSQL protocol. Use your favorite PostgreSQL CLI, including the `psql` you probably already have on your system.    Materialize supports a comprehensive variety of SQL features, all using the PostgreSQL dialect and protocol:    -   Joins, Joins, Joins! Materialize supports multi-column join conditions, multi-way joins, self-joins, cross-joins, inner joins, outer joins, etc.  -   Delta-joins avoid intermediate state blowup compared to systems that can only plan nested binary joins - tested on joins of up to 64 relations.  -   Support for subqueries. Materialize's SQL optimizer performs subquery decorrelation out-of-the-box, avoiding the need to manually rewrite subqueries into joins.  -   Materialize supports streams that contain CDC data (currently supporting the [Debezium](https://debezium.io/blog/2017/09/25/streaming-to-another-database/) format). Materialize can incrementally maintain views in the presence of arbitrary inserts, updates, and deletes. No asterisks.  -   All the aggregations. `GROUP BY` , `MIN`, `MAX`, `COUNT`, `SUM`, `STDDEV`, `HAVING`, etc.  -   `ORDER BY`  -   `LIMIT`  -   `DISTINCT`  -   JSON support in the PostgreSQL dialect including operators and functions like `->`, `->>`, `@>`, `?`, `jsonb_array_element`, `jsonb_each`. Materialize automatically plans lateral joins for efficient `jsonb_each` support.  -   Nest views on views on views!  -   Multiple views that have overlapping subplans can share underlying indices for space and compute efficiency, so just declaratively define _what you want_, and we'll worry about how to efficiently maintain them.    ### Just show us what it can do!    Here's an example join query that works fine in Materialize, `TPC-H` query 15:    ```sql  -- Views define commonly reused subqueries.  CREATE VIEW revenue (supplier_no, total_revenue) AS      SELECT          l_suppkey,          SUM(l_extendedprice * (1 - l_discount))      FROM          lineitem      WHERE          l_shipdate >= DATE '1996-01-01'          AND l_shipdate < DATE '1996-01-01' + INTERVAL '3' month      GROUP BY          l_suppkey;    -- Materialized views are maintained automatically, and can depend on non-materialized views.  CREATE MATERIALIZED VIEW tpch_q15 AS    SELECT      s_suppkey,      s_name,      s_address,      s_phone,      total_revenue  FROM      supplier,      revenue  WHERE      s_suppkey = supplier_no      AND total_revenue = (          SELECT              max(total_revenue)          FROM              revenue      )  ORDER BY      s_suppkey  ```    Stream inserts, updates, and deletes on the underlying tables (`lineitem` and `supplier`), and Materialize keeps the materialized view incrementally updated. You can type `SELECT * FROM tpch_q15` and expect to see the current results immediately!    ## Get data out    **Pull based**: Use any PostgreSQL-compatible driver in any language/environment to make `SELECT` queries against your views. Tell them they're talking to a PostgreSQL database, they don't ever need to know otherwise.    **Push based**: Or configure Materialize to stream results to a Kafka topic as soon as the views change.    If you want to use an ORM, [chat with us](https://github.com/MaterializeInc/materialize/issues/new/choose). They're surprisingly tricky.    ## Documentation    Check out [our documentation](https://materialize.com/docs/).    ## License    Materialize is source-available and [licensed](LICENSE) under the BSL 1.1, converting to the open-source Apache 2.0 license after 4 years. As stated in the BSL, Materialize is free forever on a single node.    Materialize is also available as [a paid cloud service](https://materialize.com/pricing/) with additional features such as high availability via multi-active replication.    ## How does it work?    Materialize is built on top of [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow) and [timely dataflow](https://github.com/TimelyDataflow/timely-dataflow), and builds on a decade of cutting-edge stream processing research.    ## For developers    Materialize is written entirely in Rust.    Developers can find docs at [doc/developer](doc/developer), and Rust API documentation is hosted at <https://dev.materialize.com/api/rust/>. The Materialize development roadmap is divided up into roughly month-long milestones, and [managed in GitHub](https://github.com/MaterializeInc/materialize/milestones?direction=asc&sort=due_date&state=open).    Contributions are welcome. Prospective code contributors might find the [good first issue tag](https://github.com/MaterializeInc/materialize/issues?q=is%3Aopen+is%3Aissue+label%3A%22D-good+first+issue%22) useful. We value all contributions equally, but bug reports are more equal.    ## Credits    Materialize is lovingly crafted by [a team of developers](https://github.com/MaterializeInc/materialize/graphs/contributors) and one bot. [Join us](https://materialize.com/careers/). """
Big data;https://github.com/skale-me/skale-engine;"""**Development activity is stopped, and this project is now archived.**    ![logo](docs/images/logo-skale.png)    [![Build Status](https://travis-ci.org/skale-me/skale.svg?branch=master)](https://travis-ci.org/skale-me/skale)  [![Build Status](https://ci.appveyor.com/api/projects/status/github/skale-me/skale?svg=true)](https://ci.appveyor.com/project/skaleme/skale)  [![npm badge](https://img.shields.io/npm/v/skale.svg)](https://www.npmjs.com/package/skale)      High performance distributed data processing and machine learning.    Skale provides a high-level API in Javascript and an optimized  parallel execution engine on top of NodeJS.    ## Features  * Pure javascript implementation of a Spark like engine  * Multiple data sources: filesystems, databases, cloud (S3, azure)  * Multiple data formats: CSV, JSON, Columnar (Parquet)...  * 50 high level operators to build parallel apps  * Machine learning: scalable classification, regression, clusterization  * Run interactively in a nodeJS REPL shell  * Docker [ready](docker/), simple local mode or full distributed mode  * Very fast, see [benchmark](benchmark/)    ## Quickstart  ```sh  npm install skale  ```    Word count example:     ```javascript  var sc = require('skale').context();    sc.textFile('/my/path/*.txt')    .flatMap(line => line.split(' '))    .map(word => [word, 1])    .reduceByKey((a, b) => a + b, 0)    .count(function (err, result) {      console.log(result);      sc.end();    });  ```    ### Local mode  In local mode, worker processes are automatically forked and  communicate with app through child process IPC channel. This is  the simplest way to operate, and it allows to use all machine  available cores.    To run in local mode, just execute your app script:  ```sh  node my_app.js  ```    or with debug traces:  ```sh  SKALE_DEBUG=2 node my_app.js  ```    ### Distributed mode  In distributed mode, a cluster server process and worker processes  must be started prior to start app. Processes communicate with each  other via raw TCP or via websockets.    To run in distributed cluster mode, first start a cluster server  on `server_host`:  ```sh  ./bin/server.js  ```    On each worker host, start a worker controller process which connects  to server:  ```sh  ./bin/worker.js -H server_host  ```    Then run your app, setting the cluster server host in environment:  ```sh  SKALE_HOST=server_host node my_app.js  ```    The same with debug traces:  ```sh  SKALE_HOST=server_host SKALE_DEBUG=2 node my_app.js  ```    ## Resources    * [Contributing guide](CONTRIBUTING.md)  * [Documentation](https://skale-me.github.io/skale)  * [Gitter](https://gitter.im/skale-me/skale-engine) for support and    discussion  * [Mailing list](https://groups.google.com/forum/#!forum/skale)    for discussion about use and development    ## Authors    The original authors of skale are [Cedric Artigue](https://github.com/CedricArtigue) and [Marc Vertes](https://github.com/mvertes).    [List of all  contributors](https://github.com/skale-me/skale/graphs/contributors)    ## License    [Apache-2.0](LICENSE)    ## Credits    <div>Logo Icon made by <a href=""https://www.flaticon.com/authors/smashicons"" title=""Smashicons"">Smashicons</a> from <a href=""https://www.flaticon.com/"" title=""Flaticon"">www.flaticon.com</a> is licensed by <a href=""http://creativecommons.org/licenses/by/3.0/"" title=""Creative Commons BY 3.0"" target=""_blank"">CC 3.0 BY</a></div> """
Big data;https://github.com/pingcap/tidb;"""![](docs/logo_with_text.png)    [![LICENSE](https://img.shields.io/github/license/pingcap/tidb.svg)](https://github.com/pingcap/tidb/blob/master/LICENSE)  [![Language](https://img.shields.io/badge/Language-Go-blue.svg)](https://golang.org/)  [![Build Status](https://travis-ci.org/pingcap/tidb.svg?branch=master)](https://travis-ci.org/pingcap/tidb)  [![Go Report Card](https://goreportcard.com/badge/github.com/pingcap/tidb)](https://goreportcard.com/report/github.com/pingcap/tidb)  [![GitHub release](https://img.shields.io/github/tag/pingcap/tidb.svg?label=release)](https://github.com/pingcap/tidb/releases)  [![GitHub release date](https://img.shields.io/github/release-date/pingcap/tidb.svg)](https://github.com/pingcap/tidb/releases)  [![CircleCI Status](https://circleci.com/gh/pingcap/tidb.svg?style=shield)](https://circleci.com/gh/pingcap/tidb)  [![Coverage Status](https://codecov.io/gh/pingcap/tidb/branch/master/graph/badge.svg)](https://codecov.io/gh/pingcap/tidb)  [![GoDoc](https://img.shields.io/badge/Godoc-reference-blue.svg)](https://godoc.org/github.com/pingcap/tidb)    ## What is TiDB?    TiDB (""Ti"" stands for Titanium) is an open-source NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL compatible and features horizontal scalability, strong consistency, and high availability.    - __Horizontal Scalability__        TiDB expands both SQL processing and storage by simply adding new nodes. This makes infrastructure capacity planning both easier and more cost-effective than traditional relational databases which only scale vertically.    - __MySQL Compatible Syntax__        TiDB acts like it is a MySQL 5.7 server to your applications. You can continue to use all of the existing MySQL client libraries, and in many cases, you will not need to change a single line of code in your application. Because TiDB is built from scratch, not a MySQL fork, please check out the list of [known compatibility differences](https://docs.pingcap.com/tidb/stable/mysql-compatibility).    - __Distributed Transactions__        TiDB internally shards table into small range-based chunks that we refer to as ""Regions"". Each Region defaults to approximately 100 MiB in size, and TiDB uses an [optimized](https://pingcap.com/blog/async-commit-the-accelerator-for-transaction-commit-in-tidb-5.0) Two-phase commit to ensure that Regions are maintained in a transactionally consistent way.    - __Cloud Native__        TiDB is designed to work in the cloud -- public, private, or hybrid -- making deployment, provisioning, operations, and maintenance simple.        The storage layer of TiDB, called TiKV, is a [Cloud Native Computing Foundation (CNCF) Graduated](https://www.cncf.io/announcements/2020/09/02/cloud-native-computing-foundation-announces-tikv-graduation/) project. The architecture of the TiDB platform also allows SQL processing and storage to be scaled independently of each other in a very cloud-friendly manner.    - __Minimize ETL__        TiDB is designed to support both transaction processing (OLTP) and analytical processing (OLAP) workloads. This means that while you may have traditionally transacted on MySQL and then Extracted, Transformed and Loaded (ETL) data into a column store for analytical processing, this step is no longer required.    - __High Availability__        TiDB uses the Raft consensus algorithm to ensure that data is highly available and safely replicated throughout storage in Raft groups. In the event of failure, a Raft group will automatically elect a new leader for the failed member, and self-heal the TiDB cluster without any required manual intervention. Failure and self-healing operations are also transparent to applications.    For more details and latest updates, see [TiDB docs](https://docs.pingcap.com/tidb/stable) and [release notes](https://docs.pingcap.com/tidb/dev/release-notes).    ## Community    You can join these groups and chats to discuss and ask TiDB related questions:    - [TiDB Internals Forum](https://internals.tidb.io/)  - [Slack Channel](https://slack.tidb.io/invite?team=tidb-community&channel=everyone&ref=pingcap-tidb)  - [TiDB User Group Forum (Chinese)](https://asktug.com)    In addition, you may enjoy following:    - [@PingCAP](https://twitter.com/PingCAP) on Twitter  - Question tagged [#tidb on StackOverflow](https://stackoverflow.com/questions/tagged/tidb)  - The PingCAP Team [English Blog](https://en.pingcap.com/blog) and [Chinese Blog](https://pingcap.com/blog-cn/)    For support, please contact [PingCAP](http://bit.ly/contact_us_via_github).    ## Quick start    ### To start using TiDB Cloud    We provide TiDB Cloud - a fully-managed Database as a Service for you.    See [TiDB Cloud Quick Start](https://docs.pingcap.com/tidbcloud/public-preview/tidb-cloud-quickstart).    ### To start using TiDB    See [Quick Start Guide](https://docs.pingcap.com/tidb/stable/quick-start-with-tidb).    ### To start developing TiDB    See [Get Started](https://pingcap.github.io/tidb-dev-guide/get-started/introduction.html) chapter of [TiDB Dev Guide](https://pingcap.github.io/tidb-dev-guide/index.html).    ## Contributing    The [community repository](https://github.com/pingcap/community) hosts all information about the TiDB community, including how to contribute to TiDB, how TiDB community is governed, how special interest groups are organized, etc.    [<img src=""docs/contribution-map.png"" alt=""contribution-map"" width=""180"">](https://github.com/pingcap/tidb-map/blob/master/maps/contribution-map.md#tidb-is-an-open-source-distributed-htap-database-compatible-with-the-mysql-protocol)    Contributions are welcomed and greatly appreciated. See [Contribution to TiDB](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/introduction.html) for details on typical contribution workflows. For more contributing information, click on the contributor icon above.    ## Adopters    View the current list of in-production TiDB adopters [here](https://docs.pingcap.com/tidb/stable/adopters).    ## Case studies    - [English](https://pingcap.com/case-studies)  - [简体中文](https://pingcap.com/cases-cn/)    ## Architecture    ![architecture](./docs/architecture.png)    ## License    TiDB is under the Apache 2.0 license. See the [LICENSE](./LICENSE) file for details.    ## Acknowledgments    - Thanks [cznic](https://github.com/cznic) for providing some great open source tools.  - Thanks [GolevelDB](https://github.com/syndtr/goleveldb), [BoltDB](https://github.com/boltdb/bolt), and [RocksDB](https://github.com/facebook/rocksdb) for their powerful storage engines. """
Big data;https://github.com/numenta/nupic;"""# <img src=""http://numenta.org/87b23beb8a4b7dea7d88099bfb28d182.svg"" alt=""NuPIC Logo"" width=100/> NuPIC    ## Numenta Platform for Intelligent Computing    The Numenta Platform for Intelligent Computing (**NuPIC**) is a machine intelligence platform that implements the [HTM learning algorithms](https://numenta.com/resources/papers-videos-and-more/). HTM is a detailed computational theory of the neocortex. At the core of HTM are time-based continuous learning algorithms that store and recall spatial and temporal patterns. NuPIC is suited to a variety of problems, particularly anomaly detection and prediction of streaming data sources. For more information, see [numenta.org](http://numenta.org) or the [NuPIC Forum](https://discourse.numenta.org/c/nupic).    For usage guides, quick starts, and API documentation, see <http://nupic.docs.numenta.org/>.    ## This project is in Maintenance Mode    We plan to do minor releases only, and limit changes in NuPIC and NuPIC Core to:    - Fixing critical bugs.  - Features needed to support ongoing research.    ## Installing NuPIC    NuPIC binaries are available for:    - Linux x86 64bit  - OS X 10.9  - OS X 10.10  - Windows 64bit    ### Dependencies    The following dependencies are required to install NuPIC on all operating systems.    - [Python 2.7](https://www.python.org/)  - [pip](https://pip.pypa.io/en/stable/installing/)>=8.1.2  - [setuptools](https://setuptools.readthedocs.io)>=25.2.0  - [wheel](http://pythonwheels.com)>=0.29.0  - [numpy](http://www.numpy.org/)  - C++ 11 compiler like [gcc](https://gcc.gnu.org/) (4.8+) or [clang](http://clang.llvm.org/)    Additional OS X requirements:    - [Xcode command line tools](https://developer.apple.com/library/ios/technotes/tn2339/_index.html)    ### Install    Run the following to install NuPIC:        pip install nupic    ### Test        # From the root of the repo:      py.test tests/unit    ### _Having problems?_    - You may need to use the `--user` flag for the commands above to install in a non-system location (depends on your environment). Alternatively, you can execute the `pip` commands with `sudo` (not recommended).  - You may need to add the `--use-wheel` option if you have an older pip version (wheels are now the default binary package format for pip).    For any other installation issues, please see our [search our forums](https://discourse.numenta.org/search?q=tag%3Ainstallation%20category%3A10) (post questions there). You can report bugs at https://github.com/numenta/nupic/issues.    Live Community Chat: [![Gitter](https://img.shields.io/badge/gitter-join_chat-blue.svg?style=flat)](https://gitter.im/numenta/public?utm_source=badge)    ### Installing NuPIC From Source    To install from local source code, run from the repository root:        pip install .    Use the optional `-e` argument for a developer install.    If you want to build the dependent `nupic.bindings` from source, you should build and install from [`nupic.core`](https://github.com/numenta/nupic.core) prior to installing nupic (since a PyPI release will be installed if `nupic.bindings` isn't yet installed).    - Build:  [![Build Status](https://travis-ci.org/numenta/nupic.png?branch=master)](https://travis-ci.org/numenta/nupic)  [![AppVeyor Status](https://ci.appveyor.com/api/projects/status/4toemh0qtr21mk6b/branch/master?svg=true)](https://ci.appveyor.com/project/numenta-ci/nupic/branch/master)  [![CircleCI](https://circleci.com/gh/numenta/nupic.svg?style=svg)](https://circleci.com/gh/numenta/nupic)  - To cite this codebase: [![DOI](https://zenodo.org/badge/19461/numenta/nupic.svg)](https://zenodo.org/badge/latestdoi/19461/numenta/nupic) """
Big data;https://github.com/mozilla-services/heka;"""# Heka    Data Acquisition and Processing Made Easy    Heka is a tool for collecting and collating data from a number of different  sources, performing ""in-flight"" processing of collected data, and delivering  the results to any number of destinations for further analysis.    Heka is written in [Go](http://golang.org/), but Heka plugins can be written  in either Go or [Lua](http://lua.org). The easiest way to compile Heka is by  sourcing (see below) the build script in the root directory of the project,  which will set up a Go environment, verify the prerequisites, and install all  required dependencies. The build process also provides a mechanism for easily  integrating external plug-in packages into the generated `hekad`. For more  details and additional installation options see  [Installing](https://hekad.readthedocs.org/en/latest/installing.html).    WARNING: YOU MUST *SOURCE* THE BUILD SCRIPT (i.e. `source build.sh`) TO           BUILD HEKA. Setting up the Go build environment requires changes to           the shell environment, if you simply execute the script (i.e.           `./build.sh`) these changes will not be made.             Resources:  * Heka project docs: http://hekad.readthedocs.org/  * GoDoc package docs: http://godoc.org/github.com/mozilla-services/heka  * Mailing list: https://mail.mozilla.org/listinfo/heka  * IRC: #heka on irc.mozilla.org """
Big data;https://github.com/tidwall/buntdb;"""<p align=""center"">  <img      src=""logo.png""      width=""307"" height=""150"" border=""0"" alt=""BuntDB"">  <br>  <a href=""https://godoc.org/github.com/tidwall/buntdb""><img src=""https://img.shields.io/badge/go-documentation-blue.svg?style=flat-square"" alt=""Godoc""></a>  <a href=""https://github.com/tidwall/buntdb/blob/master/LICENSE""><img src=""https://img.shields.io/github/license/tidwall/buntdb.svg?style=flat-square"" alt=""LICENSE""></a>  </p>    BuntDB is a low-level, in-memory, key/value store in pure Go.  It persists to disk, is ACID compliant, and uses locking for multiple  readers and a single writer. It supports custom indexes and geospatial  data. It's ideal for projects that need a dependable database and favor  speed over data size.    Features  ========    - In-memory database for [fast reads and writes](#performance)  - Embeddable with a [simple API](https://godoc.org/github.com/tidwall/buntdb)  - [Spatial indexing](#spatial-indexes) for up to 20 dimensions; Useful for Geospatial data  - Index fields inside [JSON](#json-indexes) documents  - [Collate i18n Indexes](#collate-i18n-indexes) using the optional [collate package](https://github.com/tidwall/collate)  - Create [custom indexes](#custom-indexes) for any data type  - Support for [multi value indexes](#multi-value-index); Similar to a SQL multi column index  - [Built-in types](#built-in-types) that are easy to get up & running; String, Uint, Int, Float  - Flexible [iteration](#iterating) of data; ascending, descending, and ranges  - [Durable append-only file](#append-only-file) format for persistence  - Option to evict old items with an [expiration](#data-expiration) TTL  - ACID semantics with locking [transactions](#transactions) that support rollbacks      Getting Started  ===============    ## Installing    To start using BuntDB, install Go and run `go get`:    ```sh  $ go get -u github.com/tidwall/buntdb  ```    This will retrieve the library.      ## Opening a database    The primary object in BuntDB is a `DB`. To open or create your  database, use the `buntdb.Open()` function:    ```go  package main    import (  	""log""    	""github.com/tidwall/buntdb""  )    func main() {  	// Open the data.db file. It will be created if it doesn't exist.  	db, err := buntdb.Open(""data.db"")  	if err != nil {  		log.Fatal(err)  	}  	defer db.Close()    	...  }  ```    It's also possible to open a database that does not persist to disk by using `:memory:` as the path of the file.    ```go  buntdb.Open("":memory:"") // Open a file that does not persist to disk.  ```    ## Transactions  All reads and writes must be performed from inside a transaction. BuntDB can have one write transaction opened at a time, but can have many concurrent read transactions. Each transaction maintains a stable view of the database. In other words, once a transaction has begun, the data for that transaction cannot be changed by other transactions.    Transactions run in a function that exposes a `Tx` object, which represents the transaction state. While inside a transaction, all database operations should be performed using this object. You should never access the origin `DB` object while inside a transaction. Doing so may have side-effects, such as blocking your application.    When a transaction fails, it will roll back, and revert all changes that occurred to the database during that transaction. There's a single return value that you can use to close the transaction. For read/write transactions, returning an error this way will force the transaction to roll back. When a read/write transaction succeeds all changes are persisted to disk.    ### Read-only Transactions  A read-only transaction should be used when you don't need to make changes to the data. The advantage of a read-only transaction is that there can be many running concurrently.    ```go  err := db.View(func(tx *buntdb.Tx) error {  	...  	return nil  })  ```    ### Read/write Transactions  A read/write transaction is used when you need to make changes to your data. There can only be one read/write transaction running at a time. So make sure you close it as soon as you are done with it.    ```go  err := db.Update(func(tx *buntdb.Tx) error {  	...  	return nil  })  ```    ## Setting and getting key/values    To set a value you must open a read/write transaction:    ```go  err := db.Update(func(tx *buntdb.Tx) error {  	_, _, err := tx.Set(""mykey"", ""myvalue"", nil)  	return err  })  ```      To get the value:    ```go  err := db.View(func(tx *buntdb.Tx) error {  	val, err := tx.Get(""mykey"")  	if err != nil{  		return err  	}  	fmt.Printf(""value is %s\n"", val)  	return nil  })  ```    Getting non-existent values will cause an `ErrNotFound` error.    ### Iterating  All keys/value pairs are ordered in the database by the key. To iterate over the keys:    ```go  err := db.View(func(tx *buntdb.Tx) error {  	err := tx.Ascend("""", func(key, value string) bool {  		fmt.Printf(""key: %s, value: %s\n"", key, value)  		return true // continue iteration  	})  	return err  })  ```    There is also `AscendGreaterOrEqual`, `AscendLessThan`, `AscendRange`, `AscendEqual`, `Descend`, `DescendLessOrEqual`, `DescendGreaterThan`, `DescendRange`, and `DescendEqual`. Please see the [documentation](https://godoc.org/github.com/tidwall/buntdb) for more information on these functions.      ## Custom Indexes  Initially all data is stored in a single [B-tree](https://en.wikipedia.org/wiki/B-tree) with each item having one key and one value. All of these items are ordered by the key. This is great for quickly getting a value from a key or [iterating](#iterating) over the keys. Feel free to peruse the [B-tree implementation](https://github.com/tidwall/btree).    You can also create custom indexes that allow for ordering and [iterating](#iterating) over values. A custom index also uses a B-tree, but it's more flexible because it allows for custom ordering.    For example, let's say you want to create an index for ordering names:    ```go  db.CreateIndex(""names"", ""*"", buntdb.IndexString)  ```    This will create an index named `names` which stores and sorts all values. The second parameter is a pattern that is used to filter on keys. A `*` wildcard argument means that we want to accept all keys. `IndexString` is a built-in function that performs case-insensitive ordering on the values    Now you can add various names:    ```go  db.Update(func(tx *buntdb.Tx) error {  	tx.Set(""user:0:name"", ""tom"", nil)  	tx.Set(""user:1:name"", ""Randi"", nil)  	tx.Set(""user:2:name"", ""jane"", nil)  	tx.Set(""user:4:name"", ""Janet"", nil)  	tx.Set(""user:5:name"", ""Paula"", nil)  	tx.Set(""user:6:name"", ""peter"", nil)  	tx.Set(""user:7:name"", ""Terri"", nil)  	return nil  })  ```    Finally you can iterate over the index:    ```go  db.View(func(tx *buntdb.Tx) error {  	tx.Ascend(""names"", func(key, val string) bool {  	fmt.Printf(buf, ""%s %s\n"", key, val)  		return true  	})  	return nil  })  ```  The output should be:  ```  user:2:name jane  user:4:name Janet  user:5:name Paula  user:6:name peter  user:1:name Randi  user:7:name Terri  user:0:name tom  ```    The pattern parameter can be used to filter on keys like this:    ```go  db.CreateIndex(""names"", ""user:*"", buntdb.IndexString)  ```    Now only items with keys that have the prefix `user:` will be added to the `names` index.      ### Built-in types  Along with `IndexString`, there is also `IndexInt`, `IndexUint`, and `IndexFloat`.  These are built-in types for indexing. You can choose to use these or create your own.    So to create an index that is numerically ordered on an age key, we could use:    ```go  db.CreateIndex(""ages"", ""user:*:age"", buntdb.IndexInt)  ```    And then add values:    ```go  db.Update(func(tx *buntdb.Tx) error {  	tx.Set(""user:0:age"", ""35"", nil)  	tx.Set(""user:1:age"", ""49"", nil)  	tx.Set(""user:2:age"", ""13"", nil)  	tx.Set(""user:4:age"", ""63"", nil)  	tx.Set(""user:5:age"", ""8"", nil)  	tx.Set(""user:6:age"", ""3"", nil)  	tx.Set(""user:7:age"", ""16"", nil)  	return nil  })  ```    ```go  db.View(func(tx *buntdb.Tx) error {  	tx.Ascend(""ages"", func(key, val string) bool {  	fmt.Printf(buf, ""%s %s\n"", key, val)  		return true  	})  	return nil  })  ```    The output should be:  ```  user:6:age 3  user:5:age 8  user:2:age 13  user:7:age 16  user:0:age 35  user:1:age 49  user:4:age 63  ```    ## Spatial Indexes  BuntDB has support for spatial indexes by storing rectangles in an [R-tree](https://en.wikipedia.org/wiki/R-tree). An R-tree is organized in a similar manner as a [B-tree](https://en.wikipedia.org/wiki/B-tree), and both are balanced trees. But, an R-tree is special because it can operate on data that is in multiple dimensions. This is super handy for Geospatial applications.    To create a spatial index use the `CreateSpatialIndex` function:    ```go  db.CreateSpatialIndex(""fleet"", ""fleet:*:pos"", buntdb.IndexRect)  ```    Then `IndexRect` is a built-in function that converts rect strings to a format that the R-tree can use. It's easy to use this function out of the box, but you might find it better to create a custom one that renders from a different format, such as [Well-known text](https://en.wikipedia.org/wiki/Well-known_text) or [GeoJSON](http://geojson.org/).    To add some lon,lat points to the `fleet` index:    ```go  db.Update(func(tx *buntdb.Tx) error {  	tx.Set(""fleet:0:pos"", ""[-115.567 33.532]"", nil)  	tx.Set(""fleet:1:pos"", ""[-116.671 35.735]"", nil)  	tx.Set(""fleet:2:pos"", ""[-113.902 31.234]"", nil)  	return nil  })  ```    And then you can run the `Intersects` function on the index:    ```go  db.View(func(tx *buntdb.Tx) error {  	tx.Intersects(""fleet"", ""[-117 30],[-112 36]"", func(key, val string) bool {  		...  		return true  	})  	return nil  })  ```    This will get all three positions.    ### k-Nearest Neighbors    Use the `Nearby` function to get all the positions in order of nearest to farthest :    ```go  db.View(func(tx *buntdb.Tx) error {  	tx.Nearby(""fleet"", ""[-113 33]"", func(key, val string, dist float64) bool {  		...  		return true  	})  	return nil  })  ```    ### Spatial bracket syntax    The bracket syntax `[-117 30],[-112 36]` is unique to BuntDB, and it's how the built-in rectangles are processed. But, you are not limited to this syntax. Whatever Rect function you choose to use during `CreateSpatialIndex` will be used to process the parameter, in this case it's `IndexRect`.    - **2D rectangle:** `[10 15],[20 25]`  *Min XY: ""10x15"", Max XY: ""20x25""*    - **3D rectangle:** `[10 15 12],[20 25 18]`  *Min XYZ: ""10x15x12"", Max XYZ: ""20x25x18""*    - **2D point:** `[10 15]`  *XY: ""10x15""*    - **LonLat point:** `[-112.2693 33.5123]`  *LatLon: ""33.5123 -112.2693""*    - **LonLat bounding box:** `[-112.26 33.51],[-112.18 33.67]`  *Min LatLon: ""33.51 -112.26"", Max LatLon: ""33.67 -112.18""*    **Notice:** The longitude is the Y axis and is on the left, and latitude is the X axis and is on the right.    You can also represent `Infinity` by using `-inf` and `+inf`.  For example, you might have the following points (`[X Y M]` where XY is a point and M is a timestamp):  ```  [3 9 1]  [3 8 2]  [4 8 3]  [4 7 4]  [5 7 5]  [5 6 6]  ```    You can then do a search for all points with `M` between 2-4 by calling `Intersects`.    ```go  tx.Intersects(""points"", ""[-inf -inf 2],[+inf +inf 4]"", func(key, val string) bool {  	println(val)  	return true  })  ```    Which will return:    ```  [3 8 2]  [4 8 3]  [4 7 4]  ```    ## JSON Indexes  Indexes can be created on individual fields inside JSON documents. BuntDB uses [GJSON](https://github.com/tidwall/gjson) under the hood.    For example:    ```go  package main    import (  	""fmt""    	""github.com/tidwall/buntdb""  )    func main() {  	db, _ := buntdb.Open("":memory:"")  	db.CreateIndex(""last_name"", ""*"", buntdb.IndexJSON(""name.last""))  	db.CreateIndex(""age"", ""*"", buntdb.IndexJSON(""age""))  	db.Update(func(tx *buntdb.Tx) error {  		tx.Set(""1"", `{""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}`, nil)  		tx.Set(""2"", `{""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}`, nil)  		tx.Set(""3"", `{""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}`, nil)  		tx.Set(""4"", `{""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}`, nil)  		return nil  	})  	db.View(func(tx *buntdb.Tx) error {  		fmt.Println(""Order by last name"")  		tx.Ascend(""last_name"", func(key, value string) bool {  			fmt.Printf(""%s: %s\n"", key, value)  			return true  		})  		fmt.Println(""Order by age"")  		tx.Ascend(""age"", func(key, value string) bool {  			fmt.Printf(""%s: %s\n"", key, value)  			return true  		})  		fmt.Println(""Order by age range 30-50"")  		tx.AscendRange(""age"", `{""age"":30}`, `{""age"":50}`, func(key, value string) bool {  			fmt.Printf(""%s: %s\n"", key, value)  			return true  		})  		return nil  	})  }  ```    Results:    ```  Order by last name  3: {""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}  4: {""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}  1: {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}  2: {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}    Order by age  4: {""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}  1: {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}  2: {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}  3: {""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}    Order by age range 30-50  1: {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}  2: {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}  ```    ## Multi Value Index  With BuntDB it's possible to join multiple values on a single index.  This is similar to a [multi column index](http://dev.mysql.com/doc/refman/5.7/en/multiple-column-indexes.html) in a traditional SQL database.    In this example we are creating a multi value index on ""name.last"" and ""age"":    ```go  db, _ := buntdb.Open("":memory:"")  db.CreateIndex(""last_name_age"", ""*"", buntdb.IndexJSON(""name.last""), buntdb.IndexJSON(""age""))  db.Update(func(tx *buntdb.Tx) error {  	tx.Set(""1"", `{""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}`, nil)  	tx.Set(""2"", `{""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}`, nil)  	tx.Set(""3"", `{""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}`, nil)  	tx.Set(""4"", `{""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}`, nil)  	tx.Set(""5"", `{""name"":{""first"":""Sam"",""last"":""Anderson""},""age"":51}`, nil)  	tx.Set(""6"", `{""name"":{""first"":""Melinda"",""last"":""Prichard""},""age"":44}`, nil)  	return nil  })  db.View(func(tx *buntdb.Tx) error {  	tx.Ascend(""last_name_age"", func(key, value string) bool {  		fmt.Printf(""%s: %s\n"", key, value)  		return true  	})  	return nil  })    // Output:  // 5: {""name"":{""first"":""Sam"",""last"":""Anderson""},""age"":51}  // 3: {""name"":{""first"":""Carol"",""last"":""Anderson""},""age"":52}  // 4: {""name"":{""first"":""Alan"",""last"":""Cooper""},""age"":28}  // 1: {""name"":{""first"":""Tom"",""last"":""Johnson""},""age"":38}  // 6: {""name"":{""first"":""Melinda"",""last"":""Prichard""},""age"":44}  // 2: {""name"":{""first"":""Janet"",""last"":""Prichard""},""age"":47}  ```    ## Descending Ordered Index  Any index can be put in descending order by wrapping it's less function with `buntdb.Desc`.    ```go  db.CreateIndex(""last_name_age"", ""*"",      buntdb.IndexJSON(""name.last""),      buntdb.Desc(buntdb.IndexJSON(""age"")),  )  ```    This will create a multi value index where the last name is ascending and the age is descending.    ## Collate i18n Indexes    Using the external [collate package](https://github.com/tidwall/collate) it's possible to create  indexes that are sorted by the specified language. This is similar to the [SQL COLLATE keyword](https://msdn.microsoft.com/en-us/library/ms174596.aspx) found in traditional databases.    To install:    ```  go get -u github.com/tidwall/collate  ```    For example:    ```go  import ""github.com/tidwall/collate""    // To sort case-insensitive in French.  db.CreateIndex(""name"", ""*"", collate.IndexString(""FRENCH_CI""))    // To specify that numbers should sort numerically (""2"" < ""12"")  // and use a comma to represent a decimal point.  db.CreateIndex(""amount"", ""*"", collate.IndexString(""FRENCH_NUM""))  ```    There's also support for Collation on JSON indexes:    ```go  db.CreateIndex(""last_name"", ""*"", collate.IndexJSON(""CHINESE_CI"", ""name.last""))  ```    Check out the [collate project](https://github.com/tidwall/collate) for more information.    ## Data Expiration  Items can be automatically evicted by using the `SetOptions` object in the `Set` function to set a `TTL`.    ```go  db.Update(func(tx *buntdb.Tx) error {  	tx.Set(""mykey"", ""myval"", &buntdb.SetOptions{Expires:true, TTL:time.Second})  	return nil  })  ```    Now `mykey` will automatically be deleted after one second. You can remove the TTL by setting the value again with the same key/value, but with the options parameter set to nil.    ## Delete while iterating  BuntDB does not currently support deleting a key while in the process of iterating.  As a workaround you'll need to delete keys following the completion of the iterator.    ```go  var delkeys []string  tx.AscendKeys(""object:*"", func(k, v string) bool {  	if someCondition(k) == true {  		delkeys = append(delkeys, k)  	}  	return true // continue  })  for _, k := range delkeys {  	if _, err = tx.Delete(k); err != nil {  		return err  	}  }  ```    ## Append-only File    BuntDB uses an AOF (append-only file) which is a log of all database changes that occur from operations like `Set()` and `Delete()`.    The format of this file looks like:  ```  set key:1 value1  set key:2 value2  set key:1 value3  del key:2  ...  ```    When the database opens again, it will read back the aof file and process each command in exact order.  This read process happens one time when the database opens.  From there on the file is only appended.    As you may guess this log file can grow large over time.  There's a background routine that automatically shrinks the log file when it gets too large.  There is also a `Shrink()` function which will rewrite the aof file so that it contains only the items in the database.  The shrink operation does not lock up the database so read and write transactions can continue while shrinking is in process.    ### Durability and fsync    By default BuntDB executes an `fsync` once every second on the [aof file](#append-only-file). Which simply means that there's a chance that up to one second of data might be lost. If you need higher durability then there's an optional database config setting `Config.SyncPolicy` which can be set to `Always`.    The `Config.SyncPolicy` has the following options:    - `Never` - fsync is managed by the operating system, less safe  - `EverySecond` - fsync every second, fast and safer, this is the default  - `Always` - fsync after every write, very durable, slower    ## Config    Here are some configuration options that can be use to change various behaviors of the database.    - **SyncPolicy** adjusts how often the data is synced to disk. This value can be Never, EverySecond, or Always. Default is EverySecond.  - **AutoShrinkPercentage** is used by the background process to trigger a shrink of the aof file when the size of the file is larger than the percentage of the result of the previous shrunk file. For example, if this value is 100, and the last shrink process resulted in a 100mb file, then the new aof file must be 200mb before a shrink is triggered. Default is 100.  - **AutoShrinkMinSize** defines the minimum size of the aof file before an automatic shrink can occur. Default is 32MB.  - **AutoShrinkDisabled** turns off automatic background shrinking. Default is false.    To update the configuration you should call `ReadConfig` followed by `SetConfig`. For example:    ```go    var config buntdb.Config  if err := db.ReadConfig(&config); err != nil{  	log.Fatal(err)  }  if err := db.SetConfig(config); err != nil{  	log.Fatal(err)  }  ```    ## Performance    How fast is BuntDB?    Here are some example [benchmarks](https://github.com/tidwall/raft-buntdb#raftstore-performance-comparison) when using BuntDB in a Raft Store implementation.    You can also run the standard Go benchmark tool from the project root directory:    ```  go test --bench=.  ```    ### BuntDB-Benchmark    There's a [custom utility](https://github.com/tidwall/buntdb-benchmark) that was created specifically for benchmarking BuntDB.    *These are the results from running the benchmarks on a MacBook Pro 15"" 2.8 GHz Intel Core i7:*    ```  $ buntdb-benchmark -q  GET: 4609604.74 operations per second  SET: 248500.33 operations per second  ASCEND_100: 2268998.79 operations per second  ASCEND_200: 1178388.14 operations per second  ASCEND_400: 679134.20 operations per second  ASCEND_800: 348445.55 operations per second  DESCEND_100: 2313821.69 operations per second  DESCEND_200: 1292738.38 operations per second  DESCEND_400: 675258.76 operations per second  DESCEND_800: 337481.67 operations per second  SPATIAL_SET: 134824.60 operations per second  SPATIAL_INTERSECTS_100: 939491.47 operations per second  SPATIAL_INTERSECTS_200: 561590.40 operations per second  SPATIAL_INTERSECTS_400: 306951.15 operations per second  SPATIAL_INTERSECTS_800: 159673.91 operations per second  ```    To install this utility:    ```  go get github.com/tidwall/buntdb-benchmark  ```        ## Contact  Josh Baker [@tidwall](http://twitter.com/tidwall)    ## License    BuntDB source code is available under the MIT [License](/LICENSE). """
Big data;https://github.com/ankane/blazer;"""# Blazer    Explore your data with SQL. Easily create charts and dashboards, and share them with your team.    [Try it out](https://blazer.dokkuapp.com)    [![Screenshot](https://blazer.dokkuapp.com/assets/blazer-a10baa40fef1ca2f5bb25fc97bcf261a6a54192fb1ad0f893c0f562b8c7c4697.png)](https://blazer.dokkuapp.com)    Blazer is also available as a [Docker image](https://github.com/ankane/blazer-docker).    :tangerine: Battle-tested at [Instacart](https://www.instacart.com/opensource)    [![Build Status](https://github.com/ankane/blazer/workflows/build/badge.svg?branch=master)](https://github.com/ankane/blazer/actions)    ## Features    - **Multiple data sources** - PostgreSQL, MySQL, Redshift, and [many more](#full-list)  - **Variables** - run the same queries with different values  - **Checks & alerts** - get emailed when bad data appears  - **Audits** - all queries are tracked  - **Security** - works with your authentication system    ## Docs    - [Installation](#installation)  - [Queries](#queries)  - [Charts](#charts)  - [Dashboards](#dashboards)  - [Checks](#checks)  - [Cohorts](#cohorts)  - [Anomaly Detection](#anomaly-detection)  - [Forecasting](#forecasting)  - [Uploads](#uploads)  - [Data Sources](#data-sources)  - [Query Permissions](#query-permissions)    ## Installation    Add this line to your application’s Gemfile:    ```ruby  gem ""blazer""  ```    Run:    ```sh  rails generate blazer:install  rails db:migrate  ```    And mount the dashboard in your `config/routes.rb`:    ```ruby  mount Blazer::Engine, at: ""blazer""  ```    For production, specify your database:    ```ruby  ENV[""BLAZER_DATABASE_URL""] = ""postgres://user:password@hostname:5432/database""  ```    Blazer tries to protect against queries which modify data (by running each query in a transaction and rolling it back), but a safer approach is to use a read-only user. [See how to create one](#permissions).    #### Checks (optional)    Be sure to set a host in `config/environments/production.rb` for emails to work.    ```ruby  config.action_mailer.default_url_options = {host: ""blazer.dokkuapp.com""}  ```    Schedule checks to run (with cron, [Heroku Scheduler](https://elements.heroku.com/addons/scheduler), etc). The default options are every 5 minutes, 1 hour, or 1 day, which you can customize. For each of these options, set up a task to run.    ```sh  rake blazer:run_checks SCHEDULE=""5 minutes""  rake blazer:run_checks SCHEDULE=""1 hour""  rake blazer:run_checks SCHEDULE=""1 day""  ```    You can also set up failing checks to be sent once a day (or whatever you prefer).    ```sh  rake blazer:send_failing_checks  ```    Here’s what it looks like with cron.    ```  */5 * * * * rake blazer:run_checks SCHEDULE=""5 minutes""  0   * * * * rake blazer:run_checks SCHEDULE=""1 hour""  30  7 * * * rake blazer:run_checks SCHEDULE=""1 day""  0   8 * * * rake blazer:send_failing_checks  ```    For Slack notifications, create an [incoming webhook](https://slack.com/apps/A0F7XDUAZ-incoming-webhooks) and set:    ```sh  BLAZER_SLACK_WEBHOOK_URL=https://hooks.slack.com/...  ```    Name the webhook “Blazer” and add a cool icon.    ## Authentication    Don’t forget to protect the dashboard in production.    ### Basic Authentication    Set the following variables in your environment or an initializer.    ```ruby  ENV[""BLAZER_USERNAME""] = ""andrew""  ENV[""BLAZER_PASSWORD""] = ""secret""  ```    ### Devise    ```ruby  authenticate :user, ->(user) { user.admin? } do    mount Blazer::Engine, at: ""blazer""  end  ```    ### Other    Specify a `before_action` method to run in `blazer.yml`.    ```yml  before_action_method: require_admin  ```    You can define this method in your `ApplicationController`.    ```ruby  def require_admin    # depending on your auth, something like...    redirect_to root_path unless current_user && current_user.admin?  end  ```    Be sure to render or redirect for unauthorized users.    ## Permissions    Blazer runs each query in a transaction and rolls it back to prevent queries from modifying data. As an additional line of defense, we recommend using a read only user.    ### PostgreSQL    Create a user with read only permissions:    ```sql  BEGIN;  CREATE ROLE blazer LOGIN PASSWORD 'secret123';  GRANT CONNECT ON DATABASE database_name TO blazer;  GRANT USAGE ON SCHEMA public TO blazer;  GRANT SELECT ON ALL TABLES IN SCHEMA public TO blazer;  ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO blazer;  COMMIT;  ```    ### MySQL    Create a user with read only permissions:    ```sql  GRANT SELECT, SHOW VIEW ON database_name.* TO blazer@’127.0.0.1′ IDENTIFIED BY ‘secret123‘;  FLUSH PRIVILEGES;  ```    ### MongoDB    Create a user with read only permissions:    ```  db.createUser({user: ""blazer"", pwd: ""password"", roles: [""read""]})  ```    Also, make sure authorization is enabled when you start the server.    ## Sensitive Data    If your database contains sensitive or personal data, check out [Hypershield](https://github.com/ankane/hypershield) to shield it.    ## Encrypted Data    If you need to search encrypted data, use [blind indexing](https://github.com/ankane/blind_index).    You can have Blazer transform specific variables with:    ```ruby  Blazer.transform_variable = lambda do |name, value|    value = User.generate_email_bidx(value) if name == ""email_bidx""    value  end  ```    ## Queries    ### Variables    Create queries with variables.    ```sql  SELECT * FROM users WHERE gender = {gender}  ```    Use `{start_time}` and `{end_time}` for time ranges. [Example](https://blazer.dokkuapp.com/queries/9-time-range-selector?start_time=1997-10-03T05%3A00%3A00%2B00%3A00&end_time=1997-10-04T04%3A59%3A59%2B00%3A00)    ```sql  SELECT * FROM ratings WHERE rated_at >= {start_time} AND rated_at <= {end_time}  ```    ### Smart Variables    [Example](https://blazer.dokkuapp.com/queries/1-smart-variable)    Suppose you have the query:    ```sql  SELECT * FROM users WHERE occupation_id = {occupation_id}  ```    Instead of remembering each occupation’s id, users can select occupations by name.    Add a smart variable with:    ```yml  smart_variables:    occupation_id: ""SELECT id, name FROM occupations ORDER BY name ASC""  ```    The first column is the value of the variable, and the second column is the label.    You can also use an array or hash for static data and enums.    ```yml  smart_variables:    period: [""day"", ""week"", ""month""]    status: {0: ""Active"", 1: ""Archived""}  ```    ### Linked Columns    [Example](https://blazer.dokkuapp.com/queries/3-linked-column) - title column    Link results to other pages in your apps or around the web. Specify a column name and where it should link to. You can use the value of the result with `{value}`.    ```yml  linked_columns:    user_id: ""/admin/users/{value}""    ip_address: ""https://www.infosniper.net/index.php?ip_address={value}""  ```    ### Smart Columns    [Example](https://blazer.dokkuapp.com/queries/2-smart-column) - occupation_id column    Suppose you have the query:    ```sql  SELECT name, city_id FROM users  ```    See which city the user belongs to without a join.    ```yml  smart_columns:    city_id: ""SELECT id, name FROM cities WHERE id IN {value}""  ```    You can also use a hash for static data and enums.    ```yml  smart_columns:    status: {0: ""Active"", 1: ""Archived""}  ```    ### Caching    Blazer can automatically cache results to improve speed. It can cache slow queries:    ```yml  cache:    mode: slow    expires_in: 60 # min    slow_threshold: 15 # sec  ```    Or it can cache all queries:    ```yml  cache:    mode: all    expires_in: 60 # min  ```    Of course, you can force a refresh at any time.    ## Charts    Blazer will automatically generate charts based on the types of the columns returned in your query.    **Note:** The order of columns matters.    ### Line Chart    There are two ways to generate line charts.    2+ columns - timestamp, numeric(s) - [Example](https://blazer.dokkuapp.com/queries/4-line-chart-format-1)    ```sql  SELECT date_trunc('week', created_at), COUNT(*) FROM users GROUP BY 1  ```    3 columns - timestamp, string, numeric - [Example](https://blazer.dokkuapp.com/queries/5-line-chart-format-2)      ```sql  SELECT date_trunc('week', created_at), gender, COUNT(*) FROM users GROUP BY 1, 2  ```    ### Column Chart    There are also two ways to generate column charts.    2+ columns - string, numeric(s) - [Example](https://blazer.dokkuapp.com/queries/6-column-chart-format-1)    ```sql  SELECT gender, COUNT(*) FROM users GROUP BY 1  ```    3 columns - string, string, numeric - [Example](https://blazer.dokkuapp.com/queries/7-column-chart-format-2)    ```sql  SELECT gender, zip_code, COUNT(*) FROM users GROUP BY 1, 2  ```    ### Scatter Chart    2 columns - both numeric - [Example](https://blazer.dokkuapp.com/queries/16-scatter-chart)    ```sql  SELECT x, y FROM table  ```    ### Pie Chart    2 columns - string, numeric - and last column named `pie` - [Example](https://blazer.dokkuapp.com/queries/17-pie-chart)    ```sql  SELECT gender, COUNT(*) AS pie FROM users GROUP BY 1  ```    ### Maps    Columns named `latitude` and `longitude` or `lat` and `lon` or `lat` and `lng` - [Example](https://blazer.dokkuapp.com/queries/15-map)    ```sql  SELECT name, latitude, longitude FROM cities  ```    To enable, get an access token from [Mapbox](https://www.mapbox.com/) and set `ENV[""MAPBOX_ACCESS_TOKEN""]`.    ### Targets    Use the column name `target` to draw a line for goals. [Example](https://blazer.dokkuapp.com/queries/8-target-line)    ```sql  SELECT date_trunc('week', created_at), COUNT(*) AS new_users, 100000 AS target FROM users GROUP BY 1  ```    ## Dashboards    Create a dashboard with multiple queries. [Example](https://blazer.dokkuapp.com/dashboards/1-dashboard-demo)    If the query has a chart, the chart is shown. Otherwise, you’ll see a table.    If any queries have variables, they will show up on the dashboard.    ## Checks    Checks give you a centralized place to see the health of your data. [Example](https://blazer.dokkuapp.com/checks)    Create a query to identify bad rows.    ```sql  SELECT * FROM ratings WHERE user_id IS NULL /* all ratings should have a user */  ```    Then create check with optional emails if you want to be notified. Emails are sent when a check starts failing, and when it starts passing again.    ## Cohorts    Create a cohort analysis from a simple SQL query. [Example](https://blazer.dokkuapp.com/queries/19-cohort-analysis-from-first-order)    Create a query with the comment `/* cohort analysis */`. The result should have columns named `user_id` and `conversion_time` and optionally `cohort_time`.    You can generate cohorts from the first conversion time:    ```sql  /* cohort analysis */  SELECT user_id, created_at AS conversion_time FROM orders  ```    (the first conversion isn’t counted in the first time period with this format)    Or from another time, like sign up:    ```sql  /* cohort analysis */  SELECT users.id AS user_id, orders.created_at AS conversion_time, users.created_at AS cohort_time  FROM users LEFT JOIN orders ON orders.user_id = users.id  ```    This feature requires PostgreSQL or MySQL 8.    ## Anomaly Detection    Blazer supports three different approaches to anomaly detection.    ### Prophet    Add [prophet-rb](https://github.com/ankane/prophet) to your Gemfile:    ```ruby  gem ""prophet-rb""  ```    And add to `config/blazer.yml`:    ```yml  anomaly_checks: prophet  ```    ### Trend    [Trend](https://trendapi.org/) uses an external service by default, but you can run it on your own infrastructure as well.    Add [trend](https://github.com/ankane/trend) to your Gemfile:    ```ruby  gem ""trend""  ```    And add to `config/blazer.yml`:    ```yml  anomaly_checks: trend  ```    For the [self-hosted API](https://github.com/ankane/trend-api), create an initializer with:    ```ruby  Trend.url = ""http://localhost:8000""  ```    ### AnomalyDetection.rb (experimental)    Add [anomaly_detection](https://github.com/ankane/AnomalyDetection.rb) to your Gemfile:    ```ruby  gem ""anomaly_detection""  ```    And add to `config/blazer.yml`:    ```yml  anomaly_checks: anomaly_detection  ```    ## Forecasting    Blazer supports for two different forecasting methods. [Example](https://blazer.dokkuapp.com/queries/18-forecast?forecast=t)    A forecast link will appear for queries that return 2 columns with types timestamp and numeric.    ### Prophet    Add [prophet-rb](https://github.com/ankane/prophet) to your Gemfile:    ```ruby  gem ""prophet-rb"", "">= 0.2.1""  ```    And add to `config/blazer.yml`:    ```yml  forecasting: prophet  ```    ### Trend    [Trend](https://trendapi.org/) uses an external service by default, but you can run it on your own infrastructure as well.    Add [trend](https://github.com/ankane/trend) to your Gemfile:    ```ruby  gem ""trend""  ```    And add to `config/blazer.yml`:    ```yml  forecasting: trend  ```    For the [self-hosted API](https://github.com/ankane/trend-api), create an initializer with:    ```ruby  Trend.url = ""http://localhost:8000""  ```    ## Uploads    Creating database tables from CSV files. [Example](https://blazer.dokkuapp.com/uploads)    Run:    ```sh  rails generate blazer:uploads  rails db:migrate  ```    And add to `config/blazer.yml`:    ```yml  uploads:    url: postgres://...    schema: uploads    data_source: main  ```    This feature requires PostgreSQL. Create a new schema just for uploads.    ```sql  CREATE SCHEMA uploads;  ```    ## Data Sources    Blazer supports multiple data sources :tada:    Add additional data sources in `config/blazer.yml`:    ```yml  data_sources:    main:      url: <%= ENV[""BLAZER_DATABASE_URL""] %>      # timeout, smart_variables, linked_columns, smart_columns    catalog:      url: <%= ENV[""CATALOG_DATABASE_URL""] %>      # ...    redshift:      url: <%= ENV[""REDSHIFT_DATABASE_URL""] %>      # ...  ```    ### Full List    - [Amazon Athena](#amazon-athena)  - [Amazon Redshift](#amazon-redshift)  - [Apache Drill](#apache-drill)  - [Apache Hive](#apache-hive)  - [Apache Ignite](#apache-ignite)  - [Apache Spark](#apache-spark)  - [Cassandra](#cassandra)  - [Druid](#druid)  - [Elasticsearch](#elasticsearch)  - [Google BigQuery](#google-bigquery)  - [IBM DB2 and Informix](#ibm-db2-and-informix)  - [InfluxDB](#influxdb)  - [MongoDB](#mongodb-1)  - [MySQL](#mysql-1)  - [Neo4j](#neo4j)  - [OpenSearch](#opensearch)  - [Oracle](#oracle)  - [PostgreSQL](#postgresql-1)  - [Presto](#presto)  - [Salesforce](#salesforce)  - [Socrata Open Data API (SODA)](#socrata-open-data-api-soda)  - [Snowflake](#snowflake)  - [SQLite](#sqlite)  - [SQL Server](#sql-server)    You can also [create an adapter](#creating-an-adapter) for any other data store.    **Note:** In the examples below, we recommend using environment variables for urls.    ```yml  data_sources:    my_source:      url: <%= ENV[""BLAZER_MY_SOURCE_URL""] %>  ```    ### Amazon Athena    Add [aws-sdk-athena](https://github.com/aws/aws-sdk-ruby) and [aws-sdk-glue](https://github.com/aws/aws-sdk-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: athena      database: database        # optional settings      output_location: s3://some-bucket/      workgroup: primary      access_key_id: ...      secret_access_key: ...  ```    Here’s an example IAM policy:    ```json  {      ""Version"": ""2012-10-17"",      ""Statement"": [          {              ""Effect"": ""Allow"",              ""Action"": [                  ""athena:GetQueryExecution"",                  ""athena:GetQueryResults"",                  ""athena:StartQueryExecution""              ],              ""Resource"": [                  ""arn:aws:athena:region:account-id:workgroup/primary""              ]          },          {              ""Effect"": ""Allow"",              ""Action"": [                  ""glue:GetTable"",                  ""glue:GetTables""              ],              ""Resource"": [                  ""arn:aws:glue:region:account-id:catalog"",                  ""arn:aws:glue:region:account-id:database/default"",                  ""arn:aws:glue:region:account-id:table/default/*""              ]          }      ]  }  ```    You also need to configure [S3 permissions](https://aws.amazon.com/premiumsupport/knowledge-center/access-denied-athena/).    ### Amazon Redshift    Add [activerecord6-redshift-adapter](https://github.com/kwent/activerecord6-redshift-adapter) or [activerecord5-redshift-adapter](https://github.com/ConsultingMD/activerecord5-redshift-adapter) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: redshift://user:password@hostname:5439/database  ```    ### Apache Drill    Add [drill-sergeant](https://github.com/ankane/drill-sergeant) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: drill      url: http://hostname:8047  ```    ### Apache Hive    Add [hexspace](https://github.com/ankane/hexspace) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: hive      url: sasl://user:password@hostname:10000/database  ```    Use a [read-only user](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Authorization). Requires [HiveServer2](https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2).    ### Apache Ignite    Add [ignite-client](https://github.com/ankane/ignite-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: ignite://user:password@hostname:10800  ```    ### Apache Spark    Add [hexspace](https://github.com/ankane/hexspace) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: spark      url: sasl://user:password@hostname:10000/database  ```    Use a read-only user. Requires the [Thrift server](https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html).    ### Cassandra    Add [cassandra-driver](https://github.com/datastax/ruby-driver) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: cassandra://user:password@hostname:9042/keyspace  ```    ### Druid    Enable [SQL support](http://druid.io/docs/latest/querying/sql.html#configuration) on the broker and set:    ```yml  data_sources:    my_source:      adapter: druid      url: http://hostname:8082  ```    ### Elasticsearch    Add [elasticsearch](https://github.com/elastic/elasticsearch-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: elasticsearch      url: http://user:password@hostname:9200  ```    ### Google BigQuery    Add [google-cloud-bigquery](https://github.com/GoogleCloudPlatform/google-cloud-ruby/tree/master/google-cloud-bigquery) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: bigquery      project: your-project      keyfile: path/to/keyfile.json  ```    ### IBM DB2 and Informix    Add [ibm_db](https://github.com/ibmdb/ruby-ibmdb) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: ibm-db://user:password@hostname:50000/database  ```    ### InfluxDB    Add [influxdb](https://github.com/influxdata/influxdb-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: influxdb      url: http://user:password@hostname:8086/database  ```    Supports [InfluxQL](https://docs.influxdata.com/influxdb/v1.8/query_language/explore-data/)    ### MongoDB    *Requires MongoDB < 4.2 at the moment*    Add [mongo](https://github.com/mongodb/mongo-ruby-driver) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: mongodb://user:password@hostname:27017/database  ```    ### MySQL    Add [mysql2](https://github.com/brianmario/mysql2) to your Gemfile (if it’s not there) and set:    ```yml  data_sources:    my_source:      url: mysql2://user:password@hostname:3306/database  ```    ### Neo4j    Add [neo4j-core](https://github.com/neo4jrb/neo4j-core) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: neo4j      url: http://user:password@hostname:7474  ```    ### OpenSearch    Add [opensearch-ruby](https://github.com/opensearch-project/opensearch-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: opensearch      url: http://user:password@hostname:9200  ```    ### Oracle    Add [activerecord-oracle_enhanced-adapter](https://github.com/rsim/oracle-enhanced) and [ruby-oci8](https://github.com/kubo/ruby-oci8) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: oracle-enhanced://user:password@hostname:1521/database  ```    ### PostgreSQL    Add [pg](https://github.com/ged/ruby-pg) to your Gemfile (if it’s not there) and set:    ```yml  data_sources:    my_source:      url: postgres://user:password@hostname:5432/database  ```    ### Presto    Add [presto-client](https://github.com/treasure-data/presto-client-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: presto://user@hostname:8080/catalog  ```    ### Salesforce    Add [restforce](https://github.com/restforce/restforce) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: salesforce  ```    And set the appropriate environment variables:    ```sh  SALESFORCE_USERNAME=""username""  SALESFORCE_PASSWORD=""password""  SALESFORCE_SECURITY_TOKEN=""security token""  SALESFORCE_CLIENT_ID=""client id""  SALESFORCE_CLIENT_SECRET=""client secret""  SALESFORCE_API_VERSION=""41.0""  ```    Supports [SOQL](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm)    ### Socrata Open Data API (SODA)    Set:    ```yml  data_sources:    my_source:      adapter: soda      url: https://soda.demo.socrata.com/resource/4tka-6guv.json      app_token: ...  ```    Supports [SoQL](https://dev.socrata.com/docs/functions/)    ### Snowflake    First, install ODBC. For Homebrew, use:    ```sh  brew install unixodbc  ```    For Ubuntu, use:    ```sh  sudo apt-get install unixodbc-dev  ```    For Heroku, use the [Apt buildpack](https://github.com/heroku/heroku-buildpack-apt) and create an `Aptfile` with:    ```text  unixodbc-dev  https://sfc-repo.snowflakecomputing.com/odbc/linux/2.21.5/snowflake-odbc-2.21.5.x86_64.deb  ```    > This installs the driver at `/app/.apt/usr/lib/snowflake/odbc/lib/libSnowflake.so`    Then, download the [Snowflake ODBC driver](https://docs.snowflake.net/manuals/user-guide/odbc-download.html). Add [odbc_adapter](https://github.com/localytics/odbc_adapter) to your Gemfile and set:    ```yml  data_sources:    my_source:      adapter: snowflake      conn_str: Driver=/path/to/libSnowflake.so;uid=user;pwd=password;server=host.snowflakecomputing.com  ```    ### SQLite    Add [sqlite3](https://github.com/sparklemotion/sqlite3-ruby) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: sqlite3:path/to/database.sqlite3  ```    ### SQL Server    Add [tiny_tds](https://github.com/rails-sqlserver/tiny_tds) and [activerecord-sqlserver-adapter](https://github.com/rails-sqlserver/activerecord-sqlserver-adapter) to your Gemfile and set:    ```yml  data_sources:    my_source:      url: sqlserver://user:password@hostname:1433/database  ```    ## Creating an Adapter    Create an adapter for any data store with:    ```ruby  class FooAdapter < Blazer::Adapters::BaseAdapter    # code goes here  end    Blazer.register_adapter ""foo"", FooAdapter  ```    See the [Presto adapter](https://github.com/ankane/blazer/blob/master/lib/blazer/adapters/presto_adapter.rb) for a good example. Then use:    ```yml  data_sources:    my_source:      adapter: foo      url: http://user:password@hostname:9200/  ```    ## Query Permissions    Blazer supports a basic permissions model.    1. Queries without a name are unlisted  2. Queries whose name starts with `#` are only listed to the creator  3. Queries whose name starts with `*` can only be edited by the creator    ## Learn SQL    Have team members who want to learn SQL? Here are a few great, free resources.    - [The Data School](https://dataschool.com/learn-sql/)  - [SQLBolt](https://sqlbolt.com/)    ## Useful Tools    For an easy way to group by day, week, month, and more with correct time zones, check out [Groupdate.sql](https://github.com/ankane/groupdate.sql).    ## Standalone Version    Looking for a standalone version? Check out [Ghost Blazer](https://github.com/buren/ghost_blazer).    ## Performance    By default, queries take up a request while they are running. To run queries asynchronously, add to your config:    ```yml  async: true  ```    **Note:** Requires Rails 5+ and caching to be enabled. If you have multiple web processes, your app must use a centralized cache store like Memcached or Redis.    ```ruby  config.cache_store = :mem_cache_store  ```    ## Archiving    Archive queries that haven’t been viewed in over 90 days.    ```sh  rake blazer:archive_queries  ```    ## Content Security Policy    If views are stuck with a `Loading...` message, there might be a problem with strict CSP settings in your app. This can be checked with Firefox or Chrome dev tools. You can allow Blazer to override these settings for its controllers with:    ```yml  override_csp: true  ```    ## Upgrading    ### 2.3    To archive queries, create a migration    ```sh  rails g migration add_status_to_blazer_queries  ```    with:    ```ruby  add_column :blazer_queries, :status, :string  Blazer::Query.update_all(status: ""active"")  ```    ### 2.0    To use Slack notifications, create a migration    ```sh  rails g migration add_slack_channels_to_blazer_checks  ```    with:    ```ruby  add_column :blazer_checks, :slack_channels, :text  ```    ## History    View the [changelog](https://github.com/ankane/blazer/blob/master/CHANGELOG.md)    ## Thanks    Blazer uses a number of awesome open source projects, including [Rails](https://github.com/rails/rails/), [Vue.js](https://github.com/vuejs/vue), [jQuery](https://github.com/jquery/jquery), [Bootstrap](https://github.com/twbs/bootstrap), [Selectize](https://github.com/brianreavis/selectize.js), [StickyTableHeaders](https://github.com/jmosbech/StickyTableHeaders), [Stupid jQuery Table Sort](https://github.com/joequery/Stupid-Table-Plugin), and [Date Range Picker](https://github.com/dangrossman/bootstrap-daterangepicker).    Demo data from [MovieLens](https://grouplens.org/datasets/movielens/).    ## Want to Make Blazer Better?    That’s awesome! Here are a few ways you can help:    - [Report bugs](https://github.com/ankane/blazer/issues)  - Fix bugs and [submit pull requests](https://github.com/ankane/blazer/pulls)  - Write, clarify, or fix documentation  - Suggest or add new features    Check out the [dev app](https://github.com/ankane/blazer-dev) to get started. """
Big data;https://github.com/intel-hadoop/HiBench;"""# HiBench Suite  ## The bigdata micro benchmark suite ##      * Current version: 7.1.1  * Homepage: https://github.com/intel-hadoop/HiBench  * Contents:    1. Overview    2. Getting Started    3. Workloads    4. Supported Releases    ---  ### OVERVIEW ###    HiBench is a big data benchmark suite that helps evaluate different big data frameworks in terms of speed, throughput and system resource utilizations. It contains a set of Hadoop, Spark and streaming workloads, including Sort, WordCount, TeraSort, Repartition, Sleep, SQL, PageRank, Nutch indexing, Bayes, Kmeans, NWeight and enhanced DFSIO, etc. It also contains several streaming workloads for Spark Streaming, Flink, Storm and Gearpump.    ### Getting Started ###   * [Build HiBench](docs/build-hibench.md)   * [Run HadoopBench](docs/run-hadoopbench.md)   * [Run SparkBench](docs/run-sparkbench.md)   * [Run StreamingBench](docs/run-streamingbench.md) (Spark streaming, Flink, Storm, Gearpump)    ### Workloads ###    There are totally 29 workloads in HiBench. The workloads are divided into 6 categories which are micro, ml(machine learning), sql, graph, websearch and streaming.      **Micro Benchmarks:**    1. Sort (sort)        This workload sorts its *text* input data, which is generated using RandomTextWriter.    2. WordCount (wordcount)        This workload counts the occurrence of each word in the input data, which are generated using RandomTextWriter. It is representative of another typical class of real world MapReduce jobs - extracting a small amount of interesting data from large data set.    3. TeraSort (terasort)        TeraSort is a standard benchmark created by Jim Gray. Its input data is generated by Hadoop TeraGen example program.        4. Repartition (micro/repartition)            This workload benchmarks shuffle performance. Input data is generated by Hadoop TeraGen. The workload randomly selects the post-shuffle partition for each record, performs shuffle write and read, evenly repartitioning the records. There are 2 parameters providing options to eliminate data source & sink I/Os: hibench.repartition.cacheinmemory(default: false) and hibench.repartition.disableOutput(default: false), controlling whether or not to 1) cache the input in memory at first 2) write the result to storage    5. Sleep (sleep)        This workload sleep an amount of seconds in each task to test framework scheduler.    6. enhanced DFSIO (dfsioe)        Enhanced DFSIO tests the HDFS throughput of the Hadoop cluster by generating a large number of tasks performing writes and reads simultaneously. It measures the average I/O rate of each map task, the average throughput of each map task, and the aggregated throughput of HDFS cluster. Note: this benchmark doesn't have Spark corresponding implementation.      **Machine Learning:**    1. Bayesian Classification (Bayes)        Naive Bayes is a simple multiclass classification algorithm with the assumption of independence between every pair of features. This workload is implemented in spark.mllib and uses the automatically generated documents whose words follow the zipfian distribution. The dict used for text generation is also from the default linux file /usr/share/dict/linux.words.ords.    2. K-means clustering (Kmeans)        This workload tests the K-means (a well-known clustering algorithm for knowledge discovery and data mining) clustering in spark.mllib. The input data set is generated by GenKMeansDataset based on Uniform Distribution and Guassian Distribution. There is also an optimized K-means implementation based on DAL (Intel Data Analytics Library), which is available in the dal module of sparkbench.        3. Gaussian Mixture Model (GMM)         Gaussian Mixture Model represents a composite distribution whereby points are drawn from one of k Gaussian sub-distributions, each with its own probability. It's implemented in spark.mllib. The input data set is generated by GenKMeansDataset based on Uniform Distribution and Guassian Distribution.        4. Logistic Regression (LR)        Logistic Regression (LR) is a popular method to predict a categorical response. This workload is implemented in spark.mllib with LBFGS optimizer and the input data set is generated by LogisticRegressionDataGenerator based on random balance decision tree. It contains three different kinds of data types, including categorical data, continuous data, and binary data.    5. Alternating Least Squares (ALS)        The alternating least squares (ALS) algorithm is a well-known algorithm for collaborative filtering. This workload is implemented in spark.mllib and the input data set is generated by RatingDataGenerator for a product recommendation system.    6. Gradient Boosted Trees (GBT)        Gradient-boosted trees (GBT) is a popular regression method using ensembles of decision trees. This workload is implemented in spark.mllib and the input data set is generated by GradientBoostedTreeDataGenerator.    7. XGBoost (XGBoost)        XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. This workload is implemented with XGBoost4J-Spark API in spark.mllib and the input data set is generated by GradientBoostedTreeDataGenerator.        8. Linear Regression (Linear)        Linear Regression (Linear) is a workload that implemented in spark.ml with ElasticNet. The input data set is generated by LinearRegressionDataGenerator.    9. Latent Dirichlet Allocation (LDA)        Latent Dirichlet allocation (LDA) is a topic model which infers topics from a collection of text documents. This workload is implemented in spark.mllib and the input data set is generated by LDADataGenerator.    10. Principal Components Analysis (PCA)        Principal component analysis (PCA) is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible. PCA is used widely in dimensionality reduction. This workload is implemented in spark.ml. The input data set is generated by PCADataGenerator.    11. Random Forest (RF)        Random forests (RF) are ensembles of decision trees. Random forests are one of the most successful machine learning models for classification and regression. They combine many decision trees in order to reduce the risk of overfitting. This workload is implemented in spark.mllib and the input data set is generated by RandomForestDataGenerator.    12. Support Vector Machine (SVM)        Support Vector Machine (SVM) is a standard method for large-scale classification tasks. This workload is implemented in spark.mllib and the input data set is generated by SVMDataGenerator.    13. Singular Value Decomposition (SVD)        Singular value decomposition (SVD) factorizes a matrix into three matrices. This workload is implemented in spark.mllib and its input data set is generated by SVDDataGenerator.      **SQL:**    1. Scan (scan) 2. Join (join), 3. Aggregate (aggregation)        These workloads are developed based on SIGMOD 09 paper ""A Comparison of Approaches to Large-Scale Data Analysis"" and HIVE-396. It contains Hive queries (Aggregation and Join) performing the typical OLAP queries described in the paper. Its input is also automatically generated Web data with hyperlinks following the Zipfian distribution.    **Websearch Benchmarks:**    1. PageRank (pagerank)        This workload benchmarks PageRank algorithm implemented in Spark-MLLib/Hadoop (a search engine ranking benchmark included in pegasus 2.0) examples. The data source is generated from Web data whose hyperlinks follow the Zipfian distribution.    2. Nutch indexing (nutchindexing)        Large-scale search indexing is one of the most significant uses of MapReduce. This workload tests the indexing sub-system in Nutch, a popular open source (Apache project) search engine. The workload uses the automatically generated Web data whose hyperlinks and words both follow the Zipfian distribution with corresponding parameters. The dict used to generate the Web page texts is the default linux dict file.    **Graph Benchmark:**    1. NWeight (nweight)         NWeight is an iterative graph-parallel algorithm implemented by Spark GraphX and pregel. The algorithm computes associations between two vertices that are n-hop away.       **Streaming Benchmarks:**    1. Identity (identity)        This workload reads input data from Kafka and then writes result to Kafka immediately, there is no complex business logic involved.    2. Repartition (streaming/repartition)        This workload reads input data from Kafka and changes the level of parallelism by creating more or fewer partitions. It tests the efficiency of data shuffle in the streaming frameworks.        3. Stateful Wordcount (wordcount)        This workload counts words cumulatively received from Kafka every few seconds. This tests the stateful operator performance and Checkpoint/Acker cost in the streaming frameworks.        4. Fixwindow (fixwindow)        The workloads performs a window based aggregation. It tests the performance of window operation in the streaming frameworks.            ### Supported Hadoop/Spark/Flink/Storm/Gearpump releases: ###      - Hadoop: Apache Hadoop 3.0.x, 3.1.x, 3.2.x, 2.x, CDH5, HDP    - Spark: Spark 2.4.x, Spark 3.0.x, Spark 3.1.x    - Flink: 1.0.3    - Storm: 1.0.1    - Gearpump: 0.8.1    - Kafka: 0.8.2.2    ---     """
Big data;https://github.com/CartoDB/cartodb;"""# What is CARTO?    [![Code Climate](https://codeclimate.com/github/CartoDB/cartodb20.png)](https://codeclimate.com/github/CartoDB/cartodb20)  ![Build Status](https://github.com/CartoDB/cartodb/workflows/Node.js%20Tests/badge.svg)    CARTO is an open, powerful, and intuitive platform for discovering and predicting the key insights underlying the location data in our world.    Empower organizations to optimize operational performance, strategic investments, and everyday decisions with CARTO Engine—our embeddable platform for web and mobile apps—and the new CARTO Builder, a drag and drop analysis tool.    It was built to make it easier for people to tell their stories by  providing them with flexible and intuitive ways to create maps and design  geospatial applications. CARTO can be installed on your own server  and we also offer a hosted service at [carto.com](https://carto.com).    If you would like to see some live demos, check out our  [videos](http://www.vimeo.com/channels/carto) on Vimeo.  We hope you like it!    ![Map view](http://cartodb.s3.amazonaws.com/github/map.png)  ![Data View](http://cartodb.s3.amazonaws.com/github/dataset.png)    ## What can I do with CARTO?    With CARTO, you can upload your geospatial data (Shapefiles, GeoJSON,  etc) using a web form and then make it public or private.    After it is uploaded, you can visualize it in a dataset or on a map, search  it using SQL, and apply map styles using CartoCSS. You can even access it  using the CARTO [APIs](https://docs.carto.com/#cartodb-platform), or export it  to a file.    In other words, with CARTO you can make awesome maps and build  powerful geospatial applications! Definitely check out the [CARTO  Platform](https://carto.com/platform) for interactive examples  and code.      ## Installing    Read the [installation guide in CARTO developers documentation](http://cartodb.readthedocs.org/en/latest/install.html)    ## How do I upgrade CARTO?    See [UPGRADE](UPGRADE) for instructions about upgrading CARTO.    For upgrade of Windshaft-CartoDB and CartoDB-SQL-API see the relative  documentation.    ## Developing & Contributing to CARTO    See [our contributing doc](CONTRIBUTING.md) for how you can improve CARTO, but you will need to sign a Contributor License Agreement (CLA) before making a submission, [learn more here](https://carto.com/contributions).    ## Testing    Check the [testing doc](TESTING.md) section.    ## Requirements    CARTO works in any modern browser, but if you want more info:    ![Chrome](https://cdnjs.cloudflare.com/ajax/libs/browser-logos/39.3.0/archive/chrome_12-48/chrome_12-48_48x48.png) | ![Firefox](https://cdnjs.cloudflare.com/ajax/libs/browser-logos/39.3.0/archive/firefox_1.5-3/firefox_1.5-3_48x48.png) | ![IE](https://cdnjs.cloudflare.com/ajax/libs/browser-logos/39.3.0/edge-tile/edge-tile_48x48.png) | ![Opera](https://cdnjs.cloudflare.com/ajax/libs/browser-logos/39.3.0/opera/opera_48x48.png) | ![Safari](https://cdnjs.cloudflare.com/ajax/libs/browser-logos/39.3.0/safari/safari_48x48.png)  --- | --- | --- | --- | --- |  31+ ✔ | 38+ ✔ | 11+ ✔ | 31+ ✔ | 8+ ✔ | """
Big data;https://github.com/Netflix/PigPen;"""![](logo.png)    PigPen is map-reduce for Clojure, or distributed Clojure. It compiles to [Apache Pig](http://pig.apache.org/) or [Cascading](http://www.cascading.org/) but you don't need to know much about either of them to use it.    # Getting Started, Tutorials & Documentation    Getting started with Clojure and PigPen is really easy.      * The [wiki](https://github.com/Netflix/PigPen/wiki) explains what PigPen does and why we made it    * The [tutorial](https://github.com/Netflix/PigPen/wiki/Tutorial) is the best way to get Clojure and PigPen installed and start writing queries    * The [full API](http://netflix.github.io/PigPen/pigpen.core.html) lists all of the operators with example usage    * [PigPen for Clojure users](https://github.com/Netflix/PigPen/wiki/Getting_Started_for_Clojure_Users) is great for Clojure users new to map-reduce    * [PigPen for Pig users](https://github.com/Netflix/PigPen/wiki/Getting_Started_for_Pig_Users) is great for Pig users new to Clojure    * [PigPen for Cascading users](https://github.com/Netflix/PigPen/wiki/Getting_Started_for_Cascading_Users) is great for Cascading users new to Clojure    _Note: It is strongly recommended to familiarize yourself with Clojure before using PigPen._    _Note: PigPen is **not** a Clojure wrapper for writing Pig scripts you can hand edit. While entirely possible, the resulting scripts are not intended for human consumption._    # Questions & Complaints      * pigpen-support@googlegroups.com    * Group discussion archives can be accessed [here](https://groups.google.com/forum/#!forum/pigpen-support)    # Artifacts    `pigpen` is available from Maven:    With Leiningen:    ``` clj  ;; core library  [com.netflix.pigpen/pigpen ""0.3.3""]    ;; pig support  [com.netflix.pigpen/pigpen-pig ""0.3.3""]    ;; cascading support  [com.netflix.pigpen/pigpen-cascading ""0.3.3""]    ;; rx support  [com.netflix.pigpen/pigpen-rx ""0.3.3""]  ```    The platform libraries all reference the core library, so you only need to reference the platform specific one that you require and the core library should be included transitively.    _Note: PigPen requires Clojure 1.5.1 or greater_    ## Parquet    To use the parquet loader, add this to your dependencies:    ``` clj  [com.netflix.pigpen/pigpen-parquet-pig ""0.3.3""]  ```    Here an example of how to write parquet data.    ``` clj  (require '[pigpen.core :as pig])  (require '[pigpen.parquet :as pqt])    ;;  ;; assuming that `data` is in tuples  ;;  ;; [[""John"" ""Smith"" 28]  ;;  [""Jane"" ""Doe""   21]]    (defn save-to-parquet    [output-file data]    (->> data         ;; turning tuples into a map         (pig/map (partial zipmap [:firstname :lastname :age]))         ;; then storing to Parquet files         (pqt/store-parquet          output-file          (pqt/message ""test-schema""                       ;; the field names here MUST match the map's keys                       (pqt/binary ""firstname"")                       (pqt/binary ""lastname"")                       (pqt/int64  ""age"")))))  ```      And how to load the records back:    ``` clj  (defn load-from-parquet    [input-file]    ;; the output will be a sequence of maps    (pqt/load-parquet     input-file     (pqt/message ""test-schema""                  (pqt/binary ""firstname"")                  (pqt/binary ""lastname"")                  (pqt/int64  ""age""))))  ```      And check out the [`pigpen.parquet`](http://netflix.github.io/PigPen/pigpen.parquet.html) namespace for usage.    _Note: Parquet is currently only supported by Pig_    ## Avro    To use the avro loader (alpha), add this to your dependencies:    ``` clj  [com.netflix.pigpen/pigpen-avro-pig ""0.3.3""]  ```    And check out the [`pigpen.avro`](http://netflix.github.io/PigPen/pigpen.avro.html) namespace for usage.    _Note: Avro is currently only supported by Pig_    # Release Notes      * 0.3.3 - 5/19/16      * Explicitly disable `*print-length*` and `*print-level*` when generating scripts      * Add a better error message for storage types that expect a map with keywords    * 0.3.2 - 1/12/16      * Allow more types in generated pig scripts    * 0.3.1 - 10/19/15      * Update cascading version to 2.7.0      * Report correct pigpen version to concurrent      * Update nippy to 2.10.0 & tune performance    * 0.3.0 - 5/18/15      * No changes    * 0.3.0-rc-7 - 4/29/15      * Fixed bug in local mode where nils weren't handled consistently    * 0.3.0-rc.6 - 4/14/15      * Add local mode code eval memoization to avoid thrashing permgen      * Added [`pigpen.pig/set-options`](http://netflix.github.io/PigPen/pigpen.pig.html#var-set-options) command to explicitly set pig options in a script. This was previously available (though undocumented) by setting `{:pig-options {...}}` in any options block, but is now official.    * 0.3.0-rc.5 - 4/9/15      * Update core.async version    * 0.3.0-rc.4 - 4/8/15      * Memoize code evaluation when run in the cluster    * 0.3.0-rc.3 - 4/2/15      * Bugfixes    * 0.3.0-rc.2 - 3/30/15      * Parquet refactor. Local parquet loading no longer depends on Pig. Parquet schemas are now defined using Parquet classes.    * 0.3.0-rc.1 - 3/23/15      * Added Cascading support        * [`pigpen.cascading/generate-flow`](http://netflix.github.io/PigPen/pigpen.cascading.html#var-generate-flow) - Generate a cascading flow from a pigpen query        * [`pigpen.cascading/load-tap`](http://netflix.github.io/PigPen/pigpen.cascading.html#var-load-tap) - Load data from an existing cascading tap        * [`pigpen.cascading/store-tap`](http://netflix.github.io/PigPen/pigpen.cascading.html#var-store-tap) - Store data using an existing cascading tap      * Added [`pigpen.core/keys-fn`](http://netflix.github.io/PigPen/pigpen.core.html#var-keys-fn), a new convenience macro to support named anonymous functions. Like keys destructuring, but less verbose.      * New function based operators to build more dynamic scripts. These are function versions of all the core pigpen macros, but you have to handle quoting user code manually. These were previously available, but not officially supported. Now they're alpha, but supported and documented. See [`pigpen.core.fn`](http://netflix.github.io/PigPen/pigpen.core.fn.html)      * New lower-level operators to build custom storage and commands. These were previously available, but not officially supported. Now they're alpha, but supported and documented. See [`pigpen.core.op`](http://netflix.github.io/PigPen/pigpen.core.op.html)      * __*** Breaking Changes ***__        * `pigpen.core/script` is now [`pigpen.core/store-many`](http://netflix.github.io/PigPen/pigpen.core.html#var-store-many)        * `pigpen.core/generate-script` is now [`pigpen.pig/generate-script`](http://netflix.github.io/PigPen/pigpen.pig.html#var-generate-script)        * `pigpen.core/write-script` is now [`pigpen.pig/write-script`](http://netflix.github.io/PigPen/pigpen.pig.html#var-write-script)        * `pigpen.core/show` is now [`pigpen.viz/show`](http://netflix.github.io/PigPen/pigpen.viz.html#var-show) (requires dependency `[com.netflix.pigpen/pigpen-viz ""...""]`)        * `pig/dump` has changed. The old version was based on rx-java, and still exists as [`pigpen.rx/dump`](http://netflix.github.io/PigPen/pigpen.rx.html#var-dump). The replacement for [`pigpen.core/dump`](http://netflix.github.io/PigPen/pigpen.core.html#var-dump) is now entirely Clojure based. The Clojure version is better for unit tests and small data. All stages are evaluated eagerly, so the stack traces are simpler to read. The rx version is lazy, including the load-* commands. This means that you can load a large file, take a few rows, and process them without loading the entire file into memory. The downside is confusing stack traces and extra dependencies. See [here](https://github.com/Netflix/PigPen/wiki/Local_Evaluation) for more details.        * The interface for building custom loaders and storage has changed. See [here](https://github.com/Netflix/PigPen/wiki/Custom_Loaders) for more details. Please email pigpen-support@googlegroups.com with any questions.    * 0.2.15 - 2/20/15      * Include sources in jars    * 0.2.14 - 2/18/15      * Avro updates    * 0.2.13 - 1/19/15      * Added `load-avro` in the pigpen-avro project: http://avro.apache.org/      * Fixed the nRepl configuration; use `gradlew nRepl` to start an nRepl      * Exclude nested relations from closures    * 0.2.12 - 12/16/14      * Added `load-csv`, which allows for quoting per RFC 4180    * 0.2.11 - 10/24/14      * Fixed a bug (feature?) introduced by new rx version. Also upgraded to rc7. This would have only affected local mode where the data being read was faster than the code consuming it.    * 0.2.10 - 9/21/14      * Removed load-pig and store-pig. The pig data format is very bad and should not be used. If you used these and want them back, email pigpen-support@googlegroups.com and we'll put it into a separate jar. The jars required for this feature were causing conflicts elsewhere.      * Upgraded the following dependencies:        * org.clojure/clojure 1.5.1 -> 1.6.0 - this was also changed to a provided dependency, so you should be able to use any version greater than 1.5.1        * org.clojure/data.json 0.2.2 -> 0.2.5        * com.taoensso/nippy 2.6.0-RC1 -> 2.6.3        * clj-time 0.5.0 - no longer needed        * joda-time 2.2 -> 2.4 - pig needs this to run locally        * instaparse 1.2.14 - no longer needed        * io.reactivex/rxjava 0.9.2 -> 1.0.0-rc.1      * Fixed the rx limit bug. `pigpen.local/*max-load-records*` is no longer required.    * 0.2.9 - 9/16/14      * Fix a local-mode bug in `pigpen.fold/avg` where some collections would produce a NPE.      * Change fake pig delimiter to \n instead of \0. Allows for \0 to exist in input data.      * Remove 1000 record limit for local-mode. This was originally introduced to mitigate an rx bug. Until #61 is fixed, bind `pigpen.local/*max-load-records*` to the maximum number of records you want to read locally when reading large files. This now defaults to `nil` (no limit).      * Fix a local dispatch bug that would prevent loading folders locally    * 0.2.8 - 7/31/14      * Fix a bug in `load-tsv` and `load-lazy`    * 0.2.7 - 7/31/14 *** Don't use ***      * Fix `load-lazy` and speed up both `load-tsv` and `load-lazy`      * Convert to multi-project build      * Added pigpen-parquet with initial support for loading the Parquet format: https://github.com/apache/incubator-parquet-mr    * 0.2.6 - 6/17/14      * Minor optimization for local mode. The creation of a UDF was occurring for every value processed, causing it to run out of perm-gen space when processing large collections locally.      * Fix `(pig/return [])`      * Fix `(pig/dump (pig/reduce + (pig/return [])))`      * Fix `Long`s in scripts that are larger than an Integer      * Memoize local UDF instances per use of `pig/dump`      * The jar location in the generated script is now configurable. Use the `:pigpen-jar-location` option with `pig/generate-script` or `pig/write-script`.    * 0.2.5 - 4/9/14      * Remove `dump&show` and `dump&show+` in favor of `pigpen.oven/bake`. Call `bake` once and pass to as many outputs as you want. This is a breaking change, but I didn't increment the version because `dump&show` was just a tool to be used in the REPL. No scripts should break because of this change.      * Remove `dymp-async`. It appeared to be broken and was a bad idea from the start.      * Fix self-joins. This was a rare issue as a self join (with the same key) just duplicates data in a very expensive way.      * Clean up functional tests      * Fix `pigpen.oven/clean`. When it was pruning the graph, it was also removing REGISTER commands.    * 0.2.4 - 4/2/14      * Fix arity checking bug (affected varargs fns)      * Fix cases where an Algebraic fold function was falling back to the Accumulator interface, which was not supported. This affected using `cogroup` with `fold` over multiple relations.      * Fix debug mode (broken in 0.1.5)      * Change UDF initialization to not rely on memoization (caused stale data in REPL)      * Enable AOT. Improves cluster perf      * Add `:partition-by` option to `distinct`    * 0.2.3 - 3/27/14      * Added `load-json`, `store-json`, `load-string`, `store-string`      * Added `filter-by`, and `remove-by`    * 0.2.2 - 3/25/14      * Fixed bug in `pigpen.fold/vec`. This would also cause `fold/map` and `fold/filter` to not work when run in the cluster.    * 0.2.1 - 3/24/14      * Fixed bug when using `for` to generate scripts      * Fixed local mode bug with `map` followed by `reduce` or `fold`    * 0.2.0 - 3/3/14      * Added pigpen.fold - Note: this includes a breaking change in the join and cogroup syntax as follows:        ``` clj      ; before      (pig/join (foo on :f)                (bar on :b optional)                (fn [f b] ...))        ; after      (pig/join [(foo :on :f)                 (bar :on :b :type :optional)]                (fn [f b] ...))      ```        Each of the select clauses must now be wrapped in a vector - there is no longer a varargs overload to either of these forms. Within each of the select clauses, :on is now a keyword instead of a symbol, but a symbol will still work if used. If `optional` or `required` were used, they must be updated to `:type :optional` and `:type :required`, respectively.      * 0.1.5 - 2/17/14      * Performance improvements        * Implemented Pig's Accumulator interface        * Tuned nippy        * Reduced number of times data is serialized    * 0.1.4 - 1/31/14      * Fix sort bug in local mode    * 0.1.3 - 1/30/14      * Change Pig & Hadoop to be transitive dependencies      * Add support for consuming user code via closure    * 0.1.2 - 1/3/14      * Upgrade instaparse to 1.2.14    * 0.1.1 - 1/3/14      * Initial Release """
Big data;https://github.com/damballa/parkour;"""# Parkour    [![Build Status](https://secure.travis-ci.org/damballa/parkour.png)](http://travis-ci.org/damballa/parkour)    Hadoop MapReduce in idiomatic Clojure.  Parkour takes your Clojure code’s  functional gymnastics and sends it free-running across the urban environment of  your Hadoop cluster.    Parkour is a Clojure library for writing distributed programs in the MapReduce  pattern which run on the Hadoop MapReduce platform.  Parkour does its best to  avoid being yet another “framework” – if you know Hadoop, and you know Clojure,  then you’re most of the way to knowing Parkour.  By combining functional  programming, direct access to Hadoop features, and interactive iteration on live  data, Parkour supports rapid development of highly efficient Hadoop MapReduce  applications.    ## Installation    Parkour is available on Clojars.  Add this `:dependency` to your Leiningen  `project.clj`:    ```clj  [com.damballa/parkour ""0.6.3""]  ```    ## Usage    The [Parkour introduction][intro] contains an overview of the key concepts, but  here is the classic “word count” example, in Parkour:    ```clj  (defn word-count-m    [coll]    (->> coll         (r/mapcat #(str/split % #""\s+""))         (r/map #(-> [% 1]))))    (defn word-count    [conf lines]    (-> (pg/input lines)        (pg/map #'word-count-m)        (pg/partition [Text LongWritable])        (pg/combine #'ptb/keyvalgroups-r #'+)        (pg/output (seqf/dsink [Text LongWritable]))        (pg/fexecute conf `word-count)))  ```    ## Documentation    Parkour’s documentation is divided into a number of separate sections:    - [Introduction][intro] – A getting-started introduction, with an overview of    Parkour’s key concepts.  - [Motivation][motivation] – An explanation of the goals Parkour exists to    achieve, with comparison to other libraries and frameworks.  - [Namespaces][namespaces] – A tour of Parkour’s namespaces, explaining how each    set of functionality fits into the whole.  - [REPL integration][repl] – A quick guide to using Parkour from a    cluster-connected REPL, for iterative development against live data.  - [MapReduce in depth][mr-detailed] – An in-depth examination of the interfaces    Parkour uses to run your code in MapReduce jobs.  - [Serialization][serialization] – How Parkour integrates Clojure with Hadoop    serialization mechanisms.  - [Unified I/O][unified-io] – Unified collection-like local and distributed I/O    via Parkour dseqs and dsinks.  - [Distributed values][dvals] – Parkour’s value-oriented interface to the Hadoop    distributed cache.  - [Multiple I/O][multi-io] – Configuring multiple inputs and/or outputs for    single Hadoop MapReduce jobs.  - [Reducers vs seqs][reducers-vs-seqs] – Why Parkour’s default idiom uses    reducers, and when to use seqs instead.  - [Testing][testing] – Patterns for testing Parkour MapReduce jobs.  - [Deployment][deployment] – Running Parkour applications on a Hadoop cluster.  - [Reference][api] – Generated API reference, via [codox][codox].    ## Contributing    There is a [Parkour mailing list][mailing-list] hosted by  [Librelist](http://librelist.com/) for announcements and discussion.  To join  the mailing list, just email parkour@librelist.org; your first message to that  address will subscribe you without being posted.  Please report issues on the  [GitHub issue tracker][issues].  And of course, pull requests welcome!    ## License    Copyright © 2013-2015 Marshall Bockrath-Vandegrift & Damballa, Inc.    Distributed under the Apache License, Version 2.0.    [intro]: https://github.com/damballa/parkour/blob/master/doc/intro.md  [motivation]: https://github.com/damballa/parkour/blob/master/doc/motivation.md  [namespaces]: https://github.com/damballa/parkour/blob/master/doc/namespaces.md  [repl]: https://github.com/damballa/parkour/blob/master/doc/repl.md  [mr-detailed]: https://github.com/damballa/parkour/blob/master/doc/mr-detailed.md  [serialization]: https://github.com/damballa/parkour/blob/master/doc/serialization.md  [unified-io]: https://github.com/damballa/parkour/blob/master/doc/unified-io.md  [dvals]: https://github.com/damballa/parkour/blob/master/doc/dvals.md  [multi-io]: https://github.com/damballa/parkour/blob/master/doc/multi-io.md  [reducers-vs-seqs]: https://github.com/damballa/parkour/blob/master/doc/reducers-vs-seqs.md  [testing]: https://github.com/damballa/parkour/blob/master/doc/testing.md  [deployment]: https://github.com/damballa/parkour/blob/master/doc/deployment.md  [api]: http://damballa.github.io/parkour/  [codox]: https://github.com/weavejester/codox  [mailing-list]: http://librelist.com/browser/parkour/  [issues]: https://github.com/damballa/parkour/issues """
Big data;https://github.com/LucidWorks/banana;"""# Banana    The Banana project was forked from [Kibana](https://github.com/elastic/kibana), and works with all kinds of time series  (and non-time series) data stored in [Apache Solr](https://lucene.apache.org/solr/). It uses Kibana's powerful dashboard  configuration capabilities, ports key panels to work with Solr, and provides significant additional capabilities,  including new panels that leverage [D3.js](http://d3js.org).    The goal is to create a rich and flexible UI, enabling users to rapidly develop end-to-end applications that leverage  the power of Apache Solr. Data can be ingested into Solr through a variety of ways, including  [Logstash](https://www.elastic.co/products/logstash), [Flume](https://flume.apache.org) and other connectors.     ## IMPORTANT    Pull the repo from the `release` branch for production deployment; version x.y.z will be tagged as x.y.z    `develop` branch is used for active development and cutting edge features.  `fusion` branch is used for Lucidworks Fusion release. The code base and features are the same as `develop`. The main difference  is in the configuration.     ## Banana 1.6.26    This release includes the following bug fixes and improvement:    1. Enhance heatmap      * Add axis and axis labels      * Add axis grid and ticks      * Add gradient legend and ranges      * Fix heatmap transpose icon      * Enhance positioning and padding of panel elements      * Fix bettermap tooltip and hint text  1. Enhance hits panel      * Add panel horizontal and vertical direction option      * Fix metrics text and label overlap and margins  1. Fix bettermap render issue when resized  1. Fix jshint warnings    ## Older Release Notes    You can find all previous [Release Notes](https://github.com/LucidWorks/banana/wiki/Release-Notes) on our wiki page.    ## Installation and Quick Start  ### Requirements  * A modern web browser. The latest version of [Chrome](http://www.google.com/chrome/) and  [Firefox](https://www.mozilla.org/en-US/firefox/new/) have been tested to work. [Safari](http://www.apple.com/safari/)  also works, except for the ""Export to File"" feature for saving dashboards. We recommend that you use Chrome or Firefox  while building dashboards.  * Solr 6.x or at least 4.4+ (Solr server's endpoint must be open, or a proxy configured to allow access to it).  * A webserver (optional).    ### Installation Options  #### Option 1: Run Banana webapp within your existing Solr instance  ##### Solr 5+ Instructions  1. Run Solr at least once to create the webapp directory (this step might be unnecessary for Solr 6):            cd $SOLR_HOME/bin          ./solr start    2. Copy banana folder to `$SOLR_HOME/server/solr-webapp/webapp/`            cd $SOLR_HOME/server/solr-webapp/webapp          cp -R $BANANA_HOME/src ./banana        NOTES: For production, you should run `grunt build` command to generate the optimized code in `dist` directory. And then copy the `dist` directory to the production web server. For example:            cd $BANANA_HOME          npm install          bower install          grunt build          cp -R ./dist $SOLR_HOME/server/solr-webapp/webapp/banana    3. Browse to [http://localhost:8983/solr/banana/index.html](http://localhost:8983/solr/banana/index.html)    ##### Solr 4 Instructions  1. Run Solr at least once to create the webapp directories:            cd $SOLR_HOME/example          java -jar start.jar        2. Copy banana folder to $SOLR_HOME/example/solr-webapp/webapp/  3. Browse to [http://localhost:8983/solr/banana/src/index.html](http://localhost:8983/solr/banana/src/index.html)    _**NOTES:**_ If your Solr server/port is different from [localhost:8983](http://localhost:8983), edit  banana/src/config.js and banana/src/app/dashboards/default.json to enter the hostname and port that you are using.  Remember that banana runs within the client browser, so provide a fully qualified domain name (FQDN), because the  hostname and port number you provide should be resolvable from the client machines.    If you have not created the data collections and ingested data into Solr, you will see an error message saying  ""Collection not found at .."" You can use any connector to get data into Solr. If you want to use Logstash, please go to  the Solr Output Plug-in for [Logstash Page](https://github.com/LucidWorks/solrlogmanager) for code, documentation and  examples.    #### Option 2: Complete SLK Stack  Lucidworks has packaged Solr, Logstash (with a Solr Output Plug-in), and Banana (the Solr port of Kibana), along with  example collections and dashboards in order to rapidly enable proof-of-concepts and initial development/testing.  See [http://www.lucidworks.com/lucidworks-silk/](http://www.lucidworks.com/lucidworks-silk/).    #### Option 3: Building and installing from a WAR file  _NOTES: This option is only applicable to Solr 5 or 4. Solr 6 has a different architecture._  1. Pull the source code of Banana version that you want from the  [release](https://github.com/LucidWorks/banana/tree/release) branch in the repo;  For example, version *x.y.z* will be tagged as `x.y.z`.    2. Run a command line `ant` from within the banana directory to build the war file:        ```bash          cd $BANANA_HOME          ant      ```  3. The war file will be called *banana-\<buildnumber\>.war* and will be located in $BANANA_HOME/build.  Copy the war file and banana's jetty context file to Solr directories:    * For Solr 5:        ```bash          cp $BANANA_HOME/build/banana-<buildnumber>.war $SOLR_HOME/server/webapps/banana.war          cp $BANANA_HOME/jetty-contexts/banana-context.xml $SOLR_HOME/server/contexts/      ```    * For Solr 4:        ```bash          cp $BANANA_HOME/build/banana-<buildnumber>.war $SOLR_HOME/example/webapps/banana.war          cp $BANANA_HOME/jetty-contexts/banana-context.xml $SOLR_HOME/example/contexts/      ```  4. Run Solr:    * For Solr 5:        ```bash          cd $SOLR_HOME/bin/          ./solr start      ```    * For Solr 4:        ```bash          cd $SOLR_HOME/example/          java -jar start.jar      ```  5. Browse to [http://localhost:8983/banana](http://localhost:8983/banana) (or the FQDN of your Solr server).        #### Option 4: Run Banana webapp in a web server  Banana is an [AngularJS app](https://angularjs.org) and can be run in any webserver that has access to Solr.  You will need to enable [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) on the Solr instances that  you query, or configure a proxy that makes requests to banana and Solr as same-origin. We typically recommend the  latter approach.    ### Storing Dashboards in Solr  If you want to save and load dashboards from Solr, then you need to create a collection called `banana-int` first. For Solr 6, here are the steps:            cd $SOLR_HOME/bin          ./solr create -c banana-int    For Solr 5 and 4, you have to create the `banana-int` collection using the configuration files provided in either  the _resources/banana-int-solr-5.0_ (for Solr 5) directory or the _resources/banana-int-solr-4.5_ directory  (for Solr 4.5). If you are using SolrCloud, you will need to upload the configuration into  [ZooKeeper](https://zookeeper.apache.org) and then create the collection using that configuration.    The Solr server configured in config.js will serve as the default node for each dashboard; you can configure each  dashboard to point to a different Solr endpoint as long as your webserver and Solr put out the correct CORS headers.  See the README file under the  _resources/enable-cors_ directory for a guide.    ### Changes to your dashboards  If you created dashboards for Banana 1.0.0, you did not have a global filtering panel. In some cases, these filter  values can be implicitly set to defaults that may lead to strange search results. We recommend updating your old  dashboards by adding a filtering panel. A good way to do it visually is to put the filtering panel on its own row and  hide it when it is not needed.    ## FAQ    __Q__: How do I secure my Solr endpoint so that users do not have access to it?    __A__: The simplest solution is to use an [Apache](http://projects.apache.org/projects/http_server.html) or  [nginx](http://nginx.org) reverse proxy (See for example https://groups.google.com/forum/#!topic/ajax-solr/pLtYfm83I98).    __Q__: Can I use banana for non-time series data?    __A__: Yes, from version 1.3 onwards, non-time series data are also supported.    ## Resources    1.	Lucidworks SILK: http://www.lucidworks.com/lucidworks-silk/  2.	Webinar on Lucidworks SILK: http://programs.lucidworks.com/SiLK-introduction_Register.html.  3.	Logstash: http://logstash.net/  4.	SILK Use Cases: https://github.com/LucidWorks/silkusecases. Provides example configuration files, schemas and  dashboards required to build applications that use Solr and Banana.    ## Publishing WAR Artifacts to Maven Central    1. 	Get hold of  [maven-ant-tasks-X.X.X.jar](http://search.maven.org/#search|gav|1|g%3A%22org.apache.maven%22%20AND%20a%3A%22maven-ant-tasks%22)  and put it in this directory  2. 	Execute *ant -lib . deploy* from this directory, this will sign the Maven artifacts (currently just .war) and send  them to a [Sonatype OSSRH](https://oss.sonatype.org/) staging repository. Details of how to set this up can be found  [here](http://central.sonatype.org/pages/ossrh-guide.html). N.B. Ensure that you have an *release* profile contained  within ~/.m2/settings.xml  3.	Once you've read, and are happy with the staging repos, close it.     ## Support    Banana uses the dashboard configuration capabilities of Kibana (from which it is forked) and ports key panels to work  with Solr. Moreover, it provides many additional capabilities like heatmaps, range facets, panel specific filters,  global parameters, and visualization of ""group-by"" style queries. We are continuing to add many new panels that go well  beyond what is available in Kibana, helping users build complete applications that leverage the data stored in  Apache Solr, HDFS and a variety of sources in the enterprise.    If you have any questions, please email banana-support@lucidworks.com    ## Trademarks    Kibana is a trademark of Elasticsearch BV    Logstash is a trademark of Elasticsearch BV """
Big data;https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers;"""# Awesome Monte Carlo Tree Search Papers.  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)[![repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/awesome-monte-carlo-tree-search-papers.svg)](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers/archive/master.zip) ![License](https://img.shields.io/github/license/benedekrozemberczki/awesome-tree-search-papers.svg?color=blue) [![benedekrozemberczki](https://img.shields.io/twitter/follow/benrozemberczki?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=benrozemberczki)   <p align=""center"">    <img width=""600"" src=""tree.png"">  </p>    -----------------------------------------------    A curated list of Monte Carlo tree search papers with implementations from the following conferences/journals:    - Machine learning     * [NeurIPS](https://nips.cc/)     * [ICML](https://icml.cc/)  - Computer vision     * [CVPR](http://cvpr2019.thecvf.com/)     * [ICCV](http://iccv2019.thecvf.com/)  - Natural language processing     * [ACL](http://www.acl2019.org/EN/index.xhtml)  - Data     * [KDD](https://www.kdd.org/)  - Artificial intelligence     * [AAAI](https://www.aaai.org/)     * [AISTATS](https://www.aistats.org/)     * [IJCAI](https://www.ijcai.org/)     * [UAI](http://www.auai.org/)  - Robotics     * [RAS](https://www.journals.elsevier.com/robotics-and-autonomous-systems)  - Games     * [CIG](http://www.ieee-cig.org/)    Similar collections about [graph classification](https://github.com/benedekrozemberczki/awesome-graph-classification), [gradient boosting](https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers), [classification/regression trees](https://github.com/benedekrozemberczki/awesome-decision-tree-papers), [fraud detection](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers), and [community detection](https://github.com/benedekrozemberczki/awesome-community-detection) papers with implementations.    ## 2021    - **Learning to Stop: Dynamic Simulation Monte-Carlo Tree Search (AAAI 2021)**    - Li-Cheng Lan, Ti-Rong Wu, I-Chen Wu, Cho-Jui Hsieh    - [[Paper]](https://arxiv.org/abs/2012.07910)    - **Dec-SGTS: Decentralized Sub-Goal Tree Search for Multi-Agent Coordination (AAAI 2021)**    - Minglong Li, Zhongxuan Cai, Wenjing Yang, Lixia Wu, Yinghui Xu, Ji Wang    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17345)    - **Improved POMDP Tree Search Planning with Prioritized Action Branching (AAAI 2021)**    - John Mern, Anil Yildiz, Lawrence Bush, Tapan Mukerji, Mykel J. Kochenderfer    - [[Paper]](https://arxiv.org/abs/2010.03599)    - **Dynamic Automaton-Guided Reward Shaping for Monte Carlo Tree Search (AAAI 2021)**    - Alvaro Velasquez, Brett Bissey, Lior Barak, Andre Beckus, Ismail Alkhouri, Daniel Melcer, George K. Atia    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17427)    - **Single Player Monte-Carlo Tree Search Based on the Plackett-Luce Model (AAAI 2021)**    - Felix Mohr, Viktor Bengs, Eyke Hüllermeier    - [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17468)    - **Learning to Pack: A Data-Driven Tree Search Algorithm for Large-Scale 3D Bin Packing Problem (CIKM 2021)**    - Qianwen Zhu, Xihan Li, Zihan Zhang, Zhixing Luo, Xialiang Tong, Mingxuan Yuan, Jia Zeng    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3459637.3481933)    - **Practical Massively Parallel Monte-Carlo Tree Search Applied to Molecular Design (ICLR 2021)**    - Xiufeng Yang, Tanuj Kr Aasawat, Kazuki Yoshizoe    - [[Paper]](https://arxiv.org/abs/2006.10504)    - **Convex Regularization in Monte-Carlo Tree Search (ICML 2021)**    - Tuan Dam, Carlo D'Eramo, Jan Peters, Joni Pajarinen    - [[Paper]](https://arxiv.org/abs/2007.00391)    - **Combining Tree Search and Action Prediction for State-of-the-Art Performance in DouDiZhu (IJCAI 2021)**    - Yunsheng Zhang, Dong Yan, Bei Shi, Haobo Fu, Qiang Fu, Hang Su, Jun Zhu, Ning Chen    - [[Paper]](https://www.ijcai.org/proceedings/2021/470)    ## 2020    - **Monte Carlo Tree Search in Continuous Spaces Using Voronoi Optimistic Optimization with Regret Bounds (AAAI 2020)**    - Beomjoon Kim, Kyungjae Lee, Sungbin Lim, Leslie Pack Kaelbling, Tomás Lozano-Pérez    - [[Paper]](https://www.aaai.org/Papers/AAAI/2020GB/AAAI-KimB.1282.pdf)    - **Neural Architecture Search Using Deep Neural Networks and Monte Carlo Tree Search (AAAI 2020)**    - Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca    - [[Paper]](https://arxiv.org/abs/1805.07440)    - [[Code]](https://github.com/linnanwang/AlphaX-NASBench101)    - **Monte-Carlo Tree Search in Continuous Action Spaces with Value Gradients (AAAI 2020)**    - Jongmin Lee, Wonseok Jeon, Geon-Hyeong Kim, Kee-Eung Kim    - [[Paper]](https://www.ijcai.org/Proceedings/16/Papers/104.pdf)    - [[Code]](https://github.com/leekwoon/KR-DL-UCT)      - **Approximate Inference in Discrete Distributions with Monte Carlo Tree Search and Value Functions (AISTATS 2020)**    - Lars Buesing, Nicolas Heess, Theophane Weber    - [[Paper]](https://arxiv.org/abs/1910.06862)    - **Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search (ICLR 2020)**    - Anji Liu, Jianshu Chen, Mingze Yu, Yu Zhai, Xuewen Zhou, Ji Liu    - [[Paper]](https://openreview.net/forum?id=BJlQtJSKDB)    - [[Code]](https://github.com/brilee/python_uct)      - **Information Particle Filter Tree: An Online Algorithm for POMDPs with Belief-Based Rewards on Continuous Domains (ICML 2020)**    - Johannes Fischer, Ömer Sahin Tas    - [[Paper]](http://proceedings.mlr.press/v119/fischer20a.html)    - [[Code]](https://github.com/johannes-fischer/icml2020_ipft)    - **Sub-Goal Trees a Framework for Goal-Based Reinforcement Learning (ICML 2020)**    - Tom Jurgenson, Or Avner, Edward Groshev, Aviv Tamar    - [[Paper]](https://arxiv.org/abs/2002.12361)      - **Monte-Carlo Tree Search for Scalable Coalition Formation (IJCAI 2020)**    - Feng Wu, Sarvapali D. Ramchurn    - [[Paper]](https://www.ijcai.org/Proceedings/2020/57)    - **Generalized Mean Estimation in Monte-Carlo Tree Search (IJCAI 2020)**    - Tuan Dam, Pascal Klink, Carlo D'Eramo, Jan Peters, Joni Pajarinen    - [[Paper]](https://arxiv.org/abs/1911.00384)    - **Sparse Tree Search Optimality Guarantees in POMDPs with Continuous Observation Spaces (IJCAI 2020)**    - Michael H. Lim, Claire Tomlin, Zachary N. Sunberg    - [[Paper]](https://arxiv.org/abs/1910.04332)      - **Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions (NeurIPS 2020)**    - Matthew Faw, Rajat Sen, Karthikeyan Shanmugam, Constantine Caramanis, Sanjay Shakkottai    - [[Paper]](https://arxiv.org/abs/1907.10154)    - **Extracting Knowledge from Web Text with Monte Carlo Tree Search (WWW 2020)**    - Guiliang Liu, Xu Li, Jiakang Wang, Mingming Sun, Ping Li    - [[Paper]](https://dl.acm.org/doi/abs/10.1145/3366423.3380010)    ## 2019  - **ACE: An Actor Ensemble Algorithm for Continuous Control with Tree Search (AAAI 2019)**    - Shangtong Zhang, Hengshuai Yao    - [[Paper]](https://arxiv.org/abs/1811.02696)    - [[Code]](https://github.com/ShangtongZhang/DeepRL)    - **A Monte Carlo Tree Search Player for Birds of a Feather Solitaire (AAAI 2019)**    - Christian Roberson, Katarina Sperduto    - [[Paper]](https://aaai.org/ojs/index.php/AAAI/article/view/5036)    - [[Code]](http://cs.gettysburg.edu/~tneller/puzzles/boaf/)    - **Vine Copula Structure Learning via Monte Carlo Tree Search (AISTATS 2019)**    - Bo Chang, Shenyi Pan, Harry Joe    - [[Paper]](http://proceedings.mlr.press/v89/chang19a/chang19a.pdf)    - [[Code]](https://github.com/changebo/Vine_MCTS)    - **Noisy Blackbox Optimization using Multi-fidelity Queries: A Tree Search Approach (AISTATS 2019)**    - Rajat Sen, Kirthevasan Kandasamy, Sanjay Shakkottai    - [[Paper]](https://arxiv.org/abs/1810.10482)    - [[Code]](https://github.com/rajatsen91/MFTREE_DET)    - **Reinforcement Learning Based Monte Carlo Tree Search for Temporal Path Discovery (ICDM 2019)**    - Pengfei Ding, Guanfeng Liu, Pengpeng Zhao, An Liu, Zhixu Li, Kai Zheng    - [[Paper]](https://zheng-kai.com/paper/icdm_2019_b.pdf)    - **Monte Carlo Tree Search for Policy Optimization (IJCAI 2019)**    - Xiaobai Ma, Katherine Rose Driggs-Campbell, Zongzhang Zhang, Mykel J. Kochenderfer    - [[Paper]](https://www.ijcai.org/proceedings/2019/0432.pdf)    - **Subgoal-Based Temporal Abstraction in Monte-Carlo Tree Search (IJCAI 2019)**    - Thomas Gabor, Jan Peter, Thomy Phan, Christian Meyer, Claudia Linnhoff-Popien    - [[Paper]](https://www.ijcai.org/proceedings/2019/0772.pdf)    - [[Code]](https://github.com/jnptr/subgoal-mcts)    - **Automated Machine Learning with Monte-Carlo Tree Search (IJCAI 2019)**    - Herilalaina Rakotoarison, Marc Schoenauer, Michèle Sebag    - [[Paper]](https://www.ijcai.org/proceedings/2019/0457.pdf)    - [[Code]](https://github.com/herilalaina/mosaic_ml)    - **Multiple Policy Value Monte Carlo Tree Search (IJCAI 2019)**    - Li-Cheng Lan, Wei Li, Ting-Han Wei, I-Chen Wu    - [[Paper]](https://www.ijcai.org/proceedings/2019/0653.pdf)    - **Learning Compositional Neural Programs with Recursive Tree Search and Planning (NeurIPS 2019)**    - Thomas Pierrot, Guillaume Ligner, Scott E. Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, Nando de Freitas    - [[Paper]](https://arxiv.org/abs/1905.12941)    ## 2018  - **Monte Carlo Methods for the Game Kingdomino (CIG 2018)**    - Magnus Gedda, Mikael Z. Lagerkvist, Martin Butler    - [[Paper]](https://arxiv.org/abs/1807.04458)    - [[Code]](https://github.com/mgedda/kdom-ai)    - [[Game Server]](https://github.com/mratin/kdom)    - **Reset-free Trial-and-Error Learning for Robot Damage Recovery (RAS 2018)**    - Konstantinos Chatzilygeroudis, Vassilis Vassiliades, Jean-Baptiste Mouret    - [[Paper]](https://arxiv.org/pdf/1610.04213.pdf)    - [[Code]](https://github.com/resibots/chatzilygeroudis_2018_rte)    - [[MCTS C++ Library]](https://github.com/resibots/mcts)    - **Memory-Augmented Monte Carlo Tree Search (AAAI 2018)**    - Chenjun Xiao, Jincheng Mei, Martin Müller    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17139)    - **Feedback-Based Tree Search for Reinforcement Learning (ICML 2018)**    - Daniel R. Jiang, Emmanuel Ekwedike, Han Liu    - [[Paper]](https://arxiv.org/abs/1805.05935)    - **Extended Increasing Cost Tree Search for Non-Unit Cost Domains (IJCAI 2018)**    - Thayne T. Walker, Nathan R. Sturtevant, Ariel Felner    - [[Paper]](https://www.ijcai.org/proceedings/2018/74)    - **Three-Head Neural Network Architecture for Monte Carlo Tree Search (IJCAI 2018)**    - Chao Gao, Martin Müller, Ryan Hayward    - [[Paper]](https://www.ijcai.org/proceedings/2018/523)    - **Bidding in Periodic Double Auctions Using Heuristics and Dynamic Monte Carlo Tree Search (IJCAI 2018)**    - Moinul Morshed Porag Chowdhury, Christopher Kiekintveld, Son Tran, William Yeoh    - [[Paper]](https://www.ijcai.org/proceedings/2018/23)    - **Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search (NIPS 2018)**    - Zhuwen Li, Qifeng Chen, Vladlen Koltun    - [[Paper]](https://arxiv.org/abs/1810.10659)    - **M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search (NIPS 2018)**    - Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing Guo, Jianfeng Gao    - [[Paper]](https://arxiv.org/abs/1802.04394)    - **Single-Agent Policy Tree Search With Guarantees (NIPS 2018)**    - Laurent Orseau, Levi Lelis, Tor Lattimore, Theophane Weber    - [[Paper]](https://arxiv.org/abs/1811.10928)    - **Monte-Carlo Tree Search for Constrained POMDPs (NIPS 2018)**    - Jongmin Lee, Geon-hyeong Kim, Pascal Poupart, Kee-Eung Kim    - [[Paper]](https://cs.uwaterloo.ca/~ppoupart/publications/constrained-pomdps/mcts-constrained-pomdps-paper.pdf)    ## 2017  - **An Analysis of Monte Carlo Tree Search (AAAI 2017)**    - Steven James, George Dimitri Konidaris, Benjamin Rosman    - [[Paper]](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14886)    - **Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural Network and Long-Term Evaluation (AAAI 2017)**    - Jinzhuo Wang, Wenmin Wang, Ronggang Wang, Wen Gao    - [[Paper]](https://arxiv.org/abs/1706.04052)    - **Designing Better Playlists with Monte Carlo Tree Search (AAAI 2017)**    - Elad Liebman, Piyush Khandelwal, Maytal Saar-Tsechansky, Peter Stone    - [[Paper]](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IAAI2017-eladlieb.pdf)    - **Learning in POMDPs with Monte Carlo Tree Search (ICML 2017)**    - Sammie Katt, Frans A. Oliehoek, Christopher Amato    - [[Paper]](https://arxiv.org/abs/1806.05631)    - **Learning to Run Heuristics in Tree Search (IJCAI 2017)**    - Elias B. Khalil, Bistra Dilkina, George L. Nemhauser, Shabbir Ahmed, Yufen Shao    - [[Paper]](https://www.ijcai.org/proceedings/2017/92)    - **Estimating the Size of Search Trees by Sampling with Domain Knowledge (IJCAI 2017)**    - Gleb Belov, Samuel Esler, Dylan Fernando, Pierre Le Bodic, George L. Nemhauser    - [[Paper]](https://www.ijcai.org/proceedings/2017/67)    - **A Monte Carlo Tree Search Approach to Active Malware Analysis (IJCAI 2017)**    - Riccardo Sartea, Alessandro Farinelli    - [[Paper]](https://www.ijcai.org/proceedings/2017/535)    - **Monte-Carlo Tree Search by Best Arm Identification (NIPS 2017)**    - Emilie Kaufmann, Wouter M. Koolen    - [[Paper]](https://arxiv.org/abs/1706.02986)    - **Thinking Fast and Slow with Deep Learning and Tree Search (NIPS 2017)**    - Thomas Anthony, Zheng Tian, David Barber    - [[Paper]](https://arxiv.org/abs/1705.08439)    - **Monte-Carlo Tree Search using Batch Value of Perfect Information (UAI 2017)**    - Shahaf S. Shperberg, Solomon Eyal Shimony, Ariel Felner    - [[Paper]](http://auai.org/uai2017/proceedings/papers/37.pdf)    ## 2016  - **Using Domain Knowledge to Improve Monte-Carlo Tree Search Performance in Parameterized Poker Squares (AAAI 2016)**    - Robert Arrington, Clay Langley, Steven Bogaerts    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11809)    - **Monte Carlo Tree Search for Multi-Robot Task Allocation (AAAI 2016)**    - Bilal Kartal, Ernesto Nunes, Julio Godoy, Maria L. Gini    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12154)    - **Large Scale Hard Sample Mining with Monte Carlo Tree Search (CVPR 2016)**    - Olivier Canévet, François Fleuret    - [[Paper]](https://www.idiap.ch/~fleuret/papers/canevet-fleuret-cvpr2016.pdf)    - **On the Analysis of Complex Backup Strategies in Monte Carlo Tree Search (ICML 2016)**    - Piyush Khandelwal, Elad Liebman, Scott Niekum, Peter Stone    - [[Paper]](https://www.cs.utexas.edu/~eladlieb/ICML2016.pdf)    - **Deep Learning for Reward Design to Improve Monte Carlo Tree Search in ATARI Games (IJCAI 2016)**    - Xiaoxiao Guo, Satinder P. Singh, Richard L. Lewis, Honglak Lee    - [[Paper]](https://arxiv.org/abs/1604.07095)    - **Monte Carlo Tree Search in Continuous Action Spaces with Execution Uncertainty (IJCAI 2016)**    - Timothy Yee, Viliam Lisý, Michael H. Bowling    - [[Paper]](https://www.ijcai.org/Proceedings/16/Papers/104.pdf)    - **Learning Predictive State Representations via Monte-Carlo Tree Search (IJCAI 2016)**    - Yunlong Liu, Hexing Zhu, Yifeng Zeng, Zongxiong Dai    - [[Paper]](https://pdfs.semanticscholar.org/8056/df11094fc96d76826403f8b339dc14aa821f.pdf)    ## 2015  - **Efficient Globally Optimal Consensus Maximisation with Tree Search (CVPR 2015)**    - Tat-Jun Chin, Pulak Purkait, Anders P. Eriksson, David Suter    - [[Paper]](https://zpascal.net/cvpr2015/Chin_Efficient_Globally_Optimal_2015_CVPR_paper.pdf)    - **Interplanetary Trajectory Planning with Monte Carlo Tree Search (IJCAI 2015)**    - Daniel Hennes, Dario Izzo    - [[Paper]](https://pdfs.semanticscholar.org/ce42/53ca1c5b16e96cdbefae75649cd2588f42f3.pdf)    ## 2014  - **State Aggregation in Monte Carlo Tree Search (AAAI 2014)**    - Jesse Hostetler, Alan Fern, Tom Dietterich    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/download/8439/8712)    - **Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning (NIPS 2014)**    - Xiaoxiao Guo, Satinder P. Singh, Honglak Lee, Richard L. Lewis, Xiaoshi Wang    - [[Paper]](https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf)    - **Learning Partial Policies to Speedup MDP Tree Search (UAI 2014)**    - Jervis Pinto, Alan Fern    - [[Paper]](http://www.jmlr.org/papers/volume18/15-251/15-251.pdf)    ## 2013  - **Monte Carlo Tree Search for Scheduling Activity Recognition (ICCV 2013)**    - Mohamed R. Amer, Sinisa Todorovic, Alan Fern, Song-Chun Zhu    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.405.5916&rep=rep1&type=pdf)    - **Convergence of Monte Carlo Tree Search in Simultaneous Move Games (NIPS 2013)**    - Viliam Lisý, Vojtech Kovarík, Marc Lanctot, Branislav Bosanský    - [[Paper]](https://papers.nips.cc/paper/5145-convergence-of-monte-carlo-tree-search-in-simultaneous-move-games)    - **Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search (NIPS 2013)**    - Aijun Bai, Feng Wu, Xiaoping Chen    - [[Paper]](https://papers.nips.cc/paper/5111-bayesian-mixture-modelling-and-inference-based-thompson-sampling-in-monte-carlo-tree-search)    ## 2012  - **Generalized Monte-Carlo Tree Search Extensions for General Game Playing (AAAI 2012)**    - Hilmar Finnsson    - [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewFile/4935/5300)    ## 2011  - **A Local Monte Carlo Tree Search Approach in Deterministic Planning (AAAI 2011)**    - Fan Xie, Hootan Nakhost, Martin Müller    - [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.699.3833&rep=rep1&type=pdf)    - **Real-Time Solving of Quantified CSPs Based on Monte-Carlo Game Tree Search (IJCAI 2011)**    - Satomi Baba, Yongjoon Joe, Atsushi Iwasaki, Makoto Yokoo    - [[Paper]](https://www.ijcai.org/Proceedings/11/Papers/116.pdf)    - **Nested Rollout Policy Adaptation for Monte Carlo Tree Search (IJCAI 2011)**    - Christopher D. Rosin    - [[Paper]](https://www.ijcai.org/Proceedings/11/Papers/115.pdf)    - **Variance Reduction in Monte-Carlo Tree Search (NIPS 2011)**    - Joel Veness, Marc Lanctot, Michael H. Bowling    - [[Paper]](https://papers.nips.cc/paper/4288-variance-reduction-in-monte-carlo-tree-search)    - **Learning Is Planning: Near Bayes-Optimal Reinforcement Learning via Monte-Carlo Tree Search (UAI 2011)**    - John Asmuth, Michael L. Littman    - [[Paper]](https://arxiv.org/abs/1202.3699)    ## 2010  - **Understanding the Success of Perfect Information Monte Carlo Sampling in Game Tree Search (AAAI 2010)**    - Jeffrey Richard Long, Nathan R. Sturtevant, Michael Buro, Timothy Furtak    - [[Paper]](https://pdfs.semanticscholar.org/011e/2c79575721764c127e210c9d8105a6305e70.pdf)    - **Bayesian Inference in Monte-Carlo Tree Search (UAI 2010)**    - Gerald Tesauro, V. T. Rajan, Richard Segal    - [[Paper]](https://arxiv.org/abs/1203.3519)    ## 2009  - **Monte Carlo Tree Search Techniques in the Game of Kriegspiel (IJCAI 2009)**    - Paolo Ciancarini, Gian Piero Favini    - [[Paper]](https://www.aaai.org/ocs/index.php/IJCAI/IJCAI-09/paper/viewFile/396/693)    - **Bootstrapping from Game Tree Search (NIPS 2009)**    - Joel Veness, David Silver, William T. B. Uther, Alan Blair    - [[Paper]](https://papers.nips.cc/paper/3722-bootstrapping-from-game-tree-search)    ## 2008  - **Direct Mining of Discriminative and Essential Frequent Patterns via Model-Based Search Tree (KDD 2008)**    - Wei Fan, Kun Zhang, Hong Cheng, Jing Gao, Xifeng Yan, Jiawei Han, Philip S. Yu, Olivier Verscheure    - [[Paper]](http://www1.se.cuhk.edu.hk/~hcheng/paper/kdd08mbt.pdf)    ## 2007  - **Bandit Algorithms for Tree Search (UAI 2007)**    - Pierre-Arnaud Coquelin, Rémi Munos    - [[Paper]](https://arxiv.org/pdf/1408.2028.pdf)    ## 2006  - **Properties of Forward Pruning in Game-Tree Search (AAAI 2006)**    - Yew Jin Lim, Wee Sun Lee    - [[Paper]](https://dl.acm.org/citation.cfm?id=1597351)    - **Graph Branch Algorithm: An Optimum Tree Search Method for Scored Dependency Graph with Arc Co-Occurrence Constraints (ACL 2006)**    - Hideki Hirakawa    - [[Paper]](https://www.aclweb.org/anthology/P06-2047/)    ## 2005  - **Game-Tree Search with Combinatorially Large Belief States (IJCAI 2005)**    - Austin Parker, Dana S. Nau, V. S. Subrahmanian    - [[Paper]](https://www.ijcai.org/Proceedings/05/Papers/0878.pdf)    ## 2003  - **Solving Finite Domain Constraint Hierarchies by Local Consistency and Tree Search (IJCAI 2003)**    - Stefano Bistarelli, Philippe Codognet, Kin Chuen Hui, Jimmy Ho-Man Lee    - [[Paper]](https://www.ijcai.org/Proceedings/03/Papers/200.pdf)    ## 2001  - **Incomplete Tree Search using Adaptive Probing (IJCAI 2001)**    - Wheeler Ruml    - [[Paper]](https://dash.harvard.edu/bitstream/handle/1/23017275/tr-02-01.pdf?sequence%3D1)    ## 1998  - **KnightCap: A Chess Programm That Learns by Combining TD with Game-Tree Search (ICML 1998)**    - Jonathan Baxter, Andrew Tridgell, Lex Weaver    - [[Paper]](https://arxiv.org/abs/cs/9901002)    ## 1988  - **A Tree Search Algorithm for Target Detection in Image Sequences (CVPR 1988)**    - Steven D. Blostein, Thomas S. Huang    - [[Paper]](https://ieeexplore.ieee.org/document/196309)    --------------------------------------------------------------------------------    **License**    - [CC0 Universal](https://github.com/benedekrozemberczki/awesome-monte-carlo-tree-search-papers/blob/master/LICENSE)    -------------------------------------------------------------------------------- """
Big data;https://github.com/tidwall/tile38;"""<p align=""center"">    <a href=""https://tile38.com""><img       src=""/.github/images/logo-light.svg""       width=""284"" border=""0"" alt=""Tile38""></a>  </p>  <p align=""center"">  <a href=""https://tile38.com/slack/""><img src=""https://img.shields.io/badge/slack-channel-orange.svg"" alt=""Slack Channel""></a>  <a href=""https://hub.docker.com/r/tile38/tile38""><img src=""https://img.shields.io/badge/docker-ready-blue.svg"" alt=""Docker Ready""></a>  </p>    Tile38 is an open source (MIT licensed), in-memory geolocation data store, spatial index, and realtime geofence. It supports a variety of object types including lat/lon points, bounding boxes, XYZ tiles, Geohashes, and GeoJSON.     <p align=""center"">  <i>This README is quick start document. You can find detailed documentation at <a href=""https://tile38.com"">https://tile38.com</a>.</i><br><br>  <a href=""#searching""><img src=""/.github/images/search-nearby.png"" alt=""Nearby"" border=""0"" width=""120"" height=""120""></a>  <a href=""#searching""><img src=""/.github/images/search-within.png"" alt=""Within"" border=""0"" width=""120"" height=""120""></a>  <a href=""#searching""><img src=""/.github/images/search-intersects.png"" alt=""Intersects"" border=""0"" width=""120"" height=""120""></a>  <a href=""https://tile38.com/topics/geofencing""><img src=""/.github/images/geofence.gif"" alt=""Geofencing"" border=""0"" width=""120"" height=""120""></a>  <a href=""https://tile38.com/topics/roaming-geofences""><img src=""/.github/images/roaming.gif"" alt=""Roaming Geofences"" border=""0"" width=""120"" height=""120""></a>  </p>    ## Features    - Spatial index with [search](#searching) methods such as Nearby, Within, and Intersects.  - Realtime [geofencing](#geofencing) through [webhooks](https://tile38.com/commands/sethook) or [pub/sub channels](#pubsub-channels).  - Object types of [lat/lon](#latlon-point), [bbox](#bounding-box), [Geohash](#geohash), [GeoJSON](#geojson), [QuadKey](#quadkey), and [XYZ tile](#xyz-tile).  - Support for lots of [Clients Libraries](#tile38-client-libraries) written in many different languages.  - Variety of protocols, including [http](#http) (curl), [websockets](#websockets), [telnet](#telnet), and the [Redis RESP](https://redis.io/topics/protocol).  - Server responses are [RESP](https://redis.io/topics/protocol) or [JSON](https://www.json.org).  - Full [command line interface](#cli).  - Leader / follower [replication](#replication).  - In-memory database that persists on disk.    ## Components  - `tile38-server    ` - The server  - `tile38-cli       ` - Command line interface tool  - `tile38-benchmark ` - Server benchmark tool    ## Getting Started    ### Getting Tile38    Perhaps the easiest way to get the latest Tile38 is to use one of the pre-built release binaries which are available for OSX, Linux, FreeBSD, and Windows. Instructions for using these binaries are on the GitHub [releases page](https://github.com/tidwall/tile38/releases).    ### Docker     To run the latest stable version of Tile38:    ```  docker pull tile38/tile38  docker run -p 9851:9851 tile38/tile38  ```    Visit the [Tile38 hub page](https://hub.docker.com/r/tile38/tile38/) for more information.    ### Homebrew (macOS)    Install Tile38 using [Homebrew](https://brew.sh/)    ```sh  brew install tile38  tile38-server  ```    ### Building Tile38     Tile38 can be compiled and used on Linux, OSX, Windows, FreeBSD, and probably others since the codebase is 100% Go. We support both 32 bit and 64 bit systems. [Go](https://golang.org/dl/) must be installed on the build machine.    To build everything simply:  ```  $ make  ```    To test:  ```  $ make test  ```    ### Running   For command line options invoke:  ```  $ ./tile38-server -h  ```    To run a single server:    ```  $ ./tile38-server    # The tile38 shell connects to localhost:9851  $ ./tile38-cli  > help  ```    #### Prometheus Metrics  Tile38 can natively export Prometheus metrics by setting the `--metrics-addr` command line flag (disabled by default). This example exposes the HTTP metrics server on port 4321:  ```  # start server and enable Prometheus metrics, listen on local interface only  ./tile38-server --metrics-addr=127.0.0.1:4321    # access metrics  curl http://127.0.0.1:4321/metrics  ```  If you need to access the `/metrics` endpoint from a different host you'll have to set the flag accordingly, e.g. set it to `0.0.0.0:<<port>>` to listen on all interfaces.    Use the [redis_exporter](https://github.com/oliver006/redis_exporter) for more advanced use cases like extracting key values or running a lua script.      ## <a name=""cli""></a>Playing with Tile38    Basic operations:  ```  $ ./tile38-cli    # add a couple of points named 'truck1' and 'truck2' to a collection named 'fleet'.  > set fleet truck1 point 33.5123 -112.2693   # on the Loop 101 in Phoenix  > set fleet truck2 point 33.4626 -112.1695   # on the I-10 in Phoenix    # search the 'fleet' collection.  > scan fleet                                 # returns both trucks in 'fleet'  > nearby fleet point 33.462 -112.268 6000    # search 6 kilometers around a point. returns one truck.    # key value operations  > get fleet truck1                           # returns 'truck1'  > del fleet truck2                           # deletes 'truck2'  > drop fleet                                 # removes all   ```    Tile38 has a ton of [great commands](https://tile38.com/commands).    ## Fields  Fields are extra data that belongs to an object. A field is always a double precision floating point. There is no limit to the number of fields that an object can have.     To set a field when setting an object:  ```  > set fleet truck1 field speed 90 point 33.5123 -112.2693               > set fleet truck1 field speed 90 field age 21 point 33.5123 -112.2693  ```    To set a field when an object already exists:  ```  > fset fleet truck1 speed 90  ```    ## Searching    Tile38 has support to search for objects and points that are within or intersects other objects. All object types can be searched including Polygons, MultiPolygons, GeometryCollections, etc.    <img src=""/.github/images/search-within.png"" width=""200"" height=""200"" border=""0"" alt=""Search Within"" align=""left"">    #### Within   WITHIN searches a collection for objects that are fully contained inside a specified bounding area.  <BR CLEAR=""ALL"">    <img src=""/.github/images/search-intersects.png"" width=""200"" height=""200"" border=""0"" alt=""Search Intersects"" align=""left"">    #### Intersects  INTERSECTS searches a collection for objects that intersect a specified bounding area.  <BR CLEAR=""ALL"">    <img src=""/.github/images/search-nearby.png"" width=""200"" height=""200"" border=""0"" alt=""Search Nearby"" align=""left"">    #### Nearby  NEARBY searches a collection for objects that intersect a specified radius.  <BR CLEAR=""ALL"">    ### Search options  **WHERE** - This option allows for filtering out results based on [field](#fields) values. For example<br>```nearby fleet where speed 70 +inf point 33.462 -112.268 6000``` will return only the objects in the 'fleet' collection that are within the 6 km radius **and** have a field named `speed` that is greater than `70`. <br><br>Multiple WHEREs are concatenated as **and** clauses. ```WHERE speed 70 +inf WHERE age -inf 24``` would be interpreted as *speed is over 70 <b>and</b> age is less than 24.*<br><br>The default value for a field is always `0`. Thus if you do a WHERE on the field `speed` and an object does not have that field set, the server will pretend that the object does and that the value is Zero.    **MATCH** - MATCH is similar to WHERE except that it works on the object id instead of fields.<br>```nearby fleet match truck* point 33.462 -112.268 6000``` will return only the objects in the 'fleet' collection that are within the 6 km radius **and** have an object id that starts with `truck`. There can be multiple MATCH options in a single search. The MATCH value is a simple [glob pattern](https://en.wikipedia.org/wiki/Glob_(programming)).    **CURSOR** - CURSOR is used to iterate though many objects from the search results. An iteration begins when the CURSOR is set to Zero or not included with the request, and completes when the cursor returned by the server is Zero.    **NOFIELDS** - NOFIELDS tells the server that you do not want field values returned with the search results.    **LIMIT** - LIMIT can be used to limit the number of objects returned for a single search request.      ## Geofencing    <img src=""/.github/images/geofence.gif"" width=""200"" height=""200"" border=""0"" alt=""Geofence animation"" align=""left"">  A <a href=""https://en.wikipedia.org/wiki/Geo-fence"">geofence</a> is a virtual boundary that can detect when an object enters or exits the area. This boundary can be a radius, bounding box, or a polygon. Tile38 can turn any standard search into a geofence monitor by adding the FENCE keyword to the search.     *Tile38 also allows for [Webhooks](https://tile38.com/commands/sethook) to be assigned to Geofences.*    <br clear=""all"">    A simple example:  ```  > nearby fleet fence point 33.462 -112.268 6000  ```  This command opens a geofence that monitors the 'fleet' collection. The server will respond with:  ```  {""ok"":true,""live"":true}  ```  And the connection will be kept open. If any object enters or exits the 6 km radius around `33.462,-112.268` the server will respond in realtime with a message such as:    ```  {""command"":""set"",""detect"":""enter"",""id"":""truck02"",""object"":{""type"":""Point"",""coordinates"":[-112.2695,33.4626]}}  ```    The server will notify the client if the `command` is `del | set | drop`.     - `del` notifies the client that an object has been deleted from the collection that is being fenced.  - `drop` notifies the client that the entire collection is dropped.  - `set` notifies the client that an object has been added or updated, and when it's position is detected by the fence.    The `detect` may be one of the following values.    - `inside` is when an object is inside the specified area.  - `outside` is when an object is outside the specified area.  - `enter` is when an object that **was not** previously in the fence has entered the area.  - `exit` is when an object that **was** previously in the fence has exited the area.  - `cross` is when an object that **was not** previously in the fence has entered **and** exited the area.    These can be used when establishing a geofence, to pre-filter responses. For instance, to limit responses to `enter` and `exit` detections:    ```  > nearby fleet fence detect enter,exit point 33.462 -112.268 6000  ```    ### Pub/sub channels    Tile38 supports delivering geofence notications over pub/sub channels.     To create a static geofence that sends notifications when a bus is within 200 meters of a point and sends to the `busstop` channel:    ```  > setchan busstop nearby buses fence point 33.5123 -112.2693 200  ```    Subscribe on the `busstop` channel:    ```  > subscribe busstop  ```    ## Object types    All object types except for XYZ Tiles and QuadKeys can be stored in a collection. XYZ Tiles and QuadKeys are reserved for the SEARCH keyword only.    #### Lat/lon point  The most basic object type is a point that is composed of a latitude and a longitude. There is an optional `z` member that may be used for auxiliary data such as elevation or a timestamp.  ```  set fleet truck1 point 33.5123 -112.2693     # plain lat/lon  set fleet truck1 point 33.5123 -112.2693 225 # lat/lon with z member  ```    #### Bounding box  A bounding box consists of two points. The first being the southwestern most point and the second is the northeastern most point.  ```  set fleet truck1 bounds 30 -110 40 -100  ```  #### Geohash  A [geohash](https://en.wikipedia.org/wiki/Geohash) is a string representation of a point. With the length of the string indicating the precision of the point.   ```  set fleet truck1 hash 9tbnthxzr # this would be equivalent to 'point 33.5123 -112.2693'  ```    #### GeoJSON  [GeoJSON](https://tools.ietf.org/html/rfc7946) is an industry standard format for representing a variety of object types including a point, multipoint, linestring, multilinestring, polygon, multipolygon, geometrycollection, feature, and featurecollection.    <i>* All ignored members will not persist.</i>    **Important to note that all coordinates are in Longitude, Latitude order.**    ```  set city tempe object {""type"":""Polygon"",""coordinates"":[[[0,0],[10,10],[10,0],[0,0]]]}  ```    #### XYZ Tile  An XYZ tile is rectangle bounding area on earth that is represented by an X, Y coordinate and a Z (zoom) level.  Check out [maptiler.org](http://www.maptiler.org/google-maps-coordinates-tile-bounds-projection/) for an interactive example.    #### QuadKey  A QuadKey used the same coordinate system as an XYZ tile except that the string representation is a string characters composed of 0, 1, 2, or 3. For a detailed explanation checkout [The Bing Maps Tile System](https://msdn.microsoft.com/en-us/library/bb259689.aspx).    ## Network protocols    It's recommended to use a [client library](#tile38-client-libraries) or the [Tile38 CLI](#running), but there are times when only HTTP is available or when you need to test from a remote terminal. In those cases we provide an HTTP and telnet options.    #### HTTP  One of the simplest ways to call a tile38 command is to use HTTP. From the command line you can use [curl](https://curl.haxx.se/). For example:    ```  # call with request in the body  curl --data ""set fleet truck3 point 33.4762 -112.10923"" localhost:9851    # call with request in the url path  curl localhost:9851/set+fleet+truck3+point+33.4762+-112.10923  ```    #### Websockets  Websockets can be used when you need to Geofence and keep the connection alive. It works just like the HTTP example above, with the exception that the connection stays alive and the data is sent from the server as text websocket messages.    #### Telnet  There is the option to use a plain telnet connection. The default output through telnet is [RESP](https://redis.io/topics/protocol).    ```  telnet localhost 9851  set fleet truck3 point 33.4762 -112.10923  +OK    ```    The server will respond in [JSON](https://json.org) or [RESP](https://redis.io/topics/protocol) depending on which protocol is used when initiating the first command.    - HTTP and Websockets use JSON.   - Telnet and RESP clients use RESP.    ## Tile38 Client Libraries    The following clients are built specifically for Tile38.    Clients that support most Tile38 features are marked with a ⭐️.    - ⭐️ Go: [xjem/t38c](https://github.com/xjem/t38c)  - ⭐️ Node.js: [node-tile38](https://github.com/phulst/node-tile38) ([example code](https://github.com/tidwall/tile38/wiki/Node.js-example-(node-tile38)))  - ⭐️ Python: [pyle38](https://github.com/iwpnd/pyle38)  - Go: [cjkreklow/t38c](https://github.com/cjkreklow/t38c)  - Python: [pytile38](https://github.com/mitghi/pytile38)  - Rust: [nazar](https://github.com/younisshah/nazar)  - Swift: [Talon](https://github.com/mikekinney/Talon)  - Java: [tile38-client-java](https://github.com/jamshidrostami/tile38-client-java)  - Java: [tile38-client](https://github.com/HkMoyun/tile38-client)    ## Redis Client Libraries    Tile38 uses the [Redis RESP](https://redis.io/topics/protocol) protocol natively.   Therefore most clients that support basic Redis commands will also support Tile38.    - C: [hiredis](https://github.com/redis/hiredis)  - C#: [StackExchange.Redis](https://github.com/StackExchange/StackExchange.Redis)  - C++: [redox](https://github.com/hmartiro/redox)  - Clojure: [carmine](https://github.com/ptaoussanis/carmine)  - Common Lisp: [CL-Redis](https://github.com/vseloved/cl-redis)  - Erlang: [Eredis](https://github.com/wooga/eredis)  - Go: [go-redis](https://github.com/go-redis/redis) ([example code](https://github.com/tidwall/tile38/wiki/Go-example-(go-redis)))  - Go: [redigo](https://github.com/gomodule/redigo) ([example code](https://github.com/tidwall/tile38/wiki/Go-example-(redigo)))  - Haskell: [hedis](https://github.com/informatikr/hedis)  - Java: [lettuce](https://github.com/mp911de/lettuce) ([example code](https://github.com/tidwall/tile38/wiki/Java-example-(lettuce)))  - Node.js: [node_redis](https://github.com/NodeRedis/node_redis) ([example code](https://github.com/tidwall/tile38/wiki/Node.js-example-(node-redis)))  - Perl: [perl-redis](https://github.com/PerlRedis/perl-redis)  - PHP: [tinyredisclient](https://github.com/ptrofimov/tinyredisclient) ([example code](https://github.com/tidwall/tile38/wiki/PHP-example-(tinyredisclient)))  - PHP: [phpredis](https://github.com/phpredis/phpredis)  - Python: [redis-py](https://github.com/andymccurdy/redis-py) ([example code](https://github.com/tidwall/tile38/wiki/Python-example))  - Ruby: [redic](https://github.com/amakawa/redic) ([example code](https://github.com/tidwall/tile38/wiki/Ruby-example-(redic)))  - Ruby: [redis-rb](https://github.com/redis/redis-rb) ([example code](https://github.com/tidwall/tile38/wiki/Ruby-example-(redis-rb)))  - Rust: [redis-rs](https://github.com/mitsuhiko/redis-rs)  - Scala: [scala-redis](https://github.com/debasishg/scala-redis)  - Swift: [Redbird](https://github.com/czechboy0/Redbird)    ## Contact    Josh Baker [@tidwall](https://twitter.com/tidwall)    ## License    Tile38 source code is available under the MIT [License](/LICENSE). """
Big data;https://github.com/senseidb/zoie;"""What is Zoie  ===============    Zoie is a realtime search/indexing system written in Java.      ------------------------------------    ### Wiki    Wiki is available at:     [http://linkedin.jira.com/wiki/display/ZOIE/Home](http://linkedin.jira.com/wiki/display/ZOIE/Home)    ### Issues    Issues are tracked at:     [http://linkedin.jira.com/browse/ZOIE](http://linkedin.jira.com/browse/ZOIE)    ### Release    Maven:    groupId: com.senseidb.zoie    artifactId: zoie-core    Latest Version: 3.0.0 """
Big data;https://github.com/apache/incubator-airflow;"""<!--   Licensed to the Apache Software Foundation (ASF) under one   or more contributor license agreements.  See the NOTICE file   distributed with this work for additional information   regarding copyright ownership.  The ASF licenses this file   to you under the Apache License, Version 2.0 (the   ""License""); you may not use this file except in compliance   with the License.  You may obtain a copy of the License at       http://www.apache.org/licenses/LICENSE-2.0     Unless required by applicable law or agreed to in writing,   software distributed under the License is distributed on an   ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY   KIND, either express or implied.  See the License for the   specific language governing permissions and limitations   under the License.  -->    # Apache Airflow    [![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow)  [![GitHub Build](https://github.com/apache/airflow/workflows/CI%20Build/badge.svg)](https://github.com/apache/airflow/actions)  [![Coverage Status](https://img.shields.io/codecov/c/github/apache/airflow/main.svg)](https://codecov.io/github/apache/airflow?branch=main)  [![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.txt)  [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/apache-airflow.svg)](https://pypi.org/project/apache-airflow/)  [![Docker Pulls](https://img.shields.io/docker/pulls/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)  [![Docker Stars](https://img.shields.io/docker/stars/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)  [![PyPI - Downloads](https://img.shields.io/pypi/dm/apache-airflow)](https://pypi.org/project/apache-airflow/)  [![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow)](https://artifacthub.io/packages/search?repo=apache-airflow)  [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)  [![Twitter Follow](https://img.shields.io/twitter/follow/ApacheAirflow.svg?style=social&label=Follow)](https://twitter.com/ApacheAirflow)  [![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://s.apache.org/airflow-slack)    [Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.    When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.    Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.    <!-- START doctoc generated TOC please keep comment here to allow auto update -->  <!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->  **Table of contents**    - [Project Focus](#project-focus)  - [Principles](#principles)  - [Requirements](#requirements)  - [Getting started](#getting-started)  - [Installing from PyPI](#installing-from-pypi)  - [Official source code](#official-source-code)  - [Convenience packages](#convenience-packages)  - [User Interface](#user-interface)  - [Semantic versioning](#semantic-versioning)  - [Version Life Cycle](#version-life-cycle)  - [Support for Python and Kubernetes versions](#support-for-python-and-kubernetes-versions)  - [Base OS support for reference Airflow images](#base-os-support-for-reference-airflow-images)  - [Approach to dependencies of Airflow](#approach-to-dependencies-of-airflow)  - [Support for providers](#support-for-providers)  - [Contributing](#contributing)  - [Who uses Apache Airflow?](#who-uses-apache-airflow)  - [Who Maintains Apache Airflow?](#who-maintains-apache-airflow)  - [Can I use the Apache Airflow logo in my presentation?](#can-i-use-the-apache-airflow-logo-in-my-presentation)  - [Airflow merchandise](#airflow-merchandise)  - [Links](#links)  - [Sponsors](#sponsors)    <!-- END doctoc generated TOC please keep comment here to allow auto update -->    ## Project Focus    Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include [Luigi](https://github.com/spotify/luigi), [Oozie](https://oozie.apache.org/) and [Azkaban](https://azkaban.github.io/).    Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's [Xcom feature](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#xcoms)). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.    Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.    ## Principles    - **Dynamic**: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.  - **Extensible**: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.  - **Elegant**: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful **Jinja** templating engine.  - **Scalable**: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers.    ## Requirements    Apache Airflow is tested with:    |                     | Main version (dev)  | Stable version (2.2.4)   |  |---------------------|---------------------|--------------------------|  | Python              | 3.7, 3.8, 3.9       | 3.6, 3.7, 3.8, 3.9       |  | Platform            | AMD64/ARM64(\*)     | AMD64                    |  | Kubernetes          | 1.20, 1.21          | 1.18, 1.19, 1.20         |  | PostgreSQL          | 10, 11, 12, 13      | 9.6, 10, 11, 12, 13      |  | MySQL               | 5.7, 8              | 5.7, 8                   |  | SQLite              | 3.15.0+             | 3.15.0+                  |  | MSSQL               | 2017(\*), 2019 (\*) |                          |    \* Experimental    **Note**: MySQL 5.x versions are unable to or have limitations with  running multiple schedulers -- please see the [Scheduler docs](https://airflow.apache.org/docs/apache-airflow/stable/scheduler.html).  MariaDB is not tested/recommended.    **Note**: SQLite is used in Airflow tests. Do not use it in production. We recommend  using the latest stable version of SQLite for local development.    **Note**: Python v3.10 is not supported yet. For details, see [#19059](https://github.com/apache/airflow/issues/19059).    **Note**: Airflow currently can be run on POSIX-compliant Operating Systems. For development it is regularly  tested on fairly modern Linux Distros and recent versions of MacOS.  On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers.  The work to add Windows support is tracked via [#10388](https://github.com/apache/airflow/issues/10388) but  it is not a high priority. You should only use Linux-based distros as ""Production"" execution environment  as this is the only environment that is supported. The only distro that is used in our CI tests and that  is used in the [Community managed DockerHub image](https://hub.docker.com/p/apache/airflow) is  `Debian Bullseye`.    ## Getting started    Visit the official Airflow website documentation (latest **stable** release) for help with  [installing Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation.html),  [getting started](https://airflow.apache.org/docs/apache-airflow/stable/start/index.html), or walking  through a more complete [tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html).    > Note: If you're looking for documentation for the main branch (latest development branch): you can find it on [s.apache.org/airflow-docs](https://s.apache.org/airflow-docs/).    For more information on Airflow Improvement Proposals (AIPs), visit  the [Airflow Wiki](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals).    Documentation for dependent projects like provider packages, Docker image, Helm Chart, you'll find it in [the documentation index](https://airflow.apache.org/docs/).    ## Installing from PyPI    We publish Apache Airflow as `apache-airflow` package in PyPI. Installing it however might be sometimes tricky  because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and  applications usually pin them, but we should do neither and both simultaneously. We decided to keep  our dependencies as open as possible (in `setup.py`) so users can install different versions of libraries  if needed. This means that `pip install apache-airflow` will not work from time to time or will  produce unusable Airflow installation.    To have repeatable installation, however, we keep a set of ""known-to-be-working"" constraint  files in the orphan `constraints-main` and `constraints-2-0` branches. We keep those ""known-to-be-working""  constraints files separately per major/minor Python version.  You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify  correct Airflow tag/version/branch and Python versions in the URL.      1. Installing just Airflow:    > Note: Only `pip` installation is currently officially supported.    While it is possible to install Airflow with tools like [Poetry](https://python-poetry.org) or  [pip-tools](https://pypi.org/project/pip-tools), they do not share the same workflow as  `pip` - especially when it comes to constraint vs. requirements management.  Installing via `Poetry` or `pip-tools` is not currently supported.    If you wish to install Airflow using those tools, you should use the constraint files and convert  them to the appropriate format and workflow that your tool requires.      ```bash  pip install 'apache-airflow==2.2.4' \   --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.2.4/constraints-3.7.txt""  ```    2. Installing with extras (i.e., postgres, google)    ```bash  pip install 'apache-airflow[postgres,google]==2.2.4' \   --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.2.4/constraints-3.7.txt""  ```    For information on installing provider packages, check  [providers](http://airflow.apache.org/docs/apache-airflow-providers/index.html).    ## Official source code    Apache Airflow is an [Apache Software Foundation](https://www.apache.org) (ASF) project,  and our official source code releases:    - Follow the [ASF Release Policy](https://www.apache.org/legal/release-policy.html)  - Can be downloaded from [the ASF Distribution Directory](https://downloads.apache.org/airflow)  - Are cryptographically signed by the release manager  - Are officially voted on by the PMC members during the    [Release Approval Process](https://www.apache.org/legal/release-policy.html#release-approval)    Following the ASF rules, the source packages released must be sufficient for a user to build and test the  release provided they have access to the appropriate platform and tools.    ## Convenience packages    There are other ways of installing and using Airflow. Those are ""convenience"" methods - they are  not ""official releases"" as stated by the `ASF Release Policy`, but they can be used by the users  who do not want to build the software themselves.    Those are - in the order of most common ways people install Airflow:    - [PyPI releases](https://pypi.org/project/apache-airflow/) to install Airflow using standard `pip` tool  - [Docker Images](https://hub.docker.com/r/apache/airflow) to install airflow via    `docker` tool, use them in Kubernetes, Helm Charts, `docker-compose`, `docker swarm`, etc. You can    read more about using, customising, and extending the images in the    [Latest docs](https://airflow.apache.org/docs/docker-stack/index.html), and    learn details on the internals in the [IMAGES.rst](https://github.com/apache/airflow/blob/main/IMAGES.rst) document.  - [Tags in GitHub](https://github.com/apache/airflow/tags) to retrieve the git project sources that    were used to generate official source packages via git    All those artifacts are not official releases, but they are prepared using officially released sources.  Some of those artifacts are ""development"" or ""pre-release"" ones, and they are clearly marked as such  following the ASF Policy.    ## User Interface    - **DAGs**: Overview of all DAGs in your environment.      ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/dags.png)    - **Tree**: Tree representation of a DAG that spans across time.      ![Tree](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/tree.png)    - **Graph**: Visualization of a DAG's dependencies and their current status for a specific run.      ![Graph](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/graph.png)    - **Task Duration**: Total time spent on different tasks over time.      ![Task Duration](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/duration.png)    - **Gantt**: Duration and overlap of a DAG.      ![Gantt](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/gantt.png)    - **Code**: Quick way to view source code of a DAG.      ![Code](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/code.png)    ## Semantic versioning    As of Airflow 2.0.0, we support a strict [SemVer](https://semver.org/) approach for all packages released.    There are few specific rules that we agreed to that define details of versioning of the different  packages:    * **Airflow**: SemVer rules apply to core airflow only (excludes any changes to providers).    Changing limits for versions of Airflow dependencies is not a breaking change on its own.  * **Airflow Providers**: SemVer rules apply to changes in the particular provider's code only.    SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version.    For example, `google 4.1.0` and `amazon 3.0.3` providers can happily be installed    with `Airflow 2.1.2`. If there are limits of cross-dependencies between providers and Airflow packages,    they are present in providers as `install_requires` limitations. We aim to keep backwards    compatibility of providers with all previously released Airflow 2 versions but    there will sometimes be breaking changes that might make some, or all    providers, have minimum Airflow version specified. Change of that minimum supported Airflow version    is a breaking change for provider because installing the new provider might automatically    upgrade Airflow (which might be an undesired side effect of upgrading provider).  * **Airflow Helm Chart**: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR    versions for the chart are independent from the Airflow version. We aim to keep backwards    compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might    only work starting from specific Airflow releases. We might however limit the Helm    Chart to depend on minimal Airflow version.  * **Airflow API clients**: SemVer MAJOR and MINOR versions follow MAJOR and MINOR versions of Airflow.    The first MAJOR or MINOR X.Y.0 release of Airflow should always be followed by X.Y.0 release of    all clients. The clients then can release their own PATCH releases with bugfixes,    independently of Airflow PATCH releases.    ## Version Life Cycle    Apache Airflow version life cycle:    <!-- This table is automatically updated by pre-commit scripts/ci/pre-commit/supported_versions.py -->  <!-- Beginning of auto-generated table -->    | Version   | Current Patch/Minor   | State     | First Release   | Limited Support   | EOL/Terminated   |  |-----------|-----------------------|-----------|-----------------|-------------------|------------------|  | 2         | 2.2.4                 | Supported | Dec 17, 2020    | TBD               | TBD              |  | 1.10      | 1.10.15               | EOL       | Aug 27, 2018    | Dec 17, 2020      | June 17, 2021    |  | 1.9       | 1.9.0                 | EOL       | Jan 03, 2018    | Aug 27, 2018      | Aug 27, 2018     |  | 1.8       | 1.8.2                 | EOL       | Mar 19, 2017    | Jan 03, 2018      | Jan 03, 2018     |  | 1.7       | 1.7.1.2               | EOL       | Mar 28, 2016    | Mar 19, 2017      | Mar 19, 2017     |    <!-- End of auto-generated table -->    Limited support versions will be supported with security and critical bug fix only.  EOL versions will not get any fixes nor support.  We always recommend that all users run the latest available minor release for whatever major version is in use.  We **highly** recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.    ## Support for Python and Kubernetes versions    As of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support.  They are based on the official release schedule of Python and Kubernetes, nicely summarized in the  [Python Developer's Guide](https://devguide.python.org/#status-of-python-branches) and  [Kubernetes version skew policy](https://kubernetes.io/docs/setup/release/version-skew-policy/).    1. We drop support for Python and Kubernetes versions when they reach EOL. We drop support for those     EOL versions in main right after EOL date, and it is effectively removed when we release the     first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow     For example, for Python 3.7 it means that we will drop support in main right after 27.06.2023, and     the first MAJOR or MINOR version of Airflow released after will not have it.    2. The ""oldest"" supported version of Python/Kubernetes is the default one until we decide to switch to     later version. ""Default"" is only meaningful in terms of ""smoke tests"" in CI PRs, which are run using this     default version and the default reference image available. Currently `apache/airflow:latest`     and `apache/airflow:2.2.4` images are Python 3.7 images. This means that default reference image will     become the default at the time when we start preparing for dropping 3.7 support which is few months     before the end of life for Python 3.7.    4. We support a new version of Python/Kubernetes in main after they are officially released, as soon as we     make them work in our CI pipeline (which might not be immediate due to dependencies catching up with     new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.    ## Base OS support for reference Airflow images    The Airflow Community provides conveniently packaged container images that are published whenever  we publish an Apache Airflow release. Those images contain:    * Base OS with necessary packages to install Airflow (stable Debian OS)  * Base Python installation in versions supported at the time of release for the MINOR version of    Airflow released (so there could be different versions for 2.3 and 2.2 line for example)  * Libraries required to connect to suppoerted Databases (again the set of databases supported depends    on the MINOR version of Airflow.  * Predefined set of popular providers (for details see the [Dockerfile](Dockerfile)).  * Possibility of building your own, custom image where the user can choose their own set of providers    and libraries (see [Building the image](https://airflow.apache.org/docs/docker-stack/build.html))  * In the future Airflow might also support a ""slim"" version without providers nor database clients installed    The version of the base OS image is the stable version of Debian. Airflow supports using all currently active  stable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for  building and testing the OS version. Approximately 6 months before the end-of-life of a previous stable  version of the OS, Airflow switches the images released to use the latest supported version of the OS.  For example since Debian Buster end-of-life is August 2022, Airflow switches the images in `main` branch  to use Debian Bullseye in February/March 2022. The version will be used in the next MINOR release after  the switch happens. In case of the Bullseye switch - 2.3.0 version will use Bullseye. The images released  in the previous MINOR version continue to use the version that all other releases for the MINOR version  used.    Users will continue to be able to build their images using stable Debian releases until the end of life and  building and verifying of the images happens in our CI but no unit tests are executed using this image in  the `main` branch.    ## Approach to dependencies of Airflow    Airflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application,  therefore our policies to dependencies has to include both - stability of installation of application,  but also ability to install newer version of dependencies for those users who develop DAGs. We developed  the approach where `constraints` are used to make sure airflow can be installed in a repeatable way, while  we do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound  version of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is  needed because of importance of the dependency as well as risk it involves to upgrade specific dependency.  We also upper-bound the dependencies that we know cause problems.    The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies  automatically (providing that all the tests pass). Our `main` build failures will indicate in case there  are versions of dependencies that break our tests - indicating that we should either upper-bind them or  that we should fix our code/tests to account for the upstream changes from those dependencies.    Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have  a good reason why dependency is upper-bound. And we should also mention what is the condition to remove the  binding.    ### Approach for dependencies for Airflow Core    Those `extras` and `providers` dependencies are maintained in `setup.cfg`.    There are few dependencies that we decided are important enough to upper-bound them by default, as they are  known to follow predictable versioning scheme, and we know that new versions of those are very likely to  bring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of  the dependencies as they are released, but this is manual process.    The important dependencies are:    * `SQLAlchemy`: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and     introduce breaking changes especially that support for different Databases varies and changes at     various speed (example: SQLAlchemy 1.4 broke MSSQL integration for Airflow)  * `Alembic`: it is important to handle our migrations in predictable and performant way. It is developed     together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version  * `Flask`: We are using Flask as the back-bone of our web UI and API. We know major version of Flask     are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense  * `werkzeug`: the library is known to cause problems in new versions. It is tightly coupled with Flask     libraries, and we should update them together    ### Approach for dependencies in Airflow Providers and extras    Those `extras` and `providers` dependencies are maintained in `setup.py`.    By default, we should not upper-bound dependencies for providers, however each provider's maintainer  might decide to add additional limits (and justify them with comment)    ## Support for providers    Providers released by the community have limitation of a minimum supported version of Airflow. The minimum  version of Airflow is the `MINOR` version (2.1, 2.2 etc.) indicating that the providers might use features  that appeared in this release. The default support timespan for the minimum version of Airflow  (there could be justified exceptions) is that we increase the minimum Airflow version, when 12 months passed  since the first release for the MINOR version of Airflow.    For example this means that by default we upgrade the minimum version of Airflow supported by providers  to 2.2.0 in the first Provider's release after 21st of May 2022 (21st of May 2021 is the date when the  first `PATCHLEVEL` of 2.1 (2.1.0) has been released.    ## Contributing    Want to help build Apache Airflow? Check out our [contributing documentation](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst).    Official Docker (container) images for Apache Airflow are described in [IMAGES.rst](https://github.com/apache/airflow/blob/main/IMAGES.rst).    ## Who uses Apache Airflow?    More than 400 organizations are using Apache Airflow  [in the wild](https://github.com/apache/airflow/blob/main/INTHEWILD.md).    ## Who Maintains Apache Airflow?    Airflow is the work of the [community](https://github.com/apache/airflow/graphs/contributors),  but the [core committers/maintainers](https://people.apache.org/committers-by-project.html#airflow)  are responsible for reviewing and merging PRs as well as steering conversations around new feature requests.  If you would like to become a maintainer, please review the Apache Airflow  [committer requirements](https://github.com/apache/airflow/blob/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer).    ## Can I use the Apache Airflow logo in my presentation?    Yes! Be sure to abide by the Apache Foundation [trademark policies](https://www.apache.org/foundation/marks/#books) and the Apache Airflow [Brandbook](https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook). The most up to date logos are found in [this repo](/docs/apache-airflow/img/logos) and on the Apache Software Foundation [website](https://www.apache.org/logos/about.html).    ## Airflow merchandise    If you would love to have Apache Airflow stickers, t-shirt, etc. then check out  [Redbubble Shop](https://www.redbubble.com/i/sticker/Apache-Airflow-by-comdev/40497530.EJUG5).    ## Links    - [Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)  - [Chat](https://s.apache.org/airflow-slack)    ## Sponsors    The CI infrastructure for Apache Airflow has been sponsored by:    <!-- Ordered by most recently ""funded"" -->    <a href=""https://astronomer.io""><img src=""https://assets2.astronomer.io/logos/logoForLIGHTbackground.png"" alt=""astronomer.io"" width=""250px""></a>  <a href=""https://aws.amazon.com/opensource/""><img src=""docs/integration-logos/aws/AWS-Cloud-alt_light-bg@4x.png"" alt=""AWS OpenSource"" width=""130px""></a> """
Big data;https://github.com/CSNW/d3.compose;"""# d3.compose    Compose rich, data-bound charts from charts (like Lines and Bars) and components (like Axis, Title, and Legend) with d3 and d3.chart.    - Advanced layout engine automatically positions and sizes charts and components, layers by z-index, and is responsive by default with automatic scaling  - Standard library of charts and components for quickly creating beautiful charts  - `Chart` and `Component` bases for creating composable and reusable charts and components  - Includes helpers and mixins that cover a range of standard functionality  - CSS class-based styling is extensible and easy to customize to match your site    [![npm version](https://img.shields.io/npm/v/d3.compose.svg?style=flat-square)](https://www.npmjs.com/package/d3.compose)    ## Getting Started    1. Download the [latest release](https://github.com/CSNW/d3.compose/releases)  2. Download the dependencies:        - [D3.js (>= 3.0.0)](http://d3js.org/)      - [d3.chart (>= 0.2.0)](http://misoproject.com/d3-chart/)    3. Add d3.compose and dependencies to your html:        ```html      <!doctype html>      <html>        <head>          <!-- ... -->            <link rel=""stylesheet"" type=""text/css"" href=""d3.compose.css"">        </head>        <body>          <!-- ... -->            <script src=""d3.js""></script>          <script src=""d3.chart.js""></script>            <script src=""d3.compose.js""></script>            <!-- Your code -->        </body>      </html>      ```    4. Create your first chart        ```js      var chart = d3.select('#chart')        .chart('Compose', function(data) {          var scales = {            x: {type: 'ordinal', data: data, key: 'x'},            y: {data: data, key: 'y'}          };            var charts = [            d3c.lines({              data: data,              xScale: scales.x,              yScale: scales.y            })          ];            var yAxis = d3c.axis({scale: scales.y});            return [            [yAxis, d3c.layered(charts)]          ];        })        .width(600)        .height(400);        chart.draw([{x: 0, y: 10}, {x: 10, y: 50}, {x: 20, y: 30}]);      ```    ## Examples and Docs    See [http://CSNW.github.io/d3.compose/](http://CSNW.github.io/d3.compose/) for live examples and docs.    ## Development    1. Install modules `npm install`  2. Test with `npm test` or `npm run test:watch`  3. Build with `npm run build`    Note on testing: Requires Node 4+ (for latest jsdom) and d3.chart doesn't currently support running from within node  and requires the following line be added inside the IIFE in `node_modules/d3.chart.js`: `window = this;` (before `use strict`). This will be resolved by a [pending PR](https://github.com/misoproject/d3.chart/pull/113) to fix this issue with d3.chart (also, the dependency on d3.chart is likely to be removed in a later version of d3.compose).    ### Release    (With all changes merged to master and on master branch)    1. `npm version {patch|minor|major|version}`  2. `npm publish`    ### Docs    1. On master, run `npm run docs`  2. Switch to `gh-pages` branch  3. Navigate to `_tasks` directory (`cd _tasks`)  4. (`npm install` _tasks, if necessary)  5. Run docs task `npm run docs`  6. Navigate back to root  7. View site with `bundle exec jekyll serve`    Note: For faster iteration, create a separate clone, switch to `gh-pages` branch, set `docs_path` environment variable to original clone (e.g. Windows: `SET docs_path=C:\...\d3.compose\_docs\`), and then run steps 3-6. """
