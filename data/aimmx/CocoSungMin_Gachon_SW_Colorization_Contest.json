{
    "visibility": {
        "visibility": "public"
    },
    "name": "Colorization with Deep learning competitions in Multimedia&lab lecture, Department of Software Gachon Uni.",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "CocoSungMin",
                "owner_type": "User",
                "name": "Gachon_SW_Colorization_Contest",
                "url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest",
                "stars": 2,
                "pushed_at": "2021-06-03 03:11:13+00:00",
                "created_at": "2021-05-25 01:48:27+00:00",
                "language": "Python",
                "description": "Multimedia&Lab lecture Hint based Image Colorization Challenge from Prof. Yong Ju Jung in Gachon Uni",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "ddbfc32f846cfb52fd23d9b34370e7836eec6f3c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest/tree/main/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "CONTRIBUTING.md",
                "sha": "676c43744dbd25162878717d6e71581639c49701",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest/blob/main/CONTRIBUTING.md"
                    }
                },
                "size": 327
            },
            {
                "type": "code",
                "name": "aug.py",
                "sha": "af71b74a9fbce2d9fb21d584b853c8b9b2a39b07",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest/blob/main/aug.py"
                    }
                },
                "size": 562
            },
            {
                "type": "code",
                "name": "dataloader.py",
                "sha": "3ac12571fe3b673fa7a7bb352afd15c1ff2c4848",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest/blob/main/dataloader.py"
                    }
                },
                "size": 4154
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "82b38c0604622dc16ccec5866c12df07c8c5de31",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest/blob/main/main.py"
                    }
                },
                "size": 5488
            },
            {
                "type": "code",
                "name": "predict.py",
                "sha": "04f27a8739c528b96011c3898006ab0197303255",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest/blob/main/predict.py"
                    }
                },
                "size": 2872
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "a0afdc7a2ce4eb0777ba0c2c9398ecf56a091984",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest/blob/main/test.py"
                    }
                },
                "size": 1413
            },
            {
                "type": "code",
                "name": "unet_resblock.py",
                "sha": "7f0fb0893a597ba3a32d17d71ccf00d5f53ef17d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest/blob/main/unet_resblock.py"
                    }
                },
                "size": 5533
            }
        ]
    },
    "authors": [
        {
            "name": "CocoSungMin",
            "github_id": "CocoSungMin"
        },
        {
            "name": "YoonHyeJu",
            "github_id": "YoonHyeJu"
        }
    ],
    "tags": [
        "colorization",
        "deep-learning"
    ],
    "description": "Multimedia&Lab lecture Hint based Image Colorization Challenge from Prof. Yong Ju Jung in Gachon Uni",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest",
            "stars": 2,
            "issues": true,
            "readme": "# Colorization with Deep learning competitions in Multimedia&lab lecture, Department of Software Gachon Uni.\n-----------------------\n## Model Overview\n![git](https://user-images.githubusercontent.com/57583574/119586484-c7bc7f80-be07-11eb-960b-2a3ffb4f4c10.gif)\n-----------------------\n### This Project code ranked 1st place in Colorization Challenge competitions in Multimedia&lab lecture ( From CVIP Lab in Gachon Univ. ).    \n<img width=\"1243\" alt=\"Screen Shot 2021-05-28 at 12 07 50 AM\" src=\"https://user-images.githubusercontent.com/57583574/119850812-d5c5e980-bf48-11eb-8afb-325c95a3c258.png\">\n\n\n\nReferenced Paper\n1. U-Net Convolutional Networks for Biomedical Image Segmentation : https://arxiv.org/abs/1505.04597.   \n2. Attention U-Net Learning Where to Look for the Pancreas : https://arxiv.org/abs/1804.03999      \n3. Real-Time User-Guided Image Colorization with Learned Deep Priors : https://arxiv.org/abs/1705.02999.  \n4. Deep Residual Learning for Image Recognition : https://arxiv.org/abs/1512.03385.  \n\n\nBasic model based on \"Hyeju Yoon\" standard u-net : \n```\nhttps://drive.google.com/file/d/1P0Vbt_V5FdcjWVyFcgQiK496nRUOWHzI/view?usp=sharing. \n``` \n\"Sungmin Lee\" replace the [ Conv - BatchNorm - ReLU ] blocks to Residual block and add attention layer in skip connection.   \n \n   \nOur model input has [ l . ab_hint , mask ] as a input.   \nMost of Fully open github project for colorization via deep learning adapted 4 channels as a input.   \nTheir 4 channels consisted with 3 color channel ( e.g. Lab format L and ab channel ) and mask as a channel.   \nThe mask channel leads models to train provided hint with location of the hint pixel color.   \nThis work shows better performance than other works in our team.\nSo 4 channels go in, output comes with 3 channels so that it can be converted to \".png\" format.    \nAlso we augment the training data ( Flip horizontal , Rotate 180 ) to expand training datasets.    \n\nWe tried L1 , MSE , 1-SSIM , 1-MSSIM as a loss function.    \nBut for our model the L1 loss return the highest score than others.   \nAlso for Learning Rate , we used , 1e-2 , 2e-2 , 1e-3 , 1e-4 , 2e-4 , 25e-5\nLike 1e-2 to 2e-4 shows a lot of blurred pixel with aritifacts but when learning with 25e-5 the epochs goes on the frequency of the artifacts goes down.\n\nFirst time we trained with batch size 4.     \nBut after we get lowest losses, the same losses has different performances ( not that big differences but not same ).     \nSo we tested the Best performance model files through test.py and select 122th epochs model.     \nThen increasing batchsize , training again from 122th epochs .     \nAfter that we got those score in leaderboard at CVIP Colorization Challenges.   \n\n------------------------\n## Team Member\n1. Sungmin Lee ( Department of Software , undergraduated Research Assistant in VMRLAB Gachon Uni.)\n2. Taeho Oh ( Department of Software , Senior student in Gachon Uni.)\n3. Hyeju Yoon ( Department of Software , Senior student in Gachon Uni.)\n4. Soeun Lee ( Department of Software , Senior student in Gachon Uni.)\n-----------------------\n## Model Architecture\n\n### Basic U-net Architecture\n![Picture1](https://user-images.githubusercontent.com/57583574/119441049-68effb00-bd60-11eb-98c1-4877df56fb21.png)\n\n### Our Model Architecture\n![Screen Shot 2021-05-26 at 11 12 05 AM](https://user-images.githubusercontent.com/57583574/119592333-3a7f2800-be13-11eb-9d99-bd4fe0bee3e2.png)\n\n\n### Residual Block Architecture\n![Recurrent_Residual_Block](https://user-images.githubusercontent.com/57583574/119463055-55528d80-bd7c-11eb-8063-09ef2857df7d.png)\n\n### Attention Block Architecture\n![Screen Shot 2021-06-02 at 8 10 37 PM](https://user-images.githubusercontent.com/57583574/120470628-b8789b80-c3de-11eb-9aa5-aac87159405d.png)\n\n\n### Upsample Block Architecture\n![Upsample Block](https://user-images.githubusercontent.com/57583574/119459857-0a834680-bd79-11eb-8f75-a0a5b149c9da.png)\n\n### Model Results\n![Model output](https://user-images.githubusercontent.com/57583574/119458995-42d65500-bd78-11eb-9b78-4dfddbc22363.png)\n\n\n\n### Specific Information about model\nLearning Rate : 25e-5.   \nEpoch : 150.    \nLoss Function : L1 loss.   \nOptimizer : Adam.\nBatch Szize : 4.   \nInput data size : ( 128 X 128 ).    \nColor-hint : randomly given by dataloader [ 1% , 3% , 5% ].  \n   \n### Reload and Training ( From epochs 122 ).    \nLearning Rate : 25e-5.   \nEpoch : 10.    \nLoss Function : L1 loss.   \nOptimizer : Adam.\nBatch Szize : 16.   \nInput data size : ( 128 X 128 ).    \nColor-hint : randomly given by dataloader [ 1% , 3% , 5% ].  \n    \n### Do Reload and Training 3 times\n##### 122 epochs -> 10 epochs ( best 8t epochs ) -> 10 epochs ( best 5th epochs ) -> 10 epochs ( best 2nd epochs ).     \n\n-----------------------\n## Required Library\n1. numpy\n2. pillow 8.2.0\n3. pytorch 1.8.1\n4. torchvision 0.9.1\n5. python-cv2\n6. tqdm\n7. cuda 10.2\n\n-----------------\n## Code Usage\n1. Before training, please use \"aug.py\" to automatically augmentation training data ( flip , rotate 180 )\n2. For training \n```\npython main.py\n```\n\n3. For testing ( predicting )\n```\npython test.py\n```\n\n4.  Best Weight files\n```\nhttps://drive.google.com/file/d/1dZBjzAy5t9W6Du_GJZPf70RUBkCOpFG3/view?usp=sharing\n```\n-----------------\n## Colorization Dataset from CVIP Lab\nRestriction : Don't use test data as training data , augmentation is free.    \ntraining image ( contest provided) \n```\nhttps://drive.google.com/file/d/1nCaLNE644mz7JEYeyAbvyO0njpGdD4s1/view?usp=sharing\n```\n\ntest image ( contest provided )\n```\nhttps://drive.google.com/file/d/1D0LLdXj447z7b88nuSWEMEoTfpcM6yY_/view?usp=sharing\n```\n--------------\n## License\nThis project belongs to Multimedia & Lab Team 6 members , Gachon Uni. ( 2021 spring semester )\n",
            "readme_url": "https://github.com/CocoSungMin/Gachon_SW_Colorization_Contest",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "arxiv": "1505.04597",
            "year": 2015,
            "url": "http://arxiv.org/abs/1505.04597v1",
            "abstract": "There is large consent that successful training of deep networks requires\nmany thousand annotated training samples. In this paper, we present a network\nand training strategy that relies on the strong use of data augmentation to use\nthe available annotated samples more efficiently. The architecture consists of\na contracting path to capture context and a symmetric expanding path that\nenables precise localization. We show that such a network can be trained\nend-to-end from very few images and outperforms the prior best method (a\nsliding-window convolutional network) on the ISBI challenge for segmentation of\nneuronal structures in electron microscopic stacks. Using the same network\ntrained on transmitted light microscopy images (phase contrast and DIC) we won\nthe ISBI cell tracking challenge 2015 in these categories by a large margin.\nMoreover, the network is fast. Segmentation of a 512x512 image takes less than\na second on a recent GPU. The full implementation (based on Caffe) and the\ntrained networks are available at\nhttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ]
        },
        {
            "title": "Real-Time User-Guided Image Colorization with Learned Deep Priors",
            "arxiv": "1705.02999",
            "year": 2017,
            "url": "http://arxiv.org/abs/1705.02999v1",
            "abstract": "We propose a deep learning approach for user-guided image colorization. The\nsystem directly maps a grayscale image, along with sparse, local user \"hints\"\nto an output colorization with a Convolutional Neural Network (CNN). Rather\nthan using hand-defined rules, the network propagates user edits by fusing\nlow-level cues along with high-level semantic information, learned from\nlarge-scale data. We train on a million images, with simulated user inputs. To\nguide the user towards efficient input selection, the system recommends likely\ncolors based on the input image and current user inputs. The colorization is\nperformed in a single feed-forward pass, enabling real-time use. Even with\nrandomly simulated user inputs, we show that the proposed system helps novice\nusers quickly create realistic colorizations, and offers large improvements in\ncolorization quality with just a minute of use. In addition, we demonstrate\nthat the framework can incorporate other user \"hints\" to the desired\ncolorization, showing an application to color histogram transfer. Our code and\nmodels are available at https://richzhang.github.io/ideepcolor.",
            "authors": [
                "Richard Zhang",
                "Jun-Yan Zhu",
                "Phillip Isola",
                "Xinyang Geng",
                "Angela S. Lin",
                "Tianhe Yu",
                "Alexei A. Efros"
            ]
        },
        {
            "title": "Attention U-Net: Learning Where to Look for the Pancreas",
            "arxiv": "1804.03999",
            "year": 2018,
            "url": "http://arxiv.org/abs/1804.03999v3",
            "abstract": "We propose a novel attention gate (AG) model for medical imaging that\nautomatically learns to focus on target structures of varying shapes and sizes.\nModels trained with AGs implicitly learn to suppress irrelevant regions in an\ninput image while highlighting salient features useful for a specific task.\nThis enables us to eliminate the necessity of using explicit external\ntissue/organ localisation modules of cascaded convolutional neural networks\n(CNNs). AGs can be easily integrated into standard CNN architectures such as\nthe U-Net model with minimal computational overhead while increasing the model\nsensitivity and prediction accuracy. The proposed Attention U-Net architecture\nis evaluated on two large CT abdominal datasets for multi-class image\nsegmentation. Experimental results show that AGs consistently improve the\nprediction performance of U-Net across different datasets and training sizes\nwhile preserving computational efficiency. The code for the proposed\narchitecture is publicly available.",
            "authors": [
                "Ozan Oktay",
                "Jo Schlemper",
                "Loic Le Folgoc",
                "Matthew Lee",
                "Mattias Heinrich",
                "Kazunari Misawa",
                "Kensaku Mori",
                "Steven McDonagh",
                "Nils Y Hammerla",
                "Bernhard Kainz",
                "Ben Glocker",
                "Daniel Rueckert"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.979207356165429,
        "task": "Object Detection",
        "task_prob": 0.6862150498508289
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "COCO"
            }
        ]
    }
}