{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "Authors' Implementation of DeepSpeech Distances proposed in *High Fidelity Speech Synthesis with Adversarial Networks*",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "mbinkowski",
                "owner_type": "User",
                "name": "DeepSpeechDistances",
                "url": "https://github.com/mbinkowski/DeepSpeechDistances",
                "stars": 111,
                "pushed_at": "2020-05-05 18:20:56+00:00",
                "created_at": "2020-01-31 13:11:28+00:00",
                "language": "Jupyter Notebook",
                "description": "Authors' implementation of DeepSpeech Distances.",
                "license": "Apache License 2.0",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "a5bcb246d733420507b70d932d62ca614d033d2b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mbinkowski/DeepSpeechDistances/blob/master/LICENSE"
                    }
                },
                "size": 11350
            },
            {
                "type": "code",
                "name": "abstract.wav",
                "sha": "9a581b196ad14e963ec9db7cc4d25d98c6a31a68",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mbinkowski/DeepSpeechDistances/blob/master/abstract.wav"
                    }
                },
                "size": 3399198
            },
            {
                "type": "code",
                "name": "audio_distance.py",
                "sha": "75291b4af442dc439d7b6a4603bb42b499722211",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mbinkowski/DeepSpeechDistances/blob/master/audio_distance.py"
                    }
                },
                "size": 10781
            },
            {
                "type": "code",
                "name": "checkpoint",
                "sha": "f5f6c16ca66b3c63c05b9641cb6d857270044301",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mbinkowski/DeepSpeechDistances/tree/master/checkpoint"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "deep_speech_distances.ipynb",
                "sha": "997fa1c1127bbb65f4a73eb239a003826b4cd0b2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mbinkowski/DeepSpeechDistances/blob/master/deep_speech_distances.ipynb"
                    }
                },
                "size": 51138
            },
            {
                "type": "code",
                "name": "preprocessing.py",
                "sha": "a3cd6c1d66c633c807322ea05807f4ea43e42f0f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mbinkowski/DeepSpeechDistances/blob/master/preprocessing.py"
                    }
                },
                "size": 5468
            },
            {
                "type": "code",
                "name": "sample_utils.py",
                "sha": "d1441956c21e540dd1447d010419a426c97c4e70",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mbinkowski/DeepSpeechDistances/blob/master/sample_utils.py"
                    }
                },
                "size": 1799
            }
        ]
    },
    "authors": [
        {
            "name": "Miko\u0142aj Bi\u0144kowski",
            "email": "mikbinkowski@gmail.com",
            "github_id": "mbinkowski"
        }
    ],
    "tags": [],
    "description": "Authors' implementation of DeepSpeech Distances.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/mbinkowski/DeepSpeechDistances",
            "stars": 111,
            "issues": true,
            "readme": "# Authors' Implementation of DeepSpeech Distances proposed in [*High Fidelity Speech Synthesis with Adversarial Networks*](https://arxiv.org/abs/1909.11646)\n\nThis repo provides a code for estimation of **DeepSpeech Distances**, new evaluation metrics for neural speech synthesis.\n\n### **Details**\n\nThe computation involves estimating Fr\u00e9chet and Kernel distances between high-level features of the reference and the examined samples extracted from hidden representation of [NVIDIA's DeepSpeech2](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html) speech recognition model.\n\nWe propose four distances:\n\n\n*   *Fr\u00e9chet DeepSpeech Distance* (*FDSD*, based on FID, see [2])\n*   *Kernel DeepSpeech Distance* (*KDSD*, based on KID, see [3])\n*   *conditional Fr\u00e9chet DeepSpeech Distance* (*cFDSD*),\n*   *conditional Kernel DeepSpeech Distance* (*cKDSD*).\n\nThe conditional distances compare samples with the same conditioning (e.g. text) and asses conditional quality of the audio. The uncoditional ones compare random samples from two distributions and asses general quality of audio. For more details, see [1].\n\n### **Usage**\n\nTo use the demo, [open the provided notebook in colab](https://colab.research.google.com/github/mbinkowski/DeepSpeechDistances/blob/master/deep_speech_distances.ipynb).\n\nAlternatively, [open a new colab notebook](https://colab.research.google.com/), mount a drive and clone this repository:\n\n```\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n!git clone https://github.com/mbinkowski/DeepSpeechDistances `/content/drive/My Drive/DeepSpeechDistances`\n```\nAfter that, go to */content/drive/My Drive/DeepSpeechDistances*, open a demo notebook *deep_speech_distances.ipynb*, and follow the instructions therein.\n\n### **Notes**\nWe provide a tensorflow meta graph file for DeepSpeech2 based on the original one available with the [checkpoint](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html). The provided file differs from the original only in the lack of map-reduce ops defined by horovod library; therefore the resulting model is equivalent to the original.\n\nThis is an 'alpha' version of the API; although fully functional it will be heavily updated and simplified soon.\n\n### **References**\n\n[1] Miko\u0142aj Bi\u0144kowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande, Luis C. Cobo, Karen Simonyan, [*High Fidelity Speech Synthesis with Adversarial Networks*](https://arxiv.org/abs/1909.11646), ICLR 2020.\n\n[2] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, [*GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium*](https://arxiv.org/abs/1706.08500), NeurIPS 2017.\n\n[3] Miko\u0142aj Bi\u0144kowski, Dougal J. Sutherland, Michael Arbel, Arthur Gretton, [*Demystifying MMD GANs*](https://arxiv.org/abs/1801.01401), ICLR 2018.\n",
            "readme_url": "https://github.com/mbinkowski/DeepSpeechDistances",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Demystifying MMD GANs",
            "arxiv": "1801.01401",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.01401v5",
            "abstract": "We investigate the training and performance of generative adversarial\nnetworks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs.\nAs our main theoretical contribution, we clarify the situation with bias in GAN\nloss functions raised by recent work: we show that gradient estimators used in\nthe optimization process for both MMD GANs and Wasserstein GANs are unbiased,\nbut learning a discriminator based on samples leads to biased gradients for the\ngenerator parameters. We also discuss the issue of kernel choice for the MMD\ncritic, and characterize the kernel corresponding to the energy distance used\nfor the Cramer GAN critic. Being an integral probability metric, the MMD\nbenefits from training strategies recently developed for Wasserstein GANs. In\nexperiments, the MMD GAN is able to employ a smaller critic network than the\nWasserstein GAN, resulting in a simpler and faster-training algorithm with\nmatching performance. We also propose an improved measure of GAN convergence,\nthe Kernel Inception Distance, and show how to use it to dynamically adapt\nlearning rates during GAN training.",
            "authors": [
                "Miko\u0142aj Bi\u0144kowski",
                "Danica J. Sutherland",
                "Michael Arbel",
                "Arthur Gretton"
            ]
        },
        {
            "title": "High Fidelity Speech Synthesis with Adversarial Networks",
            "arxiv": "1909.11646",
            "year": 2019,
            "url": "http://arxiv.org/abs/1909.11646v2",
            "abstract": "Generative adversarial networks have seen rapid development in recent years\nand have led to remarkable improvements in generative modelling of images.\nHowever, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in\ngenerative modelling of audio signals such as human speech. To address this\npaucity, we introduce GAN-TTS, a Generative Adversarial Network for\nText-to-Speech. Our architecture is composed of a conditional feed-forward\ngenerator producing raw speech audio, and an ensemble of discriminators which\noperate on random windows of different sizes. The discriminators analyse the\naudio both in terms of general realism, as well as how well the audio\ncorresponds to the utterance that should be pronounced. To measure the\nperformance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean\nOpinion Score), as well as novel quantitative metrics (Fr\\'echet DeepSpeech\nDistance and Kernel DeepSpeech Distance), which we find to be well correlated\nwith MOS. We show that GAN-TTS is capable of generating high-fidelity speech\nwith naturalness comparable to the state-of-the-art models, and unlike\nautoregressive models, it is highly parallelisable thanks to an efficient\nfeed-forward generator. Listen to GAN-TTS reading this abstract at\nhttps://storage.googleapis.com/deepmind-media/research/abstract.wav.",
            "authors": [
                "Miko\u0142aj Bi\u0144kowski",
                "Jeff Donahue",
                "Sander Dieleman",
                "Aidan Clark",
                "Erich Elsen",
                "Norman Casagrande",
                "Luis C. Cobo",
                "Karen Simonyan"
            ]
        },
        {
            "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
            "arxiv": "1706.08500",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.08500v6",
            "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images\nwith complex models for which maximum likelihood is infeasible. However, the\nconvergence of GAN training has still not been proved. We propose a two\ntime-scale update rule (TTUR) for training GANs with stochastic gradient\ndescent on arbitrary GAN loss functions. TTUR has an individual learning rate\nfor both the discriminator and the generator. Using the theory of stochastic\napproximation, we prove that the TTUR converges under mild assumptions to a\nstationary local Nash equilibrium. The convergence carries over to the popular\nAdam optimization, for which we prove that it follows the dynamics of a heavy\nball with friction and thus prefers flat minima in the objective landscape. For\nthe evaluation of the performance of GANs at image generation, we introduce the\n\"Fr\\'echet Inception Distance\" (FID) which captures the similarity of generated\nimages to real ones better than the Inception Score. In experiments, TTUR\nimproves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)\noutperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN\nBedrooms, and the One Billion Word Benchmark.",
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9990871486058293,
        "task": "Image Generation",
        "task_prob": 0.9872470398740925
    },
    "training": {
        "datasets": [
            {
                "name": "CelebA"
            },
            {
                "name": "One Billion Word"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    }
}