{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "SHA-RNN",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "alisafaya",
                "owner_type": "User",
                "name": "SHA-RNN.jl",
                "url": "https://github.com/alisafaya/SHA-RNN.jl",
                "stars": 2,
                "pushed_at": "2020-11-28 15:09:59+00:00",
                "created_at": "2020-02-12 16:32:12+00:00",
                "language": "Julia",
                "description": "Implementation of Single Headed Attention - Recurrent Neural Networks in Julia and Knet",
                "license": "MIT License",
                "frameworks": []
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "f68ca4ab6d57a669e2e61186d82ae3e9bcf209a2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alisafaya/SHA-RNN.jl/blob/master/.gitignore"
                    }
                },
                "size": 879
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "6eb70d8cce3002b9a431f4c374313dbd337fee48",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alisafaya/SHA-RNN.jl/blob/master/LICENSE"
                    }
                },
                "size": 1067
            },
            {
                "type": "code",
                "name": "SHA-RNN(2).png",
                "sha": "5f47692c0c4fdf7c1cb0f799526231e48c87762b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alisafaya/SHA-RNN.jl/blob/master/SHA-RNN(2).png"
                    }
                },
                "size": 138639
            },
            {
                "type": "code",
                "name": "data",
                "sha": "4104495ab1af23b6e33277d817820e345ff205e5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alisafaya/SHA-RNN.jl/tree/master/data"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "examples",
                "sha": "deb4dcacd0151633012e7b064d4ed5d2328417c3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alisafaya/SHA-RNN.jl/tree/master/examples"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "getdata.sh",
                "sha": "50b656d3dbb0235738cd2af8de83fb12a82e05ba",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alisafaya/SHA-RNN.jl/blob/master/getdata.sh"
                    }
                },
                "size": 840
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "523b374f48e3ff0eb317d966a77690f8551aec10",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alisafaya/SHA-RNN.jl/tree/master/notebooks"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "src",
                "sha": "129fb385903a4876abbce626328c06d9747653fc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alisafaya/SHA-RNN.jl/tree/master/src"
                    }
                },
                "num_files": 6
            }
        ]
    },
    "authors": [
        {
            "name": "Ali Safaya",
            "email": "alisafaya@gmail.com",
            "github_id": "alisafaya"
        }
    ],
    "tags": [
        "language-model",
        "lstm",
        "attention"
    ],
    "description": "Implementation of Single Headed Attention - Recurrent Neural Networks in Julia and Knet",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/alisafaya/SHA-RNN.jl",
            "stars": 2,
            "issues": true,
            "readme": "# SHA-RNN\n\nImplementation of Single Headed Attention - Recurrent Neural Networks in [Julia](https://julialang.org/) and [Knet](https://github.com/denizyuret/Knet.jl).\n\nStephan Merity. **Single Headed Attention RNN: Stop Thinking With Your Head**. _arXiv preprint arXiv:1911.11423_, 2019.\n\nhttps://arxiv.org/abs/1911.11423v2\n\n\n![SHA-RNN Model](https://raw.githubusercontent.com/alisafaya/SHA-RNN.jl/master/SHA-RNN(2).png)\n\n\nAfter downloading the data and preprocessing it using\n\n```bash\nsh getdata.sh\n```\n\nYou can train the main model of SHA-RNN paper by either:\n\n_running [sharnn-main.jl](examples/sharnn-main.jl) in shell_\n\n```bash\ncd examples\njulia sharnn-main.jl\n```\n\n_or using [SHA-RNN](notebooks/SHA-RNN.ipynb) notebook_.\n\nThis implementation is identical to the one of Smerity's original implementation [sha-rnn](https://github.com/Smerity/sha-rnn). \n\nBut it is slower, since it does not use the same performance tricks that the version of SHA-RNN that was implemented using pytorch uses.\n\n\n### Features to be added to get faster training :\n\n- Fused layer normalization (check if [Apex](https://github.com/NVIDIA/apex/) CUDA code can be used with Knet)\n- Using half precision floating point (Float16) for memory efficiency\n- Checkpoint feature similar to pytorch's [checkpoint](https://pytorch.org/docs/stable/checkpoint.html).\n",
            "readme_url": "https://github.com/alisafaya/SHA-RNN.jl",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Single Headed Attention RNN: Stop Thinking With Your Head",
            "arxiv": "1911.11423",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.11423v2",
            "abstract": "The leading approaches in language modeling are all obsessed with TV shows of\nmy youth - namely Transformers and Sesame Street. Transformers this,\nTransformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer\nscale silicon. We opt for the lazy path of old and proven techniques with a\nfancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The\nauthor's lone goal is to show that the entire field might have evolved a\ndifferent direction if we had instead been obsessed with a slightly different\nacronym and slightly different result. We take a previously strong language\nmodel based only on boring LSTMs and get it to within a stone's throw of a\nstone's throw of state-of-the-art byte level language model results on enwik8.\nThis work has undergone no intensive hyperparameter optimization and lived\nentirely on a commodity desktop machine that made the author's small studio\napartment far too warm in the midst of a San Franciscan summer. The final\nresults are achievable in plus or minus 24 hours on a single GPU as the author\nis impatient. The attention mechanism is also readily extended to large\ncontexts with minimal computation. Take that Sesame Street.",
            "authors": [
                "Stephen Merity"
            ]
        }
    ],
    "domain": {
        "domain_type": "Unknown"
    }
}