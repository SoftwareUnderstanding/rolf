{
    "visibility": {
        "visibility": "public"
    },
    "name": "BERGAN: music bar generation and techno music with GANs",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "adrienchaton",
                "owner_type": "User",
                "name": "BERGAN",
                "url": "https://github.com/adrienchaton/BERGAN",
                "stars": 1,
                "pushed_at": "2021-06-18 08:34:49+00:00",
                "created_at": "2021-05-24 14:39:21+00:00",
                "language": "Python",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "__export_interp.py",
                "sha": "558c1b6b9c73a2f66d424a6e670c659ed0b27bbd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/blob/main/__export_interp.py"
                    }
                },
                "size": 1545
            },
            {
                "type": "code",
                "name": "__export_interp_ae.py",
                "sha": "edea3a7058c3b88aa6652e29f05b4ce426a80700",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/blob/main/__export_interp_ae.py"
                    }
                },
                "size": 1960
            },
            {
                "type": "code",
                "name": "__nn_test.py",
                "sha": "9d0b70bcd6e83a88f42b4aba1a55a3f7ac15346d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/blob/main/__nn_test.py"
                    }
                },
                "size": 2467
            },
            {
                "type": "code",
                "name": "__nn_utils.py",
                "sha": "ec16a45f00b1043c7cd7038a14a32c3d8449b859",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/blob/main/__nn_utils.py"
                    }
                },
                "size": 30959
            },
            {
                "type": "code",
                "name": "__nn_utils_ae.py",
                "sha": "4e7a55f55bf383e2ae3ae1e018b74839282c516d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/blob/main/__nn_utils_ae.py"
                    }
                },
                "size": 19622
            },
            {
                "type": "code",
                "name": "__train.py",
                "sha": "2bf405ee886225ef0241d7e5482b7e91a016bfee",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/blob/main/__train.py"
                    }
                },
                "size": 9255
            },
            {
                "type": "code",
                "name": "__train_ae.py",
                "sha": "3816cb4be04e108fb04094b934e8a491b5d30f95",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/blob/main/__train_ae.py"
                    }
                },
                "size": 11400
            },
            {
                "type": "code",
                "name": "audio",
                "sha": "a60218290348ccb400801e41d9954a92e5a2f47c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/tree/main/audio"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "figures",
                "sha": "890cc147bafbb1db6bcc48c9e19d25c83796f3c0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/tree/main/figures"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "gin_configs",
                "sha": "2a6c4706e1f16f99f31a88ae0416ba64def97337",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/tree/main/gin_configs"
                    }
                },
                "num_files": 16
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "c16fa42c2b8e1bdd33104dadedf2df948681e938",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/adrienchaton/BERGAN/blob/main/requirements.txt"
                    }
                },
                "size": 51
            }
        ]
    },
    "authors": [
        {
            "name": "adrienchaton",
            "github_id": "adrienchaton"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/adrienchaton/BERGAN",
            "stars": 1,
            "issues": true,
            "readme": "# BERGAN: music bar generation and techno music with GANs\n\nwork in progress, project submitted to an open call\n\nminimal requirements, can be installed with pip in a python3 virtualenv (pip install -r requirements.txt)\n\nthese requirements do not cover the data preparation pipelines described at the bottom of the readme\n\n**_ current application is generating techno music from latent interpolations**\n\n**--> each latent point maps to a music bar**\n\n**_ next application is \"inpainting\" e.g. generation from loudness or transient curve to audio**\n\n**--> convert acoustic drum loop or hand tapping to techno music bars**\n\n\n## GANs ONLY EXPERIMENT\n\ncode base = __export_interp.py + __nn_utils.py + __train.py\n\nexperiment to make techno music with GANs, default is to train on 2 sec. audio clips at 16kHz = 1 bar 4/4 at 120BPM\n\nunconditional generator and multi-scale discriminators, default working config = WGANGP_2scales_WN_WN_crop0.gin\n\nwithout GP the GANs are very prone to mode collapse/overfitting, also avoid BN discriminator with GP\n\nthe corresponding configs have suffixes = WGAN or LSGAN = wasserstein or least-square\n\n\n## AEs+GANs EXPERIMENT\n\ncode base = __export_interp_ae.py + __nn_utils_ae.py + __train_ae.py\n\ntrain as a AE/GAN (or VAE/GAN or WAE/GAN) to avoid mode collapse of GAN = more stable training without GP and possibly better mode coverage\n\nuse deep feature reconstruction in discriminator activations and optional multi-scale spectrogram reconstruction\n\nVAE/GAN adds the KLD regularization to the encoder and WAE/GAN adds the MMD regularization = both impose a gaussian prior for sampling and interpolation\n\nthe corresponding configs have suffixes = AE_GAN or VAE_GAN (variational ae) or WAE_GAN (wasserstein ae)\n\nthe GAN is only trained as least-square without gradient penalty\n\n\n## TODO\n\ntry training at 32kHz\n\ntry other music genres with 4/4 musical structure\n\nmake a google colab demo (with pretrained models to run in __export_interp.py)\n\n\n## AUDIO SAMPLES (GANs only)\n\nexamples of random linear interpolations with 20 points equally spaced in the generator latent space = 20 bars = 40 sec.\n\ntraining data is between 5.000 and 20.000 examples of bars extracted from recordings of the \"Raster Norton\" label\n\nhttps://raster-media.net (I do not own copyrights, this is an independent research experiment)\n\nmodels were trained for 48 hours on a single V100 GPU (a 12GB GPU is fine too) ; sampling of 40 sec. on Macbook Pro CPU (2015) takes about 3 sec. so the inference speed is reasonable\n\nraw audio outputs of the models at 16kHz --> https://soundcloud.com/adrien-bitton/interpolations\n\n\n## GAN TRAINING\n\noptimize the generator to sample realistic 1 bar audio of 2 sec. (120BPM) at SR=16kHz (extendable to 32kHz or 48kHz)\n\n<p align=\"center\">\n  <img src=\"./figures/bergan_gan_train.jpg\" width=\"750\" title=\"GAN training\">\n</p>\n\n\n## AUDIO SAMPLES (AEs+GANs)\n\nto come\n\n\n## AEs+GANs TRAINING\n\nfigure to come\n\n\n## GENERATION\n\nsample series of 1 bar audio along a random linear interpolation and concatenate the generator outputs into a track at fixed BPM with progressive variation of rhythmic and acoustic contents\n\n<p align=\"center\">\n  <img src=\"./figures/bergan_interp.jpg\" width=\"750\" title=\"generator interpolation\">\n</p>\n\n\n## RELATED PAPERS AND REPOSITORIES\n\nMelGAN\n\nhttps://arxiv.org/abs/1910.06711\n\nhttps://github.com/seungwonpark/melgan\n\nWaveGAN\n\nhttps://arxiv.org/abs/1802.04208\n\nhttps://github.com/mostafaelaraby/wavegan-pytorch\n\nnice review of GAN frameworks\n\nhttps://arxiv.org/abs/1807.04720\n\nthe AEs+GANs framework \n\nhttps://arxiv.org/abs/1512.09300\n\n\n## ACKNOWLEDGEMENTS\n\nthanks to Philippe Esling (https://github.com/acids-ircam) and Thomas Haferlach (https://github.com/voodoohop) for their help in developping the data preparation pipelines\n\ndata preparation aims at extracting music bars aligned on the downbeat and stretching them to the target BPM\n\nwe either rely on python packages (e.g. librosa, madmom) or on parsing warp markers from Ableton .asd files (https://github.com/voodoohop/extract-warpmarkers)\n\nthanks as well to Antoine Caillon (https://github.com/caillonantoine) for insightful discussion on the challenges of training GANs\n\nand thanks to IRCAM and Compute Canada for the allowed computation ressources for training models\n",
            "readme_url": "https://github.com/adrienchaton/BERGAN",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis",
            "arxiv": "1910.06711",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.06711v3",
            "abstract": "Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that\ngenerating coherent raw audio waveforms with GANs is challenging. In this\npaper, we show that it is possible to train GANs reliably to generate high\nquality coherent waveforms by introducing a set of architectural changes and\nsimple training techniques. Subjective evaluation metric (Mean Opinion Score,\nor MOS) shows the effectiveness of the proposed approach for high quality\nmel-spectrogram inversion. To establish the generality of the proposed\ntechniques, we show qualitative results of our model in speech synthesis, music\ndomain translation and unconditional music synthesis. We evaluate the various\ncomponents of the model through ablation studies and suggest a set of\nguidelines to design general purpose discriminators and generators for\nconditional sequence synthesis tasks. Our model is non-autoregressive, fully\nconvolutional, with significantly fewer parameters than competing models and\ngeneralizes to unseen speakers for mel-spectrogram inversion. Our pytorch\nimplementation runs at more than 100x faster than realtime on GTX 1080Ti GPU\nand more than 2x faster than real-time on CPU, without any hardware specific\noptimization tricks.",
            "authors": [
                "Kundan Kumar",
                "Rithesh Kumar",
                "Thibault de Boissiere",
                "Lucas Gestin",
                "Wei Zhen Teoh",
                "Jose Sotelo",
                "Alexandre de Brebisson",
                "Yoshua Bengio",
                "Aaron Courville"
            ]
        },
        {
            "title": "Autoencoding beyond pixels using a learned similarity metric",
            "arxiv": "1512.09300",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.09300v2",
            "abstract": "We present an autoencoder that leverages learned representations to better\nmeasure similarities in data space. By combining a variational autoencoder with\na generative adversarial network we can use learned feature representations in\nthe GAN discriminator as basis for the VAE reconstruction objective. Thereby,\nwe replace element-wise errors with feature-wise errors to better capture the\ndata distribution while offering invariance towards e.g. translation. We apply\nour method to images of faces and show that it outperforms VAEs with\nelement-wise similarity measures in terms of visual fidelity. Moreover, we show\nthat the method learns an embedding in which high-level abstract visual\nfeatures (e.g. wearing glasses) can be modified using simple arithmetic.",
            "authors": [
                "Anders Boesen Lindbo Larsen",
                "S\u00f8ren Kaae S\u00f8nderby",
                "Hugo Larochelle",
                "Ole Winther"
            ]
        },
        {
            "title": "Adversarial Audio Synthesis",
            "arxiv": "1802.04208",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.04208v3",
            "abstract": "Audio signals are sampled at high temporal resolutions, and learning to\nsynthesize audio requires capturing structure across a range of timescales.\nGenerative adversarial networks (GANs) have seen wide success at generating\nimages that are both locally and globally coherent, but they have seen little\napplication to audio generation. In this paper we introduce WaveGAN, a first\nattempt at applying GANs to unsupervised synthesis of raw-waveform audio.\nWaveGAN is capable of synthesizing one second slices of audio waveforms with\nglobal coherence, suitable for sound effect generation. Our experiments\ndemonstrate that, without labels, WaveGAN learns to produce intelligible words\nwhen trained on a small-vocabulary speech dataset, and can also synthesize\naudio from other domains such as drums, bird vocalizations, and piano. We\ncompare WaveGAN to a method which applies GANs designed for image generation on\nimage-like audio feature representations, finding both approaches to be\npromising.",
            "authors": [
                "Chris Donahue",
                "Julian McAuley",
                "Miller Puckette"
            ]
        },
        {
            "title": "A Large-Scale Study on Regularization and Normalization in GANs",
            "arxiv": "1807.04720",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.04720v3",
            "abstract": "Generative adversarial networks (GANs) are a class of deep generative models\nwhich aim to learn a target distribution in an unsupervised fashion. While they\nwere successfully applied to many problems, training a GAN is a notoriously\nchallenging task and requires a significant number of hyperparameter tuning,\nneural architecture engineering, and a non-trivial amount of \"tricks\". The\nsuccess in many practical applications coupled with the lack of a measure to\nquantify the failure modes of GANs resulted in a plethora of proposed losses,\nregularization and normalization schemes, as well as neural architectures. In\nthis work we take a sober view of the current state of GANs from a practical\nperspective. We discuss and evaluate common pitfalls and reproducibility\nissues, open-source our code on Github, and provide pre-trained models on\nTensorFlow Hub.",
            "authors": [
                "Karol Kurach",
                "Mario Lucic",
                "Xiaohua Zhai",
                "Marcin Michalski",
                "Sylvain Gelly"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9945847657280575,
        "task": "Image Generation",
        "task_prob": 0.9605825645262218
    },
    "training": {
        "datasets": [
            {
                "name": "Music domain"
            }
        ]
    }
}