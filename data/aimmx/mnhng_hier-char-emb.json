{
    "visibility": {
        "visibility": "public",
        "license": "BSD 3-Clause \"New\" or \"Revised\" License"
    },
    "name": "Hierarchical Character Embeddings",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "mnhng",
                "owner_type": "User",
                "name": "hier-char-emb",
                "url": "https://github.com/mnhng/hier-char-emb",
                "stars": 1,
                "pushed_at": "2020-12-14 02:00:04+00:00",
                "created_at": "2020-12-14 01:58:56+00:00",
                "language": "Python",
                "license": "BSD 3-Clause \"New\" or \"Revised\" License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "c18dd8d83ceed1806b50b0aaa46beb7e335fff13",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/.gitignore"
                    }
                },
                "size": 13
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "09d493bf1fc257505c1336f3f87425568ab9da3c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/LICENSE"
                    }
                },
                "size": 1500
            },
            {
                "type": "code",
                "name": "data.py",
                "sha": "6a3aa7f02ad722c3b2e51df7132a441e7125a772",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/data.py"
                    }
                },
                "size": 1628
            },
            {
                "type": "code",
                "name": "data",
                "sha": "96fa0073f40924fe5c1c32e49516c17c97e7f02d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/tree/master/data"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "env.yml",
                "sha": "daf3783c497bc40a98495f7626a5171d31277d0b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/env.yml"
                    }
                },
                "size": 119
            },
            {
                "type": "code",
                "name": "example_hier_emb.py",
                "sha": "84c45ff18ed024d2aa923fb9b7bcfee1ecf40666",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/example_hier_emb.py"
                    }
                },
                "size": 302
            },
            {
                "type": "code",
                "name": "example_lm_hier_emb.sh",
                "sha": "1a396c985d91cfb47db58a06bad770b18b57e8bd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/example_lm_hier_emb.sh"
                    }
                },
                "size": 315
            },
            {
                "type": "code",
                "name": "example_lm_norm_emb.sh",
                "sha": "3ba64ed7a428f7e0c221d1ce0524576e39ed3f81",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/example_lm_norm_emb.sh"
                    }
                },
                "size": 315
            },
            {
                "type": "code",
                "name": "locked_dropout.py",
                "sha": "e1ff71eb9162e0d870a1d39660ed9353f37ad0aa",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/locked_dropout.py"
                    }
                },
                "size": 442
            },
            {
                "type": "code",
                "name": "main_hier.py",
                "sha": "3c7786085a7a3da9790bc41e539ff7e14926fa25",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/main_hier.py"
                    }
                },
                "size": 12178
            },
            {
                "type": "code",
                "name": "main_norm.py",
                "sha": "0c1d48de66eb40513ebea21d623e79cce8573b60",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/main_norm.py"
                    }
                },
                "size": 13158
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "ed6d5cb4aca20cd60c3e061c9af1541da64665d6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/model.py"
                    }
                },
                "size": 3182
            },
            {
                "type": "code",
                "name": "nnblk",
                "sha": "3cf20a26d237378655b440a54c2f59e1fff4c409",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/tree/master/nnblk"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "splitcross.py",
                "sha": "cda17dd8870de1418922f9daf951f001a8313c1f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/splitcross.py"
                    }
                },
                "size": 9995
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "20f70ec8be56d25db8f768a3e72f089818787457",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mnhng/hier-char-emb/blob/master/utils.py"
                    }
                },
                "size": 904
            }
        ]
    },
    "authors": [
        {
            "name": "mnhng",
            "github_id": "mnhng"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/mnhng/hier-char-emb",
            "stars": 1,
            "issues": true,
            "readme": "# Hierarchical Character Embeddings\n\nHierarchical character embeddings exploit recursive structures of Chinese characters to construct better characters' representation.\nRecursive structures of Chinese characters are obtained from the characters themselves using a rule-based parser.\nThe rule-based parser data is from [IDS data repository](https://github.com/cjkvi/cjkvi-ids) which is based on the from [CHISE](http://www.chise.org/) project.\nBoth Simplified and Traditional Chinese input are supported.\n\nIf you use this code, please cite as appropriate:\n\n```\n@article{nguyen2019hierarchical,\n  title={Hierarchical Character Embeddings: Learning Phonological and Semantic Representations in Languages of Logographic Origin Using Recursive Neural Networks},\n  author={Nguyen, Minh and Ngo, Gia H and Chen, Nancy F},\n  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n  year={2019}\n  publisher={IEEE}\n}\n```\n\n## Requirements\n\nPython and PyTorch are required for the current codebase.\nTo setup the required environment\n\n1. Install Anaconda\n2. Run `conda env create -f env.yml -n hier_emb`\n\n\n\n## Examples:\n\n### Usage\n\n```Python\nimport torch\nfrom nnblk import HierarchicalEmbedding\n\nchar2index = {'\u767d': 0, '\u5c71': 1, '\u540d': 2, '\u98a8': 3}\nemb = HierarchicalEmbedding(num_embeddings=len(char2index), embedding_dim=4, char2index=char2index)\ninput_ = torch.LongTensor(list(char2index.values()))\nprint(emb(input_))\n```\n\n### Language modeling with Hierarchical Character Embedding\n\nTry out the Chinese language modeling example by running `./example_lm_hier_emb.sh`  \nThe language model (AWD-LSTM-LM) used in the example is described in these two papers:\n\n+ [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182)\n+ [An Analysis of Neural Language Modeling at Multiple Scales](https://arxiv.org/abs/1803.08240)\n\n",
            "readme_url": "https://github.com/mnhng/hier-char-emb",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "An Analysis of Neural Language Modeling at Multiple Scales",
            "arxiv": "1803.08240",
            "year": 2018,
            "url": "http://arxiv.org/abs/1803.08240v1",
            "abstract": "Many of the leading approaches in language modeling introduce novel, complex\nand specialized architectures. We take existing state-of-the-art word level\nlanguage models based on LSTMs and QRNNs and extend them to both larger\nvocabularies as well as character-level granularity. When properly tuned, LSTMs\nand QRNNs achieve state-of-the-art results on character-level (Penn Treebank,\nenwik8) and word-level (WikiText-103) datasets, respectively. Results are\nobtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single\nmodern GPU.",
            "authors": [
                "Stephen Merity",
                "Nitish Shirish Keskar",
                "Richard Socher"
            ]
        },
        {
            "title": "Regularizing and Optimizing LSTM Language Models",
            "arxiv": "1708.02182",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02182v1",
            "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.",
            "authors": [
                "Stephen Merity",
                "Nitish Shirish Keskar",
                "Richard Socher"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999997539476605,
        "task": "Language Modelling",
        "task_prob": 0.9087066822742843
    },
    "training": {
        "datasets": [
            {
                "name": "Penn Treebank"
            },
            {
                "name": "WikiText-2"
            },
            {
                "name": "WikiText-103"
            }
        ]
    }
}