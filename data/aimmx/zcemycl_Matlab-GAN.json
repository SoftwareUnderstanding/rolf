{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Matlab-GAN ![License: MIT](https://opensource.org/licenses/MIT) [![View Matlab-GAN on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://www.mathworks.com/matlabcentral/fileexchange/74865-matlab-gan)",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "zcemycl",
                "owner_type": "User",
                "name": "Matlab-GAN",
                "url": "https://github.com/zcemycl/Matlab-GAN",
                "stars": 96,
                "pushed_at": "2021-03-19 00:16:59+00:00",
                "created_at": "2019-11-20 23:35:42+00:00",
                "language": "MATLAB",
                "description": "MATLAB implementations of Generative Adversarial Networks -- from GAN to Pixel2Pixel, CycleGAN",
                "license": "MIT License",
                "frameworks": []
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "dfe0770424b2a19faf507a501ebfc23be8f54e7b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/blob/master/.gitattributes"
                    }
                },
                "size": 66
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "20377d5ced56af1e710d41c6f2f32d5af8ae9234",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/blob/master/.gitignore"
                    }
                },
                "size": 21
            },
            {
                "type": "code",
                "name": "AAE",
                "sha": "c79eb4a605d789596ee9c7ca72786fad46409861",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/AAE"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "ACGAN",
                "sha": "2391941cd17a6e828ef8cf5d6353e24b664e7cdc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/ACGAN"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "CGAN",
                "sha": "a52554ab23894016726be45db2abc4b9733bd3c3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/CGAN"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "CycleGAN",
                "sha": "9bedf711fa83917e1731f1b6b9a090afc34372c6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/CycleGAN"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "DCGAN",
                "sha": "dd6a80a4f2e735896c676540c6965d9af30bcb9a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/DCGAN"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "GAN",
                "sha": "089f866077dc937996d181a1bd8e569ec3a3bd72",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/GAN"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "InfoGAN",
                "sha": "fb702c4246a208f53d7dce8f2ecc2dda290e0cff",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/InfoGAN"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "1f44985a9835ba9c403329c7437d84d89aceca4a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/blob/master/LICENSE"
                    }
                },
                "size": 1071
            },
            {
                "type": "code",
                "name": "LSGAN",
                "sha": "37c0a21823999361d1e737ade9d0b2f9a257d2cf",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/LSGAN"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "Notes.ipynb",
                "sha": "aebd1b8ec52c3851952346c80524056122dc1be0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/blob/master/Notes.ipynb"
                    }
                },
                "size": 6361
            },
            {
                "type": "code",
                "name": "Pix2Pix",
                "sha": "9abc94369afa6780a75830ba52c0e98df0128c63",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/Pix2Pix"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "SGAN",
                "sha": "1cb278aba1681f7456b3227cf19ad97d344b832c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/SGAN"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "WGAN",
                "sha": "24fe0937bb75ea1b15230de3b2277322586c6560",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/zcemycl/Matlab-GAN/tree/master/WGAN"
                    }
                },
                "num_files": 4
            }
        ]
    },
    "authors": [
        {
            "name": "Yui Chun Leung",
            "github_id": "zcemycl"
        }
    ],
    "tags": [
        "matlab-gan",
        "infogan",
        "cyclegan",
        "pix2pix",
        "gans",
        "image-generation",
        "deep-learning",
        "deep-neural-networks",
        "computer-vision",
        "lsgan",
        "dcgan",
        "aae",
        "acgan",
        "cgan",
        "matlab-implementations",
        "matlab"
    ],
    "description": "MATLAB implementations of Generative Adversarial Networks -- from GAN to Pixel2Pixel, CycleGAN",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/zcemycl/Matlab-GAN",
            "stars": 96,
            "issues": true,
            "readme": "# Matlab-GAN [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![View Matlab-GAN on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://www.mathworks.com/matlabcentral/fileexchange/74865-matlab-gan)\nCollection of MATLAB implementations of Generative Adversarial Networks (GANs) suggested in research papers. This repository is greatly inspired by eriklindernoren's repositories [Keras-GAN](https://github.com/eriklindernoren/Keras-GAN) and [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN), and contains codes to investigate different architectures of GAN models. \n\n## Configuration\nTo run the following codes, users should have the following packages,\n- MATLAB 2019b\n- Deep Learning Toolbox\n- Parallel Computing Toolbox (optional for GPU usage)\n\n## Datasets\n- [Facade Dataset](http://cmp.felk.cvut.cz/~tylecr1/facade/)\n- [Apple2Orange Dataset](http://www.image-net.org/)\n- [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)\n- [Mnist Dataset](http://yann.lecun.com/exdb/mnist/)\n\n## Table of Contents\n+ **G**enerative **A**dversarial **N**etwork (GAN) [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/GAN/GAN.m) [[paper]](https://arxiv.org/abs/1406.2661) \n+ **L**east **S**quares **G**enerative **A**dversarial **N**etwork (LSGAN) [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/LSGAN/LSGAN.m) [[paper]](https://arxiv.org/abs/1611.04076)\n+ **D**eep **C**onvolutional **G**enerative **A**dversarial **N**etwork (DCGAN) [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/DCGAN/DCGAN.m) [[paper]](https://arxiv.org/abs/1511.06434)\n+ **C**onditional **G**enerative **A**dversarial **N**etwork (CGAN) [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/CGAN/CGAN.m) [[paper]](https://arxiv.org/abs/1611.06430)\n+ **A**uxiliary **C**lassifier **G**enerative **A**dversarial **N**etwork (ACGAN) [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/ACGAN/ACGAN.m) [[paper]](https://arxiv.org/abs/1610.09585)\n+ InfoGAN [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/InfoGAN/InfoGAN.m) [[paper]](https://arxiv.org/abs/1606.03657)\n+ **A**dversarial **A**uto**E**ncoder (AAE) [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/AAE/AAE.m) [[paper]](https://arxiv.org/abs/1511.05644)\n+ Pix2Pix [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/Pix2Pix/PIX2PIX.m) [[paper]](https://arxiv.org/abs/1611.07004)\n+ **W**asserstein **G**enerative **A**dversarial **N**etwork (WGAN) [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/WGAN/WGAN.m) [[paper]](https://arxiv.org/abs/1701.07875)\n+ **S**emi-Supervised **G**enerative **A**dversarial **N**etwork (SGAN) [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/SGAN/SGAN.m) [[paper]](https://arxiv.org/abs/1606.01583)\n+ CycleGAN [[code]](https://github.com/zcemycl/Matlab-GAN/blob/master/CycleGAN/CycleGAN.m) [[paper]](https://arxiv.org/abs/1703.10593)\n+ DiscoGAN [[paper]](https://arxiv.org/abs/1703.05192)\n\n## Outputs\nGAN <br>-Generator, Discriminator|  LSGAN <br>-Least Squares Loss | DCGAN <br>-Deep Convolutional Layer | CGAN <br>-Condition Embedding\n:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n<img src=\"GAN/GANmnist.gif\" width=\"200\" > |<img src=\"LSGAN/LSGANresult.jpg\" width=\"200\" >|<img src=\"DCGAN/DCGANmnist.gif\" width=\"200\" >|<img src=\"CGAN/CGANmnist.gif\" width=\"200\" >\nACGAN <br>-Classification|InfoGAN mnist <br>-Continuous, Discrete Codes|AAE <br>-Encoder, Decoder, Discriminator|Pix2Pix <br>-Pair and Segments checking <br>-Decovolution and Skip Connections\n<img src=\"ACGAN/ACGANresult.jpg\" width=\"200\"> |<img src=\"InfoGAN/InfoGANmnist.gif\" width=\"200\" >|<img src=\"AAE/AAEmnist.gif\" width=\"200\">|<img src=\"Pix2Pix/p2pfacade.gif\" width=\"200\">\nWGAN |SGAN|CycleGAN <br>-Instance Normalization <br>-Mutli-agent Learning|InfoGAN CelebA\n<img src=\"WGAN/resultepoch7.jpg\" width=\"200\">|<img src=\"SGAN/SGANepoch7.jpg\" width=\"200\">|<img src=\"CycleGAN/CycleGAN.gif\" width=\"200\">|<img src=\"InfoGAN/InfoGANcelebA.gif\" width=\"200\">\n\n## References\n- Y. LeCun and C. Cortes, \u201cMNIST handwritten digitdatabase,\u201d 2010. [MNIST]\n- J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, andL. Fei-Fei, \u201cImageNet: A Large-Scale Hierarchical Image Database,\u201d inCVPR09, 2009. [Apple2Orange (ImageNet)]\n- R. Tyle\u010dek and R. \u0160\u00e1ra, \u201cSpatial pattern templates forrecognition of objects with regular structure,\u201d inProc.GCPR, (Saarbrucken, Germany), 2013. [Facade]\n- Z. Liu, P. Luo, X. Wang, and X. Tang, \u201cDeep learn-ing face attributes in the wild,\u201d inProceedings of In-ternational Conference on Computer Vision (ICCV),December 2015. [CelebA]\n- Goodfellow, Ian J. et al. \u201cGenerative Adversarial Networks.\u201d ArXiv abs/1406.2661 (2014): n. pag. (GAN)\n- Radford, Alec et al. \u201cUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.\u201d CoRR abs/1511.06434 (2015): n. pag. (DCGAN)\n- Denton, Emily L. et al. \u201cSemi-Supervised Learning with Context-Conditional Generative Adversarial Networks.\u201d ArXiv abs/1611.06430 (2017): n. pag. (CGAN)\n- Odena, Augustus et al. \u201cConditional Image Synthesis with Auxiliary Classifier GANs.\u201d ICML (2016). (ACGAN)\n- Chen, Xi et al. \u201cInfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.\u201d NIPS (2016). (InfoGAN)\n- Makhzani, Alireza et al. \u201cAdversarial Autoencoders.\u201d ArXiv abs/1511.05644 (2015): n. pag. (AAE)\n- Isola, Phillip et al. \u201cImage-to-Image Translation with Conditional Adversarial Networks.\u201d 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016): 5967-5976. (Pix2Pix)\n- J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \u201cUnpairedimage-to-image translation using cycle-consistent ad-versarial networks,\u201d 2017. (CycleGAN)\n- Arjovsky, Mart\u00edn et al. \u201cWasserstein GAN.\u201d ArXiv abs/1701.07875 (2017): n. pag. (WGAN)\n- Odena, Augustus. \u201cSemi-Supervised Learning with Generative Adversarial Networks.\u201d ArXiv abs/1606.01583 (2016): n. pag. (SGAN)\n",
            "readme_url": "https://github.com/zcemycl/Matlab-GAN",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Wasserstein GAN",
            "arxiv": "1701.07875",
            "year": 2017,
            "url": "http://arxiv.org/abs/1701.07875v3",
            "abstract": "We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions.",
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ]
        },
        {
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "arxiv": "1703.10593",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10593v7",
            "abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.",
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A. Efros"
            ]
        },
        {
            "title": "Semi-Supervised Learning with Generative Adversarial Networks",
            "arxiv": "1606.01583",
            "year": 2016,
            "url": "http://arxiv.org/abs/1606.01583v2",
            "abstract": "We extend Generative Adversarial Networks (GANs) to the semi-supervised\ncontext by forcing the discriminator network to output class labels. We train a\ngenerative model G and a discriminator D on a dataset with inputs belonging to\none of N classes. At training time, D is made to predict which of N+1 classes\nthe input belongs to, where an extra class is added to correspond to the\noutputs of G. We show that this method can be used to create a more\ndata-efficient classifier and that it allows for generating higher quality\nsamples than a regular GAN.",
            "authors": [
                "Augustus Odena"
            ]
        },
        {
            "title": "Generative Adversarial Networks",
            "arxiv": "1406.2661",
            "year": 2014,
            "url": "http://arxiv.org/abs/1406.2661v1",
            "abstract": "We propose a new framework for estimating generative models via an\nadversarial process, in which we simultaneously train two models: a generative\nmodel G that captures the data distribution, and a discriminative model D that\nestimates the probability that a sample came from the training data rather than\nG. The training procedure for G is to maximize the probability of D making a\nmistake. This framework corresponds to a minimax two-player game. In the space\nof arbitrary functions G and D, a unique solution exists, with G recovering the\ntraining data distribution and D equal to 1/2 everywhere. In the case where G\nand D are defined by multilayer perceptrons, the entire system can be trained\nwith backpropagation. There is no need for any Markov chains or unrolled\napproximate inference networks during either training or generation of samples.\nExperiments demonstrate the potential of the framework through qualitative and\nquantitative evaluation of the generated samples.",
            "authors": [
                "Ian J. Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ]
        },
        {
            "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets",
            "arxiv": "1606.03657",
            "year": 2016,
            "url": "http://arxiv.org/abs/1606.03657v1",
            "abstract": "This paper describes InfoGAN, an information-theoretic extension to the\nGenerative Adversarial Network that is able to learn disentangled\nrepresentations in a completely unsupervised manner. InfoGAN is a generative\nadversarial network that also maximizes the mutual information between a small\nsubset of the latent variables and the observation. We derive a lower bound to\nthe mutual information objective that can be optimized efficiently, and show\nthat our training procedure can be interpreted as a variation of the Wake-Sleep\nalgorithm. Specifically, InfoGAN successfully disentangles writing styles from\ndigit shapes on the MNIST dataset, pose from lighting of 3D rendered images,\nand background digits from the central digit on the SVHN dataset. It also\ndiscovers visual concepts that include hair styles, presence/absence of\neyeglasses, and emotions on the CelebA face dataset. Experiments show that\nInfoGAN learns interpretable representations that are competitive with\nrepresentations learned by existing fully supervised methods.",
            "authors": [
                "Xi Chen",
                "Yan Duan",
                "Rein Houthooft",
                "John Schulman",
                "Ilya Sutskever",
                "Pieter Abbeel"
            ]
        },
        {
            "title": "Adversarial Autoencoders",
            "arxiv": "1511.05644",
            "year": 2015,
            "url": "http://arxiv.org/abs/1511.05644v2",
            "abstract": "In this paper, we propose the \"adversarial autoencoder\" (AAE), which is a\nprobabilistic autoencoder that uses the recently proposed generative\nadversarial networks (GAN) to perform variational inference by matching the\naggregated posterior of the hidden code vector of the autoencoder with an\narbitrary prior distribution. Matching the aggregated posterior to the prior\nensures that generating from any part of prior space results in meaningful\nsamples. As a result, the decoder of the adversarial autoencoder learns a deep\ngenerative model that maps the imposed prior to the data distribution. We show\nhow the adversarial autoencoder can be used in applications such as\nsemi-supervised classification, disentangling style and content of images,\nunsupervised clustering, dimensionality reduction and data visualization. We\nperformed experiments on MNIST, Street View House Numbers and Toronto Face\ndatasets and show that adversarial autoencoders achieve competitive results in\ngenerative modeling and semi-supervised classification tasks.",
            "authors": [
                "Alireza Makhzani",
                "Jonathon Shlens",
                "Navdeep Jaitly",
                "Ian Goodfellow",
                "Brendan Frey"
            ]
        },
        {
            "title": "Image-to-Image Translation with Conditional Adversarial Networks",
            "arxiv": "1611.07004",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.07004v3",
            "abstract": "We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either.",
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A. Efros"
            ]
        },
        {
            "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "arxiv": "1511.06434",
            "year": 2015,
            "url": "http://arxiv.org/abs/1511.06434v2",
            "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations.",
            "authors": [
                "Alec Radford",
                "Luke Metz",
                "Soumith Chintala"
            ]
        },
        {
            "title": "Least Squares Generative Adversarial Networks",
            "arxiv": "1611.04076",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.04076v3",
            "abstract": "Unsupervised learning with generative adversarial networks (GANs) has proven\nhugely successful. Regular GANs hypothesize the discriminator as a classifier\nwith the sigmoid cross entropy loss function. However, we found that this loss\nfunction may lead to the vanishing gradients problem during the learning\nprocess. To overcome such a problem, we propose in this paper the Least Squares\nGenerative Adversarial Networks (LSGANs) which adopt the least squares loss\nfunction for the discriminator. We show that minimizing the objective function\nof LSGAN yields minimizing the Pearson $\\chi^2$ divergence. There are two\nbenefits of LSGANs over regular GANs. First, LSGANs are able to generate higher\nquality images than regular GANs. Second, LSGANs perform more stable during the\nlearning process. We evaluate LSGANs on five scene datasets and the\nexperimental results show that the images generated by LSGANs are of better\nquality than the ones generated by regular GANs. We also conduct two comparison\nexperiments between LSGANs and regular GANs to illustrate the stability of\nLSGANs.",
            "authors": [
                "Xudong Mao",
                "Qing Li",
                "Haoran Xie",
                "Raymond Y. K. Lau",
                "Zhen Wang",
                "Stephen Paul Smolley"
            ]
        },
        {
            "title": "Conditional Image Synthesis With Auxiliary Classifier GANs",
            "arxiv": "1610.09585",
            "year": 2016,
            "url": "http://arxiv.org/abs/1610.09585v4",
            "abstract": "Synthesizing high resolution photorealistic images has been a long-standing\nchallenge in machine learning. In this paper we introduce new methods for the\nimproved training of generative adversarial networks (GANs) for image\nsynthesis. We construct a variant of GANs employing label conditioning that\nresults in 128x128 resolution image samples exhibiting global coherence. We\nexpand on previous work for image quality assessment to provide two new\nanalyses for assessing the discriminability and diversity of samples from\nclass-conditional image synthesis models. These analyses demonstrate that high\nresolution samples provide class information not present in low resolution\nsamples. Across 1000 ImageNet classes, 128x128 samples are more than twice as\ndiscriminable as artificially resized 32x32 samples. In addition, 84.7% of the\nclasses have samples exhibiting diversity comparable to real ImageNet data.",
            "authors": [
                "Augustus Odena",
                "Christopher Olah",
                "Jonathon Shlens"
            ]
        },
        {
            "title": "Learning to Discover Cross-Domain Relations with Generative Adversarial Networks",
            "arxiv": "1703.05192",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.05192v2",
            "abstract": "While humans easily recognize relations between data from different domains\nwithout any supervision, learning to automatically discover them is in general\nvery challenging and needs many ground-truth pairs that illustrate the\nrelations. To avoid costly pairing, we address the task of discovering\ncross-domain relations given unpaired data. We propose a method based on\ngenerative adversarial networks that learns to discover relations between\ndifferent domains (DiscoGAN). Using the discovered relations, our proposed\nnetwork successfully transfers style from one domain to another while\npreserving key attributes such as orientation and face identity. Source code\nfor official implementation is publicly available\nhttps://github.com/SKTBrain/DiscoGAN",
            "authors": [
                "Taeksoo Kim",
                "Moonsu Cha",
                "Hyunsoo Kim",
                "Jung Kwon Lee",
                "Jiwon Kim"
            ]
        },
        {
            "title": "Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks",
            "arxiv": "1611.06430",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.06430v1",
            "abstract": "We introduce a simple semi-supervised learning approach for images based on\nin-painting using an adversarial loss. Images with random patches removed are\npresented to a generator whose task is to fill in the hole, based on the\nsurrounding pixels. The in-painted images are then presented to a discriminator\nnetwork that judges if they are real (unaltered training images) or not. This\ntask acts as a regularizer for standard supervised training of the\ndiscriminator. Using our approach we are able to directly train large VGG-style\nnetworks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL\ndatasets, where our approach obtains performance comparable or superior to\nexisting methods.",
            "authors": [
                "Emily Denton",
                "Sam Gross",
                "Rob Fergus"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Facade Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://cmp.felk.cvut.cz/~tylecr1/facade/"
                    }
                }
            },
            {
                "name": "Apple2Orange Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://www.image-net.org/"
                    }
                }
            },
            {
                "name": "CelebA Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"
                    }
                }
            },
            {
                "name": "Mnist Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://yann.lecun.com/exdb/mnist/"
                    }
                }
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "CUHK"
            },
            {
                "name": "MNIST"
            },
            {
                "name": "CelebA"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "STL-10"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.999999991903206,
        "task": "Image Generation",
        "task_prob": 0.5550442234328529
    }
}