{
    "visibility": {
        "visibility": "public"
    },
    "name": "Face Attention Network",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "rainofmine",
                "owner_type": "User",
                "name": "Face_Attention_Network",
                "url": "https://github.com/rainofmine/Face_Attention_Network",
                "stars": 305,
                "pushed_at": "2019-01-12 17:40:24+00:00",
                "created_at": "2018-10-20 05:22:06+00:00",
                "language": "Python",
                "description": "Pytorch implementation of face attention network",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".idea",
                "sha": "1712994f3aecaf85fab7c5f4d900bf1e35f13489",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rainofmine/Face_Attention_Network/tree/master/.idea"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "anchors.py",
                "sha": "4f255984c93bcbd06093d364223dd436b02298e5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rainofmine/Face_Attention_Network/blob/master/anchors.py"
                    }
                },
                "size": 4285
            },
            {
                "type": "code",
                "name": "csv_eval.py",
                "sha": "2969868a5cb515ef084584d13bb3e8bc42f55810",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rainofmine/Face_Attention_Network/blob/master/csv_eval.py"
                    }
                },
                "size": 9289
            },
            {
                "type": "code",
                "name": "dataloader.py",
                "sha": "ac9d67f2a85584ab57efe37f0bdac960382f2e3d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rainofmine/Face_Attention_Network/blob/master/dataloader.py"
                    }
                },
                "size": 16560
            },
            {
                "type": "code",
                "name": "img",
                "sha": "00a1ce477b396d4f5b1e16984e7d7dc8b3abd336",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rainofmine/Face_Attention_Network/tree/master/img"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "lib",
                "sha": "4c2dd54ad306107e07c9d966e894f80490824b71",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rainofmine/Face_Attention_Network/tree/master/lib"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "losses.py",
                "sha": "f41e7b85423bfcb3dfe1b4d581d6c062591ca249",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rainofmine/Face_Attention_Network/blob/master/losses.py"
                    }
                },
                "size": 9514
            },
            {
                "type": "code",
                "name": "model_level_attention.py",
                "sha": "9c485ecf35613ce93ae83128744294e5eb352ee4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rainofmine/Face_Attention_Network/blob/master/model_level_attention.py"
                    }
                },
                "size": 13556
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "a04f7901abf17d685a5ddaa55ca659731a35017d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rainofmine/Face_Attention_Network/blob/master/train.py"
                    }
                },
                "size": 6653
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "fb1fe2c8f8d244b13b5e719204611940990d5ffd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/rainofmine/Face_Attention_Network/blob/master/utils.py"
                    }
                },
                "size": 8656
            }
        ]
    },
    "authors": [
        {
            "name": "Hooks",
            "github_id": "rainofmine"
        }
    ],
    "tags": [],
    "description": "Pytorch implementation of face attention network",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/rainofmine/Face_Attention_Network",
            "stars": 305,
            "issues": true,
            "readme": "# Face Attention Network\n\nPytorch implementation of face attention network as described in [Face Attention Network: An Effective Face Detector for the Occluded Faces](https://arxiv.org/abs/1711.07246). The baseline is RetinaNet followed by this [repo](https://github.com/yhenon/pytorch-retinanet).\n\n![img1](https://github.com/rainofmine/face_attention_network/blob/master/img/1.png)\n\n## Requirements\n\n- Python3\n- Pytorch0.4\n- torchvision\n- tensorboardX\n\n## Installation\n\nInstall packages.\n\n```\nsudo apt-get install tk-dev python-tk\npip install cffi\npip install cython\npip install pandas\npip install tensorboardX\n```\n\nBuild NMS.\n\n```\ncd Face_Attention_Network/lib\nsh build.sh\n```\n\nCreate folders.\n\n```\ncd Face_Attention_Network/\nmkdir ckpt mAP_txt summary weight\n```\n\n## Datasets\nYou should prepare three CSV or TXT files including train annotations file, valid annotations file and label encoding file. \n\n### Annotations format\nTwo examples are as follows:\n\n```\n$image_path/img_1.jpg x1 y1 x2 y2 label\n$image_path/img_2.jpg . . . . .\n```\n\nImages with more than one bounding box should use one row per box. When an image does not contain any bounding box, set them '.'. \n\n### Label encoding file\nA TXT file (classes.txt) is needed to map label to ID. Each line means one label name and its ID. One example is as follows:\n\n```\nface 0\n```\n\n## Pretrained Model\n\nWe use resnet18, 34, 50, 101, 152 as the backbone. You should download them and put them to `/weight`.\n\n- resnet18: [https://download.pytorch.org/models/resnet18-5c106cde.pth](https://download.pytorch.org/models/resnet18-5c106cde.pth)\n- resnet34: [https://download.pytorch.org/models/resnet34-333f7ec4.pth](https://download.pytorch.org/models/resnet34-333f7ec4.pth)\n- resnet50: [https://download.pytorch.org/models/resnet50-19c8e357.pth](https://download.pytorch.org/models/resnet50-19c8e357.pth)\n- resnet101: [https://download.pytorch.org/models/resnet101-5d3b4d8f.pth](https://download.pytorch.org/models/resnet101-5d3b4d8f.pth)\n- resnet152: [https://download.pytorch.org/models/resnet152-b121ed2d.pth](https://download.pytorch.org/models/resnet152-b121ed2d.pth)\n\n## Training\n\n```\npython train.py --csv_train <$path/train.txt> --csv_val <$path/val.txt> --csv_classes <$path/classes.txt> --depth <50> --pretrained resnet50-19c8e357.pth --model_name <model name to save>\n```\n\n## Visualization Result\nDetection result\n\n![img2](https://github.com/rainofmine/face_attention_network/blob/master/img/2.png)\n\nAttention map at different level (P3~P7)\n\n![img3](https://github.com/rainofmine/face_attention_network/blob/master/img/3.png)\n\n## Reference\n\n- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n- [Face Attention Network: An Effective Face Detector for the Occluded Faces](https://arxiv.org/abs/1711.07246)",
            "readme_url": "https://github.com/rainofmine/Face_Attention_Network",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Focal Loss for Dense Object Detection",
            "arxiv": "1708.02002",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02002v2",
            "abstract": "The highest accuracy object detectors to date are based on a two-stage\napproach popularized by R-CNN, where a classifier is applied to a sparse set of\ncandidate object locations. In contrast, one-stage detectors that are applied\nover a regular, dense sampling of possible object locations have the potential\nto be faster and simpler, but have trailed the accuracy of two-stage detectors\nthus far. In this paper, we investigate why this is the case. We discover that\nthe extreme foreground-background class imbalance encountered during training\nof dense detectors is the central cause. We propose to address this class\nimbalance by reshaping the standard cross entropy loss such that it\ndown-weights the loss assigned to well-classified examples. Our novel Focal\nLoss focuses training on a sparse set of hard examples and prevents the vast\nnumber of easy negatives from overwhelming the detector during training. To\nevaluate the effectiveness of our loss, we design and train a simple dense\ndetector we call RetinaNet. Our results show that when trained with the focal\nloss, RetinaNet is able to match the speed of previous one-stage detectors\nwhile surpassing the accuracy of all existing state-of-the-art two-stage\ndetectors. Code is at: https://github.com/facebookresearch/Detectron.",
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ]
        },
        {
            "title": "Face Attention Network: An Effective Face Detector for the Occluded Faces",
            "arxiv": "1711.07246",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.07246v2",
            "abstract": "The performance of face detection has been largely improved with the\ndevelopment of convolutional neural network. However, the occlusion issue due\nto mask and sunglasses, is still a challenging problem. The improvement on the\nrecall of these occluded cases usually brings the risk of high false positives.\nIn this paper, we present a novel face detector called Face Attention Network\n(FAN), which can significantly improve the recall of the face detection problem\nin the occluded case without compromising the speed. More specifically, we\npropose a new anchor-level attention, which will highlight the features from\nthe face region. Integrated with our anchor assign strategy and data\naugmentation techniques, we obtain state-of-art results on public face\ndetection benchmarks like WiderFace and MAFA. The code will be released for\nreproduction.",
            "authors": [
                "Jianfeng Wang",
                "Ye Yuan",
                "Gang Yu"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999829146622922,
        "task": "Object Detection",
        "task_prob": 0.6735032733922275
    },
    "training": {
        "datasets": [
            {
                "name": "MAFA"
            },
            {
                "name": "OCCLUSION"
            }
        ]
    }
}