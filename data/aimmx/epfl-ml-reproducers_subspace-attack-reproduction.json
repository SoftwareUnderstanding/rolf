{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Subspace Attack Reproduction",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "epfl-ml-reproducers",
                "owner_type": "Organization",
                "name": "subspace-attack-reproduction",
                "url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction",
                "stars": 7,
                "pushed_at": "2019-12-27 14:23:35+00:00",
                "created_at": "2019-11-26 15:43:10+00:00",
                "language": "Jupyter Notebook",
                "description": "Attempt to reproduce the paper Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks.",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "d9f5e7f7d2114958feceabb59b87598973cfccab",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction/blob/master/.gitignore"
                    }
                },
                "size": 4635
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "b6e84a3bf3305d32405eece85815e2808be29db1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction/blob/master/LICENSE"
                    }
                },
                "size": 1076
            },
            {
                "type": "code",
                "name": "neurips_report.pdf",
                "sha": "dd35685f0a2bbc132724c93e21859de3dad134da",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction/blob/master/neurips_report.pdf"
                    }
                },
                "size": 566421
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "33456a30c82f0e30ec184d3ede6988faa2adea9f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction/tree/master/notebooks"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "report.pdf",
                "sha": "adc4f8c3c138756e4bc42545529d89c81f583a9a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction/blob/master/report.pdf"
                    }
                },
                "size": 389053
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "c3a8bfc73fc169f3817de22103e1ced8eb52b6cd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction/blob/master/requirements.txt"
                    }
                },
                "size": 71
            },
            {
                "type": "code",
                "name": "run.py",
                "sha": "8ca945d3c0c439b85c01aa41ee72fb0da03829da",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction/blob/master/run.py"
                    }
                },
                "size": 14645
            },
            {
                "type": "code",
                "name": "src",
                "sha": "fecda2b3ed44b72bad2c53d1e748a648451c8e0c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction/tree/master/src"
                    }
                },
                "num_files": 8
            }
        ]
    },
    "authors": [
        {
            "name": "Edoardo Debenedetti",
            "email": "dedeswim@gmail.com",
            "github_id": "dedeswim"
        },
        {
            "name": "dansichi",
            "github_id": "dansichi"
        }
    ],
    "tags": [],
    "description": "Attempt to reproduce the paper Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction",
            "stars": 7,
            "issues": true,
            "readme": "# Subspace Attack Reproduction\n\n## Motivation\n\nAttempt to reproduce the NeurIPS 2019 paper [Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks](https://papers.nips.cc/paper/8638-subspace-attack-exploiting-promising-subspaces-for-query-efficient-black-box-attacks).\n\nThe original code of the paper can be found [here](https://github.com/ZiangYan/subspace-attack.pytorch). We are trying to reproduce the attack to GDAS and WRN model trained on CIFAR-10 dataset, without using and looking at the original code.\n\nThis project is done as project for the [CS-433 Machine Learning Course](https://www.epfl.ch/labs/mlo/machine-learning-cs-433/) at [EPFL](https://epfl.ch/en), and as part of the [NeurIPS 2019 Reproducibility Challenge](https://reproducibility-challenge.github.io/neurips2019/).\n\n## Usage\n\nWe make use of some pretrained models, that can be downloaded [here](https://drive.google.com/file/d/1TA-UWYVDkCkNPOy1INjUU9321s-HA6RF/view?usp=sharing). They are a subset of the [models](https://drive.google.com/file/d/1aXTmN2AyNLdZ8zOeyLzpVbRHZRZD0fW0/view?usp=sharing) provided with the code of the original paper. They need to be unzipped and put in the `./pretrained` folder, in the root directory of the repo.\n\nThe dataset ([CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html)) is automatically downloaded via `torchvision.datasets` when first running the experiment, and will be saved in the `data/` folder (more info [here](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)).\n\nThe paper is implemented and tested using Python 3.7. Dependencies are listed in [requirements.txt](requirements.txt).\n\nFor the moment, it is possible to run the experiment using [VGG nets](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) and [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) as reference models and [GDAS](https://arxiv.org/pdf/1910.04465.pdf), [WRN](https://arxiv.org/pdf/1605.07146.pdf) and [PyramidNet](https://arxiv.org/pdf/1610.02915.pdf) as victim models.\n\nIn order to test our implemenation, install the dependencies with `pip3 install --user --requirement requirements.txt`, and run the following command:\n\n```bash\npython run.py\n```\n\nThis will run the experiment on line 5 of table II of our report, with the following settings:\n\n- Reference models: AlexNet+VGGs\n- Victim model: GDAS\n- Number of images: 1000\n- Maximum queries per image: 10000\n- 0 seed\n  \nAnd hyperparameters:\n\n- eta_g = 0.1\n- eta = 1/255\n- delta = 0.1\n- tau = 1.0\n- epsilon = 8/255\n\nN.B.: it takes 7 hours 45 minutes to run on a Google Cloud Platform n1-highmem-8 virtual machine, with 8 vCPU, 52 GB memory and an Nvidia Tesla T4.\n\nMoreover, the following settings can be used to customize the experiment:\n\n```bash\nusage: run.py [-h] [-ds {Dataset.CIFAR_10}]\n                     [--reference-models {vgg11_bn,vgg13_bn,vgg16_bn,vgg19_bn,AlexNet_bn} [{vgg11_bn,vgg13_bn,vgg16_bn,vgg19_bn,AlexNet_bn} ...]]\n                     [--victim-model {gdas,wrn,pyramidnet}]\n                     [--loss {ExperimentLoss.CROSS_ENTROPY,ExperimentLoss.NEG_LL}]\n                     [--tau TAU] [--epsilon EPSILON] [--delta DELTA]\n                     [--eta ETA] [--eta_g ETA_G] [--n-images N_IMAGES]\n                     [--image-limit IMAGE_LIMIT]\n                     [--compare-gradients COMPARE_GRADIENTS]\n                     [--check-success CHECK_SUCCESS]\n                     [--show-images SHOW_IMAGES] [--seed SEED]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -ds {Dataset.CIFAR_10}, --dataset {Dataset.CIFAR_10}\n                        The dataset to be used.\n  --reference-models {vgg11_bn,vgg13_bn,vgg16_bn,vgg19_bn,AlexNet_bn} [{vgg11_bn,vgg13_bn,vgg16_bn,vgg19_bn,AlexNet_bn} ...]\n                        The reference models to be used.\n  --victim-model {gdas,wrn,pyramidnet}\n                        The model to be attacked.\n  --loss {ExperimentLoss.CROSS_ENTROPY,ExperimentLoss.NEG_LL}\n                        The loss function to be used\n  --tau TAU             Bandit exploration.\n  --epsilon EPSILON     The norm budget.\n  --delta DELTA         Finite difference probe.\n  --eta ETA             Image learning rate.\n  --eta_g ETA_G         OCO learning rate.\n  --n-images N_IMAGES   The number of images on which the attack has to be run\n  --image-limit IMAGE_LIMIT\n                        Limit of iterations to be done for each image\n  --compare-gradients COMPARE_GRADIENTS\n                        Whether the program should output a comparison between\n                        the estimated and the true gradients.\n  --check-success CHECK_SUCCESS\n                        Whether the attack on each image should stop if it has\n                        been successful.\n  --show-images SHOW_IMAGES\n                        Whether each image to be attacked, and its\n                        corresponding adversarial examples should be shown\n  --seed SEED           The random seed with which the experiment should be\n                        run, to be used for reproducibility purposes.\n```\n\nIn order to run an experiment on 100 images in which the loss of the true model and the cosine similarity between the estimated and true gradient, for all 5000 iterations per image, regardless of the success of the attack (i.e. the one used for figures 1 and 2 of our report), you should run\n\n```bash\npython3 run.py --check-success=False --n-images=100 --compare-gradients=True\n```\n\nN.B.: it takes around 20 hours to run the experiment on the aforementioned machine.\n\nThe experiment results are saved in the `outputs/` folder, in a file named `YYYY-MM-DD.HH-MM.npy` a dictionary exported with `numpy.save()`. The format of the dictionary is:\n\n```python\nexperiment_info = {\n    'experiment_baseline': {\n        'victim_model': victim_model_name,\n        'reference_model_names': reference_model_names,\n        'dataset': dataset\n    },\n    'hyperparameters': {\n        'tau': tau,\n        'epsilon': epsilon,\n        'delta': delta,\n        'eta': eta,\n        'eta_g': eta_g\n    },\n    'settings': {\n        'n_images': n_images,\n        'image_limit': image_limit,\n        'compare_gradients': compare_gradients,\n        'gpu': # If the GPU has been used for the experiment,\n        'seed': seed\n    },\n    'results': {\n        'queries': # The number of queries run\n        'total_time' # The time it took to run the experiment\n        # The following are present only if compare_gradients == True\n        'gradient_products': # The cosine similarities for each image\n        'true_gradient_norms': # The norms of the true gradients for each image\n        'estimated_gradient_norms': # The norms of the estimated gradients for each image\n        'true_losses': # The true losses each iteration\n        'common_signs': # The percentages of common signs between true and est gradients\n        'subs_common_signs': # The percentages of common signs between subsequent gradients\n}\n```\n\nThe file can be imported in Python using `np.load(output_path, allow_pickle=True).item()`.\n\n## Project structure\n\nThe repository is structured in the following way:\n\n```bash\n.\n\u251c\u2500\u2500 black-box_attack_reproduce.ipynb\n\u251c\u2500\u2500 data # Should contain the dataset used\n\u251c\u2500\u2500 experiment.py # Contains the experiment\n\u251c\u2500\u2500 img # Contains images used in notebooks\n\u2502   \u2514\u2500\u2500 algo1.png\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 notebooks # Contains some notebooks used to analyze the experiments\n\u2502   \u2514\u2500\u2500 experiment_analysis.ipynb\n\u251c\u2500\u2500 outputs # Contains the .npy files obtained in the reported experiments\n\u251c\u2500\u2500 pretrained # Should contain the pretrained models (.pth files)\n\u251c\u2500\u2500 README.md # This file :)\n\u251c\u2500\u2500 requirements.txt # Contains information about dependencies\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 helpers.py # Some helper functions\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 load_data.py # Some functions used to load the dataset\n    \u251c\u2500\u2500 load_loss.py # Some functions used to load the loss function\n    \u251c\u2500\u2500 load_model.py # Some functions to load pretrained models\n    \u251c\u2500\u2500 models # Contains the classes of the models (not made by us, link to original repo above)\n    \u251c\u2500\u2500 plots.py # A function to plot images\n    \u2514\u2500\u2500 subspace_attack.py # The very attack, the core of the repo\n```\n",
            "readme_url": "https://github.com/epfl-ml-reproducers/subspace-attack-reproduction",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Searching for A Robust Neural Architecture in Four GPU Hours",
            "arxiv": "1910.04465",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.04465v2",
            "abstract": "Conventional neural architecture search (NAS) approaches are based on\nreinforcement learning or evolutionary strategy, which take more than 3000 GPU\nhours to find a good model on CIFAR-10. We propose an efficient NAS approach\nlearning to search by gradient descent. Our approach represents the search\nspace as a directed acyclic graph (DAG). This DAG contains billions of\nsub-graphs, each of which indicates a kind of neural architecture. To avoid\ntraversing all the possibilities of the sub-graphs, we develop a differentiable\nsampler over the DAG. This sampler is learnable and optimized by the validation\nloss after training the sampled architecture. In this way, our approach can be\ntrained in an end-to-end fashion by gradient descent, named Gradient-based\nsearch using Differentiable Architecture Sampler (GDAS). In experiments, we can\nfinish one searching procedure in four GPU hours on CIFAR-10, and the\ndiscovered model obtains a test error of 2.82\\% with only 2.5M parameters,\nwhich is on par with the state-of-the-art. Code is publicly available on\nGitHub: https://github.com/D-X-Y/NAS-Projects.",
            "authors": [
                "Xuanyi Dong",
                "Yi Yang"
            ]
        },
        {
            "title": "Wide Residual Networks",
            "arxiv": "1605.07146",
            "year": 2016,
            "url": "http://arxiv.org/abs/1605.07146v4",
            "abstract": "Deep residual networks were shown to be able to scale up to thousands of\nlayers and still have improving performance. However, each fraction of a\npercent of improved accuracy costs nearly doubling the number of layers, and so\ntraining very deep residual networks has a problem of diminishing feature\nreuse, which makes these networks very slow to train. To tackle these problems,\nin this paper we conduct a detailed experimental study on the architecture of\nResNet blocks, based on which we propose a novel architecture where we decrease\ndepth and increase width of residual networks. We call the resulting network\nstructures wide residual networks (WRNs) and show that these are far superior\nover their commonly used thin and very deep counterparts. For example, we\ndemonstrate that even a simple 16-layer-deep wide residual network outperforms\nin accuracy and efficiency all previous deep residual networks, including\nthousand-layer-deep networks, achieving new state-of-the-art results on CIFAR,\nSVHN, COCO, and significant improvements on ImageNet. Our code and models are\navailable at https://github.com/szagoruyko/wide-residual-networks",
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ]
        },
        {
            "title": "Deep Pyramidal Residual Networks",
            "arxiv": "1610.02915",
            "year": 2016,
            "url": "http://arxiv.org/abs/1610.02915v4",
            "abstract": "Deep convolutional neural networks (DCNNs) have shown remarkable performance\nin image classification tasks in recent years. Generally, deep neural network\narchitectures are stacks consisting of a large number of convolutional layers,\nand they perform downsampling along the spatial dimension via pooling to reduce\nmemory usage. Concurrently, the feature map dimension (i.e., the number of\nchannels) is sharply increased at downsampling locations, which is essential to\nensure effective performance because it increases the diversity of high-level\nattributes. This also applies to residual networks and is very closely related\nto their performance. In this research, instead of sharply increasing the\nfeature map dimension at units that perform downsampling, we gradually increase\nthe feature map dimension at all units to involve as many locations as\npossible. This design, which is discussed in depth together with our new\ninsights, has proven to be an effective means of improving generalization\nability. Furthermore, we propose a novel residual unit capable of further\nimproving the classification accuracy with our new network architecture.\nExperiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown\nthat our network architecture has superior generalization ability compared to\nthe original residual networks. Code is available at\nhttps://github.com/jhkim89/PyramidNet}",
            "authors": [
                "Dongyoon Han",
                "Jiwhan Kim",
                "Junmo Kim"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9990416759821564,
        "task": "Image Classification",
        "task_prob": 0.98204477503897
    }
}