{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "BERT",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "af-ai-center",
                "owner_type": "User",
                "name": "bert",
                "url": "https://github.com/af-ai-center/bert",
                "stars": 12,
                "pushed_at": "2020-02-05 11:25:00+00:00",
                "created_at": "2019-06-15 05:56:32+00:00",
                "language": "Python",
                "description": "Code and Swedish pre-trained models for BERT",
                "license": "Apache License 2.0",
                "frameworks": [
                    "scikit-learn",
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "894a44cc066a027465cd26d634948d56d13af9af",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/af-ai-center/bert/blob/master/.gitignore"
                    }
                },
                "size": 1203
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/af-ai-center/bert/blob/master/LICENSE"
                    }
                },
                "size": 11357
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "22ad6283d4c2e00449d78f5bc36409179296dd8a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/af-ai-center/bert/tree/master/utils"
                    }
                },
                "num_files": 4
            }
        ]
    },
    "authors": [
        {
            "name": "af-ai-center",
            "github_id": "af-ai-center"
        }
    ],
    "tags": [],
    "description": "Code and Swedish pre-trained models for BERT",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/af-ai-center/bert",
            "stars": 12,
            "issues": true,
            "readme": "**Note that this repository is no longer maintained. See https://github.com/af-ai-center/SweBERT instead.**\n\n# BERT\nSwedish pre-trained models for BERT.\n\n## Introduction\n\n**BERT**, or **B**idirectional **E**ncoder **R**epresentations from\n**T**ransformers, is a new method of pre-training language representations which\nobtains state-of-the-art results on a wide array of Natural Language Processing\n(NLP) tasks.\n\nBERT is based on the Transformer architecture introduced in [Attention is all you need](https://arxiv.org/abs/1706.03762).\n\nGoogles academic paper which describes BERT in detail and provides full results on a\nnumber of tasks can be found here:\n[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).\n\nGoogles Github repository where the original english models can be found here:\n[https://github.com/google-research/bert](https://github.com/google-research/bert).\n\nIncluded in the downloads below are PyTorch versions of the models based on the work of \nNLP researchers from HuggingFace.\n[PyTorch version of BERT available](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)\n\n## What is BERT?\n\nBERT is a method of pre-training language representations, meaning that we train\na general-purpose \"language understanding\" model on a large text corpus (like\nWikipedia), and then use that model for downstream NLP tasks that we care about\n(like question answering). BERT outperforms previous methods because it is the\nfirst *unsupervised*, *deeply bidirectional* system for pre-training NLP.\n\n*Unsupervised* means that BERT was trained using only a plain text corpus, which\nis important because an enormous amount of plain text data is publicly available\non the web in many languages.\n\nWe used Swedish Wikipedia with approximatelly 2 million articles and 300 million words.\n\nThe links to the models are here (right-click, 'Save link as...' on the name):\n\n*   **[`Swedish BERT-Base, Uncased`](https://storage.googleapis.com/ai-center/2019_06_15/swe-uncased_L-12_H-768_A-12.zip)**:\n    12-layer, 768-hidden, 12-heads, 110M parameters\n*   **[`Swedish BERT-Large, Uncased`](https://storage.googleapis.com/ai-center/2019_06_15/swe-uncased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n    \n## Disclaimer\n\nThis is initial pretrained Swedish versions of BERT models based on a smaller corpus than the original English versions. \n\n## Contact information\n\nIf you find these models useful or if you have suggestions for how they can be improved, please submit a GitHub issue.\n\nFor personal communication related to these Swedish versions of BERT, please contact Magnus Bjelkenhed\n(`magnus.bjelkenhed@arbetsformedlingen.se`) or Mattias Bielsa (`mattias.bielsa@arbetsformedlingen.se`)\n\n",
            "readme_url": "https://github.com/af-ai-center/bert",
            "frameworks": [
                "scikit-learn",
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "arxiv": "1810.04805",
            "year": 2018,
            "url": "http://arxiv.org/abs/1810.04805v2",
            "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ]
        },
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Wikipedia"
            },
            {
                "name": "SQuAD"
            },
            {
                "name": "MultiNLI"
            },
            {
                "name": "GLUE"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999999979266143,
        "task": "Machine Translation",
        "task_prob": 0.7841239481490196
    }
}