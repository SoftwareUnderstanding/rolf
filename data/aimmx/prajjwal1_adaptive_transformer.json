{
    "visibility": {
        "visibility": "public"
    },
    "name": "Adaptive Transformers for Learning Multimodal Representations",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "prajjwal1",
                "owner_type": "User",
                "name": "adaptive_transformer",
                "url": "https://github.com/prajjwal1/adaptive_transformer",
                "stars": 38,
                "pushed_at": "2020-11-26 11:44:44+00:00",
                "created_at": "2020-01-11 11:13:45+00:00",
                "language": "Jupyter Notebook",
                "description": "Code for the paper \"Adaptive Transformers for Learning Multimodal Representations\" (ACL SRW 2020)",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "f0c4a350a1d61c8fc4ffe0af97ad3c9882cd1c8f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/blob/master/.gitignore"
                    }
                },
                "size": 1292
            },
            {
                "type": "code",
                "name": "dataset",
                "sha": "9b0638180ccfad4e7b7fd7403ef39a10e9ee30ab",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/tree/master/dataset"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "learner.py",
                "sha": "077649533bf84504c1b6d7ff7ec5e29d2158232c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/blob/master/learner.py"
                    }
                },
                "size": 11478
            },
            {
                "type": "code",
                "name": "models",
                "sha": "4fa0d0f1f47fab811093b319049df23e93e10e1f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/tree/master/models"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "nbs",
                "sha": "d698cfae54e388d7776675073631732d507793f6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/tree/master/nbs"
                    }
                },
                "num_files": 16
            },
            {
                "type": "code",
                "name": "optimizers",
                "sha": "39dcda4e5742075a2ff2d7dd1b158e0b503b71ef",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/tree/master/optimizers"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "pretrain",
                "sha": "076a2c6c717b86bb5d66757cd77be678e589f251",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/tree/master/pretrain"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "b8503c5c066cc4529d108c63e7775d6792adce4f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/blob/master/requirements.txt"
                    }
                },
                "size": 40
            },
            {
                "type": "code",
                "name": "run_test.sh",
                "sha": "0dc353a7e8fb209da68a4c1d0e62b376e2e7acd2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/blob/master/run_test.sh"
                    }
                },
                "size": 1488
            },
            {
                "type": "code",
                "name": "run_train.sh",
                "sha": "5c3d6f9e6e37005c2c5f86a87af3a64b7413f089",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/blob/master/run_train.sh"
                    }
                },
                "size": 566
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "9a0e5cd2f20573343ef07818ae9bede23241c40e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/blob/master/train.py"
                    }
                },
                "size": 5392
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "b644dd8e5a940859a4426f98cd5fad8506c07b05",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/prajjwal1/adaptive_transformer/blob/master/utils.py"
                    }
                },
                "size": 1995
            }
        ]
    },
    "authors": [
        {
            "name": "Prajjwal",
            "github_id": "prajjwal1"
        }
    ],
    "tags": [],
    "description": "Code for the paper \"Adaptive Transformers for Learning Multimodal Representations\" (ACL SRW 2020)",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/prajjwal1/adaptive_transformer",
            "stars": 38,
            "issues": true,
            "readme": "# Adaptive Transformers for Learning Multimodal Representations\n\n<h4>\nACL SRW 2020\n</br>\nPrajjwal Bhargava\n</h4>\n<hr>\n\n\n**Paper:** [arXiv](https://arxiv.org/abs/2005.07486)\n\n\nML Code Completeness Checklist:\n- [x] Specification of dependencies\n- [x] Training code\n- [x] Evaluation code\n- [x] Pre-trained models\n- [x] README file including table of results accompanied by precise commands to run/produce those results\n\n## Dependencies:\nPlease refer `requirements.txt`.\nTo install,\n```\n$ pip install -r requirements.txt \n```\n\n## Dataset Preparation\n- Download the raw VQA 2.0 dataset from the [official website](https://visualqa.org/download.html).\n\nMake sure that your data directory looks similar to the following structure (you can change the paths if you want a different structure in `train.py`).\n\n- These instructions are from [LXMERT repo]((https://github.com/airsplay/lxmert#vqa)). Download the re-distributed JSON files.\n```\nmkdir -p data/vqa\nwget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/train.json -P data/vqa/\nwget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/nominival.json -P  data/vqa/\nwget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/minival.json -P data/vqa/\n```\nFor downloading FasterRCNN features, use these instructions:\n```\nmkdir -p data/mscoco_imgfeat\nwget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/train2014_obj36.zip -P data/mscoco_imgfeat\nunzip data/mscoco_imgfeat/train2014_obj36.zip -d data/mscoco_imgfeat && rm data/mscoco_imgfeat/train2014_obj36.zip\nwget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/val2014_obj36.zip -P data/mscoco_imgfeat\nunzip data/mscoco_imgfeat/val2014_obj36.zip -d data && rm data/mscoco_imgfeat/val2014_obj36.zip\n```\nIf the links don't work, you can use [Google drive link](https://drive.google.com/drive/folders/1Gq1uLUk6NdD0CcJOptXjxE6ssY5XAuat?usp=sharing) to get access. For more details, please refer [LXMERT repo](https://github.com/airsplay/lxmert).\n\nSetup the directory structure like this:\nIn `/home/user/`\n```\n+-- data\n|   +-- lxmert\n|   +-- mscoco_imgfeat\n|   +-- vqa\n+-- adaptive_transformer\n+-- snap\n.......\n```\nCreate a directory snap, that's where checkpoints will be store by default.\nAll of this structure can be changed but suitable modifications will be needed in `train.py`.\n\nFasterRCNN features are loaded all at once in the RAM, so you'd require an instance with >48 GB of RAM. For training, I used a single P100 Nvidia GPU. \n\n## Downloading pretrained model\nPlease download the pretrained models from this [Google drive link](https://drive.google.com/drive/folders/1V1SjSfGCqBJZi2INzCmNKxnoXI4FnVeP?usp=sharing)\n\nAlternatively, if you want to train (finetune) the model yourself, download the pretrained weights from [here](http://nlp1.cs.unc.edu/data/model_LXRT.pth). Skip this step if you're using my weights.\n\n## Training\n```\n$ git clone https://github.com/prajjwal1/adaptive_transformer\n$ cd adaptive_transformer\n$ python3 train.py --bs=128 --epochs=1 --sparse --tiny #test script\n```\nIf this worked well, then you're ready to train.\n\nUsage:\n```\npython train.py\n    [--bs]            # Specify the batch size\n    [--epochs]        # Specify the epochs\n    [--tiny]          # Runs a test example (for debugging purposes)  \n    [--adaptive]      # Uses Adaptive Attention Span\n    [--sparse]        # Uses Entmax from Adaptively Sparse Transformers instead of softmax\n    [--layerdrop]     # Enables layerdrop\n    [--load_model]    # Resume training by specifying a checkpoint\n    [--test]          # Dumps a JSON file for submission to VQA servers.\n```\nMore customizations can be done by modifying the `params` and `config` dict in `train.py`. \n\nIt looks like this \n```\nparams = {\n    \"adapt_span_enabled\": args.adaptive,\n    \"attn_span\": 1024,\n    \"adapt_span_loss_coeff\": 0.000005,\n    \"adapt_span_ramp\": 32,\n    \"adapt_span_init\": 0.002,\n    \"adapt_span_cache\": True,\n    \"nb_heads\": 12,\n    \"bs\": args.bs,\n    \"mask_size\": [20, 36],\n    \"sparse_enabled\": args.sparse,\n    \"num_attention_heads\": 4,\n    \"layer_sizes\": {\"lang\": 9, \"cross\": 5, \"vision\": 5},\n    \"from_scratch\": False,\n    \"layerdrop_enabled\": args.layerdrop,\n    \"layerdrop_num_layers\": 1,\n}\nconfig = {\n    \"adaptive_enable\": args.adaptive,\n    \"sparse_enable\": args.sparse,\n    \"measure_flops\": False,\n    \"load_model\": args.load_model,\n}\n```\nPlease check the `params` dict when starting training to see the configurations. Config should match with the config used in loaded model. \nRemove the `tiny` flag to train on whole dataset.\n\n##  Using Adaptive Attention Span\n```\npython train.py --bs=128 --epochs=1 --adaptive --tiny\n```\n By default, attention spans of each layer is printed so that you can track it.\n\n## Using Entmax \nIf `sparse` flag is enabled, softmax will be replaced with entmax to compute probability distribution of attention weights.\n```\npython train.py --bs=128 --epochs=1 --sparse --tiny\n```\n\n## Using Layerdrop\n```\npython train.py --bs=128 --epochs=1 --layerdrop --tiny\n```\nSpecify the following as per use case in `train.py`:\n- `params['layerdrop_num_layers']`  # Number of layers to drop\n- `params['layer_sizes']` # Number of layers you require\n\nNOTE: Number of layers `params['layer_sizes']` have to match with number of layers in the model checkpoint. To perform pruning during inference, default `learn.load` method is not suitable as it loads all the layers. Please refer to this [fairseq issue](https://github.com/pytorch/fairseq/issues/1667#issuecomment-581595354) to perform pruning during inference.\n\n### Resuming Training\n\nTo load a model trained with `adaptive` or `sparse` or `layerdrop` flag:\n```\npython train.py --bs=128 --epochs=1 --adaptive --tiny --load_model=adaptive_6910\npython train.py --bs=128 --epochs=1 --sparse --tiny --load_model=sparse_7\npython train.py --bs=128 --epochs=1 --layerdrop --load_model=layerdrop_1066_ldrop_1 --tiny\n```\n\n### Running evaluation on test data set\n```\npython train.py --bs=128 --test --adaptive --load_model=adaptive_6910\n```\nWhen `test` flag is passed, only inference is performed on the test set. Ground truths for test set for VQA are not publicly available. This command will dump the JSON file in the `/snap` directory. Submit the JSON file through the [EvalAI competition page](https://evalai.cloudcv.org/web/challenges/challenge-page/514/overview).\n\n## Explanation of this codebase\n- `dataset` : contains standard Pytorch dataset class for VQA\n- `models`: Contains implmentation of adaptive mechanisms and LXMERT\n- `nbs`: Probably the most interesting part. Use this to understand my workflow, attention methods I used. I used these notebooks to develop this codebase. You can also use these to understand how attention works in this context and much more.\n-  `optimizers`: implementation of LAMB and Lookahead optimizer\n- `pretrain`: utility tools\n- `train.py`: Specifies how training and testing to be carried out. You'd probably want to modify this to adapt to your work.\n- `learner.py`: Implements a Learner class to control all functionalities of this codebase.\n- `run_train.sh`: You can modify this to setting hardware specific training (Optional)\n- `run_test.sh`: Set of tests (for me).\n\n## Inference: Visualizing results\nPlease refer to [nbs/inference.ipynb](https://github.com/prajjwal1/adaptive_transformer/blob/master/nbs/inference.ipynb) to load your trained model, obtain predictions and visualize the results.\n\n## Results\n\nThese results can be reproduced by using the scripts I provided above and using the same `params` and `config` dict values.\nOur model achives the following performance on the VQA 2.0 benchmark:\n```\n| Model                                 | test-dev | test-std |\n|---------------------------------------|----------|----------|\n| LXMERT                                |          |          |\n| w/ softmax                            | 72.42    | 72.54    |\n| w/ Adaptive Attention Span            | 71.62    | 71.72    |\n| w/ Adaptive Sparse                    | 71.73    | 71.97    |\n| w/ Layerdrop (10-6-6, p=1)            | 66.4     | 66.72    |\n| w/ Layerdrop (10-6-6, p=0)            | 66.35    | 66.57    |\n| w/ Layerdrop (9-5-5, p=1)             | 66.51    | 66.81    |\n| w/ Adaptive Attention Span and Entmax | 63.07    | 63.33    |\n```\n\n## Citation\nIf you use this work in any form, please cite the paper:\n```\n@inproceedings{bhargava-2020-adaptive,\n    title = \"Adaptive Transformers for Learning Multimodal Representations\",\n    author = \"Bhargava, Prajjwal\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-srw.1\",\n    doi = \"10.18653/v1/2020.acl-srw.1\",\n    pages = \"1--7\",\n    abstract = \"The usage of transformers has grown from learning about language semantics to forming meaningful visiolinguistic representations. These architectures are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about model interpretability and computational efficiency. Specifically, we study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks. We further show that these approaches can help us learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.\",\n}\n```\n\n## Acknowledgement\n- Code for LXMERT Model was adapted from [LXMERT](https://github.com/airsplay/lxmert) repo.\n- Entmax autograd function implementation was adapted from [entmax repo](https://github.com/deep-spin/entmax)\n\n",
            "readme_url": "https://github.com/prajjwal1/adaptive_transformer",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Adaptive Transformers for Learning Multimodal Representations",
            "arxiv": "2005.07486",
            "year": 2020,
            "url": "http://arxiv.org/abs/2005.07486v3",
            "abstract": "The usage of transformers has grown from learning about language semantics to\nforming meaningful visiolinguistic representations. These architectures are\noften over-parametrized, requiring large amounts of computation. In this work,\nwe extend adaptive approaches to learn more about model interpretability and\ncomputational efficiency. Specifically, we study attention spans, sparse, and\nstructured dropout methods to help understand how their attention mechanism\nextends for vision and language tasks. We further show that these approaches\ncan help us learn more about how the network perceives the complexity of input\nsequences, sparsity preferences for different modalities, and other related\nphenomena.",
            "authors": [
                "Prajjwal Bhargava"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "VQA 2.0"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9024865074623845,
        "task": "Machine Translation",
        "task_prob": 0.4682642088752694
    }
}