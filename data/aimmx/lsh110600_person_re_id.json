{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": " Person_reID_baseline_pytorch ",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "lsh110600",
                "owner_type": "User",
                "name": "person_re_id",
                "url": "https://github.com/lsh110600/person_re_id",
                "stars": 0,
                "pushed_at": "2020-07-07 05:14:53+00:00",
                "created_at": "2020-07-07 05:08:51+00:00",
                "language": "Python",
                "description": "X_twice_person_re_id",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".finetuning.py.swp",
                "sha": "af7e3d83c4f69451f3cb98de4e743edac469d795",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/.finetuning.py.swp"
                    }
                },
                "size": 12288
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "15036ec46c99064a58e8d033d895f037252c5a0f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/.gitignore"
                    }
                },
                "size": 372
            },
            {
                "type": "code",
                "name": ".test.py.swp",
                "sha": "fa5a60ee0d81132a972e44c606007de551eab965",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/.test.py.swp"
                    }
                },
                "size": 16384
            },
            {
                "type": "code",
                "name": ".travis.yml",
                "sha": "63d77742070765b60e9c74fc39242657e907d9c6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/.travis.yml"
                    }
                },
                "size": 664
            },
            {
                "type": "code",
                "name": "Data",
                "sha": "1ad62c1688f7dba63dc3fef94754091c3eeb7e2b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/tree/master/Data"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "Data2",
                "sha": "53a30902a04f97c1c18d9d1e77a170be1192efbe",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/tree/master/Data2"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "e6ea37fa9af821621effc90936b77f9215e8e43e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/LICENSE"
                    }
                },
                "size": 1070
            },
            {
                "type": "code",
                "name": "demo.py",
                "sha": "12f3f3cbd0b152395462e5e41d2920d1583ea6ec",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/demo.py"
                    }
                },
                "size": 3731
            },
            {
                "type": "code",
                "name": "evaluate.py",
                "sha": "f3b6ff9057128c833a10d1af48f4e69f069e7fbd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/evaluate.py"
                    }
                },
                "size": 3425
            },
            {
                "type": "code",
                "name": "evaluate_gpu.py",
                "sha": "2e4b9da6900eaf0d202afc4a858b1ed7acf97ed4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/evaluate_gpu.py"
                    }
                },
                "size": 3730
            },
            {
                "type": "code",
                "name": "evaluate_rerank.py",
                "sha": "709cbacff9ba52cb3c2c41a1eb6487a6951e64f7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/evaluate_rerank.py"
                    }
                },
                "size": 2735
            },
            {
                "type": "code",
                "name": "finetuning.py",
                "sha": "ef236e8f8865109c89ec6808a54ebc04463bb6a8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/finetuning.py"
                    }
                },
                "size": 409
            },
            {
                "type": "code",
                "name": "leaderboard",
                "sha": "7865b648a3bcc657dbd87adf52a98907d8367a45",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/tree/master/leaderboard"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "2099a111341eb0fc7325a5db6f29dc0b455bb570",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/model.py"
                    }
                },
                "size": 8393
            },
            {
                "type": "code",
                "name": "model",
                "sha": "5bfad2b3f8e483b6b173d8aaff19597e84626f15",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/tree/master/model"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "prepare.py",
                "sha": "e902f9e75f0d3710384e38bdac455ae4bec44d2b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/prepare.py"
                    }
                },
                "size": 3722
            },
            {
                "type": "code",
                "name": "prepare_static.py",
                "sha": "bdfd3845a6301a427a0c919e21ddde44e6bb2194",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/prepare_static.py"
                    }
                },
                "size": 3759
            },
            {
                "type": "code",
                "name": "prepare_viper.py",
                "sha": "0c405998183f4c40b5f51a5a4d4a7da8395ca369",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/prepare_viper.py"
                    }
                },
                "size": 2157
            },
            {
                "type": "code",
                "name": "random_erasing.py",
                "sha": "9c3549bfc6b9ddb1c15faaa3fac3def467cd0755",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/random_erasing.py"
                    }
                },
                "size": 1929
            },
            {
                "type": "code",
                "name": "re_ranking.py",
                "sha": "96599bc19f166f986dc8fb349fdcda1dfc64987b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/re_ranking.py"
                    }
                },
                "size": 4287
            },
            {
                "type": "code",
                "name": "show.png",
                "sha": "46c05c589fcf53419cab1da0586c2b6666a1c1a6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/show.png"
                    }
                },
                "size": 218627
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "301d301dc9571bd3241423de5c934c3c501e7b30",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/test.py"
                    }
                },
                "size": 9158
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "9a06c6bbeca37028c11afc260be793565a45d1b5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/blob/master/train.py"
                    }
                },
                "size": 14879
            },
            {
                "type": "code",
                "name": "tutorial",
                "sha": "9883fb0321abdd230ce200d080abd7c22dc769cb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lsh110600/person_re_id/tree/master/tutorial"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Zhedong Zheng",
            "email": "Zhedong.Zheng@student.uts.edu.au",
            "github_id": "layumi"
        },
        {
            "name": "Zhun Zhong",
            "github_id": "zhunzhong07"
        },
        {
            "name": "lsh110600",
            "github_id": "lsh110600"
        },
        {
            "name": "zhangchuangnankai",
            "github_id": "zhangchuangnankai"
        }
    ],
    "tags": [],
    "description": "X_twice_person_re_id",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/lsh110600/person_re_id",
            "stars": 0,
            "issues": true,
            "readme": "<h1 align=\"center\"> Person_reID_baseline_pytorch </h1>\n\n[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/layumi/Person_reID_baseline_pytorch.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/layumi/Person_reID_baseline_pytorch/context:python)\n[![Build Status](https://travis-ci.org/layumi/Person_reID_baseline_pytorch.svg?branch=master)](https://travis-ci.org/layumi/Person_reID_baseline_pytorch)\n[![Total alerts](https://img.shields.io/lgtm/alerts/g/layumi/Person_reID_baseline_pytorch.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/layumi/Person_reID_baseline_pytorch/alerts/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n\nA tiny, friendly, strong baseline code for Person-reID (based on [pytorch](https://pytorch.org)).\n\n- **Strong.** It is consistent with the new baseline result in several top-conference works, e.g., [Joint Discriminative and Generative Learning for Person Re-identification(CVPR19)](https://arxiv.org/abs/1904.07223), [Beyond Part Models: Person Retrieval with Refined Part Pooling(ECCV18)](https://arxiv.org/abs/1711.09349), [Camera Style Adaptation for Person Re-identification(CVPR18)](https://arxiv.org/abs/1711.10295). We arrived Rank@1=88.24%, mAP=70.68% only with softmax loss. \n\n- **Small.** With fp16 (supported by Nvidia apex), our baseline could be trained with only 2GB GPU memory.\n\n- **Friendly.** You may use the off-the-shelf options to apply many state-of-the-art tricks in one line.\nBesides, if you are new to person re-ID, you may check out our **[Tutorial](https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/tutorial)** first (8 min read) :+1: .\n![](https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/show.png)\n\n## Table of contents\n* [Features](#features)\n* [Some News](#some-news)\n* [Trained Model](#trained-model)\n* [Prerequisites](#prerequisites)\n* [Getting Started](#getting-started)\n    * [Installation](#installation)\n    * [Dataset Preparation](#dataset--preparation)\n    * [Train](#train)\n    * [Test](#test)\n    * [Evaluation](#evaluation)\n* [Tips for training with other datasets](#tips)\n* [Citation](#citation)\n* [Related Repos](#related-repos)\n\n## Features\nNow we have supported:\n- Float16 to save GPU memory based on [apex](https://github.com/NVIDIA/apex)\n- Part-based Convolutional Baseline(PCB)\n- Multiple Query Evaluation\n- Re-Ranking\n- Random Erasing\n- ResNet/DenseNet\n- Visualize Training Curves\n- Visualize Ranking Result\n- [Visualize Heatmap](https://github.com/layumi/Person_reID_baseline_pytorch/blob/dev/visual_heatmap.py)\n- Linear Warm-up \n\nHere we provide hyperparameters and architectures, that were used to generate the result. \nSome of them (i.e. learning rate) are far from optimal. Do not hesitate to change them and see the effect. \n\nP.S. With similar structure, we arrived **Rank@1=87.74% mAP=69.46%** with [Matconvnet](http://www.vlfeat.org/matconvnet/). (batchsize=8, dropout=0.75) \nYou may refer to [Here](https://github.com/layumi/Person_reID_baseline_matconvnet).\nDifferent framework need to be tuned in a different way.\n\n## Some News\n**11 June 2020** People live in the 3D world. We release one new person re-id code [Person Re-identification in the 3D Space](https://github.com/layumi/person-reid-3d), which conduct representation learning in the 3D space. You are welcomed to check out it.\n\n**30 April 2020** We have applied this code to the [AICity Challenge 2020](https://www.aicitychallenge.org/),  yielding the 1st Place Submission to the re-id track :red_car:. Check out [here](https://github.com/layumi/AICIty-reID-2020).\n\n**01 March 2020** We release one new image retrieval dataset, called [University-1652](https://github.com/layumi/University1652-Baseline), for drone-view target localization and drone navigation :helicopter:. It has a similar setting with the person re-ID. You are welcomed to check out it.\n\n**07 July 2019:** I added some new functions, such as `--resume`, auto-augmentation policy, acos loss, into [developing thread](https://github.com/layumi/Person_reID_baseline_pytorch/tree/dev) and rewrite the `save` and `load` functions. I haven't tested the functions throughly. Some new functions are worthy of having a try. If you are first to this repo, I suggest you stay with the master thread.\n\n**01 July 2019:** [My CVPR19 Paper](https://arxiv.org/abs/1904.07223) is online. It is based on this baseline repo as teacher model to provide pseudo label for the generated images to train a better student model. You are welcomed to check out the opensource code at [here](https://github.com/NVlabs/DG-Net).\n\n**03 Jun 2019:** Testing with multiple-scale inputs is added. You can use `--ms 1,0.9` when extracting the feature. It could slightly improve the final result.\n\n**20 May 2019:** Linear Warm Up is added. You also can set warm-up the first K epoch by `--warm_epoch K`. If K <=0, there will be no warm-up.\n\n**What's new:** FP16 has been added. It can be used by simply added `--fp16`. You need to install [apex](https://github.com/NVIDIA/apex) and update your pytorch to 1.0. \n\nFloat16 could save about 50% GPU memory usage without accuracy drop. **Our baseline could be trained with only 2GB GPU memory.** \n```bash\npython train.py --fp16\n```\n**What's new:** Visualizing ranking result is added.\n```bash\npython prepare.py\npython train.py\npython test.py\npython demo.py --query_index 777\n```\n\n**What's new:** Multiple-query Evaluation is added. The multiple-query result is about **Rank@1=91.95% mAP=78.06%**. \n```bash\npython prepare.py\npython train.py\npython test.py --multi\npython evaluate_gpu.py\n```\n\n**What's new:** \u00a0[PCB](https://arxiv.org/abs/1711.09349) is added. You may use '--PCB' to use this model. It can achieve around **Rank@1=92.73% mAP=78.16%**. I used a GPU (P40) with 24GB Memory. You may try apply smaller batchsize and choose the smaller learning rate (for stability) to run. (For example, `--batchsize 32 --lr 0.01 --PCB`)\n```bash\npython train.py --PCB --batchsize 64 --name PCB-64\npython test.py --PCB --name PCB-64\n```\n\n**What's new:** You may try `evaluate_gpu.py` to conduct a faster evaluation with GPU.\n\n**What's new:** You may apply '--use_dense' to use `DenseNet-121`. It can arrive around Rank@1=89.91% mAP=73.58%. \n\n**What's new:** Re-ranking is added to evaluation. The re-ranked result is about **Rank@1=90.20% mAP=84.76%**.\n\n**What's new:** Random Erasing is added to train.\n\n**What's new:** I add some code to generate training curves. The figure will be saved into the model folder when training.\n\n![](https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/train.jpg)\n\n## Trained Model\nI re-trained several models, and the results may be different with the original one. Just for a quick reference, you may directly use these models. \nThe download link is [Here](https://drive.google.com/open?id=1XVEYb0TN2SbBYOqf8SzazfYZlpH9CxyE).\n\n|Methods | Rank@1 | mAP| Reference|\n| -------- | ----- | ---- | ---- |\n| [ResNet-50] | 88.84% | 71.59% |  `python train.py --train_all` |\n| [DenseNet-121] | 90.17% | 74.02% | `python train.py --name ft_net_dense --use_dense --train_all` |\n| [PCB] | 92.64% | 77.47% | `python train.py --name PCB --PCB --train_all --lr 0.02` |\n| [ResNet-50 (fp16)] | 88.03% | 71.40% | `python train.py --name fp16 --fp16 --train_all` |\n| [ResNet-50 (all tricks)] | 91.83% | 78.32% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5` |\n\n### Model Structure\nYou may learn more from `model.py`. \nWe add one linear layer(bottleneck), one batchnorm layer and relu.\n\n## Prerequisites\n\n- Python 3.6\n- GPU Memory >= 6G\n- Numpy\n- Pytorch 0.3+\n- [Optional] apex (for float16) \n- [Optional] [pretrainedmodels](https://github.com/Cadene/pretrained-models.pytorch)\n\n**(Some reports found that updating numpy can arrive the right accuracy. If you only get 50~80 Top1 Accuracy, just try it.)**\nWe have successfully run the code based on numpy 1.12.1 and 1.13.1 .\n\n## Getting started\n### Installation\n- Install Pytorch from http://pytorch.org/\n- Install Torchvision from the source\n```\ngit clone https://github.com/pytorch/vision\ncd vision\npython setup.py install\n```\n- [Optinal] You may skip it. Install apex from the source\n```\ngit clone https://github.com/NVIDIA/apex.git\ncd apex\npython setup.py install --cuda_ext --cpp_ext\n```\nBecause pytorch and torchvision are ongoing projects.\n\nHere we noted that our code is tested based on Pytorch 0.3.0/0.4.0/0.5.0/1.0.0 and Torchvision 0.2.0/0.2.1 .\n\n### Dataset & Preparation\nDownload [Market1501 Dataset](http://www.liangzheng.com.cn/Project/project_reid.html) [[Google]](https://drive.google.com/file/d/0B8-rUzbwVRk0c054eEozWG9COHM/view) [[Baidu]](https://pan.baidu.com/s/1ntIi2Op)\n\nPreparation: Put the images with the same id in one folder. You may use \n```bash\npython prepare.py\n```\nRemember to change the dataset path to your own path.\n\nFuthermore, you also can test our code on [DukeMTMC-reID Dataset]( [GoogleDriver](https://drive.google.com/open?id=1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O) or ([BaiduYun](https://pan.baidu.com/s/1jS0XM7Var5nQGcbf9xUztw) password: bhbh)).\nOur baseline code is not such high on DukeMTMC-reID **Rank@1=64.23%, mAP=43.92%**. Hyperparameters are need to be tuned.\n\n### Train\nTrain a model by\n```bash\npython train.py --gpu_ids 0 --name ft_ResNet50 --train_all --batchsize 32  --data_dir your_data_path\n```\n`--gpu_ids` which gpu to run.\n\n`--name` the name of model.\n\n`--data_dir` the path of the training data.\n\n`--train_all` using all images to train. \n\n`--batchsize` batch size.\n\n`--erasing_p` random erasing probability.\n\nTrain a model with random erasing by\n```bash\npython train.py --gpu_ids 0 --name ft_ResNet50 --train_all --batchsize 32  --data_dir your_data_path --erasing_p 0.5\n```\n\n### Test\nUse trained model to extract feature by\n```bash\npython test.py --gpu_ids 0 --name ft_ResNet50 --test_dir your_data_path  --batchsize 32 --which_epoch 59\n```\n`--gpu_ids` which gpu to run.\n\n`--batchsize` batch size.\n\n`--name` the dir name of trained model.\n\n`--which_epoch` select the i-th model.\n\n`--data_dir` the path of the testing data.\n\n\n### Evaluation\n```bash\npython evaluate.py\n```\nIt will output Rank@1, Rank@5, Rank@10 and mAP results.\nYou may also try `evaluate_gpu.py` to conduct a faster evaluation with GPU.\n\nFor mAP calculation, you also can refer to the [C++ code for Oxford Building](http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/compute_ap.cpp). We use the triangle mAP calculation (consistent with the Market1501 original code).\n\n### re-ranking\n```bash\npython evaluate_rerank.py\n```\n**It may take more than 10G Memory to run.** So run it on a powerful machine if possible. \n\nIt will output Rank@1, Rank@5, Rank@10 and mAP results.\n\n### Tips\nNotes the format of the camera id and the number of cameras.\n\nFor some dataset, e.g., MSMT17, there are more than 10 cameras. You need to modify the `prepare.py` and `test.py` to read the double-digit camera ID.\n\nFor some vehicle re-ID datasets. e.g. VeRi, you also need to modify the `prepare.py` and `test.py`.  It has different naming rules.\nhttps://github.com/layumi/Person_reID_baseline_pytorch/issues/107 (Sorry. It is in Chinese)\n\n\n## Citation\nThe following paper uses and reports the result of the baseline model. You may cite it in your paper.\n```\n@article{zheng2019joint,\n  title={Joint discriminative and generative learning for person re-identification},\n  author={Zheng, Zhedong and Yang, Xiaodong and Yu, Zhiding and Zheng, Liang and Yang, Yi and Kautz, Jan},\n  journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2019}\n}\n```\n\nThe following papers may be the first two to use the bottleneck baseline. You may cite them in your paper.\n```\n@article{DBLP:journals/corr/SunZDW17,\n  author    = {Yifan Sun and\n               Liang Zheng and\n               Weijian Deng and\n               Shengjin Wang},\n  title     = {SVDNet for Pedestrian Retrieval},\n  booktitle   = {ICCV},\n  year      = {2017},\n}\n\n@article{hermans2017defense,\n  title={In Defense of the Triplet Loss for Person Re-Identification},\n  author={Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},\n  journal={arXiv preprint arXiv:1703.07737},\n  year={2017}\n}\n```\n\nBasic Model\n```\n@article{zheng2018discriminatively,\n  title={A discriminatively learned CNN embedding for person reidentification},\n  author={Zheng, Zhedong and Zheng, Liang and Yang, Yi},\n  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},\n  volume={14},\n  number={1},\n  pages={13},\n  year={2018},\n  publisher={ACM}\n}\n```\n\n## Related Repos\n1. [Pedestrian Alignment Network](https://github.com/layumi/Pedestrian_Alignment) ![GitHub stars](https://img.shields.io/github/stars/layumi/Pedestrian_Alignment.svg?style=flat&label=Star)\n2. [2stream Person re-ID](https://github.com/layumi/2016_person_re-ID) ![GitHub stars](https://img.shields.io/github/stars/layumi/2016_person_re-ID.svg?style=flat&label=Star)\n3. [Pedestrian GAN](https://github.com/layumi/Person-reID_GAN) ![GitHub stars](https://img.shields.io/github/stars/layumi/Person-reID_GAN.svg?style=flat&label=Star)\n4. [Language Person Search](https://github.com/layumi/Image-Text-Embedding) ![GitHub stars](https://img.shields.io/github/stars/layumi/Image-Text-Embedding.svg?style=flat&label=Star)\n5. [DG-Net](https://github.com/NVlabs/DG-Net) ![GitHub stars](https://img.shields.io/github/stars/NVlabs/DG-Net.svg?style=flat&label=Star)\n6. [3D Person re-ID](https://github.com/layumi/person-reid-3d) ![GitHub stars](https://img.shields.io/github/stars/layumi/person-reid-3d.svg?style=flat&label=Star)\n",
            "readme_url": "https://github.com/lsh110600/person_re_id",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Camera Style Adaptation for Person Re-identification",
            "arxiv": "1711.10295",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.10295v2",
            "abstract": "Being a cross-camera retrieval task, person re-identification suffers from\nimage style variations caused by different cameras. The art implicitly\naddresses this problem by learning a camera-invariant descriptor subspace. In\nthis paper, we explicitly consider this challenge by introducing camera style\n(CamStyle) adaptation. CamStyle can serve as a data augmentation approach that\nsmooths the camera style disparities. Specifically, with CycleGAN, labeled\ntraining images can be style-transferred to each camera, and, along with the\noriginal training samples, form the augmented training set. This method, while\nincreasing data diversity against over-fitting, also incurs a considerable\nlevel of noise. In the effort to alleviate the impact of noise, the label\nsmooth regularization (LSR) is adopted. The vanilla version of our method\n(without LSR) performs reasonably well on few-camera systems in which\nover-fitting often occurs. With LSR, we demonstrate consistent improvement in\nall systems regardless of the extent of over-fitting. We also report\ncompetitive accuracy compared with the state of the art.",
            "authors": [
                "Zhun Zhong",
                "Liang Zheng",
                "Zhedong Zheng",
                "Shaozi Li",
                "Yi Yang"
            ]
        },
        {
            "title": "Joint Discriminative and Generative Learning for Person Re-identification",
            "arxiv": "1904.07223",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.07223v3",
            "abstract": "Person re-identification (re-id) remains challenging due to significant\nintra-class variations across different cameras. Recently, there has been a\ngrowing interest in using generative models to augment training data and\nenhance the invariance to input changes. The generative pipelines in existing\nmethods, however, stay relatively separate from the discriminative re-id\nlearning stages. Accordingly, re-id models are often trained in a\nstraightforward manner on the generated data. In this paper, we seek to improve\nlearned re-id embeddings by better leveraging the generated data. To this end,\nwe propose a joint learning framework that couples re-id learning and data\ngeneration end-to-end. Our model involves a generative module that separately\nencodes each person into an appearance code and a structure code, and a\ndiscriminative module that shares the appearance encoder with the generative\nmodule. By switching the appearance or structure codes, the generative module\nis able to generate high-quality cross-id composed images, which are online fed\nback to the appearance encoder and used to improve the discriminative module.\nThe proposed joint learning framework renders significant improvement over the\nbaseline without using generated data, leading to the state-of-the-art\nperformance on several benchmark datasets.",
            "authors": [
                "Zhedong Zheng",
                "Xiaodong Yang",
                "Zhiding Yu",
                "Liang Zheng",
                "Yi Yang",
                "Jan Kautz"
            ]
        },
        {
            "title": "Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline)",
            "arxiv": "1711.09349",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.09349v3",
            "abstract": "Employing part-level features for pedestrian image description offers\nfine-grained information and has been verified as beneficial for person\nretrieval in very recent literature. A prerequisite of part discovery is that\neach part should be well located. Instead of using external cues, e.g., pose\nestimation, to directly locate parts, this paper lays emphasis on the content\nconsistency within each part.\n  Specifically, we target at learning discriminative part-informed features for\nperson retrieval and make two contributions. (i) A network named Part-based\nConvolutional Baseline (PCB). Given an image input, it outputs a convolutional\ndescriptor consisting of several part-level features. With a uniform partition\nstrategy, PCB achieves competitive results with the state-of-the-art methods,\nproving itself as a strong convolutional baseline for person retrieval.\n  (ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs\noutliers in each part, which are in fact more similar to other parts. RPP\nre-assigns these outliers to the parts they are closest to, resulting in\nrefined parts with enhanced within-part consistency. Experiment confirms that\nRPP allows PCB to gain another round of performance boost. For instance, on the\nMarket-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1\naccuracy, surpassing the state of the art by a large margin.",
            "authors": [
                "Yifan Sun",
                "Liang Zheng",
                "Yi Yang",
                "Qi Tian",
                "Shengjin Wang"
            ]
        },
        {
            "year": "2019",
            "journal": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "author": [
                "Zheng, Zhedong",
                "Yang, Xiaodong",
                "Yu, Zhiding",
                "Zheng, Liang",
                "Yang, Yi",
                "Kautz, Jan"
            ],
            "title": "Joint discriminative and generative learning for person re-identification",
            "ENTRYTYPE": "article",
            "ID": "zheng2019joint",
            "authors": [
                "Zheng, Zhedong",
                "Yang, Xiaodong",
                "Yu, Zhiding",
                "Zheng, Liang",
                "Yang, Yi",
                "Kautz, Jan"
            ]
        },
        {
            "year": "2017",
            "booktitle": "ICCV",
            "title": "SVDNet for Pedestrian Retrieval",
            "author": [
                "Sun, Yifan",
                "Zheng, Liang",
                "Deng, Weijian",
                "Wang, Shengjin"
            ],
            "ENTRYTYPE": "article",
            "ID": "DBLP:journals/corr/SunZDW17",
            "authors": [
                "Sun, Yifan",
                "Zheng, Liang",
                "Deng, Weijian",
                "Wang, Shengjin"
            ]
        },
        {
            "year": 2017,
            "journal": "arXiv preprint arXiv:1703.07737",
            "author": [
                "Hermans, Alexander",
                "Beyer, Lucas",
                "Leibe, Bastian"
            ],
            "title": "In Defense of the Triplet Loss for Person Re-Identification",
            "ENTRYTYPE": "article",
            "ID": "hermans2017defense",
            "authors": [
                "Alexander Hermans",
                "Lucas Beyer",
                "Bastian Leibe"
            ],
            "arxiv": "1703.07737"
        },
        {
            "publisher": "ACM",
            "year": "2018",
            "pages": "13",
            "number": "1",
            "volume": "14",
            "journal": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)",
            "author": [
                "Zheng, Zhedong",
                "Zheng, Liang",
                "Yang, Yi"
            ],
            "title": "A discriminatively learned CNN embedding for person reidentification",
            "ENTRYTYPE": "article",
            "ID": "zheng2018discriminatively",
            "authors": [
                "Zheng, Zhedong",
                "Zheng, Liang",
                "Yang, Yi"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Market1501 Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://www.liangzheng.com.cn/Project/project_reid.html"
                    }
                }
            },
            {
                "name": "DukeMTMC-reID Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "[GoogleDriver](https://drive.google.com/open?id=1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O"
                    }
                }
            },
            {
                "name": "Market-1501"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999883778464126,
        "task": "Person Re-Identification",
        "task_prob": 0.9851509216709681
    }
}