{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "StyledFontGAN",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "joshpc",
                "owner_type": "User",
                "name": "StyledFontGAN",
                "url": "https://github.com/joshpc/StyledFontGAN",
                "stars": 17,
                "pushed_at": "2020-04-30 15:51:25+00:00",
                "created_at": "2020-03-23 21:02:06+00:00",
                "language": "Jupyter Notebook",
                "description": "A GAN experiment that, given an image of a letter, produces an alphabet in the same style.",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "3a97dbb26b5f28e27dbd88adb56884e4795d13d9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/blob/master/.gitignore"
                    }
                },
                "size": 1810
            },
            {
                "type": "code",
                "name": ".vscode",
                "sha": "56987ae272b6ca25005100bf494a1612807e38a6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/tree/master/.vscode"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "18937b7ceb368cf7d66fa88eff8cd435d28149e0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/blob/master/LICENSE"
                    }
                },
                "size": 1071
            },
            {
                "type": "code",
                "name": "Playground.ipynb",
                "sha": "e0ca76f21a276934e7a80ebdbfd5c6ed38ca4cb0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/blob/master/Playground.ipynb"
                    }
                },
                "size": 36468134
            },
            {
                "type": "code",
                "name": "data",
                "sha": "7573bb6697fa9c49b2bd3beb86cec8f970d35eae",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/tree/master/data"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "environment.yml",
                "sha": "f7f69e5eb8c5d0ece312d7e7222ae6baeae03ce3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/blob/master/environment.yml"
                    }
                },
                "size": 2751
            },
            {
                "type": "code",
                "name": "models",
                "sha": "23b553fe79683c92c98bbc087fd3386d82b5f921",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/tree/master/models"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "results",
                "sha": "7ebaa6b2b9dc78781a6658abb0b94f4fb9711348",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/tree/master/results"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/blob/master/test.py"
                    }
                },
                "size": 0
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "f263fa59c1d6323058ba994826ffbfc89d300098",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/tree/master/tests"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "6b8fbb34fb15a1ad1694ac767534b369df7087f1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/blob/master/train.py"
                    }
                },
                "size": 4910
            },
            {
                "type": "code",
                "name": "trained",
                "sha": "639735cb6842f40f3859ea902332e1737e002b59",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/tree/master/trained"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "util.py",
                "sha": "9951706393aa535a831f9a866c69d4e2ab730041",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/joshpc/StyledFontGAN/blob/master/util.py"
                    }
                },
                "size": 268
            }
        ]
    },
    "authors": [
        {
            "name": "Joshua Tessier",
            "email": "joshpc@gmail.com",
            "github_id": "joshpc"
        }
    ],
    "tags": [],
    "description": "A GAN experiment that, given an image of a letter, produces an alphabet in the same style.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/joshpc/StyledFontGAN",
            "stars": 17,
            "issues": true,
            "readme": "# StyledFontGAN\nThe original intent of this project was to create a generative adversarial network that, given a single image of a letter, will generate the remaining characters in the alphabet (A-Z.)\n\n## Motivation Behind This Project\nThis is a simple project that I've been using to teach myself all about GANs, and getting familiar with Python. I don't claim to be talented when it comes to machine learning, but I want to understand it's capabilities and the challenges that can arise.\n\nPlease note that this is a side project, and not academic research. Ultimately, the goal is to apply this to non-latin alphabets where producing fonts are extremely time consuming.\n\n## Prior Work and Acknowledgements\n\nA lot of this project is inspired by the following research papers and their associated code:\n\n- [MC-GAN](https://github.com/azadis/MC-GAN) - Multi-Content GAN for Few-Shot Font Style Transfer; Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen Wang, Eli Shechtman, Trevor Darrell, in arXiv, 2017.\n- [GlyphGAN](https://arxiv.org/abs/1905.12502v1) - GlyphGAN: Style-Consistent Font Generation Based on Generative Adversarial Networks; Hideaki Hayashi, Kohtaro Abe, Seiichi Uchida, in arXiv, 2019\n- [DC-GAN](https://arxiv.org/abs/1511.06434) - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks; Alec Radford, Luke Metz, Soumith Chintala, in arXiv, 2016\n- [WGAN-GP](https://arxiv.org/abs/1704.00028) - Improved Training of Wasserstein GANs; Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville, in arXiv, 2017\n\n- Some pieces of code, specifically the `Flatten` and `Unflatten` methods were pulled from the [CS231N course](https://cs231n.github.io/) samples.\n\nThis project was made possible by this research and the contributions made by the above authors. Thank you.\n\n## Architecture\n\nThe most succcessful model has been trained with the following generator and discriminators, and using an **L1 Loss** for the Generator, and the loss from **WGAN-GP** (Wasserstein Distance + Gradiant Penality) for the Discriminator.\n\n### Generator Architecture\n\nThe thought process behind this network architecture is not well informed. The inuition is that we take an image, extract its features with Conv layers, turn it into some intermediate format within the Linear layers, then use that intermediate format to generate a new image using the ConvTranspose layers.\n\nThe result is good, but it's not ideal. Currently working on identifying what layers are actually contributing and determining if there are layers that are unnecessary.\n\n```\nSequential(\n  (0): Conv2d(1, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (1): LeakyReLU(negative_slope=0.2)\n  (2): Conv2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (3): LeakyReLU(negative_slope=0.2)\n  (4): Conv2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (5): LeakyReLU(negative_slope=0.2)\n  (6): Flatten()\n  (7): Linear(in_features=1024, out_features=256, bias=True)\n  (8): ReLU()\n  (9): Linear(in_features=256, out_features=5120, bias=True)\n  (10): ReLU()\n  (11): Unflatten()\n  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (13): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (15): ReLU()\n  (16): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (18): ReLU()\n  (19): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (20): Sigmoid()\n)\n```\n\n### Discriminator Architecture\n\nThis follows DC-GAN. No magic, or added things here. As with the linked papers from above, we do not apply batch normalization which helps with training.\n\n```\n(Sequential(\n   (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n   (1): LeakyReLU(negative_slope=0.2)\n   (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n   (3): LeakyReLU(negative_slope=0.2)\n   (4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n   (5): LeakyReLU(negative_slope=0.2)\n   (6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n   (7): LeakyReLU(negative_slope=0.2)\n   (8): Flatten()\n   (9): Linear(in_features=640, out_features=1, bias=True)\n   (10): Sigmoid()\n ),\n```\n\n## Results\n\nThis was by no means an academic study nor was it an attempt to push the boundaries of current research, but the results were good.\n\nThis project used the data set from [MC-GAN](https://github.com/azadis/MC-GAN) which uses a variety of different fonts and samples. In some cases, the samples are all uppercase letters, while as others are a mix of uppercase and lowercase letters.\n\n### Samples\n\n#### Successes\nIn these examples, the results are both legible and match the original style.\n\n![First Font](results/results_2.png)\n![First Font](results/results_5.png)\n![First Font](results/results_6.png)\n![First Font](results/results_7.png)\n![First Font](results/results_8.png)\n\n### Questionable Failures\n![First Font](results/results_1.png)\nThe network succeeded in capturing the style, with the dark shadows, but couldn't produce legible letters.\n\n![First Font](results/results_4.png)\nThe letters aren't very clear, but despite the strange style, it seems to have respected it.\n\n### Failures\n![First Font](results/results_3.png)\nThe network failed to produce legible letters, and failed to copy the style.\n\n### Conclusions\nTBD\n\n## Setup\n\n1. Install `pytorch`: https://pytorch.org/get-started/locally/\n2. For now, you will also need a tool to view notebooks. I use Jupyter.\n3.\n3. Dependencies: TBD\n",
            "readme_url": "https://github.com/joshpc/StyledFontGAN",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Improved Training of Wasserstein GANs",
            "arxiv": "1704.00028",
            "year": 2017,
            "url": "http://arxiv.org/abs/1704.00028v3",
            "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but\nsuffer from training instability. The recently proposed Wasserstein GAN (WGAN)\nmakes progress toward stable training of GANs, but sometimes can still generate\nonly low-quality samples or fail to converge. We find that these problems are\noften due to the use of weight clipping in WGAN to enforce a Lipschitz\nconstraint on the critic, which can lead to undesired behavior. We propose an\nalternative to clipping weights: penalize the norm of gradient of the critic\nwith respect to its input. Our proposed method performs better than standard\nWGAN and enables stable training of a wide variety of GAN architectures with\nalmost no hyperparameter tuning, including 101-layer ResNets and language\nmodels over discrete data. We also achieve high quality generations on CIFAR-10\nand LSUN bedrooms.",
            "authors": [
                "Ishaan Gulrajani",
                "Faruk Ahmed",
                "Martin Arjovsky",
                "Vincent Dumoulin",
                "Aaron Courville"
            ]
        },
        {
            "title": "GlyphGAN: Style-Consistent Font Generation Based on Generative Adversarial Networks",
            "arxiv": "1905.12502",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.12502v2",
            "abstract": "In this paper, we propose GlyphGAN: style-consistent font generation based on\ngenerative adversarial networks (GANs). GANs are a framework for learning a\ngenerative model using a system of two neural networks competing with each\nother. One network generates synthetic images from random input vectors, and\nthe other discriminates between synthetic and real images. The motivation of\nthis study is to create new fonts using the GAN framework while maintaining\nstyle consistency over all characters. In GlyphGAN, the input vector for the\ngenerator network consists of two vectors: character class vector and style\nvector. The former is a one-hot vector and is associated with the character\nclass of each sample image during training. The latter is a uniform random\nvector without supervised information. In this way, GlyphGAN can generate an\ninfinite variety of fonts with the character and style independently\ncontrolled. Experimental results showed that fonts generated by GlyphGAN have\nstyle consistency and diversity different from the training images without\nlosing their legibility.",
            "authors": [
                "Hideaki Hayashi",
                "Kohtaro Abe",
                "Seiichi Uchida"
            ]
        },
        {
            "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "arxiv": "1511.06434",
            "year": 2015,
            "url": "http://arxiv.org/abs/1511.06434v2",
            "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations.",
            "authors": [
                "Alec Radford",
                "Luke Metz",
                "Soumith Chintala"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999946142653217,
        "task": "Image Generation",
        "task_prob": 0.9783742814460293
    },
    "training": {
        "datasets": [
            {
                "name": "CIFAR-10"
            }
        ]
    }
}