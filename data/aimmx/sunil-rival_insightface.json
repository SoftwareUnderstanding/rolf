{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "InsightFace: 2D and 3D Face Analysis Project",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "sunil-rival",
                "owner_type": "User",
                "name": "insightface",
                "url": "https://github.com/sunil-rival/insightface",
                "stars": 1,
                "pushed_at": "2018-10-02 23:08:48+00:00",
                "created_at": "2018-09-13 22:31:22+00:00",
                "language": "Python",
                "description": "Forked insightface code to train in AWS",
                "license": "MIT License",
                "frameworks": [
                    "Caffe",
                    "MXNet",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "f0232fae1f30c1864102ea428e22878d0550d11c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/blob/master/.gitignore"
                    }
                },
                "size": 1132
            },
            {
                "type": "code",
                "name": ".gitmodules",
                "sha": "6c4c7f9803d071e278b5dd1dc9f81302b6aa3c21",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/blob/master/.gitmodules"
                    }
                },
                "size": 101
            },
            {
                "type": "code",
                "name": ".python-version",
                "sha": "d15b8b06fa347d6fdb062184febdbe5fef95c4ad",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/blob/master/.python-version"
                    }
                },
                "size": 6
            },
            {
                "type": "code",
                "name": "3rdparty",
                "sha": "2312845078645b801cba42a371ee4b771564d588",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/3rdparty"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "1562c52dff26c5d846489922bd15eeb8c16f1a80",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/blob/master/LICENSE"
                    }
                },
                "size": 1082
            },
            {
                "type": "code",
                "name": "Makefile",
                "sha": "7d45384548998e6580df156c6a36c1e0e0d2cd26",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/blob/master/Makefile"
                    }
                },
                "size": 1094
            },
            {
                "type": "code",
                "name": "SSH",
                "sha": "e81a86112c83a6479b025b6e10090835110102d8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/SSH"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "alignment",
                "sha": "c641163ce0c09631f82588bbb50d0e7b06fe0161",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/alignment"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "datasets",
                "sha": "e886b4d17e9105e7c80505255bbf636b70368da0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/datasets"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "deploy",
                "sha": "6a66517868ed724796d2949b776004857cb452a3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/deploy"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "gluon",
                "sha": "a418f31f7b2f202b3b558dfed986281e0430f714",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/gluon"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "install_script.sh",
                "sha": "6261ed2c996fb6ba6cb834b8ee3c7ca412cf5387",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/blob/master/install_script.sh"
                    }
                },
                "size": 2167
            },
            {
                "type": "code",
                "name": "models",
                "sha": "38f6eff72fd6deb603d9cb0b9099e94ad078a41e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/models"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "recognition",
                "sha": "1c6d2f51e124d402064e0b52f9e2c924611026b2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/recognition"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "e27232622098e8e913c23888eb43f510622a464a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/blob/master/requirements.txt"
                    }
                },
                "size": 97
            },
            {
                "type": "code",
                "name": "resources",
                "sha": "f1b638fa66c3bf5969129fb09fbf4ff4ed028be4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/resources"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "rival-src",
                "sha": "3b2a0b5a8a1714e97bb683e7486ffe3efb9c2356",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/rival-src"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "sample-images",
                "sha": "ef62819e7d2a87df8add1b5f0d0ef9e70c4e3d4c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/sample-images"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "src",
                "sha": "6c62bed015cba51fffe0ef6e7d77072a265a4143",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/sunil-rival/insightface/tree/master/src"
                    }
                },
                "num_files": 17
            }
        ]
    },
    "authors": [
        {
            "name": "Jia Guo",
            "email": "guojia@gmail.com",
            "github_id": "nttstar"
        },
        {
            "name": "JiankangDeng",
            "email": "j.deng16@imperial.ac.uk",
            "github_id": "jiankangdeng"
        },
        {
            "name": "sunil-rival",
            "github_id": "sunil-rival"
        },
        {
            "name": "Sunil Pinnamaneni",
            "github_id": "raisin"
        },
        {
            "name": "Yingfeng",
            "email": "yingfeng.zhang@gmail.com",
            "github_id": "yingfeng"
        }
    ],
    "tags": [],
    "description": "Forked insightface code to train in AWS",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/sunil-rival/insightface",
            "stars": 1,
            "issues": true,
            "readme": "\n# InsightFace: 2D and 3D Face Analysis Project\n\nBy Jia Guo and [Jiankang Deng](https://jiankangdeng.github.io/)\n\n## License\n\nThe code of InsightFace is released under the MIT License.\n\n## Recent Update\n\n**`2018.07.17`**: [Model-Zoo](https://github.com/deepinsight/insightface/wiki/Model-Zoo), [Dataset-Zoo](https://github.com/deepinsight/insightface/wiki/Dataset-Zoo)\n\n**`2018.06.14`**: There's a large scale Asian training dataset provided by Glint, see this [discussion](https://github.com/deepinsight/insightface/issues/256) for detail.\n\n**`2018.05.16`**: A new training dataset released [here](https://pan.baidu.com/s/1S6LJZGdqcZRle1vlcMzHOQ) which can easily achieve much better accuracy. See [discussion](https://github.com/deepinsight/insightface/issues/215) for detail.\n\n**`2018.04.23`**: Our implementation of [MobileFaceNet](https://arxiv.org/abs/1804.07573) is now available. Please set `--network y1` to use this lightweight but powerful backbone.\n\n**`2018.03.26`**: We can train with combined margin(loss-type=5), see [Verification Results On Combined Margin](#verification-results-on-combined-margin).\n\n**`2018.02.13`**: We achieved state-of-the-art performance on [MegaFace-Challenge](http://megaface.cs.washington.edu/results/facescrub.html). Please check our paper and code for implementation details.\n\n## Contents\n[Deep Face Recognition](#deep-face-recognition)\n- [Introduction](#introduction)\n- [Training Data](#training-Data)\n- [Train](#train)\n- [Pretrained Models](#pretrained-models)\n- [Verification Results On Combined Margin](#verification-results-on-combined-margin)\n- [Test on MegaFace](#test-on-megaface)\n- [512-D Feature Embedding](#512-d-feature-embedding)\n- [Third-party Re-implementation](#third-party-re-implementation)\n\n[Face Alignment](#face-alignment)\n\n[Face Detection](#face-detection)\n\n[Citation](#citation)\n\n[Contact](#contact)\n\n## Deep Face Recognition\n\n### Introduction\n\nIn this repository, we provide training data, network settings and loss designs for deep face recognition.\nThe training data includes the normalised MS1M and VGG2 datasets, which were already packed in the MxNet binary format.\nThe network backbones include ResNet, InceptionResNet_v2, DenseNet, DPN and MobiletNet.\nThe loss functions include Softmax, SphereFace, CosineFace, ArcFace and Triplet (Euclidean/Angular) Loss.\n* loss-type=0:  Softmax\n* loss-type=1:  SphereFace\n* loss-type=2:  CosineFace\n* loss-type=4:  ArcFace\n* loss-type=5:  Combined Margin\n* loss-type=12: TripletLoss\n\n![margin penalty for target logit](https://github.com/deepinsight/insightface/raw/master/resources/arcface.png)\n\nOur method, ArcFace, was initially described in an [arXiv technical report](https://arxiv.org/abs/1801.07698). By using this repository, you can simply achieve LFW 99.80%+ and Megaface 98%+ by a single model. This repository can help researcher/engineer to develop deep face recognition algorithms quickly by only two steps: download the binary dataset and run the training script.\n\n### Training Data\n\nAll face images are aligned by [MTCNN](https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html) and cropped to 112x112:\n\n* [Refined-MS1M@BaiduDrive](https://pan.baidu.com/s/1nxmSCch), [Refined-MS1M@GoogleDrive](https://drive.google.com/file/d/1XRdCt3xOw7B3saw0xUSzLRub_HI4Jbk3/view)\n* [VGGFace2@BaiduDrive](https://pan.baidu.com/s/1c3KeLzy), [VGGFace2@GoogleDrive](https://drive.google.com/open?id=1KORwx_DWyIScAjD6vbo4CSRu048APoum)\n* Please check *src/data/face2rec2.py* on how to build a binary face dataset. Any public available *MTCNN* can be used to align the faces, and the performance should not change. We will improve the face normalisation step by full pose alignment methods recently.\n\n**Note:** If you use the refined [MS1M](https://arxiv.org/abs/1607.08221) dataset and the cropped [VGG2](https://arxiv.org/abs/1710.08092) dataset, please cite the original papers.\n\n### Train\n\n1. Install `MXNet` with GPU support (Python 2.7).\n\n```\npip install mxnet-cu80\n```\n\n2. Clone the InsightFace repository. We call the directory insightface as *`INSIGHTFACE_ROOT`*.\n\n```\ngit clone --recursive https://github.com/deepinsight/insightface.git\n```\n\n3. Download the training set (`MS1M`) and place it in *`$INSIGHTFACE_ROOT/datasets/`*. Each training dataset includes following 7 files:\n\n```Shell\n    faces_ms1m_112x112/\n       train.idx\n       train.rec\n       property\n       lfw.bin\n       cfp_ff.bin\n       cfp_fp.bin\n       agedb_30.bin\n```\n\nThe first three files are the training dataset while the last four files are verification sets.\n\n4. Train deep face recognition models.\nIn this part, we assume you are in the directory *`$INSIGHTFACE_ROOT/src/`*.\n```\nexport MXNET_CPU_WORKER_NTHREADS=24\nexport MXNET_ENGINE_TYPE=ThreadedEnginePerDevice\n```\nWe give some examples below. Our experiments were conducted on the Tesla P40 GPU.\n\n(1). Train ArcFace with LResNet100E-IR.\n\n```Shell\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir ../datasets/faces_ms1m_112x112  --prefix ../model-r100\n```\nIt will output verification results of *LFW*, *CFP-FF*, *CFP-FP* and *AgeDB-30* every 2000 batches. You can check all command line options in *train\\_softmax.py*.\nThis model can achieve *LFW 99.80+* and *MegaFace 98.0%+*.\n\n(2). Train CosineFace with LResNet50E-IR.\n\n```Shell\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network r50 --loss-type 2 --margin-m 0.35 --data-dir ../datasets/faces_ms1m_112x112 --prefix ../model-r50-amsoftmax\n```\n\n(3). Train Softmax with LMobileNetE.\n\n```Shell\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network m1 --loss-type 0 --data-dir ../datasets/faces_ms1m_112x112 --prefix ../model-m1-softmax\n```\n\n(4). Fine-turn the above Softmax model with Triplet loss.\n\n```Shell\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_triplet.py --network m1 --lr 0.005 --mom 0.0 --per-batch-size 150 --data-dir ../datasets/faces_ms1m_112x112 --pretrained ../model-m1-softmax,50 --prefix ../model-m1-triplet\n```\n\n(5). Train LDPN107E network with Softmax loss on VGGFace2 dataset.\n\n```Shell\nCUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python -u train_softmax.py --network p107 --loss-type 0 --per-batch-size 64 --data-dir ../datasets/faces_vgg_112x112 --prefix ../model-p107-softmax\n```\n\n5. Verification results.\n\n*LResNet100E-IR* network trained on *MS1M* dataset with ArcFace loss:\n\n| Method  | LFW(%) | CFP-FF(%) | CFP-FP(%) | AgeDB-30(%) |  \n| ------- | ------ | --------- | --------- | ----------- |  \n|  Ours   | 99.80+ | 99.85+    | 94.0+     | 97.90+      |   \n\n*LResNet50E-IR* network trained on *VGGFace2* dataset with ArcFace loss:\n\n| Method  | LFW(%) | CFP-FF(%) | CFP-FP(%) | AgeDB-30(%) |\n| ------- | ------ | --------- | --------- | ----------- |  \n|  Ours   | 99.7+  |  99.6+    |   97.1+   |   95.7+     |  \n\nWe report the verification accuracy after removing training set overlaps to strictly follow the evaluation metric. `(C) means after cleaning`\n\n| Dataset  | Identities | Images  | Identites(C) | Images(C) | Acc   | Acc(C) |\n| -------- | ---------- | ------- | ------------ | --------- | ----- | ------ |\n| LFW      | 85742      | 3850179 | 80995        | 3586128   | 99.83 | 99.81  |\n| CFP-FP   | 85742      | 3850179 | 83706        | 3736338   | 94.04 | 94.03  |\n| AgeDB-30 | 85742      | 3850179 | 83775        | 3761329   | 98.08 | 97.87  |\n\n### Pretrained Models\n\nYou can use `$INSIGHTFACE/src/eval/verification.py` to test all the pre-trained models.\n\n1. [LResNet50E-IR@BaiduDrive](https://pan.baidu.com/s/1mj6X7MK), [@GoogleDrive](https://drive.google.com/open?id=1x0-EiYX9jMUKiq-n1Bd9OCK4fVB3a54v)\nPerformance:\n\n| Method  | LFW(%)     | CFP-FF(%) | CFP-FP(%) | AgeDB-30(%) | MegaFace(%)   |\n| ------- | ------     | --------- | --------- | ----------- | ------------- |\n|  Ours   | 99.80      | 99.83     | 92.74     | 97.76       | 97.64         |\n\n2. [LResNet34E-IR@BaiduDrive](https://pan.baidu.com/s/1jKahEXw)\nPerformance:\n\n| Method  | LFW(%) | CFP-FF(%) | CFP-FP(%) | AgeDB-30(%) | MegaFace(%)   |\n| ------- | ------ | --------- | --------- | ----------- | ------------- |\n|  Ours   | 99.65  | 99.77     | 92.12     | 97.70       | 96.70         |\n\n\n*`Caffe`* [LResNet50E-IR@BaiduDrive](https://pan.baidu.com/s/1ENjcACInLfBGHZ8e7Fc-XA), converted by above MXNet model.\nPerformance:\n\n| Method  | LFW(%) | CFP-FF(%) | CFP-FP(%) | AgeDB-30(%) | MegaFace1M(%) |\n| ------- | ------ | --------- | --------- | ----------- | ------------- |\n|  Ours   | 99.74  | -TBD-     | -TBD-     | -TBD-       | -TBD-         |\n\n\n### Verification Results on Combined Margin\n\nA combined margin method was proposed as a function of target logits value and original `\u03b8`:\n\n```\nCOM(\u03b8) = cos(m_1*\u03b8+m_2) - m_3\n```\n\nFor training with `m1=0.9, m2=0.4, m3=0.15`, run following command:\n```\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network r100 --loss-type 5 --margin-a 0.9 --margin-m 0.4 --margin-b 0.15 --data-dir ../datasets/faces_ms1m_112x112  --prefix ../model-r100\n```\n\n| Method           | m1   | m2   | m3   | LFW   | CFP-FP | AgeDB-30 |\n| ---------------- | ---- | ---- | ---- | ----- | ------ | -------- |\n| W&F Norm Softmax | 1    | 0    | 0    | 99.28 | 88.50  | 95.13    |\n| SphereFace       | 1.5  | 0    | 0    | 99.76 | 94.17  | 97.30    |\n| CosineFace       | 1    | 0    | 0.35 | 99.80 | 94.4   | 97.91    |\n| ArcFace          | 1    | 0.5  | 0    | 99.83 | 94.04  | 98.08    |\n| Combined Margin  | 1.2  | 0.4  | 0    | 99.80 | 94.08  | 98.05    |\n| Combined Margin  | 1.1  | 0    | 0.35 | 99.81 | 94.50  | 98.08    |\n| Combined Margin  | 1    | 0.3  | 0.2  | 99.83 | 94.51  | 98.13    |\n| Combined Margin  | 0.9  | 0.4  | 0.15 | 99.83 | 94.20  | 98.16    |\n\n### Test on MegaFace\n\nIn this part, we assume you are in the directory *`$INSIGHTFACE_ROOT/src/megaface/`*.\n\n**Note:** We found there are overlap identities between facescrub dataset and Megaface distractors, which significantly affects the identification performance. This list is released under *`$INSIGHTFACE_ROOT/src/megaface/`*.\n\n1. Align all face images of facescrub dataset and megaface distractors. Please check the alignment scripts under *`$INSIGHTFACE_ROOT/src/align/`*.\n2. Generate feature files for both facescrub and megaface images.\n```\npython -u gen_megaface.py\n```\n3. Remove Megaface noises which generates new feature files.\n```\npython -u remove_noises.py\n```\n4. Run megaface development kit to produce final result.\n\n### 512-D Feature Embedding\n\nIn this part, we assume you are in the directory *`$INSIGHTFACE_ROOT/deploy/`*. The input face image should be generally centre cropped. We use *RNet+ONet* of *MTCNN* to further align the image before sending it to the feature embedding network.\n\n1. Prepare a pre-trained model.\n2. Put the model under *`$INSIGHTFACE_ROOT/models/`*. For example, *`$INSIGHTFACE_ROOT/models/model-r34-amf`*.\n3. Run the test script *`$INSIGHTFACE_ROOT/deploy/test.py`*.\n\nFor single cropped face image(112x112), total inference time is only 17ms on our testing server(Intel E5-2660 @ 2.00GHz, Tesla M40, *LResNet34E-IR*).\n\n### Third-party Re-implementation\n\n- TensorFlow: [InsightFace_TF](https://github.com/auroua/InsightFace_TF)\n\n## Face Alignment\n\nTodo\n\n## Face Detection\n\nTodo\n\n## Citation\n\nIf you find *InsightFace* useful in your research, please consider to cite the following related papers:\n\n```\n@article{deng2018arcface,\ntitle={ArcFace: Additive Angular Margin Loss for Deep Face Recognition},\nauthor={Deng, Jiankang and Guo, Jia and Zafeiriou, Stefanos},\njournal={arXiv:1801.07698},\nyear={2018}\n}\n```\n\n## Contact\n\n```\n[Jia Guo](guojia[at]gmail.com)\n[Jiankang Deng](jiankangdeng[at]gmail.com)\n```\n",
            "readme_url": "https://github.com/sunil-rival/insightface",
            "frameworks": [
                "Caffe",
                "MXNet",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
            "arxiv": "1801.07698",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.07698v3",
            "abstract": "One of the main challenges in feature learning using Deep Convolutional\nNeural Networks (DCNNs) for large-scale face recognition is the design of\nappropriate loss functions that enhance discriminative power. Centre loss\npenalises the distance between the deep features and their corresponding class\ncentres in the Euclidean space to achieve intra-class compactness. SphereFace\nassumes that the linear transformation matrix in the last fully connected layer\ncan be used as a representation of the class centres in an angular space and\npenalises the angles between the deep features and their corresponding weights\nin a multiplicative way. Recently, a popular line of research is to incorporate\nmargins in well-established loss functions in order to maximise face class\nseparability. In this paper, we propose an Additive Angular Margin Loss\n(ArcFace) to obtain highly discriminative features for face recognition. The\nproposed ArcFace has a clear geometric interpretation due to the exact\ncorrespondence to the geodesic distance on the hypersphere. We present arguably\nthe most extensive experimental evaluation of all the recent state-of-the-art\nface recognition methods on over 10 face recognition benchmarks including a new\nlarge-scale image database with trillion level of pairs and a large-scale video\ndataset. We show that ArcFace consistently outperforms the state-of-the-art and\ncan be easily implemented with negligible computational overhead. We release\nall refined training data, training codes, pre-trained models and training\nlogs, which will help reproduce the results in this paper.",
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Niannan Xue",
                "Stefanos Zafeiriou"
            ]
        },
        {
            "title": "VGGFace2: A dataset for recognising faces across pose and age",
            "arxiv": "1710.08092",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.08092v2",
            "abstract": "In this paper, we introduce a new large-scale face dataset named VGGFace2.\nThe dataset contains 3.31 million images of 9131 subjects, with an average of\n362.6 images for each subject. Images are downloaded from Google Image Search\nand have large variations in pose, age, illumination, ethnicity and profession\n(e.g. actors, athletes, politicians). The dataset was collected with three\ngoals in mind: (i) to have both a large number of identities and also a large\nnumber of images for each identity; (ii) to cover a large range of pose, age\nand ethnicity; and (iii) to minimize the label noise. We describe how the\ndataset was collected, in particular the automated and manual filtering stages\nto ensure a high accuracy for the images of each identity. To assess face\nrecognition performance using the new dataset, we train ResNet-50 (with and\nwithout Squeeze-and-Excitation blocks) Convolutional Neural Networks on\nVGGFace2, on MS- Celeb-1M, and on their union, and show that training on\nVGGFace2 leads to improved recognition performance over pose and age. Finally,\nusing the models trained on these datasets, we demonstrate state-of-the-art\nperformance on all the IARPA Janus face recognition benchmarks, e.g. IJB-A,\nIJB-B and IJB-C, exceeding the previous state-of-the-art by a large margin.\nDatasets and models are publicly available.",
            "authors": [
                "Qiong Cao",
                "Li Shen",
                "Weidi Xie",
                "Omkar M. Parkhi",
                "Andrew Zisserman"
            ]
        },
        {
            "title": "MobileFaceNets: Efficient CNNs for Accurate Real-Time Face Verification on Mobile Devices",
            "arxiv": "1804.07573",
            "year": 2018,
            "url": "http://arxiv.org/abs/1804.07573v4",
            "abstract": "We present a class of extremely efficient CNN models, MobileFaceNets, which\nuse less than 1 million parameters and are specifically tailored for\nhigh-accuracy real-time face verification on mobile and embedded devices. We\nfirst make a simple analysis on the weakness of common mobile networks for face\nverification. The weakness has been well overcome by our specifically designed\nMobileFaceNets. Under the same experimental conditions, our MobileFaceNets\nachieve significantly superior accuracy as well as more than 2 times actual\nspeedup over MobileNetV2. After trained by ArcFace loss on the refined\nMS-Celeb-1M, our single MobileFaceNet of 4.0MB size achieves 99.55% accuracy on\nLFW and 92.59% TAR@FAR1e-6 on MegaFace, which is even comparable to\nstate-of-the-art big CNN models of hundreds MB size. The fastest one of\nMobileFaceNets has an actual inference time of 18 milliseconds on a mobile\nphone. For face verification, MobileFaceNets achieve significantly improved\nefficiency over previous state-of-the-art mobile CNNs.",
            "authors": [
                "Sheng Chen",
                "Yang Liu",
                "Xiang Gao",
                "Zhen Han"
            ]
        },
        {
            "title": "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition",
            "arxiv": "1607.08221",
            "year": 2016,
            "url": "http://arxiv.org/abs/1607.08221v1",
            "abstract": "In this paper, we design a benchmark task and provide the associated datasets\nfor recognizing face images and link them to corresponding entity keys in a\nknowledge base. More specifically, we propose a benchmark task to recognize one\nmillion celebrities from their face images, by using all the possibly collected\nface images of this individual on the web as training data. The rich\ninformation provided by the knowledge base helps to conduct disambiguation and\nimprove the recognition accuracy, and contributes to various real-world\napplications, such as image captioning and news video analysis. Associated with\nthis task, we design and provide concrete measurement set, evaluation protocol,\nas well as training data. We also present in details our experiment setup and\nreport promising baseline results. Our benchmark task could lead to one of the\nlargest classification problems in computer vision. To the best of our\nknowledge, our training dataset, which contains 10M images in version 1, is the\nlargest publicly available one in the world.",
            "authors": [
                "Yandong Guo",
                "Lei Zhang",
                "Yuxiao Hu",
                "Xiaodong He",
                "Jianfeng Gao"
            ]
        },
        {
            "title": "Refined-MS1M@BaiduDrive",
            "url": "https://pan.baidu.com/s/1nxmSCch"
        },
        {
            "title": "VGGFace2@BaiduDrive",
            "url": "https://pan.baidu.com/s/1c3KeLzy"
        },
        {
            "title": "LResNet50E-IR@BaiduDrive",
            "url": "https://pan.baidu.com/s/1ENjcACInLfBGHZ8e7Fc-XA"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Dataset-Zoo",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/deepinsight/insightface/wiki/Dataset-Zoo"
                    }
                }
            },
            {
                "name": "MegaFace"
            },
            {
                "name": "LFW"
            },
            {
                "name": "VggFace2"
            },
            {
                "name": "IJB-B"
            },
            {
                "name": "IJB-A"
            },
            {
                "name": "IJB-C"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999968638187491,
        "task": "Face Verification",
        "task_prob": 0.9900329745348383
    }
}