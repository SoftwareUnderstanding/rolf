{
    "visibility": {
        "visibility": "public"
    },
    "name": "Tacotron",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "barronalex",
                "owner_type": "User",
                "name": "Tacotron",
                "url": "https://github.com/barronalex/Tacotron",
                "stars": 228,
                "pushed_at": "2018-05-11 22:26:22+00:00",
                "created_at": "2017-04-18 00:23:43+00:00",
                "language": "Python",
                "description": "Implementation of Google's Tacotron in TensorFlow",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "5d5dbc3703aa22e4db700b36923b47a29c0210b1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/barronalex/Tacotron/blob/master/.gitignore"
                    }
                },
                "size": 41
            },
            {
                "type": "code",
                "name": "audio.py",
                "sha": "84dada4a645b9da098d6d2b8a21fe8960b57edee",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/barronalex/Tacotron/blob/master/audio.py"
                    }
                },
                "size": 4233
            },
            {
                "type": "code",
                "name": "data_input.py",
                "sha": "a0c7e0e39dc0bb117de0bf3d30dff9a0d8acaa68",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/barronalex/Tacotron/blob/master/data_input.py"
                    }
                },
                "size": 3998
            },
            {
                "type": "code",
                "name": "download_data.sh",
                "sha": "c99cffdd95236098c9aacf5df21c7b58dae89901",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/barronalex/Tacotron/blob/master/download_data.sh"
                    }
                },
                "size": 819
            },
            {
                "type": "code",
                "name": "download_weights.sh",
                "sha": "946c40360c9dd18554bf9c1e4dde89a1b3900491",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/barronalex/Tacotron/blob/master/download_weights.sh"
                    }
                },
                "size": 346
            },
            {
                "type": "code",
                "name": "images",
                "sha": "fc4f2d14a0109d2820e5d908aa3aea2fa3af93fa",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/barronalex/Tacotron/tree/master/images"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "models",
                "sha": "f45325b3b0cbb8fb5be885bad44f09bf76909e58",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/barronalex/Tacotron/tree/master/models"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "preprocess.py",
                "sha": "314fa0da59b00fe98f492eee9253c9cb509fa21c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/barronalex/Tacotron/blob/master/preprocess.py"
                    }
                },
                "size": 6678
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "ba6082baf2e11625bb3edb3d062b7bd8a8e250cb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/barronalex/Tacotron/blob/master/test.py"
                    }
                },
                "size": 2950
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "19981d77555a57333b80960fcab44e2c3f904b3d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/barronalex/Tacotron/blob/master/train.py"
                    }
                },
                "size": 4345
            }
        ]
    },
    "authors": [
        {
            "name": "Alex Barron",
            "github_id": "barronalex"
        },
        {
            "name": "Syoyo Fujita",
            "email": "syoyo@lighttransport.com",
            "github_id": "syoyo"
        },
        {
            "name": "Yunchao He",
            "github_id": "candlewill"
        }
    ],
    "tags": [],
    "description": "Implementation of Google's Tacotron in TensorFlow",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/barronalex/Tacotron",
            "stars": 228,
            "issues": true,
            "readme": "# Tacotron\n\nImplementation of [Tacotron](https://arxiv.org/abs/1703.10135), an end-to-end neural network for speech synthesis.\n\n## Samples\n\nThe following playlist contain samples produced on unseen inputs by Tacotron trained for 180K steps on the Nancy Corpus with r=2 and scheduled sampling 0.5. \n\n[Samples r=2](https://soundcloud.com/alex-barron-440014733/sets/tacotron-samples-r2)\n\nYou can try the synthesizer for yourself by running 'download_weights.sh' and then 'test.py' as described below.\n\nWhen compared to the [old samples](https://soundcloud.com/alex-barron-440014733/sets/tacotron-samples-1), the alignment learned with r=2 is considerably better but the audio quality is noticeably rougher.\nI assume this partially a result of too little training (the original paper trained for at least 20 times longer) but I think it is also related to the scheduled sampling that was necessary to learn the alignment. I also updated the padding which fixed the repetition and corruption at the end of the samples.\n\n## Requirements\n\n[Tensorflow 1.2](https://www.tensorflow.org/versions/r1.2/install/)\n\n[Librosa](https://github.com/librosa/librosa)\n\n[tqdm](https://github.com/noamraph/tqdm)\n\n[matplotlib](https://matplotlib.org/)\n\n## Data\n\nFor best results, use the [Nancy corpus](http://www.cstr.ed.ac.uk/projects/blizzard/2011/lessac_blizzard2011/) from the 2011 Blizzard Challenge. The data is freely availiable for research use on the signing of a license. After obtaining a username and password, add them to the 'download_data.sh' script to fetch it automatically. \n\nWe also download the considerably smaller [CMU ARCTIC](http://festvox.org/cmu_arctic/) dataset for testing which can be obtained without a license, but don't expect to get good results with it.\n\nYou can add new datasets in 'preprocess.py' by writing a 'prepare' function which produces a list of prompts and corresponding list of wav filenames. This should be clear from the examples in 'preprocess.py'.\n\n## Usage\n\nTo synthesize audio:\n\nFirst fetch the weights using the script provided\n\n\tbash download_weights.sh\n\nThen pass prompts (separated by end lines) to 'test.py' through stdin. The audio appears in Tensorboard.\n\n\tpython3 test.py < prompts.txt\n\t\n\techo \"This is a test prompt for the system to say.\" | python3 test.py\n\nTo train the model:\n\nFirst run the data fetching script (preferably after obtaining a username and password for the Nancy corpus)\n\n\tbash download_data.sh\n\nThen preprocess the data\n\n\tpython3 preprocess.py arctic\n\n\tpython3 preprocess.py nancy \n\n Now we're ready to start training\n\n\tpython3 train.py --train-set nancy (--restore optional)\n\nTo see the audio outputs created by Tacotron, open up Tensorboard.\n\nMonitoring the attention alignments produced under the images tab in Tensorboard is by far the best way to debug your model while its training. You'll likely only see generalization to new examples if/when the attention becomes monotonic. The gif below shows the model learning an alignment using the default parameters on the Nancy dataset.\n\n![Attention Alignments](https://github.com/barronalex/Tacotron/raw/master/images/attention.gif)\n\n## Updates\n\nThe memory usage has been significantly reduced. An 8 cpu instance on a cloud service should run the code with standard RAM. My macbook with 16GB of RAM also runs it fine (if incredibly slowly).\nThe reason it's so high is because empirically I found that there was around a 2X speed increase when reading the data from memory instead of disk.\n\nWith a K80 and r=2, we process 1 batch every ~2.5 seconds.\nWith a GTX1080 and r=2, we process 1 batch every ~1.5 seconds. \n\nI've begun to implement the multi-speaker tacotron architecture suggested by the [Deep Voice 2](https://arxiv.org/pdf/1705.08947.pdf) paper, but it's currently untested. 'preprocess.py' has the VCTK corpus implemented but you need to download the data. Given the scale of this dataset (40 hours), I assume we'll get better results if we can get it to work.\n\nParticularly if using a smaller dataset and no scheduled sampling, you're likely to see very different audio quality at training and test time, even on training examples.\nThis is a result of how this seq2seq model is trained -- in training, the ground truth is provided at each time step in the decoder but in testing we must use the previous time step as input. This will be noisey and thus result in poorer quality future outputs. Scheduled sampling addresses this.\n\n",
            "readme_url": "https://github.com/barronalex/Tacotron",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech",
            "arxiv": "1705.08947",
            "year": 2017,
            "url": "http://arxiv.org/abs/1705.08947v2",
            "abstract": "We introduce a technique for augmenting neural text-to-speech (TTS) with\nlowdimensional trainable speaker embeddings to generate different voices from a\nsingle model. As a starting point, we show improvements over the two\nstate-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and\nTacotron. We introduce Deep Voice 2, which is based on a similar pipeline with\nDeep Voice 1, but constructed with higher performance building blocks and\ndemonstrates a significant audio quality improvement over Deep Voice 1. We\nimprove Tacotron by introducing a post-processing neural vocoder, and\ndemonstrate a significant audio quality improvement. We then demonstrate our\ntechnique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron\non two multi-speaker TTS datasets. We show that a single neural TTS system can\nlearn hundreds of unique voices from less than half an hour of data per\nspeaker, while achieving high audio quality synthesis and preserving the\nspeaker identities almost perfectly.",
            "authors": [
                "Sercan Arik",
                "Gregory Diamos",
                "Andrew Gibiansky",
                "John Miller",
                "Kainan Peng",
                "Wei Ping",
                "Jonathan Raiman",
                "Yanqi Zhou"
            ]
        },
        {
            "title": "Tacotron: Towards End-to-End Speech Synthesis",
            "arxiv": "1703.10135",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10135v2",
            "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such\nas a text analysis frontend, an acoustic model and an audio synthesis module.\nBuilding these components often requires extensive domain expertise and may\ncontain brittle design choices. In this paper, we present Tacotron, an\nend-to-end generative text-to-speech model that synthesizes speech directly\nfrom characters. Given <text, audio> pairs, the model can be trained completely\nfrom scratch with random initialization. We present several key techniques to\nmake the sequence-to-sequence framework perform well for this challenging task.\nTacotron achieves a 3.82 subjective 5-scale mean opinion score on US English,\noutperforming a production parametric system in terms of naturalness. In\naddition, since Tacotron generates speech at the frame level, it's\nsubstantially faster than sample-level autoregressive methods.",
            "authors": [
                "Yuxuan Wang",
                "RJ Skerry-Ryan",
                "Daisy Stanton",
                "Yonghui Wu",
                "Ron J. Weiss",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Ying Xiao",
                "Zhifeng Chen",
                "Samy Bengio",
                "Quoc Le",
                "Yannis Agiomyrgiannakis",
                "Rob Clark",
                "Rif A. Saurous"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Nancy corpus",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://www.cstr.ed.ac.uk/projects/blizzard/2011/lessac_blizzard2011/"
                    }
                }
            }
        ]
    },
    "domain": {
        "domain_type": "Speech",
        "domain_prob": 0.9861882336508744
    }
}