{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "mmaction2",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "open-mmlab",
                "owner_type": "Organization",
                "name": "mmaction2",
                "url": "https://github.com/open-mmlab/mmaction2",
                "stars": 1815,
                "pushed_at": "2022-03-21 15:25:39+00:00",
                "created_at": "2020-07-11 07:19:10+00:00",
                "language": "Python",
                "description": "OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark",
                "license": "Apache License 2.0",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "461529dd8efe709a93e82e48af89eb8e821acb6f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/.github"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "587b296482453bcdca7be8b0efd8c99dc8e6ab1a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/.gitignore"
                    }
                },
                "size": 1630
            },
            {
                "type": "code",
                "name": ".pre-commit-config.yaml",
                "sha": "524e2eb267aa80f588eba421e41d9bd9919fd577",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/.pre-commit-config.yaml"
                    }
                },
                "size": 1506
            },
            {
                "type": "code",
                "name": ".pylintrc",
                "sha": "b1add44f16a5107d5e4d1f8ea713cdc4253ab49d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/.pylintrc"
                    }
                },
                "size": 19013
            },
            {
                "type": "code",
                "name": ".readthedocs.yml",
                "sha": "73ea4cb7e95530cd18ed94895ca38edd531f0d94",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/.readthedocs.yml"
                    }
                },
                "size": 145
            },
            {
                "type": "code",
                "name": "CITATION.cff",
                "sha": "93a03304abf45f2a781ebe6641a0da252f932246",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/CITATION.cff"
                    }
                },
                "size": 297
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "04adf5cbc620ad190547b092fa449e36df5f7bf4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/LICENSE"
                    }
                },
                "size": 11400
            },
            {
                "type": "code",
                "name": "MANIFEST.in",
                "sha": "258c4e016b9549e4eb2767738ebcf114ac064eec",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/MANIFEST.in"
                    }
                },
                "size": 137
            },
            {
                "type": "code",
                "name": "README_zh-CN.md",
                "sha": "83b4bfdce20e64778598c954ee3097830de96ebd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/README_zh-CN.md"
                    }
                },
                "size": 21307
            },
            {
                "type": "code",
                "name": "configs",
                "sha": "4b1f9011c3d0139e134486397f83604f4fb691c6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/configs"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "demo",
                "sha": "d2ac40e3d3b4f7942489427e1ecd605a029b22b0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/demo"
                    }
                },
                "num_files": 23
            },
            {
                "type": "code",
                "name": "docker",
                "sha": "d2cbe148ecccd037964b0ee5cd59e275e92ad47d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/docker"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "1f6875d697fb5ce86a10cac8920f93cd44de7f1f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/docs"
                    }
                },
                "num_files": 20
            },
            {
                "type": "code",
                "name": "docs_zh_CN",
                "sha": "9113494cc31aff8bc65b126f0555566b59dc9bd9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/docs_zh_CN"
                    }
                },
                "num_files": 19
            },
            {
                "type": "code",
                "name": "mmaction",
                "sha": "0b3f50387ae78697c6fe63ce7313b2db09a2571a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/mmaction"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "model-index.yml",
                "sha": "e76d6e5b17d6b38c35ab5baa47804f28bb33d50b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/model-index.yml"
                    }
                },
                "size": 945
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "3f6205f8dcdc6b2e9a1e1dee0ec65f0aad0b66c6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/requirements.txt"
                    }
                },
                "size": 81
            },
            {
                "type": "code",
                "name": "requirements",
                "sha": "d5054a53209360298b3764c70cec0742b80e1530",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/requirements"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "resources",
                "sha": "0d7c9e9be290f10c8fdb0c1b0ee0aac594da78b9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/resources"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "setup.cfg",
                "sha": "ad08ec3e4c8fd4b86d1e356328a5613812ca3860",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/setup.cfg"
                    }
                },
                "size": 609
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "ce4f1be67f1711792b63bf190e8d914cd031a843",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/setup.py"
                    }
                },
                "size": 7227
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "27feee519a0dd961f4831bd245ff2e8a8b66d168",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/tests"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "tools",
                "sha": "7a84ada7e0480c85969837c328718cefd2a1d3cc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/tree/master/tools"
                    }
                },
                "num_files": 13
            }
        ]
    },
    "authors": [
        {
            "name": "Jintao Lin",
            "email": "jintaolin@smail.nju.edu.cn",
            "github_id": "dreamerlin"
        },
        {
            "name": "Haodong Duan",
            "email": "dhd.efz@gmail.com",
            "github_id": "kennymckormick"
        },
        {
            "name": "lizz",
            "github_id": "innerlee"
        },
        {
            "name": "Xuanyi Li",
            "github_id": "JoannaLXY"
        },
        {
            "name": "su",
            "email": "xusu@sensetime.com",
            "github_id": "SuX97"
        },
        {
            "name": "irvingzhang0512",
            "email": "irvingzhang0512@gmail.com",
            "github_id": "irvingzhang0512"
        },
        {
            "name": "congee",
            "github_id": "congee524"
        },
        {
            "name": "gengenkai",
            "github_id": "gengenkai"
        },
        {
            "name": "Jiaqi Tang",
            "github_id": "tony2016uestc"
        },
        {
            "name": "BigBigDream",
            "github_id": "jiaoml1996"
        },
        {
            "name": "wang shiguang",
            "github_id": "sunnyxiaohu"
        },
        {
            "name": "Kai Chen",
            "email": "chenkaidev@gmail.com",
            "github_id": "hellock"
        },
        {
            "name": "Rejnald Lleshi",
            "github_id": "rlleshi"
        },
        {
            "name": "Jamie",
            "github_id": "jamiechoi1995"
        },
        {
            "name": "Michael P. Camilleri",
            "email": "michael.p.camilleri@ed.ac.uk",
            "github_id": "michael-camilleri"
        },
        {
            "name": "Wang Xiao",
            "github_id": "SCZwangxiao"
        },
        {
            "name": "makecent",
            "github_id": "makecent"
        },
        {
            "name": "WRH",
            "github_id": "wangruohui"
        },
        {
            "name": "Jas",
            "email": "jinsheng13@foxmail.com",
            "github_id": "jin-s13"
        },
        {
            "name": "Ycr",
            "github_id": "yaochaorui"
        },
        {
            "name": "yrqUni",
            "github_id": "yrqUni"
        },
        {
            "name": "yshira",
            "github_id": "yuta1125tp"
        },
        {
            "name": "yzfly",
            "email": "yzliu.me@gmail.com",
            "github_id": "yzfly"
        },
        {
            "name": "Shoufa Chen",
            "email": "shoufach@connect.hku.hk",
            "github_id": "ShoufaChen"
        },
        {
            "name": "Arda Kayagil",
            "github_id": "Domdu"
        },
        {
            "name": "Haoyue Cheng",
            "github_id": "CarolineCheng233"
        },
        {
            "name": "Chen Xu",
            "github_id": "HypnosXC"
        },
        {
            "name": "Devbrat Anuragi",
            "github_id": "AslanDevbrat"
        },
        {
            "name": "Johan Edstedt",
            "github_id": "Parskatt"
        },
        {
            "name": "Jonas Wu",
            "email": "1017454850@qq.com",
            "github_id": "wjn922"
        },
        {
            "name": "Miguel M\u00e9ndez",
            "email": "miguelmndez@gmail.com",
            "github_id": "mmeendez8"
        },
        {
            "name": "SebastienLinker",
            "github_id": "SebastienLinker"
        },
        {
            "name": "Tang Huan",
            "github_id": "tangh"
        },
        {
            "name": "Xin Wen",
            "email": "im.xwen@gmail.com",
            "github_id": "xwen99"
        },
        {
            "name": "ZHAO",
            "email": "iezhaoxy@163.com",
            "github_id": "TRillionZxY"
        },
        {
            "name": "Zelin Zhao",
            "email": "sjtuytc@gmail.com",
            "github_id": "sjtuytc"
        },
        {
            "name": "bit-scientist",
            "github_id": "bit-scientist"
        },
        {
            "name": "Steven",
            "email": "randy19962@gmail.com",
            "github_id": "jenhaoyang"
        },
        {
            "name": "ninja",
            "github_id": "hust-nj"
        },
        {
            "name": "weida wang",
            "github_id": "wwdok"
        }
    ],
    "tags": [
        "action-recognition",
        "temporal-action-localization",
        "pytorch",
        "video-understanding",
        "tsn",
        "i3d",
        "slowfast",
        "ava",
        "spatial-temporal-action-detection",
        "benchmark",
        "tsm",
        "x3d",
        "non-local",
        "deep-learning",
        "openmmlab",
        "posec3d",
        "video-classification"
    ],
    "description": "OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/open-mmlab/mmaction2",
            "stars": 1815,
            "issues": true,
            "readme": "<div align=\"center\">\n  <img src=\"https://github.com/open-mmlab/mmaction2/raw/master/resources/mmaction2_logo.png\" width=\"600\"/>\n  <div>&nbsp;</div>\n  <div align=\"center\">\n    <b><font size=\"5\">OpenMMLab website</font></b>\n    <sup>\n      <a href=\"https://openmmlab.com\">\n        <i><font size=\"4\">HOT</font></i>\n      </a>\n    </sup>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    <b><font size=\"5\">OpenMMLab platform</font></b>\n    <sup>\n      <a href=\"https://platform.openmmlab.com\">\n        <i><font size=\"4\">TRY IT OUT</font></i>\n      </a>\n    </sup>\n  </div>\n\n  [\ud83d\udcd8Documentation](https://mmaction2.readthedocs.io/en/latest/) |\n  [\ud83d\udee0\ufe0fInstallation](https://mmaction2.readthedocs.io/en/latest/install.html) |\n  [\ud83d\udc40Model Zoo](https://mmaction2.readthedocs.io/en/latest/modelzoo.html) |\n  [\ud83c\udd95Update News](https://mmaction2.readthedocs.io/en/latest/changelog.html) |\n  [\ud83d\ude80Ongoing Projects](https://github.com/open-mmlab/mmaction2/projects) |\n  [\ud83e\udd14Reporting Issues](https://github.com/open-mmlab/mmaction2/issues/new/choose)\n</div>\n\n## Introduction\n\nEnglish | [\u7b80\u4f53\u4e2d\u6587](/README_zh-CN.md)\n\n[![Documentation](https://readthedocs.org/projects/mmaction2/badge/?version=latest)](https://mmaction2.readthedocs.io/en/latest/)\n[![actions](https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://github.com/open-mmlab/mmaction2/actions)\n[![codecov](https://codecov.io/gh/open-mmlab/mmaction2/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmaction2)\n[![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://pypi.org/project/mmaction2/)\n[![LICENSE](https://img.shields.io/github/license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/master/LICENSE)\n[![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)\n[![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)\n\nMMAction2 is an open-source toolbox for video understanding based on PyTorch.\nIt is a part of the [OpenMMLab](http://openmmlab.org/) project.\n\nThe master branch works with **PyTorch 1.3+**.\n\n<div align=\"center\">\n  <div style=\"float:left;margin-right:10px;\">\n  <img src=\"https://github.com/open-mmlab/mmaction2/raw/master/resources/mmaction2_overview.gif\" width=\"380px\"><br>\n    <p style=\"font-size:1.5vw;\">Action Recognition Results on Kinetics-400</p>\n  </div>\n  <div style=\"float:right;margin-right:0px;\">\n  <img src=\"https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif\" width=\"380px\"><br>\n    <p style=\"font-size:1.5vw;\">Skeleton-base Action Recognition Results on NTU-RGB+D-120</p>\n  </div>\n</div>\n<div align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/30782254/155710881-bb26863e-fcb4-458e-b0c4-33cd79f96901.gif\" width=\"580px\"/><br>\n    <p style=\"font-size:1.5vw;\">Skeleton-based Spatio-Temporal Action Detection and Action Recognition Results on Kinetics-400</p>\n</div>\n<div align=\"center\">\n  <img src=\"https://github.com/open-mmlab/mmaction2/raw/master/resources/spatio-temporal-det.gif\" width=\"800px\"/><br>\n    <p style=\"font-size:1.5vw;\">Spatio-Temporal Action Detection Results on AVA-2.1</p>\n</div>\n\n## Major Features\n\n- **Modular design**: We decompose a video understanding framework into different components. One can easily construct a customized video understanding framework by combining different modules.\n\n- **Support four major video understanding tasks**: MMAction2 implements various algorithms for multiple video understanding tasks, including action recognition, action localization, spatio-temporal action detection, and skeleton-based action detection. We support **27** different algorithms and **20** different datasets for the four major tasks.\n\n- **Well tested and documented**: We provide detailed documentation and API reference, as well as unit tests.\n\n## Updates\n\n- (2022-03-04) We support **Multigrid** on Kinetics400, achieve 76.07% Top-1 accuracy and accelerate training speed.\n- (2021-11-24) We support **2s-AGCN** on NTU60 XSub, achieve 86.06% Top-1 accuracy on joint stream and 86.89% Top-1 accuracy on bone stream respectively.\n- (2021-10-29) We provide a demo for skeleton-based and rgb-based spatio-temporal detection and action recognition (demo/demo_video_structuralize.py).\n- (2021-10-26) We train and test **ST-GCN** on NTU60 with 3D keypoint annotations, achieve 84.61% Top-1 accuracy (higher than 81.5% in the [paper](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17135)).\n- (2021-10-25) We provide a script(tools/data/skeleton/gen_ntu_rgbd_raw.py) to convert the NTU60 and NTU120 3D raw skeleton data to our format.\n- (2021-10-25) We provide a [guide](https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/posec3d/custom_dataset_training.md) on how to train PoseC3D with custom datasets, [bit-scientist](https://github.com/bit-scientist) authored this PR!\n- (2021-10-16) We support **PoseC3D** on UCF101 and HMDB51, achieves 87.0% and 69.3% Top-1 accuracy with 2D skeletons only. Pre-extracted 2D skeletons are also available.\n\n**Release**: v0.22.0 was released in 05/03/2022. Please refer to [changelog.md](docs/changelog.md) for details and release history.\n\n## Installation\n\nPlease refer to [install.md](docs/install.md) for installation.\n\n## Get Started\n\nPlease see [getting_started.md](docs/getting_started.md) for the basic usage of MMAction2.\nThere are also tutorials:\n\n- [learn about configs](docs/tutorials/1_config.md)\n- [finetuning models](docs/tutorials/2_finetune.md)\n- [adding new dataset](docs/tutorials/3_new_dataset.md)\n- [designing data pipeline](docs/tutorials/4_data_pipeline.md)\n- [adding new modules](docs/tutorials/5_new_modules.md)\n- [exporting model to onnx](docs/tutorials/6_export_model.md)\n- [customizing runtime settings](docs/tutorials/7_customize_runtime.md)\n\nA Colab tutorial is also provided. You may preview the notebook [here](demo/mmaction2_tutorial.ipynb) or directly [run](https://colab.research.google.com/github/open-mmlab/mmaction2/blob/master/demo/mmaction2_tutorial.ipynb) on Colab.\n\n## Supported Methods\n\n<table style=\"margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;\">\n  <tr>\n    <td colspan=\"5\" style=\"font-weight:bold;\">Action Recognition</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/c3d/README.md\">C3D</a> (CVPR'2014)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tsn/README.md\">TSN</a> (ECCV'2016)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/i3d/README.md\">I3D</a> (CVPR'2017)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/i3d/README.md\">I3D Non-Local</a> (CVPR'2018)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/r2plus1d/README.md\">R(2+1)D</a> (CVPR'2018)</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/trn/README.md\">TRN</a> (ECCV'2018)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tsm/README.md\">TSM</a> (ICCV'2019)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tsm/README.md\">TSM Non-Local</a> (ICCV'2019)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/slowonly/README.md\">SlowOnly</a> (ICCV'2019)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/slowfast/README.md\">SlowFast</a> (ICCV'2019)</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/csn/README.md\">CSN</a> (ICCV'2019)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tin/README.md\">TIN</a> (AAAI'2020)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tpn/README.md\">TPN</a> (CVPR'2020)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/x3d/README.md\">X3D</a> (CVPR'2020)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/omnisource/README.md\">OmniSource</a> (ECCV'2020)</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition_audio/resnet/README.md\">MultiModality: Audio</a> (ArXiv'2020)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tanet/README.md\">TANet</a> (ArXiv'2020)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/timesformer/README.md\">TimeSformer</a> (ICML'2021)</td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td colspan=\"5\" style=\"font-weight:bold;\">Action Localization</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/localization/ssn/README.md\">SSN</a> (ICCV'2017)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/localization/bsn/README.md\">BSN</a> (ECCV'2018)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/localization/bmn/README.md\">BMN</a> (ICCV'2019)</td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td colspan=\"5\" style=\"font-weight:bold;\">Spatio-Temporal Action Detection</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/detection/acrn/README.md\">ACRN</a> (ECCV'2018)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/detection/ava/README.md\">SlowOnly+Fast R-CNN</a> (ICCV'2019)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/detection/ava/README.md\">SlowFast+Fast R-CNN</a> (ICCV'2019)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/detection/lfb/README.md\">LFB</a> (CVPR'2019)</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td colspan=\"5\" style=\"font-weight:bold;\">Skeleton-based Action Recognition</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/stgcn/README.md\">ST-GCN</a> (AAAI'2018)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/2s-agcn/README.md\">2s-AGCN</a> (CVPR'2019)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/posec3d/README.md\">PoseC3D</a> (ArXiv'2021)</td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>\n\nResults and models are available in the *README.md* of each method's config directory.\nA summary can be found on the [**model zoo**](https://mmaction2.readthedocs.io/en/latest/recognition_models.html) page.\n\nWe will keep up with the latest progress of the community and support more popular algorithms and frameworks.\nIf you have any feature requests, please feel free to leave a comment in [Issues](https://github.com/open-mmlab/mmaction2/issues/19).\n\n## Supported Datasets\n\n<table style=\"margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;\">\n  <tr>\n    <td colspan=\"4\" style=\"font-weight:bold;\">Action Recognition</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/hmdb51/README.md\">HMDB51</a> (<a href=\"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\">Homepage</a>) (ICCV'2011)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/ucf101/README.md\">UCF101</a> (<a href=\"https://www.crcv.ucf.edu/research/data-sets/ucf101/\">Homepage</a>) (CRCV-IR-12-01)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/activitynet/README.md\">ActivityNet</a> (<a href=\"http://activity-net.org/\">Homepage</a>) (CVPR'2015)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/kinetics/README.md\">Kinetics-[400/600/700]</a> (<a href=\"https://deepmind.com/research/open-source/kinetics/\">Homepage</a>) (CVPR'2017)</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/sthv1/README.md\">SthV1</a> (<a href=\"https://20bn.com/datasets/something-something/v1/\">Homepage</a>) (ICCV'2017)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/sthv2/README.md\">SthV2</a> (<a href=\"https://20bn.com/datasets/something-something/\">Homepage</a>) (ICCV'2017)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/diving48/README.md\">Diving48</a> (<a href=\"http://www.svcl.ucsd.edu/projects/resound/dataset.html\">Homepage</a>) (ECCV'2018)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/jester/README.md\">Jester</a> (<a href=\"https://20bn.com/datasets/jester/v1\">Homepage</a>) (ICCV'2019)</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/mit/README.md\">Moments in Time</a> (<a href=\"http://moments.csail.mit.edu/\">Homepage</a>) (TPAMI'2019)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/mmit/README.md\">Multi-Moments in Time</a> (<a href=\"http://moments.csail.mit.edu/challenge_iccv_2019.html\">Homepage</a>) (ArXiv'2019)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/hvu/README.md\">HVU</a> (<a href=\"https://github.com/holistic-video-understanding/HVU-Dataset\">Homepage</a>) (ECCV'2020)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/omnisource/README.md\">OmniSource</a> (<a href=\"https://kennymckormick.github.io/omnisource/\">Homepage</a>) (ECCV'2020)</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/gym/README.md\">FineGYM</a> (<a href=\"https://sdolivia.github.io/FineGym/\">Homepage</a>) (CVPR'2020)</td>\n    <td></td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td colspan=\"4\" style=\"font-weight:bold;\">Action Localization</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/thumos14/README.md\">THUMOS14</a> (<a href=\"https://www.crcv.ucf.edu/THUMOS14/download.html\">Homepage</a>) (THUMOS Challenge 2014)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/activitynet/README.md\">ActivityNet</a> (<a href=\"http://activity-net.org/\">Homepage</a>) (CVPR'2015)</td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td colspan=\"4\" style=\"font-weight:bold;\">Spatio-Temporal Action Detection</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/ucf101_24/README.md\">UCF101-24*</a> (<a href=\"http://www.thumos.info/download.html\">Homepage</a>) (CRCV-IR-12-01)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/jhmdb/README.md\">JHMDB*</a> (<a href=\"http://jhmdb.is.tue.mpg.de/\">Homepage</a>) (ICCV'2015)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/ava/README.md\">AVA</a> (<a href=\"https://research.google.com/ava/index.html\">Homepage</a>) (CVPR'2018)</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td colspan=\"4\" style=\"font-weight:bold;\">Skeleton-based Action Recognition</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/skeleton/README.md\">PoseC3D-FineGYM</a> (<a href=\"https://kennymckormick.github.io/posec3d/\">Homepage</a>) (ArXiv'2021)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/skeleton/README.md\">PoseC3D-NTURGB+D</a> (<a href=\"https://kennymckormick.github.io/posec3d/\">Homepage</a>) (ArXiv'2021)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/skeleton/README.md\">PoseC3D-UCF101</a> (<a href=\"https://kennymckormick.github.io/posec3d/\">Homepage</a>) (ArXiv'2021)</td>\n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/skeleton/README.md\">PoseC3D-HMDB51</a> (<a href=\"https://kennymckormick.github.io/posec3d/\">Homepage</a>) (ArXiv'2021)</td>\n  </tr>\n</table>\n\nDatasets marked with * are not fully supported yet, but related dataset preparation steps are provided. A summary can be found on the [**Supported Datasets**](https://mmaction2.readthedocs.io/en/latest/supported_datasets.html) page.\n\n## Benchmark\n\nTo demonstrate the efficacy and efficiency of our framework, we compare MMAction2 with some other popular frameworks and official releases in terms of speed. Details can be found in [benchmark](docs/benchmark.md).\n\n## Data Preparation\n\nPlease refer to [data_preparation.md](docs/data_preparation.md) for a general knowledge of data preparation.\nThe supported datasets are listed in [supported_datasets.md](docs/supported_datasets.md)\n\n## FAQ\n\nPlease refer to [FAQ](docs/faq.md) for frequently asked questions.\n\n## Projects built on MMAction2\n\nCurrently, there are many research works and projects built on MMAction2 by users from community, such as:\n\n- Video Swin Transformer. [[paper]](https://arxiv.org/abs/2106.13230)[[github]](https://github.com/SwinTransformer/Video-Swin-Transformer)\n- Evidential Deep Learning for Open Set Action Recognition, ICCV 2021 **Oral**. [[paper]](https://arxiv.org/abs/2107.10161)[[github]](https://github.com/Cogito2012/DEAR)\n- Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective, ICCV 2021 **Oral**. [[paper]](https://arxiv.org/abs/2103.17263)[[github]](https://github.com/xvjiarui/VFS)\n\netc., check [projects.md](docs/projects.md) to see all related projects.\n\n## License\n\nThis project is released under the [Apache 2.0 license](LICENSE).\n\n## Citation\n\nIf you find this project useful in your research, please consider cite:\n\n```BibTeX\n@misc{2020mmaction2,\n    title={OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark},\n    author={MMAction2 Contributors},\n    howpublished = {\\url{https://github.com/open-mmlab/mmaction2}},\n    year={2020}\n}\n```\n\n## Contributing\n\nWe appreciate all contributions to improve MMAction2. Please refer to [CONTRIBUTING.md](https://github.com/open-mmlab/mmcv/blob/master/CONTRIBUTING.md) in MMCV for more details about the contributing guideline.\n\n## Acknowledgement\n\nMMAction2 is an open-source project that is contributed by researchers and engineers from various colleges and companies.\nWe appreciate all the contributors who implement their methods or add new features and users who give valuable feedback.\nWe wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their new models.\n\n## Projects in OpenMMLab\n\n- [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.\n- [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.\n- [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.\n- [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.\n- [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.\n- [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.\n- [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.\n- [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.\n- [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.\n- [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.\n- [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.\n- [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.\n- [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.\n- [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.\n- [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.\n- [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.\n- [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.\n- [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.\n",
            "readme_url": "https://github.com/open-mmlab/mmaction2",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Evidential Deep Learning for Open Set Action Recognition",
            "arxiv": "2107.10161",
            "year": 2021,
            "url": "http://arxiv.org/abs/2107.10161v2",
            "abstract": "In a real-world scenario, human actions are typically out of the distribution\nfrom training data, which requires a model to both recognize the known actions\nand reject the unknown. Different from image data, video actions are more\nchallenging to be recognized in an open-set setting due to the uncertain\ntemporal dynamics and static bias of human actions. In this paper, we propose a\nDeep Evidential Action Recognition (DEAR) method to recognize actions in an\nopen testing set. Specifically, we formulate the action recognition problem\nfrom the evidential deep learning (EDL) perspective and propose a novel model\ncalibration method to regularize the EDL training. Besides, to mitigate the\nstatic bias of video representation, we propose a plug-and-play module to\ndebias the learned representation through contrastive learning. Experimental\nresults show that our DEAR method achieves consistent performance gain on\nmultiple mainstream action recognition models and benchmarks. Code and\npre-trained models are available at\n{\\small{\\url{https://www.rit.edu/actionlab/dear}}}.",
            "authors": [
                "Wentao Bao",
                "Qi Yu",
                "Yu Kong"
            ]
        },
        {
            "title": "Video Swin Transformer",
            "arxiv": "2106.13230",
            "year": 2021,
            "url": "http://arxiv.org/abs/2106.13230v1",
            "abstract": "The vision community is witnessing a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have attained top accuracy\non the major video recognition benchmarks. These video models are all built on\nTransformer layers that globally connect patches across the spatial and\ntemporal dimensions. In this paper, we instead advocate an inductive bias of\nlocality in video Transformers, which leads to a better speed-accuracy\ntrade-off compared to previous approaches which compute self-attention globally\neven with spatial-temporal factorization. The locality of the proposed video\narchitecture is realized by adapting the Swin Transformer designed for the\nimage domain, while continuing to leverage the power of pre-trained image\nmodels. Our approach achieves state-of-the-art accuracy on a broad range of\nvideo recognition benchmarks, including on action recognition (84.9 top-1\naccuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less\npre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1\naccuracy on Something-Something v2). The code and models will be made publicly\navailable at https://github.com/SwinTransformer/Video-Swin-Transformer.",
            "authors": [
                "Ze Liu",
                "Jia Ning",
                "Yue Cao",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Han Hu"
            ]
        },
        {
            "title": "Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective",
            "arxiv": "2103.17263",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.17263v5",
            "abstract": "Learning a good representation for space-time correspondence is the key for\nvarious computer vision tasks, including tracking object bounding boxes and\nperforming video object pixel segmentation. To learn generalizable\nrepresentation for correspondence in large-scale, a variety of self-supervised\npretext tasks are proposed to explicitly perform object-level or patch-level\nsimilarity learning. Instead of following the previous literature, we propose\nto learn correspondence using Video Frame-level Similarity (VFS) learning, i.e,\nsimply learning from comparing video frames. Our work is inspired by the recent\nsuccess in image-level contrastive learning and similarity learning for visual\nrecognition. Our hypothesis is that if the representation is good for\nrecognition, it requires the convolutional features to find correspondence\nbetween similar objects or parts. Our experiments show surprising results that\nVFS surpasses state-of-the-art self-supervised approaches for both OTB visual\nobject tracking and DAVIS video object segmentation. We perform detailed\nanalysis on what matters in VFS and reveals new properties on image and frame\nlevel similarity learning. Project page with code is available at\nhttps://jerryxu.net/VFS",
            "authors": [
                "Jiarui Xu",
                "Xiaolong Wang"
            ]
        },
        {
            "year": "2020",
            "howpublished": "\\url{https://github.com/open-mmlab/mmaction2}",
            "author": [
                "Contributors, MMAction2"
            ],
            "title": "OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark",
            "ENTRYTYPE": "misc",
            "ID": "2020mmaction2",
            "authors": [
                "Contributors, MMAction2"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "adding new dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/docs/tutorials/3_new_dataset.md"
                    }
                }
            },
            {
                "name": "**Supported Datasets**",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://mmaction2.readthedocs.io/en/latest/supported_datasets.html"
                    }
                }
            },
            {
                "name": "supported_datasets.md",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/open-mmlab/mmaction2/blob/master/docs/supported_datasets.md"
                    }
                }
            },
            {
                "name": "HMDB51"
            },
            {
                "name": "UCF101"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9946980786220555,
        "task": "Vision Other",
        "task_prob": 0.6588148755450697
    }
}