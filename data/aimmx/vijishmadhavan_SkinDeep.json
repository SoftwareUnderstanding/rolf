{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "SkinDeep &nbsp; [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Skin%20Deep&url=https://github.com/vijishmadhavan/SkinDeep&via=Vijish68859437&hashtags=machinelearning,developers,100DaysOfCode,Deeplearning) &nbsp;",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "vijishmadhavan",
                "owner_type": "User",
                "name": "SkinDeep",
                "url": "https://github.com/vijishmadhavan/SkinDeep",
                "stars": 765,
                "pushed_at": "2021-11-12 14:28:12+00:00",
                "created_at": "2021-04-05 07:17:40+00:00",
                "language": "Jupyter Notebook",
                "description": "Get Deinked!!",
                "license": "Apache License 2.0",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "2721d27942d70f0c6cd6475501903ec4adc2c328",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/.gitignore"
                    }
                },
                "size": 2139
            },
            {
                "type": "code",
                "name": ".streamlit",
                "sha": "d64902bd9a8daa9a4d1bd629880744542454cf5f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/tree/master/.streamlit"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "Images",
                "sha": "7b9017aa11006668bd8acfb6fbb98e90deb80ac5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/tree/master/Images"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/LICENSE"
                    }
                },
                "size": 11357
            },
            {
                "type": "code",
                "name": "Model",
                "sha": "9c5dd0f0b678b52241a53231f94fe0bde08cf601",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/tree/master/Model"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "Procfile",
                "sha": "265de8638b4e9ff83dc0a0495bdbf708ce04dc8b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/Procfile"
                    }
                },
                "size": 41
            },
            {
                "type": "code",
                "name": "SkinDeep.ipynb",
                "sha": "d3daf65d28e36ecb323405984d74495d50127e0b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/SkinDeep.ipynb"
                    }
                },
                "size": 496514
            },
            {
                "type": "code",
                "name": "SkinDeep_good.ipynb",
                "sha": "9c837da71b61538fb070b440bc941bd174b7fe75",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/SkinDeep_good.ipynb"
                    }
                },
                "size": 487269
            },
            {
                "type": "code",
                "name": "app.py",
                "sha": "7960be3d8f9e6b0cde32a3bc22f5bd8c52065bf8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/app.py"
                    }
                },
                "size": 3096
            },
            {
                "type": "code",
                "name": "colab_requirements.txt",
                "sha": "184c037a338948af46bbf08283d7e1c6262ae584",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/colab_requirements.txt"
                    }
                },
                "size": 88
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "e0c19812423556b7d88c8d8522b7bf3e13fa088e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/requirements.txt"
                    }
                },
                "size": 2063
            },
            {
                "type": "code",
                "name": "runtime.txt",
                "sha": "795ee725a0b8f524b78d7b871628795ed0abd3d6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/runtime.txt"
                    }
                },
                "size": 13
            },
            {
                "type": "code",
                "name": "setup.sh",
                "sha": "3aba9403675f9cbac3570716faca684f4387bb6c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/setup.sh"
                    }
                },
                "size": 133
            },
            {
                "type": "code",
                "name": "videoProcessor.ipynb",
                "sha": "5e6ef26df6a88ccc9de81838757da2848f28a42a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/videoProcessor.ipynb"
                    }
                },
                "size": 30259
            }
        ]
    },
    "trained_model": {
        "binaries": [
            {
                "type": "binary",
                "name": "Dockerfile",
                "sha": "3eaf2d0f6031c6e2726c4470c52518363f6ab2a6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vijishmadhavan/SkinDeep/blob/master/Dockerfile"
                    }
                },
                "size": 174
            }
        ]
    },
    "authors": [
        {
            "name": "Vijish Madhavan",
            "github_id": "vijishmadhavan"
        },
        {
            "name": "kozobot",
            "github_id": "kozobot"
        },
        {
            "name": "dddanmar",
            "github_id": "dddanmar"
        },
        {
            "name": "3dsf",
            "github_id": "3dsf"
        },
        {
            "name": "Aleksey Morozov",
            "github_id": "amrzv"
        }
    ],
    "tags": [
        "tattoos",
        "imgur",
        "synthetic-data",
        "photoshop",
        "image-pairs",
        "deep-learning",
        "pix2pix",
        "pytorch",
        "fastai",
        "machine-learning"
    ],
    "description": "Get Deinked!!",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/vijishmadhavan/SkinDeep",
            "stars": 765,
            "issues": true,
            "readme": "![waving](https://capsule-render.vercel.app/api?type=waving&height=200&text=SkinDeep&fontAlign=80&fontAlignY=40&color=gradient)\n\n# [SkinDeep](https://github.com/vijishmadhavan/SkinDeep) &nbsp; [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Skin%20Deep&url=https://github.com/vijishmadhavan/SkinDeep&via=Vijish68859437&hashtags=machinelearning,developers,100DaysOfCode,Deeplearning) &nbsp;\n\n__Contact__: vijishmadhavan@gmail.com\n\n## Updates\n\nSkinDeep Video is getting ready, thanks to 3dsf (https://github.com/3dsf) for the tremendous effort he has put into this project to make this happen.\n\n![post](https://i.imgur.com/lcejYwD.gif)\n\nI planned this project after watching Justin Bieber's \"Anyone\" Music Video, He had his tattoo covered up with the help of artists airbrushing on him for hours. The results were amazing in the music video. Producing that sought of video output can be difficult, so I opted for Images. Can deep learning do a decent job or can it even match photoshop? This was the starting point of this project!!\n\n## Why not photoshop?\n\nPhotoshop can produce extremely good results but it needs expertise and hours of work retouching the whole image.\n\n## Highlights\n\n- [Example Outputs](#Example-Outputs) &#x1F537;\n- [Getting Started Yourself](#Getting-Started-Yourself) &#x1F537;\n\n\n# Synthetic data generation\n\nTo do such a project we need a lot of image pairs, I couldn't find any such dataset so I opted for synthetic data.\n\n(1) Overlaying APDrawing dataset image pairs along with some background removed tattoo designs, This can be easily done using Python OpenCV.\n\n(2) Apdrawing dataset has line art pairs which will mimic tattoo lines, this will help the model to learn and remove those lines.\n\n(3) APDrawing dataset only has portrait head shots, For full body images I ran my previous ArtLine(https://github.com/vijishmadhavan/ArtLine) project and overlaid the output with the input image.\n\n![Imgur](https://i.imgur.com/2NK4DRU.jpg)\n\n\n![Imgur](https://i.imgur.com/HWBiNnN.jpg)\n\n(4) ImageDraw.Draw was used with forest green colour codes and placed randomly on zoomed-in body images, Similar to Crappify in fast.ai.\n\n(5) Photoshop was also used to place tattoos in subjects where warping and angle change was needed.\n\n![Imgur](https://i.imgur.com/xbzVVEC.jpg)\n\nMail me for a modified Apdrawing dataset.\n\n\n# Example Outputs\n\n\n![Imgur](https://i.imgur.com/y7NM1f8.jpg)\n\n\n![Imgur](https://i.imgur.com/WEnbL8w.jpg)\n\n\n![Imgur](https://i.imgur.com/X5lZEse.jpg)\n\n### Heavily tattooed faces\n\n![Imgur](https://i.imgur.com/dANN7Do.jpg)\n\n![Imgur](https://i.imgur.com/Lq0i1N0.jpg)\n\n# Visual Comparison\n\n![Imgur](https://i.imgur.com/wCgMROl.jpg)\n\n\n## Technical Details\n\nThe highlight of the project is in producing synthetic data, thanks to **pyimagesearch.com** for wonderful blogs. Check below links.\n\nhttps://www.pyimagesearch.com/2016/03/07/transparent-overlays-with-opencv/\n\nhttps://www.pyimagesearch.com/2016/04/25/watermarking-images-with-opencv-and-python/\n\nhttps://www.pyimagesearch.com/2015/01/26/multi-scale-template-matching-using-python-opencv/\n\n* **Self-Attention** (https://arxiv.org/abs/1805.08318). Generator is pretrained UNET with spectral normalization and self-attention. Something that I got from Jason Antic's DeOldify(https://github.com/jantic/DeOldify), this made a huge difference, all of a sudden I started getting proper details around the facial features.\n\n* **Progressive Resizing** (https://arxiv.org/abs/1710.10196),(https://arxiv.org/pdf/1707.02921.pdf). Progressive resizing takes this idea of gradually increasing the image size, In this project the image size was gradually increased and learning rates were adjusted. Thanks to fast.ai for introducing me to Progressive resizing, this helps the model to generalise better as it sees many more different images.\n\n* **Generator Loss** :  Perceptual Loss/Feature Loss based on VGG16. (https://arxiv.org/pdf/1603.08155.pdf).\n\n## Required libraries\n\nThis project is built around the wonderful Fast.AI library.\n\n- **fastai==1.0.61** (and its dependencies).  Please don't install the higher versions\n- **PyTorch 1.6.0** Please don't install the higher versions\n\n## Getting Started Yourself\n\nThe project is still a work in progress, but I want to put it out so that I get some good suggestions.\n\nThe easiest way to get started is to simply try out on Colab: [<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\">](https://colab.research.google.com/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep_good.ipynb)\n\n[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\">](https://colab.research.google.com/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep.ipynb)\n\nThe output is limited to 500px and it needs high quality images to do well.\n\nI would request you to have a look at the limitations given below.\n\n### Docker\n\n*Note:* These instructions are for running with an NVDIA GPU on Linux.\n\n**Prequisites**\n\n* Make sure your NVIDA drivers are correctly installed.  Running ```nvidia-smi``` should confirm your driver version and memory usage\n* Approximately 3.7GB of free GPU memory\n* The nvidia-docker2 package installed.  This will allow the Docker container access to the GPU\n* Docker installed with the correct permissions for your system\n\n**Run the container**\n\nThis runs against all of the GPUs.  Check the Docker documentation for running on a specific card if you have multiple.  \nThe type=bind mounts the current directory (where you checked out the project) as the home in the container.  This allows you immediate access to the notebooks.\n```bash\n$> docker run --gpus all \\\n              --mount type=bind,source=`pwd`,target=/home/jovyan \\\n              -d \\\n              -p 8888:8888 -p 4040:4040 -p 4041:4041 \\\n              jupyter/tensorflow-notebook:python-3.8.8\n```\n\n**Launch the site**\n\nUse 'ps' to get the id of your new container, then print out the logs to get the launch URL.  This is required because an authenication token is created at startup.\n```bash\n$> docker ps\n$> docker logs <container_id>\n```\n\n## Limitations\n\n- Synthetic data does not match real tattoos, so the model struggles a bit with some images.\n- Building a huge dataset by myself was impossible, so I had to go with a limited number of image pairs.\n- Tattoo designs are unique, it differs from person to person so the model might fall in some cases.\n- Coloured tattoos won't work, the dataset had none.\n\n## Going Forward\n\nThe model still struggles and needs a lot of improvement, if you are interested please contribute lets improve it.\n\n## Acknowledgments\n\n- Thanks to fast.ai, Jason Antic(https://github.com/jantic) and pyimagesearch.com.\n",
            "readme_url": "https://github.com/vijishmadhavan/SkinDeep",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
            "arxiv": "1603.08155",
            "year": 2016,
            "url": "http://arxiv.org/abs/1603.08155v1",
            "abstract": "We consider image transformation problems, where an input image is\ntransformed into an output image. Recent methods for such problems typically\ntrain feed-forward convolutional neural networks using a \\emph{per-pixel} loss\nbetween the output and ground-truth images. Parallel work has shown that\nhigh-quality images can be generated by defining and optimizing\n\\emph{perceptual} loss functions based on high-level features extracted from\npretrained networks. We combine the benefits of both approaches, and propose\nthe use of perceptual loss functions for training feed-forward networks for\nimage transformation tasks. We show results on image style transfer, where a\nfeed-forward network is trained to solve the optimization problem proposed by\nGatys et al in real-time. Compared to the optimization-based method, our\nnetwork gives similar qualitative results but is three orders of magnitude\nfaster. We also experiment with single-image super-resolution, where replacing\na per-pixel loss with a perceptual loss gives visually pleasing results.",
            "authors": [
                "Justin Johnson",
                "Alexandre Alahi",
                "Li Fei-Fei"
            ]
        },
        {
            "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
            "arxiv": "1707.02921",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.02921v1",
            "abstract": "Recent research on super-resolution has progressed with the development of\ndeep convolutional neural networks (DCNN). In particular, residual learning\ntechniques exhibit improved performance. In this paper, we develop an enhanced\ndeep super-resolution network (EDSR) with performance exceeding those of\ncurrent state-of-the-art SR methods. The significant performance improvement of\nour model is due to optimization by removing unnecessary modules in\nconventional residual networks. The performance is further improved by\nexpanding the model size while we stabilize the training procedure. We also\npropose a new multi-scale deep super-resolution system (MDSR) and training\nmethod, which can reconstruct high-resolution images of different upscaling\nfactors in a single model. The proposed methods show superior performance over\nthe state-of-the-art methods on benchmark datasets and prove its excellence by\nwinning the NTIRE2017 Super-Resolution Challenge.",
            "authors": [
                "Bee Lim",
                "Sanghyun Son",
                "Heewon Kim",
                "Seungjun Nah",
                "Kyoung Mu Lee"
            ]
        },
        {
            "title": "Self-Attention Generative Adversarial Networks",
            "arxiv": "1805.08318",
            "year": 2018,
            "url": "http://arxiv.org/abs/1805.08318v2",
            "abstract": "In this paper, we propose the Self-Attention Generative Adversarial Network\n(SAGAN) which allows attention-driven, long-range dependency modeling for image\ngeneration tasks. Traditional convolutional GANs generate high-resolution\ndetails as a function of only spatially local points in lower-resolution\nfeature maps. In SAGAN, details can be generated using cues from all feature\nlocations. Moreover, the discriminator can check that highly detailed features\nin distant portions of the image are consistent with each other. Furthermore,\nrecent work has shown that generator conditioning affects GAN performance.\nLeveraging this insight, we apply spectral normalization to the GAN generator\nand find that this improves training dynamics. The proposed SAGAN achieves the\nstate-of-the-art results, boosting the best published Inception score from 36.8\nto 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the\nchallenging ImageNet dataset. Visualization of the attention layers shows that\nthe generator leverages neighborhoods that correspond to object shapes rather\nthan local regions of fixed shape.",
            "authors": [
                "Han Zhang",
                "Ian Goodfellow",
                "Dimitris Metaxas",
                "Augustus Odena"
            ]
        },
        {
            "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
            "arxiv": "1710.10196",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.10196v3",
            "abstract": "We describe a new training methodology for generative adversarial networks.\nThe key idea is to grow both the generator and discriminator progressively:\nstarting from a low resolution, we add new layers that model increasingly fine\ndetails as training progresses. This both speeds the training up and greatly\nstabilizes it, allowing us to produce images of unprecedented quality, e.g.,\nCelebA images at 1024^2. We also propose a simple way to increase the variation\nin generated images, and achieve a record inception score of 8.80 in\nunsupervised CIFAR10. Additionally, we describe several implementation details\nthat are important for discouraging unhealthy competition between the generator\nand discriminator. Finally, we suggest a new metric for evaluating GAN results,\nboth in terms of image quality and variation. As an additional contribution, we\nconstruct a higher-quality version of the CelebA dataset.",
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999992079579545,
        "task": "Image Generation",
        "task_prob": 0.9806232452318708
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "CelebA"
            }
        ]
    }
}