{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "arXiv-dl",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "MarkHershey",
                "owner_type": "User",
                "name": "arxiv-dl",
                "url": "https://github.com/MarkHershey/arxiv-dl",
                "stars": 6,
                "pushed_at": "2022-03-12 13:45:10+00:00",
                "created_at": "2021-01-21 06:38:52+00:00",
                "language": "Python",
                "description": "Command-line arXiv.org Papers Downloader",
                "license": "MIT License",
                "frameworks": []
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "9051767917c8ee77d8dca772d60447643b84db40",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/MarkHershey/arxiv-dl/blob/master/.gitignore"
                    }
                },
                "size": 2043
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "f11bfcfcbe387226de259fdea9d3705eb76ae1b2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/MarkHershey/arxiv-dl/blob/master/LICENSE"
                    }
                },
                "size": 1072
            },
            {
                "type": "code",
                "name": "Makefile",
                "sha": "32afb92d27dcd554ab87bdbd8acec535e244bdc8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/MarkHershey/arxiv-dl/blob/master/Makefile"
                    }
                },
                "size": 274
            },
            {
                "type": "code",
                "name": "arxiv_dl",
                "sha": "d9f35db9a1f2b73d31d7111d3d2278fc356d0d74",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/MarkHershey/arxiv-dl/tree/master/arxiv_dl"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "imgs",
                "sha": "704483fd568a47112d424e509882576ed6ad1cc7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/MarkHershey/arxiv-dl/tree/master/imgs"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "0cf4159db193d6d567286044719f0480025c6fc2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/MarkHershey/arxiv-dl/blob/master/requirements.txt"
                    }
                },
                "size": 41
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "df4c96a3269d215079c21ea5599329a0ac72237f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/MarkHershey/arxiv-dl/blob/master/setup.py"
                    }
                },
                "size": 1426
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "788c4e950fd62a57e34e92bffbcfc41bb9163ecf",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/MarkHershey/arxiv-dl/tree/master/tests"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Mark Huang",
            "email": "dev@markhh.com",
            "github_id": "MarkHershey"
        }
    ],
    "tags": [
        "arxiv",
        "paper",
        "downloader",
        "command-line-tool",
        "paper-with-code"
    ],
    "description": "Command-line arXiv.org Papers Downloader",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/MarkHershey/arxiv-dl",
            "stars": 6,
            "issues": true,
            "readme": "# arXiv-dl\n\nCommand-line [ArXiv](https://arxiv.org/) Paper Downloader.\n[[PyPI]](https://pypi.org/project/arxiv-dl/)\n[[Source]](https://github.com/MarkHershey/arxiv-dl)\n\n[![](https://img.shields.io/pypi/v/arxiv-dl)](https://pypi.org/project/arxiv-dl/)\n[![](https://img.shields.io/pypi/pyversions/arxiv-dl)](https://pypi.org/project/arxiv-dl/)\n[![](https://img.shields.io/pypi/wheel/arxiv-dl)](https://github.com/MarkHershey/arxiv-dl/releases)\n[![](https://img.shields.io/pypi/dm/Arxiv-dl)](https://pypistats.org/packages/arxiv-dl)\n[![](https://img.shields.io/badge/license-MIT-blue)](https://github.com/MarkHershey/arxiv-dl/blob/master/LICENSE)\n[![](https://img.shields.io/badge/code%20style-black-black)](https://github.com/psf/black)\n\n_Disclaimer: This is a highly-opinionated CLI tool for downloading papers. It is designed to be easy to use. Obviously, not an official project._\n\n## Features\n\n-   Download papers from `arXiv.org` via simple command line interface.\n-   Support downloading speedup by using [aria2c](https://aria2.github.io/).\n-   Automatically maintain a local list of downloaded papers.\n-   Retrieve the paper's metadata and citation:\n    -   Paper Title\n    -   Authors\n    -   Abstract\n    -   Comments (Conference acceptance info)\n    -   Source Code Links\n    -   Citation (`BibTeX`)\n-   Configure the desired download destination via environment variables.\n-   All downloaded papers will be named by its arXiv ID and paper title without whitespace.\n\n### Why?\n\n-   Save time and effort to download, rename, and organize papers.\n-   Speedup downloading process by using parallel connections.\n-   Local paper list would be handy for quick local lookup, locate, and cite papers.\n\n## Install\n\nThis is a command-line tool, use `pip` to install the package globally.\n\n-   Pre-requisite: `Python 3.x`\n\n```bash\npython3 -m pip install --upgrade arxiv-dl\n```\n\n(Optional) Install [aria2c](https://aria2.github.io/) for download speedup.\n\n-   MacOS: `brew install aria2`\n-   Linux: `sudo snap install aria2c`\n\n## Usage\n\nAfter installation, the command `getpaper` should be available in your terminal.\n\n```bash\n$ getpaper [-v] [-d DOWNLOAD_DIR] [-n N_THREADS] urls [urls ...]\n```\n\nOptions:\n\n-   `-v`, `--verbose` (optional): Print paper metadata.\n-   `-d`, `--download-dir` (optional): Specify one-time download directory. This option will override the default download directory or the one specified in the environment variable `ARXIV_DOWNLOAD_FOLDER`.\n-   `-n`, `--n-threads` (optional): Specify the number of parallel connections to be used by `aria2`.\n\nExample:\n\n![](imgs/demo.png)\n\n```bash\n# Use ArXiv Paper ID\n$ getpaper 1512.03385 2103.15538\n\n# Use ArXiv Abstract Page URL\n$ getpaper https://arxiv.org/abs/2103.15538\n\n# Use ArXiv PDF Page URL\n$ getpaper https://arxiv.org/pdf/1512.03385.pdf\n```\n\n## Configurations\n\n### Set Custom Download Destination _(Optional)_\n\n-   Default Download Destination: `~/Downloads/ArXiv_Papers`\n-   To set custom download destination, use the environment variable `ARXIV_DOWNLOAD_FOLDER`. Include the following line in your `.bashrc` or `.zshrc` file:\n    ```bash\n    export ARXIV_DOWNLOAD_FOLDER=~/Documents/Papers\n    ```\n-   Precedence:\n    1.  Command-line option `-d`\n    2.  Environment variable `ARXIV_DOWNLOAD_FOLDER`\n    3.  Default download destination\n\n### Set Custom Command Alias _(Optional)_\n\n-   You can always set your own preferred alias for the default `getpaper` command.\n-   Include the following line(s) in your `.bashrc` or `.zshrc` file to set your preferred alias:\n    ```bash\n    alias dp=\"getpaper\"\n    alias dpv=\"getpaper -v -d '~/Documents/Papers'\"\n    ```\n\n## Development\n\n### Set up development environment\n\n```bash\npython3 -m venv venv && \\\nsource venv/bin/activate && \\\npip install -e \".[dev]\"\n```\n\n### Run Tests\n\n```bash\npytest\n```\n\n### Build the package\n\n```bash\nmake\n```\n\n### Clean cache & build artifacts\n\n```bash\nmake clean\n```\n\n## TODOs\n\n-   [x] Add support for ara2c.\n-   [ ] Add support for papers on CVF Open Access.\n-   [ ] Add support for papers on OpenReview.\n\n## License\n\n[MIT License](https://github.com/MarkHershey/arxiv-dl/blob/master/LICENSE) - Copyright (c) 2021-2022 Mark Huang\n",
            "readme_url": "https://github.com/MarkHershey/arxiv-dl",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events",
            "arxiv": "2103.15538",
            "year": 2021,
            "url": "http://arxiv.org/abs/2103.15538v3",
            "abstract": "Traffic event cognition and reasoning in videos is an important task that has\na wide range of applications in intelligent transportation, assisted driving,\nand autonomous vehicles. In this paper, we create a novel dataset,\nSUTD-TrafficQA (Traffic Question Answering), which takes the form of video QA\nbased on the collected 10,080 in-the-wild videos and annotated 62,535 QA pairs,\nfor benchmarking the cognitive capability of causal inference and event\nunderstanding models in complex traffic scenarios. Specifically, we propose 6\nchallenging reasoning tasks corresponding to various traffic scenarios, so as\nto evaluate the reasoning capability over different kinds of complex yet\npractical traffic events. Moreover, we propose Eclipse, a novel Efficient\nglimpse network via dynamic inference, in order to achieve\ncomputation-efficient and reliable video reasoning. The experiments show that\nour method achieves superior performance while reducing the computation cost\nsignificantly. The project page: https://github.com/SUTDCV/SUTD-TrafficQA.",
            "authors": [
                "Li Xu",
                "He Huang",
                "Jun Liu"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9457201436573338,
        "task": "Object Detection",
        "task_prob": 0.9799031004239711
    },
    "training": {
        "datasets": [
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "COCO"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ImageNet"
            }
        ]
    }
}