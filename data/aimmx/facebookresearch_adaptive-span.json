{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "Sequential Transformer",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "facebookresearch",
                "owner_type": "Organization",
                "name": "adaptive-span",
                "url": "https://github.com/facebookresearch/adaptive-span",
                "stars": 566,
                "pushed_at": "2021-09-14 16:57:54+00:00",
                "created_at": "2019-05-31 22:02:55+00:00",
                "language": "Python",
                "description": "Transformer training code for sequential tasks",
                "license": "Other",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "5ef2556d5492dc4a1db3f5618f9c1deeaf9c175b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/.gitignore"
                    }
                },
                "size": 2660
            },
            {
                "type": "code",
                "name": "CODE_OF_CONDUCT.md",
                "sha": "ac27d8a51bcddc928a57cae0ee5c585d5c9dc149",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/CODE_OF_CONDUCT.md"
                    }
                },
                "size": 241
            },
            {
                "type": "code",
                "name": "CONTRIBUTING.md",
                "sha": "9e49d7068463818d60d4f151249303a0627ef578",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/CONTRIBUTING.md"
                    }
                },
                "size": 1262
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "6b28d560c9cbbb829dec5bf3eb23f8ddae46d697",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/LICENSE"
                    }
                },
                "size": 19332
            },
            {
                "type": "code",
                "name": "README_files",
                "sha": "fb870373bb29b9e364c5092ae81688476d4e79ce",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/tree/main/README_files"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "adagrad_with_grad_clip.py",
                "sha": "1ad9f9808fc2128072ff445cbb148fb1c20df958",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/adagrad_with_grad_clip.py"
                    }
                },
                "size": 3248
            },
            {
                "type": "code",
                "name": "adaptive_io.py",
                "sha": "4f1f6a9429b8d672da42495a6655d252478fe5ed",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/adaptive_io.py"
                    }
                },
                "size": 7925
            },
            {
                "type": "code",
                "name": "adaptive_span.py",
                "sha": "a4ee933d024004bc75311a5f56a5183b775ce209",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/adaptive_span.py"
                    }
                },
                "size": 5729
            },
            {
                "type": "code",
                "name": "config.py",
                "sha": "fe362a6669975cf8095921351eed090133f84ccc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/config.py"
                    }
                },
                "size": 7166
            },
            {
                "type": "code",
                "name": "data.py",
                "sha": "7d20c69329a54c4303192ef1ec876a7e7813d24b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/data.py"
                    }
                },
                "size": 5149
            },
            {
                "type": "code",
                "name": "experiments",
                "sha": "9e35378caf36205a3e5a9bf9880a001de68347d4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/tree/main/experiments"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "get_data.sh",
                "sha": "8f0eae234146b9cf8dfe043d10d4c2d354e7cff1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/get_data.sh"
                    }
                },
                "size": 966
            },
            {
                "type": "code",
                "name": "get_pretrained.sh",
                "sha": "21aca89d15e3a2dd3870a4034dc9fbd5fd471f3d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/get_pretrained.sh"
                    }
                },
                "size": 233
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "399dc2db36af9a312de3e68ef4b5a6d10c51410f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/main.py"
                    }
                },
                "size": 5696
            },
            {
                "type": "code",
                "name": "models.py",
                "sha": "7ae37ecd1d99d083cd814276db4e02af436a3584",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/models.py"
                    }
                },
                "size": 9003
            },
            {
                "type": "code",
                "name": "persistent_memory.py",
                "sha": "3f4259e75ead23fc1de01738092c58da18ad6b18",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/persistent_memory.py"
                    }
                },
                "size": 2218
            },
            {
                "type": "code",
                "name": "trainer.py",
                "sha": "cc0d662feb301456dff14ddeea61a5ae0a0ab2c0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/trainer.py"
                    }
                },
                "size": 5442
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "29b7775660b42d054c0fe23dbb3fb56f617b5dc7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/adaptive-span/blob/main/utils.py"
                    }
                },
                "size": 8430
            }
        ]
    },
    "authors": [
        {
            "name": "Sainbayar Sukhbaatar",
            "github_id": "tesatory"
        },
        {
            "name": "George Hotz",
            "github_id": "geohot"
        }
    ],
    "tags": [],
    "description": "Transformer training code for sequential tasks",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/facebookresearch/adaptive-span",
            "stars": 566,
            "issues": true,
            "readme": "# Sequential Transformer\nThis is a code for training Transformers on sequential tasks such as language modeling. Unlike the original Transformer architecture, it uses caching of previous representations and relative position embeddings to better adapt to sequential tasks. In addition, the code also implements the following projects as described below and in this blog [post](https://ai.facebook.com/blog/making-transformer-networks-simpler-and-more-efficient/):\n- [Adaptive Attention Span](#adaptive-attention-span)\n- [All-attention Network](#all-attention-network)\n\n## Requirements\nYou need PyTorch 0.4.1 or above and a cuda-enabled GPU to run the code. If there are multiple GPUs available, the code uses `nn.DataParallel` to utilize them. For better efficiency, enable distributed training by `--distributed` argument, which can run on multiple nodes.\n\n\n\n## Adaptive Attention Span\nThis code can be used for running experiments in [Adaptive Attention Span for Transformers](https://arxiv.org/abs/1905.07799) paper. The adaptive span allows a model to learn an optimal context size for each self-attention head from training data. As shown in the below figure, only few heads require long attention span, thus making it possible to increase the context size to 8k tokens without increasing computation time and memory footprint significantly.\n\n<div align=\"center\">\n  <img src=\"README_files/span.png\" width=\"400px\" />\n</div>\n\nAn argument `--adapt-span` enables adaptive span. Otherwise a model will have a fixed attention span. The adaptive-span is implemented as a `nn.Module` to make it easier to plug it into other models.\n\n### Running experiments in the paper\nScripts for running experiments in the paper are located in `./experiments/` directory. For example, a smaller 8-layer version of our model can be trained on a single GPU by running:\n```bash\nbash experiments/enwik8_small.sh\n```\nIt should reach about 1.3bpc on dev after 150k steps.\n\nFor training larger models, multiple GPUs are recommended. In the script files, you can configure the number of available GPUs. Increase the `--batch-split` argument if you run out of GPU memory (it splits batches into smaller pieces without changing the final result).\n\nWe  obtained the following results in our experiments:\n\n| Experiment | #params | dev | test |\n| ---------- | ---:| ---:| ----:|\n| enwik8 | 38M | 1.04 bpb | 1.02 bpb |\n| enwik8_large | 209M | 1.00 bpb | 0.98 bpb |\n| text8 | 39M | 1.05 bpc | 1.11 bpc |\n| text8_large | 209M | 1.01 bpc | 1.07 bpc |\n\nA large model training takes about 1.2sec/batch near the end (initially it's faster because the attention spans are smaller) on 8 V100 GPUs. So, for example, the whole `enwik8_large` training of 170k steps should take less than 2.4 days.\n\n### Pre-trained models\nYou can download pre-trained models by running the `get_pretrained.sh` script. Then the same scripts in `./experiments/` can be used to evaluate those models. Since the download script puts models in `./checkpoints/`, make sure there is no file with the same name. Note that these pre-trained models are obtained by rerunning the training scripts after the code cleanup, so there are small differences from the above results due to the randomness of the training.\n\n\n## All-attention Network\nThe code also can be used for training All-attention Networks introduced in [Augmenting Self-attention with Persistent Memory](https://arxiv.org/abs/1907.01470). If `--pers-mem-size` argument is set to `N`, all FF sublayers will be removed from the model and `N` persistent memory vectors will be added to every self-attention sublayer. The following experiments can be found in `./experiments/` directory.\n\n| Experiment | #params | dev | test |\n| ---------- | ---:| ---:| ----:|\n| enwik8_pers_small.sh | 39M | 1.03 bpb | 1.01 bpb |\n| enwik8_pers.sh | 114M | 1.00 bpb | 0.98 bpb |\n| wiki103_pers.sh | 133M | 18.8 ppl *| 19.7 ppl *|\n\n(\\*This number is slightly better than the paper because it includes end-of-line as a token.)\n\n## License\nThe code is licensed under CC-BY-NC license. See the LICENSE file for more details.\n\n## Acknowledgement\nWe thank Xavier Martinet for helping with cleaning the code. The data preprocessing scripts are downloaded from [awd-lstm](https://github.com/salesforce/awd-lstm-lm/) and [transformer-XL](https://github.com/kimiyoung/transformer-xl) repos. The `adagrad_with_grad_clip.py` is mostly adapted from PyTorch.\n",
            "readme_url": "https://github.com/facebookresearch/adaptive-span",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Augmenting Self-attention with Persistent Memory",
            "arxiv": "1907.01470",
            "year": 2019,
            "url": "http://arxiv.org/abs/1907.01470v1",
            "abstract": "Transformer networks have lead to important progress in language modeling and\nmachine translation. These models include two consecutive modules, a\nfeed-forward layer and a self-attention layer. The latter allows the network to\ncapture long term dependencies and are often regarded as the key ingredient in\nthe success of Transformers. Building upon this intuition, we propose a new\nmodel that solely consists of attention layers. More precisely, we augment the\nself-attention layers with persistent memory vectors that play a similar role\nas the feed-forward layer. Thanks to these vectors, we can remove the\nfeed-forward layer without degrading the performance of a transformer. Our\nevaluation shows the benefits brought by our model on standard character and\nword level language modeling benchmarks.",
            "authors": [
                "Sainbayar Sukhbaatar",
                "Edouard Grave",
                "Guillaume Lample",
                "Herve Jegou",
                "Armand Joulin"
            ]
        },
        {
            "title": "Adaptive Attention Span in Transformers",
            "arxiv": "1905.07799",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.07799v2",
            "abstract": "We propose a novel self-attention mechanism that can learn its optimal\nattention span. This allows us to extend significantly the maximum context size\nused in Transformer, while maintaining control over their memory footprint and\ncomputational time. We show the effectiveness of our approach on the task of\ncharacter level language modeling, where we achieve state-of-the-art\nperformances on text8 and enwiki8 by using a maximum context of 8k characters.",
            "authors": [
                "Sainbayar Sukhbaatar",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Armand Joulin"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Text8"
            },
            {
                "name": "enwiki8"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999963101745994,
        "task": "Machine Translation",
        "task_prob": 0.8490362974323253
    }
}