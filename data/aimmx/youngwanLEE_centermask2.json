{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "CenterMask2",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "youngwanLEE",
                "owner_type": "User",
                "name": "centermask2",
                "url": "https://github.com/youngwanLEE/centermask2",
                "stars": 692,
                "pushed_at": "2021-12-27 06:21:35+00:00",
                "created_at": "2020-02-20 08:39:46+00:00",
                "language": "Python",
                "description": "CenterMask : Real-time Anchor-Free Instance Segmentation, in CVPR 2020",
                "license": "Other",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "a5baf7a815de8d26901045da51f8d8ccb23a02cb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/youngwanLEE/centermask2/blob/master/.gitignore"
                    }
                },
                "size": 483
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "80686a1bc76caa1076094c90e2fec530cdcf814e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/youngwanLEE/centermask2/blob/master/LICENSE"
                    }
                },
                "size": 20664
            },
            {
                "type": "code",
                "name": "centermask",
                "sha": "d4fbf977ee234ac7c4366e1320dcc70415ebf26c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/youngwanLEE/centermask2/tree/master/centermask"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "configs",
                "sha": "f91dab444d8e9f8106868979047dc5b76b9d31b6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/youngwanLEE/centermask2/tree/master/configs"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "datasets",
                "sha": "e445c7946caf3cc4672e2e4c85bb5996414fad72",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/youngwanLEE/centermask2/tree/master/datasets"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "demo",
                "sha": "2ec796bc45f790ffc7a34a39a0894270c463ef1b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/youngwanLEE/centermask2/tree/master/demo"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "train_net.py",
                "sha": "05ce501191059edda410d2d9619b7d0e0b615a53",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/youngwanLEE/centermask2/blob/master/train_net.py"
                    }
                },
                "size": 5492
            }
        ]
    },
    "authors": [
        {
            "name": "Youngwan Lee",
            "email": "yw.lee@etri.re.kr",
            "github_id": "youngwanLEE"
        },
        {
            "name": "Yacob",
            "github_id": "YacobBY"
        },
        {
            "name": "stigma0617",
            "github_id": "stigma0617"
        }
    ],
    "tags": [
        "centermask",
        "detectron2",
        "object-detection",
        "instance-segmentation",
        "anchor-free",
        "vovnet",
        "vovnetv2",
        "real-time",
        "pytorch",
        "cvpr2020"
    ],
    "description": "CenterMask : Real-time Anchor-Free Instance Segmentation, in CVPR 2020",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/youngwanLEE/centermask2",
            "stars": 692,
            "issues": true,
            "readme": "# [CenterMask](https://arxiv.org/abs/1911.06667)2\n\n[[`CenterMask(original code)`](https://github.com/youngwanLEE/CenterMask)][[`vovnet-detectron2`](https://github.com/youngwanLEE/vovnet-detectron2)][[`arxiv`](https://arxiv.org/abs/1911.06667)] [[`BibTeX`](#CitingCenterMask)]\n\n**CenterMask2** is an upgraded implementation on top of [detectron2](https://github.com/facebookresearch/detectron2) beyond original [CenterMask](https://github.com/youngwanLEE/CenterMask) based on [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark).\n\n> **[CenterMask : Real-Time Anchor-Free Instance Segmentation](https://arxiv.org/abs/1911.06667) (CVPR 2020)**<br>\n> [Youngwan Lee](https://github.com/youngwanLEE) and Jongyoul Park<br>\n> Electronics and Telecommunications Research Institute (ETRI)<br>\n> pre-print : https://arxiv.org/abs/1911.06667\n\n\n<div align=\"center\">\n  <img src=\"https://dl.dropbox.com/s/yg9zr1tvljoeuyi/architecture.png\" width=\"850px\" />\n</div>\n\n  \n  \n\n## Highlights\n- ***First* anchor-free one-stage instance segmentation.** To the best of our knowledge, **CenterMask** is the first instance segmentation on top of anchor-free object detection (15/11/2019).\n- **Toward Real-Time: CenterMask-Lite.**  This works provide not only large-scale CenterMask but also lightweight CenterMask-Lite that can run at real-time speed (> 30 fps).\n- **State-of-the-art performance.**  CenterMask outperforms Mask R-CNN, TensorMask, and ShapeMask at much faster speed and CenterMask-Lite models also surpass YOLACT or YOLACT++ by large margins.\n- **Well balanced (speed/accuracy) backbone network, VoVNetV2.**  VoVNetV2 shows better performance and faster speed than ResNe(X)t or HRNet.\n\n\n## Updates\n- CenterMask2 has been released. (20/02/2020)\n- Lightweight VoVNet has ben released. (26/02/2020)\n- Panoptic-CenterMask has been released. (31/03/2020)\n- code update for compatibility with pytorch1.7 and the latest detectron2 (22/12/2020)\n## Results on COCO val\n\n### Note\n\nWe measure the inference time of all models with batch size 1 on the same V100 GPU machine.\n\n- pytorch1.7.0\n- CUDA 10.1\n- cuDNN 7.3\n- multi-scale augmentation\n- Unless speficified, no Test-Time Augmentation (TTA)\n\n\n\n### CenterMask\n\n|Method|Backbone|lr sched|inference time|mask AP|box AP|download|\n|:--------:|:--------:|:--:|:--:|----|----|:--------:|\nMask R-CNN (detectron2)|R-50|3x|0.055|37.2|41.0|<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/metrics.json\">metrics</a>\nMask R-CNN (detectron2)|V2-39|3x|0.052|39.3|43.8|<a href=\"https://dl.dropbox.com/s/dkto39ececze6l4/faster_V_39_eSE_ms_3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/dx9qz1dn65ccrwd/faster_V_39_eSE_ms_3x_metrics.json\">metrics</a>\nCenterMask (maskrcnn-benchmark)|V2-39|3x|0.070|38.5|43.5|[link](https://github.com/youngwanLEE/CenterMask#coco-val2017-results)\n**CenterMask2**|V2-39|3x|**0.050**|**39.7**|**44.2**|<a href=\"https://dl.dropbox.com/s/tczecsdxt10uai5/centermask2-V-39-eSE-FPN-ms-3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/rhoo6vkvh7rjdf9/centermask2-V-39-eSE-FPN-ms-3x_metrics.json\">metrics</a>\n||\nMask R-CNN (detectron2)|R-101|3x|0.070|38.6|42.9|<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/metrics.json\">metrics</a>\nMask R-CNN (detectron2)|V2-57|3x|0.058|39.7|44.2|<a href=\"https://dl.dropbox.com/s/c7mb1mq10eo4pzk/faster_V_57_eSE_ms_3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/3tsn218zzmuhyo8/faster_V_57_eSE_metrics.json\">metrics</a>\nCenterMask (maskrcnn-benchmark)|V2-57|3x|0.076|39.4|44.6|[link](https://github.com/youngwanLEE/CenterMask#coco-val2017-results)\n**CenterMask2**|V2-57|3x|**0.058**|**40.5**|**45.1**|<a href=\"https://dl.dropbox.com/s/lw8nxajv1tim8gr/centermask2-V-57-eSE-FPN-ms-3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/x7r5ys3c81ldgq0/centermask2-V-57-eSE-FPN-ms-3x_metrics.json\">metrics</a>\n||\nMask R-CNN (detectron2)|X-101|3x|0.129|39.5|44.3|<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x/139653917/model_final_2d9806.pkl\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x/139653917/metrics.json\">metrics</a>\nMask R-CNN (detectron2)|V2-99|3x|0.076|40.3|44.9|<a href=\"https://dl.dropbox.com/s/v64mknwzfpmfcdh/faster_V_99_eSE_ms_3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/zvaz9s8gvq2mhrd/faster_V_99_eSE_ms_3x_metrics.json\">metrics</a>\nCenterMask (maskrcnn-benchmark)|V2-99|3x|0.106|40.2|45.6|[link](https://github.com/youngwanLEE/CenterMask#coco-val2017-results)\n**CenterMask2**|V2-99|3x|**0.077**|**41.4**|**46.0**|<a href=\"https://dl.dropbox.com/s/c6n79x83xkdowqc/centermask2-V-99-eSE-FPN-ms-3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/jdzgmdatit00hq5/centermask2-V-99-eSE-FPN-ms-3x_metrics.json\">metrics</a>\n||\n**CenterMask2 (TTA)**|V2-99|3x|-|**42.5**|**48.6**|<a href=\"https://dl.dropbox.com/s/c6n79x83xkdowqc/centermask2-V-99-eSE-FPN-ms-3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/jdzgmdatit00hq5/centermask2-V-99-eSE-FPN-ms-3x_metrics.json\">metrics</a>\n* TTA denotes Test-Time Augmentation (multi-scale test).\n\n### CenterMask-Lite\n\n|Method|Backbone|lr sched|inference time|mask AP|box AP|download|\n|:--------:|:--------:|:--:|:--:|:----:|:----:|:--------:|\n|YOLACT550|R-50|4x|0.023|28.2|30.3|[link](https://github.com/dbolya/yolact)\n|CenterMask (maskrcnn-benchmark)|V-19|4x|0.023|32.4|35.9|[link](https://github.com/youngwanLEE/CenterMask#coco-val2017-results)\n|**CenterMask2-Lite**|V-19|4x|0.023|**32.8**|**35.9**|<a href=\"https://dl.dropbox.com/s/dret2ap7djty7mp/centermask2-lite-V-19-eSE-FPN-ms-4x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/zsta7azy87a833u/centermask2-lite-V-19-eSE-FPN-ms-4x-metrics.json\">metrics</a>\n||\n|YOLACT550|R-101|4x|0.030|28.2|30.3|[link](https://github.com/dbolya/yolact)\n|YOLACT550++|R-50|4x|0.029|34.1|-|[link](https://github.com/dbolya/yolact)\n|YOLACT550++|R-101|4x|0.036|34.6|-|[link](https://github.com/dbolya/yolact)\n|CenterMask (maskrcnn-benchmark)|V-39|4x|0.027|36.3|40.7|[link](https://github.com/youngwanLEE/CenterMask#coco-val2017-results)\n|**CenterMask2-Lite**|V-39|4x|0.028|**36.7**|**40.9**|<a href=\"https://dl.dropbox.com/s/uwc0ypa1jvco2bi/centermask2-lite-V-39-eSE-FPN-ms-4x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/aoa6y3i3el4edbk/centermask2-lite-V-39-eSE-FPN-ms-4x-metrics.json\">metrics</a>\n* Note that The inference time is measured on Titan Xp GPU for fair comparison with YOLACT.\n\n### Lightweight VoVNet backbone\n\n|Method|Backbone|Param.|lr sched|inference time|mask AP|box AP|download|\n|:--------:|:--------:|:--:|:--:|:--:|:----:|:----:|:--------:|\n|CenterMask2-Lite|MobileNetV2|3.5M|4x|0.021|27.2|29.8|<a href=\"https://dl.dropbox.com/s/8omou546f0n78nj/centermask_lite_Mv2_ms_4x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/2jlcwy30eq72w47/centermask_lite_Mv2_ms_4x_metrics.json\">metrics</a>\n||\n|CenterMask2-Lite|V-19|11.2M|4x|0.023|32.8|35.9|<a href=\"https://dl.dropbox.com/s/dret2ap7djty7mp/centermask2-lite-V-19-eSE-FPN-ms-4x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/zsta7azy87a833u/centermask2-lite-V-19-eSE-FPN-ms-4x-metrics.json\">metrics</a>\n|CenterMask2-Lite|V-19-**Slim**|3.1M|4x|0.021|29.8|32.5|<a href=\"https://dl.dropbox.com/s/o2n1ifl0zkbv16x/centermask-lite-V-19-eSE-slim-FPN-ms-4x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/8y71oz0kxwqk7go/centermask-lite-V-19-eSE-slim-FPN-ms-4x-metrics.json?dl=0\">metrics</a>\n|CenterMask2-Lite|V-19**Slim**-**DW**|1.8M|4x|0.020|27.1|29.5|<a href=\"https://dl.dropbox.com/s/vsvhwtqm6ko1c7m/centermask-lite-V-19-eSE-slim-dw-FPN-ms-4x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/q4idjnsgvo151zx/centermask-lite-V-19-eSE-slim-dw-FPN-ms-4x-metrics.json\">metrics</a>\n* _**DW** and **Slim** denote depthwise separable convolution and a thiner model with half the channel size, respectively._   \n* __Params.__ means the number of parameters of backbone.   \n\n### Deformable VoVNet Backbone\n\n|Method|Backbone|lr sched|inference time|mask AP|box AP|download|\n|:--------:|:--------:|:--:|:--:|:--:|:----:|:----:|\nCenterMask2|V2-39|3x|0.050|39.7|44.2|<a href=\"https://dl.dropbox.com/s/tczecsdxt10uai5/centermask2-V-39-eSE-FPN-ms-3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/rhoo6vkvh7rjdf9/centermask2-V-39-eSE-FPN-ms-3x_metrics.json\">metrics</a>\nCenterMask2|V2-39-DCN|3x|0.061|40.3|45.1|<a href=\"https://dl.dropbox.com/s/zmps03vghzirk7v/centermask-V-39-eSE-dcn-FPN-ms-3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/aj1mr8m32z11zbw/centermask-V-39-eSE-dcn-FPN-ms-3x-metrics.json\">metrics</a>\n||\nCenterMask2|V2-57|3x|0.058|40.5|45.1|<a href=\"https://dl.dropbox.com/s/lw8nxajv1tim8gr/centermask2-V-57-eSE-FPN-ms-3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/x7r5ys3c81ldgq0/centermask2-V-57-eSE-FPN-ms-3x_metrics.json\">metrics</a>\nCenterMask2|V2-57-DCN|3x|0.071|40.9|45.5|<a href=\"https://dl.dropbox.com/s/1f64azqyd2ot6qq/centermask-V-57-eSE-dcn-FPN-ms-3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/b3zpguko137r6eh/centermask-V-57-eSE-dcn-FPN-ms-3x-metrics.json\">metrics</a>\n||\nCenterMask2|V2-99|3x|0.077|41.4|46.0|<a href=\"https://dl.dropbox.com/s/c6n79x83xkdowqc/centermask2-V-99-eSE-FPN-ms-3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/jdzgmdatit00hq5/centermask2-V-99-eSE-FPN-ms-3x_metrics.json\">metrics</a>\nCenterMask2|V2-99-DCN|3x|0.110|42.0|46.9|<a href=\"https://dl.dropbox.com/s/atuph90nzm7s8x8/centermask-V-99-eSE-dcn-FPN-ms-3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/82ulexlivy19cve/centermask-V-99-eSE-dcn-FPN-ms-3x-metrics.json\">metrics</a>\n||\n\n* _DCN denotes deformable convolutional networks v2. Note that we apply deformable convolutions from stage 3 to 5 in backbones._\n\n### Panoptic-CenterMask\n\n|Method|Backbone|lr sched|inference time|mask AP|box AP|PQ|download|\n|:--------:|:--------:|:--:|:--:|:--:|:----:|:----:|:--------:|\n|Panoptic-FPN|R-50|3x|0.063|40.0|36.5|41.5|<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x/139514569/model_final_c10459.pkl\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x/139514569/metrics.json\">metrics</a>\n|Panoptic-CenterMask|R-50|3x|0.063|41.4|37.3|42.0|<a href=\"https://dl.dropbox.com/s/vxe51cdeprao94j/panoptic_centermask_R_50_ms_3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/dfddgx6rnw1zr4l/panoptic_centermask_R_50_ms_3x_metrics.json\">metrics</a>\n|Panoptic-FPN|V-39|3x|0.063|42.8|38.5|43.4|<a href=\"https://dl.dropbox.com/s/fnr9r4arv0cbfbf/panoptic_V_39_eSE_3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/vftfukrjuu7w1ao/panoptic_V_39_eSE_3x_metrics.json\">metrics</a>\n|Panoptic-CenterMask|V-39|3x|0.066|43.4|39.0|43.7|<a href=\"https://dl.dropbox.com/s/49ig16ailra1f4t/panoptic_centermask_V_39_eSE_ms_3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/wy4mn8n513k0um5/panoptic_centermask_V_39_eSE_ms_3x_metrics.json\">metrics</a>\n||\n|Panoptic-FPN|R-101|3x|0.078|42.4|38.5|43.0|<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/metrics.json\">metrics</a>\n|Panoptic-CenterMask|R-101|3x|0.076|43.5|39.0|43.6|<a href=\"https://dl.dropbox.com/s/y5stg3qx72gff5o/panoptic_centermask_R_101_ms_3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/ojljt0obp8vnr8s/panoptic_centermask_R_101_ms_3x_metrics.json\">metrics</a>\n|Panoptic-FPN|V-57|3x|0.070|43.4|39.2|44.3|<a href=\"https://www.dropbox.com/s/zhoqx5rvc0jj0oa/panoptic_V_57_eSE_3x.pth?dl=1\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/20hwrmru15dilre/panoptic_V_57_eSE_3x_metrics.json\">metrics</a>\n|Panoptic-CenterMask|V-57|3x|0.071|43.9|39.6|44.5|<a href=\"https://dl.dropbox.com/s/kqukww4y7tbgbrh/panoptic_centermask_V_57_ms_3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/4asto3b4iya74ak/panoptic_centermask_V_57_ms_3x_metrics.json\">metrics</a>\n||\n|Panoptic-CenterMask|V-99|3x|0.091|45.1|40.6|45.4|<a href=\"https://dl.dropbox.com/s/pr6a3inpasn7qlz/panoptic_centermask_V_99_ms_3x.pth\">model</a>&nbsp;\\|&nbsp;<a href=\"https://dl.dropbox.com/s/00e8x0riplme7pm/panoptic_centermask_V_99_ms_3x_metrics.json\">metrics</a>\n\n\n## Installation\nAll you need to use centermask2 is [detectron2](https://github.com/facebookresearch/detectron2). It's easy!    \nyou just install [detectron2](https://github.com/facebookresearch/detectron2) following [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).   \nPrepare for coco dataset following [this instruction](https://github.com/facebookresearch/detectron2/tree/master/datasets).\n\n## Training\n\n#### ImageNet Pretrained Models\n\nWe provide backbone weights pretrained on ImageNet-1k dataset for detectron2.\n* [MobileNet-V2](https://dl.dropbox.com/s/yduxbc13s3ip6qn/mobilenet_v2_detectron2.pth)\n* [VoVNetV2-19-Slim-DW](https://dl.dropbox.com/s/f3s7ospitqoals1/vovnet19_ese_slim_dw_detectron2.pth)\n* [VoVNetV2-19-Slim](https://dl.dropbox.com/s/8h5ybmi4ftbcom0/vovnet19_ese_slim_detectron2.pth)\n* [VoVNetV2-19](https://dl.dropbox.com/s/rptgw6stppbiw1u/vovnet19_ese_detectron2.pth)\n* [VoVNetV2-39](https://dl.dropbox.com/s/q98pypf96rhtd8y/vovnet39_ese_detectron2.pth)\n* [VoVNetV2-57](https://dl.dropbox.com/s/8xl0cb3jj51f45a/vovnet57_ese_detectron2.pth)\n* [VoVNetV2-99](https://dl.dropbox.com/s/1mlv31coewx8trd/vovnet99_ese_detectron2.pth)\n\n\nTo train a model, run\n```bash\ncd centermask2\npython train_net.py --config-file \"configs/<config.yaml>\"\n```\n\nFor example, to launch CenterMask training with VoVNetV2-39 backbone on 8 GPUs,\none should execute:\n```bash\ncd centermask2\npython train_net.py --config-file \"configs/centermask/centermask_V_39_eSE_FPN_ms_3x.yaml\" --num-gpus 8\n```\n\n## Evaluation\n\nModel evaluation can be done similarly:   \n* if you want to inference with 1 batch `--num-gpus 1` \n* `--eval-only`\n* `MODEL.WEIGHTS path/to/the/model.pth`\n\n```bash\ncd centermask2\nwget https://dl.dropbox.com/s/tczecsdxt10uai5/centermask2-V-39-eSE-FPN-ms-3x.pth\npython train_net.py --config-file \"configs/centermask/centermask_V_39_eSE_FPN_ms_3x.yaml\" --num-gpus 1 --eval-only MODEL.WEIGHTS centermask2-V-39-eSE-FPN-ms-3x.pth\n```\n\n## TODO\n - [x] Adding Lightweight models\n - [ ] Applying CenterMask for PointRend or Panoptic-FPN.\n\n\n## <a name=\"CitingCenterMask\"></a>Citing CenterMask\n\nIf you use VoVNet, please use the following BibTeX entry.\n\n```BibTeX\n@inproceedings{lee2019energy,\n  title = {An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection},\n  author = {Lee, Youngwan and Hwang, Joong-won and Lee, Sangrok and Bae, Yuseok and Park, Jongyoul},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},\n  year = {2019}\n}\n\n@inproceedings{lee2020centermask,\n  title={CenterMask: Real-Time Anchor-Free Instance Segmentation},\n  author={Lee, Youngwan and Park, Jongyoul},\n  booktitle={CVPR},\n  year={2020}\n}\n```\n\n## Special Thanks to\n\n[mask scoring for detectron2](https://github.com/lsrock1/maskscoring_rcnn.detectron2) by [Sangrok Lee](https://github.com/lsrock1)   \n[FCOS_for_detectron2](https://github.com/aim-uofa/adet) by AdeliDet team.\n",
            "readme_url": "https://github.com/youngwanLEE/centermask2",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "CenterMask : Real-Time Anchor-Free Instance Segmentation",
            "arxiv": "1911.06667",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.06667v6",
            "abstract": "We propose a simple yet efficient anchor-free instance segmentation, called\nCenterMask, that adds a novel spatial attention-guided mask (SAG-Mask) branch\nto anchor-free one stage object detector (FCOS) in the same vein with Mask\nR-CNN. Plugged into the FCOS object detector, the SAG-Mask branch predicts a\nsegmentation mask on each box with the spatial attention map that helps to\nfocus on informative pixels and suppress noise. We also present an improved\nbackbone networks, VoVNetV2, with two effective strategies: (1) residual\nconnection for alleviating the optimization problem of larger VoVNet\n\\cite{lee2019energy} and (2) effective Squeeze-Excitation (eSE) dealing with\nthe channel information loss problem of original SE. With SAG-Mask and\nVoVNetV2, we deign CenterMask and CenterMask-Lite that are targeted to large\nand small models, respectively. Using the same ResNet-101-FPN backbone,\nCenterMask achieves 38.3%, surpassing all previous state-of-the-art methods\nwhile at a much faster speed. CenterMask-Lite also outperforms the\nstate-of-the-art by large margins at over 35fps on Titan Xp. We hope that\nCenterMask and VoVNetV2 can serve as a solid baseline of real-time instance\nsegmentation and backbone network for various vision tasks, respectively. The\nCode is available at https://github.com/youngwanLEE/CenterMask.",
            "authors": [
                "Youngwan Lee",
                "Jongyoul Park"
            ]
        },
        {
            "year": "2019",
            "booktitle": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops",
            "author": [
                "Lee, Youngwan",
                "Hwang, Joong-won",
                "Lee, Sangrok",
                "Bae, Yuseok",
                "Park, Jongyoul"
            ],
            "title": "An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection",
            "ENTRYTYPE": "inproceedings",
            "ID": "lee2019energy",
            "authors": [
                "Lee, Youngwan",
                "Hwang, Joong-won",
                "Lee, Sangrok",
                "Bae, Yuseok",
                "Park, Jongyoul"
            ]
        },
        {
            "year": "2020",
            "booktitle": "CVPR",
            "author": [
                "Lee, Youngwan",
                "Park, Jongyoul"
            ],
            "title": "CenterMask: Real-Time Anchor-Free Instance Segmentation",
            "ENTRYTYPE": "inproceedings",
            "ID": "lee2020centermask",
            "authors": [
                "Lee, Youngwan",
                "Park, Jongyoul"
            ]
        },
        {
            "title": "MobileNet-V2",
            "url": "https://dl.dropbox.com/s/yduxbc13s3ip6qn/mobilenet_v2_detectron2.pth"
        },
        {
            "title": "VoVNetV2-19-Slim-DW",
            "url": "https://dl.dropbox.com/s/f3s7ospitqoals1/vovnet19_ese_slim_dw_detectron2.pth"
        },
        {
            "title": "VoVNetV2-19-Slim",
            "url": "https://dl.dropbox.com/s/8h5ybmi4ftbcom0/vovnet19_ese_slim_detectron2.pth"
        },
        {
            "title": "VoVNetV2-19",
            "url": "https://dl.dropbox.com/s/rptgw6stppbiw1u/vovnet19_ese_detectron2.pth"
        },
        {
            "title": "VoVNetV2-39",
            "url": "https://dl.dropbox.com/s/q98pypf96rhtd8y/vovnet39_ese_detectron2.pth"
        },
        {
            "title": "VoVNetV2-57",
            "url": "https://dl.dropbox.com/s/8xl0cb3jj51f45a/vovnet57_ese_detectron2.pth"
        },
        {
            "title": "VoVNetV2-99",
            "url": "https://dl.dropbox.com/s/1mlv31coewx8trd/vovnet99_ese_detectron2.pth"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9990742970967337,
        "task": "Object Detection",
        "task_prob": 0.9761137230053634
    }
}