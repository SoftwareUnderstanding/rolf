{
    "visibility": {
        "visibility": "public"
    },
    "name": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "yumoh",
                "owner_type": "User",
                "name": "speech-keras",
                "url": "https://github.com/yumoh/speech-keras",
                "stars": 0,
                "pushed_at": "2019-02-16 06:09:12+00:00",
                "created_at": "2019-02-16 06:06:47+00:00",
                "language": "Python",
                "frameworks": [
                    "Keras",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "fc64e3cda52be747be6d176256e6d404ee1b2901",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/blob/master/.gitattributes"
                    }
                },
                "size": 101
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "b5c0eabddfeb24a386d30a1f0613c8637516240c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/blob/master/.gitignore"
                    }
                },
                "size": 19
            },
            {
                "type": "code",
                "name": "data",
                "sha": "618abb935d28ff6b59596b7fe444c09b8d8c03e5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/tree/master/data"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "logs_am",
                "sha": "8838fabdd3b7f655f79fbf3e834f9b1e804700af",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/tree/master/logs_am"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "logs_lm",
                "sha": "737fdc2c2fe8cdcad033c760ef5b22bfe8ad6e68",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/tree/master/logs_lm"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "model_language",
                "sha": "4e6cd9085b8b458497248f04fef5c8a1acd22ba8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/tree/master/model_language"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "model_speech",
                "sha": "5d0f14c454afd3a8fb86c5f5efad221f503efa77",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/tree/master/model_speech"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "d48ad6a1e1d8ff50cbc0fa013a8576fbd7a56c6b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/blob/master/test.py"
                    }
                },
                "size": 2635
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "dd465e73da9b4d3dea54a4401774e2c33ad6fa67",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/blob/master/train.py"
                    }
                },
                "size": 3825
            },
            {
                "type": "code",
                "name": "tutorial",
                "sha": "977df4641669640933750e1cf9827b48215f0d40",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/tree/master/tutorial"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "a2eaedb6fd98a8d8179d2de6d3058a82a038b608",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yumoh/speech-keras/blob/master/utils.py"
                    }
                },
                "size": 9168
            }
        ]
    },
    "authors": [
        {
            "name": "Hongwen",
            "email": "hit_master@163.com",
            "github_id": "hongwen-sun"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/yumoh/speech-keras",
            "stars": 0,
            "issues": true,
            "readme": "# \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\n\n[![GPL-3.0 Licensed](https://img.shields.io/badge/License-GPL3.0-blue.svg?style=flat)](https://opensource.org/licenses/GPL-3.0) [![TensorFlow Version](https://img.shields.io/badge/Tensorflow-1.7+-blue.svg)](https://www.tensorflow.org/) [![Keras Version](https://img.shields.io/badge/Keras-2.0+-blue.svg)](https://keras.io/) [![Python Version](https://img.shields.io/badge/Python-3.x-blue.svg)](https://www.python.org/)\n\n\u5982\u679c\u89c9\u5f97\u6709\u7528\u7684\u8bdd\uff0c\u5c0f\u624b\u7ed9\u4e2astar\u5427~\n\n## 1. Introduction\n\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6df1\u5ea6\u6846\u67b6\u7684\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u58f0\u5b66\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\u5efa\u6a21\uff0c\u5176\u4e2d\u58f0\u5b66\u6a21\u578b\u5305\u62ecCNN-CTC\u3001GRU-CTC\u3001CNN-RNN-CTC\uff0c\u8bed\u8a00\u6a21\u578b\u5305\u542b[transformer](https://jalammar.github.io/illustrated-transformer/)\u3001[CBHG](https://github.com/crownpku/Somiao-Pinyin)\uff0c\u6570\u636e\u96c6\u5305\u542bstc\u3001primewords\u3001Aishell\u3001thchs30\u56db\u4e2a\u6570\u636e\u96c6\u3002\n\n\u672c\u7cfb\u7edf\u66f4\u6574\u4f53\u4ecb\u7ecd\uff1ahttps://blog.csdn.net/chinatelecom08/article/details/82557715\n\n\u672c\u9879\u76ee\u73b0\u5df2\u8bad\u7ec3\u4e00\u4e2a\u8ff7\u4f60\u7684\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u5c06\u9879\u76ee\u4e0b\u8f7d\u5230\u672c\u5730\u4e0a\uff0c\u4e0b\u8f7d[thchs\u6570\u636e\u96c6](http://www.openslr.org/resources/18/data_thchs30.tgz)\u5e76\u89e3\u538b\u81f3data\uff0c\u8fd0\u884c`test.py`\uff0c\u4e0d\u51fa\u610f\u5916\u80fd\u591f\u8fdb\u884c\u8bc6\u522b\uff0c\u7ed3\u679c\u5982\u4e0b\uff1a\n\n     the  0 th example.\n    \u6587\u672c\u7ed3\u679c\uff1a lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2\n    \u539f\u6587\u7ed3\u679c\uff1a lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2\n    \u539f\u6587\u6c49\u5b57\uff1a \u7eff\u662f\u9633\u6625\u70df\u666f\u5927\u5757\u6587\u7ae0\u7684\u5e95\u8272\u56db\u6708\u7684\u6797\u5ce6\u66f4\u662f\u7eff\u5f97\u9c9c\u6d3b\u79c0\u5a9a\u8bd7\u610f\u76ce\u7136\n    \u8bc6\u522b\u7ed3\u679c\uff1a \u7eff\u662f\u9633\u6625\u70df\u666f\u5927\u5757\u6587\u7ae0\u7684\u5e95\u8272\u56db\u6708\u7684\u6797\u5ce6\u66f4\u662f\u7eff\u5f97\u9c9c\u6d3b\u79c0\u5a9a\u8bd7\u610f\u76ce\u7136\n\n\u82e5\u81ea\u5df1\u5efa\u7acb\u6a21\u578b\u5219\u9700\u8981\u5220\u9664\u73b0\u6709\u6a21\u578b\uff0c\u91cd\u65b0\u914d\u7f6e\u53c2\u6570\u8bad\u7ec3\uff0c\u5177\u4f53\u5b9e\u73b0\u6d41\u7a0b\u53c2\u8003\u672c\u9875\u6700\u540e\u3002\n\n## 2. \u58f0\u5b66\u6a21\u578b\n\n\u58f0\u5b66\u6a21\u578b\u91c7\u7528CTC\u8fdb\u884c\u5efa\u6a21\uff0c\u91c7\u7528CNN-CTC\u3001GRU-CTC\u3001FSMN\u7b49\u6a21\u578b`model_speech`\uff0c\u91c7\u7528keras\u4f5c\u4e3a\u7f16\u5199\u6846\u67b6\u3002\n\n- \u8bba\u6587\u5730\u5740\uff1ahttp://www.infocomm-journal.com/dxkx/CN/article/openArticlePDFabs.jsp?id=166970\n\n- tutorial\uff1ahttps://blog.csdn.net/chinatelecom08/article/details/85013535\n\n\n## 3. \u8bed\u8a00\u6a21\u578b\n\n\u65b0\u589e\u57fa\u4e8eself-attention\u7ed3\u6784\u7684\u8bed\u8a00\u6a21\u578b`model_language\\transformer.py`\uff0c\u8be5\u6a21\u578b\u5df2\u7ecf\u88ab\u8bc1\u660e\u6709\u5f3a\u4e8e\u5176\u4ed6\u6846\u67b6\u7684\u8bed\u8a00\u8868\u8fbe\u80fd\u529b\u3002\n\n- \u8bba\u6587\u5730\u5740\uff1ahttps://arxiv.org/abs/1706.03762\u3002\n\n- tutorial\uff1ahttps://blog.csdn.net/chinatelecom08/article/details/85051817\n\n\u57fa\u4e8eCBHG\u7ed3\u6784\u7684\u8bed\u8a00\u6a21\u578b`model_language\\cbhg.py`\uff0c\u8be5\u6a21\u578b\u4e4b\u524d\u7528\u4e8e\u8c37\u6b4c\u58f0\u97f3\u5408\u6210\uff0c\u79fb\u690d\u5230\u8be5\u9879\u76ee\u4e2d\u4f5c\u4e3a\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u8bed\u8a00\u6a21\u578b\u3002\n\n- \u539f\u7406\u5730\u5740\uff1ahttps://github.com/crownpku/Somiao-Pinyin\n\n- tutorial\uff1ahttps://blog.csdn.net/chinatelecom08/article/details/85048019\n\n\n## 4. \u6570\u636e\u96c6\n\u5305\u62ecstc\u3001primewords\u3001Aishell\u3001thchs30\u56db\u4e2a\u6570\u636e\u96c6\uff0c\u5171\u8ba1\u7ea6430\u5c0f\u65f6, \u76f8\u5173\u94fe\u63a5\uff1a[http://www.openslr.org/resources.php](http://www.openslr.org/resources.php)\n\n\n|Name | train | dev | test\n|- | :-: | -: | -:\n|aishell | 120098| 14326 | 7176\n|primewords | 40783 | 5046 | 5073\n|thchs-30 | 10000 | 893 | 2495\n|st-cmd | 10000 | 600 | 2000\n\n\n\u6570\u636e\u6807\u7b7e\u6574\u7406\u5728`data`\u8def\u5f84\u4e0b\uff0c\u5176\u4e2dprimewords\u3001st-cmd\u76ee\u524d\u672a\u533a\u5206\u8bad\u7ec3\u96c6\u6d4b\u8bd5\u96c6\u3002\n\n\u82e5\u9700\u8981\u4f7f\u7528\u6240\u6709\u6570\u636e\u96c6\uff0c\u53ea\u9700\u89e3\u538b\u5230\u7edf\u4e00\u8def\u5f84\u4e0b\uff0c\u7136\u540e\u8bbe\u7f6eutils.py\u4e2ddatapath\u7684\u8def\u5f84\u5373\u53ef\u3002\n\n\u4e0e\u6570\u636e\u76f8\u5173\u53c2\u6570\u5728`utils.py`\u4e2d\uff1a\n- data_type: train, test, dev\n- data_path: \u5bf9\u5e94\u89e3\u538b\u6570\u636e\u7684\u8def\u5f84\n- thchs30, aishell, prime, stcmd: \u662f\u5426\u4f7f\u7528\u8be5\u6570\u636e\u96c6\n- batch_size: batch_size\n- data_length: \u6211\u81ea\u5df1\u505a\u5b9e\u9a8c\u65f6\u5199\u5c0f\u4e00\u4e9b\u770b\u6548\u679c\u7528\u7684\uff0c\u6b63\u5e38\u4f7f\u7528\u8bbe\u4e3aNone\u5373\u53ef\n- shuffle\uff1a\u6b63\u5e38\u8bad\u7ec3\u8bbe\u4e3aTrue\uff0c\u662f\u5426\u6253\u4e71\u8bad\u7ec3\u987a\u5e8f\n```py\ndef data_hparams():\n    params = tf.contrib.training.HParams(\n        # vocab\n        data_type = 'train',\n        data_path = 'data/',\n        thchs30 = True,\n        aishell = True,\n        prime = False,\n        stcmd = False,\n        batch_size = 1,\n        data_length = None,\n        shuffle = False)\n      return params\n```\n\n## 5. \u914d\u7f6e\n\n\u4f7f\u7528train.py\u6587\u4ef6\u8fdb\u884c\u6a21\u578b\u7684\u8bad\u7ec3\u3002\n\n\u58f0\u5b66\u6a21\u578b\u53ef\u9009cnn-ctc\u3001gru-ctc\uff0c\u53ea\u9700\u4fee\u6539\u5bfc\u5165\u8def\u5f84\u5373\u53ef\uff1a\n\n`from model_speech.cnn_ctc import Am, am_hparams`\n\n`from model_speech.gru_ctc import Am, am_hparams`\n\n\u8bed\u8a00\u6a21\u578b\u53ef\u9009transformer\u548ccbhg:\n\n`from model_language.transformer import Lm, lm_hparams`\n\n`from model_language.cbhg import Lm, lm_hparams`\n\n### \u6a21\u578b\u8bc6\u522b\n\n\u4f7f\u7528test.py\u68c0\u67e5\u6a21\u578b\u8bc6\u522b\u6548\u679c\u3002\n\u6a21\u578b\u9009\u62e9\u9700\u548c\u8bad\u7ec3\u4e00\u81f4\u3002\n\n\n# \u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\n\n\n# 1. \u58f0\u5b66\u6a21\u578b\u8bad\u7ec3\n\ntrain.py\u6587\u4ef6\n\n\n```python\nimport os\nimport tensorflow as tf\nfrom utils import get_data, data_hparams\n\n\n# \u51c6\u5907\u8bad\u7ec3\u6240\u9700\u6570\u636e\ndata_args = data_hparams()\ndata_args.data_length = 10\ntrain_data = get_data(data_args)\n\n\n# 1.\u58f0\u5b66\u6a21\u578b\u8bad\u7ec3-----------------------------------\nfrom model_speech.cnn_ctc import Am, am_hparams\nam_args = am_hparams()\nam_args.vocab_size = len(train_data.am_vocab)\nam = Am(am_args)\nif os.path.exists('logs_am/model.h5'):\n    print('load acoustic model...')\n    am.ctc_model.load_weights('logs_am/model.h5')\n\nepochs = 10\nbatch_num = len(train_data.wav_lst) // train_data.batch_size\n\nfor k in range(epochs):\n    print('this is the', k+1, 'th epochs trainning !!!')\n    #shuffle(shuffle_list)\n    batch = train_data.get_am_batch()\n    am.ctc_model.fit_generator(batch, steps_per_epoch=batch_num, epochs=1)\n\nam.ctc_model.save_weights('logs_am/model.h5')\n\n```\n\n    get source list...\n    load  thchs_train.txt  data...\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00<00:00, 236865.96it/s]\n    load  aishell_train.txt  data...\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 120098/120098 [00:00<00:00, 260863.15it/s]\n    make am vocab...\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 9986.44it/s]  \n    make lm pinyin vocab...\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 9946.18it/s]\n    make lm hanzi vocab...\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 9950.90it/s]\n    Using TensorFlow backend.\n\n\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    the_inputs (InputLayer)      (None, None, 200, 1)      0         \n    _________________________________________________________________\n    conv2d_1 (Conv2D)            (None, None, 200, 32)     320       \n    _________________________________________________________________\n    batch_normalization_1 (Batch (None, None, 200, 32)     128       \n    _________________________________________________________________\n    conv2d_2 (Conv2D)            (None, None, 200, 32)     9248      \n    _________________________________________________________________\n    batch_normalization_2 (Batch (None, None, 200, 32)     128       \n    _________________________________________________________________\n    max_pooling2d_1 (MaxPooling2 (None, None, 100, 32)     0         \n    _________________________________________________________________\n    conv2d_3 (Conv2D)            (None, None, 100, 64)     18496     \n    _________________________________________________________________\n    batch_normalization_3 (Batch (None, None, 100, 64)     256       \n    _________________________________________________________________\n    conv2d_4 (Conv2D)            (None, None, 100, 64)     36928     \n    _________________________________________________________________\n    batch_normalization_4 (Batch (None, None, 100, 64)     256       \n    _________________________________________________________________\n    max_pooling2d_2 (MaxPooling2 (None, None, 50, 64)      0         \n    _________________________________________________________________\n    conv2d_5 (Conv2D)            (None, None, 50, 128)     73856     \n    _________________________________________________________________\n    batch_normalization_5 (Batch (None, None, 50, 128)     512       \n    _________________________________________________________________\n    conv2d_6 (Conv2D)            (None, None, 50, 128)     147584    \n    _________________________________________________________________\n    batch_normalization_6 (Batch (None, None, 50, 128)     512       \n    _________________________________________________________________\n    max_pooling2d_3 (MaxPooling2 (None, None, 25, 128)     0         \n    _________________________________________________________________\n    conv2d_7 (Conv2D)            (None, None, 25, 128)     147584    \n    _________________________________________________________________\n    batch_normalization_7 (Batch (None, None, 25, 128)     512       \n    _________________________________________________________________\n    conv2d_8 (Conv2D)            (None, None, 25, 128)     147584    \n    _________________________________________________________________\n    batch_normalization_8 (Batch (None, None, 25, 128)     512       \n    _________________________________________________________________\n    conv2d_9 (Conv2D)            (None, None, 25, 128)     147584    \n    _________________________________________________________________\n    batch_normalization_9 (Batch (None, None, 25, 128)     512       \n    _________________________________________________________________\n    conv2d_10 (Conv2D)           (None, None, 25, 128)     147584    \n    _________________________________________________________________\n    batch_normalization_10 (Batc (None, None, 25, 128)     512       \n    _________________________________________________________________\n    reshape_1 (Reshape)          (None, None, 3200)        0         \n    _________________________________________________________________\n    dense_1 (Dense)              (None, None, 256)         819456    \n    _________________________________________________________________\n    dense_2 (Dense)              (None, None, 230)         59110     \n    =================================================================\n    Total params: 1,759,174\n    Trainable params: 1,757,254\n    Non-trainable params: 1,920\n    _________________________________________________________________\n    load acoustic model...\n\n\n# 2.\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\n\n\n```python\n# 2.\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3-------------------------------------------\nfrom model_language.transformer import Lm, lm_hparams\n\n\nlm_args = lm_hparams()\nlm_args.input_vocab_size = len(train_data.pny_vocab)\nlm_args.label_vocab_size = len(train_data.han_vocab)\nlm = Lm(lm_args)\n\nepochs = 10\nwith lm.graph.as_default():\n    saver =tf.train.Saver()\nwith tf.Session(graph=lm.graph) as sess:\n    merged = tf.summary.merge_all()\n    sess.run(tf.global_variables_initializer())\n    if os.path.exists('logs_lm/model.meta'):\n        print('loading language model...')\n        saver.restore(sess, 'logs_lm/model')\n    writer = tf.summary.FileWriter('logs_lm/tensorboard', tf.get_default_graph())\n    for k in range(epochs):\n        total_loss = 0\n        batch = train_data.get_lm_batch()\n        for i in range(batch_num):\n            input_batch, label_batch = next(batch)\n            feed = {lm.x: input_batch, lm.y: label_batch}\n            cost,_ = sess.run([lm.mean_loss,lm.train_op], feed_dict=feed)\n            total_loss += cost\n            if (k * batch_num + i) % 10 == 0:\n                rs=sess.run(merged, feed_dict=feed)\n                writer.add_summary(rs, k * batch_num + i)\n        if (k+1) % 5 == 0:\n            print('epochs', k+1, ': average loss = ', total_loss/batch_num)\n    saver.save(sess, 'logs_lm/model')\n    writer.close()\n```\n\n    loading language model...\n    INFO:tensorflow:Restoring parameters from logs_lm/model\n\n\n# 3. \u6a21\u578b\u6d4b\u8bd5\n\u6574\u5408\u58f0\u5b66\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\n\ntest.py\u6587\u4ef6\n\n\n## \u5b9a\u4e49\u89e3\u7801\u5668\n\n\n```python\nimport os\nimport tensorflow as tf\nimport numpy as np\nfrom keras import backend as K\n\n# \u5b9a\u4e49\u89e3\u7801\u5668------------------------------------\ndef decode_ctc(num_result, num2word):\n\tresult = num_result[:, :, :]\n\tin_len = np.zeros((1), dtype = np.int32)\n\tin_len[0] = result.shape[1]\n\tr = K.ctc_decode(result, in_len, greedy = True, beam_width=10, top_paths=1)\n\tr1 = K.get_value(r[0][0])\n\tr1 = r1[0]\n\ttext = []\n\tfor i in r1:\n\t\ttext.append(num2word[i])\n\treturn r1, text\n```\n\n## \u51c6\u5907\u6d4b\u8bd5\u6570\u636e\n\n\n```python\n# 0. \u51c6\u5907\u89e3\u7801\u6240\u9700\u5b57\u5178\uff0c\u9700\u548c\u8bad\u7ec3\u4e00\u81f4\uff0c\u4e5f\u53ef\u4ee5\u5c06\u5b57\u5178\u4fdd\u5b58\u5230\u672c\u5730\uff0c\u76f4\u63a5\u8fdb\u884c\u8bfb\u53d6\nfrom utils import get_data, data_hparams\ndata_args = data_hparams()\ndata_args.data_length = 10 # \u91cd\u65b0\u8bad\u7ec3\u9700\u8981\u6ce8\u91ca\u8be5\u884c\ntrain_data = get_data(data_args)\n\n\n# 3. \u51c6\u5907\u6d4b\u8bd5\u6240\u9700\u6570\u636e\uff0c \u4e0d\u5fc5\u548c\u8bad\u7ec3\u6570\u636e\u4e00\u81f4\uff0c\u901a\u8fc7\u8bbe\u7f6edata_args.data_type\u6d4b\u8bd5\uff0c\n#    \u6b64\u5904\u5e94\u8bbe\u4e3a'test'\uff0c\u6211\u7528\u4e86'train'\u56e0\u4e3a\u6f14\u793a\u6a21\u578b\u8f83\u5c0f\uff0c\u5982\u679c\u4f7f\u7528'test'\u770b\u4e0d\u51fa\u6548\u679c\uff0c\n#    \u4e14\u4f1a\u51fa\u73b0\u672a\u51fa\u73b0\u7684\u8bcd\u3002\ndata_args.data_type = 'train'\ntest_data = get_data(data_args)\nam_batch = test_data.get_am_batch()\nlm_batch = test_data.get_lm_batch()\n```\n\n    get source list...\n    load  thchs_train.txt  data...\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00<00:00, 226097.06it/s]\n    load  aishell_train.txt  data...\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 120098/120098 [00:00<00:00, 226827.96it/s]\n    make am vocab...\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 9950.90it/s]\n    make lm pinyin vocab...\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<?, ?it/s]\n    make lm hanzi vocab...\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 9953.26it/s]\n\n\n## \u52a0\u8f7d\u58f0\u5b66\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\n\n\n```python\n# 1.\u58f0\u5b66\u6a21\u578b-----------------------------------\nfrom model_speech.cnn_ctc import Am, am_hparams\n\nam_args = am_hparams()\nam_args.vocab_size = len(train_data.am_vocab)\nam = Am(am_args)\nprint('loading acoustic model...')\nam.ctc_model.load_weights('logs_am/model.h5')\n\n# 2.\u8bed\u8a00\u6a21\u578b-------------------------------------------\nfrom model_language.transformer import Lm, lm_hparams\n\nlm_args = lm_hparams()\nlm_args.input_vocab_size = len(train_data.pny_vocab)\nlm_args.label_vocab_size = len(train_data.han_vocab)\nprint('loading language model...')\nlm = Lm(lm_args)\nsess = tf.Session(graph=lm.graph)\nwith lm.graph.as_default():\n    saver =tf.train.Saver()\nwith sess.as_default():\n    saver.restore(sess, 'logs_lm/model')\n```\n\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    the_inputs (InputLayer)      (None, None, 200, 1)      0         \n    _________________________________________________________________\n    conv2d_11 (Conv2D)           (None, None, 200, 32)     320       \n    _________________________________________________________________\n    batch_normalization_11 (Batc (None, None, 200, 32)     128       \n    _________________________________________________________________\n    conv2d_12 (Conv2D)           (None, None, 200, 32)     9248      \n    _________________________________________________________________\n    batch_normalization_12 (Batc (None, None, 200, 32)     128       \n    _________________________________________________________________\n    max_pooling2d_4 (MaxPooling2 (None, None, 100, 32)     0         \n    _________________________________________________________________\n    conv2d_13 (Conv2D)           (None, None, 100, 64)     18496     \n    _________________________________________________________________\n    batch_normalization_13 (Batc (None, None, 100, 64)     256       \n    _________________________________________________________________\n    conv2d_14 (Conv2D)           (None, None, 100, 64)     36928     \n    _________________________________________________________________\n    batch_normalization_14 (Batc (None, None, 100, 64)     256       \n    _________________________________________________________________\n    max_pooling2d_5 (MaxPooling2 (None, None, 50, 64)      0         \n    _________________________________________________________________\n    conv2d_15 (Conv2D)           (None, None, 50, 128)     73856     \n    _________________________________________________________________\n    batch_normalization_15 (Batc (None, None, 50, 128)     512       \n    _________________________________________________________________\n    conv2d_16 (Conv2D)           (None, None, 50, 128)     147584    \n    _________________________________________________________________\n    batch_normalization_16 (Batc (None, None, 50, 128)     512       \n    _________________________________________________________________\n    max_pooling2d_6 (MaxPooling2 (None, None, 25, 128)     0         \n    _________________________________________________________________\n    conv2d_17 (Conv2D)           (None, None, 25, 128)     147584    \n    _________________________________________________________________\n    batch_normalization_17 (Batc (None, None, 25, 128)     512       \n    _________________________________________________________________\n    conv2d_18 (Conv2D)           (None, None, 25, 128)     147584    \n    _________________________________________________________________\n    batch_normalization_18 (Batc (None, None, 25, 128)     512       \n    _________________________________________________________________\n    conv2d_19 (Conv2D)           (None, None, 25, 128)     147584    \n    _________________________________________________________________\n    batch_normalization_19 (Batc (None, None, 25, 128)     512       \n    _________________________________________________________________\n    conv2d_20 (Conv2D)           (None, None, 25, 128)     147584    \n    _________________________________________________________________\n    batch_normalization_20 (Batc (None, None, 25, 128)     512       \n    _________________________________________________________________\n    reshape_2 (Reshape)          (None, None, 3200)        0         \n    _________________________________________________________________\n    dense_3 (Dense)              (None, None, 256)         819456    \n    _________________________________________________________________\n    dense_4 (Dense)              (None, None, 230)         59110     \n    =================================================================\n    Total params: 1,759,174\n    Trainable params: 1,757,254\n    Non-trainable params: 1,920\n    _________________________________________________________________\n    loading acoustic model...\n    loading language model...\n    INFO:tensorflow:Restoring parameters from logs_lm/model\n\n\n## \u4f7f\u7528\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\n\n\n```python\n\nfor i in range(5):\n    print('\\n the ', i, 'th example.')\n    # \u8f7d\u5165\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u8bc6\u522b\n    inputs, outputs = next(am_batch)\n    x = inputs['the_inputs']\n    y = inputs['the_labels'][0]\n    result = am.model.predict(x, steps=1)\n    # \u5c06\u6570\u5b57\u7ed3\u679c\u8f6c\u5316\u4e3a\u6587\u672c\u7ed3\u679c\n    _, text = decode_ctc(result, train_data.am_vocab)\n    text = ' '.join(text)\n    print('\u6587\u672c\u7ed3\u679c\uff1a', text)\n    print('\u539f\u6587\u7ed3\u679c\uff1a', ' '.join([train_data.am_vocab[int(i)] for i in y]))\n    with sess.as_default():\n        _, y = next(lm_batch)\n        text = text.strip('\\n').split(' ')\n        x = np.array([train_data.pny_vocab.index(pny) for pny in text])\n        x = x.reshape(1, -1)\n        preds = sess.run(lm.preds, {lm.x: x})\n        got = ''.join(train_data.han_vocab[idx] for idx in preds[0])\n        print('\u539f\u6587\u6c49\u5b57\uff1a', ''.join(train_data.han_vocab[idx] for idx in y[0]))\n        print('\u8bc6\u522b\u7ed3\u679c\uff1a', got)\nsess.close()\n```\n\n\n     the  0 th example.\n    \u6587\u672c\u7ed3\u679c\uff1a lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2\n    \u539f\u6587\u7ed3\u679c\uff1a lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2\n    \u539f\u6587\u6c49\u5b57\uff1a \u7eff\u662f\u9633\u6625\u70df\u666f\u5927\u5757\u6587\u7ae0\u7684\u5e95\u8272\u56db\u6708\u7684\u6797\u5ce6\u66f4\u662f\u7eff\u5f97\u9c9c\u6d3b\u79c0\u5a9a\u8bd7\u610f\u76ce\u7136\n    \u8bc6\u522b\u7ed3\u679c\uff1a \u7eff\u662f\u9633\u6625\u70df\u666f\u5927\u5757\u6587\u7ae0\u7684\u5e95\u8272\u56db\u6708\u7684\u6797\u5ce6\u66f4\u662f\u7eff\u5f97\u9c9c\u6d3b\u79c0\u5a9a\u8bd7\u610f\u76ce\u7136\n\n     the  1 th example.\n    \u6587\u672c\u7ed3\u679c\uff1a ta1 jin3 ping2 yao1 bu4 de li4 liang4 zai4 yong3 dao4 shang4 xia4 fan1 teng2 yong3 dong4 she2 xing2 zhuang4 ru2 hai3 tun2 yi4 zhi2 yi3 yi1 tou2 de you1 shi4 ling3 xian1\n    \u539f\u6587\u7ed3\u679c\uff1a ta1 jin3 ping2 yao1 bu4 de li4 liang4 zai4 yong3 dao4 shang4 xia4 fan1 teng2 yong3 dong4 she2 xing2 zhuang4 ru2 hai3 tun2 yi4 zhi2 yi3 yi1 tou2 de you1 shi4 ling3 xian1\n    \u539f\u6587\u6c49\u5b57\uff1a \u4ed6\u4ec5\u51ed\u8170\u90e8\u7684\u529b\u91cf\u5728\u6cf3\u9053\u4e0a\u4e0b\u7ffb\u817e\u86f9\u52a8\u86c7\u884c\u72b6\u5982\u6d77\u8c5a\u4e00\u76f4\u4ee5\u4e00\u5934\u7684\u4f18\u52bf\u9886\u5148\n    \u8bc6\u522b\u7ed3\u679c\uff1a \u4ed6\u4ec5\u51ed\u8170\u90e8\u7684\u529b\u91cf\u5728\u6cf3\u9053\u4e0a\u4e0b\u7ffb\u817e\u86f9\u52a8\u86c7\u884c\u72b6\u5982\u6d77\u8c5a\u4e00\u76f4\u4ee5\u4e00\u5934\u7684\u4f18\u52bf\u9886\u5148\n\n     the  2 th example.\n    \u6587\u672c\u7ed3\u679c\uff1a pao4 yan3 da3 hao3 le zha4 yao4 zen3 me zhuang1 yue4 zheng4 cai2 yao3 le yao3 ya2 shu1 di4 tuo1 qu4 yi1 fu2 guang1 bang3 zi chong1 jin4 le shui3 cuan4 dong4\n    \u539f\u6587\u7ed3\u679c\uff1a pao4 yan3 da3 hao3 le zha4 yao4 zen3 me zhuang1 yue4 zheng4 cai2 yao3 le yao3 ya2 shu1 di4 tuo1 qu4 yi1 fu2 guang1 bang3 zi chong1 jin4 le shui3 cuan4 dong4\n    \u539f\u6587\u6c49\u5b57\uff1a \u70ae\u773c\u6253\u597d\u4e86\u70b8\u836f\u600e\u4e48\u88c5\u5cb3\u6b63\u624d\u54ac\u4e86\u54ac\u7259\u500f\u5730\u8131\u53bb\u8863\u670d\u5149\u8180\u5b50\u51b2\u8fdb\u4e86\u6c34\u7a9c\u6d1e\n    \u8bc6\u522b\u7ed3\u679c\uff1a \u70ae\u773c\u6253\u597d\u4e86\u70b8\u836f\u600e\u4e48\u88c5\u5cb3\u6b63\u624d\u54ac\u4e86\u54ac\u7259\u500f\u5730\u8131\u53bb\u8863\u670d\u5149\u8180\u5b50\u51b2\u8fdb\u4e86\u6c34\u7a9c\u6d1e\n\n     the  3 th example.\n    \u6587\u672c\u7ed3\u679c\uff1a ke3 shei2 zhi1 wen2 wan2 hou4 ta1 yi1 zhao4 jing4 zi zhi1 jian4 zuo3 xia4 yan3 jian3 de xian4 you4 cu1 you4 hei1 yu3 you4 ce4 ming2 xian3 bu2 dui4 cheng1\n    \u539f\u6587\u7ed3\u679c\uff1a ke3 shei2 zhi1 wen2 wan2 hou4 ta1 yi1 zhao4 jing4 zi zhi1 jian4 zuo3 xia4 yan3 jian3 de xian4 you4 cu1 you4 hei1 yu3 you4 ce4 ming2 xian3 bu2 dui4 cheng1\n    \u539f\u6587\u6c49\u5b57\uff1a \u53ef\u8c01\u77e5\u7eb9\u5b8c\u540e\u5979\u4e00\u7167\u955c\u5b50\u53ea\u89c1\u5de6\u4e0b\u773c\u7751\u7684\u7ebf\u53c8\u7c97\u53c8\u9ed1\u4e0e\u53f3\u4fa7\u660e\u663e\u4e0d\u5bf9\u79f0\n    \u8bc6\u522b\u7ed3\u679c\uff1a \u53ef\u8c01\u77e5\u7eb9\u5b8c\u540e\u5979\u4e00\u7167\u955c\u5b50\u77e5\u89c1\u5de6\u4e0b\u773c\u7751\u7684\u7ebf\u53f3\u7c97\u53f3\u9ed1\u4e0e\u53f3\u4fa7\u660e\u663e\u4e0d\u5bf9\u79f0\n\n     the  4 th example.\n    \u6587\u672c\u7ed3\u679c\uff1a yi1 jin4 men2 wo3 bei4 jing1 dai1 le zhe4 hu4 ming2 jiao4 pang2 ji2 de lao3 nong2 shi4 kang4 mei3 yuan2 chao2 fu4 shang1 hui2 xiang1 de lao3 bing1 qi1 zi3 chang2 nian2 you3 bing4 jia1 tu2 si4 bi4 yi1 pin2 ru2 xi3\n    \u539f\u6587\u7ed3\u679c\uff1a yi1 jin4 men2 wo3 bei4 jing1 dai1 le zhe4 hu4 ming2 jiao4 pang2 ji2 de lao3 nong2 shi4 kang4 mei3 yuan2 chao2 fu4 shang1 hui2 xiang1 de lao3 bing1 qi1 zi3 chang2 nian2 you3 bing4 jia1 tu2 si4 bi4 yi1 pin2 ru2 xi3\n    \u539f\u6587\u6c49\u5b57\uff1a \u4e00\u8fdb\u95e8\u6211\u88ab\u60ca\u5446\u4e86\u8fd9\u6237\u540d\u53eb\u5e9e\u5409\u7684\u8001\u519c\u662f\u6297\u7f8e\u63f4\u671d\u8d1f\u4f24\u56de\u4e61\u7684\u8001\u5175\u59bb\u5b50\u957f\u5e74\u6709\u75c5\u5bb6\u5f92\u56db\u58c1\u4e00\u8d2b\u5982\u6d17\n    \u8bc6\u522b\u7ed3\u679c\uff1a \u4e00\u8fdb\u95e8\u6211\u88ab\u60ca\u5446\u4e86\u8fd9\u6237\u540d\u53eb\u5e9e\u5409\u7684\u8001\u519c\u662f\u6297\u7f8e\u63f4\u671d\u8d1f\u4f24\u56de\u4e61\u7684\u8001\u5175\u59bb\u5b50\u957f\u5e74\u6709\u75c5\u5bb6\u5f92\u56db\u58c1\u4e00\u8d2b\u5982\u6d17\n\n\n\n# \u76f8\u5173\u5185\u5bb9\n\n[\u6211\u7684github: https://github.com/audier](https://github.com/audier)\n\n[\u6211\u7684github\u535a\u5ba2: audier.github.io](https://audier.github.io)\n\n[\u6211\u7684csdn\u535a\u5ba2: https://blog.csdn.net/chinatelecom08](https://blog.csdn.net/chinatelecom08)\n",
            "readme_url": "https://github.com/yumoh/speech-keras",
            "frameworks": [
                "Keras",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.7889766714541788,
        "task": "Question Answering",
        "task_prob": 0.5267712395700529
    }
}