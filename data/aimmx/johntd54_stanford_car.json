{
    "visibility": {
        "visibility": "public"
    },
    "name": "stanford_car",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "johntd54",
                "owner_type": "User",
                "name": "stanford_car",
                "url": "https://github.com/johntd54/stanford_car",
                "stars": 5,
                "pushed_at": "2019-06-18 07:27:40+00:00",
                "created_at": "2019-06-16 05:41:06+00:00",
                "language": "Jupyter Notebook",
                "description": "Classification model for fine-grained visual classification on the Stanford Car dataset.",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "272e877da54f0583d3e4bc0de182fcd76eb7712c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/blob/master/.gitignore"
                    }
                },
                "size": 3408
            },
            {
                "type": "code",
                "name": "LimeVisualization.ipynb",
                "sha": "3e016f7ded6903949959244e2fe91f3cba7a1f61",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/blob/master/LimeVisualization.ipynb"
                    }
                },
                "size": 196732
            },
            {
                "type": "code",
                "name": "data",
                "sha": "0b7d8f996a10ed1c4320bb6cd85d9691e3b23f34",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/tree/master/data"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "dataset.py",
                "sha": "6a3169ff537d8050c9b69b79d338c723527b5536",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/blob/master/dataset.py"
                    }
                },
                "size": 5633
            },
            {
                "type": "code",
                "name": "evaluate.py",
                "sha": "7709229cdb0dc911a168a30c1fe870bcc63a5df4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/blob/master/evaluate.py"
                    }
                },
                "size": 2508
            },
            {
                "type": "code",
                "name": "helpers.py",
                "sha": "265472ba673c304eef820edcd463af544fa58d19",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/blob/master/helpers.py"
                    }
                },
                "size": 955
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "05ad2572e383b779905f58b6a8058441ba915fdd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/blob/master/model.py"
                    }
                },
                "size": 6494
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "0a4ef64d08a3251317421e02b6ae7bb876f45301",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/blob/master/requirements.txt"
                    }
                },
                "size": 92
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "654717c68d324c04fe48e541c4744f8868487d5b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/blob/master/test.py"
                    }
                },
                "size": 1356
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "ea22d17e2fc54b18a05b78d05c31349946378eed",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/blob/master/train.py"
                    }
                },
                "size": 7719
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "5a3b6cae321f9cc53b882c2afc4180028e8934df",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johntd54/stanford_car/blob/master/utils.py"
                    }
                },
                "size": 9092
            }
        ]
    },
    "authors": [
        {
            "name": "Nguyen Trung Duc (john)",
            "email": "john@cinnamon.is",
            "github_id": "johntd54"
        }
    ],
    "tags": [],
    "description": "Classification model for fine-grained visual classification on the Stanford Car dataset.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/johntd54/stanford_car",
            "stars": 5,
            "issues": true,
            "readme": "<div align=\"center\">\n  <img src=\"https://ai.stanford.edu/~jkrause/cars/class_montage.jpg\">\n</div>\n\n# Stanford Training\n\nThis is submission for the Computer Vision challenge.\n\n## Evaluation step-by-step guide\n\nThe evaluation script can take in image or folder of images (recursively). Please look at the folder *`./data/samples`* in this repo to see an example input image folder. After evaluation, the script will generate a csv file, with the leftmost column is the image filename, and all other columns contain the confidence score for corresponding class.\n\nThe pretrained model can be downloaded either from [Dropbox](https://www.dropbox.com/s/pskw0fsjhnrlmyj/010.ckpt?dl=1) (wget-friendly) or  [Gdrive](https://drive.google.com/open?id=1znFK4xtXkNrcRBVF9NCUHHjKMMawgKf0).\n\n```bash\n# clone this repo\ngit clone https://github.com/johntd54/stanford_car\ncd stanford_car\n\n# create a new python 3.7 environment with conda\nconda create -n stanford_car_eval python=3.7\nconda activate stanford_car_eval\n\nmkdir ckpt\nwget https://www.dropbox.com/s/pskw0fsjhnrlmyj/010.ckpt?dl=1 -O ckpt/010.ckpt\n# download the pre-trained model into folder `ckpt`\n# or GDrive: https://drive.google.com/open?id=1znFK4xtXkNrcRBVF9NCUHHjKMMawgKf0\n\n# install supporting libraries\npip install -r requirements.txt\nconda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n# or `pip install torch torchvision` if gpu is not available\n\n# take note of the evaluation folder and run the evaluation script\n# the result will be stored at ./data/result.csv and ./data/confidence.csv\n# remove the `--gpu` parameter on non-gpu machine\npython evaluate.py [image_folder] --ckpt [checkpoint_path] --gpu\n```\n\nThe above step-by-step operation will output 2 files:\n- *`./data/confidence.csv`*: this file contains 197 columns, the first column is filename, the other 196 columns are confidence score for corresponding class. The class label can be obtained at *`./data/meta.json`*.\n- *`./data/result.csv`*: this file contains 3 columns, the first column is filename, the second column is the class index, and the third column is class label (looked up from `./data/meta.json`.\n\n## Model detail\n\nThe model employs some of the common deep learning techniques in computer vision:\n\n- DenseNet: https://arxiv.org/abs/1608.06993\n- Weakly-Supervised Data Augmentation Network: https://arxiv.org/abs/1901.09891\n\nAdditionally, some techniques are also employed to improve training and prediction:\n\n- Super-Convergence: https://arxiv.org/abs/1708.07120. Inspired by cyclical learning rate schedule, this technique cuts training time by nearly a half, while increaseing final accuracy.\n- Temperature scaling: https://arxiv.org/abs/1706.04599. Deep learning models are usually over-confident. Temperature scaling calibrates the confidence score of model prediction, so that it is less likely for the incorrect prediction to have high confidence score. In doing so, most incorrect predictions will have low confidence score, which allows human double checking if necessary.\n- Add more output logits than the number of classes in dataset. This model has 500 units in the output layer, even though the Stanford dataset only has 196 classes. Through experiments on this dataset, I consistently observed higher accuracy for the larger model. Moreover, by adding more units into the output layer, it would be easier to fine-tune model whenever we want to incorporate new car classes (which should happen very regularly in the real world).\n- LIME model visualization: highlight parts of the input image that most influence model prediction. This code is in the notebook *`./LimeVisualization.ipynb`*. You will need to have `pillow` and `tensorwatch` installed (`pip install pillow tensorwatch`).\n\nThrough combination of the above techniques, we attain a classification model that has higher accuracy performance, is faster to train, and easier to finetune.\n\n",
            "readme_url": "https://github.com/johntd54/stanford_car",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates",
            "arxiv": "1708.07120",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.07120v3",
            "abstract": "In this paper, we describe a phenomenon, which we named \"super-convergence\",\nwhere neural networks can be trained an order of magnitude faster than with\nstandard training methods. The existence of super-convergence is relevant to\nunderstanding why deep networks generalize well. One of the key elements of\nsuper-convergence is training with one learning rate cycle and a large maximum\nlearning rate. A primary insight that allows super-convergence training is that\nlarge learning rates regularize the training, hence requiring a reduction of\nall other forms of regularization in order to preserve an optimal\nregularization balance. We also derive a simplification of the Hessian Free\noptimization method to compute an estimate of the optimal learning rate.\nExperiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet\ndatasets, and resnet, wide-resnet, densenet, and inception architectures. In\naddition, we show that super-convergence provides a greater boost in\nperformance relative to standard training when the amount of labeled training\ndata is limited. The architectures and code to replicate the figures in this\npaper are available at github.com/lnsmith54/super-convergence. See\nhttp://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of\nsuper-convergence to win the DAWNBench challenge (see\nhttps://dawn.cs.stanford.edu/benchmark/).",
            "authors": [
                "Leslie N. Smith",
                "Nicholay Topin"
            ]
        },
        {
            "title": "See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification",
            "arxiv": "1901.09891",
            "year": 2019,
            "url": "http://arxiv.org/abs/1901.09891v2",
            "abstract": "Data augmentation is usually adopted to increase the amount of training data,\nprevent overfitting and improve the performance of deep models. However, in\npractice, random data augmentation, such as random image cropping, is\nlow-efficiency and might introduce many uncontrolled background noises. In this\npaper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to\nexplore the potential of data augmentation. Specifically, for each training\nimage, we first generate attention maps to represent the object's\ndiscriminative parts by weakly supervised learning. Next, we augment the image\nguided by these attention maps, including attention cropping and attention\ndropping. The proposed WS-DAN improves the classification accuracy in two\nfolds. In the first stage, images can be seen better since more discriminative\nparts' features will be extracted. In the second stage, attention regions\nprovide accurate location of object, which ensures our model to look at the\nobject closer and further improve the performance. Comprehensive experiments in\ncommon fine-grained visual classification datasets show that our WS-DAN\nsurpasses the state-of-the-art methods, which demonstrates its effectiveness.",
            "authors": [
                "Tao Hu",
                "Honggang Qi",
                "Qingming Huang",
                "Yan Lu"
            ]
        },
        {
            "title": "Densely Connected Convolutional Networks",
            "arxiv": "1608.06993",
            "year": 2016,
            "url": "http://arxiv.org/abs/1608.06993v5",
            "abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        },
        {
            "title": "On Calibration of Modern Neural Networks",
            "arxiv": "1706.04599",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.04599v2",
            "abstract": "Confidence calibration -- the problem of predicting probability estimates\nrepresentative of the true correctness likelihood -- is important for\nclassification models in many applications. We discover that modern neural\nnetworks, unlike those from a decade ago, are poorly calibrated. Through\nextensive experiments, we observe that depth, width, weight decay, and Batch\nNormalization are important factors influencing calibration. We evaluate the\nperformance of various post-processing calibration methods on state-of-the-art\narchitectures with image and document classification datasets. Our analysis and\nexperiments not only offer insights into neural network learning, but also\nprovide a simple and straightforward recipe for practical settings: on most\ndatasets, temperature scaling -- a single-parameter variant of Platt Scaling --\nis surprisingly effective at calibrating predictions.",
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9996678255197696,
        "task": "Image Classification",
        "task_prob": 0.9467585056017395
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "MNIST"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    }
}