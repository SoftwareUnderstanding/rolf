{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "JejuNet",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "tantara",
                "owner_type": "User",
                "name": "JejuNet",
                "url": "https://github.com/tantara/JejuNet",
                "stars": 274,
                "pushed_at": "2020-09-13 08:43:13+00:00",
                "created_at": "2018-06-11 12:43:34+00:00",
                "language": "Java",
                "description": "Real-Time Video Segmentation on Mobile Devices with DeepLab V3+, MobileNet V2. Worked on the project in \ud83c\udfdd Jeju island",
                "license": "MIT License",
                "frameworks": []
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "c4f204fa1faeedbc7ddf39a451d328c6d0da9600",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tantara/JejuNet/blob/master/LICENSE"
                    }
                },
                "size": 1068
            },
            {
                "type": "code",
                "name": "android",
                "sha": "c7eae7ff77fbafb68ae02fff9ac9baeea9f37686",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tantara/JejuNet/tree/master/android"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "9caa20e789fc7592457bc81ef6582fefa8cd8bf3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tantara/JejuNet/tree/master/docs"
                    }
                },
                "num_files": 6
            }
        ]
    },
    "authors": [
        {
            "name": "Taekmin Kim",
            "email": "tantara.tm@gmail.com",
            "github_id": "tantara"
        },
        {
            "name": "Gbenga Oladipupo",
            "email": "michaelgbenga91@gmail.com",
            "github_id": "devmike01"
        }
    ],
    "tags": [
        "tf-lite-on-android",
        "deeplearning",
        "tensorflow",
        "segmentation"
    ],
    "description": "Real-Time Video Segmentation on Mobile Devices with DeepLab V3+, MobileNet V2. Worked on the project in \ud83c\udfdd Jeju island",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/tantara/JejuNet",
            "stars": 274,
            "issues": true,
            "readme": "# JejuNet\n\nReal-Time Video Segmentation on Mobile Devices\n\n##### Keywords\n\nVideo Segmentation, Mobile, Tensorflow Lite\n\n##### Tutorials\n\n* Benchmarks: Tensorflow Lite on GPU\n  * A Post on Medium [Link](https://medium.com/@tantara/benchmarks-tensorflow-lite-on-gpu-769bff8afa6d)\n  * Detail results [Link](https://www.dropbox.com/sh/6mtyfwhfasvfaun/AADG52s-5Q4aCjC8BmL1cA4xa?dl=0)\n\n## Introduction\n\nRunning vision tasks such as object detection, segmentation in real time on mobile devices. Our goal is to implement video segmentation in real time at least 24 fps on Google Pixel 2. We use efficient deep learning network specialized in mobile/embedded devices and exploit data redundancy between consecutive frames to reduce unaffordable computational cost. Moreover, the network can be optimized with 8-bits quantization provided by tf-lite.\n\n![Real-Time Video Segmentation(Credit: Google AI)](https://raw.githubusercontent.com/tantara/JejuNet/master/docs/real_time_video_segmentation_google_ai.gif)\n\n*Example: Reai-Time Video Segmentation(Credit: Google AI)*\n\n## Architecture\n\n#### Video Segmentation\n\n- Compressed [DeepLabv3+](https://github.com/tensorflow/models/tree/master/research/deeplab)[1]\n  - Backbone: [MobileNetv2](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)[2]\n\n#### Optimization\n\n* 8-bits Quantization on [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/)\n\n## Experiments\n\n* Video Segmentation on Google Pixel 2\n* Datasets\n  * PASCAL VOC 2012\n\n## Plan @Deep Learning Camp Jeju 2018\n\n### July, 2018\n\n- [x] DeepLabv3+ on tf-lite\n- [x] Use data redundancy between frames\n- Optimization\n  - [x] Quantization\n  - [x] Reduce the number of layers, filters and input size\n\n## Results\n\nMore results here [bit.ly/jejunet-output](https://bit.ly/jejunet-output)\n\n#### Demo\n\n![DeepLabv3+ on tf-lite](https://raw.githubusercontent.com/tantara/JejuNet/master/docs/20180726-current-results-deeplabv3_on_tf-lite.gif)\n\n*Video Segmentation on Google Pixel 2*\n\n#### Trade-off Between Speed(FPS) and Accuracy(mIoU) \n\n![Trade-off Between Speed(FPS) and Accuracy(mIoU)](https://raw.githubusercontent.com/tantara/JejuNet/master/docs/trade-off-between-speed-fps-and-accuracy-miou.png)\n\n#### Low Bits Quantization\n\n| Network                | Input   | Stride     | Quantization(w/a) | PASCAL mIoU | Runtime(.tflite) | File Size(.tflite) |\n| ---------------------- | ------- | ---- | ----------------- | ----------- | ---------------- | ------------------ |\n| DeepLabv3, MobileNetv2 | 512x512 | 16     | 32/32             | 79.9%       | 862ms            | 8.5MB              |\n| DeepLabv3, MobileNetv2 | 512x512 | 16     | 8/8               | 79.2%       | 451ms            | 2.2MB              |\n| DeepLabv3, MobileNetv2 | 512x512 | 16     | 6/6               | 70.7%       | -                | -                  |\n| DeepLabv3, MobileNetv2 | 512x512 | 16     | 6/4               | 30.3%       | -                | -                  |\n\n![Low Bits Quantization](https://raw.githubusercontent.com/tantara/JejuNet/master/docs/low-bits-quantization.png)\n\n## References\n\n1. **Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation**<br>\n\n   Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam. arXiv: 1802.02611.<br>\n\n   [[link]](https://arxiv.org/abs/1802.02611). arXiv: 1802.02611, 2018.\n\n2. **Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation**<br />Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen<br />[[link]](https://arxiv.org/abs/1801.04381). arXiv:1801.04381, 2018.\n\n## Authors\n\n- [Taekmin Kim](https://www.linkedin.com/in/taekminkim/)(Mentee) [@tantara](https://www.linkedin.com/in/taekminkim/)\n- Jisung Kim(Mentor) [@runhani](https://github.com/runhani)\n\n## Acknowledgement\n\nThis work was partially supported by Deep Learning Jeju Camp and sponsors such as Google, SK Telecom. Thank you for the generous support for TPU and Google Pixel 2, and thank [Hyungsuk](https://github.com/corea) and all the mentees for tensorflow impelmentations and useful discussions.\n\n## License\n\n\u00a9 [Taekmin Kim](https://www.linkedin.com/in/taekminkim/), 2018. Licensed under the [MIT](LICENSE) License.\n\n",
            "readme_url": "https://github.com/tantara/JejuNet",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
            "arxiv": "1802.02611",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.02611v3",
            "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep\nneural networks for semantic segmentation task. The former networks are able to\nencode multi-scale contextual information by probing the incoming features with\nfilters or pooling operations at multiple rates and multiple effective\nfields-of-view, while the latter networks can capture sharper object boundaries\nby gradually recovering the spatial information. In this work, we propose to\ncombine the advantages from both methods. Specifically, our proposed model,\nDeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module\nto refine the segmentation results especially along object boundaries. We\nfurther explore the Xception model and apply the depthwise separable\nconvolution to both Atrous Spatial Pyramid Pooling and decoder modules,\nresulting in a faster and stronger encoder-decoder network. We demonstrate the\neffectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets,\nachieving the test set performance of 89.0\\% and 82.1\\% without any\npost-processing. Our paper is accompanied with a publicly available reference\nimplementation of the proposed models in Tensorflow at\n\\url{https://github.com/tensorflow/models/tree/master/research/deeplab}.",
            "authors": [
                "Liang-Chieh Chen",
                "Yukun Zhu",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ]
        },
        {
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
            "arxiv": "1801.04381",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.04381v4",
            "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that\nimproves the state of the art performance of mobile models on multiple tasks\nand benchmarks as well as across a spectrum of different model sizes. We also\ndescribe efficient ways of applying these mobile models to object detection in\na novel framework we call SSDLite. Additionally, we demonstrate how to build\nmobile semantic segmentation models through a reduced form of DeepLabv3 which\nwe call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where\nthe input and output of the residual block are thin bottleneck layers opposite\nto traditional residual models which use expanded representations in the input\nan MobileNetV2 uses lightweight depthwise convolutions to filter features in\nthe intermediate expansion layer. Additionally, we find that it is important to\nremove non-linearities in the narrow layers in order to maintain\nrepresentational power. We demonstrate that this improves performance and\nprovide an intuition that led to this design. Finally, our approach allows\ndecoupling of the input/output domains from the expressiveness of the\ntransformation, which provides a convenient framework for further analysis. We\nmeasure our performance on Imagenet classification, COCO object detection, VOC\nimage segmentation. We evaluate the trade-offs between accuracy, and number of\noperations measured by multiply-adds (MAdd), as well as the number of\nparameters",
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "PASCAL VOC 2012"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999618638850752,
        "task": "Semantic Segmentation",
        "task_prob": 0.9737552659513354
    }
}