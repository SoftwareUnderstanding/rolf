{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "yoshitomo-matsubara",
                "owner_type": "User",
                "name": "torchdistill",
                "url": "https://github.com/yoshitomo-matsubara/torchdistill",
                "stars": 578,
                "pushed_at": "2022-03-05 22:21:04+00:00",
                "created_at": "2019-12-18 17:40:32+00:00",
                "language": "Python",
                "description": "A coding-free framework built on PyTorch for reproducible deep learning studies. \ud83c\udfc620 knowledge distillation methods presented at CVPR, ICLR, ECCV, NeurIPS, ICCV, etc are implemented so far. \ud83c\udf81 Trained models, training logs and configurations are available for ensuring the reproducibiliy and benchmark.",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "99f047ffd31130e82a4efd43bc45356d540a6a13",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/tree/main/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "5e1533a567e69f44eb68226df8683d5d67ba7e3c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/blob/main/.gitignore"
                    }
                },
                "size": 54
            },
            {
                "type": "code",
                "name": ".travis.yml",
                "sha": "30118bbfdd0fb679b7ecfb29cf2fd3e6dcecb342",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/blob/main/.travis.yml"
                    }
                },
                "size": 192
            },
            {
                "type": "code",
                "name": "CITATION.bib",
                "sha": "735c3ee7c74a02c322e4c34e9eab2ac9d3f66510",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/blob/main/CITATION.bib"
                    }
                },
                "size": 366
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "d9a758fd87e58e9b613b47967f64b196b2e4c088",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/blob/main/LICENSE"
                    }
                },
                "size": 1076
            },
            {
                "type": "code",
                "name": "MANIFEST.in",
                "sha": "20f348710ff08b5fe6c656d0ece02f59eb84b7bd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/blob/main/MANIFEST.in"
                    }
                },
                "size": 61
            },
            {
                "type": "code",
                "name": "configs",
                "sha": "a1c441a16fe0e0d900ea98617d168abbecbea51e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "demo",
                "sha": "4b7a8a8b795f7f2dba1c583770a1a20429b7480f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/tree/main/demo"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "examples",
                "sha": "2fb18b8e314dac913be6b19eaf27b3348336d343",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "setup.cfg",
                "sha": "754c2293aabf4d22e816b645ed341b4bf6e59f98",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/blob/main/setup.cfg"
                    }
                },
                "size": 126
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "38b65db85f77f591a653db6dc1d5a6c556da8044",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/blob/main/setup.py"
                    }
                },
                "size": 973
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "76fed4d56ea2a86f9275b9598731385b6f29de01",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/tree/main/tests"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "torchdistill",
                "sha": "cd150c119a923052c0bab52d457ad7f30a41157b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yoshitomo-matsubara/torchdistill/tree/main/torchdistill"
                    }
                },
                "num_files": 9
            }
        ]
    },
    "authors": [
        {
            "name": "Yoshitomo Matsubara",
            "github_id": "yoshitomo-matsubara"
        },
        {
            "name": "Jingyu Lee",
            "email": "dostos10@gmail.com",
            "github_id": "dostos"
        }
    ],
    "tags": [
        "knowledge-distillation",
        "pytorch",
        "image-classification",
        "imagenet",
        "object-detection",
        "coco",
        "semantic-segmentation",
        "cifar10",
        "cifar100",
        "colab-notebook",
        "google-colab",
        "pascal-voc",
        "nlp",
        "natural-language-processing",
        "transformer",
        "glue"
    ],
    "description": "A coding-free framework built on PyTorch for reproducible deep learning studies. \ud83c\udfc620 knowledge distillation methods presented at CVPR, ICLR, ECCV, NeurIPS, ICCV, etc are implemented so far. \ud83c\udf81 Trained models, training logs and configurations are available for ensuring the reproducibiliy and benchmark.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/yoshitomo-matsubara/torchdistill",
            "stars": 578,
            "issues": true,
            "readme": "# torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation\n[![PyPI version](https://badge.fury.io/py/torchdistill.svg)](https://badge.fury.io/py/torchdistill)\n[![Build Status](https://travis-ci.com/yoshitomo-matsubara/torchdistill.svg?branch=master)](https://travis-ci.com/github/yoshitomo-matsubara/torchdistill)\n[![DOI:10.1007/978-3-030-76423-4_3](https://zenodo.org/badge/DOI/10.1007/978-3-030-76423-4_3.svg)](https://doi.org/10.1007/978-3-030-76423-4_3)\n\n\n***torchdistill*** (formerly *kdkit*) offers various state-of-the-art knowledge distillation methods \nand enables you to design (new) experiments simply by editing a declarative yaml config file instead of Python code. \nEven when you need to extract intermediate representations in teacher/student models, \nyou will **NOT** need to reimplement the models, that often change the interface of the forward, but instead \nspecify the module path(s) in the yaml file. Refer to [this paper](https://github.com/yoshitomo-matsubara/torchdistill#citation) for more details.  \n\nIn addition to knowledge distillation, this framework helps you design and perform general deep learning experiments\n(**WITHOUT coding**) for reproducible deep learning studies. i.e., it enables you to train models without teachers \nsimply by excluding teacher entries from a declarative yaml config file. \nYou can find such examples below and in [configs/sample/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/).  \n\nWhen you refer to ***torchdistill*** in your paper, please cite [this paper](https://github.com/yoshitomo-matsubara/torchdistill#citation) \ninstead of this GitHub repository.  \n**Your citation is appreciated and motivates me to maintain and upgrade this framework!** \n\n## Forward hook manager\nUsing **ForwardHookManager**, you can extract intermediate representations in model without modifying the interface of its forward function.  \n[This example notebook](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/demo/extract_intermediate_representations.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/master/demo/extract_intermediate_representations.ipynb) \nwill give you a better idea of the usage such as knowledge distillation and analysis of intermediate representations.\n\n## 1 experiment \u2192 1 declarative PyYAML config file\nIn ***torchdistill***, many components and PyTorch modules are abstracted e.g., models, datasets, optimizers, losses, \nand more! You can define them in a declarative PyYAML config file so that can be seen as a summary of your experiment, \nand in many cases, you will **NOT need to write Python code at all**. \nTake a look at some configurations available in [configs/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/). You'll see what modules are abstracted and \nhow they are defined in a declarative PyYAML config file to design an experiment.\n\n## Top-1 validation accuracy for ILSVRC 2012 (ImageNet)\n| T: ResNet-34\\*  | Pretrained | KD    | AT    | FT         | CRD   | Tf-KD | SSKD  | L2    | PAD-L2 | KR    |  \n| :---            | ---:       | ---:  | ---:  | ---:       | ---:  | ---:  | ---:  | ---:  | ---:   | ---:  |  \n| S: ResNet-18    | 69.76\\*    | 71.37 | 70.90 | 71.56      | 70.93 | 70.52 | 70.09 | 71.08 | 71.71  | 71.64 |  \n| Original work   | N/A        | N/A   | 70.70 | 71.43\\*\\*  | 71.17 | 70.42 | 71.62 | 70.90 | 71.71  | 71.61 |  \n\n  \n\\* The pretrained ResNet-34 and ResNet-18 are provided by torchvision.  \n\\*\\* FT is assessed with ILSVRC 2015 in the original work.  \nFor the 2nd row (S: ResNet-18), most of the results are reported in [this paper](https://github.com/yoshitomo-matsubara/torchdistill#citation), \nand their checkpoints (trained weights), configuration and log files are [available](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/official/ilsvrc2012/yoshitomo-matsubara/), \nand the configurations reuse the hyperparameters such as number of epochs used in the original work except for KD.\n\n## Examples\nExecutable code can be found in [examples/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/) such as\n- [Image classification](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/image_classification.py): ImageNet (ILSVRC 2012), CIFAR-10, CIFAR-100, etc\n- [Object detection](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/object_detection.py): COCO 2017, etc\n- [Semantic segmentation](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/semantic_segmentation.py): COCO 2017, PASCAL VOC, etc\n- [Text classification](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/hf_transformers/text_classification.py): GLUE, etc\n\nFor CIFAR-10 and CIFAR-100, some models are reimplemented and available as pretrained models in ***torchdistill***. \nMore details can be found [here](https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.1).  \n\nSome Transformer models fine-tuned by ***torchdistill*** for GLUE tasks are available at [Hugging Face Model Hub](https://huggingface.co/yoshitomo-matsubara). \nSample GLUE benchmark results and details can be found [here](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/examples/hf_transformers#sample-benchmark-results-and-fine-tuned-models).\n\n## Google Colab Examples\nThe following examples are available in [demo/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/demo/). \nNote that the examples are for Google Colab users. Usually, [examples/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/) would be a better reference \nif you have your own GPU(s).\n\n### CIFAR-10 and CIFAR-100\n- Training without teacher models [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/master/demo/cifar_training.ipynb)\n- Knowledge distillation [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/master/demo/cifar_kd.ipynb)\n\n### GLUE\n- Fine-tuning without teacher models [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/master/demo/glue_finetuning_and_submission.ipynb)\n- Knowledge distillation [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/master/demo/glue_kd_and_submission.ipynb)\n\nThese examples write out test prediction files for you to see the test performance at [the GLUE leaderboard system](https://gluebenchmark.com/).\n\n## PyTorch Hub\nIf you find models on [PyTorch Hub](https://pytorch.org/hub/) or GitHub repositories supporting PyTorch Hub,\nyou can import them as teacher/student models simply by editing a declarative yaml config file.  \n\ne.g., If you use a pretrained ResNeSt-50 available in [rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models)\n(aka *timm*) as a teacher model for ImageNet dataset, you can import the model via PyTorch Hub with the following entry \nin your declarative yaml config file.\n\n```yaml\nmodels:\n  teacher_model:\n    name: 'resnest50d'\n    repo_or_dir: 'rwightman/pytorch-image-models'\n    params:\n      num_classes: 1000\n      pretrained: True\n```\n\n## How to setup\n- Python 3.6 >=\n- pipenv (optional)\n\n### Install by pip/pipenv\n```\npip3 install torchdistill\n# or use pipenv\npipenv install torchdistill\n```\n\n### Install from this repository \n```\ngit clone https://github.com/yoshitomo-matsubara/torchdistill.git\ncd torchdistill/\npip3 install -e .\n# or use pipenv\npipenv install \"-e .\"\n```\n\n## Issues / Questions / Requests\nThe documentation is work-in-progress. In the meantime, feel free to create an issue if you find a bug.  \nIf you have either a question or feature request, start a new discussion [here](https://github.com/yoshitomo-matsubara/torchdistill/discussions).\n\n## Citation\nIf you use ***torchdistill*** in your research, please cite the following paper.  \n[[Paper](https://link.springer.com/chapter/10.1007/978-3-030-76423-4_3)] [[Preprint](https://arxiv.org/abs/2011.12913)]  \n```bibtex\n@inproceedings{matsubara2021torchdistill,\n  title={torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation},\n  author={Matsubara, Yoshitomo},\n  booktitle={International Workshop on Reproducible Research in Pattern Recognition},\n  pages={24--44},\n  year={2021},\n  organization={Springer}\n}\n```\n\n## References\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/image_classification.py) [pytorch/vision/references/classification/](https://github.com/pytorch/vision/blob/master/references/classification/)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/object_detection.py) [pytorch/vision/references/detection/](https://github.com/pytorch/vision/tree/master/references/detection/)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/semantic_segmentation.py) [pytorch/vision/references/segmentation/](https://github.com/pytorch/vision/tree/master/references/segmentation/)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/hf_transformers/text_classification.py) [huggingface/transformers/examples/pytorch/text-classification](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-classification)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/kd) Geoffrey Hinton, Oriol Vinyals and Jeff Dean. [\"Distilling the Knowledge in a Neural Network\"](https://fb56552f-a-62cb3a1a-s-sites.googlegroups.com/site/deeplearningworkshopnips2014/65.pdf?attachauth=ANoY7co8sQACDsEYLkP11zqEAxPgYHLwkdkDP9NHfEB6pzQOUPmfWf3cVrL3WE7PNyed-lrRsF7CY6Tcme5OEQ92CTSN4f8nDfJcgt71fPtAvcTvH5BpzF-2xPvLkPAvU9Ub8XvbySAPOsMKMWmGsXG2FS1_X1LJsUfuwKdQKYVVTtRfG5LHovLHIwv6kXd3mOkDKEH7YdoyYQqjSv6ku2KDjOpVQBt0lKGVPXeRdwUcD0mxDqCe4u8%3D&attredirects=1) (Deep Learning and Representation Learning Workshop: NeurIPS 2014)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/fitnet) Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta and Yoshua Bengio. [\"FitNets: Hints for Thin Deep Nets\"](https://arxiv.org/abs/1412.6550) (ICLR 2015)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/fsp) Junho Yim, Donggyu Joo, Jihoon Bae and Junmo Kim. [\"A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning\"](http://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html) (CVPR 2017)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/at) Sergey Zagoruyko and Nikos Komodakis. [\"Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer\"](https://openreview.net/forum?id=Sks9_ajex) (ICLR 2017)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/pkt) Nikolaos Passalis and Anastasios Tefas. [\"Learning Deep Representations with Probabilistic Knowledge Transfer\"](http://openaccess.thecvf.com/content_ECCV_2018/html/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.html) (ECCV 2018)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/ft) Jangho Kim, Seonguk Park and Nojun Kwak. [\"Paraphrasing Complex Network: Network Compression via Factor Transfer\"](http://papers.neurips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer) (NeurIPS 2018)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/dab) Byeongho Heo, Minsik Lee, Sangdoo Yun and Jin Young Choi. [\"Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons\"](https://aaai.org/ojs/index.php/AAAI/article/view/4264) (AAAI 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/coco2017/multi_stage/ktaad) Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, Youliang Yan. [\"Knowledge Adaptation for Efficient Semantic Segmentation\"](https://openaccess.thecvf.com/content_CVPR_2019/html/He_Knowledge_Adaptation_for_Efficient_Semantic_Segmentation_CVPR_2019_paper.html) (CVPR 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/rkd) Wonpyo Park, Dongju Kim, Yan Lu and Minsu Cho. [\"Relational Knowledge Distillation\"](http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html) (CVPR 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/vid) Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence and Zhenwen Dai. [\"Variational Information Distillation for Knowledge Transfer\"](http://openaccess.thecvf.com/content_CVPR_2019/html/Ahn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.html) (CVPR 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/hnd) Yoshitomo Matsubara, Sabur Baidya, Davide Callegaro, Marco Levorato and Sameer Singh. [\"Distilled Split Deep Neural Networks for Edge-Assisted Real-Time Systems\"](https://dl.acm.org/doi/10.1145/3349614.3356022) (Workshop on Hot Topics in Video Analytics and Intelligent Edges: MobiCom 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/cckd) Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou and Zhaoning Zhang. [\"Correlation Congruence for Knowledge Distillation\"](http://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Correlation_Congruence_for_Knowledge_Distillation_ICCV_2019_paper.html) (ICCV 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/spkd) Frederick Tung and Greg Mori. [\"Similarity-Preserving Knowledge Distillation\"](http://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html) (ICCV 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/crd) Yonglong Tian, Dilip Krishnan and Phillip Isola. [\"Contrastive Representation Distillation\"](https://openreview.net/forum?id=SkgpBJrtvS) (ICLR 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/coco2017/single_stage/ghnd) Yoshitomo Matsubara and Marco Levorato. [\"Neural Compression and Filtering for Edge-assisted Real-time Object Detection in Challenged Networks\"](https://arxiv.org/abs/2007.15818) (ICPR 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/tfkd) Li Yuan, Francis E.H.Tay, Guilin Li, Tao Wang and Jiashi Feng. [\"Revisiting Knowledge Distillation via Label Smoothing Regularization\"](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf) (CVPR 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/sskd) Guodong Xu, Ziwei Liu, Xiaoxiao Li and Chen Change Loy. [\"Knowledge Distillation Meets Self-Supervision\"](http://www.ecva.net/papers/eccv_2020/papers_ECCV/html/898_ECCV_2020_paper.php) (ECCV 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/multi_stage/pad) Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang and Yichen Wei. [\"Prime-Aware Adaptive Distillation\"](http://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3317_ECCV_2020_paper.php) (ECCV 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/single_stage/kr) Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia. [\"Distilling Knowledge via Knowledge Review\"](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Distilling_Knowledge_via_Knowledge_Review_CVPR_2021_paper.html) (CVPR 2021)\n",
            "readme_url": "https://github.com/yoshitomo-matsubara/torchdistill",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
            "arxiv": "2011.12913",
            "year": 2020,
            "url": "http://arxiv.org/abs/2011.12913v2",
            "abstract": "While knowledge distillation (transfer) has been attracting attentions from\nthe research community, the recent development in the fields has heightened the\nneed for reproducible studies and highly generalized frameworks to lower\nbarriers to such high-quality, reproducible deep learning research. Several\nresearchers voluntarily published frameworks used in their knowledge\ndistillation studies to help other interested researchers reproduce their\noriginal work. Such frameworks, however, are usually neither well generalized\nnor maintained, thus researchers are still required to write a lot of code to\nrefactor/build on the frameworks for introducing new methods, models, datasets\nand designing experiments. In this paper, we present our developed open-source\nframework built on PyTorch and dedicated for knowledge distillation studies.\nThe framework is designed to enable users to design experiments by declarative\nPyYAML configuration files, and helps researchers complete the recently\nproposed ML Code Completeness Checklist. Using the developed framework, we\ndemonstrate its various efficient training strategies, and implement a variety\nof knowledge distillation methods. We also reproduce some of their original\nexperimental results on the ImageNet and COCO datasets presented at major\nmachine learning conferences such as ICLR, NeurIPS, CVPR and ECCV, including\nrecent state-of-the-art methods. All the source code, configurations, log files\nand trained model weights are publicly available at\nhttps://github.com/yoshitomo-matsubara/torchdistill .",
            "authors": [
                "Yoshitomo Matsubara"
            ]
        },
        {
            "title": "FitNets: Hints for Thin Deep Nets",
            "arxiv": "1412.6550",
            "year": 2014,
            "url": "http://arxiv.org/abs/1412.6550v4",
            "abstract": "While depth tends to improve network performances, it also makes\ngradient-based training more difficult since deeper networks tend to be more\nnon-linear. The recently proposed knowledge distillation approach is aimed at\nobtaining small and fast-to-execute models, and it has shown that a student\nnetwork could imitate the soft output of a larger teacher network or ensemble\nof networks. In this paper, we extend this idea to allow the training of a\nstudent that is deeper and thinner than the teacher, using not only the outputs\nbut also the intermediate representations learned by the teacher as hints to\nimprove the training process and final performance of the student. Because the\nstudent intermediate hidden layer will generally be smaller than the teacher's\nintermediate hidden layer, additional parameters are introduced to map the\nstudent hidden layer to the prediction of the teacher hidden layer. This allows\none to train deeper students that can generalize better or run faster, a\ntrade-off that is controlled by the chosen student capacity. For example, on\nCIFAR-10, a deep student network with almost 10.4 times less parameters\noutperforms a larger, state-of-the-art teacher network.",
            "authors": [
                "Adriana Romero",
                "Nicolas Ballas",
                "Samira Ebrahimi Kahou",
                "Antoine Chassang",
                "Carlo Gatta",
                "Yoshua Bengio"
            ]
        },
        {
            "title": "Neural Compression and Filtering for Edge-assisted Real-time Object Detection in Challenged Networks",
            "arxiv": "2007.15818",
            "year": 2020,
            "url": "http://arxiv.org/abs/2007.15818v2",
            "abstract": "The edge computing paradigm places compute-capable devices - edge servers -\nat the network edge to assist mobile devices in executing data analysis tasks.\nIntuitively, offloading compute-intense tasks to edge servers can reduce their\nexecution time. However, poor conditions of the wireless channel connecting the\nmobile devices to the edge servers may degrade the overall capture-to-output\ndelay achieved by edge offloading. Herein, we focus on edge computing\nsupporting remote object detection by means of Deep Neural Networks (DNNs), and\ndevelop a framework to reduce the amount of data transmitted over the wireless\nlink. The core idea we propose builds on recent approaches splitting DNNs into\nsections - namely head and tail models - executed by the mobile device and edge\nserver, respectively. The wireless link, then, is used to transport the output\nof the last layer of the head model to the edge server, instead of the DNN\ninput. Most prior work focuses on classification tasks and leaves the DNN\nstructure unaltered. Herein, our focus is on DNNs for three different object\ndetection tasks, which present a much more convoluted structure, and modify the\narchitecture of the network to: (i) achieve in-network compression by\nintroducing a bottleneck layer in the early layers on the head model, and (ii)\nprefilter pictures that do not contain objects of interest using a\nconvolutional neural network. Results show that the proposed technique\nrepresents an effective intermediate option between local and edge computing in\na parameter region where these extreme point solutions fail to provide\nsatisfactory performance. The code and trained models are available at\nhttps://github.com/yoshitomo-matsubara/hnd-ghnd-object-detectors .",
            "authors": [
                "Yoshitomo Matsubara",
                "Marco Levorato"
            ]
        },
        {
            "organization": "Springer",
            "year": "2021",
            "pages": "24--44",
            "booktitle": "International Workshop on Reproducible Research in Pattern Recognition",
            "author": [
                "Matsubara, Yoshitomo"
            ],
            "title": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
            "ENTRYTYPE": "inproceedings",
            "ID": "matsubara2021torchdistill",
            "authors": [
                "Matsubara, Yoshitomo"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "COCO 2017"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "GLUE"
            },
            {
                "name": "COCO"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "ImageNet"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.8302835931472158,
        "task": "Object Detection",
        "task_prob": 0.7972650452592966
    }
}