{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Faster R-CNN and Mask R-CNN in PyTorch 1.0",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "abcxs",
                "owner_type": "User",
                "name": "maskrcnn-contest",
                "url": "https://github.com/abcxs/maskrcnn-contest",
                "stars": 0,
                "pushed_at": "2019-06-23 16:23:07+00:00",
                "created_at": "2019-06-23 16:22:40+00:00",
                "language": "Python",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".flake8",
                "sha": "c286ad0c15dff1a3fcd779d0b0e220b6b7455185",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/.flake8"
                    }
                },
                "size": 247
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "a4ab743a95bb0cb8f4922161bbad715d074d57eb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/tree/master/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "7fe9a046b56a754222cbaef3a68c7606165c1e52",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/.gitignore"
                    }
                },
                "size": 382
            },
            {
                "type": "code",
                "name": "ABSTRACTIONS.md",
                "sha": "cdb3c428722418cdc2bb9af5018cefa454338e7d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/ABSTRACTIONS.md"
                    }
                },
                "size": 2654
            },
            {
                "type": "code",
                "name": "CODE_OF_CONDUCT.md",
                "sha": "0f7ad8bfc173eac554f0b6ef7c684861e8014bbe",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/CODE_OF_CONDUCT.md"
                    }
                },
                "size": 244
            },
            {
                "type": "code",
                "name": "CONTRIBUTING.md",
                "sha": "fc14cd3c73f6952be905f17e15a9a909a4561bb8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/CONTRIBUTING.md"
                    }
                },
                "size": 1641
            },
            {
                "type": "code",
                "name": "INSTALL.md",
                "sha": "b1bfaa293fb79947ef5b34521ec439952c782adb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/INSTALL.md"
                    }
                },
                "size": 4182
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "8585e11b83ab25bea5fbe4b8230fbf909f8b296b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/LICENSE"
                    }
                },
                "size": 1065
            },
            {
                "type": "code",
                "name": "MODEL_ZOO.md",
                "sha": "a0276d3d5d882311a5ac7633e7f3e88e71a40ce1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/MODEL_ZOO.md"
                    }
                },
                "size": 6877
            },
            {
                "type": "code",
                "name": "TROUBLESHOOTING.md",
                "sha": "63a8b598b96a643cbd11ed84bbc06dc747683d67",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/TROUBLESHOOTING.md"
                    }
                },
                "size": 2938
            },
            {
                "type": "code",
                "name": "configs",
                "sha": "fc6049a99f288e40cadd62101f1fe723627d0778",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/tree/master/configs"
                    }
                },
                "num_files": 28
            },
            {
                "type": "code",
                "name": "demo",
                "sha": "123ab8f8f7567ef9f3ff44ea11c364aae6b04004",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/tree/master/demo"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "docker",
                "sha": "b5802234471ff41cd87d292563fe86420cc10032",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/tree/master/docker"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "maskrcnn_benchmark",
                "sha": "87d455ca2e9adac93be018572b4331f201c61540",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/tree/master/maskrcnn_benchmark"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "mytools",
                "sha": "ecc9160b43ce94f41b464645f2f98bc50c36fb33",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/tree/master/mytools"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "a67b697bd543bc0648f92a63535180d18e870985",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/requirements.txt"
                    }
                },
                "size": 34
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "837c2cd15f4624f630540ef6993dcb9123adb39b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/setup.py"
                    }
                },
                "size": 2084
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "e36bd406b479d97b90086f5d2677d82029f074db",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/tree/master/tests"
                    }
                },
                "num_files": 15
            },
            {
                "type": "code",
                "name": "tools",
                "sha": "5cbb8338f3bfabdeca7bccccc4f580e04a32375a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/tree/master/tools"
                    }
                },
                "num_files": 3
            }
        ]
    },
    "authors": [
        {
            "name": "abcxs",
            "github_id": "abcxs"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/abcxs/maskrcnn-contest",
            "stars": 0,
            "issues": true,
            "readme": "# Faster R-CNN and Mask R-CNN in PyTorch 1.0\n\nThis project aims at providing the necessary building blocks for easily\ncreating detection and segmentation models using PyTorch 1.0.\n\n![alt text](demo/demo_e2e_mask_rcnn_X_101_32x8d_FPN_1x.png \"from http://cocodataset.org/#explore?id=345434\")\n\n## Highlights\n- **PyTorch 1.0:** RPN, Faster R-CNN and Mask R-CNN implementations that matches or exceeds Detectron accuracies\n- **Very fast**: up to **2x** faster than [Detectron](https://github.com/facebookresearch/Detectron) and **30%** faster than [mmdetection](https://github.com/open-mmlab/mmdetection) during training. See [MODEL_ZOO.md](MODEL_ZOO.md) for more details.\n- **Memory efficient:** uses roughly 500MB less GPU memory than mmdetection during training\n- **Multi-GPU training and inference**\n- **Mixed precision training:** trains faster with less GPU memory on [NVIDIA tensor cores](https://developer.nvidia.com/tensor-cores).\n- **Batched inference:** can perform inference using multiple images per batch per GPU\n- **CPU support for inference:** runs on CPU in inference time. See our [webcam demo](demo) for an example\n- Provides pre-trained models for almost all reference Mask R-CNN and Faster R-CNN configurations with 1x schedule.\n\n## Webcam and Jupyter notebook demo\n\nWe provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference:\n```bash\ncd demo\n# by default, it runs on the GPU\n# for best results, use min-image-size 800\npython webcam.py --min-image-size 800\n# can also run it on the CPU\npython webcam.py --min-image-size 300 MODEL.DEVICE cpu\n# or change the model that you want to use\npython webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu\n# in order to see the probability heatmaps, pass --show-mask-heatmaps\npython webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu\n# for the keypoint demo\npython webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu\n```\n\nA notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).\n\n## Installation\n\nCheck [INSTALL.md](INSTALL.md) for installation instructions.\n\n\n## Model Zoo and Baselines\n\nPre-trained models, baselines and comparison with Detectron and mmdetection\ncan be found in [MODEL_ZOO.md](MODEL_ZOO.md)\n\n## Inference in a few lines\nWe provide a helper class to simplify writing inference pipelines using pre-trained models.\nHere is how we would do it. Run this from the `demo` folder:\n```python\nfrom maskrcnn_benchmark.config import cfg\nfrom predictor import COCODemo\n\nconfig_file = \"../configs/caffe2/e2e_mask_rcnn_R_50_FPN_1x_caffe2.yaml\"\n\n# update the config options with the config file\ncfg.merge_from_file(config_file)\n# manual override some options\ncfg.merge_from_list([\"MODEL.DEVICE\", \"cpu\"])\n\ncoco_demo = COCODemo(\n    cfg,\n    min_image_size=800,\n    confidence_threshold=0.7,\n)\n# load image and then run prediction\nimage = ...\npredictions = coco_demo.run_on_opencv_image(image)\n```\n\n## Perform training on COCO dataset\n\nFor the following examples to work, you need to first install `maskrcnn_benchmark`.\n\nYou will also need to download the COCO dataset.\nWe recommend to symlink the path to the coco dataset to `datasets/` as follows\n\nWe use `minival` and `valminusminival` sets from [Detectron](https://github.com/facebookresearch/Detectron/blob/master/detectron/datasets/data/README.md#coco-minival-annotations)\n\n```bash\n# symlink the coco dataset\ncd ~/github/maskrcnn-benchmark\nmkdir -p datasets/coco\nln -s /path_to_coco_dataset/annotations datasets/coco/annotations\nln -s /path_to_coco_dataset/train2014 datasets/coco/train2014\nln -s /path_to_coco_dataset/test2014 datasets/coco/test2014\nln -s /path_to_coco_dataset/val2014 datasets/coco/val2014\n# or use COCO 2017 version\nln -s /path_to_coco_dataset/annotations datasets/coco/annotations\nln -s /path_to_coco_dataset/train2017 datasets/coco/train2017\nln -s /path_to_coco_dataset/test2017 datasets/coco/test2017\nln -s /path_to_coco_dataset/val2017 datasets/coco/val2017\n\n# for pascal voc dataset:\nln -s /path_to_VOCdevkit_dir datasets/voc\n```\n\nP.S. `COCO_2017_train` = `COCO_2014_train` + `valminusminival` , `COCO_2017_val` = `minival`\n      \n\nYou can also configure your own paths to the datasets.\nFor that, all you need to do is to modify `maskrcnn_benchmark/config/paths_catalog.py` to\npoint to the location where your dataset is stored.\nYou can also create a new `paths_catalog.py` file which implements the same two classes,\nand pass it as a config argument `PATHS_CATALOG` during training.\n\n### Single GPU training\n\nMost of the configuration files that we provide assume that we are running on 8 GPUs.\nIn order to be able to run it on fewer GPUs, there are a few possibilities:\n\n**1. Run the following without modifications**\n\n```bash\npython /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"/path/to/config/file.yaml\"\n```\nThis should work out of the box and is very similar to what we should do for multi-GPU training.\nBut the drawback is that it will use much more GPU memory. The reason is that we set in the\nconfiguration files a global batch size that is divided over the number of GPUs. So if we only\nhave a single GPU, this means that the batch size for that GPU will be 8x larger, which might lead\nto out-of-memory errors.\n\nIf you have a lot of memory available, this is the easiest solution.\n\n**2. Modify the cfg parameters**\n\nIf you experience out-of-memory errors, you can reduce the global batch size. But this means that\nyou'll also need to change the learning rate, the number of iterations and the learning rate schedule.\n\nHere is an example for Mask R-CNN R-50 FPN with the 1x schedule:\n```bash\npython tools/train_net.py --config-file \"configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\" SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025 SOLVER.MAX_ITER 720000 SOLVER.STEPS \"(480000, 640000)\" TEST.IMS_PER_BATCH 1 MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN 2000\n```\nThis follows the [scheduling rules from Detectron.](https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14-L30)\nNote that we have multiplied the number of iterations by 8x (as well as the learning rate schedules),\nand we have divided the learning rate by 8x.\n\nWe also changed the batch size during testing, but that is generally not necessary because testing\nrequires much less memory than training.\n\nFurthermore, we set `MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN 2000` as the proposals are selected for per the batch rather than per image in the default training. The value is calculated by **1000 x images-per-gpu**. Here we have 2 images per GPU, therefore we set the number as 1000 x 2 = 2000. If we have 8 images per GPU, the value should be set as 8000. Note that this does not apply if `MODEL.RPN.FPN_POST_NMS_PER_BATCH` is set to `False` during training. See [#672](https://github.com/facebookresearch/maskrcnn-benchmark/issues/672) for more details.\n\n### Multi-GPU training\nWe use internally `torch.distributed.launch` in order to launch\nmulti-gpu training. This utility function from PyTorch spawns as many\nPython processes as the number of GPUs we want to use, and each Python\nprocess will only use a single GPU.\n\n```bash\nexport NGPUS=8\npython -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"path/to/config/file.yaml\" MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN images_per_gpu x 1000\n```\nNote we should set `MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN` follow the rule in Single-GPU training.\n\n### Mixed precision training\nWe currently use [APEX](https://github.com/NVIDIA/apex) to add [Automatic Mixed Precision](https://developer.nvidia.com/automatic-mixed-precision) support. To enable, just do Single-GPU or Multi-GPU training and set `DTYPE \"float16\"`.\n\n```bash\nexport NGPUS=8\npython -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"path/to/config/file.yaml\" MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN images_per_gpu x 1000 DTYPE \"float16\"\n```\nIf you want more verbose logging, set `AMP_VERBOSE True`. See [Mixed Precision Training guide](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html) for more details.\n\n## Evaluation\nYou can test your model directly on single or multiple gpus. Here is an example for Mask R-CNN R-50 FPN with the 1x schedule on 8 GPUS:\n```bash\nexport NGPUS=8\npython -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/test_net.py --config-file \"configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\" TEST.IMS_PER_BATCH 16\n```\nTo calculate mAP for each class, you can simply modify a few lines in [coco_eval.py](https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/data/datasets/evaluation/coco/coco_eval.py). See [#524](https://github.com/facebookresearch/maskrcnn-benchmark/issues/524#issuecomment-475118810) for more details.\n\n## Abstractions\nFor more information on some of the main abstractions in our implementation, see [ABSTRACTIONS.md](ABSTRACTIONS.md).\n\n## Adding your own dataset\n\nThis implementation adds support for COCO-style datasets.\nBut adding support for training on a new dataset can be done as follows:\n```python\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\n\nclass MyDataset(object):\n    def __init__(self, ...):\n        # as you would do normally\n\n    def __getitem__(self, idx):\n        # load the image as a PIL Image\n        image = ...\n\n        # load the bounding boxes as a list of list of boxes\n        # in this case, for illustrative purposes, we use\n        # x1, y1, x2, y2 order.\n        boxes = [[0, 0, 10, 10], [10, 20, 50, 50]]\n        # and labels\n        labels = torch.tensor([10, 20])\n\n        # create a BoxList from the boxes\n        boxlist = BoxList(boxes, image.size, mode=\"xyxy\")\n        # add the labels to the boxlist\n        boxlist.add_field(\"labels\", labels)\n\n        if self.transforms:\n            image, boxlist = self.transforms(image, boxlist)\n\n        # return the image, the boxlist and the idx in your dataset\n        return image, boxlist, idx\n\n    def get_img_info(self, idx):\n        # get img_height and img_width. This is used if\n        # we want to split the batches according to the aspect ratio\n        # of the image, as it can be more efficient than loading the\n        # image from disk\n        return {\"height\": img_height, \"width\": img_width}\n```\nThat's it. You can also add extra fields to the boxlist, such as segmentation masks\n(using `structures.segmentation_mask.SegmentationMask`), or even your own instance type.\n\nFor a full example of how the `COCODataset` is implemented, check [`maskrcnn_benchmark/data/datasets/coco.py`](maskrcnn_benchmark/data/datasets/coco.py).\n\nOnce you have created your dataset, it needs to be added in a couple of places:\n- [`maskrcnn_benchmark/data/datasets/__init__.py`](maskrcnn_benchmark/data/datasets/__init__.py): add it to `__all__`\n- [`maskrcnn_benchmark/config/paths_catalog.py`](maskrcnn_benchmark/config/paths_catalog.py): `DatasetCatalog.DATASETS` and corresponding `if` clause in `DatasetCatalog.get()`\n\n### Testing\nWhile the aforementioned example should work for training, we leverage the\ncocoApi for computing the accuracies during testing. Thus, test datasets\nshould currently follow the cocoApi for now.\n\nTo enable your dataset for testing, add a corresponding if statement in [`maskrcnn_benchmark/data/datasets/evaluation/__init__.py`](maskrcnn_benchmark/data/datasets/evaluation/__init__.py):\n```python\nif isinstance(dataset, datasets.MyDataset):\n        return coco_evaluation(**args)\n```\n\n## Finetuning from Detectron weights on custom datasets\nCreate a script `tools/trim_detectron_model.py` like [here](https://gist.github.com/wangg12/aea194aa6ab6a4de088f14ee193fd968).\nYou can decide which keys to be removed and which keys to be kept by modifying the script.\n\nThen you can simply point the converted model path in the config file by changing `MODEL.WEIGHT`.\n\nFor further information, please refer to [#15](https://github.com/facebookresearch/maskrcnn-benchmark/issues/15).\n\n## Troubleshooting\nIf you have issues running or compiling this code, we have compiled a list of common issues in\n[TROUBLESHOOTING.md](TROUBLESHOOTING.md). If your issue is not present there, please feel\nfree to open a new issue.\n\n## Citations\nPlease consider citing this project in your publications if it helps your research. The following is a BibTeX reference. The BibTeX entry requires the `url` LaTeX package.\n```\n@misc{massa2018mrcnn,\nauthor = {Massa, Francisco and Girshick, Ross},\ntitle = {{maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}},\nyear = {2018},\nhowpublished = {\\url{https://github.com/facebookresearch/maskrcnn-benchmark}},\nnote = {Accessed: [Insert date here]}\n}\n```\n\n## Projects using maskrcnn-benchmark\n\n- [RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free](https://arxiv.org/abs/1901.03353). \n  Cheng-Yang Fu, Mykhailo Shvets, and Alexander C. Berg.\n  Tech report, arXiv,1901.03353.\n- [FCOS: Fully Convolutional One-Stage Object Detection](https://arxiv.org/abs/1904.01355).\n  Zhi Tian, Chunhua Shen, Hao Chen and Tong He.\n  Tech report, arXiv,1904.01355. [[code](https://github.com/tianzhi0549/FCOS)]\n\n## License\n\nmaskrcnn-benchmark is released under the MIT license. See [LICENSE](LICENSE) for additional details.\n",
            "readme_url": "https://github.com/abcxs/maskrcnn-contest",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "FCOS: Fully Convolutional One-Stage Object Detection",
            "arxiv": "1904.01355",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.01355v5",
            "abstract": "We propose a fully convolutional one-stage object detector (FCOS) to solve\nobject detection in a per-pixel prediction fashion, analogue to semantic\nsegmentation. Almost all state-of-the-art object detectors such as RetinaNet,\nSSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast,\nour proposed detector FCOS is anchor box free, as well as proposal free. By\neliminating the predefined set of anchor boxes, FCOS completely avoids the\ncomplicated computation related to anchor boxes such as calculating overlapping\nduring training. More importantly, we also avoid all hyper-parameters related\nto anchor boxes, which are often very sensitive to the final detection\nperformance. With the only post-processing non-maximum suppression (NMS), FCOS\nwith ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale\ntesting, surpassing previous one-stage detectors with the advantage of being\nmuch simpler. For the first time, we demonstrate a much simpler and flexible\ndetection framework achieving improved detection accuracy. We hope that the\nproposed FCOS framework can serve as a simple and strong alternative for many\nother instance-level tasks. Code is available at:Code is available at:\nhttps://tinyurl.com/FCOSv1",
            "authors": [
                "Zhi Tian",
                "Chunhua Shen",
                "Hao Chen",
                "Tong He"
            ]
        },
        {
            "title": "RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free",
            "arxiv": "1901.03353",
            "year": 2019,
            "url": "http://arxiv.org/abs/1901.03353v1",
            "abstract": "Recently two-stage detectors have surged ahead of single-shot detectors in\nthe accuracy-vs-speed trade-off. Nevertheless single-shot detectors are\nimmensely popular in embedded vision applications. This paper brings\nsingle-shot detectors up to the same level as current two-stage techniques. We\ndo this by improving training for the state-of-the-art single-shot detector,\nRetinaNet, in three ways: integrating instance mask prediction for the first\ntime, making the loss function adaptive and more stable, and including\nadditional hard examples in training. We call the resulting augmented network\nRetinaMask. The detection component of RetinaMask has the same computational\ncost as the original RetinaNet, but is more accurate. COCO test-dev results are\nup to 41.4 mAP for RetinaMask-101 vs 39.1mAP for RetinaNet-101, while the\nruntime is the same during evaluation. Adding Group Normalization increases the\nperformance of RetinaMask-101 to 41.7 mAP. Code is\nat:https://github.com/chengyangfu/retinamask",
            "authors": [
                "Cheng-Yang Fu",
                "Mykhailo Shvets",
                "Alexander C. Berg"
            ]
        },
        {
            "note": "Accessed: [Insert date here]",
            "howpublished": "\\url{https://github.com/facebookresearch/maskrcnn-benchmark}",
            "year": "2018",
            "title": "{maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}",
            "author": [
                "Massa, Francisco",
                "Girshick, Ross"
            ],
            "ENTRYTYPE": "misc",
            "ID": "massa2018mrcnn",
            "authors": [
                "Massa, Francisco",
                "Girshick, Ross"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "`maskrcnn_benchmark/data/datasets/coco.py`",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/maskrcnn_benchmark/data/datasets/coco.py"
                    }
                }
            },
            {
                "name": "`maskrcnn_benchmark/data/datasets/__init__.py`",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/maskrcnn_benchmark/data/datasets/__init__.py"
                    }
                }
            },
            {
                "name": "`maskrcnn_benchmark/data/datasets/evaluation/__init__.py`",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/abcxs/maskrcnn-contest/blob/master/maskrcnn_benchmark/data/datasets/evaluation/__init__.py"
                    }
                }
            },
            {
                "name": "COCO 2017"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999783148946694,
        "task": "Object Detection",
        "task_prob": 0.9906987732341626
    }
}