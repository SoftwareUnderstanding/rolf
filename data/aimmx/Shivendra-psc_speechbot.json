{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition using DeepMind's WaveNet",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Shivendra-psc",
                "owner_type": "User",
                "name": "speechbot",
                "url": "https://github.com/Shivendra-psc/speechbot",
                "stars": 0,
                "pushed_at": "2020-05-15 08:11:38+00:00",
                "created_at": "2020-05-15 08:09:55+00:00",
                "language": "Python",
                "license": "Apache License 2.0",
                "frameworks": []
            },
            {
                "type": "code",
                "name": "CHANGELOG.md",
                "sha": "cc8201afa4f6aa1cfbf895bde02c72cb5e81f798",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/blob/master/CHANGELOG.md"
                    }
                },
                "size": 409
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "8dada3edaf50dbc082c9a125058f25def75e625a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/blob/master/LICENSE"
                    }
                },
                "size": 11357
            },
            {
                "type": "code",
                "name": "data.py",
                "sha": "df95b2cccfcafc1f335f0683eff51ccf1450b07d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/blob/master/data.py"
                    }
                },
                "size": 3846
            },
            {
                "type": "code",
                "name": "docker",
                "sha": "24e3abe25e6a90d1c281be8fa97861c311bb88b2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/tree/master/docker"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "9cf516a1fb524b2b337beab2a46fc2417bec4ea8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/blob/master/model.py"
                    }
                },
                "size": 1486
            },
            {
                "type": "code",
                "name": "png",
                "sha": "a33cbf4f654f4ddab524b61cfbefa848f0ceb5fd",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/tree/master/png"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "preprocess.py",
                "sha": "5e4575e8a6d73b8078b40904f7e756132fe433b0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/blob/master/preprocess.py"
                    }
                },
                "size": 7884
            },
            {
                "type": "code",
                "name": "recognize.py",
                "sha": "6ed22fa1cac727b16949a2973d57fb950dab265c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/blob/master/recognize.py"
                    }
                },
                "size": 1465
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "294f9ace0bad24332b6470b1441a99302ae04ba7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/blob/master/requirements.txt"
                    }
                },
                "size": 76
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "2d0dabb972529aa71eb8532de837f0c8fa1ae943",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/blob/master/test.py"
                    }
                },
                "size": 2171
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "78953bbeb4ec9b1b062fa93b5d09fbdb35b24c85",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Shivendra-psc/speechbot/blob/master/train.py"
                    }
                },
                "size": 1052
            }
        ]
    },
    "authors": [
        {
            "name": "Shivendra Pratap Singh Chauhan",
            "github_id": "Shivendra-psc"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Shivendra-psc/speechbot",
            "stars": 0,
            "issues": true,
            "readme": "# Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition using DeepMind's WaveNet\nA tensorflow implementation of speech recognition based on DeepMind's [WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499). (Hereafter the Paper)\n\nAlthough [ibab](https://github.com/ibab/tensorflow-wavenet) and [tomlepaine](https://github.com/tomlepaine/fast-wavenet) have already implemented WaveNet with tensorflow, they did not implement speech recognition. That's why we decided to implement it ourselves. \n\nSome of Deepmind's recent papers are tricky to reproduce. The Paper also omitted specific details about the implementation, and we had to fill the gaps in our own way.\n\nHere are a few important notes.\n\nFirst, while the Paper used the TIMIT dataset for the speech recognition experiment, we used the free VTCK dataset.\n\nSecond, the Paper added a mean-pooling layer after the dilated convolution layer for down-sampling. We extracted [MFCC](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) from wav files and removed the final mean-pooling layer because the original setting was impossible to run on our TitanX GPU.\n\nThird, since the TIMIT dataset has phoneme labels, the Paper trained the model with two loss terms, phoneme classification and next phoneme prediction. We, instead, used a single CTC loss because VCTK provides sentence-level labels. As a result, we used only dilated conv1d layers without any dilated conv1d layers.\n\nFinally, we didn't do quantitative analyses such as BLEU score and post-processing by combining a language model due to the time constraints.\n\nThe final architecture is shown in the following figure.\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/buriburisuri/speech-to-text-wavenet/master/png/architecture.png\" width=\"1024\"/>\n</p>\n(Some images are cropped from [WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499) and [Neural Machine Translation in Linear Time](https://arxiv.org/abs/1610.10099))  \n\n\n## Version \n\nCurrent Version : __***0.0.0.2***__\n\n## Dependencies ( VERSION MUST BE MATCHED EXACTLY! )\n\n1. [tensorflow](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation) == 1.0.0\n1. [sugartensor](https://github.com/buriburisuri/sugartensor) == 1.0.0.2\n1. [pandas](http://pandas.pydata.org/pandas-docs/stable/install.html) >= 0.19.2\n1. [librosa](https://github.com/librosa/librosa) == 0.5.0\n1. [scikits.audiolab](https://pypi.python.org/pypi/scikits.audiolab)==0.11.0\n\nIf you have problems with the librosa library, try to install ffmpeg by the following command. ( Ubuntu 14.04 )  \n<pre><code>\nsudo add-apt-repository ppa:mc3man/trusty-media\nsudo apt-get update\nsudo apt-get dist-upgrade -y\nsudo apt-get -y install ffmpeg\n</code></pre>\n\n## Dataset\n\nWe used [VCTK](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html), \n[LibriSpeech](http://www.openslr.org/12/) and [TEDLIUM release 2](http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus) corpus.\nTotal number of sentences in the training set composed of the above three corpus is 240,612. \nValid and test set is built using only LibriSpeech and TEDLIUM corpuse, because VCTK corpus does not have valid and test set. \nAfter downloading the each corpus, extract them in the 'asset/data/VCTK-Corpus', 'asset/data/LibriSpeech' and \n 'asset/data/TEDLIUM_release2' directories. \n \nAudio was augmented by the scheme in the [Tom Ko et al](http://speak.clsp.jhu.edu/uploads/publications/papers/1050_pdf.pdf)'s paper. \n(Thanks @migvel for your kind information)  \n\n## Pre-processing dataset\n\nThe TEDLIUM release 2 dataset provides audio data in the SPH format, so we should convert them to some format \nlibrosa library can handle. Run the following command in the 'asset/data' directory convert SPH to wave format.  \n<pre><code>\nfind -type f -name '*.sph' | awk '{printf \"sox -t sph %s -b 16 -t wav %s\\n\", $0, $0\".wav\" }' | bash\n</code></pre>\n\nIf you don't have installed `sox`, please installed it first.\n<pre><code>\nsudo apt-get install sox\n</code></pre>\n\nWe found the main bottle neck is disk read time when training, so we decide to pre-process the whole audio data into \n  the MFCC feature files which is much smaller. And we highly recommend using SSD instead of hard drive.  \n  Run the following command in the console to pre-process whole dataset.\n<pre><code>\npython preprocess.py\n</code></pre>\n \n\n## Training the network\n\nExecute\n<pre><code>\npython train.py ( <== Use all available GPUs )\nor\nCUDA_VISIBLE_DEVICES=0,1 python train.py ( <== Use only GPU 0, 1 )\n</code></pre>\nto train the network. You can see the result ckpt files and log files in the 'asset/train' directory.\nLaunch tensorboard --logdir asset/train/log to monitor training process.\n\nWe've trained this model on a 3 Nvidia 1080 Pascal GPUs during 40 hours until 50 epochs and we picked the epoch when the \nvalidatation loss is minimum. In our case, it is epoch 40.  If you face the out of memory error, \nreduce batch_size in the train.py file from 16 to 4.  \n\nThe CTC losses at each epoch are as following table:\n\n| epoch | train set | valid set | test set | \n| :----: | ----: | ----: | ----: |\n| 20 | 79.541500 | 73.645237 | 83.607269 |\n| 30 | 72.884180 | 69.738348 | 80.145867 |\n| 40 | 69.948266 | 66.834316 | 77.316114 |\n| 50 | 69.127240 | 67.639895 | 77.866674 |\n\n\n## Testing the network\n\nAfter training finished, you can check valid or test set CTC loss by the following command.\n<pre><code>\npython test.py --set train|valid|test --frac 1.0(0.01~1.0)\n</code></pre>\nThe `frac` option will be useful if you want to test only the fraction of dataset for fast evaluation. \n\n## Transforming speech wave file to English text \n \nExecute\n<pre><code>\npython recognize.py --file <wave_file path>\n</code></pre>\nto transform a speech wave file to the English sentence. The result will be printed on the console. \n\nFor example, try the following command.\n<pre><code>\npython recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0000.flac\npython recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0001.flac\npython recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0002.flac\npython recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0003.flac\npython recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0004.flac\n</code></pre>\n\nThe result will be as follows:\n<pre><code>\nhe hoped there would be stoo for dinner turnips and charrats and bruzed patatos and fat mutton pieces to be ladled out in th thick peppered flower fatan sauce\nstuffid into you his belly counsiled him\nafter early night fall the yetl lampse woich light hop here and there on the squalled quarter of the browfles\no berty and he god in your mind\nnumbrt tan fresh nalli is waiting on nou cold nit husband\n</code></pre>\n\nThe ground truth is as follows:\n<pre><code>\nHE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE\nSTUFF IT INTO YOU HIS BELLY COUNSELLED HIM\nAFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\nHELLO BERTIE ANY GOOD IN YOUR MIND\nNUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\n</code></pre>\n\nAs mentioned earlier, there is no language model, so there are some cases where capital letters, punctuations, and words are misspelled.\n\n## pre-trained models\n\nYou can transform a speech wave file to English text with the pre-trained model on the VCTK corpus. \nExtract [the following zip file](https://drive.google.com/open?id=0B3ILZKxzcrUyVWwtT25FemZEZ1k) to the 'asset/train/' directory.\n\n## Docker support\n\nSee docker [README.md](docker/README.md).\n\n## Future works\n\n1. Language Model\n1. Polyglot(Multi-lingual) Model\n\nWe think that we should replace CTC beam decoder with a practical language model  \nand the polyglot speech recognition model will be a good candidate to future works.\n\n## Other resources\n\n1. [ibab's WaveNet(speech synthesis) tensorflow implementation](https://github.com/ibab/tensorflow-wavenet)\n1. [tomlepaine's Fast WaveNet(speech synthesis) tensorflow implementation](https://github.com/ibab/tensorflow-wavenet)\n\n## Namju's other repositories\n\n1. [SugarTensor](https://github.com/buriburisuri/sugartensor)\n1. [EBGAN tensorflow implementation](https://github.com/buriburisuri/ebgan)\n1. [Timeseries gan tensorflow implementation](https://github.com/buriburisuri/timeseries_gan)\n1. [Supervised InfoGAN tensorflow implementation](https://github.com/buriburisuri/supervised_infogan)\n1. [AC-GAN tensorflow implementation](https://github.com/buriburisuri/ac-gan)\n1. [SRGAN tensorflow implementation](https://github.com/buriburisuri/SRGAN)\n1. [ByteNet-Fast Neural Machine Translation](https://github.com/buriburisuri/ByteNet)\n\n## Citation\n\nIf you find this code useful please cite us in your work:\n\n<pre><code>\nKim and Park. Speech-to-Text-WaveNet. 2016. GitHub repository. https://github.com/buriburisuri/.\n</code></pre>\n\n# Authors\n\nNamju Kim (namju.kim@kakaocorp.com) at KakaoBrain Corp.\n\nKyubyong Park (kbpark@jamonglab.com) at KakaoBrain Corp.\n",
            "readme_url": "https://github.com/Shivendra-psc/speechbot",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Neural Machine Translation in Linear Time",
            "arxiv": "1610.10099",
            "year": 2016,
            "url": "http://arxiv.org/abs/1610.10099v2",
            "abstract": "We present a novel neural network for processing sequences. The ByteNet is a\none-dimensional convolutional neural network that is composed of two parts, one\nto encode the source sequence and the other to decode the target sequence. The\ntwo network parts are connected by stacking the decoder on top of the encoder\nand preserving the temporal resolution of the sequences. To address the\ndiffering lengths of the source and the target, we introduce an efficient\nmechanism by which the decoder is dynamically unfolded over the representation\nof the encoder. The ByteNet uses dilation in the convolutional layers to\nincrease its receptive field. The resulting network has two core properties: it\nruns in time that is linear in the length of the sequences and it sidesteps the\nneed for excessive memorization. The ByteNet decoder attains state-of-the-art\nperformance on character-level language modelling and outperforms the previous\nbest results obtained with recurrent networks. The ByteNet also achieves\nstate-of-the-art performance on character-to-character machine translation on\nthe English-to-German WMT translation task, surpassing comparable neural\ntranslation models that are based on recurrent networks with attentional\npooling and run in quadratic time. We find that the latent alignment structure\ncontained in the representations reflects the expected alignment between the\ntokens.",
            "authors": [
                "Nal Kalchbrenner",
                "Lasse Espeholt",
                "Karen Simonyan",
                "Aaron van den Oord",
                "Alex Graves",
                "Koray Kavukcuoglu"
            ]
        },
        {
            "title": "WaveNet: A Generative Model for Raw Audio",
            "arxiv": "1609.03499",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.03499v2",
            "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition.",
            "authors": [
                "Aaron van den Oord",
                "Sander Dieleman",
                "Heiga Zen",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Alex Graves",
                "Nal Kalchbrenner",
                "Andrew Senior",
                "Koray Kavukcuoglu"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "TIMIT"
            },
            {
                "name": "Librispeech"
            },
            {
                "name": "Wikipedia"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9980329548431084,
        "task": "Machine Translation",
        "task_prob": 0.9844803940430024
    }
}