{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "pytorch-retinanet",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "yhenon",
                "owner_type": "User",
                "name": "pytorch-retinanet",
                "url": "https://github.com/yhenon/pytorch-retinanet",
                "stars": 1799,
                "pushed_at": "2022-01-20 11:57:39+00:00",
                "created_at": "2018-04-25 00:15:22+00:00",
                "language": "Python",
                "description": "Pytorch implementation of RetinaNet object detection.",
                "license": "Apache License 2.0",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "f04591f5d138f261968fb100695aba75d78c4a8d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yhenon/pytorch-retinanet/blob/master/.gitignore"
                    }
                },
                "size": 1164
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yhenon/pytorch-retinanet/blob/master/LICENSE"
                    }
                },
                "size": 11357
            },
            {
                "type": "code",
                "name": "coco_validation.py",
                "sha": "e425df4ac7295d7bedc724e831bef64708cf9625",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yhenon/pytorch-retinanet/blob/master/coco_validation.py"
                    }
                },
                "size": 1464
            },
            {
                "type": "code",
                "name": "csv_validation.py",
                "sha": "6b3495c74da6a223a1b720c870479cda44a56824",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yhenon/pytorch-retinanet/blob/master/csv_validation.py"
                    }
                },
                "size": 1942
            },
            {
                "type": "code",
                "name": "images",
                "sha": "83f29af12567d0b6615453cb6c705aba4725f084",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yhenon/pytorch-retinanet/tree/master/images"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "retinanet",
                "sha": "bab428d8b1ae72c1cc642e5a8311a8431971a641",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yhenon/pytorch-retinanet/tree/master/retinanet"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "a9aabc8996066f045d5d3ec32e28a82cc8d7f25c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yhenon/pytorch-retinanet/blob/master/train.py"
                    }
                },
                "size": 6616
            },
            {
                "type": "code",
                "name": "visualize.py",
                "sha": "9bb9de8e6b0ac1cfc0c9eb624fc3d9c484a3c158",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yhenon/pytorch-retinanet/blob/master/visualize.py"
                    }
                },
                "size": 3362
            },
            {
                "type": "code",
                "name": "visualize_single_image.py",
                "sha": "fdc47b8fdfa35f136db5a66c1b4738e2ce216a26",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yhenon/pytorch-retinanet/blob/master/visualize_single_image.py"
                    }
                },
                "size": 4312
            }
        ]
    },
    "authors": [
        {
            "name": "Yann Henon",
            "github_id": "yhenon"
        },
        {
            "name": "yhenon-nextdroid",
            "github_id": "yhenon-nextdroid"
        },
        {
            "name": "Aditya Kane",
            "github_id": "AdityaKane2001"
        },
        {
            "name": "rvandeghen",
            "github_id": "rvandeghen"
        },
        {
            "name": "JingyuanHu",
            "email": "hjyq@mail.ustc.edu.cn",
            "github_id": "JingyuanHu"
        },
        {
            "name": "Miguel Morales",
            "email": "mimoralea@gmail.com",
            "github_id": "mimoralea"
        },
        {
            "name": "villa",
            "email": "xu1718191411@gmail.com",
            "github_id": "xu1718191411"
        }
    ],
    "tags": [
        "pytorch",
        "retinanet",
        "python"
    ],
    "description": "Pytorch implementation of RetinaNet object detection.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/yhenon/pytorch-retinanet",
            "stars": 1799,
            "issues": true,
            "readme": "# pytorch-retinanet\n\n![img3](https://github.com/yhenon/pytorch-retinanet/blob/master/images/3.jpg)\n![img5](https://github.com/yhenon/pytorch-retinanet/blob/master/images/5.jpg)\n\nPytorch  implementation of RetinaNet object detection as described in [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002) by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He and Piotr Doll\u00e1r.\n\nThis implementation is primarily designed to be easy to read and simple to modify.\n\n## Results\nCurrently, this repo achieves 33.5% mAP at 600px resolution with a Resnet-50 backbone. The published result is 34.0% mAP. The difference is likely due to the use of Adam optimizer instead of SGD with weight decay.\n\n## Installation\n\n1) Clone this repo\n\n2) Install the required packages:\n\n```\napt-get install tk-dev python-tk\n```\n\n3) Install the python packages:\n\t\n```\npip install pandas\npip install pycocotools\npip install opencv-python\npip install requests\n\n```\n\n## Training\n\nThe network can be trained using the `train.py` script. Currently, two dataloaders are available: COCO and CSV. For training on coco, use\n\n```\npython train.py --dataset coco --coco_path ../coco --depth 50\n```\n\nFor training using a custom dataset, with annotations in CSV format (see below), use\n\n```\npython train.py --dataset csv --csv_train <path/to/train_annots.csv>  --csv_classes <path/to/train/class_list.csv>  --csv_val <path/to/val_annots.csv>\n```\n\nNote that the --csv_val argument is optional, in which case no validation will be performed.\n\n## Pre-trained model\n\nA pre-trained model is available at: \n- https://drive.google.com/open?id=1yLmjq3JtXi841yXWBxst0coAgR26MNBS (this is a pytorch state dict)\n\nThe state dict model can be loaded using:\n\n```\nretinanet = model.resnet50(num_classes=dataset_train.num_classes(),)\nretinanet.load_state_dict(torch.load(PATH_TO_WEIGHTS))\n```\n\n## Validation\n\nRun `coco_validation.py` to validate the code on the COCO dataset. With the above model, run:\n\n`python coco_validation.py --coco_path ~/path/to/coco --model_path /path/to/model/coco_resnet_50_map_0_335_state_dict.pt`\n\n\nThis produces the following results:\n\n```\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.335\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.499\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.357\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.167\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.369\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.466\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.282\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.429\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.255\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.508\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.597\n```\n\nFor CSV Datasets (more info on those below), run the following script to validate:\n\n`python csv_validation.py --csv_annotations_path path/to/annotations.csv --model_path path/to/model.pt --images_path path/to/images_dir --class_list_path path/to/class_list.csv   (optional) iou_threshold iou_thres (0<iou_thresh<1) `\n\nIt produces following resullts:\n\n```\nlabel_1 : (label_1_mAP)\nPrecision :  ...\nRecall:  ...\n\nlabel_2 : (label_2_mAP)\nPrecision :  ...\nRecall:  ...\n```\n\nYou can also configure csv_eval.py script to save the precision-recall curve on disk.\n\n\n\n## Visualization\n\nTo visualize the network detection, use `visualize.py`:\n\n```\npython visualize.py --dataset coco --coco_path ../coco --model <path/to/model.pt>\n```\nThis will visualize bounding boxes on the validation set. To visualise with a CSV dataset, use:\n\n```\npython visualize.py --dataset csv --csv_classes <path/to/train/class_list.csv>  --csv_val <path/to/val_annots.csv> --model <path/to/model.pt>\n```\n\n## Model\n\nThe retinanet model uses a resnet backbone. You can set the depth of the resnet model using the --depth argument. Depth must be one of 18, 34, 50, 101 or 152. Note that deeper models are more accurate but are slower and use more memory.\n\n## CSV datasets\nThe `CSVGenerator` provides an easy way to define your own datasets.\nIt uses two CSV files: one file containing annotations and one file containing a class name to ID mapping.\n\n### Annotations format\nThe CSV file with annotations should contain one annotation per line.\nImages with multiple bounding boxes should use one row per bounding box.\nNote that indexing for pixel values starts at 0.\nThe expected format of each line is:\n```\npath/to/image.jpg,x1,y1,x2,y2,class_name\n```\n\nSome images may not contain any labeled objects.\nTo add these images to the dataset as negative examples,\nadd an annotation where `x1`, `y1`, `x2`, `y2` and `class_name` are all empty:\n```\npath/to/image.jpg,,,,,\n```\n\nA full example:\n```\n/data/imgs/img_001.jpg,837,346,981,456,cow\n/data/imgs/img_002.jpg,215,312,279,391,cat\n/data/imgs/img_002.jpg,22,5,89,84,bird\n/data/imgs/img_003.jpg,,,,,\n```\n\nThis defines a dataset with 3 images.\n`img_001.jpg` contains a cow.\n`img_002.jpg` contains a cat and a bird.\n`img_003.jpg` contains no interesting objects/animals.\n\n\n### Class mapping format\nThe class name to ID mapping file should contain one mapping per line.\nEach line should use the following format:\n```\nclass_name,id\n```\n\nIndexing for classes starts at 0.\nDo not include a background class as it is implicit.\n\nFor example:\n```\ncow,0\ncat,1\nbird,2\n```\n\n## Acknowledgements\n\n- Significant amounts of code are borrowed from the [keras retinanet implementation](https://github.com/fizyr/keras-retinanet)\n- The NMS module used is from the [pytorch faster-rcnn implementation](https://github.com/ruotianluo/pytorch-faster-rcnn)\n\n## Examples\n\n![img1](https://github.com/yhenon/pytorch-retinanet/blob/master/images/1.jpg)\n![img2](https://github.com/yhenon/pytorch-retinanet/blob/master/images/2.jpg)\n![img4](https://github.com/yhenon/pytorch-retinanet/blob/master/images/4.jpg)\n![img6](https://github.com/yhenon/pytorch-retinanet/blob/master/images/6.jpg)\n![img7](https://github.com/yhenon/pytorch-retinanet/blob/master/images/7.jpg)\n![img8](https://github.com/yhenon/pytorch-retinanet/blob/master/images/8.jpg)\n",
            "readme_url": "https://github.com/yhenon/pytorch-retinanet",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Focal Loss for Dense Object Detection",
            "arxiv": "1708.02002",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02002v2",
            "abstract": "The highest accuracy object detectors to date are based on a two-stage\napproach popularized by R-CNN, where a classifier is applied to a sparse set of\ncandidate object locations. In contrast, one-stage detectors that are applied\nover a regular, dense sampling of possible object locations have the potential\nto be faster and simpler, but have trailed the accuracy of two-stage detectors\nthus far. In this paper, we investigate why this is the case. We discover that\nthe extreme foreground-background class imbalance encountered during training\nof dense detectors is the central cause. We propose to address this class\nimbalance by reshaping the standard cross entropy loss such that it\ndown-weights the loss assigned to well-classified examples. Our novel Focal\nLoss focuses training on a sparse set of hard examples and prevents the vast\nnumber of easy negatives from overwhelming the detector during training. To\nevaluate the effectiveness of our loss, we design and train a simple dense\ndetector we call RetinaNet. Our results show that when trained with the focal\nloss, RetinaNet is able to match the speed of previous one-stage detectors\nwhile surpassing the accuracy of all existing state-of-the-art two-stage\ndetectors. Code is at: https://github.com/facebookresearch/Detectron.",
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999853640248406,
        "task": "Object Detection",
        "task_prob": 0.9890352136149133
    }
}