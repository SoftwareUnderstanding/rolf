{
    "visibility": {
        "visibility": "public"
    },
    "name": "Single Headed Attention RNN",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Smerity",
                "owner_type": "User",
                "name": "sha-rnn",
                "url": "https://github.com/Smerity/sha-rnn",
                "stars": 1148,
                "pushed_at": "2021-11-27 07:17:07+00:00",
                "created_at": "2019-09-16 19:52:29+00:00",
                "language": "Python",
                "description": "Single Headed Attention RNN - \"Stop thinking with your head\"",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "data.py",
                "sha": "f1c0bce2368ba1299987a5c77dcd210424e8358b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Smerity/sha-rnn/blob/master/data.py"
                    }
                },
                "size": 2009
            },
            {
                "type": "code",
                "name": "data",
                "sha": "5b5d6fa59916d2f1d3c8ae116d1e994fd097937d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Smerity/sha-rnn/tree/master/data"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "embed_regularize.py",
                "sha": "b0a40c5d3299cde8c06886b4ac4c9a7204eb90ef",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Smerity/sha-rnn/blob/master/embed_regularize.py"
                    }
                },
                "size": 1001
            },
            {
                "type": "code",
                "name": "generate.py",
                "sha": "d51019fcfa96f202ca59c1f56d8dc3992737442d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Smerity/sha-rnn/blob/master/generate.py"
                    }
                },
                "size": 6407
            },
            {
                "type": "code",
                "name": "getdata.sh",
                "sha": "b90f6fe720770cea6f3023fb583352d72d84e962",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Smerity/sha-rnn/blob/master/getdata.sh"
                    }
                },
                "size": 1434
            },
            {
                "type": "code",
                "name": "lookahead.py",
                "sha": "d2f672efbf76805d03c8f2e50e3f5c2d05a41749",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Smerity/sha-rnn/blob/master/lookahead.py"
                    }
                },
                "size": 1388
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "bafe09c44daff4ecdaa5e4bff0b42864105c33f8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Smerity/sha-rnn/blob/master/main.py"
                    }
                },
                "size": 20571
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "8fe3e622fce3c217d669794eb209489bb4657d72",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Smerity/sha-rnn/blob/master/model.py"
                    }
                },
                "size": 14886
            },
            {
                "type": "code",
                "name": "splitcross.py",
                "sha": "5e267079b748b69b927a355c20d3ef2a847d1dbc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Smerity/sha-rnn/blob/master/splitcross.py"
                    }
                },
                "size": 10207
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "c37e1d56c1057b5fffc7bf842abccc8d346c8b66",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Smerity/sha-rnn/blob/master/utils.py"
                    }
                },
                "size": 1198
            }
        ]
    },
    "authors": [
        {
            "name": "Stephen Merity",
            "email": "smerity@smerity.com",
            "github_id": "Smerity"
        }
    ],
    "tags": [],
    "description": "Single Headed Attention RNN - \"Stop thinking with your head\"",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Smerity/sha-rnn",
            "stars": 1148,
            "issues": true,
            "readme": "# Single Headed Attention RNN\n\nFor full details see the paper [Single Headed Attention RNN: Stop Thinking With Your Head](https://arxiv.org/abs/1911.11423).\n\nIn summary, \"stop thinking with your (attention) head\".\n\n- Obtain strong results on a byte level language modeling dataset (enwik8) in under 24 hours on a single GPU (12GB Titan V)\n- Support long range dependencies (up to 5000 tokens) without increasing compute time or memory usage substantially by using a simpler attention mechanism\n- Avoid the fragile training process required by standard Transformer models such as a long warmup\n- Back off toward a standard LSTM allowing you to drop retained memory states (needed for a Transformer model) if memory becomes a major constraint\n- Provide a smaller model that features only standard components such as the LSTM, single headed attention, and feed-forward modules such that they can easily be productionized using existing optimized tools and exported to various formats (i.e. ONNX)\n\n| Model                             | Test BPC | Params | LSTM Based |\n|-----------------------------------|----------|--------|------------|\n| Krause mLSTM                      | 1.24     | 46M    | \u2714          |\n| AWD-LSTM                          | 1.23    | 44M    | \u2714          |\n| **SHA-LSTM**                          | 1.07     | 63M    | \u2714          |\n| 12L Transformer-XL                | 1.06     | 41M    |            |\n| 18L Transformer-XL                | 1.03     | 88M    |            |\n| Adaptive Span Transformer (Small) | 1.02     | 38M    |            |\n\nWhilst the model is still quite some way away from state of the art (~0.98 bpc) the model is low resource and high efficiency without having yet been optimized to be so.\nThe model was trained in under 24 hours on a single GPU with the [Adaptive Span Transformer](https://github.com/facebookresearch/adaptive-span) (small) being the only recent Transformer model to achieve similar levels of training efficiency.\n\n## To recreate\n\n### Setup\n\nTo get started:\n\n- Retrieve the data with `./getdata.sh`\n- Install PyTorch version 1.2+\n- Install Nvidia's [AMP](https://github.com/NVIDIA/apex)\n- Install the minimum trust variant of LAMB from [Smerity's PyTorch-LAMB](https://github.com/Smerity/pytorch-lamb)\n\n### Training the model\n\nBy default the model trains the minimal single headed attention model from the paper, inserting a lone attention mechanism in the second last layer of a four layer LSTM.\nThis takes only half an hour per epoch on a Titan V or V100.\nIf you want slightly better results but a longer training time (an hour per epoch) set `use_attn` to True for all layers in `model.py` and decrease batch size until it fits in memory (i.e. 8).\nSadly there are no command line options for running the other models - it's manual tinkering.\nThe code is not kind.\nI'll be performing a re-write in the near future meant for long term academic and industrial use - contact me if you're interested :)\n\nNote: still [shaking out bugs from the commands below](https://github.com/Smerity/sha-rnn/issues/3). We have near third party replication but still a fix or two out. Feel free to run and note any discrepancies! If you fiddle with hyper-parameters (which I've done very little of - it's a treasure chest of opportunity to get a lower than expected BPC as your reward!) do report that too :)\n\nWhen running the training command below continue until the validation bpc stops improving. Don't worry about letting it run longer as the code will only save the model with the best validation bpc.\n\n`python -u main.py --epochs 32 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --data data/enwik8/ --save ENWIK8.pt --log-interval 10 --seed 5512 --optimizer lamb --bptt 1024 --warmup 800 --lr 2e-3 --emsize 1024 --nhid 4096 --nlayers 4 --batch_size 16`\n\nWhen the training slows down a second pass with a halved learning rate until validation bpc stops improving will get a few more bpc off. A smart learning rate decay is likely the correct way to go here but that's not what I did for my experiments.\n\n`python -u main.py --epochs 5 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --data data/enwik8/ --save ENWIK8.pt --log-interval 10 --seed 5512 --optimizer lamb --bptt 1024 --warmup 800 --lr 2e-3 --emsize 1024 --nhid 4096 --nlayers 4 --batch_size 16 --resume ENWIK8.pt --lr 1e-3 --seed 125`\n\nMost of the improvement will happen in the first few epochs of this final command.\n\nThe final test bpc should be approximately 1.07 for the full 4 layer SHA-LSTM or 1.08 for the single headed 4 layer SHA-LSTM.\n",
            "readme_url": "https://github.com/Smerity/sha-rnn",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Single Headed Attention RNN: Stop Thinking With Your Head",
            "arxiv": "1911.11423",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.11423v2",
            "abstract": "The leading approaches in language modeling are all obsessed with TV shows of\nmy youth - namely Transformers and Sesame Street. Transformers this,\nTransformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer\nscale silicon. We opt for the lazy path of old and proven techniques with a\nfancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The\nauthor's lone goal is to show that the entire field might have evolved a\ndifferent direction if we had instead been obsessed with a slightly different\nacronym and slightly different result. We take a previously strong language\nmodel based only on boring LSTMs and get it to within a stone's throw of a\nstone's throw of state-of-the-art byte level language model results on enwik8.\nThis work has undergone no intensive hyperparameter optimization and lived\nentirely on a commodity desktop machine that made the author's small studio\napartment far too warm in the midst of a San Franciscan summer. The final\nresults are achievable in plus or minus 24 hours on a single GPU as the author\nis impatient. The attention mechanism is also readily extended to large\ncontexts with minimal computation. Take that Sesame Street.",
            "authors": [
                "Stephen Merity"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.998692390114706,
        "task": "Machine Translation",
        "task_prob": 0.8204716107213628
    }
}