{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "NICE",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "fmu2",
                "owner_type": "User",
                "name": "NICE",
                "url": "https://github.com/fmu2/NICE",
                "stars": 21,
                "pushed_at": "2019-01-30 02:35:46+00:00",
                "created_at": "2019-01-21 23:44:20+00:00",
                "language": "Python",
                "description": "PyTorch implementation of NICE",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "99d172f29fed8056a327f27762d049af11bb5b5b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fmu2/NICE/blob/master/LICENSE"
                    }
                },
                "size": 1068
            },
            {
                "type": "code",
                "name": "nice.py",
                "sha": "3f720ad0b2f90da9126f22c1bd88736613991ef1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fmu2/NICE/blob/master/nice.py"
                    }
                },
                "size": 5009
            },
            {
                "type": "code",
                "name": "samples",
                "sha": "038e5092ba58e15e0be42d03d4c8a9ef8da1d2ee",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fmu2/NICE/tree/master/samples"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "statistics",
                "sha": "6e87c75e6ebbe697c148a8738866ad8b1c231a14",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fmu2/NICE/tree/master/statistics"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "552333d644bdb605b1d11772d91743b16f2e0275",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fmu2/NICE/blob/master/train.py"
                    }
                },
                "size": 7315
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "51b926a23d6648f0271a5dcbf1cda868a3d588b8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fmu2/NICE/blob/master/utils.py"
                    }
                },
                "size": 2573
            },
            {
                "type": "code",
                "name": "zca.py",
                "sha": "477c658f90b2b33607c0c5844e5b47687a545d4d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fmu2/NICE/blob/master/zca.py"
                    }
                },
                "size": 1794
            }
        ]
    },
    "authors": [
        {
            "name": "Fangzhou Mu",
            "email": "fmu2@wisc.edu",
            "github_id": "fmu2"
        }
    ],
    "tags": [],
    "description": "PyTorch implementation of NICE",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/fmu2/NICE",
            "stars": 21,
            "issues": true,
            "readme": "# NICE\n_A PyTorch implementation of the training procedure of [NICE: Nonlinear Independent Components Estimation](https://arxiv.org/pdf/1410.8516.pdf)_. The original implementation in theano and pylearn2 can be found at <https://github.com/laurent-dinh/nice>. \n\n## Imlementation Details\nThis implementation supports training on four datasets, namely **MNIST**, **Fashion-MNIST**, **SVHN** and **CIFAR-10**. For each dataset, only the training split is used for learning the distribution. Labels are left untouched. As is common practice, data is centered at zero before further processing. As suggested by the authors, random noise is added to dequantize the data. For SVHN and CIFAR-10, ZCA is performed to whiten the data. The means and ZCA transformation matrices were computed offline (see .pt files in ./statistics). The same set of hyperparameters (e.g. number of coupling layers, number of hidden layers in a coupling layer, number of hidden units in a hidden layer, etc.) as the paper suggests is used. Adam with default parameters are used for optimization. Samples (see below) generated from the learned distribution resemble those shown in the paper.\n\n**Note:** \nFor SVHN and CIFAR-10, add log-determinant from ZCA transformation to data log-likelihood. Since the log-determinant remains constant, it is not included in the loss function.\n\n## Samples\nThe samples are generated from models trained with default parameters.\n\n**MNIST**\n\n_1000 iterations_\n\n![](https://github.com/fmu2/NICE/blob/master/samples/mnist_bs200_logistic_cp4_md1000_hd5_iter1000.png \"MNIST 1000 iterations\")\n\n_25000 iterations_\n\n![](https://github.com/fmu2/NICE/blob/master/samples/mnist_bs200_logistic_cp4_md1000_hd5_iter25000.png \"MNIST 25000 iterations\")\n\n**Fashion-MNIST**\n\n_1000 iterations_\n\n![](https://github.com/fmu2/NICE/blob/master/samples/fashion-mnist_bs200_logistic_cp4_md1000_hd5_iter1000.png \"Fashion-MNIST 1000 iterations\")\n\n_25000 iterations_\n\n![](https://github.com/fmu2/NICE/blob/master/samples/fashion-mnist_bs200_logistic_cp4_md1000_hd5_iter25000.png \"Fashion-MNIST 25000 iterations\")\n\n**SVHN**\n\n_1000 iterations_\n\n![](https://github.com/fmu2/NICE/blob/master/samples/svhn_bs200_logistic_cp4_md2000_hd4_iter1000.png \"SVHN 1000 iterations\")\n\n_25000 iterations_\n\n![](https://github.com/fmu2/NICE/blob/master/samples/svhn_bs200_logistic_cp4_md2000_hd4_iter25000.png \"SVHN 25000 iterations\")\n\n**CIFAR-10**\n\n_1000 iterations_\n\n![](https://github.com/fmu2/NICE/blob/master/samples/cifar10_bs200_logistic_cp4_md2000_hd4_iter1000.png \"CIFAR-10 1000 iterations\")\n\n_25000 iterations_\n\n![](https://github.com/fmu2/NICE/blob/master/samples/cifar10_bs200_logistic_cp4_md2000_hd4_iter25000.png \"CIFAR-10 25000 iterations\")\n\n## Training\n\nCode runs on a single GPU and has been tested with\n\n- Python 3.7.2\n- torch 1.0.0\n- numpy 1.15.4\n\n```\npython train.py --dataset=mnist --batch_size=200 --latent=logistic --max_iter=25000\npython train.py --dataset=fashion-mnist --batch_size=200 --latent=logistic --max_iter=25000\npython train.py --dataset=svhn --batch_size=200 --latent=logistic --max_iter=25000\npython train.py --dataset=cifar10 --batch_size=200 --latent=logistic --max_iter=25000 \n```\n\n\n\n\n\n",
            "readme_url": "https://github.com/fmu2/NICE",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "NICE: Non-linear Independent Components Estimation",
            "arxiv": "1410.8516",
            "year": 2014,
            "url": "http://arxiv.org/abs/1410.8516v6",
            "abstract": "We propose a deep learning framework for modeling complex high-dimensional\ndensities called Non-linear Independent Component Estimation (NICE). It is\nbased on the idea that a good representation is one in which the data has a\ndistribution that is easy to model. For this purpose, a non-linear\ndeterministic transformation of the data is learned that maps it to a latent\nspace so as to make the transformed data conform to a factorized distribution,\ni.e., resulting in independent latent variables. We parametrize this\ntransformation so that computing the Jacobian determinant and inverse transform\nis trivial, yet we maintain the ability to learn complex non-linear\ntransformations, via a composition of simple building blocks, each based on a\ndeep neural network. The training criterion is simply the exact log-likelihood,\nwhich is tractable. Unbiased ancestral sampling is also easy. We show that this\napproach yields good generative models on four image datasets and can be used\nfor inpainting.",
            "authors": [
                "Laurent Dinh",
                "David Krueger",
                "Yoshua Bengio"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "MNIST"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "Fashion-MNIST"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9978499346866831,
        "task": "Image Generation",
        "task_prob": 0.6786757951285434
    }
}