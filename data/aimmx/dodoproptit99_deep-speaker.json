{
    "visibility": {
        "visibility": "public"
    },
    "name": "deep-speaker",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "dodoproptit99",
                "owner_type": "User",
                "name": "deep-speaker",
                "url": "https://github.com/dodoproptit99/deep-speaker",
                "stars": 1,
                "pushed_at": "2021-04-05 17:32:19+00:00",
                "created_at": "2019-11-08 16:40:50+00:00",
                "language": "Python",
                "description": "Speaker identification with Deep Speaker",
                "frameworks": [
                    "Keras",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "config.py",
                "sha": "8042e66bac58a22155dcd23ec6b4aac6cc05b189",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/config.py"
                    }
                },
                "size": 819
            },
            {
                "type": "code",
                "name": "models.py",
                "sha": "96a77749de2cee7225843ead9d03839e60a31474",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/models.py"
                    }
                },
                "size": 3316
            },
            {
                "type": "code",
                "name": "pre_process.py",
                "sha": "41e7eae198d23add871499073b65c901896bac0b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/pre_process.py"
                    }
                },
                "size": 5030
            },
            {
                "type": "code",
                "name": "predict.py",
                "sha": "45abdf4681bbc956cb311cb64062eadcf185d7b1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/predict.py"
                    }
                },
                "size": 5281
            },
            {
                "type": "code",
                "name": "pretraining.py",
                "sha": "6b9b818ca1320fde0c4f8468d53ec262122d03cb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/pretraining.py"
                    }
                },
                "size": 6202
            },
            {
                "type": "code",
                "name": "random_batch.py",
                "sha": "6b57dc75a7cb1ff1adb78091f128dc5f37c9b9f1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/random_batch.py"
                    }
                },
                "size": 2455
            },
            {
                "type": "code",
                "name": "select_batch.py",
                "sha": "1ce71d531874491ec906c0c00c6dfbfee11b2a41",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/select_batch.py"
                    }
                },
                "size": 9075
            },
            {
                "type": "code",
                "name": "silence_detector.py",
                "sha": "2cf93b10c36e7fc4753bb434419a4935dca40ab4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/silence_detector.py"
                    }
                },
                "size": 815
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "6ee14c7f234fe1bb5d1c85fdeb1e32a9ed88685c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/train.py"
                    }
                },
                "size": 4497
            },
            {
                "type": "code",
                "name": "triplet_loss.py",
                "sha": "6a19633d486e25dec3d31b5622183ac5499106a5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/triplet_loss.py"
                    }
                },
                "size": 1377
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "21de4380388e13e2db64266b4e2423e1ee67face",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dodoproptit99/deep-speaker/blob/master/utils.py"
                    }
                },
                "size": 1896
            }
        ]
    },
    "authors": [
        {
            "name": "M\u1ea1nh C\u01b0\u1eddng - \u0110\u00f4 \u0110\u00f4",
            "github_id": "dodoproptit99"
        }
    ],
    "tags": [],
    "description": "Speaker identification with Deep Speaker",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/dodoproptit99/deep-speaker",
            "stars": 1,
            "issues": true,
            "readme": "# deep-speaker\n## Speaker identification with Deep Speaker \n\nDataset: Private\n\nReference Paper: https://arxiv.org/pdf/1705.02304.pdf\n\nReference Code: https://github.com/philipperemy/deep-speaker\n\n## Introduction\nThe embeddings generated by Deep Speaker can be used for many tasks, including speaker identification, verification, and clustering. We experiment with ResCNN architectures to extract the acoustic features, then mean pool to produce utterance-level speaker embeddings, and train using triplet loss based on cosine similarity.\n\n## About my code\nThis part i will use vietnamese, if i have time in the future, i will translate it to english.\n\n#### ```pre_process.py```\n\n#### M\u1ee5c \u0111\u00edch: X\u1eed l\u00fd file wav. Tr\u00edch ch\u1ecdn \u0111\u1eb7c tr\u01b0ng file raw audio, l\u01b0u d\u01b0\u1edbi d\u1ea1ng file numpy array.\n\n#### Chi ti\u1ebft: \n\n  ```def VAD(audio)``` : H\u00e0m ti\u1ec1n x\u1eed l\u00ed audio, l\u1ecdc \u00e2m tr\u1eafng theo t\u1eebng chunk_size (s).\n  \n  ```def read_audio(filename, sample_rate=SAMPLE_RATE)``` : H\u00e0m \u0111\u1ecdc file raw audio, d\u00f9ng th\u01b0 vi\u1ec7n librosa \u0111\u1ec3 load\n  \u00e2m thanh v\u1edbi t\u1ea7n s\u1ed1 l\u1ea5y m\u1eabu l\u00e0 SAMPLE_RATE = 16000. Sample_rate l\u00e0 g\u00ec? V\u1eady t\u1ea1i sao kh\u00f4ng l\u1ea5y m\u1eabu t\u1ea1i 44000? \n  Tham kh\u1ea3o [t\u1ea1i \u0111\u00e2y.](https://librosa.github.io/blog/2019/07/17/resample-on-load/) \n  \n  ```def normalize_frames(m,epsilon=1e-12)``` : H\u00e0m chu\u1ea9n h\u00f3a \u0111\u1ea7u v\u00e0o m\u1ea1ng deep learning, n\u00f3 cho ph\u00e9p s\u1eed d\u1ee5ng t\u1ed1t h\u01a1n \n  learning rate, kh\u1edfi t\u1ea1o hi\u1ec7u qu\u1ea3 h\u01a1n. Chi ti\u1ebft t\u1ea1i \u0111\u00e2y: [Batch normalize](https://arxiv.org/pdf/1502.03167.pdf)\n  \n  ```def extract_features(signal, target_sample_rate=SAMPLE_RATE)``` : \n  B\u01b0\u1edbc \u0111\u1ea7u ti\u00ean trong b\u1ea5t k\u1ef3 h\u1ec7 th\u1ed1ng nh\u1eadn d\u1ea1ng gi\u1ecdng n\u00f3i t\u1ef1 \u0111\u1ed9ng n\u00e0o l\u00e0 tr\u00edch xu\u1ea5t c\u00e1c t\u00ednh n\u0103ng, \n  t\u1ee9c l\u00e0 x\u00e1c \u0111\u1ecbnh c\u00e1c th\u00e0nh ph\u1ea7n c\u1ee7a t\u00edn hi\u1ec7u \u00e2m thanh t\u1ed1t \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh n\u1ed9i dung ng\u00f4n ng\u1eef v\u00e0 \n  lo\u1ea1i b\u1ecf t\u1ea5t c\u1ea3 c\u00e1c n\u1ed9i dung kh\u00e1c mang th\u00f4ng tin nh\u01b0 nhi\u1ec5u n\u1ec1n, c\u1ea3m x\u00fac, v.v. \u0110\u1ec3 hi\u1ec3u r\u00f5 h\u01a1n v\u1ec1 mel filterbank, \n  tham kh\u1ea3o [MFCC](http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)\n  \n  Sau khi qua h\u00e0m n\u00e0y, raw audio tr\u1edf th\u00e0nh 1 np.array v\u1edbi shape (num_frames, nfilt(trong fbank), 1)\n  \n#### ```models.py```\n\n#### Chi ti\u1ebft:\n\nModel \u0111\u01b0\u1ee3c implement l\u1ea1i gi\u1ed1ng y h\u1ec7t trong paper, s\u1eed d\u1ee5ng 4 res_block v\u1edbi s\u1ed1 l\u01b0\u1ee3ng filters l\u1ea7n l\u01b0\u1ee3t l\u00e0 64, 128, 256, 512 \n\u0111\u1ec3 tr\u00edch ch\u1ecdn \u0111\u1eb7c tr\u01b0ng c\u1ee7a t\u00edn hi\u1ec7u \u00e2m thanh.\n\n\n#### ```random_batch.py```\n\n#### M\u1ee5c \u0111\u00edch: T\u1ea1o batch random \u0111\u1ec3 l\u00e0m input cho vi\u1ec7c train nh\u1eefng batch \u0111\u1ea7u.\n\n#### Chi ti\u1ebft:\n\n  ```def clipped_audio(audio)``` : H\u00e0m chu\u1ea9n h\u00f3a audio, c\u1eaft \u0111\u1ed9 d\u00e0i audio v\u1ec1 \u0111\u00fang num_frames = 160.\n  \n  ```def random_batch``` : Model train triplet loss c\u1ea7n input nh\u01b0 sau: 1 file anchor, 1 file positive, 1 file negative. H\u00e0m n\u00e0y s\u1ebd l\u1ea5y ra input theo ki\u1ec3u ch\u1ecdn ng\u1eabu nhi\u00ean t\u1eeb th\u01b0 m\u1ee5c ch\u1ee9a data, c\u00e1ch th\u1ee9c nh\u01b0 sau:\n  \n  L\u1ea5y ra ```libri``` l\u01b0u l\u1ea1i t\u00ean speaker v\u00e0 filename trong th\u01b0 m\u1ee5c ch\u1ee9a data_train, ch\u1ecdn random 1 speaker l\u00e0m anchor, l\u1ea5y 2 file .npy thu\u1ed9c anchor n\u00e0y. Ch\u1ecdn random 1 speaker kh\u00e1c l\u00e0m negative_speaker, l\u1ea5y 1 file .npy thu\u1ed9c speaker n\u00e0y. \n  \n#### ```select_batch.py```\n\n#### M\u1ee5c \u0111\u00edch: T\u1ea1o input cho vi\u1ec7c train triplet loss \u0111\u01b0\u1ee3c hi\u1ec7u qu\u1ea3 v\u00e0 h\u1ed9i t\u1ee5 nhanh h\u01a1n. C\u00e1c batch s\u1ebd \u0111\u01b0\u1ee3c ch\u1ecdn theo ph\u01b0\u01a1ng ph\u00e1p sau: Ch\u1ecdn ng\u1eabu nhi\u00ean 1/2 speaker, t\u00ednh cosine_similarity c\u1ee7a t\u1ea5t c\u1ea3 file thu\u1ed9c speaker \u0111\u01b0\u1ee3c ch\u1ecdn v\u1edbi t\u1ea5t c\u1ea3 \n\n#### Chi ti\u1ebft: \n\n  ``` def matrix_cosine_similarity(x1, x2)```: So s\u00e1nh kho\u1ea3ng c\u00e1ch cosine gi\u1eefa 2 embedding. Gi\u00e1 tr\u1ecb tr\u1ea3 v\u1ec1 n\u1eb1m trong kho\u1ea3ng [-1;1]. Sims c\u00e0ng cao ngh\u0129a l\u00e0 2 embedding \u0111\u00f3 thu\u1ed9c v\u1ec1 c\u00f9ng 1 speaker.\n  \n  \n",
            "readme_url": "https://github.com/dodoproptit99/deep-speaker",
            "frameworks": [
                "Keras",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "arxiv": "1502.03167",
            "year": 2015,
            "url": "http://arxiv.org/abs/1502.03167v3",
            "abstract": "Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters.",
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ]
        },
        {
            "title": "Deep Speaker: an End-to-End Neural Speaker Embedding System",
            "arxiv": "1705.02304",
            "year": 2017,
            "url": "http://arxiv.org/abs/1705.02304v1",
            "abstract": "We present Deep Speaker, a neural speaker embedding system that maps\nutterances to a hypersphere where speaker similarity is measured by cosine\nsimilarity. The embeddings generated by Deep Speaker can be used for many\ntasks, including speaker identification, verification, and clustering. We\nexperiment with ResCNN and GRU architectures to extract the acoustic features,\nthen mean pool to produce utterance-level speaker embeddings, and train using\ntriplet loss based on cosine similarity. Experiments on three distinct datasets\nsuggest that Deep Speaker outperforms a DNN-based i-vector baseline. For\nexample, Deep Speaker reduces the verification equal error rate by 50%\n(relatively) and improves the identification accuracy by 60% (relatively) on a\ntext-independent dataset. We also present results that suggest adapting from a\nmodel trained with Mandarin can improve accuracy for English speaker\nrecognition.",
            "authors": [
                "Chao Li",
                "Xiaokong Ma",
                "Bing Jiang",
                "Xiangang Li",
                "Xuewei Zhang",
                "Xiao Liu",
                "Ying Cao",
                "Ajay Kannan",
                "Zhenyao Zhu"
            ]
        }
    ],
    "domain": {
        "domain_type": "Unknown"
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            }
        ]
    }
}