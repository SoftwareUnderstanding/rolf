{
    "visibility": {
        "visibility": "public"
    },
    "name": "Project Introduction",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "CarpdiemLiang",
                "owner_type": "User",
                "name": "style_transfer",
                "url": "https://github.com/CarpdiemLiang/style_transfer",
                "stars": 4,
                "pushed_at": "2019-06-10 04:50:54+00:00",
                "created_at": "2019-06-02 23:58:13+00:00",
                "language": "Jupyter Notebook",
                "frameworks": [
                    "scikit-learn",
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "Ukiyoe_codes",
                "sha": "6c661a53cc7ba99f20ada341b4a797561f0a1c63",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CarpdiemLiang/style_transfer/tree/master/Ukiyoe_codes"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "cycle_gan_unet",
                "sha": "08b72ca90957dd27641f33bf8d8b7467242b941e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CarpdiemLiang/style_transfer/tree/master/cycle_gan_unet"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "neural_style_transfer",
                "sha": "78dc9fe3cc4d6c51e9f60667386045edf2a9ba12",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CarpdiemLiang/style_transfer/tree/master/neural_style_transfer"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "CarpdiemLiang",
            "email": "taliang@eng.ucsd.edu",
            "github_id": "CarpdiemLiang"
        },
        {
            "name": "Ke Han",
            "github_id": "keh101"
        },
        {
            "name": "imperatorcaesaraugustus",
            "github_id": "imperatorcaesaraugustus"
        },
        {
            "name": "iamryf",
            "email": "iamryf@163.com",
            "github_id": "iamryf"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/CarpdiemLiang/style_transfer",
            "stars": 4,
            "issues": true,
            "readme": "# Project Introduction\nThis is a project about image style transfer developed by Tao Liang, Tianrui Yu, Ke Han and Yifan Ruan. Our project contains three different models, one is in \"cycle_gan_unet\" directory which uses the u-net like cnn as generators, one is in \"Ukiyoe_codes\" directory which uses Resnet blocks as generators, which uses the model proposed in this paper https://arxiv.org/pdf/1703.10593.pdf, the other is in neural_style_transfer that implement sytle transfer using convolution neural network proposed in this paper https://arxiv.org/pdf/1508.06576.pdf.\n\n## Cycle-Gan-Unet\n### Description:\nThis model consists of 2 generators and 2 discriminators. The two generators as U-net like CNNs. During the evaluation of the model, I directly used the pretrained salient objective detection model from Joker, https://github.com/Joker316701882/Salient-Object-Detection.\n### Requirements:\nDownload the check-points for the model from the google drive link, and put them into the corresponding directorys.<br/>\n/baroque/checkpoint.pth.tar: https://drive.google.com/open?id=1oMTewhni1L7ZW0F9nNgNoE2RfkrGZ500<br/>\n/ukiyo_e/checkpoint.pth.tar: https://drive.google.com/open?id=1mEQliUwOKgSLSUuB_vBXwl03HH_p4VJO<br/>\n/salience_model/model.ckpt-200.data-00000-of-00001: https://drive.google.com/open?id=1u8gW2Oj8lZ_Cxqg561lQR9ioDaK64LwX<br/>\n\n### Structure:\n./cycle_gan_unet/baroque                         -- Store the checkpoints for baroque style translation<br/>\n./cycle_gan_unet/ukiyo_e                             -- Store the checkpoints for ukiyo_e style translation<br/>\n./cycle_gan_unet/meta_grapsh                         -- Store the information of the salient objective detection model<br/>\n./cycle_gan_unet/salience_model                      -- Store the checkpoints for salient objective detection model<br/>\n./cycle_gan_unet/images\\*.pkl                        -- All the pickle files are used to store the images according to different styles and landscape<br/>\n./cycle_gan_unet/demo.ipynb                           -- This notebook is used for demo, you can choose the image youo want by changing the index of \"val_set\"<br/>\n./cycle_gan_unet/cycle_gan_unet.ipynb                       -- This notebook is the main function of the model<br/>\n./cycle_gan_unet/nntools.py                           -- This .py file abstract the status manager and realize the training process of the model<br/>\n./cycle_gan_unet/util.py                              -- This .py file is used to realize the image pool called by nntools.py<br/>\n./cycle_gan_unet/inference.py                         -- This .py file is used to run the pretrained salient objective detection model<br/>\n\n### Usage:\nDirectly run the demo.ipynb notebook. You can see the original image and the transferred image.<br/>\nIf you want to train the model by yourself, delete /baroque and /ukiyo_e directorys. And run the cycle_gan_model.ipynb notebook. You can set all the parameters in the initialization of the experiment class.\n\n## Cycle-Gan-Resnet \nThis is the README for photo-to-ukiyoe cycle-GAN style transfer task. Over half of the codes are adopted from 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix' and then modified. The rest are written by student. \n\n### Requirements:\nInstall visdom and dominate if willing to display training progress on a webpage by:\n    pip install -visdom\n    pip install -dominate\n\n### Structure:\nsingle_test.ipynb:   run this notebook to show the Ukiyoe-style transfer result of 'test_image.jpg'. Make sure the image, latest_ukiyoe_G_A.pkl and './models' are in their original places<br/>\ntrain.ipynb:  run this notebook to train a cycle-GAN that can transfer 'datasets/trainA' style to 'datasets/trainB' style. Training options can be found and revised in './options/train_options.py' and './options/base_options.py'<br/>\ntest.ipynb:  run this notebook to test the model in './checkpoints' file. Input the model name in './options/base_options.py'<br/>\nplot_losses.ipynb:   run this to plot losses given a loss log in './checkpoints'<br/>\n\n.Ukiyoe_codes/options/base_options.py:   stores basic training and testing options.<br/>\n.Ukiyoe_codes/options/train_options.py:   stores other training options<br/>\n.Ukiyoe_codes/options/test_options.py:   stores other testing options<br/>\n\n.Ukiyoe_codes/models/base_model.py:   base class of all the models<br/>\n.Ukiyoe_codes/models/cycle_gan_model.py:   implement cycle-GAN model<br/>\n.Ukiyoe_codes/models/networks.py:   define basic network behavior methods<br/>\n.Ukiyoe_codes/models/test_model.py:   define some testing settings and run the testing from test.ipynb<br/>\n\n.Ukiyoe_codes/util/:   include python files that handle data loading and processing, webpage display and image buffer.<br/>\n\n.Ukiyoe_codes/datasets/:   a folder that stores training and testing data in trainA, trainB, testA and testB subfolders.<br/>\n\n.Ukiyoe_codes/checkpoints/:   a folder storing saved models, loss logs and training options<br/>\n\n.Ukiyoe_codes/latest_ukiyoe_G_A.pkl: the saved generator that can translate images into ukiyoe-style, used in single_test.ipynb<br/>\n\n.Ukiyoe_codes/test_image.jpg: test image used in single_test.ipynb<br/>\n\n### Usage:\nsingle_test.ipynb(for demo use):   run this notebook to show the Ukiyoe-style transfer result of 'test_image.jpg'. Make sure the image, latest_ukiyoe_G_A.pkl and './models' are in their original places<br/>\n\ntrain.ipynb:  run this notebook to train a cycle-GAN that can transfer 'datasets/trainA' style to 'datasets/trainB' style. Training options can be found and revised in './options/train_options.py' and './options/base_options.py'<br/>\n\ntest.ipynb:  run this notebook to test the model in './checkpoints' file. Input the model name in './options/base_options.py'<br/>\nplot_losses.ipynb:   run this to plot losses given a loss log in './checkpoints'<br/>\n\n\n\n## Neural Style Transfer: \n### Requirements: \nInstall package 'pillow' as: $ pip install --user pillow <br/>\nInstall package 'matplotlib' as: $ pip install --user matplotlib\n\n### Structure:\n./neural_style_transfer/Neural_Style_Transfer.ipynb      -- This notebook stores neural style transfer method as well as the demo of the model<br/>\n./neural_style_transfer/images                          -- Store the style image and content image for this part, make sure they are in the correct path\n\n### Usage:\nRun the Neural_Style_Transfer.ipynb for demo.<br/>\nThe notebook also stores model. If you want to change the network structure, choose one of content_layers_default and style_layers_default each and comment the others. For white noise input, consider decreasing the weight of style loss and increase the number of optimizing steps. \n\n\n",
            "readme_url": "https://github.com/CarpdiemLiang/style_transfer",
            "frameworks": [
                "scikit-learn",
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "A Neural Algorithm of Artistic Style",
            "arxiv": "1508.06576",
            "year": 2015,
            "url": "http://arxiv.org/abs/1508.06576v2",
            "abstract": "In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery.",
            "authors": [
                "Leon A. Gatys",
                "Alexander S. Ecker",
                "Matthias Bethge"
            ]
        },
        {
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "arxiv": "1703.10593",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10593v7",
            "abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.",
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A. Efros"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9998483065115978,
        "task": "Image-to-Image Translation",
        "task_prob": 0.8820043322076012
    }
}