{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "Pytorch-YOLOv4",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "WuJiang1996",
                "owner_type": "User",
                "name": "pytorch-YOLOv4",
                "url": "https://github.com/WuJiang1996/pytorch-YOLOv4",
                "stars": 1,
                "pushed_at": "2020-09-23 14:13:41+00:00",
                "created_at": "2020-09-23 14:11:54+00:00",
                "language": "Python",
                "description": "Forked from https://github.com/Tianxiaomo/pytorch-YOLOv4.git",
                "license": "Apache License 2.0",
                "frameworks": [
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "d20493de653ea132e8aacac2c9809d82666d5e74",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/.gitignore"
                    }
                },
                "size": 131
            },
            {
                "type": "code",
                "name": "DeepStream",
                "sha": "bed32d3a93f832c9227cedb5f2474c57137d0bd8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/tree/master/DeepStream"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "License.txt",
                "sha": "d645695673349e3947e8e5ae42332d0ac3164cd7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/License.txt"
                    }
                },
                "size": 11358
            },
            {
                "type": "code",
                "name": "Use_yolov4_to_train_your_own_data.md",
                "sha": "c60204b8d672e95a7b212b226dbbdea888584d07",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/Use_yolov4_to_train_your_own_data.md"
                    }
                },
                "size": 3111
            },
            {
                "type": "code",
                "name": "cfg.py",
                "sha": "15e03d4764adbfd80d2141a77b60ccbfaa64063a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/cfg.py"
                    }
                },
                "size": 1628
            },
            {
                "type": "code",
                "name": "cfg",
                "sha": "c21cbe7ce9ce2725a298952ee6ba75f37f1511ea",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/tree/master/cfg"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "data",
                "sha": "6325d13ae1a1a9ba22ee320e5e9050ed49850ea4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/tree/master/data"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "dataset.py",
                "sha": "968a5514e25692e1f9e40cb97a1e5d6ac1582790",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/dataset.py"
                    }
                },
                "size": 16988
            },
            {
                "type": "code",
                "name": "demo.py",
                "sha": "a8988a59e2b315484879bfd885013ea578a9e2d6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/demo.py"
                    }
                },
                "size": 4588
            },
            {
                "type": "code",
                "name": "demo_darknet2onnx.py",
                "sha": "697f1972a41d3605c3f6f98655e563b307ebd579",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/demo_darknet2onnx.py"
                    }
                },
                "size": 2223
            },
            {
                "type": "code",
                "name": "demo_pytorch2onnx.py",
                "sha": "4a7a86e5e44d375af5cb6f16203ed35ad1013ec6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/demo_pytorch2onnx.py"
                    }
                },
                "size": 3456
            },
            {
                "type": "code",
                "name": "demo_tensorflow.py",
                "sha": "ab63b23c272a0e40e87cbd148130b07b3f5b661e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/demo_tensorflow.py"
                    }
                },
                "size": 2974
            },
            {
                "type": "code",
                "name": "demo_trt.py",
                "sha": "7f096639d21823922f5e889a1fac5f9c2d9e6a24",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/demo_trt.py"
                    }
                },
                "size": 7191
            },
            {
                "type": "code",
                "name": "evaluate_on_coco.py",
                "sha": "48f31c7e2861791df96a6f7c768b6dfd8c170453",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/evaluate_on_coco.py"
                    }
                },
                "size": 11814
            },
            {
                "type": "code",
                "name": "fuck_train.md",
                "sha": "66519ebd5f4f8607c94b039d7cfdaafc5fd5c2c9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/fuck_train.md"
                    }
                },
                "size": 279
            },
            {
                "type": "code",
                "name": "models.py",
                "sha": "cc4a275cc7c383c3b39aa837b75ed8dfe5e96c75",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/models.py"
                    }
                },
                "size": 17061
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "9f0ccece1778ecd35e946ea5ee604d9437f2b059",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/requirements.txt"
                    }
                },
                "size": 158
            },
            {
                "type": "code",
                "name": "tempCodeRunnerFile.py",
                "sha": "e7aa263ca6a155530718c38a2e72782bfdc6011e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/tempCodeRunnerFile.py"
                    }
                },
                "size": 11
            },
            {
                "type": "code",
                "name": "tool",
                "sha": "ad5c5cbae5e4f67a319c5792f45e2ee255a88333",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/tree/master/tool"
                    }
                },
                "num_files": 14
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "b70dc952c29698d4aaafbfd0f6baacb53409f92f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/train.py"
                    }
                },
                "size": 28098
            },
            {
                "type": "code",
                "name": "voc_annotation.py",
                "sha": "e9925e58a966c071a89fb9f1792b6163e97265f9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/WuJiang1996/pytorch-YOLOv4/blob/master/voc_annotation.py"
                    }
                },
                "size": 1238
            }
        ]
    },
    "authors": [
        {
            "name": "xiyou",
            "github_id": "WuJiang1996"
        }
    ],
    "tags": [],
    "description": "Forked from https://github.com/Tianxiaomo/pytorch-YOLOv4.git",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/WuJiang1996/pytorch-YOLOv4",
            "stars": 1,
            "issues": true,
            "readme": "# Pytorch-YOLOv4\n\n![](https://img.shields.io/static/v1?label=python&message=3.6|3.7&color=blue)\n![](https://img.shields.io/static/v1?label=pytorch&message=1.4&color=<COLOR>)\n[![](https://img.shields.io/static/v1?label=license&message=Apache2&color=green)](./License.txt)\n\nA minimal PyTorch implementation of YOLOv4.\n- Paper Yolo v4: https://arxiv.org/abs/2004.10934\n- Source code:https://github.com/AlexeyAB/darknet\n- More details: http://pjreddie.com/darknet/yolo/\n\n\n- [x] Inference\n- [x] Train\n    - [x] Mocaic\n\n```\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 dataset.py            dataset\n\u251c\u2500\u2500 demo.py               demo to run pytorch --> tool/darknet2pytorch\n\u251c\u2500\u2500 demo_darknet2onnx.py  tool to convert into onnx --> tool/darknet2pytorch\n\u251c\u2500\u2500 demo_pytorch2onnx.py  tool to convert into onnx\n\u251c\u2500\u2500 models.py             model for pytorch\n\u251c\u2500\u2500 train.py              train models.py\n\u251c\u2500\u2500 cfg.py                cfg.py for train\n\u251c\u2500\u2500 cfg                   cfg --> darknet2pytorch\n\u251c\u2500\u2500 data            \n\u251c\u2500\u2500 weight                --> darknet2pytorch\n\u251c\u2500\u2500 tool\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 camera.py           a demo camera\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 coco_annotation.py       coco dataset generator\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 darknet2pytorch.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 region_loss.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 utils.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 yolo_layer.py\n```\n\n![image](https://user-gold-cdn.xitu.io/2020/4/26/171b5a6c8b3bd513?w=768&h=576&f=jpeg&s=78882)\n\n# 0. Weights Download\n\n## 0.1 darknet\n- baidu(https://pan.baidu.com/s/1dAGEW8cm-dqK14TbhhVetA     Extraction code:dm5b)\n- google(https://drive.google.com/open?id=1cewMfusmPjYWbrnuJRuKhPMwRe_b9PaT)\n\n## 0.2 pytorch\nyou can use darknet2pytorch to convert it yourself, or download my converted model.\n\n- baidu\n    - yolov4.pth(https://pan.baidu.com/s/1ZroDvoGScDgtE1ja_QqJVw Extraction code:xrq9) \n    - yolov4.conv.137.pth(https://pan.baidu.com/s/1ovBie4YyVQQoUrC3AY0joA Extraction code:kcel)\n- google\n    - yolov4.pth(https://drive.google.com/open?id=1wv_LiFeCRYwtpkqREPeI13-gPELBDwuJ)\n    - yolov4.conv.137.pth(https://drive.google.com/open?id=1fcbR0bWzYfIEdLJPzOsn4R5mlvR6IQyA)\n\n# 1. Train\n\n[use yolov4 to train your own data](Use_yolov4_to_train_your_own_data.md)\n\n1. Download weight\n2. Transform data\n\n    For coco dataset,you can use tool/coco_annotation.py.\n    ```\n    # train.txt\n    image_path1 x1,y1,x2,y2,id x1,y1,x2,y2,id x1,y1,x2,y2,id ...\n    image_path2 x1,y1,x2,y2,id x1,y1,x2,y2,id x1,y1,x2,y2,id ...\n    ...\n    ...\n    ```\n3. Train\n\n    you can set parameters in cfg.py.\n    ```\n     python train.py -g [GPU_ID] -dir [Dataset direction] ...\n    ```\n\n# 2. Inference\n\n## 2.1 Performance on MS COCO dataset (using pretrained DarknetWeights from <https://github.com/AlexeyAB/darknet>)\n\n**ONNX and TensorRT models are converted from Pytorch (TianXiaomo): Pytorch->ONNX->TensorRT.**\nSee following sections for more details of conversions.\n\n- val2017 dataset (input size: 416x416)\n\n| Model type          | AP          | AP50        | AP75        |  APS        | APM         | APL         |\n| ------------------- | ----------: | ----------: | ----------: | ----------: | ----------: | ----------: |\n| DarkNet (YOLOv4 paper)|     0.471 |       0.710 |       0.510 |       0.278 |       0.525 |       0.636 |\n| Pytorch (TianXiaomo)|       0.466 |       0.704 |       0.505 |       0.267 |       0.524 |       0.629 |\n| TensorRT FP32 + BatchedNMSPlugin | 0.472| 0.708 |       0.511 |       0.273 |       0.530 |       0.637 |\n| TensorRT FP16 + BatchedNMSPlugin | 0.472| 0.708 |       0.511 |       0.273 |       0.530 |       0.636 |\n\n- testdev2017 dataset (input size: 416x416)\n\n| Model type          | AP          | AP50        | AP75        |  APS        | APM         | APL         |\n| ------------------- | ----------: | ----------: | ----------: | ----------: | ----------: | ----------: |\n| DarkNet (YOLOv4 paper)|     0.412 |       0.628 |       0.443 |       0.204 |       0.444 |       0.560 |\n| Pytorch (TianXiaomo)|       0.404 |       0.615 |       0.436 |       0.196 |       0.438 |       0.552 |\n| TensorRT FP32 + BatchedNMSPlugin | 0.412| 0.625 |       0.445 |       0.200 |       0.446 |       0.564 |\n| TensorRT FP16 + BatchedNMSPlugin | 0.412| 0.625 |       0.445 |       0.200 |       0.446 |       0.563 |\n\n\n## 2.2 Image input size for inference\n\nImage input size is NOT restricted in `320 * 320`, `416 * 416`, `512 * 512` and `608 * 608`.\nYou can adjust your input sizes for a different input ratio, for example: `320 * 608`.\nLarger input size could help detect smaller targets, but may be slower and GPU memory exhausting.\n\n```py\nheight = 320 + 96 * n, n in {0, 1, 2, 3, ...}\nwidth  = 320 + 96 * m, m in {0, 1, 2, 3, ...}\n```\n\n## 2.3 **Different inference options**\n\n- Load the pretrained darknet model and darknet weights to do the inference (image size is configured in cfg file already)\n\n    ```sh\n    python demo.py -cfgfile <cfgFile> -weightfile <weightFile> -imgfile <imgFile>\n    ```\n\n- Load pytorch weights (pth file) to do the inference\n\n    ```sh\n    python models.py <num_classes> <weightfile> <imgfile> <IN_IMAGE_H> <IN_IMAGE_W> <namefile(optional)>\n    ```\n    \n- Load converted ONNX file to do inference (See section 3 and 4)\n\n- Load converted TensorRT engine file to do inference (See section 5)\n\n## 2.4 Inference output\n\nThere are 2 inference outputs.\n- One is locations of bounding boxes, its shape is  `[batch, num_boxes, 1, 4]` which represents x1, y1, x2, y2 of each bounding box.\n- The other one is scores of bounding boxes which is of shape `[batch, num_boxes, num_classes]` indicating scores of all classes for each bounding box.\n\nUntil now, still a small piece of post-processing including NMS is required. We are trying to minimize time and complexity of post-processing.\n\n\n# 3. Darknet2ONNX\n\n- **This script is to convert the official pretrained darknet model into ONNX**\n\n- **Pytorch version Recommended:**\n\n    - Pytorch 1.4.0 for TensorRT 7.0 and higher\n    - Pytorch 1.5.0 and 1.6.0 for TensorRT 7.1.2 and higher\n\n- **Install onnxruntime**\n\n    ```sh\n    pip install onnxruntime\n    ```\n\n- **Run python script to generate ONNX model and run the demo**\n\n    ```sh\n    python demo_darknet2onnx.py <cfgFile> <weightFile> <imageFile> <batchSize>\n    ```\n\n## 3.1 Dynamic or static batch size\n\n- **Positive batch size will generate ONNX model of static batch size, otherwise, batch size will be dynamic**\n    - Dynamic batch size will generate only one ONNX model\n    - Static batch size will generate 2 ONNX models, one is for running the demo (batch_size=1)\n\n# 4. Pytorch2ONNX\n\n- **You can convert your trained pytorch model into ONNX using this script**\n\n- **Pytorch version Recommended:**\n\n    - Pytorch 1.4.0 for TensorRT 7.0 and higher\n    - Pytorch 1.5.0 and 1.6.0 for TensorRT 7.1.2 and higher\n\n- **Install onnxruntime**\n\n    ```sh\n    pip install onnxruntime\n    ```\n\n- **Run python script to generate ONNX model and run the demo**\n\n    ```sh\n    python demo_pytorch2onnx.py <weight_file> <image_path> <batch_size> <n_classes> <IN_IMAGE_H> <IN_IMAGE_W>\n    ```\n\n    For example:\n\n    ```sh\n    python demo_pytorch2onnx.py yolov4.pth dog.jpg 8 80 416 416\n    ```\n\n## 4.1 Dynamic or static batch size\n\n- **Positive batch size will generate ONNX model of static batch size, otherwise, batch size will be dynamic**\n    - Dynamic batch size will generate only one ONNX model\n    - Static batch size will generate 2 ONNX models, one is for running the demo (batch_size=1)\n\n\n# 5. ONNX2TensorRT\n\n- **TensorRT version Recommended: 7.0, 7.1**\n\n## 5.1 Convert from ONNX of static Batch size\n\n- **Run the following command to convert YOLOv4 ONNX model into TensorRT engine**\n\n    ```sh\n    trtexec --onnx=<onnx_file> --explicitBatch --saveEngine=<tensorRT_engine_file> --workspace=<size_in_megabytes> --fp16\n    ```\n    - Note: If you want to use int8 mode in conversion, extra int8 calibration is needed.\n\n## 5.2 Convert from ONNX of dynamic Batch size\n\n- **Run the following command to convert YOLOv4 ONNX model into TensorRT engine**\n\n    ```sh\n    trtexec --onnx=<onnx_file> \\\n    --minShapes=input:<shape_of_min_batch> --optShapes=input:<shape_of_opt_batch> --maxShapes=input:<shape_of_max_batch> \\\n    --workspace=<size_in_megabytes> --saveEngine=<engine_file> --fp16\n    ```\n- For example:\n\n    ```sh\n    trtexec --onnx=yolov4_-1_3_320_512_dynamic.onnx \\\n    --minShapes=input:1x3x320x512 --optShapes=input:4x3x320x512 --maxShapes=input:8x3x320x512 \\\n    --workspace=2048 --saveEngine=yolov4_-1_3_320_512_dynamic.engine --fp16\n    ```\n\n## 5.3 Run the demo\n\n```sh\npython demo_trt.py <tensorRT_engine_file> <input_image> <input_H> <input_W>\n```\n\n- This demo here only works when batchSize is dynamic (1 should be within dynamic range) or batchSize=1, but you can update this demo a little for other dynamic or static batch sizes.\n    \n- Note1: input_H and input_W should agree with the input size in the original ONNX file.\n    \n- Note2: extra NMS operations are needed for the tensorRT output. This demo uses python NMS code from `tool/utils.py`.\n\n\n# 6. ONNX2Tensorflow\n\n- **First:Conversion to ONNX**\n\n    tensorflow >=2.0\n    \n    1: Thanks:github:https://github.com/onnx/onnx-tensorflow\n    \n    2: Run git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow\n    Run pip install -e .\n    \n    Note:Errors will occur when using \"pip install onnx-tf\", at least for me,it is recommended to use source code installation\n\n# 7. ONNX2TensorRT and DeepStream Inference\n  \n  1. Compile the DeepStream Nvinfer Plugin \n  \n  ```\n      cd DeepStream\n      make \n  ```\n  2. Build a TRT Engine.\n  \n   For single batch, \n   ```\n   trtexec --onnx=<onnx_file> --explicitBatch --saveEngine=<tensorRT_engine_file> --workspace=<size_in_megabytes> --fp16\n   ```\n   \n   For multi-batch, \n  ```\n  trtexec --onnx=<onnx_file> --explicitBatch --shapes=input:Xx3xHxW --optShapes=input:Xx3xHxW --maxShapes=input:Xx3xHxW --minShape=input:1x3xHxW --saveEngine=<tensorRT_engine_file> --fp16\n  ```\n  \n  Note :The maxShapes could not be larger than model original shape.\n  \n  3. Write the deepstream config file for the TRT Engine.\n  \n  \n   \nReference:\n- https://github.com/eriklindernoren/PyTorch-YOLOv3\n- https://github.com/marvis/pytorch-caffe-darknet-convert\n- https://github.com/marvis/pytorch-yolo3\n\n```\n@article{yolov4,\n  title={YOLOv4: YOLOv4: Optimal Speed and Accuracy of Object Detection},\n  author={Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao},\n  journal = {arXiv},\n  year={2020}\n}\n```\n",
            "readme_url": "https://github.com/WuJiang1996/pytorch-YOLOv4",
            "frameworks": [
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "YOLOv4: Optimal Speed and Accuracy of Object Detection",
            "arxiv": "2004.10934",
            "year": 2020,
            "url": "http://arxiv.org/abs/2004.10934v1",
            "abstract": "There are a huge number of features which are said to improve Convolutional\nNeural Network (CNN) accuracy. Practical testing of combinations of such\nfeatures on large datasets, and theoretical justification of the result, is\nrequired. Some features operate on certain models exclusively and for certain\nproblems exclusively, or only for small-scale datasets; while some features,\nsuch as batch-normalization and residual-connections, are applicable to the\nmajority of models, tasks, and datasets. We assume that such universal features\ninclude Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections\n(CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT)\nand Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation,\nMosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and\ncombine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50)\nfor the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100. Source\ncode is at https://github.com/AlexeyAB/darknet",
            "authors": [
                "Alexey Bochkovskiy",
                "Chien-Yao Wang",
                "Hong-Yuan Mark Liao"
            ]
        },
        {
            "year": "2020",
            "journal": "arXiv",
            "author": [
                "Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao"
            ],
            "title": "YOLOv4: YOLOv4: Optimal Speed and Accuracy of Object Detection",
            "ENTRYTYPE": "article",
            "ID": "yolov4",
            "authors": [
                "Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.999638461780186,
        "task": "Object Detection",
        "task_prob": 0.9594282563433176
    }
}