{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "pix2pix",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "tianhai123",
                "owner_type": "User",
                "name": "pix2pix",
                "url": "https://github.com/tianhai123/pix2pix",
                "stars": 0,
                "pushed_at": "2019-08-13 09:48:03+00:00",
                "created_at": "2019-08-13 09:47:39+00:00",
                "language": "Lua",
                "license": "Other",
                "frameworks": [
                    "Caffe"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "7ee957e55af3dee88ed25c40899a334134f8061d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/blob/master/.gitignore"
                    }
                },
                "size": 341
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "b1b718141d765da9a713ce279ba1f6bf4e424ea6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/blob/master/LICENSE"
                    }
                },
                "size": 2896
            },
            {
                "type": "code",
                "name": "data",
                "sha": "0b6954f04540b7ecae5fbd2b15565732084a7b38",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/tree/master/data"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "datasets",
                "sha": "a1169d3ce2715aff590caf54769e4d8d8d8cccbb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/tree/master/datasets"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "imgs",
                "sha": "7f6373df3e794847fb45efdacc1635d82c358233",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/tree/master/imgs"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "models.lua",
                "sha": "fb0849c285b695e2b6ae9237e9549189289e5859",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/blob/master/models.lua"
                    }
                },
                "size": 11458
            },
            {
                "type": "code",
                "name": "models",
                "sha": "687767e3d15b8b055c6d673295055b2ebe21f887",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/tree/master/models"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "37d90d8e20897ce9d5b19e421d644d0c88194d41",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/tree/master/scripts"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "test.lua",
                "sha": "24b893047953eaba4fe2e68c3168edeaea9e7635",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/blob/master/test.lua"
                    }
                },
                "size": 6792
            },
            {
                "type": "code",
                "name": "train.lua",
                "sha": "721beed8a6f0459301febebe476f1ac10ede8931",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/blob/master/train.lua"
                    }
                },
                "size": 17550
            },
            {
                "type": "code",
                "name": "util",
                "sha": "9aa2b9c904892c59447ef59058d47d343551e2e8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tianhai123/pix2pix/tree/master/util"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Phillip Isola",
            "email": "phillip.isola@gmail.com",
            "github_id": "phillipi"
        },
        {
            "name": "Jun-Yan Zhu",
            "github_id": "junyanz"
        },
        {
            "name": "Tinghui Zhou",
            "email": "tinghuiz@eecs.berkeley.edu",
            "github_id": "tinghuiz"
        },
        {
            "name": "Brannon Dorsey",
            "email": "brannon@brannondorsey.com",
            "github_id": "brannondorsey"
        },
        {
            "name": "Hungryof",
            "github_id": "Naruto-Sasuke"
        },
        {
            "name": "Ajay Chainani",
            "email": "ajay.chainani@gmail.com",
            "github_id": "achainan"
        },
        {
            "name": "Dixing (Dex) Xu",
            "email": "dixingxu@gmail.com",
            "github_id": "dexhunter"
        },
        {
            "name": "Favyen Bastani",
            "github_id": "uakfdotb"
        },
        {
            "name": "Kyle McDonald",
            "github_id": "kylemcdonald"
        },
        {
            "name": "Yulong Wang",
            "github_id": "Ag2S1"
        },
        {
            "name": "seedgou",
            "email": "i@zczc.cz",
            "github_id": "rwv"
        },
        {
            "name": "Arthur Qiu",
            "github_id": "arthur-qiu"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/tianhai123/pix2pix",
            "stars": 0,
            "issues": true,
            "readme": "\n# pix2pix\n[Project](https://phillipi.github.io/pix2pix/) | [Arxiv](https://arxiv.org/abs/1611.07004) | \n[PyTorch](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n\nTorch implementation for learning a mapping from input images to output images, for example:\n\n<img src=\"imgs/examples.jpg\" width=\"900px\"/>\n\nImage-to-Image Translation with Conditional Adversarial Networks  \n [Phillip Isola](http://web.mit.edu/phillipi/), [Jun-Yan Zhu](https://people.eecs.berkeley.edu/~junyanz/), [Tinghui Zhou](https://people.eecs.berkeley.edu/~tinghuiz/), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros/)   \n CVPR, 2017.\n\nOn some tasks, decent results can be obtained fairly quickly and on small datasets. For example, to learn to generate facades (example shown above), we trained on just 400 images for about 2 hours (on a single Pascal Titan X GPU). However, for harder problems it may be important to train on far larger datasets, and for many hours or even days.\n\n**Note**: Please check out our [PyTorch](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) implementation for pix2pix and CycleGAN. The PyTorch version is under active development and can produce results comparable to or better than this Torch version.\n\n## Setup\n\n### Prerequisites\n- Linux or OSX\n- NVIDIA GPU + CUDA CuDNN (CPU mode and CUDA without CuDNN may work with minimal modification, but untested)\n\n### Getting Started\n- Install torch and dependencies from https://github.com/torch/distro\n- Install torch packages `nngraph` and `display`\n```bash\nluarocks install nngraph\nluarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec\n```\n- Clone this repo:\n```bash\ngit clone git@github.com:phillipi/pix2pix.git\ncd pix2pix\n```\n- Download the dataset (e.g., [CMP Facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)):\n```bash\nbash ./datasets/download_dataset.sh facades\n```\n- Train the model\n```bash\nDATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA th train.lua\n```\n- (CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables ```gpu=0 cudnn=0``` forces CPU only\n```bash\nDATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua\n```\n- (Optionally) start the display server to view results as the model trains. ( See [Display UI](#display-ui) for more details):\n```bash\nth -ldisplay.start 8000 0.0.0.0\n```\n\n- Finally, test the model:\n```bash\nDATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA phase=val th test.lua\n```\nThe test results will be saved to an html file here: `./results/facades_generation/latest_net_G_val/index.html`.\n\n## Train\n```bash\nDATA_ROOT=/path/to/data/ name=expt_name which_direction=AtoB th train.lua\n```\nSwitch `AtoB` to `BtoA` to train translation in opposite direction.\n\nModels are saved to `./checkpoints/expt_name` (can be changed by passing `checkpoint_dir=your_dir` in train.lua).\n\nSee `opt` in train.lua for additional training options.\n\n## Test\n```bash\nDATA_ROOT=/path/to/data/ name=expt_name which_direction=AtoB phase=val th test.lua\n```\n\nThis will run the model named `expt_name` in direction `AtoB` on all images in `/path/to/data/val`.\n\nResult images, and a webpage to view them, are saved to `./results/expt_name` (can be changed by passing `results_dir=your_dir` in test.lua).\n\nSee `opt` in test.lua for additional testing options.\n\n\n## Datasets\nDownload the datasets using the following script. Some of the datasets are collected by other researchers. Please cite their papers if you use the data.\n```bash\nbash ./datasets/download_dataset.sh dataset_name\n```\n- `facades`: 400 images from [CMP Facades dataset](http://cmp.felk.cvut.cz/~tylecr1/facade/). [[Citation](datasets/bibtex/facades.tex)]\n- `cityscapes`: 2975 images from the [Cityscapes training set](https://www.cityscapes-dataset.com/).  [[Citation](datasets/bibtex/cityscapes.tex)]\n- `maps`: 1096 training images scraped from Google Maps\n- `edges2shoes`: 50k training images from [UT Zappos50K dataset](http://vision.cs.utexas.edu/projects/finegrained/utzap50k/). Edges are computed by [HED](https://github.com/s9xie/hed) edge detector + post-processing.\n[[Citation](datasets/bibtex/shoes.tex)]\n- `edges2handbags`: 137K Amazon Handbag images from [iGAN project](https://github.com/junyanz/iGAN). Edges are computed by [HED](https://github.com/s9xie/hed) edge detector + post-processing. [[Citation](datasets/bibtex/handbags.tex)]\n- `night2day`: around 20K natural scene images from  [Transient Attributes dataset](http://transattr.cs.brown.edu/) [[Citation](datasets/bibtex/transattr.tex)]. To train a `day2night` pix2pix model, you need to add `which_direction=BtoA`.\n\n## Models\nDownload the pre-trained models with the following script. You need to rename the model (e.g., `facades_label2image` to `/checkpoints/facades/latest_net_G.t7`) after the download has finished.\n```bash\nbash ./models/download_model.sh model_name\n```\n- `facades_label2image` (label -> facade): trained on the CMP Facades dataset.\n- `cityscapes_label2image` (label -> street scene): trained on the Cityscapes dataset.\n- `cityscapes_image2label` (street scene -> label): trained on the Cityscapes dataset.\n- `edges2shoes` (edge -> photo): trained on UT Zappos50K dataset.\n- `edges2handbags` (edge -> photo): trained on Amazon handbags images.\n- `day2night` (daytime scene -> nighttime scene): trained on around 100 [webcams](http://transattr.cs.brown.edu/).\n\n## Setup Training and Test data\n### Generating Pairs\nWe provide a python script to generate training data in the form of pairs of images {A,B}, where A and B are two different depictions of the same underlying scene. For example, these might be pairs {label map, photo} or {bw image, color image}. Then we can learn to translate A to B or B to A:\n\nCreate folder `/path/to/data` with subfolders `A` and `B`. `A` and `B` should each have their own subfolders `train`, `val`, `test`, etc. In `/path/to/data/A/train`, put training images in style A. In `/path/to/data/B/train`, put the corresponding images in style B. Repeat same for other data splits (`val`, `test`, etc).\n\nCorresponding images in a pair {A,B} must be the same size and have the same filename, e.g., `/path/to/data/A/train/1.jpg` is considered to correspond to `/path/to/data/B/train/1.jpg`.\n\nOnce the data is formatted this way, call:\n```bash\npython scripts/combine_A_and_B.py --fold_A /path/to/data/A --fold_B /path/to/data/B --fold_AB /path/to/data\n```\n\nThis will combine each pair of images (A,B) into a single image file, ready for training.\n\n### Notes on Colorization\nNo need to run `combine_A_and_B.py` for colorization. Instead, you need to prepare some natural images and set `preprocess=colorization` in the script. The program will automatically convert each RGB image into Lab color space, and create  `L -> ab` image pair during the training. Also set `input_nc=1` and `output_nc=2`.\n\n### Extracting Edges\nWe provide python and Matlab scripts to extract coarse edges from photos. Run `scripts/edges/batch_hed.py` to compute [HED](https://github.com/s9xie/hed) edges. Run `scripts/edges/PostprocessHED.m` to simplify edges with additional post-processing steps. Check the code documentation for more details.\n\n### Evaluating Labels2Photos on Cityscapes\nWe provide scripts for running the evaluation of the Labels2Photos task on the Cityscapes **validation** set. We assume that you have installed `caffe` (and `pycaffe`) in your system. If not, see the [official website](http://caffe.berkeleyvision.org/installation.html) for installation instructions. Once `caffe` is successfully installed, download the pre-trained FCN-8s semantic segmentation model (512MB) by running\n```bash\nbash ./scripts/eval_cityscapes/download_fcn8s.sh\n```\nThen make sure `./scripts/eval_cityscapes/` is in your system's python path. If not, run the following command to add it\n```bash\nexport PYTHONPATH=${PYTHONPATH}:./scripts/eval_cityscapes/\n```\nNow you can run the following command to evaluate your predictions:\n```bash\npython ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/\n```\nImages stored under `--result_dir` should contain your model predictions on the Cityscapes **validation** split, and have the original Cityscapes naming convention (e.g., `frankfurt_000001_038418_leftImg8bit.png`). The script will output a text file under `--output_dir` containing the metric.\n\n**Further notes**: The pre-trained model is **not** supposed to work on Cityscapes in the original resolution (1024x2048) as it was trained on 256x256 images that are upsampled to 1024x2048. The purpose of the resizing was to 1) keep the label maps in the original high resolution untouched and 2) avoid the need of changing the standard FCN training code for Cityscapes. To get the *ground-truth* numbers in the paper, you need to resize the original Cityscapes images to 256x256 before running the evaluation code.\n\n## Display UI\nOptionally, for displaying images during training and test, use the [display package](https://github.com/szym/display).\n\n- Install it with: `luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec`\n- Then start the server with: `th -ldisplay.start`\n- Open this URL in your browser: [http://localhost:8000](http://localhost:8000)\n\nBy default, the server listens on localhost. Pass `0.0.0.0` to allow external connections on any interface:\n```bash\nth -ldisplay.start 8000 0.0.0.0\n```\nThen open `http://(hostname):(port)/` in your browser to load the remote desktop.\n\nL1 error is plotted to the display by default. Set the environment variable `display_plot` to a comma-separated list of values `errL1`, `errG` and `errD` to visualize the L1, generator, and discriminator error respectively. For example, to plot only the generator and discriminator errors to the display instead of the default L1 error, set `display_plot=\"errG,errD\"`.\n\n## Citation\nIf you use this code for your research, please cite our paper <a href=\"https://arxiv.org/pdf/1611.07004v1.pdf\">Image-to-Image Translation Using Conditional Adversarial Networks</a>:\n\n```\n@article{pix2pix2017,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  journal={CVPR},\n  year={2017}\n}\n```\n\n## Cat Paper Collection\nIf you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper Collection:  \n[[Github]](https://github.com/junyanz/CatPapers) [[Webpage]](http://people.eecs.berkeley.edu/~junyanz/cat/cat_papers.html)\n\n## Acknowledgments\nCode borrows heavily from [DCGAN](https://github.com/soumith/dcgan.torch). The data loader is modified from [DCGAN](https://github.com/soumith/dcgan.torch) and  [Context-Encoder](https://github.com/pathak22/context-encoder).\n",
            "readme_url": "https://github.com/tianhai123/pix2pix",
            "frameworks": [
                "Caffe"
            ]
        }
    ],
    "references": [
        {
            "title": "Image-to-Image Translation with Conditional Adversarial Networks",
            "arxiv": "1611.07004",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.07004v3",
            "abstract": "We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either.",
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A. Efros"
            ]
        },
        {
            "year": "2017",
            "journal": "CVPR",
            "author": [
                "Isola, Phillip",
                "Zhu, Jun-Yan",
                "Zhou, Tinghui",
                "Efros, Alexei A"
            ],
            "title": "Image-to-Image Translation with Conditional Adversarial Networks",
            "ENTRYTYPE": "article",
            "ID": "pix2pix2017",
            "authors": [
                "Isola, Phillip",
                "Zhu, Jun-Yan",
                "Zhou, Tinghui",
                "Efros, Alexei A"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "CMP Facades dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://cmp.felk.cvut.cz/~tylecr1/facade/"
                    }
                }
            },
            {
                "name": "UT Zappos50K dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://vision.cs.utexas.edu/projects/finegrained/utzap50k/"
                    }
                }
            },
            {
                "name": "Transient Attributes dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://transattr.cs.brown.edu/"
                    }
                }
            },
            {
                "name": "Amazon"
            },
            {
                "name": "Cityscapes"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999960536602964,
        "task": "Image-to-Image Translation",
        "task_prob": 0.9892306179945588
    }
}