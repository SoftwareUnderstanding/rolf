{
    "visibility": {
        "visibility": "public"
    },
    "name": "Pretrained Language Model",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "huawei-noah",
                "owner_type": "Organization",
                "name": "Pretrained-Language-Model",
                "url": "https://github.com/huawei-noah/Pretrained-Language-Model",
                "stars": 2175,
                "pushed_at": "2022-03-16 02:24:52+00:00",
                "created_at": "2019-12-02 14:26:04+00:00",
                "language": "Python",
                "description": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab.",
                "frameworks": [
                    "scikit-learn",
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "AutoTinyBERT",
                "sha": "d8ef03741bc02cef816fc8182ae6c1cc33b540b9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/AutoTinyBERT"
                    }
                },
                "num_files": 13
            },
            {
                "type": "code",
                "name": "BBPE",
                "sha": "c32b31540266ab91ef1fa1ed3658019173d7c8ab",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BBPE"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "BinaryBERT",
                "sha": "c382dc940a97ac8c7af66fe511521a8bf43d9a83",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BinaryBERT"
                    }
                },
                "num_files": 15
            },
            {
                "type": "code",
                "name": "DynaBERT",
                "sha": "861d6091e7f7d14ee53176f9e63f072293e45c4d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "HyperText",
                "sha": "55b8ef143aa78934ff68240f34dd885aa7fae68d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/HyperText"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "JABER-PyTorch",
                "sha": "bf4a5781f38015c4bfc5c6a4f1da2b0e8abed35b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch"
                    }
                },
                "num_files": 15
            },
            {
                "type": "code",
                "name": "NEZHA-Gen-TensorFlow",
                "sha": "6deb0be0fa118d57f12adb2d5f4284052dcd28af",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "NEZHA-PyTorch",
                "sha": "2b2bff0ce05c6295d6cafd8b690a47d8c06bbe57",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-PyTorch"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "NEZHA-TensorFlow",
                "sha": "0bf9e8989b2e351ef16228d9f0f05396350e1a37",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow"
                    }
                },
                "num_files": 32
            },
            {
                "type": "code",
                "name": "PMLM",
                "sha": "d328ca8052df016ed9436fdb9c54e1b55a5cd3e2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PMLM"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "PanGu-\u03b1",
                "sha": "5917b44e8fc1765b414a4fd46f9483c1091a0337",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-\u03b1"
                    }
                },
                "num_files": 20
            },
            {
                "type": "code",
                "name": "TernaryBERT-MindSpore",
                "sha": "b2e10ef7570367c6c32cafd19de8acb6aaad64f9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TernaryBERT-MindSpore"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "TernaryBERT",
                "sha": "4c510a007a6d2d91f97b0e3a242011921ff8118c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TernaryBERT"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "TinyBERT-MindSpore",
                "sha": "0866d21ce3bf0c84071f873e72c2fae4c1e6b59e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT-MindSpore"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "TinyBERT",
                "sha": "d2fc77b42b6219c91c6dfa72686b34be87d7bb5f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT"
                    }
                },
                "num_files": 10
            }
        ]
    },
    "authors": [
        {
            "name": "jacobrxz",
            "github_id": "jacobrxz"
        },
        {
            "name": "Victor",
            "email": "wjqjsnj@gmail.com",
            "github_id": "ItachiUchihaVictor"
        },
        {
            "name": "Xin Jiang",
            "github_id": "jxfeb"
        },
        {
            "name": "zbravo",
            "github_id": "zbravo"
        },
        {
            "name": "itsucks",
            "github_id": "itsucks"
        },
        {
            "name": "Yichun Yin",
            "email": "yichunyin@pku.edu.cn",
            "github_id": "zwjyyc"
        },
        {
            "name": "leoeaton",
            "github_id": "leoeaton"
        },
        {
            "name": "ghaddarAbs",
            "github_id": "ghaddarAbs"
        },
        {
            "name": "Jiaxin Wen",
            "email": "wenjx17@mails.tsinghua.edu.cn",
            "github_id": "xwwwwww"
        },
        {
            "name": "Gowtham.R",
            "email": "gowtham.ramesh1@gmail.com",
            "github_id": "gowtham1997"
        },
        {
            "name": "Ilya Sirotkin",
            "email": "ilsir95@gmail.com",
            "github_id": "sirily"
        },
        {
            "name": "xuqiongkai",
            "github_id": "xuqiongkai"
        },
        {
            "name": "zyy-g",
            "github_id": "zyy-g"
        }
    ],
    "tags": [
        "knowledge-distillation",
        "model-compression",
        "quantization",
        "pretrained-models",
        "large-scale-distributed"
    ],
    "description": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model",
            "stars": 2175,
            "issues": true,
            "readme": "# Pretrained Language Model\n\nThis repository provides the latest pretrained language models and its related optimization techniques developed by Huawei Noah's Ark Lab.\n\n## Directory structure\n* [PanGu-\u03b1](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-\u03b1) is a Large-scale autoregressive pretrained Chinese language model with up to 200B parameter. The models are developed under the [MindSpore](https://www.mindspore.cn/en) and trained on a cluster of [Ascend](https://e.huawei.com/en/products/servers/ascend) 910 AI processors.\n* [NEZHA-TensorFlow](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow) is a pretrained Chinese language model which achieves the state-of-the-art performances on several Chinese NLP tasks developed under TensorFlow.\n* [NEZHA-PyTorch](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-PyTorch) is the PyTorch version of NEZHA.\n* [NEZHA-Gen-TensorFlow](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow) provides two GPT models. One is Yuefu (\u4e50\u5e9c), a Chinese Classical Poetry generation model, the other is a common Chinese GPT model.\n* [TinyBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT) is a compressed BERT model which achieves 7.5x smaller and 9.4x faster on inference.\n* [TinyBERT-MindSpore](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT-MindSpore) is a MindSpore version of TinyBERT.\n* [DynaBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT) is a dynamic BERT model with adaptive width and depth.\n* [BBPE](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BBPE) provides a byte-level vocabulary building tool and its correspoinding tokenizer.\n* [PMLM](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PMLM) is a probabilistically masked language model. Trained without the complex two-stream self-attention, PMLM can be treated as a simple approximation of XLNet.\n* [TernaryBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TernaryBERT) is a weights ternarization method for BERT model developed under PyTorch.\n* [TernaryBERT-MindSpore](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TernaryBERT-MindSpore) is the MindSpore version of TernaryBERT.\n* [HyperText](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/HyperText) is an efficient text classification model based on hyperbolic geometry theories.\n* [BinaryBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BinaryBERT) is a weights binarization method using ternary weight splitting for BERT model, developed under PyTorch.\n* [AutoTinyBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/AutoTinyBERT) provides a model zoo that can meet different latency requirements.\n",
            "readme_url": "https://github.com/huawei-noah/Pretrained-Language-Model",
            "frameworks": [
                "scikit-learn",
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "PanGu-\u03b1",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-\u03b1"
        },
        {
            "title": "NEZHA-TensorFlow",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow"
        },
        {
            "title": "NEZHA-PyTorch",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-PyTorch"
        },
        {
            "title": "NEZHA-Gen-TensorFlow",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow"
        },
        {
            "title": "TinyBERT",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT"
        },
        {
            "title": "TinyBERT-MindSpore",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT-MindSpore"
        },
        {
            "title": "DynaBERT",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT"
        },
        {
            "title": "BBPE",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BBPE"
        },
        {
            "title": "PMLM",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PMLM"
        },
        {
            "title": "TernaryBERT",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TernaryBERT"
        },
        {
            "title": "TernaryBERT-MindSpore",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TernaryBERT-MindSpore"
        },
        {
            "title": "HyperText",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/HyperText"
        },
        {
            "title": "BinaryBERT",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BinaryBERT"
        },
        {
            "title": "AutoTinyBERT",
            "url": "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/AutoTinyBERT"
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9997063596885398,
        "task": "Question Answering",
        "task_prob": 0.46100642005187653
    }
}