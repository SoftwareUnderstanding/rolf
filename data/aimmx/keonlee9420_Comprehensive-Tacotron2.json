{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Comprehensive Tacotron2 - PyTorch Implementation",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "keonlee9420",
                "owner_type": "User",
                "name": "Comprehensive-Tacotron2",
                "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2",
                "stars": 23,
                "pushed_at": "2022-02-20 14:46:27+00:00",
                "created_at": "2021-07-24 03:36:08+00:00",
                "language": "Python",
                "description": "PyTorch Implementation of Google's Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. This implementation supports both single-, multi-speaker TTS and several techniques to enforce the robustness and efficiency of the model.",
                "license": "MIT License",
                "frameworks": [
                    "scikit-learn",
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "d9ab917fc4e23aaf529542f7c100af7c5f03ca69",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/blob/main/.gitignore"
                    }
                },
                "size": 1388
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "7611f8b964b259c1c17c4a1d3063ab78b1728a08",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/blob/main/LICENSE"
                    }
                },
                "size": 1065
            },
            {
                "type": "code",
                "name": "audio",
                "sha": "e612f0c51413ca77ef70821e34dae64bcc926aeb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/audio"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "config",
                "sha": "3d2ef156121bdee2f24ff14bc9e8022aafb58fde",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/config"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "dataset.py",
                "sha": "bfc33d159dd014031419fc61a91e4ccab0a8b679",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/blob/main/dataset.py"
                    }
                },
                "size": 6992
            },
            {
                "type": "code",
                "name": "deepspeaker",
                "sha": "9ad4b2ff4e5bd38e4d0564d40ddafec4c32ec37e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/deepspeaker"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "demo",
                "sha": "ba1b21b3a5361275228a2d953b6c7faa5c494b39",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/demo"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "evaluate.py",
                "sha": "7d9d34c927ebdcf5f745a0a4569a6b63d00241d8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/blob/main/evaluate.py"
                    }
                },
                "size": 2923
            },
            {
                "type": "code",
                "name": "hifigan",
                "sha": "9a91678a677b26502da0b60ef7428e306385d7ad",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/hifigan"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "img",
                "sha": "9f225d1d0bdc9217bb94c23299316bd1b0c79a74",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/img"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "model",
                "sha": "69d0eb7c4f9b1439996f88691d0ca85811deb089",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/model"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "preprocess.py",
                "sha": "d6a53884fddf97d075c2264cc801f41dff0d4b95",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/blob/main/preprocess.py"
                    }
                },
                "size": 640
            },
            {
                "type": "code",
                "name": "preprocessed_data",
                "sha": "e40050d75ca4d3e346620950b63f4a738945ece9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/preprocessed_data"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "preprocessor",
                "sha": "4f0a298855a388fa5156eb1cf4de18a53e4d43cb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/preprocessor"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "a7fdc72674af20f2c8891a7b094e5de9e71ea8bb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/blob/main/requirements.txt"
                    }
                },
                "size": 270
            },
            {
                "type": "code",
                "name": "synthesize.py",
                "sha": "d02da0d71a22c4b193ba21e28371a0a15d491fc1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/blob/main/synthesize.py"
                    }
                },
                "size": 4960
            },
            {
                "type": "code",
                "name": "text",
                "sha": "89c1ca867a20ac4820ffbf89772c475b6a1e2e76",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/text"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "9f89ee2c2eecb2777acb6e8b68503925009ccf42",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/blob/main/train.py"
                    }
                },
                "size": 7340
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "87737de032a15cecb33450fb49cac59d0d0a6b21",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/tree/main/utils"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Keon Lee",
            "email": "keonlee9420@gmail.com",
            "github_id": "keonlee9420"
        }
    ],
    "tags": [
        "text-to-speech",
        "tts",
        "tacotron",
        "tacotron2",
        "pytorch",
        "speech-synthesis",
        "autoregressive",
        "single-speaker",
        "multi-speaker",
        "robustness",
        "efficiency",
        "comprehensive",
        "neural-tts",
        "mel-gan",
        "hifi-gan",
        "reduction-factor",
        "diagonal-guided-attention",
        "deep-learning"
    ],
    "description": "PyTorch Implementation of Google's Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. This implementation supports both single-, multi-speaker TTS and several techniques to enforce the robustness and efficiency of the model.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/keonlee9420/Comprehensive-Tacotron2",
            "stars": 23,
            "issues": true,
            "readme": "# Comprehensive Tacotron2 - PyTorch Implementation\n\nPyTorch Implementation of Google's [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884). Unlike many previous implementations, this is kind of a **`Comprehensive Tacotron2`** where the model supports both single-, multi-speaker TTS and several techniques such as reduction factor to enforce the robustness of the decoder alignment. The model can learn alignment only in `5k`.\n\n<p align=\"center\">\n    <img src=\"img/model.png\" width=\"80%\">\n</p>\n\nThe validation logs up to 70K of synthesized mel and alignment are shown below (LJSpeech_val_LJ038-0050 and VCTK_val_p323_008 from top to bottom).\n\n<p align=\"center\">\n    <img src=\"./img/LJSpeech_val_LJ038-0050.gif\" width=\"80%\">\n</p>\n<p align=\"center\">\n    <img src=\"./img/VCTK_val_p323_008.gif\" width=\"80%\">\n</p>\n\n# Quickstart\n\n## Dependencies\nYou can install the Python dependencies with\n```\npip3 install -r requirements.txt\n```\n\n## Inference\n\nYou have to download the [pretrained models](https://drive.google.com/drive/folders/1fGhmSVnUzdS_8i_-7cX6pfaSIQ0oiYBM?usp=sharing) and put them in `output/ckpt/LJSpeech/` or `output/ckpt/VCTK/`.\n\nFor a **single-speaker TTS**, run\n```\npython3 synthesize.py --text \"YOUR_DESIRED_TEXT\" --restore_step RESTORE_STEP --mode single --dataset LJSpeech\n```\n\nFor a **multi-speaker TTS**, run\n```\npython3 synthesize.py --text \"YOUR_DESIRED_TEXT\" --speaker_id SPEAKER_ID --restore_step RESTORE_STEP --mode single --dataset VCTK\n```\n\nThe generated utterances will be put in `output/result/`.\n\n\n## Batch Inference\nBatch inference is also supported, try\n\n```\npython3 synthesize.py --source preprocessed_data/LJSpeech/val.txt --restore_step RESTORE_STEP --mode batch --dataset LJSpeech\n```\nto synthesize all utterances in `preprocessed_data/LJSpeech/val.txt`. You can replace `LJSpeech` with `VCTK`. Note that only **1 batch size** is supported currently due to the autoregressive model architecture.\n\n# Training\n\n## Datasets\n\nThe supported datasets are\n\n- [LJSpeech](https://keithito.com/LJ-Speech-Dataset/): a **single-speaker TTS** English dataset consists of 13100 short audio clips of a female speaker reading passages from 7 non-fiction books, approximately 24 hours in total.\n- [VCTK](https://datashare.ed.ac.uk/handle/10283/3443): The CSTR VCTK Corpus includes speech data uttered by 110 English speakers (**multi-speaker TTS** ) with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive.\n- Any of both **single-speaker TTS** dataset (e.g., [Blizzard Challenge 2013](https://www.synsig.org/index.php/Blizzard_Challenge_2013)) and **multi-speaker TTS** dataset (e.g., [LibriTTS](https://openslr.org/60/)) can be added following LJSpeech and VCTK, respectively.\n\n## Preprocessing\n\n- For a **multi-speaker TTS** with external speaker embedder, download [ResCNN Softmax+Triplet pretrained model](https://drive.google.com/file/d/1F9NvdrarWZNktdX9KlRYWWHDwRkip_aP) of [philipperemy's DeepSpeaker](https://github.com/philipperemy/deep-speaker) for the speaker embedding and locate it in `./deepspeaker/pretrained_models/`.\n\n- Run the preprocessing script\n  ```\n  python3 preprocess.py --dataset DATASET\n  ```\n\n## Training\n\nTrain your model with\n```\npython3 train.py --dataset DATASET\n```\n\n# TensorBoard\n\nUse\n```\ntensorboard --logdir output/log\n```\n\nto serve TensorBoard on your localhost.\n<!-- The loss curves, synthesized mel-spectrograms, and audios are shown.\n\n![](./img/tensorboard_loss.png)\n![](./img/tensorboard_spec.png)\n![](./img/tensorboard_audio.png) -->\n\n# Implementation Issues\n\n- Support `n_frames_per_step>1` mode (which is not supported by [NVIDIA's tacotron2](https://github.com/NVIDIA/tacotron2)). This is the key factor to get the robustness of the decoder alignment as described in the paper. Also, it reduces the training & inference time by the factor time.\n- The current implementation provides pre-trained model of `n_frames_per_step==2`, but it should also work for any number greater than 2.\n- Add [espnet's implementation](https://github.com/espnet/espnet/blob/e962a3c609ad535cd7fb9649f9f9e9e0a2a27291/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py#L25) of [diagonal guided attention loss](https://arxiv.org/abs/1710.08969) to force diagonal alignment in the decoder attention module. You can toggle it by setting config.\n- Two options for embedding for the **multi-speaker TTS** setting: training speaker embedder from scratch or using a pre-trained [philipperemy's DeepSpeaker](https://github.com/philipperemy/deep-speaker) model (as [STYLER](https://github.com/keonlee9420/STYLER) did). You can toggle it by setting the config (between `'none'` and `'DeepSpeaker'`).\n- DeepSpeaker on VCTK dataset shows clear identification among speakers. The following figure shows the T-SNE plot of extracted speaker embedding.\n\n<p align=\"center\">\n    <img src=\"./preprocessed_data/VCTK/spker_embed_tsne.png\" width=\"80%\">\n</p>\n\n- For the vocoder, the current implementation supports **HiFi-GAN** and **MelGAN**, which are much better than **WaveNet**.\n- Currently, `fp16_run` mode is not supported.\n\n# Citation\n\n```\n@misc{lee2021comprehensive-tacotron2,\n  author = {Lee, Keon},\n  title = {Comprehensive-Tacotron2},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/keonlee9420/Comprehensive-Tacotron2}}\n}\n```\n\n# References\n- [NVIDIA's tacotron2](https://github.com/NVIDIA/tacotron2)\n- [keonlee9420's tacotron2_MMI](https://github.com/keonlee9420/tacotron2_MMI)\n- [keonlee9420's STYLER](https://github.com/keonlee9420/STYLER)\n- [philipperemy's DeepSpeaker](https://github.com/philipperemy/deep-speaker)\n- [espnet's diagonal guided attention loss](https://github.com/espnet/espnet/blob/e962a3c609ad535cd7fb9649f9f9e9e0a2a27291/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py#L25)\n- [Zero-Shot Multi-Speaker Text-To-Speech with State-of-the-art Neural Speaker Embeddings](https://arxiv.org/abs/1910.10838)\n",
            "readme_url": "https://github.com/keonlee9420/Comprehensive-Tacotron2",
            "frameworks": [
                "scikit-learn",
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention",
            "arxiv": "1710.08969",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.08969v2",
            "abstract": "This paper describes a novel text-to-speech (TTS) technique based on deep\nconvolutional neural networks (CNN), without use of any recurrent units.\nRecurrent neural networks (RNN) have become a standard technique to model\nsequential data recently, and this technique has been used in some cutting-edge\nneural TTS techniques. However, training RNN components often requires a very\npowerful computer, or a very long time, typically several days or weeks. Recent\nother studies, on the other hand, have shown that CNN-based sequence synthesis\ncan be much faster than RNN-based techniques, because of high\nparallelizability. The objective of this paper is to show that an alternative\nneural TTS based only on CNN alleviate these economic costs of training. In our\nexperiment, the proposed Deep Convolutional TTS was sufficiently trained\novernight (15 hours), using an ordinary gaming PC equipped with two GPUs, while\nthe quality of the synthesized speech was almost acceptable.",
            "authors": [
                "Hideyuki Tachibana",
                "Katsuya Uenoyama",
                "Shunsuke Aihara"
            ]
        },
        {
            "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions",
            "arxiv": "1712.05884",
            "year": 2017,
            "url": "http://arxiv.org/abs/1712.05884v2",
            "abstract": "This paper describes Tacotron 2, a neural network architecture for speech\nsynthesis directly from text. The system is composed of a recurrent\nsequence-to-sequence feature prediction network that maps character embeddings\nto mel-scale spectrograms, followed by a modified WaveNet model acting as a\nvocoder to synthesize timedomain waveforms from those spectrograms. Our model\nachieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for\nprofessionally recorded speech. To validate our design choices, we present\nablation studies of key components of our system and evaluate the impact of\nusing mel spectrograms as the input to WaveNet instead of linguistic, duration,\nand $F_0$ features. We further demonstrate that using a compact acoustic\nintermediate representation enables significant simplification of the WaveNet\narchitecture.",
            "authors": [
                "Jonathan Shen",
                "Ruoming Pang",
                "Ron J. Weiss",
                "Mike Schuster",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Zhifeng Chen",
                "Yu Zhang",
                "Yuxuan Wang",
                "RJ Skerry-Ryan",
                "Rif A. Saurous",
                "Yannis Agiomyrgiannakis",
                "Yonghui Wu"
            ]
        },
        {
            "title": "Zero-Shot Multi-Speaker Text-To-Speech with State-of-the-art Neural Speaker Embeddings",
            "arxiv": "1910.10838",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.10838v2",
            "abstract": "While speaker adaptation for end-to-end speech synthesis using speaker\nembeddings can produce good speaker similarity for speakers seen during\ntraining, there remains a gap for zero-shot adaptation to unseen speakers. We\ninvestigate multi-speaker modeling for end-to-end text-to-speech synthesis and\nstudy the effects of different types of state-of-the-art neural speaker\nembeddings on speaker similarity for unseen speakers. Learnable dictionary\nencoding-based speaker embeddings with angular softmax loss can improve equal\nerror rates over x-vectors in a speaker verification task; these embeddings\nalso improve speaker similarity and naturalness for unseen speakers when used\nfor zero-shot adaptation to new speakers in end-to-end speech synthesis.",
            "authors": [
                "Erica Cooper",
                "Cheng-I Lai",
                "Yusuke Yasuda",
                "Fuming Fang",
                "Xin Wang",
                "Nanxin Chen",
                "Junichi Yamagishi"
            ]
        },
        {
            "howpublished": "\\url{https://github.com/keonlee9420/Comprehensive-Tacotron2}",
            "journal": "GitHub repository",
            "publisher": "GitHub",
            "year": "2021",
            "title": "Comprehensive-Tacotron2",
            "author": [
                "Lee, Keon"
            ],
            "ENTRYTYPE": "misc",
            "ID": "lee2021comprehensive-tacotron2",
            "authors": [
                "Lee, Keon"
            ]
        }
    ],
    "domain": {
        "domain_type": "Speech",
        "domain_prob": 0.9810093319508914
    }
}