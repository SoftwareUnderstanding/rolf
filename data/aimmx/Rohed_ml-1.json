{
    "visibility": {
        "visibility": "public"
    },
    "name": "Implementing YOLO using ResNet as feature extractor",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Rohed",
                "owner_type": "User",
                "name": "ml-1",
                "url": "https://github.com/Rohed/ml-1",
                "stars": 0,
                "pushed_at": "2020-08-05 19:42:03+00:00",
                "created_at": "2020-08-05 19:23:47+00:00",
                "language": "Python",
                "frameworks": [
                    "Keras",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "c47f60e1918a1984130e5d096d95a8732826c31e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/.gitignore"
                    }
                },
                "size": 100
            },
            {
                "type": "code",
                "name": "bboxes.jpg",
                "sha": "6610e015d5fabc3e00e678d5b02b197ea1288826",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/bboxes.jpg"
                    }
                },
                "size": 412156
            },
            {
                "type": "code",
                "name": "bboxes_grid.jpg",
                "sha": "b987af2bf86ebc7c5f7d8c2fdfe36597669ea7b1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/bboxes_grid.jpg"
                    }
                },
                "size": 522095
            },
            {
                "type": "code",
                "name": "evning_test.jpg",
                "sha": "7b2f900472fd992f266504e5cb4f420a0bb8b815",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/evning_test.jpg"
                    }
                },
                "size": 327839
            },
            {
                "type": "code",
                "name": "label_frames.p",
                "sha": "952c19c39328a9fe8ec6d4e4e0f0b69870493832",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/label_frames.p"
                    }
                },
                "size": 3547651
            },
            {
                "type": "code",
                "name": "model_continue_train.py",
                "sha": "bd18dc0dc73621d8a17c3d38577258e81f423105",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/model_continue_train.py"
                    }
                },
                "size": 16597
            },
            {
                "type": "code",
                "name": "model_data.py",
                "sha": "52b08bb87b0558aacdbc23fb67059036b0c4aa9e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/model_data.py"
                    }
                },
                "size": 11653
            },
            {
                "type": "code",
                "name": "models",
                "sha": "b98d7bbea47870730221514df0202d70079e0bc2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/tree/master/models"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "predict.py",
                "sha": "55b80cbac6ee6751801d8911d47e117e7fbed81a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/predict.py"
                    }
                },
                "size": 1894
            },
            {
                "type": "code",
                "name": "project_video.mp4",
                "sha": "57c0a00db652146a8839e10c580c79f227405370",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/project_video.mp4"
                    }
                },
                "size": 25268015
            },
            {
                "type": "code",
                "name": "project_video_output.mp4",
                "sha": "0700dfd6098a6b17cf73594ed2ff7e5b2c173e4e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/project_video_output.mp4"
                    }
                },
                "size": 19172782
            },
            {
                "type": "code",
                "name": "project_video_output2.mp4",
                "sha": "f30b9c74777a577bb4163604a272680171139060",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/project_video_output2.mp4"
                    }
                },
                "size": 6449602
            },
            {
                "type": "code",
                "name": "readme_images",
                "sha": "5346ed9b57a6aaac6276f5cb9ff2ed7010dea233",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/tree/master/readme_images"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "test_img.jpg",
                "sha": "90106643a150e4e9aa9cf31f91e11de56b375e05",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/test_img.jpg"
                    }
                },
                "size": 196517
            },
            {
                "type": "code",
                "name": "test_img2.jpg",
                "sha": "ba6cfc345ac10b910c6f757f6c174362e1591f2e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/test_img2.jpg"
                    }
                },
                "size": 175413
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "e9228bbb031432f2c03befa94cdc00a431234253",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/utils.py"
                    }
                },
                "size": 5655
            }
        ]
    },
    "authors": [
        {
            "name": "makatx",
            "github_id": "makatx"
        },
        {
            "name": "Oliver",
            "email": "oliveraluloski@gmail.com",
            "github_id": "Rohed"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Rohed/ml-1",
            "stars": 0,
            "issues": true,
            "readme": "\n# Implementing YOLO using ResNet as feature extractor #\n### Trained on Udacity's car dataset (no pre-trained classifier weights) ##\n\n![img](./readme_images/udacity_dt.jpg \"\")\n\n## Abstract ##\nIn this project I have used a pre-trained ResNet50 network, removed its classifier layers so it becomes a feature extractor and add the YOLO classifier layer instead (randomly initialized). I then train the network on Udacity's crowdAI dataset to detect cars in image frames.\n\nThis project was made only as a means to learn more about deep learning, training networks, transfer learning and implementing an actual paper (my first!).\n\n\n## The Dataset ##\nUdacity has made available an annotated car (and a few other objects) dataset here:https://github.com/udacity/self-driving-car/tree/master/annotations\nI've used the 'Dataset1', annotated by CrowdAI for this project. Please take a look at the 'Dataset Exploration.ipynb' jupyter notebook where I've explored the dataset.\n\nIn summary, the dataset identifies 3 classes: Car, Truck and Pedestrian and also lists bounding box coordinates for each of the objects in datapoint (image), in a CSV file.\nThe dataset is uneven across different classes. It can additionally be noted that a certain view (rear) of cars dominates the rest (side and front). The lighting condition is constant throughout the capture and hence we will need data augumentation to help the network generalize better.\n\nIf you'd like to play with the training bit, download the dataset and extract it to 'udacity-object-detection-crowdai' in the root of the project folder.\n\n## The Model ##\n\nThe YOLO network has two components as do most networks: \n- A feature extractor\n- A classifier\n\nThe paper's author explains that they used GoogLeNet (inception) inspired architecture for their feature extractor, that was trained on PASCAL VOC dataset prior to making it part of the object detection network. We can skip this step and use a pre-trained network, that performs well on classification tasks. I've chosen ResNet for this purpose.\n\nI then add two dense/fully connected layers to the feature extractor's output that has random weight initialization and produces an output with the desired dimensions.\n\n![architecture](./readme_images/network.jpg \"Original architecture and modifications\")\n\n### YOLO ###\nThere have been many articles and videos describing this approach originally presented in the paper: https://arxiv.org/abs/1506.02640 \n\nA resource I have found useful was the demo by author of the paper itself: https://youtu.be/NM6lrxy0bxs \n\nThis README won't go into how YOLO itself works but instead focuses on how to prepare/explore dataset and train the network on a specific dataset.\n\n### ResNet ###\nResNet (https://arxiv.org/abs/1512.03385) has won several competitions and its architecture allows for better learning in deeper networks. I've used the Keras implementation with weights of ResNet50 from here https://github.com/fchollet/deep-learning-models.git and modified the code to have the YOLO classifier at the end.\n\n\n## Training and Data Augumentation ##\nThe key part of this implementation was training the network, as it required defining the custom loss function in Tensorflow and image and frame data manipulation for better generalization.\n\n### Grids and bounding boxes ###\nThe object detection approach in YOLO requires us to divide the image into gridcell(S) and that each grid cell will be responsible for the detection and prediction(C) of bounding boxes (B).\nI've chosen to use a 11x11 grid over the images and 2 bounding box predictions per grid cell, to keep sufficient resolution and at the same time have a smaller output prediction to train for.\n\nSince we have 3 classes, the output we will need is: S*S *(C + B(5)) = 121*(3+2(5)) = 1573 \n\n![tensor](./readme_images/tensor.jpg \"Output tensor from modified network\")\n\nHaving a 11x11 grid does however put some detections within the same grid cell and for the sake of simplicity (and computation power), I've only considered one detection in such cases.\n\n![bboxes](./readme_images/bboxes.jpg \"sample image with bounding boxes\")\n![grid](./readme_images/bboxes_grid.jpg \"Grid overlay - green dot shows 'responsible' grid cell\")\n\nThe network is trained on 224x224x3 images and so our dataset images are resized with their corresponding label coordinates adjusted as well.\n\n![](./readme_images/224.jpg \"scaled down to 224x224\")\n\n### The Loss function ###\nKeras and TF have the standard loss defintions however, the YOLO paper uses a custom objective function that is fine tuned to improve stability (penalize loss from grid cells that do not have an object) and weigh dimension error in smaller boxes more than that in larger boxes:\n\n![loss function](./readme_images/loss_function.jpg \"from the paper\")\n\n\nI've used Tensorflow's 'while_loop' to create the graph that calculates loss per each batch. All operations in the my loss function (see loop_body() in model_continue_train.py) are tensorflow operations, hence these will all be run only when the graph is computed, taking advantage of any hardware optimization.\n\n### Data augumentation ###\nAs mentioned in the paper, I've also randomly scaled, translated and adjusted the saturation values of the data point while **generating a batch for training and validation**:\n```\n    #translate_factor\n    tr = np.random.random() * 0.2 + 0.01\n    tr_y = np.random.randint(rows*-tr, rows*tr)\n    tr_x = np.random.randint(cols*-tr, cols*tr)\n        r = np.random.rand()\n\n    if r < 0.3:\n        #translate image\n        M = np.float32([[1,0,tr_x], [0,1,tr_y]])\n        img = cv2.warpAffine(img, M, (cols,rows))\n        frame = coord_translate(frame, tr_x, tr_y)\n    elif r < 0.6:\n        #scale image keeping the same size\n        placeholder = np.zeros_like(img)\n        meta = cv2.resize(img, (0,0), fx=sc, fy=sc)\n        if sc < 1:\n            placeholder[:meta.shape[0], :meta.shape[1]] = meta\n        else:\n            placeholder = meta[:placeholder.shape[0], :placeholder.shape[1]]\n        img = placeholder\n        frame = coord_scale(frame, sc)\n```\n![](./readme_images/augumented.jpg)\n\n### Learning Rate and Epochs ###\nAs described in the paper, I started to train with 1e-3 learning rate, then 1e-2 followed by 1e-3, 1e-4, 1e-5. All along saving model checkpoints using Keras' callback feature.\n\n\n### Amazon AWS GPU Instance ###\nTraining of this magnitude definitely needed some beefed up hardware and since I'm a console guy (PS4), I resorted to the EC instances Amazon provides (https://aws.amazon.com/ec2/instance-types/). Udacity's Amazon credits came in handy!\n\nAt first, I tried the g2.xlarge instance that Udacity's project on Traffic sign classifier had suggested (did that on my laptop back then) but the memory or the compute capability was nowhere near sufficient, since TF apparently drops to CPU and RAM after detecting that there isn't sufficient capacity on the GPU.\n\nIn the end, p2.xlarge EC2 instance were what I trained my network on. There was ~10GB GPU memory utilization and ~92% GPU at peak. My network trained pretty well on this setup.\n\nNOTE: I faced a lot of issues when getting setup on the remote instance due to issues with certain libraries being out of date and anaconda not having those updates. Luckily Amazon released its latest (v6 at time) deep learning Ubuntu AMI which worked just fine out of the box. So if you are using EC2, make sure to test sample code and library imports in python first to make sure the platform is ready for your code.\n\n\n\n\n## Testing ##\nCheck out the 'Vehicle Detection.ipynb' notebook to see the network in use. It performs well on the dataset and also the sample highway video that the network has never seen before.\n\nIf you're interested to try it out yourself you can follow the notebook ('Vehicle Detection.ipynb') or use the `predict.py` as follows:\n\n`\npython predict.py <path_to_image>\n`\n\n**Sample output images:**\n![from dataset](./readme_images/test_op3.jpg \"from dataset itself\")\n\n**Prediction on images the network has never seen:**\n![new image](./readme_images/test_op1.jpg \"\") ![new image](./readme_images/test_op2.jpg \"\")\n\n\n\n## Conclusion ##\nIt worked! Alhamdulillah.\nThe network trained from scratch and was able to detect cars in a video that it had never seen before. It has some problems with far away objects and also the detections are not very smooth across frames. \n\nIt will be worth trying to remove some layers from ResNet and see if the network performs any faster.\n\nI'd love to hear your suggestions on improving the project and also if you have any questions on this.\n\n",
            "readme_url": "https://github.com/Rohed/ml-1",
            "frameworks": [
                "Keras",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "You Only Look Once: Unified, Real-Time Object Detection",
            "arxiv": "1506.02640",
            "year": 2015,
            "url": "http://arxiv.org/abs/1506.02640v5",
            "abstract": "We present YOLO, a new approach to object detection. Prior work on object\ndetection repurposes classifiers to perform detection. Instead, we frame object\ndetection as a regression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network predicts bounding boxes\nand class probabilities directly from full images in one evaluation. Since the\nwhole detection pipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.\n  Our unified architecture is extremely fast. Our base YOLO model processes\nimages in real-time at 45 frames per second. A smaller version of the network,\nFast YOLO, processes an astounding 155 frames per second while still achieving\ndouble the mAP of other real-time detectors. Compared to state-of-the-art\ndetection systems, YOLO makes more localization errors but is far less likely\nto predict false detections where nothing exists. Finally, YOLO learns very\ngeneral representations of objects. It outperforms all other detection methods,\nincluding DPM and R-CNN, by a wide margin when generalizing from natural images\nto artwork on both the Picasso Dataset and the People-Art Dataset.",
            "authors": [
                "Joseph Redmon",
                "Santosh Divvala",
                "Ross Girshick",
                "Ali Farhadi"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "from dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/Rohed/ml-1/blob/master/./readme_images/test_op3.jpg \"from dataset itself\""
                    }
                }
            },
            {
                "name": "Amazon"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "COCO"
            },
            {
                "name": "ImageNet Detection"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999999201054645,
        "task": "Object Detection",
        "task_prob": 0.9929359226746487
    }
}