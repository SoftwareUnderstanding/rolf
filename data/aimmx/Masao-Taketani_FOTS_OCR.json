{
    "visibility": {
        "visibility": "public",
        "license": "GNU General Public License v3.0"
    },
    "name": "FOTS: Fast Oriented Text Spotting with a Unified Network",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Masao-Taketani",
                "owner_type": "User",
                "name": "FOTS_OCR",
                "url": "https://github.com/Masao-Taketani/FOTS_OCR",
                "stars": 49,
                "pushed_at": "2021-01-15 05:45:46+00:00",
                "created_at": "2019-10-17 12:33:19+00:00",
                "language": "C++",
                "description": "TensorFlow Implementation of FOTS, Fast Oriented Text Spotting with a Unified Network.",
                "license": "GNU General Public License v3.0",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "aee1e3a7492bdcf2975f66bb0e1f468da30b1bba",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/blob/master/.gitignore"
                    }
                },
                "size": 32
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "f288702d2fa16d3cdf0035b15a9fcbc552cd88e7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/blob/master/LICENSE"
                    }
                },
                "size": 35149
            },
            {
                "type": "code",
                "name": "bktree.py",
                "sha": "49d04a8fc69ebc7d21c8354b0249af918035f66b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/blob/master/bktree.py"
                    }
                },
                "size": 4513
            },
            {
                "type": "code",
                "name": "ckpt",
                "sha": "b71e6b87eb1025de13d915cb8e963ae74471e599",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/tree/master/ckpt"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "config.py",
                "sha": "d96a5e5780b7f90e16e654f6c725bd6be2cf8881",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/blob/master/config.py"
                    }
                },
                "size": 148
            },
            {
                "type": "code",
                "name": "data",
                "sha": "7910e2d8ed5e0bce8f0e87b6928a9b8d0dd0ac12",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/tree/master/data"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "data_provider",
                "sha": "6a40a5dc5bf56b51f4f2377d99854461fa018cbf",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/tree/master/data_provider"
                    }
                },
                "num_files": 13
            },
            {
                "type": "code",
                "name": "imgs",
                "sha": "c878cf9f9075b43d99e82e0cf028259e5091903a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/tree/master/imgs"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "lanms",
                "sha": "6418f39e67c4fac1f785303640f00845f2e221ee",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/tree/master/lanms"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "locality_aware_nms.py",
                "sha": "b17cc47da2808fcf2912ebed34dd539f11fc23b1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/blob/master/locality_aware_nms.py"
                    }
                },
                "size": 1540
            },
            {
                "type": "code",
                "name": "module",
                "sha": "68800435f582741cd9bf62b879fe1491c1eff08d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/tree/master/module"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "nets",
                "sha": "07dd7a0ebbd5df3fc48b86b6aca545ea59648811",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/tree/master/nets"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "notebooks",
                "sha": "b8c64319365140aaf631d7defd49d7fcf5a49f73",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/tree/master/notebooks"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "b734b8d4f6992622bde3fead00279c195b37604f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/blob/master/test.py"
                    }
                },
                "size": 16819
            },
            {
                "type": "code",
                "name": "test_imgs",
                "sha": "5bfad2b3f8e483b6b173d8aaff19597e84626f15",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/tree/master/test_imgs"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "tmp",
                "sha": "198c5c26eab78d513db59ed8f74b2b0f8579e08f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/tree/master/tmp"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "31432fbd7ba9ff34c3aeb8486bb13b345aae4a30",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/blob/master/train.py"
                    }
                },
                "size": 14025
            },
            {
                "type": "code",
                "name": "train_synthText_10eps.sh",
                "sha": "5028b7fd4ee022f145660f8fde6c36fff2e26ab3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Masao-Taketani/FOTS_OCR/blob/master/train_synthText_10eps.sh"
                    }
                },
                "size": 229
            }
        ]
    },
    "authors": [
        {
            "name": "Masao Taketani",
            "github_id": "Masao-Taketani"
        },
        {
            "name": "yu20103983",
            "github_id": "yu20103983"
        }
    ],
    "tags": [
        "ocr",
        "tensorflow",
        "scene-text-recognition",
        "deep-learning",
        "computer-vision",
        "image-recognition"
    ],
    "description": "TensorFlow Implementation of FOTS, Fast Oriented Text Spotting with a Unified Network.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Masao-Taketani/FOTS_OCR",
            "stars": 49,
            "issues": true,
            "readme": "# FOTS: Fast Oriented Text Spotting with a Unified Network\n\n**I am still working on this repo. updates and detailed instructions are coming soon!**\n\n<img src=\"imgs/img_165.jpg\" width=\"445\"/>   <img src=\"imgs/img_201.jpg\" width=\"445\"/>\n<img src=\"imgs/img_113.jpg\" width=\"445\"/>   <img src=\"imgs/img_132.jpg\" width=\"445\"/>\n\n## Table of Contens\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n\n- [TensorFlow Versions](#tensorflow-versions)\n- [Other Requirements](#other-requirements)\n- [Trained Models](#trained-models)\n- [Datasets](#datasets)\n- [Train](#train)\n  - [Pre-train with SynthText](#pre-train-with-synthtext)\n  - [Finetune with ICDAR 2015, ICDAR 2017 MLT or ICDAR 2013](#finetune-with-icdar-2015-icdar-2017-mlt-or-icdar-2013)\n- [Test](#test)\n- [References](#references)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## TensorFlow Versions\nAs for now, the pre-training code is tested on TensorFlow 1.12, 1.14 and 1.15. I may try to implement 2.x version in the future.\n\n## Other Requirements\nGCC >= 6\n\n## Trained Models\n  - [tmp pre-trained model](https://drive.google.com/drive/folders/1g5pneiBzmsU4Xw6mnAajF8HHK9L1ho_c?usp=sharing)\n  - trained model **comming soon**\n## Datasets\n- pre-training<br>\n[Synth800k](https://www.robots.ox.ac.uk/~vgg/data/scenetext/)(The dataset is only available for non-commercial research and educational purposes)\n- finetuning<br>\n[ICDAR 2015, 2017MLT, 2013](https://rrc.cvc.uab.es/)\n\n## Train\n### Pre-train with SynthText\n1. Download [pre-trained ResNet-50](http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz) from [TensorFlow-Slim image classification model library](https://github.com/tensorflow/models/tree/master/research/slim) page and place it at 'ckpt/resnet_v1_50' dir.<br>\n```\ncd ckpt/resnet_v1_50\nwget http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\ntar -zxvf resnet_v1_50_2016_08_28.tar.gz\nrm resnet_v1_50_2016_08_28.tar.gz\n```\n2. Download [Synth800k dataset](https://www.robots.ox.ac.uk/~vgg/data/scenetext/) and place it at `data/SynthText/` dir to pre-train the whole net.<br>\n\n3. Transform(Pre-process) the SynthText data into the ICDAR data format.<br>\n```\npython data_provider/SynthText2ICDAR.py\n```\n\n4. Train with SynthText for 10 epochs(with 1 GPU).<br>\n```\npython train.py \\\n  --max_steps=715625 \\\n  --gpu_list='0' \\\n  --checkpoint_path=ckpt/synthText_10eps/ \\\n  --pretrained_model_path=ckpt/resnet_v1_50/resnet_v1_50.ckpt \\\n  --training_img_data_dir=data/SynthText/ \\\n  --training_gt_data_dir=data/SynthText/ \\\n  --icdar=False \\\n```\n5. Visualize pre-pretraining progress with TensorBoard.\n```\ntensorboard --logdir=ckpt/synthText_10eps/\n```\n\n### Finetune with ICDAR 2015, ICDAR 2017 MLT or ICDAR 2013\n(if you are using [the pre-trained model](https://drive.google.com/drive/folders/1g5pneiBzmsU4Xw6mnAajF8HHK9L1ho_c?usp=sharing), place all of the files in `ckpt/synthText_10eps/`)\n\n- Combine ICDAR data before training.\n  1. Place ICDAR data under `tmp/` foler.\n  2. Run the following script to combine the data.\n  ```\n  python combine_ICDAR_data.py --year [year of ICDAR to train(13 or 15 or 17)]\n  ```\n\n- ICDAR 2017 MLT/pre-finetune for ICDAR 2013 or ICDAR 2015 (text detection task only)\n  - Train the pre-trained model with 9,000 images from ICDAR 2017 MLT training and validation datasets(with 1 GPU).\n  ```\n  python train.py \\\n    --gpu_list='0' \\\n    --checkpoint_path=ckpt/ICDAR17MLT/ \\\n    --pretrained_model_path=ckpt/synthText_10eps/ \\\n    --train_stage=0 \\\n    --training_img_data_dir=data/ICDAR17MLT/imgs/ \\\n    --training_gt_data_dir=data/ICDAR17MLT/gts/\n  ```\n\n- ICDAR 2015\n  - Train the model with 1,000 images from ICDAR 2015 training dataset and 229 images from ICDAR 2013 training datasets(with 1 GPU).\n  ```\n  python train.py \\\n    --gpu_list='0' \\\n    --checkpoint_path=ckpt/ICDAR15/ \\\n    --pretrained_model_path=ckpt/ICDAR17MLT/ \\\n    --training_img_data_dir=data/ICDAR15+13/imgs/ \\\n    --training_gt_data_dir=data/ICDAR15+13/gts/\n  ```\n\n- ICDAR 2013(horizontal text only)\n  - Train the model with 229 images from ICDAR 2013 training datasets(with 1 GPU).\n  ```\n  python train.py \\\n    --gpu_list='0' \\\n    --checkpoint_path=ckpt/ICDAR13/ \\\n    --pretrained_model_path=ckpt/ICDAR17MLT/ \\\n    --training_img_data_dir=data/ICDAR13/imgs/ \\\n    --training_gt_data_dir=data/ICDAR13/gts/\n  ```\n\n## Test\nPlace some images in `test_imgs/` dir and specify a trained checkpoint path to see the test result.\n```\npython test.py --test_data_path test_imgs/ --checkpoint_path [checkpoint path]\n```\n\n## References\n- Paper\n  - [FOTS: Fast Oriented Text Spotting with a Unified Network](https://arxiv.org/abs/1801.01671)<br>\n  - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)<br>\n  - [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144)<br>\n- Repos\n  - https://github.com/yu20103983/FOTS<br>\n  - https://github.com/Pay20Y/FOTS_TF/tree/dev<br>\n  - https://github.com/tensorflow/models/tree/master/research/slim<br>\n  - https://github.com/kaiminghe/deep-residual-networks<br>\n  - https://github.com/Parquery/lanms<br>\n",
            "readme_url": "https://github.com/Masao-Taketani/FOTS_OCR",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Feature Pyramid Networks for Object Detection",
            "arxiv": "1612.03144",
            "year": 2016,
            "url": "http://arxiv.org/abs/1612.03144v2",
            "abstract": "Feature pyramids are a basic component in recognition systems for detecting\nobjects at different scales. But recent deep learning object detectors have\navoided pyramid representations, in part because they are compute and memory\nintensive. In this paper, we exploit the inherent multi-scale, pyramidal\nhierarchy of deep convolutional networks to construct feature pyramids with\nmarginal extra cost. A top-down architecture with lateral connections is\ndeveloped for building high-level semantic feature maps at all scales. This\narchitecture, called a Feature Pyramid Network (FPN), shows significant\nimprovement as a generic feature extractor in several applications. Using FPN\nin a basic Faster R-CNN system, our method achieves state-of-the-art\nsingle-model results on the COCO detection benchmark without bells and\nwhistles, surpassing all existing single-model entries including those from the\nCOCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU\nand thus is a practical and accurate solution to multi-scale object detection.\nCode will be made publicly available.",
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie"
            ]
        },
        {
            "title": "FOTS: Fast Oriented Text Spotting with a Unified Network",
            "arxiv": "1801.01671",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.01671v2",
            "abstract": "Incidental scene text spotting is considered one of the most difficult and\nvaluable challenges in the document analysis community. Most existing methods\ntreat text detection and recognition as separate tasks. In this work, we\npropose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS)\nnetwork for simultaneous detection and recognition, sharing computation and\nvisual information among the two complementary tasks. Specially, RoIRotate is\nintroduced to share convolutional features between detection and recognition.\nBenefiting from convolution sharing strategy, our FOTS has little computation\noverhead compared to baseline text detection network, and the joint training\nmethod learns more generic features to make our method perform better than\nthese two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR\n2013 datasets demonstrate that the proposed method outperforms state-of-the-art\nmethods significantly, which further allows us to develop the first real-time\noriented text spotting system which surpasses all previous state-of-the-art\nresults by more than 5% on ICDAR 2015 text spotting task while keeping 22.6\nfps.",
            "authors": [
                "Xuebo Liu",
                "Ding Liang",
                "Shi Yan",
                "Dagui Chen",
                "Yu Qiao",
                "Junjie Yan"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Synth800k dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://www.robots.ox.ac.uk/~vgg/data/scenetext/"
                    }
                }
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "COCO"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ImageNet"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9984360705506697,
        "task": "Scene Text Detection",
        "task_prob": 0.8485454442426614
    }
}