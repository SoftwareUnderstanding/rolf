{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "Introduction",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "StillKeepTry",
                "owner_type": "User",
                "name": "Transformer-PyTorch",
                "url": "https://github.com/StillKeepTry/Transformer-PyTorch",
                "stars": 41,
                "pushed_at": "2018-10-16 09:23:29+00:00",
                "created_at": "2018-03-01 11:43:30+00:00",
                "language": "Python",
                "description": "A PyTorch implementation of Attention is all you need",
                "license": "Other",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "4221a0b7073c08fa5e55fc94d61e194b1bb68ec9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/.gitignore"
                    }
                },
                "size": 1236
            },
            {
                "type": "code",
                "name": "CONTRIBUTING.md",
                "sha": "5592b2bdef002922c5c0a1c6798eaf6ea582b61b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/CONTRIBUTING.md"
                    }
                },
                "size": 1190
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "8144372d12745346b580c7471ffac8bdc6137a15",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/LICENSE"
                    }
                },
                "size": 1539
            },
            {
                "type": "code",
                "name": "PATENTS",
                "sha": "18b09892ca58984f01cbbaaa7ee11b61765b76b7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/PATENTS"
                    }
                },
                "size": 1978
            },
            {
                "type": "code",
                "name": "Papers",
                "sha": "168c4c000e29354493d7795aba60405a60f3927f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/tree/master/Papers"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "data",
                "sha": "1a57404519050198aa738361315ff3d2d0b56997",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/tree/master/data"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "distributed_train.py",
                "sha": "04df19dd371ed3a4177976793015a34fedbdc727",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/distributed_train.py"
                    }
                },
                "size": 1875
            },
            {
                "type": "code",
                "name": "fairseq.gif",
                "sha": "5782fdbc7e0014564725c3ad0fc6be5c6bcd9983",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/fairseq.gif"
                    }
                },
                "size": 2664833
            },
            {
                "type": "code",
                "name": "fairseq",
                "sha": "2b021b204553d48eaaf26c00edc4485e7f2269e2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/tree/master/fairseq"
                    }
                },
                "num_files": 20
            },
            {
                "type": "code",
                "name": "generate.py",
                "sha": "83156f45d940abf80245e9c41f49442c9d976570",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/generate.py"
                    }
                },
                "size": 7106
            },
            {
                "type": "code",
                "name": "interactive.py",
                "sha": "f6a291abd68329728372a6f4c1394af84a0e9afa",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/interactive.py"
                    }
                },
                "size": 3181
            },
            {
                "type": "code",
                "name": "multiprocessing_train.py",
                "sha": "404ce5087237c08a877f377ef10e33eff6e5a3b1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/multiprocessing_train.py"
                    }
                },
                "size": 2930
            },
            {
                "type": "code",
                "name": "preprocess.py",
                "sha": "4f16be7ac15a7db87137182147275a44531b03fa",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/preprocess.py"
                    }
                },
                "size": 8746
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "3327ee454c34cb6d8213a1fd3b0d03992311996f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/requirements.txt"
                    }
                },
                "size": 22
            },
            {
                "type": "code",
                "name": "run_iwslt14_transformer.sh",
                "sha": "46a22e6e3b9eeefeab426667b338c610232627f7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/run_iwslt14_transformer.sh"
                    }
                },
                "size": 473
            },
            {
                "type": "code",
                "name": "run_wmt14_enfr_transformer.sh",
                "sha": "251b969fd64b3f57869870f102398ab10e2cc660",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/run_wmt14_enfr_transformer.sh"
                    }
                },
                "size": 454
            },
            {
                "type": "code",
                "name": "run_wmt14_enfr_transformer_big.sh",
                "sha": "b2a8c634739da3b8d346dc80cda2207bdf6ad901",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/run_wmt14_enfr_transformer_big.sh"
                    }
                },
                "size": 452
            },
            {
                "type": "code",
                "name": "score.py",
                "sha": "fc563388213261a9df903525d35416c5360e3b3f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/score.py"
                    }
                },
                "size": 1968
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "1d0f12521b36d39ab93d5e1c6c2726599cc7ac3e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/tree/master/scripts"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "6bcb2501efea1363250ef7bff06581af8ffdaecc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/setup.py"
                    }
                },
                "size": 1109
            },
            {
                "type": "code",
                "name": "singleprocess_train.py",
                "sha": "c0aac4786a56db4b2ccf1555783bc6d86c3d71ec",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/singleprocess_train.py"
                    }
                },
                "size": 11196
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "dce280278768b9957e2f717d6c37b5129e3568e1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/tree/master/tests"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "622be99e00e0f337ad519e57080f555048023762",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/train.py"
                    }
                },
                "size": 913
            }
        ]
    },
    "authors": [
        {
            "name": "KaiTao Song",
            "email": "kt.song@njust.edu.cn",
            "github_id": "StillKeepTry"
        }
    ],
    "tags": [],
    "description": "A PyTorch implementation of Attention is all you need",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/StillKeepTry/Transformer-PyTorch",
            "stars": 41,
            "issues": true,
            "readme": "# Introduction\nThis project provides a PyTorch implementation about [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) based on [fairseq-py](https://github.com/facebookresearch/fairseq-py) (An official toolkit of facebook research). You can also use official code about *Attention is all you need* from [tensor2tensor](https://github.com/tensorflow/tensor2tensor).\n\nIf you use this code about cnn, please cite:\n```\n@inproceedings{gehring2017convs2s,\n  author    = {Gehring, Jonas, and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},\n  title     = \"{Convolutional Sequence to Sequence Learning}\",\n  booktitle = {Proc. of ICML},\n  year      = 2017,\n}\n```\nAnd if you use this code about transformer, please cite:\n```\n@inproceedings{46201,\n  title = {Attention is All You Need},\n  author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n  year  = {2017},\n  booktitle = {Proc. of NIPS},\n}\n```\nFeel grateful for the contribution of the facebook research and google research. **Besides, if you get benefits from this repository, please give me a star.**\n\n# Details\n\n## How to install Transformer-PyTorch\nYou first need to install PyTorch >= 0.4.0 and Python = 3.6. And then\n```\npip install -r requirements.txt\npython setup.py build\npython setup.py develop\n```\n\nGenerating binary data, please follow the script under [data/](data/), i have provide a [run script](run_iwslt14_transformer.sh) for iwslt14.\n\n# Results\n\n## IWSLT14 German-English\nThis dataset contains 160K training sentences. We recommend you to use `transformer_small` setting. The beam size is set as 5. The results are as follow:\n\n|Word Type|BLEU|\n|:-:|:-:|\n|10K jointly-sub-word|31.06|\n|25K jointly-sub-word|32.12|\n\nPlease try more checkpoint, not only the last checkpoint.\n\n## Nist Chinese-English\n\nThis dataset contains 1.25M training sentences. We learn a 25K subword dictionary for source and target languages respectively. We adopt a `transformer_base` model setting. The results are as follow:\n\n||MT04|MT05|MT06|MT08|MT12|\n|:-:|:-:|:-:|:-:|:-:|:-:|\n|Beam=10|40.67|40.57|38.77|32.26|31.04|\n\n## WMT14 English-German\nThis dataset contains 4.5M sentence pairs. \n\n|model Setting| BLEU|\n|:-:|:-:|\n|transformer_big|28.48|\n\n\n## WMT14 English-French\nThis dataset include 36M sentence pairs. We learned a 40K BPE for english and french. Beam size = 5. And 8 GPUs are used in this task. For base model setting, the batch size is 4000 for each gpu.\n\n|Steps|BLEU|\n|:-:|:-:|\n|2w|34.42|\n|5w|37.14|\n|12w|38.72|\n|17w|39.06|\n|21w|39.30|\n\nAnd For big model, the batch size is 3072 for each gpu. The result is as:\n\n|Steps|BLEU|\n|:-:|:-:|\n|5.5w|38.00|\n|11w|39.44|\n|16w|40.21|\n|27w|40.46|\n|30w|40.76|\n\nLimited to resource, i just conduct experiment only once on Big model setting and do not try more parameters such as learning rate. I think you can produce better performance if you have rich GPUs.\n\n## Note\n> * This project is only maintained by myself. Therefore, there still exists some minor errors in code style.\n> * Instead of adam, i try NAG as the default optimizer, i find this optimized method can also produce better performance.\n> * If you have more suggestions for improving this project, leaving message under issues.\n\nOur many works are built upon this project, include:\n> * Double Path Networks for Sequence to Sequence Learning, (COLING 2018)\n> * Other submitted papers.\n\n# License\nfairseq is BSD-licensed. The released codes modified the original fairseq are BSD-licensed. The rest of the codes are MIT-licensed.\n",
            "readme_url": "https://github.com/StillKeepTry/Transformer-PyTorch",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        },
        {
            "year": "2017",
            "booktitle": "Proc. of ICML",
            "title": "{Convolutional Sequence to Sequence Learning}",
            "author": [
                "Gehring, Jonas,",
                "Auli, Michael",
                "Grangier, David",
                "Yarats, Denis",
                "Dauphin, Yann N"
            ],
            "ENTRYTYPE": "inproceedings",
            "ID": "gehring2017convs2s",
            "authors": [
                "Gehring, Jonas,",
                "Auli, Michael",
                "Grangier, David",
                "Yarats, Denis",
                "Dauphin, Yann N"
            ]
        },
        {
            "booktitle": "Proc. of NIPS",
            "year": "2017",
            "author": [
                "Vaswani, Ashish",
                "Shazeer, Noam",
                "Parmar, Niki",
                "Uszkoreit, Jakob",
                "Jones, Llion",
                "Gomez, Aidan N.",
                "Kaiser, Lukasz",
                "Polosukhin, Illia"
            ],
            "title": "Attention is All You Need",
            "ENTRYTYPE": "inproceedings",
            "ID": "46201",
            "authors": [
                "Vaswani, Ashish",
                "Shazeer, Noam",
                "Parmar, Niki",
                "Uszkoreit, Jakob",
                "Jones, Llion",
                "Gomez, Aidan N.",
                "Kaiser, Lukasz",
                "Polosukhin, Illia"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.999987600231576,
        "task": "Machine Translation",
        "task_prob": 0.9887170012021373
    }
}