{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "StyleGAN &mdash; Official TensorFlow Implementation",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "khurram702",
                "owner_type": "User",
                "name": "StyleBasedGAN",
                "url": "https://github.com/khurram702/StyleBasedGAN",
                "stars": 0,
                "pushed_at": "2019-12-17 07:28:02+00:00",
                "created_at": "2019-12-17 07:22:54+00:00",
                "language": "Python",
                "description": "Style Base Architecture of Generator",
                "license": "Other",
                "frameworks": [
                    "TensorFlow",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE.txt",
                "sha": "ca56419327bbeeb8094330497024f109bd52b96d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/blob/master/LICENSE.txt"
                    }
                },
                "size": 19406
            },
            {
                "type": "code",
                "name": "config.py",
                "sha": "4eb9b33526c78b63cf4e36d8ea5ab39ab34f3808",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/blob/master/config.py"
                    }
                },
                "size": 671
            },
            {
                "type": "code",
                "name": "dataset_tool.py",
                "sha": "4ddfe448e2ccaa30e04ad4b49761d406846c962f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/blob/master/dataset_tool.py"
                    }
                },
                "size": 30421
            },
            {
                "type": "code",
                "name": "dnnlib",
                "sha": "4f9230efbbebf19765a1af081e7688babb9e4644",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/tree/master/dnnlib"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "generate_figures.py",
                "sha": "45b68b86146198c701a66fb8ba7a363d901d6951",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/blob/master/generate_figures.py"
                    }
                },
                "size": 9563
            },
            {
                "type": "code",
                "name": "metrics",
                "sha": "bbe1d01ed75c610a3ec470944ac7b420960bddb8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/tree/master/metrics"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "pretrained_example.py",
                "sha": "63baef08bfa4bf34f52a0cf63e10a0b6783ac316",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/blob/master/pretrained_example.py"
                    }
                },
                "size": 1781
            },
            {
                "type": "code",
                "name": "run_metrics.py",
                "sha": "5d1597bbd4e16a2535309ea74c3559cae2a5fa53",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/blob/master/run_metrics.py"
                    }
                },
                "size": 4374
            },
            {
                "type": "code",
                "name": "stylegan-teaser.png",
                "sha": "cca876a70c84fe08176836afd583d7ba7cea6b60",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/blob/master/stylegan-teaser.png"
                    }
                },
                "size": 1635753
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "29df3c226b87816ceec25752293df08a70d63189",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/blob/master/train.py"
                    }
                },
                "size": 16580
            },
            {
                "type": "code",
                "name": "training",
                "sha": "b8e62cd72b40b0ec4490d099a1cb06bfad745506",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/tree/master/training"
                    }
                },
                "num_files": 7
            }
        ]
    },
    "authors": [
        {
            "name": "Tero Karras",
            "email": "tkarras@nvidia.com",
            "github_id": "tkarras"
        }
    ],
    "tags": [],
    "description": "Style Base Architecture of Generator",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/khurram702/StyleBasedGAN",
            "stars": 0,
            "issues": true,
            "readme": "## StyleGAN &mdash; Official TensorFlow Implementation\n![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg?style=plastic)\n![TensorFlow 1.10](https://img.shields.io/badge/tensorflow-1.10-green.svg?style=plastic)\n![cuDNN 7.3.1](https://img.shields.io/badge/cudnn-7.3.1-green.svg?style=plastic)\n![License CC BY-NC](https://img.shields.io/badge/license-CC_BY--NC-green.svg?style=plastic)\n\n![Teaser image](./stylegan-teaser.png)\n**Picture:** *These people are not real &ndash; they were produced by our generator that allows control over different aspects of the image.*\n\nThis repository contains the official TensorFlow implementation of the following paper:\n\n> **A Style-Based Generator Architecture for Generative Adversarial Networks**<br>\n> Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)<br>\n> https://arxiv.org/abs/1812.04948\n>\n> **Abstract:** *We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.*\n\nFor business inquiries, please contact [researchinquiries@nvidia.com](mailto:researchinquiries@nvidia.com)<br>\nFor press and other inquiries, please contact Hector Marinez at [hmarinez@nvidia.com](mailto:hmarinez@nvidia.com)<br>\n\n**&#9733;&#9733;&#9733; NEW: StyleGAN2 is available at [https://github.com/NVlabs/stylegan2](https://github.com/NVlabs/stylegan2) &#9733;&#9733;&#9733;**\n\n## Resources\n\nMaterial related to our paper is available via the following links:\n\n- Paper: https://arxiv.org/abs/1812.04948\n- Video: https://youtu.be/kSLJriaOumA\n- Code: https://github.com/NVlabs/stylegan\n- FFHQ: https://github.com/NVlabs/ffhq-dataset\n\nAdditional material can be found on Google Drive:\n\n| Path | Description\n| :--- | :----------\n| [StyleGAN](https://drive.google.com/open?id=1uka3a1noXHAydRPRbknqwKVGODvnmUBX) | Main folder.\n| &boxvr;&nbsp; [stylegan-paper.pdf](https://drive.google.com/open?id=1v-HkF3Ehrpon7wVIx4r5DLcko_U_V6Lt) | High-quality version of the paper PDF.\n| &boxvr;&nbsp; [stylegan-video.mp4](https://drive.google.com/open?id=1uzwkZHQX_9pYg1i0d1Nbe3D9xPO8-qBf) | High-quality version of the result video.\n| &boxvr;&nbsp; [images](https://drive.google.com/open?id=1-l46akONUWF6LCpDoeq63H53rD7MeiTd) | Example images produced using our generator.\n| &boxv;&nbsp; &boxvr;&nbsp; [representative-images](https://drive.google.com/open?id=1ToY5P4Vvf5_c3TyUizQ8fckFFoFtBvD8) | High-quality images to be used in articles, blog posts, etc.\n| &boxv;&nbsp; &boxur;&nbsp; [100k-generated-images](https://drive.google.com/open?id=100DJ0QXyG89HZzB4w2Cbyf4xjNK54cQ1) | 100,000 generated images for different amounts of truncation.\n| &boxv;&nbsp; &ensp;&ensp; &boxvr;&nbsp; [ffhq-1024x1024](https://drive.google.com/open?id=14lm8VRN1pr4g_KVe6_LvyDX1PObst6d4) | Generated using Flickr-Faces-HQ dataset at 1024&times;1024.\n| &boxv;&nbsp; &ensp;&ensp; &boxvr;&nbsp; [bedrooms-256x256](https://drive.google.com/open?id=1Vxz9fksw4kgjiHrvHkX4Hze4dyThFW6t) | Generated using LSUN Bedroom dataset at 256&times;256.\n| &boxv;&nbsp; &ensp;&ensp; &boxvr;&nbsp; [cars-512x384](https://drive.google.com/open?id=1MFCvOMdLE2_mpeLPTiDw5dxc2CRuKkzS) | Generated using LSUN Car dataset at 512&times;384.\n| &boxv;&nbsp; &ensp;&ensp; &boxur;&nbsp; [cats-256x256](https://drive.google.com/open?id=1gq-Gj3GRFiyghTPKhp8uDMA9HV_0ZFWQ) | Generated using LSUN Cat dataset at 256&times;256.\n| &boxvr;&nbsp; [videos](https://drive.google.com/open?id=1N8pOd_Bf8v89NGUaROdbD8-ayLPgyRRo) | Example videos produced using our generator.\n| &boxv;&nbsp; &boxur;&nbsp; [high-quality-video-clips](https://drive.google.com/open?id=1NFO7_vH0t98J13ckJYFd7kuaTkyeRJ86) | Individual segments of the result video as high-quality MP4.\n| &boxvr;&nbsp; [ffhq-dataset](https://drive.google.com/open?id=1u2xu7bSrWxrbUxk-dT-UvEJq8IjdmNTP) | Raw data for the [Flickr-Faces-HQ dataset](https://github.com/NVlabs/ffhq-dataset).\n| &boxur;&nbsp; [networks](https://drive.google.com/open?id=1MASQyN5m0voPcx7-9K0r5gObhvvPups7) | Pre-trained networks as pickled instances of [dnnlib.tflib.Network](./dnnlib/tflib/network.py).\n| &ensp;&ensp; &boxvr;&nbsp; [stylegan-ffhq-1024x1024.pkl](https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ) | StyleGAN trained with Flickr-Faces-HQ dataset at 1024&times;1024.\n| &ensp;&ensp; &boxvr;&nbsp; [stylegan-celebahq-1024x1024.pkl](https://drive.google.com/uc?id=1MGqJl28pN4t7SAtSrPdSRJSQJqahkzUf) | StyleGAN trained with CelebA-HQ dataset at 1024&times;1024.\n| &ensp;&ensp; &boxvr;&nbsp; [stylegan-bedrooms-256x256.pkl](https://drive.google.com/uc?id=1MOSKeGF0FJcivpBI7s63V9YHloUTORiF) | StyleGAN trained with LSUN Bedroom dataset at 256&times;256.\n| &ensp;&ensp; &boxvr;&nbsp; [stylegan-cars-512x384.pkl](https://drive.google.com/uc?id=1MJ6iCfNtMIRicihwRorsM3b7mmtmK9c3) | StyleGAN trained with LSUN Car dataset at 512&times;384.\n| &ensp;&ensp; &boxvr;&nbsp; [stylegan-cats-256x256.pkl](https://drive.google.com/uc?id=1MQywl0FNt6lHu8E_EUqnRbviagS7fbiJ) | StyleGAN trained with LSUN Cat dataset at 256&times;256.\n| &ensp;&ensp; &boxur;&nbsp; [metrics](https://drive.google.com/open?id=1MvYdWCBuMfnoYGptRH-AgKLbPTsIQLhl) | Auxiliary networks for the quality and disentanglement metrics.\n| &ensp;&ensp; &ensp;&ensp; &boxvr;&nbsp; [inception_v3_features.pkl](https://drive.google.com/uc?id=1MzTY44rLToO5APn8TZmfR7_ENSe5aZUn) | Standard [Inception-v3](https://arxiv.org/abs/1512.00567) classifier that outputs a raw feature vector.\n| &ensp;&ensp; &ensp;&ensp; &boxvr;&nbsp; [vgg16_zhang_perceptual.pkl](https://drive.google.com/uc?id=1N2-m9qszOeVC9Tq77WxsLnuWwOedQiD2) | Standard [LPIPS](https://arxiv.org/abs/1801.03924) metric to estimate perceptual similarity.\n| &ensp;&ensp; &ensp;&ensp; &boxvr;&nbsp; [celebahq-classifier-00-male.pkl](https://drive.google.com/uc?id=1Q5-AI6TwWhCVM7Muu4tBM7rp5nG_gmCX) | Binary classifier trained to detect a single attribute of CelebA-HQ.\n| &ensp;&ensp; &ensp;&ensp; &boxur;&nbsp;&#x22ef; | Please see the file listing for remaining networks.\n\n## Licenses\n\nAll material, excluding the Flickr-Faces-HQ dataset, is made available under [Creative Commons BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) license by NVIDIA Corporation. You can **use, redistribute, and adapt** the material for **non-commercial purposes**, as long as you give appropriate credit by **citing our paper** and **indicating any changes** that you've made.\n\nFor license information regarding the FFHQ dataset, please refer to the [Flickr-Faces-HQ repository](https://github.com/NVlabs/ffhq-dataset).\n\n`inception_v3_features.pkl` and `inception_v3_softmax.pkl` are derived from the pre-trained [Inception-v3](https://arxiv.org/abs/1512.00567) network by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. The network was originally shared under [Apache 2.0](https://github.com/tensorflow/models/blob/master/LICENSE) license on the [TensorFlow Models](https://github.com/tensorflow/models) repository.\n\n`vgg16.pkl` and `vgg16_zhang_perceptual.pkl` are derived from the pre-trained [VGG-16](https://arxiv.org/abs/1409.1556) network by Karen Simonyan and Andrew Zisserman. The network was originally shared under [Creative Commons BY 4.0](https://creativecommons.org/licenses/by/4.0/) license on the [Very Deep Convolutional Networks for Large-Scale Visual Recognition](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) project page.\n\n`vgg16_zhang_perceptual.pkl` is further derived from the pre-trained [LPIPS](https://arxiv.org/abs/1801.03924) weights by Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The weights were originally shared under [BSD 2-Clause \"Simplified\" License](https://github.com/richzhang/PerceptualSimilarity/blob/master/LICENSE) on the [PerceptualSimilarity](https://github.com/richzhang/PerceptualSimilarity) repository.\n\n## System requirements\n\n* Both Linux and Windows are supported, but we strongly recommend Linux for performance and compatibility reasons.\n* 64-bit Python 3.6 installation. We recommend Anaconda3 with numpy 1.14.3 or newer.\n* TensorFlow 1.10.0 or newer with GPU support.\n* One or more high-end NVIDIA GPUs with at least 11GB of DRAM. We recommend NVIDIA DGX-1 with 8 Tesla V100 GPUs.\n* NVIDIA driver 391.35 or newer, CUDA toolkit 9.0 or newer, cuDNN 7.3.1 or newer.\n\n## Using pre-trained networks\n\nA minimal example of using a pre-trained StyleGAN generator is given in [pretrained_example.py](./pretrained_example.py). When executed, the script downloads a pre-trained StyleGAN generator from Google Drive and uses it to generate an image:\n\n```\n> python pretrained_example.py\nDownloading https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ .... done\n\nGs                              Params    OutputShape          WeightShape\n---                             ---       ---                  ---\nlatents_in                      -         (?, 512)             -\n...\nimages_out                      -         (?, 3, 1024, 1024)   -\n---                             ---       ---                  ---\nTotal                           26219627\n\n> ls results\nexample.png # https://drive.google.com/uc?id=1UDLT_zb-rof9kKH0GwiJW_bS9MoZi8oP\n```\n\nA more advanced example is given in [generate_figures.py](./generate_figures.py). The script reproduces the figures from our paper in order to illustrate style mixing, noise inputs, and truncation:\n```\n> python generate_figures.py\nresults/figure02-uncurated-ffhq.png     # https://drive.google.com/uc?id=1U3r1xgcD7o-Fd0SBRpq8PXYajm7_30cu\nresults/figure03-style-mixing.png       # https://drive.google.com/uc?id=1U-nlMDtpnf1RcYkaFQtbh5oxnhA97hy6\nresults/figure04-noise-detail.png       # https://drive.google.com/uc?id=1UX3m39u_DTU6eLnEW6MqGzbwPFt2R9cG\nresults/figure05-noise-components.png   # https://drive.google.com/uc?id=1UQKPcvYVeWMRccGMbs2pPD9PVv1QDyp_\nresults/figure08-truncation-trick.png   # https://drive.google.com/uc?id=1ULea0C12zGlxdDQFNLXOWZCHi3QNfk_v\nresults/figure10-uncurated-bedrooms.png # https://drive.google.com/uc?id=1UEBnms1XMfj78OHj3_cx80mUf_m9DUJr\nresults/figure11-uncurated-cars.png     # https://drive.google.com/uc?id=1UO-4JtAs64Kun5vIj10UXqAJ1d5Ir1Ke\nresults/figure12-uncurated-cats.png     # https://drive.google.com/uc?id=1USnJc14prlu3QAYxstrtlfXC9sDWPA-W\n```\n\nThe pre-trained networks are stored as standard pickle files on Google Drive:\n\n```\n# Load pre-trained network.\nurl = 'https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ' # karras2019stylegan-ffhq-1024x1024.pkl\nwith dnnlib.util.open_url(url, cache_dir=config.cache_dir) as f:\n    _G, _D, Gs = pickle.load(f)\n    # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n    # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n    # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.\n```\n\nThe above code downloads the file and unpickles it to yield 3 instances of [dnnlib.tflib.Network](./dnnlib/tflib/network.py). To generate images, you will typically want to use `Gs` &ndash; the other two networks are provided for completeness. In order for `pickle.load()` to work, you will need to have the `dnnlib` source directory in your PYTHONPATH and a `tf.Session` set as default. The session can initialized by calling `dnnlib.tflib.init_tf()`.\n\nThere are three ways to use the pre-trained generator:\n\n1. Use `Gs.run()` for immediate-mode operation where the inputs and outputs are numpy arrays:\n   ```\n   # Pick latent vector.\n   rnd = np.random.RandomState(5)\n   latents = rnd.randn(1, Gs.input_shape[1])\n\n   # Generate image.\n   fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n   images = Gs.run(latents, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt)\n   ```\n   The first argument is a batch of latent vectors of shape `[num, 512]`. The second argument is reserved for class labels (not used by StyleGAN). The remaining keyword arguments are optional and can be used to further modify the operation (see below). The output is a batch of images, whose format is dictated by the `output_transform` argument.\n\n2. Use `Gs.get_output_for()` to incorporate the generator as a part of a larger TensorFlow expression:\n   ```\n   latents = tf.random_normal([self.minibatch_per_gpu] + Gs_clone.input_shape[1:])\n   images = Gs_clone.get_output_for(latents, None, is_validation=True, randomize_noise=True)\n   images = tflib.convert_images_to_uint8(images)\n   result_expr.append(inception_clone.get_output_for(images))\n   ```\n   The above code is from [metrics/frechet_inception_distance.py](./metrics/frechet_inception_distance.py). It generates a batch of random images and feeds them directly to the [Inception-v3](https://arxiv.org/abs/1512.00567) network without having to convert the data to numpy arrays in between.\n\n3. Look up `Gs.components.mapping` and `Gs.components.synthesis` to access individual sub-networks of the generator. Similar to `Gs`, the sub-networks are represented as independent instances of [dnnlib.tflib.Network](./dnnlib/tflib/network.py):\n   ```\n   src_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in src_seeds)\n   src_dlatents = Gs.components.mapping.run(src_latents, None) # [seed, layer, component]\n   src_images = Gs.components.synthesis.run(src_dlatents, randomize_noise=False, **synthesis_kwargs)\n   ```\n   The above code is from [generate_figures.py](./generate_figures.py). It first transforms a batch of latent vectors into the intermediate *W* space using the mapping network and then turns these vectors into a batch of images using the synthesis network. The `dlatents` array stores a separate copy of the same *w* vector for each layer of the synthesis network to facilitate style mixing.\n\nThe exact details of the generator are defined in [training/networks_stylegan.py](./training/networks_stylegan.py) (see `G_style`, `G_mapping`, and `G_synthesis`). The following keyword arguments can be specified to modify the behavior when calling `run()` and `get_output_for()`:\n\n* `truncation_psi` and `truncation_cutoff` control the truncation trick that that is performed by default when using `Gs` (&psi;=0.7, cutoff=8). It can be disabled by setting `truncation_psi=1` or `is_validation=True`, and the image quality can be further improved at the cost of variation by setting e.g. `truncation_psi=0.5`. Note that truncation is always disabled when using the sub-networks directly. The average *w* needed to manually perform the truncation trick can be looked up using `Gs.get_var('dlatent_avg')`.\n\n* `randomize_noise` determines whether to use re-randomize the noise inputs for each generated image (`True`, default) or whether to use specific noise values for the entire minibatch (`False`). The specific values can be accessed via the `tf.Variable` instances that are found using `[var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]`.\n\n* When using the mapping network directly, you can specify `dlatent_broadcast=None` to disable the automatic duplication of `dlatents` over the layers of the synthesis network.\n\n* Runtime performance can be fine-tuned via `structure='fixed'` and `dtype='float16'`. The former disables support for progressive growing, which is not needed for a fully-trained generator, and the latter performs all computation using half-precision floating point arithmetic.\n\n## Preparing datasets for training\n\nThe training and evaluation scripts operate on datasets stored as multi-resolution TFRecords. Each dataset is represented by a directory containing the same image data in several resolutions to enable efficient streaming. There is a separate *.tfrecords file for each resolution, and if the dataset contains labels, they are stored in a separate file as well. By default, the scripts expect to find the datasets at `datasets/<NAME>/<NAME>-<RESOLUTION>.tfrecords`. The directory can be changed by editing [config.py](./config.py):\n\n```\nresult_dir = 'results'\ndata_dir = 'datasets'\ncache_dir = 'cache'\n```\n\nTo obtain the FFHQ dataset (`datasets/ffhq`), please refer to the [Flickr-Faces-HQ repository](https://github.com/NVlabs/ffhq-dataset).\n\nTo obtain the CelebA-HQ dataset (`datasets/celebahq`), please refer to the [Progressive GAN repository](https://github.com/tkarras/progressive_growing_of_gans).\n\nTo obtain other datasets, including LSUN, please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided [dataset_tool.py](./dataset_tool.py):\n\n```\n> python dataset_tool.py create_lsun datasets/lsun-bedroom-full ~/lsun/bedroom_lmdb --resolution 256\n> python dataset_tool.py create_lsun_wide datasets/lsun-car-512x384 ~/lsun/car_lmdb --width 512 --height 384\n> python dataset_tool.py create_lsun datasets/lsun-cat-full ~/lsun/cat_lmdb --resolution 256\n> python dataset_tool.py create_cifar10 datasets/cifar10 ~/cifar10\n> python dataset_tool.py create_from_images datasets/custom-dataset ~/custom-images\n```\n\n## Training networks\n\nOnce the datasets are set up, you can train your own StyleGAN networks as follows:\n\n1. Edit [train.py](./train.py) to specify the dataset and training configuration by uncommenting or editing specific lines.\n2. Run the training script with `python train.py`.\n3. The results are written to a newly created directory `results/<ID>-<DESCRIPTION>`.\n4. The training may take several days (or weeks) to complete, depending on the configuration.\n\nBy default, `train.py` is configured to train the highest-quality StyleGAN (configuration F in Table 1) for the FFHQ dataset at 1024&times;1024 resolution using 8 GPUs. Please note that we have used 8 GPUs in all of our experiments. Training with fewer GPUs may not produce identical results &ndash; if you wish to compare against our technique, we strongly recommend using the same number of GPUs.\n\nExpected training times for the default configuration using Tesla V100 GPUs:\n\n| GPUs | 1024&times;1024  | 512&times;512    | 256&times;256    |\n| :--- | :--------------  | :------------    | :------------    |\n| 1    | 41 days 4 hours  | 24 days 21 hours | 14 days 22 hours |\n| 2    | 21 days 22 hours | 13 days 7 hours  | 9 days 5 hours   |\n| 4    | 11 days 8 hours  | 7 days 0 hours   | 4 days 21 hours  |\n| 8    | 6 days 14 hours  | 4 days 10 hours  | 3 days 8 hours   |\n\n## Evaluating quality and disentanglement\n\nThe quality and disentanglement metrics used in our paper can be evaluated using [run_metrics.py](./run_metrics.py). By default, the script will evaluate the Fr&eacute;chet Inception Distance (`fid50k`) for the pre-trained FFHQ generator and write the results into a newly created directory under `results`. The exact behavior can be changed by uncommenting or editing specific lines in [run_metrics.py](./run_metrics.py).\n\nExpected evaluation time and results for the pre-trained FFHQ generator using one Tesla V100 GPU:\n\n| Metric    | Time      | Result   | Description\n| :-----    | :---      | :-----   | :----------\n| fid50k    | 16 min    | 4.4159   | Fr&eacute;chet Inception Distance using 50,000 images.\n| ppl_zfull | 55 min    | 664.8854 | Perceptual Path Length for full paths in *Z*.\n| ppl_wfull | 55 min    | 233.3059 | Perceptual Path Length for full paths in *W*.\n| ppl_zend  | 55 min    | 666.1057 | Perceptual Path Length for path endpoints in *Z*.\n| ppl_wend  | 55 min    | 197.2266 | Perceptual Path Length for path endpoints in *W*.\n| ls        | 10 hours  | z: 165.0106<br>w: 3.7447 | Linear Separability in *Z* and *W*.\n\nPlease note that the exact results may vary from run to run due to the non-deterministic nature of TensorFlow.\n\n## Acknowledgements\n\nWe thank Jaakko Lehtinen, David Luebke, and Tuomas Kynk&auml;&auml;nniemi for in-depth discussions and helpful comments; Janne Hellsten, Tero Kuosmanen, and Pekka J&auml;nis for compute infrastructure and help with the code release.\n",
            "readme_url": "https://github.com/khurram702/StyleBasedGAN",
            "frameworks": [
                "TensorFlow",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Rethinking the Inception Architecture for Computer Vision",
            "arxiv": "1512.00567",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.00567v3",
            "abstract": "Convolutional networks are at the core of most state-of-the-art computer\nvision solutions for a wide variety of tasks. Since 2014 very deep\nconvolutional networks started to become mainstream, yielding substantial gains\nin various benchmarks. Although increased model size and computational cost\ntend to translate to immediate quality gains for most tasks (as long as enough\nlabeled data is provided for training), computational efficiency and low\nparameter count are still enabling factors for various use cases such as mobile\nvision and big-data scenarios. Here we explore ways to scale up networks in\nways that aim at utilizing the added computation as efficiently as possible by\nsuitably factorized convolutions and aggressive regularization. We benchmark\nour methods on the ILSVRC 2012 classification challenge validation set\ndemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%\ntop-5 error for single frame evaluation using a network with a computational\ncost of 5 billion multiply-adds per inference and with using less than 25\nmillion parameters. With an ensemble of 4 models and multi-crop evaluation, we\nreport 3.5% top-5 error on the validation set (3.6% error on the test set) and\n17.3% top-1 error on the validation set.",
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jonathon Shlens",
                "Zbigniew Wojna"
            ]
        },
        {
            "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
            "arxiv": "1801.03924",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.03924v2",
            "abstract": "While it is nearly effortless for humans to quickly assess the perceptual\nsimilarity between two images, the underlying processes are thought to be quite\ncomplex. Despite this, the most widely used perceptual metrics today, such as\nPSNR and SSIM, are simple, shallow functions, and fail to account for many\nnuances of human perception. Recently, the deep learning community has found\nthat features of the VGG network trained on ImageNet classification has been\nremarkably useful as a training loss for image synthesis. But how perceptual\nare these so-called \"perceptual losses\"? What elements are critical for their\nsuccess? To answer these questions, we introduce a new dataset of human\nperceptual similarity judgments. We systematically evaluate deep features\nacross different architectures and tasks and compare them with classic metrics.\nWe find that deep features outperform all previous metrics by large margins on\nour dataset. More surprisingly, this result is not restricted to\nImageNet-trained VGG features, but holds across different deep architectures\nand levels of supervision (supervised, self-supervised, or even unsupervised).\nOur results suggest that perceptual similarity is an emergent property shared\nacross deep visual representations.",
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A. Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ]
        },
        {
            "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
            "arxiv": "1812.04948",
            "year": 2018,
            "url": "http://arxiv.org/abs/1812.04948v3",
            "abstract": "We propose an alternative generator architecture for generative adversarial\nnetworks, borrowing from style transfer literature. The new architecture leads\nto an automatically learned, unsupervised separation of high-level attributes\n(e.g., pose and identity when trained on human faces) and stochastic variation\nin the generated images (e.g., freckles, hair), and it enables intuitive,\nscale-specific control of the synthesis. The new generator improves the\nstate-of-the-art in terms of traditional distribution quality metrics, leads to\ndemonstrably better interpolation properties, and also better disentangles the\nlatent factors of variation. To quantify interpolation quality and\ndisentanglement, we propose two new, automated methods that are applicable to\nany generator architecture. Finally, we introduce a new, highly varied and\nhigh-quality dataset of human faces.",
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ]
        },
        {
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "arxiv": "1409.1556",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.1556v6",
            "abstract": "In this work we investigate the effect of the convolutional network depth on\nits accuracy in the large-scale image recognition setting. Our main\ncontribution is a thorough evaluation of networks of increasing depth using an\narchitecture with very small (3x3) convolution filters, which shows that a\nsignificant improvement on the prior-art configurations can be achieved by\npushing the depth to 16-19 weight layers. These findings were the basis of our\nImageNet Challenge 2014 submission, where our team secured the first and the\nsecond places in the localisation and classification tracks respectively. We\nalso show that our representations generalise well to other datasets, where\nthey achieve state-of-the-art results. We have made our two best-performing\nConvNet models publicly available to facilitate further research on the use of\ndeep visual representations in computer vision.",
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ffhq-dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://drive.google.com/open?id=1u2xu7bSrWxrbUxk-dT-UvEJq8IjdmNTP"
                    }
                }
            },
            {
                "name": "Flickr-Faces-HQ dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/NVlabs/ffhq-dataset"
                    }
                }
            },
            {
                "name": "dataset_tool.py",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/khurram702/StyleBasedGAN/blob/master/./dataset_tool.py"
                    }
                }
            },
            {
                "name": "CelebA"
            },
            {
                "name": "LSUN Bedroom"
            },
            {
                "name": "ImageNet"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999940096291876,
        "task": "Image Generation",
        "task_prob": 0.9901251214351039
    }
}