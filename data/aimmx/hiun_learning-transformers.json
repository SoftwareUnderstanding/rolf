{
    "visibility": {
        "visibility": "public"
    },
    "name": "learning-transformers",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "hiun",
                "owner_type": "User",
                "name": "learning-transformers",
                "url": "https://github.com/hiun/learning-transformers",
                "stars": 1,
                "pushed_at": "2020-11-28 11:03:41+00:00",
                "created_at": "2020-10-31 09:23:01+00:00",
                "language": "Python",
                "description": "Transformers Tutorials with Open Source Implementations",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "assets",
                "sha": "848d325bde94536063a86b7548e2b518f05444a3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hiun/learning-transformers/tree/main/assets"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "modeling_bert.py",
                "sha": "9b1d939b817c9aeddce803481724a1b09703cec0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hiun/learning-transformers/blob/main/modeling_bert.py"
                    }
                },
                "size": 24390
            },
            {
                "type": "code",
                "name": "modeling_bert_test.py",
                "sha": "cdc64e2a30cccde8f93cc1b3f49bc846a1076978",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hiun/learning-transformers/blob/main/modeling_bert_test.py"
                    }
                },
                "size": 2854
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "d0d86627e365554b95973c2b0b6489da24997939",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hiun/learning-transformers/blob/main/requirements.txt"
                    }
                },
                "size": 16
            },
            {
                "type": "code",
                "name": "train_eval.log",
                "sha": "848e69759dac3b4b7abbb825298cca093cb74ad1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hiun/learning-transformers/blob/main/train_eval.log"
                    }
                },
                "size": 18011
            }
        ]
    },
    "authors": [
        {
            "name": "Hiun Kim",
            "github_id": "hiun"
        }
    ],
    "tags": [],
    "description": "Transformers Tutorials with Open Source Implementations",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/hiun/learning-transformers",
            "stars": 1,
            "issues": true,
            "readme": "# learning-transformers\nThis repository contains a fork of the Transformer model from https://github.com/huggingface/transformers. Unlike the original source code, which is a library, this repository is runnable stand-alone code for language modeling tasks.\n\nThe source code on such open source libraries are great, however, sometimes it is difficult to read all code for simply running and experimenting with a model. For example, preprocessing data, define a train-eval loop, integrating a model into that loop, these tasks are essential to write machine learning programs but it not always easy to looking source code from a large open source project.\n\nThis repository combines open source model code and tutorial level preprocessing, and train-eval code. As a result, we expect readers can easily understand how Transformer models are implemented and work from scratch.\n\nThis repository partly uses source code from the following sources:\n## reference materials\n\n- Preprocessing, Train-Eval loop (except model): https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n\n- Transformer model: https://github.com/huggingface/transformers/blob/a7d46a060930242cd1de7ead8821f6eeebb0cd06/src/transformers/models/bert/modeling_bert.py\n\n\n## informal description of transformers model\nWhat is a model? What is their role? I think a good model needs to have a good [Inductive Bias](https://en.wikipedia.org/wiki/Inductive_bias), which means it has good generalization capability to unseen example during training.\n\nThe difference between the Neural Network method of learning and other learning paradigm is that the Neural Network method learns from data by making a good representation of that data. On the contrary, many other methods learn by features that are manually selected by humans.\n\nThe Transformer model is one of the most popular representation generators of Neural Network methods of learning. Because of its general representation processing mechanism such as Attention-based learning, many recent advancements of deep learning rely on it.\n\nSo what actually Transformers do? What modules comprise Transformers? What are their implications? This is a natural question of mine as a beginner.\n\n## What Transformers do? (What Models do?)\nBecause the Transformer model is a special case of the deep learning model. Let's look at what the Deep Learning model generally does. The Deep Learning model can be viewed as [Directed Acyclic Graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (DAG; a graph structure that has no connection to ancestry node).\n\n<center>\n<img src=\"./assets/dag.png\" width=\"400\" />\n</center>\n\nDeep learning model process matrix, which roles a computable representation of input (such as text, image, etc.). The computation of the Deep Learning model to retrieve more abstract representation of input representation.\n\n<center>\n<img src=\"./assets/imagenet.png\" width=\"500\" />\n</center>\n\nAs [example shows in imagenet](https://towardsdatascience.com/transfer-learning-and-image-classification-using-keras-on-kaggle-kernels-c76d3b030649), the earlier part of DAG (a matrix transformation 'layer' of Deep Learning model) do generate low-level features (e.g. shape of stroke that consists cars) from input data. While the latter part of DAG does transform them into high-level features (e.g. wheels of a car or this is Ford Model T).\n\n<center>\n<img src=\"./assets/self-attention.png\" width=\"700\" />\n</center>\n\nWe can [observe the same for natural language processing](https://nlp.seas.harvard.edu/2018/04/03/attention.html). The above example shows a step in making a representation of an input sentence for a Machine Translation task using the Transformer Model. The `Encoder Layer 2` shows attention in the earlier part of DAG. It visualizes what to Attend for given sentences in given sentences. The diagonal line shows most of the words are self-attended since it is natural that low-level features of the text are in word-level. However, in the latter part of DAG, we show the trend that attention is concentrated on specific words, which refers to an abstract representation of given sentences that are crucial for a given task (such as word 'bad' is important for movie review sentiment analysis)\n\n\nYou may wonder about rules for shaping abstract representation, that is the role of the learning algorithm.\nFor example, Neural Network is a Learning algorithm that depends on its task setup.\nA task like classifying text or image can be viewed as a classic Supervised Learning algorithm.\nIn Supervised Deep Learning cases, shaping abstract representation is the role of optimization methods (e.g. backpropagation with learning rate scheduling) that utilize label data.\nMore specifically in the training model, when the model outputs inference result, that is compared against the label, and the difference is backpropagated through the model, this enforces (thus trains) model parameters to generate more task-oriented representation. (Note that the task can be multiple, in the case of multi-task learning. Or task can be much more general, in the case of few-shot learning.)\n     \n\n## Modules of Transformers and their Implications\n\nPreviously, we revisit the single significant property of the machine learning model that good models have good inductive bias performance to achieve good generalization performance.\nDeep learning is Neural Network methods of learning, focusing on learning thus able to deriving task-oriented representation from data.\nThe Transformer is one of the popular methods of Deep learning that is good at learning textual or visual representation from data.\nThe Deep Learning model can be viewed as matrix processing DAG; where input data is matrix and DAG is gradually learning representation of data from its low to high-level features.\nHigh-level features are determined by a learning algorithm, especially backpropagation with label data. \n \nI think all of the above parts have meaning to build good machine learning algorithms and systems. Let's look at how the above concepts reflect in the Deep Learning model (the DAG) of Transformers Encoder.\n\n### Model Architecture\n\n![arch](./assets/arch.png)\nThe above figure shows the Transformer model and its mapping between Python classes.\nWe only look at the Transformer encoder, because the encoder and decoder share core functionality DAGs. \n\n### Input Embedding Layer\nThe role of the input embedding layer is to create a meaningful (task-oriented) representation of sentences - embeddings - from natural language. In this example, we receive word ids and create a 768 dimension vector.\nThe 768 dimension vector - the embeddings is a low-level representation of sentences, it is shaped by a mixture of the literal representation of text w and weight defined by backpropagation for objectives of the task.\nThe embedding layer concatenates embedding from other data, such as token type and positional information.\nTo add regularization for embedding, it applies layer normalization and Dropout to reduce training time and improve generalization respectively.\nThis is implemented in `BertEmbedding` Python Class.\n\n### Model Scaffolding\nMultiple layers of transformation are required to make a high-level representation.\nThe `BertEncoder` class stacks multiple self-attention layers of a transformer to implement this.\nIt initializes the transformer layer to input matrix one another.\n\n### Inside of Single Model\nThe single transformer model is comprised of the attention part, intermediate processing part, and the output processing part.\nThe first module `BertAttention` applies self-attention and by call `BertSelfAttention`and normalizes its output with `BertSelfOutput` module.\nThe second module `BertIntermediate` layer is for increasing model capacity, by adding one sparser layer (`hidden_size` 768 dim to `intermediate_size` 3072 dim).\nThe last layer `BertOutput` is for bottleneck layer (`intermediate_size` 3072 dim to `hidden_size` 768 dim again) and do additional regularization.\n\n`BertLayer` connects above three layers, to be stackable.\n\n### Self-Attention\n\n<center>\n<img src=\"./assets/seq2seq.png\" width=\"250\" />\n</center>\n\nI think the concept of attention no more than just matrix transformation using *representations from relevant information sources*.\nAbove figure shows [early work on attention](https://arxiv.org/abs/1409.0473).\nIn this case, the sequential encoder (RNN) generates a matrix in a bidirectional context, and their output probabilities are concatenated and applied to a given step in the decoding process for making relevant representation for a given task.\n\nInformally, attention is just a probability-contained matrix to *scale* representation to be more appropriate for the task.\nAttention is a signal for adjustment of representation, thus it reduces the model size to achieve the same quality performance.\n\nIn terms of viewing attention by *scale* representation, what is a good information source for such scaling?\nThe idea of self-attention is finding the importance of each word and score them, and apply them to scale representation for a given task.\nThe mechanism of finding scores are multiply query and key matrix.\nMore specifically it obtains attention score matrix by element-wise multiplication result between single query word and all key words.\nFinally, the value vector is scaled by the respected attention score of its word.\nA higher score implies that given words have a strong relation to words on those sentences, the training procedures reveal a neural network to define a relationship by adjusting weights which are guided by loss function through backpropagation.\nAttention score totally depends on representation, which is the result of linear transformations from the embedding. \nIn the training setting, weights of such embedding and linear transformations will be adjusted for generating maximum attention score helpful to resolve the given task.\n\n\n`BertSelfAttention` class implements this. It initializes, a linear layer of query, key, and value.\nPerform matrix multiplication with a query and key value.\nScaling it and make probabilities using Softmax function before applying to value.\nValue is an adjusted representation for specific tasks.\nNote that the adjustment is gradually refined across stacked layers.\n\n\n![self-attention-vis](./assets/self-attention-vis.png)\n\n[Above image](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb) shows encoder self-attention (input-input) for machine translation. The image illustrates words related to `it_`. At layer 1, the word has a relationship to most words however, as layer getting compute abstract representation, it shows a strong relationship to a noun like `animal_` or `street_`.\n\n\nNote that the multi-head means just splitting input representation by number of heads,\nTo allows the model to attend different subspaces of representation without averaging it ([ref](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#multi-head-self-attention) and [original paper](https://arxiv.org/abs/1706.03762)).\n\nSo Multi-head property can be viewed as regularizing to attend different subspace of representation, by preventing averaging overall attention score.\n\nIn summary, self-attention is just matrix multiplication that essentially does adjust the importance of each word in a sentence to make a better representation.\n\n### Additional Model Capacity and Regularization\n\nAfter retrieving self-attention applied representation from the previous step,\nThe `BertSelfOutput` module provides additional linear transformation with layer normalization and Dropout.\nI think the role of this component is to provide additional model capacity and add Regularization.\n\n## Our Task: Language Modeling\nWe use our model as language modeling.\nLanguage modeling is an NLP task that predicts the next word from a given sequence of words.\nThe Transformer approach to language modeling is text classification, where calculate conditional probabilities of the next word in given previous words.\n\n`BertModel` creates embedding and passes it to the encoder.\n The encoder provides a representation of a sentence,\n and also the whole sentence (accumulated hidden states from all layers) is passed to the pooler for the whole sentence by making representation for whole sentences.\n \n \n`BertForLM` takes the last hidden state (since language modeling is a classification for the next word) and apply Dropout and add passes to the linear layer to get a conditional distribution result.\nIt calculates loss after inference is done.\n\n \n\n### Data Preprocessing\nFirst of all, it splits data with N number of the batch for each train, eval loop.\n\n\n<center>\n<img src=\"./assets/lm-data.png\" width=\"750\" />\n</center>\n\nThe above figure shows the data prepared for training.\nEach row in the tensor contains a batch size number of words (20 in our case).\nAs example shows, a single sentence is represented by multiple batches with the same column number.\n\n\n### Train/Eval Loop\n\n#### training function\n- Set model to training mode\n- Define criterion, optimizer, and scheduler\n- Fetch data\n- Pass to model\n- Calculate loss\n- Backpropagation\n- Gradient clipping\n- Optimizer stepping\n- Loss calculation\n- Logging (epoch number, batches, loss, ppl)\n\n#### evaluate function\n- Set model to evaluation mode\n- No grad definition\n- Fetch data\n- Pass to model (deterministic process)\n- Total loss calculation\n\n### Train/Eval Result & Ablation Studies\n\n- `train_eval.log` shows numbers. Since train data is small, the model is close to underfitting.\n- In larger transformer layer, more underfitting occur, thus loss function gets higher\n\n- 1layer transformer encoder for LM task\n![train-valid-graph](./assets/train_val_graph.png)\n\n- 12layer transformer encoder for LM task (underfitting due to large model capacity)\n![train-valid-graph_12layer](./assets/train_val_graph_12layer.png)\n\n\n\n### Model Option\n\nA common configuration for constructing the bert model. Options include,\n> https://huggingface.co/transformers/main_classes/configuration.html#transformers.PretrainedConfig\n\n- `vocab_size` for word embedding\n- `hidden_size` for model internal(hidden) representation size\n- `num_hidden_layers` for number of transformer layer to transforms hidden representation\n- `num_attention_heads` split output of feedforward layer for different attention heads\n- `intermediate_size` size of intermediate layer after self attention\n- `hidden_act` activation function for intermediate layer after self attention\n- `hidden_dropout_prob` hidden layer dropout prob. for regularization\n- `attention_probs_dropout_prob` self attention layer dropout prob. for regularization\n- `max_position_embeddings` max size of positional embedding legnth\n- `type_vocab_size` max size of type vocab length (...)\n- `layer_norm_eps` layer norm option\n- `output_attentions` option for output attention prob\n- `output_hidden_states` option for output hidden state\n\n\n## References & Resources\n\n### On Implementations\n- Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.\n> https://github.com/huggingface/transformers\n- Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n> https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n- transformers/modeling_bert.py at a7d46a060930242cd1de7ead8821f6eeebb0cd06 \u00b7 huggingface/transformers (GitHub)\n> https://github.com/huggingface/transformers/blob/a7d46a060930242cd1de7ead8821f6eeebb0cd06/src/transformers/models/bert/modeling_bert.py\n- Configuration - transformers 3.5.0 documentation \n> https://huggingface.co/transformers/main_classes/configuration.html#transformers.PretrainedConfig\n- The Annotated Transformer\n> https://nlp.seas.harvard.edu/2018/04/03/attention.html\n- Tensor2Tensor Intro\n> https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb\n\n### On Concepts\n- Inductive bias - Wikipedia\n> https://en.wikipedia.org/wiki/Inductive_bias\n- Directed acyclic graph - Wikipedia \n> https://en.wikipedia.org/wiki/Directed_acyclic_graph\n- Transfer learning and Image classification using Keras on Kaggle kernels.\n> https://towardsdatascience.com/transfer-learning-and-image-classification-using-keras-on-kaggle-kernels-c76d3b030649\n- Attention? Attention!\n> https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#multi-head-self-attention\n- Attention Is All You Need\n> https://arxiv.org/abs/1706.03762\n",
            "readme_url": "https://github.com/hiun/learning-transformers",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
            "arxiv": "1409.0473",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.0473v7",
            "abstract": "Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.",
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ]
        },
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "Wikipedia"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999997532569063,
        "task": "Machine Translation",
        "task_prob": 0.9815807429172781
    }
}