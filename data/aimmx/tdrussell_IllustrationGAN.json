{
    "visibility": {
        "visibility": "public"
    },
    "name": "IllustrationGAN",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "tdrussell",
                "owner_type": "User",
                "name": "IllustrationGAN",
                "url": "https://github.com/tdrussell/IllustrationGAN",
                "stars": 265,
                "pushed_at": "2016-08-31 16:00:56+00:00",
                "created_at": "2016-08-30 15:38:42+00:00",
                "language": "Python",
                "description": "A simple, clean TensorFlow implementation of Generative Adversarial Networks with a focus on modeling illustrations.",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "custom_ops.py",
                "sha": "cfc07ad3b3c0199b44ba0f5a6d5bbbad17b7ece4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tdrussell/IllustrationGAN/blob/master/custom_ops.py"
                    }
                },
                "size": 10460
            },
            {
                "type": "code",
                "name": "generate.py",
                "sha": "e74aca31a7663d1c4f3ce0a9813ef75e7976a60a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tdrussell/IllustrationGAN/blob/master/generate.py"
                    }
                },
                "size": 2165
            },
            {
                "type": "code",
                "name": "images",
                "sha": "20aa7bc301aabb5fdc1350b6d52f36ed37c1731c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tdrussell/IllustrationGAN/tree/master/images"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "input.py",
                "sha": "904b26bff81bc5eb914a77d17d07dcdc78a7fe85",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tdrussell/IllustrationGAN/blob/master/input.py"
                    }
                },
                "size": 5201
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "dc923eb7ba04121dcb6c60183db2739553e9e84b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tdrussell/IllustrationGAN/blob/master/model.py"
                    }
                },
                "size": 7387
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "f629159f80ad42d67986502d34a17f109d590733",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/tdrussell/IllustrationGAN/blob/master/train.py"
                    }
                },
                "size": 6230
            }
        ]
    },
    "authors": [
        {
            "name": "tdrussell",
            "github_id": "tdrussell"
        }
    ],
    "tags": [],
    "description": "A simple, clean TensorFlow implementation of Generative Adversarial Networks with a focus on modeling illustrations.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/tdrussell/IllustrationGAN",
            "stars": 265,
            "issues": true,
            "readme": "# IllustrationGAN\nA simple, clean TensorFlow implementation of Generative Adversarial Networks with a focus on modeling illustrations.\n\n## Generated Images\nThese images were generated by the model after being trained on a custom dataset of about 20,000 anime faces that were automatically cropped from illustrations using a face detector.\n![Generated Images](images/montage.png?raw=True)\n\n## Checking for Overfitting\nIt is theoretically possible for the generator network to memorize training set images rather than actually generalizing and learning to produce novel images of its own. To check for this, I randomly generate images and display the \"closest\" images in the training set according to mean squared error. The top row is randomly generated images, the columns are the closest 5 images in the training set.\n\n![Overfitting Check](images/overfitting_check.png?raw=True)\n\nIt is clear that the generator does not merely learn to copy training set images, but rather generalizes and is able to produce its own unique images.\n\n## How it Works\nGenerative Adversarial Networks consist of two neural networks: a discriminator and a generator. The discriminator receives both real images from the training set and generated images produced by the generator. The discriminator outputs the probability that an image is real, so it is trained to output high values for the real images and low values for the generated ones. The generator is trained to produce images that the discriminator thinks are real. Both the discriminator and generator are trainined simultaneously so that they compete against each other. As a result of this, the generator learns to produce more and more realistic images as it trains.\n\n## Model Architecture\nThe model is based on [DCGANs](http://arxiv.org/abs/1511.06434), but with a few important differences:\n\n1. No strided convolutions. The generator uses bilinear upsampling to upscale a feature blob by a factor of 2, followed by a stride-1 convolution layer. The discriminator uses a stride-1 convolution followed by 2x2 max pooling.\n\n2. Minibatch discrimination. See [Improved Techniques for Training GANs](http://arxiv.org/abs/1606.03498) for more details.\n\n3. More fully connected layers in both the generator and discriminator. In DCGANs, both networks have only one fully connected layer.\n\n4. A novel regularization term applied to the generator network. Normally, increasing the number of fully connected layers in the generator beyond one triggers one of the most common failure modes when training GANs: the generator \"collapses\" the z-space and produces only a very small number of unique examples. In other words, very different z vectors will produce nearly the same generated image. To fix this, I add a small auxiliary z-predictor network that takes as input the output of the last fully connected layer in the generator, and predicts the value of z. In other words, it attempts to learn the inverse of whatever function the generator fully connected layers learn. The z-predictor network and generator are trained together to predict the value of z. This forces the generator fully connected layers to only learn those transformations that preserve information about z. The result is that the aformentioned collapse no longer occurs, and the generator is able to leverage the power of the additional fully connected layers.\n\n## Training the Model\nDependencies: TensorFlow, PrettyTensor, numpy, matplotlib\n\nThe custom dataset I used is too large to add to a Github repository; I am currently finding a suitable way to distribute it. Instructions for training the model will be in this readme after I make the dataset available.\n",
            "readme_url": "https://github.com/tdrussell/IllustrationGAN",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Improved Techniques for Training GANs",
            "arxiv": "1606.03498",
            "year": 2016,
            "url": "http://arxiv.org/abs/1606.03498v1",
            "abstract": "We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes.",
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ]
        },
        {
            "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "arxiv": "1511.06434",
            "year": 2015,
            "url": "http://arxiv.org/abs/1511.06434v2",
            "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations.",
            "authors": [
                "Alec Radford",
                "Luke Metz",
                "Soumith Chintala"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999998366766825,
        "task": "Image Generation",
        "task_prob": 0.9658280246243702
    },
    "training": {
        "datasets": [
            {
                "name": "SVHN"
            },
            {
                "name": "MNIST"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ImageNet"
            }
        ]
    }
}