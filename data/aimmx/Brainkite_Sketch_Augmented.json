{
    "visibility": {
        "visibility": "public"
    },
    "name": "Sketch_Augmented",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Brainkite",
                "owner_type": "User",
                "name": "Sketch_Augmented",
                "url": "https://github.com/Brainkite/Sketch_Augmented",
                "stars": 5,
                "pushed_at": "2020-05-21 08:52:13+00:00",
                "created_at": "2020-02-24 05:24:37+00:00",
                "language": "Jupyter Notebook",
                "description": "WIP - Project that generates a realistic view from an ongoing architecture sketch",
                "frameworks": []
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "aaa145ea18f16acfeea5236017c46c06251cf95c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Brainkite/Sketch_Augmented/blob/master/.gitignore"
                    }
                },
                "size": 143
            },
            {
                "type": "code",
                "name": "Cyclegan_model.ipynb",
                "sha": "1357e756650a40e946e774a1d62eb2e457f4df42",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Brainkite/Sketch_Augmented/blob/master/Cyclegan_model.ipynb"
                    }
                },
                "size": 2040130
            },
            {
                "type": "code",
                "name": "FeatLoss_Unet_GAN.ipynb",
                "sha": "082d6f845191554073ddf2dc9f649680da6fc7ee",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Brainkite/Sketch_Augmented/blob/master/FeatLoss_Unet_GAN.ipynb"
                    }
                },
                "size": 4852634
            },
            {
                "type": "code",
                "name": "Utils.ipynb",
                "sha": "24eb193f4bf24449708ed65523cd2faf5cef6e4f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Brainkite/Sketch_Augmented/blob/master/Utils.ipynb"
                    }
                },
                "size": 128674
            },
            {
                "type": "code",
                "name": "post_sketch_aug_files",
                "sha": "191dcb9be20cc5307e5b7b90020b071aa138c2e1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Brainkite/Sketch_Augmented/tree/master/post_sketch_aug_files"
                    }
                },
                "num_files": 12
            }
        ]
    },
    "authors": [
        {
            "name": "Antonin Sumner",
            "email": "antonin.sumner@gmail.com",
            "github_id": "Brainkite"
        }
    ],
    "tags": [
        "cyclegan",
        "architecture",
        "neural-networks",
        "hand-drawings",
        "sketches",
        "generative-adversarial-network",
        "style-transfer",
        "edge-detection"
    ],
    "description": "WIP - Project that generates a realistic view from an ongoing architecture sketch",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Brainkite/Sketch_Augmented",
            "stars": 5,
            "issues": true,
            "readme": "# Sketch_Augmented\n\nHaving worked as an architect for the last 6 years, I have witnessed hand drawing and sketching disapear more and more from the architect's toolbox. Even though this tool is still essential for the architect to reflect and find innovative ideas and esthetic principles.\n\nThe vision driving this project is to make evolve the architect's workflow with the help of Artificial Intelligence and more specificly here the tool of architecture sketching itself. To invent a more organic way of drawing and shaping buildings.\n\nWith the help of Artificial inteligence methods, the end product will generate realistic views interpreted from the sketch as it's being drawn. Giving instant feedback to the user, he will be able to work with a more objective and informed representation of his idea.\n\nIn this article we will explain our approach and our research process while explainning some of the artificial intelligence techniques we're using.\n\n![image.png](/post_sketch_aug_files/att_00000.png)\n\n## 1. Finding the right model\n\nTo be able to map a hand drawn sketch to a realistic image we need the help of artificial neural networks. They have proven in the recent years to be quite efficient in vision and image generation. These neural networks are huge mathematical functions with an enormous amount of adjustable parameters. They are able learn a task by seeing a collection of input and output examples. By \"seeing\" we mean passing each input through the model, comparing the result and the target output with the help a \"loss function\" and correcting the model's parameters. The loss function is here to process the difference there is between the 2 outputs. Finaly, this learning process is regulated by an optimizer algorithm that will allow the neural net to learn quicker and better.\n\nFor out project we will need a specific kind of neural networks called \"generative adversarial networks\" (GAN), these models have the amazing ability to generate new images in differents ways.\nCurrently 2 major types of GANs seems to be suited for our task: U-net GANs and CycleGan.\n\nWe will begin to explore the capabilities of those 2 models and how they fit to our project's needs.\n\n### 1.1. What is a U-net models\n\nThe U-net architecture was initialy created for biomedical image segmentation (detecting and detouring objects in an image).\n\nThe general logic behind this architecture is to have a downsampling phase called the \"encoder\" and an upsampling phase called \"decoder\".\nDuring the encoding phase, the image size is progressively reduced as more and more semantic and content is extracted. At the end of the encoding phase we get a \"semantic\" vector that will then be progressively upsamplede. But since this vector has lost all shape information, we progressively mix the generated image with the original image's shape using what we call \"skip connexions\". (You can read more about this architecture in the original paper https://arxiv.org/abs/1505.04597)\n\n![image.png](/post_sketch_aug_files/att_00002.png)\n\nThis kind of architecture was also proven efficient for image enhancement and restauration when paired with a \"feature loss\" function. They can enhance image resolution, clean signal noise or even fill up holes in the image.\n\n![image.png](/post_sketch_aug_files/att_00003.png)\n\nFor a generative model to be able to learn we need a tool to evaluate the accuracy of the generated image. We usualy use a second model called a \"critic\" that will learn to identify the generated image or the true image. But this method has not always proven good result for realistic image generation. Instead we use a pre-trained \"classification\" model that is normaly able to predict what objects are in the image. But instead of using the output of this model(it's a car or a horse), we pick values inside the model's layers that will represent features found in the image (textures, shapes, etc...). So when we pass the generated image and the target image, we want those values to be as close as possible.\n\n![image.png](/post_sketch_aug_files/att_00004.png)\n\n### 1.2. What is CycleGan\n\nCyclegan model basically can transfer image texture style to another texture style (style transfer). It is called this way because it has the ability to make the convertion in both directions. The most popular example is the photo to painting and reverse application:\n\n![image.png](/post_sketch_aug_files/att_00005.png)\n\nWhile this model is very good at treating textures, it handles poorly shapes and objects. Also CycleGan can be more convenient for the dataset creation because it doesn't need pair-wise input and outputs.\n\nYou can learn more about CycleGAN on their creator's web-page: https://junyanz.github.io/CycleGAN/\n\n## 2. Building the dataset\n\nThe dataset is the collection of input and output example that will be used to train our model.\nFor our first implementation we will use a U-net model. These models need pair-wise examples so we have to build a dataset with pictures of buildings and their corresponding drawing. But it would be too long to produce real hand-drawn copies of enough architecture photos for such a dataset. Our first approach is to create an an image treatment script to transforms photos in something close to a hand-drawn image.\n\nThe script is pretty simple, we first reduce the contrast of the image and then apply a contour finding filter from the Pillow image treatment library.\n\n```python\nfrom PIL import Image\nfrom PIL import ImageFilter\n```\n\n```python\ndef change_contrast(img, level):\n    factor = (259 * (level + 255)) / (255 * (259 - level))\n    def contrast(c):\n        value = 128 + factor * (c - 128)\n        return max(0, min(255, value))\n    return img.point(contrast)\n```\n\n```python\ndef photo2sketch(fn, a):\n    img = Image.open(fn)\n    i = change_contrast(img, -100)\n    i = img.filter(ImageFilter.SMOOTH_MORE).filter(ImageFilter.CONTOUR).filter(ImageFilter.SMOOTH_MORE)\n    i = i.convert('L')\n    i = i.convert('RGB')\n    i.save(inp_dir/fn.name)\n```\n\n```python\nparallel(photo2sketch, get_imgfs(out_dir))\n```\n\n\n\n\n\n![image.png](/post_sketch_aug_files/att_00006.png)\n\nThis sketch effect script has a tendancy to produce image that would correspond to very detailed hand drawings but we will work from that and enhance it later if needed.\n\nThe dataset is currently composed of 623 curated architecture pictures with their fake-sketch equivalent. This normaly isn't enough to train such a model but thanks to a technique called \"data augmentation\" we will automaticaly generate multiple variants of each image during the training by randomly cropping and flipping horizontally the images.\n\nWe then need to split this dataset in 3 separated subsets:\n- The training set, used to train the model\n- The validation set, composed of examples that won't be used to train the model but to evaluate his performaces and adjust our trainning strategies.\n- The test set, to estimate the model's accuracy and our train strategies.\n\nGood practice in deep learning teaches to split these sets with 95%, 25% and 25% of the dataset.\n\nBut good image generation is quite subjective so we will proceed differently and provide as much data as possible to the model so 90% in train set and 10% in valid set. Also Since the model will only train on fake sketch image, we will compose the test set with real hand made sketches and we will evaluate ourself the performance of the results generated.\n\n![image.png](/post_sketch_aug_files/att_00007.png)\n\n(Yes, we wand the model to ultimately be able to produce an interesting representation from the third image)\n\n# 3. The training results\n\nWe wont decribe in details here the code to build the model and the trainning process because it would need a hole new article to explain but you can access the jupyter notebook where it's done here: https://github.com/Brainkite/Sketch_Augmented/blob/master/FeatLoss_Unet_GAN.ipynb\n\nAfter 20 min of training on a NVIDIA P100 GPU, we quickly get pretty good results on the realistic image generation from the fake sketches.\nBellow are presented the input image shown to the model, the generated image by model and the target image wich is the original image from wich the fake sketch was created. This image beeing in the validation set, it has never been seen by the model, wich is pretty impressive.\n\n![image.png](/post_sketch_aug_files/att_00008.png)\n\nIn these results we realise that the model learned to accurately recognize the volumes and use the appropriate lighting and shadowing. Also it has a surprising ability to generate materials textures and transparencies (or is it?).\n\nNow we need to evaluate the model's performances on the test set with real life sketches.\n\n![image.png](/post_sketch_aug_files/att_00009.png)\n\nThe generated images are not as good but it's pretty encouraging. The model is still able to identify volumes and infer accurate lighting. Some shaded areas are in the dark while the outer faces are brighter. The model even added reflexion on some faces. On the other hand, vegetation is pretty poor because it's drawn in a very stylized way and the model can't make the connection between that and a real tree. But more importantly,  there is a total lack of materials textures. The model is capable to identify the concrete from the wood cladding, the paved ground from the asphalt and use appropriate colors, but it's unable to produce any texture on them.\n\n# 4. Next steps to improve the model\n\nThe conclusion that we could draw from these first results is that our script to generate sketch-like images is still providing too much information to the model. On the first look in a small resolution it doesn't look like it but whenn zoomed, the script is still keeping micro-contrasts in the textures that will not be provided in a real sketch but provide plenty information to the model to re-create accurate textures (on another note this may be an interesting lead to explore new image compression algorithms).\n\nTo make our model trainning more accurate and closer to real life examples, we need to combine our edge finding sketch effect with a texture filtering algorithm that will smoothen the textures while preserving shapes edges. Then running our initial script on the resulting images will produce cleaner sketch-like images.\n\nTo make this possible we will look into signal processing and computer graphics researchs. We will implement a texture filtering algorithm called \"Scale-aware Structure-Preserving Texture Filtering\" (Safiltering) described in this paper: http://cg.postech.ac.kr/papers/safiltering.pdf\n\n![image.png](/post_sketch_aug_files/att_00010.png)\n\nAlso we will explore other generative models architectures like CycleGan but also NVDIA's MUNIT (https://arxiv.org/abs/1804.04732) and GAUGAN (https://arxiv.org/pdf/1903.07291.pdf)\n",
            "readme_url": "https://github.com/Brainkite/Sketch_Augmented",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
            "arxiv": "1903.07291",
            "year": 2019,
            "url": "http://arxiv.org/abs/1903.07291v2",
            "abstract": "We propose spatially-adaptive normalization, a simple but effective layer for\nsynthesizing photorealistic images given an input semantic layout. Previous\nmethods directly feed the semantic layout as input to the deep network, which\nis then processed through stacks of convolution, normalization, and\nnonlinearity layers. We show that this is suboptimal as the normalization\nlayers tend to ``wash away'' semantic information. To address the issue, we\npropose using the input layout for modulating the activations in normalization\nlayers through a spatially-adaptive, learned transformation. Experiments on\nseveral challenging datasets demonstrate the advantage of the proposed method\nover existing approaches, regarding both visual fidelity and alignment with\ninput layouts. Finally, our model allows user control over both semantic and\nstyle. Code is available at https://github.com/NVlabs/SPADE .",
            "authors": [
                "Taesung Park",
                "Ming-Yu Liu",
                "Ting-Chun Wang",
                "Jun-Yan Zhu"
            ]
        },
        {
            "title": "Multimodal Unsupervised Image-to-Image Translation",
            "arxiv": "1804.04732",
            "year": 2018,
            "url": "http://arxiv.org/abs/1804.04732v2",
            "abstract": "Unsupervised image-to-image translation is an important and challenging\nproblem in computer vision. Given an image in the source domain, the goal is to\nlearn the conditional distribution of corresponding images in the target\ndomain, without seeing any pairs of corresponding images. While this\nconditional distribution is inherently multimodal, existing approaches make an\noverly simplified assumption, modeling it as a deterministic one-to-one\nmapping. As a result, they fail to generate diverse outputs from a given source\ndomain image. To address this limitation, we propose a Multimodal Unsupervised\nImage-to-image Translation (MUNIT) framework. We assume that the image\nrepresentation can be decomposed into a content code that is domain-invariant,\nand a style code that captures domain-specific properties. To translate an\nimage to another domain, we recombine its content code with a random style code\nsampled from the style space of the target domain. We analyze the proposed\nframework and establish several theoretical results. Extensive experiments with\ncomparisons to the state-of-the-art approaches further demonstrates the\nadvantage of the proposed framework. Moreover, our framework allows users to\ncontrol the style of translation outputs by providing an example style image.\nCode and pretrained models are available at https://github.com/nvlabs/MUNIT",
            "authors": [
                "Xun Huang",
                "Ming-Yu Liu",
                "Serge Belongie",
                "Jan Kautz"
            ]
        },
        {
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "arxiv": "1505.04597",
            "year": 2015,
            "url": "http://arxiv.org/abs/1505.04597v1",
            "abstract": "There is large consent that successful training of deep networks requires\nmany thousand annotated training samples. In this paper, we present a network\nand training strategy that relies on the strong use of data augmentation to use\nthe available annotated samples more efficiently. The architecture consists of\na contracting path to capture context and a symmetric expanding path that\nenables precise localization. We show that such a network can be trained\nend-to-end from very few images and outperforms the prior best method (a\nsliding-window convolutional network) on the ISBI challenge for segmentation of\nneuronal structures in electron microscopic stacks. Using the same network\ntrained on transmitted light microscopy images (phase contrast and DIC) we won\nthe ISBI cell tracking challenge 2015 in these categories by a large margin.\nMoreover, the network is fast. Segmentation of a 512x512 image takes less than\na second on a recent GPU. The full implementation (based on Caffe) and the\ntrained networks are available at\nhttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999470934053748,
        "task": "Image-to-Image Translation",
        "task_prob": 0.9692215004294056
    }
}