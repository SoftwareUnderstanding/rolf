{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Mutual Mean-Teaching (MMT)",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "yxgeee",
                "owner_type": "User",
                "name": "MMT",
                "url": "https://github.com/yxgeee/MMT",
                "stars": 408,
                "pushed_at": "2020-07-01 03:21:07+00:00",
                "created_at": "2020-01-06 06:46:37+00:00",
                "language": "Python",
                "description": "[ICLR-2020] Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification.",
                "license": "MIT License",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "1f12efb3865bc392b2a77547ecea0d6c16d3b071",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yxgeee/MMT/blob/master/.gitignore"
                    }
                },
                "size": 3664
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "c9f2357c01c100f8b89d636499e8495b962d7990",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yxgeee/MMT/blob/master/LICENSE"
                    }
                },
                "size": 1066
            },
            {
                "type": "code",
                "name": "examples",
                "sha": "95202e3b726e533b48dea5a3c728d02f9b677ce9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yxgeee/MMT/tree/master/examples"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "figs",
                "sha": "ba3cdca4b3904fb11f8bcd7afb949b9b9883a94d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yxgeee/MMT/tree/master/figs"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "mmt",
                "sha": "74a33f8c8a0146d2bcd454c633ddb3b42e9908fc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yxgeee/MMT/tree/master/mmt"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "294b0b109065656d96cea66a1a762002a9c263db",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yxgeee/MMT/tree/master/scripts"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "setup.cfg",
                "sha": "b88034e414bc7b80d686e3c94d516305348053ea",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yxgeee/MMT/blob/master/setup.cfg"
                    }
                },
                "size": 40
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "45f8280c7079e69d10f7bb23d8889e2fde7b0f15",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/yxgeee/MMT/blob/master/setup.py"
                    }
                },
                "size": 665
            }
        ]
    },
    "authors": [
        {
            "name": "Yixiao Ge",
            "email": "geyixiao831@gmail.com",
            "github_id": "yxgeee"
        },
        {
            "name": "Chris Andrew",
            "email": "chris.andrew@research.iiit.ac.in",
            "github_id": "chrizandr"
        }
    ],
    "tags": [
        "unsupervised-domain-adaptation",
        "person-re-identification",
        "unsupervised-learning",
        "pseudo-labels",
        "person-reid",
        "image-retrieval",
        "domain-adaptation",
        "cross-domain",
        "person-retrieval",
        "open-set-domain-adaptation",
        "iclr2020"
    ],
    "description": "[ICLR-2020] Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/yxgeee/MMT",
            "stars": 408,
            "issues": true,
            "readme": "![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)\n![PyTorch 1.1](https://img.shields.io/badge/pytorch-1.1-yellow.svg)\n\n# Mutual Mean-Teaching (MMT)\n\nThe *official* implementation for the [Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification](https://openreview.net/forum?id=rJlnOhVYPS) which is accepted by [ICLR-2020](https://iclr.cc).\n\n![framework](figs/framework.png)\n\n## What's New\n#### [Jul 1st, 2020]\n+ We did the code refactoring to support distributed training and more features. Please see [OpenUnReID](https://github.com/open-mmlab/OpenUnReID).\n\n#### [Mar 27th, 2020]\n+ We wrote a Chinese blog about this paper at [[Zhihu]](https://zhuanlan.zhihu.com/p/116074945).\n\n#### [Mar 20th, 2020]\n+ We supported **DBSCAN-based MMT** which could achieve better performance. *Note that* we effectively accelerated the Jaccard distance computing process for DBSCAN (around 200s for CPU and 60s for GPU, compared to around 10min in other repos). \n+ We added the **general clustering-based baseline** training for UDA re-ID, i.e. single model training with only hard pseudo labels. \n+ We slightly modified the setting of training iterations `--iters` from  `800` to `400` in the training scripts, achieving similar performance with less time cost.\n+ We discovered **a minor hint**, changing the setting of `--dropout` from `0` to `0.5` will achieve supervising improvements in MMT. Intuitively, the dual models are more de-coupled with independent dropout functions.\n+ A reminder here, changing the hyper-parameter `--soft-tri-weight 0.8` to `--soft-tri-weight 1.0` may achieve better performance in some cases. Please refer to ablation study results in Table 2 in our paper.\n\n\n## Installation\n\n```shell\ngit clone https://github.com/yxgeee/MMT.git\ncd MMT\npython setup.py install\n```\n\n## Prepare Datasets\n\n```shell\ncd examples && mkdir data\n```\nDownload the raw datasets [DukeMTMC-reID](https://arxiv.org/abs/1609.01775), [Market-1501](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf), [MSMT17](https://arxiv.org/abs/1711.08565),\nand then unzip them under the directory like\n```\nMMT/examples/data\n\u251c\u2500\u2500 dukemtmc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 DukeMTMC-reID\n\u251c\u2500\u2500 market1501\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Market-1501-v15.09.15\n\u2514\u2500\u2500 msmt17\n    \u2514\u2500\u2500 MSMT17_V1\n```\n\n## Prepare Pre-trained Models\nWhen *training with the backbone of [IBN-ResNet-50](https://arxiv.org/abs/1807.09441)*, you need to download the [ImageNet](http://www.image-net.org/) pre-trained model from this [link](https://drive.google.com/drive/folders/1thS2B8UOSBi_cJX6zRy6YYRwz_nVFI_S) and save it under the path of `logs/pretrained/`.\n```shell\nmkdir logs && cd logs\nmkdir pretrained\n```\nThe file tree should be\n```\nMMT/logs\n\u2514\u2500\u2500 pretrained\n \u00a0\u00a0 \u2514\u2500\u2500 resnet50_ibn_a.pth.tar\n```\n\n## Example #1:\nTransferring from [DukeMTMC-reID](https://arxiv.org/abs/1609.01775) to [Market-1501](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf) on the backbone of [ResNet-50](https://arxiv.org/abs/1512.03385), *i.e. Duke-to-Market (ResNet-50)*.\n\n### Train\nWe utilize 4 GTX-1080TI GPUs for training.\n\n**An explanation about the number of GPUs and the size of mini-batches:**\n+ We adopted 4 GPUs with a batch size of 64, since we found 16 images out of 4 identities in a mini-batch benefits the learning of BN layers, achieving optimal performance. This setting may affect IBN-ResNet-50 in a larger extent.\n+ It is fine to try other hyper-parameters, i.e. GPUs and batch sizes. I recommend to remain a mini-batch of 16 images for the BN layers, e.g. use a batch size of 32 for 2 GPUs training, etc.\n\n#### Stage I: Pre-training on the source domain\n\n```shell\nsh scripts/pretrain.sh dukemtmc market1501 resnet50 1\nsh scripts/pretrain.sh dukemtmc market1501 resnet50 2\n```\n\n#### Stage II: End-to-end training with MMT-500 \nWe utilized K-Means clustering algorithm in the paper.\n\n```shell\nsh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet50 500\n```\n\nWe supported DBSCAN clustering algorithm currently.\n**Note that** you could add `--rr-gpu` in the training scripts for faster clustering but requiring more GPU memory.\n\n```shell\nsh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet50\n```\n\n### Test\nWe utilize 1 GTX-1080TI GPU for testing.\nTest the trained model with best performance by\n```shell\nsh scripts/test.sh market1501 resnet50 logs/dukemtmcTOmarket1501/resnet50-MMT-500/model_best.pth.tar\n```\n\n\n\n## Other Examples:\n**Duke-to-Market (IBN-ResNet-50)**\n```shell\n# pre-training on the source domain\nsh scripts/pretrain.sh dukemtmc market1501 resnet_ibn50a 1\nsh scripts/pretrain.sh dukemtmc market1501 resnet_ibn50a 2\n# end-to-end training with MMT-500\nsh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet_ibn50a 500\n# or MMT-700\nsh scripts/train_mmt_kmeans.sh dukemtmc market1501 resnet_ibn50a 700\n# or MMT-DBSCAN\nsh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet_ibn50a \n# testing the best model\nsh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-500/model_best.pth.tar\nsh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-700/model_best.pth.tar\nsh scripts/test.sh market1501 resnet_ibn50a logs/dukemtmcTOmarket1501/resnet_ibn50a-MMT-DBSCAN/model_best.pth.tar\n```\n**Duke-to-MSMT (ResNet-50)**\n```shell\n# pre-training on the source domain\nsh scripts/pretrain.sh dukemtmc msmt17 resnet50 1\nsh scripts/pretrain.sh dukemtmc msmt17 resnet50 2\n# end-to-end training with MMT-500\nsh scripts/train_mmt_kmeans.sh dukemtmc msmt17 resnet50 500\n# or MMT-1000\nsh scripts/train_mmt_kmeans.sh dukemtmc msmt17 resnet50 1000\n# or MMT-DBSCAN\nsh scripts/train_mmt_dbscan.sh dukemtmc market1501 resnet50 \n# testing the best model\nsh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-500/model_best.pth.tar\nsh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-1000/model_best.pth.tar\nsh scripts/test.sh msmt17 resnet50 logs/dukemtmcTOmsmt17/resnet50-MMT-DBSCAN/model_best.pth.tar\n```\n\n## General Clustering-based Baseline Training\n<div align=center><img width=\"400\" height=\"163\" src=\"figs/baseline.png\"/></div>\n\n**Note that the baseline mentioned in our paper is slightly different from the general clustering-based baseline:**\n+ For fair comparison in the ablation study, the baseline in our paper utilized the same dual-model framework as our MMT but using only hard pseudo labels (no soft labels and no mean-teachers), i.e. setting `--soft-ce-weight 0 --soft-tri-weight 0 --alpha 0` in the training scripts.\n+ The general clustering-based baseline is illustrated as above, which contains only one model. The model is training with a cross-entropy loss and a triplet loss, supervised by hard pseudo labels.\n+ Although the baseline in our paper adopted dual models that are *independently* trained with hard losses, the features extracted for clustering are averaged from dual models. It is **the only difference** from the general clustering-based baseline.\n\nHere, we supported training with the general clustering-based baseline for further academic usage.\nFor example, Duke-to-Market with ResNet-50\n```shell\n# for K-Means\nsh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 500\nsh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 700\nsh scripts/train_baseline_kmeans.sh dukemtmc market1501 resnet50 900\n# for DBSCAN\nsh scripts/train_baseline_dbscan.sh dukemtmc market1501 resnet50 \n```\n\n\n## Download Trained Models\n*Source-domain pre-trained models and all our MMT models in the paper can be downloaded from the [link](https://drive.google.com/open?id=1WC4JgbkaAr40uEew_JEqjUxgKIiIQx-W).*\n![results](figs/results.png)\n\n\n\n## Citation\nIf you find this code useful for your research, please cite our paper\n```\n@inproceedings{\n  ge2020mutual,\n  title={Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification},\n  author={Yixiao Ge and Dapeng Chen and Hongsheng Li},\n  booktitle={International Conference on Learning Representations},\n  year={2020},\n  url={https://openreview.net/forum?id=rJlnOhVYPS}\n}\n```\n",
            "readme_url": "https://github.com/yxgeee/MMT",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking",
            "arxiv": "1609.01775",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.01775v2",
            "abstract": "To help accelerate progress in multi-target, multi-camera tracking systems,\nwe present (i) a new pair of precision-recall measures of performance that\ntreats errors of all types uniformly and emphasizes correct identification over\nsources of error; (ii) the largest fully-annotated and calibrated data set to\ndate with more than 2 million frames of 1080p, 60fps video taken by 8 cameras\nobserving more than 2,700 identities over 85 minutes; and (iii) a reference\nsoftware system as a comparison baseline. We show that (i) our measures\nproperly account for bottom-line identity match performance in the multi-camera\nsetting; (ii) our data set poses realistic challenges to current trackers; and\n(iii) the performance of our system is comparable to the state of the art.",
            "authors": [
                "Ergys Ristani",
                "Francesco Solera",
                "Roger S. Zou",
                "Rita Cucchiara",
                "Carlo Tomasi"
            ]
        },
        {
            "title": "Person Transfer GAN to Bridge Domain Gap for Person Re-Identification",
            "arxiv": "1711.08565",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.08565v2",
            "abstract": "Although the performance of person Re-Identification (ReID) has been\nsignificantly boosted, many challenging issues in real scenarios have not been\nfully investigated, e.g., the complex scenes and lighting variations, viewpoint\nand pose changes, and the large number of identities in a camera network. To\nfacilitate the research towards conquering those issues, this paper contributes\na new dataset called MSMT17 with many important features, e.g., 1) the raw\nvideos are taken by an 15-camera network deployed in both indoor and outdoor\nscenes, 2) the videos cover a long period of time and present complex lighting\nvariations, and 3) it contains currently the largest number of annotated\nidentities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe\nthat, domain gap commonly exists between datasets, which essentially causes\nsevere performance drop when training and testing on different datasets. This\nresults in that available training data cannot be effectively leveraged for new\ntesting domains. To relieve the expensive costs of annotating new training\nsamples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to\nbridge the domain gap. Comprehensive experiments show that the domain gap could\nbe substantially narrowed-down by the PTGAN.",
            "authors": [
                "Longhui Wei",
                "Shiliang Zhang",
                "Wen Gao",
                "Qi Tian"
            ]
        },
        {
            "title": "Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net",
            "arxiv": "1807.09441",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.09441v3",
            "abstract": "Convolutional neural networks (CNNs) have achieved great successes in many\ncomputer vision problems. Unlike existing works that designed CNN architectures\nto improve performance on a single task of a single domain and not\ngeneralizable, we present IBN-Net, a novel convolutional architecture, which\nremarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as\nwell as its generalization capacity on another domain (e.g. GTA5) without\nfinetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch\nNormalization (BN) as building blocks, and can be wrapped into many advanced\ndeep networks to improve their performances. This work has three key\ncontributions. (1) By delving into IN and BN, we disclose that IN learns\nfeatures that are invariant to appearance changes, such as colors, styles, and\nvirtuality/reality, while BN is essential for preserving content related\ninformation. (2) IBN-Net can be applied to many advanced deep architectures,\nsuch as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their\nperformance without increasing computational cost. (3) When applying the\ntrained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves\ncomparable improvements as domain adaptation methods, even without using data\nfrom the target domain. With IBN-Net, we won the 1st place on the WAD 2018\nChallenge Drivable Area track, with an mIoU of 86.18%.",
            "authors": [
                "Xingang Pan",
                "Ping Luo",
                "Jianping Shi",
                "Xiaoou Tang"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "url": "https://openreview.net/forum?id=rJlnOhVYPS",
            "year": "2020",
            "booktitle": "International Conference on Learning Representations",
            "author": [
                "Ge, Yixiao",
                "Chen, Dapeng",
                "Li, Hongsheng"
            ],
            "title": "Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification",
            "ENTRYTYPE": "inproceedings",
            "ID": "ge2020mutual",
            "authors": [
                "Ge, Yixiao",
                "Chen, Dapeng",
                "Li, Hongsheng"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Market-1501"
            },
            {
                "name": "DukeMTMC-reID"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "COCO"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ILSVRC 2015"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9993902284275756,
        "task": "Person Re-Identification",
        "task_prob": 0.6477945911001518
    }
}