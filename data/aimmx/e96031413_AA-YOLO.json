{
    "visibility": {
        "visibility": "public"
    },
    "name": "Attention ALL-CNN Twin Head YOLO (AA -YOLO)",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "e96031413",
                "owner_type": "User",
                "name": "AA-YOLO",
                "url": "https://github.com/e96031413/AA-YOLO",
                "stars": 5,
                "pushed_at": "2022-01-28 04:03:50+00:00",
                "created_at": "2020-12-22 14:12:01+00:00",
                "language": "Python",
                "description": "The proposed Attention ALL-CNN Twin Head YOLO (AA -YOLO) outperforms the original YOLOv4-tiny on the COCO dataset by 3.3% and reduces the model parameters by 7.26%. Source code is at https://github.com/e96031413/AA-YOLO",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "cfg",
                "sha": "4539d0ddda17acceca1c3fa8048907dc646543fc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/tree/main/cfg"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "data",
                "sha": "f60df4e1b6b15477bc4cf76f2a6e6f29034b1c41",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/tree/main/data"
                    }
                },
                "num_files": 15
            },
            {
                "type": "code",
                "name": "detect.py",
                "sha": "de7909310340230dea29965c420db87c7e25f912",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/blob/main/detect.py"
                    }
                },
                "size": 8306
            },
            {
                "type": "code",
                "name": "experiment-info.md",
                "sha": "981ef3f37a8f376e9b766b92b50f4481732a3288",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/blob/main/experiment-info.md"
                    }
                },
                "size": 2405
            },
            {
                "type": "code",
                "name": "experiments.md",
                "sha": "5ab5865130dbc827f8782c7d8990efa2776313ea",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/blob/main/experiments.md"
                    }
                },
                "size": 1011
            },
            {
                "type": "code",
                "name": "models.py",
                "sha": "1b626dbba269c0c93642cf306c50d7758c41f414",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/blob/main/models.py"
                    }
                },
                "size": 32634
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "bed0f9be58fa77c7dc84af6044de15d1e4018073",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/blob/main/requirements.txt"
                    }
                },
                "size": 355
            },
            {
                "type": "code",
                "name": "results.txt",
                "sha": "f982b28894e98bb08684cdbebd5d9b1bbebf46b4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/blob/main/results.txt"
                    }
                },
                "size": 45149
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "3fd2dea886378b2ff9d75d35a7533d1adc05103f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/blob/main/test.py"
                    }
                },
                "size": 12657
            },
            {
                "type": "code",
                "name": "test_half.py",
                "sha": "f1506525b7c2f117ceb9e31587fe4c7a4bf815f9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/blob/main/test_half.py"
                    }
                },
                "size": 12682
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "231edcdaa3b2689b9332a67e34f63da84a342867",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/blob/main/train.py"
                    }
                },
                "size": 22290
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "c4a0eba00d30d9134ecb8f07320ba1312dc9f9ac",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/tree/main/utils"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "weights",
                "sha": "d208018e59a2cf57ca394a713927aa598106e815",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/e96031413/AA-YOLO/tree/main/weights"
                    }
                },
                "num_files": 1
            }
        ]
    },
    "authors": [
        {
            "name": "e96031413",
            "github_id": "e96031413"
        }
    ],
    "tags": [
        "pytorch",
        "yolov4",
        "yolov4-tiny",
        "aa-yolo",
        "darknet",
        "yolo",
        "yolov3"
    ],
    "description": "The proposed Attention ALL-CNN Twin Head YOLO (AA -YOLO) outperforms the original YOLOv4-tiny on the COCO dataset by 3.3% and reduces the model parameters by 7.26%. Source code is at https://github.com/e96031413/AA-YOLO",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/e96031413/AA-YOLO",
            "stars": 5,
            "issues": true,
            "readme": "# Attention ALL-CNN Twin Head YOLO (AA -YOLO)\n\nOfficial implementation of **Improving Tiny YOLO with Fewer Model Parameters**, IEEE BigMM 2021\n\n### Abstract\nWith the rapid development of convolutional neural networks (CNNs), there are a variety of techniques that can improve existing CNN models, including attention mechanisms, activation functions, and data augmentation. However, integrating these techniques can lead to a significant increase in the number of parameters and FLOPs. Here, we integrated Efficient Channel Attention Net(ECA-Net), Mish activation function, All Convolutional Net (ALL-CNN), and a twin detection head architecture into YOLOv4-tiny, yielding an AP 50 of 44.2% on the MS COCO 2017 dataset. The proposed Attention ALL-CNN Twin Head YOLO (A 2 -YOLO) outperforms the original YOLOv4-tiny on the same dataset by 3.3% and reduces the model parameters by 7.26%. Source code is at https://github.com/e96031413/AA-YOLO\n\n### Note\nThis project is based on [WongKinYiu/PyTorch_YOLOv4 u3_preview branch](https://github.com/WongKinYiu/PyTorch_YOLOv4/tree/u3_preview) with some modification\n\nThe AA-YOLO architecture file is at [AA-YOLO-twin-head.cfg](https://github.com/e96031413/PyTorch_YOLOv4-tiny/blob/main/cfg/AA-YOLO-twin-head.cfg)\n\n- You can use this project to train 416x416 YOLOv4-tiny.\n[GitHub Issues](https://github.com/WongKinYiu/ScaledYOLOv4/issues/41)\n\n- View our experiment environment infos [here](https://github.com/e96031413/PyTorch_YOLOv4-tiny/blob/main/experiment-info.md)\n\n### Development Log\n<details><summary> <b>Expand Development Log</b> </summary>\n\n### 2021/03/04\u66f4\u65b0\uff1a\n\u4f7f\u7528test.py\u91dd\u5c0dCross-Stitch\u67b6\u69cb\u9032\u884cAP\u6e2c\u8a66\u6642\uff0c\u5fc5\u9808\u5230test.py\u7684\u7b2c43\u884c\u5c07model.fuse()\u529f\u80fd\u95dc\u9589\n```\n#model.fuse()\n```\n\n### 2021/02/08\u66f4\u65b0\uff1a\n\u65b0\u589edetect.py\u8a08\u7b97FPS\u529f\u80fd (detect.py\u7684\u7b2c8\u884c\u3001138~140\u884c\u3001171\u884c)\n```\n# line 8\nFPS_avg = []\n\n# line 138~140\nfrom statistics import mean \nprint('%sDone. (FPS:%.1f)' % (s, 1/(t2 - t1)))\nFPS_avg.append(1/(t2-t1))\n\n# line 171\nprint('Avg FPS: (%.1f)' % (mean(FPS_avg)))\n```\n\n\u89e3\u6c7atest.py\u57f7\u884c\u6642\u9047\u5230numpy\u5fc5\u9808\u89811.17\u7248\u672c\u53caNo module named 'pycocotools'\u7684\u65b9\u6cd5:\n```bash\n#\u79fb\u9664\u6240\u6709numpy\npip uninstall numpy\n#\u5b89\u88dd1.17\u7248\u672cnumpy\npip install numpy==1.17\n#\u5b89\u88ddpycocotools\npip install pycocotools\n#\u57f7\u884ctest.py\npython test.py --data coco2017.data --cfg yolov4-tiny.cfg --weights yolov4-tiny.pt --img 416 --augment\n```\n\n### 2021/01/10\u66f4\u65b0\uff1a\n\u65b0\u589e\u652f\u63f4yolov4.conv.137\u7684Pre-trained weight\u529f\u80fd(\u5728model.py\u7b2c456~456\u884c)\n```\n    elif file == 'yolov4.conv.137':\n        cutoff = 137\n```\n\n### 2020/12/29\u66f4\u65b0\uff1a\n\u65b0\u589eYOLOv4-tiny\u7684RouteGroup\u529f\u80fd\n\n[Feature-request: YOLOv4-tiny #1350](https://github.com/ultralytics/yolov3/issues/1350#issuecomment-651602149)\n\n\u65b0\u589eReLU6(\u5728utils/layers.py)\u3001ReLU(\u5728utils/layers.py)\u3001DepthWise Convolution(\u5728models.py)\u3001ShuffleNetUnit(\u5728models.py)\n\n**\u5982\u4f55\u900f\u904e[u5\u7248\u672c](https://github.com/WongKinYiu/PyTorch_YOLOv4/tree/u5)\u7684yaml\u6a94\u6848\u9032\u884cbackbone\u4fee\u6539\uff1f**\n\n[Yolov4 with Efficientnet b0-b7 Backbone](https://shihyung1221.medium.com/yolov4-with-efficientnet-b0-b7-backbone-529d0ce67cf0)\n\n[Yolov4 with MobileNet V2/V3 Backbone](https://shihyung1221.medium.com/yolov4-with-mobilenet-v2-v3-backbone-c18c0f10bc29)\n\n[Yolov4 with Resnext50/ SE-Resnet50 Backbones](https://shihyung1221.medium.com/yolov4-with-resnext50-se-resnet50-backbones-c324242c48f4)\n\n[Yolov4 with Resnet Backbone](https://shihyung1221.medium.com/yolov4-with-resnet-backbone-eb141b6e79ca)\n\n[Yolov4 with VGG Backbone](https://shihyung1221.medium.com/yolov4-with-vgg-backbone-ae0cedab4f0f)\n\n### 2020/12/28\u66f4\u65b0\uff1a\n\u65b0\u589e\u4ee5\u4e0b\u56db\u7bc7Paper\u7684\u7a0b\u5f0f\u78bc(\u5728models.py\u3001utils/layers.py)\n\nSE Block paper : [arxiv.org/abs/1709.01507](arxiv.org/abs/1709.01507)\n\nCBAM Block paper : [arxiv.org/abs/1807.06521](arxiv.org/abs/1807.06521)\n\nECA Block paper : [arxiv.org/abs/1910.03151](arxiv.org/abs/1910.03151)\n\nFunnel Activation for Visual Recognition : [arxiv.org/abs/2007.11824](arxiv.org/abs/2007.11824)\n\n\n### 2020/12/1\u66f4\u65b0\uff1a\n\u4fee\u6539\u4e86PyTorch_YOLOv4\u7684u3_preview\u7576\u4e2d\uff0cmodels.py\u7b2c355\u884c(\u652f\u63f4pre-trained weight)\u3001train.py\u7b2c67\u884c(\u652f\u63f432\u500d\u6578\u7684\u89e3\u6790\u5ea6)\u3001dataset.py\u7b2c262\u53ca267\u884c(\u8655\u7406dataset\u76f8\u5c0d\u8def\u5f91\u7684\u554f\u984c)\nmodels.py line 355\n```python\ndef load_darknet_weights(self, weights, cutoff=-1):\n    # Parses and loads the weights stored in 'weights'\n    # Establish cutoffs (load layers between 0 and cutoff. if cutoff = -1 all are loaded)\n    file = Path(weights).name\n    if file == 'darknet53.conv.74':\n        cutoff = 75\n    elif file == 'yolov3-tiny.conv.15':\n        cutoff = 15\n    elif file == 'yolov4-tiny.conv.29':\n        cutoff = 29\n\t.\n\t.\n\t.\n\t.\n```\ntrain.py line 67\n```python\n    gs = 32  # (pixels) grid size # \u539f\u672cgs = 64\u6539\u621032\n    assert math.fmod(imgsz_min, gs) == 0, '--img-size %g must be a %g-multiple' % (imgsz_min, gs)\n```\nutils/dataset.py  line 262 and 267\n```python\nclass LoadImagesAndLabels(Dataset):  # for training/testing\n    def __init__(self, path, img_size=416, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,\n                 cache_images=False, single_cls=False):\n        path = str(Path(path))  # os-agnostic\n        parent = str(Path(path).parent) + os.sep                              # add this\n        assert os.path.isfile(path), 'File not found %s. See %s' % (path, help_url)\n        with open(path, 'r') as f:\n            self.img_files = [x.replace('/', os.sep) for x in f.read().splitlines()  # os-agnostic\n                              if os.path.splitext(x)[-1].lower() in img_formats]\n            self.img_files = [x.replace('./', parent) if x.startswith('./') else x for x in self.img_files]    # add this\n```\n</details>\n\n## Experiment Environment\n\nOur environment setting on [https://www.twcc.ai/](TWCC)\n```\nNVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0\npython 3.6.9 \nPyTorch 1.6.0\nTorchvision 0.7.0\ntensorflow-20.06-tf2-py3:tag1614508696283\nnumpy 1.17.0\n```\n\n## Important note!!!\n2021/03/31 update:\n\n**Be careful to use NAS (Network Attached Storage) to store the coco dataset, it might slow down the tranining speed. Store your dataset locally.**\n\n## Requirements\n\n```\n# Install mish-cuda if you want to use fewer GPU memory during training (from 12G -> 8G) \npip install git+https://github.com/thomasbrandon/mish-cuda/\npip install -r requirements.txt\n```\n\n## Download COCO 2017 dataset\n```\nsh data/get_coco.sh\n```\n\n## Training\n\n```\nCUDA_VISIBLE_DEVICES=0 python train.py --data coco2017.data --cfg AA-YOLO-twin-head.cfg --weights 'yolov4-tiny.conv.29' --name yolov4-tiny --img 416\n```\n\n## Testing\n\n```\nCUDA_VISIBLE_DEVICES=0 python test.py --data coco2017.data --cfg AA-YOLO-twin-head.cfg --weights yolov4-tiny.pt --img 416 --augment\n```\n\n## Inference\n```\nCUDA_VISIBLE_DEVICES=0 python detect.py --weights AA-YOLO-twin-head.pt --img 416 --source file.mp4  # video\n                                                                                    file.jpg  # image \n```\n\n## Training Result Visualization\n```\npython -c from utils import utils;utils.plot_results().\n```\n\n## Citation\n```\n@inproceedings{9643269,  \n    title={Improving Tiny YOLO with Fewer Model Parameters},\n    author={Liu, Yanwei and Ma, Ching-Wen},  \n    booktitle={2021 IEEE Seventh International Conference on Multimedia Big Data (BigMM)},   \n    pages={61-64},\n    year={2021},\n    doi={10.1109/BigMM52142.2021.00017}}\n```\n\n## Acknowledgements\n\n* [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet)\n* [https://github.com/ultralytics/yolov3](https://github.com/ultralytics/yolov3)\n* [https://github.com/WongKinYiu/PyTorch_YOLOv4](https://github.com/WongKinYiu/PyTorch_YOLOv4)\n* [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)\n* [HaloTrouvaille/YOLO-Multi-Backbones-Attention](https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/tree/1f425d379783b6d132b44f14ecfd251d8e2448fa)\n* [SpursLipu/YOLOv3v4-ModelCompression-MultidatasetTraining-Multibackbone](https://github.com/SpursLipu/YOLOv3v4-ModelCompression-MultidatasetTraining-Multibackbone)\n",
            "readme_url": "https://github.com/e96031413/AA-YOLO",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "doi": "10.1109/BigMM52142.2021.00017",
            "year": "2021",
            "pages": "61-64",
            "booktitle": "2021 IEEE Seventh International Conference on Multimedia Big Data (BigMM)",
            "author": [
                "Liu, Yanwei",
                "Ma, Ching-Wen"
            ],
            "title": "Improving Tiny YOLO with Fewer Model Parameters",
            "ENTRYTYPE": "inproceedings",
            "ID": "9643269",
            "authors": [
                "Liu, Yanwei",
                "Ma, Ching-Wen"
            ]
        },
        {
            "title": "https://github.com/AlexeyAB/darknet",
            "url": "https://github.com/AlexeyAB/darknet"
        },
        {
            "title": "https://github.com/ultralytics/yolov3",
            "url": "https://github.com/ultralytics/yolov3"
        },
        {
            "title": "https://github.com/WongKinYiu/PyTorch_YOLOv4",
            "url": "https://github.com/WongKinYiu/PyTorch_YOLOv4"
        },
        {
            "title": "https://github.com/ultralytics/yolov5",
            "url": "https://github.com/ultralytics/yolov5"
        },
        {
            "title": "HaloTrouvaille/YOLO-Multi-Backbones-Attention",
            "url": "https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/tree/1f425d379783b6d132b44f14ecfd251d8e2448fa"
        },
        {
            "title": "SpursLipu/YOLOv3v4-ModelCompression-MultidatasetTraining-Multibackbone",
            "url": "https://github.com/SpursLipu/YOLOv3v4-ModelCompression-MultidatasetTraining-Multibackbone"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "SpursLipu/YOLOv3v4-ModelCompression-MultidatasetTraining-Multibackbone",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/SpursLipu/YOLOv3v4-ModelCompression-MultidatasetTraining-Multibackbone"
                    }
                }
            },
            {
                "name": "COCO 2017"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9988536791347453,
        "task": "Object Detection",
        "task_prob": 0.9767928504977801
    }
}