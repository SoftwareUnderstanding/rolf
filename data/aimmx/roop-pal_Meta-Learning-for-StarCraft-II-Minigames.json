{
    "visibility": {
        "visibility": "public"
    },
    "name": "Meta-Learning for StarCraft II Minigame Strategy",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "roop-pal",
                "owner_type": "User",
                "name": "Meta-Learning-for-StarCraft-II-Minigames",
                "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames",
                "stars": 24,
                "pushed_at": "2021-03-30 03:37:10+00:00",
                "created_at": "2018-03-09 20:57:31+00:00",
                "language": "Python",
                "description": "We reproduced DeepMind's results and implement a meta-learning (MLSH) agent which can generalize across minigames.",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "666b0d185df987bcee3e81177d413f2ab1e1db02",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/blob/master/.gitignore"
                    }
                },
                "size": 68
            },
            {
                "type": "code",
                "name": "doc",
                "sha": "9e6429258a66a5c12b51ef2879e3612ced40ba32",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/tree/master/doc"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "0ad4a85d1a9001065a09a89799fd9c553a4ca5f8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/blob/master/main.py"
                    }
                },
                "size": 10123
            },
            {
                "type": "code",
                "name": "meta-learning-starcraft.pdf",
                "sha": "45fdda2eadbe656d0605e3ca08d0a1cdce0f300a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/blob/master/meta-learning-starcraft.pdf"
                    }
                },
                "size": 655609
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "89e49120097b7a5c1da3a3aa9e0767ab06fc53b9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/blob/master/requirements.txt"
                    }
                },
                "size": 99
            },
            {
                "type": "code",
                "name": "rl_agents",
                "sha": "9c220b2e4f2b706e651fae484ef54f13e0170495",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/tree/master/rl_agents"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "run_loop.py",
                "sha": "e58e6d48204f48dd1886b22df80e57ad96ed67c8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/blob/master/run_loop.py"
                    }
                },
                "size": 2305
            },
            {
                "type": "code",
                "name": "scripted_agents",
                "sha": "24f9b544c5f52ca0b01c36ec5a11395b5754f079",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/tree/master/scripted_agents"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "5e19a593b36f0ae4c3eb6fef6e78d2e7eecd58d3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/blob/master/utils.py"
                    }
                },
                "size": 4784
            }
        ]
    },
    "authors": [
        {
            "name": "jkafrouni",
            "email": "j.kafrouni@gmail.com",
            "github_id": "jkafrouni"
        },
        {
            "name": "Roop Pal",
            "email": "roop.pal@columbia.edu",
            "github_id": "roop-pal"
        },
        {
            "name": "connorhargus",
            "email": "connorhargus@gmail.com",
            "github_id": "connorhargus"
        }
    ],
    "tags": [
        "sc2le",
        "deep-reinforcement-learning",
        "reinforcement-learning",
        "starcraft2-ai",
        "starcraft2",
        "minigames",
        "multi-agent-learning"
    ],
    "description": "We reproduced DeepMind's results and implement a meta-learning (MLSH) agent which can generalize across minigames.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames",
            "stars": 24,
            "issues": true,
            "readme": "# Meta-Learning for StarCraft II Minigame Strategy\nFor this project we implemented and tested deep reinforcement learning methods for five SC2LE minigames involving moving units to target locations as well as battles between groups of agents. This project was developed for the course COMS 4995 Deep Learning taught by Prof. Iddo Drori at Columbia University, in Spring 2018. This work is done by Connor Hargus, Jerome Kafrouni and Roop Pal who have contributed equally.\n\nWe started our project by partially reproducing the results obtained by DeepMind in their SC2LE publication, as shown by the table above. Then, we implemented a meta-learning strategy showing how an agent's skills can be transferred between minigames.\n\nA draft of our paper can be found [here.](./meta-learning-starcraft.pdf)\n## Getting started\n\nTo get started, follow the instructions on the [pysc2 repository](https://github.com/deepmind/pysc2). As described in their instructions, make sure that the environment is set up correctly by running:\n\n```\n$ python -m pysc2.bin.agent --map Simple64\n```\n\nOur project relies on a few more packages, that can be installed by running:\n\n```\n$ pip install -r requirements.txt\n```\n\nWe have tested our project using python 3 and pysc2 version 1.2, which is the main version currently available.\n\nWe are currently training our agents on a google cloud instance with a 4 core CPU and two Tesla K80 GPUs. This configuration might evolve during the project.\n\n## Running agents\n\nTo run an agent, instead of calling pysc2 directly as in the instructions from DeepMind, run the main.py script of our project, with the agent class passed as a flag. For example, to run the q table agent or the MLSH agent:\n\n```\n$ python -m main --agent=rl_agents.qtable_agent.QTableAgent --map=DefeatRoaches\n$ python -m main --agent=rl_agents.mlsh_agent.MLSHAgent --num_subpol=3 --subpol_steps=5 --training\n```\n\nIf no agent is specified, the A3C agent is run by default:\n\n```\n$ python -m main --map=DefeatRoaches\n```\nA full list of the flags that can be used along with their descriptions is available in the main.py of script. The most important and useful flags are:\n\n- map: the map on which to run the agent. Should not be used with MLSHAgent which uses a list of maps to use, since MLSH trains on multiple maps.\n- max_agent_steps: the number of steps to perform per episode (after which, episode is stopped). This is used to speed up training by focusing on early states of episodes\n- parallel: number of threads to run, defaults at 1.\n\nFlags specific to the MLSHAgent:\n\n- num_subpol: number of subpolicies to train and use\n- subpol_steps: periodicity of subpolicy choices done by the master policy (in game steps)\n- warmup_len: number of episodes during which only the master subpolicy is trained\n- join_len: number of episodes during which both master and subpolicies are trained\n\n\n\n\n## Introduction\n\nDeep reinforcement learning has made significant strides in recent years, with results achieved in board games such as Go. However, there are a number of obstacles preventing such methods from being applied to more real-world situations. For instance, more realistic strategic situations often involve much larger spaces of possible states and actions, an environment state which is only partially observed, multiple agents to control, and a necessity for long-term strategies involving not hundreds but thousands or tens of thousands of steps. It has thus been suggested that creating learning algorithms which outperform humans in playing real-time strategy (RTS) video games would signal a more generalizable result about the ability of a computer to make decisions in the real world.\n\nOf the current RTS games on the market, StarCraft II is one of the most popular. The recent release by Google\u2019s DeepMind of SC2LE (StarCraft II Learning Environment) presents an interface with which to train deep reinforcement learners to compete in the game, both in smaller \u201cminigames\u201d and on full matches. The SC2LE environment is described on [DeepMind's github repo.](https://github.com/deepmind/pysc2) \n\nIn this project, we focus on solving a variety of minigames, which capture various aspects of the full StarCraft II game. These minigames focus on tasks such as gathering resources, moving to waypoints, finding enemies, or skirmishing with units. In each case the player is given a homogeneous set of units (marines), and a reward is based off the minigame (+5 for defeating each enemy roach in DefeatRoaches, for example).\n\n## Our work\n\nWe first implemented and tested \"baseline\" agents that will let us evaluate more complex reinforcement learning agents. We compare our results with \"random\" agents that choose any random action at each step, and simple scripted agents that intend to solve the minigame with a simple deterministic policy. The scripted agents can be found in the folder *scripted_agents*.\n\nWe then implemented a \"smarter\" baseline agent using a Q-table. For this to be possible, we reduced the action space to a few basic actions (mainly selecting units and attacking points), and also reduced the state space (a 4 by 4 grid indicating where the roaches are along with the number of marines left).\n\nWe then made a review of the current architectures used to solve these minigames. In their paper, DeepMind use the A3C algorithm (Asynchronous Advantage Actor Critic) with several architectures (*Atari-Net*, *FullyConv*, *FullyConv LSTM*) that are described in [section 4.3](https://deepmind.com/documents/110/sc2le.pdf) of the SC2LE paper. DeepMind did not include open source implementations of the architectures used in their paper, yet a few research teams shared implementations, and our work relies on theirs. Useful github resources can be found in the *readme* of the *docs* folder of this repo. All agents based on different reinforcement learning ideas (MLSH, A3C) will be in the *rl_agents* folder. Our A3C agent is mainly based on the work of [\nHu](https://github.com/xhujoy) who provided an implementation of A3C for pysc2.\n\nThe main contribution is an implementation of a MLSH (Meta-Learning Shared Hierarchies) agent, which can be trained on multiple minigames, sharing sub-policies. A master policy selects which sub-policy to use given observations. This allows the agent to generalize to previously unseen minigames by just training a master policy. A more detailed explanation of the algorithm can be found in the [paper](#MLSH).\n\n## Results\n\n![alt text](./doc/table.PNG \"Results Table\")\n\nWe trained our agents on 5 of the 7 minigames: MoveToBeacon, CollectMineralShards, FindAndDefeatZerglings, DefeatRoaches and DefeatZerglingsAndBanelings. We also tried a simpler approach: we wrote scripted bots to solve these games, and implemented a simple Q-Learning agent with simpler action and state spaces. We implemented our MLSH algorithm from scratch as an adaptation of the A3C using an AtariNet architecture produced by [Xiaowei Hu](https://github.com/xhujoy). The results presented in the table above describe test results after 5 million game steps of training.\n\nThe videos below show (1) our A3C agent trained with Atarinet architecture, on 25,000 episodes, playing DefeatRoaches, (2) our simple Q-Learning agent trained on MoveToBeacon, and (3) our MLSH agent trained on 4 minigames, playing DefeatRoaches.\n\n<div align=\"center\">\n\n  <a href=\"https://youtu.be/dEAh0g9SVS0\"\n     target=\"_blank\">\n    <img src=\"https://img.youtube.com/vi/dEAh0g9SVS0/0.jpg\"\n         alt=\"Trained A3C Atarinet agent playing DefeatRoaches\"\n         width=\"240\" height=\"180\" border=\"10\" />\n  </a>\n  <a href=\"https://youtu.be/Z-H1QQKXbhQ\"\n     target=\"_blank\">\n     <img src=\"https://img.youtube.com/vi/Z-H1QQKXbhQ/0.jpg\"\n         alt=\"Trained A3C Atarinet agent playing DefeatRoaches\"\n         width=\"240\" height=\"180\" border=\"10\" />\n  </a>\n   <a href=\"https://youtu.be/s5wGk7tql0c\"\n     target=\"_blank\">\n     <img src=\"https://img.youtube.com/vi/s5wGk7tql0c/0.jpg\"\n         alt=\"Trained MLSH Atarinet agent playing DefeatRoaches\"\n         width=\"240\" height=\"180\" border=\"10\" />\n  </a>\n\n</div>\n\nWe find that the MLSH scores reasonably well to the previously unseen DefeatZerglingsAndBanelings minigame, though it unsurprisingly does not achieve the score of an agent trained on that single minigame. Interestingly, it actually surpassed A3C's performance on the FindAndDefeatZerglings minigame, perhaps due to similarities between this minigame and targeting locations/units in other minigames. The performance on DefeatZerglingsAndBanelings results show the capabilities of the agent to generalize across minigames. We believe that such an algorithm, with properly tuned hyper-parameters and stronger computational power, could in the future be very powerful in developing a strong reinforcement learning agent playing the full game.\n\n\n## Acknowledgements\n\nOur code is based on the work of Xiaowei Hu (xhujoy) who shared his implementation of A3C for pysc2.\n\nSpecial thanks to Professor Iddo Drori, our instructor at Columbia University, as well as Niels Justesen for their expertise and guidance.\n\n## References\n1. [O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev. et al. StarCraft II: A New Challenge for Reinforcement Learning. Google DeepMind, 2017.](https://deepmind.com/documents/110/sc2le.pdf)\n2. [V. Mnih, A. Badia, M. Mirza1, A. Graves, T. Harley, T. Lillicrap, D. Silver, K. Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning, 2016.](https://arxiv.org/pdf/1602.01783.pdf)\n3. [K. Frans, J. Ho, X. Chen, P. Abbeel, J. Schulman. Meta Learning Shared Hierarchies. arXiv preprint arXiv:1710.09767v2, 2017.](https://arxiv.org/pdf/1710.09767.pdf)<a name=\"MLSH\"></a>\n4. [Xiaowei Hu's PySC2 Agents](https://github.com/xhujoy/pysc2-agents) \n\n",
            "readme_url": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Meta Learning Shared Hierarchies",
            "arxiv": "1710.09767",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.09767v1",
            "abstract": "We develop a metalearning approach for learning hierarchically structured\npolicies, improving sample efficiency on unseen tasks through the use of shared\nprimitives---policies that are executed for large numbers of timesteps.\nSpecifically, a set of primitives are shared within a distribution of tasks,\nand are switched between by task-specific policies. We provide a concrete\nmetric for measuring the strength of such hierarchies, leading to an\noptimization problem for quickly reaching high reward on unseen tasks. We then\npresent an algorithm to solve this problem end-to-end through the use of any\noff-the-shelf reinforcement learning method, by repeatedly sampling new tasks\nand resetting task-specific policies. We successfully discover meaningful motor\nprimitives for the directional movement of four-legged robots, solely by\ninteracting with distributions of mazes. We also demonstrate the\ntransferability of primitives to solve long-timescale sparse-reward obstacle\ncourses, and we enable 3D humanoid robots to robustly walk and crawl with the\nsame policy.",
            "authors": [
                "Kevin Frans",
                "Jonathan Ho",
                "Xi Chen",
                "Pieter Abbeel",
                "John Schulman"
            ]
        },
        {
            "title": "Asynchronous Methods for Deep Reinforcement Learning",
            "arxiv": "1602.01783",
            "year": 2016,
            "url": "http://arxiv.org/abs/1602.01783v2",
            "abstract": "We propose a conceptually simple and lightweight framework for deep\nreinforcement learning that uses asynchronous gradient descent for optimization\nof deep neural network controllers. We present asynchronous variants of four\nstandard reinforcement learning algorithms and show that parallel\nactor-learners have a stabilizing effect on training allowing all four methods\nto successfully train neural network controllers. The best performing method,\nan asynchronous variant of actor-critic, surpasses the current state-of-the-art\non the Atari domain while training for half the time on a single multi-core CPU\ninstead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control problems as well as on a new task\nof navigating random 3D mazes using a visual input.",
            "authors": [
                "Volodymyr Mnih",
                "Adri\u00e0 Puigdom\u00e8nech Badia",
                "Mehdi Mirza",
                "Alex Graves",
                "Timothy P. Lillicrap",
                "Tim Harley",
                "David Silver",
                "Koray Kavukcuoglu"
            ]
        }
    ],
    "domain": {
        "domain_type": "Playing Games",
        "domain_prob": 0.9962369541788445
    }
}