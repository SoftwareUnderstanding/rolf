{
    "visibility": {
        "visibility": "public"
    },
    "name": "Overview",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "alexandra-chron",
                "owner_type": "User",
                "name": "ntua-slp-wassa-iest2018",
                "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018",
                "stars": 18,
                "pushed_at": "2020-05-20 10:16:06+00:00",
                "created_at": "2018-06-15 15:26:00+00:00",
                "language": "Python",
                "description": "Deep-learning Transfer Learning models of NTUA-SLP team submitted at the IEST of WASSA 2018 at EMNLP 2018.",
                "frameworks": [
                    "NLTK",
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "08e8649099336cdf00e52b6422bdd6a417c0224c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/.gitignore"
                    }
                },
                "size": 1296
            },
            {
                "type": "code",
                "name": "__init__.py",
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/__init__.py"
                    }
                },
                "size": 0
            },
            {
                "type": "code",
                "name": "checkpoints",
                "sha": "d564d0bc3dd917926892c55e3706cc116d5b165e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/tree/master/checkpoints"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "config.py",
                "sha": "1917e7ec513774882d8f10d0dfc77ef71a568a94",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/config.py"
                    }
                },
                "size": 440
            },
            {
                "type": "code",
                "name": "datasets",
                "sha": "151402076f1a1fd0ea7b324a8d93da6c639b6e73",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/tree/master/datasets"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "ekphrasis",
                "sha": "e4d7374a2de37d6c0ead9151573c94e7d5f95f1e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/tree/master/ekphrasis"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "embeddings",
                "sha": "d564d0bc3dd917926892c55e3706cc116d5b165e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/tree/master/embeddings"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "model",
                "sha": "c2176a37f34299218a841182de18ed0901d0d79b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/tree/master/model"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "modules",
                "sha": "f5fff09ccadb2da65497db72f5ad6f14eb39de7c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/tree/master/modules"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "pre_cls.pdf",
                "sha": "f1db9e3055e26b53f95ed8359fbf8f2958753983",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/pre_cls.pdf"
                    }
                },
                "size": 74607
            },
            {
                "type": "code",
                "name": "pre_cls.png",
                "sha": "fd49055adc65fa55dd693aecaa0ea4321794b980",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/pre_cls.png"
                    }
                },
                "size": 94365
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "81153847801e890e6b7b48ad1fd18789a2ea8277",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/requirements.txt"
                    }
                },
                "size": 263
            },
            {
                "type": "code",
                "name": "submissions",
                "sha": "0f86ec4ff74344e3b6272beb3b8f1ed7a03916ba",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/tree/master/submissions"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "ulmfit.pdf",
                "sha": "494616cb9530de7fc141f60f9c5d21091bc6c9b2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/ulmfit.pdf"
                    }
                },
                "size": 78411
            },
            {
                "type": "code",
                "name": "ulmfit.png",
                "sha": "72fe6f91ff9eae9556a9ec7da1a8da03b557b82e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/ulmfit.png"
                    }
                },
                "size": 85145
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "defa8788dc9f878ebc770633916476f457ba2a38",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/tree/master/utils"
                    }
                },
                "num_files": 6
            }
        ]
    },
    "authors": [
        {
            "name": "Alexandra Chronopoulou",
            "github_id": "alexandra-chron"
        },
        {
            "name": "Katerina Margatina",
            "github_id": "mourga"
        },
        {
            "name": "Christos Baziotis",
            "github_id": "cbaziotis"
        }
    ],
    "tags": [
        "transfer-learning",
        "language-models",
        "twitter",
        "python",
        "pytorch",
        "emotion-analysis",
        "deep-learning",
        "deep-neural-networks",
        "lstm",
        "sentiment-analysis"
    ],
    "description": "Deep-learning Transfer Learning models of NTUA-SLP team submitted at the IEST of WASSA 2018 at EMNLP 2018.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018",
            "stars": 18,
            "issues": true,
            "readme": "# Overview\nThis repository contains the source code of the models submitted by NTUA-SLP team in IEST of WASSA 2018 at EMNLP 2018. \nThe model is described in the paper: http://aclweb.org/anthology/W18-6209\n\nCitation:\n```\n@InProceedings{W18-6209,\n  author = \t\"Chronopoulou, Alexandra\n\t\tand Margatina, Aikaterini\n\t\tand Baziotis, Christos\n\t\tand Potamianos, Alexandros\",\n  title = \t\"NTUA-SLP at IEST 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification\",\n  booktitle = \t\"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis\",\n  year = \t\"2018\",\n  publisher = \t\"Association for Computational Linguistics\",\n  pages = \t\"57--64\",\n  location = \t\"Brussels, Belgium\",\n  url = \t\"http://aclweb.org/anthology/W18-6209\"\n}\n```\n# Implicit Emotion Classification Task\nTask: Classify twitter messages in one of **six emotion categories** (happy, sad, fear, anger, surprise, disgust) **without** the emotion word. \nA typical tweet in this dataset has the following form:\n\n ```I'm \\[#TARGETWORD#\\] because I love you, I love you and I hate you.```  (correct label: **angry**) \n\n# Our approach\nWe use an ensemble of 3 different Transfer Learning approaches:\n\nBefore you proceed, ```pip install -r ./requirements.txt```\n\n**First) Pretrain a LSTM-based language model (LM) and transfer it to a target-task classification model:**\n\n<img src=\"https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/ulmfit.png\" width=\"380\">\n\n```cd model/```\n\nYou can skip steps 1 and 2 (time-consuming) and use pretrained and fine-tuned LM, which should be put under  ```checkpoints/```\n\nDownload it here: [pretrained + fine-tuned LM](https://drive.google.com/file/d/1zrP5coIKlCyLSf-0rLYRog_WYJw9yT5e/view?usp=sharing)\n\n1) (can be skipped) Pretrain the LM: ```python lm.py```\n2) (can be skipped) Fine-tune the LM on your own (target) dataset: ```python lm_ft.py```\n\n3) Train the classification model: ```python wassa_pretr_lm.py``` (initializes the weights of the embedding and hidden layer with the LM and adds a Self-Attention mechanism and a classification layer)\n\n*This follows to a great degree ULMFiT by Howard and Ruder.*\n\n**Second) Pretrain a LSTM-based attentive classification model on a different dataset and transfer its feature extractor to the target-task classification model:**\n\n<img src=\"https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/pre_cls.png\" width=\"300\">\n\n\n1) Pretrain a classifier: ```python sentiment.py```\n2) Train the final classifier: ```python wassa.py``` (set ```pretrained_classifier = True``` and provide the correspondent config file.)\n\n**Third) Use pretrained word vectors to initialize the embedding layer of a classification model:**\n-  ```python wassa.py``` (set ```pretrained_classifier = False``` and provide the correspondent word2idx, idx2word and weights of the pretrained word vectors (word2vec, GloVe, fastText)).\n\n# Quick Notes\nOur pretrained word embeddings are available here: [ntua_twitter_300.txt](https://drive.google.com/file/d/1b-w7xf0d4zFmVoe9kipBHUwfoefFvU2t/view)\n\n# Documentation\n\nIn order to make our codebase more accessible and easier to extend, we provide an overview of the structure of our project. \n\n`datasets` : contains the datasets for the pretraining :\n- ```twitter100K/``` contains unlabeled data used for pretraining an LM\n- ```semeval2017A/``` and ```wassa_2018/``` contain the labeled datasets used for SemEval17 Task4A and WASSA IEST 2018 respectively\n\n`embeddings`: pretrained word2vec embeddings should be put here.\n\n\n`model`: scripts for running:\n- IEST classifier ```wassa.py```\n- SE17 Task4 classifier ```sentiment.py```\n- language model ```lm.py```.\n\n`modules`: the source code of the PyTorch deep-learning models and the baseline models.\n\n`submissions`: contains the script to test trained model and create submission file for WASSA.\n\n`utils`: contains helper functions.\n\n**Bibliography**\n\nA few relevant and very important papers to our work are presented below:\n\n```Universal Language Model Fine-tuning for Text Classification``` https://arxiv.org/abs/1801.06146\n\n```Regularizing and Optimizing LSTM Language Models``` https://arxiv.org/abs/1708.02182\n\n```Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm``` http://arxiv.org/abs/1708.00524\n",
            "readme_url": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018",
            "frameworks": [
                "NLTK",
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Universal Language Model Fine-tuning for Text Classification",
            "arxiv": "1801.06146",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.06146v5",
            "abstract": "Inductive transfer learning has greatly impacted computer vision, but\nexisting approaches in NLP still require task-specific modifications and\ntraining from scratch. We propose Universal Language Model Fine-tuning\n(ULMFiT), an effective transfer learning method that can be applied to any task\nin NLP, and introduce techniques that are key for fine-tuning a language model.\nOur method significantly outperforms the state-of-the-art on six text\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\nFurthermore, with only 100 labeled examples, it matches the performance of\ntraining from scratch on 100x more data. We open-source our pretrained models\nand code.",
            "authors": [
                "Jeremy Howard",
                "Sebastian Ruder"
            ]
        },
        {
            "title": "Regularizing and Optimizing LSTM Language Models",
            "arxiv": "1708.02182",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02182v1",
            "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.",
            "authors": [
                "Stephen Merity",
                "Nitish Shirish Keskar",
                "Richard Socher"
            ]
        },
        {
            "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
            "arxiv": "1708.00524",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.00524v2",
            "abstract": "NLP tasks are often limited by scarcity of manually annotated data. In social\nmedia sentiment analysis and related tasks, researchers have therefore used\nbinarized emoticons and specific hashtags as forms of distant supervision. Our\npaper shows that by extending the distant supervision to a more diverse set of\nnoisy labels, the models can learn richer representations. Through emoji\nprediction on a dataset of 1246 million tweets containing one of 64 common\nemojis we obtain state-of-the-art performance on 8 benchmark datasets within\nsentiment, emotion and sarcasm detection using a single pretrained model. Our\nanalyses confirm that the diversity of our emotional labels yield a performance\nimprovement over previous distant supervision approaches.",
            "authors": [
                "Bjarke Felbo",
                "Alan Mislove",
                "Anders S\u00f8gaard",
                "Iyad Rahwan",
                "Sune Lehmann"
            ]
        },
        {
            "url": "http://aclweb.org/anthology/W18-6209",
            "location": "Brussels, Belgium",
            "pages": "57--64",
            "publisher": "Association for Computational Linguistics",
            "year": "2018",
            "booktitle": "Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
            "title": "NTUA-SLP at IEST 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification",
            "author": [
                "Chronopoulou, Alexandra",
                "Margatina, Aikaterini",
                "Baziotis, Christos",
                "Potamianos, Alexandros"
            ],
            "ENTRYTYPE": "inproceedings",
            "ID": "W18-6209",
            "authors": [
                "Chronopoulou, Alexandra",
                "Margatina, Aikaterini",
                "Baziotis, Christos",
                "Potamianos, Alexandros"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Social media"
            },
            {
                "name": "Penn Treebank"
            },
            {
                "name": "WikiText-2"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.999999594849421,
        "task": "Language Modelling",
        "task_prob": 0.9727133430257912
    }
}