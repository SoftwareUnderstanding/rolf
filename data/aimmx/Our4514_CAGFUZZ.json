{
    "visibility": {
        "visibility": "public"
    },
    "name": "CAGFuzz",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Our4514",
                "owner_type": "Organization",
                "name": "CAGFUZZ",
                "url": "https://github.com/Our4514/CAGFUZZ",
                "stars": 0,
                "pushed_at": "2020-07-29 12:54:58+00:00",
                "created_at": "2020-07-29 12:54:46+00:00",
                "language": "Python",
                "frameworks": [
                    "Keras",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "Coverage_Calculate",
                "sha": "ba477be246215fb9136e3c9126df9855178ab252",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Our4514/CAGFUZZ/tree/master/Coverage_Calculate"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "CycleGAN",
                "sha": "9dd95ef0f79c4b29eb29e742ed7dd02931853d38",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Our4514/CAGFUZZ/tree/master/CycleGAN"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "model",
                "sha": "de21a1af424de88dd5b41b21860b0d0d71512c9f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Our4514/CAGFUZZ/tree/master/model"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "picture",
                "sha": "1642b70e108b8bfbbb4a493f33dad28126f0330c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Our4514/CAGFUZZ/tree/master/picture"
                    }
                },
                "num_files": 13
            },
            {
                "type": "code",
                "name": "similarity",
                "sha": "af2d52d0d8e7f9edab3ed70386c138bfd14e2c98",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Our4514/CAGFUZZ/tree/master/similarity"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "wuli_4514",
            "github_id": "QXL4515"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Our4514/CAGFUZZ",
            "stars": 0,
            "issues": true,
            "readme": "# CAGFuzz\nCAGFuzz, a Coverage-guided Adversarial Generative Fuzzing testing approach for DL systems. The goal of the CAGFuzz is to maximize the neuron coverage and generate adversarial test examples as much as possible with minor perturbations for the target DNNs. It mainly consists of four folders\uff1a\n* Coverage_Calculate\n* CycleGAN\n* model\n* similarity\n\n\n## Coverage_Calculate\nThis folder contains the code to calculate the neuron coverage. You can call the functions in the python file to run directly. An example is as follows:\n```python\nfrom Keras_coverage import NCoverage\nfrom keras.models import load_model\nmodel = load_model(\"./model/model_LeNet-1.h5\")\ncoverage = NCoverage(model, 0.1)\nimg = Image.open('./datasets/cifar-10/coverage/img-0-frog.png')\nimg = np.array(img).astype('float32').reshape(-1, 32, 32, 3)\ncoverage.update_coverage(img)\ncovered, total, p = coverage.curr_neuron_cov()\nprint(covered, total, p)\n```\n\n## CycleGAN\nImplementation of Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.\nPaper:https://arxiv.org/abs/1703.10593\n#### Example\n```\n$ cd CycleGAN/\n$ python CycleGAN_model.py\n```\nAn example of the generated adversarial examples is as follows:\n\n<img src=\"https://github.com/QXL4515/CAGFuzz/blob/master/picture/D1.jpg\" width=\"290\"/><img src=\"https://github.com/QXL4515/CAGFuzz/blob/master/picture/D2.jpg\" width=\"290\"/><img src=\"https://github.com/QXL4515/CAGFuzz/blob/master/picture/D3.jpg\" width=\"290\"/><img src=\"https://github.com/QXL4515/CAGFuzz/blob/master/picture/D4.jpg\" width=\"290\"/><img src=\"https://github.com/QXL4515/CAGFuzz/blob/master/picture/D5.jpg\" width=\"290\"/><img src=\"https://github.com/QXL4515/CAGFuzz/blob/master/picture/D6.jpg\" width=\"290\"/>\n\n\n## model\nThis folder contains six neural networks for image recognition and a function for recording training loss, namely:\n* LeNet-1\n* LeNet-4\n* LeNet-5\n* VGG-16\n* VGG-19\n* ResNet-20\n* LossHistory\n\nIf you want to train a LeNet-1 model of your own, please do as follows:\n```\npython LeNet-1.py\n```\nIf you want to train a VGG-16 model of your own, please do as follows:\n```\npython VGG-16.py\n```\n\n## similarity\nThis folder contains two Python files, one is `vgg19_feature.py`, which is used to extract the depth features of pictures, the other is `utility.py`, which is used to compare the cosine similarity between the depth features of two pictures.\n\nIf you want to extract the depth features of an image, you can do this:\n```python\nfrom keras.applications.vgg19 import VGG19\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nimport numpy as np\ndef get_feature(img_dir):\n    base_model = VGG19(weights='imagenet')\n    model = Model(input=base_model.input, output=base_model.get_layer('fc2').output)\n    img = image.load_img(img_dir, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    f = model.predict(x)\n    print(f.shape)\n    print(f)\n    return f\n```\nIf you want to compare the cosine similarity between the depth features of two images, you can do this:\n```python\nfrom utility import get_cossimi\ns1 = get_feature('1.png')\ns2 = get_feature('0_1_000.png')\nsim = get_cossimi(s1, s2)\nprint(sim)\n```\n\n## The overall process of realizing CAGFuzz\n\n<img src=\"https://github.com/QXL4515/CAGFuzz/blob/master/picture/The overall process of realizing CAGFuzz.jpg\" width=\"500\"/>\n\nThe general process of CAG is shown in the figure above. The specific process can be as follows:\n* First, we need to call `CycleGAN_,odel.py` in `CycleGAN` to train `AGE`.\n* Then, the function of feature extraction is realized by `vgg19_feature.py` file in folder `similarity`.\n* Finally, the implementation of neuron coverage needs file `Keras_coverage.py` under folder `Coverage_Calculate`.\n\n\n\n\n",
            "readme_url": "https://github.com/Our4514/CAGFUZZ",
            "frameworks": [
                "Keras",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "arxiv": "1703.10593",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10593v7",
            "abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.",
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A. Efros"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9989186729872267,
        "task": "Image-to-Image Translation",
        "task_prob": 0.8915060881794082
    }
}