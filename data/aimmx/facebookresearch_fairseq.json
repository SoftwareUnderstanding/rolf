{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "Introduction",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "facebookresearch",
                "owner_type": "Organization",
                "name": "fairseq",
                "url": "https://github.com/facebookresearch/fairseq",
                "stars": 3781,
                "pushed_at": "2021-09-17 09:21:31+00:00",
                "created_at": "2017-03-12 16:13:32+00:00",
                "language": "Lua",
                "description": "Facebook AI Research Sequence-to-Sequence Toolkit",
                "license": "Other",
                "frameworks": []
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "9d2a024d8c0769f3327800440457694ccbd41557",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/.gitignore"
                    }
                },
                "size": 25
            },
            {
                "type": "code",
                "name": "CMakeLists.txt",
                "sha": "925d58a269cf9dd6348242d7d1cdca44ec734777",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/CMakeLists.txt"
                    }
                },
                "size": 1226
            },
            {
                "type": "code",
                "name": "CODE_OF_CONDUCT.md",
                "sha": "0f7ad8bfc173eac554f0b6ef7c684861e8014bbe",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/CODE_OF_CONDUCT.md"
                    }
                },
                "size": 244
            },
            {
                "type": "code",
                "name": "CONTRIBUTING.md",
                "sha": "4dcef2b6a5b5fa16ea2367470f4c195858b1f6ec",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/CONTRIBUTING.md"
                    }
                },
                "size": 1357
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "8144372d12745346b580c7471ffac8bdc6137a15",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/LICENSE"
                    }
                },
                "size": 1539
            },
            {
                "type": "code",
                "name": "PATENTS",
                "sha": "18b09892ca58984f01cbbaaa7ee11b61765b76b7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/PATENTS"
                    }
                },
                "size": 1978
            },
            {
                "type": "code",
                "name": "data",
                "sha": "d29cc8e160787793a4ed6a61696a2772c54021f8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/tree/main/data"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "fairseq.gif",
                "sha": "5782fdbc7e0014564725c3ad0fc6be5c6bcd9983",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/fairseq.gif"
                    }
                },
                "size": 2664833
            },
            {
                "type": "code",
                "name": "fairseq",
                "sha": "e16a6bebbe02cf898f70ddf5dc6f0a86ea4848c0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/tree/main/fairseq"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "generate-lines.lua",
                "sha": "2a848693d30f31b91808195650c5a1efff693e57",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/generate-lines.lua"
                    }
                },
                "size": 8246
            },
            {
                "type": "code",
                "name": "generate.lua",
                "sha": "1f125f6370dd21b054526d5895e5ce02e91174bc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/generate.lua"
                    }
                },
                "size": 8517
            },
            {
                "type": "code",
                "name": "help.lua",
                "sha": "47869c78f8ea3388ffeff56c6852e84db9402d3e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/help.lua"
                    }
                },
                "size": 1172
            },
            {
                "type": "code",
                "name": "optimize-fconv.lua",
                "sha": "ea31509bb6ef1f6efcc43bff7f4734d6bdff9af6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/optimize-fconv.lua"
                    }
                },
                "size": 1051
            },
            {
                "type": "code",
                "name": "preprocess.lua",
                "sha": "910b637fcd940acefe7963c00d0d51b236f86693",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/preprocess.lua"
                    }
                },
                "size": 6171
            },
            {
                "type": "code",
                "name": "rocks",
                "sha": "0f2d6f6009cdf068b14b07a63ed641699c410036",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/tree/main/rocks"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "run.lua",
                "sha": "35b1ac56961441b52cb97ec067892072ae7346d4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/run.lua"
                    }
                },
                "size": 494
            },
            {
                "type": "code",
                "name": "score.lua",
                "sha": "d510c9eeab568a487d353c977ca0a97b64c5235b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/score.lua"
                    }
                },
                "size": 1652
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "eeff75e536d301d070caac8a6f45c5effc1115c3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/tree/main/scripts"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "test",
                "sha": "b6bec807ee000984b578193a514d0271c7af33b6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/tree/main/test"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "tofloat.lua",
                "sha": "7cca466cf5075ee621ea9006c1a6dc7b9e484b29",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/tofloat.lua"
                    }
                },
                "size": 874
            },
            {
                "type": "code",
                "name": "train.lua",
                "sha": "9b412f2de5ae6d8c4d41e95ecabd43a0a0e087ef",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/facebookresearch/fairseq/blob/main/train.lua"
                    }
                },
                "size": 17567
            }
        ]
    },
    "authors": [
        {
            "name": "Jonas Gehring",
            "email": "jonas@jgehring.net",
            "github_id": "jgehring"
        },
        {
            "name": "Michael Auli",
            "github_id": "michaelauli"
        },
        {
            "name": "Yangbin Zhang",
            "email": "nikefd@gmail.com",
            "github_id": "nikefd"
        },
        {
            "name": "DavidGrangier",
            "github_id": "DavidGrangier"
        },
        {
            "name": "Denis Yarats",
            "github_id": "denisyarats"
        },
        {
            "name": "Jayasimha T",
            "github_id": "iamsimha"
        },
        {
            "name": "koheishingai",
            "email": "koheishingai+hq@gmail.com",
            "github_id": "KoheiShingaiHQ"
        },
        {
            "name": "Myle Ott",
            "email": "myleott@gmail.com",
            "github_id": "myleott"
        },
        {
            "name": "Sergey Edunov",
            "github_id": "edunov"
        }
    ],
    "tags": [],
    "description": "Facebook AI Research Sequence-to-Sequence Toolkit",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/facebookresearch/fairseq",
            "stars": 3781,
            "issues": true,
            "readme": "# Introduction\n\n***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here, but is provided without any support.\n\nThis is fairseq, a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT).\nIt implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model.\nIt features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU.\nWe provide pre-trained models for English to French, English to German and English to Romanian translation.\n\n![Model](fairseq.gif)\n\n# Citation\n\nIf you use the code in your paper, then please cite it as:\n\n```\n@article{gehring2017convs2s,\n  author          = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},\n  title           = \"{Convolutional Sequence to Sequence Learning}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1705.03122},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2017,\n  month           = May,\n}\n```\n\nand\n\n```\n@article{gehring2016convenc,\n  author          = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann N},\n  title           = \"{A Convolutional Encoder Model for Neural Machine Translation}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1611.02344},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2016,\n  month           = Nov,\n}\n```\n\n# Requirements and Installation\n* A computer running macOS or Linux\n* For training new models, you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* A [Torch installation](http://torch.ch/docs/getting-started.html). For maximum speed, we recommend using LuaJIT and [Intel MKL](https://software.intel.com/en-us/intel-mkl).\n* A recent version [nn](https://github.com/torch/nn). The minimum required version is from May 5th, 2017. A simple `luarocks install nn` is sufficient to update your locally installed version.\n\nInstall fairseq by cloning the GitHub repository and running\n```\nluarocks make rocks/fairseq-scm-1.rockspec\n```\nLuaRocks will fetch and build any additional dependencies that may be missing.\nIn order to install the CPU-only version (which is only useful for translating new data with an existing model), do\n```\nluarocks make rocks/fairseq-cpu-scm-1.rockspec\n```\n\nThe LuaRocks installation provides a command-line tool that includes the following functionality:\n* `fairseq preprocess`: Data pre-processing: build vocabularies and binarize training data\n* `fairseq train`: Train a new model on one or multiple GPUs\n* `fairseq generate`: Translate pre-processed data with a trained model\n* `fairseq generate-lines`: Translate raw text with a trained model\n* `fairseq score`: BLEU scoring of generated translations against reference translations\n* `fairseq tofloat`: Convert a trained model to a CPU model\n* `fairseq optimize-fconv`: Optimize a fully convolutional model for generation. This can also be achieved by passing the `-fconvfast` flag to the generation scripts.\n\n# Quick Start\n\n## Training a New Model\n\n### Data Pre-processing\nThe fairseq source distribution contains an example pre-processing script for\nthe IWSLT14 German-English corpus.\nPre-process and binarize the data as follows:\n```\n$ cd data/\n$ bash prepare-iwslt14.sh\n$ cd ..\n$ TEXT=data/iwslt14.tokenized.de-en\n$ fairseq preprocess -sourcelang de -targetlang en \\\n  -trainpref $TEXT/train -validpref $TEXT/valid -testpref $TEXT/test \\\n  -thresholdsrc 3 -thresholdtgt 3 -destdir data-bin/iwslt14.tokenized.de-en\n```\nThis will write binarized data that can be used for model training to data-bin/iwslt14.tokenized.de-en.\n\n### Training\nUse `fairseq train` to train a new model.\nHere a few example settings that work well for the IWSLT14 dataset:\n```\n# Standard bi-directional LSTM model\n$ mkdir -p trainings/blstm\n$ fairseq train -sourcelang de -targetlang en -datadir data-bin/iwslt14.tokenized.de-en \\\n  -model blstm -nhid 512 -dropout 0.2 -dropout_hid 0 -optim adam -lr 0.0003125 -savedir trainings/blstm\n\n# Fully convolutional sequence-to-sequence model\n$ mkdir -p trainings/fconv\n$ fairseq train -sourcelang de -targetlang en -datadir data-bin/iwslt14.tokenized.de-en \\\n  -model fconv -nenclayer 4 -nlayer 3 -dropout 0.2 -optim nag -lr 0.25 -clip 0.1 \\\n  -momentum 0.99 -timeavg -bptt 0 -savedir trainings/fconv\n\n# Convolutional encoder, LSTM decoder\n$ mkdir -p trainings/convenc\n$ fairseq train -sourcelang de -targetlang en -datadir data-bin/iwslt14.tokenized.de-en \\\n  -model conv -nenclayer 6 -dropout 0.2 -dropout_hid 0 -savedir trainings/convenc\n```\n\nBy default, `fairseq train` will use all available GPUs on your machine.\nUse the [CUDA_VISIBLE_DEVICES](http://acceleware.com/blog/cudavisibledevices-masking-gpus) environment variable to select specific GPUs or `-ngpus` to change the number of GPU devices that will be used.\n\n### Generation\nOnce your model is trained, you can translate with it using `fairseq generate` (for binarized data) or `fairseq generate-lines` (for text).\nHere, we'll do it for a fully convolutional model:\n```\n# Optional: optimize for generation speed\n$ fairseq optimize-fconv -input_model trainings/fconv/model_best.th7 -output_model trainings/fconv/model_best_opt.th7\n\n# Translate some text\n$ DATA=data-bin/iwslt14.tokenized.de-en\n$ fairseq generate-lines -sourcedict $DATA/dict.de.th7 -targetdict $DATA/dict.en.th7 \\\n  -path trainings/fconv/model_best_opt.th7 -beam 10 -nbest 2\n| [target] Dictionary: 24738 types\n| [source] Dictionary: 35474 types\n> eine sprache ist ausdruck des menschlichen geistes .\nS\teine sprache ist ausdruck des menschlichen geistes .\nO\teine sprache ist ausdruck des menschlichen geistes .\nH\t-0.23804219067097\ta language is expression of human mind .\nA\t2 2 3 4 5 6 7 8 9\nH\t-0.23861141502857\ta language is expression of the human mind .\nA\t2 2 3 4 5 7 6 7 9 9\n```\n\n### CPU Generation\nUse `fairseq tofloat` to convert a trained model to use CPU-only operations (this has to be done on a GPU machine):\n```\n# Optional: optimize for generation speed\n$ fairseq optimize-fconv -input_model trainings/fconv/model_best.th7 -output_model trainings/fconv/model_best_opt.th7\n\n# Convert to float\n$ fairseq tofloat -input_model trainings/fconv/model_best_opt.th7 \\\n  -output_model trainings/fconv/model_best_opt-float.th7\n\n# Translate some text\n$ fairseq generate-lines -sourcedict $DATA/dict.de.th7 -targetdict $DATA/dict.en.th7 \\\n  -path trainings/fconv/model_best_opt-float.th7 -beam 10 -nbest 2\n> eine sprache ist ausdruck des menschlichen geistes .\nS\teine sprache ist ausdruck des menschlichen geistes .\nO\teine sprache ist ausdruck des menschlichen geistes .\nH\t-0.2380430996418\ta language is expression of human mind .\nA\t2 2 3 4 5 6 7 8 9\nH\t-0.23861189186573\ta language is expression of the human mind .\nA\t2 2 3 4 5 7 6 7 9 9\n```\n\n# Pre-trained Models\n\nGeneration with the binarized test sets can be run in batch mode as follows, e.g. for English-French on a GTX-1080ti:\n```\n$ fairseq generate -sourcelang en -targetlang fr -datadir data-bin/wmt14.en-fr -dataset newstest2014 \\\n  -path wmt14.en-fr.fconv-cuda/model.th7 -beam 5 -batchsize 128 | tee /tmp/gen.out\n...\n| Translated 3003 sentences (95451 tokens) in 136.3s (700.49 tokens/s)\n| Timings: setup 0.1s (0.1%), encoder 1.9s (1.4%), decoder 108.9s (79.9%), search_results 0.0s (0.0%), search_prune 12.5s (9.2%)\n| BLEU4 = 43.43, 68.2/49.2/37.4/28.8 (BP=0.996, ratio=1.004, sys_len=92087, ref_len=92448)\n\n# Word-level BLEU scoring:\n$ grep ^H /tmp/gen.out | cut -f3- | sed 's/@@ //g' > /tmp/gen.out.sys\n$ grep ^T /tmp/gen.out | cut -f2- | sed 's/@@ //g' > /tmp/gen.out.ref\n$ fairseq score -sys /tmp/gen.out.sys -ref /tmp/gen.out.ref\nBLEU4 = 40.55, 67.6/46.5/34.0/25.3 (BP=1.000, ratio=0.998, sys_len=81369, ref_len=81194)\n```\n\n# Join the fairseq community\n\n* Facebook page: https://www.facebook.com/groups/fairseq.users\n* Google group: https://groups.google.com/forum/#!forum/fairseq-users\n* Contact: [jgehring@fb.com](mailto:jgehring@fb.com), [michaelauli@fb.com](mailto:michaelauli@fb.com)\n\n# License\nfairseq is BSD-licensed.\nThe license applies to the pre-trained models as well.\nWe also provide an additional patent grant.\n",
            "readme_url": "https://github.com/facebookresearch/fairseq",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Convolutional Sequence to Sequence Learning",
            "arxiv": "1705.03122",
            "year": 2017,
            "url": "http://arxiv.org/abs/1705.03122v3",
            "abstract": "The prevalent approach to sequence to sequence learning maps an input\nsequence to a variable length output sequence via recurrent neural networks. We\nintroduce an architecture based entirely on convolutional neural networks.\nCompared to recurrent models, computations over all elements can be fully\nparallelized during training and optimization is easier since the number of\nnon-linearities is fixed and independent of the input length. Our use of gated\nlinear units eases gradient propagation and we equip each decoder layer with a\nseparate attention module. We outperform the accuracy of the deep LSTM setup of\nWu et al. (2016) on both WMT'14 English-German and WMT'14 English-French\ntranslation at an order of magnitude faster speed, both on GPU and CPU.",
            "authors": [
                "Jonas Gehring",
                "Michael Auli",
                "David Grangier",
                "Denis Yarats",
                "Yann N. Dauphin"
            ]
        },
        {
            "title": "A Convolutional Encoder Model for Neural Machine Translation",
            "arxiv": "1611.02344",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.02344v3",
            "abstract": "The prevalent approach to neural machine translation relies on bi-directional\nLSTMs to encode the source sentence. In this paper we present a faster and\nsimpler architecture based on a succession of convolutional layers. This allows\nto encode the entire source sentence simultaneously compared to recurrent\nnetworks for which computation is constrained by temporal dependencies. On\nWMT'16 English-Romanian translation we achieve competitive accuracy to the\nstate-of-the-art and we outperform several recently published results on the\nWMT'15 English-German task. Our models obtain almost the same accuracy as a\nvery deep LSTM setup on WMT'14 English-French translation. Our convolutional\nencoder speeds up CPU decoding by more than two times at the same or higher\naccuracy as a strong bi-directional LSTM baseline.",
            "authors": [
                "Jonas Gehring",
                "Michael Auli",
                "David Grangier",
                "Yann N. Dauphin"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999151914098128,
        "task": "Machine Translation",
        "task_prob": 0.9834144196387276
    }
}