{
    "visibility": {
        "visibility": "public"
    },
    "name": "How to Train a GAN? Tips and tricks to make GANs work",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "kingcheng2000",
                "owner_type": "User",
                "name": "GAN",
                "url": "https://github.com/kingcheng2000/GAN",
                "stars": 0,
                "pushed_at": "2019-01-07 09:09:06+00:00",
                "created_at": "2019-01-07 09:08:53+00:00",
                "language": null,
                "frameworks": []
            },
            {
                "type": "code",
                "name": "images",
                "sha": "e5d9f49fff59950bdadce6311d9295159335d34d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kingcheng2000/GAN/tree/master/images"
                    }
                },
                "num_files": 4
            }
        ]
    },
    "authors": [
        {
            "name": "Soumith Chintala",
            "github_id": "soumith"
        },
        {
            "name": "ELM",
            "github_id": "zdx3578"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/kingcheng2000/GAN",
            "stars": 0,
            "issues": true,
            "readme": "# How to Train a GAN? Tips and tricks to make GANs work\n\nWhile research in Generative Adversarial Networks (GANs) continues to improve the\nfundamental stability of these models,\nwe use a bunch of tricks to train them and make them stable day to day.\n\nHere are a summary of some of the tricks.\n\n[Here's a link to the authors of this document](#authors)\n\nIf you find a trick that is particularly useful in practice, please open a Pull Request to add it to the document.\nIf we find it to be reasonable and verified, we will merge it in.\n\n## 1. Normalize the inputs\n\n- normalize the images between -1 and 1\n- Tanh as the last layer of the generator output\n\n## 2: A modified loss function\n\nIn GAN papers, the loss function to optimize G is `min (log 1-D)`, but in practice folks practically use `max log D`\n  - because the first formulation has vanishing gradients early on\n  - Goodfellow et. al (2014)\n\nIn practice, works well:\n  - Flip labels when training generator: real = fake, fake = real\n\n## 3: Use a spherical Z\n- Dont sample from a Uniform distribution\n\n![cube](images/cube.png \"Cube\")\n\n- Sample from a gaussian distribution\n\n![sphere](images/sphere.png \"Sphere\")\n\n- When doing interpolations, do the interpolation via a great circle, rather than a straight line from point A to point B\n- Tom White's [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) ref code https://github.com/dribnet/plat has more details\n\n\n## 4: BatchNorm\n\n- Construct different mini-batches for real and fake, i.e. each mini-batch needs to contain only all real images or all generated images.\n- when batchnorm is not an option use instance normalization (for each sample, subtract mean and divide by standard deviation).\n\n![batchmix](images/batchmix.png \"BatchMix\")\n\n## 5: Avoid Sparse Gradients: ReLU, MaxPool\n- the stability of the GAN game suffers if you have sparse gradients\n- LeakyReLU = good (in both G and D)\n- For Downsampling, use: Average Pooling, Conv2d + stride\n- For Upsampling, use: PixelShuffle, ConvTranspose2d + stride\n  - PixelShuffle: https://arxiv.org/abs/1609.05158\n\n## 6: Use Soft and Noisy Labels\n\n- Label Smoothing, i.e. if you have two target labels: Real=1 and Fake=0, then for each incoming sample, if it is real, then replace the label with a random number between 0.7 and 1.2, and if it is a fake sample, replace it with 0.0 and 0.3 (for example).\n  - Salimans et. al. 2016\n- make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator\n\n## 7: DCGAN / Hybrid Models\n\n- Use DCGAN when you can. It works!\n- if you cant use DCGANs and no model is stable, use a hybrid model :  KL + GAN or VAE + GAN\n\n## 8: Use stability tricks from RL\n\n- Experience Replay\n  - Keep a replay buffer of past generations and occassionally show them\n  - Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations\n- All stability tricks that work for deep deterministic policy gradients\n- See Pfau & Vinyals (2016)\n\n## 9: Use the ADAM Optimizer\n\n- optim.Adam rules!\n  - See Radford et. al. 2015\n- Use SGD for discriminator and ADAM for generator\n\n## 10: Track failures early\n\n- D loss goes to 0: failure mode\n- check norms of gradients: if they are over 100 things are screwing up\n- when things are working, D loss has low variance and goes down over time vs having huge variance and spiking\n- if loss of generator steadily decreases, then it's fooling D with garbage (says martin)\n\n## 11: Dont balance loss via statistics (unless you have a good reason to)\n\n- Dont try to find a (number of G / number of D) schedule to uncollapse training\n- It's hard and we've all tried it.\n- If you do try it, have a principled approach to it, rather than intuition\n\nFor example\n```\nwhile lossD > A:\n  train D\nwhile lossG > B:\n  train G\n```\n\n## 12: If you have labels, use them\n\n- if you have labels available, training the discriminator to also classify the samples: auxillary GANs\n\n## 13: Add noise to inputs, decay over time\n\n- Add some artificial noise to inputs to D (Arjovsky et. al., Huszar, 2016)\n  - http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/\n  - https://openreview.net/forum?id=Hk4_qw5xe\n- adding gaussian noise to every layer of generator (Zhao et. al. EBGAN)\n  - Improved GANs: OpenAI code also has it (commented out)\n\n## 14: [notsure] Train discriminator more (sometimes)\n\n- especially when you have noise\n- hard to find a schedule of number of D iterations vs G iterations\n\n## 15: [notsure] Batch Discrimination\n\n- Mixed results\n\n## 16: Discrete variables in Conditional GANs\n\n- Use an Embedding layer\n- Add as additional channels to images\n- Keep embedding dimensionality low and upsample to match image channel size\n\n## 17: Use Dropouts in G in both train and test phase\n- Provide noise in the form of dropout (50%).\n- Apply on several layers of our generator at both training and test time\n- https://arxiv.org/pdf/1611.07004v1.pdf\n\n\n## Authors\n- Soumith Chintala\n- Emily Denton\n- Martin Arjovsky\n- Michael Mathieu\n",
            "readme_url": "https://github.com/kingcheng2000/GAN",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network",
            "arxiv": "1609.05158",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.05158v2",
            "abstract": "Recently, several models based on deep neural networks have achieved great\nsuccess in terms of both reconstruction accuracy and computational performance\nfor single image super-resolution. In these methods, the low resolution (LR)\ninput image is upscaled to the high resolution (HR) space using a single\nfilter, commonly bicubic interpolation, before reconstruction. This means that\nthe super-resolution (SR) operation is performed in HR space. We demonstrate\nthat this is sub-optimal and adds computational complexity. In this paper, we\npresent the first convolutional neural network (CNN) capable of real-time SR of\n1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN\narchitecture where the feature maps are extracted in the LR space. In addition,\nwe introduce an efficient sub-pixel convolution layer which learns an array of\nupscaling filters to upscale the final LR feature maps into the HR output. By\ndoing so, we effectively replace the handcrafted bicubic filter in the SR\npipeline with more complex upscaling filters specifically trained for each\nfeature map, whilst also reducing the computational complexity of the overall\nSR operation. We evaluate the proposed approach using images and videos from\npublicly available datasets and show that it performs significantly better\n(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster\nthan previous CNN-based methods.",
            "authors": [
                "Wenzhe Shi",
                "Jose Caballero",
                "Ferenc Husz\u00e1r",
                "Johannes Totz",
                "Andrew P. Aitken",
                "Rob Bishop",
                "Daniel Rueckert",
                "Zehan Wang"
            ]
        },
        {
            "title": "Sampling Generative Networks",
            "arxiv": "1609.04468",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.04468v3",
            "abstract": "We introduce several techniques for sampling and visualizing the latent\nspaces of generative models. Replacing linear interpolation with spherical\nlinear interpolation prevents diverging from a model's prior distribution and\nproduces sharper samples. J-Diagrams and MINE grids are introduced as\nvisualizations of manifolds created by analogies and nearest neighbors. We\ndemonstrate two new techniques for deriving attribute vectors: bias-corrected\nvectors with data replication and synthetic vectors with data augmentation.\nBinary classification using attribute vectors is presented as a technique\nsupporting quantitative analysis of the latent space. Most techniques are\nintended to be independent of model type and examples are shown on both\nVariational Autoencoders and Generative Adversarial Networks.",
            "authors": [
                "Tom White"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999534454440009,
        "task": "Image-to-Image Translation",
        "task_prob": 0.9469333781664571
    }
}