{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "Face Mask Detector using MobileNetV2 :india:",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "ikigai-aa",
                "owner_type": "User",
                "name": "Face-Mask-Detector-using-MobileNetV2",
                "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2",
                "stars": 7,
                "pushed_at": "2022-03-12 00:44:29+00:00",
                "created_at": "2020-07-27 11:18:04+00:00",
                "language": "Jupyter Notebook",
                "description": "This is a simple image classification project trained on the top of Keras/Tensorflow API with MobileNetV2 deep neural network architecture having weights considered as pre-trained 'imagenet' weights. The trained model (mask-detector-model.h5) takes the real-time video from webcam as an input and predicts if the face landmarks in Region of Interest (ROI) is 'Mask' or 'No Mask' with real-time on screen accuracy.",
                "license": "Apache License 2.0",
                "frameworks": [
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "Data Augmentation and Model Training.ipynb",
                "sha": "e659e7006a14b8de888fac7c1e6f7f9f369a695e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/Data Augmentation and Model Training.ipynb"
                    }
                },
                "size": 45098
            },
            {
                "type": "code",
                "name": "Facemask Detection.pptx",
                "sha": "ada2b803b9cc37e273ca1a024cc039fae0048df0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/Facemask Detection.pptx"
                    }
                },
                "size": 5005524
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/LICENSE"
                    }
                },
                "size": 11357
            },
            {
                "type": "code",
                "name": "Milestone Report- FMD.pdf",
                "sha": "7086e13b304c4b3dfad5bbe311cd759c44d5e6e1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/Milestone Report- FMD.pdf"
                    }
                },
                "size": 551390
            },
            {
                "type": "code",
                "name": "Proposal.pdf",
                "sha": "28899e8e04257fed0edb41c4b0f90b8af1a93efb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/Proposal.pdf"
                    }
                },
                "size": 274883
            },
            {
                "type": "code",
                "name": "demo video.mp4",
                "sha": "87c95e86f3c326ea2ba0dd48c230704169de75c2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/demo video.mp4"
                    }
                },
                "size": 2274252
            },
            {
                "type": "code",
                "name": "detect_mask_from_webcam.py",
                "sha": "024e45b77271345a32d6cea9e6fe548a632096c7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/detect_mask_from_webcam.py"
                    }
                },
                "size": 4914
            },
            {
                "type": "code",
                "name": "evaluation.png",
                "sha": "94a969f580ac546843a8206f6e9437732df5715f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/evaluation.png"
                    }
                },
                "size": 23566
            },
            {
                "type": "code",
                "name": "face_detector.zip",
                "sha": "4a157b8e0c3d89fc18404d2988e0bb89290abe73",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/face_detector.zip"
                    }
                },
                "size": 10037614
            },
            {
                "type": "code",
                "name": "images",
                "sha": "5e0b005161fb917704d73a5653e191093d31e48f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/tree/master/images"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "mask-detector-model.model",
                "sha": "3d8384568d7c579914164d7e600e0bfbdac2c355",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/mask-detector-model.model"
                    }
                },
                "size": 11483520
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "332d70cdac66730163479188c4c8da6ecb3699c1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/requirements.txt"
                    }
                },
                "size": 880
            }
        ]
    },
    "authors": [
        {
            "name": "Ashish Agarwal",
            "github_id": "ikigai-aa"
        }
    ],
    "tags": [
        "face-mask-detector",
        "mobilenet",
        "opencv",
        "coronavirus",
        "python",
        "keras-tensorflow",
        "scikit-learn",
        "machine-learning",
        "deep-learning",
        "jupyter-notebook"
    ],
    "description": "This is a simple image classification project trained on the top of Keras/Tensorflow API with MobileNetV2 deep neural network architecture having weights considered as pre-trained 'imagenet' weights. The trained model (mask-detector-model.h5) takes the real-time video from webcam as an input and predicts if the face landmarks in Region of Interest (ROI) is 'Mask' or 'No Mask' with real-time on screen accuracy.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2",
            "stars": 7,
            "issues": true,
            "readme": "# Face Mask Detector using MobileNetV2 :india:\n\n## Table of Content\n  * [Demo](#demo)\n  * [Overview](#overview)\n  * [Motivation](#motivation)\n  * [Technical Aspect](#technical-aspect)\n  * [Installation](#installation)\n  * [Run](#run)\n  * [Directory Tree](#directory-tree)\n  * [To Do](#to-do)\n  * [Bug / Feature Request](#bug---feature-request)\n  * [Technologies Used](#technologies-used)\n  * [Team](#team)\n  * [License](#license)\n  * [Resources](#resources)\n  \n  \n## Demo\nLink: [https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/demo%20video.mp4](https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/demo%20video.mp4)\n\n\n## Overview\nThis is a simple image classification project trained on the top of Keras/Tensorflow API with MobileNetV2 deep neural network architecture having weights considered as pre-trained 'imagenet' weights. The trained model (`mask-detector-model.h5`) takes the real-time video from webcam as an input and predicts if the face landmarks in Region of Interest (ROI) is 'Mask' or 'No Mask' with real-time on screen accuracy.\n\n\n## Motivation\n\nGlobally, the coronavirus stats says it has more than 16.3M confirmed cases and claimed over 649K lives so far, according to the Johns Hopkins University when I am writing this project repo. As many as 9.41 M people have recovered.\n\nIndia\u2019s coronavirus cases are increasing in an unimaginable rate and is breaking the record in the highest single-day increase so far every new day.. The country\u2019s tally rose to 14,35,453 and the toll stood at 32,771. India is now the third worst-affected country by the pandemic and has overtaken Italy, according to Johns Hopkins University.\n\nThe World Health Organization said that new information showed that protective masks could be a barrier for potentially infectious droplets. The coronavirus primarily spreads through the transmission of respiratory droplets from infected people.On 5th day of June changed its guidelines about the use of protective face masks in public, saying that they must be worn at all places where physical distancing is not possible. The global health body had said in April that there was not enough evidence to show that healthy people should wear masks to shield themselves from the coronavirus.\n\n![](https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/images/WHO.png)\n\n\nLink: [https://twitter.com/i/status/1268986094042992640](https://twitter.com/i/status/1268986094042992640)\n\n\nWHO also said that high-risk groups should wear medical grade masks in cases where physical distancing is not possible.Several countries, including India, have made wearing masks in public compulsory. In many states, people have been fined for not wearing masks. Maintaining hygiene and using protective equipment has become even more important ahead of the reopening of religious places, malls and restaurants in India from next week.\n\nThis motivated me to create a the COVID-19 Mask Detector with some of my ML/DL skills and making it such accurate that it could potentially be used to help ensure your safety and the safety of others (Leaving it on to the medical professionals to decide on, implement in public places).\n\n\n## Technical Aspect\nIn order to train a Face Mask Detector, we need to break our project into two distinct phases, each with its own respective sub-steps:\n\nTraining: Here we\u2019ll focus on loading our face mask detection dataset from disk, training a model (using Keras/TensorFlow) on this dataset, and then serializing the face mask detector to disk\n\nDeployment: Once the face mask detector is trained, we can then move on to loading the mask detector, performing face detection, and then classifying each face as with_mask or without_mask\n\n![](https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/images/face_mask_detection_phases.png)\\\n\n\n### Dataset Resource:\n\nLink: https://drive.google.com/drive/folders/1FHPJRCab-cyLq8IVz83LkU71gOc7gTS8?usp=sharing\n\n![](https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/images/face_mask_detection_dataset.jpg)\n\n\n### Project structure\n\n```\n\u251c\u2500\u2500 dataset\n\u2502   \u251c\u2500\u2500 with_mask [690 entries]\n\u2502   \u2514\u2500\u2500 without_mask [686 entries]\n\u251c\u2500\u2500 examples\n\u2502   \u251c\u2500\u2500 example_01.png\n\u2502   \u251c\u2500\u2500 example_02.png\n\u2502   \u2514\u2500\u2500 example_03.png\n\u251c\u2500\u2500 face_detector\n\u2502   \u251c\u2500\u2500 deploy.prototxt\n\u2502   \u2514\u2500\u2500 res10_300x300_ssd_iter_140000.caffemodel\n\u251c\u2500\u2500 detect_mask_image.py\n\u251c\u2500\u2500 detect_mask_video.py\n\u251c\u2500\u2500 mask_detector.model\n\u251c\u2500\u2500 evaluation.png\n\u2514\u2500\u2500 Data Augmentation and Model Training.ipynb\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 mask-detector-model.model\n```\n\n\n### Important Python Scripts:\n\n1. Data Augmentation and Preprocessing.ipynb: In this notebook Accepts our dataset is taken as input and fine-tuning is donw with MobileNetV2 DNN architecture upon it to create our mask-detector-model.model. A training history evaluation.png containing accuracy/loss curves is also produced for better visualization of Model Evaluation through a plot.Some important processes which we performed here:\n\na. Data augmentation\nb. Loading the MobilNetV2 classifier (we will fine-tune this model with pre-trained ImageNet weights)\nc. Building a new fully-connected (FC) head\nd. Pre-processing\ne. Loading image data\n\nLibraries Significance:\n\nscikit-learn: for binarizing class labels, segmenting our dataset, and printing a classification report.\nimutils: To find and list images in our dataset. \nmatplotlib: To plot our training curves.\n\n2. detect_mask_from_webcam.py: Using your webcam, this script applies face mask detection to every frame in the stream using webcom to read the real-time video.\n\nSome command line arguments in this script include:\n```\n--image: The path to the input image containing faces for inference\n--face: The path to the face detector model directory (we need to localize faces prior to classifying them)\n--model: The path to the face mask detector model that we trained earlier in this tutorial\n--confidence: An optional probability threshold can be set to override 50% to filter weak face detections\n```\n\n\n## Installation\nThe Code is written in Python 3.7. If you don't have Python installed you can find it [here](https://www.python.org/downloads/). If you are using a lower version of Python you can upgrade using the pip package, ensuring you have the latest version of pip. To install the required packages and libraries, run this command in the project directory after [cloning](https://www.howtogeek.com/451360/how-to-clone-a-github-repository/) the repository:\n\n```bash\n## Run\n> STEP 1\nAfter unzipping the forked zip file of this project into your local machine, type the follwing command from the directory where you saved the project files in the command prompt: \npip install -r requirements.txt\n\nThis will install thw following libraries:\n\nabsl-py==0.9.0\nastunparse==1.6.3\ncachetools==4.1.1\ncertifi==2020.6.20\nchardet==3.0.4\ncycler==0.10.0\ngast==0.3.3\ngoogle-auth==1.19.2\ngoogle-auth-oauthlib==0.4.1\ngoogle-pasta==0.2.0\ngrpcio==1.30.0\nh5py==2.10.0\nidna==2.10\nimportlib-metadata==1.7.0\nimutils==0.5.3\njoblib==0.16.0\nKeras-Preprocessing==1.1.2\nkiwisolver==1.2.0\nMarkdown==3.2.2\nmatplotlib==3.3.0\nnumpy==1.19.1\noauthlib==3.1.0\nopencv-python==4.3.0.36\nopt-einsum==3.3.0\nPillow==7.2.0\nprotobuf==3.12.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npyparsing==2.4.7\npython-dateutil==2.8.1\nrequests==2.24.0\nrequests-oauthlib==1.3.0\nrsa==4.6\nscikit-learn==0.23.1\nscipy==1.4.1\nsix==1.15.0\nsklearn==0.0\ntensorboard==2.2.2\ntensorboard-plugin-wit==1.7.0\ntensorflow==2.2.0\ntensorflow-estimator==2.2.0\ntermcolor==1.1.0\nthreadpoolctl==2.1.0\nurllib3==1.25.10\n\n\n> STEP 2\nOpen Jupyter Notebook and run Data Augmentation and Preprocessing.ipynb in order to train your custom dataset within your loacl machine and preprocess the images meanwhile.\n\n> STEP 3\nRun detect_mask_from_webcam.py from the same directory of your project folder in the command prompt in order to test the detector in real- time using the webcam.\n```\n\n## Results/Classification Report\n\n```\n              precision    recall  f1-score   support\n\n   with_mask       0.97      1.00      0.99       138\nwithout_mask       1.00      0.97      0.99       138\n\n    accuracy                           0.99       276\n   macro avg       0.99      0.99      0.99       276\nweighted avg       0.99      0.99      0.99       276\n\n```\n\n## Accuracy/Loss Plot\n\n![](https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/blob/master/evaluation.png)\\\n\n...\n## To Do\n1. This approach reduces our computer vision pipeline to a single step \u2014 rather than applying face detection and then our face mask detector model, all we need to do is apply the object detector to give us bounding boxes for people both with_mask and without_mask in a single forward pass of the network.\n\n2. An integration of this project to a web app/android app.\n\n\n## Bug / Feature Request\nIf you find a bug (the website couldn't handle the query and / or gave undesired results), kindly open an issue [here](https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/issues/new) by including your search query and the expected result.\n\nIf you'd like to request a new function, feel free to do so by opening an issue [here](https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2/issues/new). Please include sample queries and their corresponding results.\n\n\n## Technologies Used\n\n![](https://forthebadge.com/images/badges/made-with-python.svg)\n\n[<img target=\"_blank\" src=\"https://keras.io/img/logo.png\" width=200>](https://keras.io/) \n\n[<img target=\"_blank\" src=\"https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" width=170>](https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png)\n\n[<img target=\"_blank\" src=\"https://www.gstatic.com/devrel-devsite/prod/vbf66214f2f7feed2e5d8db155bab9ace53c57c494418a1473b23972413e0f3ac/tensorflow/images/lockup.svg\" width=280>](https://www.gstatic.com/devrel-devsite/prod/vbf66214f2f7feed2e5d8db155bab9ace53c57c494418a1473b23972413e0f3ac/tensorflow/images/lockup.svg)\n\n[<img target=\"_blank\" src=\"http://image-net.org/index_files/logo.jpg\" width=200>](http://image-net.org/index_files/logo.jpg) \n\n[<img target=\"_blank\" src=\"https://jupyter.org/assets/nav_logo.svg\" width=200>](https://jupyter.org/assets/nav_logo.svg) \n\n\n## Team\nAshish Agarwal\n\nLinkedIn Profile: [https://www.linkedin.com/in/ashish-agarwal-502203113/](https://www.linkedin.com/in/ashish-agarwal-502203113/)\n\n\n## License\n[![Apache license](https://img.shields.io/badge/license-apache-blue?style=for-the-badge&logo=appveyor)](http://www.apache.org/licenses/LICENSE-2.0e)\n\nCopyright 2020 Ashish Agarwal\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\n## Resources\n\n1. https://www.who.int/publications/i/item/advice-on-the-use-of-masks-in-the-community-during-home-care-and-in-healthcare-settings-in-the-context-of-the-novel-coronavirus-(2019-ncov)-outbreak\n2. https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/\n3. http://www.image-net.org/\n4. https://arxiv.org/abs/1801.04381\n",
            "readme_url": "https://github.com/ikigai-aa/Face-Mask-Detector-using-MobileNetV2",
            "frameworks": [
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
            "arxiv": "1801.04381",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.04381v4",
            "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that\nimproves the state of the art performance of mobile models on multiple tasks\nand benchmarks as well as across a spectrum of different model sizes. We also\ndescribe efficient ways of applying these mobile models to object detection in\na novel framework we call SSDLite. Additionally, we demonstrate how to build\nmobile semantic segmentation models through a reduced form of DeepLabv3 which\nwe call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where\nthe input and output of the residual block are thin bottleneck layers opposite\nto traditional residual models which use expanded representations in the input\nan MobileNetV2 uses lightweight depthwise convolutions to filter features in\nthe intermediate expansion layer. Additionally, we find that it is important to\nremove non-linearities in the narrow layers in order to maintain\nrepresentational power. We demonstrate that this improves performance and\nprovide an intuition that led to this design. Finally, our approach allows\ndecoupling of the input/output domains from the expressiveness of the\ntransformation, which provides a convenient framework for further analysis. We\nmeasure our performance on Imagenet classification, COCO object detection, VOC\nimage segmentation. We evaluate the trade-offs between accuracy, and number of\noperations measured by multiply-adds (MAdd), as well as the number of\nparameters",
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9994407950621388,
        "task": "Object Detection",
        "task_prob": 0.9300981747799149
    }
}