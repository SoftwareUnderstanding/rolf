{
    "visibility": {
        "visibility": "public",
        "license": "BSD 3-Clause \"New\" or \"Revised\" License"
    },
    "name": "Changelog 0.2:",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "fuchason",
                "owner_type": "Organization",
                "name": "NTM-keras",
                "url": "https://github.com/fuchason/NTM-keras",
                "stars": 0,
                "pushed_at": "2019-07-01 03:21:38+00:00",
                "created_at": "2019-07-01 03:21:23+00:00",
                "language": "Python",
                "license": "BSD 3-Clause \"New\" or \"Revised\" License",
                "frameworks": [
                    "Keras",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "c8c270869c7cb81f40c0d68e1c696849f3c5bff4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fuchason/NTM-keras/blob/master/LICENSE"
                    }
                },
                "size": 1513
            },
            {
                "type": "code",
                "name": "The_NTM_-_Introduction_And_Implementation.pdf",
                "sha": "31aaf279d4617b5d853e2fa5db9159962d4a5705",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fuchason/NTM-keras/blob/master/The_NTM_-_Introduction_And_Implementation.pdf"
                    }
                },
                "size": 273800
            },
            {
                "type": "code",
                "name": "copyTask.py",
                "sha": "abc37b78ec105c8236ea3355c1972c5583e3443f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fuchason/NTM-keras/blob/master/copyTask.py"
                    }
                },
                "size": 2350
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "eb563f9899feb8c641194d2f5903fd94d6f14fe5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fuchason/NTM-keras/blob/master/main.py"
                    }
                },
                "size": 4095
            },
            {
                "type": "code",
                "name": "model_dense.py",
                "sha": "43f015402fe531ae375408c59e5fff63742c8ac1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fuchason/NTM-keras/blob/master/model_dense.py"
                    }
                },
                "size": 718
            },
            {
                "type": "code",
                "name": "model_lstm.py",
                "sha": "924663c9f9b7ca5b9f7852b230bc0e5430bd71b5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fuchason/NTM-keras/blob/master/model_lstm.py"
                    }
                },
                "size": 870
            },
            {
                "type": "code",
                "name": "model_ntm.py",
                "sha": "8b4fccd90e74f46df0e4696708ff1d8fee75ef3f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fuchason/NTM-keras/blob/master/model_ntm.py"
                    }
                },
                "size": 1347
            },
            {
                "type": "code",
                "name": "ntm.py",
                "sha": "e653e93c9b1cc1cda8f1de7e2be66fee3f6931dc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fuchason/NTM-keras/blob/master/ntm.py"
                    }
                },
                "size": 18337
            },
            {
                "type": "code",
                "name": "testing_utils.py",
                "sha": "bd03e49de81c4e0fba59b1975d5dadbcc1bfa72c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fuchason/NTM-keras/blob/master/testing_utils.py"
                    }
                },
                "size": 3225
            },
            {
                "type": "code",
                "name": "view_weights.py",
                "sha": "4927c3f667f611c21c5100fbee8a0d3232038ccb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/fuchason/NTM-keras/blob/master/view_weights.py"
                    }
                },
                "size": 552
            }
        ]
    },
    "authors": [
        {
            "name": "flomlo",
            "github_id": "flomlo"
        },
        {
            "name": "Keld Lundgaard",
            "email": "keld.lundgaard@gmail.com",
            "github_id": "keldLundgaard"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/fuchason/NTM-keras",
            "stars": 0,
            "issues": true,
            "readme": "### Changelog 0.2:\n* API CHANGE: Controller models now must have linear activation. The activation of the NTM-Layer is selected\n  by the new parameter \"activation\" (default: \"linear\"). For all the stuff that interacts with the memory we now\n  have very precise handselected activations which asume that there was no prior de-linearisation.\n  This requirement on the controller will probably be final.\n* There is now support for multiple read/write heads! Use the parameters read_heads resp. write_heads at initialisation\n  (by default both are 1).\n* The code around controller output splitting and activation was completely rewritten and cleaned from a lot of\n  copy-paste-code.\n* Unfortunately we lost backend neutrality: As tf.slice is used extensivly, we have to either try getting K.slice or\n  have to do a case distinction over backend. Use the old version if you need another backend than tensorflow! And\n  please write me a message.\n* As less activations have to be computed, it is now a tiny little bit faster (~1%).\n* Stateful models do not work anymore. Actually they never worked, the testing routine was just broken. Will be repaired\n  asap.\n\n# The Neural Turing Machine\n### Introduction\nThis code tries to implement the Neural Turing Machine, as found in \nhttps://arxiv.org/abs/1410.5401, as a backend neutral recurrent keras layer.\n\nA very default experiment, the copy task, is provided, too.\n\nIn the end there is a TODO-List. Help would be appreciated!\n\nNOTE:\n* There is a nicely formatted paper describing the rough idea of the NTM, implementation difficulties and which discusses the\n  copy experiment. It is available here in the repository as The_NTM_-_Introduction_And_Implementation.pdf. \n* You may want to change the LOGDIR_BASE in testing_utils.py to something that works for you or just set a symbolic\n  link.\n\n\n### User guide\nFor a quick start on the copy task, type \n\n    python main.py -v ntm\n\nwhile in a python enviroment which has tensorflow, keras and numpy.\nHaving tensorflow-gpu is recommend, as everything is about 20x faster.\nIn my case this experiment takes about 100 minutes on a NVIDIA GTX 1050 Ti.\nThe -v is optional and offers much more detailed information about the achieved accuracy, and also after every training\nepoch.\nLogging data is written LOGDIR_BASE, which is ./logs/ by default. View them with tensorboard:\n\n    tensorboard --logdir ./logs\n\nIf you've luck and not had a terrible run (that can happen, unfortunately), you now have a machine capable of copying a\ngiven sequence! I wonder if we could have achieved that any other way ...\n\nThese results are especially interesting compared to an LSTM model: Run\n\n    python main.py lstm\n\nThis builds 3 layers of LSTM with and goes through the same testing procedure\nas above, which for me resulted in a training time of approximately 1h (same GPU) and \n(roughly) 100%, 100%, 94%, 50%, 50% accuracy at the respective test lengths.\nThis shows that the NTM has advantages over LSTM in some cases. Especially considering the LSTM model has about 807.200\ntrainable parameters while the NTM had a mere 3100! \n\nHave fun playing around, maybe with other controllers? dense, double_dense and lstm are build in.\n\n\n### API\nFrom the outside, this implementation looks like a regular recurrent layer in keras.\nIt has however a number of non-obvious parameters:\n\n#### Hyperparameters\n\n  \n*  `n_width`: This is the width of the memory matrix. Increasing this increases computational complexity in O(n^2). The\n   controller shape is not dependant on this, making weight transfer possible.\n\n*  `m_depth`: This is the depth of the memory matrix. Increasing this increases the number of trainable weights in O(m^2). It also changes controller shape. \n\n*  `controller_model`: This parameter allows you to place a keras model of appropriate shape as the controller. The\nappropriate shape can be calculated via controller_input_output_shape. If None is set, a single dense layer will be\nused. \n\n*  `read_heads`: The number of read heads this NTM should have. Has quadratic influence on the number of trainable\n   weights. Default: 1\n\n*  `write_heads`: The number of write heads this NTM should have. Has quadratic influence on the number of trainable\n   weights, but for small numbers a *huge* impact. Default: 1\n\n\n#### Usage\n\nMore or less minimal code example:\n\n    from keras.models import Sequential\n    from keras.optimizers import Adam\n    from ntm import NeuralTuringMachine as NTM\n\n    model = Sequential()\n    model.name = \"NTM_-_\" + controller_model.name\n\n    ntm = NTM(output_dim, n_slots=50, m_depth=20, shift_range=3,\n              controller_model=None,\n              return_sequences=True,\n              input_shape=(None, input_dim), \n              batch_size = 100)\n    model.add(ntm)\n\n    sgd = Adam(lr=learning_rate, clipnorm=clipnorm)\n    model.compile(loss='binary_crossentropy', optimizer=sgd,\n                   metrics = ['binary_accuracy'], sample_weight_mode=\"temporal\")\n\nWhat if we instead want a more complex controller? Design it, e.g. double LSTM:\n\n    controller = Sequential()\n    controller.name=ntm_controller_architecture\n    controller.add(LSTM(units=150,\n                        stateful=True,\n                        implementation=2,   # best for gpu. other ones also might not work.\n                        batch_input_shape=(batch_size, None, controller_input_dim)))\n    controller.add(LSTM(units=controller_output_dim,\n                        activation='linear',\n                        stateful=True,\n                        implementation=2))   # best for gpu. other ones also might not work.\n\n    controller.compile(loss='binary_crossentropy', optimizer=sgd,\n                     metrics = ['binary_accuracy'], sample_weight_mode=\"temporal\")\n\nAnd now use the same code as above, only with controller_model=controller.\n\nNote that we used linear as the last activation layer! This is of critical importance.\nThe activation of the NTM-layer can be set the parameter activation (default: linear).\n\nNote that a correct controller_input_dim and controller_output_dim can be calculated via controller_input_output_shape:\n\n    from ntm import controller_input_output_shape\n    controller_input_dim, controller_output_dim = ntm.controller_input_output_shape(\n                input_dim, output_dim, m_depth, n_slots, shift_range, read_heads, write_heads) \n\n\nAlso note that every statefull controller must carry around his own state, as was done here with \n\n    stateful=True\n\n\n\n\n\n## TODO:\n- [x] Arbitrary number of read and write heads\n- [ ] Support of masking, and maybe dropout, one has to reason about it theoretically first.\n- [ ] Support for get and set config to better enable model saving\n- [x] A bit of code cleaning: especially the controller output splitting is ugly as hell.\n- [x] Support for arbitrary activation functions would be nice, currently restricted to sigmoid.\n- [ ] Make it backend neutral again! Some testing might be nice, too. \n- [ ] Maybe add the other experiments of the original paper?\n- [ ] Mooaaar speeeed. Look if there are platant performance optimizations possible. \n",
            "readme_url": "https://github.com/fuchason/NTM-keras",
            "frameworks": [
                "Keras",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Neural Turing Machines",
            "arxiv": "1410.5401",
            "year": 2014,
            "url": "http://arxiv.org/abs/1410.5401v2",
            "abstract": "We extend the capabilities of neural networks by coupling them to external\nmemory resources, which they can interact with by attentional processes. The\ncombined system is analogous to a Turing Machine or Von Neumann architecture\nbut is differentiable end-to-end, allowing it to be efficiently trained with\ngradient descent. Preliminary results demonstrate that Neural Turing Machines\ncan infer simple algorithms such as copying, sorting, and associative recall\nfrom input and output examples.",
            "authors": [
                "Alex Graves",
                "Greg Wayne",
                "Ivo Danihelka"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.7967812067023479,
        "task": "Question Answering",
        "task_prob": 0.4073244084910221
    }
}