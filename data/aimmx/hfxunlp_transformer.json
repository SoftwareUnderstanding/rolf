{
    "visibility": {
        "visibility": "public",
        "license": "GNU General Public License v3.0"
    },
    "name": "Neutron",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "hfxunlp",
                "owner_type": "User",
                "name": "transformer",
                "url": "https://github.com/hfxunlp/transformer",
                "stars": 59,
                "pushed_at": "2021-12-23 11:28:06+00:00",
                "created_at": "2019-01-11 08:04:15+00:00",
                "language": "Python",
                "description": "Neutron: A pytorch based implementation of Transformer and its variants.",
                "license": "GNU General Public License v3.0",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "befedc98b153cc947c55bd5f4f98ebcff1e39c9d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/.gitignore"
                    }
                },
                "size": 1270
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "f288702d2fa16d3cdf0035b15a9fcbc552cd88e7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/LICENSE"
                    }
                },
                "size": 35149
            },
            {
                "type": "code",
                "name": "adv",
                "sha": "484f146f09a9bbe3f0e9eff898793dbf362a603a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/adv"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "cnfg",
                "sha": "b705f2b6c9215bf20c68102f58e1fc6a1ecd934b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/cnfg"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "datautils",
                "sha": "00174f9d8fdcf281e34e0306b05831aaf3ce5702",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/datautils"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "loss",
                "sha": "e49879adba86d1bae76913b9d23e743dbbb48eef",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/loss"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "lrsch.py",
                "sha": "a014a9b533cbf4926e8d7c7a27c425aaee62e9e2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/lrsch.py"
                    }
                },
                "size": 2277
            },
            {
                "type": "code",
                "name": "mkcy.py",
                "sha": "d3fff0abe3297d9a240cf569f1b193bb8b41d838",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/mkcy.py"
                    }
                },
                "size": 1617
            },
            {
                "type": "code",
                "name": "modules",
                "sha": "1e9fb55350752285e677c72a19784d29e0f77477",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/modules"
                    }
                },
                "num_files": 16
            },
            {
                "type": "code",
                "name": "optm",
                "sha": "e10eeb90af48d9366643ee97b4f548061feb4d72",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/optm"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "parallel",
                "sha": "6a0a4301de2b80161ea4616a5966e93f1b3b37c9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/parallel"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "predict.py",
                "sha": "e0f437d44484f3b39c71bfdad7c8c76f15cda99d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/predict.py"
                    }
                },
                "size": 2677
            },
            {
                "type": "code",
                "name": "rank_loss.py",
                "sha": "d2d0559e4c539adf2b4677f6aca25db53f2428de",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/rank_loss.py"
                    }
                },
                "size": 3027
            },
            {
                "type": "code",
                "name": "requirements.opt.txt",
                "sha": "d1ce33f75fea988d770a8d5a762b2f2236e50de7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/requirements.opt.txt"
                    }
                },
                "size": 118
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "a381cf3e69928e6da825e823ab840d9c9e781cda",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/requirements.txt"
                    }
                },
                "size": 39
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "be10768c7ef02ebdcac823ec044ff5c0fe450c30",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/scripts"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "server.py",
                "sha": "05c5034063098715e7ced58fb98be5c2efb849eb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/server.py"
                    }
                },
                "size": 1961
            },
            {
                "type": "code",
                "name": "templates",
                "sha": "6c971a1bbd989e6eef91cffc6ff7ff3a5a8ca0b3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/templates"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "tools",
                "sha": "1e975cf29a2241e036cfb012076f782421d06816",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/tools"
                    }
                },
                "num_files": 20
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "26f496f9d43723c6e565195736108735451dafcc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/train.py"
                    }
                },
                "size": 15111
            },
            {
                "type": "code",
                "name": "transformer",
                "sha": "b8e59a98bcefc8c3bdca1b551b62e55aaace71d9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/transformer"
                    }
                },
                "num_files": 23
            },
            {
                "type": "code",
                "name": "translator.py",
                "sha": "113f4333533303eac934216ece8fd4b66cfd33de",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/translator.py"
                    }
                },
                "size": 7865
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "61feebb80a660adf7c30fa405168fc08e0f9f472",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/tree/master/utils"
                    }
                },
                "num_files": 20
            },
            {
                "type": "code",
                "name": "wsgi.ini",
                "sha": "5ad2e0adde4a77ef496a2eeabd6e7904bf4eb194",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hfxunlp/transformer/blob/master/wsgi.ini"
                    }
                },
                "size": 157
            }
        ]
    },
    "authors": [
        {
            "name": "liuqiuhui2015",
            "github_id": "liuqiuhui2015"
        },
        {
            "name": "Hongfei Xu",
            "github_id": "hfxunlp"
        }
    ],
    "tags": [
        "neural-machine-translation",
        "transformer",
        "seq2seq",
        "natural-language-processing",
        "attention-is-all-you-need",
        "average-attention-network",
        "dynamic-sentence-sampling",
        "robust-neural-machine-translation",
        "beam-search",
        "ensemble",
        "average-models",
        "pytorch",
        "python3",
        "multi-gpu",
        "sentential-context",
        "context-aware-nmt",
        "relative-position",
        "optimizers",
        "dynamic-batch-size",
        "multilingual-nmt"
    ],
    "description": "Neutron: A pytorch based implementation of Transformer and its variants.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/hfxunlp/transformer",
            "stars": 59,
            "issues": true,
            "readme": "# Neutron\nNeutron: A pytorch based implementation of the [Transformer](https://arxiv.org/abs/1706.03762) and its variants.\n\nThis project is developed with python 3.8.\n\n## Setup dependencies\n\nTry `pip install -r requirements.txt` after you clone the repository.\n\nIf you want to use [BPE](https://github.com/rsennrich/subword-nmt), to enable convertion to C libraries, to try the simple MT server and to support Chinese word segmentation supported by [pynlpir](https://github.com/tsroten/pynlpir) in this implementation, you should also install those dependencies in `requirements.opt.txt` with `pip install -r requirements.opt.txt`.\n\n## Data preprocessing\n\n### BPE\n\nWe provide scripts to apply Byte-Pair Encoding (BPE) under `scripts/bpe/`.\n\n### convert plain text to tensors for training\n\nGenerate training data for `train.py` with `bash scripts/mktrain.sh`, [configure variables](scripts/README.md#mktrainsh) in `scripts/mktrain.sh` for your usage (the other variables shall comply with those in `scripts/bpe/mk.sh`).\n\n## Configuration for training and testing\n\nMost [configurations](cnfg/README.md#basepy) are managed in `cnfg/base.py`. [Configure advanced details](cnfg/README.md#hyppy) with `cnfg/hyp.py`.\n\n## Training\n\nJust execute the following command to launch the training:\n\n`python train.py`\n\n## Generation\n\n`bash scripts/mktest.sh`, [configure variables](scripts/README.md#mktestsh) in `scripts/mktest.sh` for your usage (while keep the other settings consistent with those in `scripts/mkbpe.sh` and `scripts/mktrain.sh`).\n\n## Exporting python files to C libraries\n\nYou can convert python classes into C libraries with `python mkcy.py build_ext --inplace`, and codes will be checked before compiling, which can serve as a simple to way to find typo and bugs as well. This function is supported by [Cython](https://cython.org/). These files can be removed by commands `tools/clean/cython.py .` and `rm -fr build/`. Loading modules from compiled C libraries may also accelerate, but not significantly.\n\n## Ranking\n\nYou can rank your corpus with pre-trained model, per token perplexity will be given for each sequence pair. Use it with:\n\n`python rank.py rsf h5f models`\n\nwhere `rsf` is the result file, `h5f` is HDF5 formatted input of file of your corpus (genrated like training set with `tools/mkiodata.py` like in `scripts/mktrain.sh`), `models` is a (list of) model file(s) to make perplexity evaluation.\n\n## The other files' discription\n\n### `modules/`\n\nFoundamental models needed for the construction of transformer.\n\n### `loss/`\n\nImplementation of label smoothing loss function required by the training of transformer.\n\n### `lrsch.py`\n\nLearning rate schedule model needed according to the paper.\n\n### `utils/`\n\nFunctions for basic features, for example, freeze / unfreeze parameters of models, padding list of tensors to same size on assigned dimension.\n\n### `translator.py`\n\nProvide an encapsulation for the whole translation procedure with which you can use the trained model in your application easier.\n\n### `server.py`\n\nAn example depends on Flask to provide simple Web service and REST API about how to use the `translator`, configure [those variables](server.py#L13-L23) before you use it.\n\n### `transformer/`\n\nImplementations of seq2seq models.\n\n### `parallel/`\n\nMulti-GPU parallelization implementation.\n\n### `datautils/`\n\nSupportive functions for data segmentation.\n\n### `tools/`\n\nScripts to support data processing (e.g. text to tensor), analyzing, model file handling, etc.\n\n## Performance\n\nSettings: WMT 2014, English -> German, 32k joint BPE with 8 as vocabulary threshold for BPE. 2 nVidia GTX 1080 Ti GPU(s) for training, 1 for decoding.\n\nTokenized case-sensitive BLEU measured with [multi-bleu.perl](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl), Training speed and decoding speed are measured by the number of target tokens (`<eos>` counted and `<pad>` discounted) per second and the number of sentences per second:\n\n| | BLEU | Training Speed | Decoding Speed |\n| :------| ------: | ------: | ------: |\n| Attention is all you need | 27.3 | | |\n| Neutron | 28.07 | 23213.65 | 150.15 |\n\n## Acknowledgments\n\nHongfei Xu enjoys a doctoral grant from [China Scholarship Council](https://www.csc.edu.cn/) ([2018]3101, 201807040056) while maintaining this project.\n\nDetails of this project can be found [here](https://arxiv.org/abs/1903.07402), and please cite it if you enjoy the implementation :)\n\n```\n@article{xu2019neutron,\n  author = {Xu, Hongfei and Liu, Qiuhui},\n  title = \"{Neutron: An Implementation of the Transformer Translation Model and its Variants}\",\n  journal = {arXiv preprint arXiv:1903.07402},\n  archivePrefix = \"arXiv\",\n  eprinttype = {arxiv},\n  eprint = {1903.07402},\n  primaryClass = \"cs.CL\",\n  keywords = {Computer Science - Computation and Language},\n  year = 2019,\n  month = \"March\",\n  url = {https://arxiv.org/abs/1903.07402},\n  pdf = {https://arxiv.org/pdf/1903.07402}\n}\n```\n",
            "readme_url": "https://github.com/hfxunlp/transformer",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        },
        {
            "title": "Neutron: An Implementation of the Transformer Translation Model and its Variants",
            "arxiv": "1903.07402",
            "year": 2019,
            "url": "http://arxiv.org/abs/1903.07402v2",
            "abstract": "The Transformer translation model is easier to parallelize and provides\nbetter performance compared to recurrent seq2seq models, which makes it popular\namong industry and research community. We implement the Neutron in this work,\nincluding the Transformer model and its several variants from most recent\nresearches. It is highly optimized, easy to modify and provides comparable\nperformance with interesting features while keeping readability.",
            "authors": [
                "Hongfei Xu",
                "Qiuhui Liu"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999292803603994,
        "task": "Machine Translation",
        "task_prob": 0.9851825387217124
    }
}