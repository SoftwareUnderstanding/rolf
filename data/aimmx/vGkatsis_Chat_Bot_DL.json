{
    "visibility": {
        "visibility": "public"
    },
    "name": "Introduction",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "vGkatsis",
                "owner_type": "User",
                "name": "Chat_Bot_DL",
                "url": "https://github.com/vGkatsis/Chat_Bot_DL",
                "stars": 1,
                "pushed_at": "2021-07-05 05:04:39+00:00",
                "created_at": "2021-05-11 16:55:51+00:00",
                "language": "Jupyter Notebook",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "Python_Chatbot.ipynb",
                "sha": "fbdfe1717dcd29f3b24cbfdeb733d5692b9f286f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vGkatsis/Chat_Bot_DL/blob/main/Python_Chatbot.ipynb"
                    }
                },
                "size": 101320
            },
            {
                "type": "code",
                "name": "images",
                "sha": "3ebc918241bfc454940f89525ce69cefd3a7339c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vGkatsis/Chat_Bot_DL/tree/main/images"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "presentation",
                "sha": "e27c46c21b0725d7089b9801c09864c747f797bd",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vGkatsis/Chat_Bot_DL/tree/main/presentation"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "fdc30a67b3f3b35999f08473c271b97d7db833e0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/vGkatsis/Chat_Bot_DL/blob/main/requirements.txt"
                    }
                },
                "size": 44
            }
        ]
    },
    "authors": [
        {
            "name": "Vasileios Gkatsis",
            "email": "vgkatsis@gmail.com",
            "github_id": "vGkatsis"
        },
        {
            "name": "Varsou Panagiota",
            "email": "varsoup@yahoo.com",
            "github_id": "VarsouPenny"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/vGkatsis/Chat_Bot_DL",
            "stars": 1,
            "issues": true,
            "readme": "## Introduction\n\nChat-bots are becoming more and more useful in various simple professional tasks as they get more and more able to capture the essence of communicating with people. Still the development of good chat-bots that will answer more complicated questions, in general subjects, is a growing domain of research. \n\nThe goal of this project is to create a chat-bot able to answer python related questions.  Our project started with the main idea being that a programming assistant would be a much needed help by many people working or studying computer science. Although it sounds simple it soon proved to be a difficult task. The main challenge is that the model has to extract a technical correlation between Questions and Answers in order to be able to communicate effectively. The model that we used in order to achieve our goal is a recurrent sequence-to-sequence model. The main steps that we followed are described bellow.\n\n- We found, downloaded, and processed data taken from stack overflow concerning questions that contained at least one python tag.[7]\n- Implement a sequence-to-sequence model.\n- Jointly train encoder and decoder models using mini-batches\n- Used greedy-search decoding\n- Interact with trained chatbot\n\n## Table of Contents\n\n1. [Execution](#execution)\n2. [Pre-Processing](#pre-processing)\n3. [Data Preparation](#datapreparation)\n4. [Models](#models)\n5. [Training](#training)\n6. [Evaluation](#evaluation)\n7. [Results](#results)\n8. [References](#references)\n\n## <a name=\"execution\"></a> 1. Execution\n\nFor this project we used the pytorch framework of python. The project code is  located in the Python-Chatbot.ipynb  jupyter notebook. It was executed in Google Colab environment, using a GPU. \n\nAnyone who wants to run the code can do it from the beginning or if a pre-trained model is available they can jump directly to the part where the model is loaded. Comments inside the notebook explain which parts can be skipped.\n\nSince data files are very large (approximately 800Mb each) we are not going to upload them on this repository. Instead we provide the download link in the References section and we suggest that they should be uploaded to a goole drive account. Then the google account can be easily connected with the colab platform in order for the files to be loaded. Code for this purpose already exists in the jupyter notebook. \n\n## <a name=\"pre-processing\"></a> 2. Pre-Processing\n\nData preprocessing is done in two phases.\n\n- Phase 1 Read row data and:\n  - Keep all questions with at least one answer.\n  - Pair each question with its most up voted answer.\n  - Remove all questions that need code in order to be answered.\n\nThat last step is needed in order to simplify the task, as feeding code blocks to the model would require special handling.\n\n- Phase 2:\n  - Remove punctuation and special characters.\n  - Remove HTML and Markdown tags.\n  - Filter sentences with length greater than a given value.\n  - Filter pairs containing  rare words (words with an appearance frequency lower than a given value).\n\n## <a name=\"datapreparation\"></a> 3. Data Preparation\n\nNow it is time to prepare our data to be fed in to the model. For this reason the following steps are followed:\n\n- Create torch tensors of data.\n- Create tensors of shape (max_length, batch_size) in order to help train using mini-batches instead of 1 sentence at a time. \n\n- Zero pad tensors to fit the maximum sentence length.\n- Create tensors of length for each sentence in the batch.\n- Create mask tensors with a value of  1 if token is not a PAD_Token else value is 0.\n\n \n\n## <a name=\"models\"></a> 4. Models\n\nWe use a sequence two sequence (seq2seq) model composed from 2 Recursive Neural Networks (RNNs) one acting as an encoder and the other acting as a decoder.\n\n- Encoder:\n\n  The Encoder iterates through the input sentence one word at a time, at each time step outputting an:\n\n  -  Output vector. \n  - Hidden state vector. \n  - We used a bidirectional variant of the multi-layered Gated Recurrent Unit [4], \n  - Two independent RNNs.\n  - One that is fed the input sequence in normal sequential order. \n  - And one that is fed the input sequence in reverse order. \n  - The outputs of each network are summed at each time step. Using a bidirectional GRU.\n\n- Decoder:\n\n  The decoder RNN generates the response sentence in a token-by-token fashion using:\n\n  - Context vectors\n  - Internal hidden states\n\n    from the encoder to generate the next word in the sequence. \n\n  In order to minimize information loss during encoding process we will use the **Global attention** mechanism by [5] which improved upon Bahdanau et al.\u2019s [6] attention mechanism.\n\n\n\nSo the flow of our seq2seq model is:\n\n1. Get embedding of current input word. \n2. Forward through unidirectional GRU. \n3. Calculate attention weights from the current GRU output.\n4. Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector. \n5. Concatenate weighted context vector and GRU output using Luong. \n6. Predict next word. \n7. Return output and final hidden state.\n\n## <a name=\"training\"></a> 5. Training\n\nThe training procedure consists of the following steps:\n\n1. Forward pass entire input batch through encoder. \n\n2. Initialize decoder inputs as SOS_token, and hidden state as the encoder's final hidden state. \n\n3. Forward input batch sequence through decoder one time step at a time. \n\n4. If teacher forcing: set next decoder input as the current target else set next decoder input as current decoder output. \n\n5. Calculate and accumulate loss. \n\n6. Perform backpropagation. \n\n7. Clip gradients. \n\n8. Update encoder and decoder model parameters.\n\n   \n\nDuring the training process we use a these tricks to aid in convergence:\n\n- **Teacher forcing:** At some probability, set by **teacher_forcing_ratio**, we use the current target word as the decoder\u2019s next input rather than using the decoder\u2019s current guess. \n- **Gradient clipping**. Commonly technique for countering the \u201cexploding gradient\u201d problem. In essence, by clipping or thresholding gradients to a maximum value, we prevent the gradients from growing exponentially and either overflow (NaN), or overshoot steep cliffs in the cost function.\n\n## <a name=\"evaluation\"></a> 6. Evaluation\n\nEvaluation Decoding Flow:\n\n**Decoding Method**\n\n1. Forward input through encoder model. \n2. Prepare encoder's final hidden layer to be first hidden input to the decoder.\n3. Initialize decoder's first input as SOS_token. \n4. Initialize tensors to append decoded words to. \n5. Iteratively decode one word token at a time: \n   1. Forward pass through decoder. \n   2. Obtain most likely word token and its softmax score. \n   3. Record token and score. \n   4. Prepare current token to be next decoder input. \n\n6. Return collections of word tokens and scores.\n\n\n\nGreedy decoding\n\n- Greedy decoding is the decoding method that we use during training when we are **NOT** using teacher forcing. \n- For each time step we choose the word from decoder_output with the highest softmax value. \n- This decoding method is optimal on a single time-step level.\n\n\n\nEvaluation Process\n\n- Format the sentence to be evaluated as an input batch of word indexes with *batch_size==1*. \n- Create a **lengths** tensor which contains the length of the input sentence.\n- Obtain the decoded response sentence tensor using **GreedySearchDecoder**. \n- Convert the response\u2019s indexes to words and return the list of decoded words.\n- When chatting with the bot this evaluation process is followed in order for it to respond.\n\n\n\n## <a name=\"results\"></a> 7. Results\n\nExperiment results confirm the this is a complicated task and that further work may still to be done. Bellow are some good and some bad examples from different training and executions of the program:\n\n- Good results\n\n  <img src=\"./images/good_res1.png\" alt=\"alt text\" width=\"400\" height=\"100\" />\n\n  <img src=\"./images/good_res2.png\" alt=\"alt text\" width=\"400\" height=\"100\" />\n\n- Bad results\n\n  <img src=\"./images/bad_res1.png\" alt=\"alt text\" width=\"300\" height=\"100\" />\n\n  <img src=\"./images/bad_res3.png\" alt=\"alt text\" width=\"400\" height=\"100\" />\n\n## <a name=\"references\"></a> 8. References\n\n1. ChatbotTutorial by Matthew Inkawhich  \n   https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\u200b\n2. Pytorch Chatbot by Wu,Yuan-Kuei \n   https://github.com/ywk991112/pytorch-chatbot\u200b\n3. Sutskever et al. \n   https://arxiv.org/abs/1409.3215\u200b\n4. Cho et al. \n   https://arxiv.org/pdf/1406.1078v3.pdf\u200b\n5. Luong et al. \n   https://arxiv.org/abs/1508.04025\u200b\n6. Bahdanau et al \n   https://arxiv.org/abs/1409.0473\u200b\n7. Python Questions from Stack Overflow \n   https://www.kaggle.com/stackoverflow/pythonquestions\u200b\n\n",
            "readme_url": "https://github.com/vGkatsis/Chat_Bot_DL",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Sequence to Sequence Learning with Neural Networks",
            "arxiv": "1409.3215",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.3215v3",
            "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.",
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
            "arxiv": "1409.0473",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.0473v7",
            "abstract": "Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.",
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ]
        },
        {
            "title": "Effective Approaches to Attention-based Neural Machine Translation",
            "arxiv": "1508.04025",
            "year": 2015,
            "url": "http://arxiv.org/abs/1508.04025v5",
            "abstract": "An attentional mechanism has lately been used to improve neural machine\ntranslation (NMT) by selectively focusing on parts of the source sentence\nduring translation. However, there has been little work exploring useful\narchitectures for attention-based NMT. This paper examines two simple and\neffective classes of attentional mechanism: a global approach which always\nattends to all source words and a local one that only looks at a subset of\nsource words at a time. We demonstrate the effectiveness of both approaches\nover the WMT translation tasks between English and German in both directions.\nWith local attention, we achieve a significant gain of 5.0 BLEU points over\nnon-attentional systems which already incorporate known techniques such as\ndropout. Our ensemble model using different attention architectures has\nestablished a new state-of-the-art result in the WMT'15 English to German\ntranslation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over\nthe existing best system backed by NMT and an n-gram reranker.",
            "authors": [
                "Minh-Thang Luong",
                "Hieu Pham",
                "Christopher D. Manning"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999999803177798,
        "task": "Machine Translation",
        "task_prob": 0.9882997516829095
    }
}