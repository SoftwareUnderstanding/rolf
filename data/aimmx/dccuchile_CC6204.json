{
    "visibility": {
        "visibility": "public"
    },
    "name": "CC6204 Deep Learning",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "dccuchile",
                "owner_type": "Organization",
                "name": "CC6204",
                "url": "https://github.com/dccuchile/CC6204",
                "stars": 136,
                "pushed_at": "2021-03-06 19:51:56+00:00",
                "created_at": "2018-03-09 17:04:13+00:00",
                "language": "Jupyter Notebook",
                "description": "Material del curso de Deep Learning de la Universidad de Chile",
                "frameworks": [
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "bc50888cd9ecafa6983a7180ad5ed2b35c241b75",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dccuchile/CC6204/blob/master/.gitignore"
                    }
                },
                "size": 209
            },
            {
                "type": "code",
                "name": "2020",
                "sha": "b39eaecb26effef68eef3567cfb4b1a9b3d981f9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dccuchile/CC6204/tree/master/2020"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "autocorrect-server",
                "sha": "e7327ef1a6948cc6ad62b7d71c2036467ea84315",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dccuchile/CC6204/tree/master/autocorrect-server"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "autocorrect",
                "sha": "08dc178edd88bfb2f08d04253917b7540353ac9a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dccuchile/CC6204/tree/master/autocorrect"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "versiones_anteriores",
                "sha": "4cc5e1dab65d0da5a7605002c332cb169f383bb7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/dccuchile/CC6204/tree/master/versiones_anteriores"
                    }
                },
                "num_files": 2
            }
        ]
    },
    "authors": [
        {
            "name": "Jesus Perez-Martin",
            "email": "jperezmartin90@gmail.com",
            "github_id": "jssprz"
        },
        {
            "name": "Juan-Pablo Silva",
            "email": "jpsilva@dcc.uchile.cl",
            "github_id": "juanpablos"
        },
        {
            "name": "Jorge",
            "email": "jorge.perez.rojas@gmail.com",
            "github_id": "jorgeperezrojas"
        },
        {
            "name": "Mauricio Romero",
            "github_id": "fluowhy"
        },
        {
            "name": "Roberto A. Ibanez",
            "github_id": "ribanez"
        },
        {
            "name": "Diego Francisco Valenzuela Iturra",
            "email": "diegovalenzuela@protonmail.com",
            "github_id": "diegovalenzuelaiturra"
        },
        {
            "name": "Gabriel Chaperon",
            "email": "gabrielchaperonb@gmail.com",
            "github_id": "gchaperon"
        }
    ],
    "tags": [
        "uchile",
        "pytorch",
        "deep-learning"
    ],
    "description": "Material del curso de Deep Learning de la Universidad de Chile",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/dccuchile/CC6204",
            "stars": 136,
            "issues": true,
            "readme": "# CC6204 Deep Learning\n\nCurso introductorio (en espa\u00f1ol) al \u00e1rea de aprendizaje basado en redes neuronales profundas, com\u00fanmente conocida como Deep Learning. Durante el curso aprender\u00e1n la teor\u00eda detr\u00e1s de los modelos de Deep Learning, su funcionamiento y usos posibles. Ser\u00e1n capaces de construir y entrenar modelos para resolver problemas reales.\n\n* Profesor: [Jorge P\u00e9rez](https://users.dcc.uchile.cl/~jperez/)\n* Auxiliares: [Gabriel Chaperon](https://github.com/gchaperon), [Ho Jin Kang](https://github.com/hojink1996), [Juan-Pablo Silva](https://github.com/juanpablos), [Mauricio Romero](https://github.com/fluowhy), [Jes\u00fas P\u00e9rez-Mart\u00edn](https://jssprz.github.io/)\n* Versiones anteriores del curso: [Oto\u00f1o 2018](versiones_anteriores/2018), [Primavera 2019](versiones_anteriores/2019)\n\n### Primavera 2020\n\n* [Calendario del curso](2020/calendar.md)\n* [YouTube playlist](https://www.youtube.com/playlist?list=PLBjZ-ginWc1e0_Dp4heHglsjJmacV_F20)\n* [Tareas](2020/tareas/README.md)\n\n## Requerimientos\n\n* Una cuenta de Google para usar [Google Collaboratory](https://colab.research.google.com/), o\n* Tener instalados los siguientes paquetes de Python:\n    1. [PyTorch](https://pytorch.org/)\n    2. [Numpy](https://numpy.org/)\n    3. [Jupyter Notebook](https://jupyter.org/install)\n\n## Organizaci\u00f3n del Curso\n\n### 1. Fundamentos\n\nIntroducci\u00f3n, IA vs ML vs DL, \u00bfPor qu\u00e9 DL ahora? ([video](https://www.youtube.com/watch?v=BASByOlqqkc&list=PLBjZ-ginWc1e0_Dp4heHglsjJmacV_F20&index=1))\n\n#### 1.1. Redes neuronales modernas\n\n* Perceptr\u00f3n, funciones de activaci\u00f3n, y representaci\u00f3n matricial ([video](https://www.youtube.com/watch?v=mDCxK2Pu0mA&list=PLBjZ-ginWc1e0_Dp4heHglsjJmacV_F20&index=2))\n* UAT, Redes Feed-Forward, y funci\u00f3n de salida (softmax) ([video](https://www.youtube.com/watch?v=eV-N1ozcZrk&list=PLBjZ-ginWc1e0_Dp4heHglsjJmacV_F20&index=3))\n* Descenso de Gradiente para encontrar los par\u00e1metros de una red ([video](https://www.youtube.com/watch?v=G4dnRSSC6Kw))\n* Grafos de computaci\u00f3n y el algoritmo de BackPropagation ([video1](https://www.youtube.com/watch?v=1EUAoM1EhM0), [video2](https://www.youtube.com/watch?v=Gp2rY7LvTyQ))\n* Tensores, Notaci\u00f3n de Einstein, y Regla de la Cadena Tensorial ([video](https://www.youtube.com/watch?v=pLUNS_tK-K8))\n* Entrop\u00eda Cruzada y Backpropagation a mano con Tensores ([video](https://www.youtube.com/watch?v=e_1lis8ByyI))\n* Aspectos pr\u00e1cticos de entrenamiento y Red FF a mano en pytorch ([video](https://www.youtube.com/watch?v=y6aD4WG-rOw))\n\nReadings: [Chapter 2. Lineal Algebra](http://www.deeplearningbook.org/contents/linear_algebra.html), [Chapter 3. Probability and Information Theory](http://www.deeplearningbook.org/contents/prob.html), [Chapter 6. Deep Feedforward Networks](http://www.deeplearningbook.org/contents/mlp.html)\n\n#### 1.2. Inicializaci\u00f3n, Regularizaci\u00f3n y Optimizaci\u00f3n\n\n* Generalizaci\u00f3n, Test-Dev-Train set y Regularizaci\u00f3n ([video](https://www.youtube.com/watch?v=5gAJeY-HHtg))\n* Ensemble, Dropout, y Desvanecimiento de Gradiente ([video](https://www.youtube.com/watch?v=4cJlTns7noE))\n* Inicializaci\u00f3n de par\u00e1metros y Normalizaci\u00f3n ([video](https://www.youtube.com/watch?v=izOwC2my1Kw))\n* Algoritmos de Optimizaci\u00f3n, SGD con Momentum, RMSProp, Adam ([video](https://www.youtube.com/watch?v=FBsiDndtdVg))\n\nReadings: [Chapter 7. Regularization for Deep Learning](http://www.deeplearningbook.org/contents/regularization.html), [Chapter 8. Optimization for Training DeepModels](http://www.deeplearningbook.org/contents/optimization.html), [Chapter 11. Practical Methodology](http://www.deeplearningbook.org/contents/guidelines.html)\n\n### 2. Redes Neuronales Convolucionales (CNN)\n\n* Introducci\u00f3n a Redes Convolucionales ([video](https://www.youtube.com/watch?v=vSHSmiKiiDw))\n* Arquitecturas m\u00e1s conocidas: AlexNet, VGG, GoogLeNet, ResNet, DenseNet ([video1](https://www.youtube.com/watch?v=ju7nKaFaFvc), [video2](https://www.youtube.com/watch?v=AxWG1aLWODE), [video3](https://www.youtube.com/watch?v=C7S7wBsg2KE))\n\nReadings: [Chapter 9. Convolutional Networks](http://www.deeplearningbook.org/contents/convnets.html), [Chapter 12. Applications](http://www.deeplearningbook.org/contents/applications.html)\n\n### 3. Redes Neuronales Recurrentes (RNN)\n\n* Introducci\u00f3n a Redes Recurrentes ([video](https://www.youtube.com/watch?v=yHzflmQ9EoY))\n* Arquitectura de Redes Recurrentes ([video](https://www.youtube.com/watch?v=Bcy_no-u_BM))\n* Auto-regresi\u00f3n, Language Modelling, y Arquitecturas Seq-to-Seq ([video](https://www.youtube.com/watch?v=bsKwb7wjYYc))\n* RNNs con Compuertas y Celdas de Memoria: GRU y LSTM ([video](https://www.youtube.com/watch?v=cDT9oYyXgjo))\n\nReadings: [Chapter 10. Sequence Modeling: Recurrentand Recursive Nets](http://www.deeplearningbook.org/contents/rnn.html), [Chapter 12. Applications](http://www.deeplearningbook.org/contents/applications.html)\n\n### 4. T\u00f3picos avanzados\n\n* Atenci\u00f3n Neuronal ([video](https://www.youtube.com/watch?v=B9hMAvoWE7w))\n* Transformers ([video](https://www.youtube.com/watch?v=QTX6VgOWwE4))\n* Variational Autoencoders\n* Generative Adversarial Networks\n* Neural Turing Machine (NeuralTM)\n* Differentiable Neural Computers (DNC)\n\nReadings: [Chapter 14. Autoencoders](http://www.deeplearningbook.org/contents/autoencoders.html), [Chapter 20. Deep Generative Models](http://www.deeplearningbook.org/contents/generative_models.html)\n\n## Libros\n\nNo hay ning\u00fan libro de texto obligatorio para el curso. Algunas conferencias incluir\u00e1n lecturas sugeridas de \"Deep Learning\" de Ian Goodfellow, Yoshua Bengio, and Aaron Courville; sin embargo, no es necesario comprar una copia, ya que est\u00e1 disponible de forma [gratuita en l\u00ednea](http://www.deeplearningbook.org/).\n\n1. [Deep Learning](http://www.deeplearningbook.org/) by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (bibliograf\u00eda fundamental del curso)\n2. [Dive into Deep Learning](https://d2l.ai/) by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola\n3. [Deep Learning for Vision Systems](https://livebook.manning.com/book/grokking-deep-learning-for-computer-vision/deep-learning-for-vision-systems/7) by Mohamed Elgendy\n4. [Probabilistic and Statistical Models for Outlier Detection](https://www.springer.com/cda/content/document/cda_downloaddocument/9783319475776-c1.pdf?SGWID=0-0-45-1597574-p180317591) by Charu Aggarwal\n5. [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf) by Daniel Jurafsky and James Martin\n6. [Notes on Deep Learning for NLP](https://arxiv.org/abs/1808.09772) by Antoine J.-P. Tixier\n7. [AutoML: Methods, Systems, Challenges](https://www.automl.org/book/) edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren\n\n## Tutoriales\n\n1. [Quickstart tutorial numpy](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html)\n2. [DeepLearning con PyTorch en 60 minutos](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n\n## Otros Cursos de DL\n\n1. [Introduction to Deep Learning](http://introtodeeplearning.com/)\n2. [Deep learning course on Coursera](https://www.coursera.org/specializations/deep-learning) by Andrew Ng\n3. [CS231n course](http://cs231n.stanford.edu/) by Stanford University\n4. [Courses](http://www.fast.ai/) by fast.ai\n\n## Videos\n\n1. [Visualizing and Understanding Recurrent Networks](https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks)\n2. [More on Transformers: BERT and Friends](https://tv.vera.com.uy/video/55388) by Jorge P\u00e9rez\n3. [Atenci\u00f3n neuronal y el transformer](https://www.youtube.com/watch?v=4cY1H-QVlZM) by Jorge P\u00e9rez\n\n## Otras Fuentes\n\n1. [How To Improve Deep Learning Performance](https://machinelearningmastery.com/improve-deep-learning-performance/)\n2. [An Overview of ResNet and its Variants](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)\n3. [CNN Architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet and more](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5)\n4. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n5. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n6. [Attention is all you need explained](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)\n7. [BERT exaplained](http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)\n",
            "readme_url": "https://github.com/dccuchile/CC6204",
            "frameworks": [
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Notes on Deep Learning for NLP",
            "arxiv": "1808.09772",
            "year": 2018,
            "url": "http://arxiv.org/abs/1808.09772v2",
            "abstract": "My notes on Deep Learning for NLP.",
            "authors": [
                "Antoine J. -P. Tixier"
            ]
        },
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        },
        {
            "title": "YouTube playlist",
            "url": "https://www.youtube.com/playlist?list=PLBjZ-ginWc1e0_Dp4heHglsjJmacV_F20"
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.7577611075406478,
        "task": "Machine Translation",
        "task_prob": 0.9529101104651941
    }
}