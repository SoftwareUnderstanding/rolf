{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "TCN Keras Examples",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "ashishpatel26",
                "owner_type": "User",
                "name": "tcn-keras-Examples",
                "url": "https://github.com/ashishpatel26/tcn-keras-Examples",
                "stars": 27,
                "pushed_at": "2020-09-15 13:32:59+00:00",
                "created_at": "2020-09-14 12:56:00+00:00",
                "language": "Jupyter Notebook",
                "description": "This repository contains example of keras-tcn on easy way.",
                "license": "MIT License",
                "frameworks": [
                    "Keras",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "4be41aa225c06c8ab68b349369407e14324a7568",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/LICENSE"
                    }
                },
                "size": 1069
            },
            {
                "type": "code",
                "name": "TCN_IMDB.ipynb",
                "sha": "26d1ce76bad34442674739e3135967344136fa84",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_IMDB.ipynb"
                    }
                },
                "size": 83954
            },
            {
                "type": "code",
                "name": "TCN_MNIST.ipynb",
                "sha": "5ca24f232db490bed5568ac72a841a0bac834353",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_MNIST.ipynb"
                    }
                },
                "size": 10568
            },
            {
                "type": "code",
                "name": "TCN_Many_to_Many_Regression.ipynb",
                "sha": "d2fcd9cfd4967f1e4406c07b11564db11258797b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Many_to_Many_Regression.ipynb"
                    }
                },
                "size": 16924
            },
            {
                "type": "code",
                "name": "TCN_Self_generated_Data_Training.ipynb",
                "sha": "21a7ab3f4d3158acdbac8a9004ef18dba7c1549d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Self_generated_Data_Training.ipynb"
                    }
                },
                "size": 63531
            },
            {
                "type": "code",
                "name": "TCN_TimeSeries_Approach.ipynb",
                "sha": "d082e24118a63a39311f4af6bab59b8d109ea1f5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_TimeSeries_Approach.ipynb"
                    }
                },
                "size": 161159
            },
            {
                "type": "code",
                "name": "TCN_cifar10.ipynb",
                "sha": "6459a0d4620899957b8bfa198a0457eb3bee4597",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_cifar10.ipynb"
                    }
                },
                "size": 147919
            }
        ]
    },
    "authors": [
        {
            "name": "Ashish Patel",
            "email": "shriganesh.patel@gmail.com",
            "github_id": "ashishpatel26"
        }
    ],
    "tags": [
        "tcn",
        "keras-tcn",
        "tensorflow-tcn",
        "temporal-convolution-network",
        "keras",
        "keras-tensorflow",
        "neuralnetwork",
        "time-series",
        "sequence-to-sequence"
    ],
    "description": "This repository contains example of keras-tcn on easy way.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/ashishpatel26/tcn-keras-Examples",
            "stars": 27,
            "issues": true,
            "readme": "# TCN Keras Examples\n### TCN Example Notebooks by Topic\n\n***Note : This All Notebook Contains Step by Step code of TCN in different domain Application.***\n\n#### Installation \n\n```python\npip install keras-tcn\n```\n\n---\n\n| Topic                            | Github                                                       | Colab                                                        |\n| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| MNIST Dataset                    | [MNIST Dataset](https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_MNIST.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_MNIST.ipynb) |\n| IMDB Dataset                     | [IMDA Dataset](https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_IMDB.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_IMDB.ipynb) |\n| Time Series Dataset Milk         | [Time Series Dataset Milk](https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_TimeSeries_Approach.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_TimeSeries_Approach.ipynb) |\n| Many to Many Regression Approach | [MtoM Regression](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Many_to_Many_Regression.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Many_to_Many_Regression.ipynb) |\n| Self Generated Dataset Approach  | [Self Generated Dataset](https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Self_generated_Data_Training.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Self_generated_Data_Training.ipynb) |\n| Cifar10 Image Classification | [Cifar10 Image Classification](https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_cifar10.ipynb) | [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashishpatel26/tcn-keras-Examples/blob/master/TCN_cifar10.ipynb) |\n\n\nArticle : https://arxiv.org/pdf/1803.01271.pdf\n\ngithub : https://github.com/philipperemy/keras-tcn\n\n## Why Temporal Convolutional Network?\n\n- TCNs exhibit longer memory than recurrent architectures with the same capacity.\n- Constantly performs better than LSTM/GRU architectures on a vast range of tasks (Seq. MNIST, Adding Problem, Copy Memory, Word-level PTB...).\n- Parallelism, flexible receptive field size, stable gradients, low memory requirements for training, variable length inputs...\n\n[![img](https://github.com/philipperemy/keras-tcn/raw/master/misc/Dilated_Conv.png)](https://github.com/philipperemy/keras-tcn/blob/master/misc/Dilated_Conv.png) **Visualization of a stack of dilated causal convolutional layers (Wavenet, 2016)**\n\n### Arguments\n\n```\nTCN(nb_filters=64, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16, 32], padding='causal', use_skip_connections=False, dropout_rate=0.0, return_sequences=True, activation='relu', kernel_initializer='he_normal', use_batch_norm=False, **kwargs)\n```\n\n- `nb_filters`: Integer. The number of filters to use in the convolutional layers. Would be similar to `units` for LSTM.\n- `kernel_size`: Integer. The size of the kernel to use in each convolutional layer.\n- `dilations`: List. A dilation list. Example is: [1, 2, 4, 8, 16, 32, 64].\n- `nb_stacks`: Integer. The number of stacks of residual blocks to use.\n- `padding`: String. The padding to use in the convolutions. 'causal' for a causal network (as in the original implementation) and 'same' for a non-causal network.\n- `use_skip_connections`: Boolean. If we want to add skip connections from input to each residual block.\n- `return_sequences`: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n- `dropout_rate`: Float between 0 and 1. Fraction of the input units to drop.\n- `activation`: The activation used in the residual blocks o = activation(x + F(x)).\n- `kernel_initializer`: Initializer for the kernel weights matrix (Conv1D).\n- `use_batch_norm`: Whether to use batch normalization in the residual layers or not.\n- `kwargs`: Any other arguments for configuring parent class Layer. For example \"name=str\", Name of the model. Use unique names when using multiple TCN.\n\n### Input shape\n\n3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n`timesteps` can be None. This can be useful if each sequence is of a different length: [Multiple Length Sequence Example](https://github.com/philipperemy/keras-tcn/blob/master/tasks/multi_length_sequences.py).\n\n### Output shape\n\n- if `return_sequences=True`: 3D tensor with shape `(batch_size, timesteps, nb_filters)`.\n- if `return_sequences=False`: 2D tensor with shape `(batch_size, nb_filters)`.\n\n### Supported task types\n\n- Regression (Many to one) e.g. adding problem\n- Classification (Many to many) e.g. copy memory task\n- Classification (Many to one) e.g. sequential mnist task\n\nFor a Many to Many regression, a cheap fix for now is to change the [number of units of the final Dense layer](https://github.com/philipperemy/keras-tcn/blob/8151b4a87f906fd856fd1c113c48392d542d0994/tcn/tcn.py#L90).\n\n### Receptive field\n\n- Receptive field = **nb_stacks_of_residuals_blocks \\* kernel_size \\* last_dilation**.\n- If a TCN has only one stack of residual blocks with a kernel size of 2 and dilations [1, 2, 4, 8], its receptive field is 2 * 1 * 8 = 16. The image below illustrates it:\n\n[![img](https://user-images.githubusercontent.com/40159126/41830054-10e56fda-7871-11e8-8591-4fa46680c17f.png)](https://user-images.githubusercontent.com/40159126/41830054-10e56fda-7871-11e8-8591-4fa46680c17f.png) **ks = 2, dilations = [1, 2, 4, 8], 1 block**\n\n\n\n- If the TCN has now 2 stacks of residual blocks, wou would get the situation below, that is, an increase in the receptive field to 32:\n\n[![img](https://user-images.githubusercontent.com/40159126/41830618-a8f82a8a-7874-11e8-9d4f-2ebb70a31465.jpg)](https://user-images.githubusercontent.com/40159126/41830618-a8f82a8a-7874-11e8-9d4f-2ebb70a31465.jpg) **ks = 2, dilations = [1, 2, 4, 8], 2 blocks**\n\n\n\n- If we increased the number of stacks to 3, the size of the receptive field would increase again, such as below:\n\n[![img](https://user-images.githubusercontent.com/40159126/41830628-ae6e73d4-7874-11e8-8ecd-cea37efa33f1.jpg)](https://user-images.githubusercontent.com/40159126/41830628-ae6e73d4-7874-11e8-8ecd-cea37efa33f1.jpg) **ks = 2, dilations = [1, 2, 4, 8], 3 blocks**\n\n\n\nThanks to [@alextheseal](https://github.com/alextheseal) for providing such visuals.\n\n### Non-causal TCN\n\nMaking the TCN architecture non-causal allows it to take the future into consideration to do its prediction as shown in the figure below.\n\nHowever, it is not anymore suitable for real-time applications.\n\n[![img](https://github.com/philipperemy/keras-tcn/raw/master/misc/Non_Causal.png)](https://github.com/philipperemy/keras-tcn/blob/master/misc/Non_Causal.png) **Non-Causal TCN - ks = 3, dilations = [1, 2, 4, 8], 1 block**\n\n\n\nTo use a non-causal TCN, specify `padding='valid'` or `padding='same'` when initializing the TCN layers.\n\n---\n\n#### References:\n\n- https://github.com/philipperemy/keras-tcn ( TCN Keras Version)\n- https://github.com/locuslab/TCN/ (TCN for Pytorch)\n- https://arxiv.org/pdf/1803.01271.pdf (An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling)\n- https://arxiv.org/pdf/1609.03499.pdf (Original Wavenet paper)\n- ***Note : All the rights reserved by original Author. This Repository creation intense for Educational purpose only.***\n\n",
            "readme_url": "https://github.com/ashishpatel26/tcn-keras-Examples",
            "frameworks": [
                "Keras",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "WaveNet: A Generative Model for Raw Audio",
            "arxiv": "1609.03499",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.03499v2",
            "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition.",
            "authors": [
                "Aaron van den Oord",
                "Sander Dieleman",
                "Heiga Zen",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Alex Graves",
                "Nal Kalchbrenner",
                "Andrew Senior",
                "Koray Kavukcuoglu"
            ]
        },
        {
            "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
            "arxiv": "1803.01271",
            "year": 2018,
            "url": "http://arxiv.org/abs/1803.01271v2",
            "abstract": "For most deep learning practitioners, sequence modeling is synonymous with\nrecurrent networks. Yet recent results indicate that convolutional\narchitectures can outperform recurrent networks on tasks such as audio\nsynthesis and machine translation. Given a new sequence modeling task or\ndataset, which architecture should one use? We conduct a systematic evaluation\nof generic convolutional and recurrent architectures for sequence modeling. The\nmodels are evaluated across a broad range of standard tasks that are commonly\nused to benchmark recurrent networks. Our results indicate that a simple\nconvolutional architecture outperforms canonical recurrent networks such as\nLSTMs across a diverse range of tasks and datasets, while demonstrating longer\neffective memory. We conclude that the common association between sequence\nmodeling and recurrent networks should be reconsidered, and convolutional\nnetworks should be regarded as a natural starting point for sequence modeling\ntasks. To assist related work, we have made code available at\nhttp://github.com/locuslab/TCN .",
            "authors": [
                "Shaojie Bai",
                "J. Zico Kolter",
                "Vladlen Koltun"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "MNIST Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_MNIST.ipynb"
                    }
                }
            },
            {
                "name": "IMDA Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_IMDB.ipynb"
                    }
                }
            },
            {
                "name": "Time Series Dataset Milk",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_TimeSeries_Approach.ipynb"
                    }
                }
            },
            {
                "name": "Self Generated Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/ashishpatel26/tcn-keras-Examples/blob/master/TCN_Self_generated_Data_Training.ipynb"
                    }
                }
            },
            {
                "name": "Sequential MNIST"
            },
            {
                "name": "IMDb"
            }
        ]
    },
    "domain": {
        "domain_type": "Speech",
        "domain_prob": 0.9640527398823479
    }
}