{
    "visibility": {
        "visibility": "public"
    },
    "name": "Solution of Team DeepZ for 2018 DACSDC",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "jndeng",
                "owner_type": "User",
                "name": "DACSDC-DeepZ",
                "url": "https://github.com/jndeng/DACSDC-DeepZ",
                "stars": 24,
                "pushed_at": "2018-09-04 12:43:50+00:00",
                "created_at": "2018-06-23 13:42:12+00:00",
                "language": "C",
                "description": "Proposed solution of team DeepZ for 2018 DAC System Design Contest",
                "frameworks": []
            },
            {
                "type": "code",
                "name": "Inference",
                "sha": "5f11b33fea00de2af70bac1a58e89122436cee75",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jndeng/DACSDC-DeepZ/tree/master/Inference"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "Train",
                "sha": "840f352ab0adb118f74263f08c273588eb51fe1a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jndeng/DACSDC-DeepZ/tree/master/Train"
                    }
                },
                "num_files": 7
            }
        ]
    },
    "authors": [
        {
            "name": "jndeng",
            "github_id": "jndeng"
        }
    ],
    "tags": [],
    "description": "Proposed solution of team DeepZ for 2018 DAC System Design Contest",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/jndeng/DACSDC-DeepZ",
            "stars": 24,
            "issues": true,
            "readme": "# Solution of Team DeepZ for 2018 DACSDC\n\nThis repository contains the proposed solution of team DeepZ(GPU Platform) for [2018 System Design Contest](https://dac.com/content/2018-system-design-contest).\n\n**UPD: Official dataset is available [in this repo](https://github.com/xyzxinyizhang/2018-DAC-System-Design-Contest).\nAnd we just learn that the dataset has been updated and reduced from 98 classes to 95 classes. Unfortunately, we did not notice the update during the contest, which means all of our experiments were based on the former 98 classes dataset. This should not have a big impact on our model, but the division of train/valid set will be different with the new dataset, breaking some of the scripts. For now, we do not have time to review and update those scripts, so feel free to ask here if you encounter any problems.**\n\n## Introduction\nDue to the speed limitation of 20 FPS, we started with [YOLOv2-Tiny detector](https://pjreddie.com/darknet/yolov2/), which consists of a backbone network for feature extraction and a detection network for candidate bounding box generation. Considering that there is no need to classify in our task, we reduced the detection network to a location network, in which a candidate bounding box is only represented by a confidence socre and a position.\n\nHowever, with such a simple model, we were soon faced with the challenges of tiny objects, occlusions and distractions from the provided data set. In order to tackle to the aforementioned challenges, we investigated various network architectures for both training and inference. \n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/jndeng/DACSDC-DeepZ/master/Train/cfg/architecture.png\" alt=\"network architecture\" width=\"380px\" height=\"400px\">\n</p>\n\nWe later combined [Feature Pyramid Network](https://arxiv.org/abs/1612.03144v2) to fuse fine-grained features with strong semantic features to enhance the ability in detecting small objects. Meanwhile, we utilized [Focal Loss](https://arxiv.org/abs/1708.02002) function to mitigate the imbalance between the single ground truth box and the candidate boxes at training phase, thereby partially resolving occlusions and distractions. With the combined techniques, we achieved the inference network as shown in the figure with an accuracy improvement of ~ 0.042. \n\nMoreover, we used multithreading to accelerate the process of prediction by loading images and infering in parallel, which improved about 7 FPS on NVIDIA Jetson TX2.\n\n\nThe performance of our model is as follow:\n\n| Self-Test Accuracy (mean IoU) | Organizer-Test Accuracy (mean IoU) | Speed (FPS on Jetson TX2)\n|:-----:|:-----:|:-----:|\n| 0.866 | 0.691 | ~25 |\n\n**Note:**  \n\nWe develop two projects for different purposes in this repository. Project `Train` is mainly used for model training and accuracy evaluation on powerful GPU(NVIDIA Titan X Pascal in our experiments). While project `Inference` is dedicated to inference on embedded GPU(NVIDIA Jetson TX2) with better optimization in speed and energy consumption.\n\n\n## Usage\n\n### Installation\n\n*Prerequisites:*\n * OpenCV\n * CUDA/cuDNN\n * Python2/Python2-Numpy\n\n*Project download and installation:*\n1. Download the source code on the appropriate devices respectively. Project `Train` is recommended using on device with powerful GPU. While project `Inference` should be used on NVIDIA Jetson TX2 in order to make a fair evaluation of speed.\n```Shell\n# You may use this command twice to download the source code on different devices\ngit clone https://github.com/jndeng/DACSDC-DeepZ.git\n```\n2. Build the source code of two projects separately on the corresponding device. We will use `$TRAIN_ROOT` and `$INFERENCE_ROOT` to call the directory of project `Train` and project `Inference` respectively.\n```Shell\n# For project 'Train'\ncd $TRAIN_ROOT\nmake -j8\n```\n```Shell\n# For project 'Inference'\ncd $INFERENCE_ROOT\nmake -j8\n```\n\n**Note:**\n1. Our implementation is based on [Darknet framework](https://pjreddie.com/darknet/). You can also refer to the [installation guide](https://pjreddie.com/darknet/install/) of the original Darknet framework.\n2. For convenience, we only implement the code for **single GPU mode**, which means **CPU mode** and **multiple GPUs mode** are not supported in both of our projects.\n\n\n### Data Preparation\n~~1. Download the raw dataset [dac_origin.tar (6.3GB)]() (about 100,000 images and the corresponding labels) and unzip it to `$TRAIN_ROOT/data/raw_dataset`.~~\n1. Download the official dataset, unzip it, rename and move the folder contains all subclass folders to `$TRAIN_ROOT/data/raw_dataset`.\n2. Use the raw dataset `$TRAIN_ROOT/data/raw_dataset` to generate the proper dataset `$TRAIN_ROOT/data/train_dataset` for training. The entire process of dataset generation takes about 14GB of hard disk space, and the raw dataset will no longer be needed once we obtain `$TRAIN_ROOT/data/train_dataset`.\n```Shell\ncd $TRAIN_ROOT/data/script\npython generate_dataset.py\n```\n3. Randomly divide the entire dataset into two disjoint parts: training set and validation set according to 8:2 ratio. The result of division will be stored in `$TRAIN_ROOT/data/dataset` as the meta files. You can make a new division by yourself, or just apply the pre-divided dataset used in our experiments.\n```Shell\n# Make a new division\ncd $TRAIN_ROOT/data/script\npython divide_dataset_randomly.py\n```\n```Shell\n# Use a pre-divided dataset\ncd $TRAIN_ROOT/data/script\npython divide_dataset.py\n```\n\n### Train/Validation\n*Train:*\n1. Download the [convolutional weights](https://drive.google.com/open?id=1wlJtQKObDzTsxAUVh33zI-Pzr07N5ZoX) which are pre-trained on COCO dataset into `$TRAIN_ROOT/model` to initialize our model.\n2. Configurate project path in `$TRAIN_ROOT/script/train_model.sh`.\n3. Start training.\n```Shell\ncd $TRAIN_ROOT/script\nbash train_model.sh\n```\n\nBy default, training log will be written to file `$TRAIN_ROOT/log/yolo_tiny_dacsdc.out`, and validation will be performed on validation set every 20000 batch automatically. The accuracy of each validation will be stored in file `$TRAIN_ROOT/log/yolo_tiny_dacsdc.log`. Besides, weights of the best model among all validated models will be saved as `$TRAIN_ROOT/model/yolo_tiny_dacsdc_best.weights`.\n\n\n*Validation:*\n\nYou can also validate a model trained by yourself manually. Or just download our trained model [here (43MB)](https://drive.google.com/open?id=1wlJtQKObDzTsxAUVh33zI-Pzr07N5ZoX) and put it into `$TRAIN_ROOT/model`.\n1. Configurate project path in `$TRAIN_ROOT/script/valid_model.sh`.\n2. Start validating.\n```Shell\ncd $TRAIN_ROOT/script\nbash valid_model.sh\n```\n\n### Inference on Jetson TX2\nWe provide a python interface for inference on Jetson TX2. Assume that all the images to be detected are stored in `$INFERENCE_ROOT/data/images`. \n1. Copy the trained weights of the model from  `$TRAIN_ROOT/model/yolo_tiny_dacsdc_best.weights` to `$INFERENCE_ROOT/model/yolo_tiny_dacsdc_best.weights`\n2. Start inference. \n```Shell\ncd $INFERENCE_ROOT/script\npython main.py\n```\n3. Wait until the process is finished, and then you can obtain the inference result of each image in `$INFERENCE_ROOT/data/result/xml`, where each .xml file contains the predicted bounding box of the corresponding image. Besides, the speed of the model will be recorded in `$INFERENCE_ROOT/data/result/time/time.txt`.\n",
            "readme_url": "https://github.com/jndeng/DACSDC-DeepZ",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Focal Loss for Dense Object Detection",
            "arxiv": "1708.02002",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02002v2",
            "abstract": "The highest accuracy object detectors to date are based on a two-stage\napproach popularized by R-CNN, where a classifier is applied to a sparse set of\ncandidate object locations. In contrast, one-stage detectors that are applied\nover a regular, dense sampling of possible object locations have the potential\nto be faster and simpler, but have trailed the accuracy of two-stage detectors\nthus far. In this paper, we investigate why this is the case. We discover that\nthe extreme foreground-background class imbalance encountered during training\nof dense detectors is the central cause. We propose to address this class\nimbalance by reshaping the standard cross entropy loss such that it\ndown-weights the loss assigned to well-classified examples. Our novel Focal\nLoss focuses training on a sparse set of hard examples and prevents the vast\nnumber of easy negatives from overwhelming the detector during training. To\nevaluate the effectiveness of our loss, we design and train a simple dense\ndetector we call RetinaNet. Our results show that when trained with the focal\nloss, RetinaNet is able to match the speed of previous one-stage detectors\nwhile surpassing the accuracy of all existing state-of-the-art two-stage\ndetectors. Code is at: https://github.com/facebookresearch/Detectron.",
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ]
        },
        {
            "title": "Feature Pyramid Networks for Object Detection",
            "arxiv": "1612.03144",
            "year": 2016,
            "url": "http://arxiv.org/abs/1612.03144v2",
            "abstract": "Feature pyramids are a basic component in recognition systems for detecting\nobjects at different scales. But recent deep learning object detectors have\navoided pyramid representations, in part because they are compute and memory\nintensive. In this paper, we exploit the inherent multi-scale, pyramidal\nhierarchy of deep convolutional networks to construct feature pyramids with\nmarginal extra cost. A top-down architecture with lateral connections is\ndeveloped for building high-level semantic feature maps at all scales. This\narchitecture, called a Feature Pyramid Network (FPN), shows significant\nimprovement as a generic feature extractor in several applications. Using FPN\nin a basic Faster R-CNN system, our method achieves state-of-the-art\nsingle-model results on the COCO detection benchmark without bells and\nwhistles, surpassing all existing single-model entries including those from the\nCOCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU\nand thus is a practical and accurate solution to multi-scale object detection.\nCode will be made publicly available.",
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "COCO"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999315744189718,
        "task": "Object Detection",
        "task_prob": 0.9744841396995106
    }
}