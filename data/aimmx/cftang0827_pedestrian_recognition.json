{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "pedestrian_recognition",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "cftang0827",
                "owner_type": "User",
                "name": "pedestrian_recognition",
                "url": "https://github.com/cftang0827/pedestrian_recognition",
                "stars": 29,
                "pushed_at": "2022-02-10 04:55:22+00:00",
                "created_at": "2018-07-02 09:13:23+00:00",
                "language": "Jupyter Notebook",
                "description": "A simple human recognition api for re-ID usage, power by paper https://arxiv.org/abs/1703.07737",
                "license": "MIT License",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "894a44cc066a027465cd26d634948d56d13af9af",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cftang0827/pedestrian_recognition/blob/master/.gitignore"
                    }
                },
                "size": 1203
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "f21fb6b205c819460e831a6841439b117ea18cf6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cftang0827/pedestrian_recognition/blob/master/LICENSE"
                    }
                },
                "size": 1063
            },
            {
                "type": "code",
                "name": "api.py",
                "sha": "a991f58cb0da466eb01e0ad7ff64b11ac28741cf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cftang0827/pedestrian_recognition/blob/master/api.py"
                    }
                },
                "size": 2740
            },
            {
                "type": "code",
                "name": "heads",
                "sha": "b0cfa82e3a4c46ab2c5a71489b9e424ad7f8e525",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cftang0827/pedestrian_recognition/tree/master/heads"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "model",
                "sha": "29a422c19251aeaeb907175e9b3219a9bed6c616",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cftang0827/pedestrian_recognition/tree/master/model"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "nets",
                "sha": "3838bc485e052e0c42abd47cda08b5921bc0b26e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cftang0827/pedestrian_recognition/tree/master/nets"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "a54b74064cabd3d8d048dbb67925986d249f0046",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cftang0827/pedestrian_recognition/blob/master/requirements.txt"
                    }
                },
                "size": 42
            },
            {
                "type": "code",
                "name": "test.ipynb",
                "sha": "96c4d1039376f20ce65f9a4e6dce757a6475885e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cftang0827/pedestrian_recognition/blob/master/test.ipynb"
                    }
                },
                "size": 268206
            },
            {
                "type": "code",
                "name": "test",
                "sha": "c92f0781eae35905bada4376ba56402129257f12",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cftang0827/pedestrian_recognition/tree/master/test"
                    }
                },
                "num_files": 3
            }
        ]
    },
    "authors": [
        {
            "name": "Paul Tang",
            "email": "cftang0827@gmail.com",
            "github_id": "cftang0827"
        },
        {
            "name": "Tino Fuhrmann",
            "github_id": "TiFu"
        }
    ],
    "tags": [
        "re-identification",
        "tensorflow",
        "caffe2",
        "mobilenet-ssd",
        "siamese-network",
        "pedestrian-detection",
        "human-detection",
        "mobilenet"
    ],
    "description": "A simple human recognition api for re-ID usage, power by paper https://arxiv.org/abs/1703.07737",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/cftang0827/pedestrian_recognition",
            "stars": 29,
            "issues": true,
            "readme": "# pedestrian_recognition\nA simple human recognition api for re-ID usage, power by paper [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737) and [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications]( https://arxiv.org/abs/1704.04861)\n\n\n## Testing Environment\n### Operating system\n1. MacOS Sierra \n2. Ubuntu 16.04\n\n### Python package (Python 3.5 or Python3.6)\n1. Tensorflow 1.8 \n2. opencv 3.3 (Need opencv dnn library)\n3. Numpy\n\n- Install package with requirements.txt file\n```bash\npip install -r requirements.txt\n```\n\n## Prepare the model\nSince we are using third-party pretrain model, therefore, I will prepare the way to download it rather than package them toghther.\nSpecial thanks to these two repo for providing model.\n1. https://github.com/VisualComputingInstitute/triplet-reid\n2. https://github.com/chuanqi305/MobileNet-SSD\n\n```bash\n#opencv MobileNet model\nwget https://raw.githubusercontent.com/chuanqi305/MobileNet-SSD/master/deploy.prototxt -P model\nwget https://drive.google.com/u/0/uc?id=0B3gersZ2cHIxVFI1Rjd5aDgwOG8&export=download -O model/MobileNetSSD_deploy.caffemodel\n#reid model\nwget https://github.com/VisualComputingInstitute/triplet-reid/releases/download/250eb1/market1501_weights.zip -P model\nunzip model/market1501_weights.zip -d model\n```\n## Workflow\n1. Use opencv dnn module and use caffemodel to detection human in an image.\n2. Crop and resize all human(pedestrian) and resize to 256x128 images.\n3. Put image to resnet-50 human feature embedding extractor and get a 128-D feature array.\n4. Compare two human by using euclidean distance, the distance means the similarity of two image.\n\n## Example code\n```\nimport cv2\nimport api\n\nimg1 = cv2.imread('test/test1.png')[:,:,::-1]\nimg1_location = api.human_locations(img1)\nimg_1_human = api.crop_human(img1, img1_location)\nhuman_1_1 = img_1_human[0]\nhuman_1_1_vector = api.human_vector(human_1_1)\n# Do another people, and compare\n```\n\n## Add Mobilenet backbone support\nThanks to the original repo, I trained a mobilenet backbone model which can accerlerate the speed of human embedding. You can check the time difference between mobilenet and resnet-50\n\nAlso, attached is the mobilenet backbone pretrained model that I trained.\nHere is the google drive link:\nhttps://drive.google.com/file/d/1JoJJ-rIrqXNrzrx12Ih4zFk09SYsKINC/view?usp=sharing\n\nAnd the evaluation score of the model is:\n```\nmAP: 66.28% | top-1: 83.11% top-2: 88.42% | top-5: 93.79% | top-10: 95.90%\n```\n![GitHub Logo](https://github.com/cftang0827/human_recognition/blob/mobilenet/mobilenet_train_result.png?raw=true)\n\n\nPlease use mobilenet branch and download the pretrained model from the link and replace original resnet model\n\n## Acknowledgement and reference\n1. https://github.com/VisualComputingInstitute/triplet-reid\n2. https://github.com/chuanqi305/MobileNet-SSD\n3. https://github.com/opencv/opencv/tree/master/samples/dnn\n\n\n```\n@article{HermansBeyer2017Arxiv,\n  title       = {{In Defense of the Triplet Loss for Person Re-Identification}},\n  author      = {Hermans*, Alexander and Beyer*, Lucas and Leibe, Bastian},\n  journal     = {arXiv preprint arXiv:1703.07737},\n  year        = {2017}\n}\n```\n",
            "readme_url": "https://github.com/cftang0827/pedestrian_recognition",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "In Defense of the Triplet Loss for Person Re-Identification",
            "arxiv": "1703.07737",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.07737v4",
            "abstract": "In the past few years, the field of computer vision has gone through a\nrevolution fueled mainly by the advent of large datasets and the adoption of\ndeep convolutional neural networks for end-to-end learning. The person\nre-identification subfield is no exception to this. Unfortunately, a prevailing\nbelief in the community seems to be that the triplet loss is inferior to using\nsurrogate losses (classification, verification) followed by a separate metric\nlearning step. We show that, for models trained from scratch as well as\npretrained ones, using a variant of the triplet loss to perform end-to-end deep\nmetric learning outperforms most other published methods by a large margin.",
            "authors": [
                "Alexander Hermans",
                "Lucas Beyer",
                "Bastian Leibe"
            ]
        },
        {
            "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
            "arxiv": "1704.04861",
            "year": 2017,
            "url": "http://arxiv.org/abs/1704.04861v1",
            "abstract": "We present a class of efficient models called MobileNets for mobile and\nembedded vision applications. MobileNets are based on a streamlined\narchitecture that uses depth-wise separable convolutions to build light weight\ndeep neural networks. We introduce two simple global hyper-parameters that\nefficiently trade off between latency and accuracy. These hyper-parameters\nallow the model builder to choose the right sized model for their application\nbased on the constraints of the problem. We present extensive experiments on\nresource and accuracy tradeoffs and show strong performance compared to other\npopular models on ImageNet classification. We then demonstrate the\neffectiveness of MobileNets across a wide range of applications and use cases\nincluding object detection, finegrain classification, face attributes and large\nscale geo-localization.",
            "authors": [
                "Andrew G. Howard",
                "Menglong Zhu",
                "Bo Chen",
                "Dmitry Kalenichenko",
                "Weijun Wang",
                "Tobias Weyand",
                "Marco Andreetto",
                "Hartwig Adam"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999905673416787,
        "task": "Person Re-Identification",
        "task_prob": 0.886971341782112
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            }
        ]
    }
}