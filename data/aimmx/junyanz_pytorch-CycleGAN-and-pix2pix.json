{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "pytorch-CycleGAN-and-pix2pix",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "junyanz",
                "owner_type": "User",
                "name": "pytorch-CycleGAN-and-pix2pix",
                "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix",
                "stars": 17150,
                "pushed_at": "2022-03-26 06:41:10+00:00",
                "created_at": "2017-04-18 10:33:05+00:00",
                "language": "Python",
                "description": "Image-to-Image Translation in PyTorch",
                "license": "Other",
                "frameworks": [
                    "Caffe",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "4fdef3e174597ae8a3e728800fdc654c0fcdf28b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/.gitignore"
                    }
                },
                "size": 746
            },
            {
                "type": "code",
                "name": ".replit",
                "sha": "94513bcb6de8effa36d8335fba26ff7b3d11a703",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/.replit"
                    }
                },
                "size": 795
            },
            {
                "type": "code",
                "name": "CycleGAN.ipynb",
                "sha": "9214606867cef851e30c2f909f9eb8bf03e33c43",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb"
                    }
                },
                "size": 8009
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "d75f0ee8466f00cf04da906a6fca115c7910399f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/LICENSE"
                    }
                },
                "size": 3565
            },
            {
                "type": "code",
                "name": "data",
                "sha": "c6c212ef4207dd05c1a8de6e57a73f521802bdb2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/data"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "datasets",
                "sha": "fdce078797909f71d6f82955d00842f862eb5dcc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/datasets"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "e639b015daf2434dc89d86f4d1138a42bb7aa4b3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/docs"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "environment.yml",
                "sha": "ecbc55d67acc4dd98014a8c434c98389d8d546d1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/environment.yml"
                    }
                },
                "size": 227
            },
            {
                "type": "code",
                "name": "imgs",
                "sha": "9fec2b95a61099851a4c7ba86681a8ab07550bf9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/imgs"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "models",
                "sha": "621658c4e68c224044004a7d5a1be32f47249fdc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/models"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "options",
                "sha": "3d575f548e815917d1bc10f113afe2f0adbef26b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/options"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "pix2pix.ipynb",
                "sha": "0e48235636f4bddd66ee97f8071438f753d3915b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb"
                    }
                },
                "size": 7118
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "082a9b57f2190810ac87267ef749335932a237aa",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/requirements.txt"
                    }
                },
                "size": 70
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "7bfd25bdb4340c89ebca0f14c38977deea95a88f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/scripts"
                    }
                },
                "num_files": 14
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "509773b686db60e06bae8dfa2a44fb563399c7ff",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py"
                    }
                },
                "size": 4545
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "2852652df82abe91807e69285ae39fceae0fe651",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/train.py"
                    }
                },
                "size": 4933
            },
            {
                "type": "code",
                "name": "util",
                "sha": "e3e033e5fe0bf412751cbd6b6f69bc44a7b91bb1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/util"
                    }
                },
                "num_files": 6
            }
        ]
    },
    "authors": [
        {
            "name": "Jun-Yan Zhu",
            "github_id": "junyanz"
        },
        {
            "name": "taesungp",
            "github_id": "taesungp"
        },
        {
            "name": "Tongzhou Wang",
            "github_id": "SsnL"
        },
        {
            "name": "Hungryof",
            "github_id": "Naruto-Sasuke"
        },
        {
            "name": "Ayush Chaurasia",
            "email": "ayush.chaurarsia@gmail.com",
            "github_id": "AyushExel"
        },
        {
            "name": "Charles Dove",
            "github_id": "CharlesDove"
        },
        {
            "name": "LambdaWill",
            "github_id": "LambdaWill"
        },
        {
            "name": "Alex Kassil",
            "email": "contactalexkassil@gmail.com",
            "github_id": "alexkassil"
        },
        {
            "name": "Shreyas Bhat Kera",
            "email": "shreykera7@gmail.com",
            "github_id": "shreyas-bk"
        },
        {
            "name": "Iver Jordal",
            "github_id": "iver56"
        },
        {
            "name": "Bilal Khan",
            "email": "bilal.khan1@uwaterloo.ca",
            "github_id": "bilal2vec"
        },
        {
            "name": "Alan Yee",
            "github_id": "alanyee"
        },
        {
            "name": "Andy Li",
            "email": "andy@onthewings.net",
            "github_id": "andyli"
        },
        {
            "name": "Jean-Philippe Mercier",
            "github_id": "jpmerc"
        },
        {
            "name": "cmacho",
            "github_id": "cmacho"
        },
        {
            "name": "Salas",
            "email": "gae.m.project@gmail.com",
            "github_id": "leVirve"
        },
        {
            "name": "Ahmed M. Hasan",
            "github_id": "ahmedhasandrlnd"
        },
        {
            "name": "AlexWilkinsonnn",
            "github_id": "AlexWilkinsonnn"
        },
        {
            "name": "Anders Christiansen",
            "email": "andersasac@gmail.com",
            "github_id": "AndersAsa"
        },
        {
            "name": "Augusto Pertence",
            "github_id": "Pertence"
        },
        {
            "name": "Chuizheng Meng",
            "email": "chuizhem@usc.edu",
            "github_id": "mengcz13"
        },
        {
            "name": "Gr\u00e9goire Payen de La Garanderie",
            "email": "gregoire.payen.de.la.garanderie@intel.com",
            "github_id": "gdlg"
        },
        {
            "name": "Hao",
            "github_id": "zsdonghao"
        },
        {
            "name": "Hao Tang",
            "email": "hao.tang@vision.ee.ethz.ch",
            "github_id": "Ha0Tang"
        },
        {
            "name": "Jonathan Chang",
            "github_id": "cccntu"
        },
        {
            "name": "Liang Depeng",
            "email": "liangdepeng@gmail.com",
            "github_id": "Ldpe2G"
        },
        {
            "name": "Muyang Li",
            "github_id": "lmxyy"
        },
        {
            "name": "Michael Melesse",
            "email": "micmelesse@gmail.com",
            "github_id": "micmelesse"
        },
        {
            "name": "Mike Heavers",
            "github_id": "heaversm"
        },
        {
            "name": "M.Inomata",
            "github_id": "ecoopnet"
        },
        {
            "name": "Partha Das",
            "github_id": "Morpheus3000"
        },
        {
            "name": "Peter Fogh",
            "github_id": "PeterFogh"
        },
        {
            "name": "Phillip Isola",
            "email": "phillip.isola@gmail.com",
            "github_id": "phillipi"
        },
        {
            "name": "Risto Kuuster\u00e4",
            "github_id": "rkuustera"
        },
        {
            "name": "strob",
            "github_id": "strob"
        },
        {
            "name": "Syed Rubab Redwan",
            "email": "rubabredwan@gmail.com",
            "github_id": "rubabredwan"
        },
        {
            "name": "Ruotian(RT) Luo",
            "email": "rluo@ttic.edu",
            "github_id": "ruotianluo"
        },
        {
            "name": "Salisbury",
            "email": "salisbury.espinosa@gmail.com",
            "github_id": "salisbury-espinosa"
        },
        {
            "name": "Sigur\u00f0ur Sk\u00fali Sigurgeirsson",
            "email": "siggiskuli@gmail.com",
            "github_id": "Skuldur"
        },
        {
            "name": "SimonTreu",
            "github_id": "SimonTreu"
        },
        {
            "name": "Tariq Hassan",
            "github_id": "TariqAHassan"
        },
        {
            "name": "Tyler Carberry",
            "github_id": "TylerCarberry"
        },
        {
            "name": "WangPenghui",
            "github_id": "3288103265"
        },
        {
            "name": "Zhedong Zheng",
            "email": "Zhedong.Zheng@student.uts.edu.au",
            "github_id": "layumi"
        },
        {
            "name": "Zirveda Aytimur",
            "github_id": "ZirvedaAytimur"
        },
        {
            "name": "bernhardoj",
            "github_id": "bernhardoj"
        },
        {
            "name": "eickenberg",
            "github_id": "eickenberg"
        },
        {
            "name": "guopzhao",
            "github_id": "guopzhao"
        },
        {
            "name": "jacky841102",
            "github_id": "jacky841102"
        },
        {
            "name": "kagudkovArt",
            "github_id": "kagudkovArt"
        },
        {
            "name": "mathbbN",
            "github_id": "nat-chan"
        },
        {
            "name": "Oren Katzir",
            "github_id": "orenkatzir"
        },
        {
            "name": "wangyi111",
            "github_id": "wangyi111"
        },
        {
            "name": "yt6t6t",
            "github_id": "yt6t6t"
        }
    ],
    "tags": [
        "pytorch",
        "gan",
        "cyclegan",
        "pix2pix",
        "deep-learning",
        "computer-vision",
        "computer-graphics",
        "image-manipulation",
        "image-generation",
        "generative-adversarial-network",
        "gans"
    ],
    "description": "Image-to-Image Translation in PyTorch",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix",
            "stars": 17150,
            "issues": true,
            "readme": "\n<img src='imgs/horse2zebra.gif' align=\"right\" width=384>\n\n<br><br><br>\n\n# CycleGAN and pix2pix in PyTorch\n\n**New**:  Please check out [contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training.\n\nWe provide PyTorch implementations for both unpaired and paired image-to-image translation.\n\nThe code was written by [Jun-Yan Zhu](https://github.com/junyanz) and [Taesung Park](https://github.com/taesungp), and supported by [Tongzhou Wang](https://github.com/SsnL).\n\nThis PyTorch implementation produces results comparable to or better than our original Torch software. If you would like to reproduce the same results as in the papers, check out the original [CycleGAN Torch](https://github.com/junyanz/CycleGAN) and [pix2pix Torch](https://github.com/phillipi/pix2pix) code in Lua/Torch.\n\n**Note**: The current software works well with PyTorch 1.4. Check out the older [branch](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/pytorch0.3.1) that supports PyTorch 0.1-0.3.\n\nYou may find useful information in [training/test tips](docs/tips.md) and [frequently asked questions](docs/qa.md). To implement custom models and datasets, check out our [templates](#custom-model-and-dataset). To help users better understand and adapt our codebase, we provide an [overview](docs/overview.md) of the code structure of this repository.\n\n**CycleGAN: [Project](https://junyanz.github.io/CycleGAN/) |  [Paper](https://arxiv.org/pdf/1703.10593.pdf) |  [Torch](https://github.com/junyanz/CycleGAN) |\n[Tensorflow Core Tutorial](https://www.tensorflow.org/tutorials/generative/cyclegan) | [PyTorch Colab](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb)**\n\n<img src=\"https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg\" width=\"800\"/>\n\n**Pix2pix:  [Project](https://phillipi.github.io/pix2pix/) |  [Paper](https://arxiv.org/pdf/1611.07004.pdf) |  [Torch](https://github.com/phillipi/pix2pix) |\n[Tensorflow Core Tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix) | [PyTorch Colab](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb)**\n\n<img src=\"https://phillipi.github.io/pix2pix/images/teaser_v3.png\" width=\"800px\"/>\n\n\n**[EdgesCats Demo](https://affinelayer.com/pixsrv/) | [pix2pix-tensorflow](https://github.com/affinelayer/pix2pix-tensorflow) | by [Christopher Hesse](https://twitter.com/christophrhesse)**\n\n<img src='imgs/edges2cats.jpg' width=\"400px\"/>\n\nIf you use this code for your research, please cite:\n\nUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.<br>\n[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)\\*,  [Taesung Park](https://taesung.me/)\\*, [Phillip Isola](https://people.eecs.berkeley.edu/~isola/), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros). In ICCV 2017. (* equal contributions) [[Bibtex]](https://junyanz.github.io/CycleGAN/CycleGAN.txt)\n\n\nImage-to-Image Translation with Conditional Adversarial Networks.<br>\n[Phillip Isola](https://people.eecs.berkeley.edu/~isola), [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/), [Tinghui Zhou](https://people.eecs.berkeley.edu/~tinghuiz), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros). In CVPR 2017. [[Bibtex]](https://www.cs.cmu.edu/~junyanz/projects/pix2pix/pix2pix.bib)\n\n## Talks and Course\npix2pix slides: [keynote](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.key) | [pdf](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.pdf),\nCycleGAN slides: [pptx](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pptx) | [pdf](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pdf)\n\nCycleGAN course assignment [code](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip) and [handout](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-handout.pdf) designed by Prof. [Roger Grosse](http://www.cs.toronto.edu/~rgrosse/) for [CSC321](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/) \"Intro to Neural Networks and Machine Learning\" at University of Toronto. Please contact the instructor if you would like to adopt it in your course.\n\n## Colab Notebook\nTensorFlow Core CycleGAN Tutorial: [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb) | [Code](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb)\n\nTensorFlow Core pix2pix Tutorial: [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb) | [Code](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb)\n\nPyTorch Colab notebook: [CycleGAN](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb) and [pix2pix](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb)\n\nZeroCostDL4Mic Colab notebook: [CycleGAN](https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks_Beta/CycleGAN_ZeroCostDL4Mic.ipynb) and [pix2pix](https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks_Beta/pix2pix_ZeroCostDL4Mic.ipynb)\n\n## Other implementations\n### CycleGAN\n<p><a href=\"https://github.com/leehomyc/cyclegan-1\"> [Tensorflow]</a> (by Harry Yang),\n<a href=\"https://github.com/architrathore/CycleGAN/\">[Tensorflow]</a> (by Archit Rathore),\n<a href=\"https://github.com/vanhuyz/CycleGAN-TensorFlow\">[Tensorflow]</a> (by Van Huy),\n<a href=\"https://github.com/XHUJOY/CycleGAN-tensorflow\">[Tensorflow]</a> (by Xiaowei Hu),\n<a href=\"https://github.com/LynnHo/CycleGAN-Tensorflow-2\"> [Tensorflow2]</a> (by Zhenliang He),\n<a href=\"https://github.com/luoxier/CycleGAN_Tensorlayer\"> [TensorLayer1.0]</a> (by luoxier),\n<a href=\"https://github.com/tensorlayer/cyclegan\"> [TensorLayer2.0]</a> (by zsdonghao),\n<a href=\"https://github.com/Aixile/chainer-cyclegan\">[Chainer]</a> (by Yanghua Jin),\n<a href=\"https://github.com/yunjey/mnist-svhn-transfer\">[Minimal PyTorch]</a> (by yunjey),\n<a href=\"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN\">[Mxnet]</a> (by Ldpe2G),\n<a href=\"https://github.com/tjwei/GANotebooks\">[lasagne/Keras]</a> (by tjwei),\n<a href=\"https://github.com/simontomaskarlsson/CycleGAN-Keras\">[Keras]</a> (by Simon Karlsson),\n<a href=\"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Oneflow-Python/CycleGAN\">[OneFlow]</a> (by Ldpe2G)\n</p>\n</ul>\n\n### pix2pix\n<p><a href=\"https://github.com/affinelayer/pix2pix-tensorflow\"> [Tensorflow]</a> (by Christopher Hesse),\n<a href=\"https://github.com/Eyyub/tensorflow-pix2pix\">[Tensorflow]</a> (by Eyy\u00fcb Sariu),\n<a href=\"https://github.com/datitran/face2face-demo\"> [Tensorflow (face2face)]</a> (by Dat Tran),\n<a href=\"https://github.com/awjuliani/Pix2Pix-Film\"> [Tensorflow (film)]</a> (by Arthur Juliani),\n<a href=\"https://github.com/kaonashi-tyc/zi2zi\">[Tensorflow (zi2zi)]</a> (by Yuchen Tian),\n<a href=\"https://github.com/pfnet-research/chainer-pix2pix\">[Chainer]</a> (by mattya),\n<a href=\"https://github.com/tjwei/GANotebooks\">[tf/torch/keras/lasagne]</a> (by tjwei),\n<a href=\"https://github.com/taey16/pix2pixBEGAN.pytorch\">[Pytorch]</a> (by taey16)\n</p>\n</ul>\n\n## Prerequisites\n- Linux or macOS\n- Python 3\n- CPU or NVIDIA GPU + CUDA CuDNN\n\n## Getting Started\n### Installation\n\n- Clone this repo:\n```bash\ngit clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\ncd pytorch-CycleGAN-and-pix2pix\n```\n\n- Install [PyTorch](http://pytorch.org) and 0.4+ and other dependencies (e.g., torchvision, [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate)).\n  - For pip users, please type the command `pip install -r requirements.txt`.\n  - For Conda users, you can create a new Conda environment using `conda env create -f environment.yml`.\n  - For Docker users, we provide the pre-built Docker image and Dockerfile. Please refer to our [Docker](docs/docker.md) page.\n  - For Repl users, please click [![Run on Repl.it](https://repl.it/badge/github/junyanz/pytorch-CycleGAN-and-pix2pix)](https://repl.it/github/junyanz/pytorch-CycleGAN-and-pix2pix).\n\n### CycleGAN train/test\n- Download a CycleGAN dataset (e.g. maps):\n```bash\nbash ./datasets/download_cyclegan_dataset.sh maps\n```\n- To view training results and loss plots, run `python -m visdom.server` and click the URL http://localhost:8097.\n- To log training progress and test images to W&B dashboard, set the `--use_wandb` flag with train and test script\n- Train a model:\n```bash\n#!./scripts/train_cyclegan.sh\npython train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n```\nTo see more intermediate results, check out `./checkpoints/maps_cyclegan/web/index.html`.\n- Test the model:\n```bash\n#!./scripts/test_cyclegan.sh\npython test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n```\n- The test results will be saved to a html file here: `./results/maps_cyclegan/latest_test/index.html`.\n\n### pix2pix train/test\n- Download a pix2pix dataset (e.g.[facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)):\n```bash\nbash ./datasets/download_pix2pix_dataset.sh facades\n```\n- To view training results and loss plots, run `python -m visdom.server` and click the URL http://localhost:8097.\n- To log training progress and test images to W&B dashboard, set the `--use_wandb` flag with train and test script\n- Train a model:\n```bash\n#!./scripts/train_pix2pix.sh\npython train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n```\nTo see more intermediate results, check out  `./checkpoints/facades_pix2pix/web/index.html`.\n\n- Test the model (`bash ./scripts/test_pix2pix.sh`):\n```bash\n#!./scripts/test_pix2pix.sh\npython test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n```\n- The test results will be saved to a html file here: `./results/facades_pix2pix/test_latest/index.html`. You can find more scripts at `scripts` directory.\n- To train and test pix2pix-based colorization models, please add `--model colorization` and `--dataset_mode colorization`. See our training [tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#notes-on-colorization) for more details.\n\n### Apply a pre-trained model (CycleGAN)\n- You can download a pretrained model (e.g. horse2zebra) with the following script:\n```bash\nbash ./scripts/download_cyclegan_model.sh horse2zebra\n```\n- The pretrained model is saved at `./checkpoints/{name}_pretrained/latest_net_G.pth`. Check [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_cyclegan_model.sh#L3) for all the available CycleGAN models.\n- To test the model, you also need to download the  horse2zebra dataset:\n```bash\nbash ./datasets/download_cyclegan_dataset.sh horse2zebra\n```\n\n- Then generate the results using\n```bash\npython test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout\n```\n- The option `--model test` is used for generating results of CycleGAN only for one side. This option will automatically set `--dataset_mode single`, which only loads the images from one set. On the contrary, using `--model cycle_gan` requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at `./results/`. Use `--results_dir {directory_path_to_save_result}` to specify the results directory.\n\n- For pix2pix and your own models, you need to explicitly specify `--netG`, `--norm`, `--no_dropout` to match the generator architecture of the trained model. See this [FAQ](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#runtimeerror-errors-in-loading-state_dict-812-671461-296) for more details.\n\n### Apply a pre-trained model (pix2pix)\nDownload a pre-trained model with `./scripts/download_pix2pix_model.sh`.\n\n- Check [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_pix2pix_model.sh#L3) for all the available pix2pix models. For example, if you would like to download label2photo model on the Facades dataset,\n```bash\nbash ./scripts/download_pix2pix_model.sh facades_label2photo\n```\n- Download the pix2pix facades datasets:\n```bash\nbash ./datasets/download_pix2pix_dataset.sh facades\n```\n- Then generate the results using\n```bash\npython test.py --dataroot ./datasets/facades/ --direction BtoA --model pix2pix --name facades_label2photo_pretrained\n```\n- Note that we specified `--direction BtoA` as Facades dataset's A to B direction is photos to labels.\n\n- If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use `--model test` option. See `./scripts/test_single.sh` for how to apply a model to Facade label maps (stored in the directory `facades/testB`).\n\n- See a list of currently available models at `./scripts/download_pix2pix_model.sh`\n\n## [Docker](docs/docker.md)\nWe provide the pre-built Docker image and Dockerfile that can run this code repo. See [docker](docs/docker.md).\n\n## [Datasets](docs/datasets.md)\nDownload pix2pix/CycleGAN datasets and create your own datasets.\n\n## [Training/Test Tips](docs/tips.md)\nBest practice for training and testing your models.\n\n## [Frequently Asked Questions](docs/qa.md)\nBefore you post a new question, please first look at the above Q & A and existing GitHub issues.\n\n## Custom Model and Dataset\nIf you plan to implement custom models and dataset for your new applications, we provide a dataset [template](data/template_dataset.py) and a model [template](models/template_model.py) as a starting point.\n\n## [Code structure](docs/overview.md)\nTo help users better understand and use our code, we briefly overview the functionality and implementation of each package and each module.\n\n## Pull Request\nYou are always welcome to contribute to this repository by sending a [pull request](https://help.github.com/articles/about-pull-requests/).\nPlease run `flake8 --ignore E501 .` and `python ./scripts/test_before_push.py` before you commit the code. Please also update the code structure [overview](docs/overview.md) accordingly if you add or remove files.\n\n## Citation\nIf you use this code for your research, please cite our papers.\n```\n@inproceedings{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},\n  year={2017}\n}\n\n\n@inproceedings{isola2017image,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},\n  year={2017}\n}\n```\n\n## Other Languages\n[Spanish](docs/README_es.md)\n\n## Related Projects\n**[contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) (CUT)**<br>\n**[CycleGAN-Torch](https://github.com/junyanz/CycleGAN) |\n[pix2pix-Torch](https://github.com/phillipi/pix2pix) | [pix2pixHD](https://github.com/NVIDIA/pix2pixHD)|\n[BicycleGAN](https://github.com/junyanz/BicycleGAN) | [vid2vid](https://tcwang0509.github.io/vid2vid/) | [SPADE/GauGAN](https://github.com/NVlabs/SPADE)**<br>\n**[iGAN](https://github.com/junyanz/iGAN) | [GAN Dissection](https://github.com/CSAILVision/GANDissect) | [GAN Paint](http://ganpaint.io/)**\n\n## Cat Paper Collection\nIf you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper [Collection](https://github.com/junyanz/CatPapers).\n\n## Acknowledgments\nOur code is inspired by [pytorch-DCGAN](https://github.com/pytorch/examples/tree/master/dcgan).\n",
            "readme_url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix",
            "frameworks": [
                "Caffe",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "arxiv": "1703.10593",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10593v7",
            "abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.",
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A. Efros"
            ]
        },
        {
            "title": "Image-to-Image Translation with Conditional Adversarial Networks",
            "arxiv": "1611.07004",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.07004v3",
            "abstract": "We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either.",
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A. Efros"
            ]
        },
        {
            "year": "2017",
            "booktitle": "Computer Vision (ICCV), 2017 IEEE International Conference on",
            "author": [
                "Zhu, Jun-Yan",
                "Park, Taesung",
                "Isola, Phillip",
                "Efros, Alexei A"
            ],
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "ENTRYTYPE": "inproceedings",
            "ID": "CycleGAN2017",
            "authors": [
                "Zhu, Jun-Yan",
                "Park, Taesung",
                "Isola, Phillip",
                "Efros, Alexei A"
            ]
        },
        {
            "year": "2017",
            "booktitle": "Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on",
            "author": [
                "Isola, Phillip",
                "Zhu, Jun-Yan",
                "Zhou, Tinghui",
                "Efros, Alexei A"
            ],
            "title": "Image-to-Image Translation with Conditional Adversarial Networks",
            "ENTRYTYPE": "inproceedings",
            "ID": "isola2017image",
            "authors": [
                "Isola, Phillip",
                "Zhu, Jun-Yan",
                "Zhou, Tinghui",
                "Efros, Alexei A"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Datasets",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md"
                    }
                }
            },
            {
                "name": "MNIST"
            },
            {
                "name": "SVHN"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999998183518618,
        "task": "Image-to-Image Translation",
        "task_prob": 0.994117914552181
    }
}