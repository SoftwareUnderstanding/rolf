{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "stylegan2",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "cto300",
                "owner_type": "User",
                "name": "stylegan2",
                "url": "https://github.com/cto300/stylegan2",
                "stars": 1,
                "pushed_at": "2022-03-12 00:26:10+00:00",
                "created_at": "2020-04-28 05:14:26+00:00",
                "language": "Jupyter Notebook",
                "license": "Other",
                "frameworks": [
                    "TensorFlow",
                    "Keras",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE.txt",
                "sha": "d7e85075defbd96f9e7a9d756aa7db0e7e30ccf7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/LICENSE.txt"
                    }
                },
                "size": 4767
            },
            {
                "type": "code",
                "name": "Process WikiArt Dataset.ipynb",
                "sha": "fd1b41ff64417fff7dd3a1af541c7d7c4569a626",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/Process WikiArt Dataset.ipynb"
                    }
                },
                "size": 7611420
            },
            {
                "type": "code",
                "name": "StyleGAN_Encoder_Tutorial.ipynb",
                "sha": "f8ea1419cdf635f072d9aff11a70288b7c9ff54d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/StyleGAN_Encoder_Tutorial.ipynb"
                    }
                },
                "size": 4677553
            },
            {
                "type": "code",
                "name": "WikiArt Example Generation.ipynb",
                "sha": "3059737810bf5d66d6fa8175ea843c20ba1dc070",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/WikiArt Example Generation.ipynb"
                    }
                },
                "size": 8610
            },
            {
                "type": "code",
                "name": "adaptive.py",
                "sha": "1a5a24ab4609720575f835a92c4c9cb4a74a1123",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/adaptive.py"
                    }
                },
                "size": 19790
            },
            {
                "type": "code",
                "name": "align_images.py",
                "sha": "28df6872e4a57d5c7c673c74f7ff390e44512d6c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/align_images.py"
                    }
                },
                "size": 1515
            },
            {
                "type": "code",
                "name": "dataset_tool.py",
                "sha": "86667f4d4e61285f80e66c591b134226e65617e3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/dataset_tool.py"
                    }
                },
                "size": 35786
            },
            {
                "type": "code",
                "name": "dnnlib",
                "sha": "f511790f59a65d4541f2026aaf9223fde1cf94f7",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/tree/master/dnnlib"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "5a9eee159cfad1fe7d11a116fc77e7400b32aa66",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/tree/master/docs"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "encode_images.py",
                "sha": "dfbe8dd4f5f045aed7d371295c21121818d25a7f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/encode_images.py"
                    }
                },
                "size": 15156
            },
            {
                "type": "code",
                "name": "encoder",
                "sha": "2d8747bbd2480a5f0d5acbff05922dc78a585d0b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/tree/master/encoder"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "fake_art_portrait.jpg",
                "sha": "0eb988d8e04c0f61758ea4ea9b058cd3f389cae7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/fake_art_portrait.jpg"
                    }
                },
                "size": 6094233
            },
            {
                "type": "code",
                "name": "ffhq_dataset",
                "sha": "fa8cd966e4a0c731a303e4dfa444ef5de24e2616",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/tree/master/ffhq_dataset"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "metrics",
                "sha": "e53d7bf5ca54c50d48dced7d9a47f2b056543f83",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/tree/master/metrics"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "pretrained_networks.py",
                "sha": "40ccfd9eb70417149dd7e8ee77735b26c2f919d5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/pretrained_networks.py"
                    }
                },
                "size": 7665
            },
            {
                "type": "code",
                "name": "projector.py",
                "sha": "affbdecb77da9e478d9d8cb44c3ec75c5558abe5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/projector.py"
                    }
                },
                "size": 8982
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "1f61f3465fc2bc55576c148715b00121c22a6b8c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/requirements.txt"
                    }
                },
                "size": 497
            },
            {
                "type": "code",
                "name": "robust_loss",
                "sha": "493faf56c1af10f8145e70e79135beb9298a813b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/tree/master/robust_loss"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "run_generator.py",
                "sha": "2f154143fa3755551212f8d953b1af76c801b56a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/run_generator.py"
                    }
                },
                "size": 8239
            },
            {
                "type": "code",
                "name": "run_metrics.py",
                "sha": "5043b100faf3f58273cdf00239611d950962324c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/run_metrics.py"
                    }
                },
                "size": 3447
            },
            {
                "type": "code",
                "name": "run_projector.py",
                "sha": "bf18bd7348c469451dab7e720f43417b97cc7930",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/run_projector.py"
                    }
                },
                "size": 6957
            },
            {
                "type": "code",
                "name": "run_training.py",
                "sha": "b45b4f53f46a09a3f2ba3fa0e314ebd4c26eab3b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/run_training.py"
                    }
                },
                "size": 9920
            },
            {
                "type": "code",
                "name": "runway.yml",
                "sha": "48409a725d8cb538d58bbfbf1c68ea39ccf90e4c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/runway.yml"
                    }
                },
                "size": 174
            },
            {
                "type": "code",
                "name": "runway_model.py",
                "sha": "4bf35e067dff03ff5ea239db494f1663cb6be01e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/runway_model.py"
                    }
                },
                "size": 1218
            },
            {
                "type": "code",
                "name": "swa.py",
                "sha": "ddd272a6f4632560abf1b2aa513976ff0d2672ee",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/swa.py"
                    }
                },
                "size": 2221
            },
            {
                "type": "code",
                "name": "test_nvcc.cu",
                "sha": "8b4804120cc3fe71de8f643edeebe3989645feac",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/test_nvcc.cu"
                    }
                },
                "size": 715
            },
            {
                "type": "code",
                "name": "train_resnet.py",
                "sha": "268fe4a0f4437ecabad93ab9580827d12fcff692",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/train_resnet.py"
                    }
                },
                "size": 14091
            },
            {
                "type": "code",
                "name": "training",
                "sha": "00c5a913be1ef7c208d93cdd7585c7a01528dc34",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/tree/master/training"
                    }
                },
                "num_files": 7
            }
        ]
    },
    "trained_model": {
        "binaries": [
            {
                "type": "binary",
                "name": "Dockerfile",
                "sha": "ab45a553e0d49878585054e690aba74f2ca939ff",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/cto300/stylegan2/blob/master/Dockerfile"
                    }
                },
                "size": 362
            }
        ]
    },
    "authors": [
        {
            "name": "skyflynil",
            "github_id": "skyflynil"
        },
        {
            "name": "Tero Karras",
            "email": "tkarras@nvidia.com",
            "github_id": "tkarras"
        },
        {
            "name": "Peter Baylies",
            "github_id": "pbaylies"
        },
        {
            "name": "Gene Kogan",
            "email": "kogan.gene@gmail.com",
            "github_id": "genekogan"
        },
        {
            "name": "rolux",
            "email": "rolux@rolux.org",
            "github_id": "rolux"
        },
        {
            "name": "cto300",
            "github_id": "cto300"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/cto300/stylegan2",
            "stars": 1,
            "issues": true,
            "readme": "![Teaser image](./fake_art_portrait.jpg)\n\n* Conditional model trained on WikiArt images [now available for download](https://archive.org/details/wikiart-stylegan2-conditional-model)\n* Take a look at the included notebooks for examples\n* Conditional support originally from @davidstap\n* port of my encoder from @robertluxemburg\n* fp16 branch from @veqtor\n* tpu and swarm branches from @shawwn\n* runwayml support from @genekogan\n\n<a href=\"https://open-app.runwayml.com/?model=pbaylies/stylegan2\" target=\"_blank\"><img src=\"https://open-app.runwayml.com/gh-badge.svg\" /></a>\n\n**Various Improvements from skyflynil to make StyleGAN2 more suitible to be trained on Google Colab**\n* Supports Non-Square images, for example, 768x512, which basically as 6x4 (x2^7), or 640x384 as 5x3 (x2^7), etc.\n* Supports vertical mirror augmentation\n* Supports train from latest pkl automatically\n* Optimized dataset creation and access for non-progressive training and for colab training, which includes: create only the maximum size tfrecord; use raw JPEG instead of decoded numpy array, which reduce both tfrecord creation time and dataset size dramatically. (* Only tested for config-e and config-f, as no-progressive for these configurations)\n\n**Detailed instruction for training your stylegan2**\n\n* Create training image set. Instead of image size of 2^n * 2^n, now you can process your image size as of (min_h x 2^n) X (min_w * 2^n) natually. For example, 640x384, min_h = 5, min_w =3, n=7. Please make sure all your raw images are preprocessed to the exact same size. To reduce the training set size, JPEG format is preferred.\n* Create tfrecord, clone this repo, then\n```\npython dataset_tool.py create_from_images_raw dataset_dir raw_image_dir\n```\n* To train, for example, 640x384 training set\n```\npython run_training.py --num-gpus=your_gpu_num --data-dir=your_data_dir --config=config-e(or config_f) --dataset=your_data_set --mirror-augment=true --metric=none --total-kimg=12000 --min-h=5 --min-w=3 --res-log2=7 --result-dir=your_result_dir\n```\n\n**Tips for Colab training**\n* Clone this repo\n```\n%tensorflow_version 1.x\nimport tensorflow as tf\n\n# Download the code\n!git clone https://github.com/skyflynil/stylegan2.git\n%cd stylegan2\n!nvcc test_nvcc.cu -o test_nvcc -run\n\nprint('Tensorflow version: {}'.format(tf.__version__) )\n!nvidia-smi -L\nprint('GPU Identified at: {}'.format(tf.test.gpu_device_name()))\n```\n* Tar your raw data and upload to google drive, share it as data_url\n* In colab, mount your google drive, and make a result dir if there is none, for example, 'stylegan2/results'\n```\nfrom google.colab import drive\ndrive.mount(\"/content/drive\", force_remount=True)\n```\n\n* download raw dataset to colab using \n```\n!gdown data_url\n```\n* create your dataset for train\n```\n!mkdir dataset\n!tar -xf your_downloaded_tar\n!python dataset_tool.py create_from_images_raw ./dataset/dataset_name untared_raw_image_dir\n```\n* start training\n```\n!python run_training.py --num-gpus=1 --data-dir=./dataset --config=config-f --dataset=your_dataset_name --mirror-augment=true --metric=none --total-kimg=20000 --min-h=5 --min-w=3 --res-log2=7 --result-dir=\"/content/drive/My Drive/stylegan2/results\"\n```\nand it will automatically resume from last saved pkl.\n\n* You may also save a generated tfrecord directly in your google drive, and pin your dataset dir to your google drive. The benefit of creating a new tfrecord everytime is: Google colab disconnects after around 9-12 hours, since there is no true randomness for tfrecord, you may end up using some data more often then others. Also, the read/transfer speed from mounted google drive is kind of slow. It only takes about 2 min to gdown and create dataset for 30k/2G jpeg files.\n\n* You may also try this to boost your instance memory before training. \n\nhttps://github.com/googlecolab/colabtools/issues/253\n\n* For image size 1280x768 (hxw), you may choose (min_h, min_w, res_log2) as (10, 6, 7) or (5, 3, 8) , the latter setup is preferred due to deeper and smaller network, change res_log2 argument for dataset creation and training accordingly.\n```\n!python dataset_tool.py create_from_images_raw --res_log2=8 ./dataset/dataset_name untared_raw_image_dir\n!python run_training.py --num-gpus=1 --data-dir=./dataset --config=config-f --dataset=your_dataset_name --mirror-augment=true --metric=none --total-kimg=20000 --min-h=5 --min-w=3 --res-log2=8 --result-dir=\"/content/drive/My Drive/stylegan2/results\"\n```\n* You may change relevant arguments in run_traing.py for fakeimage/checkpoint interval, D/G learning rate, and minibatch_gpu_base to suit your needs or workaround gpu memory issues.\n\n**Credits**\n* https://github.com/NVlabs/stylegan2\n* https://github.com/akanimax/msg-stylegan-tf\n* https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH\n\n\n## StyleGAN2 &mdash; Encoder for Official TensorFlow Implementation\n![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg?style=plastic)\n![TensorFlow 1.10](https://img.shields.io/badge/tensorflow-1.10-green.svg?style=plastic)\n![cuDNN 7.3.1](https://img.shields.io/badge/cudnn-7.3.1-green.svg?style=plastic)\n![License CC BY-NC](https://img.shields.io/badge/license-CC_BY--NC-green.svg?style=plastic)\n\nThis is an experimental port of [pbaylies/stylegan-encoder](https://github.com/pbaylies/stylegan-encoder) for [NVlabs/stylegan2](https://github.com/NVlabs/stylegan2).\n\n![Teaser image](./docs/stylegan2encoder-teaser-1024x256.png)\n\nTo test this, try out the notebook.\n\n### Original Readme\n\n![Teaser image](./docs/stylegan2-teaser-1024x256.png)\n\n**Analyzing and Improving the Image Quality of StyleGAN**<br>\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila<br>\n\nPaper: http://arxiv.org/abs/1912.04958<br>\nVideo: https://youtu.be/c-NJtV9Jvp0<br>\n\nAbstract: *The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.*\n\nFor business inquiries, please contact [researchinquiries@nvidia.com](mailto:researchinquiries@nvidia.com)<br>\nFor press and other inquiries, please contact Hector Marinez at [hmarinez@nvidia.com](mailto:hmarinez@nvidia.com)<br>\n\n| Additional material | &nbsp;\n| :--- | :----------\n| [StyleGAN2](https://drive.google.com/open?id=1QHc-yF5C3DChRwSdZKcx1w6K8JvSxQi7) | Main Google Drive folder\n| &boxvr;&nbsp; [stylegan2-paper.pdf](https://drive.google.com/open?id=1fnF-QsiQeKaxF-HbvFiGtzHF_Bf3CzJu) | High-quality version of the paper\n| &boxvr;&nbsp; [stylegan2-video.mp4](https://drive.google.com/open?id=1f_gbKW6FUUHKkUxciJ_lQx29mCq_fSBy) | High-quality version of the video\n| &boxvr;&nbsp; [images](https://drive.google.com/open?id=1Sak157_DLX84ytqHHqZaH_59HoEWzfB7) | Example images produced using our method\n| &boxv;&nbsp; &boxvr;&nbsp;  [curated-images](https://drive.google.com/open?id=1ydWb8xCHzDKMTW9kQ7sL-B1R0zATHVHp) | Hand-picked images showcasing our results\n| &boxv;&nbsp; &boxur;&nbsp;  [100k-generated-images](https://drive.google.com/open?id=1BA2OZ1GshdfFZGYZPob5QWOGBuJCdu5q) | Random images with and without truncation\n| &boxvr;&nbsp; [videos](https://drive.google.com/open?id=1yXDV96SFXoUiZKU7AyE6DyKgDpIk4wUZ) | Individual clips of the video as high-quality MP4\n| &boxur;&nbsp; [networks](https://drive.google.com/open?id=1yanUI9m4b4PWzR0eurKNq6JR1Bbfbh6L) | Pre-trained networks\n| &ensp;&ensp; &boxvr;&nbsp;  [stylegan2-ffhq-config-f.pkl](http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-ffhq-config-f.pkl) | StyleGAN2 for <span style=\"font-variant:small-caps\">FFHQ</span> dataset at 1024&times;1024\n| &ensp;&ensp; &boxvr;&nbsp;  [stylegan2-car-config-f.pkl](http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-car-config-f.pkl) | StyleGAN2 for <span style=\"font-variant:small-caps\">LSUN Car</span> dataset at 512&times;384\n| &ensp;&ensp; &boxvr;&nbsp;  [stylegan2-cat-config-f.pkl](http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-cat-config-f.pkl) | StyleGAN2 for <span style=\"font-variant:small-caps\">LSUN Cat</span> dataset at 256&times;256\n| &ensp;&ensp; &boxvr;&nbsp;  [stylegan2-church-config-f.pkl](http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-church-config-f.pkl) | StyleGAN2 for <span style=\"font-variant:small-caps\">LSUN Church</span> dataset at 256&times;256\n| &ensp;&ensp; &boxvr;&nbsp;  [stylegan2-horse-config-f.pkl](http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-horse-config-f.pkl) | StyleGAN2 for <span style=\"font-variant:small-caps\">LSUN Horse</span> dataset at 256&times;256\n| &ensp;&ensp; &boxur;&nbsp;&#x22ef;  | Other training configurations used in the paper\n\n## Requirements\n\n* Both Linux and Windows are supported. Linux is recommended for performance and compatibility reasons.\n* 64-bit Python 3.6 installation. We recommend Anaconda3 with numpy 1.14.3 or newer.\n* TensorFlow 1.14 or 1.15 with GPU support. The code does not support TensorFlow 2.0.\n* On Windows, you need to use TensorFlow 1.14 &mdash; TensorFlow 1.15 will not work.\n* One or more high-end NVIDIA GPUs, NVIDIA drivers, CUDA 10.0 toolkit and cuDNN 7.5. To reproduce the results reported in the paper, you need an NVIDIA GPU with at least 16 GB of DRAM.\n* Docker users: use the [provided Dockerfile](./Dockerfile) to build an image with the required library dependencies.\n\nStyleGAN2 relies on custom TensorFlow ops that are compiled on the fly using [NVCC](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html). To test that your NVCC installation is working correctly, run:\n\n```.bash\nnvcc test_nvcc.cu -o test_nvcc -run\n| CPU says hello.\n| GPU says hello.\n```\n\nOn Windows, the compilation requires Microsoft Visual Studio to be in `PATH`. We recommend installing [Visual Studio Community Edition](https://visualstudio.microsoft.com/vs/) and adding into `PATH` using `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"`.\n\n## Using pre-trained networks\n\nPre-trained networks are stored as `*.pkl` files on the [StyleGAN2 Google Drive folder](https://drive.google.com/open?id=1QHc-yF5C3DChRwSdZKcx1w6K8JvSxQi7). Below, you can either reference them directly using the syntax `gdrive:networks/<filename>.pkl`, or download them manually and reference by filename.\n\n```.bash\n# Generate uncurated ffhq images (matches paper Figure 12)\npython run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\\n  --seeds=6600-6625 --truncation-psi=0.5\n\n# Generate curated ffhq images (matches paper Figure 11)\npython run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\\n  --seeds=66,230,389,1518 --truncation-psi=1.0\n\n# Generate uncurated car images\npython run_generator.py generate-images --network=gdrive:networks/stylegan2-car-config-f.pkl \\\n  --seeds=6000-6025 --truncation-psi=0.5\n\n# Example of style mixing (matches the corresponding video clip)\npython run_generator.py style-mixing-example --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\\n  --row-seeds=85,100,75,458,1500 --col-seeds=55,821,1789,293 --truncation-psi=1.0\n```\n\nThe results are placed in `results/<RUNNING_ID>/*.png`. You can change the location with `--result-dir`. For example, `--result-dir=~/my-stylegan2-results`.\n\nYou can import the networks in your own Python code using `pickle.load()`. For this to work, you need to include the `dnnlib` source directory in `PYTHONPATH` and create a default TensorFlow session by calling `dnnlib.tflib.init_tf()`. See [run_generator.py](./run_generator.py) and [pretrained_networks.py](./pretrained_networks.py) for examples.\n\n## Preparing datasets\n\nDatasets are stored as multi-resolution TFRecords, similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory, e.g., `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections, the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments, e.g., `--dataset=ffhq --data-dir=~/datasets`.\n\n**FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords, run:\n\n```.bash\npushd ~\ngit clone https://github.com/NVlabs/ffhq-dataset.git\ncd ffhq-dataset\npython download_ffhq.py --tfrecords\npopd\npython dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq\n```\n\n**LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords, run:\n\n```.bash\npython dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384\npython dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256\npython dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256\npython dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256\n```\n\n**Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords, run:\n\n```.bash\npython dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images\npython dataset_tool.py display ~/datasets/my-custom-dataset\n```\n\n## Projecting images to latent space\n\nTo find the matching latent vectors for a set of images, run:\n\n```.bash\n# Project generated images\npython run_projector.py project-generated-images --network=gdrive:networks/stylegan2-car-config-f.pkl \\\n  --seeds=0,1,5\n\n# Project real images\npython run_projector.py project-real-images --network=gdrive:networks/stylegan2-car-config-f.pkl \\\n  --dataset=car --data-dir=~/datasets\n```\n\n## Training networks\n\nTo reproduce the training runs for config F in Tables 1 and 3, run:\n\n```.bash\npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\\n  --dataset=ffhq --mirror-augment=true\npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\\n  --dataset=car --total-kimg=57000\npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\\n  --dataset=cat --total-kimg=88000\npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\\n  --dataset=church --total-kimg 88000 --gamma=100\npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\\n  --dataset=horse --total-kimg 100000 --gamma=100\n```\n\nFor other configurations, see `python run_training.py --help`.\n\nWe have verified that the results match the paper when training with 1, 2, 4, or 8 GPUs. Note that training FFHQ at 1024&times;1024 resolution requires GPU(s) with at least 16 GB of memory. The following table lists typical training times using NVIDIA DGX-1 with 8 Tesla V100 GPUs:\n\n| Configuration | Resolution      | Total kimg | 1 GPU   | 2 GPUs  | 4 GPUs  | 8 GPUs | GPU mem |\n| :------------ | :-------------: | :--------: | :-----: | :-----: | :-----: | :----: | :-----: |\n| `config-f`    | 1024&times;1024 | 25000      | 69d 23h | 36d 4h  | 18d 14h | 9d 18h | 13.3 GB |\n| `config-f`    | 1024&times;1024 | 10000      | 27d 23h | 14d 11h | 7d 10h  | 3d 22h | 13.3 GB |\n| `config-e`    | 1024&times;1024 | 25000      | 35d 11h | 18d 15h | 9d 15h  | 5d 6h  | 8.6 GB  |\n| `config-e`    | 1024&times;1024 | 10000      | 14d 4h  | 7d 11h  | 3d 20h  | 2d 3h  | 8.6 GB  |\n| `config-f`    | 256&times;256   | 25000      | 32d 13h | 16d 23h | 8d 21h  | 4d 18h | 6.4 GB  |\n| `config-f`    | 256&times;256   | 10000      | 13d 0h  | 6d 19h  | 3d 13h  | 1d 22h | 6.4 GB  |\n\nTraining curves for FFHQ config F (StyleGAN2) compared to original StyleGAN using 8 GPUs:\n\n![Training curves](./docs/stylegan2-training-curves.png)\n\nAfter training, the resulting networks can be used the same way as the official pre-trained networks:\n\n```.bash\n# Generate 1000 random images without truncation\npython run_generator.py generate-images --seeds=0-999 --truncation-psi=1.0 \\\n  --network=results/00006-stylegan2-ffhq-8gpu-config-f/networks-final.pkl\n```\n\n## Evaluation metrics\n\nTo reproduce the numbers for config F in Tables 1 and 3, run:\n\n```.bash\npython run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\\n  --metrics=fid50k,ppl_wend --dataset=ffhq --mirror-augment=true\npython run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-car-config-f.pkl \\\n  --metrics=fid50k,ppl2_wend --dataset=car\npython run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-cat-config-f.pkl \\\n  --metrics=fid50k,ppl2_wend --dataset=cat\npython run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-church-config-f.pkl \\\n  --metrics=fid50k,ppl2_wend --dataset=church\npython run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-horse-config-f.pkl \\\n  --metrics=fid50k,ppl2_wend --dataset=horse\n```\n\nFor other configurations, see the [StyleGAN2 Google Drive folder](https://drive.google.com/open?id=1QHc-yF5C3DChRwSdZKcx1w6K8JvSxQi7).\n\nNote that the metrics are evaluated using a different random seed each time, so the results will vary between runs. In the paper, we reported the average result of running each metric 10 times. The following table lists the available metrics along with their expected runtimes and random variation:\n\n| Metric      | FFHQ config F  | 1 GPU  | 2 GPUs  | 4 GPUs | Description |\n| :---------- | :------------: | :----: | :-----: | :----: | :---------- |\n| `fid50k`    | 2.84 &pm; 0.03 | 22 min | 14 min  | 10 min | [Fr&eacute;chet Inception Distance](https://arxiv.org/abs/1706.08500)\n| `is50k`     | 5.13 &pm; 0.02 | 23 min | 14 min  | 8 min  | [Inception Score](https://arxiv.org/abs/1606.03498)\n| `ppl_zfull` | 348.0 &pm; 3.8 | 41 min | 22 min  | 14 min | [Perceptual Path Length](https://arxiv.org/abs/1812.04948) in Z, full paths\n| `ppl_wfull` | 126.9 &pm; 0.2 | 42 min | 22 min  | 13 min | [Perceptual Path Length](https://arxiv.org/abs/1812.04948) in W, full paths\n| `ppl_zend`  | 348.6 &pm; 3.0 | 41 min | 22 min  | 14 min | [Perceptual Path Length](https://arxiv.org/abs/1812.04948) in Z, path endpoints\n| `ppl_wend`  | 129.4 &pm; 0.8 | 40 min | 23 min  | 13 min | [Perceptual Path Length](https://arxiv.org/abs/1812.04948) in W, path endpoints\n| `ppl2_wend` | 145.0 &pm; 0.5 | 41 min | 23 min  | 14 min | [Perceptual Path Length](https://arxiv.org/abs/1812.04948) without center crop\n| `ls`        | 154.2 / 4.27   | 10 hrs | 6 hrs   | 4 hrs  | [Linear Separability](https://arxiv.org/abs/1812.04948)\n| `pr50k3`    | 0.689 / 0.492  | 26 min | 17 min  | 12 min | [Precision and Recall](https://arxiv.org/abs/1904.06991)\n\nNote that some of the metrics cache dataset-specific data on the disk, and they will take somewhat longer when run for the first time.\n\n## License\n\nCopyright &copy; 2019, NVIDIA Corporation. All rights reserved.\n\nThis work is made available under the Nvidia Source Code License-NC. To view a copy of this license, visit https://nvlabs.github.io/stylegan2/license.html\n\n## Citation\n\n```\n@article{Karras2019stylegan2,\n  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},\n  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n  journal = {CoRR},\n  volume  = {abs/1912.04958},\n  year    = {2019},\n}\n```\n\n## Acknowledgements\n\nWe thank Ming-Yu Liu for an early review, Timo Viitanen for his help with code release, and Tero Kuosmanen for compute infrastructure.\n",
            "readme_url": "https://github.com/cto300/stylegan2",
            "frameworks": [
                "TensorFlow",
                "Keras",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Analyzing and Improving the Image Quality of StyleGAN",
            "arxiv": "1912.04958",
            "year": 2019,
            "url": "http://arxiv.org/abs/1912.04958v2",
            "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results\nin data-driven unconditional generative image modeling. We expose and analyze\nseveral of its characteristic artifacts, and propose changes in both model\narchitecture and training methods to address them. In particular, we redesign\nthe generator normalization, revisit progressive growing, and regularize the\ngenerator to encourage good conditioning in the mapping from latent codes to\nimages. In addition to improving image quality, this path length regularizer\nyields the additional benefit that the generator becomes significantly easier\nto invert. This makes it possible to reliably attribute a generated image to a\nparticular network. We furthermore visualize how well the generator utilizes\nits output resolution, and identify a capacity problem, motivating us to train\nlarger models for additional quality improvements. Overall, our improved model\nredefines the state of the art in unconditional image modeling, both in terms\nof existing distribution quality metrics as well as perceived image quality.",
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ]
        },
        {
            "title": "Improved Techniques for Training GANs",
            "arxiv": "1606.03498",
            "year": 2016,
            "url": "http://arxiv.org/abs/1606.03498v1",
            "abstract": "We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes.",
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ]
        },
        {
            "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
            "arxiv": "1706.08500",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.08500v6",
            "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images\nwith complex models for which maximum likelihood is infeasible. However, the\nconvergence of GAN training has still not been proved. We propose a two\ntime-scale update rule (TTUR) for training GANs with stochastic gradient\ndescent on arbitrary GAN loss functions. TTUR has an individual learning rate\nfor both the discriminator and the generator. Using the theory of stochastic\napproximation, we prove that the TTUR converges under mild assumptions to a\nstationary local Nash equilibrium. The convergence carries over to the popular\nAdam optimization, for which we prove that it follows the dynamics of a heavy\nball with friction and thus prefers flat minima in the objective landscape. For\nthe evaluation of the performance of GANs at image generation, we introduce the\n\"Fr\\'echet Inception Distance\" (FID) which captures the similarity of generated\nimages to real ones better than the Inception Score. In experiments, TTUR\nimproves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)\noutperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN\nBedrooms, and the One Billion Word Benchmark.",
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ]
        },
        {
            "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
            "arxiv": "1812.04948",
            "year": 2018,
            "url": "http://arxiv.org/abs/1812.04948v3",
            "abstract": "We propose an alternative generator architecture for generative adversarial\nnetworks, borrowing from style transfer literature. The new architecture leads\nto an automatically learned, unsupervised separation of high-level attributes\n(e.g., pose and identity when trained on human faces) and stochastic variation\nin the generated images (e.g., freckles, hair), and it enables intuitive,\nscale-specific control of the synthesis. The new generator improves the\nstate-of-the-art in terms of traditional distribution quality metrics, leads to\ndemonstrably better interpolation properties, and also better disentangles the\nlatent factors of variation. To quantify interpolation quality and\ndisentanglement, we propose two new, automated methods that are applicable to\nany generator architecture. Finally, we introduce a new, highly varied and\nhigh-quality dataset of human faces.",
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ]
        },
        {
            "title": "Improved Precision and Recall Metric for Assessing Generative Models",
            "arxiv": "1904.06991",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.06991v3",
            "abstract": "The ability to automatically estimate the quality and coverage of the samples\nproduced by a generative model is a vital requirement for driving algorithm\nresearch. We present an evaluation metric that can separately and reliably\nmeasure both of these aspects in image generation tasks by forming explicit,\nnon-parametric representations of the manifolds of real and generated data. We\ndemonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing\nseveral illustrative examples where existing metrics yield uninformative or\ncontradictory results. Furthermore, we analyze multiple design variants of\nStyleGAN to better understand the relationships between the model architecture,\ntraining methods, and the properties of the resulting sample distribution. In\nthe process, we identify new variants that improve the state-of-the-art. We\nalso perform the first principled analysis of truncation methods and identify\nan improved method. Finally, we extend our metric to estimate the perceptual\nquality of individual samples, and use this to study latent space\ninterpolations.",
            "authors": [
                "Tuomas Kynk\u00e4\u00e4nniemi",
                "Tero Karras",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ]
        },
        {
            "year": "2019",
            "volume": "abs/1912.04958",
            "journal": "CoRR",
            "author": [
                "Karras, Tero",
                "Laine, Samuli",
                "Aittala, Miika",
                "Hellsten, Janne",
                "Lehtinen, Jaakko",
                "Aila, Timo"
            ],
            "title": "Analyzing and Improving the Image Quality of {StyleGAN}",
            "ENTRYTYPE": "article",
            "ID": "Karras2019stylegan2",
            "authors": [
                "Karras, Tero",
                "Laine, Samuli",
                "Aittala, Miika",
                "Hellsten, Janne",
                "Lehtinen, Jaakko",
                "Aila, Timo"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "FFHQ"
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "MNIST"
            },
            {
                "name": "CelebA"
            },
            {
                "name": "One Billion Word"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.999982013469135,
        "task": "Image Generation",
        "task_prob": 0.9863360612717599
    }
}