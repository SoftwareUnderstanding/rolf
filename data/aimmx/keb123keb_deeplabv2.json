{
    "visibility": {
        "visibility": "public"
    },
    "name": "DeepLab v2",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "keb123keb",
                "owner_type": "User",
                "name": "deeplabv2",
                "url": "https://github.com/keb123keb/deeplabv2",
                "stars": 0,
                "pushed_at": "2018-07-14 18:49:14+00:00",
                "created_at": "2018-07-14 18:41:33+00:00",
                "language": null,
                "frameworks": []
            }
        ]
    },
    "authors": [
        {
            "name": "keb123keb",
            "github_id": "keb123keb"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/keb123keb/deeplabv2",
            "stars": 0,
            "issues": true,
            "readme": "## DeepLab v2\n\n### Introduction\n\nDeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).\n\nIt combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks, (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views, and (3) densely connected conditional random fields (CRF) as post processing.\n\nThis distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915).\nThis version also supports the experiments (DeepLab v1) in our ICLR'15. You only need to modify the old prototxt files. For example, our proposed atrous convolution is called dilated convolution in CAFFE framework, and you need to change the convolution parameter \"hole\" to \"dilation\" (the usage is exactly the same). For the experiments in ICCV'15, there are some differences between our argmax and softmax_loss layers and Caffe's. Please refer to [DeepLabv1](https://bitbucket.org/deeplab/deeplab-public/) for details.\n\nPlease consult and consider citing the following papers:\n\n    @article{CP2016Deeplab,\n      title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},\n      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n      journal={arXiv:1606.00915},\n      year={2016}\n    }\n\n    @inproceedings{CY2016Attention,\n      title={Attention to Scale: Scale-aware Semantic Image Segmentation},\n      author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille},\n      booktitle={CVPR},\n      year={2016}\n    }\n\n    @inproceedings{CB2016Semantic,\n      title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform},\n      author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille},\n      booktitle={CVPR},\n      year={2016}\n    }\n\n    @inproceedings{PC2015Weak,\n      title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation},\n      author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille},\n      booktitle={ICCV},\n      year={2015}\n    }\n\n    @inproceedings{CP2015Semantic,\n      title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs},\n      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n      booktitle={ICLR},\n      year={2015}\n    }\n\n\nNote that if you use the densecrf implementation, please consult and cite the following paper:\n\n    @inproceedings{KrahenbuhlK11,\n      title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},\n      author={Philipp Kr{\\\"{a}}henb{\\\"{u}}hl and Vladlen Koltun},\n      booktitle={NIPS},\n      year={2011}\n    }\n\n### Performance\n\n*DeepLabv2* currently achieves **79.7%** on the challenging PASCAL VOC 2012 semantic image segmentation task -- see the [leaderboard](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6). \n\nPlease refer to our project [website](http://liangchiehchen.com/projects/DeepLab.html) for details.\n\n### Pre-trained models\n\nWe have released several trained models and corresponding prototxt files at [here](http://liangchiehchen.com/projects/DeepLab_Models.html). Please check it for more model details.\n\n### Experimental set-up\n\n1. The scripts we used for our experiments can be downloaded from this [link](https://ucla.box.com/s/4grlj8yoodv95936uybukjh5m0tdzvrf):\n    1. run_pascal.sh: the script for training/testing on the PASCAL VOC 2012 dataset. __Note__ You also need to download sub.sed script.\n    2. run_densecrf.sh and run_densecrf_grid_search.sh: the scripts we used for post-processing the DCNN computed results by DenseCRF.\n2. The image list files used in our experiments can be downloaded from this [link](https://ucla.box.com/s/rd9z2xvwsfpksi7mi08i2xqrj7ab4keb):\n    * The zip file stores the list files for the PASCAL VOC 2012 dataset.\n3. To use the mat_read_layer and mat_write_layer, please download and install [matio](http://sourceforge.net/projects/matio/files/matio/1.5.2/).\n\n### FAQ\n\nCheck [FAQ](http://liangchiehchen.com/projects/DeepLab_FAQ.html) if you have some problems while using the code.\n\n### How to run DeepLab\n\nThere are several variants of DeepLab. To begin with, we suggest DeepLab-LargeFOV, which has good performance and faster training time.\n\nSuppose the codes are located at deeplab/code\n\n1. mkdir deeplab/exper (Create a folder for experiments)\n2. mkdir deeplab/exper/voc12 (Create a folder for your specific experiment. Let's take PASCAL VOC 2012 for example.)\n3. Create folders for config files and so on.\n    1. mkdir deeplab/exper/voc12/config  (where network config files are saved.)\n    2. mkdir deeplab/exper/voc12/features  (where the computed features will be saved (when train on train))\n    3. mkdir deeplab/exper/voc12/features2 (where the computed features will be saved (when train on trainval))\n    4. mkdir deeplab/exper/voc12/list (where you save the train, val, and test file lists)\n    5. mkdir deeplab/exper/voc12/log (where the training/test logs will be saved)\n    6. mkdir deeplab/exper/voc12/model (where the trained models will be saved)\n    7. mkdir deeplab/exper/voc12/res (where the evaluation results will be saved)\n4. mkdir deeplab/exper/voc12/config/deeplab_largeFOV (test your own network. Create a folder under config. For example, deeplab_largeFOV is the network you want to experiment with. Add your train.prototxt and test.prototxt in that folder (you can check some provided examples for reference).)\n5. Set up your init.caffemodel at deeplab/exper/voc12/model/deeplab_largeFOV. You may want to soft link init.caffemodel to the modified VGG-16 net. For example, run \"ln -s vgg16.caffemodel init.caffemodel\" at voc12/model/deeplab_largeFOV.\n6. Modify the provided script, run_pascal.sh, for experiments. You should change the paths according to your setting. For example, you should specify where the caffe is by changing CAFFE_DIR. Note You may need to modify sub.sed, if you want to replace some variables with your desired values in train.prototxt or test.prototxt.\n7. The computed features are saved at folders features or features2, and you can run provided MATLAB scripts to evaluate the results (e.g., check the script at code/matlab/my_script/EvalSegResults).\n\n### Python\n\nSeyed Ali Mousavi has implemented a python version of run_pascal.sh (Thanks, Ali!). If you are more familiar with Python, you may want to take a look at [this](https://github.com/TheLegendAli/CCVL).",
            "readme_url": "https://github.com/keb123keb/deeplabv2",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
            "arxiv": "1606.00915",
            "year": 2016,
            "url": "http://arxiv.org/abs/1606.00915v2",
            "abstract": "In this work we address the task of semantic image segmentation with Deep\nLearning and make three main contributions that are experimentally shown to\nhave substantial practical merit. First, we highlight convolution with\nupsampled filters, or 'atrous convolution', as a powerful tool in dense\nprediction tasks. Atrous convolution allows us to explicitly control the\nresolution at which feature responses are computed within Deep Convolutional\nNeural Networks. It also allows us to effectively enlarge the field of view of\nfilters to incorporate larger context without increasing the number of\nparameters or the amount of computation. Second, we propose atrous spatial\npyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP\nprobes an incoming convolutional feature layer with filters at multiple\nsampling rates and effective fields-of-views, thus capturing objects as well as\nimage context at multiple scales. Third, we improve the localization of object\nboundaries by combining methods from DCNNs and probabilistic graphical models.\nThe commonly deployed combination of max-pooling and downsampling in DCNNs\nachieves invariance but has a toll on localization accuracy. We overcome this\nby combining the responses at the final DCNN layer with a fully connected\nConditional Random Field (CRF), which is shown both qualitatively and\nquantitatively to improve localization performance. Our proposed \"DeepLab\"\nsystem sets the new state-of-art at the PASCAL VOC-2012 semantic image\nsegmentation task, reaching 79.7% mIOU in the test set, and advances the\nresults on three other datasets: PASCAL-Context, PASCAL-Person-Part, and\nCityscapes. All of our code is made publicly available online.",
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Iasonas Kokkinos",
                "Kevin Murphy",
                "Alan L. Yuille"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "PASCAL VOC 2012"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "PASCAL-Person-Part"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999687744228305,
        "task": "Semantic Segmentation",
        "task_prob": 0.9913577157579737
    }
}