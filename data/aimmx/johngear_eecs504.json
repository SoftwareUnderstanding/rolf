{
    "visibility": {
        "visibility": "public"
    },
    "name": "EECS504 Final Project",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "johngear",
                "owner_type": "User",
                "name": "eecs504",
                "url": "https://github.com/johngear/eecs504",
                "stars": 0,
                "pushed_at": "2020-12-14 09:44:25+00:00",
                "created_at": "2020-12-03 02:09:28+00:00",
                "language": "Jupyter Notebook",
                "description": "team Privacy Advocates ",
                "frameworks": [
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "64ffbde158b25553605c1703597341950423c227",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/.gitignore"
                    }
                },
                "size": 22
            },
            {
                "type": "code",
                "name": "DSFDDetect.py",
                "sha": "fb484c03a0e71d9ae36ac6a9549a3f34259a93c0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/DSFDDetect.py"
                    }
                },
                "size": 847
            },
            {
                "type": "code",
                "name": "Detection Accuracy.ipynb",
                "sha": "bee39a4354eddc0401bdc4eac36e85633d344be9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/Detection Accuracy.ipynb"
                    }
                },
                "size": 5481832
            },
            {
                "type": "code",
                "name": "Images",
                "sha": "6810dd9f06fec0424b77aff0398660128a579dc0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/tree/main/Images"
                    }
                },
                "num_files": 61
            },
            {
                "type": "code",
                "name": "cascade.pkl",
                "sha": "341aeb74af83ee52e4abd2da3b6ff5f6af7b11b6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/cascade.pkl"
                    }
                },
                "size": 12191
            },
            {
                "type": "code",
                "name": "cascade.py",
                "sha": "6b5855f5e5598cadb63510bb0bc4fd33ddd09e74",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/cascade.py"
                    }
                },
                "size": 1959
            },
            {
                "type": "code",
                "name": "checkpoints",
                "sha": "383e9c370a6669a23cb07972252b0b2b6f3fc480",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/tree/main/checkpoints"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "custom_cascade.py",
                "sha": "4d7c2342beacf243860775d5f2f9ca9680d82909",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/custom_cascade.py"
                    }
                },
                "size": 981
            },
            {
                "type": "code",
                "name": "custom_cnn.py",
                "sha": "a2dc6c40efc7a59a2ce8f3f0fc75975153b8f47f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/custom_cnn.py"
                    }
                },
                "size": 1792
            },
            {
                "type": "code",
                "name": "haarcascade_eye.xml",
                "sha": "b21e3b93d74b5130b5a1323be9fc46017ab0e8c7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/haarcascade_eye.xml"
                    }
                },
                "size": 341406
            },
            {
                "type": "code",
                "name": "haarcascade_frontalface_default.xml",
                "sha": "cbd1aa89e927d8d54b49fe666bf17244c3c46a7b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/haarcascade_frontalface_default.xml"
                    }
                },
                "size": 930127
            },
            {
                "type": "code",
                "name": "lightface.ipynb",
                "sha": "f83e49f9a4b0fe03a256497af2076a1998cb6fee",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/lightface.ipynb"
                    }
                },
                "size": 8025
            },
            {
                "type": "code",
                "name": "messi.jpg",
                "sha": "31d01e9af4b7759d58e48dc47786e19325fd9104",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/messi.jpg"
                    }
                },
                "size": 59799
            },
            {
                "type": "code",
                "name": "messi2.jpeg",
                "sha": "8fded1991a3b948b39afb7248444a349be079e82",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/messi2.jpeg"
                    }
                },
                "size": 76262
            },
            {
                "type": "code",
                "name": "project_starter_code.ipynb",
                "sha": "a21ca5daee20ca9767cc8f5cb32ec08229d52774",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/project_starter_code.ipynb"
                    }
                },
                "size": 330137
            },
            {
                "type": "code",
                "name": "strict_cascade.py",
                "sha": "757d0d8850c8627f1f9ebd27e39d800cf9a03fe4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/strict_cascade.py"
                    }
                },
                "size": 1508
            },
            {
                "type": "code",
                "name": "validation_clean.ipynb",
                "sha": "5fa9591b333a227675f6cf5eae0cdbce3105b1bb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/validation_clean.ipynb"
                    }
                },
                "size": 339575
            },
            {
                "type": "code",
                "name": "validation_functions.py",
                "sha": "13586f1d3e59c8e5c83604ab2290c75be3f4b4a2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/validation_functions.py"
                    }
                },
                "size": 8508
            },
            {
                "type": "code",
                "name": "viola_jones.py",
                "sha": "fb3acba83f9c206a5075d17edb03b1c6d706207c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/viola_jones.py"
                    }
                },
                "size": 13945
            }
        ]
    },
    "trained_model": {
        "binaries": [
            {
                "type": "binary",
                "name": "deploy.prototxt",
                "sha": "905580ee46dcb9f0b8ac3703987e561eb4245a42",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/deploy.prototxt"
                    }
                },
                "size": 28092
            },
            {
                "type": "binary",
                "name": "res10_300x300_ssd_iter_140000.caffemodel",
                "sha": "809dfd7877c8774d36424ad71ed9a7830ce3c3ad",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/johngear/eecs504/blob/main/res10_300x300_ssd_iter_140000.caffemodel"
                    }
                },
                "size": 10666211
            }
        ]
    },
    "authors": [
        {
            "name": "17wegienkaj",
            "github_id": "17wegienkaj"
        },
        {
            "name": "thomcope",
            "github_id": "thomcope"
        },
        {
            "name": "johngear",
            "github_id": "johngear"
        },
        {
            "name": "GetShiggyWithIt",
            "github_id": "GetShiggyWithIt"
        }
    ],
    "tags": [],
    "description": "team Privacy Advocates ",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/johngear/eecs504",
            "stars": 0,
            "issues": true,
            "readme": "# EECS504 Final Project\nteam:     Privacy Advocates\n\nproject:  Facial Anonymization in Video \n\ndue:      December 14, 2020\n\n## List of Materials used for reference\nResidual NN for Images: https://arxiv.org/abs/1512.03385\n\nDocumentation for how OpenCV NN was trained: https://github.com/opencv/opencv/blob/master/samples/dnn/face_detector/how_to_train_face_detector.txt\n\nSqueezeNet: https://arxiv.org/pdf/1602.07360.pdf   https://github.com/forresti/SqueezeNet  TF Implement: https://github.com/vonclites/squeezenet\n\nEdgeNet is a small sized SqueezeNet-like architecture with FPGA implementation. Sort of useless other than proof of concept. Used on drones for edge computing. https://ieeexplore-ieee-org.proxy.lib.umich.edu/document/8617876\n\nZynqNet seems like a different variant of EdgeNet. https://arxiv.org/pdf/2005.06892.pdf\n\nDataset Used: WIDER Face: A Face Detection Benchmark (Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n\n\n\n## From the submitted project proposal:\n\n### Description \nOur project will process videos that contain human faces and return video with all facial features removed, either by performing a blur with randomized parameters, or by omitting facial pixels all together. We will likely do this by training a convolution neural network with a dataset used for video facial recognition, such as \u2018Youtube Faces with Facial Keypoints\u2019 found here. \n\nExisting video editing software has blurring functionality, but the user often has to select the features, and it\u2019s unclear whether deblurring could reveal the identity after-the-fact. There are a few papers and similar projects available online that have demonstrated such work, such as this research paper, the following two articles, and the work of Terrance Boult and Walter Schierer.\n\nIf time and project complexity allow, an additional portion of the project could be examining feasibility of an on-device-algorithm that could be used on a camera so there was no back-door to deanonymize the data. \n\n### Demo\nWe hope to provide side-by-side video of before and after the algorithm runs on a variety of scenes containing people. It would be cool to implement it so that we could run it on live video, but achieving this level of efficiency with our methods may not be feasible. \n",
            "readme_url": "https://github.com/johngear/eecs504",
            "frameworks": [
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
            "arxiv": "1602.07360",
            "year": 2016,
            "url": "http://arxiv.org/abs/1602.07360v4",
            "abstract": "Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet",
            "authors": [
                "Forrest N. Iandola",
                "Song Han",
                "Matthew W. Moskewicz",
                "Khalid Ashraf",
                "William J. Dally",
                "Kurt Keutzer"
            ]
        },
        {
            "title": "ZynqNet: An FPGA-Accelerated Embedded Convolutional Neural Network",
            "arxiv": "2005.06892",
            "year": 2020,
            "url": "http://arxiv.org/abs/2005.06892v1",
            "abstract": "Image Understanding is becoming a vital feature in ever more applications\nranging from medical diagnostics to autonomous vehicles. Many applications\ndemand for embedded solutions that integrate into existing systems with tight\nreal-time and power constraints. Convolutional Neural Networks (CNNs) presently\nachieve record-breaking accuracies in all image understanding benchmarks, but\nhave a very high computational complexity. Embedded CNNs thus call for small\nand efficient, yet very powerful computing platforms. This master thesis\nexplores the potential of FPGA-based CNN acceleration and demonstrates a fully\nfunctional proof-of-concept CNN implementation on a Zynq System-on-Chip. The\nZynqNet Embedded CNN is designed for image classification on ImageNet and\nconsists of ZynqNet CNN, an optimized and customized CNN topology, and the\nZynqNet FPGA Accelerator, an FPGA-based architecture for its evaluation.\nZynqNet CNN is a highly efficient CNN topology. Detailed analysis and\noptimization of prior topologies using the custom-designed Netscope CNN\nAnalyzer have enabled a CNN with 84.5% top-5 accuracy at a computational\ncomplexity of only 530 million multiplyaccumulate operations. The topology is\nhighly regular and consists exclusively of convolutional layers, ReLU\nnonlinearities and one global pooling layer. The CNN fits ideally onto the FPGA\naccelerator. The ZynqNet FPGA Accelerator allows an efficient evaluation of\nZynqNet CNN. It accelerates the full network based on a nested-loop algorithm\nwhich minimizes the number of arithmetic operations and memory accesses. The\nFPGA accelerator has been synthesized using High-Level Synthesis for the Xilinx\nZynq XC-7Z045, and reaches a clock frequency of 200MHz with a device\nutilization of 80% to 90 %.",
            "authors": [
                "David Gschwend"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.999915977150527,
        "task": "Object Detection",
        "task_prob": 0.6700714259648874
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "COCO"
            }
        ]
    }
}