{
    "visibility": {
        "visibility": "public",
        "license": "BSD 3-Clause \"New\" or \"Revised\" License"
    },
    "name": "Overlapping with language modelling and emotion detection",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "nkcr",
                "owner_type": "User",
                "name": "overlap-ml",
                "url": "https://github.com/nkcr/overlap-ml",
                "stars": 3,
                "pushed_at": "2022-02-10 00:31:47+00:00",
                "created_at": "2019-01-01 22:28:47+00:00",
                "language": "Python",
                "description": "Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes - CoNLL 2019",
                "license": "BSD 3-Clause \"New\" or \"Revised\" License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "dfe0770424b2a19faf507a501ebfc23be8f54e7b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/blob/master/.gitattributes"
                    }
                },
                "size": 66
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "0f4005bd5329f91435b539a8423c0c8402b77030",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/blob/master/.gitignore"
                    }
                },
                "size": 1189
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "bdfff0ec1c955002f5f729e983f0997d02773929",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/blob/master/LICENSE"
                    }
                },
                "size": 1515
            },
            {
                "type": "code",
                "name": "awd",
                "sha": "0964697ddb53c4e760f11e5bd1a1021c4ae0877a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/tree/master/awd"
                    }
                },
                "num_files": 14
            },
            {
                "type": "code",
                "name": "common",
                "sha": "99a1b362a40ffc8aa9a3b6aae2324960ba665b33",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/tree/master/common"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "docker",
                "sha": "236b045d20d39efdc5f7ee0aae0049f0af9525bc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/tree/master/docker"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "emotions",
                "sha": "19adf0036846e9f04cf64f1183833d23cb1aae91",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/tree/master/emotions"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "get_data.sh",
                "sha": "bd3716c42900936cd588f1219f61d0b57196bb17",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/blob/master/get_data.sh"
                    }
                },
                "size": 638
            },
            {
                "type": "code",
                "name": "main_run.py",
                "sha": "92a90b9111a01b2b3048c51d8cedca18540a0209",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/blob/master/main_run.py"
                    }
                },
                "size": 23067
            },
            {
                "type": "code",
                "name": "mos",
                "sha": "7bca6f742f2780f6849911aee89d2d4d0a71508f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/tree/master/mos"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "poster-conll.pdf",
                "sha": "a4bea0e33d698eeb69a61862d43a09daf86d0dfb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/blob/master/poster-conll.pdf"
                    }
                },
                "size": 1439801
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "eb2e96f210e979c3578d891ad896573057b2e8cd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/blob/master/requirements.txt"
                    }
                },
                "size": 171
            },
            {
                "type": "code",
                "name": "simple",
                "sha": "206d5fdaef9e6efb3e1a0251195cbd4896d48169",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nkcr/overlap-ml/tree/master/simple"
                    }
                },
                "num_files": 3
            }
        ]
    },
    "authors": [
        {
            "name": "No\u00e9mien Kocher",
            "github_id": "nkcr"
        },
        {
            "name": "christian-5-28",
            "email": "christian.sciuto@epfl.ch",
            "github_id": "christian-5-28"
        },
        {
            "name": "lorenzotara",
            "github_id": "lorenzotara"
        }
    ],
    "tags": [
        "pytorch",
        "natural-language-processing",
        "language-modeling",
        "deep-learning"
    ],
    "description": "Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes - CoNLL 2019",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/nkcr/overlap-ml",
            "stars": 3,
            "issues": true,
            "readme": "# Overlapping with language modelling and emotion detection\n\nPytorch implementation to reproduce experiments from \"[Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes](https://arxiv.org/abs/1909.08700)\" - ([poster](poster-conll.pdf)).\n\nIf you use this code or our results in your research, please cite as appropriate:\n\n```\n@inproceedings{kocher-etal-2019-alleviating,\n    title = \"Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes\",\n    author = \"Kocher, No{\\'e}mien  and\n      Scuito, Christian  and\n      Tarantino, Lorenzo  and\n      Lazaridis, Alexandros  and\n      Fischer, Andreas  and\n      Musat, Claudiu\",\n    booktitle = \"Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)\",\n    month = nov,\n    year = \"2019\",\n    address = \"Hong Kong, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/K19-1083\",\n    doi = \"10.18653/v1/K19-1083\",\n    pages = \"890--899\",\n}\n```\n\nThis repo holds experiments on 4 models using the \"overlapping\" method:\n\n- **awd**, [AWD](https://arxiv.org/abs/1708.02182) ASGD Weight-Dropped LSTM, (`/awd`)\n- **text simple**, a very basic lstm for language modelling, (`/simple`)\n- **mos**, [MOS](https://arxiv.org/abs/1711.03953) Mixture of Softmaxes, (`/mos`)\n- **voice simple**, a very basic LSTM for emotion detection on voice, (`/emotions`)\n\nTo specify which model to run, use `--main-model {simple-lstm | awd-lstm |\u00a0mos-lstm | emotions-simple-lstm}` argument. There are additional common paramaters, as well as specific parameters for each model. Those can be found in `main_run.py`.\n\nThe taxonomy in the code may differe a bit from the paper, especially regarding the type of experiments. Here is the corresponding terms:\n\n|In the code|In the paper|\n|-----------|------------|\n|No order|Extreme TOI|\n|Local order|Inter-batch TOI|\n|Standard order|Standard TOI|\n|Total order (P)|Alleviated TOI (P)|\n\nExperiments were run on a Tesla P100 GPU. Results are very likely to differ based on the GPU used.\n\n## Set-up\n\nDownload the data (PTB, WT2, WT103):\n\n```bash\nchmod +x get_data.sh\n./get_data.sh\n```\n\nFor emotions, add in `data/IEMOCAP/` the `all_features_cv` files.\n\nWe use python `3.6` with Pytorch `0.4.1`. To create a new python environement and install dependencies, run:\n\n```bash\npython3 -m virtualenv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\n```\n\nYou can check your setup by launching a quick training over one epoch with the following command:\n\n```bash\npython3 main_run.py --main-model awd-lstm --batch-size 20 --data data/penn --epochs 1 --nhid 5 --emsize 5 --nlayers 1 --bptt 5\n```\n\nThe program should exit without error and write the logs in the `logs/` folder. You can watch the logs with tensorboard by launching the following command:\n\n```bash\ntensorboard --logdir logs/\n```\n\n## About the files\n\n`main_run.py` is the main entry point that parses arguments, does the global initialization and runs the corresponding model and task.\n\n`awd/`, `emotions/`, `mos/` and `simple/` are the different models directories. `common/` holds the common initilization and utilities, such as the different data iterators, which are in the `DataSelector` class in `common/excavator.py`.\n\nThe `main_run.py` file, after performing the common initilizations, imports the `main.py` file corresponding to the choosen model.\n\n\n# Commands to reproduce the experiments\n\n**Note**: Those results do not use prime batch size, but the default parameters. To have better results, adapt the `--batch-size` param to the closest prime number.\n\n**Quick anchors navigation**:\n\n<table>\n    <tr>\n        <th>Model</th><th>Dataset</th><th>Experiments</th>\n    </tr>\n    <tr>\n        <td rowspan=\"3\">AWD</td>\n        <td>PTB</td>\n        <td><a href=\"#awd-ptb\">Extreme / Inter-batch / Original / Alleviated TOI</a></td>\n    </tr>\n        <td>WT2</td>\n        <td><a href=\"#awd-wt2\">Extreme / Inter-batch / Original / Alleviated TOI</a></td>\n    </tr>\n    <tr>\n        <td>WT103</td>\n        <td><a href=\"#awd-wt103\">Extreme / Inter-batch / Original / Alleviated TOI</a></td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">Text simple LSTM</td>\n        <td>PTB</td>\n        <td><a href=\"#simple-ptb\">Extreme / Inter-batch / Original / Alleviated TOI</a></td>\n    </tr>\n        <td>WT2</td>\n        <td><a href=\"#simple-wt2\">Extreme / Inter-batch / Original / Alleviated TOI</a></td>\n    </tr>\n    <tr>\n        <td rowspan=\"1\">MOS</td>\n        <td>PTB</td>\n        <td><a href=\"#mos-ptb\">Original / Alleviated TOI</a></td>\n    </tr>\n    <tr>\n        <td rowspan=\"1\">Voice simple LSTM</td>\n        <td>IEMOCAP</td>\n        <td><a href=\"#voice-simple-lstm\">Extreme / Inter-batch / Original / Alleviated TOI</a></td>\n    </tr>\n</table>\n\n## AWD PTB\n\n**Extreme TOI**:\n\nExpected results: `66.38` / `63.49` (validation / testing)\n\n```bash\npython3 main_run.py --main-model awd-lstm --batch-size 20 --data data/penn --dropouti 0.4 --dropouth 0.25 --seed 141 --seed-shuffle 141 --epochs 1000 --shuffle-full-seq\n```\n\n**Inter-batch TOI**:\n\nExpected results: `66.96` / `64.20` (validation / testing)\n\n```bash\npython3 main_run.py --main-model awd-lstm --batch-size 20 --data data/penn --dropouti 0.4 --dropouth 0.25 --seed 141 --seed-shuffle 141 --epochs 1000 --shuffle-row-seq\n```\n\n**Standard TOI**:\n\nExpected results: `61.28` / `58.94` (validation / testing)\n\n```bash\npython3 main_run.py --main-model awd-lstm --batch-size 20 --data data/penn --dropouti 0.4 --dropouth 0.25 --seed 141 --epochs 1000\n```\n\n**Alleviated TOI {2,5,7,10}**:\n\nExpected results (validation / testing): \n\n* 2: `61.73` / `59.37`\n* 5: `63.37` / `60.50`\n* 7: `59.22` / `56.7`\n* 10: `68.09` / `65.88`\n\n```bash\noverlaps=(2 5 7 10)\nepochs=1000\nfor k in \"${overlaps[@]}\"\ndo\n    :\n    python3 main_run.py --main-model awd-lstm --batch-size 20 --data data/penn --dropouti 0.4 --dropouth 0.25 --seed 141 --epochs \"$(($epochs/$k))\" --init-seq \"overlapCN_${k}\"\n    sleep 10\ndone\n```\n\n\ud83d\udca5 With a prime batch size:\n\nExpected results (validation / testing): \n\n* 2: `60.56` / `57.97`\n* 5: `59.52` / `57.14`\n* 7: `59.43` / `57.16`\n* 10: `58.96` / **`56.46`**\n\n```bash\noverlaps=(2 5 7 10)\nepochs=1000\nfor k in \"${overlaps[@]}\"\ndo\n    :\n    python3 main_run.py --main-model awd-lstm --batch-size 19 --data data/penn --dropouti 0.4 --dropouth 0.25 --seed 141 --epochs \"$(($epochs/$k))\" --init-seq \"overlapCN_${k}\"\n    sleep 10\ndone\n```\n\n## AWD WT2\n\n**Extreme TOI**\n\nExpected results: `77.14` / `73.52` (validation / testing)\n\n```bash\npython3 main_run.py --main-model awd-lstm --epochs 750 --data /data/noemien.kocher/datasets/wikitext-2 --dropouth 0.2 --seed 1882 --batch-size 80 --shuffle-full-seq\n```\n\n**Inter-batch TOI**\n\nExpected results: `76.08` / `72.61` (validation / testing)\n\n```bash\npython main_run.py --main-model awd-lstm --epochs 750 --data /data/noemien.kocher/datasets/wikitext-2 --dropouth 0.2 --seed 1882 --batch-size 80 --shuffle-row-seq\n```\n\n**Standard TOI**\n\nExpected results: `68.50` / `65.86` (validation / testing)\n\n```bash\npython3 main_run.py --main-model awd-lstm --epochs 750 --data /data/noemien.kocher/datasets/wikitext-2 --dropouth 0.2 --seed 1882 --batch-size 80\n```\n\n**Alleviated TOI {2,5,7,10}**\n\nExpected results (validation / testing): \n\n* 2: `68.56` / `65.51`\n* 5: `69.56` / `66.33`\n* 7: `67.48` / `64.87`\n* 10: `72.95` / `69.69`\n\n```bash\noverlaps=(2 5 7 10)\nepochs=750\nfor k in \"${overlaps[@]}\"\ndo\n    :\n    python3 main_run.py --main-model awd-lstm --data /data/noemien.kocher/datasets/wikitext-2 --dropouth 0.2 --seed 1882 --batch-size 80 --epochs \"$(($epochs/$k))\" --init-seq \"overlapCN_${k}\"\n    sleep 10\ndone\n```\n\n\ud83d\udca5 With a prime batch size:\n\nExpected results (validation / testing): \n\n* 2: `68.11` / `65.14`\n* 5: `67.74` / `65.11`\n* 7: `67.79` / `64.79`\n* 10: `67.47` / **`64.73`**\n\n```bash\noverlaps=(2 5 7 10)\nepochs=750\nfor k in \"${overlaps[@]}\"\ndo\n    :\n    python3 main_run.py --main-model awd-lstm --data /data/noemien.kocher/datasets/wikitext-2 --dropouth 0.2 --seed 1882 --batch-size 79 --epochs \"$(($epochs/$k))\" --init-seq \"overlapCN_${k}\"\n    sleep 10\ndone\n```\n\n## AWD WT103\n\n**Extreme TOI**\n\nExpected results: `35.22` / `36.19` (validation / testing)\n\n```bash\npython3 -u main_run.py --main-model awd-lstm --epochs 14 --nlayers 4 --emsize 400 --nhid 2500 --alpha 0 --beta 0 --dropoute 0 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --wdrop 0 --wdecay 0 --bptt 140 --batch-size 60 --optimizer adam --lr 1e-3 --data /data/noemien.kocher/datasets/wikitext-103 --when 12 --model QRNN --shuffle-full-seq\n```\n\n**Inter-batch TOI**\n\nExpected results: `35.41` / `36.39` (validation / testing)\n\n```bash\npython3 -u main_run.py --main-model awd-lstm --epochs 14 --nlayers 4 --emsize 400 --nhid 2500 --alpha 0 --beta 0 --dropoute 0 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --wdrop 0 --wdecay 0 --bptt 140 --batch-size 60 --optimizer adam --lr 1e-3 --data /data/noemien.kocher/datasets/wikitext-103 --when 12 --model QRNN --shuffle-row-seq\n```\n\n**Standard TOI**\n\nExpected results: `32.18` / `32.94` (validation / testing)\n\n```bash\npython3 -u main_run.py --main-model awd-lstm --epochs 14 --nlayers 4 --emsize 400 --nhid 2500 --alpha 0 --beta 0 --dropoute 0 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --wdrop 0 --wdecay 0 --bptt 140 --batch-size 60 --optimizer adam --lr 1e-3 --data /data/noemien.kocher/datasets/wikitext-103 --when 12 --model QRNN\n```\n\n**Alleviated TOI {2,5,7,10}**\n\nExpected results (validation / testing): \n\n* 2: `36.94` / `34.31`\n* 5: `38.50` / `40.04`\n* 7: `31.78` / `32.72`\n* 10: `48.28` / `49.49`\n\n```bash\n# base num epochs is 14\noverlaps=(2 5 7 10)\nwhen_steps=147456\nmax_steps=172032\nfor i in \"${!overlaps[@]}\"\ndo\n        :\n        python3 -u main_run.py --main-model awd-lstm --epochs 14 --nlayers 4 --emsize 400 --nhid 2500 --alpha 0 --beta 0 --dropoute 0 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --wdrop 0 --wdecay 0 --bptt 140 --batch-size 60 --optimizer adam --lr 1e-3 --data /data/noemien.kocher/datasets/wikitext-103 --when-steps \"$when_steps\" --model QRNN --init-seq \"overlapCN_${overlaps[$i]}\" --log-dir /data/noemien.kocher/logs/ --max-steps \"$max_steps\"\n        sleep 10\ndone\n```\n\n\ud83d\udca5 With a prime batch size:\n\nExpected results (validation / testing): \n\n* 2: `32.00` / `32.98`\n* 5: `31.93` / `33.07`\n* 7: `31.78` / `32.89`\n* 10: `31.92` / **`32.85`**\n\n```bash\n# base num epochs is 14\noverlaps=(2 5 7 10)\nwhen_steps=147456\nmax_steps=172032\nfor i in \"${!overlaps[@]}\"\ndo\n        :\n        python3 -u main_run.py --main-model awd-lstm --epochs 14 --nlayers 4 --emsize 400 --nhid 2500 --alpha 0 --beta 0 --dropoute 0 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --wdrop 0 --wdecay 0 --bptt 140 --batch-size 59 --optimizer adam --lr 1e-3 --data /data/noemien.kocher/datasets/wikitext-103 --when-steps \"$when_steps\" --model QRNN --init-seq \"overlapCN_${overlaps[$i]}\" --log-dir /data/noemien.kocher/logs/ --max-steps \"$max_steps\"\n        sleep 10\ndone\n```\n\n## Simple PTB\n\n**Extreme TOI**:\n\nExpected results: `81.97` / `79.08` (validation / testing)\n\n```bash\npython3 main_run.py --main-model simple-lstm --epochs 100 --batch-size 20 --dropout 0.15 --nlayers 2 --bptt 70 --nhid 1500 --lr-decay 1 --shuffle-full-seq\n```\n\n**Inter-batch TOI**:\n\nExpected results: `81.67` / `78.59` (validation / testing)\n\n```bash\npython3 main_run.py --main-model simple-lstm --epochs 100 --batch-size 20 --dropout 0.15 --nlayers 2 --bptt 70 --nhid 1500 --lr-decay 1 --shuffle-row-seq\n```\n\n**Standard TOI**:\n\nExpected results: `77.54` / `75.36` (validation / testing)\n\n```bash\npython3 main_run.py --main-model simple-lstm --epochs 100 --batch-size 20 --dropout 0.15 --nlayers 2 --bptt 70 --nhid 1500 --lr-decay 1\n```\n\n**Alleviated TOI {2,5,7,10}**:\n\nExpected results (validation / testing): \n\n* 2: `78.48` / `76.55`\n* 5: `91.95` / `89.64`\n* 7: `77.47` / `74.98`\n* 10: `92.92` / `92.07`\n\n```bash\noverlaps=(2 5 7 10)\nepochs=100\nfor k in \"${overlaps[@]}\"\ndo\n    :\n    python3 main_run.py --main-model simple-lstm --epochs \"$(($epochs/$k))\" --batch-size 20 --dropout 0.15 --nlayers 2 --bptt 70 --nhid 1500 --lr-decay 1 --init-seq \"overlapCN_${k}\"\n    sleep 10\ndone\n```\n\n## Simple WT2\n\n**Extreme TOI**\n\nExpected results: `101.3` / `96.08` (validation / testing)\n\n```bash\npython3 main_run.py --main-model simple-lstm --epochs 100 --batch-size 80 --dropout 0.15 --nlayers 2 --bptt 70 --nhid 1150 --lr-decay 1 --data /data/noemien.kocher/datasets/wikitext-2 --shuffle-full-seq\n```\n\n**Inter-batch TOI**\n\nExpected results: `101.7` / `96.89` (validation / testing)\n\n```bash\npython3 main_run.py --main-model simple-lstm --epochs 100 --batch-size 80 --dropout 0.15 --nlayers 2 --bptt 70 --nhid 1150 --lr-decay 1 --data /data/noemien.kocher/datasets/wikitext-2 --shuffle-row-seq\n```\n\n**Standard TOI**\n\nExpected results: `98.85` / `93.15` (validation / testing)\n\n```bash\npython3 main_run.py --main-model simple-lstm --epochs 100 --batch-size 80 --dropout 0.15 --nlayers 2 --bptt 70 --nhid 1150 --lr-decay 1 --data /data/noemien.kocher/datasets/wikitext-2\n```\n\n**Alleviated TOI {2,5,7,10}**\n\nExpected results (validation / testing): \n\n* 2: `100.4` / `94.49`\n* 5: `113.5` / `106.1`\n* 7: `98.25` / `92.77`\n* 10: `151.0` / `135.1`\n\n```bash\noverlaps=(2 5 7 10)\nepochs=100\nfor k in \"${overlaps[@]}\"\ndo\n    :\n    python3 main_run.py --main-model simple-lstm --epochs \"$(($epochs/$k))\" --batch-size 80 --dropout 0.15 --nlayers 2 --bptt 70 --nhid 1150 --lr-decay 1 --data /data/noemien.kocher/datasets/wikitext-2 --init-seq \"overlapCN_${k}\"\n    sleep 10\ndone\n```\n\n## MOS PTB\n\n**Standard TOI**:\n\nExpected results: `58.49` / `56.19` (validation / testing)\n\n```bash\npython3 main_run.py --main-model mos-lstm --data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 28 --batch-size 12 --lr 20.0 --epochs 1000 --nhid 960 --nhidlast 620 --emsize 280 --n-experts 15\n```\n\n**Alleviated TOI {1..40}**:\n\n\ud83d\udca5 With a prime batch size:\n\n```bash\nepochs=2000\nfor k in {1..70}\ndo\n        :\n        python3 main_run.py --main-model mos-lstm --data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 28 --batch-size 13 --lr 20.0 --epochs \"$(($epochs/$k))\" --nhid 960 --nhidlast 620 --emsize 280 --n-experts 15 --init-seq \"overlapCNF_${k}\"\n        sleep 10\ndone\n```\n\nExpected results (validation / testing): \n\n* 1: `58.36` /\t`56.21`\n* 2: `58.07` /\t`55.76`\n* 3: `58.03` /\t`55.79`\n* 4: `52.82` /\t`55.63`\n* 5: `57.81` /\t`55.63`\n* 6: `57.55` /\t`55.32`\n* 7: `57.47` /\t`55.23`\n* 8: `57.47` /\t`55.34`\n* 9: `57.16` /\t`54.93`\n* 10: `57.34` / `54.90`\n* 11: `57.11` / `54.98`\n* 12: `57.47` / `55.44`\n* 13: `67.77` / `66.01`\n* 14: `56.76` / **`54.58`** (paper's result)\n* 15: `57.44` / `55.20`\n* 16: `56.95` / `54.86`\n* 17: `57.64` / `55.14`\n* 18: `57.38` / `54.93`\n* 19: `57.55` / `55.35`\n* 20: `57.00` / `54.67`\n* 21: `57.55` / `55.22`\n* 22: `57.54` / `55.19`\n* 23: `57.29` / `54.90`\n* 24: `57.47` / `55.11`\n* 25: `57.12` / `54.85`\n* 26: `66.14` / `63.81`\n* 27: `57.08` / `54.85`\n* 28: `--.--` / `--.--`\n* 29: `--.--` / `--.--`\n* 30: `--.--` / `--.--`\n* 31: `57.74` / `55.37`\n* 32: `57.21` / `55.26`\n* 33: `57.66` / `55.40`\n* 34: `57.48` / `55.44`\n* 35: `56.44` / **`54.33`** (post-result, not in the paper)\n* 36: `57.10` / `55.09`\n* 37: `57.55` / `55.29`\n* 38: `57.04` / `54.87`\n* 39: `64.37` / `62.54`\n* 40: `57.52` / `54.99`\n\n## Voice simple LSTM\n\n**Extreme TOI**:\n\nExpected result: `0.475` / `0.377` (WA / UA)\n\n```bash\npython3 main_run.py --main-model emotions-simple-lstm --cv 5 --data data/IEMOCAP/all_features_cv --test-batch-size 20 --lr 0.05 --log-interval 20 --lr-decay 1 --step-size 0.1 --epochs 60 --order complete_random\n```\n\n**Inter-batch TOI**:\n\nExpected result: `0.478` / `0.386` (WA / UA)\n\n```bash\npython3 main_run.py --main-model emotions-simple-lstm --cv 5 --data data/IEMOCAP/all_features_cv --test-batch-size 20 --lr 0.05 --log-interval 20 --lr-decay 1 --step-size 0.1 --epochs 60 --window-size 300 --order local_order\n```\n\n**Standard TOI**:\n\nExpected result: `0.486` / `0.404` (WA / UA)\n\n```bash\npython3 main_run.py --main-model emotions-simple-lstm --cv 5 --data data/IEMOCAP/all_features_cv --test-batch-size 20 --lr 0.05 --log-interval 20 --lr-decay 1 --step-size 0.1 --epochs 60 --order standard_order\n```\n\n**Alleviated TOI 10**:\n\nExpected result: \n\n* 15k steps: `0.553` / `0.489` (WA / UA)\n* 60 epochs: `0.591` / `0.523` (WA / UA)\n\n```bash\npython3 main_run.py --main-model emotions-simple-lstm --cv 5 --data data/IEMOCAP/all_features_cv --test-batch-size 20 --lr 0.05 --log-interval 20 --lr-decay 1 --step-size 0.1 --epochs 60 --order total_order\n```\n\n## Delayed-reset standard TOI {1,2,5,7,10} with PTB\n\nExpected results (validation / testing): \n\n* 1: `61.28` / `58.94`\n* 2: `60.76` / `58.55`\n* 5: `60.10` / `57.83`\n* 7: `60.08` / `57.76`\n* 10: `60.05` / `57.78`\n\n```bash\nP=(1 2 5 7 10)\nepochs=1000\nfor k in \"${P[@]}\"\ndo\n    :\n    python3 main_run.py --main-model awd-lstm-repetitions --batch-size 20 --data data/penn --dropouti 0.4 --dropouth 0.25 --seed 141 --epochs 1000 --use-repetitions \"${k}\"\n    sleep 10\ndone\n```\n\n# Acknowledgements\n\nCode is heavily borrowed from the following sources:\n\n- simple-lstm (`simple/`): https://github.com/deeplearningathome/pytorch-language-model\n- awd-lstm (`awd/`): https://github.com/salesforce/awd-lstm-lm\n- mos-lstm: (`mos/`) https://github.com/zihangdai/mos\n",
            "readme_url": "https://github.com/nkcr/overlap-ml",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
            "arxiv": "1711.03953",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.03953v4",
            "abstract": "We formulate language modeling as a matrix factorization problem, and show\nthat the expressiveness of Softmax-based models (including the majority of\nneural language models) is limited by a Softmax bottleneck. Given that natural\nlanguage is highly context-dependent, this further implies that in practice\nSoftmax with distributed word embeddings does not have enough capacity to model\nnatural language. We propose a simple and effective method to address this\nissue, and improve the state-of-the-art perplexities on Penn Treebank and\nWikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on\nthe large-scale 1B Word dataset, outperforming the baseline by over 5.6 points\nin perplexity.",
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Ruslan Salakhutdinov",
                "William W. Cohen"
            ]
        },
        {
            "title": "Regularizing and Optimizing LSTM Language Models",
            "arxiv": "1708.02182",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02182v1",
            "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.",
            "authors": [
                "Stephen Merity",
                "Nitish Shirish Keskar",
                "Richard Socher"
            ]
        },
        {
            "title": "Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes",
            "arxiv": "1909.08700",
            "year": 2019,
            "url": "http://arxiv.org/abs/1909.08700v1",
            "abstract": "In sequence modeling tasks the token order matters, but this information can\nbe partially lost due to the discretization of the sequence into data points.\nIn this paper, we study the imbalance between the way certain token pairs are\nincluded in data points and others are not. We denote this a token order\nimbalance (TOI) and we link the partial sequence information loss to a\ndiminished performance of the system as a whole, both in text and speech\nprocessing tasks. We then provide a mechanism to leverage the full token order\ninformation -Alleviated TOI- by iteratively overlapping the token composition\nof data points. For recurrent networks, we use prime numbers for the batch size\nto avoid redundancies when building batches from overlapped data points. The\nproposed method achieved state of the art performance in both text and speech\nrelated tasks.",
            "authors": [
                "No\u00e9mien Kocher",
                "Christian Scuito",
                "Lorenzo Tarantino",
                "Alexandros Lazaridis",
                "Andreas Fischer",
                "Claudiu Musat"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "WikiText-2"
            },
            {
                "name": "WikiText-103"
            },
            {
                "name": "IEMOCAP"
            },
            {
                "name": "Penn Treebank"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9978302925520038,
        "task": "Language Modelling",
        "task_prob": 0.7238318721263586
    }
}