{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Challenges for NTIRE 2019 is open!",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "markmaxt",
                "owner_type": "User",
                "name": "VideoSR",
                "url": "https://github.com/markmaxt/VideoSR",
                "stars": 3,
                "pushed_at": "2020-02-18 19:30:59+00:00",
                "created_at": "2019-03-27 09:40:43+00:00",
                "language": "Python",
                "description": "attempt",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "fe5c52449578f8e15199f98cc3eee3611a42a166",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/.gitignore"
                    }
                },
                "size": 761
            },
            {
                "type": "code",
                "name": ".idea",
                "sha": "5042fff293700d8208b3c1d9b6b8271f83172539",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/tree/master/.idea"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "11738a43fd38c4ceb17ac572afaf2b01df92287a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/LICENSE"
                    }
                },
                "size": 1069
            },
            {
                "type": "code",
                "name": "_windows",
                "sha": "8ab5650bd78308337774d7195427520135a519f4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/tree/master/_windows"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "codestyles",
                "sha": "6f600ba0d140136e9f18aa54457fc6c40c6b3a30",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/tree/master/codestyles"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "colors.scheme.xml",
                "sha": "39308cdb16bb241cfd7f76f54f89479ea0f52b35",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/colors.scheme.xml"
                    }
                },
                "size": 131
            },
            {
                "type": "code",
                "name": "databaseDrivers.xml",
                "sha": "b25364ce1ea2e5e805ebc37601dd64651a66c638",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/databaseDrivers.xml"
                    }
                },
                "size": 3251
            },
            {
                "type": "code",
                "name": "debugger.xml",
                "sha": "544ab155e35bc4e2b79741cfbf67c0036ffe5257",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/debugger.xml"
                    }
                },
                "size": 291
            },
            {
                "type": "code",
                "name": "editor.codeinsight.xml",
                "sha": "b43f2d41816e73c6ff855236522bb3bb85bd3b2c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/editor.codeinsight.xml"
                    }
                },
                "size": 194
            },
            {
                "type": "code",
                "name": "editor.xml",
                "sha": "927d99962da4430da7920d01218a20adc2807519",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/editor.xml"
                    }
                },
                "size": 170
            },
            {
                "type": "code",
                "name": "experiment",
                "sha": "f43af72e0e15d9837ba78c89281f1b7de1540d2d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/tree/master/experiment"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "figs",
                "sha": "9acb465d8e5724c7b9ee57f37b7a6e954cdffebf",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/tree/master/figs"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "filetypes.xml",
                "sha": "c5cccb25fad578f2df021aca0fa6f0a0b379ed27",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/filetypes.xml"
                    }
                },
                "size": 561
            },
            {
                "type": "code",
                "name": "github.xml",
                "sha": "61f30d312de718c81c5db828f89d16cb01a3966a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/github.xml"
                    }
                },
                "size": 307
            },
            {
                "type": "code",
                "name": "ide.general.xml",
                "sha": "4ae98e0d99604b41050f5a02363add5bdf6f34a7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/ide.general.xml"
                    }
                },
                "size": 237
            },
            {
                "type": "code",
                "name": "markdown.xml",
                "sha": "92e74be48cb5b04b5d0e698295d4a9d2d8e301b2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/markdown.xml"
                    }
                },
                "size": 120
            },
            {
                "type": "code",
                "name": "print.xml",
                "sha": "f3dd921cfa340f8514a7a62729b3546777f45c9e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/print.xml"
                    }
                },
                "size": 121
            },
            {
                "type": "code",
                "name": "project.default.xml",
                "sha": "f429fc14fc01d426260d190d6fbcd4bbac661209",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/project.default.xml"
                    }
                },
                "size": 443
            },
            {
                "type": "code",
                "name": "src",
                "sha": "b1de23a0862fa494ca9f6d95a3576caa8cf3c962",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/tree/master/src"
                    }
                },
                "num_files": 13
            },
            {
                "type": "code",
                "name": "test",
                "sha": "c1b0b0441b3cb433b8fb4c731b9179f1e6f585b2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/tree/master/test"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "vcs.xml",
                "sha": "115c440e8e64b21f133c5d1cf65654e1c3931746",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/vcs.xml"
                    }
                },
                "size": 591
            },
            {
                "type": "code",
                "name": "webServers.xml",
                "sha": "6ecca1990c2dca115933e2d3915e912d916792fd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/webServers.xml"
                    }
                },
                "size": 4922
            },
            {
                "type": "code",
                "name": "wsl.distributions.xml",
                "sha": "760782d5ef7957f7fc2fd31ea8062e5177fbac85",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/markmaxt/VideoSR/blob/master/wsl.distributions.xml"
                    }
                },
                "size": 2578
            }
        ]
    },
    "authors": [
        {
            "name": "Sanghyun Son",
            "email": "thstkdgus35@snu.ac.kr",
            "github_id": "sanghyun-son"
        },
        {
            "name": "markmaxt",
            "github_id": "markmaxt"
        },
        {
            "name": "trantoan89",
            "github_id": "trantoan89"
        },
        {
            "name": "Changjiang Yang",
            "github_id": "yangcha"
        },
        {
            "name": "Yulun Zhang",
            "email": "yulun100@gmail.com",
            "github_id": "yulunzhang"
        },
        {
            "name": "tabetomo",
            "github_id": "tabetomo"
        }
    ],
    "tags": [],
    "description": "attempt",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/markmaxt/VideoSR",
            "stars": 3,
            "issues": true,
            "readme": "# [Challenges for NTIRE 2019 is open!](http://www.vision.ee.ethz.ch/ntire19/)\n\n**The challenge winners will be awarded at the CVPR 2019 Workshop.**\n![](/figs/ntire2019.png)\n\n# EDSR-PyTorch\n\n**About PyTorch 1.0.0**\n  * We support PyTorch 1.0.0. If you prefer the previous versions of PyTorch, use legacy branches.\n  * ``--ext bin`` is not supported. Also, please erase your bin files with ``--ext sep-reset``. Once you successfully build those bin files, you can remove ``-reset`` from the argument.\n\n![](/figs/main.png)\n\nThis repository is an official PyTorch implementation of the paper **\"Enhanced Deep Residual Networks for Single Image Super-Resolution\"** from **CVPRW 2017, 2nd NTIRE**.\nYou can find the original code and more information from [here](https://github.com/LimBee/NTIRE2017).\n\nIf you find our work useful in your research or publication, please cite our work:\n\n[1] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee, **\"Enhanced Deep Residual Networks for Single Image Super-Resolution,\"** <i>2nd NTIRE: New Trends in Image Restoration and Enhancement workshop and challenge on image super-resolution in conjunction with **CVPR 2017**. </i> [[PDF](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.pdf)] [[arXiv](https://arxiv.org/abs/1707.02921)] [[Slide](https://cv.snu.ac.kr/research/EDSR/Presentation_v3(release).pptx)]\n```\n@InProceedings{Lim_2017_CVPR_Workshops,\n  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},\n  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n  month = {July},\n  year = {2017}\n}\n```\nWe provide scripts for reproducing all the results from our paper. You can train your own model from scratch, or use pre-trained model to enlarge your images.\n\n**Differences between Torch version**\n* Codes are much more compact. (Removed all unnecessary parts.)\n* Models are smaller. (About half.)\n* Slightly better performances.\n* Training and evaluation requires less memory.\n* Python-based.\n\n## Dependencies\n* Python 3.6\n* PyTorch >= 1.0.0\n* numpy\n* skimage\n* **imageio**\n* matplotlib\n* tqdm\n* cv2 >= 3.xx (Only if you want to use video input/output)\n\n## Code\nClone this repository into any place you want.\n```bash\ngit clone https://github.com/thstkdgus35/EDSR-PyTorch\ncd EDSR-PyTorch\n```\n\n## Quick start (Demo)\nYou can test our super-resolution algorithm with your own images. Place your images in ``test`` folder. (like ``test/<your_image>``) We support **png** and **jpeg** files.\n\nRun the script in ``src`` folder. Before you run the demo, please uncomment the appropriate line in ```demo.sh``` that you want to execute.\n```bash\ncd src       # You are now in */EDSR-PyTorch/src\nsh demo.sh\n```\n\nYou can find the result images from ```experiment/test/results``` folder.\n\n| Model | Scale | File name (.pt) | Parameters | ****PSNR** |\n|  ---  |  ---  | ---       | ---        | ---  |\n| **EDSR** | 2 | EDSR_baseline_x2 | 1.37 M | 34.61 dB |\n| | | *EDSR_x2 | 40.7 M | 35.03 dB |\n| | 3 | EDSR_baseline_x3 | 1.55 M | 30.92 dB |\n| | | *EDSR_x3 | 43.7 M | 31.26 dB |\n| | 4 | EDSR_baseline_x4 | 1.52 M | 28.95 dB |\n| | | *EDSR_x4 | 43.1 M | 29.25 dB |\n| **MDSR** | 2 | MDSR_baseline | 3.23 M | 34.63 dB |\n| | | *MDSR | 7.95 M| 34.92 dB |\n| | 3 | MDSR_baseline | | 30.94 dB |\n| | | *MDSR | | 31.22 dB |\n| | 4 | MDSR_baseline | | 28.97 dB |\n| | | *MDSR | | 29.24 dB |\n\n*Baseline models are in ``experiment/model``. Please download our final models from [here](https://cv.snu.ac.kr/research/EDSR/model_pytorch.tar) (542MB)\n**We measured PSNR using DIV2K 0801 ~ 0900, RGB channels, without self-ensemble. (scale + 2) pixels from the image boundary are ignored.\n\nYou can evaluate your models with widely-used benchmark datasets:\n\n[Set5 - Bevilacqua et al. BMVC 2012](http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html),\n\n[Set14 - Zeyde et al. LNCS 2010](https://sites.google.com/site/romanzeyde/research-interests),\n\n[B100 - Martin et al. ICCV 2001](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/),\n\n[Urban100 - Huang et al. CVPR 2015](https://sites.google.com/site/jbhuang0604/publications/struct_sr).\n\nFor these datasets, we first convert the result images to YCbCr color space and evaluate PSNR on the Y channel only. You can download [benchmark datasets](https://cv.snu.ac.kr/research/EDSR/benchmark.tar) (250MB). Set ``--dir_data <where_benchmark_folder_located>`` to evaluate the EDSR and MDSR with the benchmarks.\n\nYou can download some results from [here](https://cv.snu.ac.kr/research/EDSR/result_image/edsr-results.tar).\nThe link contains **EDSR+_baseline_x4** and **EDSR+_x4**.\nOtherwise, you can easily generate result images with ``demo.sh`` scripts.\n\n## How to train EDSR and MDSR\nWe used [DIV2K](http://www.vision.ee.ethz.ch/%7Etimofter/publications/Agustsson-CVPRW-2017.pdf) dataset to train our model. Please download it from [here](https://cv.snu.ac.kr/research/EDSR/DIV2K.tar) (7.1GB).\n\nUnpack the tar file to any place you want. Then, change the ```dir_data``` argument in ```src/option.py``` to the place where DIV2K images are located.\n\nWe recommend you to pre-process the images before training. This step will decode all **png** files and save them as binaries. Use ``--ext sep_reset`` argument on your first run. You can skip the decoding part and use saved binaries with ``--ext sep`` argument.\n\nIf you have enough RAM (>= 32GB), you can use ``--ext bin`` argument to pack all DIV2K images in one binary file.\n\nYou can train EDSR and MDSR by yourself. All scripts are provided in the ``src/demo.sh``. Note that EDSR (x3, x4) requires pre-trained EDSR (x2). You can ignore this constraint by removing ```--pre_train <x2 model>``` argument.\n\n```bash\ncd src       # You are now in */EDSR-PyTorch/src\nsh demo.sh\n```\n\n**Update log**\n* Jan 04, 2018\n  * Many parts are re-written. You cannot use previous scripts and models directly.\n  * Pre-trained MDSR is temporarily disabled.\n  * Training details are included.\n\n* Jan 09, 2018\n  * Missing files are included (```src/data/MyImage.py```).\n  * Some links are fixed.\n\n* Jan 16, 2018\n  * Memory efficient forward function is implemented.\n  * Add --chop_forward argument to your script to enable it.\n  * Basically, this function first split a large image to small patches. Those images are merged after super-resolution. I checked this function with 12GB memory, 4000 x 2000 input image in scale 4. (Therefore, the output will be 16000 x 8000.)\n\n* Feb 21, 2018\n  * Fixed the problem when loading pre-trained multi-gpu model.\n  * Added pre-trained scale 2 baseline model.\n  * This code now only saves the best-performing model by default. For MDSR, 'the best' can be ambiguous. Use --save_models argument to save all the intermediate models.\n  * PyTorch 0.3.1 changed their implementation of DataLoader function. Therefore, I also changed my implementation of MSDataLoader. You can find it on feature/dataloader branch.\n\n* Feb 23, 2018\n  * Now PyTorch 0.3.1 is default. Use legacy/0.3.0 branch if you use the old version.\n  * With a new ``src/data/DIV2K.py`` code, one can easily create new data class for super-resolution.\n  * New binary data pack. (Please remove the ``DIV2K_decoded`` folder from your dataset if you have.)\n  * With ``--ext bin``, this code will automatically generates and saves the binary data pack that corresponds to previous ``DIV2K_decoded``. (This requires huge RAM (~45GB, Swap can be used.), so please be careful.)\n  * If you cannot make the binary pack, just use the default setting (``--ext img``).\n\n  * Fixed a bug that PSNR in the log and PSNR calculated from the saved images does not match.\n  * Now saved images have better quality! (PSNR is ~0.1dB higher than the original code.)\n  * Added performance comparison between Torch7 model and PyTorch models.\n\n* Mar 5, 2018\n  * All baseline models are uploaded.\n  * Now supports half-precision at test time. Use ``--precision half``  to enable it. This does not degrade the output images.\n\n* Mar 11, 2018\n  * Fixed some typos in the code and script.\n  * Now --ext img is default setting. Although we recommend you to use --ext bin when training, please use --ext img when you use --test_only.\n  * Skip_batch operation is implemented. Use --skip_threshold argument to skip the batch that you want to ignore. Although this function is not exactly same with that of Torch7 version, it will work as you expected.\n\n* Mar 20, 2018\n  * Use ``--ext sep_reset`` to pre-decode large png files. Those decoded files will be saved to the same directory with DIV2K png files. After the first run, you can use ``--ext sep`` to save time.\n  * Now supports various benchmark datasets. For example, try ``--data_test Set5`` to test your model on the Set5 images.\n  * Changed the behavior of skip_batch.\n\n* Mar 29, 2018\n  * We now provide all models from our paper.\n  * We also provide ``MDSR_baseline_jpeg`` model that suppresses JPEG artifacts in original low-resolution image. Please use it if you have any trouble.\n  * ``MyImage`` dataset is changed to ``Demo`` dataset. Also, it works more efficient than before.\n  * Some codes and script are re-written.\n\n* Apr 9, 2018\n  * VGG and Adversarial loss is implemented based on [SRGAN](http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf). [WGAN](https://arxiv.org/abs/1701.07875) and [gradient penalty](https://arxiv.org/abs/1704.00028) are also implemented, but they are not tested yet.\n  * Many codes are refactored. If there exists a bug, please report it.\n  * [D-DBPN](https://arxiv.org/abs/1803.02735) is implemented. Default setting is D-DBPN-L.\n\n* Apr 26, 2018\n  * Compatible with PyTorch 0.4.0\n  * Please use the legacy/0.3.1 branch if you are using the old version of PyTorch.\n  * Minor bug fixes\n\n* July 22, 2018\n  * Thanks for recent commits that contains RDN and RCAN. Please see ``code/demo.sh`` to train/test those models.\n  * Now the dataloader is much stable than the previous version. Please erase ``DIV2K/bin`` folder that is created before this commit. Also, please avoid to use ``--ext bin`` argument. Our code will automatically pre-decode png images before training. If you do not have enough spaces(~10GB) in your disk, we recommend ``--ext img``(But SLOW!).\n\n* Oct 18, 2018\n  * with ``--pre_train download``, pretrained models will be automatically downloaded from server.\n  * Supports video input/output (inference only). Try with ``--data_test video --dir_demo [video file directory]``.\n\n",
            "readme_url": "https://github.com/markmaxt/VideoSR",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Wasserstein GAN",
            "arxiv": "1701.07875",
            "year": 2017,
            "url": "http://arxiv.org/abs/1701.07875v3",
            "abstract": "We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions.",
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ]
        },
        {
            "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
            "arxiv": "1707.02921",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.02921v1",
            "abstract": "Recent research on super-resolution has progressed with the development of\ndeep convolutional neural networks (DCNN). In particular, residual learning\ntechniques exhibit improved performance. In this paper, we develop an enhanced\ndeep super-resolution network (EDSR) with performance exceeding those of\ncurrent state-of-the-art SR methods. The significant performance improvement of\nour model is due to optimization by removing unnecessary modules in\nconventional residual networks. The performance is further improved by\nexpanding the model size while we stabilize the training procedure. We also\npropose a new multi-scale deep super-resolution system (MDSR) and training\nmethod, which can reconstruct high-resolution images of different upscaling\nfactors in a single model. The proposed methods show superior performance over\nthe state-of-the-art methods on benchmark datasets and prove its excellence by\nwinning the NTIRE2017 Super-Resolution Challenge.",
            "authors": [
                "Bee Lim",
                "Sanghyun Son",
                "Heewon Kim",
                "Seungjun Nah",
                "Kyoung Mu Lee"
            ]
        },
        {
            "title": "Improved Training of Wasserstein GANs",
            "arxiv": "1704.00028",
            "year": 2017,
            "url": "http://arxiv.org/abs/1704.00028v3",
            "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but\nsuffer from training instability. The recently proposed Wasserstein GAN (WGAN)\nmakes progress toward stable training of GANs, but sometimes can still generate\nonly low-quality samples or fail to converge. We find that these problems are\noften due to the use of weight clipping in WGAN to enforce a Lipschitz\nconstraint on the critic, which can lead to undesired behavior. We propose an\nalternative to clipping weights: penalize the norm of gradient of the critic\nwith respect to its input. Our proposed method performs better than standard\nWGAN and enables stable training of a wide variety of GAN architectures with\nalmost no hyperparameter tuning, including 101-layer ResNets and language\nmodels over discrete data. We also achieve high quality generations on CIFAR-10\nand LSUN bedrooms.",
            "authors": [
                "Ishaan Gulrajani",
                "Faruk Ahmed",
                "Martin Arjovsky",
                "Vincent Dumoulin",
                "Aaron Courville"
            ]
        },
        {
            "title": "Deep Back-Projection Networks For Super-Resolution",
            "arxiv": "1803.02735",
            "year": 2018,
            "url": "http://arxiv.org/abs/1803.02735v1",
            "abstract": "The feed-forward architectures of recently proposed deep super-resolution\nnetworks learn representations of low-resolution inputs, and the non-linear\nmapping from those to high-resolution output. However, this approach does not\nfully address the mutual dependencies of low- and high-resolution images. We\npropose Deep Back-Projection Networks (DBPN), that exploit iterative up- and\ndown-sampling layers, providing an error feedback mechanism for projection\nerrors at each stage. We construct mutually-connected up- and down-sampling\nstages each of which represents different types of image degradation and\nhigh-resolution components. We show that extending this idea to allow\nconcatenation of features across up- and down-sampling stages (Dense DBPN)\nallows us to reconstruct further improve super-resolution, yielding superior\nresults and in particular establishing new state of the art results for large\nscaling factors such as 8x across multiple data sets.",
            "authors": [
                "Muhammad Haris",
                "Greg Shakhnarovich",
                "Norimichi Ukita"
            ]
        },
        {
            "year": "2017",
            "month": "July",
            "booktitle": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops",
            "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
            "author": [
                "Lim, Bee",
                "Son, Sanghyun",
                "Kim, Heewon",
                "Nah, Seungjun",
                "Lee, Kyoung Mu"
            ],
            "ENTRYTYPE": "inproceedings",
            "ID": "Lim_2017_CVPR_Workshops",
            "authors": [
                "Lim, Bee",
                "Son, Sanghyun",
                "Kim, Heewon",
                "Nah, Seungjun",
                "Lee, Kyoung Mu"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "benchmark datasets",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://cv.snu.ac.kr/research/EDSR/benchmark.tar"
                    }
                }
            },
            {
                "name": "CIFAR-10"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999732753759175,
        "task": "Image Generation",
        "task_prob": 0.9858042254223065
    }
}