{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Robotic Assembly using Deep Reinforcement Learning",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "arrival-ltd",
                "owner_type": "Organization",
                "name": "catalyst-rl-tutorial",
                "url": "https://github.com/arrival-ltd/catalyst-rl-tutorial",
                "stars": 84,
                "pushed_at": "2020-09-23 12:10:44+00:00",
                "created_at": "2020-09-23 08:59:16+00:00",
                "language": "Python",
                "description": "Using Catalyst.RL to train a robot to perform peg-in-hole insertion in simulation.",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "63c6591eda913cf837573b751d9b66190a0b1535",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/arrival-ltd/catalyst-rl-tutorial/blob/master/LICENSE"
                    }
                },
                "size": 1064
            },
            {
                "type": "code",
                "name": "configs",
                "sha": "10fc2c3f2805fc1a586873bbfdacc4979c95623f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/arrival-ltd/catalyst-rl-tutorial/tree/master/configs"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "images",
                "sha": "e8173a04b8fe27252554bd19af04b942651d39d9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/arrival-ltd/catalyst-rl-tutorial/tree/master/images"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "b42c37e59b9f1594d12d8e1c4cc85677d5012d56",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/arrival-ltd/catalyst-rl-tutorial/blob/master/requirements.txt"
                    }
                },
                "size": 269
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "0f8193174d2854920a18cadbc921449eb27e1784",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/arrival-ltd/catalyst-rl-tutorial/tree/master/scripts"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "simulation",
                "sha": "433c1fd29b920c4124279ec0bf7106be67165abb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/arrival-ltd/catalyst-rl-tutorial/tree/master/simulation"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "src",
                "sha": "d653232906eaf38086f334409139f60ede2dd1c9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/arrival-ltd/catalyst-rl-tutorial/tree/master/src"
                    }
                },
                "num_files": 5
            }
        ]
    },
    "authors": [
        {
            "name": "Fedor Chervinskii",
            "github_id": "fedor-chervinskii"
        }
    ],
    "tags": [
        "sim2real",
        "robotics",
        "deep-reinforcement-learning"
    ],
    "description": "Using Catalyst.RL to train a robot to perform peg-in-hole insertion in simulation.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/arrival-ltd/catalyst-rl-tutorial",
            "stars": 84,
            "issues": true,
            "readme": "# Robotic Assembly using Deep Reinforcement Learning\n\n![](./images/tutorial_gif.gif)\n\n## Introduction\nOne of the most exciting advancements, that has pushed the frontier of the Artificial Intelligence (AI) in recent years, is Deep Reinforcement Learning (DRL). DRL belongs to the family of machine learning algorithms. It assumes that intelligent machines can learn from their actions similar to the way humans learn from experience. Over the recent years we could witness some impressive [real-world applications of DRL](https://neptune.ai/blog/reinforcement-learning-applications). The algorithms allowed for major progress especially in the field of robotics. If you are interested in learning more about DRL, we encourage you to get familiar with the exceptional [**Introduction to RL**](https://spinningup.openai.com/en/latest) by OpenAI. We believe this is the best place to start your adventure with DRL.\n\nThe **goal of this tutorial is to show how you can apply DRL to solve your own robotic challenge**. For the sake of this tutorial we have chosen one of the classic assembly tasks: peg-in-hole insertion. By the time you finish the tutorial, you will understand how to create a complete, end-to-end pipeline for training the robot in the simulation using DRL.\n\nThe accompanying code together with all the details of the implementation can be found in our [GitHub repository](https://github.com/arrival-ltd/catalyst-rl-tutorial).\n\n## Setup\n1. Download the **robot simulation platform**, CoppeliaSim, from [the official website](https://www.coppeliarobotics.com/downloads). This tutorial is compatible with the version 4.1.0. \n\n2. Setup **toolkit for robot learning research**, PyRep, from their [github repository](https://github.com/stepjam/PyRep). PyRep library is built on top of CoppeliaSim to facilitate prototyping in python. \n\n3. Create **an environment for the RL agent**: It could be either a simulation or a real environment. We limit ourselves to simulation for faster prototyping and training. The agent interacts with the environment to collect experience. This allows it to learn a policy which maximizes the expected (discounted) sum of future rewards and hence solves the designed task. Most RL practitioners are familiar with the [OpenAI Gym environments](https://gym.openai.com/envs/#classic_control), a toolkit with toy environments used for developing and benchmarking reinforcement learning algorithms. However, our use case, robotic assembly task, is very specific. The goal is to train a robot to perform peg-in-hole insertion. This is why we created our simulation environment in [CoppeliaSim](https://www.coppeliarobotics.com). The simulator comes with various robot manipulators and grippers. For our tutorial, we picked UR5 robot with RG2 gripper (Figure 1).\n   ![](./images/sim_env.png) \n\n   <em>Figure 1: UR5 manipulator with a peg attached to its gripper. The mating part is placed on the ground in the scene. CoppeliaSim caters to a variety of different robotic tasks. Feel free to come up with your own challenge and design your own simulation! [RLBench](https://github.com/stepjam/RLBench/tree/master/rlbench/task_ttms) (the robot learning benchmark and learning environment) also provides more off-the-shelf, advanced simulation environments. </em> \n\n4. Create **a gym environment wrapped around the simulation scene**:  \n\n```python\nimport os\nimport cv2\nimport logging\nimport numpy as np\n\nfrom gym import Space\nfrom gym.spaces.box import Box\nfrom gym.spaces.dict import Dict\nfrom pyrep import PyRep, objects\n\nfrom catalyst_rl.rl.core import EnvironmentSpec\nfrom catalyst_rl.rl.utils import extend_space\n\n\nclass CoppeliaSimEnvWrapper(EnvironmentSpec):\n    def __init__(self, visualize=True,\n                 mode=\"train\",\n                 **params):\n        super().__init__(visualize=visualize, mode=mode)\n\n        # Scene selection\n        scene_file_path = os.path.join(os.getcwd(), 'simulation/UR5.ttt')\n\n        # Simulator launch\n        self.env = PyRep()\n        self.env.launch(scene_file_path, headless=False)\n        self.env.start()\n        self.env.step()\n\n        # Task related initialisations in Simulator\n        self.vision_sensor = objects.vision_sensor.VisionSensor(\"Vision_sensor\")\n        self.gripper = objects.dummy.Dummy(\"UR5_target\")\n        self.gripper_zero_pose = self.gripper.get_pose()\n        self.goal = objects.dummy.Dummy(\"goal_target\")\n        self.goal_STL = objects.shape.Shape(\"goal\")\n        self.goal_STL_zero_pose = self.goal_STL.get_pose()\n        self.grasped_STL = objects.shape.Shape(\"Peg\")\n        self.stacking_area = objects.shape.Shape(\"Plane\")\n        self.vision_sensor = objects.vision_sensor.VisionSensor(\"Vision_sensor\")\n\n        self.step_counter = 0\n        self.max_step_count = 100\n        self.target_pose = None\n        self.initial_distance = None\n        self.image_width, self.image_height = 320, 240\n        self.vision_sensor.set_resolution((self.image_width, self.image_height))\n        self._history_len = 1\n\n        self._observation_space = Dict(\n                {\"cam_image\": Box(0, 255,\n                                  [self.image_height, self.image_width, 1],\n                                  dtype=np.uint8)})\n\n        self._action_space = Box(-1, 1, (3,))\n        self._state_space = extend_space(self._observation_space, self._history_len)\n\n    @property\n    def history_len(self):\n        return self._history_len\n\n    @property\n    def observation_space(self) -> Space:\n        return self._observation_space\n\n    @property\n    def state_space(self) -> Space:\n        return self._state_space\n\n    @property\n    def action_space(self) -> Space:\n        return self._action_space\n\n    def step(self, action):\n        done = False\n        info = {}\n        prev_distance_to_goal = self.distance_to_goal()\n\n        # Make a step in simulation\n        self.apply_controls(action)\n        self.env.step()\n        self.step_counter += 1\n\n        # Reward calculations\n        success_reward = self.success_check()\n        distance_reward = (prev_distance_to_goal - self.distance_to_goal()) / self.initial_distance\n\n        reward = distance_reward + success_reward\n\n        # Check reset conditions\n        if self.step_counter > self.max_step_count:\n            done = True\n            logging.info('--------Reset: Timeout--------')\n        elif self.distance_to_goal() > 0.8:\n            done = True\n            logging.info('--------Reset: Too far from target--------')\n        elif self.collision_check():\n            done = True\n            logging.info('--------Reset: Collision--------')\n\n        return self.get_observation(), reward, done, info\n\n    def reset(self):\n        logging.info(\"Episode reset...\")\n        self.step_counter = 0\n        self.env.stop()\n        self.env.start()\n        self.env.step()\n        self.setup_scene()\n        observation = self.get_observation()\n        return observation\n# -------------- all methods above are required for any Gym environment, everything below is env-specific --------------\n\n    def distance_to_goal(self):\n        goal_pos = self.goal.get_position()\n        tip_pos = self.gripper.get_position()\n        return np.linalg.norm(np.array(tip_pos) - np.array(goal_pos))\n\n    def setup_goal(self):\n        goal_position = self.goal_STL_zero_pose[:3]\n        # 2D goal randomization\n        self.target_pose = [goal_position[0] + (2 * np.random.rand() - 1.) * 0.1,\n                            goal_position[1] + (2 * np.random.rand() - 1.) * 0.1,\n                            goal_position[2]]\n        self.target_pose = np.append(self.target_pose,\n                                     self.goal_STL_zero_pose[3:]).tolist()\n        self.goal_STL.set_pose(self.target_pose)\n\n        # Randomizing the RGB of the goal and the plane\n        rgb_values_goal = list(np.random.rand(3,))\n        rgb_values_plane = list(np.random.rand(3,))\n        self.goal_STL.set_color(rgb_values_goal)\n        self.stacking_area.set_color(rgb_values_plane)\n\n        self.initial_distance = self.distance_to_goal()\n\n    def setup_scene(self):\n        self.setup_goal()\n        self.gripper.set_pose(self.gripper_zero_pose)\n\n    def get_observation(self):\n        cam_image = self.vision_sensor.capture_rgb()\n        gray_image = np.uint8(cv2.cvtColor(cam_image, cv2.COLOR_BGR2GRAY) * 255)\n        obs_image = np.expand_dims(gray_image, axis=2)\n        return {\"cam_image\": obs_image}\n\n    def collision_check(self):\n        return self.grasped_STL.check_collision(\n            self.stacking_area) or self.grasped_STL.check_collision(self.goal_STL)\n\n    def success_check(self):\n        success_reward = 0.\n        if self.distance_to_goal() < 0.01:\n            success_reward = 0.01\n            logging.info('--------Success state--------')\n        return success_reward\n\n    def apply_controls(self, action):\n        gripper_position = self.gripper.get_position()\n        # predicted action is in range (-1, 1) so we are normalizing it to physical units\n        new_position = [gripper_position[i] + (action[i] / 200.) for i in range(3)]\n        self.gripper.set_position(new_position)\n```\nFor our reinforcement learning project we use [Catalyst RL](https://github.com/Scitator/catalyst-rl-framework), a distributed framework for reproducible RL research. This is just one of the elements of the marvellous [Catalyst](https://github.com/catalyst-team/catalyst) project. Catalyst is a [PyTorch ecosystem](https://pytorch.org/ecosystem/) framework for Deep Learning research and development. It focuses on reproducibility, rapid experimentation and codebase reuse. This means that the user can seamlessly run training loop with metrics, model checkpointing, advanced logging and distributed training support without the boilerplate code. We strongly encourage you to get familiar with the [Intro to Catalyst](https://medium.com/pytorch/catalyst-101-accelerated-pytorch-bd766a556d92) and incorporating the framework into your daily work!\n\nWe reuse its general Catalyst RL environment (`EnvironmentSpec`) class to create our custom environment. By inheriting from the `EnvironmentSpec`, you can quickly design your own environment, be it an [Atari game](https://gym.openai.com/envs/#atari), [classic control task](https://gym.openai.com/envs/#classic_control) or [robotic simulation](https://gym.openai.com/envs/#robotics). Finally, we specify states/observations, actions and rewards using OpenAI's gym [spaces](https://gym.openai.com/docs/#spaces) type. \n\n### A brief summary of the `CoppeliaSimEnvWrapper` in `src/env.py` \n\nThis class wraps around the general RL environment class to launch the CoppeliaSim with our custom scene. Additionally, in the beginning of every episode, it initialises the properties of the mating part: 2D position in the workspace (`setup_goal()` method), as well as its colour.\n\nThe environment wrapper contains following methods:\n\n* `get_observation()`, capture a grayscale image as an observation.\n\n*  `distance_to_goal()`, compute the distance between the target and current position. The distance is used in reward design.\n   \n*  `success_check()`, check whether the goal state is reached. If yes, significantly boost agent's reward.\n   \n* `collision_check()`, check whether an agent collided with any object.\n  \n\nEpisode termination occurs when the robot gets too far from the target, collides with any object in the environment or exceeds the maximum number of time steps. Those conditions are specified at the end of `step()` method and are checked at each step taken in the environment by the agent. Once the episode terminates, the whole cycle is repeated for the next episode.  \n\n### Defining the RL algorithm\n\nSo far we have created an environment and specified how the agent can act (action space) and what the agent observes (observation space). But the intelligence of the robot is determined by the neural network. This \"brain\" of the robot is being trained using Deep Reinforcement Learning. Depending on the modality of the input (defined in `self.observation_space` property of the environment wrapper) , the architecture of agent's brain changes. It could be a multi-layer perceptron (MLP) or a convolutional neural network (CNN).\nCatalyst provides an easy way to configure an agent using a `YAML` file. Additionally, it provides implementations of state-of-the-art RL algorithms like `PPO, DDPG, TD3, SAC` etc. One could pick the type of the algorithm by changing `algorithm:` variable in `configs/config.yml`. The hyper-parameters related to training can also be configured here.\n\nIn this tutorial, an off-policy, model-free RL algorithm [TD3](https://arxiv.org/pdf/1802.09477.pdf) is used. \n\n![](./images/blog_related_TD3.png) \n\n<em>Figure 2: Architecture of the actor and critic in our TD3 algorithm.</em> \n\nAs depicted in Figure 2, the actor and critic(s) (TD3 concurrently learns two value networks) are modelled as `agent` classes in Catalyst. We customize them and configure the config file by setting `agent: UR5Actor` and `agent: UR5StateActionCritic`. The details of the neural network architecture for both actor and critic(s) can be configured by further editing the `YAML` file.\n\nThe CNN network `image_net`, used to process camera images, can be created as shown below. The layers of network are defined by  `channels `, `bias `,  `dropout `, `normalization ` (booleans)  and `activation ` functions (strings). These parameters are used by the function `get_convolution_net` in `src/network.py`.     \n\n```\nimage_net_params:\n   history_len: *history_len\n   channels: [16, 32, 32, 32, 16]\n   use_bias: True\n   use_groups: False\n   use_normalization: True\n   use_dropout: False\n   activation: ReLU\n```\nA MLP can be created using the block shown below. In our example, `main_net`, `action_net` are created\nin similar fashion through `get_linear_net` function.\n\n```\nfeatures: [64, 64]\nuse_bias: False\nuse_normalization: False\nuse_dropout: False\nactivation: ReLU\n\n```\nOnce the actor and critic network architectures are defined, we are ready to start the training.\n\n## Training\n![](./images/training.png) <em> Figure 3: **Samplers** explore the environment and collect the data. **Trainer** uses the collected data to train a policy. Both the trainer and samplers are also configurable in `configs/config.yml`. The sampler starts with a random policy and after certain transitions, governed by `save_period` variable, the sampler updates its policy with the latest trainer weights. As the training progresses, the sampler keeps on gathering data collected by better policies while the trainer improves the policy until convergence. All the collected data is stored in a database. Source: [Sample Efficient Ensemble Learning with Catalyst.RL](https://arxiv.org/pdf/2003.14210.pdf). </em> \n\nOnce the parameters of trainer and sampler (in the tutorial we use a single sampler) are configured, the training process can be started by launching `scripts/run-training.sh`. \n\nThis opens a tmux session, which starts sampler, trainer, database, and tensorboard to monitor the training process.\n\n**Once you clone our repository, install CoppeliaSim and PyRep, you are ready to start training**. Even though Catalyst is very much focused on reproducibility, due to asynchronous manner of training we can not guarantee the convergence of the training pipeline. If you don't see a progress of the robot after ~1h of training, you can try changing random seed, noise and action step values. In any case, we encourage you to play with the parameters and alter the code to your liking. \n\n**You can launch the pipeline by running** `scripts/run-training.sh`. The moment the training starts, the agents progress can be also monitored visually in the CoppeliaSim simulation.\n\n## Final Results\n![](./images/graph_plot.001.png)\n<em>Figure 4: Reward per episode, collected over around 10k episodes.</em>\n\nOnce the policy converges, you can either test it (run inference) in the simulator or directly on the real robot. This is can be done by editing `configs/config_inference.yml` and passing the path of converged policy (.pth file) to `resume:` variable.  Finally, launch run `scripts/run-inference.sh`.\n\n### **Inference on a real robot**  \n![](./images/real_infer.gif)\n\n## About the Team\n<p align=\"center\">\n  <img src=\"./images/logo.png\" alt=\"Sublime's custom image\"/>\n</p>\n\n\nThis tutorial is based on the research done at [ARRIVAL](https://arrival.com/?gclid=CjwKCAjwnef6BRAgEiwAgv8mQby9ldRbN6itD_fEpRZ2TdgFBeKltK-EPSVPNUhvdoH2s8PnNAYMLxoC5OAQAvD_BwE) by the outstanding robotics team: \n* [Damian Bogunowicz](https://dtransposed.github.io)\n* [Fedor Chervinskii](https://www.linkedin.com/in/chervinskii/)\n* [Alexander Rybnikov](https://www.linkedin.com/in/aleksandr-rybnikov-9a264ab0/)\n* [Komal Vendidandi](https://de.linkedin.com/in/komal-vendidandi). \n\nThe team is creating flexible factories of the future for the assembly of Arrival electric vehicles. One of the topics we are actively working on is transferring the knowledge obtained in the simulation to the physical robot. We encourage you to check out our recent research publication: [Sim2Real for Peg-Hole Insertion with Eye-in-Hand Camera](https://arxiv.org/pdf/2005.14401.pdf). If you have any questions about the contents of that tutorial or simply want to chat about robots, feel free to reach out to us!\n\n\n\n\n\n\n\n",
            "readme_url": "https://github.com/arrival-ltd/catalyst-rl-tutorial",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Sample Efficient Ensemble Learning with Catalyst.RL",
            "arxiv": "2003.14210",
            "year": 2020,
            "url": "http://arxiv.org/abs/2003.14210v2",
            "abstract": "We present Catalyst.RL, an open-source PyTorch framework for reproducible and\nsample efficient reinforcement learning (RL) research. Main features of\nCatalyst.RL include large-scale asynchronous distributed training, efficient\nimplementations of various RL algorithms and auxiliary tricks, such as n-step\nreturns, value distributions, hyperbolic reinforcement learning, etc. To\ndemonstrate the effectiveness of Catalyst.RL, we applied it to a physics-based\nreinforcement learning challenge \"NeurIPS 2019: Learn to Move -- Walk Around\"\nwith the objective to build a locomotion controller for a human musculoskeletal\nmodel. The environment is computationally expensive, has a high-dimensional\ncontinuous action space and is stochastic. Our team took the 2nd place,\ncapitalizing on the ability of Catalyst.RL to train high-quality and\nsample-efficient RL agents in only a few hours of training time. The\nimplementation along with experiments is open-sourced so results can be\nreproduced and novel ideas tried out.",
            "authors": [
                "Sergey Kolesnikov",
                "Valentin Khrulkov"
            ]
        },
        {
            "title": "Addressing Function Approximation Error in Actor-Critic Methods",
            "arxiv": "1802.09477",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.09477v3",
            "abstract": "In value-based reinforcement learning methods such as deep Q-learning,\nfunction approximation errors are known to lead to overestimated value\nestimates and suboptimal policies. We show that this problem persists in an\nactor-critic setting and propose novel mechanisms to minimize its effects on\nboth the actor and the critic. Our algorithm builds on Double Q-learning, by\ntaking the minimum value between a pair of critics to limit overestimation. We\ndraw the connection between target networks and overestimation bias, and\nsuggest delaying policy updates to reduce per-update error and further improve\nperformance. We evaluate our method on the suite of OpenAI gym tasks,\noutperforming the state of the art in every environment tested.",
            "authors": [
                "Scott Fujimoto",
                "Herke van Hoof",
                "David Meger"
            ]
        },
        {
            "title": "Sim2Real for Peg-Hole Insertion with Eye-in-Hand Camera",
            "arxiv": "2005.14401",
            "year": 2020,
            "url": "http://arxiv.org/abs/2005.14401v1",
            "abstract": "Even though the peg-hole insertion is one of the well-studied problems in\nrobotics, it still remains a challenge for robots, especially when it comes to\nflexibility and the ability to generalize. Successful completion of the task\nrequires combining several modalities to cope with the complexity of the real\nworld. In our work, we focus on the visual aspect of the problem and employ the\nstrategy of learning an insertion task in a simulator. We use Deep\nReinforcement Learning to learn the policy end-to-end and then transfer the\nlearned model to the real robot, without any additional fine-tuning. We show\nthat the transferred policy, which only takes RGB-D and joint information\n(proprioception) can perform well on the real robot.",
            "authors": [
                "Damian Bogunowicz",
                "Aleksandr Rybnikov",
                "Komal Vendidandi",
                "Fedor Chervinskii"
            ]
        },
        {
            "title": "Damian Bogunowicz",
            "url": "https://dtransposed.github.io"
        },
        {
            "title": "Fedor Chervinskii",
            "url": "https://www.linkedin.com/in/chervinskii/"
        },
        {
            "title": "Alexander Rybnikov",
            "url": "https://www.linkedin.com/in/aleksandr-rybnikov-9a264ab0/"
        },
        {
            "title": "Komal Vendidandi",
            "url": "https://de.linkedin.com/in/komal-vendidandi"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "OpenAI Gym"
            }
        ]
    },
    "domain": {
        "domain_type": "Playing Games",
        "domain_prob": 0.6377257286163144
    }
}