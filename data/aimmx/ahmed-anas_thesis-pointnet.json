{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "PointNet: *Deep Learning on Point Sets for 3D Classification and Segmentation*",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "ahmed-anas",
                "owner_type": "User",
                "name": "thesis-pointnet",
                "url": "https://github.com/ahmed-anas/thesis-pointnet",
                "stars": 0,
                "pushed_at": "2018-07-05 06:52:56+00:00",
                "created_at": "2018-04-22 13:05:31+00:00",
                "language": "Python",
                "license": "Other",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "22c5dcad1fd5a94afc5e5298ecd3682892851450",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/blob/master/.gitignore"
                    }
                },
                "size": 43
            },
            {
                "type": "code",
                "name": ".vscode",
                "sha": "10ac9c99784520aec330d5781d606b36c7eaef58",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/tree/master/.vscode"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "ef6faceb5f72730fb0a8f51ac6713b7dd93999ab",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/blob/master/LICENSE"
                    }
                },
                "size": 1231
            },
            {
                "type": "code",
                "name": "doc",
                "sha": "0c48f0deaf8c7a2e1f2f9ceb3ada72c6945e261e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/tree/master/doc"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "evaluate.py",
                "sha": "b39e579982ab056002bdfe2fb7b2193f097d1eb8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/blob/master/evaluate.py"
                    }
                },
                "size": 7196
            },
            {
                "type": "code",
                "name": "indoor3d_sem_seg_hdf5_data",
                "sha": "60e8096303dd9404ab76b19eb04fafec675c17b6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/tree/master/indoor3d_sem_seg_hdf5_data"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "models",
                "sha": "f3d9d4973d6fa94ee8fd5ba2d4b57d004a63ec76",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/tree/master/models"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "part_seg",
                "sha": "77a39a2605a20aa825b2796342ecd97a00e97673",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/tree/master/part_seg"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "provider.py",
                "sha": "da751789d93e557309503c61166dceda9c8c41cc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/blob/master/provider.py"
                    }
                },
                "size": 3618
            },
            {
                "type": "code",
                "name": "sem_seg",
                "sha": "8f92b7a94b777285ba84e6dc3f6caae199594d20",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/tree/master/sem_seg"
                    }
                },
                "num_files": 14
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "453b01c729aae7352b54ce1cd4c0a2c292199a82",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/blob/master/train.py"
                    }
                },
                "size": 10928
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "87e544173fa74e82b85dc8a6c3de3ffd94de6771",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ahmed-anas/thesis-pointnet/tree/master/utils"
                    }
                },
                "num_files": 5
            }
        ]
    },
    "authors": [
        {
            "name": "Charles R. Qi",
            "email": "charlesq34@gmail.com",
            "github_id": "charlesq34"
        },
        {
            "name": "Ahmed Anas",
            "email": "ahmed.anas@trilogy.com",
            "github_id": "ahmed-anas"
        },
        {
            "name": "Barry Ridge",
            "github_id": "barryridge"
        },
        {
            "name": "Chengcheng Tang",
            "github_id": "tangchengcheng"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/ahmed-anas/thesis-pointnet",
            "stars": 0,
            "issues": true,
            "readme": "## PointNet: *Deep Learning on Point Sets for 3D Classification and Segmentation*\nCreated by <a href=\"http://charlesrqi.com\" target=\"_blank\">Charles R. Qi</a>, <a href=\"http://ai.stanford.edu/~haosu/\" target=\"_blank\">Hao Su</a>, <a href=\"http://cs.stanford.edu/~kaichun/\" target=\"_blank\">Kaichun Mo</a>, <a href=\"http://geometry.stanford.edu/member/guibas/\" target=\"_blank\">Leonidas J. Guibas</a> from Stanford University.\n\n![prediction example](https://github.com/charlesq34/pointnet/blob/master/doc/teaser.png)\n\n### Introduction\nThis work is based on our [arXiv tech report](https://arxiv.org/abs/1612.00593), which is going to appear in CVPR 2017. We proposed a novel deep net architecture for point clouds (as unordered point sets). You can also check our [project webpage](http://stanford.edu/~rqi/pointnet) for a deeper introduction.\n\nPoint cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input.  Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective.\n\nIn this repository, we release code and data for training a PointNet classification network on point clouds sampled from 3D shapes, as well as for training a part segmentation network on ShapeNet Part dataset.\n\n### Citation\nIf you find our work useful in your research, please consider citing:\n\n\t@article{qi2016pointnet,\n\t  title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},\n\t  author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},\n\t  journal={arXiv preprint arXiv:1612.00593},\n\t  year={2016}\n\t}\n   \n### Installation\n\nInstall <a href=\"https://www.tensorflow.org/get_started/os_setup\" target=\"_blank\">TensorFlow</a>. You may also need to install h5py. The code has been tested with Python 2.7, TensorFlow 1.0.1, CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.\n\nIf you are using PyTorch, you can find a third-party pytorch implementation <a href=\"https://github.com/fxia22/pointnet.pytorch\" target=\"_blank\">here</a>.\n\nTo install h5py for Python:\n```bash\nsudo apt-get install libhdf5-dev\nsudo pip install h5py\n```\n\n### Usage\nTo train a model to classify point clouds sampled from 3D shapes:\n\n    python train.py\n\nLog files and network parameters will be saved to `log` folder in default. Point clouds of <a href=\"http://modelnet.cs.princeton.edu/\" target=\"_blank\">ModelNet40</a> models in HDF5 files will be automatically downloaded (416MB) to the data folder. Each point cloud contains 2048 points uniformly sampled from a shape surface. Each cloud is zero-mean and normalized into an unit sphere. There are also text files in `data/modelnet40_ply_hdf5_2048` specifying the ids of shapes in h5 files.\n\nTo see HELP for the training script:\n\n    python train.py -h\n\nWe can use TensorBoard to view the network architecture and monitor the training progress.\n\n    tensorboard --logdir log\n\nAfter the above training, we can evaluate the model and output some visualizations of the error cases.\n\n    python evaluate.py --visu\n\nPoint clouds that are wrongly classified will be saved to `dump` folder in default. We visualize the point cloud by rendering it into three-view images.\n\nIf you'd like to prepare your own data, you can refer to some helper functions in `utils/data_prep_util.py` for saving and loading HDF5 files.\n\n### Part Segmentation\nTo train a model for object part segmentation, firstly download the data:\n\n    cd part_seg\n    sh download_data.sh\n\nThe downloading script will download <a href=\"http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html\" target=\"_blank\">ShapeNetPart</a> dataset (around 1.08GB) and our prepared HDF5 files (around 346MB).\n\nThen you can run `train.py` and `test.py` in the `part_seg` folder for training and testing (computing mIoU for evaluation).\n\n### License\nOur code is released under MIT License (see LICENSE file for details).\n\n### Selected Projects that Use PointNet\n\n* <a href=\"http://stanford.edu/~rqi/pointnet2/\" target=\"_blank\">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> by Qi et al. (NIPS 2017) A hierarchical feature learning framework on point clouds. The PointNet++ architecture applies PointNet recursively on a nested partitioning of the input point set. It also proposes novel layers for point clouds with non-uniform densities.\n* <a href=\"http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w13/Engelmann_Exploring_Spatial_Context_ICCV_2017_paper.pdf\" target=\"_blank\">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</a> by Engelmann et al. (ICCV 2017 workshop). This work extends PointNet for large-scale scene segmentation.\n* <a href=\"https://arxiv.org/abs/1710.04954\" target=\"_blank\">PCPNET: Learning Local Shape Properties from Raw Point Clouds</a> by Guerrero et al. (arXiv). The work adapts PointNet for local geometric properties (e.g. normal and curvature) estimation in noisy point clouds.\n* <a href=\"https://arxiv.org/abs/1711.06396\" target=\"_blank\">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> by Zhou et al. from Apple (arXiv) This work studies 3D object detection using LiDAR point clouds. It splits space into voxels, use PointNet to learn local voxel features and then use 3D CNN for region proposal, object classification and 3D bounding box estimation.\n* <a href=\"https://arxiv.org/abs/1711.08488\" target=\"_blank\">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (arXiv) A novel framework for 3D object detection with RGB-D data. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017).\n",
            "readme_url": "https://github.com/ahmed-anas/thesis-pointnet",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "PCPNET: Learning Local Shape Properties from Raw Point Clouds",
            "arxiv": "1710.04954",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.04954v4",
            "abstract": "In this paper, we propose PCPNet, a deep-learning based approach for\nestimating local 3D shape properties in point clouds. In contrast to the\nmajority of prior techniques that concentrate on global or mid-level\nattributes, e.g., for shape classification or semantic labeling, we suggest a\npatch-based learning method, in which a series of local patches at multiple\nscales around each point is encoded in a structured manner. Our approach is\nespecially well-adapted for estimating local shape properties such as normals\n(both unoriented and oriented) and curvature from raw point clouds in the\npresence of strong noise and multi-scale features. Our main contributions\ninclude both a novel multi-scale variant of the recently proposed PointNet\narchitecture with emphasis on local shape information, and a series of novel\napplications in which we demonstrate how learning from training data arising\nfrom well-structured triangle meshes, and applying the trained model to noisy\npoint clouds can produce superior results compared to specialized\nstate-of-the-art techniques. Finally, we demonstrate the utility of our\napproach in the context of shape reconstruction, by showing how it can be used\nto extract normal orientation information from point clouds.",
            "authors": [
                "Paul Guerrero",
                "Yanir Kleiman",
                "Maks Ovsjanikov",
                "Niloy J. Mitra"
            ]
        },
        {
            "title": "Frustum PointNets for 3D Object Detection from RGB-D Data",
            "arxiv": "1711.08488",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.08488v2",
            "abstract": "In this work, we study 3D object detection from RGB-D data in both indoor and\noutdoor scenes. While previous methods focus on images or 3D voxels, often\nobscuring natural 3D patterns and invariances of 3D data, we directly operate\non raw point clouds by popping up RGB-D scans. However, a key challenge of this\napproach is how to efficiently localize objects in point clouds of large-scale\nscenes (region proposal). Instead of solely relying on 3D proposals, our method\nleverages both mature 2D object detectors and advanced 3D deep learning for\nobject localization, achieving efficiency as well as high recall for even small\nobjects. Benefited from learning directly in raw point clouds, our method is\nalso able to precisely estimate 3D bounding boxes even under strong occlusion\nor with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection\nbenchmarks, our method outperforms the state of the art by remarkable margins\nwhile having real-time capability.",
            "authors": [
                "Charles R. Qi",
                "Wei Liu",
                "Chenxia Wu",
                "Hao Su",
                "Leonidas J. Guibas"
            ]
        },
        {
            "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
            "arxiv": "1612.00593",
            "year": 2016,
            "url": "http://arxiv.org/abs/1612.00593v2",
            "abstract": "Point cloud is an important type of geometric data structure. Due to its\nirregular format, most researchers transform such data to regular 3D voxel\ngrids or collections of images. This, however, renders data unnecessarily\nvoluminous and causes issues. In this paper, we design a novel type of neural\nnetwork that directly consumes point clouds and well respects the permutation\ninvariance of points in the input. Our network, named PointNet, provides a\nunified architecture for applications ranging from object classification, part\nsegmentation, to scene semantic parsing. Though simple, PointNet is highly\nefficient and effective. Empirically, it shows strong performance on par or\neven better than state of the art. Theoretically, we provide analysis towards\nunderstanding of what the network has learnt and why the network is robust with\nrespect to input perturbation and corruption.",
            "authors": [
                "Charles R. Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J. Guibas"
            ]
        },
        {
            "title": "VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection",
            "arxiv": "1711.06396",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.06396v1",
            "abstract": "Accurate detection of objects in 3D point clouds is a central problem in many\napplications, such as autonomous navigation, housekeeping robots, and\naugmented/virtual reality. To interface a highly sparse LiDAR point cloud with\na region proposal network (RPN), most existing efforts have focused on\nhand-crafted feature representations, for example, a bird's eye view\nprojection. In this work, we remove the need of manual feature engineering for\n3D point clouds and propose VoxelNet, a generic 3D detection network that\nunifies feature extraction and bounding box prediction into a single stage,\nend-to-end trainable deep network. Specifically, VoxelNet divides a point cloud\ninto equally spaced 3D voxels and transforms a group of points within each\nvoxel into a unified feature representation through the newly introduced voxel\nfeature encoding (VFE) layer. In this way, the point cloud is encoded as a\ndescriptive volumetric representation, which is then connected to a RPN to\ngenerate detections. Experiments on the KITTI car detection benchmark show that\nVoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a\nlarge margin. Furthermore, our network learns an effective discriminative\nrepresentation of objects with various geometries, leading to encouraging\nresults in 3D detection of pedestrians and cyclists, based on only LiDAR.",
            "authors": [
                "Yin Zhou",
                "Oncel Tuzel"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ModelNet40"
            },
            {
                "name": "ShapeNet"
            },
            {
                "name": "OCCLUSION"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999684996892785,
        "task": "Object Localization",
        "task_prob": 0.9897091181071989
    }
}