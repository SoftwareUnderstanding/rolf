{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "The CycleGAN Notebook",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "nhannguyencsd",
                "owner_type": "User",
                "name": "vision_cyclegan",
                "url": "https://github.com/nhannguyencsd/vision_cyclegan",
                "stars": 0,
                "pushed_at": "2020-10-30 18:56:06+00:00",
                "created_at": "2020-10-30 16:25:57+00:00",
                "language": "Jupyter Notebook",
                "description": "An image filter using CycleGAN model",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "9847950bf58f152ba946995755761f509ab6a67a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nhannguyencsd/vision_cyclegan/blob/master/LICENSE"
                    }
                },
                "size": 1067
            },
            {
                "type": "code",
                "name": "inference_cyclegan.ipynb",
                "sha": "6b8e0875bdab8b5d3649a33ffa52c5c5c345fd46",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nhannguyencsd/vision_cyclegan/blob/master/inference_cyclegan.ipynb"
                    }
                },
                "size": 2352486
            },
            {
                "type": "code",
                "name": "static",
                "sha": "3a0a1d137d461d76b36d342288158b8ba1859887",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nhannguyencsd/vision_cyclegan/tree/master/static"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "training_cyclegan.ipynb",
                "sha": "f31247ed0ec87723aded0e57dd3f7e1b1a08b9e9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/nhannguyencsd/vision_cyclegan/blob/master/training_cyclegan.ipynb"
                    }
                },
                "size": 22737
            }
        ]
    },
    "authors": [
        {
            "name": "nhannguyencsd",
            "github_id": "nhannguyencsd"
        }
    ],
    "tags": [],
    "description": "An image filter using CycleGAN model",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/nhannguyencsd/vision_cyclegan",
            "stars": 0,
            "issues": true,
            "readme": "# The CycleGAN Notebook\nIn this project, I focus on the implementation of a cycle generative adversarial network model (CycleGAN) in an interactive way by using a jupyter notebook which is helpable to read and run for the training/inference of the model.\n<br/><br/>\n\n## CycleGAN Architecture\n#### High Level\n|[![](static/depict/high_level_from_A.png)]( https://hardikbansal.github.io/CycleGANBlog/ \"Image from https://hardikbansal.github.io/CycleGANBlog/\")| [![](static/depict/high_level_from_B.png)]( https://hardikbansal.github.io/CycleGANBlog/ \"Image from https://hardikbansal.github.io/CycleGANBlog/\")|\n|:---:|:---:|\n#### Low Level\n|[![](static/depict/cyclegan_generator.png)](static/depict/cyclegan_generator.png \"cyclegan_generator\")| [![](static/depict/cyclegan_discriminator.png)](static/depict/cyclegan_discriminator.png \"cyclegan_discriminator\")|\n|:---:|:---:|\n<div></div><br/>\n\n## How It Works\nThe CycleGAN model takes a real image from domain A and converts that image to a fake image in domain B. At the same time, it takes a real image from domain B and then converts it to a fake image in domain A. Here are some results that I ran on the horse2zebra dataset. The first row contains real horse images (domain A). The second row contains fake zebra images(domain B). The third row contains real zebra images (domain B). The last row contains fake horse images(domain A).\n\n|||\n|:---:|:---:|\n|[![epoch 1](static/depict/epoch_1.png)](static/depict/epoch_1.png \"epoch 1\") epoch 1 | [![epoch 33](static/depict/epoch_33.png)](static/depict/epoch_33.png \"epoch 33\") epoch 33|\n| [![epoch 66](static/depict/epoch_66.png)](static/depict/epoch_66.png \"epoch 66\") epoch 66 | [![epoch 99](static/depict/epoch_99.png)](static/depict/epoch_99.png \"epoch 99\") epoch 99 |\n<div></div><br/>\n\n## Technologies\n- Python\n- Pytorch\n- Jupyter notebook\n- Pillow\n- Matplotlib\n<br/><br/>\n\n## Installation and Running\n    $ git clone https://github.com/nhannguyencsd/vision_cyclegan.git\n    $ cd vision_cyclegan\n    $ python3 -m venv venv \n    $ source venv/bin/activate\n    $ pip install -r static/libraries/requirements.txt\n    $ jupyter notebook\n* Once your jupyter notebook is opened, you can run a training_cyclegan.ipynb or inference_cyclegan.ipynb.</li>\n* If you are not able to install libraries from requirements.txt or run on any notebooks, you are welcome [run](https://nhancs.com/project/2) the model on my website.\n<br/><br/>\n\n## Contributing\nIf you found any problems with this project, please let me know by opening an issue. Thanks in advance!\n<br/><br/>\n\n## License\nThis project is licensed under the MIT [License](LICENSE)\n<br/><br/>\n\n## References\nThe CycleGAN paper: [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593) <br/>\nCycleGAN datasets: [https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/) <br/>\nCNN Padding formular: [https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L13_intro-cnn-part2_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L13_intro-cnn-part2_slides.pdf) <br/>\nModel architecture 1: [https://hardikbansal.github.io/CycleGANBlog/](https://hardikbansal.github.io/CycleGANBlog/) <br/>\nModel architecture 2: [https://towardsdatascience.com/cyclegan-learning-to-translate-images-without-paired-training-data-5b4e93862c8d3](https://towardsdatascience.com/cyclegan-learning-to-translate-images-without-paired-training-data-5b4e93862c8d) <br/>\n\n",
            "readme_url": "https://github.com/nhannguyencsd/vision_cyclegan",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "arxiv": "1703.10593",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10593v7",
            "abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.",
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A. Efros"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/"
                    }
                }
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9994609199437401,
        "task": "Image-to-Image Translation",
        "task_prob": 0.9856195165941333
    }
}