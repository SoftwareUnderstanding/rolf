{
    "visibility": {
        "visibility": "public"
    },
    "name": "CheXpert : A Large Chest X-Ray Dataset and Competition",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "gaetandi",
                "owner_type": "User",
                "name": "cheXpert",
                "url": "https://github.com/gaetandi/cheXpert",
                "stars": 40,
                "pushed_at": "2020-01-21 15:01:35+00:00",
                "created_at": "2019-03-05 12:25:08+00:00",
                "language": "Jupyter Notebook",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": "cheXpert_final.ipynb",
                "sha": "3ea2b75e641f5191e5b773ad002aca33149dcd1c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/gaetandi/cheXpert/blob/master/cheXpert_final.ipynb"
                    }
                },
                "size": 319797
            },
            {
                "type": "code",
                "name": "model_ones_2epoch_densenet.tar",
                "sha": "8d85e97b2fd21c6de7f76a3cb647f94f80f0b0c0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/gaetandi/cheXpert/blob/master/model_ones_2epoch_densenet.tar"
                    }
                },
                "size": 84253645
            },
            {
                "type": "code",
                "name": "model_ones_3epoch_densenet.tar",
                "sha": "16edd127aa2dd5f08e201bd14712f247355d3f7c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/gaetandi/cheXpert/blob/master/model_ones_3epoch_densenet.tar"
                    }
                },
                "size": 84253756
            },
            {
                "type": "code",
                "name": "model_zeroes_1epoch_densenet.pth.tar",
                "sha": "1fad54efd68f84ad523e59006717eaf34923835a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/gaetandi/cheXpert/blob/master/model_zeroes_1epoch_densenet.pth.tar"
                    }
                },
                "size": 84253756
            },
            {
                "type": "code",
                "name": "presentation.pdf",
                "sha": "1d0b6cef592f5811a734679c44397b50e55327e9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/gaetandi/cheXpert/blob/master/presentation.pdf"
                    }
                },
                "size": 2011597
            },
            {
                "type": "code",
                "name": "results",
                "sha": "506d8207079de0e7d2205a8f1fc76440581cbfa5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/gaetandi/cheXpert/tree/master/results"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "view1_frontal.jpg",
                "sha": "9f2a383ed8c3f81a805a005f4689338b69578e69",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/gaetandi/cheXpert/blob/master/view1_frontal.jpg"
                    }
                },
                "size": 47919
            }
        ]
    },
    "authors": [
        {
            "name": "Ga\u00ebtan Dissez",
            "github_id": "gaetandi"
        },
        {
            "name": "guillaumeduboc",
            "github_id": "guillaumeduboc"
        }
    ],
    "tags": [
        "chexpert",
        "pytorch",
        "heatmap",
        "densenet-pytorch",
        "resnet"
    ],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/gaetandi/cheXpert",
            "stars": 40,
            "issues": true,
            "readme": "# CheXpert : A Large Chest X-Ray Dataset and Competition\n*A repository created for the MAP583 Deep Learning project*\n\nAuthors: Ga\u00ebtan Dissez & Guillaume Duboc\n\nThis repository uses different sources, listed in the references, and especially ref n\u00b04 for the coding part.\n\nThis competition launched by the Stanford ML group aims at finding a prediction model which could perform as well as radiologist to find different pathologies thanks to chest X-Ray. The Dataset available to train our model is composed of 223,414 chest radiographs of 65,240 patients.\n\nThe **dataset** (the smaller dataset size is 11 GB) can be requested on the [website](https://stanfordmlgroup.github.io/competitions/chexpert/) of the competition.\n\nThis GitHub repository is composed of:\n1- All the code in a jupyter notebook\n2- A few pretrained and saved models\n3- Different plots showing main results\n\n\n# 1. Code\n\nWe implemented this project using Python 3 in the notebook *cheXpert_final.ipynb*.\n\nTo run this organized notebook, you need the following packages: pytorch, PIL, cv2.\n\n# 2. Models\n\nRunning the code, you may ignore the training process if you use one of our pretrained models:\n-  *model_ones_2epoch_densenet.tar* is a DenseNet121 trained for 2 epochs using the policy \"ones\" (uncertain labels are considered positive)\n-  *model_ones_3epoch_densenet.tar* is a DenseNet121 trained for 3 epochs using the policy \"ones\" (uncertain labels are considered positive)\n-  *model_zeroes_1epoch_densenet.pth.tar* is a DenseNet121 trainet for 1 epoch using the policy \"zeroes\" (uncertain labels are considered negative)\n\n# 3. Results\n\nWe first trained a DenseNet121 model using the policy \"ones\" (uncertain labels are considered positive).\nFor this model, we had the following loss during the training of 3 epochs:\n\n<p align=\"center\">\n  <img src=\"https://github.com/gaetandi/cheXpert/blob/master/results/loss_ones_densenet.png\" width=\"480\">\n</p>\n\nAnd the following ROC curves (after the first epoch, the second epoch and the third epoch):\n\n<p align=\"center\">\n  <img src=\"https://github.com/gaetandi/cheXpert/blob/master/results/ROC_densenet.png\" width=\"1800\">\n</p>\n\nYou may check our presentation to see further details about our results.\nThese results do not outperform the results given by the Stanford ML group or evn radiologist, but the are encouraging as you can see on the following plot:\n\n<p align=\"center\">\n  <img src=\"https://github.com/gaetandi/cheXpert/blob/master/results/Edema_radiologistscompare.png\" width=\"400\">\n</p>\n\nThese charts are available in the *results* directory.\n\n# 4. References\n\nPublications : \n1. CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison, *Irvin, Jeremy, et al.*, 2019  [[Arxiv:1901.07031]](https://arxiv.org/pdf/1901.07031.pdf)\n2. CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning, *Rajpurkar, Irvin, et al.*, 2017 [[Arxiv:1711.05225]](https://arxiv.org/pdf/1711.05225.pdf)\n3. Densely Connected Convolutional Networks, *Huang et al.*, 2018 [[Arxiv:1608.06993]](https://arxiv.org/pdf/1608.06993.pdf)\n4. [GitHub Repository](https://github.com/zoogzog/chexnet?fbclid=IwAR11GtcTJDglJpNYbqNIZFPeE4Zk9Ac132-fIVwqIkMItk3GGKY8OvhvVQA)\n",
            "readme_url": "https://github.com/gaetandi/cheXpert",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison",
            "arxiv": "1901.07031",
            "year": 2019,
            "url": "http://arxiv.org/abs/1901.07031v1",
            "abstract": "Large, labeled datasets have driven deep learning methods to achieve\nexpert-level performance on a variety of medical imaging tasks. We present\nCheXpert, a large dataset that contains 224,316 chest radiographs of 65,240\npatients. We design a labeler to automatically detect the presence of 14\nobservations in radiology reports, capturing uncertainties inherent in\nradiograph interpretation. We investigate different approaches to using the\nuncertainty labels for training convolutional neural networks that output the\nprobability of these observations given the available frontal and lateral\nradiographs. On a validation set of 200 chest radiographic studies which were\nmanually annotated by 3 board-certified radiologists, we find that different\nuncertainty approaches are useful for different pathologies. We then evaluate\nour best model on a test set composed of 500 chest radiographic studies\nannotated by a consensus of 5 board-certified radiologists, and compare the\nperformance of our model to that of 3 additional radiologists in the detection\nof 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the\nmodel ROC and PR curves lie above all 3 radiologist operating points. We\nrelease the dataset to the public as a standard benchmark to evaluate\nperformance of chest radiograph interpretation models.\n  The dataset is freely available at\nhttps://stanfordmlgroup.github.io/competitions/chexpert .",
            "authors": [
                "Jeremy Irvin",
                "Pranav Rajpurkar",
                "Michael Ko",
                "Yifan Yu",
                "Silviana Ciurea-Ilcus",
                "Chris Chute",
                "Henrik Marklund",
                "Behzad Haghgoo",
                "Robyn Ball",
                "Katie Shpanskaya",
                "Jayne Seekins",
                "David A. Mong",
                "Safwan S. Halabi",
                "Jesse K. Sandberg",
                "Ricky Jones",
                "David B. Larson",
                "Curtis P. Langlotz",
                "Bhavik N. Patel",
                "Matthew P. Lungren",
                "Andrew Y. Ng"
            ]
        },
        {
            "title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning",
            "arxiv": "1711.05225",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.05225v3",
            "abstract": "We develop an algorithm that can detect pneumonia from chest X-rays at a\nlevel exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer\nconvolutional neural network trained on ChestX-ray14, currently the largest\npublicly available chest X-ray dataset, containing over 100,000 frontal-view\nX-ray images with 14 diseases. Four practicing academic radiologists annotate a\ntest set, on which we compare the performance of CheXNet to that of\nradiologists. We find that CheXNet exceeds average radiologist performance on\nthe F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and\nachieve state of the art results on all 14 diseases.",
            "authors": [
                "Pranav Rajpurkar",
                "Jeremy Irvin",
                "Kaylie Zhu",
                "Brandon Yang",
                "Hershel Mehta",
                "Tony Duan",
                "Daisy Ding",
                "Aarti Bagul",
                "Curtis Langlotz",
                "Katie Shpanskaya",
                "Matthew P. Lungren",
                "Andrew Y. Ng"
            ]
        },
        {
            "title": "Densely Connected Convolutional Networks",
            "arxiv": "1608.06993",
            "year": 2016,
            "url": "http://arxiv.org/abs/1608.06993v5",
            "abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9992186235210877,
        "task": "Image Classification",
        "task_prob": 0.9245023230427507
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            }
        ]
    }
}