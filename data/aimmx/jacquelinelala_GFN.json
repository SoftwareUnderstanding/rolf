{
    "visibility": {
        "visibility": "public"
    },
    "name": "GFN",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "jacquelinelala",
                "owner_type": "User",
                "name": "GFN",
                "url": "https://github.com/jacquelinelala/GFN",
                "stars": 129,
                "pushed_at": "2021-02-22 02:23:03+00:00",
                "created_at": "2018-07-17 07:06:54+00:00",
                "language": "Python",
                "description": "Gated Fusion Network for Joint Image Deblurring and Super-Resolution(BMVC 2018 Oral)",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".DS_Store",
                "sha": "4bb725a29ddb07ecefbe9248d3150a7cddbbb493",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jacquelinelala/GFN/blob/master/.DS_Store"
                    }
                },
                "size": 10244
            },
            {
                "type": "code",
                "name": "datasets",
                "sha": "610e3758057223ae951b6fd00aac04dd40c479d9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jacquelinelala/GFN/tree/master/datasets"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "evaluation",
                "sha": "a1ba7e6d21d6f9081aaf4eff63784036c7b1c16e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jacquelinelala/GFN/tree/master/evaluation"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "h5_generator",
                "sha": "dde0a8a461879147e01e93d9963288af31671d9c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jacquelinelala/GFN/tree/master/h5_generator"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "models",
                "sha": "f30c13d79a664044529718996a62903b05cee1cb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jacquelinelala/GFN/tree/master/models"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "networks",
                "sha": "74397801a0001f543d0c4d915a4ec8b018cc8fcb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jacquelinelala/GFN/tree/master/networks"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "test_GFN_4x.py",
                "sha": "3e5c2919c30b99f7b8420c7a725ae06cdd3a67dd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jacquelinelala/GFN/blob/master/test_GFN_4x.py"
                    }
                },
                "size": 5354
            },
            {
                "type": "code",
                "name": "train_GFN_4x.py",
                "sha": "345e25ed8474c34aa7b20066158741f63a57ac13",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/jacquelinelala/GFN/blob/master/train_GFN_4x.py"
                    }
                },
                "size": 6616
            }
        ]
    },
    "authors": [
        {
            "name": "xinyi zhang",
            "email": "cvxinyizhang@gmail.com",
            "github_id": "jacquelinelala"
        }
    ],
    "tags": [],
    "description": "Gated Fusion Network for Joint Image Deblurring and Super-Resolution(BMVC 2018 Oral)",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/jacquelinelala/GFN",
            "stars": 129,
            "issues": true,
            "readme": "# GFN\n\n**\"Gated Fusion Network for Joint Image Deblurring and Super-Resolution\"** by [Xinyi Zhang](http://xinyizhang.tech), Hang Dong, [Zhe Hu](http://eng.ucmerced.edu/people/zhu), [Wei-Sheng Lai](http://graduatestudents.ucmerced.edu/wlai24/), Fei Wang, [Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/)**(oral presentation on BMVC2018)**.\n\n* [arXiv](https://arxiv.org/abs/1807.10806)\n* [Slide](http://xinyizhang.tech/files/BMVC_slides.ppt.zip)\n* [Supplementary materials](http://xinyizhang.tech/files/supplement_bmvc2018.zip)\n\nThere are more details you can find on [Project Website : http://xinyizhang.tech/bmvc2018](http://xinyizhang.tech/bmvc2018/).\n\n![Archi](http://xinyizhang.tech/content/images/2019/03/archi.jpg)\n![heatmap](http://xinyizhang.tech/content/images/2019/02/feature.jpg)\n\n## Improve the training process\nIn order to obtain a more stable training process, now we adopt a three-step training strategy, which differs from our paper and improves PSNR from 27.74dB to 27.81dB on LR-GOPRO 4x dataset.\n\n| Model | LR-GOPRO 4x PSNR(dB) | Time(s) |\n|  :-----  |  :-----:  | :-----:  |\n|  [SCGAN](https://sites.google.com/view/xiangyuxu/deblursr_iccv17)  |  22.74  | 0.66  |\n|  [SRResNet](https://arxiv.org/abs/1609.04802)  |  24.40  | 0.07  |\n|  ED-DSRN  |  26.44  | 0.10  |\n|  [DeepDeblur](https://github.com/SeungjunNah/DeepDeblur_release) + [EDSR](https://github.com/thstkdgus35/EDSR-PyTorch)  |  25.09  | 2.70  |\n|  [EDSR](https://github.com/thstkdgus35/EDSR-PyTorch) + [DeepDeblur](https://github.com/SeungjunNah/DeepDeblur_release)  |  26.35  | 8.10  |\n|  GFN(BMVC paper)  |  27.74  | 0.07  |\n|  GFN(Now)  |  27.81  | 0.07  |\n\n## Dependencies\n* Python 3.6\n* PyTorch >= 0.4.0\n* torchvision\n* numpy\n* skimage\n* h5py\n* MATLAB\n\n## How to test:\n### Test on LR-GOPRO Validation\n#### Test on the latest trained model\nThis model is the result of the third step with 55 epoch.\n1. Git clone this repository.\n```bash\n$git clone https://github.com/jacquelinelala/GFN.git\n$cd GFN\n```\n2. Download the original GOPRO_Large dataset from [Google Drive](https://drive.google.com/file/d/1H0PIXvJH4c40pk7ou6nAwoxuR4Qh_Sa2/view?usp=sharing).\n3. Generate the validation images of LR-GOPRO dataset: Run matlab function ``GFN/h5_generator/gopro_val_generator.m``. The generated test images will be stored in your_downloads_directory/GOPRO_Large/Validation_4x.\n\n(If you don't have access to MATLAB, we offer a validation dataset for testing. You can download it from [GoogleDrive](https://drive.google.com/open?id=11TD3gVRtjlOobT8k9x2oXjEOx-dLtoDt) or [Pan Baidu](https://pan.baidu.com/s/1vsVTLoBA8pmOz_omNLUQTw).)\n```bash\n>> folder = 'your_downloads_directory/GOPRO_Large'; # You should replace the your_downloads_directory by your GOPRO_Large's directory.\n>> gopro_val_generator(folder)\n```\n4. Download the trained model ``GFN_epoch_55.pkl`` from [here](http://xinyizhang.tech/files/GFN_epoch_55.pkl.zip), then unzip and move the ``GFN_epoch_55.pkl`` to ``GFN/models`` folder.\n\n5. Run the ``GFN/test_GFN_x4.py`` with cuda on command line: \n```bash\nGFN/$python test_GFN_x4.py --dataset your_downloads_directory/GOPRO_Large/Validation_4x\n```\nThen the deblurring and super-solving images ending with GFN_4x.png are in the directory of GOPRO_Large/Validation/Results.\n\n6. Calculate the PSNR using Matlab function ``GFN/evaluation/test_RGB.m``. The output of the average PSNR is 27.810232 dB. You can also use the ``GFN/evaluation/test_bicubic.m`` to calculate the bicubic method.  \n```bash\n>> folder = 'your_downloads_directory/GOPRO_Large';\n>> test_RGB(folder)\n```\n\n## How to train\n### Train on LR-GOPRO dataset\nYou should accomplish the first two steps in **Test on LR-GOPRO Validation** before the following steps.\n#### Train from scratch\n1. Generate the train hdf5 files of LR-GOPRO dataset: Run the matlab function ``gopro_hdf5_generator.m`` which is in the directory of GFN/h5_generator. The generated hdf5 files are stored in the your_downloads_directory/GOPRO_Large/GOPRO_train256_4x_HDF5.\n```bash\n>> folder = 'your_downloads_directory/GOPRO_Large';\n>> gopro_hdf5_generator(folder)\n```\n2. Run the ``GFN/train_GFN_4x.py`` with cuda on command line:\n```bash\nGFN/$python train_GFN_4x.py --dataset your_downloads_directory/GOPRO_Large/GOPRO_train256_4x_HDF5\n```\n3. The three step intermediate models will be respectively saved in models/1/ models/2 and models/3. You can also use the following command to test the intermediate results during the training process.\nRun the ``GFN/test_GFN_x4.py`` with cuda on command line: \n```bash\nGFN/$python test_GFN_x4.py --dataset your_downloads_directory/GOPRO_Large/Validation_4x --intermediate_process models/1/GFN_epoch_30.pkl # We give an example of step1 epoch30. You can replace another pkl file in models/.\n```\n#### Resume training from breakpoints\nSince the training process will take 3 or 4 days, you can use the following command to resume the training process from any breakpoints.\nRun the ``GFN/train_GFN_4x.py`` with cuda on command line:\n```bash\nGFN/$python train_GFN_4x.py --dataset your_downloads_directory/GOPRO_Large/GOPRO_train256_4x_HDF5 --resume models/1/GFN_epoch_30.pkl # Just an example of step1 epoch30.\n```\n## Citation\n\nIf you use these models in your research, please cite:\n\n\t@conference{Zhang2018,\n\t\tauthor = {Xinyi Zhang and Hang Dong and Zhe Hu and Wei-Sheng Lai and Fei Wang and Ming-Hsuan Yang},\n\t\ttitle = {Gated Fusion Network for Joint Image Deblurring and Super-Resolution},\n\t\tbooktitle = {BMVC},\n\t\tyear = {2018}\n\t}\n\n",
            "readme_url": "https://github.com/jacquelinelala/GFN",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Gated Fusion Network for Joint Image Deblurring and Super-Resolution",
            "arxiv": "1807.10806",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.10806v1",
            "abstract": "Single-image super-resolution is a fundamental task for vision applications\nto enhance the image quality with respect to spatial resolution. If the input\nimage contains degraded pixels, the artifacts caused by the degradation could\nbe amplified by super-resolution methods. Image blur is a common degradation\nsource. Images captured by moving or still cameras are inevitably affected by\nmotion blur due to relative movements between sensors and objects. In this\nwork, we focus on the super-resolution task with the presence of motion blur.\nWe propose a deep gated fusion convolution neural network to generate a clear\nhigh-resolution frame from a single natural image with severe blur. By\ndecomposing the feature extraction step into two task-independent streams, the\ndual-branch design can facilitate the training process by avoiding learning the\nmixed degradation all-in-one and thus enhance the final high-resolution\nprediction results. Extensive experiments demonstrate that our method generates\nsharper super-resolved images from low-resolution inputs with high\ncomputational efficiency.",
            "authors": [
                "Xinyi Zhang",
                "Hang Dong",
                "Zhe Hu",
                "Wei-Sheng Lai",
                "Fei Wang",
                "Ming-Hsuan Yang"
            ]
        },
        {
            "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
            "arxiv": "1609.04802",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.04802v5",
            "abstract": "Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.",
            "authors": [
                "Christian Ledig",
                "Lucas Theis",
                "Ferenc Huszar",
                "Jose Caballero",
                "Andrew Cunningham",
                "Alejandro Acosta",
                "Andrew Aitken",
                "Alykhan Tejani",
                "Johannes Totz",
                "Zehan Wang",
                "Wenzhe Shi"
            ]
        },
        {
            "title": "Slide",
            "url": "http://xinyizhang.tech/files/BMVC_slides.ppt.zip"
        },
        {
            "title": "Supplementary materials",
            "url": "http://xinyizhang.tech/files/supplement_bmvc2018.zip"
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999375866877571,
        "task": "Image Generation",
        "task_prob": 0.8903888691383877
    }
}