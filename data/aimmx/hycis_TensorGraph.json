{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "`master` ![Build Status](http://54.222.242.222:1010/job/TensorGraph/master)",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "hycis",
                "owner_type": "User",
                "name": "TensorGraph",
                "url": "https://github.com/hycis/TensorGraph",
                "stars": 68,
                "pushed_at": "2021-06-30 04:45:56+00:00",
                "created_at": "2016-01-06 04:59:00+00:00",
                "language": "Python",
                "description": "A tensorflow library for building all kinds of models",
                "license": "Apache License 2.0",
                "frameworks": [
                    "NLTK",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "47e0ce40944c153325d4f7504b4acc40b579da5a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/.gitattributes"
                    }
                },
                "size": 48
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "ed8f3056db95732ebd05d28f9a0f01a0111ab8a6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/.gitignore"
                    }
                },
                "size": 93
            },
            {
                "type": "code",
                "name": ".travis.yml",
                "sha": "9dff6dd23eeefb6d6adc061ad014ff6ea63eb653",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/.travis.yml"
                    }
                },
                "size": 780
            },
            {
                "type": "code",
                "name": "Jenkinsfile",
                "sha": "46a2344886110cc3f84f2adef5a266feabaf5366",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/Jenkinsfile"
                    }
                },
                "size": 10835
            },
            {
                "type": "code",
                "name": "LICENCE",
                "sha": "d4443a9f0760cad0b2a7a243607677dfc6165cfb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/LICENCE"
                    }
                },
                "size": 11418
            },
            {
                "type": "code",
                "name": "MANIFEST.in",
                "sha": "d9eaa8006023ce4d393da85d3e60dee9808f6290",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/MANIFEST.in"
                    }
                },
                "size": 61
            },
            {
                "type": "code",
                "name": "__init__.py",
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/__init__.py"
                    }
                },
                "size": 0
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "08b54726a6692610c13e0160e6f56220ec5053b9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/tree/master/docs"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "draw",
                "sha": "619510546c01395eb161f60f3e886d1e58ce4f0d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/tree/master/draw"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "examples",
                "sha": "e58e9e2f762168325bce5eb4722df26c36a460ea",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/tree/master/examples"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "exclude.txt",
                "sha": "14ce49fcb5fb01bddd36b6a58f471db62dec3956",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/exclude.txt"
                    }
                },
                "size": 11
            },
            {
                "type": "code",
                "name": "pipupdate.sh",
                "sha": "c07982cc973f3875d3f0914fe7225d788e7e2991",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/pipupdate.sh"
                    }
                },
                "size": 168
            },
            {
                "type": "code",
                "name": "setup.cfg",
                "sha": "b88034e414bc7b80d686e3c94d516305348053ea",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/setup.cfg"
                    }
                },
                "size": 40
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "9e4a51af09b2dcb3ca36a262b2c6106a4ec32149",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/blob/master/setup.py"
                    }
                },
                "size": 781
            },
            {
                "type": "code",
                "name": "tensorgraph",
                "sha": "18cc72518a29d91900f824ba1d0b382dc66c7b81",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/tree/master/tensorgraph"
                    }
                },
                "num_files": 14
            },
            {
                "type": "code",
                "name": "test",
                "sha": "5fca102b391a9523743a550de3f41154913d3dfe",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/hycis/TensorGraph/tree/master/test"
                    }
                },
                "num_files": 12
            }
        ]
    },
    "authors": [
        {
            "name": "Joe Wu",
            "github_id": "hycis"
        },
        {
            "name": "Joe Wu",
            "github_id": "hyciswu"
        },
        {
            "name": "Abin Simon",
            "email": "mail@meain.io",
            "github_id": "meain"
        },
        {
            "name": "Liying LIU",
            "github_id": "liying-liu"
        },
        {
            "name": "Vinay M",
            "github_id": "rmdort"
        },
        {
            "name": "Sean Saito",
            "email": "saitosean@ymail.com",
            "github_id": "seansaito"
        }
    ],
    "tags": [
        "tensorflow",
        "deep-learning"
    ],
    "description": "A tensorflow library for building all kinds of models",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/hycis/TensorGraph",
            "stars": 68,
            "issues": true,
            "readme": "`master` [![Build Status](http://54.222.242.222:1010/buildStatus/icon?job=TensorGraph/master)](http://54.222.242.222:1010/job/TensorGraph/master)\n`develop` [![Build Status](http://54.222.242.222:1010/buildStatus/icon?job=TensorGraph/develop)](http://54.222.242.222:1010/job/TensorGraph/develop)\n\n\n# TensorGraph\nTensorGraph is a simple, lean, and clean framework on TensorFlow for building any imaginable models.\n\nAs deep learning becomes more and more common and the architectures becoming more\nand more complicated, it seems that we need some easy to use framework to quickly\nbuild these models and that's what TensorGraph is designed for. It's a very simple\nframework that adds a very thin layer above tensorflow. It is for more advanced\nusers who want to have more control and flexibility over his model building and\nwho wants efficiency at the same time.\n\n-----\n## Target Audience\nTensorGraph is targeted more at intermediate to advance users who feel keras or\nother packages is having too much restrictions and too much black box on model\nbuilding, and someone who don't want to rewrite the standard layers in tensorflow\nconstantly. Also for enterprise users who want to share deep learning models\neasily between teams.\n\n-----\n## Install\n\nFirst you need to install [tensorflow](https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html)\n\nTo install tensorgraph for bleeding edge version via pip\n```bash\nsudo pip install --upgrade git+https://github.com/hycis/TensorGraph.git@master\n```\nor simply clone and add to `PYTHONPATH`.\n```bash\ngit clone https://github.com/hycis/TensorGraph.git\nexport PYTHONPATH=/path/to/TensorGraph:$PYTHONPATH\n```\nin order for the install to persist via export `PYTHONPATH`. Add `PYTHONPATH=/path/to/TensorGraph:$PYTHONPATH` to your `.bashrc` for linux or\n`.bash_profile` for mac. While this method works, you will have to ensure that\nall the dependencies in [setup.py](setup.py) are installed.\n\n-----\n## Everything in TensorGraph is about Layers\nEverything in TensorGraph is about layers. A model such as VGG or Resnet can be a layer. An identity block from Resnet or a dense block from Densenet can be a layer as well. Building models in TensorGraph is same as building a toy with lego. For example you can create a new model (layer) by subclass the `BaseModel` layer and use `DenseBlock` layer inside your `ModelA` layer.\n\n```python\nfrom tensorgraph.layers import DenseBlock, BaseModel, Flatten, Linear, Softmax\nimport tensorgraph as tg\nclass ModelA(BaseModel):\n    @BaseModel.init_name_scope\n    def __init__(self):\n        layers = []\n        layers.append(DenseBlock())\n        layers.append(Flatten())\n        layers.append(Linear())\n        layers.append(Softmax())\n        self.startnode = tg.StartNode(input_vars=[None])\n        hn = tg.HiddenNode(prev=[self.startnode], layers=layers)\n        self.endnode = tg.EndNode(prev=[hn])\n```\n\nif someone wants to use your `ModelA` in his `ModelB`, he can easily do this\n```python\nclass ModelB(BaseModel):\n    @BaseModel.init_name_scope\n    def __int__(self):\n        layers = []\n        layers.append(ModelA())\n        layers.append(Linear())\n        layers.append(Softmax())\n        self.startnode = tg.StartNode(input_vars=[None])\n        hn = tg.HiddenNode(prev=[self.startnode], layers=layers)\n        self.endnode = tg.EndNode(prev=[hn])\n```\n\ncreating a layer only created all the `Variables`. To connect the `Variables` into a graph, you can do a `train_fprop(X)` or `test_fprop(X)` to create the tensorflow graph. By abstracting `Variable` creation away from linking the `Variable` nodes into graph prevent the problem of certain tensorflow layers that always reinitialise its weights when it's called, example the [`tf.nn.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization) layer. Also having a separate channel for training and testing is to cater to layers with different training and testing behaviours such as batchnorm and dropout.\n\n```python\nmodelb = ModelB()\nX_ph = tf.placeholder()\ny_train = modelb.train_fprop(X_ph)\ny_test = modelb.test_fprop(X_ph)\n```\n\ncheckout some well known models in TensorGraph\n1. [VGG16 code](tensorgraph/layers/backbones.py#L37) and [VGG19 code](tensorgraph/layers/backbones.py#L125) - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n2. [DenseNet code](tensorgraph/layers/backbones.py#L477) - [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)\n3. [ResNet code](tensorgraph/layers/backbones.py#L225) - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n4. [Unet code](tensorgraph/layers/backbones.py#L531) - [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n\n-----\n## Type of Layers\nThere are three types of layers, `BaseLayer`, `BaseModel` and `Merge`.\n\n### BaseLayer\n`BaseLayer` is a low lying layer that wraps tensorflow codes directly, and define\nthe low level operations that we want the tensorflow to perform within a layer.\nWhen implementing `BaseLayer` we need to implement `_train_fprop()` and `_test_fprop()`,\nby default `_test_fprop()` calls `_train_fprop()`.\n\n```python\nclass MyLayer(BaseLayer):\n\n    @BaseLayer.init_name_scope\n    def __init__(self):\n        ''' place all your variables and variables initialization here. '''\n        pass\n\n    @BaseLayer.init_name_scope\n    def __init_var__(self, state_below):\n        '''Define variables which requires input information from state_below,\n           this is called during forward propagation\n        '''\n        pass\n\n    def _train_fprop(self, state_below):\n        '''\n        your tensorflow operations for training,\n        do not initialize variables here.\n        '''\n        pass\n\n    def _test_fprop(self, state_below):\n        '''\n        your tensorflow operations for testing, do not initialize variables\n        here. Defaults to _train_fprop.\n        '''\n        pass\n```\n\nTo use `BaseLayer`, we can initialize the `Variables` inside ```__init__``` and/or\n```__init_var__(self, state_below)``` if our layer requires information from the\nlayer below.\n\n\n### BaseModel\n`BaseModel` is a higher level layer that can be made up of BaseLayers and\nBaseModels. For BaseModel, a default implementation of `_train_fprop`\nand `_test_fprop` has been done for a single `StartNode` and single `EndNode`\nGraph, to use this default implementation, we have to define `self.startnode`\nand `self.endnode` inside BaseModel's ```__init__```.\n\nFor Graph defined inside `BaseModel`, `BaseModel` will automatically call\nthe `_train_fprop` and `_test_fprop` within each layer inside its model.\n\n```python\nclass MyLayer(BaseModel):\n    def __init__(self):\n        '''\n         place all your layers inside here and define self.startnode and\n         self.endnode\n         example:\n           layers = []\n           layers.append(DenseBlock())\n           layers.append(Flatten())\n           layers.append(Linear())\n           layers.append(Softmax())\n           self.startnode = tg.StartNode(input_vars=[None])\n           hn = tg.HiddenNode(prev=[self.startnode], layers=layers)\n           self.endnode = tg.EndNode(prev=[hn])\n        '''\n        pass\n```\nIt is possible for BaseModel to return multiple outputs, example\n```python\nclass MyLayerFork(BaseModel):\n\n    @BaseModel.init_name_scope\n    def __init__(self):\n       # a Y shape model, where we have one input and two outputs\n       self.startnode = tg.StartNode(input_vars=[None])\n       # first fork output\n       layers = []\n       layers.append(Linear())\n       layers.append(Softmax())\n       hn = tg.HiddenNode(prev=[self.startnode], layers=layers)\n\n       # second fork output\n       layers2 = []\n       layers2.append(Linear())\n       layers2.append(Softmax())\n       hn2 = tg.HiddenNode(prev=[self.startnode], layers=layers2)\n\n       # two forks outputs\n       self.endnode = tg.EndNode(prev=[hn, h2])\n```\nIn this case, a call to `train_fprop` will return two outputs\n```python\nmylayer = MylayerFork()\ny1, y2 = mylayer.train_fprop(X_ph)\n```\n\n#### Customize inputs and outputs for BaseModel\n\nAnother way to customize your own inputs and outputs is to redefine `_train_fprop`\nand `_test_fprop` within `BaseModel`.\n\nThe default `_train_fprop` and `_test_fprop` in `BaseModel` looks like this\n\n```python\nclass BaseModel(Template):\n\n    @staticmethod\n    def check_y(y):\n        if len(y) == 1:\n            return y[0]\n        elif len(y) > 1:\n            return y\n        else:\n            raise Exception('{} is empty or not a list'.format(y))\n\n\n    def _train_fprop(self, *state_belows):\n        self.startnode.input_vars = state_belows\n        graph = Graph(start=[self.startnode], end=[self.endnode])\n        y = graph.train_fprop()\n        return BaseModel.check_y(y)\n\n\n    def _test_fprop(self, *state_belows):\n        self.startnode.input_vars = state_belows\n        graph = Graph(start=[self.startnode], end=[self.endnode])\n        y = graph.test_fprop()\n        return BaseModel.check_y(y)\n```\n\nfor the `MyLayerFork` Model, for two inputs and two outputs, we can redefine it\nwith multiple `StartNodes` and `EndNodes` within `_train_fprop` and `_test_fprop`.\n\n```python\nclass MyLayerFork(BaseModel):\n\n    @BaseModel.init_name_scope\n    def __init__(self):\n       # multiple inputs and multiple outputs\n\n       self.startnode1 = tg.StartNode(input_vars=[None])\n       self.startnode2 = tg.StartNode(input_vars=[None])\n\n       layers1 = []\n       layers1.append(Linear())\n       layers1.append(Softmax())\n       hn1 = tg.HiddenNode(prev=[self.startnode1], layers=layers)\n\n       layers2 = []\n       layers2.append(Linear())\n       layers2.append(Softmax())\n       hn2 = tg.HiddenNode(prev=[self.startnode2], layers=layers2)\n\n       # two forks outputs\n       self.endnode1 = tg.EndNode(prev=[hn1])\n       self.endnode2 = tg.EndNode(prev=[hn2])\n\n\n     def _train_fprop(self, input1, input2):\n         self.startnode1.input_vars = [input1]\n         self.startnode2.input_vars = [input2]\n         graph = Graph(start=[self.startnode1, self.startnode2], end=[self.endnode1, self.endnode2])\n         y = graph.train_fprop()\n         return BaseModel.check_y(y)\n\n\n     def _test_fprop(self, input1, input2):\n         self.startnode1.input_vars = [input1]\n         self.startnode2.input_vars = [input2]\n         graph = Graph(start=[self.startnode1, self.startnode2], end=[self.endnode1, self.endnode2])\n         y = graph.test_fprop()\n         return BaseModel.check_y(y)\n\n\nif __name__ == '__main__':\n    model = MyLayerFork()\n    y1, y2 = model.train_fprop(X1, X2)\n```\n\n\n### Merge\nWhen we have more than one outputs from previous layer and we want to merge them,\nwe can use the `Merge` layer in [tensorgraph.layers.merge.Merge](./tensorgraph/layers/merge.py)\nto merge multiple inputs into one.\n\n```python\nclass Concat(Merge):\n    @Merge.init_name_scope\n    def __init__(self, axis=1):\n        '''\n        Concat which is a Merge layer is used to concat the list of states from\n        layer below into one state\n        '''\n        self.axis = axis\n\n    def _train_fprop(self, state_list):\n        return tf.concat(axis=self.axis, values=state_list)\n```\n\nWe can use `Merge` layer in conjunction with `BaseModel` layer with multiple outputs,\nexample\n\n```python\nclass MyLayerMergeFork(BaseModel):\n    def __init__(self):\n        layers = []\n        # fork layer from above example\n        layers.append(MyLayerFork())\n        # merge layer\n        layers.append(Concat())\n        self.startnode = tg.StartNode(input_vars=[None])\n        hn = tg.HiddenNode(prev=[self.startnode], input_merge_mode=NoChange(), layers=layers)\n        self.endnode = tg.EndNode(prev=[hn])\n```\n\n-----\n## How TensorGraph Works?\nIn TensorGraph models, layers are put into nodes and nodes are connected together\ninto graph. When we create nodes and layers, we also initializes all the tensorflow\n`Variables`, then we connect the nodes together to form a computational graph.\nThe initialization of `Variables` and the linking of `Variables` into a computational\ngraph are two separate steps. By splitting them into two separate steps, we ensure\nthe flexibility of building our computational graph without the worry of accidental\nreinitialization of the `Variables`.\nWe defined three types of nodes\n\n1. StartNode : for inputs to the graph\n2. HiddenNode : for putting sequential layers inside\n3. EndNode : for getting outputs from the model\n\nWe put all the sequential layers into a `HiddenNode`, `HiddenNode` can be connected\nto another `HiddenNode` or `StartNode`, the nodes are connected together to form\nan architecture. The graph always starts with `StartNode` and ends with `EndNode`.\nOnce we have defined an architecture, we can use the `Graph` object to connect the\npath we want in the architecture, there can be multiple StartNodes (s1, s2, etc)\nand multiple EndNodes (e1, e2, etc), we can define which path we want in the\nentire architecture, example to link from `s2` to `e1`. The `StartNode` is where you place\nyour starting point, it can be a `placeholder`, a symbolic output from another graph,\nor data output from `tfrecords`. `EndNode` is where you want to get an output from\nthe graph, where the output can be used to calculate loss or simply just a peek at the\noutputs at that particular layer. Below shows an\n[example](examples/example.py) of building a tensor graph.\n\n-----\n## Graph Example\n<img src=\"draw/graph.png\" height=\"250\">\n\nFirst define the `StartNode` for putting the input placeholder\n```python\ny1_dim = 50\ny2_dim = 100\nbatchsize = 32\nlearning_rate = 0.01\n\ny1 = tf.placeholder('float32', [None, y1_dim])\ny2 = tf.placeholder('float32', [None, y2_dim])\ns1 = StartNode(input_vars=[y1])\ns2 = StartNode(input_vars=[y2])\n```\nThen define the `HiddenNode` for putting the sequential layers in each `HiddenNode`\n```python\nh1 = HiddenNode(prev=[s1, s2],\n                input_merge_mode=Concat(),\n                layers=[Linear(y2_dim), RELU()])\nh2 = HiddenNode(prev=[s2],\n                layers=[Linear(y2_dim), RELU()])\nh3 = HiddenNode(prev=[h1, h2],\n                input_merge_mode=Sum(),\n                layers=[Linear(y1_dim), RELU()])\n                layers=[Linear(y1_dim+y2_dim, y2_dim), RELU()])\nh2 = HiddenNode(prev=[s2],\n                layers=[Linear(y2_dim, y2_dim), RELU()])\nh3 = HiddenNode(prev=[h1, h2],\n                input_merge_mode=Sum(),\n                layers=[Linear(y2_dim, y1_dim), RELU()])\n```\nThen define the `EndNode`. `EndNode` is used to back-trace the graph to connect\nthe nodes together.\n```python\ne1 = EndNode(prev=[h3])\ne2 = EndNode(prev=[h2])\n```\nFinally build the graph by putting `StartNodes` and `EndNodes` into `Graph`, we\ncan choose to use the entire architecture by using all the `StartNodes` and `EndNodes`\nand run the forward propagation to get symbolic output from train mode. The number\nof outputs from `graph.train_fprop` is the same as the number of `EndNodes` put\ninto `Graph`\n```python\ngraph = Graph(start=[s1, s2], end=[e1, e2])\no1, o2 = graph.train_fprop()\n```\nor we can choose which node to start and which node to end, example\n```python\ngraph = Graph(start=[s2], end=[e1])\no1, = graph.train_fprop()\n```\nFinally build an optimizer to optimize the objective function\n```python\no1_mse = tf.reduce_mean((y1 - o1)**2)\no2_mse = tf.reduce_mean((y2 - o2)**2)\nmse = o1_mse + o2_mse\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)\n```\n\n-----\n## TensorGraph on Multiple GPUS\nTo use tensorgraph on multiple gpus, you can easily integrate it with [horovod](https://github.com/uber/horovod).\n\n```python\nimport horovod.tensorflow as hvd\nfrom tensorflow.python.framework import ops\nimport tensorflow as tf\nhvd.init()\n\n# tensorgraph model derived previously\nmodelb = ModelB()\nX_ph = tf.placeholder()\ny_ph = tf.placeholder()\ny_train = modelb.train_fprop(X_ph)\ny_test = modelb.test_fprop(X_ph)\n\ntrain_cost = mse(y_train, y_ph)\ntest_cost = mse(y_test, y_ph)\n\nopt = tf.train.RMSPropOptimizer(0.001)\nopt = hvd.DistributedOptimizer(opt)\n\n# required for BatchNormalization layer\nupdate_ops = ops.get_collection(ops.GraphKeys.UPDATE_OPS)\nwith ops.control_dependencies(update_ops):\n    train_op = opt.minimize(train_cost)\n\ninit_op = tf.group(tf.global_variables_initializer(),\n                   tf.local_variables_initializer())\nbcast = hvd.broadcast_global_variables(0)\n\n# Pin GPU to be used to process local rank (one GPU per process)\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list = str(hvd.local_rank())\n\nwith tf.Session(graph=graph, config=config) as sess:\n    sess.run(init_op)\n    bcast.run()\n\n    # training model\n    for epoch in range(100):\n        for X,y in train_data:\n            _, loss_train = sess.run([train_op, train_cost], feed_dict={X_ph:X, y_ph:y})\n```\n\nfor a full example on [tensorgraph on horovod](./examples/multi_gpus_horovod.py)\n\n-----\n## Hierachical Softmax Example\nBelow is another example for building a more powerful [hierachical softmax](examples/hierachical_softmax.py)\nwhereby the lower hierachical softmax layer can be conditioned on all the upper\nhierachical softmax layers.\n\n<img src=\"draw/hsoftmax.png\" height=\"250\">\n\n```python\n## params\nx_dim = 50\ncomponent_dim = 100\nbatchsize = 32\nlearning_rate = 0.01\n\n\nx_ph = tf.placeholder('float32', [None, x_dim])\n# the three hierachical level\ny1_ph = tf.placeholder('float32', [None, component_dim])\ny2_ph = tf.placeholder('float32', [None, component_dim])\ny3_ph = tf.placeholder('float32', [None, component_dim])\n\n# define the graph model structure\nstart = StartNode(input_vars=[x_ph])\n\nh1 = HiddenNode(prev=[start], layers=[Linear(component_dim), Softmax()])\nh2 = HiddenNode(prev=[h1], layers=[Linear(component_dim), Softmax()])\nh3 = HiddenNode(prev=[h2], layers=[Linear(component_dim), Softmax()])\nh1 = HiddenNode(prev=[start], layers=[Linear(x_dim, component_dim), Softmax()])\nh2 = HiddenNode(prev=[h1], layers=[Linear(component_dim, component_dim), Softmax()])\nh3 = HiddenNode(prev=[h2], layers=[Linear(component_dim, component_dim), Softmax()])\n\n\ne1 = EndNode(prev=[h1], input_merge_mode=Sum())\ne2 = EndNode(prev=[h1, h2], input_merge_mode=Sum())\ne3 = EndNode(prev=[h1, h2, h3], input_merge_mode=Sum())\n\ngraph = Graph(start=[start], end=[e1, e2, e3])\n\no1, o2, o3 = graph.train_fprop()\n\no1_mse = tf.reduce_mean((y1_ph - o1)**2)\no2_mse = tf.reduce_mean((y2_ph - o2)**2)\no3_mse = tf.reduce_mean((y3_ph - o3)**2)\nmse = o1_mse + o2_mse + o3_mse\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)\n```\n\n-----\n## Transfer Learning Example\nBelow is an example on transfer learning with bi-modality inputs and merge at\nthe middle layer with shared representation, in fact, TensorGraph can be used\nto build any number of modalities for transfer learning.\n\n<img src=\"draw/transferlearn.png\" height=\"250\">\n\n```python\n## params\nx1_dim = 50\nx2_dim = 100\nshared_dim = 200\ny_dim = 100\nbatchsize = 32\nlearning_rate = 0.01\n\n\nx1_ph = tf.placeholder('float32', [None, x1_dim])\nx2_ph = tf.placeholder('float32', [None, x2_dim])\ny_ph = tf.placeholder('float32', [None, y_dim])\n\n# define the graph model structure\ns1 = StartNode(input_vars=[x1_ph])\ns2 = StartNode(input_vars=[x2_ph])\n\nh1 = HiddenNode(prev=[s1], layers=[Linear(shared_dim), RELU()])\nh2 = HiddenNode(prev=[s2], layers=[Linear(shared_dim), RELU()])\nh3 = HiddenNode(prev=[h1,h2], input_merge_mode=Sum(),\n                layers=[Linear(y_dim), Softmax()])\nh1 = HiddenNode(prev=[s1], layers=[Linear(x1_dim, shared_dim), RELU()])\nh2 = HiddenNode(prev=[s2], layers=[Linear(x2_dim, shared_dim), RELU()])\nh3 = HiddenNode(prev=[h1,h2], input_merge_mode=Sum(),\n                layers=[Linear(shared_dim, y_dim), Softmax()])\n\ne1 = EndNode(prev=[h3])\n\ngraph = Graph(start=[s1, s2], end=[e1])\no1, = graph.train_fprop()\n\nmse = tf.reduce_mean((y_ph - o1)**2)\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)\n```\n",
            "readme_url": "https://github.com/hycis/TensorGraph",
            "frameworks": [
                "NLTK",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "arxiv": "1409.1556",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.1556v6",
            "abstract": "In this work we investigate the effect of the convolutional network depth on\nits accuracy in the large-scale image recognition setting. Our main\ncontribution is a thorough evaluation of networks of increasing depth using an\narchitecture with very small (3x3) convolution filters, which shows that a\nsignificant improvement on the prior-art configurations can be achieved by\npushing the depth to 16-19 weight layers. These findings were the basis of our\nImageNet Challenge 2014 submission, where our team secured the first and the\nsecond places in the localisation and classification tracks respectively. We\nalso show that our representations generalise well to other datasets, where\nthey achieve state-of-the-art results. We have made our two best-performing\nConvNet models publicly available to facilitate further research on the use of\ndeep visual representations in computer vision.",
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ]
        },
        {
            "title": "Densely Connected Convolutional Networks",
            "arxiv": "1608.06993",
            "year": 2016,
            "url": "http://arxiv.org/abs/1608.06993v5",
            "abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        },
        {
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "arxiv": "1505.04597",
            "year": 2015,
            "url": "http://arxiv.org/abs/1505.04597v1",
            "abstract": "There is large consent that successful training of deep networks requires\nmany thousand annotated training samples. In this paper, we present a network\nand training strategy that relies on the strong use of data augmentation to use\nthe available annotated samples more efficiently. The architecture consists of\na contracting path to capture context and a symmetric expanding path that\nenables precise localization. We show that such a network can be trained\nend-to-end from very few images and outperforms the prior best method (a\nsliding-window convolutional network) on the ISBI challenge for segmentation of\nneuronal structures in electron microscopic stacks. Using the same network\ntrained on transmitted light microscopy images (phase contrast and DIC) we won\nthe ISBI cell tracking challenge 2015 in these categories by a large margin.\nMoreover, the network is fast. Segmentation of a 512x512 image takes less than\na second on a recent GPU. The full implementation (based on Caffe) and the\ntrained networks are available at\nhttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ]
        }
    ],
    "domain": {
        "domain_type": "Medical",
        "domain_prob": 0.9829760633990419
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "COCO"
            }
        ]
    }
}