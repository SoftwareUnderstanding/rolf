{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "aloception",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Visual-Behavior",
                "owner_type": "Organization",
                "name": "aloception",
                "url": "https://github.com/Visual-Behavior/aloception",
                "stars": 75,
                "pushed_at": "2022-03-25 09:42:21+00:00",
                "created_at": "2021-09-06 14:54:41+00:00",
                "language": "Jupyter Notebook",
                "description": "Aloception is a set of package for computer vision: aloscene, alodataset, alonet.",
                "license": "Other",
                "frameworks": [
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "354ae14f605f3cca415a9d6db66dd264a0f51383",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/blob/master/.gitignore"
                    }
                },
                "size": 594
            },
            {
                "type": "code",
                "name": "DEVELOPMENT.md",
                "sha": "e064c1034ad3a39031e1ab1d0939696d25c1865d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/blob/master/DEVELOPMENT.md"
                    }
                },
                "size": 1578
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "cbe5ad1670406e4402217edfb82d2c56af7e8631",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/blob/master/LICENSE"
                    }
                },
                "size": 20849
            },
            {
                "type": "code",
                "name": "alodataset",
                "sha": "cd13ce8dd97b525469500b25ef6d8bf7d580a12c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/tree/master/alodataset"
                    }
                },
                "num_files": 22
            },
            {
                "type": "code",
                "name": "alonet",
                "sha": "33d09cb385ef14a118ff1cc0b40853cbe56f7cfb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/tree/master/alonet"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "aloscene",
                "sha": "1f71470bf44f031e1a5ab9dc2d454f13a17396b1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/tree/master/aloscene"
                    }
                },
                "num_files": 18
            },
            {
                "type": "code",
                "name": "dev_requirements.txt",
                "sha": "039bceacd5067e3c27d44432fedb14733fb5d982",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/blob/master/dev_requirements.txt"
                    }
                },
                "size": 64
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "1488c0692307c2bbd483bfedebad7492ab57e2a6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/tree/master/docs"
                    }
                },
                "num_files": 16
            },
            {
                "type": "code",
                "name": "docsource",
                "sha": "f965528a0b8aa2e777444bb64bd97cf293cf14eb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/tree/master/docsource"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "images",
                "sha": "efc62850d6d14b77d54370f51abe552cc318620e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/tree/master/images"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "b7d74aa2e1adad257da9b49409237c88cf7d68b7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/blob/master/requirements.txt"
                    }
                },
                "size": 334
            },
            {
                "type": "code",
                "name": "tutorials",
                "sha": "3bf3efe461f880e53f8d88ab6d7724967421af17",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/tree/master/tutorials"
                    }
                },
                "num_files": 18
            },
            {
                "type": "code",
                "name": "unittest",
                "sha": "a2c22eef5e7bdc69bedb2315c7716ce8c9bb9cda",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Visual-Behavior/aloception/tree/master/unittest"
                    }
                },
                "num_files": 10
            }
        ]
    },
    "authors": [
        {
            "name": "Thibault Neveu",
            "github_id": "thibo73800"
        },
        {
            "name": "jsalotti",
            "github_id": "jsalotti"
        },
        {
            "name": "Taha Azzim",
            "github_id": "Data-Iab"
        },
        {
            "name": "LucBourrat",
            "github_id": "LucBourrat1"
        },
        {
            "name": "ragier",
            "github_id": "ragier"
        },
        {
            "name": "Johan MEJIA",
            "github_id": "Johansmm"
        },
        {
            "name": "Anh Tu NGUYEN",
            "github_id": "anhtu293"
        }
    ],
    "tags": [],
    "description": "Aloception is a set of package for computer vision: aloscene, alodataset, alonet.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Visual-Behavior/aloception",
            "stars": 75,
            "issues": true,
            "readme": "<p align=\"center\">\n  <img src=\"images/aloception.png\" style=\"text-align:center; width: 50%;\" alt=\"Logo aloception\" />\n</p>\n\n<a href=\"https://visual-behavior.github.io/aloception/\">Documentation</a>\n\n[![Conventional Commits](https://img.shields.io/badge/Conventional%20Commits-0.1.0-green.svg)](https://conventionalcommits.org)\n\n# Aloception\n\n**Aloception** is a set of packages for computer vision built on top of popular deep learning libraries:\n[pytorch](<https://pytorch.org/>)  and  [pytorch lightning](https://www.pytorchlightning.ai/).\n\n\n### Aloscene\n\n**Aloscene** extend the use of\n[tensors](https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html) with **Augmented Tensors** designed to facilitate the use of computer vision data\n(such as frames, 2d boxes, 3d boxes, optical flow, disparity, camera parameters...).\n\n\n```python\nframe = aloscene.Frame(\"/path/to/image.jpg\")\nframe = frame.to(\"cpu\")\nframe.get_view().render()\n```\n\n### Alodataset\n\n**Alodataset** implement ready-to-use datasets for computer vision with the help of **aloscene** and **augmented tensors** to make it easier to transform and display your vision data.\n\n```python\ncoco_dataset = alodataset.CocoBaseDataset(sample=True)\nfor frame in coco_dataset.stream_loader():\n    frame.get_view().render()\n```\n\n### Alonet\n\n**Alonet** integrates several promising computer vision architectures. You can use it for research purposes or to finetune and deploy your model using TensorRT. Alonet is mainly built on top  of [lightning](https://www.pytorchlightning.ai/) with the help of\n  **aloscene** and **alodataset**.\n\n**Training**\n\n```python\n# Init the training pipeline\ndetr = alonet.detr.LitDetr()\n# Init the data module\ncoco_loader = alonet.detr.CocoDetection2Detr()\n# Run the training using the two components\ndetr.run_train(data_loader=coco_loader, project=\"detr\", expe_name=\"test_experiment\")\n```\n\n**Inference**\n\n```python\n# Load model\nmodel = alonet.detr.DetrR50(num_classes=91, weights=\"detr-r50\").eval()\n\n# Open and normalized frame\nframe = aloscene.Frame(\"/path/to/image.jpg\").norm_resnet()\n\n# Run inference\npred_boxes = model.inference(model([frame]))\n\n# Add and display the predicted boxes\nframe.append_boxes2d(pred_boxes[0], \"pred_boxes\")\nframe.get_view().render()\n```\n\n\n### Note\nOne can use **aloscene** independently than the two other packages to handle computer vision data, or to improve its\ntraining pipelines with **augmented tensors**.\n\n## Install\n\nAloception's packages are built on top of multiple libraries. Most of them are listed in the **requirements.txt**\n```\npip install -r requirements.txt\n```\n\nOnce the others packages are installed, you still need to install pytorch based on your hardware and environment\nconfiguration. Please, ref to the `pytorch website <https://pytorch.org/>`_  for this install.\n\n## Getting started\n\n<ul>\n  <li><a href=\"https://visual-behavior.github.io/aloception/getting_started/getting_started.html\">Getting started</a></li>\n  <li><a href=\"https://visual-behavior.github.io/aloception/getting_started/aloscene.html\">Aloscene: Computer vision with ease</a></li>\n  <li><a href=\"https://visual-behavior.github.io/aloception/getting_started/alodataset.html\">Alodataset: Loading your vision datasets</a></li>\n  <li><a href=\"https://visual-behavior.github.io/aloception/getting_started/alonet.html\">Alonet: Loading & training your models</a></li>\n  <li><a href=\"https://visual-behavior.github.io/aloception/getting_started/augmented_tensor.html\">About augmented tensors</a></li>\n</ul>\n\n\n## Tutorials\n\n<ul>\n  <li><a href=\"https://visual-behavior.github.io/aloception/tutorials/data_setup.html\">How to setup your data?</a></li>\n  <li><a href=\"https://visual-behavior.github.io/aloception/tutorials/training_detr.html\">Training Detr</a></li>\n  <li><a href=\"https://visual-behavior.github.io/aloception/tutorials/finetuning_detr.html\">Finetuning DETR</a></li>\n  <li><a href=\"https://visual-behavior.github.io/aloception/tutorials/training_panoptic.html\">Training Panoptic Head</a></li>\n  <li><a href=\"https://visual-behavior.github.io/aloception/tutorials/training_deformable_detr.html\">Training Deformable DETR</a></li>\n  <li><a href=\"https://visual-behavior.github.io/aloception/tutorials/finetuning_deformable_detr.html\">Finetuning Deformanble DETR</a></li>\n  <li><a href=\"https://visual-behavior.github.io/aloception/tutorials/tensort_inference.html\">Exporting DETR / Deformable-DETR to TensorRT</a></li>\n</ul>\n\n# Alonet\n\n## Models\n\n| Model name  | Link    | alonet location  | Learn more\n|---|---|---|---|\n| detr-r50  | https://arxiv.org/abs/2005.12872   | alonet.detr.DetrR50 | <a href=\"#detr\">Detr</a>\n| deformable-detr  | https://arxiv.org/abs/2010.04159  | alonet.deformable_detr.DeformableDETR  | <a href=\"#deformable-detr\">Deformable detr</a>\n| RAFT | https://arxiv.org/abs/2003.12039 | alonet.raft.RAFT  | <a href=\"#raft\">  RAFT </a> |   |\n| detr-r50-panoptic  | https://arxiv.org/abs/2005.12872   | alonet.detr_panoptic.PanopticHead | <a href=\"#detr-panoptic\">DetrPanoptic</a>\n\n## Detr\n\nHere is a simple example to get started with **Detr** and aloception. To learn more about Detr, you can checkout the <a href=\"#tutorials\">Tutorials<a/> or the <a href=\"./alonet/detr\">detr README</a>.\n\n```python\n# Load model\nmodel = alonet.detr.DetrR50(num_classes=91, weights=\"detr-r50\").eval()\n\n# Open and normalized frame\nframe = aloscene.Frame(\"/path/to/image.jpg\").norm_resnet()\n\n# Run inference\npred_boxes = model.inference(model([frame]))\n\n# Add and display the predicted boxes\nframe.append_boxes2d(pred_boxes[0], \"pred_boxes\")\nframe.get_view().render()\n```\n\n## Deformable Detr\n\nHere is a simple example to get started with **Deformable Detr** and aloception. To learn more about Deformable, you can checkout the <a href=\"#tutorials\">Tutorials<a/> or the <a href=\"./alonet/deformable_detr\">deformable detr README</a>.\n\n```python\n# Loading Deformable model\nmodel = alonet.deformable_detr.DeformableDetrR50(num_classes=91, weights=\"deformable-detr-r50\").eval()\n\n# Open, normalize frame and send frame on the device\nframe = aloscene.Frame(\"/home/thibault/Desktop/yoga.jpg\").norm_resnet().to(torch.device(\"cuda\"))\n\n# Run inference\npred_boxes = model.inference(model([frame]))\n\n# Add and display the predicted boxes\nframe.append_boxes2d(pred_boxes[0], \"pred_boxes\")\nframe.get_view().render()\n```\n\n## RAFT\n\nHere is a simple example to get started with **RAFT** and aloception. To learn more about RAFT, you can checkout the <a href=\"./alonet/raft\">raft README</a>.\n\n```python\n# Use the left frame from the  Sintel Flow dataset and normalize the frame for the RAFT Model\nframe = alodataset.SintelFlowDataset(sample=True).getitem(0)[\"left\"].norm_minmax_sym()\n\n# Load the model using the sintel weights\nraft = alonet.raft.RAFT(weights=\"raft-sintel\")\n\n# Compute optical flow\npadder = alonet.raft.utils.Padder()\nflow = raft.inference(raft(padder.pad(frame[0:1]), padder.pad(frame[1:2])))\n\n# Render the flow along with the first frame\nflow[0].get_view().render()\n```\n\n## Detr Panoptic\n\nHere is a simple example to get started with **PanopticHead** and aloception. To learn more about PanopticHead, you can checkout the <a href=\"./alonet/detr_panoptic\">panoptic README</a>.\n\n```python\n# Open and normalized frame\nframe = aloscene.Frame(\"/path/to/image.jpg\").norm_resnet()\n\n# Load the model using pre-trained weights\ndetr_model = alonet.detr.DetrR50(num_classes=250, background_class=250)\nmodel = alonet.detr_panoptic.PanopticHead(DETR_module=detr_model, weights=\"detr-r50-panoptic\")\n\n# Run inference\npred_boxes, pred_masks = model.inference(model([frame]))\n\n# Add and display the boxes/masks predicted\nframe.append_boxes2d(pred_boxes[0], \"pred_boxes\")\nframe.append_segmentation(pred_masks[0], \"pred_masks\")\nframe.get_view().render()\n```\n\n# Alodataset\n\nHere is a list of all the datasets you can use on Aloception. If you're dataset is not in the list but is important for computer vision. Please let us know using the issues or feel free to contribute.\n\n\n## Datasets\n\n| Dataset name  | alodataset location  | To try\n|---|---|---|\n| CocoDetection  | alodataset.CocoBaseDataset   | `python alodataset/coco_base_dataset.py`\n| CocoPanoptic  | alodataset.CocoPanopticDataset   | `python alodataset/coco_panopic_dataset.py`\n| CrowdHuman  | alodataset.CrowdHumanDataset   | `python alodataset/crowd_human_dataset.py `\n| Waymo  | alodataset.WaymoDataset   | `python alodataset/waymo_dataset.py`\n| ChairsSDHom | alodataset.ChairsSDHomDataset | `python alodataset/chairssdhom_dataset.py`\n| FlyingThings3DSubset | alodataset.FlyingThings3DSubsetDataset | `python alodataset/flyingthings3D_subset_dataset.py`\n| FlyingChairs2 | alodataset.FlyingChairs2Dataset | `python alodataset/flying_chairs2_dataset.py`\n| SintelDisparityDataset | alodataset.SintelDisparityDataset | `python alodataset/sintel_disparity_dataset.py`\n| SintelFlowDataset | alodataset.SintelFlowDataset | `python alodataset/sintel_flow_dataset.py`\n| MOT17 | alodataset.Mot17 | `python alodataset/mot17.py`\n\n\n\n# Unit tests\n\n```\npython -m pytest\n```\n\n# Licence\n\nShield: [![CC BY-NC-SA 4.0][cc-by-nc-sa-shield]][cc-by-nc-sa]\n\nThis work is licensed under a\n[Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License][cc-by-nc-sa].\n\n[![CC BY-NC-SA 4.0][cc-by-nc-sa-image]][cc-by-nc-sa]\n\n[cc-by-nc-sa]: http://creativecommons.org/licenses/by-nc-sa/4.0/\n[cc-by-nc-sa-image]: https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\n[cc-by-nc-sa-shield]: https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg\n",
            "readme_url": "https://github.com/Visual-Behavior/aloception",
            "frameworks": [
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
            "arxiv": "2003.12039",
            "year": 2020,
            "url": "http://arxiv.org/abs/2003.12039v3",
            "abstract": "We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network\narchitecture for optical flow. RAFT extracts per-pixel features, builds\nmulti-scale 4D correlation volumes for all pairs of pixels, and iteratively\nupdates a flow field through a recurrent unit that performs lookups on the\ncorrelation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT\nachieves an F1-all error of 5.10%, a 16% error reduction from the best\npublished result (6.10%). On Sintel (final pass), RAFT obtains an\nend-point-error of 2.855 pixels, a 30% error reduction from the best published\nresult (4.098 pixels). In addition, RAFT has strong cross-dataset\ngeneralization as well as high efficiency in inference time, training speed,\nand parameter count. Code is available at https://github.com/princeton-vl/RAFT.",
            "authors": [
                "Zachary Teed",
                "Jia Deng"
            ]
        },
        {
            "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
            "arxiv": "2010.04159",
            "year": 2020,
            "url": "http://arxiv.org/abs/2010.04159v4",
            "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed\ncomponents in object detection while demonstrating good performance. However,\nit suffers from slow convergence and limited feature spatial resolution, due to\nthe limitation of Transformer attention modules in processing image feature\nmaps. To mitigate these issues, we proposed Deformable DETR, whose attention\nmodules only attend to a small set of key sampling points around a reference.\nDeformable DETR can achieve better performance than DETR (especially on small\nobjects) with 10 times less training epochs. Extensive experiments on the COCO\nbenchmark demonstrate the effectiveness of our approach. Code is released at\nhttps://github.com/fundamentalvision/Deformable-DETR.",
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ]
        },
        {
            "title": "End-to-End Object Detection with Transformers",
            "arxiv": "2005.12872",
            "year": 2020,
            "url": "http://arxiv.org/abs/2005.12872v3",
            "abstract": "We present a new method that views object detection as a direct set\nprediction problem. Our approach streamlines the detection pipeline,\neffectively removing the need for many hand-designed components like a\nnon-maximum suppression procedure or anchor generation that explicitly encode\nour prior knowledge about the task. The main ingredients of the new framework,\ncalled DEtection TRansformer or DETR, are a set-based global loss that forces\nunique predictions via bipartite matching, and a transformer encoder-decoder\narchitecture. Given a fixed small set of learned object queries, DETR reasons\nabout the relations of the objects and the global image context to directly\noutput the final set of predictions in parallel. The new model is conceptually\nsimple and does not require a specialized library, unlike many other modern\ndetectors. DETR demonstrates accuracy and run-time performance on par with the\nwell-established and highly-optimized Faster RCNN baseline on the challenging\nCOCO object detection dataset. Moreover, DETR can be easily generalized to\nproduce panoptic segmentation in a unified manner. We show that it\nsignificantly outperforms competitive baselines. Training code and pretrained\nmodels are available at https://github.com/facebookresearch/detr.",
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ]
        }
    ],
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9997708517702948,
        "task": "Object Detection",
        "task_prob": 0.9362585758256214
    },
    "training": {
        "datasets": [
            {
                "name": "COCO"
            }
        ]
    }
}