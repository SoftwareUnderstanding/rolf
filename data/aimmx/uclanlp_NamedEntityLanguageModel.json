{
    "visibility": {
        "visibility": "public",
        "license": "BSD 3-Clause \"New\" or \"Revised\" License"
    },
    "name": "This is the codebase of the paper: Building Language Model for Text with Named Enitities, Rizwan et., al., (ACL, 18).",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "uclanlp",
                "owner_type": "Organization",
                "name": "NamedEntityLanguageModel",
                "url": "https://github.com/uclanlp/NamedEntityLanguageModel",
                "stars": 31,
                "pushed_at": "2019-06-14 05:59:55+00:00",
                "created_at": "2018-05-11 06:21:39+00:00",
                "language": "Python",
                "license": "BSD 3-Clause \"New\" or \"Revised\" License",
                "frameworks": [
                    "NLTK",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "e80fa4b50f5a1a1c900dbb12b50199ea03b43326",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/.gitignore"
                    }
                },
                "size": 18
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "09d493bf1fc257505c1336f3f87425568ab9da3c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/LICENSE"
                    }
                },
                "size": 1500
            },
            {
                "type": "code",
                "name": "data.py",
                "sha": "6a3aa7f02ad722c3b2e51df7132a441e7125a772",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/data.py"
                    }
                },
                "size": 1628
            },
            {
                "type": "code",
                "name": "data",
                "sha": "e30833cbc885ecfdcc8b9f01a0dffe5a6efc1b2c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/tree/master/data"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "data2.py",
                "sha": "678b9d677af2fb20ea7d6a8df7ca0e626cdb6fb0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/data2.py"
                    }
                },
                "size": 1938
            },
            {
                "type": "code",
                "name": "embed_regularize.py",
                "sha": "386e1ec94b40d6d184aeb1f533660924d4503a7a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/embed_regularize.py"
                    }
                },
                "size": 1089
            },
            {
                "type": "code",
                "name": "finetune.py",
                "sha": "c320cd59c95bd401a6d3bec503d8277743c56192",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/finetune.py"
                    }
                },
                "size": 10104
            },
            {
                "type": "code",
                "name": "generate.py",
                "sha": "7973f3a3a8c8a54e49d33b45b854ef6d111bfee3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/generate.py"
                    }
                },
                "size": 2760
            },
            {
                "type": "code",
                "name": "getdata.sh",
                "sha": "d4615b5d7c716d25c707f0969a4d39040c8a4b7a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/getdata.sh"
                    }
                },
                "size": 757
            },
            {
                "type": "code",
                "name": "inference.py",
                "sha": "24f46267174e4baacde5785ed34857a847bac6ef",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/inference.py"
                    }
                },
                "size": 14309
            },
            {
                "type": "code",
                "name": "inference3.py",
                "sha": "24f46267174e4baacde5785ed34857a847bac6ef",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/inference3.py"
                    }
                },
                "size": 14309
            },
            {
                "type": "code",
                "name": "locked_dropout.py",
                "sha": "486cb46da9be906835090365766bedfeb7ccf24e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/locked_dropout.py"
                    }
                },
                "size": 473
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "cd0d506ff53f15939ee462b62f21f7768abb1786",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/main.py"
                    }
                },
                "size": 10895
            },
            {
                "type": "code",
                "name": "main3.py",
                "sha": "641546d521cfa6f58762d19f3ebc46cae95fdd9d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/main3.py"
                    }
                },
                "size": 11326
            },
            {
                "type": "code",
                "name": "main_ori_with_type.py",
                "sha": "ee0dccd679e5df4ac03123c54a88e639f853ea68",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/main_ori_with_type.py"
                    }
                },
                "size": 11332
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "9edf1e09408162fd6fb9255bc89a9a8621b9244b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/model.py"
                    }
                },
                "size": 5023
            },
            {
                "type": "code",
                "name": "model3.py",
                "sha": "f306d239502d8c0770e13a9df61a499d3fa3de8a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/model3.py"
                    }
                },
                "size": 5397
            },
            {
                "type": "code",
                "name": "model_ori_with_type.py",
                "sha": "05fea428d3056fca5255b13c871f7453d6bdbbda",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/model_ori_with_type.py"
                    }
                },
                "size": 5384
            },
            {
                "type": "code",
                "name": "model_t.py",
                "sha": "81e7d84b606d3b76187ee11f78f448e24959c191",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/model_t.py"
                    }
                },
                "size": 5068
            },
            {
                "type": "code",
                "name": "pointer.py",
                "sha": "f1f149a84264defcb89e9b9133c8378096831b6f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/pointer.py"
                    }
                },
                "size": 5676
            },
            {
                "type": "code",
                "name": "raw_data",
                "sha": "f0db44491a5634ef9ec43b0d36352dd3005dd61a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/tree/master/raw_data"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "superingredients",
                "sha": "b8bd5b5f83d41d6d814a591e0b0d2d0448656df3",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/tree/master/superingredients"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "util.py",
                "sha": "f6dc8c23313f18842e4545d69955741245357d33",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/util.py"
                    }
                },
                "size": 13792
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "82bb1574b84e3e1b91f8f20f79e91d64bc366bb5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/utils.py"
                    }
                },
                "size": 961
            },
            {
                "type": "code",
                "name": "weight_drop.py",
                "sha": "15a196fb8a9e929f2238347d4b6920edb01004a5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uclanlp/NamedEntityLanguageModel/blob/master/weight_drop.py"
                    }
                },
                "size": 3185
            }
        ]
    },
    "authors": [
        {
            "name": "Md Rizwan Parvez",
            "email": "rizwan@ucla.edu",
            "github_id": "rizwan09"
        },
        {
            "name": "Stephen Merity",
            "email": "smerity@smerity.com",
            "github_id": "Smerity"
        },
        {
            "name": "Nitish Shirish Keskar",
            "github_id": "keskarnitish"
        },
        {
            "name": "Rachel Thomas",
            "github_id": "racheltho"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/uclanlp/NamedEntityLanguageModel",
            "stars": 31,
            "issues": false,
            "readme": "## This is the codebase of the paper: Building Language Model for Text with Named Enitities, [Rizwan et., al., (ACL, 18)](https://arxiv.org/abs/1805.04836).\n\n##### 1. Setting the [data path](https://github.com/rizwan09/awd-lstm-lm/tree/master/data) accordingly, we use command ``python3 main.py`` with default params to train baseline AWD_LSTM model, and type model. [uncleaned data is here](https://github.com/rizwan09/awd-lstm-lm/tree/master/raw_data).\n##### 2. To train entity composite model we use command ``python3 main_ori_with_type.py`` with default params. \n##### 3. At inference, we use ``inference.py`` file.\n\n##### 4. To reproduce our result, simply run command ``python3 inference_loaded.py`` of [full_pretrained_project](https://drive.google.com/drive/folders/1VhFvOHDQcTV7_gJ5BhAqccG4otRWIdLY?usp=sharing) which will use the already trained models. In this version, we also show that with our joint inference schema, AWD_LSTM itself can work suffciiently well and replace the entity composite model. Also note that we used nltk tokenizer while annotationg the types in this version. So it is slightly different from our current release. \n\n##### 5. The corresponding data are in awd-lstm-lm/data folder in the link shared above.\n\n##### 6. The uncleaned datasets are also relesead for future challenge (can be found in awd-lstm-lm/recipies/data/corpus in the google drive path shared above)\n\n#### 7. The code corpus can be found at [here](https://drive.google.com/drive/folders/1DJev2PwKBAPlUMy-OsT4kKVlTM1Mj_XN?usp=sharing). Although we report basic LSTM performance in the paper, running the AWD_LSTM model on this dataset may give better result. To reproduce our result, run with inference3.py from [here](https://drive.google.com/drive/folders/1fY0n89bWE_vGwKgNYWAhY_4_HjNLlwXy?usp=sharing). Please note that, for the code corpus,  as the variable scope is limited to each method, the context is initialized anew for each method instance. To train either a simple Type model or original state-of-art language model (i.e., both of forward and backward LSTM) main.py (python3 main.py) is used with respective data file, and for training an entity composite model main2.py (``python3 main2.py`` is used. inference3.py does the the joint inference laterwards. Another important note is this version does NOT support cuda. Our code does not support 'bidir' rather it computes forward and bckward seperately.\n\n#### If you use this code or data or our results in your research, please cite:\n\n```\n@InProceedings{P18-1221,\n  author = \t\"Parvez, Md Rizwan\n\t\tand Chakraborty, Saikat\n\t\tand Ray, Baishakhi\n\t\tand Chang, Kai-Wei\",\n  title = \t\"Building Language Models for Text with Named Entities\",\n  booktitle = \t\"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n  year = \t\"2018\",\n  publisher = \t\"Association for Computational Linguistics\",\n  pages = \t\"2373--2383\",\n  location = \t\"Melbourne, Australia\",\n  url = \t\"http://aclweb.org/anthology/P18-1221\"\n}\n```\n\n\n## About Baseline (forked from the baseline source code path)\n\n\n## AWD-LSTM / AWD-QRNN Language Model\n\n### Averaged Stochastic Gradient Descent with Weight Dropped LSTM or QRNN\n\nThis repository contains the code used for [Salesforce Research](https://einstein.ai/)'s [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182) paper, originally forked from the [PyTorch word level language modeling example](https://github.com/pytorch/examples/tree/master/word_language_model).\nThe model comes with instructions to train a word level language model over the Penn Treebank (PTB) and [WikiText-2](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset) (WT2) datasets, though the model is likely extensible to many other datasets.\nThe model can be composed of an LSTM or a [Quasi-Recurrent Neural Network](https://github.com/salesforce/pytorch-qrnn/) (QRNN) which is two or more times faster than the cuDNN LSTM in this setup while achieving equivalent or better accuracy.\n\n+ Install PyTorch 0.2\n+ Run `getdata.sh` to acquire the Penn Treebank and WikiText-2 datasets\n+ Train the base model using `main.py`\n+ Finetune the model using `finetune.py`\n+ Apply the [continuous cache pointer](https://arxiv.org/abs/1612.04426) to the finetuned model using `pointer.py`\n\nIf you use this code or our results in your research, please cite:\n\n```\n@article{merityRegOpt,\n  title={{Regularizing and Optimizing LSTM Language Models}},\n  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},\n  journal={arXiv preprint arXiv:1708.02182},\n  year={2017}\n}\n```\n\n## Software Requirements\n\nPython 3 and PyTorch 0.2 are required for the current codebase.\n\nIncluded below are hyper parameters to get equivalent or better results to those included in the original paper.\n\nIf you need to use an earlier version of the codebase, the original code and hyper parameters accessible at the [PyTorch==0.1.12](https://github.com/salesforce/awd-lstm-lm/tree/PyTorch%3D%3D0.1.12) release, with Python 3 and PyTorch 0.1.12 are required.\nIf you are using Anaconda, installation of PyTorch 0.1.12 can be achieved via:\n`conda install pytorch=0.1.12 -c soumith`.\n\n\n### Experiments For recipe dataset\n##### python main.py --batch_size 20 --data ../data/recipe_ori/ --dropouti 0.4 --dropouth 0.25 --seed 141 --epoch 50 --save RCP_LSTM_ori_with_type.pt\n\n##### python main.py --batch_size 20 --data ../data/recipe_type/ --dropouti 0.4 --dropouth 0.25 --seed 141 --epoch 50 --save RCP_type_LSTM_one_vocab.pt \n\n## Experiments\nThe codebase was modified during the writing of the paper, preventing exact reproduction due to minor differences in random seeds or similar.\nWe have also seen exact reproduction numbers change when changing underlying GPU.\nThe guide below produces results largely similar to the numbers reported.\n\nFor data setup, run `./getdata.sh`.\nThis script collects the Mikolov pre-processed Penn Treebank and the WikiText-2 datasets and places them in the `data` directory.\n\nNext, decide whether to use the QRNN or the LSTM as the underlying recurrent neural network model.\nThe QRNN is many times faster than even Nvidia's cuDNN optimized LSTM (and dozens of times faster than a naive LSTM implementation) yet achieves similar or better results than the LSTM.\nAt the time of writing, the QRNN models use the same number of parameters and are slightly deeper networks but are two to four times faster per epoch and require less epochs to converge.\n\nThe QRNN model uses a QRNN with convolutional size 2 for the first layer, allowing the model to view discrete natural language inputs (i.e. \"New York\"), while all other layers use a convolutional size of 1.\n\n**Finetuning Note:** Fine-tuning modifies the original saved model `model.pt` file - if you wish to keep the original weights you must copy the file.\n\n**Pointer note:** BPTT just changes the length of the sequence pushed onto the GPU but won't impact the final result.\n\n### Penn Treebank (PTB) with LSTM\n\nThe instruction below trains a PTB model that without finetuning achieves perplexities of approximately `61.2` / `58.8` (validation / testing), with finetuning achieves perplexities of approximately `58.8` / `56.5`, and with the continuous cache pointer augmentation achieves perplexities of approximately `53.2` / `52.5`.\n\n+ `python main.py --batch_size 20 --data data/penn --dropouti 0.4 --dropouth 0.25 --seed 141 --epoch 500 --save PTB.pt`\n+ `python finetune.py --batch_size 20 --data data/penn --dropouti 0.4 --dropouth 0.25 --seed 141 --epoch 500 --save PTB.pt`\n+ `python pointer.py --data data/penn --save PTB.pt --lambdasm 0.1 --theta 1.0 --window 500 --bptt 5000`\n\n### Penn Treebank (PTB) with QRNN\n\nThe instruction below trains a QRNN model that without finetuning achieves perplexities of approximately `60.6` / `58.3` (validation / testing), with finetuning achieves perplexities of approximately `59.1` / `56.7`, and with the continuous cache pointer augmentation achieves perplexities of approximately `53.4` / `52.6`.\n\n+ `python -u main.py --model QRNN --batch_size 20 --clip 0.2 --wdrop 0.1 --nhid 1550 --nlayers 4 --emsize 400 --dropouth 0.3 --seed 9001 --dropouti 0.4 --epochs 550 --save PTB.pt`\n+ `python -u finetune.py --model QRNN --batch_size 20 --clip 0.2 --wdrop 0.1 --nhid 1550 --nlayers 4 --emsize 400 --dropouth 0.3 --seed 404 --dropouti 0.4 --epochs 300 --save PTB.pt`\n+ `python pointer.py --model QRNN --lambdasm 0.1 --theta 1.0 --window 500 --bptt 5000 --save PTB.pt`\n\n### WikiText-2 (WT2) with LSTM\nThe instruction below trains a PTB model that without finetuning achieves perplexities of approximately `68.7` / `65.6` (validation / testing), with finetuning achieves perplexities of approximately `67.4` / `64.7`, and with the continuous cache pointer augmentation achieves perplexities of approximately `52.2` / `50.6`.\n\n+ `python main.py --epochs 750 --data data/wikitext-2 --save WT2.pt --dropouth 0.2 --seed 1882`\n+ `python finetune.py --epochs 750 --data data/wikitext-2 --save WT2.pt --dropouth 0.2 --seed 1882`\n+ `python pointer.py --save WT2.pt --lambdasm 0.1279 --theta 0.662 --window 3785 --bptt 2000 --data data/wikitext-2`\n\n### WikiText-2 (WT2) with QRNN\n\nThe instruction below will a QRNN model that without finetuning achieves perplexities of approximately `69.3` / `66.8` (validation / testing), with finetuning achieves perplexities of approximately `68.5` / `65.9`, and with the continuous cache pointer augmentation achieves perplexities of approximately `53.6` / `52.1`.\nBetter numbers are likely achievable but the hyper parameters have not been extensively searched. These hyper parameters should serve as a good starting point however.\n\n+ `python -u main.py --epochs 500 --data data/wikitext-2 --clip 0.25 --dropouti 0.4 --dropouth 0.2 --nhid 1550  --nlayers 4 --seed 4002 --model QRNN --wdrop 0.1 --batch_size 40 --save WT2.pt`\n+ `python finetune.py --epochs 500 --data data/wikitext-2 --clip 0.25 --dropouti 0.4 --dropouth 0.2 --nhid 1550 --nlayers 4 --seed 4002 --model QRNN --wdrop 0.1 --batch_size 40 --save WT2.pt`\n+ `python -u pointer.py --save WT2.pt --model QRNN --lambdasm 0.1279 --theta 0.662 --window 3785 --bptt 2000 --data data/wikitext-2`\n\n## Speed\n\nThe default speeds for the models during training on an NVIDIA Quadro GP100:\n\n+ Penn Treebank (batch size 20): LSTM takes 65 seconds per epoch, QRNN takes 28 seconds per epoch\n+ WikiText-2 (batch size 20): LSTM takes 180 seconds per epoch, QRNN takes 90 seconds per epoch\n\nThe default QRNN models can be far faster than the cuDNN LSTM model, with the speed-ups depending on how much of a bottleneck the RNN is. The majority of the model time above is now spent in softmax or optimization overhead (see [PyTorch QRNN discussion on speed](https://github.com/salesforce/pytorch-qrnn#speed)).\n\nSpeeds are approximately three times slower on a K80. On a K80 or other memory cards with less memory you may wish to enable [the cap on the maximum sampled sequence length](https://github.com/salesforce/awd-lstm-lm/blob/ef9369d277f8326b16a9f822adae8480b6d492d0/main.py#L131) to prevent out-of-memory (OOM) errors, especially for WikiText-2.\n\nIf speed is a major issue, SGD converges more quickly than our non-monotonically triggered variant of ASGD though achieves a worse overall perplexity.\n\n### Details of the QRNN optimization\n\nFor full details, refer to the [PyTorch QRNN repository](https://github.com/salesforce/pytorch-qrnn).\n\n### Details of the LSTM optimization\n\nAll the augmentations to the LSTM, including our variant of [DropConnect (Wan et al. 2013)](https://cs.nyu.edu/~wanli/dropc/dropc.pdf) termed weight dropping which adds recurrent dropout, allow for the use of NVIDIA's cuDNN LSTM implementation.\nPyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed.\nThis ensures the model is fast to train even when convergence may take many hundreds of epochs.\n",
            "readme_url": "https://github.com/uclanlp/NamedEntityLanguageModel",
            "frameworks": [
                "NLTK",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Building Language Models for Text with Named Entities",
            "arxiv": "1805.04836",
            "year": 2018,
            "url": "http://arxiv.org/abs/1805.04836v1",
            "abstract": "Text in many domains involves a significant amount of named entities.\nPredict- ing the entity names is often challenging for a language model as they\nappear less frequent on the training corpus. In this paper, we propose a novel\nand effective approach to building a discriminative language model which can\nlearn the entity names by leveraging their entity type information. We also\nintroduce two benchmark datasets based on recipes and Java programming codes,\non which we evalu- ate the proposed model. Experimental re- sults show that our\nmodel achieves 52.2% better perplexity in recipe generation and 22.06% on code\ngeneration than the state-of-the-art language models.",
            "authors": [
                "Md Rizwan Parvez",
                "Saikat Chakraborty",
                "Baishakhi Ray",
                "Kai-Wei Chang"
            ]
        },
        {
            "title": "Regularizing and Optimizing LSTM Language Models",
            "arxiv": "1708.02182",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02182v1",
            "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.",
            "authors": [
                "Stephen Merity",
                "Nitish Shirish Keskar",
                "Richard Socher"
            ]
        },
        {
            "title": "Improving Neural Language Models with a Continuous Cache",
            "arxiv": "1612.04426",
            "year": 2016,
            "url": "http://arxiv.org/abs/1612.04426v1",
            "abstract": "We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.",
            "authors": [
                "Edouard Grave",
                "Armand Joulin",
                "Nicolas Usunier"
            ]
        },
        {
            "url": "http://aclweb.org/anthology/P18-1221",
            "location": "Melbourne, Australia",
            "pages": "2373--2383",
            "publisher": "Association for Computational Linguistics",
            "year": "2018",
            "booktitle": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "title": "Building Language Models for Text with Named Entities",
            "author": [
                "Parvez, Md Rizwan",
                "Chakraborty, Saikat",
                "Ray, Baishakhi",
                "Chang, Kai-Wei"
            ],
            "ENTRYTYPE": "inproceedings",
            "ID": "P18-1221",
            "authors": [
                "Parvez, Md Rizwan",
                "Chakraborty, Saikat",
                "Ray, Baishakhi",
                "Chang, Kai-Wei"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Penn Treebank"
            },
            {
                "name": "WikiText-2"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999948966630355,
        "task": "Language Modelling",
        "task_prob": 0.9836612071804801
    }
}