{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "IR2VI_journal",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "lukeandshuo",
                "owner_type": "User",
                "name": "IR2VI_journal",
                "url": "https://github.com/lukeandshuo/IR2VI_journal",
                "stars": 1,
                "pushed_at": "2022-03-12 00:25:04+00:00",
                "created_at": "2020-04-20 02:42:56+00:00",
                "language": "Python",
                "license": "Other",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".DS_Store",
                "sha": "2cd18170f59f206787451461e8635e881099cc9e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/blob/master/.DS_Store"
                    }
                },
                "size": 6148
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "92abe4a830720f5bc2b16a49e6e37e871cd40a03",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/blob/master/.gitignore"
                    }
                },
                "size": 92
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "d75f0ee8466f00cf04da906a6fca115c7910399f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/blob/master/LICENSE"
                    }
                },
                "size": 3565
            },
            {
                "type": "code",
                "name": "data",
                "sha": "3ed966b2a21ccc35ebcb30b233d32113dccc7adc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/tree/master/data"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "filtering.py",
                "sha": "fb745eddacfeedd493b410b7e34489410a0de48d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/blob/master/filtering.py"
                    }
                },
                "size": 1281
            },
            {
                "type": "code",
                "name": "histogram_equalization.py",
                "sha": "a831bdbe3e81b3194ec2e57938b7f6da4f0366a6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/blob/master/histogram_equalization.py"
                    }
                },
                "size": 639
            },
            {
                "type": "code",
                "name": "imgs",
                "sha": "2997443aa687a00861e20857da46dc2b16b59528",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/tree/master/imgs"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "models",
                "sha": "8d9215fab326b732889a354815bdc930802a30b1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/tree/master/models"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "options",
                "sha": "2d34b281412466179a51161ab7bcf819c7be9b78",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/tree/master/options"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "5e484553b15273e6f05b3a1e396c502686a5761b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/blob/master/requirements.txt"
                    }
                },
                "size": 86
            },
            {
                "type": "code",
                "name": "scripts",
                "sha": "1bc2a56585e5d8e093a0db6a96d73d48098f42e1",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/tree/master/scripts"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "73514b8f4a4fd8d03067a415d129bb7152c0704c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/blob/master/test.py"
                    }
                },
                "size": 1437
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "6dbd66b437eab307e64b6a9830551f001e1ba598",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/blob/master/train.py"
                    }
                },
                "size": 1951
            },
            {
                "type": "code",
                "name": "util",
                "sha": "5787ba2914fa3c3e5f8d0a1765de3feaf9318923",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lukeandshuo/IR2VI_journal/tree/master/util"
                    }
                },
                "num_files": 12
            }
        ]
    },
    "authors": [
        {
            "name": "lukeshuo",
            "github_id": "lukeshuo"
        },
        {
            "name": "lukeandshuo",
            "github_id": "lukeandshuo"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/lukeandshuo/IR2VI_journal",
            "stars": 1,
            "issues": true,
            "readme": "<img src='imgs/horse2zebra.gif' align=\"right\" width=384>\n\n<br><br><br>\n\n# CycleGAN and pix2pix in PyTorch\n\nThis is our ongoing PyTorch implementation for both unpaired and paired image-to-image translation.\n\nThe code was written by [Jun-Yan Zhu](https://github.com/junyanz) and [Taesung Park](https://github.com/taesung89).\n\nCheck out the original [CycleGAN Torch](https://github.com/junyanz/CycleGAN) and [pix2pix Torch](https://github.com/phillipi/pix2pix) code if you would like to reproduce the exact same results as in the papers.\n\n\n#### CycleGAN: [[Project]](https://junyanz.github.io/CycleGAN/) [[Paper]](https://arxiv.org/pdf/1703.10593.pdf) [[Torch]](https://github.com/junyanz/CycleGAN)\n<img src=\"https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg\" width=\"900\"/>\n\n#### Pix2pix:  [[Project]](https://phillipi.github.io/pix2pix/) [[Paper]](https://arxiv.org/pdf/1611.07004v1.pdf) [[Torch]](https://github.com/phillipi/pix2pix)\n\n<img src=\"https://phillipi.github.io/pix2pix/images/teaser_v3.png\" width=\"900px\"/>\n\n#### [[EdgesCats Demo]](https://affinelayer.com/pixsrv/)  [[pix2pix-tensorflow]](https://github.com/affinelayer/pix2pix-tensorflow)   \nWritten by [Christopher Hesse](https://twitter.com/christophrhesse)  \n\n<img src='imgs/edges2cats.jpg' width=\"600px\"/>\n\nIf you use this code for your research, please cite:\n\nUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks  \n[Jun-Yan Zhu](https://people.eecs.berkeley.edu/~junyanz/)\\*,  [Taesung Park](https://taesung.me/)\\*, [Phillip Isola](https://people.eecs.berkeley.edu/~isola/), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros)  \nIn arxiv, 2017. (* equal contributions)  \n\n\nImage-to-Image Translation with Conditional Adversarial Networks  \n[Phillip Isola](https://people.eecs.berkeley.edu/~isola), [Jun-Yan Zhu](https://people.eecs.berkeley.edu/~junyanz), [Tinghui Zhou](https://people.eecs.berkeley.edu/~tinghuiz), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros)   \nIn CVPR 2017.\n\n## Other implementations:\n### CycleGAN\n<p><a href=\"https://github.com/leehomyc/cyclegan-1\"> [Tensorflow]</a> (by Harry Yang),\n<a href=\"https://github.com/architrathore/CycleGAN/\">[Tensorflow]</a> (by Archit Rathore),\n<a href=\"https://github.com/vanhuyz/CycleGAN-TensorFlow\">[Tensorflow]</a> (by Van Huy),\n<a href=\"https://github.com/XHUJOY/CycleGAN-tensorflow\">[Tensorflow]</a> (by Xiaowei Hu),\n<a href=\"https://github.com/LynnHo/CycleGAN-Tensorflow-Simple\"> [Tensorflow-simple]</a> (by Zhenliang He),\n<a href=\"https://github.com/luoxier/CycleGAN_Tensorlayer\"> [TensorLayer]</a> (by luoxier),\n<a href=\"https://github.com/Aixile/chainer-cyclegan\">[Chainer]</a> (by Yanghua Jin),\n<a href=\"https://github.com/yunjey/mnist-svhn-transfer\">[Minimal PyTorch]</a> (by yunjey),\n<a href=\"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN\">[Mxnet]</a> (by Ldpe2G),\n<a href=\"https://github.com/tjwei/GANotebooks\">[lasagne/keras]</a> (by tjwei)</p>\n</ul>\n\n### pix2pix\n<p><a href=\"https://github.com/affinelayer/pix2pix-tensorflow\"> [Tensorflow]</a> (by Christopher Hesse),\n<a href=\"https://github.com/Eyyub/tensorflow-pix2pix\">[Tensorflow]</a> (by Eyy\u00fcb Sariu),\n<a href=\"https://github.com/datitran/face2face-demo\"> [Tensorflow (face2face)]</a> (by Dat Tran),\n<a href=\"https://github.com/awjuliani/Pix2Pix-Film\"> [Tensorflow (film)]</a> (by Arthur Juliani),\n<a href=\"https://github.com/kaonashi-tyc/zi2zi\">[Tensorflow (zi2zi)]</a> (by Yuchen Tian),\n<a href=\"https://github.com/pfnet-research/chainer-pix2pix\">[Chainer]</a> (by mattya),\n<a href=\"https://github.com/tjwei/GANotebooks\">[tf/torch/keras/lasagne]</a> (by tjwei),\n<a href=\"https://github.com/taey16/pix2pixBEGAN.pytorch\">[Pytorch]</a> (by taey16)\n</p>\n</ul>\n\n## Prerequisites\n- Linux or macOS\n- Python 2 or 3\n- CPU or NVIDIA GPU + CUDA CuDNN\n\n## Getting Started\n### Installation\n- Install PyTorch and dependencies from http://pytorch.org\n- Install Torch vision from the source.\n```bash\ngit clone https://github.com/pytorch/vision\ncd vision\npython setup.py install\n```\n- Install python libraries [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate).\n```bash\npip install visdom\npip install dominate\n```\n- Clone this repo:\n```bash\ngit clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\ncd pytorch-CycleGAN-and-pix2pix\n```\n\n### CycleGAN train/test\n- Download a CycleGAN dataset (e.g. maps):\n```bash\nbash ./datasets/download_cyclegan_dataset.sh maps\n```\n- Train a model:\n```bash\n#!./scripts/train_cyclegan.sh\npython train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --no_dropout\n```\n- To view training results and loss plots, run `python -m visdom.server` and click the URL http://localhost:8097. To see more intermediate results, check out `./checkpoints/maps_cyclegan/web/index.html`\n- Test the model:\n```bash\n#!./scripts/test_cyclegan.sh\npython test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --phase test --no_dropout\n```\nThe test results will be saved to a html file here: `./results/maps_cyclegan/latest_test/index.html`.\n\n### pix2pix train/test\n- Download a pix2pix dataset (e.g.facades):\n```bash\nbash ./datasets/download_pix2pix_dataset.sh facades\n```\n- Train a model:\n```bash\n#!./scripts/train_pix2pix.sh\npython train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --which_model_netG unet_256 --which_direction BtoA --lambda_A 100 --dataset_mode aligned --no_lsgan --norm batch --pool_size 0\n```\n- To view training results and loss plots, run `python -m visdom.server` and click the URL http://localhost:8097. To see more intermediate results, check out  `./checkpoints/facades_pix2pix/web/index.html`\n- Test the model (`bash ./scripts/test_pix2pix.sh`):\n```bash\n#!./scripts/test_pix2pix.sh\npython test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --which_model_netG unet_256 --which_direction BtoA --dataset_mode aligned --norm batch\n```\nThe test results will be saved to a html file here: `./results/facades_pix2pix/latest_val/index.html`.\n\nMore example scripts can be found at `scripts` directory.\n\n### Apply a pre-trained model (CycleGAN)\n\nIf you would like to apply a pre-trained model to a collection of input photos (without image pairs), please use `--dataset_mode single` and `--model test` options. Here is a script to apply a model to Facade label maps (stored in the directory `facades/testB`).\n``` bash\n#!./scripts/test_single.sh\npython test.py --dataroot ./datasets/facades/testA/ --name {my_trained_model_name} --model test --dataset_mode single\n```\nYou might want to specify `--which_model_netG` to match the generator architecture of the trained model.\n\nYou can download a few pretrained models from the authors. For example, if you would like to download horse2zebra model, \n\n```bash\nbash pretrained_models/download_cyclegan_model.sh horse2zebra\n```\nThe pretrained model is saved at `./checkpoints/{name}_pretrained/latest_net_G.pth`. \nThen generate the results using\n\n```bash\npython test.py --dataroot datasets/horse2zebra/testA --checkpoints_dir ./checkpoints/ --name horse2zebra_pretrained --no_dropout --model test --dataset_mode single --loadSize 256 --results_dir {directory_path_to_save_result}\n```\n\nNote: We currently don't have all pretrained models using PyTorch. This is in part because the models trained using Torch and PyTorch produce slightly different results, although we were not able to decide which result is better. If you would like to generate the same results that appeared in our paper, we recommend using the pretrained models in the Torch codebase. \n\n### Apply a pre-trained model (pix2pix)\n\nDownload the pre-trained models using `./pretrained_models/download_pix2pix_model.sh`. For example, if you would like to download label2photo model on the Facades dataset,\n\n```bash\nbash pretrained_models/download_pix2pix_model.sh facades_label2photo\n```\n\nThen generate the results using\n```bash\npython test.py --dataroot ./datasets/facades/ --which_direction BtoA --model pix2pix --name facades_label2photo_pretrained --dataset_mode aligned --which_model_netG unet_256 --norm batch\n```\nNote that we specified `--which_direction BtoA` to accomodate the fact that the Facades dataset's A to B direction is photos to labels.\n\nAlso, the models that are currently available to download can be found by reading the output of `bash pretrained_models/download_pix2pix_model.sh`\n\n## Training/test Details\n- Flags: see `options/train_options.py` and `options/base_options.py` for all the training flags; see `options/test_options.py` and `options/base_options.py` for all the test flags.\n- CPU/GPU (default `--gpu_ids 0`): set`--gpu_ids -1` to use CPU mode; set `--gpu_ids 0,1,2` for multi-GPU mode. You need a large batch size (e.g. `--batchSize 32`) to benefit from multiple GPUs.  \n- Visualization: during training, the current results can be viewed using two methods. First, if you set `--display_id` > 0, the results and loss plot will appear on a local graphics web server launched by [visdom](https://github.com/facebookresearch/visdom). To do this, you should have `visdom` installed and a server running by the command `python -m visdom.server`. The default server URL is `http://localhost:8097`. `display_id` corresponds to the window ID that is displayed on the `visdom` server. The `visdom` display functionality is turned on by default. To avoid the extra overhead of communicating with `visdom` set `--display_id 0`. Second, the intermediate results are saved to `[opt.checkpoints_dir]/[opt.name]/web/` as an HTML file. To avoid this, set `--no_html`.\n- Preprocessing: images can be resized and cropped in different ways using `--resize_or_crop` option. The default option `'resize_and_crop'` resizes the image to be of size `(opt.loadSize, opt.loadSize)` and does a random crop of size `(opt.fineSize, opt.fineSize)`. `'crop'` skips the resizing step and only performs random cropping. `'scale_width'` resizes the image to have width `opt.fineSize` while keeping the aspect ratio. `'scale_width_and_crop'` first resizes the image to have width `opt.loadSize` and then does random cropping of size `(opt.fineSize, opt.fineSize)`.\n- Fine-tuning/Resume training: to fine-tune a pre-trained model, or resume the previous training, use the `--continue_train` flag. The program will then load the model based on `which_epoch`. By default, the program will initialize the epoch count as 1. Set `--epoch_count <int>` to specify a different starting epoch count.\n\n\n### CycleGAN Datasets\nDownload the CycleGAN datasets using the following script. Some of the datasets are collected by other researchers. Please cite their papers if you use the data.\n```bash\nbash ./datasets/download_cyclegan_dataset.sh dataset_name\n```\n- `facades`: 400 images from the [CMP Facades dataset](http://cmp.felk.cvut.cz/~tylecr1/facade). [[Citation](datasets/bibtex/facades.tex)]\n- `cityscapes`: 2975 images from the [Cityscapes training set](https://www.cityscapes-dataset.com). [[Citation](datasets/bibtex/cityscapes.tex)]\n- `maps`: 1096 training images scraped from Google Maps.\n- `horse2zebra`: 939 horse images and 1177 zebra images downloaded from [ImageNet](http://www.image-net.org) using keywords `wild horse` and `zebra`\n- `apple2orange`: 996 apple images and 1020 orange images downloaded from [ImageNet](http://www.image-net.org) using keywords `apple` and `navel orange`.\n- `summer2winter_yosemite`: 1273 summer Yosemite images and 854 winter Yosemite images were downloaded using Flickr API. See more details in our paper.\n- `monet2photo`, `vangogh2photo`, `ukiyoe2photo`, `cezanne2photo`: The art images were downloaded from [Wikiart](https://www.wikiart.org/). The real photos are downloaded from Flickr using the combination of the tags *landscape* and *landscapephotography*. The training set size of each class is Monet:1074, Cezanne:584, Van Gogh:401, Ukiyo-e:1433, Photographs:6853.\n- `iphone2dslr_flower`: both classes of images were downlaoded from Flickr. The training set size of each class is iPhone:1813, DSLR:3316. See more details in our paper.\n\nTo train a model on your own datasets, you need to create a data folder with two subdirectories `trainA` and `trainB` that contain images from domain A and B. You can test your model on your training set by setting `--phase train` in `test.py`. You can also create subdirectories `testA` and `testB` if you have test data.\n\nYou should **not** expect our method to work on just any random combination of input and output datasets (e.g. `cats<->keyboards`). From our experiments, we find it works better if two datasets share similar visual content. For example, `landscape painting<->landscape photographs` works much better than `portrait painting <-> landscape photographs`. `zebras<->horses` achieves compelling results while `cats<->dogs` completely fails.\n\n### pix2pix datasets\nDownload the pix2pix datasets using the following script. Some of the datasets are collected by other researchers. Please cite their papers if you use the data.\n```bash\nbash ./datasets/download_pix2pix_dataset.sh dataset_name\n```\n- `facades`: 400 images from [CMP Facades dataset](http://cmp.felk.cvut.cz/~tylecr1/facade). [[Citation](datasets/bibtex/facades.tex)]\n- `cityscapes`: 2975 images from the [Cityscapes training set](https://www.cityscapes-dataset.com). [[Citation](datasets/bibtex/cityscapes.tex)]\n- `maps`: 1096 training images scraped from Google Maps\n- `edges2shoes`: 50k training images from [UT Zappos50K dataset](http://vision.cs.utexas.edu/projects/finegrained/utzap50k). Edges are computed by [HED](https://github.com/s9xie/hed) edge detector + post-processing. [[Citation](datasets/bibtex/shoes.tex)]\n- `edges2handbags`: 137K Amazon Handbag images from [iGAN project](https://github.com/junyanz/iGAN). Edges are computed by [HED](https://github.com/s9xie/hed) edge detector + post-processing. [[Citation](datasets/bibtex/handbags.tex)]\n\nWe provide a python script to generate pix2pix training data in the form of pairs of images {A,B}, where A and B are two different depictions of the same underlying scene. For example, these might be pairs {label map, photo} or {bw image, color image}. Then we can learn to translate A to B or B to A:\n\nCreate folder `/path/to/data` with subfolders `A` and `B`. `A` and `B` should each have their own subfolders `train`, `val`, `test`, etc. In `/path/to/data/A/train`, put training images in style A. In `/path/to/data/B/train`, put the corresponding images in style B. Repeat same for other data splits (`val`, `test`, etc).\n\nCorresponding images in a pair {A,B} must be the same size and have the same filename, e.g., `/path/to/data/A/train/1.jpg` is considered to correspond to `/path/to/data/B/train/1.jpg`.\n\nOnce the data is formatted this way, call:\n```bash\npython datasets/combine_A_and_B.py --fold_A /path/to/data/A --fold_B /path/to/data/B --fold_AB /path/to/data\n```\n\nThis will combine each pair of images (A,B) into a single image file, ready for training.\n\n## Citation\nIf you use this code for your research, please cite our papers.\n```\n@article{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  journal={arXiv preprint arXiv:1703.10593},\n  year={2017}\n}\n\n@article{pix2pix2016,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  journal={arxiv},\n  year={2016}\n}\n```\n\n## Related Projects\n[CycleGAN](https://github.com/junyanz/CycleGAN): Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks  \n[pix2pix](https://github.com/phillipi/pix2pix): Image-to-image translation with conditional adversarial nets  \n[iGAN](https://github.com/junyanz/iGAN): Interactive Image Generation via Generative Adversarial Networks\n\n## Cat Paper Collection\nIf you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper Collection:  \n[[Github]](https://github.com/junyanz/CatPapers) [[Webpage]](https://people.eecs.berkeley.edu/~junyanz/cat/cat_papers.html)\n\n## Acknowledgments\nCode is inspired by [pytorch-DCGAN](https://github.com/pytorch/examples/tree/master/dcgan).\n",
            "readme_url": "https://github.com/lukeandshuo/IR2VI_journal",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "arxiv": "1703.10593",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10593v7",
            "abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.",
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A. Efros"
            ]
        },
        {
            "year": "2016",
            "journal": "arxiv",
            "author": [
                "Isola, Phillip",
                "Zhu, Jun-Yan",
                "Zhou, Tinghui",
                "Efros, Alexei A"
            ],
            "title": "Image-to-Image Translation with Conditional Adversarial Networks",
            "ENTRYTYPE": "article",
            "ID": "pix2pix2016",
            "authors": [
                "Isola, Phillip",
                "Zhu, Jun-Yan",
                "Zhou, Tinghui",
                "Efros, Alexei A"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "CMP Facades dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://cmp.felk.cvut.cz/~tylecr1/facade"
                    }
                }
            },
            {
                "name": "CMP Facades dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://cmp.felk.cvut.cz/~tylecr1/facade"
                    }
                }
            },
            {
                "name": "UT Zappos50K dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "http://vision.cs.utexas.edu/projects/finegrained/utzap50k"
                    }
                }
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "MNIST"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "Amazon"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.999999970347486,
        "task": "Image-to-Image Translation",
        "task_prob": 0.9932332796754542
    }
}