{
    "visibility": {
        "visibility": "public"
    },
    "name": "ELMo-keras",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "kafura-kafiri",
                "owner_type": "User",
                "name": "tf2-elmo",
                "url": "https://github.com/kafura-kafiri/tf2-elmo",
                "stars": 0,
                "pushed_at": "2020-03-13 15:06:01+00:00",
                "created_at": "2020-03-13 15:00:55+00:00",
                "language": "Python",
                "description": "elmo layer compatible with tf2 keras",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "87c74587b510b6b2cfb9b5a80dd1eceb2e385791",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kafura-kafiri/tf2-elmo/blob/master/.gitignore"
                    }
                },
                "size": 143
            },
            {
                "type": "code",
                "name": "data",
                "sha": "4c816f94ba0c57cddece841e1366ea3e5945e019",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kafura-kafiri/tf2-elmo/tree/master/data"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "elmo",
                "sha": "3e9628f3ad6d001c246634a245dcf1e197002f76",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kafura-kafiri/tf2-elmo/tree/master/elmo"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "f59efe9afa27aa52cd61e7f02cc37a4a2585c215",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kafura-kafiri/tf2-elmo/tree/master/tests"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "train_demo.py",
                "sha": "4c1b82801850a346bbc2f4119df7866b6be3afa9",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kafura-kafiri/tf2-elmo/blob/master/train_demo.py"
                    }
                },
                "size": 3621
            }
        ]
    },
    "authors": [
        {
            "name": "kafura-kafiri",
            "github_id": "kafura-kafiri"
        }
    ],
    "tags": [],
    "description": "elmo layer compatible with tf2 keras",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/kafura-kafiri/tf2-elmo",
            "stars": 0,
            "issues": true,
            "readme": "# ELMo-keras\nRe-implementation of ELMo in Keras based on the tensorflow implementation presented by Allen NLP (https://github.com/allenai/bilm-tf), based on Peters et al. article in NAACL 2018 (https://arxiv.org/abs/1802.05365):\n\n_Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee and Luke Zettlemoyer. 2018. Deep contextualized word representations_\n\nNotice: The project includes WikiText-2 datasets for experimentation as published in (https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset), presented in Merity et al. 2016 (https://arxiv.org/abs/1609.07843):\n\n_Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models_\n\n## Why in the heck did you do that?\n\n- This was the easiest way to understand ELMo deeply, find its pros and cons and also consider improvements (e.g., make it more computational efficient). \n- I also consider Keras as the most user-friendly and industry-ready library to work with.\n- Now we are also able to integrate ELMo for practical use at Cognitiv+, where we rely on Keras for our NLP engine.\n- It was really fun! This took me more than a month, in which period I had to learn many things and vastly improve my understading and skills around Keras and Tensorflow, so be kind.\n\n## How to use it?\n\n```\nimport os\nimport keras.backend as K\n\nfrom data import DATA_SET_DIR\nfrom elmo.lm_generator import LMDataGenerator\nfrom elmo.model import ELMo\n\nparameters = {\n    'multi_processing': False,\n    'n_threads': 4,\n    'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False,\n    'train_dataset': 'wikitext-2/wiki.train.tokens',\n    'valid_dataset': 'wikitext-2/wiki.valid.tokens',\n    'test_dataset': 'wikitext-2/wiki.test.tokens',\n    'vocab': 'wikitext-2/wiki.vocab',\n    'vocab_size': 28914,\n    'num_sampled': 1000,\n    'charset_size': 262,\n    'sentence_maxlen': 100,\n    'token_maxlen': 50,\n    'token_encoding': 'word',\n    'epochs': 10,\n    'patience': 2,\n    'batch_size': 1,\n    'clip_value': 5,\n    'cell_clip': 5,\n    'proj_clip': 5,\n    'lr': 0.2,\n    'shuffle': True,\n    'n_lstm_layers': 2,\n    'n_highway_layers': 2,\n    'cnn_filters': [[1, 32],\n                    [2, 32],\n                    [3, 64],\n                    [4, 128],\n                    [5, 256],\n                    [6, 512],\n                    [7, 512]\n                    ],\n    'lstm_units_size': 400,\n    'hidden_units_size': 200,\n    'char_embedding_size': 16,\n    'dropout_rate': 0.1,\n    'word_dropout_rate': 0.05,\n    'weight_tying': True,\n}\n\n# Set-up Generators\ntrain_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['train_dataset']),\n                                  os.path.join(DATA_SET_DIR, parameters['vocab']),\n                                  sentence_maxlen=parameters['sentence_maxlen'],\n                                  token_maxlen=parameters['token_maxlen'],\n                                  batch_size=parameters['batch_size'],\n                                  shuffle=parameters['shuffle'],\n                                  token_encoding=parameters['token_encoding'])\n\nval_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['valid_dataset']),\n                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n                                sentence_maxlen=parameters['sentence_maxlen'],\n                                token_maxlen=parameters['token_maxlen'],\n                                batch_size=parameters['batch_size'],\n                                shuffle=parameters['shuffle'],\n                                token_encoding=parameters['token_encoding'])\n\ntest_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['test_dataset']),\n                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n                                sentence_maxlen=parameters['sentence_maxlen'],\n                                token_maxlen=parameters['token_maxlen'],\n                                batch_size=parameters['batch_size'],\n                                shuffle=parameters['shuffle'],\n                                token_encoding=parameters['token_encoding'])\n\n# Compile ELMo\nelmo_model = ELMo(parameters)\nelmo_model.compile_elmo(print_summary=True)\n\n# Train ELMo\nelmo_model.train(train_data=train_generator, valid_data=val_generator)\n\n# Persist ELMo Bidirectional Language Model in disk\nelmo_model.save(sampled_softmax=False)\n\n# Evaluate Bidirectional Language Model\nelmo_model.evaluate(test_generator)\n\n# Build ELMo meta-model to deploy for production and persist in disk\nelmo_model.wrap_multi_elmo_encoder(print_summary=True, save=True)\n\n# Load ELMo encoder\nelmo_model.load_elmo_encoder()\n\n# Get ELMo embeddings to feed as inputs for downstream tasks\nelmo_embeddings = elmo_model.get_outputs(test_generator, output_type='word', state='mean')\n\n# BUILD & TRAIN NEW KERAS MODEL FOR DOWNSTREAM TASK (E.G., TEXT CLASSIFICATION)\n\n```\n\n## What is missing?\n\n- Turn sampled softmax into full softmax dynamically in evaluation mode (TODO) ([Read comment](https://github.com/iliaschalkidis/ELMo-keras/commit/35fa4f9b3245a9c1078d4c7975064b19bd9742f4#commitcomment-31314484))\n- More testing (TODO)\n- Options to build a unidirectional LM (TODO)\n- Proof-reading, you're all welcome!\n\n## Credits for proof-reading and reporting so far...\n\n[@seolhokim](https://github.com/seolhokim)\n[@geneva0901](https://github.com/geneva0901)\n[@masepehr](https://github.com/masepehr)\n[@dilaratorunoglu](https://github.com/dilaratorunoglu)\n[@Adherer](https://github.com/Adherer)\n",
            "readme_url": "https://github.com/kafura-kafiri/tf2-elmo",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep contextualized word representations",
            "arxiv": "1802.05365",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.05365v2",
            "abstract": "We introduce a new type of deep contextualized word representation that\nmodels both (1) complex characteristics of word use (e.g., syntax and\nsemantics), and (2) how these uses vary across linguistic contexts (i.e., to\nmodel polysemy). Our word vectors are learned functions of the internal states\nof a deep bidirectional language model (biLM), which is pre-trained on a large\ntext corpus. We show that these representations can be easily added to existing\nmodels and significantly improve the state of the art across six challenging\nNLP problems, including question answering, textual entailment and sentiment\nanalysis. We also present an analysis showing that exposing the deep internals\nof the pre-trained network is crucial, allowing downstream models to mix\ndifferent types of semi-supervision signals.",
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer"
            ]
        },
        {
            "title": "Pointer Sentinel Mixture Models",
            "arxiv": "1609.07843",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.07843v1",
            "abstract": "Recent neural network sequence models with softmax classifiers have achieved\ntheir best language modeling performance only with very large hidden states and\nlarge vocabularies. Even then they struggle to predict rare or unseen words\neven if the context makes the prediction unambiguous. We introduce the pointer\nsentinel mixture architecture for neural sequence models which has the ability\nto either reproduce a word from the recent context or produce a word from a\nstandard softmax classifier. Our pointer sentinel-LSTM model achieves state of\nthe art language modeling performance on the Penn Treebank (70.9 perplexity)\nwhile using far fewer parameters than a standard softmax LSTM. In order to\nevaluate how well language models can exploit longer contexts and deal with\nmore realistic vocabularies and larger corpora we also introduce the freely\navailable WikiText corpus.",
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "WikiText-2"
            },
            {
                "name": "Penn Treebank"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999857994884402,
        "task": "Question Answering",
        "task_prob": 0.9724025074171223
    }
}