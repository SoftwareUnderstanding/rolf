{
    "visibility": {
        "visibility": "public"
    },
    "name": "EfficientNets",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "mingxingtan",
                "owner_type": "User",
                "name": "efficientnet",
                "url": "https://github.com/mingxingtan/efficientnet",
                "stars": 86,
                "pushed_at": "2019-05-30 04:52:43+00:00",
                "created_at": "2019-05-30 04:14:01+00:00",
                "language": "Jupyter Notebook",
                "description": "EfficientNets snapshot",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "efficientnet_builder.py",
                "sha": "1b80bbe564fd3a4f5b6324a851551f43d09b1e62",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mingxingtan/efficientnet/blob/master/efficientnet_builder.py"
                    }
                },
                "size": 8183
            },
            {
                "type": "code",
                "name": "efficientnet_model.py",
                "sha": "515518ff69b3a087769595e680c16d416caaf169",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mingxingtan/efficientnet/blob/master/efficientnet_model.py"
                    }
                },
                "size": 14894
            },
            {
                "type": "code",
                "name": "eval_ckpt_example.ipynb",
                "sha": "2ba95319908ff0c549f148e5a2653dc00e05cb50",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mingxingtan/efficientnet/blob/master/eval_ckpt_example.ipynb"
                    }
                },
                "size": 163441
            },
            {
                "type": "code",
                "name": "eval_ckpt_main.py",
                "sha": "e869d4ee767f3444e9872a023da00730a95cd4ad",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mingxingtan/efficientnet/blob/master/eval_ckpt_main.py"
                    }
                },
                "size": 8524
            },
            {
                "type": "code",
                "name": "g3doc",
                "sha": "58cbb659d68686525e87d2d390348e5fdd7f3398",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mingxingtan/efficientnet/tree/master/g3doc"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "imagenet_input.py",
                "sha": "1830c723a2ea4ffdfc145007bf9b8c57cc216d21",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mingxingtan/efficientnet/blob/master/imagenet_input.py"
                    }
                },
                "size": 12146
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "1143fc72e6a1a5ef4a0d94cf519e3bfb6a643ecc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mingxingtan/efficientnet/blob/master/main.py"
                    }
                },
                "size": 28185
            },
            {
                "type": "code",
                "name": "preprocessing.py",
                "sha": "c19006a06fd4d6e55aa6df07da02bf70cd86f5ad",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mingxingtan/efficientnet/blob/master/preprocessing.py"
                    }
                },
                "size": 7121
            },
            {
                "type": "code",
                "name": "utils.py",
                "sha": "fa32ac3f5edd40976c53485fbc9df55c42a9c057",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mingxingtan/efficientnet/blob/master/utils.py"
                    }
                },
                "size": 7963
            }
        ]
    },
    "authors": [
        {
            "name": "Mingxing",
            "email": "tanmingxing@google.com",
            "github_id": "mingxingtan"
        }
    ],
    "tags": [],
    "description": "EfficientNets snapshot",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/mingxingtan/efficientnet",
            "stars": 86,
            "issues": true,
            "readme": "# EfficientNets\n\n[1] Mingxing Tan and Quoc V. Le.  EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. ICML 2019.\n   Arxiv link: https://arxiv.org/abs/1905.11946.\n\n\n## 1. About EfficientNet Models\n\nEfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, yet being an order-of-magnitude smaller and faster than previous models.\n\nWe develop EfficientNets based on AutoML and Compound Scaling. In particular, we first use [AutoML Mobile framework](https://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html) to develop a mobile-size baseline network, named as EfficientNet-B0; Then, we use the compound scaling method to scale up this baseline to obtain EfficientNet-B1 to B7.\n\n<table border=\"0\">\n<tr>\n    <td>\n    <img src=\"./g3doc/params.png\" width=\"100%\" />\n    </td>\n    <td>\n    <img src=\"./g3doc/flops.png\", width=\"90%\" />\n    </td>\n</tr>\n</table>\n\nEfficientNets achieve state-of-the-art accuracy on ImageNet with an order of magnitude better efficiency:\n\n\n* In high-accuracy regime, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet with 66M parameters and 37B FLOPS, being 8.4x smaller and 6.1x faster on CPU inference than previous best [Gpipe](https://arxiv.org/abs/1811.06965).\n\n* In middle-accuracy regime, our EfficientNet-B1 is 7.6x smaller and 5.7x faster on CPU inference than [ResNet-152](https://arxiv.org/abs/1512.03385), with similar ImageNet accuracy.\n\n* Compared with the widely used [ResNet-50](https://arxiv.org/abs/1512.03385), our EfficientNet-B4 improves the top-1 accuracy from 76.3% of ResNet-50 to 82.6% (+6.3%), under similar FLOPS constraint.\n\n## 2. Using Pretrained EfficientNet Checkpoints\n\nWe have provided a list of EfficientNet checkpoints for [EfficientNet-B0](https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/efficientnet-b0.tar.gz), [EfficientNet-B1](https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/efficientnet-b1.tar.gz), [EfficientNet-B2](https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/efficientnet-b2.tar.gz), and [EfficientNet-B3](https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/efficientnet-b3.tar.gz). A quick way to use these checkpoints is to run:\n\n    $ export MODEL=efficientnet-b0\n    $ wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/${MODEL}.tar.gz\n    $ tar zxf ${MODEL}.tar.gz\n    $ wget https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG -O panda.jpg\n    $ wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/eval_data/labels_map.txt\n    $ python eval_ckpt_main.py --model_name=$MODEL --ckpt_dir=$MODEL --example_img=panda.jpg --labels_map_file=labels_map.txt\n\nPlease refer to the following colab for more instructions on how to obtain and use those checkpoints.\n\n  * [`eval_ckpt_example.ipynb`](eval_ckpt_example.ipynb): A colab example to load\n EfficientNet pretrained checkpoints files and use the restored model to classify images.\n\n\n## 3. Training EfficientNets on TPUs.\n\n\nTo train this model on Cloud TPU, you will need:\n\n   * A GCE VM instance with an associated Cloud TPU resource\n   * A GCS bucket to store your training checkpoints (the \"model directory\")\n   * Install TensorFlow version >= 1.13 for both GCE VM and Cloud.\n\nThen train the model:\n\n    $ export PYTHONPATH=\"$PYTHONPATH:/path/to/models\"\n    $ python main.py --tpu=TPU_NAME --data_dir=DATA_DIR --model_dir=MODEL_DIR\n\n    # TPU_NAME is the name of the TPU node, the same name that appears when you run gcloud compute tpus list, or ctpu ls.\n    # MODEL_DIR is a GCS location (a URL starting with gs:// where both the GCE VM and the associated Cloud TPU have write access\n    # DATA_DIR is a GCS location to which both the GCE VM and associated Cloud TPU have read access.\n\n\nFor more instructions, please refer to our tutorial: https://cloud.google.com/tpu/docs/tutorials/efficientnet\n",
            "readme_url": "https://github.com/mingxingtan/efficientnet",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "arxiv": "1905.11946",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.11946v5",
            "abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed\nresource budget, and then scaled up for better accuracy if more resources are\navailable. In this paper, we systematically study model scaling and identify\nthat carefully balancing network depth, width, and resolution can lead to\nbetter performance. Based on this observation, we propose a new scaling method\nthat uniformly scales all dimensions of depth/width/resolution using a simple\nyet highly effective compound coefficient. We demonstrate the effectiveness of\nthis method on scaling up MobileNets and ResNet.\n  To go even further, we use neural architecture search to design a new\nbaseline network and scale it up to obtain a family of models, called\nEfficientNets, which achieve much better accuracy and efficiency than previous\nConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3%\ntop-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on\ninference than the best existing ConvNet. Our EfficientNets also transfer well\nand achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%),\nand 3 other transfer learning datasets, with an order of magnitude fewer\nparameters. Source code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.",
            "authors": [
                "Mingxing Tan",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
            "arxiv": "1811.06965",
            "year": 2018,
            "url": "http://arxiv.org/abs/1811.06965v5",
            "abstract": "Scaling up deep neural network capacity has been known as an effective\napproach to improving model quality for several different machine learning\ntasks. In many cases, increasing model capacity beyond the memory limit of a\nsingle accelerator has required developing special algorithms or\ninfrastructure. These solutions are often architecture-specific and do not\ntransfer to other tasks. To address the need for efficient and task-independent\nmodel parallelism, we introduce GPipe, a pipeline parallelism library that\nallows scaling any network that can be expressed as a sequence of layers. By\npipelining different sub-sequences of layers on separate accelerators, GPipe\nprovides the flexibility of scaling a variety of different networks to gigantic\nsizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining\nalgorithm, resulting in almost linear speedup when a model is partitioned\nacross multiple accelerators. We demonstrate the advantages of GPipe by\ntraining large-scale neural networks on two different tasks with distinct\nnetwork architectures: (i) Image Classification: We train a\n557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on\nImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single\n6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100\nlanguages and achieve better quality than all bilingual models.",
            "authors": [
                "Yanping Huang",
                "Youlong Cheng",
                "Ankur Bapna",
                "Orhan Firat",
                "Mia Xu Chen",
                "Dehao Chen",
                "HyoukJoong Lee",
                "Jiquan Ngiam",
                "Quoc V. Le",
                "Yonghui Wu",
                "Zhifeng Chen"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "Wikipedia"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "COCO"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "ILSVRC 2015"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9994681361492392,
        "task": "Object Detection",
        "task_prob": 0.5869374602070596
    }
}