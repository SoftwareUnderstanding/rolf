{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "DeepIllumination (More datasets coming soon)| Video",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "CreativeCodingLab",
                "owner_type": "Organization",
                "name": "DeepIllumination",
                "url": "https://github.com/CreativeCodingLab/DeepIllumination",
                "stars": 51,
                "pushed_at": "2019-02-07 18:47:13+00:00",
                "created_at": "2017-08-01 18:00:36+00:00",
                "language": "Python",
                "description": "Code and examples from our paper \"Deep Illumination: Approximating Dynamic Global Illumination with Generative Adversarial Networks,\" by Manu Mathew Thomas and Angus Forbes",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".vscode",
                "sha": "65c6c46571d0870fad39ced3ccbbdfcadb658a11",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CreativeCodingLab/DeepIllumination/tree/master/.vscode"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "7965b149329218731f726b9c68352070a629946f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CreativeCodingLab/DeepIllumination/blob/master/LICENSE"
                    }
                },
                "size": 1077
            },
            {
                "type": "code",
                "name": "__pycache__",
                "sha": "eaee2bb8680f31bda6d581b724b0b2ac6bc7f4bd",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CreativeCodingLab/DeepIllumination/tree/master/__pycache__"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "data.py",
                "sha": "c45806d1d3c1296b0af19f4508c33564df9e8489",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CreativeCodingLab/DeepIllumination/blob/master/data.py"
                    }
                },
                "size": 1128
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "69b00d95a3912ea03b975ae27e7b66a3931c84ce",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CreativeCodingLab/DeepIllumination/blob/master/model.py"
                    }
                },
                "size": 4770
            },
            {
                "type": "code",
                "name": "output.png",
                "sha": "e76e8fe96a6dfddfcd2c40fcdd64582c5dc94529",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CreativeCodingLab/DeepIllumination/blob/master/output.png"
                    }
                },
                "size": 1129987
            },
            {
                "type": "code",
                "name": "teaser.png",
                "sha": "29fd9a9788961ed1c7de20febc143c48d04cd52f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CreativeCodingLab/DeepIllumination/blob/master/teaser.png"
                    }
                },
                "size": 329883
            },
            {
                "type": "code",
                "name": "test.py",
                "sha": "c305f8ff0a733079c81330b603ad474cd5b13225",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CreativeCodingLab/DeepIllumination/blob/master/test.py"
                    }
                },
                "size": 2051
            },
            {
                "type": "code",
                "name": "train.py",
                "sha": "663b37d031bce7caad38507990957484fde9f33e",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CreativeCodingLab/DeepIllumination/blob/master/train.py"
                    }
                },
                "size": 8336
            },
            {
                "type": "code",
                "name": "util.py",
                "sha": "6813e2edb467b092545852a665f37463730b5aec",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CreativeCodingLab/DeepIllumination/blob/master/util.py"
                    }
                },
                "size": 937
            }
        ]
    },
    "authors": [
        {
            "name": "manumathewthomas",
            "email": "manu.mathew.thomas@intel.com",
            "github_id": "manumathewthomas"
        }
    ],
    "tags": [],
    "description": "Code and examples from our paper \"Deep Illumination: Approximating Dynamic Global Illumination with Generative Adversarial Networks,\" by Manu Mathew Thomas and Angus Forbes",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/CreativeCodingLab/DeepIllumination",
            "stars": 51,
            "issues": true,
            "readme": "# DeepIllumination (More datasets coming soon)| [Video](https://www.youtube.com/watch?v=z_zmRWxU-PY)\n\n<img src=\"https://github.com/CreativeCodingLab/DeepRendering/blob/master/teaser.png\" alt=\"alt text\" width=\"945\" height=\"322.5\">\n\n#### A paper introducing our DeepIllumination approach is currently in preparation to EuroGraphics. Pre-print is available on arXiv at https://arxiv.org/abs/1710.09834\n\n## Introduction\n\nAnimation movie studios like Pixar uses a technique called Pathtracing which produces high-quality photorealistic images. Due to the computational complexity of this approach, it will take 8-16 hours to render depending on the composition of the scene. This time-consuming process makes Pathtracing unsuitable for interactive image synthesis. To achieve this increased visual quality in a real time application many approaches have been proposed in the recent past to approximate global illumination effects like ambient occlusion, reflections, indirect light, scattering, depth of field, motion blur and caustics. While these techniques improve the visual quality, the results are incomparable to the one produce by Pathtracing. We propose a novel technique where we make use of a deep generative model to generate high-quality photorealistic frames from a geometry buffer(G-buffer). The main idea here is to train a deep convolutional neural network to find a mapping from G-buffer to pathtraced image of the same scene. This trained network can then be used in a real time scene to get high-quality results.\n\n\n#### Table of Contents\n\n* [Installation](#installation)\n* [Running](#running)\n* [Dataset](#dataset)\n* [Hyperparameters](#hyperparameter)\n* [Results](#results)\n* [Notes](#notes)\n* [Credits](#credits)\n\n## Installation\n\nTo run the project you will need:\n * python 3.5\n * pytorch\n * [CHECKPOINT FILE](https://uofi.box.com/v/DeepRenderingCheckpointFile)\n * [Dataset](https://uofi.box.com/v/DeepRenderingDataset)\n\n## Running\n\nOnce you have all the depenedencies ready, do the folowing:\nDownload the dataset and extract it.\nDownload the checkpoint file and extract it.\nNow you will have two folders checkpoint and dataset.\n\nTo train, move your training set to dataset/[name of your dataset]/train and validation set to dataset/[name of your dataset]/val\n```\nRun python train.py --dataset dataset/[name of your dataset]/ --n_epochs num of epochs\n```\n```\npython train.py --dataset dataset/DeepRendering --n_epochs 200\n```\ncheck train.py for more options.\nValidation is done afer every epoch and will be inside validation/DeepRendering/\n\nTo test, \nRun \n``` \npython test.py --dataset dataset/[name of your dataset]/ --model checkpoint/[name of your checkpoint] \n```\n``` \npython train.py --dataset dataset/DeepRendering --model checkpoint/\n```\nCheck results/DeepRendering for the output.\n\n## Dataset\nDataset was created using a simple cornell box made with Unity3D. GBuffers(depth, normal, albedo, direct light) and VXGI outputs are extracted.\n\nFor training, we used 4 object - sphere, cylinder, cube, stanford bunny and a dragon.\n\nFor testing we used 2 object - statue and buddha\n\n## Hyperparameters\n* Number of epochs - 12\n* L1 factor - 100\n* lr - 0.0002\n* n_filters - 128\n\n## Results\n\n<img src=\"https://github.com/CreativeCodingLab/DeepRendering/blob/master/output.png\" alt=\"alt text\" width=\"960\" height=\"480\">\n\n ## Notes\n \n* One network for one world - training a network by introducing it to the rules of one enviornment produce good result rather than to make a generalized network to work for all kinds of enviornment. Also we only need fewer parameters.\n* Network gives a good approximatation even though it has only seen the training set 12 times.\n* Our network failed to learn ambient occlsuion. We believe with more dataset focusing on ambient occlusion we can force the network to learn AO.\n\n ## Credits\n* [Imgae to Image Translation](https://arxiv.org/abs/1611.07004)\n* [SRGAN](https://arxiv.org/pdf/1609.04802.pdf)\n* [Creating photorealistic images from gameboy camera](http://www.pinchofintelligence.com/photorealistic-neural-network-gameboy/)\n* [CS231n](https://cs231n.github.io/)\n\n\n",
            "readme_url": "https://github.com/CreativeCodingLab/DeepIllumination",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Deep Illumination: Approximating Dynamic Global Illumination with Generative Adversarial Network",
            "arxiv": "1710.09834",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.09834v2",
            "abstract": "We present Deep Illumination, a novel machine learning technique for\napproximating global illumination (GI) in real-time applications using a\nConditional Generative Adversarial Network. Our primary focus is on generating\nindirect illumination and soft shadows with offline rendering quality at\ninteractive rates. Inspired from recent advancement in image-to-image\ntranslation problems using deep generative convolutional networks, we introduce\na variant of this network that learns a mapping from Gbuffers (depth map,\nnormal map, and diffuse map) and direct illumination to any global illumination\nsolution. Our primary contribution is showing that a generative model can be\nused to learn a density estimation from screen space buffers to an advanced\nillumination model for a 3D environment. Once trained, our network can\napproximate global illumination for scene configurations it has never\nencountered before within the environment it was trained on. We evaluate Deep\nIllumination through a comparison with both a state of the art real-time GI\ntechnique (VXGI) and an offline rendering GI technique (path tracing). We show\nthat our method produces effective GI approximations and is also\ncomputationally cheaper than existing GI techniques. Our technique has the\npotential to replace existing precomputed and screen-space techniques for\nproducing global illumination effects in dynamic scenes with physically-based\nrendering quality.",
            "authors": [
                "Manu Mathew Thomas",
                "Angus G. Forbes"
            ]
        },
        {
            "title": "Image-to-Image Translation with Conditional Adversarial Networks",
            "arxiv": "1611.07004",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.07004v3",
            "abstract": "We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either.",
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A. Efros"
            ]
        },
        {
            "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
            "arxiv": "1609.04802",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.04802v5",
            "abstract": "Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.",
            "authors": [
                "Christian Ledig",
                "Lucas Theis",
                "Ferenc Huszar",
                "Jose Caballero",
                "Andrew Cunningham",
                "Alejandro Acosta",
                "Andrew Aitken",
                "Alykhan Tejani",
                "Johannes Totz",
                "Zehan Wang",
                "Wenzhe Shi"
            ]
        },
        {
            "title": "CHECKPOINT FILE",
            "url": "https://uofi.box.com/v/DeepRenderingCheckpointFile"
        },
        {
            "title": "Dataset",
            "url": "https://uofi.box.com/v/DeepRenderingDataset"
        },
        {
            "title": "Creating photorealistic images from gameboy camera",
            "url": "http://www.pinchofintelligence.com/photorealistic-neural-network-gameboy/"
        },
        {
            "title": "CS231n",
            "url": "https://cs231n.github.io/"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://uofi.box.com/v/DeepRenderingDataset"
                    }
                }
            },
            {
                "name": "OCCLUSION"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9999953740243794,
        "task": "Image-to-Image Translation",
        "task_prob": 0.9577076585496281
    }
}