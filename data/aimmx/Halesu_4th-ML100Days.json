{
    "visibility": {
        "visibility": "public"
    },
    "name": "4th-ML100Days",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Halesu",
                "owner_type": "User",
                "name": "4th-ML100Days",
                "url": "https://github.com/Halesu/4th-ML100Days",
                "stars": 9,
                "pushed_at": "2020-08-17 04:28:42+00:00",
                "created_at": "2020-02-18 00:07:58+00:00",
                "language": "Jupyter Notebook",
                "frameworks": [
                    "Keras",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": "doc",
                "sha": "49fc4c6064b6f100f17175e9a3b54ebe3002e757",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Halesu/4th-ML100Days/tree/master/doc"
                    }
                },
                "num_files": 108
            },
            {
                "type": "code",
                "name": "homework",
                "sha": "b648faa85785bac0d4f70c5235694864403477ad",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Halesu/4th-ML100Days/tree/master/homework"
                    }
                },
                "num_files": 111
            }
        ]
    },
    "authors": [
        {
            "name": "HaleSu",
            "github_id": "Halesu"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Halesu/4th-ML100Days",
            "stars": 9,
            "issues": true,
            "readme": "# 4th-ML100Days\n\n### \u6a5f\u5668\u5b78\u7fd2\u6982\u8ad6 Introduction of Machine Learning\n* **Day_01 : \u8cc7\u6599\u4ecb\u7d39\u8207\u8a55\u4f30\u6307\u6a19**\n    * \u63a2\u7d22\u6d41\u7a0b : \u627e\u5230\u554f\u984c -> \u521d\u63a2 -> \u6539\u9032 -> \u5206\u4eab -> \u7df4\u7fd2 -> \u5be6\u6230\n    * \u601d\u8003\u95dc\u9375\u9ede :\n        * \u70ba\u4ec0\u9ebc\u9019\u500b\u554f\u984c\u91cd\u8981\uff1f\n        * \u8cc7\u6599\u5f9e\u4f55\u800c\u4f86\uff1f\n        * \u8cc7\u6599\u578b\u614b\u662f\u4ec0\u9ebc\uff1f\n        * \u56de\u7b54\u554f\u984c\u7684\u95dc\u9375\u6307\u6a19\u662f\u4ec0\u9ebc\uff1f\n* **Day_02 : \u6a5f\u5668\u5b78\u7fd2\u6982\u8ad6**\n    * \u6a5f\u5668\u5b78\u7fd2\u7bc4\u7587 : **\u6df1\u5ea6\u5b78\u7fd2 (Deep Learning)** \u2282 **\u6a5f\u5668\u5b78\u7fd2 (Machine Learning)** \u2282 **\u4eba\u5de5\u667a\u6167 (Artificial Intelligence)**\n    * \u6a5f\u5668\u5b78\u7fd2\u662f\u4ec0\u9ebc :\n        * \u8b93\u6a5f\u5668\u5f9e\u8cc7\u6599\u627e\u5c0b\u898f\u5f8b\u8207\u8da8\u52e2\uff0c\u4e0d\u9700\u8981\u7d66\u5b9a\u7279\u6b8a\u898f\u5247\n        * \u7d66\u5b9a\u76ee\u6a19\u51fd\u6578\u8207\u8a13\u7df4\u8cc7\u6599\uff0c\u5b78\u7fd2\u51fa\u80fd\u8b93\u76ee\u6a19\u51fd\u6578\u6700\u4f73\u7684\u6a21\u578b\u53c3\u6578\n    * \u6a5f\u5668\u5b78\u7fd2\u7e3d\u985e :\n        * **\u76e3\u7763\u662f\u5b78\u7fd2 (Supervised Learning)** : \u5716\u50cf\u5206\u985e (Classification)\u3001\u8a50\u9a19\u5075\u6e2c (Fraud detection)\uff0c\u9700\u6210\u5c0d\u8cc7\u6599 (x,y)\n        * **\u975e\u76e3\u7763\u662f\u5b78\u7fd2 (Unsupervised Learning)** : \u964d\u7dad (Dimension Reduction)\u3001\u5206\u7fa4 (Clustering)\u3001\u58d3\u7e2e\uff0c\u53ea\u9700\u8cc7\u6599 (x)\n        * **\u5f37\u5316\u5b78\u7fd2 (Reinforcement Learning)** : \u4e0b\u570d\u68cb\u3001\u6253\u96fb\u73a9\uff0c\u900f\u904e\u4ee3\u7406\u6a5f\u5668\u4eba (Agent) \u8207\u74b0\u5883 (Environment) \u4e92\u52d5\uff0c\u5b78\u7fd2\u5982\u4f55\u7372\u53d6\u6700\u9ad8\u734e\u52f5 (Reward)\uff0c\u4f8b\u5982 Alpha GO\n* **Day_03 : \u6a5f\u5668\u5b78\u7fd2\u6d41\u7a0b\u8207\u6b65\u9a5f**\n    * **\u8cc7\u6599\u8490\u96c6\u3001\u524d\u8655\u7406**\n        * \u653f\u5e9c\u516c\u958b\u8cc7\u6599\u3001Kaggle \u8cc7\u6599\n            * \u7d50\u69cb\u5316\u8cc7\u6599 : Excel \u6a94\u3001CSV \u6a94\n            * \u975e\u7d50\u69cb\u5316\u8cc7\u6599 : \u5716\u7247\u3001\u5f71\u97f3\u3001\u6587\u5b57\n        * \u4f7f\u7528 Python \u5957\u4ef6\n            * \u958b\u555f\u5716\u7247 : `PIL`\u3001`skimage`\u3001`open-cv`\n            * \u958b\u555f\u6587\u4ef6 : `pandas`\n        * \u8cc7\u6599\u524d\u8655\u7406 :\n            * \u7f3a\u5931\u503c\u586b\u88dc\n            * \u96e2\u7fa4\u503c\u8655\u7406\n            * \u6a19\u6e96\u5316\n    * **\u5b9a\u7fa9\u76ee\u6a19\u8207\u8a55\u4f30\u6e96\u5247**\n        * \u56de\u6b78\u554f\u984c\uff1f\u5206\u985e\u554f\u984c\uff1f\n        * \u9810\u6e2c\u76ee\u6a19\u662f\u4ec0\u9ebc\uff1f(target or y)\n        * \u7528\u4ec0\u9ebc\u8cc7\u6599\u9032\u884c\u9810\u6e2c\uff1f(predictor or x)\n        * \u5c07\u8cc7\u6599\u5206\u70ba :\n            * \u8a13\u7df4\u96c6\uff0ctraining set\n            * \u9a57\u8b49\u96c6\uff0cvalidation set\n            * \u6e2c\u8a66\u96c6\uff0ctest set\n        * \u8a55\u4f30\u6307\u6a19\n            * \u56de\u6b78\u554f\u984c (\u9810\u6e2c\u503c\u70ba\u5be6\u6578)\n                * RMSE : Root Mean Squeare Error\n                * MAE : Mean Absolute Error\n                * R-Square\n            * \u5206\u985e\u554f\u984c (\u9810\u6e2c\u503c\u70ba\u985e\u5225)\n                * Accuracy\n                * [F1-score](https://en.wikipedia.org/wiki/F1_score)\n                * [AUC](https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF)\uff0cArea Under Curve\n    * **\u5efa\u7acb\u6a21\u578b\u8207\u8abf\u6574\u53c3\u6578**\n        * Regression\uff0c\u56de\u6b78\u6a21\u578b\n        * Tree-base model\uff0c\u6a39\u6a21\u578b\n        * Neural network\uff0c\u795e\u7d93\u7db2\u8def\n        * Hyperparameter\uff0c\u6839\u64da\u5c0d\u6a21\u578b\u4e86\u89e3\u548c\u8a13\u7df4\u60c5\u5f62\u9032\u884c\u8abf\u6574\n    * **\u5c0e\u5165**\n        * \u5efa\u7acb\u8cc7\u6599\u8490\u96c6\u3001\u524d\u8655\u7406(Preprocessing)\u7b49\u6d41\u7a0b\n        * \u9001\u9032\u6a21\u578b\u9032\u884c\u9810\u6e2c\n        * \u8f38\u51fa\u9810\u6e2c\u7d50\u679c\n        * \u8996\u5c08\u6848\u9700\u6c42\u8abf\u6574\u524d\u5f8c\u7aef\n* **Day_04 : \u8b80\u53d6\u8cc7\u6599\u8207\u5206\u6790\u6d41\u7a0b (EDA\uff0cExploratory Data Analysis)**   \n    * \u900f\u904e\u8996\u89ba\u5316\u548c\u7d71\u8a08\u5de5\u5177\u9032\u884c\u5206\u6790\n        * \u4e86\u89e3\u8cc7\u6599 : \u7372\u53d6\u8cc7\u6599\u5305\u542b\u7684\u8cc7\u8a0a\u3001\u7d50\u69cb\u3001\u7279\u9ede\n        * \u767c\u73fe outlier \u6216\u7570\u5e38\u6578\u503c : \u6aa2\u67e5\u8cc7\u6599\u662f\u5426\u6709\u8aa4\n        * \u5206\u6790\u5404\u8b8a\u6578\u9593\u7684\u95dc\u806f\u6027 : \u627e\u51fa\u91cd\u8981\u7684\u8b8a\u6578\n    * \u6536\u96c6\u8cc7\u6599 -> \u6578\u64da\u6e05\u7406 -> \u7279\u5fb5\u8403\u53d6 -> \u8cc7\u6599\u8996\u89ba\u5316 -> \u5efa\u7acb\u6a21\u578b -> \u9a57\u8b49\u6a21\u578b -> \u6c7a\u7b56\u61c9\u7528\n### \u8cc7\u6599\u6e05\u7406\u8207\u6578\u64da\u524d\u8655\u7406 Data Cleaning and Preprocessing\n* **Day_05 : \u5982\u4f55\u5efa\u7acb\u4e00\u500b DataFrame\uff1f\u5982\u4f55\u8b80\u53d6\u5176\u4ed6\u8cc7\u6599\uff1f**\n    * \u7528 [pd.DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) \u4f86\u5efa\u7acb\uff0c[\u7df4\u7fd2\u7db2\u7ad9](https://github.com/guipsamora/pandas_exercises)\n    * CSV\n        ```py\n        import pandas as pd\n        df = pd.read_csv('example.csv') # sep=','\n        df = pd.read_table('example.csv') # sep='\\t'\n        ```\n    * \u6587\u672c (txt)\n        ```py\n        with open('example.txt','r') as f:\n            data = f.readlines()\n        print(data)\n        ```\n    * Json\n        ```py\n        import json\n        with open('example.json','r') as f:\n            data = json.load(f)\n        print(data)\n        ```\n    * \u77e9\u9663\u6a94 (mat)\n        ```py\n        import scipy.io as sio\n        data = sio.load('example.mat')\n        ```\n    * \u5716\u50cf\u6a94 (PNG/JPG...)\n        ```py\n        import cv2\n        image = cv2.imread('example.jpg') # Cv2 \u6703\u4ee5 GBR \u8b80\u5165\n        image = cv2.cvtcolor(image,cv2.COLOR_BGR2RGB)\n        ```\n    * Python npy\n        ```py\n        import numpy as np\n        arr = np.load('example.npy')\n        ```    \n    * Picke (pkl)\n        ```py\n        import pickle\n        with open('example.pkl','rb') as f:\n            arr = pickle.load(f)\n        ```\n* **Day_06 : \u6b04\u4f4d\u7684\u8cc7\u6599\u985e\u578b\u4ecb\u7d39\u53ca\u8655\u7406**\n    * \u8cc7\u6599\u985e\u578b :\n        * \u96e2\u6563\u8b8a\u6578 : \u623f\u9593\u6578\u91cf\u3001\u6027\u5225\u3001\u570b\u5bb6\n        * \u9023\u7e8c\u8b8a\u6578 : \u8eab\u9ad8\u3001\u82b1\u8cbb\u6642\u9593\u3001\u8eca\u901f\n        * \u5e38\u898b\u7684[\u8cc7\u6599\u985e\u578b](https://blog.csdn.net/claroja/article/details/72622375) : \n            * float64 : \u6d6e\u9ede\u6578\uff0c\u53ef\u8868\u793a\u96e2\u6563\u6216\u9023\u7e8c\u8b8a\u6578\n            * int64 : \u6574\u6578\uff0c\u53ef\u8868\u793a\u96e2\u6563\u6216\u9023\u7e8c\u8b8a\u6578\n            * object : \u5305\u542b\u5b57\u4e32\uff0c\u8868\u793a\u985e\u5225\u578b\u8b8a\u6578\n                * Label encoding : \u5c0d\u8c61\u6709\u5e8f\u8cc7\u6599\uff0c\u4f8b\u5982\u5e74\u9f61\n                    ```py\n                    from sklearn.preprocessing import LabelEncoder\n                    df[col] = LabelEncoder().fit_transform(df[col])\n                    ```\n                * One Hot encoding : \u5c0d\u8c61\u7121\u5e8f\u8cc7\u6599\uff0c\u4f8b\u5982\u570b\u5bb6\n                    ```py\n                    df = pd.get_dummies(df)\n                    ```\n            * \u5176\u4ed6 : \u65e5\u671f\u3001boolean\n* **Day_07 : \u7279\u5fb5\u985e\u578b**\n    * \u6578\u503c\u578b\u7279\u5fb5 : \u6709\u4e0d\u540c\u8f49\u63db\u65b9\u5f0f\uff0c\u51fd\u6578/\u689d\u4ef6\u5f0f\u90fd\u53ef\u4ee5\n    * \u985e\u5225\u578b\u7279\u5fb5 : \u901a\u5e38\u4e00\u7a2e\u985e\u5225\u5c0d\u61c9\u4e00\u7a2e\u5206\u6578\n    * \u4e8c\u5143\u578b\u7279\u5fb5 : True/False\uff0c\u53ef\u7576\u985e\u5225\u6216\u6578\u503c\u8655\u7406\n    * \u6392\u5e8f\u578b\u7279\u5fb5 : \u6709\u5927\u5c0f\u95dc\u4fc2\u6642\u7576\u6578\u503c\u578b\u7279\u5fb5\u8655\u7406\n    * \u6642\u9593\u578b\u7279\u5fb5 : \u53ef\u7576\u6578\u503c\u6216\u985e\u5225\u8655\u7406\uff0c\u4f46\u6703\u5931\u53bb\u9031\u671f\u6027\uff0c\u9700\u7279\u5225\u8655\u7406\n* **Day_08 : EDA \u4e4b\u8cc7\u6599\u5206\u4f48**\n    * \u4ee5\u55ae\u8b8a\u91cf\u9032\u884c\u5206\u6790\n        * \u8a08\u7b97\u96c6\u4e2d\u8da8\u52e2\n            * \u5e73\u5747\u6578\uff0c`mean()`\n            * \u4e2d\u4f4d\u6578\uff0c`median()`\n            * \u773e\u6578\uff0c`mode()`\n        * \u8a08\u7b97\u8cc7\u6599\u5206\u6563\u7a0b\u5ea6\n            * \u6700\u5c0f\u503c\uff0c`min()`\n            * \u6700\u5927\u503c\uff0c`max()`\n            * \u7bc4\u570d\n            * \u56db\u5206\u4f4d\u5dee\uff0c`quantile()`\n            * \u8b8a\u7570\u6578\uff0c`var()`\n            * \u6a19\u6e96\u5dee\uff0c`std()`\n    * \u8996\u89ba\u5316\u65b9\u5f0f\n        * [matplotlib](https://matplotlib.org/gallery/index.html)\n            ```py\n            import matplotlib.pyplot as plt\n            %matplotlib inline # \u5167\u5d4c\u65b9\u5f0f\uff0c\u4e0d\u9700\u8981 plt.show()\n            %matplotlib notebook # \u4e92\u52d5\u65b9\u5f0f\n            plt.style.use('ggplot') # \u6539\u8b8a\u7e6a\u5716\u98a8\u683c\n            plt.style.available # \u67e5\u8a62\u6709\u54ea\u4e9b\u98a8\u683c\n            ```\n        * [seaborn](https://seaborn.pydata.org/examples/index.html)\n            ```py\n            import seaborn as sns\n            sns.set()   # \u4f7f\u7528\u9810\u8a2d\u98a8\u683c\n            sns.set_style('whitegrid')  # \u8b8a\u66f4\u5176\u4ed6\u98a8\u683c\n            sns.axes_style(...) # \u66f4\u8a73\u7d30\u7684\u8a2d\u5b9a\n            ```\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [\u6558\u8ff0\u7d71\u8a08\u8207\u6a5f\u7387\u5206\u4f48](http://www.hmwu.idv.tw/web/R_AI_M/AI-M1-hmwu_R_Stat&Prob_v2.pdf)\n* **Day_09 : \u96e2\u7fa4\u503c (Outlier) \u53ca\u5176\u8655\u7406**\n    * \u7570\u5e38\u503c\u51fa\u73fe\u53ef\u80fd\u539f\u56e0\n        * \u672a\u77e5\u503c\u96a8\u610f\u586b\u88dc\u6216\u7d04\u5b9a\u6210\u4fd7\u4ee3\u5165\n        * \u932f\u8aa4\u7d00\u9304/\u624b\u8aa4/\u7cfb\u7d71\u6027\u932f\u8aa4\n    * \u6aa2\u67e5\u7570\u5e38\u503c\u7684\u65b9\u6cd5\n        * \u7d71\u8a08\u503c : \u5982\u5e73\u5747\u6578\u3001\u6a19\u6e96\u5dee\u3001\u4e2d\u4f4d\u6578\u3001\u5206\u4f4d\u6578\n            ```py\n            df.describe()\n            df[col].value_counts()\n            ```\n        * \u756b\u5716 : \u5982\u503c\u65b9\u5716\u3001\u5408\u5716\u3001\u6b21\u6578\u7d2f\u7a4d\u5206\u4f48\u7b49\n            ```py\n            df.plot.hist()  # \u76f4\u65b9\u5716\n            df.boxplot()    # \u76d2\u5716\n            df = df[col].value_counts().sort_index().cumsum()\n            plt.plot(list(df.index), df/df.max())   # \u6b21\u6578\u7d2f\u7a4d\u5716\n            ```\n    * \u7570\u5e38\u503c\u7684\u8655\u7406\u65b9\u6cd5\n        * \u53d6\u4ee3\u88dc\u690d : \u4e2d\u4f4d\u6578\u3001\u5e73\u5747\u6578\u7b49\n        * \u53e6\u5efa\u6b04\u4f4d : \u6a19\u793a\u7570\u5e38(Y/N)\n        * \u6574\u6b04\u4e0d\u7528\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [\u8fa8\u8b58\u7570\u5e38\u503c](https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/)\n        * [IQR](https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/)\n* **Day_10 : \u6578\u503c\u7279\u5fb5 - \u53bb\u9664\u96e2\u7fa4\u503c**\n    * \u65b9\u6cd5\u4e00 : \u53bb\u9664\u96e2\u7fa4\u503c\uff0c\u53ef\u80fd\u522a\u9664\u6389\u91cd\u8981\u8cc7\u8a0a\uff0c\u4f46\u4e0d\u522a\u9664\u6703\u9020\u6210\u7279\u5fb5\u7e2e\u653e (\u6a19\u6e96\u5316/\u6700\u5927\u6700\u5c0f\u5316) \u6709\u5f88\u5927\u554f\u984c\n        ```py\n        mask = df[col] > threshold_lower & df[col] < threshold_upper\n        df = df[mask]\n        ```\n    * \u65b9\u6cd5\u4e8c : \u8abf\u6574\u96e2\u7fa4\u503c\n        ```py\n        df[col] = df[col].clip(lower, upper) # \u5c07\u6578\u503c\u9650\u5236\u5728\u7bc4\u570d\u5167\n        ```\n* **Day_11 : \u6578\u503c\u586b\u88dc\u8207\u9023\u7e8c\u6578\u503c\u6a19\u6e96\u5316**\n    * \u5e38\u7528\u65bc\u586b\u88dc\u7684\u7d71\u8a08\u503c\n        * \u4e2d\u4f4d\u6578 (median) : `np.median(df[col])`\n        * \u5206\u4f4d\u6578 (quantiles) : `np.quantile(df[col],q=...)`\n        * \u773e\u6578 (mode) : \n            ```py\n            from scipy.stats import mode\n            mode(df[col])\n            ```\n        * \u5e73\u5747\u6578 (mean) : `np.mean(df[col])`\n    * \u9023\u7e8c\u578b\u6578\u503c[\u6a19\u6e96\u5316](https://blog.csdn.net/pipisorry/article/details/52247379)\n        * \u70ba\u4f55\u8981\u6a19\u6e96\u5316 : \u6bcf\u500b\u8b8a\u6578 x \u5c0d y \u7684\u5f71\u97ff\u529b\u4e0d\u540c\n        * \u662f\u5426\u8981\u505a\u6a19\u6e96\u5316 : \u5c0d\u65bc\u6b0a\u91cd\u654f\u611f\u6216\u640d\u5931\u51fd\u6578\u5e73\u6ed1\u6709\u5e6b\u52a9\u8005\n            * \u975e\u6a39\u72c0\u6a21\u578b : \u7dda\u6027\u56de\u6b78\u3001\u7f85\u5409\u65af\u56de\u6b78\u3001\u985e\u795e\u7d93\uff0c\u5c0d\u9810\u6e2c\u6703\u6709\u5f71\u97ff\n            * \u6a39\u72c0\u6a21\u578b : \u6c7a\u7b56\u6a39\u3001\u96a8\u6a5f\u68ee\u6797\u6a39\u3001\u68af\u5ea6\u63d0\u5347\u6a39\uff0c\u5c0d\u9810\u6e2c\u4e0d\u6703\u6709\u5f71\u97ff\n        * \u512a\u9ede : \u52a0\u901f\u6a21\u578b\u6536\u6582\uff0c\u63d0\u5347\u6a21\u578b\u7cbe\u6e96\u5ea6\n        * \u7f3a\u9ede : \u91cf\u7684\u55ae\u4f4d\u6709\u5f71\u97ff\u529b\u6642\u4e0d\u9069\u7528\n        * \u6a19\u6e96\u5316\u65b9\u6cd5 :\n            * Z-transform : $ \\frac{(x - mean(x))}{std(x)} $\n            * Range (0 ~ 1) : $ \\frac{x - min(x)}{max(x) - min(x)} $\n            * Range (-1 ~ 1) : $ (\\frac{x - min(x)}{max(x) - min(x)} - 0.5) * 2 $\n* **Day_12 : \u6578\u503c\u578b\u7279\u5fb5 - \u88dc\u7f3a\u5931\u503c\u8207\u6a19\u6e96\u5316**\n    * [\u7f3a\u5931\u503c\u8655\u7406](https://juejin.im/post/5b5c4e6c6fb9a04f90791e0c) : \u6700\u91cd\u8981\u7684\u662f\u6b04\u4f4d\u9818\u57df\u77e5\u8b58\u8207\u6b04\u4f4d\u4e2d\u7684\u975e\u7f3a\u6578\u503c\uff0c\u9808\u6ce8\u610f\u4e0d\u8981\u7834\u58de\u8cc7\u6599\u5206\u4f48\n        * \u586b\u88dc\u7d71\u8a08\u503c :\n            * \u586b\u88dc\u5e73\u5747\u6578 (mean) : \u6578\u503c\u578b\u6b04\u4f4d\uff0c\u504f\u614b\u4e0d\u660e\u986f\n            * \u586b\u88dc\u4e2d\u4f4d\u6578 (meadian) : \u6578\u503c\u578b\u6b04\u4f4d\uff0c\u504f\u614b\u660e\u986f\n            * \u586b\u88dc\u91cd\u6578 (mode) : \u985e\u5225\u578b\u6b04\u4f4d\n        * \u586b\u88dc\u6307\u5b9a\u503c : \u9808\u5c0d\u6b04\u4f4d\u9818\u57df\u77e5\u8b58\u5df2\u6709\u4e86\u89e3\n            * \u88dc 0 : \u7a7a\u7f3a\u539f\u672c\u5c31\u6709\u96f6\u7684\u542b\u610f\n            * \u88dc\u4e0d\u53ef\u80fd\u51fa\u73fe\u7684\u6578\u503c : \u985e\u5225\u578b\u6b04\u4f4d\uff0c\u4f46\u4e0d\u9069\u5408\u7528\u773e\u6578\u6642\n            ```py\n            df.fillna(0)\n            df.fillna(-1)\n            df.fillna(df.mean())\n            ```\n        * \u586b\u88dc\u9810\u6e2c\u503c : \u901f\u5ea6\u6162\u4f46\u7cbe\u78ba\uff0c\u5f9e\u5176\u4ed6\u6b04\u4f4d\u5b78\u5f97\u586b\u88dc\u77e5\u8b58\n            * \u82e5\u586b\u88dc\u7bc4\u570d\u5ee3\uff0c\u4e14\u662f\u91cd\u8981\u7684\u7279\u5fb5\u6b04\u4f4d\u6642\u53ef\u7528\u672c\u65b9\u5f0f\n            * \u9808\u63d0\u9632 overfitting : \u53ef\u80fd\u9000\u5316\u6210\u5176\u4ed6\u7279\u5fb5\u7d44\u5408\n        * \u6a19\u6e96\u5316 : \u4ee5\u5408\u7406\u7684\u65b9\u5f0f\u5e73\u8861\u7279\u5fb5\u9593\u7684\u5f71\u97ff\u529b\n            * \u6a19\u6e96\u5316 (Standard Scaler) : \u5047\u8a2d\u6578\u503c\u70ba\u5e38\u614b\u5206\n            \u4f48\uff0c\u9069\u7528\u672c\u65b9\u5f0f\u5e73\u8861\u7279\u5fb5\uff0c\u4e0d\u6613\u53d7\u6975\u7aef\u503c\u5f71\u97ff\n            * \u6700\u5927\u6700\u5c0f\u5316 (MinMax Scaler) : \u5047\u8a2d\u6578\u503c\u70ba\u5747\u52fb\u5206\u5e03\uff0c\u9069\u7528\u672c\u65b9\u5f0f\u5e73\u8861\u7279\u5fb5\uff0c\u6613\u53d7\u6975\u7aef\u503c\u5f71\u97ff\n                ```py\n                from sklearn.preprocessing import MinMaxScaler, StandardScaler\n                df_temp = MinMaxScaler().fit_transform(df)\n                df_temp = StandardScaler().fit_transform(df)\n                ```\n* **Day_13 : \u5e38\u7528\u7684 DataFrame \u64cd\u4f5c**\n    * [Panda \u5b98\u65b9 Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n    * [Panda Cheet Sheet](https://assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf)\n    * \u8f49\u63db\u8207\u5408\u4f75\n        * \u5c07\"column\"\u8f49\u63db\u6210\"row\" \n            ```py\n            df.melt(id_vars=['A'], value_vars=['B', 'C'])\n            ```\n        * \u5c07\"row\"\u8f49\u63db\u6210\"column\"\n            ```py\n            df.pivot(index='A', columns='B', values='C')\n            ```\n        * \u6cbf\"row\"\u9032\u884c\u5408\u4f75\n            ```py\n            pd.concat([df1,df2])\n            ```\n        * \u6cbf\"column\"\u9032\u884c\u5408\u4f75\n            ```py\n            pd.concat([df1,df2],axis=1)\n        * \u4ee5\"id\"\u6b04\u4f4d\u505a\u5168\u5408\u4f75(\u907a\u5931\u4ee5na\u88dc)\n            ```py\n            pd.merge(df1,df2,on='id',how='outer')\n            ```\n        * \u4ee5'id'\u6b04\u4f4d\u505a\u90e8\u5206\u5408\u4f75\n            ```py\n            pd.merge(df1,df2,on='id',how='inner')\n            ```\n    * subset \u64cd\u4f5c\n        * \u908f\u8f2f\u64cd\u4f5c(>,<,=,&,|,~,^)\n            ```py\n            sub_df = df[df.age>20]\n            ```\n        * \u79fb\u9664\u91cd\u8907\u7684\"row\"\n            ```py\n            df = df.drop_duplicates()\n            ```\n        * \u524d 5 \u7b46\u3001\u5f8c n \u7b46\n            ```py\n            sub_df = df.head()   # default = 5\n            sub_df = df.tail(10)\n            ```\n        * \u96a8\u6a5f\u62bd\u6a23\n            ```\n            sub_df = df.sample(frac=0.5)    # \u62bd50%\n            sub_df = df.sample(n=10)    # \u62bd10\u7b46\n            ```\n        * \u7b2c n \u5230 m \u7b46\n            ```py\n            sub_df = df.iloc[n:m]\n            ```\n        * \u6b04\u4f4d\u4e2d\u5305\u542b value\n            ```py\n            df.column.isin(value)\n            ```\n        * \u5224\u65b7 Nan\n            ```py\n            pd.isnull(obj)  # df.isnull()\n            pd.notnull(obj) # df.notnull()\n            ```\n        * \u6b04\u4f4d\u7be9\u9078\n            ```py\n            new_df = df['col1'] # df.col1\n            new_df = df[['col1','col2','col3']]\n            df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n                        index=['mouse', 'rabbit'],\n                        columns=['one', 'two', 'three'])\n            new_df = df.filter(items=['one', 'three'])\n            new_df = df.filter(regex='e$', axis=1)\n            new_df = df.filter(like='bbi', axis=0)\n            ```\n    * Groupby \u64cd\u4f5c : \n        ```py\n        sub_df_obj = df.groupby(['col1'])\n        ```\n        * \u8a08\u7b97\u5404\u7d44\u7684\u6578\u91cf\n            ```py\n            sub_df_obj.size()\n            ```\n        * \u5f97\u5230\u5404\u7d44\u7684\u7d71\u8a08\u503c\n            ```py\n            sub_df_obj.describe()\n            ```\n        * \u6839\u64da col1 \u5206\u7d44\u5f8c\u8a08\u7b97 col2 \u7684\u7d71\u8a08\u503c\n            ```py\n            sub_df_obj['col2'].mean()\n            ```\n        * \u6839\u64da col1 \u5206\u7d44\u5f8c\u7684 col2 \u5f15\u7528\u64cd\u4f5c\n            ```py\n            sub_df_obj['col2'].apply(...)\n            ``` \n        * \u6839\u64da col1 \u5206\u7d44\u5f8c\u7684 col2 \u7e6a\u5716\n            ```py\n            sub_df_obj['col2'].hist()\n            ```\n* **Day_14 : \u76f8\u95dc\u4fc2\u6578\u7c21\u4ecb**\n    * \u60f3\u8981\uf9ba\u89e3\uf978\u500b\u8b8a\u6578\u4e4b\u9593\u7684**\u7dda\u6027\u95dc\u4fc2**\u6642\uff0c\u76f8\u95dc\u4fc2\u6578\u662f\u4e00\u500b\u9084\u4e0d\u932f\u7684\u7c21\u55ae\u65b9\u6cd5\uff0c\u80fd\u7d66\u51fa\u4e00\u500b -1~1 \u4e4b\u9593\u7684\u503c\uf92d\u8861\u91cf\u5316\uf978\u500b\u8b8a\u6578\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\n    * Correlation Coefficient :\n        $$r=\\frac{1}{n-1} \\sum_{i=1}^n\\frac{(x_i-\\bar{x})}{s_x}\\frac{(y_i-\\bar{y})}{s_y}$$\n    * [\u76f8\u95dc\u4fc2\u6578\u5c0f\u904a\u6232](http://guessthecorrelation.com/)\n* **Day_15 : \u76f8\u95dc\u4fc2\u6578\u5be6\u4f5c**\n    ```py\n    df = pd.DataFrame(np.concatenate([np.random.randn(20).reshape(-1,1),np.random.randint(0,2,20).reshape(-1,1)],axis=1), columns=[\"X\",\"y\"])\n    df.corr()\n    np.corrcoef(df)\n    ```    \n    * \u901a\u5e38\u642d\u914d\u7e6a\u5716\u9032\u4e00\u6b65\u4e86\u89e3\u76ee\u6a19\u8207\u8b8a\u6578\u7684\u95dc\u4fc2\n        ```py\n        df.plot(x='X',y='y',kind='scatter')\n        df.boxplot(column=['X'], by=['y'])\n        ```\n* **Day_16 : \u7e6a\u5716\u8207\u6a23\u5f0f \uff06 Kernel Density Estimation (KDE)**\n    * \u7e6a\u5716\u98a8\u683c : \u7528\u5df2\u7d93\u88ab\u8a2d\u8a08\u904e\u7684\u98a8\u683c\uff0c\u8b93\u89c0\u770b\u8005\u66f4\u6e05\u695a\u660e\u77ad\uff0c\u5305\u542b\u8272\u5f69\u9078\u64c7\u3001\u7dda\u689d\u3001\u6a23\u5f0f\u7b49\u3002\n        ```py\n        plt.style.use('default') # \u4e0d\u9700\u8a2d\u5b9a\u5c31\u6703\u4f7f\u7528\u9810\u8a2d\n        plt.style.use('ggplot') \n        plt.style.use('seaborn') # \u63a1\u2f64 seaborn \u5957\u4ef6\u7e6a\u5716\n        ```\n    * KDE\n        * \u63a1\u7528\u7121\u6bcd\u6578\u65b9\u6cd5\u5283\u51fa\u89c0\u5bdf\u8b8a\u6578\u7684\u6a5f\u7387\u5bc6\u5ea6\u51fd\u6578\n        * Density Plot \u7279\u6027 :\n            * \u6b78\u4e00 : \u7dda\u4e0b\u9762\u7a4d\u548c\u70ba1\n        * \u5e38\u7528\u7684 kernal function :\n            * Gaussian(Normal dist)\n            * Cosine\n        * \u512a\u9ede : \u7121\u6bcd\u6578\u65b9\u6cd5\uff0c\u5c0d\u5206\u5e03\u6c92\u6709\u5047\u8a2d\n        * \u7f3a\u9ede : \u8a08\u7b97\u91cf\u5927\n        * \u900f\u904e KDE Plot \u53ef\u4ee5\u6e05\u695a\u770b\u51fa\u4e0d\u540c\u7d44\u9593\u7684\u5206\u5e03\u60c5\u5f62\n        ```py\n        import matplotlib.pyplot as plt\n        import seaborn as sns   \n\n        # \u5c07\u6b04\u4f4d\u5206\u6210\u591a\u5206\u9032\u884c\u7d71\u8a08\u7e6a\u5716\n        plt.hist(df[col], edgecolor = 'k', bins = 25)\n\n        # KDE, \u6bd4\u8f03\u4e0d\u540c\u7684 kernel function\n        sns.kdeplot(df[col], label = 'Gaussian esti.', kernel='gau')\n        sns.kdeplot(adf[col], label = 'Cosine esti.', kernel='cos')\n        sns.kdeplot(df[col], label = 'Triangular esti.', kernel='tri')\n\n        # \u5b8c\u6574\u5206\u5e03\u5716 (distplot) : \u5c07 bar \u8207 Kde \u540c\u6642\u5448\u73fe\n        sns.distplot(df[col])\n\n        # \u7e6a\u88fd barplot \u4e26\u986f\u793a\u76ee\u6a19\u7684 variables \n        sns.barplot(x=\"BIRTH_RANGE\", y=\"TARGET\", data=df)\n        plt.xticks(rotation=45) # \u65cb\u8f49\u523b\u5ea6\u6a19\u7c64\n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Python Graph Gallery](https://python-graph-gallery.com/)\n        * [R Graph Gallery](https://www.r-graph-gallery.com/)\n        * [Interactive plot\uff0c\u4e92\u52d5\u5716](https://bl.ocks.org/mbostock)\n* **Day_17 : \u628a\u9023\u7e8c\u578b\u8b8a\u6578\u96e2\u6563\u5316**\n    * [\u96e2\u6563\u5316\u76ee\u7684](https://www.zhihu.com/question/31989952) :\n        * \u8b93\u4e8b\u60c5\u8b8a\u7c21\u55ae\uff0c\u589e\u52a0\u904b\u7b97\u901f\u5ea6\n        * \u6e1b\u5c11 outlier \u5c0d\u6a21\u578b\u7684\u5f71\u97ff\n        * \u5f15\u5165\u975e\u7dda\u6027\uff0c\u63d0\u5347\u6a21\u578b\u8868\u9054\u80fd\u529b\n        * \u63d0\u5347\u9b6f\u68d2\u6027\uff0c\u6e1b\u5c11\u904e\u4f3c\u5408\n    * \u4e3b\u8981\u65b9\u6cd5 :\n        * \u7b49\u5bec\u5283\u5206 `pd.cut()`\uff0c\u53ef\u4f7f\u7528 `np.linspace()` \u9032\u884c\u7b49\u8ddd\u5207\u5206\n        * \u7b49\u983b\u5283\u5206 `pd.qcut()`\n        * \u805a\u985e\u5283\u5206\n* **Day_18 : \u628a\u9023\u7e8c\u578b\u8b8a\u6578\u96e2\u6563\u5316\u5be6\u4f5c**\n    * \u628a\u9023\u7e8c\u578b\u7684\u7279\u5fb5\u96e2\u6563\u5316\u5f8c\uff0c\u53ef\u4ee5\u914d\u5408 `groupby` \u5283\u51fa\u8207\u9810\u6e2c\u76ee\u6a19\u7684\u5716\uff0c\u4f86\u5224\u65b7\u5169\u8005\u4e4b\u9593\u662f\u5426\u6709\u67d0\u4e9b\u95dc\u4fc2\u548c\u8da8\u52e2\u3002\n        ```py\n        # \u5c07\u5e74\u9f61\u76f8\u95dc\u8cc7\u6599, \u53e6\u5916\u5b58\u6210\u4e00\u500b DataFrame \u4f86\u8655\u7406\n        age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n        age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n        # \u5c07\u5e74\u9f61\u8cc7\u6599\u96e2\u6563\u5316 / \u5206\u7d44\n        age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n\n        age_groups  = age_data.groupby('YEARS_BINNED').mean()\n\n        plt.figure(figsize = (8, 8))\n\n        # \u7e6a\u88fd\u76ee\u6a19\u503c\u5e73\u5747\u8207\u5206\u7d44\u7d44\u5225\u7684\u9577\u689d\u5716\n        plt.bar(range(len(age_groups.index)), age_groups['TARGET'])\n        # \u52a0\u4e0a X, y \u5ea7\u6a19\u8aaa\u660e, \u4ee5\u53ca\u5716\u8868\u7684\u6a19\u984c\n        plt.xticks(range(len(age_groups.index)), age_groups.index, rotation = 75)\n        plt.xlabel('Age Group (years)')\n        plt.ylabel('Average Failure to Repay')\n        plt.title('Failure to Repay by Age Group')\n        ```\n* **Day_19 : Subplot**\n    * \u4f7f\u7528\u6642\u6a5f :\n        * \u6709\u5f88\u591a\u76f8\u4f3c\u7684\u8cc7\u6599\u8981\u5448\u73fe\u6642 (\u5982\u4e0d\u540c\u7d44\u5225)\n        * \u540c\u4e00\u7d44\u8cc7\u6599\uff0c\u60f3\u8981\u540c\u6642\u7528\u4e0d\u540c\u7684\u5716\u5448\u73fe\n        * \u9069\u6642\u7684\u4f7f\u7528\u6709\u52a9\u65bc\u8cc7\u8a0a\u50b3\u9054\uff0c\u4f46\u904e\u5ea6\u4f7f\u7528\u6703\u8b93\u91cd\u9ede\u6df7\u6dc6\n    * subplot \u5750\u6a19\u7cfb (\u5217-\u6b04-\u4f4d\u7f6e)\n        * (321) \u4ee3\u8868\u5728\u2f00\u500b **3\uf99c2\u6b04** \u7684\u6700\u5de6\u4e0a\u2f93 **\uf99c1\uf91d1**\n        * (232) \u4ee3\u8868\u5728\u4e00\u500b **2\u52173\u6b04** \u7684 **\uf99c1\uf91d2** \u4f4d\u7f6e\n        ```py\n        # \u65b9\u6cd5\u4e00 : \u6578\u91cf\u5c11\u7684\u6642\u5019\u6216\u7e6a\u5716\u65b9\u6cd5\u4e0d\u540c\u6642\n        plt.figure(figsize=(8,8))\n        plt.subplot(321)\n        plt.plot([0,1],[0,1], label = 'I am subplot1')\n        plt.legend()\n        plt.subplot(322)\n        plt.plot([0,1],[1,0], label = 'I am subplot2')\n        plt.legend()\n\n        # \u65b9\u6cd5\u4e8c : \u6578\u91cf\u591a\u7684\u6642\u5019\u6216\u7e6a\u5716\u65b9\u6cd5\u96f7\u540c\u6642\n        nrows = 5\n        ncols = 2\n        plt.figure(figsize=(10,30))\n        for i in range(nrows*ncols):\n            plt.subplot(nrows, ncols, i+1)\n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [matplotlib \u5b98\u2f45\u65b9\u7bc4\u4f8b\uf9b5](https://matplotlib.org/examples/pylab_examples/subplots_demo.html)\n        * [\u8907\u96dc\u7248 subplot \u5beb\u6cd5](https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html)\n        * [\u53e6\u985e\u2f26\u5b50\u5716 Seaborn.jointplot](https://seaborn.pydata.org/generated/seaborn.jointplot.html)\n* **Day_20 : Heatmap & Grid-plot**\n    * `heatmap`\n        * \u5e38\u7528\u65bc\u5448\u73fe\u8b8a\u6578\u9593\u7684\u76f8\u95dc\u6027\n        * \u4e5f\u53ef\u4ee5\u7528\u65bc\u5448\u73fe\u4e0d\u540c\u689d\u4ef6\u4e0b\u7684\u6578\u91cf\u95dc\u4fc2\n        * \u5e38\u7528\u65bc\u5448\u73fe**\u6df7\u6dc6\u77e9\u9663 (Confusion matrix)**\n        ```py\n        plt.figure(figsize = (8, 6))\n        # \u7e6a\u88fd\u76f8\u95dc\u4fc2\u6578 (correlations) \u7684 Heatmap\n        sns.heatmap(df.corr(), cmap = plt.cm.RdYlBu_r, vmin = -1.0, annot = True, vmax = 1.0)\n        plt.title('Correlation Heatmap')\n        ```\n    * `pairplot`\n        * \u5c0d\u89d2\u7dda : \u8a72\u8b8a\u6578\u7684\u5206\u5e03(distribution)\n        * \u975e\u5c0d\u89d2\u7dda : \u5006\u5006\u8b8a\u6578\u9593\u7684\u6563\u4f48\u5716\n        ```py\n        import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n        iris = sns.load_dataset(\"iris\")\n        g = sns.pairplot(iris)\n        ```\n    * `PairGrid`\n        * \u53ef\u4ee5\u81ea\u8a02\u5c0d\u89d2\u7dda\u548c\u975e\u5c0d\u89d2\u7dda\u7684\u7e6a\u5716\u985e\u578b\n        ```py\n        g = sns.PairGrid(iris, hue=\"species\")\n        g = g.map_diag(plt.hist)    # \u5c0d\u89d2\u7dda\u7e6a\u5716\u985e\u578b\n        g = g.map_offdiag(plt.scatter)  # \u975e\u5c0d\u89d2\u7dda\u7e6a\u5716\u985e\u578b\n        g = g.add_legend()\n\n        g = sns.PairGrid(iris)\n        g = g.map_upper(sns.scatterplot)    # \u4e0a\u4e09\u89d2\u7e6a\u5716\u985e\u578b\n        g = g.map_lower(sns.kdeplot, colors=\"C0\")   # \u4e0b\u4e09\u89d2\u7e6a\u5716\u985e\u578b\n        g = g.map_diag(sns.kdeplot, lw=2)\n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u57fa\u672c Heatmap](https://matplotlib.org/gallery/images_contours_and_fields/image_annotated_heatmap.html)\n        * [\u9032\u968e Heatmap](https://www.jianshu.com/p/363bbf6ec335)\n        * [pairplot \u66f4\u591a\u61c9\u7528](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166)\n* **Day_21 : \u6a21\u578b\u521d\u9ad4\u9a57 - Logistic Regression**\n    * [Logistic Regression](https://www.youtube.com/playlist?list=PLNeKWBMsAzboR8vvhnlanxCNr2V7ITuxy)\n        ```py\n        from sklearn.linear_model import LogisticRegression\n\n        # \u8a2d\u5b9a\u6a21\u578b\u8207\u6a21\u578b\u53c3\u6578\n        log_reg = LogisticRegression(C = 0.0001)\n\n        # \u4f7f\u7528 Train \u8cc7\u6599\u8a13\u7df4\u6a21\u578b\n        log_reg.fit(train, train_labels)\n\n        # \u7528\u6a21\u578b\u9810\u6e2c\u7d50\u679c\n        # \u8acb\u6ce8\u610f\u7f85\u5409\u65af\u8ff4\u6b78\u662f\u5206\u985e\u9810\u6e2c (\u6703\u8f38\u51fa 0 \u7684\u6a5f\u7387, \u8207 1 \u7684\u6a5f\u7387), \u800c\u6211\u5011\u53ea\u9700\u8981\u7559\u4e0b 1 \u7684\u6a5f\u7387\u9019\u6392\n        log_reg_pred = log_reg.predict_proba(test)[:, 1]\n\n        # \u5c07 DataFrame/Series \u8f49\u6210 CSV \u6a94\u6848\u65b9\u6cd5\n        df['Target'] = log_reg_pred\n        df.to_csv(file_name, encoding='utf-8', index=False)\n        ```\n### \u8cc7\u6599\u79d1\u5b78\u8207\u7279\u5fb5\u5de5\u7a0b\u6280\u8853 Data Science & Feature Engineering\n* **Day_22 : \u7279\u5fb5\u5de5\u7a0b\u7c21\u4ecb**\n    * \u8cc7\u6599\u5de5\u7a0b\u662f**\u5c07\u4e8b\u5be6\u5c0d\u61c9\u5230\u5206\u6578\u7684\u8f49\u63db**\n    * \u7531\u65bc\u8cc7\u6599\u5305\u542b\u985e\u5225\u7279\u5fb5 (\u6587\u5b57) \u548c\u6578\u503c\u7279\u5fb5\uff0c\u6240\u4ee5\u6700\u5c0f\u7684\u7279\u5fb5\u5de5\u7a0b\u81f3\u5c11\u5305\u542b\u4e00\u7a2e**\u985e\u5225\u7de8\u78bc** (\u4f8b\u5982:\u6a19\u7c64\u7de8\u78bc)\u548c**\u7279\u5fb5\u7e2e\u653e**\u65b9\u6cd5 (\u4f8b\u5982:\u6700\u5c0f\u6700\u5927\u5316)\n        ```py\n        from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n        LEncoder = LabelEncoder()\n        MMEncoder = MinMaxScaler()\n        for c in df.columns:\n            df[c] = df[c].fillna(-1)\n            if df[c].dtype == 'object':\n                df[c] = LEncoder.fit_transform(list(df[c].values))\n            df[c] = MMEncoder.fit_transform(df[c].values.reshape(-1, 1))\n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [\u7279\u5fb5\u5de5\u7a0b\u662f\u4ec0\u9ebc](https://www.zhihu.com/question/29316149)\n* **Day_23 : \u6578\u503c\u578b\u7279\u5fb5 - \u53bb\u9664\u504f\u614b**\n    * \u7576**\u96e2\u7fa4\u503c\u8cc7\u6599\u6bd4\u4f8b\u592a\u9ad8**\uff0c\u6216\u8005**\u5e73\u5747\u503c\u6c92\u6709\u4ee3\u8868\u6027**\u6642\uff0c\u53ef\u4ee5\u8003\u616e\u53bb\u9664\u504f\u614b\n    * \u53bb\u9664\u504f\u614b\u5305\u542b : \u5c0d\u6578\u53bb\u504f (log1p)\u3001\u65b9\u6839\u53bb\u504f (sqrt)\u3001\u5206\u5e03\u53bb\u504f (boxcox)\n    * \u4f7f\u7528 box-cox \u5206\u5e03\u53bb\u504f\u6642\uff0c\u9664\u4e86\u6ce8\u610f $\\lambda$ \u53c3\u6578\u8981\u754c\u65bc 0 \u5230 0.5 \u4e4b\u9593\uff0c\u4e26\u4e14\u8981\u6ce8\u610f\u8f49\u63db\u524d\u7684\u6578\u503c\u4e0d\u53ef\u5c0f\u65bc\u7b49\u65bc 0\n        ```py\n        # \u5c0d\u6578\u53bb\u504f\n        df_fixed['Fare'] = np.log1p(df_fixed['Fare'])\n\n        # \u65b9\u6839\u53bb\u504f\n        df['score'] = np.sqrt(df['score']) * 10\n\n        from scipy import stats\n        # \u4fee\u6b63\u65b9\u5f0f : \u52a0\u5165\u4e0b\u9762\u9019\u4e00\u884c, \u4f7f\u6700\u5c0f\u503c\u5927\u65bc 0, \u985e\u4f3clog1p\u7684\u6982\u5ff5\n        df_fixed['Fare'] = df_fixed['Fare'] + 1\n        df_fixed['Fare'] = stats.boxcox(df_fixed['Fare'])[0]\n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [\u504f\u5ea6\u8207\u5cf0\u5ea6](https://blog.csdn.net/u013555719/article/details/78530879)\n* **Day_24 : \u985e\u5225\u578b\u7279\u5fb5 - \u57fa\u790e\u8655\u7406**\n    * \u985e\u5225\u578b\u7279\u5fb5\u6709**\u6a19\u7c64\u7de8\u78bc (Label Encoding)** \u8207**\u7368\u71b1\u7de8\u78bc (One Hot Encoding)** \u5169\u7a2e\u57fa\u790e\u7de8\u78bc\u65b9\u5f0f\n    * \u6a19\u7c64\u7de8\u78bc\u5c07\u7279\u5fb5\u4f9d\u5e8f\u8f49\u70ba\u4ee3\u78bc\uff0c\u82e5\u7279\u5fb5\u6c92\u6709\u5927\u5c0f\u9806\u5e8f\u4e4b\u5225\uff0c\u5247\u5927\u5c0f\u9806\u5e8f\u6c92\u6709\u610f\u7fa9\uff0c\u5e38\u7528\u65bc\u975e\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\uff0c\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u4e3b\u8981\u4f9d\u8cf4\u5012\u50b3\u5c0e\uff0c\u6a19\u7c64\u7de8\u78bc\u4e0d\u6613\u6536\u6582\n    * \u7576\u7279\u5fb5\u91cd\u8981\u6027\u9ad8\u4e14\u53ef\u80fd\u503c\u5c11\u6642\uff0c\u53ef\u8003\u616e\u7368\u71b1\u7de8\u78bc\n        ```py\n        from sklearn.preprocessing import LabelEncoder\n        \n        object_features = []\n        for dtype, feature in zip(df.dtypes, df.columns):\n            if dtype == 'object':\n                object_features.append(feature)\n\n        df = df[object_features]\n        df = df.fillna('None')\n        # \u6a19\u7c64\u7de8\u78bc\n        for c in df.columns:\n            df_temp[c] = LabelEncoder().fit_transform(df[c])\n        # \u7368\u71b1\u7de8\u78bc\n        df_temp = pd.get_dummies(df)\n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [\u6a19\u7c64\u7de8\u78bc\u8207\u7368\u71b1\u7de8\u78bc](https://blog.csdn.net/u013555719/article/details/78530879)\n* **Day_25 : \u985e\u5225\u578b\u7279\u5fb5 - \u5747\u503c\u7de8\u78bc**\n    * **\u5747\u503c\u7de8\u78bc (Mean Encoding)** : \u4f7f\u7528\u76ee\u6a19\u503c\u7684\u5e73\u5747\u503c\u53d6\u4ee3\u539f\u672c\u985e\u5225\u578b\u7279\u5fb5\n    * \u7576\u985e\u5225\u7279\u5fb5\u8207\u76ee\u6a19\u660e\u986f\u76f8\u95dc\u6642\uff0c\u8a72\u8003\u616e\u63a1\u7528\u5747\u503c\u7de8\u78bc\n    * \u6a23\u672c\u6578\u5c11\u6642\u53ef\u80fd\u662f\u6975\u7aef\u503c\uff0c\u5e73\u5747\u7d50\u679c\u53ef\u80fd\u8aa4\u5dee\u5f88\u5927\uff0c\u9700\u4f7f\u7528\u5e73\u6ed1\u516c\u5f0f\u4f86\u8abf\u6574\n        * \u7576\u5e73\u5747\u503c\u53ef\u9760\u5ea6\u4f4e\u5247\u50be\u5411\u76f8\u4fe1\u7e3d\u5e73\u5747\n        * \u7576\u5e73\u5747\u503c\u53ef\u9760\u6027\u9ad8\u5247\u50be\u5411\u76f8\u4fe1\u985e\u5225\u7684\u5e73\u5747\n        * \u4f9d\u7167\u7d00\u9304\u7684\u6bd4\u6578\uff0c\u5728\u5169\u8005\u9593\u53d6\u6298\u8877 : \n        $$\u65b0\u985e\u5225\u5747\u503c = \\frac{\u539f\u985e\u5225\u5e73\u5747*\u985e\u5225\u6a23\u672c\u6578 + \u5168\u90e8\u7684\u7e3d\u5e73\u5747*\u8abf\u6574\u56e0\u5b50}{\u985e\u5225\u6a23\u672c\u6578 + \u8abf\u6574\u56e0\u5b50}$$\n    * \u76f8\u7576\u5bb9\u6613 overfitting \u8acb\u5c0f\u5fc3\u4f7f\u7528\n        ```py\n        data = pd.concat([df[:train_num], train_Y], axis=1)\n        for c in df.columns:\n            mean_df = data.groupby([c])['target'].mean().reset_index()\n            mean_df.columns = [c, f'{c}_mean']\n            data = pd.merge(data, mean_df, on=c, how='left')\n            data = data.drop([c] , axis=1)\n        data = data.drop(['target'] , axis=1)\n        ```\n* **Day_26 : \u985e\u5225\u578b\u7279\u5fb5 - \u5176\u4ed6\u9032\u968e\u8655\u7406**\n    * **\u8a08\u6578\u7de8\u78bc (Counting)** : \u8a08\u7b97\u985e\u5225\u5728\u8cc7\u6599\u4e2d\u51fa\u73fe\u6b21\u6578\uff0c\u7576\u76ee\u524d\u5e73\u5747\u503c\u8207\u985e\u5225\u7b46\u6578\u5448\u73fe\u6b63/\u8ca0\u76f8\u95dc\u6642\uff0c\u53ef\u4ee5\u8003\u616e\u4f7f\u7528\n        ```py\n        count_df = df.groupby(['Ticket'])['Name'].agg({'Ticket_Count':'size'}).reset_index()\n        df = pd.merge(df, count_df, on=['Ticket'], how='left')\n        ```\n    * **\u96dc\u6e4a\u7de8\u78bc** : \u5c07\u985e\u5225\u7531\u96dc\u6e4a\u51fd\u6578\u5c0d\u61c9\u5230\u4e00\u7d44\u6578\u5b57\n        * \u8abf\u6574\u96dc\u6e4a\u51fd\u6578\u5c0d\u61c9\u503c\u7684\u6578\u91cf\uff0c\u5728\u8a08\u7b97\u7a7a\u9593/\u6642\u9593\u8207\u9451\u5225\u5ea6\u9593\u53d6\u6298\u8877\n        * \u63d0\u9ad8\u8a0a\u606f\u5bc6\u5ea6\u4e26\u6e1b\u5c11\u7121\u7528\u7684\u6a19\u7c64\n        ```py\n        df_temp['Ticket_Hash'] = df['Ticket'].map(lambda x:hash(x) % 10)\n        ```\n    * \u96dc\u6e4a\u7de8\u78bc\u4e5f\u4e0d\u4f73\u6642\u53ef\u4f7f\u7528**\u5d4c\u5165\u5f0f\u7de8\u78bc (Embedding)**\uff0c\u4f46\u9700\u8981\u57fa\u65bc\u6df1\u5ea6\u5b78\u7fd2\u524d\u63d0\u4e0b\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u7279\u5fb5\u54c8\u5e0c](https://blog.csdn.net/laolu1573/article/details/79410187)\n        * [\u6587\u672c\u7279\u5fb5\u62bd\u53d6](https://www.jianshu.com/p/063840752151)\n* **Day_27 : \u6642\u9593\u578b\u7279\u5fb5**\n    * \u6642\u9593\u578b\u7279\u5fb5\u5206\u89e3 :\n        * \u4f9d\u7167\u539f\u610f\u7fa9\u5206\u6b04\u8655\u7406\uff0c\u5e74\u3001\u6708\u3001\u65e5\u3001\u6642\u3001\u5206\u3001\u79d2\u6216\u52a0\u4e0a\u7b2c\u5e7e\u5468\u548c\u661f\u671f\u5e7e\n        * \u9031\u671f\u5faa\u74b0\u7279\u5fb5\n            * \u5e74\u9031\u671f : \u8207\u5b63\u7bc0\u6eab\u5ea6\u76f8\u95dc\n            * \u6708\u9031\u671f : \u8207\u85aa\u6c34\u3001\u7e73\u8cbb\u76f8\u95dc\n            * \u5468\u9031\u671f : \u8207\u5468\u4f11\u3001\u6d88\u8cbb\u7fd2\u6163\u76f8\u95dc\n            * \u65e5\u9031\u671f : \u8207\u751f\u7406\u6642\u9418\u76f8\u95dc\n        * \u9031\u671f\u6578\u503c\u9664\u4e86\u7531\u6b04\u4f4d\u7d44\u6210\u9084\u9700**\u982d\u5c3e\u76f8\u63a5**\uff0c\u56e0\u6b64\u4e00\u822c\u4ee5**\u6b63(\u9918)\u5f26\u51fd\u6578**\u52a0\u4ee5\u7d44\u5408\n            * \u5e74\u9031\u671f : ( \u6b63 : \u51b7 / \u8ca0 : \u71b1 )\n                $cos((\u6708/6 + \u65e5/180)\\pi)$\n            * \u5468\u9031\u671f : ( \u6b63 : \u7cbe\u795e\u98fd\u6eff / \u8ca0 : \u75b2\u5026 )\n                $sin((\u661f\u671f\u5e7e/3.5 + \u5c0f\u6642/84)\\pi)$\n            * \u65e5\u9031\u671f : ( \u6b63 : \u7cbe\u795e\u98fd\u6eff / \u8ca0 : \u75b2\u5026 )\n                $sin((\u5c0f\u6642/12 + \u5206/720 + \u79d2/43200)\\pi)$\n            * \u9808\u6ce8\u610f\u6700\u9ad8\u9ede\u8207\u6700\u4f4e\u9ede\u7684\u8a2d\u7f6e\n        ```py\n        import datetime\n\n        # \u6642\u9593\u7279\u5fb5\u5206\u89e3\u65b9\u5f0f:\u4f7f\u7528datetime\n        df['pickup_datetime'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S UTC'))\n        df['pickup_year'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%Y')).astype('int64')\n        df['pickup_month'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%m')).astype('int64')\n        df['pickup_day'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%d')).astype('int64')\n        df['pickup_hour'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%H')).astype('int64')\n        df['pickup_minute'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%M')).astype('int64')\n        df['pickup_second'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%S')).astype('int64')\n\n        # \u52a0\u5165\u661f\u671f\u5e7e(day of week)\u548c\u7b2c\u5e7e\u5468(week of year)\n        df['pickup_dow'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%w')).astype('int64')\n        df['pickup_woy'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%W')).astype('int64')\n\n        # \u52a0\u4e0a\"\u65e5\u9031\u671f\"\u7279\u5fb5 (\u53c3\u8003\u8b1b\u7fa9\"\u9031\u671f\u5faa\u74b0\u7279\u5fb5\")\n        import math\n        df['day_cycle'] = df['pickup_hour']/12 + df['pickup_minute']/720 + df['pickup_second']/43200\n        df['day_cycle'] = df['day_cycle'].map(lambda x:math.sin(x*math.pi))\n\n        # \u52a0\u4e0a\"\u5e74\u9031\u671f\"\u8207\"\u5468\u9031\u671f\"\u7279\u5fb5\n        df['year_cycle'] = df['pickup_month']/6 + df['pickup_day']/180\n        df['year_cycle'] = df['year_cycle'].map(lambda x:math.cos(x*math.pi))\n        df['week_cycle'] = df['pickup_dow']/3.5 + df['pickup_hour']/84\n        df['week_cycle'] = df['week_cycle'].map(lambda x:math.sin(x*math.pi))\n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u6642\u9593\u65e5\u671f\u8655\u7406](http://www.wklken.me/posts/2015/03/03/python-base-datetime.html)\n        * [datetime](https://docs.python.org/3/library/datetime.html)\n* **Day_28 : \u7279\u5fb5\u7d44\u5408 - \u6578\u503c\u8207\u6578\u503c\u7d44\u5408**\n    * \u9664\u4e86\u57fa\u672c\u7684\u52a0\u6e1b\u4e58\u9664\uff0c\u6700\u95dc\u9375\u7684\u662f**\u9818\u57df\u77e5\u8b58**\uff0c\u4f8b\u5982\u5c07\u7d93\u7def\u5ea6\u8cc7\u6599\u7d44\u5408\u6210\u9ad8\u65af\u8ddd\u96e2\n        ```py\n        # \u589e\u52a0\u7def\u5ea6\u5dee, \u7d93\u5ea6\u5dee, \u5ea7\u6a19\u8ddd\u96e2\u7b49\u4e09\u500b\u7279\u5fb5\n        df['longitude_diff'] = df['dropoff_longitude'] - df['pickup_longitude']\n        df['latitude_diff'] = df['dropoff_latitude'] - df['pickup_latitude']\n        df['distance_2D'] = (df['longitude_diff']**2 + df['latitude_diff']**2)**0.5\n\n        import math\n        latitude_average = df['pickup_latitude'].mean()\n        latitude_factor = math.cos(latitude_average/180*math.pi)\n        df['distance_real'] = ((df['longitude_diff']*latitude_factor)**2 + df['latitude_diff']**2)**0.5\n        ```\n    * **\u6a5f\u5668\u5b78\u7fd2\u7684\u95dc\u9375\u662f\u7279\u5fb5\u5de5\u7a0b**\uff0c\u80fd\u6709\u6548\u5730\u63d0\u5347\u6a21\u578b\u9810\u6e2c\u80fd\u529b            \n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [\u7279\u5fb5\u4ea4\u53c9](https://segmentfault.com/a/1190000014799038)\n* **Day_29 : \u7279\u5fb5\u7d44\u5408 - \u985e\u5225\u8207\u6578\u503c\u7d44\u5408**\n    * **\u7fa4\u805a\u7de8\u78bc (Group by Encoding)** : \u985e\u5225\u7279\u5fb5\u8207\u6578\u503c\u7279\u5fb5\u53ef\u4ee5\u4f7f\u7528\u7fa4\u805a\u7de8\u78bc\u7d44\u5408\u51fa\u65b0\u7684\u7279\u5fb5\n        * \u5e38\u898b\u7684\u7d44\u5408\u65b9\u5f0f\u6709 `mean`,`mdian`,`mode`,`max`,`min`,`count`\n        * \u8207\u5747\u503c\u7de8\u78bc (Mean Encoding) \u7684\u6bd4\u8f03\n            | \u540d\u7a31                  | \u5747\u503c\u7de8\u78bc Encoding | \u7fa4\u805a\u7de8\u78bc Group by Encoding |\n            |-----------------------|------------------|---------------------------|\n            | \u5e73\u5747\u5c0d\u8c61                | \u76ee\u6a19\u503c           | \u5176\u4ed6\u6578\u503c\u578b\u7279\u5fb5                |\n            | \u904e\u64ec\u5408 (Overfitting)  | \u5bb9\u6613            | \u4e0d\u5bb9\u6613                    |\n            | \u5c0d\u5747\u503c\u5e73\u6ed1\u5316 (Smoothing) | \u9700\u8981            | \u4e0d\u9700\u8981                    |\n        * \u6a5f\u5668\u5b78\u7fd2\u7684\u7279\u5fb5\u662f **\u5be7\u721b\u52ff\u7f3a** \u7684\uff0c\u4ee5\u524d\u975e\u6a39\u72c0\u6a21\u578b\u70ba\u4e86\u907f\u514d\u5171\u7dda\u6027\uff0c\u6703\u5e0c\u671b\u985e\u4f3c\u7279\u5fb5\u4e0d\u8981\u592a\u591a\uff0c\u4f46\u73fe\u5728\u5f37\u529b\u6a21\u578b\u5927\u591a\u662f\u6a39\u72c0\u6a21\u578b\uff0c\u6240\u4ee5\u901a\u901a\u505a\u6210~~\u96de\u7cbe~~\u7279\u5fb5\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [\u6578\u64da\u805a\u5408\u8207\u5206\u7d44](https://zhuanlan.zhihu.com/p/27590154)\n        ```py\n        # \u53d6\u8239\u7968\u7968\u865f(Ticket), \u5c0d\u4e58\u5ba2\u5e74\u9f61(Age)\u505a\u7fa4\u805a\u7de8\u78bc\n        df['Ticket'] = df['Ticket'].fillna('None')\n        df['Age'] = df['Age'].fillna(df['Age'].mean())\n\n        mean_df = df.groupby(['Ticket'])['Age'].mean().reset_index()\n        mode_df = df.groupby(['Ticket'])['Age'].apply(lambda x: x.mode()[0]).reset_index()\n        median_df = df.groupby(['Ticket'])['Age'].median().reset_index()\n        max_df = df.groupby(['Ticket'])['Age'].max().reset_index()\n        min_df = df.groupby(['Ticket'])['Age'].min().reset_index()\n        temp = pd.merge(mean_df, mode_df, how='left', on=['Ticket'])\n        temp = pd.merge(temp, median_df, how='left', on=['Ticket'])\n        temp = pd.merge(temp, max_df, how='left', on=['Ticket'])\n        temp = pd.merge(temp, min_df, how='left', on=['Ticket'])\n        temp.columns = ['Ticket', 'Age_Mean', 'Age_Mode', 'Age_Median', 'Age_Max', 'Age_Min']\n        temp.head()\n        ```\n        | Index | Ticket | Age\\_Mean  | Age\\_Mode  | Age\\_Median | Age\\_Max | Age\\_Min   |\n        |-------|--------|------------|------------|-------------|----------|------------|\n        | 0     | 110152 | 26\\.333333 | 16\\.000000 | 30\\.000000  | 33\\.0    | 16\\.000000 |\n        | 1     | 110413 | 36\\.333333 | 18\\.000000 | 39\\.000000  | 52\\.0    | 18\\.000000 |\n        | 2     | 110465 | 38\\.349559 | 29\\.699118 | 38\\.349559  | 47\\.0    | 29\\.699118 |\n        | 3     | 110564 | 28\\.000000 | 28\\.000000 | 28\\.000000  | 28\\.0    | 28\\.000000 |\n        | 4     | 110813 | 60\\.000000 | 60\\.000000 | 60\\.000000  | 60\\.0    | 60\\.000000 |\n* **Day_30 : \u7279\u5fb5\u9078\u64c7**\n    * \u7279\u5fb5\u9700\u8981\u9069\u7576\u7684\u589e\u52a0\u8207\u6e1b\u5c11\uff0c\u4ee5\u63d0\u5347\u7cbe\u78ba\u5ea6\u4e26\u6e1b\u5c11\u8a08\u7b97\u6642\u9593\n        * \u589e\u52a0\u7279\u5fb5 : \u7279\u5fb5\u7d44\u5408 (Day_28)\uff0c\u7fa4\u805a\u7de8\u78bc (Day_29)\n        * \u6e1b\u5c11\u7279\u5fb5 : \u7279\u5fb5\u9078\u64c7 (Day_30)\n    * \u7279\u5fb5\u9078\u64c7\u6709\u4e09\u5927\u985e\u65b9\u6cd5\n        * **\u904e\u6ffe\u6cd5 (Filter)** : \u9078\u5b9a\u7d71\u8a08\u503c\u8207\u8a2d\u5b9a\u9580\u6abb\uff0c\u522a\u9664\u4f4e\u65bc\u9580\u6abb\u7684\u7279\u5fb5\n        * **\u5305\u88dd\u6cd5 (Wrapper)** : \u6839\u64da\u76ee\u6a19\u51fd\u6578\uff0c\u9010\u6b65\u52a0\u5165\u7279\u5fb5\u6216\u522a\u9664\u7279\u5fb5\n        * **\u5d4c\u5165\u6cd5 (Embedded)** : \u4f7f\u7528\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\uff0c\u6839\u64da\u64ec\u5408\u5f8c\u7684\u4fc2\u6578\uff0c\u522a\u9664\u4fc2\u6578\u4f4e\u9918\u9580\u6abb\u7684\u7279\u5fb5\n    * **\u76f8\u95dc\u4fc2\u6578\u904e\u6ffe\u6cd5**\n        ```py\n        # \u8a08\u7b97df\u6574\u9ad4\u76f8\u95dc\u4fc2\u6578, \u4e26\u7e6a\u88fd\u6210\u71b1\u5716\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        corr = df.corr()\n        sns.heatmap(corr)\n        plt.show()\n\n        # \u7be9\u9078\u76f8\u95dc\u4fc2\u6578\u5927\u65bc 0.1 \u6216\u5c0f\u65bc -0.1 \u7684\u7279\u5fb5\n        high_list = list(corr[(corr['SalePrice']>0.1) | (corr['SalePrice']<-0.1)].index)\n        # \u522a\u9664\u76ee\u6a19\u6b04\u4f4d\n        high_list.pop(-1)\n        ```\n    * **Lasso (L1) \u5d4c\u5165\u6cd5**\n        * \u4f7f\u7528 Lasso Regression \u6642\uff0c\u8abf\u6574\u4e0d\u540c\u7684\u6b63\u898f\u5316\u7a0b\u5ea6\uff0c\u5c31\u6703\u81ea\u7136\u4f7f\u5f97\u4e00\u90e8\u5206\u7279\u5fb5\u4fc2\u6578\u70ba 0\uff0c\u56e0\u6b64\u522a\u9664\u4fc2\u6578\u662f 0 \u7684\u7279\u5fb5\uff0c\u4e0d\u9808\u984d\u5916\u6307\u5b9a\u9580\u6abb\uff0c\u4f46\u9700\u8abf\u6574\u6b63\u898f\u5316\u7a0b\u5ea6\n        ```py\n        from sklearn.linear_model import Lasso\n        L1_Reg = Lasso(alpha=0.001)\n        train_X = MMEncoder.fit_transform(df)\n        L1_Reg.fit(train_X, train_Y)\n        L1_Reg.coef_\n\n        L1_mask = list((L1_Reg.coef_>0) | (L1_Reg.coef_<0))\n        df.columns[L1_mask] # index type\n\n        from itertools import compress\n        L1_mask = list((L1_Reg.coef_>0) | (L1_Reg.coef_<0))\n        L1_list = list(compress(list(df), list(L1_mask)))   # list type\n        ```\n    * **GDBT (\u68af\u5ea6\u63d0\u5347\u6a39) \u5d4c\u5165\u6cd5**\n        * \u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u6a39\u64ec\u5408\u5f8c\uff0c\u4ee5\u7279\u5fb5\u5728\u7bc0\u9ede\u51fa\u73fe\u7684\u983b\u7387\u7576\u4f5c\u7279\u5fb5\u91cd\u8981\u6027\uff0c\u4ee5\u6b64\u522a\u9664\u91cd\u8981\u6027\u4f4e\u65bc\u9580\u6abb\u7684\u7279\u5fb5\n\n            |           | \u8a08\u7b97\u6642\u9593 | \u5171\u7dda\u6027  | \u7279\u5fb5\u7a69\u5b9a\u6027 |\n            |-----------|------|------|-------|\n            | \u76f8\u95dc\u4fc2\u6578\u904e\u6ffe\u6cd5   | \u5feb\u901f   | \u7121\u6cd5\u6392\u9664 | \u7a69\u5b9a    |\n            | Lasso \u5d4c\u5165\u6cd5 | \u5feb\u901f   | \u80fd\u6392\u9664  | \u4e0d\u7a69\u5b9a   |\n            | GDBT \u5d4c\u5165\u6cd5  | \u8f03\u6162   | \u80fd\u6392\u9664  | \u7a69\u5b9a    |\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u7279\u5fb5\u9078\u64c7](https://zhuanlan.zhihu.com/p/32749489)\n        * [\u7279\u5fb5\u9078\u64c7\u624b\u518a](https://machine-learning-python.kspax.io/intro-1)\n* **Day_31 : \u7279\u5fb5\u8a55\u4f30**\n    * \u7279\u5fb5\u7684\u91cd\u8981\u6027 : \u5206\u652f\u6b21\u6578\u3001\u7279\u5fb5\u8986\u84cb\u5ea6\u3001\u640d\u5931\u51fd\u6578\u964d\u4f4e\u91cf\n    * sklearn \u7576\u4e2d\u7684\u6a39\u72c0\u6a21\u578b\uff0c\u90fd\u6709\u7279\u5fb5\u91cd\u8981\u6027\u9019\u9805\u65b9\u6cd5 `.feature_importance_`\uff0c\u800c\u5be6\u969b\u4e0a\u90fd\u662f\u5206\u652f\u6b21\u6578\n        ```py\n        from sklearn.ensemble import RandomForestRegressor\n        # \u96a8\u6a5f\u68ee\u6797\u64ec\u5408\u5f8c, \u5c07\u7d50\u679c\u4f9d\u7167\u91cd\u8981\u6027\u7531\u9ad8\u5230\u4f4e\u6392\u5e8f\n        estimator = RandomForestRegressor()\n        estimator.fit(df.values, train_Y)\n        # estimator.feature_importances_ \u5c31\u662f\u6a21\u578b\u7684\u7279\u5fb5\u91cd\u8981\u6027, \u9019\u908a\u5148\u8207\u6b04\u4f4d\u540d\u7a31\u7d50\u5408\u8d77\u4f86, \u624d\u80fd\u770b\u5230\u91cd\u8981\u6027\u8207\u6b04\u4f4d\u540d\u7a31\u7684\u5c0d\u7167\u8868\n        feats = pd.Series(data=estimator.feature_importances_, index=df.columns)\n        feats = feats.sort_values(ascending=False)\n        ```\n    * \u9032\u968e\u7248\u7684 GDBT \u6a21\u578b(`Xgboost`\u3001`lightbm`\u3001`catboost`)\u4e2d\uff0c\u624d\u6709\u4e0a\u8ff0\u4e09\u7a2e\u4e0d\u540c\u7684\u91cd\u8981\u6027\n        |                | Xgboost \u5c0d\u61c9\u53c3\u6578 | \u8a08\u7b97\u6642\u9593 | \u4f30\u8a08\u7cbe\u78ba\u6027 | sklearn \u6709\u6b64\u529f\u80fd |\n        |----------------|--------------|------|-------|--------------|\n        | \u5206\u652f\u6b21\u6578           | weight       | \u6700\u5feb   | \u6700\u4f4e    | O            |\n        | \u5206\u652f\u8986\u84cb\u5ea6          | cover        | \u5feb    | \u4e2d     | X            |\n        | \u640d\u5931\u964d\u4f4e\u91cf (\u8cc7\u8a0a\u589e\u76ca\u5ea6) | gain         | \u8f03\u6162   | \u6700\u9ad8    | X            |\n    * \u6a5f\u5668\u5b78\u7fd2\u7684\u512a\u5316\u5faa\u74b0\n        1. \u539f\u59cb\u7279\u5fb5\n        2. \u9032\u968e\u7248 GDBT \u6a21\u578b\u64ec\u5408\n        3. \u7528\u7279\u5fb5\u91cd\u8981\u6027\u589e\u522a\u7279\u5fb5\n            * \u7279\u5fb5\u9078\u64c7(\u522a\u9664) : \u6311\u9078\u9580\u6abb\uff0c\u522a\u9664\u4e00\u90e8\u5206\u91cd\u8981\u6027\u8f03\u4f4e\u7684\u7279\u5fb5\n            * \u7279\u5fb5\u7d44\u5408(\u589e\u52a0) : \u4f9d\u9818\u57df\u77e5\u8b58\uff0c\u5c0d\u524d\u5e7e\u540d\u7684\u7279\u5fb5\u505a\u7279\u5fb5\u7d44\u5408\u6216\u7fa4\u805a\u7de8\u78bc\uff0c\u5f62\u6210\u66f4\u5f37\u529b\u7279\u5fb5\n        4. \u4ea4\u53c9\u9a57\u8b49 (cross validation)\uff0c\u78ba\u8a8d\u7279\u5fb5\u6548\u679c\u662f\u5426\u6539\u5584\n    * \u6392\u5e8f\u91cd\u8981\u6027 (Permutation Importance)\n        * \u96d6\u7136\u7279\u5fb5\u91cd\u8981\u6027\u76f8\u7576\u5be6\u7528\uff0c\u7136\u800c\u8a08\u7b97\u539f\u7406\u5fc5\u9808\u57fa\u65bc\u6a39\u72c0\u6a21\u578b\uff0c\u65bc\u662f\u6709\u4e86\u53ef\u5ef6\u4f38\u81f3\u975e\u6a39\u72c0\u6a21\u578b\u7684\u6392\u5e8f\u91cd\u8981\u6027\n        * \u6392\u5e8f\u91cd\u8981\u6027\u662f\u6253\u6563\u55ae\u4e00\u7279\u5fb5\u7684\u8cc7\u6599\u6392\u5e8f\uff0c\u518d\u7528\u539f\u672c\u6a21\u578b\u91cd\u65b0\u9810\u6e2c\uff0c\u89c0\u5bdf\u6253\u6563\u524d\u5f8c\u8aa4\u5dee\u8b8a\u5316\u6709\u591a\u5c11\n\n            |        | \u7279\u5fb5\u91cd\u8981\u6027 Feature Importance | \u6392\u5e8f\u91cd\u8981\u6027 Permutation Importance |\n            |--------|--------------------------|------------------------------|\n            | \u9069\u7528\u6a21\u578b   | \u9650\u5b9a\u6a39\u72c0\u6a21\u578b                   | \u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u5747\u53ef                     |\n            | \u8a08\u7b97\u539f\u7406   | \u6a39\u72c0\u6a21\u578b\u7684\u5206\u6b67\u7279\u5fb5                | \u6253\u6563\u539f\u59cb\u8cc7\u6599\u4e2d\u55ae\u4e00\u7279\u5fb5\u7684\u6392\u5e8f               |\n            | \u984d\u5916\u8a08\u7b97\u6642\u9593 | \u8f03\u77ed                       | \u8f03\u9577                           |\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u7279\u5fb5\u9078\u64c7\u7684\u512a\u5316\u6d41\u7a0b](https://juejin.im/post/5a1f7903f265da431c70144c)\n        * [Permutation Importance](https://www.kaggle.com/dansbecker/permutation-importance?utm_medium=email&utm_source=mailchimp&utm_campaign=ml4insights)\n* **Day_32 : \u5206\u985e\u578b\u7279\u5fb5\u512a\u5316 - \u8449\u7de8\u78bc**\n    * \u8449\u7de8\u78bc(leaf encoding) : \u63a1\u7528\u6c7a\u7b56\u6a39\u7684\u8449\u9ede\u4f5c\u70ba\u7de8\u78bc\u4f9d\u64da\u91cd\u65b0\u7de8\u78bc\n        * \u8449\u7de8\u78bc\u7684\u76ee\u7684\u662f**\u91cd\u65b0\u6a19\u8a08**\u8cc7\u6599\uff0c\u4ee5\u64ec\u5408\u5f8c\u7684\u6a39\u72c0\u6a21\u578b\u5206\u6b67\u689d\u4ef6\uff0c\u5c07\u8cc7\u6599**\u96e2\u6563\u5316**\uff0c\u9019\u6a23\u6bd4\u4eba\u70ba\u5beb\u4f5c\u7684\u5224\u65b7\u689d\u4ef6\u66f4\u7cbe\u6e96\uff0c\u66f4\u7b26\u5408\u8cc7\u6599\u7684\u5206\u5e03\u60c5\u5f62\n        * \u8449\u7de8\u78bc\u5b8c\u5f8c\uff0c\u56e0\u7279\u5fb5\u6578\u91cf\u8f03\u591a\uff0c\u901a\u5e38\u642d\u914d**\u7f85\u5409\u65af\u56de\u6b78**\u6216\u8005**\u5206\u89e3\u6a5f**\u505a\u9810\u6e2c\uff0c\u5176\u4ed6\u6a21\u578b\u8f03\u4e0d\u9069\u5408\n        ```py\n        from sklearn.linear_model import LogisticRegression\n        from sklearn.ensemble import RandomForestClassifier\n        # \u56e0\u70ba\u64ec\u5408(fit)\u8207\u7de8\u78bc(transform)\u9700\u8981\u5206\u958b, \u56e0\u6b64\u4e0d\u4f7f\u7528.get_dummy, \u800c\u63a1\u7528 sklearn \u7684 OneHotEncoder\n        from sklearn.preprocessing import OneHotEncoder\n        from sklearn.metrics import roc_curve\n        # \u96a8\u6a5f\u68ee\u6797\u64ec\u5408\u5f8c, \u518d\u5c07\u8449\u7de8\u78bc (*.apply) \u7d50\u679c\u505a\u7368\u71b1 / \u908f\u8f2f\u65af\u8ff4\u6b78\n        rf = RandomForestClassifier(n_estimators=20, min_samples_split=10, min_samples_leaf=5, \n                            max_features=4, max_depth=3, bootstrap=True)\n        onehot = OneHotEncoder()\n        lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n\n        rf.fit(train_X, train_Y)\n        onehot.fit(rf.apply(train_X))\n        lr.fit(onehot.transform(rf.apply(val_X)), val_Y)\n\n        # \u5c07\u96a8\u6a5f\u68ee\u6797+\u8449\u7de8\u78bc+\u908f\u8f2f\u65af\u8ff4\u6b78\u7d50\u679c\u8f38\u51fa\n        pred_rf_lr = lr.predict_proba(onehot.transform(rf.apply(test_X)))[:, 1]\n        fpr_rf_lr, tpr_rf_lr, _ = roc_curve(test_Y, pred_rf_lr)\n        # \u5c07\u96a8\u6a5f\u68ee\u6797\u7d50\u679c\u8f38\u51fa\n        pred_rf = rf.predict_proba(test_X)[:, 1]\n        fpr_rf, tpr_rf, _ = roc_curve(test_Y, pred_rf)\n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#example-ensemble-plot-feature-transformation-py)\n        * [Algorithm-GBDT Encoder](https://zhuanlan.zhihu.com/p/31734283)\n        * [\u5206\u89e3\u6a5f\uff0cFactorization Machine\uff0cFM](https://kknews.cc/code/62k4rml.html)\n### \u6a5f\u5668\u5b78\u7fd2\u57fa\u790e\u6a21\u578b\u5efa\u7acb\n* **Day_33 : \u6a5f\u5668\u5982\u4f55\u5b78\u7fd2**\n    * \u5b9a\u7fa9\u6a21\u578b : \u7dda\u6027\u56de\u6b78\u3001\u6c7a\u7b56\u6a39\u3001\u795e\u7d93\u7db2\u8def\u7b49\u7b49\n        * \u4f8b\u5982\u7dda\u6027\u56de\u6b78 : $ y = b + w * x $\n            * $w$ : weight \u548c $b$ : bias \u5c31\u662f\u6a21\u578b\u53c3\u6578\n            * \u4e0d\u540c\u53c3\u6578\u6a21\u578b\u6703\u7522\u751f\u4e0d\u540c\u7684 $\\hat{y}$\n            * \u5e0c\u671b\u7522\u751f\u51fa\u4f86\u7684 $\\hat{y}$ \u8207\u771f\u5be6\u7b54\u6848 $y$ \u8d8a\u63a5\u8fd1\u8d8a\u597d\n            * \u627e\u51fa\u4e00\u7d44\u53c3\u6578\u8b93\u6a21\u578b\u7522\u751f\u7684 $\\hat{y}$ \u8207\u771f\u6b63\u7684 $y$ \u5f88\u63a5\u8fd1\uff0c\u9019\u500b\u904e\u7a0b\u6709\u9ede\u50cf\u662f\u5b78\u7fd2\u7684\u6982\u5ff5\u3002\n    * \u8a55\u4f30\u6a21\u578b\u597d\u58de : \u5b9a\u7fa9\u4e00\u500b**\u76ee\u6a19\u51fd\u6578 (objective function)** \u4e5f\u53ef\u7a31\u70ba**\u640d\u5931\u51fd\u6578 (Loss function)**\uff0c\u4f86\u8861\u91cf\u6a21\u578b\u597d\u58de\n        * \u4f8b\u5982\u7dda\u6027\u56de\u6b78\u53ef\u4ee5\u4f7f\u7528**\u5747\u65b9\u5dee**(mean square error)\u4f86\u8861\u91cf\n            $$ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2$$\n        * Loss \u8d8a\u5927\u4ee3\u8868\u6a21\u578b\u9810\u6e2c\u6108\u4e0d\u51c6\uff0c\u4ee3\u8868\u4e0d\u8a72\u9078\u64c7\u9019\u500b\u53c3\u6578\n    * \u627e\u51fa\u6700\u4f73\u53c3\u6578 : \u53ef\u4ee5\u4f7f\u7528\u7206\u529b\u6cd5\u3001\u68af\u5ea6\u4e0b\u964d (Gradient Descent)\u3001\u589e\u91cf\u8a13\u7df4 (Addtive Training) \u7b49\u65b9\u5f0f\n        * **\u904e\u64ec\u5408 (over-fitting)** : \u8a13\u7df4\u904e\u7a0b\u5b78\u7fd2\u5230\u4e86\u566a\u97f3\u5c0e\u81f4\u5728\u5be6\u969b\u61c9\u7528\u5931\u6e96\n        * **\u6b20\u64ec\u5408 (under-fitting)** : \u6a21\u578b\u7121\u6cd5\u597d\u597d\u7684\u64ec\u5408\u8a13\u7df4\u6578\u64da\n            * \u5982\u4f55\u77e5\u9053 : \u89c0\u5bdf\u8a13\u7df4\u8cc7\u6599\u8207\u6e2c\u8a66\u8cc7\u6599\u7684\u8aa4\u5dee\u8da8\u52e2\n            * \u5982\u4f55\u6539\u5584 :\n                * \u904e\u64ec\u5408 : \n                    * \u589e\u52a0\u8cc7\u6599\u91cf\n                    * \u964d\u4f4e\u6a21\u578b\u8907\u96dc\u5ea6\n                    * \u4f7f\u7528\u6b63\u898f\u5316 (Regularization)\n                * \u6b20\u64ec\u5408 :\n                    * \u589e\u52a0\u6a21\u578b\u8907\u96dc\u5ea6\n                    * \u6e1b\u8f15\u6216\u4e0d\u4f7f\u7528\u6b63\u898f\u5316\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [\u5b78\u7fd2\u66f2\u7dda\u8207 bias/variance trade-off](http://bangqu.com/yjB839.html)\n* **Day_34 : \u8a13\u7df4\u8207\u6e2c\u8a66\u96c6\u5207\u5206**\n    * \u70ba\u4f55\u9700\u8981\u5207\u5206 :\n        * \u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u9700\u8981\u8cc7\u6599\u8a13\u7df4\n        * \u82e5\u6240\u6709\u8cc7\u6599\u90fd\u9001\u9032\u8a13\u7df4\u6a21\u578b\uff0c\u5c31\u6c92\u6709\u984d\u5916\u8cc7\u6599\u4f86\u8a55\u4f30\u6a21\u578b\n        * \u6a5f\u5668\u6a21\u578b\u53ef\u80fd\u904e\u64ec\u5408\uff0c\u9700\u8981\u9a57\u8b49/\u6e2c\u8a66\u96c6\u4f86\u8a55\u4f30\u6a21\u578b\u662f\u5426\u904e\u64ec\u5408\n    * \u4f7f\u7528 [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) \u9032\u884c\u5207\u5206\n        ```py\n        from sklearn.model_selection import train_test_split\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=False)\n        ```\n    * [K-fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) Cross Validation : \u82e5\u50c5\u505a\u4e00\u6b21\u5207\u5206\uff0c\u6709\u4e9b\u8cc7\u6599\u6703\u6c92\u6709\u88ab\u62ff\u4f86\u8a13\u7df4\uff0c\u56e0\u6b64\u6709 cross validation \u65b9\u6cd5\uff0c\u8b93\u7d50\u679c\u66f4\u7a69\u5b9a\n        ```py\n        from sklearn.model_selection import KFold\n        X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n        y = np.array([1, 2, 3, 4])\n        kf = KFold(n_splits=2, shuffle=False)\n        kf.get_n_splits(X)\n        for train_index, test_index in kf.split(X):\n            print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n        ```\n    * \u9a57\u8b49\u96c6\u8207\u6e2c\u8a66\u96c6\u5dee\u7570 : \u9a57\u8b49\u96c6\u5e38\u7528\u4f86\u8a55\u4f30\u4e0d\u540c\u8d85\u53c3\u6578\u6216\u4e0d\u540c\u6a21\u578b\u7684\u7d50\u679c\uff0c\u6e2c\u8a66\u96c6\u5247\u662f\u9810\u5148\u4fdd\u7559\u7684\u8cc7\u6599\uff0c\u5728\u5c08\u6848\u958b\u767c\u904e\u7a0b\u4e2d\u90fd\u4e0d\u4f7f\u7528\uff0c\u6700\u7d42\u5728\u62ff\u4f86\u505a\u6e2c\u8a66\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [\u8a13\u7df4\u3001\u9a57\u8b49\u8207\u6e2c\u8a66\u96c6\u7684\u610f\u7fa9](https://www.youtube.com/watch?v=D_S6y0Jm6dQ&feature=youtu.be&t=1948)\n* **Day_35 : Regression & Classification**\n    * \u6a5f\u5668\u5b78\u7fd2\u4e3b\u8981\u5206\u70ba**\u56de\u6b78**\u554f\u984c\u8207**\u5206\u985e**\u554f\u984c\n        * \u56de\u6b78\u554f\u984c : \u9810\u6e2c\u76ee\u6a19\u503c\u70ba\u5be6\u6578($-\\infty$\u81f3$\\infty$)(continuous)\n        * \u5206\u985e\u554f\u984c : \u9810\u6e2c\u76ee\u6a19\u503c\u70ba\u985e\u5225(0\u62161)(discrete)\n        * \u56de\u6b78\u554f\u984c\u53ef\u4ee5\u8f49\u5316\u70ba\u5206\u985e\u554f\u984c :\n            * \u539f\u672c\u9810\u6e2c\u8eab\u9ad8(cm)\u554f\u984c\u53ef\u4ee5\u8f49\u5316\u70ba\u9810\u6e2c\u9ad8\u3001\u4e2d\u7b49\u3001\u77ee(\u985e\u5225)\n    * \u4e8c\u5143\u5206\u985e(binary-class) vs. \u591a\u5143\u5206\u985e(multi-class)\n        * \u4e8c\u5143\u5206\u985e : \u76ee\u6a19\u985e\u5225\u53ea\u6709\u5169\u500b\uff0c\u4f8b\u5982\u8a50\u9a19\u5206\u6790(\u6b63\u5e38\u7528\u6236\u3001\u7570\u5e38\u7528\u6236)\u3001\u7455\u75b5\u5075\u6e2c(\u7455\u75b5\u3001\u6b63\u5e38)\n        * \u591a\u5143\u5206\u985e : \u76ee\u6a19\u985e\u5225\u6709\u5169\u7a2e\u4ee5\u4e0a\uff0c\u4f8b\u5982\u624b\u5beb\u6578\u5b57\u8fa8\u8b58(0~9)\uff0c\u5f71\u50cf\u7af6\u8cfd(ImageNet)\u6709\u9ad8\u90541000\u500b\u985e\u5225\n    * \u591a\u5143\u5206\u985e vs. \u591a\u6a19\u7c64(multi-label)\n        * \u591a\u5143\u5206\u985e : \u6bcf\u500b\u6a23\u672c\u53ea\u80fd\u6b78\u5728\u4e00\u500b\u985e\u5225\n        * \u591a\u6a19\u7c64 : \u4e00\u500b\u6a23\u672c\u53ef\u4ee5\u5c6c\u65bc\u591a\u500b\u985e\u5225\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [\u56de\u6b78\u8207\u5206\u985e\u6bd4\u8f03](http://zylix666.blogspot.com/2016/06/supervised-classificationregression.html)\n        * [Multi-class vs. Multi-label ](https://medium.com/coinmonks/multi-label-classification-blog-tags-prediction-using-nlp-b0b5ee6686fc)\n* **Day_36 : \u8a55\u4f30\u6307\u6a19\u9078\u5b9a**\n    * \u8a2d\u5b9a\u5404\u9805\u6307\u6a19\u4f86\u8a55\u4f30\u6a21\u578b\u7684\u6e96\u78ba\u6027\uff0c\u6700\u5e38\u898b\u7684\u70ba**\u6e96\u78ba\u7387 (Accuracy) = \u6b63\u78ba\u5206\u985e\u6a23\u672c\u6578/\u7e3d\u6a23\u672c\u6578**\n    * \u4e0d\u540c\u7684\u8a55\u4f30\u6307\u6a19\u6709\u4e0d\u540c\u7684\u8a55\u4f30\u6e96\u5247\u8207\u9762\u5411\uff0c\u8861\u91cf\u7684\u91cd\u9ede\u6709\u6240\u4e0d\u540c\n    * \u8a55\u4f30\u6307\u6a19\n        * \u56de\u6b78 : \u89c0\u5bdf\u9810\u6e2c\u503c (prediction) \u8207\u5be6\u969b\u503c (ground truth) \u7684**\u5dee\u8ddd**\n            * **MAE (mean absolute error)**\uff0c\u7bc4\u570d[0,inf]\n            * **MSE (mean square error)**\uff0c\u7bc4\u570d[0,inf]\n            * **R-square**\uff0c\u7bc4\u570d[0,1]\n        * \u5206\u985e : \u89c0\u5bdf\u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684**\u6b63\u78ba\u7a0b\u5ea6**\n            * **AUC (area under curve)**\uff0c\u7bc4\u570d[0,1]\n            * **F1-score (precision, recall)**\uff0c\u7bc4\u570d[0,1]\n            * **\u6df7\u6dc6\u77e9\u9663 (Confusion Matrix)**\n    * \u56de\u6b78\u554f\u984c\u53ef\u900f\u904e R-square \u5feb\u901f\u4e86\u89e3\u6e96\u78ba\u5ea6\uff0c\u4e8c\u5143\u5206\u985e\u554f\u984c\u901a\u5e38\u4f7f\u7528 AUC \u8a55\u4f30\uff0c\u5e0c\u671b\u54ea\u500b\u985e\u5225\u4e0d\u8981\u5206\u932f\u5247\u53ef\u4f7f\u7528 F1-score \u4e26\u89c0\u5bdf precision \u8207 recall \u6578\u503c\uff0c\u591a\u5206\u985e\u554f\u984c\u5247\u53ef\u4f7f\u7528 top-k accuracy\uff0c\u4f8b\u5982 ImageNet \u7af6\u8cfd\u901a\u5e38\u63a1\u7528 top-5 accuracy\n    * Q&A :\n        * AUC \u8a08\u7b97\u602a\u602a\u7684\uff0cAUC \u7684 y_pred \u7684\u503c\u586b\u5165\u6bcf\u500b\u6a23\u672c**\u9810\u6e2c\u6a5f\u7387 (probility)** \u800c\u975e\u5206\u985e\u7d50\u679c\n        * F1-score \u8a08\u7b97\u5247\u586b\u5165\u6bcf\u500b\u6a23\u672c\u5206\u985e\u7d50\u679c\uff0c\u5982\u6a5f\u7387 >= 0.5 \u5247\u8996\u70ba 1\uff0c\u800c\u975e\u586b\u5165\u6a5f\u7387\u503c\n        ```py\n        from sklearn import metrics, datasets\n        from sklearn.linear_model import LinearRegression\n        from sklearn.model_selection import train_test_split\n\n        # MAE, MSE, R-square\n        X, y = datasets.make_regression(n_features=1, random_state=42, noise=4) # \u751f\u6210\u8cc7\u6599\n        model = LinearRegression() # \u5efa\u7acb\u56de\u6b78\u6a21\u578b\n        model.fit(X, y) # \u5c07\u8cc7\u6599\u653e\u9032\u6a21\u578b\u8a13\u7df4\n        prediction = model.predict(X) # \u9032\u884c\u9810\u6e2c\n        mae = metrics.mean_absolute_error(prediction, y) # \u4f7f\u7528 MAE \u8a55\u4f30\n        mse = metrics.mean_squared_error(prediction, y) # \u4f7f\u7528 MSE \u8a55\u4f30\n        r2 = metrics.r2_score(prediction, y) # \u4f7f\u7528 r-square \u8a55\u4f30\n\n        # AUC\n        cancer = datasets.load_breast_cancer() # \u6211\u5011\u4f7f\u7528 sklearn \u5167\u542b\u7684\u4e73\u764c\u8cc7\u6599\u96c6\n        X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=50, random_state=0)\n        y_pred = np.random.random((50,)) # \u6211\u5011\u5148\u96a8\u6a5f\u751f\u6210 50 \u7b46\u9810\u6e2c\u503c\uff0c\u7bc4\u570d\u90fd\u5728 0~1 \u4e4b\u9593\uff0c\u4ee3\u8868\u6a5f\u7387\u503c\n        auc = metrics.roc_auc_score(y_test, y_pred) # \u4f7f\u7528 roc_auc_score \u4f86\u8a55\u4f30\u3002 **\u9019\u908a\u7279\u5225\u6ce8\u610f y_pred \u5fc5\u9808\u8981\u653e\u6a5f\u7387\u503c\u9032\u53bb!**\n\n        # F1-score, precision, recall\n        threshold = 0.5 \n        y_pred_binarized = np.where(y_pred>threshold, 1, 0) # \u4f7f\u7528 np.where \u51fd\u6578, \u5c07 y_pred > 0.5 \u7684\u503c\u8b8a\u70ba 1\uff0c\u5c0f\u65bc 0.5 \u7684\u70ba 0\n        f1 = metrics.f1_score(y_test, y_pred_binarized) # \u4f7f\u7528 F1-Score \u8a55\u4f30\n        precision = metrics.precision_score(y_test, y_pred_binarized) # \u4f7f\u7528 Precision \u8a55\u4f30\n        recall  = metrics.recall_score(y_test, y_pred_binarized) # \u4f7f\u7528 recall \u8a55\u4f30\n        ```\n\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u8d85\u8a73\u89e3 AUC](https://www.dataschool.io/roc-curves-and-auc-explained/)\n        * [\u66f4\u591a\u8a55\u4f30\u6307\u6a19](https://zhuanlan.zhihu.com/p/30721429)\n* **Day_37 : Regression \u6a21\u578b**\n    * Linear Regression (\u7dda\u6027\u56de\u6b78) : \u7c21\u55ae\u7dda\u6027\u6a21\u578b\uff0c\u53ef\u7528\u65bc\u56de\u6b78\u554f\u984c\uff0c\u9808\u6ce8\u610f[\u8cc7\u6599\u5171\u7dda\u6027\u8207\u8cc7\u6599\u6a19\u6e96\u5316\u554f\u984c](https://blog.csdn.net/Noob_daniel/article/details/76087829)\uff0c\u901a\u5e38\u53ef\u505a\u70ba baseline \u4f7f\u7528\n    * Logistic Regression (\u7f85\u5409\u65af\u56de\u6b78) : \u5206\u985e\u6a21\u578b\uff0c\u5c07\u7dda\u6027\u6a21\u578b\u7d50\u679c\u52a0\u4e0a [sigmoid](https://baike.baidu.com/item/Sigmoid%E5%87%BD%E6%95%B0/7981407) \u51fd\u6578\uff0c\u5c07\u9810\u6e2c\u503c\u9650\u5236\u5728 0~1 \u4e4b\u9593\uff0c\u5373\u70ba\u9810\u6e2c\u6a5f\u7387\u503c\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Andrew Ng \u6559\u4f60 Linear Regression](https://zh-tw.coursera.org/lecture/machine-learning/model-representation-db3jS)\n        * [Logistic Regression \u6578\u5b78\u539f\u7406](https://blog.csdn.net/qq_23269761/article/details/81778585)\n        * [Linear Regression \u8a73\u7d30\u4ecb\u7d39](https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_linear_regression_works.html)\n        * [Logistic Regression \u8a73\u7d30\u4ecb\u7d39](https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-3%E8%AC%9B-%E7%B7%9A%E6%80%A7%E5%88%86%E9%A1%9E-%E9%82%8F%E8%BC%AF%E6%96%AF%E5%9B%9E%E6%AD%B8-logistic-regression-%E4%BB%8B%E7%B4%B9-a1a5f47017e5)\n        * [\u4f60\u53ef\u80fd\u4e0d\u77e5\u9053\u7684 Logistic Regression](https://taweihuang.hpd.io/2017/12/22/logreg101/)\n* **Day_38 : Regression \u6a21\u578b </>**\n    * \u4f7f\u7528 scikit-learn \u5957\u4ef6\n        ```py\n        from sklearn.linear_model import LinearRegression, LogisticRegression\n        from sklearn.metrics import mean_squared_error, accuracy_score\n        # \u7dda\u6027\u56de\u6b78\n        reg = LinearRegression().fit(x_train, y_train)\n        y_pred = reg.predict(x_test)\n        print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n        # \u7f85\u5409\u65af\u56de\u6b78\n        logreg = LogisticRegression().fit(x_train, y_train)\n        y_pred = logreg.predict(x_test)\n        print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n        ```\n    * Logistic Regression \u53c3\u6578\n        * *Penalty* : \u4f7f\u7528 \"L1\" or \"L2\" \u6b63\u5247\u5316\u53c3\u6578\n        * *C* : \u6b63\u5219\u5316\u7cfb\u6570 **$\\lambda$ \u7684\u5012\u6570**\uff0c\u6578\u5b57\u8d8a\u5c0f\u6a21\u578b\u8d8a\u7c21\u55ae\n        * *Solver* : \u5c0d\u640d\u5931\u51fd\u6578\u7684\u512a\u5316\u65b9\u6cd5\uff0c\u8a73\u7d30\u53c3\u8003[\u9023\u7d50](https://blog.csdn.net/lc574260570/article/details/82116197)\n        * *Multi-class* : \u9078\u64c7 one-vs-rest \u6216 multi-nominal \u5206\u985e\u65b9\u5f0f\uff0c\uf974\u6709 10 class\uff0c ovr \u662f\u8a13\u7df4 10 \u500b\u4e8c\u5206\u985e\u6a21\u578b\uff0c\u7b2c\u4e00\u500b\u6a21\u578b\u8ca0\u8cac\u5206\u985e (class1, non-class1)\uff1b\u7b2c\u4e8c\u500b\u8ca0\u8cac(class2, non-class2)\uff0c\u4ee5\u6b64\u985e\u63a8\u3002multi-nominal \u662f\u76f4\u63a5\u8a13\u7df4\u591a\u5206\u985e\u6a21\u578b\u3002\u8a73\u7d30\uf96b\u8003[\u9023\u7d50](https://www.quora.com/What-is-the-difference-between-one-vs-all-binary-logistic-regression-and-multinomial-logistic-regression)\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u66f4\u591a Linear regression \u548c Logistic regression \u7bc4\u4f8b](https://github.com/trekhleb/homemade-machine-learning)\n        * [\u6df1\u5165\u4e86\u89e3 multi-nominal Logistic Regresson \u539f\u7406](http://dataaspirant.com/2017/05/15/implement-multinomial-logistic-regression-python/)\n* **Day_39 : LASSO, Ridge regression**\n    * \u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u7684\u76ee\u6a19\u51fd\u6578\u6709\u5169\u500b\u975e\u5e38\u91cd\u8981\u7684\u5143\u7d20\n        * **\u640d\u5931\u51fd\u6578 (Loss function)** : \u8861\u91cf\u5be6\u969b\u503c\u8207\u9810\u6e2c\u503c\u5dee\u7570\uff0c\u8b93\u6a21\u578b\u5f80\u6b63\u78ba\u65b9\u5411\u5b78\u7fd2\n        * **\u6b63\u5247\u5316 (Regularization)** : \u907f\u514d\u6a21\u578b\u904e\u65bc\u8907\u96dc\uff0c\u9020\u6210\u904e\u64ec\u5408\n        * \u70ba\u4e86\u907f\u514d\u904e\u64ec\u5408\u6211\u5011\u628a\u6b63\u5247\u5316\u52a0\u5165\u76ee\u6a19\u51fd\u6578\uff0c**\u76ee\u6a19\u51fd\u6578 = \u640d\u5931\u51fd\u6578 + \u6b63\u5247\u5316**\n        * \u6b63\u5247\u5316\u53ef\u4ee5\u61f2\u7f70\u6a21\u578b\u7684\u8907\u96dc\u5ea6\uff0c\u7576\u6a21\u578b\u8d8a\u5927\u5176\u503c\u8d8a\u5927\n    * \u6b63\u5247\u5316\u51fd\u6578 : \u7528\u4f86\u8861\u91cf\u6a21\u578b\u7684\u8907\u96dc\u5ea6\n        * L1 : $\\alpha\\sum|weight|$\n        * L2 : $\\alpha\\sum(weight)^2$\n        * \u9019\uf978\u7a2e\u90fd\u662f\u5e0c\u671b\u6a21\u578b\u7684\uf96b\u6578\u6578\u503c\u4e0d\u8981\u592a\u5927\uff0c\u539f\u56e0\u662f\uf96b\u6578\u7684\u6578\u503c\u8b8a\u5c0f\uff0c\u566a\u97f3\u5c0d\u6700\u7d42\u8f38\u51fa\u7684\u7d50\u679c\u5f71\u97ff\u8d8a\u5c0f\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\uf98a\uff0c\u4f46\u4e5f\u8b93\u6a21\u578b\u7684\u64ec\u5408\u80fd\uf98a\u4e0b\u964d\n    * LASSO \u70ba Linear Regression \u52a0\u4e0a L1\n    * Ridge \u70ba Linear Regression \u52a0\u4e0a L2\n    * \u5176\u4e2d\u6709\u500b\u8d85\u53c3\u6578 $\\alpha$ \u53ef\u4ee5\u8abf\u6574\u6b63\u5247\u5316\u5f37\u5ea6 \n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [Linear, Lasso, Ridge Regression \u672c\u8cea\u5340\u5225](https://www.zhihu.com/question/38121173)\n        * [PCA \u8207 Ridge regression \u7684\u95dc\u4fc2](https://taweihuang.hpd.io/2018/06/10/pca-%E8%88%87-ridge-regression-%E7%9A%84%E9%97%9C%E4%BF%82/\n        )\n* **Day_40 : LASSO, Ridge regression </>**\n    ```py\n    from sklearn import datasets\n    from sklearn.linear_model import Lasso, Ridge\n    # \u8b80\u53d6\u7cd6\u5c3f\u75c5\u8cc7\u6599\u96c6\n    diabetes = datasets.load_diabetes()\n    # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\n    x_train, x_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=4)\n    \n    # \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b\n    lasso = Lasso(alpha=1.0)\n    # \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4\n    lasso.fit(x_train, y_train)\n    # \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c\n    y_pred = lasso.predict(x_test)\n    # \u5370\u51fa\u8a13\u7df4\u5f8c\u7684\u6a21\u578b\u53c3\uf96b\u6578\n    print(lasso.coef_)\n\n    # \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b\n    ridge = Ridge(alpha=1.0)\n    # \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4\n    ridge.fit(x_train, y_train)\n    # \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c\n    y_pred = regr.predict(x_test)\n    # \u5370\u51fa\u8a13\u7df4\u5f8c\u7684\u6a21\u578b\u53c3\uf96b\u6578\n    print(ridge.coef_)\n    ```\n* **Day_41 : \u6c7a\u7b56\u6a39 Decision Tree**\n    * **\u6c7a\u7b56\u6a39 (Decision Tree)** : \u900f\u904e\u4e00\u7cfb\u5217\u7684**\u662f\u975e\u554f\u984c**\uff0c\u5e6b\u52a9\u6211\u5011\u5c07\u8cc7\u6599\u5207\u5206\uff0c\u53ef\u8996\u89ba\u5316\u6bcf\u500b\u5207\u5206\u904e\u7a0b\uff0c\u662f\u500b\u5177\u6709\u975e\u5e38\u9ad8\u89e3\u91cb\u6027\u7684\u6a21\u578b\n        * \u5f9e\u8a13\u7df4\u8cc7\u6599\u4e2d\u627e\u51fa\u898f\u5247\uff0c\u8b93\u6bcf\u4e00\u6b21\u6c7a\u7b56\u4f7f**\u8a0a\u606f\u589e\u76ca (information Gain)** \u6700\u5927\u5316\n        * \u8a0a\u606f\u589e\u76ca\u8d8a\u5927\u4ee3\u8868\u5207\u5206\u5f8c\u7684\u5169\u7fa4\uff0c\u7fa4\u5167\u76f8\u4f3c\u5ea6\u8d8a\u9ad8\uff0c\u4f8b\u5982\u4f7f\u7528\u5065\u6aa2\u8cc7\u6599\u4f86\u9810\u6e2c\u6027\u5225\uff0c\u82e5\u4f7f\u7528\u982d\u9aee\u9577\u5ea6 50 \u516c\u5206\u9032\u884c\u5207\u5206\uff0c\u5247\u5207\u5206\u5f8c\u7684\u5169\u7fa4\u8cc7\u6599\u5f88\u6709\u53ef\u80fd\u591a\u6578\u70ba\u7537\u751f\u6216\u5973\u751f(\u76f8\u4f3c\u7a0b\u5ea6\u9ad8)\uff0c\u9019\u6a23\u982d\u9aee\u9577\u5ea6\u5c31\u662f\u500b\u597d feature\n    * \u5982\u4f55\u8861\u91cf\u76f8\u4f3c\u7a0b\u5ea6\n        * **\u5409\u5c3c\u4fc2\u6578 (gini-index)** (\u4e0d\u7d14\u5ea6)\n            $$Gini = 1 - \\sum_jp_j^2$$\n        * **\u71b5 (entropy)**\n            $$Entropy = -\\sum_jp_jlog_2p_j$$\n    * \u6c7a\u7b56\u6a39\u7684\u7279\u5fb5\u91cd\u8981\u6027\n        * \u6211\u5011\u53ef\u4ee5\u5f9e\u69cb\u5efa\u6a39\u7684\u904e\u7a0b\u4e2d\uff0c\u900f\u904e feature \u88ab\u7528\uf92d\u5207\u5206\u7684\u6b21\u6578\uff0c\uf92d\u5f97\u77e5\u54ea\u4e9b features \u662f\u76f8\u5c0d\u6709\u7528\u7684\n        * \u6240\u6709 feature importance \u7684\u7e3d\u548c\u70ba 1\n        * \u5be6\u52d9\u4e0a\u53ef\u4ee5\u4f7f\u7528 feature importance \uf92d\uf9ba\u89e3\u6a21\u578b\u5982\u4f55\u9032\ufa08\u5206\u985e\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u6c7a\u7b56\u6a39\u904b\u4f5c](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)\n        * [\u6c7a\u7b56\u6a39\u8207\u56de\u6b78\u554f\u984c](https://www.saedsayad.com/decision_tree_reg.htm)\n* **Day_42 : \u6c7a\u7b56\u6a39 </>**\n    * \u6a5f\u5668\u5b78\u7fd2\u7684\u5efa\u6a21\u6b65\u9a5f :\n        1. \u8b80\u53d6\u548c\u6aa2\u67e5\u8cc7\u6599\n            * \u4f7f\u7528 pandas \u8b80\u53d6 .csv \u6a94 : `pd.read_csv`\n            * \u4f7f\u7528 numpy \u8b80\u53d6 txt \u6a94 : `np.loadtxt`\n            * \u4f7f\u7528 sklearn \u5167\u5efa\u8cc7\u6599\u96c6 : `sklearn.datasets.load_xxx`\n            * \u6aa2\u67e5\u8cc7\u6599\u6578\u91cf : `data.shape`\n        2. \u5c07\u8cc7\u6599\u5207\u5206\u70ba\u8a13\u7df4 (train) \u8207\u6e2c\u8a66\u96c6 (test)\n            * `train_test_split(data)`\n        3. \u5efa\u7acb\u6a21\u578b\u958b\u59cb\u8a13\u7df4 (fit)\n            ```py\n            clf = DecisionTreeClassifier()\n            clf.fit(x_train, y_train)\n            ```\n        4. \u5c07\u6e2c\u8a66\u8cc7\u6599\u653e\u9032\u8a13\u7df4\u597d\u7684\u6a21\u578b\u9032\u884c\u9810\u6e2c (predict)\uff0c\u4e26\u548c\u6e2c\u8a66\u8cc7\u6599\u7684 label (y_test) \u505a\u8a55\u4f30\n            ```py\n            clf.predict(x_test)\n            accuracy_score(y_test, y_pred)\n            f1_score(y_test, y_pred)\n            ```\n    * \u6839\u64da\u56de\u6b78/\u5206\u985e\u554f\u984c\u5efa\u7acb\u4e0d\u540c\u7684 Classifier\n        ```py\n        from sklearn.tree_model import DecisionTreeRegressor\n        from sklearn.tree_model import DecisionTreeClassifier\n        \n        # \u8b80\u53d6\u9cf6\u5c3e\u82b1\u8cc7\u6599\u96c6\n        iris = datasets.load_iris()\n        # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\n        x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)\n        # \u5efa\u7acb\u6a21\u578b\n        clf = DecisionTreeClassifier()\n        # \u8a13\u7df4\u6a21\u578b\n        clf.fit(x_train, y_train)\n        # \u9810\u6e2c\u6e2c\u8a66\u96c6\n        y_pred = clf.predict(x_test)\n        print(\"Acuuracy: \", metrics.accuracy_score(y_test, y_pred))\n        # \u5217\u51fa\u7279\u5fb5\u548c\u91cd\u8981\u6027\n        print(iris.feature_names)\n        print(\"Feature importance: \", clf.feature_importances_)\n        ```\n    * \u6c7a\u7b56\u6a39\u7684\u8d85\u53c3\u6578\n        * *Criterion*: \u8861\uf97e\u8cc7\u6599\u76f8\u4f3c\u7a0b\u5ea6\u7684 metric\n        * *Max_depth*: \u6a39\u80fd\u751f\u9577\u7684\u6700\u6df1\u9650\u5236\n        * *Min_samples_split*: \u81f3\u5c11\u8981\u591a\u5c11\u6a23\u672c\u4ee5\u4e0a\u624d\u9032\ufa08\u5207\u5206\n        * *Min_samples_lear*: \u6700\u7d42\u7684\u8449\u5b50(\u7bc0\u9ede)\u4e0a\u81f3\u5c11\u8981\u6709\u591a\u5c11\u6a23\u672c\n            ```py\n            clf = DecisionTreeClassifier(\n                    criterion = 'gini',\n                    max_depth = None,\n                    min_samples_split = 2,\n                    min_samples_left = 1,\n            )\n            ```\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [Creating and Visualizing Decision Trees with Python](https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176)\n* **Day_43 : \u96a8\u6a5f\u68ee\u6797\u6a39 Random Forest**\n    * \u6c7a\u7b56\u6a39\u7f3a\u9ede :\n        * \u82e5\u4e0d\u5c0d\u6c7a\u7b56\u6a39\u9032\u884c\u9650\u5236 (\u6a39\u6df1\u5ea6\u3001\u8449\u5b50\u4e0a\u81f3\u5c11\u8981\u6709\u591a\u5c11\u6a23\u672c\u7b49)\uff0c\u6c7a\u7b56\u6a39\u975e\u5e38\u5bb9\u6613 over-fitting\n    * \u96c6\u6210\u6a21\u578b - \u96a8\u6a5f\u68ee\u6797 (Random Forest)\n        * **\u96c6\u6210 (Ensemble)** : \u5c07\u591a\u500b\u6a21\u578b\u7684\u7d50\u679c\u7d44\u5408\u5728\u4e00\u8d77\uff0c\u900f\u904e**\u6295\u7968**\u6216\u662f**\u52a0\u6b0a**\u7684\u65b9\u5f0f\u7372\u5f97\u6700\u7d42\u7d50\u679c\n        * \u6bcf\u68f5\u6a39\u4f7f\u7528\u90e8\u5206\u8a13\u7df4\u8cc7\u6599\u8207\u7279\u5fb5\u9032\u884c\u8a13\u7df4\u800c\u6210\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u96a8\u6a5f\u68ee\u6797](http://hhtucode.blogspot.com/2013/06/ml-random-forest.html)\n        * [How Random Forest Algorithm Works](https://medium.com/@Synced/how-random-forest-algorithm-works-in-machine-learning-3c0fe15b6674)\n        * [bootstrap](http://sofasofa.io/forum_main_post.php?postid=1000691)\n* **Day_44 : \u96a8\u6a5f\u68ee\u6797\u6a39 </>**\n    ```py\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestRegressor\n    \n    clf = RandomForestClassifier(\n            n_estimators=10, #\u6c7a\u7b56\u6a39\u7684\u6578\u91cf\uf97e\n            criterion=\"gini\",\n            max_features=\"auto\", #\u5982\u4f55\u9078\u53d6 features         \n            max_depth=10,\n            min_samples_split=2,\n            min_samples_leaf=1)\n    # \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4\n    clf.fit(x_train, y_train)\n    # \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c\n    y_pred = clf.predict(x_test)\n    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n\n    # \u5efa\u7acb\u4e00\u500b\u7dda\u6027\u56de\u6b78\u6a21\u578b\n    regr = RandomForestRegressor()\n    # \u5c07\u8a13\u7df4\u8cc7\u6599\u4e1f\u9032\u53bb\u6a21\u578b\u8a13\u7df4\n    regr.fit(x_train, y_train)\n    # \u5c07\u6e2c\u8a66\u8cc7\u6599\u4e1f\u9032\u6a21\u578b\u5f97\u5230\u9810\u6e2c\u7d50\u679c\n    y_pred = regr.predict(x_test)\n    print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n    ```\n* **Day_45 : \u68af\u5ea6\u63d0\u5347\u6a5f Gradient Boosting Machine**\n    * \u96a8\u6a5f\u68ee\u6797\u4f7f\u7528\u7684\u65b9\u6cd5\u70ba Bagging (Bootstrap Aggregating)\uff0c\u7528\u62bd\u6a23\u8cc7\u6599\u8207\u7279\u5fb5\u751f\u6210\u6bcf\u4e00\u68f5\u6a39\uff0c\u6700\u5f8c\u5728\u53d6\u5e73\u5747\n    * Boosting \u662f\u53e6\u4e00\u7a2e\u96c6\u6210\u65b9\u6cd5\uff0c\u5e0c\u671b\u7531\u5f8c\u9762\u751f\u6210\u7684\u6a39\u4f86\u4fee\u6b63\u524d\u9762\u5b78\u4e0d\u597d\u7684\u5730\u65b9\n    * **Bagging** :\n        * \u900f\u904e\u62bd\u6a23 (sampling) \u65b9\u5f0f\u751f\u6210\u6bcf\u4e00\u68f5\u6a39\uff0c\u6a39\u8207\u6a39\u4e4b\u9593\u662f\u7368\u7acb\u7684\n        * \u964d\u4f4e over-fitting\n        * \u6e1b\u5c11 variance\n        * Independent classifiers\n    * **Boosting** :\n        * \u900f\u904e\u5e8f\u5217 (additive) \u65b9\u5f0f\u751f\u6210\u6bcf\u4e00\u68f5\u6a39\uff0c\u6bcf\u68f5\u6a39\u8207\u524d\u9762\u7684\u6a39\u95dc\u806f\n        * \u53ef\u80fd\u6703 over-fitting\n        * \u6e1b\u5c11 bias \u548c variance\n        * Sequential classifiers\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u68af\u5ea6\u63d0\u5347\u6c7a\u7b56\u6a39](https://ifun01.com/84A3FW7.html)\n        * [XGboost](https://www.youtube.com/watch?v=ufHo8vbk6g4)\n        * [\u9673\u5929\u5947 - Boosted Tree](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)\n        * [\u674e\u5b8f\u6bc5 - Ensemble](https://www.youtube.com/watch?v=tH9FH1DH5n0)\n* **Day_46 : \u68af\u5ea6\u63d0\u5347\u6a5f </>**\n    ```py\n    from sklearn.ensemble import GradientBoostingClassifier\n    from sklearn.ensemble import GradientBoostingRegressor\n\n    clf = GradientBoostingClassifier(\n        loss=\"deviance\", #Loss \u7684\u9078\u64c7\uff0c\u82e5\uf974\u6539\u70ba exponential \u5247\u6703\u8b8a\u6210Adaboosting \u6f14\u7b97\u6cd5\uff0c\u6982\u5ff5\uf9a3\u76f8\u540c\u4f46\u5be6\u4f5c\u7a0d\u5fae\u4e0d\u540c\n        learning_rate=0.1, #\u6bcf\u68f5\u6a39\u5c0d\u6700\u7d42\u7d50\u679c\u7684\u5f71\u97ff\uff0c\u61c9\u8207 n_estimators \u6210\u53cd\u6bd4\n        n_estimators=100 #\u6c7a\u7b56\u6a39\u7684\u6578\u91cf\uf97e\n        )\n    # \u8a13\u7df4\u6a21\u578b\n    clf.fit(x_train, y_train)\n    # \u9810\u6e2c\u6e2c\u8a66\u96c6\n    y_pred = clf.predict(x_test)\n    print(\"Acuuracy: \", metrics.accuracy_score(y_test, y_pred))\n    ```\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)\n### \u6a5f\u5668\u5b78\u7fd2\u8abf\u6574\u53c3\u6578\n* **Day_47 : \u8d85\u53c3\u6578\u8abf\u6574**\n    * \u6a5f\u5668\u5b78\u7fd2\u4e2d\u7684\u8d85\u53c3\u6578\n        * LASSO, Ridge : $\\alpha$ \u7684\u5927\u5c0f\n        * \u6c7a\u7b56\u6a39 : \u6a39\u7684\u6df1\u5ea6\u3001\u7bc0\u9ede\u6700\u5c0f\u6a23\u672c\u6578\n        * \u96a8\u6a5f\u68ee\u6797 : \u6a39\u7684\u6578\u91cf\n    * \u8d85\u53c3\u6578\u6703\u5f71\u97ff\u6a21\u578b\u8a13\u7df4\u7684\u7d50\u679c\uff0c\u5efa\u8b70\u5148\u4f7f\u7528\u9810\u8a2d\u518d\u6162\u6162\u9032\u884c\u8abf\u6574\n    * \u8d85\u53c3\u6578\u6703\u5f71\u97ff\u7d50\u679c\u4f46\u63d0\u5347\u6548\u679c\u6709\u9650\uff0c**\u8cc7\u6599\u6e05\u7406**\u548c**\u7279\u5fb5\u5de5\u7a0b**\u624d\u80fd\u6700\u6709\u6548\u7684\u63d0\u5347\u6e96\u78ba\u7387\n    * \u8d85\u53c3\u6578\u7684\u8abf\u6574\u65b9\u6cd5\n        * **\u7aae\u8209\u6cd5 (Grid Search)** : \u76f4\u63a5\u6307\u5b9a\u8d85\u53c3\u6578\u7684\u7bc4\u570d\u7d44\u5408\uff0c\u6bcf\u4e00\u7d44\u53c3\u6578\u90fd\u8a13\u7df4\u5b8c\u6210\uff0c\u518d\u6839\u64da\u9a57\u8b49\u96c6\u7684\u7d50\u679c\u9078\u64c7\u6700\u4f73\u53c3\u6578\n        * **\u96a8\u6a5f\u641c\u5c0b (Random Search)** : \u6307\u5b9a\u8d85\u53c3\u6578\u7bc4\u570d\uff0c\u7528\u5747\u52fb\u5206\u5e03\u9032\u884c\u53c3\u6578\u62bd\u6a23\uff0c\u7528\u62bd\u5230\u7684\u53c3\u6578\u9032\u884c\u8a13\u7df4\uff0c\u518d\u6839\u64da\u9a57\u8b49\u96c6\u7684\u7d50\u679c\u9078\u64c7\u6700\u4f73\u53c3\u6578\uff0c[\u96a8\u6a5f\u641c\u5c0b](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881)\u901a\u5e38\u80fd\u7372\u5f97\u8f03\u597d\u7684\u7d50\u679c\n    * \u6b63\u78ba\u7684\u8d85\u53c3\u6578\u8abf\u6574\u6b65\u9a5f : \u82e5\u4f7f\u7528\u540c\u4e00\u4efd\u9a57\u8b49\u96c6 (validation) \u4f86\u8abf\u53c3\uff0c\u53ef\u80fd\u8b93\u6a21\u578b\u904e\u65bc\u64ec\u5408\u9a57\u8b49\u96c6\uff0c\u6b63\u78ba\u6b65\u9a5f\u662f\u4f7f\u7528 Cross-validation \u78ba\u4fdd\u6a21\u578b\u7684\u6cdb\u5316\u6027\n        1. \u5c07\u8cc7\u6599\u5207\u5206\u70ba\u8a13\u7df4/\u6e2c\u8a66\u96c6\uff0c\u6e2c\u8a66\u96c6\u5148\u4fdd\u7559\u4e0d\u7528\n        2. \u5c07\u525b\u5207\u597d\u7684\u8a13\u7df4\u96c6\uff0c\u518d\u4f7f\u7528 Cross-validation \u5207\u6210 K \u4efd\u8a13\u7df4/\u9a57\u8b49\u96c6\n        3. \u7528 gird/random search \u7684\u8d85\u53c3\u6578\u9032\u884c\u8a13\u7df4\u8207\u8a55\u4f30\n        4. \u9078\u51fa\u6700\u4f73\u53c3\u6578\uff0c\u7528\u8a72\u53c3\u6578\u8207\u5168\u90e8\u8a13\u7df4\u96c6\u5efa\u6a21\n        5. \u6700\u5f8c\u4f7f\u7528\u6e2c\u8a66\u96c6\u8a55\u4f30\u7d50\u679c\n        ```py\n        from sklearn import datasets, metrics\n        from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n        from sklearn.ensemble import GradientBoostingRegressor\n        \n        # \u8b80\u53d6\u624b\u5beb\u8fa8\u8b58\u8cc7\u6599\u96c6\n        boston = datasets.load_boston()\n        # \u5207\u5206\u8a13\u7df4\u96c6/\u6e2c\u8a66\u96c6\n        x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.25, random_state=42)\n        # \u5efa\u7acb\u6a21\u578b\n        clf = GradientBoostingRegressor(random_state=7)\n        # \u8a2d\u5b9a\u8981\u8a13\u7df4\u7684\u8d85\u53c3\u6578\u7d44\u5408\n        n_estimators = [100, 200, 300]\n        max_depth = [1, 3, 5]\n        param_grid = dict(n_estimators=n_estimators, max_depth=max_depth)\n        # \u5efa\u7acb\u641c\u5c0b\u7269\u4ef6\uff0c\u653e\u5165\u6a21\u578b\u53ca\u53c3\u6578\u7d44\u5408\u5b57\u5178 (n_jobs=-1 \u6703\u4f7f\u7528\u5168\u90e8 cpu \u5e73\u884c\u904b\u7b97)\n        grid_search = GridSearchCV(clf, param_grid, scoring=\"neg_mean_squared_error\", n_jobs=-1, verbose=1)\n        # \u958b\u59cb\u641c\u5c0b\u6700\u4f73\u53c3\u6578\n        grid_result = grid_search.fit(x_train, y_train)\n        # \u9810\u8a2d\u6703\u8dd1 3-fold cross-validadtion\uff0c\u7e3d\u5171 9 \u7a2e\u53c3\u6578\u7d44\u5408\uff0c\u7e3d\u5171\u8981 train 27 \u6b21\u6a21\u578b\n        # \u5370\u51fa\u6700\u4f73\u7d50\u679c\u8207\u6700\u4f73\u53c3\u6578\n        print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n        # \u4f7f\u7528\u6700\u4f73\u53c3\u6578\u91cd\u65b0\u5efa\u7acb\u6a21\u578b\n        clf_bestparam = GradientBoostingRegressor(\n                            max_depth=grid_result.best_params_['max_depth'],\n                            n_estimators=grid_result.best_params_['n_estimators'])\n        # \u8a13\u7df4\u6a21\u578b\n        clf_bestparam.fit(x_train, y_train)\n        # \u9810\u6e2c\u6e2c\u8a66\u96c6\n        y_pred = clf_bestparam.predict(x_test)\n        print(metrics.mean_squared_error(y_test, y_pred))\n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [how to tune machine learning models](https://cambridgecoding.wordpress.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/)\n        * [Hyperparameter Tuning the Random Forest in Python](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n* **Day_48 : Kaggle**\n    * Kaggle \u70ba\u5168\u7403\u8cc7\u6599\u79d1\u5b78\u7af6\u8cfd\u7684\u7db2\u7ad9\uff0c\u8a31\u591a\u8cc7\u6599\u79d1\u5b78\u7684\u7af6\u8cfd\u5747\u6703\u5728\u6b64\u8209\u8fa6\uff0c\u5438\u5f15\u5168\u7403\u512a\u79c0\u7684\u8cc7\u6599\u79d1\u5b78\u5bb6\uf96b\u52a0\n    * \u4e3b\u8fa6\u55ae\u4f4d\u901a\u5e38\u6703\u628a\u6e2c\u8a66\u8cc7\u6599\u5206\u70ba public set \u8207 private set\uff0c\uf96b\u8cfd\u8005\u4e0a\u50b3\u9810\u6e2c\u7d50\u679c\u53ef\u4ee5\u770b\u5230 public set \u7684\u6210\u7e3e\uff0c\u4f46\u6bd4\u8cfd\u6700\u7d42\u6703\u4f7f\u7528 private set \u7684\u6210\u7e3e\u4f5c\u70ba\u6392\u540d\n    * Kernels \u53ef\u4ee5\u770b\ufa0a\u8a31\u591a\u9ad8\u624b\u5011\u5206\u4eab\u7684\u7a0b\u5f0f\u78bc\u8207\u7d50\u679c\uff0c\u591a\u534a\u6703\u4ee5 jupyter notebook \u5448\u73fe\n    * Discussion \u53ef\u4ee5\u770b\u5230\u9ad8\u624b\u5011\u4e92\u76f8\u8a0e\u8ad6\u53ef\u80fd\u7684\u505a\u6cd5\uff0c\u6216\u662f\u8cc7\u6599\u4e2d\u662f\u5426\u5b58\u5728\u67d0\u4e9b\u554f\u984c\n    * [scikit-learn-practice](https://www.kaggle.com/c/data-science-london-scikit-learn)\n* **Day_49 : \u6df7\u548c\u6cdb\u5316 (Blending)**\n    * **\u96c6\u6210**\u662f\u4f7f\u7528\u4e0d\u540c\u65b9\u5f0f\uff0c\u7d50\u5408\u591a\u500b\u6216\u591a\u7a2e\u5206\u985e\u5668\uff0c\u4f5c\u70ba\u7d9c\u5408\u9810\u6e2c\u7684\u505a\u6cd5\u7e3d\u7a31\n        * \u5c07\u6a21\u578b\u622a\u9577\u88dc\u77ed\uff0c\u53ef\u4ee5\u8aaa\u662f\u6a5f\u5668\u5b78\u7fd2\u7684\u548c\u8b70\u5236/\u591a\u6578\u6c7a\n        * \u5176\u4e2d\u5206\u70ba\u8cc7\u6599\u5c64\u9762\u7684\u96c6\u6210 : \u5982\u88dd\u888b\u6cd5(Bagging)\u3001\u63d0\u5347\u6cd5(Boosting)\n        * \u4ee5\u53ca\u6a21\u578b\u7684\u7279\u5fb5\u96c6\u6210 : \u5982\u6df7\u548c\u6cdb\u5316(Blending)\u3001\u5806\u758a\u6cdb\u5316(Stacking)\n        * **\u88dd\u888b\u6cd5 (Bagging)** : \u5c07\u8cc7\u6599\u653e\u5165\u888b\u4e2d\u62bd\u53d6\uff0c\u6bcf\u56de\u5408\u7d50\u675f\u5f8c\u91cd\u65b0\u653e\u56de\u888b\u4e2d\u91cd\u62bd\uff0c\u5728\u642d\u914d\u5f31\u5206\u985e\u5668\u53d6\u5e73\u5747\u6216\u591a\u6578\u6c7a\u7d50\u679c\uff0c\u4f8b\u5982\u96a8\u6a5f\u68ee\u6797\n        * **\u63d0\u5347\u6cd5 (Boosting)** : \u7531\u4e4b\u524d\u6a21\u578b\u7684\u9810\u6e2c\u7d50\u679c\uff0c\u53bb\u6539\u4fbf\u8cc7\u6599\u88ab\u62bd\u5230\u7684\u6b0a\u91cd\u6216\u76ee\u6a19\u503c\n        * \u5c07\u932f\u5224\u7684\u8cc7\u6599\u6a5f\u7387\u653e\u5927\uff0c\u6b63\u78ba\u7684\u7e2e\u5c0f\uff0c\u5c31\u662f**\u81ea\u9069\u61c9\u63d0\u5347(AdaBoost, Adaptive Boosting)**\n        * \u5982\u679c\u662f\u4f9d\u7167\u4f30\u8a08\u8aa4\u5dee\u7684\u6b98\u5dee\u9805\u8abf\u6574\u65b0\u76ee\u6a19\u503c\uff0c\u5247\u5c31\u662f**\u68af\u5ea6\u63d0\u5347\u6a5f (Gradient Boosting Machine)** \u7684\u4f5c\u6cd5\uff0c\u53ea\u662f\u68af\u5ea6\u63d0\u5347\u6a5f\u9084\u52a0\u4e0a\u7528\u68af\u5ea6\uf92d\u9078\u64c7\u6c7a\u7b56\u6a39\u5206\u652f\n        * Bagging/Boosting : \u4f7f\u7528\u4e0d\u540c\u8cc7\u6599\u3001\u76f8\u540c\u6a21\u578b\uff0c\u591a\u6b21\u4f30\u8a08\u7684\u7d50\u679c\u5408\u6210\u6700\u7d42\u9810\u6e2c\n        * Voting/Blending/Stacking : \u4f7f\u7528\u540c\u4e00\u8cc7\u6599\u4e0d\u540c\u6a21\u578b\uff0c\u5408\u6210\u51fa\u4e0d\u540c\u9810\u6e2c\u7d50\u679c\n    * \u6df7\u5408\u6cdb\u5316 (Blending)\n        * \u5c07\u4e0d\u540c\u6a21\u578b\u7684\u9810\u6e2c\u503c\u52a0\u6b0a\u5408\u6210\uff0c\u6b0a\u91cd\u548c\u70ba1\u5982\u679c\u53d6\u9810\u6e2c\u7684\u5e73\u5747 or \u4e00\u4eba\u4e00\u7968\u591a\u6578\u6c7a(\u6bcf\u500b\u6a21\u578b\u6b0a\u91cd\u76f8\u540c)\uff0c\u5247\u53c8\u7a31\u70ba\u6295\u7968\u6cdb\u5316(Voting)\n        * \u5bb9\u6613\u4f7f\u7528\u4e14\u6709\u6548\n        * \u4f7f\u7528\u524d\u63d0 : \u500b\u5225**\u55ae\u6a21\u6548\u679c\u597d**(\u6709\u8abf\u6559)\u4e26\u4e14**\u6a21\u578b\u5dee\u7570\u5927**\uff0c\u55ae\u6a21\u8981\u597d\u5c24\u5176\u91cd\u8981\n        * \u5ef6\u4f38\u95b1\u8b80 :\n            * [Blending and Stacking](https://www.youtube.com/watch?v=mjUKsp0MvMI&list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&index=27&t=0s)\n            * [superblend](https://www.kaggle.com/tunguz/superblend/code)\n            ```py\n            new_train = pd.DataFrame([lr_y_prob, gdbt_y_prob, rf_y_prob])\n            new_train = new_train.T\n\n            clf = LogisticRegression(tol=0.001, penalty='l2', fit_intercept=False, C=1.0)\n            clf.fit(new_train, train_Y)\n\n            coef = clf.coef_[0] / sum(clf.coef_[0]) \n\n            blending_pred = lr_pred*coef[0]  + gdbt_pred*coef[1] + rf_pred*coef[2]\n            ```\n* **Day_50 : \u5806\u758a\u6cdb\u5316 (Stacking)**\n    * \u4e0d\u53ea\u5c07\u9810\u6e2c\u7d50\u679c\u6df7\u548c\uff0c\u800c\u662f\u4f7f\u7528\u9810\u6e2c\u7d50\u679c**\u7576\u65b0\u7279\u5fb5**\n    * Stacking \u4e3b\u8981\u662f\u628a\u6a21\u578b\u7576\u4f5c\u4e0b\u4e00\u968e\u7684**\u7279\u5fb5\u7de8\u78bc\u5668**\u4f86\u4f7f\u7528\uff0c\u4f46\u662f**\u5f85\u7de8\u78bc\u8cc7\u6599**\u8207**\u8a13\u7df4\u7de8\u78bc\u5668**\u7684\u8cc7\u6599\u4e0d\u53ef\u91cd\u8907(\u8a13\u7df4\u6e2c\u8a66\u7684\u4e0d\u53ef\u91cd\u8907\u6027)\n    * \u82e5\u662f\u5c07\u8a13\u7df4\u8cc7\u6599\u5207\u6210\u5169\u4efd\uff0c\u5f85\u7de8\u78bc\u8cc7\u6599\u592a\u5c11\uff0c\u4e0b\u4e00\u5c64\u7684\u8cc7\u6599\u7b46\u6578\u5c31\u6703\u592a\u5c11\uff0c\u8a13\u7df4\u7de8\u78bc\u5668\u7684\u8cc7\u6599\u592a\u5c11\uff0c\u5247\u7de8\u78bc\u5668\u7684\u5f37\u5ea6\u5c31\u6703\u4e0d\u5920\uff0c\u56e0\u6b64\u4f7f\u7528 K-fold \u62c6\u5206\u8cc7\u6599\u8a13\u7df4\uff0c\u9019\u6a23\u8cc7\u6599\u6c92\u6709\u8b8a\u5c11\uff0c\u7de8\u78bc\u5668\u4e5f\u5920\u5f37\u97cc\uff0c\u4f46 K \u503c\u8d8a\u5927\u8a13\u7df4\u6642\u9593\u8d8a\u9577\n    * \u81ea\u6211\u905e\u8ff4\u7684 Stacking :\n        * Q1\uff1a\u80fd\u4e0d\u80fd\u65b0\u820a\u7279\u5fb5\u4e00\u8d77\u7528\uff0c\u518d\u7528\u6a21\u578b\u9810\u6e2c\u5462?\n            * A1\uff1a\u53ef\u4ee5\uff0c\u9019\u88e1\u5176\u5be6\u6709\u500b\u6709\u8da3\u7684\u601d\u8003\uff0c\u4e5f\u5c31\u662f : \u9019\u6a23\u4e0d\u5c31\u53ef\u4ee5\u4e00\u76f4\u4e00\u76f4\u7121\u9650\u589e\u52a0\u7279\u5fb5\u4e0b\u53bb? \u9019\u6a23\u5f8c\u9762\u7684\u7279\u5fb5\u9084\u6709\u610f\u7fa9\u55ce? \u4e0d\u6703 Overfitting \u55ce?...\u5176\u5be6\u52a0\u592a\u591a\u6b21\u662f\u6703 Overfitting \u7684\uff0c\u5fc5\u9700\u8b39\u614e\u5207\u5206 Fold \u4ee5\u53ca\u65b0\u589e\u6b21\u6578\n        * Q2\uff1a\u65b0\u7684\u7279\u5fb5\uff0c\u80fd\u4e0d\u80fd\u518d\u642d\u914d\u6a21\u578b\u5275\u7279\u5fb5\uff0c\u7b2c\u4e09\u5c64\u7b2c\u56db\u5c64...\u4e00\u76f4\u4e0b\u53bb\u5462?\n            * A2\uff1a\u53ef\u4ee5\uff0c\u4f46\u662f\u6bcf\u591a\u4e00\u5c64\uff0c\u6a21\u578b\u6703\u8d8a\u8907\u96dc : \u56e0\u6b64\u6cdb\u5316 (\u53c8\u7a31\u70ba\uf939\u68d2\u6027) \u6703\u505a\u5f97\uf901\u597d\uff0c\u7cbe\u6e96\u5ea6\u4e5f\u6703\u4e0b\u964d\uff0c\u6240\u4ee5\u9664\u975e\u7b2c\u4e00\u5c64\u7684\u55ae\u6a21\u8abf\u5f97\u5f88\u597d\uff0c\u5426\u5247\uf978\u4e09\u5c64\u5c31\u4e0d\u9700\u8981\u7e7c\u7e8c\u5f80\u4e0b\uf9ba\n        * Q3\uff1a\u65e2\u7136\u540c\u5c64\u65b0\u7279\u5fb5\u6703 Overfitting\uff0c\u5c64\u6578\u52a0\u6df1\u6703\u589e\u52a0\u6cdb\u5316\uff0c\uf978\u8005\u540c\u6642\u7528\u662f\u4e0d\u662f\u5c31\u80fd\u628a\u7f3a\u9ede\u4e92\u76f8\u62b5\u92b7\u5462?\n            * A3\uff1a\u53ef\u4ee5!!\u800c\u4e14\u9019\u6b63\u662f Stacking \u6700\u6709\u8da3\u7684\u5730\u65b9\uff0c\u4f46\u771f\u6b63\u5be6\u8e10\u6642\uff0c\u7a0b\u5f0f\u8907\u96dc\uff0c\u904b\u7b97\u6642\u9593\u53c8\u8981\u518d\u5f80\u4e0a\u4e00\u500b\uf97e\u7d1a\uff0c\u4e4b\u524d\u66fe\u6709\u5927\u795e\u5beb\u904e StackNet \u5be6\u73fe\u9019\u500b\u60f3\u6cd5\uff0c\u7528JVM \u52a0\u901f\u904b\u7b97\uff0c\u4f46\u5be6\u969b\u4e0a\u4f7f\u7528\u6642\u8abf\uf96b\u56f0\u96e3\uff0c\u5f8c\u7e7c\u4f7f\u7528\u7684\u4eba\u5c31\u5c11\u4e86\n        * Q4 : \u5be6\u969b\u4e0a\u5beb Stacking \u6709\u9019\u9ebc\u56f0\u96e3\u55ce?\n            * A4 : \u5176\u5be6\u4e0d\u96e3\uff0c\u5c31\u50cf sklearn \u5e6b\u6211\u5011\u5beb\u597d\uf9ba\u8a31\u591a\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\uff0c**mlxtend** \u4e5f\u5df2\u7d93\u5e6b\u6211\u5011\u5beb\u597d\uf9ba Stacking \u7684\u6a21\u578b\uff0c\u6240\u4ee5\u7528\u5c31\u53ef\u4ee5\uf9ba (\uf96b\u8003\u4eca\u65e5\u7bc4\uf9b5\u6216 mlxtrend \u5b98\u7db2)\n        * Q5 : Stacking \u7d50\u679c\u5206\u6578\u771f\u7684\u6bd4\u8f03\u9ad8\u55ce?\n            * A5 : \u4e0d\u4e00\u5b9a\uff0c\u6709\u6642\u5019\u55ae\u6a21\uf901\u9ad8\uff0c\u6709\u6642\u5019 Blending \u6548\u679c\u5c31\u4e0d\u932f\uff0c\u8996\u8cc7\u6599\uf9fa\u6cc1\u800c\u5b9a\n        * Q6 : Stacking \u53ef\u4ee5\u505a\uf96b\u6578\u8abf\u6574\u55ce?\n            * A6 : \u53ef\u4ee5\uff0c\u8acb\uf96b\u8003 mlxtrend \u7684[\u8abf\uf96b\u7bc4\uf9b5](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)\uff0c\u4e3b\u8981\u5dee\uf962\u662f\uf96b\u6578\u540d\u7a31\u5beb\u6cd5\u7a0d\u6709\u4e0d\u540c\n        * Q7 : \u9084\u6709\u5176\u4ed6\u505a Stacking \u6642\u9700\u8981\u6ce8\u610f\u7684\u4e8b\u9805\u55ce?\n            * A7 :\u300c\u5206\u985e\u554f\u984c\u300d\u7684 Stacking \u8981\u6ce8\u610f\uf978\u4ef6\u4e8b\uff1a\u8a18\u5f97\u52a0\u4e0a use_probas=True (\u8f38\u51fa\u7279\u5fb5\u624d\u6703\u662f\u6a5f\u7387\u503c)\uff0c\u4ee5\u53ca\u8f38\u51fa\u7684\u7e3d\u7279\u5fb5\u6578\u6703\u662f\uff1a\u6a21\u578b\u6578\uf97e*\u5206\u985e\u6578\uf97e (\u56de\u6b78\u554f\u984c\u7279\u5fb5\u6578=\u6a21\u578b\u6578\u91cf\uf97e)\n            ```py\n            from mlxtend.classifier import StackingClassifier\n\n            meta_estimator = GradientBoostingClassifier(tol=100, subsample=0.70, n_estimators=50, \n                                                    max_features='sqrt', max_depth=4, learning_rate=0.3)\n            stacking = StackingClassifier(classifiers =[lr, gdbt, rf], use_probas=True, meta_classifier=meta_estimator)\n            stacking.fit(train_X, train_Y)\n            stacking_pred = stacking.predict(test_X)\n            ```\n### Kaggle \u7b2c\u4e00\u6b21\u671f\u4e2d\u8003\n* **Day_51~53 : Kaggle\u671f\u4e2d\u8003**\n    * [Enron Fraud Dataset \u5b89\u9686\u516c\u53f8\u8a50\u6b3a\u6848\u8cc7\u6599\u96c6](https://www.kaggle.com/c/ml100)\n        * \u5982\u4f55\u8655\uf9e4\u5b58\u5728\u5404\u7a2e\u7f3a\u9677\u7684\u771f\u5be6\u8cc7\u6599\n        * \u4f7f\u7528 val / test data \uf92d\uf9ba\u89e3\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u7684\u8a13\u7df4\u60c5\u5f62\n        * \u4f7f\u7528\u9069\u7576\u7684\u8a55\u4f30\u51fd\u6578\uf9ba\u89e3\u9810\u6e2c\u7d50\u679c\n        * \u61c9\u7528\u9069\u7576\u7684\u7279\u5fb5\u5de5\u7a0b\u63d0\u5347\u6a21\u578b\u7684\u6e96\u78ba\u7387\n        * \u8abf\u6574\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u7684\u8d85\uf96b\u6578\uf92d\u63d0\u5347\u6e96\u78ba\u7387\n        * \u6e05\u695a\u7684\u8aaa\u660e\u6587\u4ef6\u8b93\u5225\u4eba\uf9ba\u89e3\u4f60\u7684\u6210\u679c\n### \u975e\u76e3\u7763\u5f0f\u6a5f\u5668\u5b78\u7fd2\n* **Day_54 : \u975e\u76e3\u7763\u5f0f\u6a5f\u5668\u5b78\u7fd2\u7c21\u4ecb**\n    * \u975e\u76e3\u7763\u5b78\u7fd2\u5141\u8a31\u6211\u5011\u5728\u5c0d\u7d50\u679c\u7121\u6cd5\u9810\u77e5\u6642\u63a5\u8fd1\u554f\u984c\u3002\u975e\u76e3\u7763\u5b78\u7fd2\u6f14\u7b97\u6cd5\u53ea\u57fa\u65bc\u8f38\u5165\u8cc7\u6599\u627e\u51fa\u6a21\u5f0f\u3002\u7576\u6211\u5011\u7121\u6cd5\u78ba\u5b9a\u5c0b\u627e\u5167\u5bb9\uff0c\u6216\u7121\u6a19\u8a18 (y) \u8cc7\u6599\u6642\uff0c\u901a\u5e38\u6703\u7528\u9019\u500b\u6f14\u7b97\u6cd5\uff0c\u5e6b\u52a9\u6211\u5011\uf9ba\u89e3\u8cc7\u6599\u6a21\u5f0f\n    * \u975e\u76e3\u7763\u5b78\u7fd2\u7b97\u6cd5\u6982\u8981\n        * **\u805a\u985e\u5206\u6790** : \u5c0b\u627e\u8cc7\u6599\u7684\u96b1\u85cf\u6a21\u5f0f\n        * **\u964d\u4f4e\u7dad\u5ea6** : \u7279\u5fb5\u6578\u592a\u5927\u4e14\u7279\u5fb5\u9593\u76f8\u95dc\u6027\u9ad8\uff0c\u4ee5\u6b64\u65b9\u5f0f\u7e2e\u6e1b\u7279\u5fb5\u7dad\u5ea6\n        * \u5176\u4ed6 : \u95dc\uf997\u6cd5\u5247 (\u8cfc\u7269\u7c43\u5206\u6790)\u3001\uf962\u5e38\u503c\u5075\u6e2c\u3001\u63a2\u7d22\u6027\u8cc7\u6599\u5206\u6790\u7b49\n    * \u61c9\u7528\u6848\u4f8b :\n        * \u5ba2\u6236\u5206\u7fa4 : \u5728\u8cc7\u6599\u6c92\u6709\u4efb\u4f55\u6a19\u8a18\uff0c\u6216\u662f\u554f\u984c\u9084\u6c92\u5b9a\u7fa9\u6e05\u695a\u524d\uff0c\u53ef\u7528\u5206\u7fa4\u7684\u65b9\u5f0f\u5e6b\u52a9\uf9e4\u6e05\u8cc7\u6599\u7279\u6027\u3002\n        * \u7279\u5fb5\u62bd\u8c61\u5316 : \u5fb5\u6578\u592a\u591a\u96e3\u65bc\uf9e4\u89e3\u53ca\u5448\u73fe\u7684\u60c5\u6cc1\u4e0b\uff0c\u85c9\u7531\u62bd\u8c61\u5316\u7684\u6280\u8853\u5e6b\u52a9\u964d\u4f4e\u8cc7\u6599\u7dad\u5ea6\uff0c\u540c\u6642\u4e0d\u5931\u53bb\u539f\u6709\u7684\u8cc7\u8a0a\uff0c\u7d44\u5408\u6210\u65b0\u7684\u7279\u5fb5\u3002\n        * \u8cfc\u7269\u7c43\u5206\u6790 : \u8cc7\u6599\u63a2\u52d8\u7684\u7d93\u5178\u6848\uf9b5\uff0c\u9069\u7528\u65bc\u7dda\u4e0b\u6216\u7dda\u4e0a\u96f6\u552e\u7684\u5546\u54c1\u7d44\u5408\u63a8\u85a6\u3002\n        * \u975e\u7d50\u69cb\u5316\u8cc7\u6599\u5206\u6790 : \u975e\u7d50\u69cb\u5316\u8cc7\u6599\u5982\u6587\u5b57\u3001\u5f71\u50cf\u7b49\uff0c\u53ef\u4ee5\u85c9\u7531\u4e00\u4e9b\u975e\u76e3\u7763\u5f0f\u5b78\u7fd2\u7684\u6280\u8853\uff0c\u5e6b\u52a9\u5448\u73fe\u53ca\u63cf\u8ff0\u8cc7\u6599\uff0c\u4f8b\u5982\u4e3b\u984c\u6a21\u578b (topic model)\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u674e\u5b8f\u6bc5 - Unsupervised learning](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/PCA.mp4)\n        * [scikit-learn unsupervised learning](https://scikit-learn.org/stable/unsupervised_learning.html)\n        * [Andrew Ng - Unsupervised learning](https://youtu.be/jAA2g9ItoAc)\n* **Day_55 : K-means \u805a\u985e\u7b97\u6cd5**\n    * Supervised learning : \u76ee\u6a19\u5728\u627e\u51fa\u6c7a\u7b56\u908a\u754c (decision boundary)\n    * Unsupervised learning : \u76ee\u6a19\u5728\u627e\u51fa\u8cc7\u6599\u7d50\u69cb\n    * Why clustering ?\n        * \u5728\u8cc7\u6599\u9084\u6c92\u6709\u6a19\u8a18\u3001\u554f\u984c\u9084\u6c92\u5b9a\u7fa9\u6e05\u695a\u6642\uff0c\u805a\u985e\u7b97\u6cd5\u53ef\u4ee5\u5e6b\u52a9\u6211\u5011\u7406\uf9e4\u89e3\u8cc7\u6599\u7279\u6027\uff0c\u8a55\u4f30\u6a5f\u5668\u5b78\u7fd2\u554f\u984c\u65b9\u5411\u7b49\uff0c\u4e5f\u662f\u4e00\u7a2e\u5448\u73fe\u8cc7\u6599\u7684\u65b9\u5f0f\n    * **K-means \u805a\u985e\u7b97\u6cd5**\n        * \u628a\u6240\u6709\u8cc7\u6599\u9ede\u5206\u6210 k \u500b cluster\uff0c\u4f7f\u5f97\u76f8\u540c cluster \u4e2d\u7684\u6240\u6709\u8cc7\u6599\u9ede\u5f7c\u6b64\u5118\uf97e\u76f8\u4f3c\uff0c\u800c\u4e0d\u540c cluster \u7684\u8cc7\u6599\u9ede\u5118\uf97e\u4e0d\u540c\u3002\n        * \u8ddd\u96e2\u6e2c\uf97e\uff08e.g. \u6b50\u6c0f\u8ddd\u96e2\uff09\u7528\u65bc\u8a08\u7b97\u8cc7\u6599\u9ede\u7684\u76f8\u4f3c\u5ea6\u548c\u76f8\uf962\u5ea6\u3002\u6bcf\u500b cluster \u6709\u4e00\u500b\u4e2d\u5fc3\u9ede\u3002\u4e2d\u5fc3\u9ede\u53ef\uf9e4\u89e3\u70ba\u6700\u80fd\u4ee3\u8868 cluster \u7684\u9ede\u3002\n        * \u7b97\u6cd5\u6d41\u7a0b :\n            1. \u96a8\u6a5f\u53d6 K \u500b\u9ede (cluster centroid)\n            2. \u5c0d\u6bcf\u4e00\u500b training example \u6839\u64da\u5b83\u8ddd\u96e2\u54ea\u4e00\u500b cluster centroid \u8f03\u8fd1\uff0c\u6a19\u8a18\u7232\u5176\u4e2d\u4e4b\u4e00 (cluster assignment)\n            3. \u7136\u5f8c\u628a centroid \u79fb\u5230\u540c\u4e00\u7fa4 training examples \u7684\u4e2d\u5fc3\u9ede (update centroid)\n            4. \u53cd\u8986\u9032\ufa08 cluster assignment \u53ca update centroid, \u76f4\u5230 cluster assignment \u4e0d\u518d\u5c0e\u81f4 training example \u88ab assign \u7232\u4e0d\u540c\u7684\u6a19\u8a18 (\u7b97\u6cd5\u6536\u6582)\n        * K-means \u76ee\u6a19\u662f\u4f7f\u7e3d\u9ad4\u7fa4\u5167\u5e73\u65b9\u8aa4\u5dee\u6700\u5c0f\n        $$\\sum_{i=0}^n\\min_{\\mu_j\\in{C}} (||x_i-\\mu_j||^2) $$\n        * \u6ce8\u610f\u4e8b\u9805 :\n            * Random initialization : initial \u8a2d\u5b9a\u7684\u4e0d\u540c\uff0c\u6703\u5c0e\u81f4\u5f97\u5230\u4e0d\u540c clustering \u7684\u7d50\u679c\uff0c\u53ef\u80fd\u5c0e\u81f4 local optima\uff0c\u800c\u975e global optima\u3002\n            * \u56e0\u7232\u6c92\u6709\u9810\u5148\u7684\u6a19\u8a18\uff0c\u5c0d\u65bc cluster \u6578\u91cf\u591a\u5c11\u624d\u662f\u6700\u4f73\u89e3\uff0c\u6c92\u6709\u6a19\u6e96\u7b54\u6848\uff0c\u5f97\u9760\u624b\u52d5\u6e2c\u8a66\u89c0\u5bdf\u3002\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Andrew Ng - K-means](https://www.youtube.com/watch?v=hDmNF9JG3lo)\n        * [Unsupervised machine learning](https://pythonprogramming.net/flat-clustering-machine-learning-python-scikit-learn/\n        )\n            ```py\n            from sklearn.cluster import KMeans  \n            kmeans = KMeans(n_clusters=3, n_init=1, init='random')\n            kmeans.fit(X)\n            print(kmeans.cluster_centers_)\n            print(kmeans.labels_)\n            ```\n* **Day_56 : K-means \u89c0\u5bdf : \u4f7f\u7528\u8f2a\u5ed3\u5206\u6790**\n    * \u5206\u7fa4\u6a21\u578b\u7684\u8a55\u4f30\n        * \u56f0\u96e3\u9ede : \u975e\u76e3\u7763\u6a21\u578b\u56e0\u70ba\u6c92\u6709\u76ee\u6a19\u503c\uff0c\u56e0\u6b64\u7121\u6cd5\u4f7f\u7528\u76ee\u6a19\u503c\u8207\u9810\u6e2c\u503c\u7684\u5dee\u8ddd\u4f86\u8861\u91cf\u512a\u52a3\n        * \u8a55\u4f30\u65b9\u5f0f\u985e\u578b\n            * \u6709\u76ee\u6a19\u503c\u7684\u5206\u7fa4 : \u5982\u679c\u8cc7\u6599\u6709\u76ee\u6a19\u503c\uff0c\u53ea\u662f\u5148\u5ffd\uf976\u76ee\u6a19\u503c\u505a\u975e\u76e3\u7763\u5b78\u7fd2\uff0c\u5247\u53ea\u8981\u5fae\u8abf\u5f8c\uff0c\u5c31\u53ef\u4ee5\u4f7f\u7528\u539f\u672c\u76e3\u7763\u7684\u6e2c\uf97e\u51fd\u6578\u8a55\u4f30\u6e96\u78ba\u6027\n            * \u7121\u76ee\u6a19\u503c\u7684\u5206\u7fa4 : \u901a\u5e38\u6c92\u6709\u76ee\u6a19\u503c/\u76ee\u6a19\u503c\u975e\u5e38\u5c11\u624d\u6703\u7528\u975e\u76e3\u7763\u6a21\u578b\uff0c\u9019\u7a2e\u60c5\u6cc1\u4e0b\uff0c\u53ea\u80fd\u4f7f\u7528\u8cc7\u6599\u672c\u8eab\u7684\u5206\u5e03\u8cc7\u8a0a\uff0c\uf92d\u505a\u6a21\u578b\u7684\u8a55\u4f30\n    * \uf9d7\u5ed3\u5206\u6790 (Silhouette analysis)\n        * \u6b77\u53f2 : \u6700\u65e9\u7531 Peter J. Rousseeuw \u65bc 1986 \u63d0\u51fa\u3002\u5b83\u540c\u6642\u8003\u616e\uf9ba\u7fa4\u5167\u4ee5\u53ca\u76f8\u9130\u7fa4\u7684\u8ddd\u96e2\uff0c\u9664\uf9ba\u53ef\u4ee5\u8a55\u4f30\u8cc7\u6599\u9ede\u5206\u7fa4\u662f\u5426\u5f97\u7576\uff0c\u4e5f\u53ef\u4ee5\u7528\u4f86\uf92d\u8a55\u4f30\u4e0d\u540c\u5206\u7fa4\u65b9\u5f0f\u5c0d\u65bc\u8cc7\u6599\u7684\u5206\u7fa4\u6548\u679c\n        * \u8a2d\u8a08\u7cbe\u795e : \u540c\u4e00\u7fa4\u7684\u8cc7\u6599\u9ede\u61c9\u8a72\u5f88\u8fd1\uff0c\u4e0d\u540c\u7fa4\u7684\u8cc7\u6599\u9ede\u61c9\u8a72\u5f88\u9060\uff0c\u6240\u4ee5\u8a2d\u8a08\u4e00\u7a2e\u7576\u540c\u7fa4\u8cc7\u6599\u9ede\u8d8a\u8fd1 / \u4e0d\u540c\u7fa4\u8cc7\u6599\u9ede\u8d8a\u9060\u6642\u8d8a\u5927\u7684\u5206\u6578\uff0c\u7576\u8cc7\u6599\u9ede\u5728\uf978\u7fa4\u4ea4\u754c\u9644\u8fd1\uff0c\u5e0c\u671b\u5206\u6578\u63a5\u8fd1 0\n        * \u55ae\u9ede\u8f2a\u5ed3\u503c : \n            * \u5c0d\u4efb\u610f\u55ae\u4e00\u8cc7\u6599\u9ede i\uff0c\u300c\u8207 i \u540c\u4e00\u7fa4\u300d\u7684\u8cc7\u6599\u9ede\uff0c\u8ddd\u96e2 i \u7684\u5e73\u5747\u7a31\u70ba ai\n            * \u300c\u8207 i \u4e0d\u540c\u7fa4\u300d\u7684\u8cc7\u6599\u9ede\u4e2d\uff0c\u4e0d\u540c\u7fa4\u8ddd\u96e2 i \u5e73\u5747\u4e2d\uff0c\u6700\u8fd1\u7684\u7a31\u70ba bi ( \u5176\u5be6\u5c31\u662f\u8981\u53d6\u7b2c\u4e8c\u9760\u8fd1 i \u7684\u90a3\u4e00\u7fa4\u5e73\u5747\uff0c\u6eff\u8db3\u4ea4\u754c\u4e0a\u5206\u6578\u70ba 0 \u7684\u8a2d\u8a08) \n            * i \u9ede\u7684\uf9d7\u5ed3\u5206\u6578 si : (bi-ai) / max{bi, ai} \u5176\u5be6\u53ea\u8981\u4e0d\u662f\u523b\u610f\u5206\u932f\uff0cbi \u901a\u5e38\u6703\u5927\u65bc\u7b49\u65bc ai\uff0c\u6240\u4ee5\u4e0a\u8ff0\u516c\u5f0f\u5728\u6b64\u689d\u4ef6\u4e0b\u53ef\u4ee5\u5316\u7c21\u70ba 1 - ai / bi\n        * \u6574\u9ad4\u8f2a\u5ed3\u5206\u6790 : \n            * \u5206\u7d44\u89c0\u5bdf\uff0c\u4f9d\u7167\u4e0d\u540c\u7684\u985e\u5225\uff0c\u5c07\u540c\u985e\u5225\u7684\uf9d7\u5ed3\u5206\u6578\u6392\u5e8f\u5f8c\u986f\u793a\uff0c\u767c\u73fe\uf978\u7d44\u7684\uf9d7\u5ed3\u503c\u5927\u591a\u5728\u5e73\u5747\u4ee5\u4e0b\uff0c\u4e14\u6bd4\u4f8b\\\uf9b5\u4e0a\u63a5\u8fd1 0 \u7684\u9ede\u4e5f\u6bd4\u8f03\u591a\uff0c\u9019\u4e9b\u60c5\u6cc1\u90fd\u8868\u793a\u9019\uf978\u7d44\u4f3c\u4e4e\u6c92\u5206\u5f97\u90a3\u9ebc\u958b\n            * \u5e73\u5747\u503c\u89c0\u5bdf\uff0c\u8a08\u7b97\u5206\u7fa4\u7684\uf9d7\u5ed3\u5206\u6578\u7e3d\u5e73\u5747\uff0c\u5206\u7684\u7fa4\u6578\u8d8a\u591a\u61c9\u8a72\u5206\u6578\u8d8a\u5c0f\uff0c\u5982\u679c\u7e3d\u5e73\u5747\u503c\u6c92\u6709\u96a8\u8457\u5206\u7fa4\u6578\u589e\u52a0\u800c\u8b8a\u5c0f\uff0c\u5c31\u8aaa\u660e\uf9ba\u90a3\u4e9b\u5206\u7fa4\u6578\u8f03\u4e0d\u6d3d\u7576\n            ```py\n            from sklearn.cluster import KMeans\n            from sklearn.metrics import silhouette_samples, silhouette_score\n\n            # \u5ba3\u544a KMean \u5206\u7fa4\u5668, \u5c0d X \u8a13\u7df4\u4e26\u9810\u6e2c\n            clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n            cluster_labels = clusterer.fit_predict(X)\n\n            # \u8a08\u7b97\u6240\u6709\u9ede\u7684 silhouette_score \u5e73\u5747\n            silhouette_avg = silhouette_score(X, cluster_labels)\n            print(\"For n_clusters =\", n_clusters,\n                \"The average silhouette_score is :\", silhouette_avg)\n\n            # \u8a08\u7b97\u6240\u6709\u6a23\u672c\u7684 The silhouette_score\n            sample_silhouette_values = silhouette_samples(X, cluster_labels)\n            ```\n* **Day_57 : \u968e\u5c64\u5206\u7fa4\u6cd5**\n    * \u968e\u5c64\u5f0f\u5206\u6790 : \u4e00\u7a2e\u69cb\u5efa cluster \u7684\u5c64\u6b21\u7d50\u69cb\u7684\u7b97\u6cd5\u3002\u8a72\u7b97\u6cd5\u5f9e\u5206\u914d\u7d66\u81ea\u5df1 cluster \u7684\u6240\u6709\u8cc7\u6599\u9ede\u958b\u59cb\u3002\u7136\u5f8c\uff0c\uf978\u500b\u8ddd\u96e2\u6700\u8fd1\u7684 cluster \u5408\u4f75\u70ba\u540c\u4e00\u500b cluster\u3002\u6700\u5f8c\uff0c\u7576\u53ea\u5269\u4e0b\u4e00\u500b cluster \u6642\uff0c\u8a72\u7b97\u6cd5\u7d50\u675f\u3002\n    * K-means vs. \u968e\u5c64\u5206\u7fa4 :\n        * K-means : \u9700\u8981\u5b9a\u7fa9\u7fa4\u6578 (n of clusters)\n        * \u968e\u5c64\u5206\u7fa4 : \u53ef\u6839\u64da\u5b9a\u7fa9\u8ddd\u96e2\u4f86\u5206\u7fa4 (bottom-up)\uff0c\u4e5f\u53ef\u4ee5\u6c7a\u5b9a\u7fa4\u6578\u505a\u5206\u7fa4 (top-down)\n    * \u968e\u5c64\u5206\u7fa4\u6f14\u7b97\u6cd5\u6d41\u7a0b : \u4e0d\u6307\u5b9a\u5206\u7fa4\u6578\u91cf\n        1. \u6bcf\u7b46\u8cc7\u6599\u70ba\u4e00\u500b cluster\n        2. \u8a08\u7b97\u6bcf\uf978\u7fa4\u4e4b\u9593\u7684\u8ddd\u96e2\n        3. \u5c07\u6700\u8fd1\u7684\uf978\u7fa4\u5408\u4f75\u6210\u4e00\u7fa4\n        4. \u91cd\u8986\u6b65\u9a5f 2\u30013\uff0c\u76f4\u5230\u6240\u6709\u8cc7\u6599\u5408\u4f75\u6210\u540c\u4e00 cluster\n    * \u968e\u5c64\u5206\u7fa4\u8ddd\u96e2\u8a08\u7b97\u65b9\u5f0f :\n        * single-link : \u7fa4\u805a\u8207\u7fa4\u805a\u9593\u7684\u8ddd\u96e2\u53ef\u4ee5\u5b9a\u7fa9\u70ba\u4e0d\u540c\u7fa4\u805a\u4e2d\u6700\u63a5\u8fd1\uf978\u9ede\u9593\u7684\u8ddd\u96e2\u3002\n        * complete-link : \u7fa4\u805a\u9593\u7684\u8ddd\u96e2\u5b9a\u7fa9\u70ba\u4e0d\u540c\u7fa4\u805a\u4e2d\u6700\u9060\uf978\u9ede\u9593\u7684\u8ddd\u96e2\uff0c\u9019\u6a23\u53ef\u4ee5\u4fdd\u8b49\u9019\uf978\u500b\u96c6\u5408\u4f75\u5f8c\uff0c\u4efb\u4f55\u4e00\u5c0d\u7684\u8ddd\u96e2\u4e0d\u6703\u5927\u65bc d\u3002\n        * average-link : \u7fa4\u805a\u9593\u7684\u8ddd\u96e2\u5b9a\u7fa9\u70ba\u4e0d\u540c\u7fa4\u805a\u9593\u5404\u9ede\u8207\u5404\u9ede\u9593\u8ddd\u96e2\u7e3d\u548c\u7684\u5e73\u5747\u3002\n    * \u968e\u5c64\u5206\u7fa4\u7684\u512a\u52a3\u5206\u6790 :\n        * \u512a\u9ede : \u6982\u5ff5\u7c21\u55ae\uff0c\u6613\u65bc\u5448\u73fe\uff0c\u4e0d\u9700\u6307\u5b9a\u7fa4\u6578\n        * \u7f3a\u9ede : \u53ea\u9069\u7528\u65bc\u5c11\u91cf\u8cc7\u6599\uff0c\u5927\u91cf\u8cc7\u6599\u6703\u5f88\u96e3\u8655\u7406\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Hierarchical Clustering](https://www.youtube.com/watch?v=Tuuc9Y06tAc)\n        * [Example : Breast cancer Miroarray study](https://www.youtube.com/watch?v=yUJcTpWNY_o)\n        ```py\n        from sklearn.cluster import AgglomerativeClustering\n        from sklearn import datasets\n\n        iris = datasets.load_iris()\n        X = iris.data\n        y = iris.target\n\n        estimators = [('hc_iris_ward', AgglomerativeClustering(n_clusters=3, linkage=\"ward\")),\n                      ('hc_iris_complete', AgglomerativeClustering(n_clusters=3, linkage=\"complete\")),\n                      ('hc_iris_average', AgglomerativeClustering(n_clusters=3, linkage=\"average\"))]\n        \n        for name, est in estimators:\n            est.fit(X)\n            labels = est.labels_\n        ```\n* **Day_58 : \u968e\u5c64\u5206\u7fa4\u6cd5 \u89c0\u5bdf : 2D \u6a23\u7248\u8cc7\u6599\u96c6**\n    * \u8cc7\u6599\u96c6\u7528\u9014 : \u901a\u5e38\u9019\u6a23\u7684\u8cc7\u6599\u96c6\uff0c\u662f\u7528\uf92d\u8b93\u4eba\u773c\u8a55\u4f30\u975e\u76e3\u7763\u6a21\u578b\u7684\u597d\u58de\uff0c\u56e0\u70ba\u975e\u76e3\u7763\u6a21\u578b\u7684\u4efb\u52d9\u5305\u542b\u5206\u7fa4 (\u5c0d\u61c9\u65bc\u76e3\u7763\u7684\u5206\u985e) \u8207\u6d41\u5f62\u9084\u539f (\u5c0d\u61c9\u76e3\u7763\u7684\u8ff4\u6b78)\uff0c\u6240\u4ee5 2D \u6a23\u677f\u8cc7\u6599\u96c6\u5728\u8a2d\u8a08\u4e0a\u4e5f\u5305\u542b\u9019\uf978\u7a2e\u985e\u578b\u7684\u8cc7\u6599\u96c6\n    * sklearn \u7684 2D \u6a23\u7248\u8cc7\u6599\u96c6 : \n        * sklearn \u7684\u8cc7\u6599\u96c6\u4e3b\u8981\u5206\u70ba\uf978\u7a2e : \u8f09\u5165\u5f0f (Loaders) \u8207\u751f\u6210\u5f0f (Samples generator)\uff0c\u8f09\u5165\u5f0f\u7684\u662f\u56fa\u5b9a\u8cc7\u6599\uff0c\u751f\u6210\u5f0f\u7684\u5247\u662f\u5148\u6709\u65e2\u5b9a\u6a21\u5f0f\uff0c\u5728\u6a21\u5f0f\u4e0b\u6709\u9650\u5ea6\u7684\u96a8\u6a5f\u751f\u6210\u6bcf\u6b21\u4f7f\u7528\u7684\u8cc7\u6599\u96c6\n        * 2D \u6a23\u7248\u8cc7\u6599\u96c6\u5c6c\u65bc\u751f\u6210\u5f0f\u8cc7\u6599\u96c6\uff0c\u4f7f\u7528\u4e0d\u540c\u5206\u5e03\uff0c\u7528\u4ee5\u986f\u793a\u5404\u7a2e\u975e\u76e3\u7763\u6a21\u578b\u7684\u512a\u7f3a\u9ede\uff0c\u63d0\u4f9b\u4f7f\u7528\u8005\uf96b\u8003\n    * 2D \u6a23\u677f\u8cc7\u6599\u96c6\u5f88\u591a\u5957\u4ef6\u90fd\u6709\uff0c\u4e5f\u4e0d\u9650\u65bc\u53ea\u6709 Python \u4e0a\u4f7f\u7528\u7684\u5957\u4ef6 : \u5982 sklearn / mathworks / mlbench \u90fd\u6709\u5c0d\u61c9\u7684\u8cc7\u6599\u96c6\n        ```py\n        from sklearn import cluster, datasets\n        # \u8a2d\u5b9a 2D \u6a23\u677f\u8cc7\u6599\n        n_samples = 1500\n        random_state = 100\n\n        # \u751f\u6210 \u540c\u5fc3\u5713 \u8cc7\u6599\u9ede\n        noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n\n        # \u751f\u6210 \u659c\u5411\u4e09\u7fa4 \u8cc7\u6599\u9ede (\u4f7f\u7528\u8f49\u63db\u77e9\u9663)\n        X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n        transformation = [[0.6, -0.6], [-0.4, 0.8]]\n        X_aniso = np.dot(X, transformation)\n        aniso = (X_aniso, y)\n\n        # \u751f\u6210 \u7a00\u758f\u4e09\u7fa4 \u8cc7\u6599\u9ede\n        varied = datasets.make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)\n        \n        # \u751f\u6210 \u96d9\u534a\u6708 \u8cc7\u6599\u9ede\n        noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n\n        # \u751f\u6210 \u7dca\u5bc6\u4e09\u7fa4 \u8cc7\u6599\u9ede\n        blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n\n        # \u751f\u6210 2\u7dad\u5747\u52fb\u5206\u5e03 \u8cc7\u6599\u9ede\n        no_structure = np.random.rand(n_samples, 2), None\n        ```\n* **Day_59 : \u964d\u7dad\u65b9\u6cd5 - \u4e3b\u6210\u5206\u5206\u6790**\n    * \u70ba\u751a\u9ebc\u9700\u8981**\u964d\u4f4e\u7dad\u5ea6 (Dimension reduction)**\n        * \u6e1b\u5c11 RAM or disk space \u4f7f\u7528\uff0c\u6709\u52a9\u65bc\u52a0\u901f learning algorithm\n        * \u5f71\u50cf\u58d3\u7e2e : \u539f\u59cb\u5f71\u50cf\u7dad\u5ea6\u7232 512\uff0c\u5728\u964d\u4f4e\u7dad\u5ea6\u5230 16 \u7684\u60c5\u6cc1\u4e0b\uff0c\u5716\u7247\u96d6\u7136\u6709\u4e9b\u8a31\u6a21\u7cca\uff0c\u4f46\u4f9d\u7136\u4fdd\u6709\u660e\u986f\u7684\uf9d7\u5ed3\u548c\u7279\u5fb5\n        * \u58d3\u7e2e\u8cc7\u6599\u53ef\u9032\u800c\u7d44\u5408\u51fa\u65b0\u7684\u3001\u62bd\u8c61\u5316\u7684\u7279\u5fb5\uff0c\u6e1b\u5c11\u5197\u9918\u7684\u8cc7\u8a0a\n        * \u7279\u5fb5\u592a\u591a\u6642\uff0c\u5f88\u96e3 visualize data\uff0c\u4e0d\u5bb9\uf9e0\u89c0\u5bdf\u8cc7\u6599\u3002\u628a\u8cc7\u6599\u7dad\u5ea6 (\u7279\u5fb5) \u964d\u5230 2 \u5230 3 \u500b\uff0c\u5247\u80fd\u5920\u7528\u4e00\u822c\u7684 2D \u6216 3D \u5716\u8868\u5448\u73fe\u8cc7\u6599\n    * **\u4e3b\u6210\u5206\u5206\u6790 (PCA)** : \u900f\u904e\u8a08\u7b97 eigen value\uff0ceigen vector\uff0c\u53ef\u4ee5\u5c07\u539f\u672c\u7684 features \u964d\u7dad\u81f3\u7279\u5b9a\u7684\u7dad\u5ea6\n        * \u539f\u672c\u8cc7\u6599\u6709 100 \u500b features\uff0c\u900f\u904e PCA\uff0c\u53ef\u4ee5\u5c07\u9019 100 \u500b features \u964d\u6210 2 \u500b features\n        * \u65b0 features \u70ba\u820a features \u7684\u7dda\u6027\u7d44\u5408\uff0c\u4e14\u5f7c\u6b64\u4e0d\u76f8\u95dc\n        * \u5728\u7dad\u5ea6\u592a\u5927\u767c\u751f overfitting \u7684\u60c5\u6cc1\u4e0b\uff0c\u53ef\u4ee5\u5617\u8a66\u7528PCA \u7d44\u6210\u7684\u7279\u5fb5\uf92d\u505a\u76e3\u7763\u5f0f\u5b78\u7fd2\n        * \u4e0d\u5efa\u8b70\u5728\u65e9\u671f\u6642\u505a\uff0c\u5426\u5247\u53ef\u80fd\u6703\u4e1f\u5931\u91cd\u8981\u7684 features \u800c underfitting         \n        * \u53ef\u4ee5\u5728 optimization \u968e\u6bb5\u6642\uff0c\u8003\u616e PCA \uff0c\u4e26\u89c0\u5bdf\u904b\u7528\uf9ba PCA \u5f8c\u5c0d\u6e96\u78ba\u5ea6\u7684\u5f71\u97ff\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Unsupervised Learning](https://www.youtube.com/watch?v=ipyxSYXgzjQ)\n        * [Further Principal Components](https://www.youtube.com/watch?v=dbuSGWCgdzw)\n        * [Principal Components Regression](https://www.youtube.com/watch?v=eYxwWGJcOfw)\n        * [Andrew Ng - Dimensionality Reduction](https://www.youtube.com/watch?time_continue=1&v=rng04VJxUt4)\n        ```py\n        from sklearn import decomposition\n\n        pca = decomposition.PCA(n_components=3)\n        pca.fit(X)\n        X = pca.transform(X)\n        ```\n* **Day_60 : PCA \u89c0\u5bdf : \u4f7f\u7528\u624b\u5beb\u8fa8\u8b58\u8cc7\u6599\u96c6**\n    * \u624b\u5beb\u8fa8\u8b58\u8cc7\u6599\u96c6\u7684\uf92d\u6e90 : \u624b\u5beb\u8fa8\u8b58\u8cc7\u6599\u96c6 (MNIST, Modified National Institute of Standards and Technology databas) \u539f\u59cb\uf92d\u6e90\u7684 NIST\uff0c\u61c9\u8a72\u662f\uf92d\u81ea\u65bc\u7f8e\u570b\u4eba\u53e3\u666e\u67e5\u5c40\u7684\u54e1\u5de5\u4ee5\u53ca\u5b78\u751f\u624b\u5beb\u6240\u5f97\uff0c\u5176\u4e2d\u7684 Modified \u6307\u7684\u662f\u8cc7\u6599\u96c6\u70ba\uf9ba\u9069\u5408\u6a5f\u5668\u5b78\u7fd2\u505a\uf9ba\u4e00\u4e9b\u8abf\u6574 : \u5c07\u539f\u59cb\u5716\u6848\u4e00\uf9d8\u8f49\u6210\u9ed1\u5e95\u767d\u5b57\uff0c\u505a\uf9ba\u5c0d\u61c9\u7684\u6297\u92f8\u9f52\u7684\u8abf\u6574\uff0c\u6700\u5f8c\u5b58\u6210 28x28 \u7684\u7070\u968e\u5716\u6848\uff0c\u6210\u70ba\uf9ba\u76ee\u524d\u6700\u5e38\u807d\u5230\u7684\u57fa\u790e\u5f71\u50cf\u8cc7\u6599\u96c6\n    * sklearn \u4e2d\u7684\u624b\u5beb\u8fa8\u8b58\u8cc7\u6599\u96c6 : \u8207\u5b8c\u6574\u7684 MNIST \u4e0d\u540c\uff0csklearn \u70ba\uf9ba\u65b9\uf965\u975e\u6df1\u5ea6\u5b78\u7fd2\u7684\u8a08\u7b97\uff0c\u518d\u4e00\u6b21\u5c07\u5716\u7247\u7684\u5927\u5c0f\u58d3\u7e2e\u5230 8x8 \u7684\u5927\u5c0f\uff0c\u96d6\u7136\u4ecd\u662f\u7070\u968e\uff0c\u4f46\u5c31\u5f62\uf9fa\u4e0a\u5df2\u7d93\u6709\u9ede\u96e3\u4ee5\u7528\u8089\u773c\u8fa8\u8b58\uff0c\u4f46\u58d3\u7e2e\u5230\u5982\u6b64\u5927\u5c0f\u6642\uff0c\u6bcf\u5f35\u624b\u5beb\u5716\u5c31\u53ef\u4ee5\u7576\u4f5c 64 (8x8=64) \u500b\u7279\u5fb5\u7684\u4e00\u7b46\u8cc7\u6599\uff0c\u642d\u914d\u4e00\u822c\u7684\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u505a\u51fa\u5b78\u7fd2\u8207\u9810\u6e2c\n    * \u70ba\uf9fd\u9ebc\u6311 MNIST \u6aa2\u9a57 PCA \u7684\u964d\u7dad\u6548\u679c :\n        * \u9ad8\u7dad\u5ea6\u3001\u9ad8\u8907\u96dc\u6027\u3001\u4eba\u53ef\uf9e4\u89e3\u7684\u8cc7\u6599\u96c6\n        * \u7531\u65bc PCA \u7684\u5f37\u5927\uff0c\u5982\u679c\u8cc7\u6599\u6709\u610f\u7fa9\u7684\u7dad\u5ea6\u592a\u4f4e\uff0c\u5247\u524d\u5e7e\u500b\u4e3b\u6210\u5206\u5c31\u53ef\u4ee5\u5c07\u8cc7\u6599\u89e3\u91cb\u5b8c\u7562\n        * \u4f7f\u7528\u4e00\u822c\u5716\u5f62\u8cc7\u6599\uff0c\u7dad\u5ea6\u53c8\u6703\u592a\u9ad8\uff0c\u56e0\u6b64\u6211\u5011\u4f7f\u7528 sklearn \u7248\u672c\u7684 MNIST \u6aa2\u9a57 PCA\uff0c\u4ee5\u517c\u9867\u5167\u5bb9\u7684\u8907\u96dc\u6027\u8207\u53ef\uf9e4\u89e3\u6027\n        ```py\n        # \u8f09\u5165\u5957\u4ef6\n        import numpy as np\n        import matplotlib.pyplot as plt\n        import pandas as pd\n\n        from sklearn import datasets\n        from sklearn.decomposition import PCA\n        from sklearn.linear_model import SGDClassifier\n        from sklearn.pipeline import Pipeline\n        from sklearn.model_selection import GridSearchCV\n        import warnings\n        warnings.filterwarnings(\"ignore\")\n\n        # \u5b9a\u7fa9 PCA \u8207\u96a8\u5f8c\u7684\u908f\u8f2f\u65af\u8ff4\u6b78\u51fd\u6578\n        logistic = SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5, random_state=0)\n        pca = PCA()\n        pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n\n        # \u8f09\u5165\u624b\u5beb\u6578\u5b57\u8fa8\u8b58\u96c6\n        digits = datasets.load_digits()\n        X_digits = digits.data\n        y_digits = digits.target\n\n        # \u5148\u57f7\u884c GridSearchCV \u8dd1\u51fa\u6700\u4f73\u53c3\u6578\n        param_grid = {\n            'pca__n_components': [4, 10, 20, 30, 40, 50, 64],\n            'logistic__alpha': np.logspace(-4, 4, 5),\n        }\n        search = GridSearchCV(pipe, param_grid, iid=False, cv=5, return_train_score=False)\n        search.fit(X_digits, y_digits)\n        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n        print(search.best_params_)\n\n        # \u7e6a\u88fd\u4e0d\u540c components \u7684 PCA \u89e3\u91cb\u5ea6\n        pca.fit(X_digits)\n\n        fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n        ax0.plot(pca.explained_variance_ratio_, linewidth=2)\n        ax0.set_ylabel('PCA explained variance')\n\n        ax0.axvline(search.best_estimator_.named_steps['pca'].n_components, linestyle=':', label='n_components chosen')\n        ax0.legend(prop=dict(size=12))\n\n        # \u7e6a\u88fd\u4e0d\u540c\u63a1\u6a23\u9ede\u7684\u5206\u985e\u6b63\u78ba\u7387\n        results = pd.DataFrame(search.cv_results_)\n        components_col = 'param_pca__n_components'\n        best_clfs = results.groupby(components_col).apply(lambda g: g.nlargest(1, 'mean_test_score'))\n\n        best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score', legend=False, ax=ax1)\n        ax1.set_ylabel('Classification accuracy (val)')\n        ax1.set_xlabel('n_components')\n        plt.tight_layout()\n        plt.show()\n        ```\n* **Day_61 : \u964d\u7dad\u65b9\u6cd5 - t-SNE**\n    * PCA \u7684\u554f\u984c\n        * \u6c42\u5171\u8b8a\uf962\u6578\u77e9\u9663\u9032\ufa08\u5947\uf962\u503c\u5206\u89e3\uff0c\u56e0\u6b64\u6703\u88ab\u8cc7\u6599\u7684\u5dee\uf962\u6027\u5f71\u97ff\uff0c\u7121\u6cd5\u5f88\u597d\u7684\u8868\u73fe\u76f8\u4f3c\u6027\u53ca\u5206\u4f48\u3002\n        * PCA \u662f\u4e00\u7a2e\u7dda\u6027\u964d\u7dad\u65b9\u5f0f\uff0c\u7279\u5fb5\u9593\u7232\u975e\u7dda\u6027\u95dc\u4fc2\u6642 (e.g. \u6587\u5b57\u3001\u5f71\u50cf\u8cc7\u6599)\uff0cPCA \u5f88\u5bb9\uf9e0 underfitting\n    * t-SNE\n        * t-SNE \u4e5f\u662f\u4e00\u7a2e\u964d\u7dad\u65b9\u5f0f\uff0c\u4f46\u5b83\u7528\uf9ba\uf901\u8907\u96dc\u7684\u516c\u5f0f\uf92d\u8868\u9054\u9ad8\u7dad\u548c\u4f4e\u7dad\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\n        * \u4e3b\u8981\u662f\u5c07\u9ad8\u7dad\u7684\u8cc7\u6599\u7528 gaussian distribution \u7684\u6a5f\u7387\u5bc6\u5ea6\u51fd\u6578\u8fd1\u4f3c\uff0c\u800c\u4f4e\u7dad\u8cc7\u6599\u7684\u90e8\u5206\u7528 t \u5206\u4f48\u4f86\uf92d\u8fd1\u4f3c\uff0c\u5728\u7528 KL divergence \u8a08\u7b97\u76f8\u4f3c\u5ea6\uff0c\u518d\u4ee5\u68af\u5ea6\u4e0b\u964d (gradient descent) \u6c42\u6700\u4f73\u89e3\u3002\n        * t-SNE \u512a\u52a3\n            * \u512a\u9ede : \n                * \u7576\u7279\u6578\uf97e\u904e\u591a\u6642\uff0c\u4f7f\u7528 PCA \u53ef\u80fd\u6703\u9020\u6210\u964d\u7dad\u5f8c\u7684 underfitting\uff0c\u9019\u6642\u53ef\u4ee5\u8003\u616e\u4f7f\u7528 t-SNE \uf92d\u964d\u7dad\uff0c\n                * t-SNE \u5c0d\u65bc\u7279\u5fb5\u975e\u7dda\u6027\u8cc7\u6599\u6709\uf901\u597d\u7684\u964d\u7dad\u5448\u73fe\u80fd\u529b\n            * \u7f3a\u9ede : \n                * t-SNE \u7684\u9700\u8981\u6bd4\u8f03\u591a\u7684\u6642\u9593\u57f7\ufa08\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [visualing data use t-SNE](https://www.youtube.com/watch?v=RJVL80Gg3lA)\n        * [\u674e\u5b8f\u6bc5 - Unsupervised learning](https://www.youtube.com/watch?v=GBUEjkpoxXc)\n        ```py\n        from sklearn import manifold\n\n        tsne = manifold.TSNE(n_components=2, random_state=0, init='pca', learning_rate=200., early_exaggeration=12.)\n        X_tsne = tsne.fit_transform(X)\n        ```\n* **Day_62 : t-sne \u89c0\u5bdf : \u5206\u7fa4\u8207\u6d41\u5f62\u9084\u539f**\n    * \u6d41\u5f62\u9084\u539f\u7684\u610f\u7fa9\n        * \u6d41\u5f62\u9084\u539f\u5c31\u662f\u5c07\u9ad8\u7dad\u5ea6\u4e0a\u76f8\u8fd1\u7684\u9ede\uff0c\u5c0d\u61c9\u5230\u4f4e\u7dad\u5ea6\u4e0a\u76f8\u8fd1\u7684\u9ede\uff0c\u6c92\u6709\u8cc7\u6599\u9ede\u7684\u5730\u65b9\u4e0d\uf99c\u5165\u8003\uf97e\u7bc4\u570d\n        * \u7c21\u55ae\u7684\u8aaa\uff0c\u5982\u679c\u8cc7\u6599\u7d50\u69cb\u50cf\u745e\u58eb\u6372\u4e00\u6a23\uff0c\u90a3\u9ebc\u6d41\u5f62\u9084\u539f\u5c31\u662f\u628a\u5b83\u6524\u958b\u92ea\u5e73 (\u6d41\u5f62\u9084\u539f\u8cc7\u6599\u96c6\u7684\u5176\u4e2d\u4e00\u7a2e\uff0c\u5c31\u662f\u53eb\u505a\u745e\u58eb\u6372 - Swiss Roll)\n        * \u5176\u5be6\u6d41\u5f62\u9084\u539f\u7684\u6a21\u578b\u76f8\u7576\u591a\u7a2e\uff0c\u53ea\u662f\u61c9\u7528\u4e0a\u8f03\u5c11\uff0c\u5e38\ufa0a\u7684\u9664\u4e86\uf9ba t-sne \u4e4b\u5916\uff0c\u9084\u6709 Isomap / LLE / MDS \u7b49\u7b49\uff0c\u56e0\u70ba\u5be6\u7528\u5ea6\u4e0d\u9ad8\uff0c\u4e4b\u5f8c\u7684\u8ab2\u7a0b\u4e2d\u6211\u5011\u4e5f\u4e0d\u6703\u6559\uff0c\u56e0\u6b64\u53ea\u5728\u6b64\u5c55\u793a\u5e7e\u7a2e\u6d41\u5f62\u9084\u539f\u7684\u7d50\u679c\u5716\n        ```py\n        from sklearn import manifold\n        from time import time\n\n        n_components = 2\n        perplexities = [4, 6, 9, 14, 21, 30, 45, 66, 100]\n        \n        for i, perplexity in enumerate(perplexities):\n            t0 = time()\n            tsne = manifold.TSNE(n_components=n_components, init='random',\n                                random_state=0, perplexity=perplexity)\n            Y = tsne.fit_transform(X)\n            t1 = time()\n            # perplexity \u8d8a\u9ad8\uff0c\u6642\u9593\u8d8a\u9577\uff0c\u6548\u679c\u8d8a\u597d\n            print(\"S-curve, perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))\n        ```\n### \u6df1\u5ea6\u5b78\u7fd2\u7406\u8ad6\u8207\u5be6\u4f5c\n* **Day_63 : \u6df1\u5ea6\u5b78\u7fd2\u7c21\u4ecb**\n    * \u985e\u795e\u7d93\u7db2\uf937 (Neural Network)\n        * \u57281956\uf98e\u7684\u9054\u7279\u8305\u65af\u6703\u8b70\u4e2d\u8a95\u751f\uff0c\u4ee5\u6578\u5b78\u6a21\u64ec\u795e\u7d93\u50b3\u5c0e\u8f38\u51fa\u9810\u6e2c\uff0c\u5728\u521d\u671f\u4eba\u5de5\u667a\u6167\u9818\u57df\u4e2d\u5c31\u662f\u91cd\u8981\u5206\u652f\n        * \u56e0\u5c64\u6578\u4e00\u591a\u8a08\u7b97\uf97e\u5c31\u5927\u5e45\u589e\u52a0\u7b49\u554f\u984c\uff0c\u904e\u53bb\u7121\u6cd5\u89e3\u6c7a\uff0c\u96d6\u4e0d\u65b7\u6709\u5b78\u8005\u8a66\u5716\u6539\u5584\uff0c\u5728\u6b77\u53f2\u4e2d\u4ecd\u4e0d\u514d\u5927\u8d77\u5927\uf918\n        * \u76f4\u5230\u8fd1\u5e7e\uf98e\u5728**\u7b97\u6cd5**\u3001**\u786c\u9ad4\u80fd\uf98a**\u8207**\u5de8\u91cf\u8cc7\u6599**\u7684\u6539\u5584\u4e0b\uff0c\u591a\u5c64\u7684\u985e\u795e\u7d93\u7db2\uf937\u624d\u91cd\u65b0\u6210\u70ba\u7576\u524d\u4eba\u5de5\u667a\u6167\u7684\u61c9\u7528\u4e3b\u6d41\n    * \u985e\u795e\u7d93\u7db2\uf937\u8207\u6df1\u5ea6\u5b78\u7fd2\u7684\u6bd4\u8f03\n        * \u5c31\u57fa\u790e\u8981\u7d20\u800c\u8a00\uff0c\u6df1\u5ea6\u5b78\u7fd2\u662f**\u6bd4\u8f03\u591a\u5c64**\u7684\u985e\u795e\u7d93\u7db2\uf937\n        * \u4f46\u5c31\u5be6\u52d9\u61c9\u7528\u7684\u5c64\u6b21\u4e0a\uff0c\u56e0\u8457**\u8a2d\u8a08\u601d\u8def**\u8207**\u9023\u7d50\u67b6\u69cb**\u7684\u4e0d\u540c\uff0c\uf978\u8005\u6709\uf9ba\u5f88\u5927\u7684\u5dee\uf962\u6027\n\n        |       | \u985e\u795e\u7d93\u7db2\u8def<br>(Neural Network)                        | \u6df1\u5ea6\u5b78\u7fd2<br>(Deep Learning)     |\n        |-------|:-------------------------------------------------------|-----------------------------------|\n        | \u96b1\u85cf\u5c64\u6578\u91cf | 1~2\u5c64                                           | \u5341\u6578\u5c64\u5230\u767e\u5c64\u4ee5\u4e0a\u4e0d\u7b49                 |\n        | \u6d3b\u8e8d\u5e74\u4ee3  | 1956~1974                                      | 2011\u2f84\u81f3\u4eca                    |\n        | \u4ee3\u8868\u7d50\u69cb  | \u611f\u77e5\u5668 (Perceptron)<br>\u555f\u52d5\u51fd\u6578 (Activation Function) | \u5377\u7a4d\u795e\u7d93\u7db2\uf937(CNN)<br>\u905e\u6b78\u795e\u7d93\u7db2\uf937(RNN) |\n        | \u89e3\u6c7a\u554f\u984c  | \u57fa\u790e\u8ff4\u6b78\u554f\u984c                                         | \u5f71\u50cf\u3001\u81ea\u7136\u8a9e\u8a00\u8655\uf9e4\u7b49\u591a\u6a23\u554f\u984c|\n    * \u6df1\u5ea6\u5b78\u7fd2\u61c9\u7528\u7206\u767c\u7684\u4e09\u5927\u95dc\u9375\n        * \u985e\u795e\u7d93\u7684\u61c9\u7528\u66fe\u6c89\u5bc2\u4e8c\u4e09\u5341\uf98e\uff0c\u76f4\u5230 2012 \uf98e AlexNet \u5728 ImageNet \u5716\u50cf\u5206\u985e\u7af6\u8cfd\u7372\u5f97\u9a5a\u8277\u8868\u73fe\u5f8c\uff0c\u624d\u91cd\u56de\u4e3b\u6d41\u821e\u53f0\n        * \u6df1\u5ea6\u5b78\u7fd2\u76f8\u6bd4\u65bc\u904e\u53bb\uff0c\u5230\u5e95\u6709\u54ea\u4e9b\u95dc\u9375\u512a\u52e2\u5462\n            * \u7b97\u6cd5\u6539\u826f\n                * \u7db2\u8def\u7d50\u69cb\uff1a**CNN \u8207 RNN** \u7b49\u7d50\u69cb\u5728\u795e\u7d93\u9023\u7d50\u4e0a\u505a\u6709\u610f\u7fa9\u7684\u7cbe\u7701\uff0c\u4f7f\u5f97\u8a08\u7b97\uf98a\u5f97\u4ee5\u7528\u5728\u5200\u53e3\u4e0a\n                * \u7d30\u7bc0\u6539\u826f\uff1a**DropOut (\u96a8\u6a5f\u79fb\u9664)** \u540c\u6642\u6709\u7bc0\u7701\u9023\u7d50\u8207\u96c6\u6210\u7684\u6548\u679c\uff0c**BatchNormalization (\u6279\u6b21\u6b63\u898f\u5316)** \u8b93\u795e\u7d93\u5c64\u9593\u6709\uf901\u597d\u7684\u50b3\u5c0e\uf98a\n            * \u8a08\u7b97\u6a5f\u786c\u9ad4\u80fd\uf98a\u63d0\u5347\n                * **\u5716\u5f62\u8655\uf9e4\u5668 (GPU)** \u7684\u8a95\u751f\uff0c\u6301\u7e8c\uf9ba\u6676\u7247\u6469\u723e\u5b9a\uf9d8\uff0c\u8b93\u8a08\u7b97\u6210\u70ba\u53ef\ufa08\n            * \u5de8\uf97e\u8cc7\u6599\n                * \u500b\u4eba\ufa08\u52d5\u88dd\u7f6e\u7684\u666e\u53ca\u7db2\uf937\u901f\u5ea6\u7684\u6301\u7e8c\u63d0\u5347\uff0c\u5e36\uf92d\u5de8\uf97e\u7684\u8cc7\u6599\uf97e\uff0c\u4f7f\u5f97\u6df1\u5ea6\u5b78\u7fd2\u6709\uf9ba\u53ef\u4ee5\u5b78\u7fd2\u7684\u7d20\u6750\n    * **\u5377\u7a4d\u795e\u7d93\u7db2\uf937** (CNN, Convolutional Neural Network)\n        * \u8a2d\u8a08\u76ee\u6a19\uff1a\u5f71\u50cf\u8655\uf9e4\n        * \u7d50\u69cb\u6539\u9032\uff1aCNN \uf96b\u8003\u50cf\u7d20\u9060\u8fd1\u7701\uf976\u795e\u7d93\u5143\uff0c\u4e26\u4e14\u7528\u5f71\u50cf\u7279\u5fb5\u7684\u5e73\u79fb\u4e0d\u8b8a\u6027\uf92d\u5171\u7528\u6b0a\u91cd\uff0c\u5927\u5e45\u6e1b\u5c11\uf9ba\u5f71\u50cf\u8a08\u7b97\u7684\u8ca0\u64d4\n        * \u884d\u4f38\u61c9\u7528\uff1a\u53ea\u8981\u7b26\u5408\u4e0a\u8ff0\uf978\u7a2e\u7279\u6027\u7684\u61c9\u7528\uff0c\u90fd\u53ef\u4ee5\u4f7f\u7528 CNN \u4f86\uf92d\u8a08\u7b97\uff0c\uf9b5\u5982 AlphaGo \u7684 v18 \u7248\u7684\uf978\u500b\u4e3b\u7db2\uf937\u90fd\u662f CNN \u5716\u7247\n    * **\u905e\u6b78\u795e\u7d93\u7db2\uf937** (RNN, Recurrent Neural Network)\n        * \u8a2d\u8a08\u76ee\u6a19\uff1a\u6642\u5e8f\u8cc7\u6599\u8655\uf9e4\n        * \u7d50\u69cb\u6539\u9032\uff1aRNN \u96d6\u7136\u770b\u4f3c\u5728 NN \u5916\u589e\u52a0\uf9ba\u6642\u5e8f\u9593\u7684\u6a6b\u5411\u50b3\u905e\uff0c\u4f46\u5be6\u969b\u4e0a\u9084\u662f\u4f9d\u7167\u6642\u9593\u9060\u8fd1\u7701\uf976\uf9ba\u90e8\u5206\u9023\u7d50\n        * \u884d\u4f38\u61c9\u7528\uff1a\u53ea\u8981\u8cc7\u6599\u662f\u6709**\u9806\u5e8f\u6027**\u7684\u61c9\u7528\uff0c\u90fd\u53ef\u4ee5\u4f7f\u7528 RNN \uf92d\u8a08\u7b97\uff0c\u8fd1\uf98e\u5728**\u81ea\u7136\u8a9e\u8a00\u8655\uf9e4 (NLP)** \u4e0a\u7684\u61c9\u7528\u53cd\u800c\u6210\u70ba\u5927\u5b97\n    * \u6df1\u5ea6\u5b78\u7fd2 - \u5de8\u89c0\u7d50\u69cb\n        * **\u8f38\u5165\u5c64 (input layer)**\uff1a\u8f38\u5165\u8cc7\u6599\u9032\u5165\u7684\u4f4d\u7f6e\n        * **\u8f38\u51fa\u5c64 (hidden layer)**\uff1a\u8f38\u51fa\u9810\u6e2c\u503c\u7684\u6700\u5f8c\u4e00\u5c64\n        * **\u96b1\u85cf\u5c64 (output layer)**\uff1a\u9664\uf9ba\u4e0a\u8ff0\uf978\u5c64\u5916\uff0c\u5176\u4ed6\u5c64\u90fd\u7a31\u70ba\u96b1\u85cf\n    * \u6df1\u5ea6\u5b78\u7fd2 - \u5fae\u89c0\u7d50\u69cb\n        * **\u555f\u52d5\u51fd\u6578 (Activation Function)**\uff1a\u4f4d\u65bc\u795e\u7d93\u5143\u5167\u90e8\uff0c\u5c07\u4e0a\u4e00\u5c64\u795e\u7d93\u5143\u7684\u8f38\u5165\u7e3d\u548c\uff0c\u8f49\u63db\u6210\u9019\u4e00\u500b\u795e\u7d93\u5143\u8f38\u51fa\u503c\u7684\u51fd\u6578\n        * **\u640d\u5931\u51fd\u6578 (Loss Function)**\uff1a\u5b9a\u7fa9\u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684\u8aa4\u5dee\u5927\u5c0f\n        * **\u5012\u50b3\u905e (Back-Propagation)**\uff1a\u5c07\u640d\u5931\u503c\uff0c\u8f49\u63db\u6210\u985e\u795e\u7d93\u6b0a\u91cd\uf901\u65b0\u7684\u65b9\u6cd5\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u2f08\u5de5\u667a\u6167\u5927\u6b77\u53f2](https://medium.com/@suipichen/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7%E5%A4%A7%E6%AD%B7%E5%8F%B2-ffe46a350543)\n        * [3 \u5206\u9418\u641e\u61c2\u6df1\u5ea6\u5b78\u7fd2\u5230\u5e95\u5728\u6df1\uf9fd\u9ebc](https://panx.asia/archives/53209)\n* **Day_64 : \u6df1\u5ea6\u5b78\u7fd2\u9ad4\u9a57 - \u6a21\u578b\u8abf\u6574\u8207\u5b78\u7fd2\u66f2\u7dda**\n    * \u6df1\u5ea6\u5b78\u7fd2\u9ad4\u9a57\u5e73\u53f0\uff1a[TensorFlowPlayGround](https://playground.tensorflow.org/)\n        * TensorFlow PlayGround \u662f Google \u7cbe\u5fc3\u958b\u767c\u7684\u9ad4\u9a57\u7db2\u2fb4\u9801\uff0c\u63d0\u4f9b\u5b78\u7fd2\u8005\u5728\u63a5\u89f8\u8a9e\u8a00\u4e4b\u524d\uff0c\u5c31\u53ef\u4ee5\u5c0d\u6df1\u5ea6\u5b78\u7fd2\u80fd\u6982\uf976\uf9ba\n    * \u5e73\u53f0\u4e0a\u76ee\u524d\u6709 4 \u500b\u5206\u985e\u554f\u984c\u8207 2 \u500b\u8ff4\u6b78\u554f\u984c\n    * \u7df4\u7fd2 1\uff1a\u6309\u4e0b\u555f\u52d5\uff0c\u89c0\u5bdf\u6307\u6a19\u8b8a\u5316\n        * \u5168\u90e8\u4f7f\u7528\u9810\u8a2d\u503c\uff0c\u6309\u4e0b\u555f\u52d5\u6309\u9215\uff0c\u770b\u767c\u751f\uf9ba\uf9fd\u9ebc\u8b8a\u5316?\n        * \u905e\u8ff4\u6b21\u6578\uff08Epoch\uff0c\u5de6\u4e0a\uff09\uff1a\u9010\u6f38\u589e\u52a0\n        * \u795e\u7d93\u5143\uff08\u4e2d\u592e\uff09\uff1a\u2f45\u65b9\u6846\u5716\u6848\u9010\u6f38\u660e\u986f\uff0c\u6b0a\u91cd\u9010\u6f38\u52a0\u7c97\uff0c\u6ed1\u9f20\u79fb\u81f3\u4e0a\u65b9\u6703\u986f\u793a\u6b0a\u91cd\n        * \u8a13\u7df4/\u6e2c\u8a66\u8aa4\u5dee\uff1a\u958b\u59cb\u6642\u660e\u986f\u4e0b\u964d\uff0c\u5e45\u5ea6\u6f38\u6f38\u8da8\u7de9\n        * \u5b78\u7fd2\u66f2\u7dda\uff1a\u8a13\u7df4/\u6e2c\u8a66\u8aa4\u5dee\n        * \u7d50\u679c\u5716\u50cf\u5316\uff1a\u5716\u50cf\u9010\u6f38\u7a69\u5b9a\u5f8c\u7e8c\u8a0e\u8ad6\u89c0\u5bdf\uff0c\u5982\u679c\u6c92\u6709\u7279\u5225\u8a3b\u660e\uff0c\u5747\u4ee5\u8a13\u7df4/\u6e2c\u8a66\u8aa4\u5dee\u662f\u5426\u8da8\u8fd1 0 \u70ba\u4e3b\uff0c\u9019\u7a2e\u60c5\u6cc1\u6211\u5011\u5e38\u7a31\u70ba**\u6536\u6582**\n    * \u7df4\u7fd2 2\uff1a\u589e\u6e1b\u96b1\u85cf\u5c64\u6578\n        * \u7df4\u7fd2\u64cd\u4f5c\n            * \u8cc7\u6599\u96c6\u5207\u63db\uff1a\u5206\u985e\u8cc7\u6599\u96c6(\u5de6\u4e0b) - 2 \u7fa4\uff0c\u8abf\u6574\u5c64\u6578\u5f8c\u555f\u52d5\u5b78\u7fd2\n            * \u8cc7\u6599\u96c6\u5207\u63db\uff1a\u5206\u985e\u8cc7\u6599\u96c6(\u5de6\u4e0a) - \u540c\u5fc3\u5713\uff0c\u8abf\u6574\u5c64\u6578\u5f8c\u555f\u52d5\u5b78\u7fd2\n            * \u8cc7\u6599\u96c6\u5207\u63db\uff1a\u8ff4\u6b78\u8cc7\u6599\u96c6(\u5de6) - \u5c0d\u2f93\u89d2\u7dda\uff0c\u8abf\u6574\u5c64\u6578\u5f8c\u555f\u52d5\u5b78\u7fd2\n        * \u5be6\u9a57\u7d50\u679c\n            * 2 \u7fa4\u8207\u5c0d\u89d2\u7dda\uff1a\u56e0\u8cc7\u6599\u96c6\u7d50\u69cb\u7c21\u55ae\uff0c\u5373\u4f7f\u6c92\u6709\u96b1\u85cf\u5c64\u4e5f\u6703\u6536\u6582\n            * \u540c\u5fc3\u5713\uff1a\u8cc7\u6599\u96c6\u7a0d\u5fae\u8907\u96dc (\u7121\u6cd5\u7dda\u6027\u5206\u5272)\uff0c\u56e0\u6b64\u6700\u5c11\u8981\u2f00\u4e00\u5c64\u96b1\u85cf\u5c64\u624d\u6703\u6536\u6582\n    * \u7df4\u7fd2 3\uff1a\u589e\u6e1b\u795e\u7d93\u5143\u6578\n        * \u7df4\u7fd2\u64cd\u4f5c\n            * \u8cc7\u6599\u96c6\u5207\u63db\uff1a\u5206\u985e\u8cc7\u6599\u96c6(\u5de6\u4e0a)-\u540c\u5fc3\u5713\uff0c\u96b1\u85cf\u5c64\u8a2d\u70ba 1 \u5f8c\u555f\u52d5\u5b78\u7fd2\n            * \u5207\u63db\u4e0d\u540c\u96b1\u85cf\u5c64\u795e\u7d93\u5143\u6578\uf97e\u5f8c\uff0c\u770b\u770b\u5b78\u7fd2\u6548\u679c\u6709\u4f55\u4e0d\u540c\uff1f\n        * \u5be6\u9a57\u7d50\u679c\n            * \u7576\u795e\u7d93\u5143\u5c11\u65bc\u7b49\u65bc\uf978\u500b\u4ee5\u4e0b\u6642\uff0c\u5c07\u7121\u6cd5\u6536\u6582\n    * \u7df4\u7fd2 4\uff1a\u5207\u63db\u4e0d\u540c\u7279\u5fb5\n        * \u7df4\u7fd2\u64cd\u4f5c\n            * \u8cc7\u6599\u96c6\u5207\u63db\uff1a\u5206\u985e\u8cc7\u6599\u96c6(\u5de6\u4e0a) - \u540c\u5fc3\u5713\uff0c\u96b1\u85cf\u5c64 1 \u5c64\uff0c\u96b1\u85cf\u795e\u7d93\u5143 2 \u500b\n            * \u5207\u63db\u4efb\u9078\u4e0d\u540c\u7684 2 \u500b\u7279\u5fb5\u5f8c\u555f\u52d5\uff0c\u770b\u770b\u5b78\u7fd2\u6548\u679c\u6709\u4f55\u4e0d\u540c?\n        * \u5be6\u9a57\u7d50\u679c\n            * \u7576\u7279\u5fb5\u9078\u5230\uf978\u500b\u7279\u5fb5\u7684\u5e73\u65b9\u6642\uff0c\u5373\u4f7f\u4e2d\u9593\u53ea\u6709 2 \u500b\u795e\u7d93\u5143\u4e5f\u6703\u6536\u6582\n    * \u77e5\u8b58\u8981\u9ede\n        * \u96d6\u7136\u5716\u50cf\u5316\uf901\u76f4\u89ba\uff0c\u4f46\u662f\u4e26\u975e\uf97e\u5316\u6307\u6a19\u4e14\u53ef\u8996\u5316\u4e0d\u5bb9\uf9e0\uff0c\u6545\u6df1\u5ea6\u5b78\u7fd2\u7684\u89c0\u5bdf\u6307\u6a19\u4ecd\u4ee5**\u640d\u5931\u51fd\u6578/\u8aa4\u5dee**\u70ba\u4e3b\n        * \u5c0d\u65bc\u4e0d\u540c\u8cc7\u6599\u985e\u578b\uff0c\u9069\u5408\u52a0\u6df1\u8207\u52a0\u5bec\u7684\u554f\u984c\u90fd\u6709\uff0c\u4f46**\u52a0\u6df1**\u9069\u5408\u7684\u554f\u984c\u985e\u578b\u8f03\u591a\n        * \u8f38\u5165\u7279\u5fb5\u7684\u9078\u64c7\u5f71\u97ff\u7d50\u679c\u751a\u9245\uff0c\u56e0\u6b64\u6df1\u5ea6\u5b78\u7fd2\u4e5f\u9700\u8981\u8003\u616e**\u7279\u5fb5\u5de5\u7a0b**\n* **Day_65 : \u6df1\u5ea6\u5b78\u7fd2\u9ad4\u9a57 - \u555f\u52d5\u51fd\u6578\u8207\u6b63\u898f\u5316**\n    * \u7df4\u7fd2 5\uff1a\u5207\u63db\u6279\u6b21\u5927\u5c0f\n        * \u7df4\u7fd2\u64cd\u4f5c\n            * \u8cc7\u6599\u96c6\u5207\u63db : \u5206\u985e\u8cc7\u6599\u96c6(\u53f3\u4e0b) - \u87ba\u65cb\u96d9\u81c2\uff0c\u7279\u5fb5\u5168\u9078\uff0c\u96b1\u85cf\u5c641\u5c64 / 8\u795e\u7d93\u5143\n            * \u8abf\u6574\u4e0d\u540c\u7684\u6279\u6b21\u5927\u5c0f\u5f8c\u57f7\ufa08 500 \u6b21\u905e\u8ff4\uff0c\u770b\u770b\u5b78\u7fd2\u6548\u679c\u6709\u4f55\u4e0d\u540c?\n        * \u5be6\u9a57\u7d50\u679c\n            * \u6279\u6b21\u5927\u5c0f\u5f88\u5c0f\u6642\uff0c\u96d6\u7136\u6536\u6582\u904e\u7a0b\u975e\u5e38\u4e0d\u7a69\u5b9a\uff0c\u4f46\u5e73\u5747\u800c\u8a00\u6703\u6536\u6582\u5230\u8f03\u597d\u7684\u7d50\u679c\n            * \u5be6\u52d9\u4e0a\uff0c\u6279\u6b21\u5927\u5c0f\u5982\u679c\u6975\u5c0f\uff0c\u6548\u679c\u78ba\u5be6\u6bd4\u8f03\u597d\uff0c\u4f46\u8a08\u7b97\u6642\u9593\u6703\u76f8\u7576\u4e45\uff0c\u56e0\u6b64\u901a\u5e38\u6703\u4f9d\u7167\u6642\u9593\u9700\u8981\u800c\u6298\u8877\n    * \u7df4\u7fd2 6\uff1a\u5207\u63db\u5b78\u7fd2\u901f\u7387\n        * \u7df4\u7fd2\u64cd\u4f5c\n            * \u8cc7\u6599\u96c6\u5207\u63db\uff1a\u5206\u985e\u8cc7\u6599\u96c6(\u53f3\u4e0b) - \u87ba\u65cb\u96d9\u81c2\uff0c\u7279\u5fb5\u5168\u9078\uff0c\u96b1\u85cf\u5c641\u5c64 / 8\u795e\u7d93\u5143\uff0c\u6279\u6b21\u5927\u5c0f\u56fa\u5b9a 10\n            * \u8abf\u6574\u4e0d\u540c\u7684\u5b78\u7fd2\u901f\u7387\u5f8c\u57f7\ufa08 500 \u6b21\u905e\u8ff4\uff0c\u770b\u770b\u5b78\u7fd2\u6548\u679c\u6709\u4f55\u4e0d\u540c?\n        * \u5be6\u9a57\u7d50\u679c\n            * \u5c0f\u65bc 0.3 \u6642\u5b78\u7fd2\u901f\u7387\u8f03\u5927\u6642\uff0c\u6536\u6582\u904e\u7a0b\u6703\u8d8a\u4e0d\u7a69\u5b9a\uff0c\u4f46\u6703\u6536\u6582\u5230\u8f03\u597d\u7684\u7d50\u679c\n            * \u5927\u65bc 1 \u6642\u56e0\u70ba\u904e\u5ea6\u4e0d\u7a69\u5b9a\u800c\u5c0e\u81f4\u7121\u6cd5\u6536\u6582\n    * \u7df4\u7fd2 7\uff1a\u5207\u63db\u555f\u52d5\u51fd\u6578\n        * \u7df4\u7fd2\u64cd\u4f5c\n            * \u8cc7\u6599\u96c6\u5207\u63db : \u5206\u985e\u8cc7\u6599\u96c6(\u53f3\u4e0b) - \u87ba\u65cb\u96d9\u81c2\uff0c\u7279\u5fb5\u5168\u9078\uff0c\u96b1\u85cf\u5c641\u5c64 / 8\u795e\u7d93\u5143\uff0c\u6279\u6b21\u5927\u5c0f\u56fa\u5b9a 10\uff0c\u5b78\u7fd2\u901f\u7387\u56fa\u5b9a 1\n            * \u8abf\u6574\u4e0d\u540c\u7684\u555f\u52d5\u51fd\u6578\u5f8c\u57f7\ufa08 500 \u6b21\u905e\u8ff4\uff0c\u770b\u770b\u5b78\u7fd2\u6548\u679c\u6709\u4f55\u4e0d\u540c?\n        * \u5be6\u9a57\u7d50\u679c\n            * \u5728\u9019\u7a2e\u6975\u7aef\u7684\u60c5\u5f62\u4e0b\uff0cTanh \u6703\u7121\u6cd5\u6536\u6582\uff0cRelu \u5f88\u5feb\u5c31\u7a69\u5b9a\u5728\u5f88\u7cdf\u7cd5\u7684\u5206\u985e\u72c0\uf9fa\u614b\uff0c\u60df\u6709 Sigmoid \u9084\u53ef\u4ee5\u6536\u6582\u5230\u4e0d\u932f\u7684\u7d50\u679c\n            * \u4f46\u5be6\u52d9\u4e0a\uff0cSigmoid \u9700\u8981\u5927\uf97e\u8a08\u7b97\u6642\u9593\uff0c\u800cRelu \u5247\u76f8\u5c0d\u5feb\u5f97\u5f88\u591a\uff0c\u9019\u4e5f\u662f\u9700\u8981\u53d6\u6368\u7684\uff0c\u5728\u672c\uf9b5\u4e2d\u56e0\u70ba\u53ea\u6709\u4e00\u5c64\uff0c\u6240\u4ee5\uf9fa\u6cc1\u4e0d\u592a\u660e\u986f\n    * \u7df4\u7fd2 8\uff1a\u5207\u63db\u6b63\u898f\u5316\u9078\u9805\u8207\uf96b\u6578\n        * \u7df4\u7fd2\u64cd\u4f5c\n            * \u8cc7\u6599\u96c6\u5207\u63db : \u5206\u985e\u8cc7\u6599\u96c6(\u53f3\u4e0b) - \u87ba\u65cb\u96d9\u81c2\uff0c\u7279\u5fb5\u5168\u9078\uff0c\u96b1\u85cf\u5c641\u5c64 / 8\u795e\u7d93\u5143\uff0c\u6279\u6b21\u5927\u5c0f\u56fa\u5b9a 10\uff0c\u5b78\u7fd2\u901f\u7387\u56fa\u5b9a 0.3\uff0c\u555f\u52d5\u51fd\u6578\u8a2d\u70ba Tanh\n            * \u8abf\u6574\u4e0d\u540c\u7684\u6b63\u898f\u5316\u9078\u9805\u8207\uf96b\u6578\u5f8c\u57f7\ufa08 500 \u6b21\u905e\u8ff4\uff0c\u770b\u770b\u5b78\u7fd2\u6548\u679c\u6709\u4f55\u4e0d\u540c?\n        * \u5be6\u9a57\u7d50\u679c\n            * \u6211\u5011\u5df2\u7d93\u77e5\u9053\u4e0a\u8ff0\u8a2d\u5b9a\u672c\uf92d\u5c31\u6703\u6536\u6582\uff0c\u53ea\u662f\u5728\u8f03\u5c0f\u7684 L1 / L2 \u6b63\u898f\u5283\uf96b\u6578\u4e0b\u6536\u6582\u6bd4\u8f03\u7a69\u5b9a\u4e00\u9ede\n            * \u4f46\u6b63\u898f\u5316\uf96b\u6578\u53ea\u8981\uf976\u5927\uff0c\u53cd\u800c\u6703\u8b93\u672c\uf92d\u80fd\u6536\u6582\u7684\u8a2d\u5b9a\u8b8a\u5f97\u7121\u6cd5\u6536\u6582\uff0c\u9019\u9ede L1 \u6bd4 L2\u60c5\u6cc1\u7565\uf976\u56b4\u91cd\uff0c\u56e0\u6b64\u672c\u4f8b\uf9b5\u4e2d\u6700\u9069\u5408\u7684\u6b63\u898f\u5316\uf96b\u6578\u662f L2 + \uf96b\u6578 0.001\n            * \u5be6\u52d9\u4e0a\uff1aL1 / L2 \u8f03\u5e38\u4f7f\u7528\u5728\u975e\u6df1\u5ea6\u5b78\u7fd2\u4e0a\uff0c\u6df1\u5ea6\u5b78\u7fd2\u4e0a\u6548\u679c\u6709\u9650\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [Understanding neural networks with TensorFlow Playground](https://cloud.google.com/blog/products/gcp/understanding-neural-networks-with-tensorflow-playground)\n        * [\u6df1\u5ea6\u5b78\u7fd2\u7db2\u8def\u8abf\u53c3\u6280\u5de7](https://zhuanlan.zhihu.com/p/24720954)\n### \u521d\u63a2\u6df1\u5ea6\u5b78\u7fd2\u4f7f\u7528Keras\n* **Day_66 : Keras \u5b89\u88dd\u8207\u4ecb\u7d39**\n    * Keras \u662f\uf9fd\u9ebc?\n        * \uf9e0\u5b78\uf9e0\u61c2\u7684\u6df1\u5ea6\u5b78\u7fd2\u5957\u4ef6\n            * Keras \u8a2d\u8a08\u51fa\u767c\u9ede\u5728\u65bc\u5bb9\uf9e0\u4e0a\u624b\uff0c\u56e0\u6b64\u96b1\u85cf\uf9ba\u5f88\u591a\u5be6\u4f5c\u7d30\u7bc0\uff0c\u96d6\u7136\u81ea\u7531\u5ea6\u7a0d\u5acc\u4e0d\u5920\uff0c\u4f46\u5f88\u9069\u5408\u6559\u5b78\n            * Keras \u5be6\u4f5c\u4e26\u512a\u5316\uf9ba\u5404\u5f0f\u7d93\u5178\u7d44\u4ef6\uff0c\u56e0\u6b64\u5373\u4f7f\u662f\u540c\u6642\u719f\u6089 TensorFlow \u8207 Keras \u7684\u8001\u624b\uff0c\u958b\u767c\u6642\u4e5f\u6703\uf978\u8005\u4e26\u7528\u4e92\u88dc\n        * Keras \u5305\u542b\u7684\u7d44\u4ef6\u6709\u54ea\u4e9b?\n            * Keras \u7684\u7d44\u4ef6\u5f88\u8cbc\u8fd1\u76f4\u89ba\uff0c\u56e0\u6b64\u6211\u5011\u53ef\u4ee5\u7528 TensorFlow PlayGround \u9ad4\u9a57\u6240\u5b78\u5230\u7684\u6982\uf9a3\uff0c\u5206\u70ba\uf978\u5927\u985e\uf92d\uf9e4\u89e3 ( \u975e\u4e00\u4e00\u5c0d\u61c9 )\n            * \u6a21\u578b\u5f62\u72c0\uf9fa\u985e\n                * \u76f4\u89ba\u6982\uf9a3\uff1a\u795e\u7d93\u5143\u6578 / \u96b1\u85cf\u5c64\u6578 / \u555f\u52d5\u51fd\u6578\n                * Keras \u7d44\u4ef6 : Sequential Model / Functional Model / Layers\n            * \u914d\u7f6e\uf96b\u6578\u985e\n                * \u76f4\u89ba\u6982\uf9a3\uff1a\u5b78\u7fd2\u901f\u7387 / \u6279\u6b21\u5927\u5c0f / \u6b63\u898f\u5316\n                * Keras \u7d44\u4ef6 : Optimier / Reguliarizes / Callbacks\n        * \u6df1\u5ea6\u5b78\u7fd2\u5beb\u6cd5\u5c01\u88dd\n            * TensorFlow \u5c07\u6df1\u5ea6\u5b78\u7fd2\u4e2d\u7684 GPU/CPU \u6307\u4ee4\u5c01\u88dd\u4f86\uf92d\uff0c\u6e1b\u5c11\u8a9e\u6cd5\u5dee\uf962\uff0cKeras \u5247\u662f\u5c07\u524d\u8005\uf901\u8fd1\u4e00\u6b65\u5c01\u88dd\u6210\u55ae\u4e00\u5957\u4ef6\uff0c\u7528\u5c11\uf97e\u7684\u7a0b\u5f0f\uf965\u80fd\u5be6\u73fe\u7d93\u5178\u6a21\u578b\n        * Keras \u7684\u5f8c\u7aef\n            * Keras \u7684\u5be6\u73fe\uff0c\u5be6\u969b\u4e0a\u5b8c\u5168\u4f9d\u8cf4 TensorFlow \u7684\u8a9e\u6cd5\u5b8c\u6210\uff0c\u9019\u7a2e\u60c5\u5f62\u6211\u5011\u7a31 TensorFlow \u662f Keras \u7684\u4e00\u7a2e\u5f8c\u7aef (Backend)\n        * Keras/TensorFlow \u7684\u6bd4\u8f03\n\n            |      | Keras        | Tensorflow                     |\n            |------|:-----------:|:------------------------------:|\n            | \u5b78\u7fd2\u96e3\u5ea6 | \u4f4e            | \u9ad8                              |\n            | \u6a21\u578b\u5f48\u6027 | \u4e2d            | \u9ad8                              |\n            | \u4e3b\u8981\u5dee\u7570 | \u8655\u7406\u795e\u7d93\u5c64        | \u8655\u7406\u8cc7\u6599\u6d41                          |\n            | \u4ee3\u8868\u7d44\u4ef6 | Layers/Model | Tensor /<br> Session /<br> Placeholder |\n    * Keras \u5b89\u88dd\u6d41\u7a0b\n        * \u5b89\u88dd\u5206\u6b67\u9ede\n            * \u662f\u5426\u6709 GPU : \n                * \u56e0\u70ba\u6709 GPU \u5247\u9700\u8981\u5148\u88dd GPU \u7684\u6307\u4ee4\u96c6\uff0c\u6240\u4ee5\u6709 GPU \u5247\u9700\u8981 4 \u500b\u6b65\u9a5f\uff0c\u6c92\u6709\u5c31\u53ea\u9700\u8981 2 \u6b65\u9a5f\n            * \u4f5c\u696d\u7cfb\u7d71 : \n                * \u56e0\u70ba\u4e0d\u540c\u4f5c\u696d\u7cfb\u7d71\u9593\uff0cGPU \u7684\u5b89\u88dd\u6b65\u9a5f\u6703\u56e0\u4ecb\u9762\u6216\u6307\u4ee4\u6709\u6240\u4e0d\u540c\uff0c\u6240\u4ee5\u6211\u5011\u6703\u5206 Windows / Linux (\u4ee5Ubuntu\u70ba\uf9b5) / Mac \u5206\u5225\u4ecb\u7d39\u6d41\u7a0b\n        * Keras \u5b89\u88dd\u6ce8\u610f\u4e8b\u9805\n            * \u662f\u5426\u4f7f\u7528\uf9ba Anaconda \u865b\u64ec\u74b0\u5883 : \n                * \u5982\u679c\u60a8\u7684 Python \u74b0\u5883\u662f\u63a1\u7528 Anaconda \u5b89\u88dd\uff0c\u90a3\u9ebc\u9032\ufa08\u5f8c\u7e8c\u5b89\u88dd\u6642\uff0c\u8acb\u5148\u5207\u63db\u5230\u4f60\u5e38\u7528\u7684\u865b\u64ec\u74b0\u5883\u4e0b\u5b89\u88dd (\u9ede\u9078 Anaconda / Anaconda Prompt \u5f8c\u518d\u5b89\u88dd)\uff0c\u4ee5\u78ba\u4fdd\u5b89\u88dd\u8207\u5e38\u7528\u74b0\u5883\u662f\u540c\u4e00\u76ee\u9304\n            * \u8edf\u786c\u9ad4\u9593\u7248\u672c\u642d\u914d :\n                * \u7531\u65bc GPU \u7684 CUDA / cuDNN \u7248\u672c\u7d93\u5e38\u5347\u7d1a\uff0c\u56e0\u6b64 TensorFlow / Keras \u7684\u7248\u672c\u4e5f\u9700\u8981\u983b\u7e41\uf901\u63db\u7248\u672c\uff0c\u56e0\u6b64\u5efa\u8b70\u4ee5\u5b89\u88dd\u7576\u6642\u7684[\u5b98\u7db2\u8cc7\u8a0a\u70ba\u6e96](https://www.tensorflow.org/install/gpu)\n\n\n        * \u5b89\u88dd Keras \u5927\u81f4\u4e0a\u5206\u70ba\u56db\u500b\u6b65\u9a5f : \u4f9d\u5e8f\u5b89\u88dd CUDA / cuDNN / TensorFlow / Keras\uff0c\u53ea\u8981\u6ce8\u610f\u56db\u500b\u7a0b\u5f0f\u9593\u7684\u7248\u672c\u554f\u984c\u4ee5\u53ca\u865b\u64ec\u74b0\u5883\u554f\u984c\uff0c\u57fa\u672c\u4e0a\u61c9\u8a72\u80fd\u9806\uf9dd\u5b89\u88dd\u5b8c\u6210\n    * \u5b89\u88dd\u6d41\u7a0b - \u6c92\u6709 GPU \u7248\n        * Step 1 - \u5b89\u88dd TensorFlow\n            `pip install tensorflow`\n            * Ubuntu \u524d\u9762\u52a0\u4e0a `sudo`\n        * Step 2 - \u5b89\u88dd Keras\n            `pip install keras`\n            * Python \u627e\u4e0d\u5230 `pip` \u6307\u4ee4\uff0c\u53ef\u4ee5\u63a1\u7528 `pip3` \u4ee3\u66ff\u57f7\ufa08\u5b89\u88dd\n    * \u5b89\u88dd\u6d41\u7a0b - \u6709 GPU \u7248\n        * Step 1 - \u5b89\u88dd [CUDA](https://developer.nvidia.com/cuda-downloads)\n        * Step 2 - \u5b89\u88dd [cuDNN](https://developer.nvidia.com/cudnn)\n        * Step 3 - \u5b89\u88dd TensorFlow GPU \u7248\n            `pip install tensorflow-gpu`\n        * Step 4 - \u5b89\u88dd Keras\n            `pip install keras`\n        * Step 4-2 - \u65b0\u589e\u74b0\u5883\u8b8a\u6578\u65bc PATH\n            * (\u53ea\u6709 Windows \u9700\u8981, \u5176\u4ed6\u4f5c\u696d\u7cfb\u7d71\u8acb\u8df3\u904e) \u5982\u679c\u662f Win10\uff0c\u53ef\u5f9e\u958b\u59cb / \u63a7\u5236\u53f0 / \u7cfb\u7d71\u958b\u555f\u8996\u7a97\u5f8c\uff0c\u9ede\u9078\"\u9032\u968e\"\u5206\u9801\u6700\u4e0b\u9762\u7684\u6309\u9215\"\u74b0\u5883\u8b8a\u6578\"\uff0c\u6703\u8df3\u51fa\u4e0b\u5217\uf99c\u8996\u7a97\uff0c\u8acb\u5728\u4e0b\u534a\u8996\u7a97\u4e2d\u5c0b\u627e\"Path\"\u8b8a\u6578\uff0c\u628a\u4e0b\u5217\uf99c\uf978\u500b\uf937\u5f91\u52a0\u5165\n                ```\n                C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\n                C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp\n                ```\n                \u9805\u76ee\u9593\u8981\u7528\u5206\u865f `;` \u9694\u958b / CUDA \u7248\u865f\u8acb\u4f9d Step1 \u5be6\u969b\u5b89\u88dd\u7248\u672c\u70ba\u6e96\n        * \u9a57\u8b49\u5b89\u88dd\n            * \u5b89\u88dd\u5b8c\u5f8c\uff0c\u53ef\u4ee5\u958b\u555f\u4e00\u500b .ipynb \u6a94\u8f38\u5165\u4e0b\uf99c\u6307\u4ee4\u4e26\u57f7\ufa08\uff0c\u5982\u679c\u90fd\u6709\u9806\u5229\uf9dd\u57f7\ufa08\uff0c\u5c31\u662f\u5b89\u88dd\u6210\u529f\uf9ba!!\n                ```py\n                import tensorflow\n                import keras\n                ```\n    * \u5ef6\u4f38\u95b1\u8b80 : [Keras \u4e2d\u6587\u6587\u6a94](https://keras.io/zh/#keras_1)\n        ```py\n        import keras\n        from keras import backend as K\n\n        # \u6aa2\u67e5 backend\n        keras.backend.backend()\n        # \u6aa2\u67e5 fuzz factor\n        keras.backend.epsilon()       \n        # \u6aa2\u67e5Keras float \n        K.floatx()\n        # \u8a2d\u5b9a Keras \u6d6e\u9ede\u904b\u7b97\u70bafloat16\n        K.set_floatx('float16')\n        ```\n* **Day_67 : Keras embedded dataset \u7684\u4ecb\u7d39\u8207\u61c9\u7528**\n    * Keras \u81ea\u5e36\u7684\u6578\u64da\u96c6 : \n        * CIFAR10 \u5c0f\u5716\u50cf\u5206\u985e\n        * CIFAR100 \u5c0f\u5716\u50cf\u5206\u985e\n        * IMDB \u96fb\u5f71\u8a55\u8ad6\u60c5\u7dd2\u5206\u985e\n        * \uf937\u900f\u793e newswire \u8a71\u984c\u5206\u985e\n        * \u624b\u5beb\u6578\u5b57\u7684 MNIST \u6578\u64da\u5eab\n        * \u6642\u5c1a\u6587\u7ae0\u7684\u6642\u5c1a MNIST \u6578\u64da\u5eab\n        * \u6ce2\u58eb\u9813\u623f\u5c4b\u50f9\u683c\u56de\u6b78\u6578\u64da\u96c6\n    * \u4e0b\u8f09\u5f8c\u9810\u8a2d\u5b58\u5132\u76ee\u9304 `C:Users\\Administrator\\.keras\\datasets` \u4e0b\u7684\u540c\u540d\u6a94\uff0c\u6ce8\u610f\u6709\u500b\u9ede `.keras`\n    * \u57f7\ufa08\u4e0b\u8f09\u6642\uff0c\u8981 import \u76f8\u61c9\u7684\u6a21\u7d44\uff0c\uf9dd\u7528\u8cc7\u6599\u96c6\u6a21\u7d44\u63d0\u4f9b\u7684\u51fd\u6578\u4e0b\u8f09\u8cc7\u6599\n    * CIFAR10\n        * \u5c0f\u5716\u50cf\u5206\u985e\n        * \u6578\u64da\u96c6 50,000 \u5f35 32x32 \u5f69\u8272\u8a13\u7df4\u5716\u50cf\uff0c\u6a19\u8a3b\u8d85\u904e 10 \u500b\u985e\u5225\uff0c10,000 \u5f35\u6e2c\u8a66\u5716\u50cf\u3002\n            ```py\n            '''Label description\n            0 : airplane\n            1 : automobile\n            2 : bird\n            3 : cat\n            4 : deer\n            5 : dog\n            6 : frog\n            7 : horse\n            8 : ship\n            9 : truck\n            '''\n            from keras.datasets import cifar10\n            (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n            ```\n    * CIFAR100\n        * \u5c0f\u5716\u50cf\u5206\u985e\n        * \u6578\u64da\u96c6 50,000 \u5f35 32x32 \u5f69\u8272\u8a13\u7df4\u5716\u50cf\uff0c\u6a19\u8a3b\u8d85\u904e100\u500b\u985e\u5225\uff0c10,000 \u5f35\u6e2c\u8a66\u5716\u50cf\u3002\n            ```py\n            from keras.datasets import cifar100\n            (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=\u2018fine\u2019) \n            ```\n    * MNIST \u6578\u64da\u5eab\n        * \u624b\u5beb\u6578\u5b57\u7684 MNIST \u6578\u64da\u5eab\n        * \u6578\u64da\u96c6\u5305\u542b 10 \u500b\u6578\u5b57\u7684 60,000 \u500b 28x28 \u7070\u5ea6\u5716\u50cf\uff0c\u4ee5\u53ca 10,000 \u500b\u5716\u50cf\u7684\u6e2c\u8a66\u96c6\u3002\n            ```py\n            from keras.datasets import mnist\n            (x_train, y_train), (x_test, y_test) = mnsit.load_data()\n            ```\n    * \u6642\u5c1a\u6587\u7ae0\u7684\u6642\u5c1a MNIST \u6578\u64da\u5eab\n        * Zalando's article images\n        * \u6578\u64da\u96c6\u5305\u542b 10 \u500b\u6642\u5c1a\u985e\u5225\u7684 60,000 \u500b 28x28 \u7070\u5ea6\u5716\u50cf\uff0c\u4ee5\u53ca 10,000 \u500b\u5716\u50cf\u7684\u6e2c\u8a66\u96c6\u3002\u9019\u500b\u6578\u64da\u96c6\u53ef\u4ee5\u7528\u4f5c MNIST \u7684\u76f4\u63a5\u66ff\u63db\u3002\n            ```py\n            '''Label description\n            0 : T-shirt / top\n            1 : Trouser\n            2 : Pullover\n            3 : Dress\n            4 : Coat\n            5 : Sandal\n            6 : Shirt\n            7 : Sneaker\n            8 : Bag\n            9 : Ankle boot\n            '''\n            from keras.datasets import fashion_mnsit\n            (x_train, y_train), (x_test, y_test) = fashion_mnsit.load_data()\n            ```\n    * \u6ce2\u58eb\u9813\u623f\u5c4b\u50f9\u683c\u56de\u6b78\n        * \u53d6\u81ea\u5361\u5167\u57fa\u6885\uf9dc\u5927\u5b78\u7dad\u8b77\u7684 StatLib \u5eab\n        * 20 \u4e16\u7d00 70 \uf98e\u4ee3\u5f8c\u671f\uff0c\u6a23\u672c\u5728\u6ce2\u58eb\u9813\u90ca\u5340\u7684\u4e0d\u540c\u4f4d\u7f6e\u5305\u542b 13 \u500b\u623f\u5c4b\u5c6c\u6027\u3002\u76ee\u6a19\u662f\u4e00\u500b\u5730\u9ede\u623f\u5c4b\u7684\u4e2d\u4f4d\u503c (\u55ae\u4f4d\uff1ak $)\n            ```py\n            from keras.datasets import boston_housing   \n            (x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n            ```\n    * IMDB \u96fb\u5f71\u8a55\u8ad6\u60c5\u7dd2\u5206\u985e\n        * \uf92d\u81ea IMDB \u7684 25,000 \u90e8\u96fb\u5f71\u8a55\u8ad6\u7684\u6578\u64da\u96c6\uff0c\u6a19\u6709\u60c5\u7dd2 (\u6b63\u9762 / \u8ca0\u9762)\u3002\u8a55\u8ad6\u5df2\u7d93\u904e\u9810\u8655\uf9e4\uff0c\u6bcf\u500b\u8a55\u8ad6\u90fd\u88ab\u7de8\u78bc\u70ba\u4e00\u7cfb\uf99c\u55ae\u8a5e\u7d22\u5f15 (\u6574\u6578)\n        * \u55ae\u8a5e\u7531\u6578\u64da\u96c6\u4e2d\u7684\u6574\u9ad4\u983b\u7387\u7d22\u5f15\n            * \u6574\u6578\"3\"\u7de8\u78bc\u6578\u64da\u4e2d\u7b2c 3 \u500b\u6700\u983b\u7e41\u7684\u55ae\u8a5e\u3002\n            * \"0\"\u4e0d\u4ee3\u8868\u7279\u5b9a\u55ae\u8a5e\uff0c\u800c\u662f\u7528\u65bc\u7de8\u78bc\u4efb\u4f55\u672a\u77e5\u55ae\u8a5e\n            ```py\n            '''\n            path\uff1a\u5982\u679c\u60a8\u6c92\u6709\u672c\u5730\u6578\u64da\uff08at'~/.keras/datasets/' + path\uff09\uff0c\u5b83\u5c07\u88ab\u4e0b\u8f09\u5230\u6b64\u4f4d\u7f6e\u3002\n            num_words\uff1a\u6574\u6578\u6216\u7121\u3002\u6700\u5e38\ufa0a\u7684\u8a5e\u5f59\u9700\u8981\u8003\u616e\u3002\u4efb\u4f55\u4e0d\u592a\u983b\u7e41\u7684\u55ae\u8a5e\u5c07 oov_char \u5728\u5e8f\uf99c\u6578\u64da\u4e2d\u986f\u793a\u70ba\u503c\u3002\n            skip_top\uff1a\u6574\u6578\u3002\u6700\u5e38\u88ab\u5ffd\uf976\u7684\u8a5e\uff08\u5b83\u5011\u5c07 oov_char \u5728\u5e8f\uf99c\u6578\u64da\u4e2d\u986f\u793a\u70ba\u503c\uff09\u3002\n            maxlen\uff1aint\u3002\u6700\u5927\u5e8f\uf99c\u9577\u5ea6\u3002\u4efb\u4f55\uf901\u9577\u7684\u5e8f\uf99c\u90fd\u5c07\u88ab\u622a\u65b7\u3002\n            seed\uff1aint\u3002\u7528\u65bc\u53ef\u91cd\u8907\u6578\u64da\u6539\u7d44\u7684\u7a2e\u5b50\u3002\n            start_char\uff1aint\u3002\u5e8f\uf99c\u7684\u958b\u982d\u5c07\u6a19\u6709\u6b64\u5b57\u7b26\u3002\u8a2d\u7f6e\u70ba 1\uff0c\u56e0\u70ba 0 \u901a\u5e38\u662f\u586b\u5145\u5b57\u7b26\u3002\n            oov_char\uff1aint\u3002\u9019\u662f\u56e0\u70ba\u5207\u51fa\u5b57 num_words \u6216 skip_top \u9650\u5236\u5c07\u9019\u500b\u5b57\u7b26\u66ff\u63db\u3002\n            index_from\uff1aint\u3002\u4f7f\u7528\u6b64\u7d22\u5f15\u548c\uf901\u9ad8\u7684\u7d22\u5f15\u5be6\u969b\u55ae\u8a5e\u3002\n            '''\n            from keras.datasets import imdb\n            (x_train, y_train), (x_test, y_test) = imdb.load_data(path=\u201cimdb.npz\u201d,num_words= None,skip_top=0,maxlen=None, seed=113,start_char=1,oov_char=2,index_from=3)  \n            ```\n    * \uf937\u900f\u793e\u65b0\u805e\u5c08\u984c\u4e3b\u984c\u5206\u985e\n        * \uf92d\u81ea\uf937\u900f\u793e\u7684 11,228 \u689d\u65b0\u805e\u5c08\u7dda\u7684\u6578\u64da\u96c6\uff0c\u6a19\u8a3b\uf9ba 46 \u500b\u4e3b\u984c\u3002\u8207 IMDB \u6578\u64da\u96c6\u4e00\u6a23\uff0c\u6bcf\u689d\u7dda\u90fd\u88ab\u7de8\u78bc\u70ba\u4e00\u7cfb\uf99c\u5b57\u7d22\u5f15\n            ```py\n            from keras.datasets import reuters\n            (x_train, y_train), (x_test, y_test) = reuters.load_data(path=\u201creuters npz\u201d,num_words= None,skip_top=0,maxlen=None, test_split=0.2,seed=113,start_char=1,oov_char=2,index_from=3)\n            ```\n    * \u5982\u4f55\u4f7f\u7528 Keras \u81ea\u5e36\u6578\u64da\u96c6\u505a\u76ee\u6a19\u5b78\u7fd2\n        * \u9069\u7528\u65bc\u6587\u672c\u5206\u6790\u8207\u60c5\u7dd2\u5206\u985e\n            * IMDB \u96fb\u5f71\u8a55\u8ad6\u60c5\u7dd2\u5206\u985e\n            * \uf937\u900f\u793e\u65b0\u805e\u5c08\u984c\u4e3b\u984c\u5206\u985e\n        * \u9069\u7528\u65bc\u5f71\u50cf\u5206\u985e\u8207\u8b58\u5225\u5b78\u7fd2\n            * CIFAR10 / CIFAR100\n            * MNIST / Fashion-MNIST\n        * \u9069\u7528\u65bc Data / Numerical \u5b78\u7fd2\n            * Boston housing price regression dataset\n        * \u91dd\u5c0d\u5c0f\u6578\u64da\u96c6\u7684\u6df1\u5ea6\u5b78\u7fd2\n            * \u6578\u64da\u9810\u8655\uf9e4\u8207\u6578\u64da\u63d0\u5347\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Keras : The Python Deep Learning Library](https://github.com/keras-team/keras/)\n        * [Keras dataset](https://keras.io/api/datasets/)\n        * [Predicting Boston House Prices](https://www.kaggle.com/sagarnildass/predicting-boston-house-prices)\n        * [imagenet](http://www.image-net.org/about-stats)\n        * [COCO(Common Objects in Context)](http://cocodataset.org/)\n* **Day_68 : \u5e8f\u5217\u6a21\u578b\u642d\u5efa\u7db2\u8def Sequential API**\n    * \u5e8f\u5217\u6a21\u578b\u662f\u591a\u500b\u7db2\u8def\u5c64\u7684\u7dda\u6027\u5806\u758a\n        * Sequential \u662f\u4e00\u7cfb\u5217\u6a21\u578b\u7684\u7c21\u55ae\u7dda\u6027\u758a\u52a0\uff0c\u53ef\u4ee5\u5728\u69cb\u9020\u51fd\u6578\u4e2d\u50b3\u5165\u4e00\u4e9b\u5217\u7684\u7db2\u8def\u5c64\n            ```py\n            from keras.models import Sequential\n            from keras.layers import Dense, Activation\n            model = Sequential([Dense(32, _input_shap=(784,)), Activation(\u201crelu\u201d)])\n            ```\n        * \u4e5f\u53ef\u4ee5\u900f\u904e `.add`\n            ```py\n            model = Sequential()\n            model.add(Dense(32, _input_dim=784))\n            model.add(Activation(\u201crelu\u201d))\n            ```\n    * \u6307\u5b9a\u6a21\u578b\u8f38\u5165\u7dad\u5ea6\n        * Sequential \u7684\u7b2c\u4e00\u5c64 (\u53ea\u6709\u7b2c\u4e00\u5c64\uff0c\u5f8c\u9762\u7684\u5c64\u6703\u81ea\u52d5\u5339\u914d) \u9700\u8981\u77e5\u9053\u8f38\u5165\u7684 shape\n            * \u5728\u7b2c\u4e00\u5c64\u52a0\u5165\u4e00\u500b input_shape \u53c3\u6578\uff0cinput_shape \u61c9\u8a72\u662f\u4e00\u500b shape \u7684 tuple \u8cc7\u6599\u985e\u578b\n            * input_shape \u662f\u4e00\u7cfb\u5217\u6574\u6578\u7684 tuple\uff0c\u67d0\u4e9b\u4f4d\u7f6e\u53ef\u4ee5\u70ba None\n            * input_shape \u4e2d\u4e0d\u7528\u6307\u660e batch_size \u7684\u6578\u76ee\n            * 2D \u7684\u7db2\u8def\u5c64\uff0c\u5982 Dense\uff0c\u5141\u8a31\u5728\u5c64\u7684\u69cb\u9020\u51fd\u6578\u7684 input_dim \u4e2d\u6307\u5b9a\u8f38\u5165\u7684\u7dad\u5ea6\u3002\n            * \u5c0d\u65bc\u67d0\u4e9b 3D \u6642\u9593\u5c64\uff0c\u53ef\u4ee5\u5728\u69cb\u9020\u51fd\u6578\u4e2d\u6307\u5b9a input_dim \u548c input_length \u4f86\u5be6\u73fe\u3002\n            * \u5c0d\u65bc\u67d0\u4e9b RNN\uff0c\u53ef\u4ee5\u6307\u5b9a batch_size\u3002\u9019\u6a23\u5f8c\u9762\u7684\u8f38\u5165\u5fc5\u9808\u662f (batch_size, input_shape) \u7684\u8f38\u5165\n    * \u5e38\u7528\u53c3\u6578\u8aaa\u660e\n\n        | \u540d\u7a31         | \u4f5c\u7528                     | \u539f\u578b\u53c3\u6578                                                                                                      |\n        |------------|------------------------|-----------------------------------------------------------------------------------------------------------|\n        | Dense      | \u5be6\u73fe\u5168\u9023\u63a5\u5c64                 | Dense\\(units,activation,use_bias=True,kernel_initializer='golorot_uniform',bias_initializer='zeros') |\n        | Activation | \u5c0d\u4e0a\u5c64\u8f38\u51fa\u61c9\u7528\u6fc0\u6d3b\u51fd\u6578            | Activation(activation)                                                                                  |\n        | Dropout    | \u5c0d\u4e0a\u5c64\u8f38\u51fa\u61c9\u7528 dropout \u4ee5\u9632\u6b62\u904e\u64ec\u5408 | Dropout(ratio)                                                                                          |\n        | Flatten    | \u5c0d\u4e0a\u5c64\u8f38\u51fa\u4e00\u7dad\u5316               | Flatten()                                                                                               |\n        | Reshape    | \u5c0d\u4e0a\u5c64\u8f38\u51fa reshape          | Reshape(target_shape)                                                                                  |\n    * Sequential \u6a21\u578b\u7684\u57fa\u672c\u5143\u4ef6\u4e00\u822c\u9700\u8981 :\n        * Model \u5ba3\u544a\n        * model.add \u6dfb\u52a0\u5c64\n        * model.compile \u914d\u7f6e\u5b78\u7fd2\u904e\u7a0b\u53c3\u6578\n        * model.fit \u6a21\u578b\u8a13\u7df4\u53c3\u6578\u8a2d\u7f6e+\u8a13\u7df4\n        * \u6a21\u578b\u8a55\u4f30\n        * \u6a21\u578b\u9810\u6e2c\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [Getting started with Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/)\n        ```py\n        # \u8f09\u5165\u5fc5\u9808\u4f7f\u7528\u7684 Library\n        import keras\n        from keras.datasets import cifar10\n        from keras.models import Sequential, load_model\n        from keras.layers import Dense, Dropout, Activation, Flatten\n        from keras.layers import Conv2D, MaxPooling2D\n\n        batch_size = 32\n        num_classes = 10\n        epochs = 10\n\n        # The data, shuffled and split between train and test sets:\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n        print('x_train shape:', x_train.shape)\n        print(x_train.shape[0], 'train samples')\n        print(x_test.shape[0], 'test samples')\n\n        # Convert class vectors to binary class matrices.\n        y_train = keras.utils.to_categorical(y_train, num_classes)\n        y_test = keras.utils.to_categorical(y_test, num_classes)\n\n        # build our CNN model, \u591a\u52a0\u5e7e\u5c64\n        model = Sequential()\n        model.add(Conv2D(32, (5, 5), padding='same',\n                        input_shape=x_train.shape[1:]))\n        model.add(Activation('relu'))\n        model.add(Conv2D(64, (5, 5)))\n        model.add(Activation('relu'))\n        model.add(Conv2D(128, (5, 5)))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        model.add(Flatten())\n        model.add(Dense(1024))\n        model.add(Activation('relu'))\n        model.add(Dropout(0.25))\n        model.add(Dense(num_classes))\n        model.add(Activation('softmax'))\n\n        print(model.summary())\n        ```\n* **Day_69 : Keras Module API \u7684\u4ecb\u7d39\u8207\u61c9\u7528**\n    * \u51fd\u6578\u5f0f API\n        * \u7528\u6236\u5b9a\u7fa9\u591a\u8f38\u51fa\u6a21\u578b\u3001\u975e\u5faa\u74b0\u6709\u5411\u6a21\u578b\u6216\u5177\u6709\u5171\u4eab\u5c64\u7684\u6a21\u578b\u7b49\u8907\u96dc\u6a21\u578b\u7684\u9014\u5f91\n        * \u5b9a\u7fa9\u5fa9\u96dc\u6a21\u578b\uff08\u5982\u591a\u8f38\u51fa\u6a21\u578b\u3001\u6709\u5411\u7121\u74b0\u5716\uff0c\u6216\u5177\u6709\u5171\u4eab\u5c64\u7684\u6a21\u578b\uff09\u7684\u65b9\u6cd5\u3002\n        * \u6240\u6709\u7684\u6a21\u578b\u90fd\u53ef\u8abf\u7528\uff0c\u5c31\u50cf\u7db2\u7d61\u5c64\u4e00\u6a23\n            * \uf9dd\u7528\u51fd\u6578\u5f0f API\uff0c\u53ef\u4ee5\u8f15\uf9e0\u5730\u91cd\u7528\u8a13\u7df4\u597d\u7684\u6a21\u578b\uff1a\u53ef\u4ee5\u5c07\u4efb\u4f55\u6a21\u578b\u770b\u4f5c\u662f\u4e00\u500b\u5c64\uff0c\u7136\u5f8c\u901a\u904e\u50b3\u905e\u4e00\u500b\u5f35\uf97e\uf92d\u8abf\u7528\u5b83\u3002\u6ce8\u610f\uff0c\u5728\u8abf\u7528\u6a21\u578b\u6642\uff0c\u60a8\u4e0d\u50c5\u91cd\u7528\u6a21\u578b\u7684\u7d50\u69cb\uff0c\u9084\u91cd\u7528\uf9ba\u5b83\u7684\u6b0a\u91cd\u3002\n    * \u51fd\u6578\u5f0f API \u8207\u9806\u5e8f\u6a21\u578b\n        * \u6a21\u578b\u9700\u8981\u591a\u65bc\u4e00\u500b\u7684\u8f38\u51fa\uff0c\u90a3\u9ebc\u4f60\u7e3d\u61c9\u8a72\u9078\u64c7\u51fd\u6578\u5f0f\u6a21\u578b\u3002\n            * \u51fd\u6578\u5f0f\u6a21\u578b\u662f\u6700\u5ee3\u6cdb\u7684\u4e00\u985e\u6a21\u578b\uff0c\u5e8f\u8cab\u6a21\u578b\uff08Sequential\uff09\u53ea\u662f\u5b83\u7684\u4e00\u7a2e\u7279\u6b8a\u60c5\u6cc1\u3002\n        * \u5ef6\u4f38\u8aaa\u660e\n            * \u5c64\u5c0d\u8c61\u63a5\u53d7\u5f35\uf97e\u70ba\uf96b\u6578\uff0c\u8fd4\u56de\u4e00\u500b\u5f35\uf97e\u3002\n            * \u8f38\u5165\u662f\u5f35\uf97e\uff0c\u8f38\u51fa\u4e5f\u662f\u5f35\uf97e\u7684\u4e00\u500b\u6846\u67b6\u5c31\u662f\u4e00\u500b\u6a21\u578b\uff0c\u901a\u904e Model \u5b9a\u7fa9\u3002\n            * \u9019\u6a23\u7684\u6a21\u578b\u53ef\u4ee5\u88ab\u50cf Keras \u7684 Sequential \u4e00\u6a23\u88ab\u8a13\u7df4\u3002\n    * \u5982\u4f55\u914d\u7f6e\n        * \u4f7f\u7528\u51fd\u6578\u5f0f\u6a21\u578b\u7684\u4e00\u500b\u5178\u578b\u5834\u666f\u662f\u642d\u5efa\u591a\u8f38\u5165\u3001\u591a\u8f38\u51fa\u7684\u6a21\u578b\n    * \u5ef6\u4f38\u95b1\u8b80 : \n        * [Getting started with the Keras function API](https://keras.io/guides/functional_api/)\n\n        ```py\n        from keras.layers import Input, Embedding, LSTM, Dense\n        from keras.models import Model\n\n        #\u4e3b\u8981\u8f38\u5165\u63a5\u6536\u65b0\u805e\u6a19\u984c\u672c\u8eab\uff0c\u5373\u4e00\u500b\u6574\u6578\u5e8f\u5217\uff08\u6bcf\u500b\u6574\u6578\u7de8\u78bc\u4e00\u500b\u8a5e\uff09\u3002\n        #\u9019\u4e9b\u6574\u6578\u57281 \u523010,000 \u4e4b\u9593\uff0810,000 \u500b\u8a5e\u7684\u8a5e\u5f59\u8868\uff09\uff0c\u4e14\u5e8f\u5217\u9577\u5ea6\u70ba100 \u500b\u8a5e\n        #\u5ba3\u544a\u4e00\u500b NAME \u53bb\u5b9a\u7fa9Input\n        main_input = Input(shape=(100,), dtype='int32', name='main_input')\n\n        # Embedding \u5c64\u5c07\u8f38\u5165\u5e8f\u5217\u7de8\u78bc\u70ba\u4e00\u500b\u7a20\u5bc6\u5411\u91cf\u7684\u5e8f\u5217\uff0c\n        # \u6bcf\u500b\u5411\u91cf\u7dad\u5ea6\u70ba 512\u3002\n        x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n\n        # LSTM \u5c64\u628a\u5411\u91cf\u5e8f\u5217\u8f49\u63db\u6210\u55ae\u500b\u5411\u91cf\uff0c\n        # \u5b83\u5305\u542b\u6574\u500b\u5e8f\u5217\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\n        lstm_out = LSTM(32)(x)\n\n        #\u63d2\u5165\u8f14\u52a9\u640d\u5931\uff0c\u4f7f\u5f97\u5373\u4f7f\u5728\u6a21\u578b\u4e3b\u640d\u5931\u5f88\u9ad8\u7684\u60c5\u6cc1\u4e0b\uff0cLSTM \u5c64\u548cEmbedding \u5c64\u90fd\u80fd\u88ab\u5e73\u7a69\u5730\u8a13\u7df4\n        news_output = Dense(1, activation='sigmoid', name='news_out')(lstm_out)\n\n        #\u8f14\u52a9\u8f38\u5165\u6578\u64da\u8207LSTM \u5c64\u7684\u8f38\u51fa\u9023\u63a5\u8d77\u4f86\uff0c\u8f38\u5165\u5230\u6a21\u578b\n        import keras\n        news_input = Input(shape=(5,), name='news_in')\n        x = keras.layers.concatenate([lstm_out, news_input])\n\n\n        # \u5806\u758a\u591a\u500b\u5168\u9023\u63a5\u7db2\u8def\u5c64\n        x = Dense(64, activation='relu')(x)\n        x = Dense(64, activation='relu')(x)\n        #\u4f5c\u696d\u89e3\u7b54: \u65b0\u589e\u5169\u5c64\n        x = Dense(64, activation='relu')(x)\n        x = Dense(64, activation='relu')(x)\n\n        # \u6700\u5f8c\u6dfb\u52a0\u4e3b\u8981\u7684\u908f\u8f2f\u56de\u6b78\u5c64\n        main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n\n        # \u5ba3\u544a MODEL API, \u5206\u5225\u63a1\u7528\u81ea\u884c\u5b9a\u7fa9\u7684 Input/Output Layer\n        model = Model(inputs=[main_input, news_input], outputs=[main_output, news_output])\n\n        #\u8f14\u52a9\u8f38\u5165\u6578\u64da\u8207LSTM \u5c64\u7684\u8f38\u51fa\u9023\u63a5\u8d77\u4f86\uff0c\u8f38\u5165\u5230\u6a21\u578b\n        import keras\n        news_input = Input(shape=(5,), name='news_in')\n        x = keras.layers.concatenate([lstm_out, news_input])\n\n        # \u5806\u758a\u591a\u500b\u5168\u9023\u63a5\u7db2\u8def\u5c64\n        x = Dense(64, activation='relu')(x)\n        x = Dense(64, activation='relu')(x)\n\n        # \u6700\u5f8c\u6dfb\u52a0\u4e3b\u8981\u7684\u908f\u8f2f\u56de\u6b78\u5c64\n        main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n\n        model.compile(optimizer='rmsprop',\n              loss={'main_output': 'binary_crossentropy', 'news_out': 'binary_crossentropy'},\n              loss_weights={'main_output': 1., 'news_out': 0.2})\n        model.summary()\n        ```\n* **Day_70 : Multi-layer Perception \u7c21\u4ecb**\n    * Multi-layer Perceptron (MLP) \u591a\u5c64\u611f\u77e5\u5668 :\n        * \u70ba\u4e00\u7a2e\u76e3\u7763\u5f0f\u5b78\u7fd2\u7684\u6f14\u7b97\u6cd5\n        * \u6b64\u7b97\u6cd5\u5c07\u53ef\u4ee5\u4f7f\u7528\u975e\u7dda\u6027\u8fd1\u4f3c\u5c07\u8cc7\u6599\u5206\u985e\u6216\u9032\ufa08\u8ff4\u6b78\u904b\u7b97\n        * \u591a\u5c64\u611f\u77e5\u6a5f\u662f\u4e00\u7a2e\u524d\u5411\u50b3\u905e\u985e\u795e\u7d93\u7db2\uf937\uff0c\u81f3\u5c11\u5305\u542b\u4e09\u5c64\u7d50\u69cb (\u8f38\u5165\u5c64\u3001\u96b1\u85cf\u5c64\u548c\u8f38\u51fa\u5c64)\uff0c\u4e26\u4e14\uf9dd\u7528\u5230\u300c\u5012\u50b3\u905e\u300d\u7684\u6280\u8853\u9054\u5230\u5b78\u7fd2 (model learning) \u7684\u76e3\u7763\u5f0f\u5b78\u7fd2\uff0c\u4ee5\u4e0a\u662f\u50b3\u7d71\u7684\u5b9a\u7fa9\u3002\n        * \u73fe\u5728\u6df1\u5ea6\u5b78\u7fd2\u7684\u767c\u5c55\uff0c\u5176\u5be6 MLP \u662f\u6df1\u5ea6\u795e\u7d93\u7db2\uf937(deep neural network, DNN) \u7684\u4e00\u7a2e special case\uff0c\u6982\uf9a3\u57fa\u672c\u4e0a\u4e00\u6a23\uff0cDNN \u53ea\u662f\u5728\u5b78\u7fd2\u904e\u7a0b\u4e2d\u591a\uf9ba\u4e00\u4e9b\u624b\u6cd5\u548c\u5c64\u6578\u6703\uf901\u591a\uf901\u6df1\u3002\n        * \uf974\u6bcf\u500b\u795e\u7d93\u5143\u7684\u6fc0\u6d3b\u51fd\u6578\u90fd\u662f\u7dda\u6027\u51fd\u6578\uff0c\u90a3\u9ebc\u4efb\u610f\u5c64\u6578\u7684 MLP \u90fd\u53ef\u88ab\u7d04\u7c21\u6210\u4e00\u500b\u7b49\u50f9\u7684\u55ae\u5c64\u611f\u77e5\u5668\n    * MLP \u512a\u9ede\uff1a\n        * \u6709\u80fd\uf98a\u5efa\u7acb\u975e\u7dda\u6027\u7684\u6a21\u578b\n        * \u53ef\u4ee5\u4f7f\u7528 partial_fit \u5efa\u7acb real-time \u6a21\u578b\n    * MLP \u7f3a\u9ede :\n        * \u64c1\u6709\u5927\u65bc\u4e00\u500b\u5340\u57df\u6700\u5c0f\u503c\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u521d\u59cb\u6b0a\u91cd\uff0c\u6703\u8b93\u9a57\u8b49\u6642\u7684\u6e96\u78ba\u7387\u6d6e\u52d5\n        * MLP \u6a21\u578b\u9700\u8981\u8abf\u6574\u6bcf\u5c64\u795e\u7d93\u5143\u6578\u3001\u5c64\u6578\u3001\u758a\u4ee3\u6b21\u6578\n        * \u5c0d\u65bc\u7279\u5fb5\u7684\u9810\u5148\u8655\uf9e4\u5f88\u654f\u611f\n        ```py\n        from keras.utils import np_utils\n        import numpy as np\n        #\u8f09\u5165\u624b\u5beb\u8fa8\u8b58\u7684\u8cc7\u6599\u96c6\n        from keras.datasets import mnist\n        (x_train_image,y_train_label),\\\n        (x_test_image,y_test_label)= mnist.load_data()\n\n        #\u6307\u5b9a\u6e2c\u8a66\u96c6\u8207\u8a13\u7df4\u8cc7\u6599\u96c6\n        x_Train =x_train_image.reshape(60000, 784).astype('float32')\n        x_Test = x_test_image.reshape(10000, 784).astype('float32')\n\n        # normalize inputs from 0-255 to 0-1\n        x_Train_normalize = x_Train / 255\n        x_Test_normalize = x_Test / 255\n\n        #\u628aLABEL\u8f49\u6210NUMERICAL Categorical \n        y_Train_OneHot = np_utils.to_categorical(y_train_label)\n        y_Test_OneHot = np_utils.to_categorical(y_test_label)\n\n        #\u5efa\u7acb\u6a21\u578b\n        from keras.models import Sequential\n        from keras.layers import Dense\n\n        #\u5ba3\u544a\u63a1\u7528\u5e8f\u5217\u6a21\u578b\n        model = Sequential()\n\n        #\u5efa\u69cb\u8f38\u5165\u5c64\n        model.add(Dense(units=256, \n                input_dim=784, \n                kernel_initializer='normal', \n                activation='relu'))\n        #\u5efa\u69cb\u8f38\u51fa\u5c64\n        model.add(Dense(units=10, \n                kernel_initializer='normal', \n                activation='softmax'))\n        print(model.summary())\n\n        #\u6a21\u578b\u8a13\u7df4\n        model.compile(loss='categorical_crossentropy', \n              optimizer='adam', metrics=['accuracy'])\n        train_history = model.fit(x=x_Train_normalize,\n                                  y=y_Train_OneHot,validation_split=0.2, \n                                  epochs=10, batch_size=32,verbose=1)\n\n        #\u4ee5\u5716\u5f62\u986f\u793a\u8a13\u7df4\u904e\u7a0b\n        import matplotlib.pyplot as plt\n        def show_train_history(train_history,train,validation):\n            plt.plot(train_history.history[train])\n            plt.plot(train_history.history[validation])\n            plt.title('Train History')\n            plt.ylabel(train)\n            plt.xlabel('Epoch')\n            plt.legend(['train', 'validation'], loc='upper left')\n            plt.show()\n\n        show_train_history(train_history,'accuracy','val_accuracy')\n        show_train_history(train_history,'loss','val_loss')\n\n        #\u8a55\u4f30\u6a21\u578b\u6e96\u78ba\u7387\n        scores = model.evaluate(x_Test_normalize, y_Test_OneHot)\n        print('accuracy=',scores[1])\n        ```\n* **Day_71 : \u640d\u5931\u51fd\u6578\u7684\u4ecb\u7d39\u8207\u61c9\u7528**\n    * \u640d\u5931\u51fd\u6578\n        * \u6a5f\u5668\u5b78\u7fd2\u4e2d\u6240\u6709\u7684\u7b97\u6cd5\u90fd\u9700\u8981\u6700\u5927\u5316\u6216\u6700\u5c0f\u5316\u4e00\u500b\u51fd\u6578\uff0c\u9019\u500b\u51fd\u6578\u88ab\u7a31\u70ba\u300c**\u76ee\u6a19\u51fd\u6578**\u300d\u3002\u5176\u4e2d\uff0c\u6211\u5011\u4e00\u822c\u628a**\u6700\u5c0f\u5316**\u7684\u4e00\u985e\u51fd\u6578\uff0c\u7a31\u70ba\u300c**\u640d\u5931\u51fd\u6578**\u300d\u3002\u5b83\u80fd\u6839\u64da\u9810\u6e2c\u7d50\u679c\uff0c\u8861\uf97e\u51fa\u6a21\u578b\u9810\u6e2c\u80fd\uf98a\u7684\u597d\u58de\n        * \u640d\u5931\u51fd\u6578\u5927\u81f4\u53ef\u5206\u70ba\uff1a**\u5206\u985e\u554f\u984c**\u7684\u640d\u5931\u51fd\u6578\u548c**\u56de\u6b78\u554f\u984c**\u7684\u640d\u5931\u51fd\u6578\n    * \u640d\u5931\u51fd\u6578\u70ba\uf9fd\u9ebc\u662f\u6700\u5c0f\u5316\n        * \u671f\u671b\uff1a\u5e0c\u671b\u6a21\u578b\u9810\u6e2c\u51fa\uf92d\u7684\u6771\u897f\u53ef\u4ee5\u8ddf\u5be6\u969b\u7684\u503c\u4e00\u6a23\n        * \u640d\u5931\u51fd\u6578\u4e2d\u7684\u640d\u5931\u5c31\u662f\u300c\u5be6\u969b\u503c\u548c\u9810\u6e2c\u503c\u7684\uf918\u5dee\u300d\n        * \u9810\u6e2c\u51fa\uf92d\u7684\u6771\u897f\u57fa\u672c\u4e0a\u8ddf\u5be6\u969b\u503c\u90fd\u6703\u6709\uf918\u5dee\n        * \u5728\u56de\u6b78\u554f\u984c\u7a31\u70ba\u300c**\u6b98\u5dee (residual)**\u300d\n        * \u5728\u5206\u985e\u554f\u984c\u7a31\u70ba\u300c**\u932f\u8aa4\u7387 (error rate)**\u300d\n        * $y$ \u8868\u793a\u5be6\u969b\u503c\uff0c$\\hat{y}$ \u8868\u793a\u9810\u6e2c\u503c\n        $$ loss/residual = y - \\hat{y}$$\n        $$ error\\_rate = \\frac{\\sum_{i=1}^n sign(y_i\\neq \\hat{y_i})}{n}$$\n    * \u640d\u5931\u51fd\u6578\u7684\u5206\u985e\u4ecb\u7d39\n        * **\u5747\u65b9\u8aa4\u5dee (mean_squared_error)**\uff1a\n            * \u5c31\u662f\u6700\u5c0f\u5e73\u65b9\u6cd5 (Least Square) \u7684\u76ee\u6a19\u51fd\u6578 -- \u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u7684\u5dee\u8ddd\u4e4b\u5e73\u5747\u503c\u3002\u9084\u6709\u5176\u4ed6\u8b8a\u5f62\u7684\u51fd\u6578, \u5982 mean_absolute_error\u3001mean_absolute_percentage_error\u3001mean_squared_logarithmic_error\n            $$ MSE = \\frac{\\sum{(\\hat{y}-y)^2}}{N} $$\n            * \u4f7f\u7528\u6642\u6a5f\uff1a\n                * n \u500b\u6a23\u672c\u7684\u9810\u6e2c\u503c\uff08$y$\uff09\u8207\uff08$\\hat{y}$\uff09\u7684\u5dee\u8ddd\n                * Numerical \u76f8\u95dc\n            ```py\n            from keras import losses\n            model.compile(loss= 'mean_squared_error', optimizer='sgd')\n            #\u5176\u4e2d\uff0c\u5305\u542b y_true\uff0c y_pred \u7684\u50b3\u905e\uff0c\u51fd\u6578\u662f\u8868\u9054\u5982\u4e0b\uff1a\n            keras.losses.mean_squared_error(y_true, y_pred)\n            ```\n        * **Cross Entropy**\n            * \u7576\u9810\u6e2c\u503c\u8207\u5be6\u969b\u503c\u6108\u76f8\u8fd1\uff0c\u640d\u5931\u51fd\u6578\u5c31\u6108\u5c0f\uff0c\u53cd\u4e4b\u5dee\u8ddd\u5f88\u5927\uff0c\u5c31\u6703\uf901\u5f71\u97ff\u640d\u5931\u51fd\u6578\u7684\u503c\n            * \u8981\u7528 Cross Entropy \u53d6\u4ee3 MSE\uff0c\u56e0\u70ba\u5728\u68af\u5ea6\u4e0b\u6642\uff0cCross Entropy \u8a08\u7b97\u901f\u5ea6\u8f03\u5feb\n            * \u4f7f\u7528\u6642\u6a5f\uff1a\n                * \u6574\u6578\u76ee\u6a19\uff1aSparse categorical_crossentropy\n                * \u5206\u985e\u76ee\u6a19\uff1acategorical_crossentropy\n                * \u4e8c\u5206\u985e\u76ee\u6a19\uff1abinary_crossentropy\n            ```py\n            from keras import losses\n            model.compile(loss= \u2018categorical_crossentropy \u2018, optimizer='sgd\u2019)\n            #\u5176\u4e2d, \u5305\u542by_true\uff0c y_pred\u7684\u50b3\u905e, \u51fd\u6578\u662f\u8868\u9054\u5982\u4e0b\uff1a\n            keras.losses.categorical_crossentropy(y_true, y_pred)\n            ```\n        * **Hinge Error (hinge)**\n            * \u662f\u4e00\u7a2e\u55ae\u908a\u8aa4\u5dee\uff0c\u4e0d\u8003\u616e\u8ca0\u503c\u540c\u6a23\u4e5f\u6709\u591a\u7a2e\u8b8a\u5f62\uff0csquared_hinge\u3001categorical_hinge\n            $$ l(y) = max(0,1-t\uff0ey)$$\n            * \u4f7f\u7528\u6642\u6a5f\uff1a\n                * \u9069\u7528\u65bc\u300e\u652f\u63f4\u5411\uf97e\u6a5f\u300f(SVM) \u7684\u6700\u5927\u9593\u9694\u5206\u985e\u6cd5 (maximum-margin classification)\n            ```py\n            from keras import losses\n            model.compile(loss= \u2018hinge\u2018, optimizer='sgd\u2019)\n            #\u5176\u4e2d\uff0c\u5305\u542b y_true\uff0cy_pred \u7684\u50b3\u905e, \u51fd\u6578\u662f\u8868\u9054\u5982\u4e0b\n            keras.losses.hinge(y_true, y_pred)\n            ```\n        * **\u81ea\u5b9a\u7fa9\u640d\u5931\u51fd\u6578**\n            * \u6839\u64da\u554f\u984c\u7684\u5be6\u969b\u60c5\u6cc1\uff0c\u5b9a\u5236\u5408\uf9e4\u7684\u640d\u5931\u51fd\u6578\n            * \u8209\uf9b5\uff1a\u9810\u6e2c\u679c\u6c41\u65e5\u92b7\uf97e\u554f\u984c\uff0c\u5982\u679c\u9810\u6e2c\u92b7\uf97e\u5927\u65bc\u5be6\u969b\u92b7\u91cf\uf97e\u5247\u6703\u640d\u5931\u6210\u672c\uff1b\u5982\u679c\u9810\u6e2c\u92b7\uf97e\u5c0f\u65bc\u5be6\u969b\u92b7\uf97e\u5247\u6703\u640d\u5931\uf9dd\u6f64\u3002\n            * \u8003\u616e\u91cd\u9ede\uff1a\u88fd\u9020\u4e00\u76d2\u679c\u6c41\u7684\u6210\u672c\u548c\u92b7\u552e\u4e00\u76d2\u679c\u6c41\u7684\uf9dd\u6f64\u4e0d\u662f\u7b49\u50f9\u7684\n            * \u9700\u8981\u4f7f\u7528\u7b26\u5408\u8a72\u554f\u984c\u7684\u81ea\u5b9a\u7fa9\u640d\u5931\u51fd\u6578\u81ea\u5b9a\u7fa9\u640d\u5931\u51fd\u6578\u70ba\n                * \uf974\u9810\u6e2c\u7d50\u679c $y$ \u5c0f\u65bc\u6a19\u6e96\u7b54\u6848 $\\hat{y}$\uff0c\u640d\u5931\u51fd\u6578\u70ba\uf9dd\u6f64\u4e58\u4ee5\u9810\u6e2c\u7d50\u679c $y$ \u8207\u6a19\u6e96\u7b54\u6848\u4e4b\u5dee\n                * \uf974\u9810\u6e2c\u7d50\u679c $y$ \u5927\u65bc\u6a19\u6e96\u7b54\u6848 $\\hat{y}$\uff0c\u640d\u5931\u51fd\u6578\u70ba\u6210\u672c\u4e58\u4ee5\u9810\u6e2c\u7d50\u679c y \u8207\u6a19\u6e96\u7b54\u6848\u4e4b\u5dee\u7528\n                ```py\n                loss = tf.reduce_sum(tf.where(tf.greater(y, y_), COST*(y-y_), PROFIT*(y_-y)))\n                ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u4ea4\u53c9\u71b5](https://blog.csdn.net/qq_40147863/article/details/82015360)\n        * [losses function](https://keras.io/losses/)\n* **Day_72 : \u555f\u52d5\u51fd\u6578\u7684\u4ecb\u7d39\u8207\u61c9\u7528**   \n    * \u4f55\u8b02\u555f\u52d5\u51fd\u6578\n        * **\u555f\u52d5\u51fd\u6578**\u5b9a\u7fa9\u4e86\u6bcf\u500b\u7bc0\u9ede(\u795e\u7d93\u5143)\u7684**\u8f38\u51fa\u548c\u8f38\u5165\u7684\u95dc\u4fc2\u7684\u51fd\u6578**\uff0c\u70ba\u795e\u7d93\u5143\u63d0\u4f9b\u898f\u6a21\u5316**\u975e\u7dda\u6027\u80fd\u529b**\uff0c\u8b93\u795e\u7d93\u7db2\u8def\u5177\u5099\u4e86\u5f37\u5927\u7684\u64ec\u5408\u80fd\u529b\n        * \u8f38\u51fa\u503c\u7684\u7bc4\u570d :\n            * \u7576\u8f38\u51fa\u503c\u7bc4\u570d\u662f\u6709\u9650\u7684\u6642\u5019\uff0c\u57fa\u65bc\u68af\u5ea6\u7684\u512a\u5316\u65b9\u6cd5\u6703\u66f4\u52a0\u7a69\u5b9a\uff0c\u56e0\u70ba\u7279\u5fb5\u7684\u8868\u73fe\u53d7\u6709\u9650\u7684\u6b0a\u503c\u7684\u5f71\u97ff\u66f4\u70ba\u986f\u8457\n            * \u7576\u8f38\u51fa\u503c\u7bc4\u570d\u662f\u7121\u9650\u7684\u6642\u5019\uff0c\u6a21\u578b\u7684\u8a13\u7df4\u6703\u66f4\u52a0\u9ad8\u6548\n    * \u555f\u52d5\u51fd\u6578\u7684\u4f5c\u7528\n        * \u6df1\u5ea6\u5b78\u7fd2\u7684\u57fa\u672c\u539f\u7406\u662f\u57fa\u65bc\u4eba\u5de5\u795e\u7d93\u7db2\u8def\uff0c\u4fe1\u865f\u5f9e\u4e00\u500b\u795e\u7d93\u5143\u9032\u5165\uff0c\u7d93\u904e\u975e\u7dda\u6027\u7684 activation function\uff0c\u5982\u6b64\u5faa\u74b0\u5f80\u5fa9\u76f4\u5230\u8f38\u51fa\u5c64\uff0c\u6b63\u662f\u7531\u65bc\u9019\u4e9b\u975e\u7dda\u6027\u51fd\u6578\u7684\u53cd\u8986\u758a\u52a0\uff0c\u5c64\u4f7f\u5f97\u795e\u7d93\u5f80\u5fa9\u6709\u8db3\u5920\u7684\u80fd\u529b\u4f86\u6293\u53d6\u8907\u96dc\u7684 pattern\n        * \u555f\u52d5\u51fd\u6578\u7684\u6700\u5927\u4f5c\u7528\u5c31\u662f\u975e\u7dda\u6027\u5316\n            * \u5982\u679c\u4e0d\u7528\u555f\u52d5\u51fd\u6578\uff0c\u7121\u8ad6\u795e\u7d93\u7db2\u8def\u6709\u591a\u5c11\u5c64\uff0c\u8f38\u51fa\u90fd\u662f\u8f38\u5165\u7684\u7dda\u6027\u7d44\u5408\n        * \u555f\u52d5\u51fd\u6578\u7684\u53e6\u4e00\u500b\u7279\u5fb5\u662f\n            * \u4ed6\u61c9\u8a72\u53ef\u4ee5\u5340\u5206\u524d\u884c\u7db2\u8def\u548c\u53cd\u5411\u5f0f\u50b3\u64ad\u7db2\u8def\u7684\u7db2\u8def\u53c3\u6578\u66f4\u65b0\uff0c\u7136\u5f8c\u76f8\u61c9\u7684\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6216\u5176\u4ed6\u512a\u5316\u6280\u8853\u512a\u5316\u6b0a\u91cd\u4ee5\u6e1b\u5c11\u8aa4\u5dee\n    * \u5e38\u7528\u7684\u555f\u52d5\u51fd\u6578\u4ecb\u7d39\n        * **Sigmoid**\n        $$ f(z) = \\frac{1}{1+exp(-z)} $$\n            * \u7279\u9ede\u662f\u6703\u628a\u8f38\u51fa\u9650\u5728 0~1 \u4e4b\u9593\uff0c\u7576 x < 0\uff0c\u8f38\u51fa\u5c31\u662f 0\uff0c\u7576 x > 0\uff0c\u8f38\u51fa\u5c31\u662f 1\uff0c\u9019\u6a23\u4f7f\u5f97\u8f38\u51fa\u5728\u50b3\u905e\u904e\u7a0b\u4e2d\u4e0d\u5bb9\u6613\u767c\u6563\n            * \u5169\u500b\u4e3b\u8981\u7f3a\u9ede\n                * \u5bb9\u6613\u904e\u98fd\u548c\uff0c\u4e1f\u5931\u68af\u5ea6\u3002\u9019\u6a23\u5728\u53cd\u5411\u50b3\u64ad\u6642\uff0c\u5f88\u5bb9\u6613\u51fa\u73fe\u68af\u5ea6\u6d88\u5931\u7684\u60c5\u6cc1\uff0c\u5c0e\u81f4\u8a13\u7df4\u7121\u6cd5\u5b8c\u6574\n                * \u8f38\u51fa\u5747\u503c\u4e0d\u662f 0\n        * **Softmax**\n        $$ \\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^Ke^{z_k}} $$\n            * \u628a\u4e00\u500b k \u7dad\u7684 real value \u5411\u91cf (a1, a2, a3, ...) \u6620\u5c04\u6210\u4e00\u500b (b1, b2, b3, ...) \u5176\u4e2d bi \u662f\u4e00\u500b 0~1 \u7684\u5e38\u6578\uff0c\u8f38\u51fa\u795e\u7d93\u5143\u4e4b\u548c\u70ba 1.0\uff0c\u6240\u4ee5\u53ef\u4ee5\u62ff\u4f86\u505a\u591a\u5206\u985e\u9810\u6e2c\n            * \u70ba\u4ec0\u9ebc\u8981\u53d6\u6307\u6578\n                * \u6a21\u64ec max \u884c\u70ba\uff0c\u8981\u8b93\u5927\u8005\u66f4\u5927\n                * \u9700\u8981\u4e00\u500b\u53ef\u5c0e\u51fd\u6578\n        * **Tanh**\n        $$ tanh(x) = 2\\sigma(2x) - 1$$\n        $$tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n            * Tanh \u8b80\u505a Hyperbolic tangent\n            * \u7a31\u70ba\u96d9\u6b63\u5207\u51fd\u6578\uff0c\u53d6\u503c\u7bc4\u570d [-1, 1]\n            * \u5728\u7279\u5fb5\u76f8\u5dee\u660e\u986f\u6642\u7684\u6548\u679c\u5f88\u597d\uff0c\u5728\u5faa\u74b0\u904e\u7a0b\u4e2d\u4e0d\u65b7\u64f4\u5927\u7279\u5fb5\u6548\u679c\n        * **ReLU**\n        $$ f(x) = max(0, x)$$\n            * \u4fee\u6b63\u7dda\u6027\u55ae\u5143 (Rectified linear unit)\n            * \u5728 x > 0\uff0c\u5c0e\u6578\u6046\u70ba 1\n            * \u5728 x < 0\uff0c\u68af\u5ea6\u6046\u70ba 0\uff0c\u9019\u6642\u5019\u4ed6\u4e5f\u6703\u51fa\u73fe\u98fd\u548c\u73fe\u8c61\uff0c\u751a\u81f3\u4f7f\u795e\u7d93\u5143\u76f4\u63a5\u7121\u6548\uff0c\u5f9e\u800c\u9f4a\u6b0a\u91cd\u7121\u6cd5\u5f97\u5230\u66f4\u65b0 (\u9019\u7a2e\u60c5\u6cc1\u4e0b\u901a\u5e38\u7a31\u70ba dying ReLU)\n            * Leak ReLU \u548c PReLU \u7684\u63d0\u51fa\u6b63\u662f\u70ba\u4e86\u89e3\u6c7a\u9019\u4e00\u554f\u984c\n        * **ELU**\n        $$ f(x))= \\begin{cases} x & \\text {, if $x > 0$} \\\\ a(e^x-1) & \\text{, if $x \\leq 0$} \\end{cases} $$\n            * ELU \u662f\u91dd\u5c0d ReLU \u7684\u4e00\u500b\u6539\u9032\u578b\uff0c\u76f8\u6bd4\u65bc ReLU \u51fd\u6578\uff0c\u5728\u8f38\u5165\u70ba\u8ca0\u7684\u60c5\u6cc1\u4e0b\uff0c\u662f\u6709\u4e00\u5b9a\u7684\u8f38\u51fa\u7684\n            * \u9019\u6a23\u53ef\u4ee5\u6d88\u9664 ReLU \u6b7b\u6389\u7684\u554f\u984c\n            * \u9084\u662f\u6709\u68af\u5ea6\u98fd\u548c\u548c\u6307\u6578\u904b\u7b97\u7684\u554f\u984c\n        * **PReLU**\n        $$ f(x) = max(ax, x)$$\n            * \u53c3\u6578\u5316\u4fee\u6b63\u7dda\u6027\u55ae\u5143 (Parameteric Rectified linear unit)\n            * Leaky ReLU\uff0c\u7576 $a = 0.1$ \u6642\uff0c\u6211\u5011\u53eb PReLU \u70ba Leaky ReLU\n            * PReLU \u548c Leaky ReLU \u6709\u4e00\u4e9b\u5171\u9ede\uff0c\u5373\u70ba\u8ca0\u503c\u8f38\u5165\u6dfb\u52a0\u4e86\u4e00\u500b\u7dda\u6027\u9805\n        * **Maxout**\n        $$ f(x) = max(w^T_1x + b_1, w^T_2x + b_2)$$\n            * Maxout \u662f\u6df1\u5ea6\u5b78\u7fd2\u7db2\u8def\u4e2d\u7684\u4e00\u5c64\u7db2\u8def\uff0c\u5c31\u50cf\u6c60\u5316\u5c64\u3001\u5377\u7a4d\u5c64\u4f9d\u6a23\uff0c\u53ef\u4ee5\u770b\u6210\u662f\u7db2\u8def\u7684\u555f\u52d5\u5c64\n            * Maxout \u795e\u7d93\u5143\u7684\u555f\u52d5\u51fd\u6578\u662f\u53d6\u5f97\u6240\u6709\u300c\u51fd\u6578\u5c64\u300d\u4e2d\u7684\u6700\u5927\u503c \n            * \u64ec\u5408\u529b\u975e\u5e38\u5f37\uff0c\u512a\u9ede\u662f\u7c21\u55ae\u8a2d\u8a08\uff0c\u4e0d\u6703\u904e\u98fd\u548c\uff0c\u540c\u6642\u53c8\u6c92\u6709 ReLU \u7684\u7f3a\u9ede\n            * \u7f3a\u9ede\u662f\u904e\u7a0b\u53c3\u6578\u76f8\u7576\u65bc\u591a\u4e86\u4e00\u500d\n    * Sigmoid vs. Tanh\n        * Tanh \u5c07\u8f38\u51fa\u503c\u58d3\u7e2e\u5230\u4e86 [-1, 1] \u7bc4\u570d\uff0c\u56e0\u6b64\u4ed6\u662f 0 \u5747\u503c\u7684\uff0c\u89e3\u6c7a\u4e86 Sigmoid \u51fd\u6578\u975e zero-centered \u554f\u984c\uff0c\u4f46\u662f\u4ed6\u4e5f\u5b58\u5728\u68af\u5ea6\u6d88\u5931\u548c\u51aa\u904b\u7b97\u7684\u554f\u984c\n        * \u5176\u5be6 Tanh(x) = 2 * Sigmoid(2x) - 1 \n    * Sigmoid vs. Softmax\n        * Sigmoid \u5c07\u4e00\u500b real value \u6620\u5c04\u5230 (0, 1) \u7684\u5340\u9593\uff0c\u7528\u4f86\u505a\u4e8c\u5206\u985e\n        * \u628a\u4e00\u500b k \u7dad\u7684 real value \u5411\u91cf (a1, a2, a3, ...) \u6620\u5c04\u6210\u4e00\u500b (b1, b2, b3, ...) \u5176\u4e2d bi \u662f\u4e00\u500b 0~1 \u7684\u5e38\u6578\uff0c\u8f38\u51fa\u795e\u7d93\u5143\u4e4b\u548c\u70ba 1.0\uff0c\u6240\u4ee5\u53ef\u4ee5\u62ff\u4f86\u505a\u591a\u5206\u985e\u9810\u6e2c\n        * \u4e8c\u5206\u985e\u554f\u984c\u6642 Sigmoid \u548c Softmax \u662f\u4e00\u6a23\u7684\uff0c\u6c42\u7684\u90fd\u662f cross entropy loss\n    * \u68af\u5ea6\u6d88\u5931 (Vanishing gradient problem)\n        * \u539f\u56e0 : \u524d\u9762\u7684\u5c64\u6bd4\u5f8c\u9762\u7684\u5c64\u68af\u5ea6\u8b8a\u5316\u66f4\u5c0f\uff0c\u6545\u8b8a\u5316\u66f4\u6162\n        * \u7d50\u679c : Output \u8b8a\u5316\u6162 -> Gradient \u8b8a\u5316\u5c0f -> \u5b78\u5f97\u6162\n        * Sigmoid \u548c Tanh \u90fd\u6709\u9019\u6a23\u7279\u6027\uff0c\u4e0d\u9069\u5408\u7528\u5728 layers \u591a\u7684 DNN \u67b6\u69cb\n    * \u5982\u4f55\u9078\u64c7\u6b63\u78ba\u7684\u555f\u52d5\u51fd\u6578\n        * \u6839\u64da\u5404\u51fd\u6578\u7684\u7684\u512a\u7f3a\u9ede\u4f86\u914d\u7f6e\n            * \u5982\u679c\u4f7f\u7528 ReLU\uff0c\u8981\u5c0f\u5fc3 learning rate\uff0c\u6ce8\u610f\u4e0d\u8981\u8b93\u7db2\u8def\u51fa\u73fe\u5f88\u591a\u300cdead\u300d\u795e\u7d93\u5143\uff0c\u5982\u679c\u4e0d\u597d\u89e3\u6c7a\uff0c\u53ef\u4ee5\u8a66\u8a66 Leaky ReLU\u3001PReLU\u3001Maxout \n        * \u6839\u64da\u554f\u984c\u7684\u6027\u8cea\n            * \u7528\u65bc\u5206\u985e\u5668\uff0cSigmoid \u51fd\u6578\u53ca\u5176\u7d44\u5408\u901a\u5e38\u6548\u679c\u66f4\u597d\n            * \u7531\u65bc\u68af\u5730\u6d88\u5931\u554f\u984c\uff0c\u6709\u6642\u8981\u907f\u514d\u4f7f\u7528 Sigmoid \u548c Tanh \u51fd\u6578\u3002ReLU \u51fd\u6578\u662f\u4e00\u500b\u901a\u7528\u7684\u555f\u52d5\u51fd\u6578\uff0c\u76ee\u524d\u7684\u5927\u591a\u60c5\u6cc1\u4e0b\u4f7f\u7528\n            * \u5982\u679c\u795e\u7d93\u7db2\u8def\u4e2d\u51fa\u73fe\u6b7b\u7684\u795e\u7d93\u5143\uff0c\u90a3\u9ebc PReLU \u51fd\u6578\u5c31\u662f\u6700\u597d\u7684\u9078\u64c7\n            * ReLU \u51fd\u6578\u53ea\u5efa\u8b70\u7528\u5728\u96b1\u85cf\u5c64\n        * \u8003\u616e DNN \u640d\u5931\u51fd\u6578\u548c\u555f\u52d5\u51fd\u6578\n            * \u5982\u679c\u4f7f\u7528 Sigmoid \u555f\u52d5\u51fd\u6578\uff0c\u5247\u4ea4\u53c9\u71b5\u640d\u5931\u51fd\u6578\u80af\u5b9a\u6bd4\u5747\u65b9\u5dee\u640d\u5931\u51fd\u6578\u7684\u597d\n            * \u5982\u679c\u662f DNN \u7528\u65bc\u5206\u985e\uff0c\u64c7\u4e00\u822c\u5728\u8f38\u51fa\u5c64\u4f7f\u7528 Softmax \u555f\u52d5\u51fd\u6578\n            * ReLU \u555f\u52d5\u51fd\u6578\u5c0d\u68af\u5ea6\u6d88\u5931\u554f\u984c\u6709\u4e00\u5b9a\u5c64\u5ea6\u7684\u89e3\u6c7a\uff0c\u5c24\u5176\u662f\u5728 CNN \u6a21\u578b\u4e2d\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u795e\u7d93\u7db2\u8def\u5e38\u7528\u555f\u52d5\u51fd\u6578\u7e3d\u7d50](https://zhuanlan.zhihu.com/p/39673127)\n        * [CS231N Lecture](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf)\n        ```py\n        import numpy as np\n\n        #sigmoid \u6578\u5b78\u51fd\u6578\u8868\u793a\u65b9\u5f0f\n        def sigmoid(x):\n            return (1 / (1 + np.exp(-x)))\n        #Sigmoid \u5fae\u5206\n        def dsigmoid(x):\n            return (x * (1 - x))\n\n        #Softmax \u6578\u5b78\u51fd\u6578\u8868\u793a\u65b9\u5f0f\n        def softmax(x):\n            return np.exp(x) / float(sum(np.exp(x)))\n\n        #tanh \u6578\u5b78\u51fd\u6578\u8868\u793a\u65b9\u5f0f\n        def tanh(x):\n            return(np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n        #tanh \u5fae\u5206\n        def dtanh(x):\n            return 1 - np.square(x)\n\n        #ReLU \u6578\u5b78\u51fd\u6578\u8868\u793a\u65b9\u5f0f\n        def ReLU(x):\n            return abs(x) * (x > 0)\n        #ReLU \u5fae\u5206\n        def dReLU(x):\n            return (1 * (x > 0))\n        ```\n* **Day_73 : \u68af\u5ea6\u4e0b\u964d Gradient Descent \u7c21\u4ecb** \n    * \u6a5f\u5668\u5b78\u7fd2\u7b97\u6cd5\u7576\u4e2d\uff0c\u512a\u5316\u7b97\u6cd5\u7684\u529f\u80fd\uff0c\u662f\u901a\u904e\u6539\u5584\u8a13\u7df4\u65b9\u5f0f\uff0c\uf92d\u6700\u5c0f\u5316 (\u6216\u6700\u5927\u5316) \u640d\u5931\u51fd\u6578\n    * \u6700\u5e38\u7528\u7684\u512a\u5316\u7b97\u6cd5\u662f**\u68af\u5ea6\u4e0b\u964d**\n        * \u901a\u904e\u5c0b\u627e\u6700\u5c0f\u503c\uff0c\u63a7\u5236\u65b9\u5dee\uff0c\uf901\u65b0\u6a21\u578b\uf96b\u6578\uff0c\u6700\u7d42\u4f7f\u6a21\u578b\u6536\u6582\n        * $w_{i+1} = w_i - d_i\u00b7\u03b7_i , i=0,1,...$\n        * \uf96b\u6578 $\u03b7$ \u662f**\u5b78\u7fd2\u7387**\u3002\u9019\u500b\uf96b\u6578\u65e2\u53ef\u4ee5\u8a2d\u7f6e\u70ba\u56fa\u5b9a\u503c\uff0c\u4e5f\u53ef\u4ee5\u7528\u4e00\u7dad\u512a\u5316\u65b9\u6cd5\u6cbf\u8457\u8a13\u7df4\u7684\u65b9\u5411\u9010\u6b65\uf901\u65b0\u8a08\u7b97\n        * \uf96b\u6578\u7684\uf901\u65b0\u5206\u70ba\uf978\u6b65\uff1a\u7b2c\u4e00\u6b65\u8a08\u7b97\u68af\u5ea6\u4e0b\u964d\u7684\u65b9\u5411\uff0c\u7b2c\u4e8c\u6b65\u8a08\u7b97\u5408\u9069\u7684\u5b78\u7fd2\n    * \u5b78\u7fd2\u7387\u5c0d\u68af\u5ea6\u4e0b\u964d\u7684\u5f71\u97ff\n        * \u5b78\u7fd2\u7387\u5b9a\u7fa9\uf9ba\u6bcf\u6b21\u758a\u4ee3\u4e2d\u61c9\u8a72\uf901\u6539\u7684\uf96b\u6578\uf97e\u3002\u63db\uf906\u8a71\u8aaa\uff0c\u5b83\u63a7\u5236\u6211\u5011\u61c9\u8a72\u6536\u6582\u5230\u6700\u4f4e\u7684\u901f\u5ea6\u3002\u5c0f\u5b78\u7fd2\u7387\u53ef\u4ee5\u4f7f\u8fed\u4ee3\u6536\u6582\uff0c\u5927\u5b78\u7fd2\u7387\u53ef\u80fd\u8d85\u904e\u6700\u5c0f\u503c\n        $$ compute : \\frac{\\partial{L}}{\\partial{w}} $$\n        $$ w \\leftarrow w - \\eta\\frac{\\partial{L}}{\\partial{w}} $$\n    * \u68af\u5ea6\u4e0b\u964d\u6cd5\u7684\u904e\u7a0b\n        * \u9996\u5148\u9700\u8981\u8a2d\u5b9a\u4e00\u500b\u521d\u59cb\uf96b\u6578\u503c\uff0c\u901a\u5e38\u60c5\u6cc1\u4e0b\u5c07\u521d\u503c\u8a2d\u70ba\u96f6 (w=0)\uff0c\u63a5\u4e0b\uf92d\u9700\u8981\u8a08\u7b97\u6210\u672c\u51fd\u6578 cost \n        * \u7136\u5f8c\u8a08\u7b97\u51fd\u6578\u7684\u5c0e\u6578 (\u67d0\u500b\u9ede\u8655\u7684\u659c\u7387\u503c)\uff0c\u4e26\u8a2d\u5b9a\u5b78\u7fd2\u6548\u7387\uf96b\u6578 (lr) \u7684\u503c\u3002\n        * \u91cd\u8907\u57f7\ufa08\u4e0a\u8ff0\u904e\u7a0b\uff0c\u76f4\u5230\uf96b\u6578\u503c\u6536\u6582\uff0c\u9019\u6a23\u6211\u5011\u5c31\u80fd\u7372\u5f97\u51fd\u6578\u7684\u6700\u512a\u89e3\n    * \u600e\u9ebc\u78ba\u5b9a\u5230\u6975\u503c\u9ede\uf9ba\u5462\uff1f\n        * $\u03b7$ \u53c8\u7a31\u5b78\u7fd2\u7387\uff0c\u662f\u4e00\u500b\u632a\u52d5\u6b65\u9577\u7684\u57fa\u6578\uff0c$\\frac{df(x)}{dx}$ \u662f\u5c0e\u51fd\u6578\uff0c\u7576\u96e2\u5f97\u9060\u7684\u6642\u5019\u5c0e\u6578\u5927\uff0c\u79fb\u52d5\u7684\u5c31\u5feb\uff0c\u7576\u63a5\u8fd1\u6975\u503c\u6642\uff0c\u5c0e\u6578\u975e\u5e38\u5c0f\uff0c\u79fb\u52d5\u7684\u5c31\u975e\u5e38\u5c0f\uff0c\u9632\u6b62\u8de8\u904e\u6975\u503c\u9ede\n        * Gradient descent never guarantee global minima\n        * Different initial point will be caused reach different minima, so different results\n        * avoid local minima\n            * \u5728\u8a13\u7df4\u795e\u7d93\u7db2\u7d61\u7684\u6642\u5019\uff0c\u901a\u5e38\u5728\u8a13\u7df4\u525b\u958b\u59cb\u7684\u6642\u5019\u4f7f\u7528\u8f03\u5927\u7684 learning rate\uff0c\u96a8\u8457\u8a13\u7df4\u7684\u9032\ufa08\uff0c\u6211\u5011\u6703\u6162\u6162\u7684\u6e1b\u5c0f learning rate\uff0c\u5177\u9ad4\u5c31\u662f\u6bcf\u6b21\u8fed\u4ee3\u7684\u6642\u5019\u6e1b\u5c11\u5b78\u7fd2\u7387\u7684\u5927\u5c0f\uff0c\uf901\u65b0\u516c\u5f0f\uff1a\n            decayed_learning_rate\uff1dlearning_rate*                                            decay_rate\uff3e(global_step/decay_steps)\n\n                | \u53c3\u6578                      | \u610f\u7fa9       |\n                |-------------------------|----------|\n                | decayed\\_learning\\_rate | \u8870\u6e1b\u5f8c\u7684\u5b78\u7fd2\u7387  |\n                | learning\\_rate          | \u521d\u59cb\u5b78\u7fd2\u7387    |\n                | decay\\_rate             | \u8870\u6e1b\u7387      |\n                | global\\_step            | \u7576\u524d\u7684 step |\n                | decay\\_steps            | \u8870\u6e1b\u9031\u671f     |\n        * \u4f7f\u7528 **momentum**\uff0c\u662f\u68af\u5ea6\u4e0b\u964d\u6cd5\u4e2d\u4e00\u7a2e\u5e38\u7528\u7684\u52a0\u901f\u6280\u8853\u3002\n            * Gradient Descent \u7684\u5be6\u73fe\uff1aSGD, \u5c0d\u65bc\u4e00\u822c\u7684SGD\uff0c\u5176\u8868\u9054\u5f0f\u70ba\n            $x \u2190 x\\ \u2212\\ \\alpha\\ \u2217\\ dx$ (x\u6cbf\u8ca0\u68af\u5ea6\u2f45\u65b9\u5411\u4e0b\u964d)\n            \u800c\u5e36 momentum \u9805\u7684 SGD \u5247\u5beb\u6210\u5982\u4e0b\u5f62\u5f0f\uff1a\n            $v\\ = \\beta\\ \u2217\\ v\\ \u2212\\ \\alpha\\ \u2217\\ dx$\n            $x\\ \u2190\\ x\\ +\\ v$\n            * \u5176\u4e2d $\\beta$ \u5373 momentum \u4fc2\u6578\uff0c\u901a\u4fd7\u7684\uf9e4\u89e3\u4e0a\u9762\u5f0f\u2f26\u5b50\u5c31\u662f\uff0c\u5982\u679c\u4e0a\u4e00\u6b21\u7684 momentum\uff08\u5373 $\\beta$\uff09\u8207\u9019\u4e00\u6b21\u7684\u8ca0\u68af\u5ea6\u65b9\u5411\u662f\u76f8\u540c\u7684\uff0c\u90a3\u9019\u6b21\u4e0b\u964d\u7684\u5e45\u5ea6\u5c31\u6703\u52a0\u5927\uff0c\u6240\u4ee5\u9019\u6a23\u505a\u80fd\u5920\u9054\u5230\u52a0\u901f\u6536\u6582\u7684\u904e\u7a0b\n    * \u68af\u5ea6\u4e0b\u964d\u6cd5\u7684\u7f3a\u9ede\u5305\u62ec\uff1a\n        * \u9760\u8fd1\u6975\u5c0f\u503c\u6642\u901f\u5ea6\u6e1b\u6162\u3002\n        * \u76f4\u7dda\u641c\u7d22\u53ef\u80fd\u6703\u7522\u751f\u4e00\u4e9b\u554f\u984c\u3002\n        * \u53ef\u80fd\u6703\u300c\u4e4b\u5b57\u578b\u300d\u5730\u4e0b\u964d\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [learning rate decay](https://zhuanlan.zhihu.com/p/32923584)\n        * [\u6a5f\u5668/\u6df1\u5ea6\u5b78\u7fd2 - \u57fa\u790e\u6578\u5b78 : \u68af\u5ea6\u4e0b\u964d](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E6%95%B8%E5%AD%B8-%E4%BA%8C-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-gradient-descent-406e1fd001f)\n        * \u5404\u7a2e\u8870\u6e1b\u65b9\u6cd5\n            * exponential_decay : \u6307\u6578\u8870\u6e1b\n                `\n                decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n                `\n            * natural_exp_decay : \u81ea\u7136\u6307\u6578\u8870\u6e1b\n                `\n                decayed_learning_rate = learning_rate * exp(-decay_rate * global_step)\n                `\n            * inverse_time_decay : \u9006\u6642\u9593\u8870\u6e1b\n                `\n                decayed_learning_rate = learning_rate / (1 + decay_rate * global_step / decay_step)\n                `\n            * polynomial_decay : \u591a\u9805\u5f0f\u8870\u6e1b\n                `\n                global_step = min(global_step, decay_steps)\n                `\n                `\n                decayed_learning_rate = (learning_rate - end_learning_rate) *(1 - global_step / decay_steps) ^ (power) + end_learning_rate\n                `\n        ```py\n        import numpy as np\n        import matplotlib.pyplot as plt\n        %matplotlib inline\n\n        # \u76ee\u6a19\u51fd\u6578:y=(x+3)^2\n        def func(x): \n            return np.square(x+3)\n\n        # \u76ee\u6a19\u51fd\u6578\u4e00\u968e\u5c0e\u6578:dy/dx=2*(x+3)\n        def dfunc(x): \n            return 2 * (x+3)\n\n        def GD(w_init, df, epochs, lr):    \n            \"\"\"  \u68af\u5ea6\u4e0b\u964d\u6cd5\u3002\u7d66\u5b9a\u8d77\u59cb\u9ede\u8207\u76ee\u6a19\u51fd\u6578\u7684\u4e00\u968e\u5c0e\u51fd\u6578\uff0c\u6c42\u5728epochs\u6b21\u53cd\u8986\u904b\u7b97\u4e2dx\u7684\u66f4\u65b0\u503c\n                :param w_init: w\u7684init value    \n                :param df: \u76ee\u6a19\u51fd\u6578\u7684\u4e00\u968e\u5c0e\u51fd\u6578    \n                :param epochs: \u53cd\u8986\u904b\u7b97\u9031\u671f    \n                :param lr: \u5b78\u7fd2\u7387    \n                :return: x\u5728\u6bcf\u6b21\u53cd\u8986\u904b\u7b97\u5f8c\u7684\u4f4d\u7f6e   \n            \"\"\"    \n            xs = np.zeros(epochs+1) # \u628a \"epochs+1\" \u8f49\u6210dtype=np.float32    \n            x = w_init    \n            xs[0] = x    \n            for i in range(epochs):         \n                dx = df(x)        \n                # v\u8868\u793ax\u8981\u8de8\u51fa\u7684\u5e45\u5ea6        \n                v = - dx * lr        \n                x += v        \n                xs[i+1] = x    \n            return xs\n\n        # \u8d77\u59cb\u6b0a\u91cd\n        w_init = 3    \n        # \u57f7\u884c\u9031\u671f\u6578\n        epochs = 20 \n        # \u5b78\u7fd2\u7387   \n        #lr = 0.3\n        lr = 0.01\n        # \u68af\u5ea6\u4e0b\u964d\u6cd5 \n        x = GD(w_init, dfunc, epochs, lr=lr) \n        print (x)\n\n        #\u5283\u51fa\u66f2\u7dda\u5716\n        color = 'r'    \n        \n        from numpy import arange\n        t = arange(-6.0, 6.0, 0.01)\n        plt.plot(t, func(t), c='b')\n        plt.plot(x, func(x), c=color, label='lr={}'.format(lr))    \n        plt.scatter(x, func(x), c=color, )    \n        plt.legend()\n\n        plt.show()\n        ```\n* **Day_74 : Gradient Descent \u6578\u5b78\u539f\u7406**\n    * Gradient \u68af\u5ea6 \n        * \u5728\u5fae\u7a4d\u5206\u88e1\u9762\uff0c\u5c0d\u591a\u5143\u51fd\u6578\u7684\uf96b\u6578\u6c42 \u2202 \u504f\u5c0e\u6578\uff0c\u628a\u6c42\u5f97\u7684\u5404\u500b\uf96b\u6578\u7684**\u504f\u5c0e\u6578\u4ee5\u5411\uf97e\u7684\u5f62\u5f0f\u5beb\u51fa\uf92d\uff0c\u5c31\u662f\u68af\u5ea6**\u3002\n        * \u6bd4\u5982\u51fd\u6578 f(x), \u5c0d x \u6c42\u504f\u5c0e\u6578\uff0c\u6c42\u5f97\u7684\u68af\u5ea6\u5411\uf97e\u5c31\u662f (\u2202f/\u2202x)\uff0c\u7c21\u7a31 grad f(x)\u6216\u8005\u25bdf (x)\n    * \u6700\u5e38\u7528\u7684\u512a\u5316\u7b97\u6cd5 - \u68af\u5ea6\u4e0b\u964d\n        * \u76ee\u7684\uff1a\u6cbf\u8457\u76ee\u6a19\u51fd\u6578\u68af\u5ea6\u4e0b\u964d\u7684\u65b9\u5411\u641c\u7d22\u6975\u5c0f\u503c (\u4e5f\u53ef\u4ee5\u6cbf\u8457\u68af\u5ea6\u4e0a\u5347\u7684\u65b9\u5411\u641c\u7d22\u6975\u5927\u503c)\n        * \u8981\u8a08\u7b97 Gradient Descent\uff0c\u8003\u616e\n            * Loss = (\u5be6\u969b) ydata \u2013 (\u9810\u6e2c) ydata = w * (\u5be6\u969b) xdata \u2013 w * (\u9810\u6e2c) xdata (bias \u70ba init value\uff0c\u88ab\u6d88\u9664)\n            * Gradient = \u25bdf (\u03b8) (Gradient = \u2202L/\u2202w)\n            * \u8abf\u6574\u5f8c\u7684\u6b0a\u91cd = \u539f\u6b0a\u91cd \u2013 \u03b7(Learning rate) * Gradient\n            $$ w \\leftarrow w - \\eta\\frac{\\partial{L}}{\\partial{w}}$$\n    * \u68af\u5ea6\u4e0b\u964d\u7684\u7b97\u6cd5\u8abf\u512a\n        * Learning rate \u9078\u64c7\uff0c\u5be6\u969b\u4e0a\u53d6\u503c\u53d6\u6c7a\u65bc\u6578\u64da\u6a23\u672c\uff0c\u5982\u679c\u640d\u5931\u51fd\u6578\u5728\u8b8a\u5c0f\uff0c\u8aaa\u660e\u53d6\u503c\u6709\u6548\uff0c\u5426\u5247\u8981\u589e\u5927 Learning rate\n        * \u81ea\u52d5\uf901\u65b0 Learning rate  - \u8870\u6e1b\u56e0\u5b50 decay\n            * \u7b97\u6cd5\uf96b\u6578\u7684\u521d\u59cb\u503c\u9078\u64c7\u3002\u521d\u59cb\u503c\u4e0d\u540c\uff0c\u7372\u5f97\u7684\u6700\u5c0f\u503c\u4e5f\u6709\u53ef\u80fd\u4e0d\u540c\uff0c\u56e0\u6b64\u68af\u5ea6\u4e0b\u964d\u6c42\u5f97\u7684\u53ea\u662f\u5c40\u90e8\u6700\u5c0f\u503c\uff1b\u7576\u7136\u5982\u679c\u640d\u5931\u51fd\u6578\u662f\u51f8\u51fd\u6578\u5247\u4e00\u5b9a\u662f\u6700\u512a\u89e3\u3002\n            * \u5b78\u7fd2\u7387\u8870\u6e1b\u516c\u5f0f\n                * lr_i = lr_start * 1.0 / (1.0 + decay * i)\n                * \u5176\u4e2d lr_i \u70ba\u7b2c\u4e00\u8fed\u4ee3 i \u6642\u7684\u5b78\u7fd2\u7387\uff0clr_start \u70ba\u521d\u59cb\u503c\uff0cdecay \u70ba\u4e00\u500b\u4ecb\u65bc[0.0, 1.0]\u7684\u5c0f\u6578\u3002\u5f9e\u516c\u5f0f\u4e0a\u53ef\u770b\u51fa\uff1a\n                    * decay \u8d8a\u5c0f\uff0c\u5b78\u7fd2\u7387\u8870\u6e1b\u5730\u8d8a\u6162\uff0c\u7576 decay = 0 \u6642\uff0c\u5b78\u7fd2\u7387\u4fdd\u6301\u4e0d\u8b8a\n                    * decay \u8d8a\u5927\uff0c\u5b78\u7fd2\u7387\u8870\u6e1b\u5730\u8d8a\u5feb\uff0c\u7576 decay = 1 \u6642\uff0c\u5b78\u7fd2\u7387\u8870\u6e1b\u6700\u5feb\n        * \u4f7f\u7528 momentum \u662f\u68af\u5ea6\u4e0b\u964d\u6cd5\u4e2d\u4e00\u7a2e\u5e38\u7528\u7684\u52a0\u901f\u6280\u8853\u3002\n        $x \u2190 x\\ \u2212\\ \\alpha\\ \u2217\\ dx$ (x\u6cbf\u8ca0\u68af\u5ea6\u65b9\u5411\u4e0b\u964d)\n        $v\\ = \\beta\\ \u2217\\ v\\ \u2212\\ \\alpha\\ \u2217\\ dx$\n        $x\\ \u2190\\ x\\ +\\ v$\n        * \u5176\u4e2d $\\beta$ \u5373 momentum \u4fc2\u6578\uff0c\u901a\u4fd7\u7684\uf9e4\u89e3\u4e0a\u9762\u5f0f\u5b50\u5c31\u662f\uff0c\u5982\u679c\u4e0a\u4e00\u6b21\u7684 momentum\uff08\u5373 $\\beta$ \uff09\u8207\u9019\u4e00\u6b21\u7684\u8ca0\u68af\u5ea6\u65b9\u5411\u662f\u76f8\u540c\u7684\uff0c\u90a3\u9019\u6b21\u4e0b\u964d\u7684\u5e45\u5ea6\u5c31\u6703\u52a0\u5927\uff0c\u6240\u4ee5\u9019\u6a23\u505a\u80fd\u5920\u9054\u5230\u52a0\u901f\u6536\u6582\u7684\u904e\u7a0b\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [gradient descent using python and numpy](https://stackoverflow.com/questions/17784587/gradient-descent-using-python-and-numpy)\n        * [\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u53c2\u6570\u66f4\u65b0\u516c\u5f0f](https://blog.csdn.net/hrkxhll/article/details/80395033)\n        ```py\n        import matplotlib\n        import matplotlib.pyplot as plt\n        %matplotlib inline \n        #\u9069\u7528\u65bc Jupyter Notebook, \u5ba3\u544a\u76f4\u63a5\u5728cell \u5167\u5370\u51fa\u57f7\u884c\u7d50\u679c\n        import random as random\n        import numpy as np\n        import csv\n\n        # \u7d66\u5b9a\u521d\u59cb\u7684data\n        x_data = [ 338., 333., 328., 207., 226., 25., 179.,  60., 208.,  606.]\n        y_data = [ 640., 633., 619., 393., 428., 27., 193.,  66., 226., 1591.]\n\n        #\u7d66\u5b9a\u795e\u7d93\u7db2\u8def\u53c3\u6578:bias \u8ddfweight\n        x = np.arange(-200,-100,1) #\u7d66\u5b9abias\n        y = np.arange(-5,5,0.1) #\u7d66\u5b9aweight\n\n        Z =  np.zeros((len(x), len(y)))\n        #meshgrid\u8fd4\u56de\u7684\u5169\u500b\u77e9\u9663X\u3001Y\u5fc5\u5b9a\u662f column \u6578\u3001row \u6578\u76f8\u7b49\u7684\uff0c\u4e14X\u3001Y\u7684 column \u6578\u90fd\u7b49\n        #meshgrid\u51fd\u6578\u7528\u5169\u500b\u5750\u6a19\u8ef8\u4e0a\u7684\u9ede\u5728\u5e73\u9762\u4e0a\u756b\u683c\u3002\n        X, Y = np.meshgrid(x, y)\n        for i in range(len(x)):\n            for j in range(len(y)):\n                b = x[i]\n                w = y[j]\n                Z[j][i] = 0  \n                for n in range(len(x_data)):\n                    Z[j][i] = Z[j][i] +  (y_data[n] - b - w*x_data[n])**2\n                Z[j][i] = Z[j][i]/len(x_data)\n\n        # ydata = b + w * xdata \n        b = -120 # initial b\n        w = -4 # initial w\n        lr = 0.000001 # learning rate\n        iteration = 100000\n\n        # Store initial values for plotting.\n        b_history = [b]\n        w_history = [w]\n\n        #\u7d66\u5b9a\u521d\u59cb\u503c\n        lr_b = 0.0\n        lr_w = 0.0\n\n        '''\n        Loss = (\u5be6\u969bydata \u2013 \u9810\u6e2cydata)\n        Gradient = -2*input * Loss \n        \u8abf\u6574\u5f8c\u7684\u6b0a\u91cd = \u539f\u6b0a\u91cd \u2013 Learning * Gradient\n        '''\n        # Iterations\n        for i in range(iteration):\n            \n            b_grad = 0.0\n            w_grad = 0.0\n            for n in range(len(x_data)):        \n                b_grad = b_grad  - 2.0*(y_data[n] - b - w*x_data[n])*1.0\n                w_grad = w_grad  - 2.0*(y_data[n] - b - w*x_data[n])*x_data[n]\n                \n            lr_b = lr_b + b_grad ** 2\n            lr_w = lr_w + w_grad ** 2\n            \n            # Update parameters.\n            b = b - lr * b_grad \n            w = w - lr * w_grad\n            \n            # Store parameters for plotting\n            b_history.append(b)\n            w_history.append(w)\n\n        # plot the figure\n        plt.contourf(x,y,Z, 50, alpha=0.5, cmap=plt.get_cmap('jet'))\n        plt.plot([-188.4], [2.67], 'x', ms=12, markeredgewidth=3, color='orange')\n        plt.plot(b_history, w_history, 'o-', ms=3, lw=1.5, color='black')\n        plt.xlim(-200,-100)\n        plt.ylim(-5,5)\n        plt.xlabel(r'$b$', fontsize=16)\n        plt.ylabel(r'$w$', fontsize=16)\n        plt.show()\n        ```\n* **Day_75 : \u53cd\u5411\u5f0f\u50b3\u64ad\u7c21\u4ecb**\n    * \u4f55\u8b02\u53cd\u5411\u50b3\u64ad\n        * **\u53cd\u5411\u50b3\u64ad**\uff08BP\uff1aBackpropagation\uff09\u662f\u300c\u8aa4\u5dee\u53cd\u5411\u50b3\u64ad\u300d\u7684\u7c21\u7a31\uff0c\u662f\u4e00\u7a2e\u8207\u6700\u512a\u5316\u65b9\u6cd5\uff08\u5982\u68af\u5ea6\u4e0b\u964d\u6cd5\uff09\u7d50\u5408\u4f7f\u7528\u7684\u8a72\u65b9\u6cd5\u5c0d\u7db2\uf937\u4e2d\u6240\u6709\u6b0a\u91cd\u8a08\u7b97\u640d\u5931\u51fd\u6578\u7684\u68af\u5ea6\u3002\u9019\u500b\u68af\u5ea6\u6703\u53cd\u994b\u7d66\u6700\u512a\u5316\u65b9\u6cd5\uff0c\u7528\uf92d\uf901\u65b0\u6b0a\u503c\u4ee5\u6700\u5c0f\u5316\u640d\u5931\u51fd\u6578\u3002\n        * \u53cd\u5411\u50b3\u64ad\u8981\u6c42\u6709\u5c0d\u6bcf\u500b\u8f38\u5165\u503c\u60f3\u5f97\u5230\u7684\u5df2\u77e5\u8f38\u51fa\uff0c\uf92d\u8a08\u7b97\u640d\u5931\u51fd\u6578\u68af\u5ea6\u3002\u56e0\u6b64\uff0c\u5b83\u901a\u5e38\u88ab\u8a8d\u70ba\u662f\u4e00\u7a2e\u76e3\u7763\u5f0f\u5b78\u7fd2\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c0d\u6bcf\u5c64\u758a\u4ee3\u8a08\u7b97\u68af\u5ea6\u3002\u53cd\u5411\u50b3\u64ad\u8981\u6c42\u4eba\u5de5\u795e\u7d93\u5143\uff08\u6216\u300c\u7bc0\u9ede\u300d\uff09\u7684\u555f\u52d5\u51fd\u6578\u53ef\u5fae\u3002\n    * \u63a8\u5c0e\u6d41\u7a0b :\n        * $\\to$ \u5efa\u7acb\u795e\u7d93\u7db2\u8def (Input\u3001Hidden\u3001Output)  \n        $\\to$ \u62c6\u89e3\u795e\u7d93\u7db2\u8def\u70ba\u5c40\u90e8\u8a2d\u8a08\u7b97\u55ae\u5143 \n        $\\to$  \u7d66\u5b9a\u8a08\u7b97\u55ae\u5143 \n        $\\to$ \u795e\u7d93\u7db2\u8def\u521d\u59cb\u5316 (init value) \n        $\\to$ Forward Propagation \n        $\\to$ \u53d6\u5f97 OUTPUT \n        $\\to$ \u89e3\u51fd\u6578\u5fae\u5206 \n        $\\to$ Back Propagation\n    * BP \u795e\u7d93\u7db2\uf937\u662f\u4e00\u7a2e\u6309\u7167\u9006\u5411\u50b3\u64ad\u7b97\u6cd5\u8a13\u7df4\u7684\u591a\u5c64\u524d\u994b\u795e\u7d93\u7db2\uf937\n        * \u512a\u9ede\uff1a\u5177\u6709\u4efb\u610f\u8907\u96dc\u7684\u6a21\u5f0f\u5206\u985e\u80fd\uf98a\u548c\u512a\u826f\u7684\u591a\u7dad\u51fd\u6578\u6620\u5c04\u80fd\uf98a\uff0c\u89e3\u6c7a\uf9ba\u7c21\u55ae\u611f\u77e5\u5668\u4e0d\u80fd\u89e3\u6c7a\u7684\u4e00\u4e9b\u5176\u4ed6\u7684\u554f\u984c\u3002\n            * \u5f9e\u7d50\u69cb\u4e0a\u8b1b\uff0cBP \u795e\u7d93\u7db2\uf937\u5177\u6709\u8f38\u5165\u5c64\u3001\u96b1\u542b\u5c64\u548c\u8f38\u51fa\u5c64\u3002\n            * \u5f9e\u672c\u8cea\u4e0a\u8b1b\uff0cBP \u7b97\u6cd5\u5c31\u662f\u4ee5\u7db2\uf937\u8aa4\u5dee\u5e73\u65b9\u76ee\u6a19\u51fd\u6578\u3001\u63a1\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\uf92d\u8a08\u7b97\u76ee\u6a19\u51fd\u6578\u7684\u6700\u5c0f\u503c\u3002\n        * \u7f3a\u9ede\uff1a\n            * \u2460\u5b78\u7fd2\u901f\u5ea6\u6162\uff0c\u5373\u4f7f\u662f\u4e00\u500b\u7c21\u55ae\u7684\u904e\u7a0b\uff0c\u4e5f\u9700\u8981\u5e7e\u767e\u6b21\u751a\u81f3\u4e0a\u5343\u6b21\u7684\u5b78\u7fd2\u624d\u80fd\u6536\u6582\u3002\n            * \u2461\u5bb9\uf9e0\u9677\u5165\u5c40\u90e8\u6975\u5c0f\u503c\u3002\n            * \u2462\u7db2\uf937\u5c64\u6578\u3001\u795e\u7d93\u5143\u500b\u6578\u7684\u9078\u64c7\u6c92\u6709\u76f8\u61c9\u7684\uf9e4\u8ad6\u6307\u5c0e\u3002\n            * \u2463\u7db2\uf937\u63a8\u5ee3\u80fd\uf98a\u6709\u9650\u3002\n        * \u61c9\u7528\uff1a\n            * \u2460\u51fd\u6578\u903c\u8fd1\u3002\n            * \u2461\u6a21\u5f0f\u8b58\u5225\u3002\n            * \u2462\u5206\u985e\u3002\n            * \u2463\u6578\u64da\u58d3\u7e2e\u3002\n    * \u91cd\u8981\u77e5\u8b58\u9ede\u8907\u7fd2\uff1a\n        * \u7b2c1\u968e\u6bb5\uff1a\u89e3\u51fd\u6578\u5fae\u5206\n            * \u6bcf\u6b21\u758a\u4ee3\u4e2d\u7684\u50b3\u64ad\u74b0\u7bc0\u5305\u542b\uf978\u6b65\uff1a\n                * \uff08\u524d\u5411\u50b3\u64ad\u968e\u6bb5\uff09\u5c07\u8a13\u7df4\u8f38\u5165\u9001\u5165\u7db2\uf937\u4ee5\u7372\u5f97\u555f\u52d5\u97ff\u61c9\uff1b\n                * \uff08\u53cd\u5411\u50b3\u64ad\u968e\u6bb5\uff09\u5c07\u555f\u52d5\u97ff\u61c9\u540c\u8a13\u7df4\u8f38\u5165\u5c0d\u61c9\u7684\u76ee\u6a19\u8f38\u51fa\u6c42\u5dee\uff0c\u5f9e\u800c\u7372\u5f97\u8f38\u51fa\u5c64\u548c\u96b1\u85cf\u5c64\u7684\u97ff\u61c9\u8aa4\u5dee\u3002\n        * \u7b2c2\u968e\u6bb5\uff1a\u6b0a\u91cd\uf901\u65b0\n            * Follow Gradient Descent \n            * \u7b2c 1 \u548c\u7b2c 2 \u968e\u6bb5\u53ef\u4ee5\u53cd\u8986\u5faa\u74b0\u758a\u4ee3\uff0c\u76f4\u5230\u7db2\uf937\u5c0d\u8f38\u5165\u7684\u97ff\u61c9\u9054\u5230\u6eff\u610f\u7684\u9810\u5b9a\u7684\u76ee\u6a19\u7bc4\u570d\u70ba\u6b62\u3002\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u6df1\u5ea6\u5b78\u7fd2(Deep Learning)-\u53cd\u5411\u50b3\u64ad](https://ithelp.ithome.com.tw/articles/10198813)\n        * [BP\u795e\u7ecf\u7f51\u7edc\u7684\u539f\u7406\u53caPython\u5b9e\u73b0](https://blog.csdn.net/conggova/article/details/77799464)\n        ```py\n        #\u5b9a\u7fa9\u4e26\u5efa\u7acb\u4e00\u795e\u7d93\u7db2\u8def\n        class mul_layer():\n            def _ini_(self):\n                self.x = None\n                self.y = None\n            def forward(self, x, y):\n                self.x = x\n                self.y = y\n                out = x*y\n                return out\n            def backward(self, dout):\n                dx = dout * self.y\n                dy = dout * self.x\n                return dx, dy\n\n        # \u521d\u59cb\u503c\u8a2d\u5b9a\n        n_X = 2\n        price_Y = 100\n        b_TAX = 1.1\n\n        # \u6307\u5b9aBuild _Network\u7d44\u5408\n        mul_fruit_layer = mul_layer()\n        mul_tax_layer = mul_layer()\n\n        #forward \n        fruit_price = mul_fruit_layer.forward(price_Y, n_X)\n        total_price = mul_tax_layer.forward(fruit_price, b_TAX)\n\n        #backward \n        dtotal_price = 1 #this is linear function, which y=x, dy/dx=1\n        d_fruit_price, d_b_TAX = mul_tax_layer.backward(dtotal_price)\n        d_price_Y, d_n_X =  mul_tax_layer.backward(d_fruit_price)\n        ```\n* **Day_76 : \u512a\u5316\u5668 Optimizers \u7c21\u4ecb**\n    * \uf9fd\u9ebc\u662f**\u512a\u5316\u7b97\u6cd5 - Optimizer**\n        * \u6a5f\u5668\u5b78\u7fd2\u7b97\u6cd5\u7576\u4e2d\uff0c\u5927\u90e8\u5206\u7b97\u6cd5\u7684\u672c\u8cea\u5c31\u662f\u5efa\u7acb\u512a\u5316\u6a21\u578b\uff0c\u901a\u904e\u6700\u512a\u5316\u65b9\u6cd5\u5c0d\u76ee\u6a19\u51fd\u6578\u9032\ufa08\u512a\u5316\u5f9e\u800c\u8a13\u7df4\u51fa\u6700\u597d\u7684\u6a21\u578b\n        * \u512a\u5316\u7b97\u6cd5\u7684\u529f\u80fd\uff0c\u662f\u901a\u904e\u6539\u5584\u8a13\u7df4\u65b9\u5f0f\uff0c\uf92d\u6700\u5c0f\u5316 (\u6216\u6700\u5927\u5316) \u640d\u5931\u51fd\u6578 E(x)\n        * \u512a\u5316\u7b56\uf976\u548c\u7b97\u6cd5\uff0c\u662f\u7528\uf92d\uf901\u65b0\u548c\u8a08\u7b97\u5f71\u97ff\u6a21\u578b\u8a13\u7df4\u548c\u6a21\u578b\u8f38\u51fa\u7684\u7db2\u7d61\uf96b\u6578\uff0c\u4f7f\u5176\u903c\u8fd1\u6216\u9054\u5230\u6700\u512a\u503c\n    * \u6700\u5e38\u7528\u7684\u512a\u5316\u7b97\u6cd5\n        * Gradient Descent\n            * \u6700\u5e38\u7528\u7684\u512a\u5316\u7b97\u6cd5\u662f\u68af\u5ea6\u4e0b\u964d\n            * \u9019\u7a2e\u7b97\u6cd5\u4f7f\u7528\u5404\uf96b\u6578\u7684\u68af\u5ea6\u503c\uf92d\u6700\u5c0f\u5316\u6216\u6700\u5927\u5316\u640d\u5931\u51fd\u6578 E(x)\u3002\n            * \u901a\u904e\u5c0b\u627e\u6700\u5c0f\u503c\uff0c\u63a7\u5236\u65b9\u5dee\uff0c\uf901\u65b0\u6a21\u578b\uf96b\u6578\uff0c\u6700\u7d42\u4f7f\u6a21\u578b\u6536\u6582\n        * \u52d5\u91cf Momentum\n            * \u300c\u4e00\u9846\u7403\u5f9e\u5c71\u4e0a\u6efe\u4e0b\uf92d\uff0c\u5728\u4e0b\u5761\u7684\u6642\u5019\u901f\u5ea6\u8d8a\uf92d\u8d8a\u5feb\uff0c\u9047\u5230\u4e0a\u5761\uff0c\u65b9\u5411\u6539\u8b8a\uff0c\u901f\u5ea6\u4e0b\u964d\u300d\n            $$ V_t \\leftarrow \\beta V_{t-1} - \\eta\\frac{\\partial{L}}{\\partial{w}}$$\n            $$ w \\leftarrow w + V_t $$\n            * $v_t$ : \u300c\u65b9\u5411\u901f\u5ea6\u300d\uff0c\u6703\u8ddf\u4e0a\u4e00\u6b21\u7684\uf901\u65b0\u6709\u95dc\n            * \u5982\u679c\u4e0a\u4e00\u6b21\u7684\u68af\u5ea6\u8ddf\u9019\u6b21\u540c\u65b9\u5411\u7684\u8a71\uff0c$|V_t|$ (\u901f\u5ea6)\u6703\u8d8a\u4f86\uf92d\u8d8a\u5927(\u4ee3\u8868\u68af\u5ea6\u589e\u5f37)\uff0c$w$ \uf96b\u6578\u7684\uf901\u65b0\u68af\u5ea6\uf965\u6703\u8d8a\uf92d\u8d8a\u5feb\uff0c\u5982\u679c\u65b9\u5411\u4e0d\u540c\uff0c$|V_t|$ \uf965\u6703\u6bd4\u4e0a\u6b21\uf901\u5c0f(\u68af\u5ea6\u6e1b\u5f31)\uff0c$w$ \uf96b\u6578\u7684\uf901\u65b0\u68af\u5ea6\uf965\u6703\u8b8a\u5c0f\n            * \u52a0\u5165\u7684\u9019\u4e00\u9805\uff0c\u53ef\u4ee5\u4f7f\u5f97\u68af\u5ea6\u65b9\u5411\u4e0d\u8b8a\u7684\u7dad\u5ea6\u4e0a\u901f\u5ea6\u8b8a\u5feb\uff0c\u68af\u5ea6\u65b9\u5411\u6709\u6240\u6539\u8b8a\u7684\u7dad\u5ea6\u4e0a\u7684\uf901\u65b0\u901f\u5ea6\u8b8a\u6162\uff0c\u9019\u6a23\u5c31\u53ef\u4ee5\u52a0\u5feb\u6536\u6582\u4e26\u6e1b\u5c0f\u9707\u76ea\n        * SGD - \u96a8\u6a5f\u68af\u5ea6\u4e0b\u964d\u6cd5 (stochastic gradient decent)\n            * \u627e\u51fa\uf96b\u6578\u7684\u68af\u5ea6 (\uf9dd\u7528\u5fae\u5206\u7684\u65b9\u6cd5)\uff0c\u5f80\u68af\u5ea6\u7684\u65b9\u5411\u53bb\uf901\u65b0\uf96b\u6578 (weight)\n            $$ w \\leftarrow w -\\eta \\frac{\\partial{L}}{\\partial{w}}$$\n            * $w$ \u70ba\u6b0a\u91cd (weight) \uf96b\u6578\uff0c$L$ \u70ba\u640d\u5931\u51fd\u6578 (loss function)\uff0c $\\eta$ \u662f\u5b78\u7fd2\u7387 (learning rate)\uff0c $\\frac{\\partial{L}}{\\partial{w}}$ \u662f\u640d\u5931\u51fd\u6578\u5c0d\uf96b\u6578\u7684\u68af\u5ea6 (\u5fae\u5206)\n            * \u512a\u9ede\uff1aSGD \u6bcf\u6b21\uf901\u65b0\u6642\u5c0d\u6bcf\u500b\u6a23\u672c\u9032\ufa08\u68af\u5ea6\uf901\u65b0\uff0c\u5c0d\u65bc\u5f88\u5927\u7684\u6578\u64da\u96c6\uf92d\u8aaa\uff0c\u53ef\u80fd\u6703\u6709\u76f8\u4f3c\u7684\u6a23\u672c\uff0c\u800c SGD \u4e00\u6b21\u53ea\u9032\ufa08\u4e00\u6b21\uf901\u65b0\uff0c\u5c31\u6c92\u6709\u5197\u9918\uff0c\u800c\u4e14\u6bd4\u8f03\u5feb\n            * \u7f3a\u9ede\uff1a\u4f46\u662f SGD \u56e0\u70ba\uf901\u65b0\u6bd4\u8f03\u983b\u7e41\uff0c\u6703\u9020\u6210 cost function \u6709\u56b4\u91cd\u7684\u9707\u76ea\n        * SGD \u8abf\u7528\n            * `keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)`\n                * lr\uff1a<float> \u5b78\u7fd2\u7387\u3002\n                * Momentum \u52d5\uf97e\uff1a<float> \uf96b\u6578\uff0c\u7528\u65bc\u52a0\u901f SGD \u5728\u76f8\u95dc\u65b9\u5411\u4e0a\u524d\u9032\uff0c\u4e26\u6291\u5236\u9707\u76ea\u3002\n                * Decay (\u8870\u8b8a)\uff1a<float> \u6bcf\u6b21\uf96b\u6578\uf901\u65b0\u5f8c\u5b78\u7fd2\u7387\u8870\u6e1b\u503c\u3002\n                * nesterov\uff1a\u5e03\u723e\u503c\u3002\u662f\u5426\u4f7f\u7528 Nesterov \u52d5\uf97e\u3002\n            ```py\n            from keras import optimizers\n\n            model = Sequential() model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))model.add(Activation('softmax\u2019)) \n            #\u5be6\uf9b5\u5316\u4e00\u500b\u512a\u5316\u5668\u5c0d\u8c61\uff0c\u7136\u5f8c\u5c07\u5b83\u50b3\u5165model.compile()\uff0c\u53ef\u4ee5\u4fee\u6539\uf96b\u6578\n            sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd)\n            # \u901a\u904e\u540d\u7a31\uf92d\u8abf\u7528\u512a\u5316\u5668\uff0c\u5c07\u4f7f\u7528\u512a\u5316\u5668\u7684\u9ed8\u8a8d\uf96b\u6578\u3002\n            model.compile(loss='mean_squared_error', optimizer='sgd')\n            ```\n        * mini-batch gradient descent\n            * batch-gradient\uff0c\u5176\u5be6\u5c31\u662f\u666e\u901a\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f46\u662f\u63a1\u2f64\u7528\u6279\uf97e\u8655\uf9e4\u3002\n                * \u7576\u6578\u64da\u96c6\u5f88\u5927\uff08\u6bd4\u5982\u6709 100000 \u500b\u5de6\u53f3\u6642\uff09\uff0c\u6bcf\u6b21 iteration \u90fd\u8981\u5c07 1000000 \u500b\u6578\u64da\u8dd1\u4e00\u904d\uff0c\u6a5f\u5668\u5e36\u4e0d\u52d5\u3002\u65bc\u662f\u6709\uf9ba mini-batch-gradient \u2014\u2014 \u5c07 1000000 \u500b\u6a23\u672c\u5206\u6210 1000 \u4efd\uff0c\u6bcf\u4efd 1000 \u500b\uff0c\u90fd\u770b\u6210\u4e00\u7d44\u7368\u7acb\u7684\u6578\u64da\u96c6\uff0c\u9032\ufa08 forward_propagation \u548c backward_propagation\u3002\n            * \u5728\u6574\u500b\u7b97\u6cd5\u7684\u6d41\u7a0b\u4e2d\uff0ccost function \u662f\u5c40\u90e8\u7684\uff0c\u4f46\u662f W \u548c b \u662f\u5168\u5c40\u7684\u3002\n                * \u6279\uf97e\u68af\u5ea6\u4e0b\u964d\u5c0d\u8a13\u7df4\u96c6\u4e0a\u6bcf\u4e00\u500b\u6578\u64da\u90fd\u8a08\u7b97\u8aa4\u5dee\uff0c\u4f46\u53ea\u5728\u6240\u6709\u8a13\u7df4\u6578\u64da\u8a08\u7b97\u5b8c\u6210\u5f8c\u624d\uf901\u65b0\u6a21\u578b\u3002\n                * \u5c0d\u8a13\u7df4\u96c6\u4e0a\u7684\u4e00\u6b21\u8a13\u7df4\u904e\u7a0b\u7a31\u70ba\u4e00\u4ee3\uff08epoch\uff09\u3002\u56e0\u6b64\uff0c\u6279\uf97e\u68af\u5ea6\u4e0b\u964d\u662f\u5728\u6bcf\u4e00\u500b\u8a13\u7df4 epoch \u4e4b\u5f8c\uf901\u65b0\u6a21\u578b\u3002\n            * batchsize\uff1a\u6279\uf97e\u5927\u5c0f\uff0c\u5373\u6bcf\u6b21\u8a13\u7df4\u5728\u8a13\u7df4\u96c6\u4e2d\u53d6batchsize \u500b\u6a23\u672c\u8a13\u7df4\uff1b\n                * batchsize = 1; \n                * batchsize = mini-batch; \n                * batchsize = whole training set \n            * iteration\uff1a1 \u500b iteration \u7b49\u65bc\u4f7f\u7528 batchsize \u500b\u6a23\u672c\u8a13\u7df4\u4e00\u6b21\uff1b\n            * epoch\uff1a1 \u500b epoch \u7b49\u65bc\u4f7f\u7528\u8a13\u7df4\u96c6\u4e2d\u7684\u5168\u90e8\u6a23\u672c\u8a13\u7df4\u4e00\u6b21\uff1b\n            * Example:\n                ```\n                features is (50000, 400) \n                labels is (50000, 10) \n                batch_size is 128\n                Iteration = 50000/128+1 = 391\n                ```\n            * \u600e\u9ebc\u914d\u7f6e mini-batch \u68af\u5ea6\u4e0b\u964d\n                * Mini-batch sizes\uff0c\u7c21\u7a31\u70ba\u300cbatch sizes\u300d\uff0c\u662f\u7b97\u6cd5\u8a2d\u8a08\u4e2d\u9700\u8981\u8abf\u7bc0\u7684\uf96b\u6578\u3002\n                * \u8f03\u5c0f\u7684\u503c\u8b93\u5b78\u7fd2\u904e\u7a0b\u6536\u6582\uf901\u5feb\uff0c\u4f46\u662f\u7522\u751f\uf901\u591a\u566a\u8072\u3002\n                * \u8f03\u5927\u7684\u503c\u8b93\u5b78\u7fd2\u904e\u7a0b\u6536\u6582\u8f03\u6162\uff0c\u4f46\u662f\u6e96\u78ba\u7684\u4f30\u8a08\u8aa4\u5dee\u68af\u5ea6\u3002\n                * batch size \u7684\u9ed8\u8a8d\u503c\u6700\u597d\u662f 32 \u76e1\uf97e\u9078\u64c7 2 \u7684\u51aa\u6b21\u2f45\u65b9\uff0c\u6709\uf9dd\u65bc GPU \u7684\u52a0\u901f\u3002\n                * \u8abf\u7bc0 batch size \u6642\uff0c\u6700\u597d\u89c0\u5bdf\u6a21\u578b\u5728\u4e0d\u540c batch size \u4e0b\u7684\u8a13\u7df4\u6642\u9593\u548c\u9a57\u8b49\u8aa4\u5dee\u7684\u5b78\u7fd2\u66f2\u7dda\u3002\n                * \u8abf\u6574\u5176\u4ed6\u6240\u6709\u8d85\uf96b\u6578\u4e4b\u5f8c\u518d\u8abf\u6574 batch size \u548c\u5b78\u7fd2\u7387\u3002\n        * Adagrad\n            * \u5c0d\u65bc\u5e38\ufa0a\u7684\u6578\u64da\u7d66\u4e88\u6bd4\u8f03\u5c0f\u7684\u5b78\u7fd2\u7387\u53bb\u8abf\u6574\uf96b\u6578\uff0c\u5c0d\u65bc\u4e0d\u5e38\ufa0a\u7684\u6578\u64da\u7d66\u4e88\u6bd4\u8f03\u5927\u7684\u5b78\u7fd2\u7387\u8abf\u6574\uf96b\u6578\n                * \u6bcf\u500b\uf96b\u6578\u90fd\u6709\u4e0d\u540c\u7684 learning rate\n                * \u6839\u64da\u4e4b\u524d\u6240\u6709 gradient \u7684 root mean square \u4fee\u6539\n            * \u7b2c t \u6b21\u66f4\u65b0\n                $$ g^t = \\frac{\\partial{L}}{\\partial{\\theta}}|_{\\theta={\\theta^t}}$$\n                * Gradient descent\n                    $$ \\theta^{t+1} = \\theta^t - \\eta g^t$$\n                * Adagrad\n                    $$ \\theta^{t+1} = \\theta^t - \\frac{\\eta}{\\sigma^t} g^t$$\n                    $$ \\sigma^t = \\sqrt{\\frac{(g^0)^2+...+(g^t)^2}{t+1}} $$\n        * Adagrad \u8abf\u7528\n            * \u8d85\uf96b\u6578\u8a2d\u5b9a\u503c :\n            `keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)`\n                * lr\uff1afloat >= 0. \u5b78\u7fd2\u7387\u3002\u4e00\u822c $\\eta$ \u5c31\u53d6 0.01\n                * epsilon\uff1a float >= 0\u3002\uf974\u70ba None\uff0c\u9ed8\u8a8d\u70ba K.epsilon()\n                * decay\uff1afloat >= 0\u3002\u6bcf\u6b21\uf96b\u6578\uf901\u65b0\u5f8c\u5b78\u7fd2\u7387\u8870\u6e1b\u503c\n                ```py\n                from keras import optimizers \n\n                model = Sequential() \n                model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n                model.add(Activation('softmax\u2019)) \n\n                #\u5be6\uf9b5\u5316\u4e00\u500b\u512a\u5316\u5668\u5c0d\u8c61\uff0c\u7136\u5f8c\u5c07\u5b83\u50b3\u5165model.compile() , \u53ef\u4ee5\u4fee\u6539\uf96b\u6578 \n                opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n                model.compile(loss='mean_squared_error', optimizer=opt) \n                ```\n        * RMSprop\n            * RMSProp \u7b97\u6cd5\u4e5f\u65e8\u5728\u6291\u5236\u68af\u5ea6\u7684\u92f8\u9f52\u4e0b\u964d\uff0c\u4f46\u8207\u52d5\uf97e\u76f8\u6bd4\uff0c RMSProp \u4e0d\u9700\u8981\u624b\u52d5\u914d\u7f6e\u5b78\u7fd2\u7387\u8d85\uf96b\u6578\uff0c\u7531\u7b97\u6cd5\u81ea\u52d5\u5b8c\u6210\u3002\uf901\u91cd\u8981\u7684\u662f\uff0cRMSProp \u53ef\u4ee5\u70ba\u6bcf\u500b\uf96b\u6578\u9078\u64c7\u4e0d\u540c\u7684\u5b78\u7fd2\u7387\u3002\n            * RMSprop \u662f\u70ba\uf9ba\u89e3\u6c7a Adagrad \u5b78\u7fd2\u7387\u6025\u5287\u4e0b\u964d\u554f\u984c\u7684\uff0c\u6240\u4ee5\u6bd4\u5c0d\u68af\u5ea6\uf901\u65b0\u898f\u5247\uff1a\n                * Adagrad\n                    $$ \\theta^{t+1} = \\theta^t - \\frac{\\eta}{\\sigma^t} g^t$$\n                    $$ \\sigma^t = \\sqrt{\\frac{(g^0)^2+...+(g^t)^2}{t+1}} $$\n                    Root mean square (RMS) of all Gradient\n                * RMSprop\n                    $$ \\theta^{t+1} = \\theta^t - \\frac{\\eta}{\\sqrt{r^t}} g^t$$\n                    $$ r^t = (1 - p)(g^t)^2 + pr^{t-1}$$\n                    \u5206\u6bcd\u63db\u6210\uf9ba\u904e\u53bb\u7684\u68af\u5ea6\u5e73\u65b9\u7684\u8870\u6e1b\u5e73\u5747\u503c\n        * RMSprop \u8abf\u7528\n            `keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)`\n            * This optimizer is usually a good choice for recurrent neural networks.Arguments\n                * lr\uff1afloat >= 0\u3002Learning rate. \n                * rho\uff1afloat >= 0\u3002 \n                * epsilon\uff1afloat >= 0\u3002Fuzz factor. If None, defaults to K.epsilon()\u3002\n                * decay\uff1afloat >= 0\u3002 Learning rate decay over each update\u3002\n                ```py\n                from keras import optimizers\n                \n                model = Sequential() \n                model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n                model.add(Activation('softmax\u2019)) \n                \n                #\u5be6\uf9b5\u5316\u4e00\u500b\u512a\u5316\u5668\u5c0d\u8c61\uff0c\u7136\u5f8c\u5c07\u5b83\u50b3\u5165model.compile() , \u53ef\u4ee5\u4fee\u6539\uf96b\u6578\n                opt = optimizers.RMSprop(lr=0.001, epsilon=None, decay=0.0) \n                model.compile(loss='mean_squared_error', optimizer=opt) \n                ```\n        * Adam \u8aaa\u660e\n            * \u9664\uf9ba\u50cf RMSprop \u4e00\u6a23\u5b58\u5132\uf9ba\u904e\u53bb\u68af\u5ea6\u7684\u5e73\u2f45\u65b9 $v_t$ \u7684\u6307\u6578\u8870\u6e1b\u5e73\u5747\u503c\uff0c\u4e5f\u50cf momentum \u4e00\u6a23\u4fdd\u6301\uf9ba\u904e\u53bb\u68af\u5ea6 $m_t$ \u7684\u6307\u6578\u8870\u6e1b\u5e73\u5747\u503c, \u300c t \u300d\uff1a\n                * the first moment (the mean)\n                $$ m_t = \\beta_1m_t + (1 - \\beta_1)g_t$$\n                * the second moment (the uncentered variance)\n                $$ v_t = \\beta_2m_t + (1 - \\beta_2){g_t}^2$$\n            * \u8a08\u7b97\u68af\u5ea6\u7684\u6307\u6578\u79fb\u52d5\u5e73\u5747\u6578\uff0c$m_0$ \u521d\u59cb\u5316\u70ba 0\u3002\u7d9c\u5408\u8003\u616e\u4e4b\u524d\u6642\u9593\u6b65\u7684\u68af\u5ea6\u52d5\uf97e\u3002\n            * \u03b21 \u4fc2\u6578\u70ba\u6307\u6578\u8870\u6e1b\u7387\uff0c\u63a7\u5236\u6b0a\u91cd\u5206\u914d\uff08\u52d5\uf97e\u8207\u7576\u524d\u68af\u5ea6\uff09\uff0c\u901a\u5e38\u53d6\u63a5\u8fd1\u65bc 1 \u7684\u503c\u3002\u9ed8\u8a8d\u70ba 0.9 \n            * \u5176\u6b21\uff0c\u8a08\u7b97\u68af\u5ea6\u5e73\u65b9\u7684\u6307\u6578\u79fb\u52d5\u5e73\u5747\u6578\uff0c$v_0$ \u521d\u59cb\u5316\u70ba 0\u3002\n            * \u03b22 \u4fc2\u6578\u70ba\u6307\u6578\u8870\u6e1b\u7387\uff0c\u63a7\u5236\u4e4b\u524d\u7684\u68af\u5ea6\u5e73\u2f45\u65b9\u7684\u5f71\u97ff\u60c5\u6cc1\u3002\u985e\u4f3c\u65bc RMSProp \u7b97\u6cd5\uff0c\u5c0d\u68af\u5ea6\u5e73\u65b9\u9032\ufa08\u52a0\u6b0a\u5747\u503c\u3002\u9ed8\u8a8d\u70ba 0.999\n            * \u7531\u65bc $m_0$ \u521d\u59cb\u5316\u70ba 0\uff0c\u6703\u5c0e\u81f4 $m_t$ \u504f\u5411\u65bc 0\uff0c\u5c24\u5176\u5728\u8a13\u7df4\u521d\u671f\u968e\u6bb5\u3002\u6240\u4ee5\uff0c\u6b64\u8655\u9700\u8981\u5c0d\u68af\u5ea6\u5747\u503c $m_t$ \u9032\ufa08\u504f\u5dee\u7cfe\u6b63\uff0c\u964d\u4f4e\u504f\u5dee\u5c0d\u8a13\u7df4\u521d\u671f\u7684\u5f71\u97ff\u3002\u8207 $m_0$ \u985e\u4f3c\uff0c\u56e0\u70ba $v_0$ \u521d\u59cb\u5316\u70ba 0 \u5c0e\u81f4\u8a13\u7df4\u521d\u59cb\u968e\u6bb5 $v_t$ \u504f\u5411 0\uff0c\u5c0d\u5176\u9032\ufa08\u7cfe\u6b63\u3002\n                $$ \\hat{m_t} = \\frac{m_t}{1 - {{\\beta^t}_1}}$$\n                $$ \\hat{v_t} = \\frac{v_t}{1 - {{\\beta^t}_2}}$$\n            * \uf901\u65b0\uf96b\u6578\uff0c\u521d\u59cb\u7684\u5b78\u7fd2\u7387 lr \u4e58\u4ee5\u68af\u5ea6\u5747\u503c\u8207\u68af\u5ea6\u65b9\u5dee\u7684\u5e73\u65b9\u6839\u4e4b\u6bd4\u3002\u5176\u4e2d\u9ed8\u8a8d\u5b78\u7fd2\u7387 lr =0.001, eplison (\u03b5=10^-8)\uff0c\u907f\u514d\u9664\u6578\u8b8a\u70ba 0\u3002\n            * \u5c0d\uf901\u65b0\u7684\u6b65\u9577\u8a08\u7b97\uff0c\u80fd\u5920\u5f9e\u68af\u5ea6\u5747\u503c\u53ca\u68af\u5ea6\u5e73\u65b9\uf978\u500b\u89d2\u5ea6\u9032\ufa08\u81ea\u9069\u61c9\u5730\u8abf\u7bc0\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u7531\u7576\u524d\u68af\u5ea6\u6c7a\u5b9a\n        * Adam \u8abf\u7528\n        `keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)`\n            * lr\uff1afloat >= 0. \u5b78\u7fd2\u7387\u3002\n            * beta_1\uff1afloat, 0 < beta < 1. \u901a\u5e38\u63a5\u8fd1\u65bc 1\u3002\n            * beta_2\uff1afloat, 0 < beta < 1. \u901a\u5e38\u63a5\u8fd1\u65bc 1\u3002\n            * epsilon\uff1afloat >= 0. \u6a21\u7cca\u56e0\u6578. \uf974\u70baNone, \u9ed8\u8a8d\u70ba K.epsilon()\u3002\n            * amsgrad\uff1aboolean. \u662f\u5426\u61c9\u7528\u6b64\u6f14\u7b97\u6cd5\u7684 AMSGrad \u8b8a\u7a2e\uff0c\uf92d\u81ea\u8ad6\u6587\u300cOn the Convergence of Adam and Beyond\u300d\n            * decay\uff1afloat >= 0. \u6bcf\u6b21\uf96b\u6578\uf901\u65b0\u5f8c\u5b78\u7fd2\u7387\u8870\u6e1b\u503c\u3002\n            ```py\n            from keras import optimizers \n\n            model = Sequential() \n            model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n            model.add(Activation('softmax\u2019)) \n\n            #\u5be6\uf9b5\u5316\u4e00\u500b\u512a\u5316\u5668\u5c0d\u8c61\uff0c\u7136\u5f8c\u5c07\u5b83\u50b3\u5165 model.compile() , \u53ef\u4ee5\u4fee\u6539\uf96b\u6578\n            opt = optimizers.Adam(lr=0.001, epsilon=None, decay=0.0) \n            model.compile(loss='mean_squared_error', optimizer=opt) \n            ```\n    * \u5982\u4f55\u9078\u64c7\u512a\u5316\u5668\n        * \u96a8\u6a5f\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\uff1aSGD \u6307\u7684\u662f mini batch gradient descent \u512a\u9ede\uff1a\u91dd\u5c0d\u5927\u6578\u64da\u96c6\uff0c\u8a13\u7df4\u901f\u5ea6\u5f88\u5feb\u3002\u5f9e\u8a13\u7df4\u96c6\u6a23\u672c\u4e2d\u96a8\u6a5f\u9078\u53d6\u4e00\u500b batch \u8a08\u7b97\u4e00\u6b21\u68af\u5ea6\uff0c\uf901\u65b0\u4e00\u6b21\u6a21\u578b\uf96b\u6578\u3002\n            * \u7f3a\u9ede\uff1a\u5c0d\u6240\u6709\uf96b\u6578\u4f7f\u7528\u76f8\u540c\u7684\u5b78\u7fd2\u7387\u3002\u5c0d\u65bc\u7a00\u758f\u6578\u64da\u6216\u7279\u5fb5\uff0c\u5e0c\u671b\u76e1\u5feb\uf901\u65b0\u4e00\u4e9b\u4e0d\u7d93\u5e38\u51fa\u73fe\u7684\u7279\u5fb5\uff0c\u6162\u4e00\u4e9b\uf901\u65b0\u5e38\u51fa\u73fe\u7684\u7279\u5fb5\u3002\u6240\u4ee5\u9078\u64c7\u5408\u9069\u7684\u5b78\u7fd2\u7387\u6bd4\u8f03\u56f0\u96e3\u3002\n        * \u5bb9\uf9e0\u6536\u6582\u5230\u5c40\u90e8\u6700\u512a Adam\uff1a\uf9dd\u7528\u68af\u5ea6\u7684\u4e00\u968e\u77e9\u4f30\u8a08\u548c\u4e8c\u968e\u77e9\u4f30\u8a08\u52d5\u614b\u8abf\u7bc0\u6bcf\u500b\uf96b\u6578\u7684\u5b78\u7fd2\u7387\u3002\n            * \u512a\u9ede\uff1a\n                1. \u7d93\u904e\u504f\u7f6e\u6821\u6b63\u5f8c\uff0c\u6bcf\u4e00\u6b21\u8fed\u4ee3\u90fd\u6709\u78ba\u5b9a\u7684\u7bc4\u570d\uff0c\u4f7f\u5f97\uf96b\u6578\u6bd4\u8f03\u5e73\u7a69\u3002\u5584\u65bc\u8655\uf9e4\u7a00\u758f\u68af\u5ea6\u548c\u975e\u5e73\u7a69\u76ee\u6a19\u3002\n                2. \u5c0d\u5167\u5b58\u9700\u6c42\u5c0f\n                3. \u5c0d\u4e0d\u540c\u5167\u5b58\u8a08\u7b97\u4e0d\u540c\u7684\u5b78\u7fd2\n        * RMSProp\uff1a\u81ea\u9069\u61c9\u8abf\u7bc0\u5b78\u7fd2\u7387\u3002\u5c0d\u5b78\u7fd2\u7387\u9032\ufa08\uf9ba\u7d04\u675f\uff0c\u9069\u5408\u8655\uf9e4\u975e\u5e73\u7a69\u76ee\u6a19\u548c RNN\u3002\n        * \u5982\u679c\u8f38\u5165\u6578\u64da\u96c6\u6bd4\u8f03\u7a00\u758f\uff0cSGD\u3001NAG \u548c\u52d5\uf97e\u9805\u7b49\u65b9\u6cd5\u53ef\u80fd\u6548\u679c\u4e0d\u597d\u3002\u56e0\u6b64\u5c0d\u65bc\u7a00\u758f\u6578\u64da\u96c6\uff0c\u61c9\u8a72\u4f7f\u7528\u67d0\u7a2e\u81ea\u9069\u61c9\u5b78\u7fd2\u7387\u7684\u65b9\u6cd5\uff0c\u4e14\u53e6\u4e00\u597d\u8655\u70ba\u4e0d\u9700\u8981\u4eba\u70ba\u8abf\u6574\u5b78\u7fd2\u7387\uff0c\u4f7f\u7528\u9ed8\u8a8d\uf96b\u6578\u5c31\u53ef\u80fd\u7372\u5f97\u6700\u512a\u503c\u3002\n            * Adagrad, RMSprop, Adam\u3002\n        * \u5982\u679c\u60f3\u4f7f\u8a13\u7df4\u6df1\u5c64\u7db2\u7d61\u6a21\u578b\u5feb\u901f\u6536\u6582\u6216\u6240\u69cb\u5efa\u7684\u795e\u7d93\u7db2\u7d61\u8f03\u70ba\u8907\u96dc\uff0c\u5247\u61c9\u8a72\u4f7f\u7528 Adam \u6216\u5176\u4ed6\u81ea\u9069\u61c9\u5b78\u7fd2\u901f\u7387\u7684\u65b9\u6cd5\uff0c\u56e0\u70ba\u9019\u4e9b\u65b9\u6cd5\u7684\u5be6\u969b\u6548\u679c\uf901\u512a\u3002  \n            * Adam \u5c31\u662f\u5728 RMSprop \u7684\u57fa\u790e\u4e0a\u52a0\uf9ba bias-correction \u548c momentum\uff0c\n            * \u96a8\u8457\u68af\u5ea6\u8b8a\u7684\u7a00\u758f\uff0cAdam \u6bd4 RMSprop \u6548\u679c\u6703\u597d\u3002\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)\n        * [Optimizers](https://keras.io/api/optimizers/)\n        * [\u4f18\u5316\u5668\u5982\u4f55\u9009\u62e9](https://blog.csdn.net/qq_35860352/article/details/80772142)\n        * [Second Order Optimization Algorithms](https://web.stanford.edu/class/msande311/lecture13.pdf)\n* **Day_77 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7**\n    * \uf9fd\u9ebc\u662f Overfitting\n        * \u904e\u5ea6\u64ec\u5408 (overfitting) \u4ee3\u8868\n            * \u8a13\u7df4\u96c6\u7684\u640d\u5931\u4e0b\u964d\u7684\u9060\u6bd4\u9a57\u8b49\u96c6\u7684\u640d\u5931\u9084\uf92d\u7684\u5feb\n            * \u9a57\u8b49\u96c6\u7684\u640d\u5931\u96a8\u8a13\u7df4\u6642\u9593\u589e\u9577\uff0c\u53cd\u800c\u4e0a\u5347\n        * \u5982\u4f55\u6aa2\u8996\u6211\u7684\u6a21\u578b\u6709\u6c92\u6709 overfitting\n            * \u5728 Keras \u4e2d\uff0c\u52a0\u5165\u9a57\u8b49\u96c6\n            ```py\n            # \u8a13\u7df4\u6a21\u578b\u4e26\u6aa2\u8996\u9a57\u8b49\u96c6\u7684\u7d50\u679c\n            model.fit(x_train, y_train, \n                    epochs=100, \n                    batch_size=256, \n                    validation_data=(x_test, y_test), \n                    shuffle=True)\n            # \u5c07\u8a13\u7df4\u96c6\u5207\u5206\u4e00\u90e8\u5206\u7576\u4f5c\u9a57\u8b49\u96c6\n            model.fit(x_train, y_train, \n                    epochs=100, \n                    batch_size=256, \n                    validation_split=0.2, \n                    shuffle=True)\n            ```\n            * \u6ce8\u610f\uff1a\u4f7f\u7528 validation_split \u8207 shuffle \u6642\uff0cKeras \u662f\u5148\u81ea x_train/y_train \u53d6\u6700\u5f8c (1-x)% \u505a\u70ba\u9a57\u8b49\u96c6\u4f7f\u7528\uff0c\u518d\ufa08 shuffle\u3002\n            * \u5728\u8a13\u7df4\u5b8c\u6210\u5f8c\uff0c\u5c07 training loss \u8207 validation loss \u53d6\u51fa\u4e26\u7e6a\u5716\n            ```py\n            # \u4ee5\u8996\u89ba\u756b\u65b9\u5f0f\u6aa2\u8996\u8a13\u7df4\u904e\u7a0b\n            import matplotlib.pyplot as plt\n            train_loss = model.history.history[\"loss\"]\n            valid_loss = model.history.history[\"val_loss\"]\n\n            train_acc = model.history.history[\"accuracy\"]\n            valid_acc = model.history.history[\"val_accuracy\"]\n\n            plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n            plt.plot(range(len(valid_loss)), valid_loss, label=\"valid loss\")\n            plt.legend()\n            plt.title(\"Loss\")\n            plt.show()\n\n            plt.plot(range(len(train_acc)), train_acc, label=\"train accuracy\")\n            plt.plot(range(len(valid_acc)), valid_acc, label=\"valid accuracy\")\n            plt.legend()\n            plt.title(\"Accuracy\")\n            plt.show()\n            ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [The Problem of Overfitting](https://medium.com/@ken90242/machine-learning%E5%AD%B8%E7%BF%92%E6%97%A5%E8%A8%98-coursera%E7%AF%87-week-3-4-the-c05b8ba3b36f)\n        * [Overfitting in Machine Learning](https://elitedatascience.com/overfitting-in-machine-learning)\n        * [Overfitting vs. Underfitting](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765)\n* **Day_78 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u524d\u7684\u6ce8\u610f\u4e8b\u9805**\n    * \u8a13\u7df4\u6a21\u578b\u524d\u7684\u6aa2\u67e5\n        * \u70ba\u4f55\u8981\u505a\u4e8b\u524d\u6aa2\u67e5\n            * \u8a13\u7df4\u6a21\u578b\u7684\u6642\u9593\u8ddf\u6210\u672c\u90fd\u5f88\u5927 (\u5982 GPU quota & \u4f60/\u59b3\u7684\u4eba\u751f)\n        * \u8981\u505a\u54ea\u4e9b\u6aa2\u67e5\uff1a\n            1. \u4f7f\u7528\u7684\u88dd\u7f6e\uff1a\u662f\u4f7f\u7528 CPU or GPU / \u60f3\u8981\u4f7f\u7528\u7684 GPU \u662f\u5426\u5df2\u7d93\u88ab\u5225\u4eba\u4f54\u7528?\n                * nvidia-smi \u53ef\u4ee5\u770b\u5230\u76ee\u524d\u53ef\u4ee5\u53d6\u5f97\u7684GPU \u88dd\u7f6e\u4f7f\u7528\uf9fa\u614b   \n            2. Input preprocessing\uff1a\u8cc7\u6599 (Xs) \u662f\u5426\u6709\u9032\ufa08\u904e\u9069\u7576\u7684\u6a19\u6e96\u5316?   \n            3. Output preprocessing\uff1a\u76ee\u6a19 (Ys) \u662f\u5426\u7d93\u904e\u9069\u7576\u7684\u8655\uf9e4? (\u5982 onehot-encoded)\n                * \u900f\u904e Function \u9032\ufa08\u8655\uf9e4\uff0c\u800c\u975e\u5728 Cell \u4e2d\u55ae\u7368\u9032\ufa08\u907f\u514d\u907a\u6f0f\u3001\u932f\u7f6e\n            4. Model Graph\uff1a\u6a21\u578b\u7684\u67b6\u69cb\u662f\u5426\u5982\u9810\u671f\u6240\u60f3?\n                * model.summary() \u53ef\u4ee5\u770b\u5230\u6a21\u578b\u5806\u758a\u7684\u67b6\u69cb\n            5. \u8d85\uf96b\u6578\u8a2d\u5b9a(Hyper-parameters)\uff1a\u8a13\u7df4\u6a21\u578b\u7684\u76f8\u95dc\uf96b\u6578\u662f\u5426\u8a2d\u5b9a\u5f97\u7576?\n                * \u5c07\u6a21\u578b/\u7a0b\u5f0f\u6240\u4f7f\u7528\u5230\u7684\u76f8\u95dc\uf96b\u6578\u96c6\u4e2d\u7ba1\uf9e4\uff0c\u907f\u514d\u6563\uf918\u5728\u5404\u8655\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Troubleshooting Deep Neural Networks](http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf)\n            * \u6aa2\u67e5\u7a0b\u5f0f\u78bc\n            * \u990a\u6210\u597d\u7684\u7a0b\u5f0f\u64b0\u5beb\u7fd2\u6163([PEP8](https://www.python.org/dev/peps/pep-0008/))\n            * \u78ba\u8a8d\uf96b\u6578\u8a2d\u5b9a\u6b32\u5be6\u4f5c\u7684\u6a21\u578b\u662f\u5426\u5408\u9069\u7576\u524d\u7684\u8cc7\u6599\n            * \u78ba\u8a8d\u8cc7\u6599\u7d50\u69cb\u8cc7\u6599\u662f\u5426\u8db3\u5920\n            * \u662f\u5426\u4e7e\u6de8\n            * \u662f\u5426\u6709\u9069\u7576\u7684\u524d\u8655\uf9e4\n            * \u4ee5\u7c21\u55ae\u7684\u2f45\u65b9\u5f0f\u5be6\u73fe\u60f3\u6cd5\u5efa\u7acb\u8a55\u4f30\u6a5f\u5236\u958b\u59cb\u5faa\u74b0\u6e2c\u8a66 (evaluate - tuning - debugging)\n* **Day_79 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - Learning Rate Effect**\n    * \u5982\u679c Learning rate (LR, alpha) \u592a\u5927\uff0c\u5c07\u6703\u5c0e\u81f4\u6bcf\u6b65\uf901\u65b0\u6642\uff0c\u7121\u6cd5\u5728\u9661\u5ced\u7684\u640d\u5931\u5c71\u8c37\u4e2d\uff0c\u9806\uf9dd\u7684\u5f80\u4e0b\u6ed1\u52d5\uff1b\u4f46\uf974\u592a\u5c0f\uff0c\u5247\u8981\u6ed1\u5230\u8c37\u5e95\u7684\u6642\u9593\u904e\u65bc\u5197\u9577\uff0c\u4e14\uf974\u9047\u5230\u5e73\u539f\u5340\u5247\u7121\u6cd5\u627e\u5230\u6b63\u78ba\u7684\u65b9\u5411\u3002\n    * Options in SGD optimizer\n        * Momentum\uff1a\u52d5\uf97e \u2013 \u5728\uf901\u65b0\u65b9\u5411\u4ee5\u5916\uff0c\u52a0\u4e0a\u4e00\u500b\u56fa\u5b9a\u5411\uf97e\uff0c\u4f7f\u5f97\u771f\u5be6\u79fb\u52d5\u65b9\u5411\u6703\u4ecb\u65bc\u7b97\u51fa\uf92d\u7684 gradient step \u8207 momentum \u9593\u3002\n            * Actual step = momentum step + gradient step\n        * Nesterov Momentum\uff1a\u62d4\u8349\u6e2c\u98a8\u5411\n            * \u5c07 momentum \u7d0d\u5165 gradient \u7684\u8a08\u7b97\n            * Gradient step computation is based on x + momentum\n    * \u91cd\u8981\u77e5\u8b58\u9ede\u8907\u7fd2\n        * \u5b78\u7fd2\u7387\u5c0d\u8a13\u7df4\u9020\u6210\u7684\u5f71\u97ff\n            * \u5b78\u7fd2\u7387\u904e\u5927\uff1a\u6bcf\u6b21\u6a21\u578b\uf96b\u6578\u6539\u8b8a\u904e\u5927\uff0c\u7121\u6cd5\u6709\u6548\u6536\u6582\u5230\uf901\u4f4e\u7684\u640d\u5931\u5e73\u9762\n            * \u5b78\u7fd2\u7387\u904e\u5c0f\uff1a\u6bcf\u6b21\uf96b\u6578\u7684\u6539\u8b8a\uf97e\u5c0f\uff0c\u5c0e\u81f4\n                * \u640d\u5931\u6539\u8b8a\u7684\u5e45\u5ea6\u5c0f\n                * \u5e73\u539f\u5340\u57df\u7121\u6cd5\u627e\u5230\u6b63\u78ba\u7684\u65b9\u5411\u5728 \n        * SGD \u4e2d\u7684\u52d5\uf97e\u65b9\u6cd5\u2022\n            * \u5728\u640d\u5931\u65b9\u5411\u4e0a\uff0c\u52a0\u4e0a\u4e00\u5b9a\u6bd4\u7387\u7684\u52d5\uf97e\u5354\u52a9\u64fa\u812b\u5e73\u539f\u6216\u662f\u5c0f\u5c71\u8c37\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Estimating an Optimal Learning Rate For a Deep Neural Network](https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)\n        * [cs231n : learning and evaluation](https://cs231n.github.io/neural-networks-3/)\n        * [\u6df1\u5ea6\u5b66\u4e60\u8d85\u53c2\u6570\u7b80\u5355\u7406\u89e3](https://zhuanlan.zhihu.com/p/23906526)\n* **Day_80 : \u512a\u5316\u5668\u8207\u5b78\u7fd2\u7387\u7684\u7d44\u5408\u8207\u6bd4\u8f03** \n    * </>\n        ```py\n        import os\n        import keras\n        import itertools\n\n        # \u5f9e Keras \u7684\u5167\u5efa\u529f\u80fd\u4e2d\uff0c\u53d6\u5f97 train \u8207 test \u8cc7\u6599\u96c6\n        train, test = keras.datasets.cifar10.load_data()\n\n        ## \u8cc7\u6599\u524d\u8655\u7406\n        def preproc_x(x, flatten=True):\n            x = x / 255.\n            if flatten:\n                x = x.reshape((len(x), -1))\n            return x\n\n        def preproc_y(y, num_classes=10):\n            if y.shape[-1] == 1:\n                y = keras.utils.to_categorical(y, num_classes)\n            return y \n        \n        x_train, y_train = train\n        x_test, y_test = test\n\n        # \u8cc7\u6599\u524d\u8655\u7406 - X \u6a19\u6e96\u5316\n        x_train = preproc_x(x_train)\n        x_test = preproc_x(x_test)\n\n        # \u8cc7\u6599\u524d\u8655\u7406 -Y \u8f49\u6210 onehot\n        y_train = preproc_y(y_train)\n        y_test = preproc_y(y_test)\n\n        \"\"\"\n        \u5efa\u7acb\u795e\u7d93\u7db2\u8def\n        \"\"\"\n        def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128]):\n            input_layer = keras.layers.Input(input_shape)\n            \n            for i, n_units in enumerate(num_neurons):\n                if i == 0:\n                    x = keras.layers.Dense(units=n_units, activation=\"relu\", name=\"hidden_layer\"+str(i+1))(input_layer)\n                else:\n                    x = keras.layers.Dense(units=n_units, activation=\"relu\", name=\"hidden_layer\"+str(i+1))(x)\n            \n            out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n            \n            model = keras.models.Model(inputs=[input_layer], outputs=[out])\n            return model\n        \n        ## \u8d85\u53c3\u6578\u8a2d\u5b9a\n        LEARNING_RATE = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n        EPOCHS = 50\n        BATCH_SIZE = 256\n        OPTIMIZER = [keras.optimizers.SGD, keras.optimizers.RMSprop, keras.optimizers.Adagrad, keras.optimizers.Adam]\n\n        results = {}\n        for lr, opti in itertools.product(LEARNING_RATE, OPTIMIZER):\n            keras.backend.clear_session() # \u628a\u820a\u7684 Graph \u6e05\u6389\n            print(\"Experiment with LR = %.6f, Optimizer = %s\" % (lr, str(opti)))\n            model = build_mlp(input_shape=x_train.shape[1:])\n            model.summary()\n            \n            optimizer = opti(lr=lr)\n            model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n\n            model.fit(x_train, y_train, \n                    epochs=EPOCHS, \n                    batch_size=BATCH_SIZE, \n                    validation_data=(x_test, y_test), \n                    shuffle=True)\n            \n            # Collect results\n            train_loss = model.history.history[\"loss\"]\n            valid_loss = model.history.history[\"val_loss\"]\n            train_acc = model.history.history[\"accuracy\"]\n            valid_acc = model.history.history[\"val_accuracy\"]\n            \n            exp_name_tag = \"exp-lr-%s-optimizer-%s\" % (str(lr), str(opti))\n            results[exp_name_tag] = {'train-loss': train_loss,\n                                    'valid-loss': valid_loss,\n                                    'train-acc': train_acc,\n                                    'valid-acc': valid_acc}\n        \"\"\"\n        Plot results\n        \"\"\"\n        import matplotlib.pyplot as plt\n        %matplotlib inline\n\n        NUM_COLORS = len(results.keys())\n        cm = plt.get_cmap('gist_rainbow')\n        color_bar = [cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)]\n\n        plt.figure(figsize=(8,6))\n        for i, cond in enumerate(results.keys()):\n            plt.plot(range(len(results[cond]['train-loss'])),results[cond]['train-loss'], '-', label=cond, color=color_bar[i])\n            plt.plot(range(len(results[cond]['valid-loss'])),results[cond]['valid-loss'], '--', label=cond, color=color_bar[i])\n        plt.title(\"Loss\")\n        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n        plt.show()\n\n        plt.figure(figsize=(8,6))\n        for i, cond in enumerate(results.keys()):\n            plt.plot(range(len(results[cond]['train-acc'])),results[cond]['train-acc'], '-', label=cond, color=color_bar[i])\n            plt.plot(range(len(results[cond]['valid-acc'])),results[cond]['valid-acc'], '--', label=cond, color=color_bar[i])\n        plt.title(\"Accuracy\")\n        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n        plt.show()\n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u4f18\u5316\u65b9\u6cd5\u603b\u7ed3](https://blog.csdn.net/u010089444/article/details/76725843)\n            * SGD (mini-batch)\n                * \u5728\u55ae\u6b65\uf901\u65b0\u8207\u5168\u5c40\uf901\u65b0\u7684\u6298\u8877\u8fa6\u6cd5\uff0c\u901a\u5e38\u642d\u914d momentum \u7a69\u5b9a\u6536\u6582\u65b9\u5411\u8207\u7d50\u679c\u3002\n                * \u6536\u6582\u901f\u5ea6\u8f03\u6162\u3002\n            * RMSprop\n                * \u5b78\u7fd2\u7387\u7684\u8abf\u6574\u662f\u6839\u64da\u904e\u53bb\u68af\u5ea6\u7684\uf9fa\u6cc1\u8abf\u6574\uff0c\u6536\u6582\u901f\u5ea6\u5feb\u53c8\u4e0d\uf9e0\u6703\u51fa\u73fe learning rate \u5feb\u901f\u4e0b\u964d\u7684\uf9fa\u6cc1\u3002\n            * Adam\n                * \u540c\u6a23\u662f\u53ef\u4ee5\u6839\u64da\u904e\u53bb\u7684\u68af\u5ea6\u81ea\ufa08\u8abf\u6574 learning rate\uff0c\u4f46\u6821\u6b63\u65b9\u5f0f\u8003\uf97e\u4e00\u3001\u4e8c\u968e\u77e9\u9663\uff0c\u4f7f\u5176\uf901\u52a0\u5e73\u7a69\u3002\n            * \u5728\u5be6\u4f5c\u904e\u7a0b\u4e2d\uff0c\u5efa\u8b70\u5148\u4f7f\u7528 Adam \u9a57\u8b49\uff0c\uf974\u8981\u505a\u6700\u7d42\u7684\u512a\u5316\uff0c\u5247\u518d\u6539\u7528 SGD \u627e\u5230\u6700\u4f73\uf96b\u6578\u3002\n        * [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)\n* **Day_81 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - Regularization**\n    * **\u6b63\u898f\u5316 (Regularization)**    \n        * Cost function = Loss + Regularization\n        * \u900f\u904e regularization\uff0c\u53ef\u4ee5\u4f7f\u7684\u6a21\u578b\u7684 weights \u8b8a\u5f97\u6bd4\u8f03\u5c0f\n        * wi \u8f03\u5c0f \n        $\\to$ \u0394xi \u5c0d $\\hat{y}$ \u9020\u6210\u7684\u5f71\u97ff(\u0394$\\hat{y}$)\u8f03\u5c0f\n        $\\to$ \u5c0d input \u8b8a\u5316\u6bd4\u8f03\u4e0d\u654f\u611f \u2794 better generalization\n    * Regularizer \u7684\u6548\u679c\uff1a\u8b93\u6a21\u578b\uf96b\u6578\u7684\u6578\u503c\u8f03\u5c0f\n        * \u4f7f\u5f97 Inputs \u7684\u6539\u8b8a\u4e0d\u6703\u8b93 Outputs \u6709\u5927\u5e45\u7684\u6539\u8b8a\u3002\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Regularization in Machine Learning](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)\n        ```py\n        from keras.regularizers import l1_l2\n\n        \"\"\"\n        \u5efa\u7acb\u795e\u7d93\u7db2\u8def\n        \"\"\"\n        def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128], l1_ratio=0.0, l2_ratio=0.0):\n            input_layer = keras.layers.Input(input_shape)\n            \n            for i, n_units in enumerate(num_neurons):\n                if i == 0:\n                    x = keras.layers.Dense(units=n_units, \n                                        activation=\"relu\", \n                                        name=\"hidden_layer\"+str(i+1), \n                                        kernel_regularizer=l1_l2(l1=l1_ratio, l2=l2_ratio))(input_layer)\n                else:\n                    x = keras.layers.Dense(units=n_units, \n                                        activation=\"relu\", \n                                        name=\"hidden_layer\"+str(i+1),\n                                        kernel_regularizer=l1_l2(l1=l1_ratio, l2=l2_ratio))(x)\n            \n            out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n            \n            model = keras.models.Model(inputs=[input_layer], outputs=[out])\n            return model\n\n        ## \u8d85\u53c3\u6578\u8a2d\u5b9a\n        LEARNING_RATE = 1e-3\n        EPOCHS = 10\n        BATCH_SIZE = 256\n        MOMENTUM = 0.95\n        L1_EXP = [1e-2, 1e-4, 1e-8, 1e-12, 0.0]\n        L2_EXP = [1e-2, 1e-4, 1e-8, 1e-12, 0.0]\n\n        results = {}\n        for l1r, l2r in itertools.product(L1_EXP, L2_EXP):\n            keras.backend.clear_session() # \u628a\u820a\u7684 Graph \u6e05\u6389\n            print(\"Experiment with L1 = %.6f, L2 = %.6f\" % (l1r, l2r))\n            model = build_mlp(input_shape=x_train.shape[1:], l1_ratio=l1r, l2_ratio=l2r)\n            model.summary()\n            optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n            model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n\n            model.fit(x_train, y_train, \n                    epochs=EPOCHS, \n                    batch_size=BATCH_SIZE, \n                    validation_data=(x_test, y_test), \n                    shuffle=True)\n            \n            # Collect results\n            train_loss = model.history.history[\"loss\"]\n            valid_loss = model.history.history[\"val_loss\"]\n            train_acc = model.history.history[\"accuracy\"]\n            valid_acc = model.history.history[\"val_accuracy\"]\n            \n            exp_name_tag = \"exp-l1-%s-l2-%s\" % (str(l1r), str(l2r))\n            results[exp_name_tag] = {'train-loss': train_loss,\n                                    'valid-loss': valid_loss,\n                                    'train-acc': train_acc,\n                                    'valid-acc': valid_acc}\n        ```\n* **Day_82 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - Dropout**\n    * **\u96a8\u6a5f\u79fb\u9664 (Dropout)**\n        * \u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\uff0c\u5728\u539f\u672c\u5168\u9023\u7d50\u7684\u524d\u5f8c\uf978\u5c64 layers \uff0c\u96a8\u6a5f\u62ff\u6389\u4e00\u4e9b\u9023\u7d50 (weights \u8a2d\u70ba 0)\n        * \u89e3\u91cb1\uff1a\u589e\u52a0\u8a13\u7df4\u7684\u96e3\u5ea6 \u2013 \u7576\u4f60\u77e5\u9053\u4f60\u7684\u540c\u4f34\u4e2d\u6709\u8c6c\u968a\u53cb\u6642\uff0c\u4f60\u6703\u8b8a\u5f97\u8981\uf901\u52aa\uf98a\u5b78\u7fd2\n        * \u89e3\u91cb2\uff1a\u88ab\u8996\u70ba\u4e00\u7a2e model \u81ea\u8eab\u7684 ensemble \u65b9\u6cd5\uff0c\u56e0\u70ba model \u53ef\u4ee5\u6709 2^n \u7a2e weights combination\n        * \u512a\u9ede : \u5f37\u8feb\u6a21\u578b\u7684\u6bcf\u500b\uf96b\u6578\u6709\uf901\u5f37\u7684\u6cdb\u5316\u80fd\uf98a\uff0c\u4e5f\u8b93\u7db2\uf937\u80fd\u5728\uf901\u591a\uf96b\u6578\u7d44\u5408\u7684\uf9fa\u614b\u4e0b\u7fd2\u5f97\u8868\u5fb5\u3002\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u7406\u89e3dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443)\n        * [Dropout in (Deep) Machine learning](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)\n        ```py\n        from keras.layers import Dropout\n\n        def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128], drp_ratio=0.2):\n            input_layer = keras.layers.Input(input_shape)\n            \n            for i, n_units in enumerate(num_neurons):\n                if i == 0:\n                    x = keras.layers.Dense(units=n_units, \n                                        activation=\"relu\", \n                                        name=\"hidden_layer\"+str(i+1))(input_layer)\n                    x = Dropout(drp_ratio)(x)\n                else:\n                    x = keras.layers.Dense(units=n_units, \n                                        activation=\"relu\", \n                                        name=\"hidden_layer\"+str(i+1))(x)\n                    x = Dropout(drp_ratio)(x)\n            \n            out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n            \n            model = keras.models.Model(inputs=[input_layer], outputs=[out])\n            return model\n        ```\n* **Day_83 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - Batch normalization**\n    * **\u6279\u6b21\u6a19\u6e96\u5316 (Batch normalization)**\n        * \u5c0d\u65bc Input \u7684\u6578\u503c\uff0c\u524d\u9762\u63d0\u5230\u5efa\u8b70\u8981 re-scale\n            * Weights \u4fee\u6b63\u7684\uf937\u5f91\u6bd4\u8f03\u6703\u5728\u540c\u5fc3\u5713\u5c71\u8c37\u4e2d\u5f80\u4e0b\u6ed1\n            * \u53ea\u52a0\u5728\u8f38\u5165\u5c64 re-scale \u4e0d\u5920\uff0c\u4f60\u53ef\u4ee5\u6bcf\u4e00\u5c64\u90fd re-scale !!\n        * \u6bcf\u500b input feature \u7368\u7acb\u505a normalization\n        * \uf9dd\u7528 batch statistics \u505a normalization \u800c\u975e\u6574\u4efd\u8cc7\u6599\n        * \u540c\u4e00\u7b46\u8cc7\u6599\u5728\u4e0d\u540c\u7684 batch \u4e2d\u6703\u6709\u4e9b\u5fae\u4e0d\u540c\n        * BN\uff1a\u5c07\u8f38\u5165\u7d93\u904e t \u8f49\u63db\u5f8c\u8f38\u51fa\n            * \u8a13\u7df4\u6642\uff1a\u4f7f\u7528 Batch \u7684\u5e73\u5747\u503c\n            * \u63a8\u8ad6\u6642\uff1a\u4f7f\u7528 Moving Average\n        * \u53ef\u4ee5\u89e3\u6c7a Gradient vanishing \u7684\u554f\u984c\n        * \u53ef\u4ee5\u7528\u6bd4\u8f03\u5927\u7684 learning rate \u52a0\u901f\u8a13\u7df4\n        * \u53d6\u4ee3 dropout & regularizes\n        * \u76ee\u524d\u5927\u591a\u6578\u7684 Deep neural network \u90fd\u6703\u52a0\n\n        **Input** : Values of x over a mini-batch : $B = \\{x_{1...m}\\}$;\n                    Parameters to be learn : $\\gamma, \\beta$\n        **Output** : $\\{y_i = BN_{\\gamma,\\beta}(x_i) \\}$\n        $$ \\mu_B \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}x_i$$ // mini-batch mean\n        $$ \\sigma^2_B \\leftarrow \\frac{1}{m}\\sum_{i=1}^{m}(x_i - \\mu_B)^2$$   // mini-batch variance\n        $$\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}} $$    // normalize\n        $$y_i \\leftarrow \\gamma\\hat{x_i} + \\beta \\equiv BN_{\\gamma, \\beta}(x_i)$$ // scale and shift\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u70ba\u4f55\u8981\u6279\u6b21\u6a19\u6e96\u5316](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-08-batch-normalization/)\n        * [Batch Normalization \u539f\u7406\u4e0e\u5b9e\u6218](https://zhuanlan.zhihu.com/p/34879333)\n        ```py\n        from keras.layers import BatchNormalization, Activation\n        \n        def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128], pre_activate=False):\n            input_layer = keras.layers.Input(input_shape)\n            \n            for i, n_units in enumerate(num_neurons):\n                if i == 0:\n                    x = keras.layers.Dense(units=n_units, \n                                        name=\"hidden_layer\"+str(i+1))(input_layer)\n                    if pre_activate:\n                        x = BatchNormalization()(x)\n                        x = Activation(\"relu\")(x)\n                    else:\n                        x = Activation(\"relu\")(x)\n                        x = BatchNormalization()(x)\n                else:\n                    x = keras.layers.Dense(units=n_units, \n                                        name=\"hidden_layer\"+str(i+1))(x)\n                    if pre_activate:\n                        x = BatchNormalization()(x)\n                        x = Activation(\"relu\")(x)\n                    else:\n                        x = Activation(\"relu\")(x)\n                        x = BatchNormalization()(x)\n                        \n            \n            out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n            \n            model = keras.models.Model(inputs=[input_layer], outputs=[out])\n            return model\n        ```\n* **Day_84 : \u6b63\u898f\u5316/\u6a5f\u79fb\u9664/\u6279\u6b21\u6a19\u6e96\u5316\u7684 \u7d44\u5408\u8207\u6bd4\u8f03**\n    * </>\n        ```py\n        import os\n        import keras\n        import itertools\n\n        train, test = keras.datasets.cifar10.load_data()\n\n        ## \u8cc7\u6599\u524d\u8655\u7406\n        def preproc_x(x, flatten=True):\n            x = x / 255.\n            if flatten:\n                x = x.reshape((len(x), -1))\n            return x\n\n        def preproc_y(y, num_classes=10):\n            if y.shape[-1] == 1:\n                y = keras.utils.to_categorical(y, num_classes)\n            return y \n\n        x_train, y_train = train\n        x_test, y_test = test\n\n        # Preproc the inputs\n        x_train = preproc_x(x_train)\n        x_test = preproc_x(x_test)\n\n        # Preprc the outputs\n        y_train = preproc_y(y_train)\n        y_test = preproc_y(y_test)\n\n        from keras.layers import BatchNormalization, Activation, Dropout, regularizers\n\n        def build_mlp(input_shape, \n                    output_units=10, \n                    num_neurons=[512, 256, 128],\n                    use_bn=True,\n                    drp_ratio=0.,\n                    l2_ratio=0.):\n            input_layer = keras.layers.Input(input_shape)\n            \n            for i, n_units in enumerate(num_neurons):\n                if i == 0:\n                    x = keras.layers.Dense(units=n_units, \n                                        kernel_regularizer=regularizers.l2(l2_ratio),\n                                        name=\"hidden_layer\"+str(i+1))(input_layer)\n\n                    if use_bn:\n                        x = BatchNormalization()(x)\n                    x = Activation(\"relu\")(x)\n                    x = Dropout(drp_ratio)(x)\n\n                else:\n                    x = keras.layers.Dense(units=n_units, \n                                        kernel_regularizer=regularizers.l2(l2_ratio),\n                                        name=\"hidden_layer\"+str(i+1))(x)\n                    if use_bn:\n                        x = BatchNormalization()(x)\n                    x = Activation(\"relu\")(x)\n                    x = Dropout(drp_ratio)(x)\n                    \n            out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n            model = keras.models.Model(inputs=[input_layer], outputs=[out])\n            return model\n        \n        ## \u8d85\u53c3\u6578\u8a2d\u5b9a\n        \"\"\"\n        Set your hyper-parameters\n        \"\"\"\n        LEARNING_RATE = 1e-3\n        EPOCHS = 3\n        BATCH_SIZE = 128\n\n        \"\"\"\n        \u5efa\u7acb\u5be6\u9a57\u7d44\u5408\n        \"\"\"\n        USE_BN = [True, False]\n        DRP_RATIO = [0., 0.4, 0.8]\n        L2_RATIO = [0., 1e-6, 1e-8]\n\n        import keras.backend as K\n\n        \"\"\"\n        \u4ee5\u8ff4\u5708\u65b9\u5f0f\u904d\u6b77\u7d44\u5408\u4f86\u8a13\u7df4\u6a21\u578b\n        \"\"\"\n        results = {}\n        for i, (use_bn, drp_ratio, l2_ratio) in enumerate(itertools.product(USE_BN, DRP_RATIO, L2_RATIO)):\n            K.clear_session()\n            print(\"Numbers of exp: %i, with bn: %s, drp_ratio: %.2f, l2_ratio: %.2f\" % (i, use_bn, drp_ratio, l2_ratio))\n            model = build_mlp(input_shape=x_train.shape[1:], use_bn=use_bn, drp_ratio=drp_ratio, l2_ratio=l2_ratio)\n            model.summary()\n            optimizer = keras.optimizers.Adam(lr=LEARNING_RATE)\n            model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n\n            model.fit(x_train, y_train, \n                    epochs=EPOCHS, \n                    batch_size=BATCH_SIZE, \n                    validation_data=(x_test, y_test), \n                    verbose=1,\n                    shuffle=True)\n            \n            # Collect results\n            exp_name_tag = (\"exp-%s\" % (i))\n            results[exp_name_tag] = {'train-loss': model.history.history[\"loss\"],\n                                    'valid-loss': model.history.history[\"val_loss\"],\n                                    'train-acc': model.history.history[\"accuracy\"],\n                                    'valid-acc': model.history.history[\"val_accuracy\"]}\n        ```\n* **Day_85 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - \u4f7f\u7528 callbacks \u51fd\u6578\u505a earlystop**\n    * **\u63d0\u524d\u7d42\u6b62 (EarlyStopping)**\n        * \u5047\u5982\u80fd\u5920\u65e9\u9ede\u505c\u4e0b\uf92d\u5c31\u597d\n        * \u5728 Overfitting \u524d\u505c\u4e0b\uff0c\u907f\u514d model weights \u88ab\u641e\u721b\n        * \u6ce8\u610f\uff1aEarlystop \u4e0d\u6703\u4f7f\u6a21\u578b\u5f97\u5230\uf901\u597d\u7684\u7d50\u679c\uff0c\u50c5\u662f\u907f\u514d\uf901\u7cdf\n    * Callbacks function\uff1a\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\uff0c\u6211\u5011\u53ef\u4ee5\u900f\u904e\u4e00\u4e9b\u51fd\u5f0f\uf92d\u76e3\u63a7/\u4ecb\u5165\u8a13\u7df4\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [keras\u7684EarlyStopping callbacks\u7684\u4f7f\u7528\u4e0e\u6280\u5de7](https://blog.csdn.net/silent56_th/article/details/72845912)\n        ```py\n        \"\"\"\n        # \u8f09\u5165 Callbacks, \u4e26\u5c07 monitor \u8a2d\u5b9a\u70ba\u76e3\u63a7 validation loss\n        \"\"\"\n        from keras.callbacks import EarlyStopping\n\n        earlystop = EarlyStopping(monitor=\"val_loss\", \n                                patience=5, \n                                verbose=1\n                                )\n\n        model.fit(x_train, y_train, \n                epochs=EPOCHS, \n                batch_size=BATCH_SIZE, \n                validation_data=(x_test, y_test), \n                shuffle=True,\n                callbacks=[earlystop]\n                )\n        ```\n* **Day_86 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - \u4f7f\u7528 callbacks \u51fd\u6578\u5132\u5b58 model**\n    * **Model CheckPoint**\n        * \u70ba\u4f55\u8981\u4f7f\u7528 Model Check Point?\n            * ModelCheckPoint\uff1a\u81ea\u52d5\u5c07\u76ee\u524d\u6700\u4f73\u7684\u6a21\u578b\u6b0a\u91cd\u5b58\u4e0b\n        * \u5047\u5982\u96fb\u8166\u7a81\u7136\u65b7\u7dda\u3001\u7576\u6a5f\u8a72\u600e\u9ebc\u8fa6? \u96e3\u9053\u6211\u53ea\u80fd\u91cd\u65b0\u958b\u59cb?\n            * \u5047\u5982\u4e0d\u5e78\u65b7\u7dda : \u53ef\u4ee5\u91cd\u65b0\u81ea\u6700\u4f73\u7684\u6b0a\u91cd\u958b\u59cb\n            * \u5047\u5982\u8981\u505a Inference : \u53ef\u4ee5\u4fdd\u8b49\u4f7f\u7528\u7684\u662f\u5c0d monitor metric \u6700\u4f73\u7684\u6b0a\u91cd\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u83ab\u7169 Python - \u5132\u5b58\u8207\u8f09\u56de\u6a21\u578b](https://morvanzhou.github.io/tutorials/machine-learning/keras/3-1-save/)\n        ```py\n        \"\"\"\n        # \u8f09\u5165 Callbacks, \u4e26\u5c07\u76e3\u63a7\u76ee\u6a19\u8a2d\u70ba validation loss, \u4e14\u53ea\u5b58\u6700\u4f73\u53c3\u6578\u6642\u7684\u6a21\u578b\n        \"\"\"\n        from keras.callbacks import ModelCheckpoint\n\n        model_ckpt = ModelCheckpoint(filepath=\"./tmp.h5\", \n                                    monitor=\"val_loss\", \n                                    save_best_only=True)\n\n        model.fit(x_train, y_train, \n                epochs=EPOCHS, \n                batch_size=BATCH_SIZE, \n                validation_data=(x_test, y_test), \n                shuffle=True,\n                callbacks=[model_ckpt]\n                )\n        \n        model.save(\"final_model.h5\")\n        model.save_weights(\"model_weights.h5\")\n\n        pred_final = model.predict(x_test)\n        # Load back\n        model = keras.models.load_model(\"./tmp.h5\")\n        pred_loadback = model.predict(x_test)\n\n        from sklearn.metrics import accuracy_score\n\n        new_model = build_mlp(input_shape=x_train.shape[1:])\n        new_model_pred = new_model.predict(x_test)\n        new_model_acc = accuracy_score(y_true=y_test.argmax(axis=-1), y_pred=new_model_pred.argmax(axis=-1))\n        print(\"Accuracy of best weights: %.3f\" % new_model_acc)\n\n        new_model.load_weights(\"./model_weights.h5\")\n        new_model_pred = new_model.predict(x_test)\n        new_model_loadback_acc = accuracy_score(y_true=y_test.argmax(axis=-1), y_pred=new_model_pred.argmax(axis=-1))\n        print(\"Accuracy of best weights: %.3f\" % new_model_loadback_acc)\n        ```\n* **Day_87 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - \u4f7f\u7528 callbacks \u51fd\u6578\u505a reduce learning rate**\n    * Reduce Learning Rate: \u96a8\u8a13\u7df4\uf901\u65b0\u6b21\u6578\uff0c\u5c07 Learning rate \u9010\u6b65\u6e1b\u5c0f\n        * \u56e0\u70ba\u901a\u5e38\u640d\u5931\u51fd\u6578\u8d8a\u63a5\u8fd1\u8c37\u5e95\u7684\u4f4d\u7f6e\uff0c\u958b\u53e3\u8d8a\u5c0f \u2013 \u9700\u8981\u8f03\u2f29\u7684 Learning rate \u624d\u53ef\u4ee5\u518d\u6b21\u4e0b\u964d\n    * \u53ef\ufa08\u7684\u8abf\u964d\u65b9\u5f0f\n        * \u6bcf\uf901\u65b0 n \u6b21\u5f8c\uff0c\u5c07 Learning rate \u505a\u4e00\u6b21\u8abf\u964d \u2013 schedule decay\n        * \u7576\u7d93\u904e\u5e7e\u500b epoch \u5f8c\uff0c\u767c\u73fe performance \u6c92\u6709\u9032\u6b65 \u2013 Reduce on plateau\n    * Reduce learning rate on plateau\uff1a\u6a21\u578b\u6c92\u8fa6\u6cd5\u9032\u6b65\u7684\u53ef\u80fd\u662f\u56e0\u70ba\u5b78\u7fd2\u7387\u592a\u5927\u5c0e\u81f4\u6bcf\u6b21\u6539\u8b8a\uf97e\u592a\u5927\u800c\u7121\u6cd5\uf918\u5165\u8f03\u4f4e\u7684\u640d\u5931\u5e73\u9762\uff0c\u900f\u904e\u9069\u5ea6\u7684\u964d\u4f4e\uff0c\u5c31\u6709\u6a5f\u6703\u5f97\u5230\uf901\u597d\u7684\u7d50\u679c\n    * \u56e0\u70ba\u6211\u5011\u53ef\u4ee5\u900f\u904e\u9019\u6a23\u7684\u76e3\u63a7\u6a5f\u5236\uff0c\u521d\u59cb\u7684 Learning rate \u53ef\u4ee5\u8abf\u5f97\u6bd4\u8f03\u9ad8\uff0c\u8b93\u8a13\u7df4\u904e\u7a0b\u8207 callback \uf92d\u505a\u9069\u7576\u7684 learning rate \u8abf\u964d\u3002\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Callbacks API](https://keras.io/api/callbacks/)\n        * A. LearningRateScheduler\n            1. \u5728\u6bcf\u500b epoch \u958b\u59cb\u524d\uff0c\u5f97\u5230\u76ee\u524d lr\n            2. \u6839\u64da schedule function \u91cd\u65b0\u8a08\u7b97 lr\uff0c\u6bd4\u5982 epoch = n \u6642\uff0c new_lr = lr * 0.1\n            3. \u5c07 optimizer \u7684 lr \u8a2d\u5b9a\u70ba new_lr\n            4. \u6839\u64da shhedule \u51fd\u5f0f\uff0c\u5047\u8a2d\u8981\u81ea\u8a02\u7684\u8a71\uff0c\u5b83\u61c9\u8a72\u5403\uf978\u500b\uf96b\u6578\uff1aepoch & lr\n        * B. ReduceLR\n            1. \u5728\u6bcf\u500b epoch \u7d50\u675f\u6642\uff0c\u5f97\u5230\u76ee\u524d\u76e3\u63a7\u76ee\u6a19\u7684\u6578\u503c\n            2. \u5982\u679c\u76ee\u6a19\u6bd4\u76ee\u524d\u5132\u5b58\u7684\u9084\u8981\u5dee\u7684\u8a71\uff0cwait+1\uff1b\uf974\u5426\u5247 wait \u8a2d\u70ba 0\uff0c\u76ee\u524d\u76e3\u63a7\u6578\u503c\uf901\u65b0\u7684\u6578\u503c\n            3. \u5982\u679c wait >= patient\uff0cnew_lr = lr * factor\uff0c\u5c07 optimizer \u7684 lr \u8a2d\u5b9a\u70ba new_lr\uff0c\u4e26\u4e14 wait \u8a2d\u56de 0\n        ```py\n        \"\"\"\n        # \u8f09\u5165 Callbacks, \u4e26\u8a2d\u5b9a\u76e3\u63a7\u76ee\u6a19\u70ba validation loss\n        \"\"\"\n        from keras.callbacks import ReduceLROnPlateau\n\n        reduce_lr = ReduceLROnPlateau(factor=0.5, \n                                    min_lr=1e-12, \n                                    monitor='val_loss', \n                                    patience=5, \n                                    verbose=1)\n\n        model.fit(x_train, y_train, \n                epochs=EPOCHS, \n                batch_size=BATCH_SIZE, \n                validation_data=(x_test, y_test), \n                shuffle=True,\n                callbacks=[reduce_lr]\n                )\n        ```\n* **Day_88 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - \u64b0\u5beb\u81ea\u5df1\u7684 callbacks \u51fd\u6578**\n    * Callbacks\n        * Callback \u5728\u8a13\u7df4\u6642\u7684\u547c\u53eb\u6642\u6a5f\n            * on_train_begin\uff1a\u5728\u8a13\u7df4\u6700\u958b\u59cb\u6642\n            * on_train_end\uff1a\u5728\u8a13\u7df4\u7d50\u675f\u6642\n            * on_batch_begin\uff1a\u5728\u6bcf\u500b batch \u958b\u59cb\u6642\n            * on_batch_end\uff1a\u5728\u6bcf\u500b batch \u7d50\u675f\u6642\n            * on_epoch_begin\uff1a\u5728\u6bcf\u500b epoch \u958b\u59cb\u6642\n            * on_epoch_end\uff1a\u5728\u6bcf\u500b epoch \u7d50\u675f\u6642\n        * \u5728 Keras \u4e2d\uff0c\u50c5\u9700\u8981\u5be6\u4f5c\u4f60\u60f3\u8981\u555f\u52d5\u7684\u90e8\u5206\u5373\u53ef\n        * \u8209\uf9b5\uf92d\u8aaa\uff0c\u5047\u5982\u4f60\u60f3\u8981\u6bcf\u500b batch \u90fd\u8a18\u9304 loss \u7684\u8a71\n        ```py\n        from keras.callbacks import Callback\n\n        class My_callback(Callback):\n            def on_train_begin(self, logs={}):\n                return\n            def on_train_end(self, logs={}):\n                return\n            def on_epoch_begin(self, logs={}):\n                return\n            def on_epoch_end(self, logs={}):\n                return\n            def on_batch_begin(self, batch, logs={}):\n                return\n            def on_batch_end(self, batch, logs={}):\n                self.losses.append(logs.get(\"loss\"))\n                return  \n        ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Keras \u4e2d\u4fdd\u7559 f1-score \u6700\u9ad8\u7684\u6a21\u578b (per epoch)](https://zhuanlan.zhihu.com/p/51356820)\n        ```py\n        \"\"\"\n        # \u8f09\u5165 Callbacks\uff0c\u64b0\u5beb\u4e00\u500b f1 score \u7684 callback function\n        \"\"\"\n\n        from keras.callbacks import Callback\n        from sklearn.metrics import f1_score\n\n        class f1sc(Callback):\n            def on_train_begin(self, epoch, logs = {}):\n                logs = logs or {}\n                record_items = [\"val_f1sc\"]\n                for i in record_items:\n                    if i not in self.params['metrics']:\n                        self.params['metrics'].append(i)\n            \n            def on_epoch_end(self, epoch, logs = {}, thres=0.5):\n                logs = logs or {}\n                y_true = self.validation_data[1].argmax(axis = 1)\n                y_pred = self.model.predict(self.validation_data[0])\n                y_pred = (y_pred[:, 1] >= thres) * 1\n                \n                logs[\"val_f1sc\"] = f1_score(y_true = y_true, y_pred = y_pred, average=\"weighted\")\n                \n        log_f1sc = f1sc()\n\n        model.fit(x_train, y_train, \n                epochs=EPOCHS, \n                batch_size=BATCH_SIZE, \n                validation_data=(x_test, y_test), \n                shuffle=True,\n                callbacks=[log_f1sc]\n                )\n\n        # \u5728\u8a13\u7df4\u5f8c\uff0c\u5c07 f1sc \u7d00\u9304\u8abf\u51fa\n        valid_f1sc = model.history.history['val_f1sc']\n\n        import matplotlib.pyplot as plt\n        %matplotlib inline\n\n        plt.plot(range(len(valid_f1sc)), valid_f1sc, label=\"valid f1-score\")\n        plt.legend()\n        plt.title(\"F1-score\")\n        plt.show()\n        ```\n        ```py\n         # \u8f09\u5165 Callbacks\n        from keras.callbacks import Callback\n\n        # Record_fp_tp\n        class Record_tp_tn(Callback):\n            def on_train_begin(self, epoch, logs = {}):\n                logs = logs or {}\n                record_items = [\"val_tp\", \"val_tn\"]\n                for i in record_items:\n                    if i not in self.params['metrics']:\n                        self.params['metrics'].append(i)\n            \n            def on_epoch_end(self, epoch, logs = {}, thres=0.5):\n                logs = logs or {}\n                y_true = self.validation_data[1].argmax(axis = 1)\n                y_pred = self.model.predict(self.validation_data[0])\n                y_pred = (y_pred[:, 1] >= thres) * 1\n                \n                val_tp = sum(y_true*y_pred)\n                val_tn = sum((y_true==0) & (y_pred==0))\n                \n                logs[\"val_tp\"] = val_tp\n                logs[\"val_tn\"] = val_tn\n                \n        rec_tptn = Record_tp_tn()\n\n        model.fit(x_train, y_train, \n                epochs=EPOCHS, \n                batch_size=BATCH_SIZE, \n                validation_data=(x_test, y_test), \n                shuffle=True,\n                callbacks=[rec_tptn]\n                )\n        \n        valid_tp = model.history.history['val_tp']\n        valid_tn = model.history.history['val_tn']\n        ```\n* **Day_89 : \u8a13\u7df4\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - \u64b0\u5beb\u81ea\u5df1\u7684 Loss function**\n    * \u5728 Keras \u4e2d\uff0c\u9664\uf9ba\u4f7f\u7528\u5b98\u65b9\u63d0\u4f9b\u7684 Loss function \u5916\uff0c\u4ea6\u53ef\u4ee5\u81ea\ufa08\u5b9a\u7fa9/\u4fee\u6539 loss function \u6240\u5b9a\u7fa9\u7684\u51fd\u6578\n        * \u6700\u5167\u5c64\u51fd\u5f0f\u7684\uf96b\u6578\u8f38\u5165\u9808\u6839\u64da output tensor \u800c\u5b9a\uff0c\u8209\uf9b5\uf92d\u8aaa\uff0c\u5728\u5206\u985e\u6a21\u578b\u4e2d\u9700\u8981\u6709 y_true, y_pred\n        * \u9700\u8981\u4f7f\u7528 tensor operations \u2013 \u5373\u5728 tensor \u4e0a\u904b\u7b97\u800c\u975e\u5728 numpy array \u4e0a\u9032\ufa08\u904b\u7b97\n        * \u56de\u50b3\u7684\u7d50\u679c\u662f\u4e00\u500b tensor\n        ```py\n        import keras.backend as K\n\n        def dice_coef(y_true, y_pred, smooth):\n            # \u7686\u9808\u4f7f\u7528 tensor operations\n            y_pred = y_pred >= 0.5\n            y_true_f = K.flatten(y_true)\n            y_pred_f = K.flatten(y_pred)\n            intersection = K.sum(y_true_f * y_pred_f)\n\n            return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n        \n        def dice_loss(smooth, thresh):\n            # \u6700\u5167\u5c64\u7684\u51fd\u5f0f \u2013 \u5728\u5206\u985e\u554f\u984c\u4e2d\uff0c\u53ea\u80fd\u6709 y_true \u8207 y_pred\uff0c\u5176\u4ed6\u8abf\u63a7\uf96b\u6578\u61c9\u81f3\u65bc\u5916\u5c64\u51fd\u5f0f\n            def dice(y_true, y_pred):\n                return dice_coef(y_true, y_pred):\n            # \u8f38\u51fa\u70ba Tensor\n            return dice\n        ```\n    * \u5728 Keras \u4e2d\uff0c\u6211\u5011\u53ef\u4ee5\u81ea\ufa08\u5b9a\u7fa9\u51fd\u5f0f\uf92d\u9032\ufa08\u640d\u5931\u7684\u904b\u7b97\u3002\u4e00\u500b\u640d\u5931\u51fd\u6578\u5fc5\u9808\uff1a\n        * \u6709 y_true \u8207 y_pred \uf978\u500b\u8f38\u5165\n        * \u5fc5\u9808\u53ef\u4ee5\u5fae\u5206\n        * \u5fc5\u9808\u4f7f\u7528 tensor operation\uff0c\u4e5f\u5c31\u662f\u5728 tensor \u7684\u72c0\uf9fa\u614b\u4e0b\uff0c\u9032\ufa08\u904b\u7b97\u3002\u5982 K.sum ...\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Keras\u81ea\u5b9a\u4e49Loss\u51fd\u6570](https://blog.csdn.net/A_a_ron/article/details/79050204)\n        * [focal loss](https://blog.csdn.net/u014380165/article/details/77019084)\n        ```py\n        import tensorflow as tf\n        import keras.backend as K\n\n        \"\"\"\n        # \u64b0\u5beb\u81ea\u5b9a\u7fa9\u7684 loss function: focal loss (https://blog.csdn.net/u014380165/article/details/77019084)\n        \"\"\"\n        def focal_loss(gamma=2., alpha=4.):\n            gamma = float(gamma)\n            alpha = float(alpha)\n            def focal_loss_fixed(y_true, y_pred):\n                \"\"\"Focal loss for multi-classification\n                FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n                \"\"\"\n                epsilon = 1e-8\n                y_true = tf.convert_to_tensor(y_true, tf.float32)\n                y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n\n                model_out = tf.add(y_pred, epsilon)\n                ce = tf.multiply(y_true, -tf.log(model_out))\n                weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n                fl = tf.multiply(alpha, tf.multiply(weight, ce))\n                reduced_fl = tf.reduce_max(fl, axis=1)\n                return tf.reduce_mean(reduced_fl)\n            return focal_loss_fixed\n\n        model = build_mlp(input_shape=x_train.shape[1:])\n        model.summary()\n        optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n        \"\"\"\n        # \u5728 compile \u6642\uff0c\u4f7f\u7528\u81ea\u5b9a\u7fa9\u7684 loss function\n        \"\"\"\n        model.compile(loss=focal_loss(), metrics=[\"accuracy\"], optimizer=optimizer)\n\n        model.fit(x_train, y_train, \n                epochs=EPOCHS,\n                batch_size=BATCH_SIZE,\n                validation_data=(x_test, y_test), \n                shuffle=True\n                )\n        ```\n        ```py\n        # \u81ea\u884c\u5b9a\u7fa9\u4e00\u500b loss function, \u70ba 0.3 * focal loss + 0.7 cross-entropy\n        import tensorflow as tf\n        import keras.backend as K\n\n        def combined_loss(gamma=2., alpha=4., ce_weights=0.7, fcl_weights=0.3):\n            gamma = float(gamma)\n            alpha = float(alpha)\n            def CE_focal_loss(y_true, y_pred):\n                \"\"\"Focal loss for multi-classification\n                FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n                \"\"\"\n                epsilon = 1e-8\n                y_true = tf.convert_to_tensor(y_true, tf.float32)\n                y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n\n                model_out = tf.add(y_pred, epsilon)\n                ce = tf.multiply(y_true, -tf.log(model_out))\n                weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n                fl = tf.multiply(alpha, tf.multiply(weight, ce))\n                reduced_fl = tf.reduce_max(fl, axis=1)\n                \n                ce_loss = keras.losses.categorical_crossentropy(y_true, y_pred)\n                return (ce_weights*ce_loss) + (fcl_weights*tf.reduce_mean(reduced_fl) )\n            return CE_focal_loss\n\n        ce_weights_list = [0., 0.3, 0.5, 0.7, 1]\n\n        import itertools\n        results = {}\n\n        for i, ce_w in enumerate(ce_weights_list):\n            print(\"Numbers of exp: %i, ce_weight: %.2f\" % (i, ce_w))\n\n            model = build_mlp(input_shape=x_train.shape[1:])\n            model.summary()\n            optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n            model.compile(loss=combined_loss(ce_weights=ce_w, fcl_weights=1.-ce_w), \n                        metrics=[\"accuracy\"], optimizer=optimizer)\n\n            model.fit(x_train, y_train, \n                    epochs=EPOCHS, \n                    batch_size=BATCH_SIZE, \n                    validation_data=(x_test, y_test), \n                    shuffle=True\n                    )\n        ```\n* **Day_90 : \u50b3\u7d71\u96fb\u8166\u8996\u89ba\u8207\u5f71\u50cf\u8fa8\u8b58**\n    * \u5f71\u50cf\u8fa8\u8b58\u7684\u50b3\u7d71\u65b9\u6cd5\u662f\u7279\u5fb5\u63cf\u8ff0\u53ca\u6aa2\u6e2c\uff0c\u9700\u8981\u8fa6\u6cd5\u628a\u5f71\u50cf\u7d20\uf97e\u5316\u70ba\u7279\u5fb5\uff08\u7279\u5fb5\u5de5\u7a0b\uff09\uff0c\u7136\u5f8c\u628a\u7279\u5fb5\u4e1f\u7d66\u6211\u5011\u4e4b\u524d\u5b78\u904e\u7684\u6a5f\u5668\u5b78\u7fd2\u7b97\u6cd5\uf92d\u505a\u5206\u985e\u6216\u56de\u6b78\u3002\n    * \u70ba\uf9ba\u6709\uf901\u76f4\u89c0\u7684\uf9e4\u89e3\uff0c\u9019\u88e1\u4ecb\u7d39\u4e00\u7a2e\u6700\u7c21\u55ae\u63d0\u53d6\u7279\u5fb5\u7684\u65b9\u6cd5\n        * \u5982\u4f55\u63cf\u8ff0\u984f\u8272\uff1f\n            * \u984f\u8272\u76f4\u65b9\u5716 : \u984f\u8272\u76f4\u65b9\u5716\u662f\u5c07\u984f\u8272\u4fe1\u606f\u8f49\u5316\u70ba\u7279\u5fb5\u4e00\u7a2e\u2f45\u65b9\u6cd5\uff0c\u5c07\u984f\u8272\u503c RGB \u8f49\u70ba\u76f4\u65b9\u5716\u503c\uff0c\uf92d\u63cf\u8ff0\u8272\u5f69\u548c\u5f37\u5ea6\u7684\u5206\u4f48\u60c5\u6cc1\u3002\u8209\uf9b5\uf92d\u8aaa\uff0c\u4e00\u5f35\u5f69\u8272\u5716\u6709 3 \u500bchannel\uff0c RGB\uff0c\u984f\u8272\u503c\u90fd\u4ecb\u65bc 0-255 \u4e4b\u9593\uff0c\u6700\u5c0f\u53ef\u4ee5\u53bb\u7d71\u8a08\u6bcf\u500b\u50cf\u7d20\u503c\u51fa\u73fe\u5728\u5716\u7247\u7684\u6578\uf97e\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u500b\u5340\u9593\u5982 (0 - 15)\u3001(16 - 31)\u3001...\u3001(240 - 255)\u3002\n            * \u5728\u8981\u8fa8\u8a8d\u984f\u8272\u7684\u5834\u666f\u5c31\u6703\u975e\u5e38\u6709\u7528\uff0c\u4f46\u53ef\u80fd\u5c31\u4e0d\u9069\u5408\u7528\uf92d\u505a\u908a\u7de3\u6aa2\u6e2c\u7684\u4efb\u52d9\uff0c\u56e0\u70ba\u5f9e\u984f\u8272\u7684\u5206\u4f48\u6c92\u6709\u8003\uf97e\u5230\u7a7a\u9593\u4e0a\u7684\u4fe1\u606f\u3002\n            * \u4e0d\u540c\u7684\u4efb\u52d9\uff0c\u6211\u5011\u5c31\u8981\u60f3\u8fa6\u6cd5\u91dd\u5c0d\u6027\u5730\u8a2d\u8a08\u7279\u5fb5\uf92d\u9032\ufa08\u5f8c\u7e8c\u5f71\u50cf\u8fa8\u8b58\u7684\u4efb\u52d9\u3002\n            ```py\n            import os\n            import keras\n            import cv2 # \u8f09\u5165 cv2 \u5957\u4ef6\n            import matplotlib.pyplot as plt\n\n            train, test = keras.datasets.cifar10.load_data()\n\n            image = train[0][0] # \u8b80\u53d6\u5716\u7247\n            # \u628a\u5f69\u8272\u7684\u5716\u7247\u8f49\u70ba\u7070\u5ea6\u5716\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n            ''' \u8abf\u7528 cv2.calcHist \u51fd\u6578\uff0c\u56de\u50b3\u503c\u5c31\u662f histogram\n            images (list of array)\uff1a\u8981\u5206\u6790\u7684\u5716\u7247\n            channels\uff1a\u7522\u751f\u7684\u76f4\u65b9\u5716\u985e\u578b\u3002\u4f8b\uff1a[0]\u2192\u7070\u5ea6\u5716\uff0c[0, 1, 2]\u2192RGB\u4e09\u8272\u3002\n            mask\uff1aoptional\uff0c\u82e5\u6709\u63d0\u4f9b\u5247\u50c5\u8a08\u7b97 mask \u90e8\u4efd\u7684\u76f4\u65b9\u5716\u3002\n            histSize\uff1a\u8981\u5207\u5206\u7684\u50cf\u7d20\u5f37\u5ea6\u503c\u7bc4\u570d\uff0c\u9810\u8a2d\u70ba256\u3002\u6bcf\u500bchannel\u7686\u53ef\u6307\u5b9a\u4e00\u500b\u7bc4\u570d\u3002\u4f8b\u5982\uff0c[32,32,32] \u8868\u793aRGB\u4e09\u500bchannels\u7686\u5207\u5206\u70ba32\u5340\u6bb5\u3002\n            ranges\uff1a\u50cf\u7d20\u7684\u7bc4\u570d\uff0c\u9810\u8a2d\u70ba[0,256]\uff0c\u8868\u793a<256\u3002\n            '''\n            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n            plt.figure()\n            plt.title(\"Grayscale Histogram\")\n            plt.xlabel(\"Bins\")\n            plt.ylabel(\"# of Pixels\")\n            plt.plot(hist)\n            plt.xlim([0, 256])\n            plt.show()\n\n            chans = cv2.split(image) # \u628a\u5716\u50cf\u7684 3 \u500b channel \u5207\u5206\u51fa\u4f86\n            colors = (\"r\", \"g\", \"b\")\n            plt.figure()\n            plt.title(\"'Flattened' Color Histogram\")\n            plt.xlabel(\"Bins\")\n            plt.ylabel(\"# of Pixels\")\n\n            # \u5c0d\u65bc\u6240\u6709 channel\n            for (chan, color) in zip(chans, colors):\n                # \u8a08\u7b97\u8a72 channel \u7684\u76f4\u65b9\u5716\n                hist = cv2.calcHist([chan], [0], None, [256], [0, 256])\n            \n                # \u756b\u51fa\u8a72 channel \u7684\u76f4\u65b9\u5716\n                plt.plot(hist, color = color)\n                plt.xlim([0, 256])\n            plt.show()\n            ```\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u56fe\u50cf\u5206\u7c7b|\u6df1\u5ea6\u5b66\u4e60PK\u4f20\u7edf\u673a\u5668\u5b66\u4e60](https://cloud.tencent.com/developer/article/1111702)\n        * [OpenCv - \u76f4\u65b9\u5716](https://chtseng.wordpress.com/2016/12/05/opencv-histograms%E7%9B%B4%E6%96%B9%E5%9C%96/)\n        * [OpenCv - \u6559\u5b78\u6587\u6a94](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html)\n        * [Introduction to Computer Vision](https://www.udacity.com/course/introduction-to-computer-vision--ud810)\n* **Day_91 : \u50b3\u7d71\u96fb\u8166\u8996\u89ba\u8207\u5f71\u50cf\u8fa8\u8b58 </>**        \n    * \u9760\u4eba\u5de5\u8a2d\u8a08\u7684\u7279\u5fb5\u5728\u7c21\u55ae\u7684\u4efb\u52d9\u4e0a\u4e5f\u8a31\u662f\u582a\u7528\uff0c\u4f46\u8907\u96dc\u7684\u60c5\u6cc1\uff0c\u6bd4\u5982\u8aaa\u5206\u985e\u7684\u985e\u5225\u591a\u8d77\uf92d\uff0c\u5c31\u80fd\u660e\u986f\u611f\u89ba\u5230\u9019\u4e9b\u7279\u5fb5\u7684\u4e0d\u8db3\u4e4b\u8655\uff0c\u9ad4\u6703\u9019\u4e00\u9ede\u80fd\uf901\u5e6b\u52a9\uf9e4\u89e3\u63a5\u4e0b\uf92d\u7684\u5377\u7a4d\u795e\u7d93\u7db2\uf937\u7684\u610f\u7fa9\u3002\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u7d22\u4f2f\u7b97\u5b50](https://zh.wikipedia.org/wiki/%E7%B4%A2%E8%B2%9D%E7%88%BE%E7%AE%97%E5%AD%90)\n        * [\u57fa\u4e8e\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u7684\u76ee\u6807\u68c0\u6d4b\u4e0e\u8bc6\u522b(HOG+SVM\u9644\u4ee3\u7801)](https://www.cnblogs.com/zyly/p/9651261.html)\n        * [\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u662f\u4ec0\u4e48](https://www.zhihu.com/question/21094489)\n        ```py\n        import os\n        import keras\n        import tensorflow as tf\n        import numpy as np\n        import cv2 # \u8f09\u5165 cv2 \u5957\u4ef6\n        import matplotlib.pyplot as plt\n\n        train, test = keras.datasets.cifar10.load_data()\n        x_train, y_train = train\n        x_test, y_test = test\n        y_train = y_train.astype(int)\n        y_test = y_test.astype(int)\n\n        # \u7522\u751f\u76f4\u65b9\u5716\u7279\u5fb5\u7684\u8a13\u7df4\u8cc7\u6599\n        x_train_histogram = []\n        x_test_histogram = []\n\n        # \u5c0d\u65bc\u6240\u6709\u8a13\u7df4\u8cc7\u6599\n        for i in range(len(x_train)):\n            chans = cv2.split(x_train[i]) # \u628a\u5716\u50cf\u7684 3 \u500b channel \u5207\u5206\u51fa\u4f86\n            # \u5c0d\u65bc\u6240\u6709 channel\n            hist_feature = []\n            for chan in chans:\n                # \u8a08\u7b97\u8a72 channel \u7684\u76f4\u65b9\u5716\n                hist = cv2.calcHist([chan], [0], None, [16], [0, 256]) # \u5207\u6210 16 \u500b bin\n                hist_feature.extend(hist.flatten())\n            # \u628a\u8a08\u7b97\u7684\u76f4\u65b9\u5716\u7279\u5fb5\u6536\u96c6\u8d77\u4f86\n            x_train_histogram.append(hist_feature)\n\n        # \u5c0d\u65bc\u6240\u6709\u6e2c\u8a66\u8cc7\u6599\u4e5f\u505a\u4e00\u6a23\u7684\u8655\u7406\n        for i in range(len(x_test)):\n            chans = cv2.split(x_test[i]) # \u628a\u5716\u50cf\u7684 3 \u500b channel \u5207\u5206\u51fa\u4f86\n            # \u5c0d\u65bc\u6240\u6709 channel\n            hist_feature = []\n            for chan in chans:\n                # \u8a08\u7b97\u8a72 channel \u7684\u76f4\u65b9\u5716\n                hist = cv2.calcHist([chan], [0], None, [16], [0, 256]) # \u5207\u6210 16 \u500b bin\n                hist_feature.extend(hist.flatten())\n            x_test_histogram.append(hist_feature)\n\n        x_train_histogram = np.array(x_train_histogram)\n        x_test_histogram = np.array(x_test_histogram)\n\n        # \u7522\u751f HOG \u7279\u5fb5\u7684\u8a13\u7df4\u8cc7\u6599\n        bin_n = 16 # Number of bins\n\n        def hog(img):\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n            gx = cv2.Sobel(img, cv2.CV_32F, 1, 0)\n            gy = cv2.Sobel(img, cv2.CV_32F, 0, 1)\n            mag, ang = cv2.cartToPolar(gx, gy)\n            bins = np.int32(bin_n*ang/(2*np.pi))    # quantizing binvalues in (0...16)\n            bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:]\n            mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]\n            hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)]\n            hist = np.hstack(hists)     # hist is a 64 bit vector\n            return hist.astype(np.float32)\n\n        x_train_hog = np.array([hog(x) for x in x_train])\n        x_test_hog = np.array([hog(x) for x in x_test])\n\n        # \u7528 histogram \u7279\u5fb5\u8a13\u7df4 SVM \u6a21\u578b\n        SVM_hist = cv2.ml.SVM_create()\n        SVM_hist.setKernel(cv2.ml.SVM_LINEAR)\n        SVM_hist.setGamma(5.383)\n        SVM_hist.setType(cv2.ml.SVM_C_SVC)\n        SVM_hist.setC(2.67)\n\n        #training\n        SVM_hist.train(x_train_histogram, cv2.ml.ROW_SAMPLE, y_train)\n\n        # prediction\n        _, y_hist_train = SVM_hist.predict(x_train_histogram)\n        _, y_hist_test = SVM_hist.predict(x_test_histogram)\n\n        # \u7528 HOG \u7279\u5fb5\u8a13\u7df4 SVM \u6a21\u578b\n        SVM_hog = cv2.ml.SVM_create()\n        SVM_hog.setKernel(cv2.ml.SVM_LINEAR)\n        SVM_hog.setGamma(5.383)\n        SVM_hog.setType(cv2.ml.SVM_C_SVC)\n        SVM_hog.setC(2.67)\n\n        #training\n        SVM_hog.train(x_train_hog, cv2.ml.ROW_SAMPLE, y_train)\n\n        # prediction\n        _, y_hog_train = SVM_hog.predict(x_train_hog)\n        _, y_hog_test = SVM_hog.predict(x_test_hog)\n\n        # accuracy\n        acc_hist_train = (sum(y_hist_train == y_train) / len(y_hist_train))[0] * 100\n        acc_hog_train = (sum(y_hog_train == y_train) / len(y_hog_train))[0] * 100\n        acc_hist_test = (sum(y_hist_test == y_test) / len(y_hist_test))[0] * 100\n        acc_hog_test = (sum(y_hog_test == y_test) / len(y_hog_test))[0] * 100\n\n        import numpy as np\n\n        labels = ['training', 'testing']\n        hist_acc = [acc_hist_train, acc_hist_test]\n        hog_acc = [acc_hog_train, acc_hog_test]\n\n        x = np.arange(len(labels))  # the label locations\n        width = 0.35  # the width of the bars\n\n        fig, ax = plt.subplots()\n        rects1 = ax.bar(x - width/2, hist_acc, width, label='histogram')\n        rects2 = ax.bar(x + width/2, hog_acc, width, label='HOG')\n\n        # Add some text for labels, title and custom x-axis tick labels, etc.\n        ax.set_ylabel('accuracy %')\n        ax.set_title('cifar10 by SVM with different features')\n        ax.set_ylim(0,30)\n        ax.set_xticks(x)\n        ax.set_xticklabels(labels)\n        ax.legend()\n\n\n        def autolabel(rects):\n            \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n            for rect in rects:\n                height = rect.get_height()\n                ax.annotate('{0:.3f}'.format(height),\n                            xy=(rect.get_x() + rect.get_width() / 2, height),\n                            xytext=(0, 3),  # 3 points vertical offset\n                            textcoords=\"offset points\",\n                            ha='center', va='bottom')\n\n\n        autolabel(rects1)\n        autolabel(rects2)\n\n        fig.tight_layout()\n\n        plt.show()\n        ```\n### \u6df1\u5ea6\u5b78\u7fd2\u61c9\u7528\u5377\u7a4d\u795e\u7d93\u7db2\u8def\n* **Day_92 : \u5377\u7a4d\u795e\u7d93\u7db2\uf937(Convolution Neural Network, CNN) \u7c21\u4ecb**\n    * ImageNet Challenge \u662f\u96fb\u8166\u8996\u89ba\u7684\u7af6\u8cfd\uff0c\u9700\u8981\u5c0d\u5f71\u50cf\u9032\ufa08 1000 \u500b\u985e\u5225\u7684\u9810\u6e2c\uff0c\u5728 CNN \u51fa\u73fe\u5f8c\u9996\u6b21\u6709\u8d85\u8d8a\u4eba\u985e\u6e96\u78ba\u7387\u7684\u6a21\u578b\n    * \u5377\u7a4d\u662f\u751a\u9ebc\uff1f\n        * \u5377\u7a4d\u5176\u5be6\u53ea\u662f\u7c21\u55ae\u7684\u6578\u5b78\u4e58\u6cd5\u8207\u52a0\u6cd5\n        * \uf9dd\u7528\u6ffe\u6ce2\u5668 (filter) \u5c0d\u5716\u50cf\u505a\u5377\u7a4d\uf92d\u627e\u5c0b\u898f\u5247\n        * \u5377\u7a4d\u662f\u5c07\u5f71\u50cf\u8207 filter \u7684\u503c\u76f8\u4e58\u5f8c\u518d\u9032\ufa08\u52a0\u7e3d\uff0c\u5373\u53ef\u5f97\u5230\u7279\u5fb5\u5716 (Feature Map)\n    * \u5377\u7a4d\u7684\u76ee\u7684\n        * \u900f\u904e\u5377\u7a4d\uff0c\u6211\u5011\u53ef\u4ee5\u627e\u51fa\u5716\u50cf\u4e0a\u8207\u6ffe\u6ce2\u5668\u5177\u6709\u76f8\u540c\u7279\u5fb5\u7684\u5340\u57df\n    * \u6ffe\u6ce2\u5668 (filter)\n        * \u6ffe\u6ce2\u5668\u662f\u7528\uf92d\u627e\u5716\u50cf\u4e0a\u662f\u5426\u6709\u540c\u6a23\u7279\u5fb5\n        * \u90a3\u6ffe\u6ce2\u5668 (filter) \u4e2d\u7684\u6578\u5b57\u662f\u600e\u9ebc\u5f97\uf92d\u7684\u5462?\n            * \u5176\u5be6\u662f\u900f\u904e\u8cc7\u6599\u5b78\u7fd2\u800c\uf92d\u7684! \u9019\u4e5f\u5c31\u662f CNN \u6a21\u578b\u4e2d\u7684\uf96b\u6578 (\u6216\u53eb\u6b0a\u91cd weights)\n            * CNN \u6703\u81ea\u52d5\u5f9e\u8a13\u7df4\u8cc7\u6599\u4e2d\u5b78\u7fd2\u51fa\u9069\u5408\u7684\u6ffe\u6ce2\u5668\uf92d\u5b8c\u6210\u4f60\u7684\u4efb\u52d9 (\u5206\u985e\u3001\u5075\u6e2c\u7b49)\n        * \u6ffe\u6ce2\u5668 (Filter) \u8996\u89ba\u5316\n            * \u900f\u904e\u4e00\u5c64\u53c8\u4e00\u5c64\u7684\u795e\u7d93\u7db2\uf937\u758a\u52a0\uff0c\u53ef\u4ee5\u770b\u5230\u5e95\u5c64\u7684\u6ffe\u6ce2\u5668\u5728\u627e\u7dda\u689d\u8207\u984f\u8272\u7684\u7279\u5fb5\uff0c\u4e2d\u5c64\u5247\u662f\u8f2a\uf9d7\u5ed3\u8207\u5f62\uf9fa (\uf9d7\u80ce)\uff0c\u9ad8\u5c64\u7684\u5247\u662f\u76f8\u5c0d\u5b8c\u6574\u7684\u7279\u5fb5 (\u5982\uf902\u7a97\u3001\u5f8c\u7167\u93e1\u7b49)\n* **Day_93 : \u5377\u7a4d\u795e\u7d93\u7db2\uf937\u67b6\u69cb\u7d30\u7bc0**\n    * \u5377\u7a4d\u795e\u7d93\u7db2\uf937\u8ddf\u6df1\u5ea6\u7db2\uf937  \n        * \u50b3\u7d71\u7684 DNN\uff08\u5373 Deep neural network\uff09\u6700\u5927\u554f\u984c\u5728\u65bc\u5b83\u6703\u5ffd\uf976\u8cc7\u6599\u7684\u5f62\uf9fa\u3002\n            * \uf9b5\u5982\uff0c\u8f38\u5165\u5f71\u50cf\u7684\u8cc7\u6599\u6642\uff0c\u8a72 data \u901a\u5e38\u5305\u542b\uf9ba\u6c34\u5e73\u3001\u5782\u76f4\u3001color channel \u7b49\u4e09\u7dad\u8cc7\u8a0a\uff0c\u4f46\u50b3\u7d71 DNN \u7684\u8f38\u5165\u8655\uf9e4\u5fc5\u9808\u662f\u5e73\u9762\u7684\u3001\u4e5f\u5c31\u662f\u9808\u4e00\u7dad\u7684\u8cc7\u6599\u3002\n            * \u4e00\u4e9b\u91cd\u8981\u7684\u7a7a\u9593\u8cc7\u6599\uff0c\u53ea\u6709\u5728\u4e09\u7dad\u5f62\uf9fa\u4e2d\u624d\u80fd\u4fdd\uf9cd\u4e0b\uf92d\u3002\n            * RGB \u4e0d\u540c\u7684 channel \u4e4b\u9593\u4e5f\u53ef\u80fd\u5177\u6709\u67d0\u4e9b\u95dc\u9023\u6027\u3001\u800c\u9060\u8fd1\u4e0d\u540c\u7684\u50cf\u7d20\u5f7c\u6b64\u4e5f\u61c9\u5177\u6709\u4e0d\u540c\u7684\u95dc\uf997\u6027\n        * \u6df1\u5ea6\u5b78\u7fd2\uff08Deep learning\uff09\u4e2d\u7684 CNN \u8f03\u50b3\u7d71\u7684 DNN \u591a\uf9ba Convolutional\uff08\u5377\u7a4d\uff09\u53ca\u6c60\u5316\uff08Pooling\uff09\uf978\u5c64 layer\uff0c\u7528\u4ee5\u7dad\u6301\u5f62\uf9fa\u8cc7\u8a0a\u4e26\u4e14\u907f\u514d\uf96b\u6578\u5927\u5e45\u589e\u52a0\u3002\n        * Convolution \u539f\uf9e4\u662f\u900f\u904e\u4e00\u500b\u6307\u5b9a\u5c3a\u5bf8\u7684window\uff0c\u7531\u4e0a\u800c\u4e0b\u4f9d\u5e8f\u6ed1\u52d5\u53d6\u5f97\u5716\u50cf\u4e2d\u5404\u5c40\u90e8\u7279\u5fb5\u4f5c\u70ba\u4e0b\u4e00\u5c64\u7684\u8f38\u5165\uff0c\u9019\u500b sliding window \u5728 CNN \u4e2d\u7a31\u70ba Convolution kernel \u5229\u7528\u6b64\u65b9\u5f0f\uf92d\u53d6\u5f97\u5716\u50cf\u4e2d\u5404\u5c40\u90e8\u7684\u5340\u57df\u52a0\u7e3d\u8a08\u7b97\u5f8c\uff0c\u900f\u904e ReLU activation function \u8f38\u51fa\u70ba\u7279\u5fb5\u503c\u518d\u63d0\u4f9b\u7d66\u4e0b\u4e00\u5c64\u4f7f\u7528\n    * **\u6c60\u5316\u5c64 (Pooling Layer)**\n        * Pooling layer \u7a31\u70ba\u6c60\u5316\u5c64\uff0c\u5b83\u7684\u529f\u80fd\u5f88\u55ae\u7d14\uff0c\u5c31\u662f\u5c07\u8f38\u5165\u7684\u5716\u7247\u5c3a\u5bf8\u7e2e\u5c0f\uff08\u5927\u90e8\u4efd\u70ba\u7e2e\u5c0f\u4e00\u534a\uff09\u4ee5\u6e1b\u5c11\u6bcf\u5f35 feature map \u7dad\u5ea6\u4e26\u4fdd\uf9cd\u91cd\u8981\u7684\u7279\u5fb5\uff0c\u5176\u597d\u8655\u6709\uff1a\n            * \u7279\u5fb5\u964d\u7dad\uff0c\u6e1b\u5c11\u5f8c\u7e8c layer \u9700\u8981\uf96b\u6578\u3002\n            * \u5177\u6709\u6297\u5e72\u64fe\u7684\u4f5c\u7528\uff1a\u5716\u50cf\u4e2d\u67d0\u4e9b\u50cf\u7d20\u5728\u9130\u8fd1\u5340\u57df\u6709\u5fae\u5c0f\u504f\u79fb\u6216\u5dee\uf962\u6642\uff0c\u5c0d Pooling layer \u7684\u8f38\u51fa\u5f71\u97ff\u4e0d\u5927\uff0c\u7d50\u679c\u4ecd\u662f\u4e0d\u8b8a\u7684\u3002\n            * \u6e1b\u5c11\u904e\u5ea6\u64ec\u5408 over-fitting \u7684\u60c5\u6cc1\u3002\u8207\u5377\u7a4d\u5c64\u76f8\u540c\uff0c\u6c60\u5316\u5c64\u6703\u4f7f\u7528 kernel \uf92d\u53d6\u51fa\u5404\u5340\u57df\u7684\u503c\u4e26\u904b\u7b97\uff0c\u4f46\u6700\u5f8c\u7684\u8f38\u51fa\u4e26\u4e0d\u900f\u904e Activate function\uff08\u5377\u7a4d\u5c64\u4f7f\u7528\u7684 function\u662f ReLU\uff09\n    * \u5377\u7a4d\u7db2\uf937\u7684\u7d44\u6210\n        * Convolution Layer \u5377\u7a4d\u5c64\n        * Pooling Layer \u6c60\u5316\u5c64\n        * Flatten Layer \u5e73\u5766\u5c64\n        * Fully connection Layer \u5168\u9023\u63a5\u5c64\n    * Flatten \u2013 \u5e73\u5766\u5c64\n        * Flatten\uff1a\u5c07\u7279\u5fb5\u8cc7\u8a0a\u4e1f\u5230 Full connected layer \uf92d\u9032\ufa08\u5206\u985e\uff0c\u5176\u795e\u7d93\u5143\u53ea\u8207\u4e0a\u4e00\u5c64 kernel \u7684\u50cf\u7d20\u9023\u7d50\uff0c\u800c\u4e14\u5404\u9023\u7d50\u7684\u6b0a\u91cd\u5728\u540c\u5c64\u4e2d\u662f\u76f8\u540c\u4e14\u5171\u4eab\u7684\n    * Fully connected layers - \u5168\u9023\u63a5\u5c64\n        * \u5377\u7a4d\u548c\u6c60\u5316\u5c64\uff0c\u5176\u6700\u4e3b\u8981\u7684\u76ee\u7684\u5206\u5225\u662f\u63d0\u53d6\u7279\u5fb5\u53ca\u6e1b\u5c11\u5716\u50cf\uf96b\u6578\uff0c\u7136\u5f8c\u5c07\u7279\u5fb5\u8cc7\u8a0a\u4e1f\u5230 Full connected layer \uf92d\u9032\ufa08\u5206\u985e\uff0c\u5176\u795e\u7d93\u5143\u53ea\u8207\u4e0a\u4e00\u5c64 kernel \u7684\u50cf\u7d20\u9023\u7d50\uff0c\u800c\u4e14\u5404\u9023\u7d50\u7684\u6b0a\u91cd\u5728\u540c\u5c64\u4e2d\u662f\u76f8\u540c\u4e14\u5171\u4eab\u7684\n        ```py\n        #\u5c0e\u5165\u76f8\u95dc\u6a21\u7d44\n        import keras\n        from keras import layers\n        from keras import models\n        from keras.models import Sequential\n        from keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense\n\n        #\u5efa\u7acb\u4e00\u500b\u5e8f\u5217\u6a21\u578b\n        model = models.Sequential()\n\n        #\u5efa\u7acb\u4e00\u500b\u5377\u7e3e\u5c64, 32 \u500b\u5167\u6838, \u5167\u6838\u5927\u5c0f 3x3, \n        #\u8f38\u5165\u5f71\u50cf\u5927\u5c0f 28x28x1\n        model.add(layers.Conv2D(32, (3, 3), input_shape=(28, 28, 1)))\n        #\u5efa\u7acb\u7b2c\u4e8c\u500b\u5377\u7e3e\u5c64,\n        #\u8acb\u6ce8\u610f, \u4e0d\u9700\u8981\u518d\u8f38\u5165 input_shape\n        model.add(layers.Conv2D(25, (3, 3)))\n\n        #\u65b0\u589e\u5e73\u5766\u5c64\n        model.add(Flatten())\n\n        #\u5efa\u7acb\u4e00\u500b\u5168\u9023\u63a5\u5c64\n        model.add(Dense(units=100))\n        model.add(Activation('relu'))\n\n        #\u5efa\u7acb\u4e00\u500b\u8f38\u51fa\u5c64, \u4e26\u63a1\u7528softmax\n        model.add(Dense(units=10))\n        model.add(Activation('softmax'))\n\n        #\u8f38\u51fa\u6a21\u578b\u7684\u5806\u758a\n        model.summary()\n        ```\n* **Day_94 : \u5377\u7a4d\u795e\u7d93\u7db2\uf937 - \u5377\u7a4d(Convolution)\u5c64\u8207\uf96b\u6578\u8abf\u6574**\n    * \u5377\u7a4d (Convolution) \u7684\u8d85\uf96b\u6578 (Hyper parameter)\n        * \u5377\u7a4d\u5167\u6838 (kernel)\n        * Depth (kernels\u7684\u7e3d\u6578)\n        * Padding (\u662f\u5426\u52a0\u4e00\u5708 0 \u503c\u7684 pixel)\n        * Stride (\u9078\u6846\u6bcf\u6b21\u79fb\u52d5\u7684\u6b65\u6578)\n    * \u586b\u5145\u6216\u79fb\u52d5\u6b65\u6578 (Padding/Stride) \u7684\u7528\u9014\n        * RUN \u904e CNN\uff0c\uf978\u500b\u554f\u984c\n            * \u662f\u4e0d\u662f\u5377\u7a4d\u8a08\u7b97\u5f8c\uff0c\u5377\u7a4d\u5f8c\u7684\u5716\u662f\u4e0d\u662f\u5c31\u4e00\u5b9a\u53ea\u80fd\u8b8a\u5c0f?\n                * \u53ef\u4ee5\u9078\u64c7\u7dad\u6301\u4e00\u6a23\u5927\n            * \u5377\u7a4d\u8a08\u7b97\u662f\u4e0d\u662f\u4e00\u6b21\u53ea\u80fd\u79fb\u52d5\u4e00\u683c?\n        * \u63a7\u5236\u5377\u7a4d\u8a08\u7b97\u7684\u5716\u5927\u5c0f - Valid and Same convolutions\n            * padding = \u2018VALID\u2019 \u7b49\u65bc\u6700\u4e00\u958b\u59cb\u6558\u8ff0\u7684\u5377\u7a4d\u8a08\u7b97\uff0c\u5716\u6839\u64da filter \u5927\u5c0f\u548c stride \u5927\u5c0f\u800c\u8b8a\u5c0f\n            * new_height = new_width = (W-F + 1) / S\n            * padding = \u2018 Same\u2019\u7684\u610f\u601d\u662f\u5c31\u662f\u8981\u8b93\u8f38\u5165\u548c\u8f38\u51fa\u7684\u5927\u5c0f\u662f\u4e00\u6a23\u7684\n            * pad=1\uff0c\u8868\u793a\u5716\u5916\u5708\u984d\u5916\u52a0 1 \u5708 0\uff0c\u5047\u8a2d pad=2\uff0c\u5716\u5916\u5708\u984d\u5916\u52a0 2 \u5708 0\uff0c\u4ee5\u6b64\u985e\u63a8\n    * \u8209\uf9b5\n        `Model.add(Convolution2D(32, 3, 3), input_shape=(1, 28, 28), strides=2, padding='valid\u2019)`\n        * \u9019\u4ee3\u8868\u5377\u7a4d\u5c64 filter \u6578\u8a2d\u5b9a\u70ba 32\uff0cfilter \u7684 kernel size \u662f 3\uff0c\u6b65\u4f10 stride \u662f 2\uff0cpad \u662f1\u3002\n            * pad = 1\uff0c\u8868\u793a\u5716\u5916\u5708\u984d\u5916\u52a0 1 \u5708 0\uff0c\u5047\u8a2d pad = 2\uff0c\u5716\u5916\u5708\u984d\u5916\u52a0 2 \u5708 0\uff0c\u4ee5\u6b64\u985e\u63a8\n                1. kernel size \u662f 3 \u7684\u6642\u5019\uff0c\u5377\u7a4d\u5f8c\u5716\u7684\u5bec\u9ad8\u4e0d\u8981\u8b8a\uff0cpad \u5c31\u8981\u8a2d\u5b9a\u70ba 1\n                2. kernel size \u662f 5 \u7684\u6642\u5019\uff0c\u5377\u7a4d\u5f8c\u5716\u7684\u5bec\u9ad8\u4e0d\u8981\u8b8a\uff0cpad \u5c31\u8981\u8a2d\u5b9a\u70ba 2\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n* **Day_95 : \u5377\u7a4d\u795e\u7d93\u7db2\uf937 - \u6c60\u5316(Pooling)\u5c64\u8207\uf96b\u6578\u8abf\u6574**\n    * \u6c60\u5316\u5c64 (Pooling Layer) \u5982\u4f55\u8abf\u7528\n        `keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n        * pool_size\uff1a\u6574\u6578\uff0c\u6cbf\uff08\u5782\u76f4\uff0c\u6c34\u5e73\uff09\u65b9\u5411\u7e2e\u5c0f\u6bd4\uf9b5\u7684\u56e0\u6578\u3002\n            * (2\uff0c2)\u6703\u628a\u8f38\u5165\u5f35\uf97e\u7684\uf978\u500b\u7dad\u5ea6\u90fd\u7e2e\u5c0f\u4e00\u534a\n        * strides\uff1a\u6574\u6578\uff0c2 \u500b\u6574\u6578\u8868\u793a\u7684\u5143\u7d44\uff0c\u6216\u8005\u662f\u201dNone\u201d\u3002\u8868\u793a\u6b65\u9577\u503c\u3002\n            * \u5982\u679c\u662f None\uff0c\u90a3\u9ebc\u9ed8\u8a8d\u503c\u662f pool_size\u3002\n            * padding\uff1a\"valid\"\u6216\u8005\"same\"\uff08\u5340\u5206\u5927\u5c0f\u5beb\uff09\u3002\n            * data_format\uff1achannels_last(\u9ed8\u8a8d)\u6216 channels_first \u4e4b\u4e00\u3002\u8868\u2f70\\\u793a\u8f38\u5165\u5404\u7dad\u5ea6\u7684\u9806\u5e8f\n                * channels_last \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, height, width, channels) \u7684\u8f38\u5165\u5f35\uf97e\u3002\n                * channels_first \u4ee3\u8868\u5c3a\u5bf8\u662f (batch, channels, height, width) \u7684\u8f38\u5165\u5f35\uf97e\u3002\n    * \u6c60\u5316\u5c64 (Pooling Layer) \u8d85\uf96b\u6578\n        * \u524d\u7aef\u8f38\u5165 feature map \u7dad\u5ea6\uff1aW1\u00d7H1\u00d7D1 \u6709\uf978\u500b hyperparameters\uff1a\n            * Pooling filter \u7684\u7dad\u5ea6 - F,\n            * \u79fb\u52d5\u7684\u6b65\u6578S,\n        * \u6240\u4ee5\u9810\u8a08\u751f\u6210\u7684\u8f38\u51fa\u662f W2\u00d7H2\u00d7D2:\n            * W2=(W1\u2212F)/S+1\n            * H2=(H1\u2212F)/S+1\n            * D2=D1\n    * \u6c60\u5316\u5c64 (Pooling Layer ) \u5e38\u7528\u7684\u985e\u578b\n        * Max pooling (\u6700\u5927\u6c60\u5316)\n        * Average pooling (\u5e73\u5747\u6c60\u5316)\n    * \u5377\u7a4d\u795e\u7d93\u7db2\uf937 (CNN) \u7279\u6027\n        * \u9069\u5408\u7528\u5728\u5f71\u50cf\u4e0a\n            * \u56e0\u70ba fully-connected networking (\u5168\u9023\u63a5\u5c64) \u5982\u679c\u7528\u5728\u5f71\u50cf\u8fa8\u8b58\u4e0a\uff0c\u6703\u5c0e\u81f4\uf96b\u6578\u904e\u591a(\u56e0\u70ba\u50cf\u7d20\u5f88\u591a)\uff0c\u5c0e\u81f4 over-fitting (\u904e\u5ea6\u64ec\u5408)\n            * CNN \u91dd\u5c0d\u5f71\u50cf\u8fa8\u8b58\u7684\u7279\u6027\uff0c\u7279\u5225\u8a2d\u8a08\u904e\uff0c\uf92d\u6e1b\u5c11\uf96b\u6578\n            * Convolution(\u5377\u7a4d) : \u5b78\u51fa filter \u6bd4\u5c0d\u539f\u59cb\u5716\u7247\uff0c\u7522\u751f\u51fa feature map (\u7279\u5fb5\u5716, \u4e5f\u7576\u6210image)\n            * Max Pooling (\u6700\u5927\u6c60\u5316)\uff1a\u5c07 feature map \u7e2e\u5c0f\n            * Flatten (\u5e73\u5766\u5c64)\uff1a\u5c07\u6bcf\u500b\u50cf\u7d20\u7684 channels (\u6709\u591a\u5c11\u500b filters) \u5c55\u958b\u6210 fully connected feedforward network (\u5168\u9023\u63a5\u7684\u524d\ufa08\u7db2\uf937) \n        * AlphaGo \u4e5f\u7528\uf9ba CNN\uff0c\u4f46\u662f\u6c92\u6709\u7528 Max Pooling (\u6240\u4ee5\u4e0d\u554f\u984c\u9700\u8981\u4e0d\u540c model)\n    * Pooling Layer (\u6c60\u5316\u5c64) \u9069\u7528\u7684\u5834\u666f\n        * \u7279\u5fb5\u63d0\u53d6\u7684\u8aa4\u5dee\u4e3b\u8981\uf92d\u81ea\uf978\u500b\u65b9\u9762\uff1a\n            1. \u9130\u57df\u5927\u5c0f\u53d7\u9650\u9020\u6210\u7684\u4f30\u8a08\u503c\u65b9\u5dee\u589e\u5927\uff1b\n            2. \u5377\u7a4d\u5c64\u8d85\uf96b\u6578\u8207\u5167\u6838\u9020\u6210\u4f30\u8a08\u5747\u503c\u7684\u504f\u79fb\n        * \u4e00\u822c\u4f86\uf92d\u8aaa\uff0c\n            * average-pooling \u80fd\u6e1b\u5c0f\u7b2c\u4e00\u7a2e\u8aa4\u5dee\uff0c\uf901\u591a\u7684\u4fdd\uf9cd\u5716\u50cf\u7684\u80cc\u666f\u4fe1\u606f\n            * max-pooling \u80fd\u6e1b\u5c0f\u7b2c\u4e8c\u7a2e\u8aa4\u5dee\uff0c\uf901\u591a\u7684\u4fdd\uf9cd\u7d0b\uf9e4\u4fe1\u606f\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u57fa\u4e8eKeras\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u53ef\u89c6\u5316](https://blog.csdn.net/weiwei9363/article/details/79112872)\n        ```py\n        # GRADED FUNCTION: zero_pad\n        def zero_pad(X, pad):\n            \"\"\"\n            \u5c0dimage X \u505a zero-padding. \n            \u53c3\u6578\u5b9a\u7fa9\u5982\u4e0b:\n            X -- python numpy array, \u5448\u73fe\u7dad\u5ea6 (m, n_H, n_W, n_C), \u4ee3\u8868\u4e00\u6279 m \u500b\u5716\u50cf\n                n_H: \u5716\u9ad8, n_W: \u5716\u5bec, n_C: color channels \u6578\n            pad -- \u6574\u6578, \u52a0\u5e7e\u5708\u7684 zero padding.\n            Returns:\n            X_pad -- image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C) \u505a\u5b8czero-padding \u7684\u7d50\u679c\n            \"\"\"\n            ### Code \u8d77\u59cb\u4f4d\u7f6e\n            X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=(0, 0))\n            \n            return X_pad\n\n        # GRADED FUNCTION: pool_forward\n        def pool_forward(A_prev, hparameters, mode = \"max\"):\n            \"\"\"\n            \u8a2d\u8a08\u4e00\u500b\u524d\u884c\u7db2\u8def\u7684\u6c60\u5316\u5c64\n            \u53c3\u6578\u5b9a\u7fa9\u5982\u4e0b:\n            A_prev -- \u8f38\u5165\u7684numpy \u9663\u5217, \u7dad\u5ea6 (m, n_H_prev, n_W_prev, n_C_prev)\n            hparameter \u8d85\u53c3\u6578 --  \"f\" and \"stride\" \u6240\u5f62\u6210\u7684python \u5b57\u5178\n            mode -- \u6c60\u5316\u7684\u6a21\u5f0f: \"max\" or \"average\"\n            \n            \u8fd4\u56de:\n                A -- \u8f38\u51fa\u7684\u6c60\u5316\u5c64, \u7dad\u5ea6\u70ba (m, n_H, n_W, n_C) \u7684 numpy \u9663\u5217\n                cache -- \u53ef\u4ee5\u61c9\u7528\u5728 backward pass pooling layer \u8cc7\u6599, \u5305\u542b input and hparameter\n            \"\"\"\n\n            # \u6aa2\u7d22\u5c3a\u5bf8 from the input shape\n            (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n\n            # \u6aa2\u7d22\u8d85\u53c3\u6578 from \"hparameters\"\n            f = hparameters[\"f\"]\n            stride = hparameters[\"stride\"]\n\n            # \u5b9a\u7fa9\u8f38\u51fa\u7684dimensions\n            n_H = int(1 + (n_H_prev - f) / stride)\n            n_W = int(1 + (n_W_prev - f) / stride)\n            n_C = n_C_prev\n\n            # \u521d\u59cb\u5316\u8f38\u51fa\u7684 matrix A\n            A = np.zeros((m, n_H, n_W, n_C))\n\n            ### \u7a0b\u5f0f\u8d77\u59cb\u4f4d\u7f6e ###\n            for i in range(m): # \u8a13\u7df4\u6a23\u672c\u7684for \u8ff4\u5708\n                for h in range(n_H): # \u8f38\u51fa\u6a23\u672c\u7684for \u8ff4\u5708, \u91dd\u5c0dvertical axis\n                    for w in range(n_W): #  \u8f38\u51fa\u6a23\u672c\u7684for \u8ff4\u5708, \u91dd\u5c0d horizontal axis\n                        for c in range (n_C): #  \u8f38\u51fa\u6a23\u672c\u7684for \u8ff4\u5708, \u91dd\u5c0dchannels\n\n                            # \u627e\u51fa\u7279\u5fb5\u5716\u7684\u5bec\u5ea6\u8ddf\u9ad8\u5ea6\u56db\u500b\u9ede\n                            vert_start = h * stride\n                            vert_end = h * stride+ f\n                            horiz_start = w * stride\n                            horiz_end = w * stride + f\n\n                            # \u5b9a\u7fa9\u7b2ci\u500b\u8a13\u7df4\u793a\u4f8b\u4e2d\n                            a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end,c]\n\n                            # \u8a08\u7b97\u8f38\u5165data \u7684\u6c60\u5316\u7d50\u679c. \u4f7f\u7528 if statment \u53bb\u505a\u5206\u985e\n                            if mode == \"max\":\n                                A[i, h, w, c] = np.max(a_prev_slice)\n                            elif mode == \"average\":\n                                A[i, h, w, c] = np.mean(a_prev_slice)\n\n                                ### \u7a0b\u5f0f\u7d50\u675f ###\n            \n            # \u5132\u5b58\u8f38\u5165\u7684\u7279\u5fb5\u5716\u8ddf\u6240\u8a2d\u5b9a\u7684\u8d85\u53c3\u6578, \u53ef\u4ee5\u7528\u5728 pool_backward()\n            cache = (A_prev, hparameters)\n            \n            # \u78ba\u8a8d\u8f38\u51fa\u7684\u8cc7\u6599\u7dad\u5ea6\n            assert(A.shape == (m, n_H, n_W, n_C))\n            \n            return A, cache\n        ```\n* **Day_96 : Keras \u4e2d\u7684 CNN layers**\n    * \u5377\u7a4d\u5c64 Convolution layer\n        * \u5377\u7a4d\u795e\u7d93\u7db2\uf937\u5c31\u662f\u900f\u904e\u758a\u8d77\u4e00\u5c64\u53c8\u4e00\u5c64\u7684\u5377\u7a4d\u5c64\u3001\u6c60\u5316\u5c64\u7522\u751f\u7684\u3002\n        * \u5f71\u50cf\u7d93\u904e\u5377\u7a4d\u5f8c\u7a31\u4f5c\u7279\u5fb5\u5716 (feature map)\uff0c\u7d93\u904e\u591a\u6b21\u5377\u7a4d\u5c64\u5f8c\uff0c\u7279\u5fb5\u5716\u7684\u5c3a\u5bf8 (width, height) \u6703\u8d8a\uf92d\u8d8a\u5c0f\uff0c\u4f46\u662f\u901a\u9053\u6578 (Channel) \u5247\u6703\u8d8a\uf92d\u8d8a\u5927\n    * Keras \u4e2d\u7684 CNN layers - Conv2D\n        ```py\n        from keras.layers import Conv2D\n        feature_maps = Conv2D(filters=128, kernel_size=(3,3), input_shape=input_image.shape)(input_image)\n        ```\n        * \u4e0a\u65b9\u7684\u7a0b\u5f0f\u78bc\u5148 import Keras \u4e2d\u7684 Conv2D\uff0c\u63a5\u4e0b\uf92d\u5c0d input_image \u9032\ufa082D \u5377\u7a4d\uff0c\u5373\u53ef\u5f97\u5230\u6211\u5011\u7684\u7279\u5fb5\u5716 feature maps\n        * Conv2D \u4e2d\u7684\uf96b\u6578\u610f\u7fa9\n            * filters: \u6ffe\u6ce2\u5668\u7684\u6578\uf97e\u3002\u6b64\u6578\u5b57\u6703\u7b49\u65bc\u505a\u5b8c\u5377\u7a4d\u5f8c\u7279\u5fb5\u5716\u7684\u901a\u9053\u6578\uff0c\u901a\u5e38\u8a2d\u5b9a\u70ba 2 \u7684 n \u6b21\u2f45\u65b9\n            * kernel_size: \u6ffe\u6ce2\u5668\u7684\u5927\u5c0f\u3002\u901a\u5e38\u90fd\u662f\u4f7f\u2f64\u7528 3x3 \u6216\u662f 5x5\n            * input_shape: \u53ea\u6709\u5c0d\u5f71\u50cf\u505a\u7b2c\u4e00\u6b21\u5377\u7a4d\u6642\u8981\u6307\u5b9a\uff0c\u4e4b\u5f8c Keras \u6703\u81ea\u52d5\u8a08\u7b97input_shape\n            * strides: \u505a\u5377\u7a4d\u6642\uff0c\u6ffe\u6ce2\u5668\u79fb\u52d5\u7684\u6b65\u9577\u3002[\u6b64\u8655](https://cdn-images-1.medium.com/max/1600/1*ZCjPUFrB6eHPRi4eyP6aaA.gif)\u7684 stirides \u5c31\u662f 1 (\u4e00\u6b21\u79fb\u52d5\u4e00\u683c)\n            * padding: \u662f\u5426\u8981\u5c0d\u8f38\u5165\u5f71\u50cf\u7684\u908a\u7de3\u88dc\u503c\u3002[\u6b64\u8655](https://cdn-images-1.medium.com/max/666/1*noYcUAa_P8nRilg3Lt_nuA.png)\u7684 padding=same \u5247\u662f\u908a\u7de3\u88dc\u4e00\u5c64 0\uff0c\u7a31\u70ba same \u7684\u539f\u56e0\u662f\u56e0\u70ba\u505a\u5b8c padding \u518d\u5377\u7a4d\u5f8c\uff0c\u8f38\u51fa\u7684\u7279\u5fb5\u5716\u5c3a\u5bf8\u8207\u8f38\u5165\u5f71\u50cf\u7684\u5c3a\u5bf8\u4e0d\u6703\u6539\u8b8a\n    * Keras \u4e2d\u7684 CNN layers - SeparableConv2D\n        * \u5168\u540d\u7a31\u505a Depthwise Separable Convolution\uff0c\u8207\u5e38\u7528\u7684 Conv2D \u6548\u679c\u985e\u4f3c\uff0c\u4f46\u662f\uf96b\u6578\uf97e\u53ef\u4ee5\u5927\u5e45\u6e1b\u5c11\uff0c\u6e1b\u8f15\u5c0d\u786c\u9ad4\u7684\u9700\u6c42\n        * \u5c0d\u5f71\u50cf\u505a\uf978\u6b21\u5377\u7a4d\n            * \u7b2c\u4e00\u6b21\u7a31\u70ba DetphWise Conv\uff0c\u5c0d\u5f71\u50cf\u7684\u4e09\u500b\u901a\u9053\u7368\u7acb\u505a\u5377\u7a4d\uff0c\u5f97\u5230\u4e09\u5f35\u7279\u5fb5\u5716\uff1b\n            * \u7b2c\u4e8c\u6b21\u7a31\u70ba PointWise Conv\uff0c\u4f7f\u7528 1x1 \u7684 filter \u5c3a\u5bf8\u505a\u5377\u7a4d\u3002\n            * \uf978\u6b21\u5377\u7a4d\u7d50\u5408\u8d77\uf92d\u53ef\u4ee5\u8ddf\u5e38\u7528\u7684\u5377\u7a4d\u9054\u5230\u63a5\u8fd1\u7684\u6548\u679c\uff0c\u4f46\uf96b\u6578\uf97e\u537b\u9060\u5c11\u65bc\u5e38\ufa0a\u7684\u5377\u7a4d\uf901\u591a\u8cc7\u8a0a\u53ef\uf96b\u8003[\u9023\u7d50](http://www.icode9.com/content-4-93052.html)\n        * SeparableConv2D \u4e2d\u7684\uf96b\u6578\u610f\u7fa9\n            * filters, kernel_size, strides, padding \u90fd\u8207 Conv2D \u76f8\u540c\n            * depth_multiplier : \u5728\u505a DepthWise Conv \u6642\uff0c\u8f38\u51fa\u7684\u7279\u5fb5\u5716 Channel \u6578\uf97e\u6703\u662f filters * depth_multiplier\uff0c\u9810\u8a2d\u70ba 1\n        ```py\n        from keras.layers import Conv2D, SeparableConv2D, Input\n        from keras.models import Model, Sequential\n\n        input_image = Input((224, 224, 3))\n        feature_maps = Conv2D(filters=32, kernel_size=(3,3))(input_image)\n        feature_maps2 = Conv2D(filters=64, kernel_size=(3,3))(feature_maps)\n        model = Model(inputs=input_image, outputs=feature_maps2)\n        model.summary()\n        '''\n        _________________________________________________________________\n        Layer (type)                 Output Shape              Param #   \n        =================================================================\n        input_5 (InputLayer)         (None, 224, 224, 3)       0         \n        _________________________________________________________________\n        conv2d_5 (Conv2D)            (None, 222, 222, 32)      896       \n        _________________________________________________________________\n        conv2d_6 (Conv2D)            (None, 220, 220, 64)      18496     \n        =================================================================\n        Total params: 19,392\n        Trainable params: 19,392\n        Non-trainable params: 0\n        _________________________________________________________________\n        '''\n        input_image = Input((224, 224, 3))\n        feature_maps = SeparableConv2D(filters=32, kernel_size=(3,3))(input_image)\n        feature_maps2 = SeparableConv2D(filters=64, kernel_size=(3,3))(feature_maps)\n        model = Model(inputs=input_image, outputs=feature_maps2)\n        model.summary()\n        '''\n        _________________________________________________________________\n        Layer (type)                 Output Shape              Param #   \n        =================================================================\n        input_6 (InputLayer)         (None, 224, 224, 3)       0         \n        _________________________________________________________________\n        separable_conv2d_1 (Separabl (None, 222, 222, 32)      155       \n        _________________________________________________________________\n        separable_conv2d_2 (Separabl (None, 220, 220, 64)      2400      \n        =================================================================\n        Total params: 2,555\n        Trainable params: 2,555\n        Non-trainable params: 0\n        _________________________________________________________________\n        \n        # \u53ef\u4ee5\u770b\u5230\u4f7f\u7528 Seperable Conv2D\uff0c\u5373\u4f7f\u6a21\u578b\u8a2d\u7f6e\u90fd\u4e00\u6a21\u4e00\u6a23\uff0c\u4f46\u662f\u53c3\u6578\u91cf\u660e\u986f\u6e1b\u5c11\u975e\u5e38\u591a\uff01\n        '''\n        ```\n* **Day_97 : \u4f7f\u7528 CNN \u5b8c\u6210 CIFAR-10 \u8cc7\u6599\u96c6 </>**\n    * Cifar-10\n        * \u5982\u540c\u5148\u524d\u8ab2\u7a0b\u4e2d\u7684 Scikit-learn.datasets\uff0c\u6df1\u5ea6\u5b78\u7fd2\u7684\u5f71\u50cf\u8cc7\u6599\u96c6\u4ee5 MNIST (\u624b\u5beb\u6578\u5b57\u8fa8\u8b58) \u8207 Cifar-10 (\u81ea\u7136\u5f71\u50cf\u5206\u985e) \u4f5c\u70ba\u5e38\ufa0a\n        * Cifar-10 \u662f 10 \u500b\u985e\u5225\uff0c\u5f71\u50cf\u5927\u5c0f\u70ba 32x32 \u7684\u4e00\u500b\u8f15\uf97e\u8cc7\u6599\u96c6\uff0c\u975e\u5e38\u9069\u5408\u62ff\uf92d\u505a\u6df1\u5ea6\u5b78\u7fd2\u7684\u7df4\u7fd2\n    * CNN \u76f8\u6bd4 DNN\uff0c\uf901\u9069\u5408\u7528\u4f86\uf92d\u8655\uf9e4\u5f71\u50cf\u7684\u8cc7\u6599\u96c6\n        ```py\n        import keras\n        from keras.datasets import cifar10\n        from keras.models import Sequential\n        from keras.layers import Dense, Dropout, Activation, Flatten\n        from keras.layers import Conv2D, MaxPooling2D\n        from keras.optimizers import RMSprop, Adam\n\n        batch_size = 128 # batch \u7684\u5927\u5c0f\uff0c\u5982\u679c\u51fa\u73fe OOM error\uff0c\u8acb\u964d\u4f4e\u9019\u500b\u503c\n        num_classes = 10 # \u985e\u5225\u7684\u6578\u91cf\uff0cCifar 10 \u5171\u6709 10 \u500b\u985e\u5225\n        epochs = 10 # \u8a13\u7df4\u7684 epochs \u6578\u91cf\n\n        # \u8b80\u53d6\u8cc7\u6599\u4e26\u6aa2\u8996\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n        print('x_train shape:', x_train.shape)\n        print(x_train.shape[0], 'train samples')\n        print(x_test.shape[0], 'test samples')\n\n        # \u5c0d label \u9032\u884c one-hot encoding (y_trian \u539f\u672c\u662f\u7d14\u6578\u5b57)\n        y_train = keras.utils.to_categorical(y_train, num_classes)\n        y_test = keras.utils.to_categorical(y_test, num_classes)\n\n        # \u9996\u5148\u6211\u5011\u4f7f\u7528\u4e00\u822c\u7684 DNN (MLP) \u4f86\u8a13\u7df4\n        # \u7531\u65bc DNN \u53ea\u80fd\u8f38\u5165\u4e00\u7dad\u7684\u8cc7\u6599\uff0c\u6211\u5011\u8981\u5148\u5c07\u5f71\u50cf\u9032\u884c\u6524\u5e73\uff0c\u82e5 (50000, 32, 32, 3) \u7684\u5f71\u50cf\uff0c\u6524\u5e73\u5f8c\u6703\u8b8a\u6210 (50000, 32x32x3) = (50000, 3072)\n\n        # \u5c07\u8cc7\u6599\u6524\u5e73\u6210\u4e00\u7dad\u8cc7\u6599\n        x_train = x_train.reshape(50000, 3072) \n        x_test = x_test.reshape(10000, 3072)\n\n        # \u5c07\u8cc7\u6599\u8b8a\u70ba float32 \u4e26\u6a19\u6e96\u5316\n        x_train = x_train.astype('float32')\n        x_test = x_test.astype('float32')\n        x_train /= 255\n        x_test /= 255\n        print(x_train.shape[0], 'train samples')\n        print(x_test.shape[0], 'test samples')\n\n        model = Sequential()\n        model.add(Dense(512, activation='relu', input_shape=(3072,)))\n        model.add(Dropout(0.2))\n        model.add(Dense(512, activation='relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(num_classes, activation='softmax'))\n\n        model.summary()\n\n        model.compile(loss='categorical_crossentropy',\n                    optimizer=RMSprop(),\n                    metrics=['accuracy'])\n\n        history = model.fit(x_train, y_train,\n                            batch_size=batch_size,\n                            epochs=epochs,\n                            verbose=1,\n                            validation_data=(x_test, y_test))\n        score = model.evaluate(x_test, y_test, verbose=0)\n        print('Test loss:', score[0])\n        print('Test accuracy:', score[1])\n\n        # \u63a5\u4e0b\u4f86\u6211\u5011\u4f7f\u7528 CNN \u4f86\u8a13\u7df4\u795e\u7d93\u7db2\u8def\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n        print('x_train shape:', x_train.shape)\n        print(x_train.shape[0], 'train samples')\n        print(x_test.shape[0], 'test samples')\n        x_train = x_train.astype('float32')\n        x_test = x_test.astype('float32')\n        x_train /= 255\n        x_test /= 255\n\n        # Convert class vectors to binary class matrices.\n        y_train = keras.utils.to_categorical(y_train, num_classes)\n        y_test = keras.utils.to_categorical(y_test, num_classes)\n\n        model = Sequential()\n        model.add(Conv2D(32, (3, 3), padding='same',\n                        input_shape=x_train.shape[1:]))\n        model.add(Activation('relu'))\n        model.add(Conv2D(32, (3, 3)))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        model.add(Conv2D(64, (3, 3), padding='same'))\n        model.add(Activation('relu'))\n        model.add(Conv2D(64, (3, 3)))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        model.add(Flatten())\n        model.add(Dense(512))\n        model.add(Activation('relu'))\n        model.add(Dropout(0.5))\n        model.add(Dense(num_classes))\n        model.add(Activation('softmax'))\n        model.summary()\n\n        model.compile(loss='categorical_crossentropy',\n                    optimizer=RMSprop(),\n                    metrics=['accuracy'])\n\n        history = model.fit(x_train, y_train,\n                            batch_size=batch_size,\n                            epochs=epochs,\n                            verbose=1,\n                            validation_data=(x_test, y_test))\n        score = model.evaluate(x_test, y_test, verbose=0)\n        print('Test loss:', score[0])\n        print('Test accuracy:', score[1])\n        ```\n* **Day_98 : \u8a13\u7df4\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - \u8655\u7406\u5927\u91cf\u6578\u64da**\n    * \u5927\u6578\u64da\uff1f\n        * Cifar-10 \u8cc7\u6599\u96c6\u76f8\u5c0d\u65bc\u5e38\u7528\u5230\u7684\u5f71\u50cf\uf92d\u8aaa\u662f\u975e\u5e38\u5c0f\uff0c\u6240\u4ee5\u53ef\u4ee5\u5148\u628a\u8cc7\u6599\u96c6\u5168\u90e8\u8b80\u9032\u8a18\u61b6\u9ad4\u88e1\u9762\uff0c\u8981\u4f7f\u7528\u6642\u76f4\u63a5\u5f9e\u8a18\u61b6\u9ad4\u4e2d\u5b58\u53d6\uff0c\u901f\u5ea6\u6703\u76f8\u7576\u5feb\n        * \u4f46\u662f\u5982\u679c\u6211\u5011\u8981\u8655\uf9e4\u7684\u8cc7\u6599\u96c6\u8d85\u904e\u96fb\u8166\u8a18\u61b6\u9ad4\u7684\u5bb9\uf97e\u5462\uff1f\u684c\u4e0a\u96fb\u8166\u7684\u8a18\u61b6\u9ad4\u591a\u70ba 32, 64, 128 GB\uff0c\u7576\u8655\uf9e4\u8d85\u5927\u5716\u7247\u30013D \u5f71\u50cf\u6216\u5f71\u7247\u6642\uff0c\u5c31\u53ef\u80fd\u9047\u5230 Out of Memory error\n    * \u6279\u6b21 (batch) \u8b80\u53d6\n        * \u5982\u540c\u8a13\u7df4\u795e\u7d93\u7db2\uf937\u6642\uff0cBatch (\u6279\u6b21) \u7684\u6982\uf9a3\u4e00\u6a23\u3002\u6211\u5011\u53ef\u4ee5\u5c07\u8cc7\u6599\u4e00\u6279\u4e00\u6279\u7684\u8b80\u9032\u8a18\u61b6\u9ad4\uff0c\u7576\u5f9e GPU/CPU \u8a13\u7df4\u5b8c\u5f8c\uff0c\u5c07\u9019\u6279\u8cc7\u6599\u5f9e\u8a18\u61b6\u9ad4\u91cb\u51fa\uff0c\u5728\u8b80\u53d6\u4e0b\u4e00\u6279\u8cc7\u6599\n    * \u5982\u4f55\u7528 Python \u64b0\u5beb\u6279\u6b21\u8b80\u53d6\u8cc7\u6599\u7684\u7a0b\u5f0f\u78bc\n        * \u4f7f\u7528 Python \u7684 generator \uf92d\u5e6b\u4f60\u5b8c\u6210\u9019\u500b\u4efb\u52d9\uff01\n        * Generator \u53ef\u4ee5\u4f7f\u7528 next(your_generator) \uf92d\u57f7\ufa08\u4e0b\u4e00\u6b21\u5faa\u74b0\n        * \u5047\u8a2d\u6709\u4e00\u500b list\uff0c\u5176\u4e2d\u6709 5 \u500b\u6578\u5b57\uff0c\u6211\u5011\u53ef\u4ee5\u64b0\u5beb\u4e00\u500b generator\uff0c\u7528 next(generator) \u6703\u81ea\u52d5\u5410\u51fa list \u7684\u7b2c\u4e00\u500b\u6578\u5b57\uff0c\u518d\u7528\u7b2c\u4e8c\u6b21 next \u5247\u6703\u5410\u51fa\u7b2c\u4e8c\u500b\u6578\u5b57\uff0c\u4ee5\u6b64\u985e\u63a8\n        * \u5c07\u539f\u672c Python function \u4e2d\u7684 return \u6539\u70ba yield\uff0c\u9019\u6a23 Python \u5c31\u77e5\u9053\u9019\u662f\u4e00\u500b Generator \u56c9\n        ```py\n        from keras.datasets import cifar10\n\n        (x_train, x_test), (y_train, y_test) = cifar10.load_data()\n\n        def cifar_generator(image_array, batch_size=32):\n            while True:\n                for indexs in range(0, len(image_array), batch_size):\n                    images = image_array[indexs: indexs+batch_size]\n                    yield images, labels\n\n        cifar_gen = cifar_generator(x_train)\n        images, labels = next(cifar_gen)\n        ```\n        ```py\n        import keras\n        from keras.datasets import cifar10\n        from keras.models import Sequential\n        from keras.layers import Dense, Dropout, Activation, Flatten\n        from keras.layers import Conv2D, MaxPooling2D\n        from keras.optimizers import RMSprop, Adam\n\n        batch_size = 128 # batch \u7684\u5927\u5c0f\uff0c\u5982\u679c\u51fa\u73fe OOM error\uff0c\u8acb\u964d\u4f4e\u9019\u500b\u503c\n        num_classes = 10 # \u985e\u5225\u7684\u6578\u91cf\uff0cCifar 10 \u5171\u6709 10 \u500b\u985e\u5225\n        epochs = 10 # \u8a13\u7df4\u7684 epochs \u6578\u91cf\n\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n        print('x_train shape:', x_train.shape)\n        print(x_train.shape[0], 'train samples')\n        print(x_test.shape[0], 'test samples')\n        x_train = x_train.astype('float32')\n        x_test = x_test.astype('float32')\n        x_train /= 255\n        x_test /= 255\n\n        # Convert class vectors to binary class matrices.\n        y_train = keras.utils.to_categorical(y_train, num_classes)\n        y_test = keras.utils.to_categorical(y_test, num_classes)\n\n        model = Sequential()\n        model.add(Conv2D(32, (3, 3), padding='same',\n                        input_shape=x_train.shape[1:]))\n        model.add(Activation('relu'))\n        model.add(Conv2D(32, (3, 3)))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        model.add(Conv2D(64, (3, 3), padding='same'))\n        model.add(Activation('relu'))\n        model.add(Conv2D(64, (3, 3)))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        model.add(Flatten())\n        model.add(Dense(512))\n        model.add(Activation('relu'))\n        model.add(Dropout(0.5))\n        model.add(Dense(num_classes))\n        model.add(Activation('softmax'))\n        model.summary()\n\n        model.compile(loss='categorical_crossentropy',\n                    optimizer=RMSprop(),\n                    metrics=['accuracy'])\n\n        from sklearn.utils import shuffle\n        def my_generator(x, y, batch_size):\n            while True:\n                for idx in range(0, len(x), batch_size): # \u8b93 idx \u5f9e 0 \u958b\u59cb\uff0c\u4e00\u6b21\u589e\u52a0 batch size\u3002\u5047\u8a2d batch_size=32, idx = 0, 32, 64, 96, ....\n                    batch_x, batch_y = x[idx:idx+batch_size], y[idx:idx+batch_size]\n                    yield batch_x, batch_y\n                x, y = shuffle(x, y) # loop \u7d50\u675f\u5f8c\uff0c\u5c07\u8cc7\u6599\u9806\u5e8f\u6253\u4e82\u518d\u91cd\u65b0\u5faa\u74b0\n        \n        train_generator = my_generator(x_train, y_train, batch_size) # \u5efa\u7acb\u597d\u6211\u5011\u5beb\u597d\u7684 generator\n    \n        history = model.fit_generator(train_generator,\n                            steps_per_epoch=int(len(x_train)/batch_size), # \u4e00\u500b epochs \u8981\u57f7\u884c\u5e7e\u6b21 update\uff0c\u901a\u5e38\u662f\u8cc7\u6599\u91cf\u9664\u4ee5 batch size\n                            epochs=epochs,\n                            verbose=1,\n                            validation_data=(x_test, y_test))\n        score = model.evaluate(x_test, y_test, verbose=0)\n        print('Test loss:', score[0])\n        print('Test accuracy:', score[1])\n        ```\n* **Day_99 : \u8a13\u7df4\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - \u8655\u7406\u5c0f\u91cf\u6578\u64da**    \n    * \u2f29\u6578\u64da\uff1f\n        * \u5be6\u52d9\u4e0a\u9032\u2f8f\u5404\u7a2e\u6a5f\u5668\u5b78\u7fd2\u5c08\u6848\u6642\uff0c\u6211\u5011\u7d93\u5e38\u6703\u9047\u5230\u8cc7\u6599\u91cf\u4e0d\u2f9c\u7684\u60c5\u5f62\uff0c\u5e38\u2f92\u539f\u56e0\uff1a\n            * \u8cc7\u6599\u641c\u96c6\u56f0\u96e3\u6216\u662f\u6210\u672c\u6975\u2fbc\n            * \u8cc7\u6599\u6a19\u8a3b\u4e0d\u6613\n            * \u8cc7\u6599\u54c1\u8cea\u4e0d\u4f73\n        * \u9664\u4e86\u7e7c\u7e8c\u641c\u96c6\u8cc7\u6599\u4ee5\u5916\uff0c\u8cc7\u6599\u589e\u5f37 (Data augmentation) \u662f\u5f88\u5e38\u2f92\u7684\u2f45\u6cd5\u4e4b\u2f00\n    * \u8cc7\u6599\u589e\u5f37 (Data augmentation) \n        * \u5176\u5be6\u5c31\u662f\u5c0d\u5f71\u50cf\u9032\u2f8f\u2f00\u4e9b\u96a8\u6a5f\u7684\u8655\u7406\u5982\u7ffb\u8f49\u3001\u5e73\u79fb\u3001\u65cb\u8f49\u3001\u6539\u8b8a\u4eae\u5ea6\u7b49\u5404\u6a23\u7684\u5f71\u50cf\u64cd\u4f5c\uff0c\u85c9\u6b64\u5c07\u2f00\u5f35\u5f71\u50cf\u589e\u52a0\u5230\u591a\u5f35   \n    * \u8cc7\u6599\u589e\u5f37\u4e26\u975e\u842c\u9748\u4e39\uff01\n        * \u9069\u5ea6\u7684\u8cc7\u6599\u589e\u5f37\u901a\u5e38\u90fd\u53ef\u4ee5\u63d0\u5347\u6e96\u78ba\u7387\u3002\u9078\u2f64\u7684\u589e\u5f37\u2f45\u6cd5\u5247\u9808\u8996\u8cc7\u6599\u96c6\u2f7d\u5b9a\n            * \u4f8b\u5982\u2f08\u81c9\u8fa8\u8b58\u5c31\u4e0d\u592a\u9069\u5408\u2f64\u4e0a\u4e0b\u7ffb\u8f49\uff0c\u56e0\u70ba\u5be6\u969b\u4f7f\u2f64\u6642\u4e0d\u6703\u6709\u4e0a\u4e0b\u985b\u5012\u7684\u81c9\u90e8\n            * \u53e6\u5916\u9700\u7279\u5225\u6ce8\u610f\u8981\u5148\u5c0d\u8cc7\u6599\u505a train/test split \u5f8c\u518d\u505a\u8cc7\u6599\u589e\u5f37\uff01\u5426\u5247\u5176\u5be6\u90fd\u662f\u540c\u6a23\u7684\u5f71\u50cf\uff0c\u8aa4\u4ee5\u70ba\u6a21\u578b\u8a13\u7df4\u5f97\u975e\u5e38\u597d\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [Keras ImageDataGenerator \u7bc4\u4f8b\u8207\u4ecb\u7d39](https://zhuanlan.zhihu.com/p/30197320)\n        * [imgaug](https://github.com/aleju/imgaug)\n    * \u5e38\u898b\u554f\u984c :\n        * Q: \u8dd1\u8cc7\u6599\u589e\u5f37\u6642\u7a0b\u5f0f\u78bc\u597d\u50cf\u90fd\u6703\u51fa\u932f\uff1f\n        * A: \u8981\u7279\u5225\u6ce8\u610f\uff0c\u8cc7\u6599\u589e\u5f37\u61c9\u8a72\u8981\u5728\u5716\u50cf\u6a19\u6e96\u5316\u4e4b\n        \u524d\u5b8c\u6210 (e.g. \u9664\u4ee5 255\u3001\u6e1b\u53bb\u5e73\u5747\u503c)\uff01\u56e0\u70ba\u591a\u6578\u8cc7\n        \u6599\u589e\u5f37\u7684\u51fd\u6578\u591a\u662f\u4ee5\u5716\u50cf\u70ba int32 \u7684 RGB \u5f71\u50cf\u4f86\n        \u8a2d\u8a08\u7684\uff0c\u82e5\u5df2\u7d93\u5148\u7d93\u904e\u6a19\u6e96\u5316\uff0c\u6709\u53ef\u80fd\u9020\u6210\u7a0b\u5f0f\n        \u78bc\u932f\u8aa4\n        ```py\n        import keras\n        from keras.datasets import cifar10\n        from keras.preprocessing.image import ImageDataGenerator\n        from keras.models import Sequential\n        from keras.layers import Dense, Dropout, Activation, Flatten\n        from keras.layers import Conv2D, MaxPooling2D\n        from keras.optimizers import RMSprop, Adam\n\n        batch_size = 128 # batch \u7684\u5927\u5c0f\uff0c\u5982\u679c\u51fa\u73fe OOM error\uff0c\u8acb\u964d\u4f4e\u9019\u500b\u503c\n        num_classes = 10 # \u985e\u5225\u7684\u6578\u91cf\uff0cCifar 10 \u5171\u6709 10 \u500b\u985e\u5225\n        epochs = 10 # \u8a13\u7df4\u7684 epochs \u6578\u91cf\n\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n        print('x_train shape:', x_train.shape)\n        print(x_train.shape[0], 'train samples')\n        print(x_test.shape[0], 'test samples')\n        x_train = x_train.astype('float32')\n        x_test = x_test.astype('float32')\n        x_train /= 255\n        x_test /= 255\n\n        # Convert class vectors to binary class matrices.\n        y_train = keras.utils.to_categorical(y_train, num_classes)\n        y_test = keras.utils.to_categorical(y_test, num_classes)\n\n        model = Sequential()\n        model.add(Conv2D(32, (3, 3), padding='same',\n                        input_shape=x_train.shape[1:]))\n        model.add(Activation('relu'))\n        model.add(Conv2D(32, (3, 3)))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        model.add(Conv2D(64, (3, 3), padding='same'))\n        model.add(Activation('relu'))\n        model.add(Conv2D(64, (3, 3)))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        model.add(Flatten())\n        model.add(Dense(512))\n        model.add(Activation('relu'))\n        model.add(Dropout(0.5))\n        model.add(Dense(num_classes))\n        model.add(Activation('softmax'))\n        model.summary()\n\n        model.compile(loss='categorical_crossentropy',\n                    optimizer=RMSprop(),\n                    metrics=['accuracy'])\n\n        augment_generator = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n\n        history = model.fit_generator(augment_generator.flow(x_train, y_train, batch_size=batch_size),\n                            steps_per_epoch=int(len(x_train)/batch_size), # \u4e00\u500b epochs \u8981\u57f7\u884c\u5e7e\u6b21 update\uff0c\u901a\u5e38\u662f\u8cc7\u6599\u91cf\u9664\u4ee5 batch size\n                            epochs=epochs,\n                            verbose=1,\n                            validation_data=(x_test, y_test))\n        score = model.evaluate(x_test, y_test, verbose=0)\n        print('Test loss:', score[0])\n        print('Test accuracy:', score[1])\n        ```\n* **Day_100 : \u8a13\u7df4\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u7684\u7d30\u7bc0\u8207\u6280\u5de7 - \u8f49\u79fb\u5b78\u7fd2 (Transfer learning)** \n    * \u9077\u79fb\u5b78\u7fd2\uff0cTransfer Learning\n        * \u8cc7\u6599\u91cf\u4e0d\u8db3\u6642\uff0c\u9077\u79fb\u5b78\u7fd2\u4e5f\u662f\u5f88\u5e38\u898b\u7684\u65b9\u6cd5\n        * \u795e\u7d93\u7db2\u8def\u8a13\u7df4\u524d\u7684\u521d\u59cb\u53c3\u6578\u662f\u96a8\u6a5f\u7522\u751f\u7684\uff0c\u4e0d\u5177\u5099\u4efb\u4f55\u610f\u7fa9\n        * \u900f\u904e\u5176\u4ed6\u9f90\u5927\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u597d\u7684\u6a21\u578b\u53c3\u6578\uff0c\u6211\u5011\u4f7f\u7528\u9019\u500b\u53c3\u6578\u7576\u6210\u8d77\u59cb\u9ede\uff0c\u6539\u7528\u5728\u81ea\u5df1\u7684\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\uff01\n    * \u70ba\u4f55\u53ef\u4ee5\u7528\u9077\u79fb\u5b78\u7fd2\uff1f\n        * \u524d\u9762 CNN \u7684\u8ab2\u7a0b\u6709\u63d0\u5230\uff0cCNN \u6dfa\u5c64\u7684\u904e\u6ffe\u5668 (filter) \u662f\u7528\u4f86\u5075\u6e2c\u7dda\u689d\u8207\u984f\u8272\u7b49\u7c21\u55ae\u7684\u5143\u7d20\u3002\u56e0\u6b64\u4e0d\u7ba1\u5716\u50cf\u662f\u4ec0\u9ebc\u985e\u578b\uff0c\u57fa\u672c\u7684\u7d44\u6210\u61c9\u8a72\u8981\u662f\u4e00\u6a23\u7684\n        * \u5927\u578b\u8cc7\u6599\u96c6 (\u5982 ImageNet) \u8a13\u7df4\u597d\u7684\u53c3\u6578\u5177\u6709\u5b8c\u6574\u7684\u984f\u8272\u3001\u7dda\u689d filters\uff0c\u5f9e\u6b64\u53c3\u6578\u958b\u59cb\u8a13\u7df4\u6211\u5011\u65e2\u81ea\u5df1\u7684\u8cc7\u6599\u96c6\uff0c\u9010\u6b65\u628a filters \u4fee\u6b63\u70ba\u9069\u5408\u81ea\u5df1\u8cc7\u6599\u96c6\u7684\u7d50\u679c\u3002\n    * \u53c3\u8003\u5927\u795e\u5011\u7684\u7db2\u8def\u67b6\u69cb\n        * \u8a31\u591a\u5b78\u8005\u5011\u7814\u7a76\u4e86\u8a31\u591a\u67b6\u69cb\u8207\u591a\u6b21\u8abf\u6574\u8d85\u53c3\u6578\uff0c\u4e26\u5728\u5927\u578b\u8cc7\u6599\u96c6\u5982 ImageNet \u4e0a\u9032\u884c\u6e2c\u8a66\u5f97\u5230\u6e96\u78ba\u6027\u9ad8\u4e26\u5bb9\u6613\u6cdb\u5316\u7684\u7db2\u8def\u67b6\u69cb\uff0c\u6211\u5011\u53ef\u4ee5\u5f9e\u9019\u6a23\u7684\u67b6\u69cb\u958b\u59cb\uff01\n    * Transfer learning in Keras: ResNet-50\n        ```py\n        from keras.applications.resnet50 import ResNet50\n\n        resnet_model = ResNet50(input_shape=(224,224,3), weights='imagenet', pooling='avg'), include_top=False)\n\n        last_featuremaps = resnet_model.output\n        flatten_featuremap = Flatten()(last_featuremaps)\n        output = Dense(num_classes)(flatten_featuremap)\n\n        New_resnet_model = Model(inputs=resnet_model.input, outputs=output)\n        ```\n        * \u6211\u5011\u4f7f\u7528\u4e86 ResNet50 \u7db2\u8def\u7d50\u69cb\uff0c\u5176\u4e2d\u53ef\u4ee5\u770b\u5230 weight='imagenet'\uff0c\u4ee3\u8868\u6211\u5011\u4f7f\u7528\u5f9e imagenet \u8a13\u7df4\u597d\u7684\u53c3\u6578\u4f86\u521d\u59cb\u5316\uff0c\u4e26\u6307\u5b9a\u8f38\u5165\u7684\u5f71\u50cf\u5927\u5c0f\u70ba (224,224,3)\n        * pooling='avg' \u4ee3\u8868\u6700\u5f8c\u4e00\u5c64\u4f7f\u7528 [Global Average pooling](https://blog.csdn.net/Losteng/article/details/51520555)\uff0c\u628a feature maps \u8b8a\u6210\u4e00\u7dad\u7684\u5411\u91cf\n        * include_top=False \u4ee3\u8868\u5c07\u539f\u672c Dense layer \u62d4\u6389\uff0c\u56e0\u70ba\u539f\u672c\u9019\u500b\u7db2\u8def\u662f\u7528\u4f86\u505a 1000 \u500b\u5206\u985e\u6a21\u578b\uff0c\u6211\u5011\u5fc5\u9808\u66ff\u63db\u6210\u81ea\u5df1\u7684 Dense layer \u4f86\u7b26\u5408\u6211\u5011\u81ea\u5df1\u8cc7\u6599\u96c6\u7684\u985e\u5225\u6578\u91cf\n        * \u6211\u5011\u5c07\u6a21\u578b\u8a2d\u5b9a\u6210\u6c92\u6709 Dense layers\uff0c\u4e14\u6700\u5f8c\u4e00\u5c64\u505a GAP\uff0c\u4f7f\u7528 resnet_model.output \u6211\u5011\u5c31\u53ef\u4ee5\u53d6\u51fa\u6700\u5f8c\u4e00\u5c64\u7684 featuremaps\n        * \u5c07\u5176\u4f7f\u7528 Flatten \u6524\u5e73\u5f8c\uff0c\u5728\u63a5\u4e0a\u6211\u5011\u7684 Dense layer\uff0c\u795e\u7d93\u5143\u6578\u91cf\u8207\u8cc7\u6599\u96c6\u7684\u985e\u5225\u6578\u91cf\u4e00\u81f4\uff0c\u91cd\u5efa\u7acb\u6a21\u578b\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u500b\u65b0\u7684 ResNet-50 \u6a21\u578b\uff0c\u4e14\u53c3\u6578\u662f\u6839\u64da ImageNet \u5927\u578b\u8cc7\u6599\u96c6\u80b2\u8a13\u7df4\u597d\u7684\n        * [\u6574\u9ad4\u6d41\u7a0b\u53c3\u8003](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb)\uff0c\u6211\u5011\u4fdd\u7559 Trained convolutional base\uff0c\u4e26\u5efa\u7acb New classifier (Dense \u90e8\u5206)\uff0c\u6700\u5f8c convolutional base \u662f\u5426\u8981 frozen (\u4e0d\u8a13\u7df4)\uff0c\u5247\u662f\u8981\u770b\u8cc7\u6599\u96c6\u8207\u80b2\u8a13\u7df4\u7684 ImageNet \u662f\u5426\u76f8\u4f3c\uff0c\u5982\u679c\u5dee\u7570\u5f88\u5927\u5247\u5efa\u8b70\u8a13\u7df4\u6642\u4e0d\u8981 frozen\uff0c\u8b93 CNN \u7684\u53c3\u6578\u53ef\u4ee5\u7e7c\u7e8c\u66f4\u65b0\n    * \u91cd\u8981\u77e5\u8b58\u8907\u7fd2\n        * \u9077\u79fb\u5b78\u7fd2\u662f\u900f\u904e\u9810\u5148\u518d\u5927\u578b\u8cc7\u6599\u96c6\u8a13\u7df4\u597d\u7684\u6b0a\u91cd\uff0c\u518d\u6839\u64da\u81ea\u5df1\u7684\u8cc7\u6599\u53ca\u9032\u884c\u5fae\u8abf (finetune) \u7684\u4e00\u7a2e\u5b78\u7fd2\u65b9\u6cd5\n        * Keras \u4e2d\u7684\u6a21\u578b\uff0c\u53ea\u8981\u6307\u5b9a\u6b0a\u91cd weights='imagenet' \u5373\u53ef\u4f7f\u7528\u9077\u79fb\u5b78\u7fd2\n    * \u5ef6\u4f38\u95b1\u8b80 :\n        * [\u7c21\u55ae\u4f7f\u7528 Keras \u5b8c\u6210 Transfer learning](https://ithelp.ithome.com.tw/articles/10190971)\n        * [Keras \u4f5c\u8005\u6559\u4f60\u7528 pre-trained CNN \u6a21\u578b](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb)\n        * [Keras \u4ee5 ResNet-50 \u9810\u8a13\u7df4\u6a21\u578b\u5efa\u7acb\u72d7\u8207\u8c93\u8fa8\u8b58\u7a0b\u5f0f](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)\n        ```py\n        \"\"\"\n        #Trains a ResNet on the CIFAR10 dataset.\n\n        ResNet \u5171\u6709\u5169\u500b\u7248\u672c\uff0c\u6b64\u8655\u89e3\u7b54\u6211\u5011\u4f7f\u7528 v1 \u4f86\u505a\u8a13\u7df4\u3002\n        ResNet v1:\n        [Deep Residual Learning for Image Recognition\n        ](https://arxiv.org/pdf/1512.03385.pdf)\n        ResNet v2:\n        [Identity Mappings in Deep Residual Networks\n        ](https://arxiv.org/pdf/1603.05027.pdf)\n        \"\"\"\n\n        import keras\n        from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n        from keras.layers import AveragePooling2D, Input, Flatten\n        from keras.optimizers import Adam\n        from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n        from keras.callbacks import ReduceLROnPlateau\n        from keras.preprocessing.image import ImageDataGenerator\n        from keras.regularizers import l2\n        from keras import backend as K\n        from keras.models import Model\n        from keras.datasets import cifar10\n        import numpy as np\n        import os\n\n        # \u8a13\u7df4\u7528\u7684\u8d85\u53c3\u6578\n        batch_size = 128  \n        epochs = 200\n        data_augmentation = True\n        num_classes = 10\n\n        # \u8cc7\u6599\u6a19\u6e96\u5316\u7684\u65b9\u5f0f\uff0c\u6b64\u8655\u4f7f\u7528\u6e1b\u53bb\u6240\u6709\u5f71\u50cf\u7684\u5e73\u5747\u503c\n        subtract_pixel_mean = True\n\n        # Model parameter\n        # ----------------------------------------------------------------------------\n        #           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n        # Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n        #           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n        # ----------------------------------------------------------------------------\n        # ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n        # ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n        # ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n        # ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n        # ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n        # ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n        # ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n        # ---------------------------------------------------------------------------\n        n = 9 # \u4f7f\u7528 ResNet-56 \u7684\u7db2\u8def\u67b6\u69cb\n\n        # \u4f7f\u7528\u7684 ResNet \u6a21\u578b\u7248\u672c\n        # Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n        version = 1\n\n        # \u8a08\u7b97\u4e0d\u540c ResNet \u7248\u672c\u5c0d\u61c9\u7684\u7db2\u8def\u6df1\u5ea6\uff0c\u6b64\u8655\u90fd\u662f\u6839\u64da paper \u7684\u5b9a\u7fa9\u4f86\u8a08\u7b97\n        depth = n * 6 + 2\n\n        # \u6a21\u578b\u7684\u540d\u7a31\n        model_type = 'ResNet%dv%d' % (depth, version)\n\n        # \u8b80\u53d6 Cifar-10 \u8cc7\u6599\u96c6\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n        # \u5f71\u50cf\u8f38\u5165\u7684\u7dad\u5ea6\n        input_shape = x_train.shape[1:]\n\n        # \u5148\u628a\u5f71\u50cf\u7e2e\u653e\u5230 0-1 \u4e4b\u9593\n        x_train = x_train.astype('float32') / 255\n        x_test = x_test.astype('float32') / 255\n\n        # \u518d\u6e1b\u53bb\u6240\u6709\u5f71\u50cf\u7684\u5e73\u5747\u503c\n        if subtract_pixel_mean:\n            x_train_mean = np.mean(x_train, axis=0)\n            x_train -= x_train_mean \n            x_test -= x_train_mean # \u6b64\u8655\u8981\u6ce8\u610f\uff01\u6e2c\u8a66\u8cc7\u6599\u4e5f\u662f\u6e1b\u53bb\u8a13\u7df4\u8cc7\u6599\u7684\u5e73\u5747\u503c\u4f86\u505a\u6a19\u6e96\u5316\uff0c\u4e0d\u53ef\u4ee5\u6e1b\u6e2c\u8a66\u8cc7\u6599\u7684\u5e73\u5747\u503c (\u56e0\u70ba\u7406\u8ad6\u4e0a\u4f60\u662f\u4e0d\u80fd\u77e5\u9053\u6e2c\u8a66\u8cc7\u6599\u7684\u5e73\u5747\u503c\u7684\uff01)\n\n        print('x_train shape:', x_train.shape)\n        print(x_train.shape[0], 'train samples')\n        print(x_test.shape[0], 'test samples')\n        print('y_train shape:', y_train.shape)\n\n        # \u5c0d label \u505a one-hot encoding\n        y_train = keras.utils.to_categorical(y_train, num_classes)\n        y_test = keras.utils.to_categorical(y_test, num_classes)\n\n        # \u5b78\u7fd2\u7387\u52d5\u614b\u8abf\u6574\u3002\u7576\u8dd1\u5230\u7b2c\u5e7e\u500b epcoh \u6642\uff0c\u6839\u64da\u8a2d\u5b9a\u4fee\u6539\u5b78\u7fd2\u7387\u3002\u9019\u908a\u7684\u6578\u503c\u90fd\u662f\u53c3\u8003\u539f paper\n        def lr_schedule(epoch):\n            \"\"\"Learning Rate Schedule\n            Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n            Called automatically every epoch as part of callbacks during training.\n            # Arguments\n                epoch (int): The number of epochs\n            # Returns\n                lr (float32): learning rate\n            \"\"\"\n            lr = 1e-3\n            if epoch > 180:\n                lr *= 0.5e-3\n            elif epoch > 160:\n                lr *= 1e-3\n            elif epoch > 120:\n                lr *= 1e-2\n            elif epoch > 80:\n                lr *= 1e-1\n            print('Learning rate: ', lr)\n            return lr\n\n        # \u4f7f\u7528 resnet_layer \u4f86\u5efa\u7acb\u6211\u5011\u7684 ResNet \u6a21\u578b\n        def resnet_layer(inputs,\n                        num_filters=16,\n                        kernel_size=3,\n                        strides=1,\n                        activation='relu',\n                        batch_normalization=True,\n                        conv_first=True):\n            \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n            # Arguments\n                inputs (tensor): input tensor from input image or previous layer\n                num_filters (int): Conv2D number of filters\n                kernel_size (int): Conv2D square kernel dimensions\n                strides (int): Conv2D square stride dimensions\n                activation (string): activation name\n                batch_normalization (bool): whether to include batch normalization\n                conv_first (bool): conv-bn-activation (True) or\n                    bn-activation-conv (False)\n            # Returns\n                x (tensor): tensor as input to the next layer\n            \"\"\"\n            # \u5efa\u7acb\u5377\u7a4d\u5c64\n            conv = Conv2D(num_filters,\n                        kernel_size=kernel_size,\n                        strides=strides,\n                        padding='same',\n                        kernel_initializer='he_normal',\n                        kernel_regularizer=l2(1e-4))\n\n            # \u5c0d\u8f38\u5165\u9032\u884c\u5377\u6a5f\uff0c\u6839\u64da conv_first \u4f86\u6c7a\u5b9a conv. bn, activation \u7684\u9806\u5e8f\n            x = inputs\n            if conv_first:\n                x = conv(x)\n                if batch_normalization:\n                    x = BatchNormalization()(x)\n                if activation is not None:\n                    x = Activation(activation)(x)\n            else:\n                if batch_normalization:\n                    x = BatchNormalization()(x)\n                if activation is not None:\n                    x = Activation(activation)(x)\n                x = conv(x)\n            return x\n\n        # Resnet v1 \u5171\u6709\u4e09\u500b stage\uff0c\u6bcf\u7d93\u904e\u4e00\u6b21 stage\uff0c\u5f71\u50cf\u5c31\u6703\u8b8a\u5c0f\u4e00\u534a\uff0c\u4f46 channels \u6578\u91cf\u589e\u52a0\u4e00\u500d\u3002ResNet-20 \u4ee3\u8868\u5171\u6709 20 \u5c64 layers\uff0c\u758a\u8d8a\u6df1\u53c3\u6578\u8d8a\u591a\n        def resnet_v1(input_shape, depth, num_classes=10):\n            \"\"\"ResNet Version 1 Model builder [a]\n            Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n            Last ReLU is after the shortcut connection.\n            At the beginning of each stage, the feature map size is halved (downsampled)\n            by a convolutional layer with strides=2, while the number of filters is\n            doubled. Within each stage, the layers have the same number filters and the\n            same number of filters.\n            Features maps sizes:\n            stage 0: 32x32, 16\n            stage 1: 16x16, 32\n            stage 2:  8x8,  64\n            The Number of parameters is approx the same as Table 6 of [a]:\n            ResNet20 0.27M\n            ResNet32 0.46M\n            ResNet44 0.66M\n            ResNet56 0.85M\n            ResNet110 1.7M\n            # Arguments\n                input_shape (tensor): shape of input image tensor\n                depth (int): number of core convolutional layers\n                num_classes (int): number of classes (CIFAR10 has 10)\n            # Returns\n                model (Model): Keras model instance\n            \"\"\"\n            if (depth - 2) % 6 != 0:\n                raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n            # \u6a21\u578b\u7684\u521d\u59cb\u8a2d\u7f6e\uff0c\u8981\u7528\u591a\u5c11 filters\uff0c\u5171\u6709\u5e7e\u500b residual block \uff08\u7d44\u6210 ResNet \u7684\u55ae\u5143\uff09\n            num_filters = 16\n            num_res_blocks = int((depth - 2) / 6)\n            \n            # \u5efa\u7acb Input layer\n            inputs = Input(shape=input_shape)\n            \n            # \u5148\u5c0d\u5f71\u50cf\u505a\u7b2c\u4e00\u6b21\u5377\u6a5f\n            x = resnet_layer(inputs=inputs)\n            \n            # \u7e3d\u5171\u5efa\u7acb 3 \u500b stage\n            for stack in range(3):\n                # \u6bcf\u500b stage \u5efa\u7acb\u6578\u500b residual blocks (\u6578\u91cf\u8996\u4f60\u7684\u5c64\u6578\u800c\u8a02\uff0c\u8d8a\u591a\u5c64\u8d8a\u591a block)\n                for res_block in range(num_res_blocks):\n                    strides = 1\n                    if stack > 0 and res_block == 0:  # first layer but not first stack\n                        strides = 2  # downsample\n                    y = resnet_layer(inputs=x,\n                                    num_filters=num_filters,\n                                    strides=strides)\n                    y = resnet_layer(inputs=y,\n                                    num_filters=num_filters,\n                                    activation=None)\n                    if stack > 0 and res_block == 0:  # first layer but not first stack\n                        # linear projection residual shortcut connection to match\n                        # changed dims\n                        x = resnet_layer(inputs=x,\n                                        num_filters=num_filters,\n                                        kernel_size=1,\n                                        strides=strides,\n                                        activation=None,\n                                        batch_normalization=False)\n                    x = keras.layers.add([x, y]) # \u6b64\u8655\u628a featuremaps \u8207 \u4e0a\u4e00\u5c64\u7684\u8f38\u5165\u52a0\u8d77\u4f86 (\u6b32\u66f4\u4e86\u89e3\u7d50\u69cb\u9700\u95b1\u8b80\u539f\u8ad6\u6587)\n                    x = Activation('relu')(x)\n                num_filters *= 2\n\n            # \u5efa\u7acb\u5206\u985e\n            # \u4f7f\u7528 average pooling\uff0c\u4e14 size \u8ddf featuremaps \u7684 size \u4e00\u6a23 \uff08\u76f8\u7b49\u65bc\u505a GlobalAveragePooling\uff09\n            x = AveragePooling2D(pool_size=8)(x)\n            y = Flatten()(x)\n            \n            # \u63a5\u4e0a Dense layer \u4f86\u505a\u5206\u985e\n            outputs = Dense(num_classes,\n                            activation='softmax',\n                            kernel_initializer='he_normal')(y)\n\n            # \u5efa\u7acb\u6a21\u578b\n            model = Model(inputs=inputs, outputs=outputs)\n            return model\n\n        # \u5efa\u7acb ResNet v1 \u6a21\u578b\n        model = resnet_v1(input_shape=input_shape, depth=depth)\n\n        # \u7de8\u8b6f\u6a21\u578b\uff0c\u4f7f\u7528 Adam \u512a\u5316\u5668\u4e26\u4f7f\u7528\u5b78\u7fd2\u7387\u52d5\u614b\u8abf\u6574\u7684\u51fd\u6578\uff0c\uff10\u4ee3\u8868\u5728\u7b2c\u4e00\u500b epochs\n        model.compile(loss='categorical_crossentropy',\n                    optimizer=Adam(lr=lr_schedule(0)),\n                    metrics=['accuracy'])\n        model.summary()\n        print(model_type)\n\n        # \u4f7f\u7528\u52d5\u614b\u8abf\u6574\u5b78\u7fd2\u7387\n        lr_scheduler = LearningRateScheduler(lr_schedule)\n\n        # \u4f7f\u7528\u81ea\u52d5\u964d\u4f4e\u5b78\u7fd2\u7387 (\u7576 validation loss \u9023\u7e8c 5 \u6b21\u6c92\u6709\u4e0b\u964d\u6642\uff0c\u81ea\u52d5\u964d\u4f4e\u5b78\u7fd2\u7387)\n        lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n                                    cooldown=0,\n                                    patience=5,\n                                    min_lr=0.5e-6)\n        # \u8a2d\u5b9a callbacks\n        callbacks = [lr_reducer, lr_scheduler]\n\n\n        print('Using real-time data augmentation.')\n        datagen = ImageDataGenerator(\n            # set input mean to 0 over the dataset\n            featurewise_center=False,\n            # set each sample mean to 0\n            samplewise_center=False,\n            # divide inputs by std of dataset\n            featurewise_std_normalization=False,\n            # divide each input by its std\n            samplewise_std_normalization=False,\n            # apply ZCA whitening\n            zca_whitening=False,\n            # epsilon for ZCA whitening\n            zca_epsilon=1e-06,\n            # randomly rotate images in the range (deg 0 to 180)\n            rotation_range=0,\n            # randomly shift images horizontally\n            width_shift_range=0.1,\n            # randomly shift images vertically\n            height_shift_range=0.1,\n            # set range for random shear\n            shear_range=0.,\n            # set range for random zoom\n            zoom_range=0.,\n            # set range for random channel shifts\n            channel_shift_range=0.,\n            # set mode for filling points outside the input boundaries\n            fill_mode='nearest',\n            # value used for fill_mode = \"constant\"\n            cval=0.,\n            # randomly flip images\n            horizontal_flip=True,\n            # randomly flip images\n            vertical_flip=False,\n            # set rescaling factor (applied before any other transformation)\n            rescale=None,\n            # set function that will be applied on each input\n            preprocessing_function=None,\n            # image data format, either \"channels_first\" or \"channels_last\"\n            data_format=None,\n            # fraction of images reserved for validation (strictly between 0 and 1)\n            validation_split=0.0)\n\n        # \u5c07\u8cc7\u6599\u9001\u9032 ImageDataGenrator \u4e2d\u505a\u589e\u5f37\n        datagen.fit(x_train)\n\n        # \u8a13\u7df4\u6a21\u578b\u56c9\uff01\n        model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n                            steps_per_epoch=int(len(x_train)//batch_size),\n                            validation_data=(x_test, y_test),\n                            epochs=epochs, verbose=1, workers=4,\n                            callbacks=callbacks)\n\n        # \u8a55\u4f30\u6211\u5011\u7684\u6a21\u578b\n        scores = model.evaluate(x_test, y_test, verbose=1)\n        print('Test loss:', scores[0])\n        print('Test accuracy:', scores[1])\n        ```\n### Kaggle \u671f\u672b\u8003\n* **Day_101~103 : \u5f71\u50cf\u8fa8\u8b58**\n    * [\u6a5f\u5668\u5b78\u7fd2\u767e\u65e5\u99ac\u62c9\u677e\u671f\u672b\u8003 - \u82b1\u6735\u8fa8\u8b58](https://www.kaggle.com/c/4th-cupoy-ml-100-marathon-finalexam)\n### \u9032\u968e\u88dc\u5145 - \u96fb\u8166\u8996\u89ba\u5be6\u52d9\u5ef6\u4f38\n* **Day_104 : \u4e92\u52d5\u5f0f\u7db2\u9801\u795e\u7d93\u7db2\u8def\u8996\u89ba\u5316**\n    * \u4f55\u8b02 ConvNetJS?\n        * ConvNetJS \u662f\u4e00\u500b Javascript \u5eab\uff0c\u7528\u65bc\u5b8c\u5168\u5728\u60a8\u7684\u700f\u89bd\u5668\u4e2d\u8a13\u7df4\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\uff08\u795e\u7d93\u7db2\uf937\uff09\n        * [\u7dda\u4e0a\u7db2\u5740](https://cs.stanford.edu/people/karpathy/convnetjs/)\n* **Day_105 : CNN \u5377\u7a4d\u7db2\uf937\u6f14\u9032\u548c\u61c9\u7528**\n    * \u5377\u7a4d\u7db2\uf937 CNN \u7684\u9032\u5c55\u7c21\u5716\n        ```flow\n        st1=>start: Perception\n        st2=>start: Neocognition\n        st3=>start: LeNet\n        st4=>condition: AlexNet\n        a1=>start: MIN\n        a2=>start: Inception V1\n        a3=>start: Inception V2\n        a4=>start: Inception V3\n        b1=>start: VGG\n        b2=>start: MSRANet\n        b3=>start: ResNet\n        b4=>start: ResNet V2\n        e=>start: Inception ResNet V2\n        st1->st2->st3->st4\n        st4(yes)->a1->a2->a3->a4->e\n        st4(no)->b1->b2->b3->b4->e\n        ```\n    * \u5377\u7a4d\u7db2\uf937\u67b6\u69cb\u56de\u9867\n        * \u5377\u7a4d\u7db2\uf937\u7684\u512a\u9ede\uff1a\n            * \u5c40\u90e8\u611f\u77e5\u8207\u6b0a\u91cd\u5171\u4eab\uff0c\u85c9\u7531\u5377\u7a4d\u6838\u62bd\u53d6\u5f71\u50cf\u7684\u5c40\u90e8\u7279\u5fb5\uff0c\u4e26\u4e14\u8b93\u5f71\u50cf\u5404\u5340\u57df\u5171\u4eab\u9019\u500b\u5377\u7a4d\u6838\n        * \uf978\u5927\u90e8\u5206\uff1a\n            * \u5f71\u50cf\u7279\u5fb5\u63d0\u53d6\uff1aCNN_Layer1+Pooling_1+CNN_Layer2+Pooling_2 \u7684\u8655\uf9e4\n            * Fully Connected Layer\uff1a\u5305\u542b Flatten Layer, Hidden Layer, Output Layer\n    * CNN \u5716\u50cf\u8655\uf9e4\u4e0a\u6709\u4e0d\u540c\u7684\u6a21\u578b\n        * \u4e3b\u8981\u7684\u5143\u7d20\uff1aROI, CNN feature map, anchor boxes, mask\n    * \u6709\u8da3\u5ef6\u4f38\u61c9\u7528 :\n        * \u7121\u4eba\u5546\u5e97 : \u8ca8\u67b6\u6383\u63cf\u6a5f\u5668\u4eba\n            * \u4e3b\u8981\u529f\u80fd :   \n                * \u5229\u7528\u96fb\u8166\u8996\u89ba\u5c0e\u822a\uff0c\u907f\u514d\u649e\u4e0a\u9867\u5ba2\u6216\u63a8\u8eca\n                * \u7f3a\u8ca8\u88dc\u4e0a\n                * \u50f9\u683c\u6a19\u932f\u6821\u6b63\n                * \u50f9\u683c\u7f3a\u6a19\u6821\u6b63\n        * \u8aaa\u5716\u4eba (Image Caption)\n            * Activity Recognition : CNNs + LSTM\n            * Image Description : CNN + LSTM\n            * Vedio Description : CRF + LSTM\n        * R-CNN (Regional CNN)\n            * R-CNN\uff0c\u5b83\u628a\u7269\u9ad4\u6aa2\u6e2c\u6280\u8853\ufa02\u5c55\u5230\u63d0\u4f9b\u50cf\u7d20\u7d1a\u5225\u7684\u5206\u5272\u3002\n                * R-CNN\uff1ahttps://arxiv.org/abs/1311.2524\n            * R-CNN \u7684\u76ee\u6a19\u662f\uff1a\u5c0e\u5165\u4e00\u5f35\u5716\u7247\uff0c\u901a\u904e\u65b9\u6846\u6b63\u78ba\u8b58\u5225\u4e3b\u8981\u7269\u9ad4\u5728\u5716\u50cf\u7684\u54ea\u500b\u5730\u65b9\u3002\n                * \u8f38\u5165\uff1a\u5716\u50cf\n                * \u8f38\u51fa\uff1a\u65b9\u6846 + \u6bcf\u500b\u7269\u9ad4\u7684\u6a19\u7c64\n            * \u4f46\u600e\u9ebc\u77e5\u9053\u9019\u4e9b\u65b9\u6846\u61c9\u8a72\u5728\u54ea\u88e1\u5462\uff1f\n                * R-CNN \u7684\u8655\uf9e4\u65b9\u5f0f \u2014 \u5728\u5716\u50cf\u4e2d\u641e\u51fa\u4e00\u5927\u5806\u65b9\u6846\uff0c\u770b\u770b\u662f\u5426\u6709\u4efb\u4f55\u4e00\u500b\u8207\u67d0\u500b\u7269\u9ad4\u91cd\u758a\n                * \u751f\u6210\u9019\u4e9b\u908a\u6846\u3001\u6216\u8005\u8aaa\u662f\u63a8\u85a6\u5c40\u57df\uff0cR-CNN \u63a1\u7528\u7684\u662f\u4e00\u9805\u540d\u70ba Selective Search \u7684\u6d41\u7a0b\n                    * Selective Search \u901a\u904e\u4e0d\u540c\u5c3a\u5bf8\u7684\u7a97\u53e3\uf92d\u67e5\u770b\u5716\u50cf\n                    * \u5c0d\u65bc\u6bcf\u4e00\u500b\u5c3a\u5bf8\uff0c\u5b83\u901a\u904e\u7d0b\uf9e4\u3001\u8272\u5f69\u6216\u5bc6\u5ea6\u628a\u76f8\u9130\u50cf\u7d20\u5283\u70ba\u4e00\u7d44\uff0c\uf92d\u9032\u884c\u7269\u9ad4\u8b58\u5225\u3002\n                    * \u7576\u908a\u6846\u65b9\u6848\u751f\u6210\u4e4b\u5f8c\uff0cR-CNN \u628a\u9078\u53d6\u5340\u57df\u8b8a\u5f62\u70ba\u6a19\u6e96\u7684\u65b9\u5f62\n                    * \u5728 CNN \u7684\u6700\u5f8c\u4e00\u5c64\uff0cR-CNN \u52a0\u5165\uf9ba\u4e00\u500b\u652f\u6301\u5411\uf97e\u6a5f\uff0c\u5b83\u8981\u505a\u7684\u4e8b\u5f88\u7c21\u55ae\uff1a\u5c0d\u9019\u662f\u5426\u662f\u4e00\u500b\u7269\u9ad4\u9032\ufa08\u5206\u985e\uff0c\u5982\u679c\u662f\uff0c\u662f\uf9fd\u9ebc\u7269\u9ad4\u3002\n                * \u662f\u5426\u80fd\u7e2e\u5c0f\u908a\u6846\uff0c\u8b93\u5b83\uf901\u7b26\u5408\u7269\u9ad4\u7684\u4e09\u7dad\u5c3a\u5bf8\uff1f\n                    * \u7b54\u6848\u662f\u80af\u5b9a\u7684\uff0c\u9019\u662f R-CNN \u7684\u6700\u5f8c\u4e00\u6b65\u3002\n                * R-CNN \u5728\u63a8\u85a6\u5340\u57df\u4e0a\u904b\ufa08\u4e00\u500b\u7c21\u55ae\u7684\u7dda\u6027\u56de\u6b78\uff0c\u751f\u6210\uf901\u7dca\u7684\u908a\u6846\u5750\u6a19\u4ee5\u5f97\u5230\u6700\u7d42\u7d50\u679c\u3002\n                    * \u56de\u6b78\u6a21\u578b\u7684\u8f38\u5165\u548c\u8f38\u51fa\uff1a\n                        * \u8f38\u5165\uff1a\u5c0d\u61c9\u7269\u9ad4\u7684\u5716\u50cf\u5b50\u5340\u57df\n                        * \u8f38\u51fa\uff1a\u91dd\u5c0d\u8a72\u7269\u9ad4\u7684\u65b0\u908a\u6846\u7cfb\u7d71\n            * \u6982\u62ec\u4e0b\uf92d\uff0cR-CNN \u53ea\u662f\u4ee5\u4e0b\u9019\u5e7e\u500b\u6b65\u9a5f\uff1a\n                * \u751f\u6210\u5c0d\u908a\u6846\u7684\u63a8\u85a6\n                * \u5728\u9810\u8a13\u7df4\u7684 AlexNet \u4e0a\u904b\ufa08\u65b9\u6846\u88e1\u7684\u7269\u9ad4\u3002\u7528\u652f\u6301\u5411\uf97e\u6a5f\uf92d\u770b\u908a\u6846\u88e1\u7684\u7269\u9ad4\u662f\uf9fd\u9ebc\u3002\n                * \u5728\u7dda\u6027\u56de\u6b78\u6a21\u578b\u4e0a\u8dd1\u8a72\u908a\u6846\uff0c\u5728\u7269\u9ad4\u5206\u985e\u4e4b\u5f8c\u8f38\u51fa\uf901\u7dca\u7684\u908a\u6846\u7684\u5ea7\u6a19\n* **Day_106 : \u96fb\u8166\u8996\u89ba\u5e38\u7528\u516c\u958b\u8cc7\u6599\u96c6**\n    * \u641c\u96c6\u8207\u6a19\u6ce8\u8cc7\u6599\u662f\u8017\u6642\u52de\uf98a\u7684\u5de5\u4f5c\n    * \uf974\u5c08\u6848\u7684\u76ee\u6a19\u76f8\u8fd1\uff0c\u7db2\u8def\u4e0a\u662f\u6709\u975e\u5e38\u591a\u516c\u958b\u4e14\u6a19\u6ce8\u597d\u7684\u8cc7\u6599\u96c6\u53ef\u4ee5\u4f7f\u7528\uff01\n        * Kaggle\n        * ImageNet dataset\n        * COCO dataset\n    * Kaggle\n        * \u5404\u5f0f\u5404\u6a23\u7684\u5f71\u50cf\u8fa8\u8b58\u984c\u76ee\uff0c\uf9b5\u5982[\u6578\u6d77\u7345\u6578\uf97e](https://zhuanlan.zhihu.com/p/29096434)\n    * ImageNet\n        * \u7531\u53f2\uf95e\u4f5b\u674e\u98db\u98db\u6559\u6388\u5718\u968a\u6536\u96c6\uff0c\u5171 1400 \u842c\u5f35\u5f71\u50cf\uff0c1000 \u500b\u985e\u5225\u8981\u5206\u985e\uff0c\u5176\u4e2d\u985e\u5225\u975e\u5e38\u7d30\u7dfb\uff0c\u9700\u8981\u5340\u5206\u5404\u7a2e\u9ce5\u985e\u3001\u74f6\u5b50\u8207\uf902\u8f1b\u7b49\n        * \u76ee\u524d\u5206\u985e\u6a21\u578b\u7684 benchmark \u5e7e\u4e4e\u90fd\u662f\u8dd1\u5728 ImageNet\n    * COCO dataset\n        * \u5e38\ufa0a\u7684 80 \u500b\u985e\u5225\uff0c\u662f\u76ee\u524d\u6700\u5b8c\u6574\u6a19\u8a3b\u7684\u8cc7\u6599\u96c6\uff0c\u5305\u542b\u5206\u5272\u3001\u5075\u6e2c\u3001\u6587\u5b57\u63cf\u8ff0\u3002\n        * \u5075\u6e2c\u6a21\u578b\u3001\u5206\u5272\u6a21\u578b\u7684 benchmark \u90fd\u6703\u4f7f\u7528\u5728 COCO dataset \u4e0a\n* **Day_107 : \u96fb\u8166\u8996\u89ba\u61c9\u7528\u4ecb\u7d39 - \u5f71\u50cf\u5206\u985e, \u5f71\u50cf\u5206\u5272, \u7269\u4ef6\u5075\u6e2c**\n    * \u96fb\u8166\u8996\u89ba\u4e2d\uff0c\u6709\u8a31\u591a\u4e0d\u540c\u7684\u61c9\u7528\uff0c\u6bd4\u5982\uff1a\n        * \u5f71\u50cf\u5206\u985e\n        * \u5f71\u50cf\u5206\u5272\n        * \u7269\u4ef6\u5075\u6e2c\n        * \u4eba\u81c9\u5075\u6e2c\n        * \u95dc\u9375\u9ede\u5075\u6e2c\n        * \u5be6\uf9b5\u5206\u5272\n    * \u9019\u4e9b\u61c9\u7528\u591a\u534a\u6709\u5927\u578b\u4e14\u6a19\u6ce8\u597d\u7684\u8cc7\u6599\u96c6\u4f5c\u70ba\u8a55\u4f30 (benchmark)\n    * \u5f71\u50cf\u5206\u985e Image Classification\n        * \u6700\u5e38\ufa0a\u7684\u6709\n            * \u624b\u5beb\u6578\u5b57\u8fa8\u8b58 (MNIST)\u3001\n            * Cifar-10\u3001\n            * Cifar-100\u3001\n            * ImageNet (1,000 \u500b\u985e\u5225\u5206\u985e)\u3001\n            * ImageNet-10k (10,000 \u500b\u985e\u5225\u5206\u985e)\n    * \u5f71\u50cf\u5206\u5272 Image Segmentation\n        * \u5c07\u5f71\u50cf\u4e2d\u7684\u985e\u5225\uf9d7\u5ed3\u5206\u5272\u51fa\uf92d\uff0c\u53ef\u4ee5\u5f97\u5230\u6bcf\u500b\u985e\u5225\u7684\uf9d7\u5ed3\uff0c\u5e38\ufa0a\u7684\u6709\n            * Pascal VOC dataset\n            * cityscapes dataset\n    * \u7269\u4ef6\u5075\u6e2c Object Detection\n        * \u5c07\u5f71\u50cf\u4e2d\u7684\u985e\u5225\u5ea7\u6a19\u5075\u6e2c\u51fa\uf92d\uff0c\u53ef\u4ee5\u5f97\u5230\u6bcf\u500b\u985e\u5225\u6db5\u84cb\u7684\u7bc4\u570d\uff0c\u53ef\u4ee5\u9032\ufa08\u6578\uf97e\u8a08\u7b97\uff0c\u5e38\ufa0a\u7684\u6709\n            * COCO dataset\n            * Home objects dataset\n    * \u4eba\u81c9\u5075\u6e2c Face Detection\n        * \u5c07\u5f71\u50cf\u4e2d\u7684\u4eba\u81c9\u4f4d\u7f6e\u627e\u51fa\uff0c\u4e26\u9032\ufa08\u5206\u985e\u3002\u5e38\ufa0a\u7684\u81c9\u90e8\u8cc7\u6599\u96c6\u6709\n            * CelebFaces, \n            * Labeled Face in the wilds\n    * \u95dc\u9375\u9ede\u5075\u6e2c Keypoint Detection\n        * \u5c07\u5f71\u50cf\u4e2d\u4eba\u7269\u7684\u95dc\u9375\u9ede\u5075\u6e2c\u51fa\uf92d\uff0c\u5f8c\u7e8c\u53ef\u505a\u59ff\u52e2\u9810\u6e2c\u6216\u662f\u6b65\u614b\u8fa8\u5225\u7b49\u61c9\u7528\u3002\n            * COCO dataset\n    * \u5be6\uf9b5\u5206\u5272 Instance Segmentation \n        * \u8207\u7269\u4ef6\u5075\u6e2c\u975e\u5e38\u985e\u4f3c\uff0c\u4f46\u9664\uf9ba\u7269\u4ef6\u7684\u6846\u6846\u4ee5\u5916\uff0c\u9700\u8981\u5c07\uf9d7\u5ed3\u4e5f\u4e00\u4f75\u5075\u6e2c\u51fa\uf92d\u3002\n            * COCO dataset \n\n\n",
            "readme_url": "https://github.com/Halesu/4th-ML100Days",
            "frameworks": [
                "Keras",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "arxiv": "1311.2524",
            "year": 2013,
            "url": "http://arxiv.org/abs/1311.2524v5",
            "abstract": "Object detection performance, as measured on the canonical PASCAL VOC\ndataset, has plateaued in the last few years. The best-performing methods are\ncomplex ensemble systems that typically combine multiple low-level image\nfeatures with high-level context. In this paper, we propose a simple and\nscalable detection algorithm that improves mean average precision (mAP) by more\nthan 30% relative to the previous best result on VOC 2012---achieving a mAP of\n53.3%. Our approach combines two key insights: (1) one can apply high-capacity\nconvolutional neural networks (CNNs) to bottom-up region proposals in order to\nlocalize and segment objects and (2) when labeled training data is scarce,\nsupervised pre-training for an auxiliary task, followed by domain-specific\nfine-tuning, yields a significant performance boost. Since we combine region\nproposals with CNNs, we call our method R-CNN: Regions with CNN features. We\nalso compare R-CNN to OverFeat, a recently proposed sliding-window detector\nbased on a similar CNN architecture. We find that R-CNN outperforms OverFeat by\na large margin on the 200-class ILSVRC2013 detection dataset. Source code for\nthe complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
            "authors": [
                "Ross Girshick",
                "Jeff Donahue",
                "Trevor Darrell",
                "Jitendra Malik"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "An overview of gradient descent optimization algorithms",
            "arxiv": "1609.04747",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.04747v2",
            "abstract": "Gradient descent optimization algorithms, while increasingly popular, are\noften used as black-box optimizers, as practical explanations of their\nstrengths and weaknesses are hard to come by. This article aims to provide the\nreader with intuitions with regard to the behaviour of different algorithms\nthat will allow her to put them to use. In the course of this overview, we look\nat different variants of gradient descent, summarize challenges, introduce the\nmost common optimization algorithms, review architectures in a parallel and\ndistributed setting, and investigate additional strategies for optimizing\ngradient descent.",
            "authors": [
                "Sebastian Ruder"
            ]
        },
        {
            "title": "Identity Mappings in Deep Residual Networks",
            "arxiv": "1603.05027",
            "year": 2016,
            "url": "http://arxiv.org/abs/1603.05027v3",
            "abstract": "Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "F1-score",
            "url": "https://en.wikipedia.org/wiki/F1_score"
        },
        {
            "title": "matplotlib",
            "url": "https://matplotlib.org/gallery/index.html"
        },
        {
            "title": "seaborn",
            "url": "https://seaborn.pydata.org/examples/index.html"
        },
        {
            "title": "\u6558\u8ff0\u7d71\u8a08\u8207\u6a5f\u7387\u5206\u4f48",
            "url": "http://www.hmwu.idv.tw/web/R_AI_M/AI-M1-hmwu_R_Stat&Prob_v2.pdf"
        },
        {
            "title": "\u8fa8\u8b58\u7570\u5e38\u503c",
            "url": "https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/"
        },
        {
            "title": "IQR",
            "url": "https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/"
        },
        {
            "title": "\u7f3a\u5931\u503c\u8655\u7406",
            "url": "https://juejin.im/post/5b5c4e6c6fb9a04f90791e0c"
        },
        {
            "title": "Panda \u5b98\u65b9 Cheat Sheet",
            "url": "https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf"
        },
        {
            "title": "Panda Cheet Sheet",
            "url": "https://assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf"
        },
        {
            "title": "\u76f8\u95dc\u4fc2\u6578\u5c0f\u904a\u6232",
            "url": "http://guessthecorrelation.com/"
        },
        {
            "title": "Python Graph Gallery",
            "url": "https://python-graph-gallery.com/"
        },
        {
            "title": "R Graph Gallery",
            "url": "https://www.r-graph-gallery.com/"
        },
        {
            "title": "Interactive plot\uff0c\u4e92\u52d5\u5716",
            "url": "https://bl.ocks.org/mbostock"
        },
        {
            "title": "\u96e2\u6563\u5316\u76ee\u7684",
            "url": "https://www.zhihu.com/question/31989952"
        },
        {
            "title": "matplotlib \u5b98\u2f45\u65b9\u7bc4\u4f8b\uf9b5",
            "url": "https://matplotlib.org/examples/pylab_examples/subplots_demo.html"
        },
        {
            "title": "\u8907\u96dc\u7248 subplot \u5beb\u6cd5",
            "url": "https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html"
        },
        {
            "title": "\u53e6\u985e\u2f26\u5b50\u5716 Seaborn.jointplot",
            "url": "https://seaborn.pydata.org/generated/seaborn.jointplot.html"
        },
        {
            "title": "\u57fa\u672c Heatmap",
            "url": "https://matplotlib.org/gallery/images_contours_and_fields/image_annotated_heatmap.html"
        },
        {
            "title": "\u9032\u968e Heatmap",
            "url": "https://www.jianshu.com/p/363bbf6ec335"
        },
        {
            "title": "pairplot \u66f4\u591a\u61c9\u7528",
            "url": "https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166"
        },
        {
            "title": "Logistic Regression",
            "url": "https://www.youtube.com/playlist?list=PLNeKWBMsAzboR8vvhnlanxCNr2V7ITuxy"
        },
        {
            "title": "\u7279\u5fb5\u5de5\u7a0b\u662f\u4ec0\u9ebc",
            "url": "https://www.zhihu.com/question/29316149"
        },
        {
            "title": "\u504f\u5ea6\u8207\u5cf0\u5ea6",
            "url": "https://blog.csdn.net/u013555719/article/details/78530879"
        },
        {
            "title": "\u6a19\u7c64\u7de8\u78bc\u8207\u7368\u71b1\u7de8\u78bc",
            "url": "https://blog.csdn.net/u013555719/article/details/78530879"
        },
        {
            "title": "\u7279\u5fb5\u54c8\u5e0c",
            "url": "https://blog.csdn.net/laolu1573/article/details/79410187"
        },
        {
            "title": "\u6587\u672c\u7279\u5fb5\u62bd\u53d6",
            "url": "https://www.jianshu.com/p/063840752151"
        },
        {
            "title": "\u6642\u9593\u65e5\u671f\u8655\u7406",
            "url": "http://www.wklken.me/posts/2015/03/03/python-base-datetime.html"
        },
        {
            "title": "datetime",
            "url": "https://docs.python.org/3/library/datetime.html"
        },
        {
            "title": "\u7279\u5fb5\u4ea4\u53c9",
            "url": "https://segmentfault.com/a/1190000014799038"
        },
        {
            "title": "\u6578\u64da\u805a\u5408\u8207\u5206\u7d44",
            "url": "https://zhuanlan.zhihu.com/p/27590154"
        },
        {
            "title": "\u7279\u5fb5\u9078\u64c7",
            "url": "https://zhuanlan.zhihu.com/p/32749489"
        },
        {
            "title": "\u7279\u5fb5\u9078\u64c7\u624b\u518a",
            "url": "https://machine-learning-python.kspax.io/intro-1"
        },
        {
            "title": "\u7279\u5fb5\u9078\u64c7\u7684\u512a\u5316\u6d41\u7a0b",
            "url": "https://juejin.im/post/5a1f7903f265da431c70144c"
        },
        {
            "title": "Permutation Importance",
            "url": "https://www.kaggle.com/dansbecker/permutation-importance?utm_medium=email&utm_source=mailchimp&utm_campaign=ml4insights"
        },
        {
            "title": "Feature transformations with ensembles of trees",
            "url": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#example-ensemble-plot-feature-transformation-py"
        },
        {
            "title": "Algorithm-GBDT Encoder",
            "url": "https://zhuanlan.zhihu.com/p/31734283"
        },
        {
            "title": "\u5206\u89e3\u6a5f\uff0cFactorization Machine\uff0cFM",
            "url": "https://kknews.cc/code/62k4rml.html"
        },
        {
            "title": "\u5b78\u7fd2\u66f2\u7dda\u8207 bias/variance trade-off",
            "url": "http://bangqu.com/yjB839.html"
        },
        {
            "title": "K-fold",
            "url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold"
        },
        {
            "title": "\u8a13\u7df4\u3001\u9a57\u8b49\u8207\u6e2c\u8a66\u96c6\u7684\u610f\u7fa9",
            "url": "https://www.youtube.com/watch?v=D_S6y0Jm6dQ&feature=youtu.be&t=1948"
        },
        {
            "title": "\u56de\u6b78\u8207\u5206\u985e\u6bd4\u8f03",
            "url": "http://zylix666.blogspot.com/2016/06/supervised-classificationregression.html"
        },
        {
            "title": "Multi-class vs. Multi-label ",
            "url": "https://medium.com/coinmonks/multi-label-classification-blog-tags-prediction-using-nlp-b0b5ee6686fc"
        },
        {
            "title": "\u8d85\u8a73\u89e3 AUC",
            "url": "https://www.dataschool.io/roc-curves-and-auc-explained/"
        },
        {
            "title": "\u66f4\u591a\u8a55\u4f30\u6307\u6a19",
            "url": "https://zhuanlan.zhihu.com/p/30721429"
        },
        {
            "title": "Andrew Ng \u6559\u4f60 Linear Regression",
            "url": "https://zh-tw.coursera.org/lecture/machine-learning/model-representation-db3jS"
        },
        {
            "title": "Logistic Regression \u6578\u5b78\u539f\u7406",
            "url": "https://blog.csdn.net/qq_23269761/article/details/81778585"
        },
        {
            "title": "Linear Regression \u8a73\u7d30\u4ecb\u7d39",
            "url": "https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_linear_regression_works.html"
        },
        {
            "title": "\u4f60\u53ef\u80fd\u4e0d\u77e5\u9053\u7684 Logistic Regression",
            "url": "https://taweihuang.hpd.io/2017/12/22/logreg101/"
        },
        {
            "title": "\u66f4\u591a Linear regression \u548c Logistic regression \u7bc4\u4f8b",
            "url": "https://github.com/trekhleb/homemade-machine-learning"
        },
        {
            "title": "\u6df1\u5165\u4e86\u89e3 multi-nominal Logistic Regresson \u539f\u7406",
            "url": "http://dataaspirant.com/2017/05/15/implement-multinomial-logistic-regression-python/"
        },
        {
            "title": "Linear, Lasso, Ridge Regression \u672c\u8cea\u5340\u5225",
            "url": "https://www.zhihu.com/question/38121173"
        },
        {
            "title": "\u6c7a\u7b56\u6a39\u8207\u56de\u6b78\u554f\u984c",
            "url": "https://www.saedsayad.com/decision_tree_reg.htm"
        },
        {
            "title": "Creating and Visualizing Decision Trees with Python",
            "url": "https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176"
        },
        {
            "title": "\u96a8\u6a5f\u68ee\u6797",
            "url": "http://hhtucode.blogspot.com/2013/06/ml-random-forest.html"
        },
        {
            "title": "How Random Forest Algorithm Works",
            "url": "https://medium.com/@Synced/how-random-forest-algorithm-works-in-machine-learning-3c0fe15b6674"
        },
        {
            "title": "bootstrap",
            "url": "http://sofasofa.io/forum_main_post.php?postid=1000691"
        },
        {
            "title": "\u68af\u5ea6\u63d0\u5347\u6c7a\u7b56\u6a39",
            "url": "https://ifun01.com/84A3FW7.html"
        },
        {
            "title": "XGboost",
            "url": "https://www.youtube.com/watch?v=ufHo8vbk6g4"
        },
        {
            "title": "\u9673\u5929\u5947 - Boosted Tree",
            "url": "https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf"
        },
        {
            "title": "\u674e\u5b8f\u6bc5 - Ensemble",
            "url": "https://www.youtube.com/watch?v=tH9FH1DH5n0"
        },
        {
            "title": "Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting",
            "url": "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
        },
        {
            "title": "how to tune machine learning models",
            "url": "https://cambridgecoding.wordpress.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/"
        },
        {
            "title": "Hyperparameter Tuning the Random Forest in Python",
            "url": "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
        },
        {
            "title": "scikit-learn-practice",
            "url": "https://www.kaggle.com/c/data-science-london-scikit-learn"
        },
        {
            "title": "Blending and Stacking",
            "url": "https://www.youtube.com/watch?v=mjUKsp0MvMI&list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&index=27&t=0s"
        },
        {
            "title": "superblend",
            "url": "https://www.kaggle.com/tunguz/superblend/code"
        },
        {
            "title": "Enron Fraud Dataset \u5b89\u9686\u516c\u53f8\u8a50\u6b3a\u6848\u8cc7\u6599\u96c6",
            "url": "https://www.kaggle.com/c/ml100"
        },
        {
            "title": "\u674e\u5b8f\u6bc5 - Unsupervised learning",
            "url": "http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/PCA.mp4"
        },
        {
            "title": "scikit-learn unsupervised learning",
            "url": "https://scikit-learn.org/stable/unsupervised_learning.html"
        },
        {
            "title": "Andrew Ng - Unsupervised learning",
            "url": "https://youtu.be/jAA2g9ItoAc"
        },
        {
            "title": "Andrew Ng - K-means",
            "url": "https://www.youtube.com/watch?v=hDmNF9JG3lo"
        },
        {
            "title": "Hierarchical Clustering",
            "url": "https://www.youtube.com/watch?v=Tuuc9Y06tAc"
        },
        {
            "title": "Example : Breast cancer Miroarray study",
            "url": "https://www.youtube.com/watch?v=yUJcTpWNY_o"
        },
        {
            "title": "Unsupervised Learning",
            "url": "https://www.youtube.com/watch?v=ipyxSYXgzjQ"
        },
        {
            "title": "Further Principal Components",
            "url": "https://www.youtube.com/watch?v=dbuSGWCgdzw"
        },
        {
            "title": "Principal Components Regression",
            "url": "https://www.youtube.com/watch?v=eYxwWGJcOfw"
        },
        {
            "title": "Andrew Ng - Dimensionality Reduction",
            "url": "https://www.youtube.com/watch?time_continue=1&v=rng04VJxUt4"
        },
        {
            "title": "visualing data use t-SNE",
            "url": "https://www.youtube.com/watch?v=RJVL80Gg3lA"
        },
        {
            "title": "3 \u5206\u9418\u641e\u61c2\u6df1\u5ea6\u5b78\u7fd2\u5230\u5e95\u5728\u6df1\uf9fd\u9ebc",
            "url": "https://panx.asia/archives/53209"
        },
        {
            "title": "Understanding neural networks with TensorFlow Playground",
            "url": "https://cloud.google.com/blog/products/gcp/understanding-neural-networks-with-tensorflow-playground"
        },
        {
            "title": "\u6df1\u5ea6\u5b78\u7fd2\u7db2\u8def\u8abf\u53c3\u6280\u5de7",
            "url": "https://zhuanlan.zhihu.com/p/24720954"
        },
        {
            "title": "Keras : The Python Deep Learning Library",
            "url": "https://github.com/keras-team/keras/"
        },
        {
            "title": "Keras dataset",
            "url": "https://keras.io/api/datasets/"
        },
        {
            "title": "Predicting Boston House Prices",
            "url": "https://www.kaggle.com/sagarnildass/predicting-boston-house-prices"
        },
        {
            "title": "imagenet",
            "url": "http://www.image-net.org/about-stats"
        },
        {
            "title": "COCO(Common Objects in Context)",
            "url": "http://cocodataset.org/"
        },
        {
            "title": "Getting started with Keras Sequential model",
            "url": "https://keras.io/getting-started/sequential-model-guide/"
        },
        {
            "title": "Getting started with the Keras function API",
            "url": "https://keras.io/guides/functional_api/"
        },
        {
            "title": "\u4ea4\u53c9\u71b5",
            "url": "https://blog.csdn.net/qq_40147863/article/details/82015360"
        },
        {
            "title": "losses function",
            "url": "https://keras.io/losses/"
        },
        {
            "title": "\u795e\u7d93\u7db2\u8def\u5e38\u7528\u555f\u52d5\u51fd\u6578\u7e3d\u7d50",
            "url": "https://zhuanlan.zhihu.com/p/39673127"
        },
        {
            "title": "CS231N Lecture",
            "url": "http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf"
        },
        {
            "title": "learning rate decay",
            "url": "https://zhuanlan.zhihu.com/p/32923584"
        },
        {
            "title": "gradient descent using python and numpy",
            "url": "https://stackoverflow.com/questions/17784587/gradient-descent-using-python-and-numpy"
        },
        {
            "title": "\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u53c2\u6570\u66f4\u65b0\u516c\u5f0f",
            "url": "https://blog.csdn.net/hrkxhll/article/details/80395033"
        },
        {
            "title": "\u6df1\u5ea6\u5b78\u7fd2(Deep Learning)-\u53cd\u5411\u50b3\u64ad",
            "url": "https://ithelp.ithome.com.tw/articles/10198813"
        },
        {
            "title": "BP\u795e\u7ecf\u7f51\u7edc\u7684\u539f\u7406\u53caPython\u5b9e\u73b0",
            "url": "https://blog.csdn.net/conggova/article/details/77799464"
        },
        {
            "title": "Optimizers",
            "url": "https://keras.io/api/optimizers/"
        },
        {
            "title": "\u4f18\u5316\u5668\u5982\u4f55\u9009\u62e9",
            "url": "https://blog.csdn.net/qq_35860352/article/details/80772142"
        },
        {
            "title": "Second Order Optimization Algorithms",
            "url": "https://web.stanford.edu/class/msande311/lecture13.pdf"
        },
        {
            "title": "Overfitting in Machine Learning",
            "url": "https://elitedatascience.com/overfitting-in-machine-learning"
        },
        {
            "title": "Overfitting vs. Underfitting",
            "url": "https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765"
        },
        {
            "title": "Troubleshooting Deep Neural Networks",
            "url": "http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf"
        },
        {
            "title": "Estimating an Optimal Learning Rate For a Deep Neural Network",
            "url": "https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0"
        },
        {
            "title": "cs231n : learning and evaluation",
            "url": "https://cs231n.github.io/neural-networks-3/"
        },
        {
            "title": "\u6df1\u5ea6\u5b66\u4e60\u8d85\u53c2\u6570\u7b80\u5355\u7406\u89e3",
            "url": "https://zhuanlan.zhihu.com/p/23906526"
        },
        {
            "title": "\u4f18\u5316\u65b9\u6cd5\u603b\u7ed3",
            "url": "https://blog.csdn.net/u010089444/article/details/76725843"
        },
        {
            "title": "An overview of gradient descent optimization algorithms",
            "url": "https://ruder.io/optimizing-gradient-descent/"
        },
        {
            "title": "Regularization in Machine Learning",
            "url": "https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a"
        },
        {
            "title": "\u7406\u89e3dropout",
            "url": "https://blog.csdn.net/stdcoutzyx/article/details/49022443"
        },
        {
            "title": "Dropout in (Deep) Machine learning",
            "url": "https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5"
        },
        {
            "title": "\u70ba\u4f55\u8981\u6279\u6b21\u6a19\u6e96\u5316",
            "url": "https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-08-batch-normalization/"
        },
        {
            "title": "Batch Normalization \u539f\u7406\u4e0e\u5b9e\u6218",
            "url": "https://zhuanlan.zhihu.com/p/34879333"
        },
        {
            "title": "keras\u7684EarlyStopping callbacks\u7684\u4f7f\u7528\u4e0e\u6280\u5de7",
            "url": "https://blog.csdn.net/silent56_th/article/details/72845912"
        },
        {
            "title": "\u83ab\u7169 Python - \u5132\u5b58\u8207\u8f09\u56de\u6a21\u578b",
            "url": "https://morvanzhou.github.io/tutorials/machine-learning/keras/3-1-save/"
        },
        {
            "title": "Callbacks API",
            "url": "https://keras.io/api/callbacks/"
        },
        {
            "title": "Keras \u4e2d\u4fdd\u7559 f1-score \u6700\u9ad8\u7684\u6a21\u578b (per epoch)",
            "url": "https://zhuanlan.zhihu.com/p/51356820"
        },
        {
            "title": "Keras\u81ea\u5b9a\u4e49Loss\u51fd\u6570",
            "url": "https://blog.csdn.net/A_a_ron/article/details/79050204"
        },
        {
            "title": "focal loss",
            "url": "https://blog.csdn.net/u014380165/article/details/77019084"
        },
        {
            "title": "\u56fe\u50cf\u5206\u7c7b|\u6df1\u5ea6\u5b66\u4e60PK\u4f20\u7edf\u673a\u5668\u5b66\u4e60",
            "url": "https://cloud.tencent.com/developer/article/1111702"
        },
        {
            "title": "OpenCv - \u6559\u5b78\u6587\u6a94",
            "url": "https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html"
        },
        {
            "title": "Introduction to Computer Vision",
            "url": "https://www.udacity.com/course/introduction-to-computer-vision--ud810"
        },
        {
            "title": "\u57fa\u4e8e\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u7684\u76ee\u6807\u68c0\u6d4b\u4e0e\u8bc6\u522b(HOG+SVM\u9644\u4ee3\u7801)",
            "url": "https://www.cnblogs.com/zyly/p/9651261.html"
        },
        {
            "title": "\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u662f\u4ec0\u4e48",
            "url": "https://www.zhihu.com/question/21094489"
        },
        {
            "title": "An Intuitive Explanation of Convolutional Neural Networks",
            "url": "https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/"
        },
        {
            "title": "\u57fa\u4e8eKeras\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u53ef\u89c6\u5316",
            "url": "https://blog.csdn.net/weiwei9363/article/details/79112872"
        },
        {
            "title": "Keras ImageDataGenerator \u7bc4\u4f8b\u8207\u4ecb\u7d39",
            "url": "https://zhuanlan.zhihu.com/p/30197320"
        },
        {
            "title": "imgaug",
            "url": "https://github.com/aleju/imgaug"
        },
        {
            "title": "\u7c21\u55ae\u4f7f\u7528 Keras \u5b8c\u6210 Transfer learning",
            "url": "https://ithelp.ithome.com.tw/articles/10190971"
        },
        {
            "title": "Keras \u4ee5 ResNet-50 \u9810\u8a13\u7df4\u6a21\u578b\u5efa\u7acb\u72d7\u8207\u8c93\u8fa8\u8b58\u7a0b\u5f0f",
            "url": "https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/"
        },
        {
            "title": "\u6a5f\u5668\u5b78\u7fd2\u767e\u65e5\u99ac\u62c9\u677e\u671f\u672b\u8003 - \u82b1\u6735\u8fa8\u8b58",
            "url": "https://www.kaggle.com/c/4th-cupoy-ml-100-marathon-finalexam"
        },
        {
            "title": "\u7dda\u4e0a\u7db2\u5740",
            "url": "https://cs.stanford.edu/people/karpathy/convnetjs/"
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Enron Fraud Dataset \u5b89\u9686\u516c\u53f8\u8a50\u6b3a\u6848\u8cc7\u6599\u96c6",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://www.kaggle.com/c/ml100"
                    }
                }
            },
            {
                "name": "Keras dataset",
                "connection": {
                    "name": "url",
                    "source": {
                        "url": "https://keras.io/api/datasets/"
                    }
                }
            },
            {
                "name": "ImageNet"
            },
            {
                "name": "MNIST"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "Fashion-MNIST"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "COCO"
            },
            {
                "name": "Wikipedia"
            },
            {
                "name": "IMDb"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "ILSVRC 2015"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9631695621217562,
        "task": "Image Classification",
        "task_prob": 0.7743246161157992
    }
}