{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "Real-Time Voice Cloning",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "CorentinJ",
                "owner_type": "User",
                "name": "Real-Time-Voice-Cloning",
                "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning",
                "stars": 34081,
                "pushed_at": "2022-03-13 12:50:45+00:00",
                "created_at": "2019-05-26 08:56:15+00:00",
                "language": "Python",
                "description": "Clone a voice in 5 seconds to generate arbitrary speech in real-time",
                "license": "Other",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitattributes",
                "sha": "5be91f972c7b8865ddced78b6e12ea3453ba2daf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/.gitattributes"
                    }
                },
                "size": 26
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "9401d2ebba5b5f79cd139c11374330a0dea4e144",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/.gitignore"
                    }
                },
                "size": 206
            },
            {
                "type": "code",
                "name": "LICENSE.md",
                "sha": "5ed721bf8f29f5c8d947c2d333cc371021135fb0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/LICENSE.md"
                    }
                },
                "size": 1355
            },
            {
                "type": "code",
                "name": "demo_cli.py",
                "sha": "4a0ae653fbe1fccb883f83aba5065f77ce2a79de",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/demo_cli.py"
                    }
                },
                "size": 9589
            },
            {
                "type": "code",
                "name": "demo_toolbox.py",
                "sha": "e69eb6261f08fb51916bf7ad575e7f102bef3fd0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/demo_toolbox.py"
                    }
                },
                "size": 1342
            },
            {
                "type": "code",
                "name": "encoder",
                "sha": "2c5a1b2890643be9889537b08af5a168fbd2aa33",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/master/encoder"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "encoder_preprocess.py",
                "sha": "dac3e145515d573e84972488013de1552cbc424d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder_preprocess.py"
                    }
                },
                "size": 3340
            },
            {
                "type": "code",
                "name": "encoder_train.py",
                "sha": "7d70f636bed67de94b2f55a199aad7f9f4a38473",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/encoder_train.py"
                    }
                },
                "size": 2398
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "f0c24bfe22b275f8b81c8ad61df318e843dd71e6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/requirements.txt"
                    }
                },
                "size": 562
            },
            {
                "type": "code",
                "name": "samples",
                "sha": "aaf0a54dfb4f20e0368a7adfe17919ec9829443e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/master/samples"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "synthesizer",
                "sha": "36daefe347a137f16aafd7d574d0abae62f9280e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/master/synthesizer"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "synthesizer_preprocess_audio.py",
                "sha": "4ac9071dd76d7d97de32c1c420b6bc27bbcd3673",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer_preprocess_audio.py"
                    }
                },
                "size": 2372
            },
            {
                "type": "code",
                "name": "synthesizer_preprocess_embeds.py",
                "sha": "84dd86a61e16ad07f74f82e27907752230fdefcc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer_preprocess_embeds.py"
                    }
                },
                "size": 1181
            },
            {
                "type": "code",
                "name": "synthesizer_train.py",
                "sha": "a01b4fd6596549e6ca6da9e81894bb29439cae23",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/synthesizer_train.py"
                    }
                },
                "size": 1750
            },
            {
                "type": "code",
                "name": "toolbox",
                "sha": "66baad44f149b5dd52261915013ab83d759af4ed",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/master/toolbox"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "utils",
                "sha": "3a6022f13d2ea514abde2c6aaed1646bb776cd6a",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/master/utils"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "vocoder",
                "sha": "61c71c70f540d7716248e5b954bd54e4b8866285",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/master/vocoder"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "vocoder_preprocess.py",
                "sha": "32a4d450aa6c935d06caea0973bc4688509c1dbc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/vocoder_preprocess.py"
                    }
                },
                "size": 2200
            },
            {
                "type": "code",
                "name": "vocoder_train.py",
                "sha": "2a90cfd48378921e30f83de6733d405be53c6e02",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/vocoder_train.py"
                    }
                },
                "size": 2825
            }
        ]
    },
    "authors": [
        {
            "name": "Corentin Jemine",
            "email": "corentin.jemine@gmail.com",
            "github_id": "CorentinJ"
        },
        {
            "name": "Alex Peattie",
            "email": "alexpeattie@gmail.com",
            "github_id": "alexpeattie"
        },
        {
            "name": "Matheus Fillipe",
            "email": "matheusfillipeag@gmail.com",
            "github_id": "matheusfillipe"
        },
        {
            "name": "Chris Van Pelt",
            "email": "vanpelt@wandb.com",
            "github_id": "vanpelt"
        },
        {
            "name": "Dalei Li",
            "email": "dalei.li@icloud.com",
            "github_id": "lidalei"
        },
        {
            "name": "Mathias Gatti",
            "email": "mathigatti@gmail.com",
            "github_id": "mathigatti"
        },
        {
            "name": "Niwala",
            "github_id": "Niwala"
        },
        {
            "name": "Pat",
            "github_id": "cforcomputer"
        },
        {
            "name": "Rami Mouro",
            "github_id": "ramalamadingdong"
        },
        {
            "name": "Rancoud",
            "github_id": "rancoud"
        },
        {
            "name": "Rishi R",
            "github_id": "Rishi0812"
        },
        {
            "name": "Sven Eschlbeck",
            "github_id": "sveneschlbeck"
        },
        {
            "name": "Tomcattwo",
            "email": "colt.coltrain@gmail.com",
            "github_id": "Tomcattwo"
        },
        {
            "name": "ak9250",
            "github_id": "ak9250"
        },
        {
            "name": "Christian Clauss",
            "email": "cclauss@me.com",
            "github_id": "cclauss"
        },
        {
            "name": "CaraDuf",
            "github_id": "Ca-ressemble-a-du-fake"
        }
    ],
    "tags": [
        "deep-learning",
        "pytorch",
        "tensorflow",
        "tts",
        "voice-cloning",
        "python"
    ],
    "description": "Clone a voice in 5 seconds to generate arbitrary speech in real-time",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning",
            "stars": 34081,
            "issues": true,
            "readme": "# Real-Time Voice Cloning\nThis repository is an implementation of [Transfer Learning from Speaker Verification to\nMultispeaker Text-To-Speech Synthesis](https://arxiv.org/pdf/1806.04558.pdf) (SV2TTS) with a vocoder that works in real-time. This was my [master's thesis](https://matheo.uliege.be/handle/2268.2/6801).\n\nSV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.\n\n**Video demonstration** (click the picture):\n\n[![Toolbox demo](https://i.imgur.com/8lFUlgz.png)](https://www.youtube.com/watch?v=-O_hYhToKoA)\n\n\n\n### Papers implemented  \n| URL | Designation | Title | Implementation source |\n| --- | ----------- | ----- | --------------------- |\n|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS** | **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis** | This repo |\n|[1802.08435](https://arxiv.org/pdf/1802.08435.pdf) | WaveRNN (vocoder) | Efficient Neural Audio Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n|[1703.10135](https://arxiv.org/pdf/1703.10135.pdf) | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)\n|[1710.10467](https://arxiv.org/pdf/1710.10467.pdf) | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | This repo |\n\n## News\n**10/01/22**: I recommend checking out [CoquiTTS](https://github.com/coqui-ai/tts). It's a good and up-to-date TTS repository targeted for the ML community. It can also do voice cloning and more, such as cross-language cloning or voice conversion.\n\n**28/12/21**: I've done a [major maintenance update](https://github.com/CorentinJ/Real-Time-Voice-Cloning/pull/961). Mostly, I've worked on making setup easier. Find new instructions in the section below.\n\n**14/02/21**: This repo now runs on PyTorch instead of Tensorflow, thanks to the help of @bluefish.\n\n**13/11/19**: I'm now working full time and I will rarely maintain this repo anymore. To anyone who reads this:\n- **If you just want to clone your voice (and not someone else's):** I recommend our free plan on [Resemble.AI](https://www.resemble.ai/). You will get a better voice quality and less prosody errors.\n- **If this is not your case:** proceed with this repository, but you might end up being disappointed by the results. If you're planning to work on a serious project, my strong advice: find another TTS repo. Go [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/364) for more info.\n\n**20/08/19:** I'm working on [resemblyzer](https://github.com/resemble-ai/Resemblyzer), an independent package for the voice encoder (inference only). You can use your trained encoder models from this repo with it.\n\n\n## Setup\n\n### 1. Install Requirements\n1. Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.\n2. Python 3.7 is recommended. Python 3.5 or greater should work, but you'll probably have to tweak the dependencies' versions. I recommend setting up a virtual environment using `venv`, but this is optional.\n3. Install [ffmpeg](https://ffmpeg.org/download.html#get-packages). This is necessary for reading audio files.\n4. Install [PyTorch](https://pytorch.org/get-started/locally/). Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.\n5. Install the remaining requirements with `pip install -r requirements.txt`\n\n### 2. (Optional) Download Pretrained Models\nPretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models).\n\n### 3. (Optional) Test Configuration\nBefore you download any dataset, you can begin by testing your configuration with:\n\n`python demo_cli.py`\n\nIf all tests pass, you're good to go.\n\n### 4. (Optional) Download Datasets\nFor playing with the toolbox alone, I only recommend downloading [`LibriSpeech/train-clean-100`](https://www.openslr.org/resources/12/train-clean-100.tar.gz). Extract the contents as `<datasets_root>/LibriSpeech/train-clean-100` where `<datasets_root>` is a directory of your choosing. Other datasets are supported in the toolbox, see [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets). You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.\n\n### 5. Launch the Toolbox\nYou can then try the toolbox:\n\n`python demo_toolbox.py -d <datasets_root>`  \nor  \n`python demo_toolbox.py`  \n\ndepending on whether you downloaded any datasets. If you are running an X-server or if you have the error `Aborted (core dumped)`, see [this issue](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590).\n",
            "readme_url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Generalized End-to-End Loss for Speaker Verification",
            "arxiv": "1710.10467",
            "year": 2017,
            "url": "http://arxiv.org/abs/1710.10467v5",
            "abstract": "In this paper, we propose a new loss function called generalized end-to-end\n(GE2E) loss, which makes the training of speaker verification models more\nefficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike\nTE2E, the GE2E loss function updates the network in a way that emphasizes\nexamples that are difficult to verify at each step of the training process.\nAdditionally, the GE2E loss does not require an initial stage of example\nselection. With these properties, our model with the new loss function\ndecreases speaker verification EER by more than 10%, while reducing the\ntraining time by 60% at the same time. We also introduce the MultiReader\ntechnique, which allows us to do domain adaptation - training a more accurate\nmodel that supports multiple keywords (i.e. \"OK Google\" and \"Hey Google\") as\nwell as multiple dialects.",
            "authors": [
                "Li Wan",
                "Quan Wang",
                "Alan Papir",
                "Ignacio Lopez Moreno"
            ]
        },
        {
            "title": "Efficient Neural Audio Synthesis",
            "arxiv": "1802.08435",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.08435v2",
            "abstract": "Sequential models achieve state-of-the-art results in audio, visual and\ntextual domains with respect to both estimating the data distribution and\ngenerating high-quality samples. Efficient sampling for this class of models\nhas however remained an elusive problem. With a focus on text-to-speech\nsynthesis, we describe a set of general techniques for reducing sampling time\nwhile maintaining high output quality. We first describe a single-layer\nrecurrent neural network, the WaveRNN, with a dual softmax layer that matches\nthe quality of the state-of-the-art WaveNet model. The compact form of the\nnetwork makes it possible to generate 24kHz 16-bit audio 4x faster than real\ntime on a GPU. Second, we apply a weight pruning technique to reduce the number\nof weights in the WaveRNN. We find that, for a constant number of parameters,\nlarge sparse networks perform better than small dense networks and this\nrelationship holds for sparsity levels beyond 96%. The small number of weights\nin a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile\nCPU in real time. Finally, we propose a new generation scheme based on\nsubscaling that folds a long sequence into a batch of shorter sequences and\nallows one to generate multiple samples at once. The Subscale WaveRNN produces\n16 samples per step without loss of quality and offers an orthogonal method for\nincreasing sampling efficiency.",
            "authors": [
                "Nal Kalchbrenner",
                "Erich Elsen",
                "Karen Simonyan",
                "Seb Noury",
                "Norman Casagrande",
                "Edward Lockhart",
                "Florian Stimberg",
                "Aaron van den Oord",
                "Sander Dieleman",
                "Koray Kavukcuoglu"
            ]
        },
        {
            "title": "Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis",
            "arxiv": "1806.04558",
            "year": 2018,
            "url": "http://arxiv.org/abs/1806.04558v4",
            "abstract": "We describe a neural network-based system for text-to-speech (TTS) synthesis\nthat is able to generate speech audio in the voice of many different speakers,\nincluding those unseen during training. Our system consists of three\nindependently trained components: (1) a speaker encoder network, trained on a\nspeaker verification task using an independent dataset of noisy speech from\nthousands of speakers without transcripts, to generate a fixed-dimensional\nembedding vector from seconds of reference speech from a target speaker; (2) a\nsequence-to-sequence synthesis network based on Tacotron 2, which generates a\nmel spectrogram from text, conditioned on the speaker embedding; (3) an\nauto-regressive WaveNet-based vocoder that converts the mel spectrogram into a\nsequence of time domain waveform samples. We demonstrate that the proposed\nmodel is able to transfer the knowledge of speaker variability learned by the\ndiscriminatively-trained speaker encoder to the new task, and is able to\nsynthesize natural speech from speakers that were not seen during training. We\nquantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we\nshow that randomly sampled speaker embeddings can be used to synthesize speech\nin the voice of novel speakers dissimilar from those used in training,\nindicating that the model has learned a high quality speaker representation.",
            "authors": [
                "Ye Jia",
                "Yu Zhang",
                "Ron J. Weiss",
                "Quan Wang",
                "Jonathan Shen",
                "Fei Ren",
                "Zhifeng Chen",
                "Patrick Nguyen",
                "Ruoming Pang",
                "Ignacio Lopez Moreno",
                "Yonghui Wu"
            ]
        },
        {
            "title": "Tacotron: Towards End-to-End Speech Synthesis",
            "arxiv": "1703.10135",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.10135v2",
            "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such\nas a text analysis frontend, an acoustic model and an audio synthesis module.\nBuilding these components often requires extensive domain expertise and may\ncontain brittle design choices. In this paper, we present Tacotron, an\nend-to-end generative text-to-speech model that synthesizes speech directly\nfrom characters. Given <text, audio> pairs, the model can be trained completely\nfrom scratch with random initialization. We present several key techniques to\nmake the sequence-to-sequence framework perform well for this challenging task.\nTacotron achieves a 3.82 subjective 5-scale mean opinion score on US English,\noutperforming a production parametric system in terms of naturalness. In\naddition, since Tacotron generates speech at the frame level, it's\nsubstantially faster than sample-level autoregressive methods.",
            "authors": [
                "Yuxuan Wang",
                "RJ Skerry-Ryan",
                "Daisy Stanton",
                "Yonghui Wu",
                "Ron J. Weiss",
                "Navdeep Jaitly",
                "Zongheng Yang",
                "Ying Xiao",
                "Zhifeng Chen",
                "Samy Bengio",
                "Quoc Le",
                "Yannis Agiomyrgiannakis",
                "Rob Clark",
                "Rif A. Saurous"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Librispeech"
            }
        ]
    },
    "domain": {
        "domain_type": "Speech",
        "domain_prob": 0.987879219020366
    }
}