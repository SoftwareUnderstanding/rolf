{
    "visibility": {
        "visibility": "public",
        "license": "GNU General Public License v3.0"
    },
    "name": "Variational-AutoEncoder For Novelty Detection",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "LordAlucard90",
                "owner_type": "User",
                "name": "Variational-AutoEncoder-For-Novelty-Detection",
                "url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection",
                "stars": 9,
                "pushed_at": "2018-07-15 14:07:49+00:00",
                "created_at": "2018-06-22 18:56:49+00:00",
                "language": "Python",
                "description": "A Variational AutoEncoder implemented with Keras and used to perform Novelty Detection with the EMNIST-Letters Dataset. ",
                "license": "GNU General Public License v3.0",
                "frameworks": [
                    "Keras",
                    "scikit-learn",
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "a1622c4c78c737af3fe187b934c368b8ea50a376",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection/blob/master/.gitignore"
                    }
                },
                "size": 768
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "94a9ed024d3859793618152ea559a168bbcbb5e2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection/blob/master/LICENSE"
                    }
                },
                "size": 35147
            },
            {
                "type": "code",
                "name": "experiment.py",
                "sha": "adf1c9bfb833ad2509da1a0d87204b65c6436654",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection/blob/master/experiment.py"
                    }
                },
                "size": 731
            },
            {
                "type": "code",
                "name": "helper",
                "sha": "d82cf9ecb6c054f8a06d5fe3e404a688507d6e2e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection/tree/master/helper"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "imgs",
                "sha": "36a7c5b110f50180931fea633b5b69c799fe072c",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection/tree/master/imgs"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "indices.npy",
                "sha": "8a1bd8ff39490b6f174ec866a715b3180b9d9716",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection/blob/master/indices.npy"
                    }
                },
                "size": 578202
            },
            {
                "type": "code",
                "name": "results.npy",
                "sha": "b34f0f0d9210c5df0a580766e692855704060cfb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection/blob/master/results.npy"
                    }
                },
                "size": 159104
            },
            {
                "type": "code",
                "name": "saved",
                "sha": "10648760bb9d9924bcf0624e16f8f63a8d65c22f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection/tree/master/saved"
                    }
                },
                "num_files": 1
            }
        ]
    },
    "authors": [
        {
            "name": "Khaufra",
            "github_id": "LordAlucard90"
        }
    ],
    "tags": [
        "variational-autoencoder",
        "python3",
        "keras",
        "neural-network",
        "novelty-detection",
        "anomaly-detection",
        "oneclasssvm"
    ],
    "description": "A Variational AutoEncoder implemented with Keras and used to perform Novelty Detection with the EMNIST-Letters Dataset. ",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection",
            "stars": 9,
            "issues": true,
            "readme": "# Variational-AutoEncoder For Novelty Detection\n\n# Abstract\nUsing a `Variational AutoEncoder`[1], the generation process of data will be learned.\n\nUsing `mean squared error`, the difference between original data and the reconstrued one will be calculated and used to determine a threshold.\n\nThat `threshold` will be used to discriminate the regular data from the novelties ones.\n\nFinally the results will be compared with the `OneClass-SVM`[2].\n\n# Dataset\nFor the dataset has been used `EMNIST-Letters`[3] a set of 26 balanced classes (from A to Z) composed by images with 28x28 pixels.\n\nTo simulate a novelty dataset has been added to the first class (A) some examples taken from the others classes to achieve about 3% of impurities both in Train set and Test set.\n\n# Experiment Details\nHas been trained different autoencoders changing :\n- Reparametrization Trick sizes (see model bottom)\n- L2 regularization values for convolutional and dense layers\n- dropout values for dropout layers (see model.py)\n\nThe threshold has been chosen (in the Train set) from the ordered mean squared errors vector such that there are about 3% elements greater than the threshold.\n\n### Model\n![model](imgs/vae.png)\n\n## Results\n### Reconstruction error by changing R. Trick size\n![reparametrization trick](imgs/hids.png)\n\n### Reconstruction error by changing Regularization Values (R. Trick = 32)\n![regularization](imgs/H32.png)\n\n### Reconstruction error by changing Dropout Values (R. Trick = 32, Regularization = 0.001)\n![dropout](imgs/H32_R001.png)\n\n### Best Reconstruction Losses\nr. trick | reg. | dropout | loss  \n-------- | ---- | ------- | ----\n32 | 1e-05 | 0.1    | 144.07\n32 | None | None | 144.09\n32 | 0.0001 | 0.1   | 144.14\n32 | 1e-05 |  None | 144.22\n32 | 0.001 | 0.1 | 144.50\n\n### Best Reconstruction Example (R. Trick = 32, Regularization = 1e-05, Dropout = 0.1)\n![best reconstruction](imgs/best_rec.png)\n\n### Best F1-Scores\nr. trick | reg. | dropout | loss | precision | recall | f1-score \n-------- | ---- | ------- | ---- | --------- | ------ | --------\n32 | 1e-05 | 0.5    |  181.39  |  0.983  |  0.993  |  0.988  \n16 | 1e-05 | 0.7    |  190.79  |  0.983  |  0.991  |  0.987  \n2 | 0.1 | 0.7      |  234.33  |  0.979  |  0.990  |  0.984  \n2 | 0.001 | 0.5    |  213.25  |  0.980  |  0.988  |  0.984 \n64 | 0.0001 | 0.6   |  194.82  |  0.980  |  0.988  |  0.984 \n\n### Best OneClass-SVM\ngamma |  Precision | recall | f1-score\n----- |  --------- | ------ | --------\n0.1   |    0.9664  | 0.8988 |  0.9313 \n\n### Remarks\n- Because of the random components in the process the result should be cross-validated.\n- The model can represent a standard autoencoder, I don't have tested it.\n- I trained the models in Google Colab and saved the results in Google Drive for speed up the process.\n\n# Contents\n```\nhelper/       : helper to recreate the experiment\nimgs/         : images for this README\nsaved/        : best F1-score model weights\nexperiment.py : example to recreate experiment\nindices.npy   : my indices \nresults.npy   : my results\n```\n\n# Tools\n- Python 3.6\n- numpy 1.14.5\n- Tensorflow 1.8.0\n- Keras 2.1.6\n- matplotlib 2.2.2\n- sklearn 0.19.1\n\n# References\n- [1] [KINGMA, Diederik P.; WELLING, Max. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.](https://arxiv.org/pdf/1312.6114.pdf)\n- [2] [SCH\u00d6LKOPF, Bernhard, et al. Estimating the support of a high-dimensional distribution. Neural computation, 2001, 13.7: 1443-1471.](https://www.mitpressjournals.org/doi/pdfplus/10.1162/089976601750264965)\n- [3] [Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension of MNIST to handwritten letters](https://www.nist.gov/itl/iad/image-group/emnist-dataset)\n",
            "readme_url": "https://github.com/LordAlucard90/Variational-AutoEncoder-For-Novelty-Detection",
            "frameworks": [
                "Keras",
                "scikit-learn",
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Auto-Encoding Variational Bayes",
            "arxiv": "1312.6114",
            "year": 2013,
            "url": "http://arxiv.org/abs/1312.6114v10",
            "abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.",
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "MNIST"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.6560210638171854,
        "task": "Image Classification",
        "task_prob": 0.9486474220151162
    }
}