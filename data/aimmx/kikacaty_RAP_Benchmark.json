{
    "visibility": {
        "visibility": "public",
        "license": "Other"
    },
    "name": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "kikacaty",
                "owner_type": "User",
                "name": "RAP_Benchmark",
                "url": "https://github.com/kikacaty/RAP_Benchmark",
                "stars": 0,
                "pushed_at": "2021-11-21 00:38:26+00:00",
                "created_at": "2021-11-09 05:08:56+00:00",
                "language": "Python",
                "license": "Other",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "306db4fda1fea41ff79900b4aaf55ccd2b222e1b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/blob/main/.gitignore"
                    }
                },
                "size": 1334
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "c14f578420e1f6ff6f60ca714839d0ed329a1051",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/blob/main/LICENSE"
                    }
                },
                "size": 4065
            },
            {
                "type": "code",
                "name": "configs",
                "sha": "0841c3a5edca4958f5108b947400cfb7de77c7c5",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/configs"
                    }
                },
                "num_files": 28
            },
            {
                "type": "code",
                "name": "demo",
                "sha": "b707da8fc7a19201627c22e97e3e5af84d511ff6",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/demo"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "docker",
                "sha": "ba9428482619fb05f7c48b1030d46ba54e6e9441",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/docker"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "f9d8b6e3884c7ca9d05e664e1850e4f91b6579b2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/docs"
                    }
                },
                "num_files": 14
            },
            {
                "type": "code",
                "name": "init.sh",
                "sha": "511ad12839179d84f613d515c33c787a2431d89f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/blob/main/init.sh"
                    }
                },
                "size": 635
            },
            {
                "type": "code",
                "name": "local_configs",
                "sha": "9596a2cb5835681bd2e915afebf8e5abefc6d5a0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/local_configs"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "mmseg",
                "sha": "271a0112bc08f9c4d90b04f7d3250fe345c41893",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/mmseg"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "phy_exp",
                "sha": "083ed6ba22517675aad73977cd8c592e8c029df4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/phy_exp"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "pytest.ini",
                "sha": "9796e871e70c7c67345b1d6bcf708c0c82377a98",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/blob/main/pytest.ini"
                    }
                },
                "size": 293
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "6da5adea757ffc79ac35e544d4afe85c5f44a90d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/blob/main/requirements.txt"
                    }
                },
                "size": 83
            },
            {
                "type": "code",
                "name": "requirements",
                "sha": "e88cafc19e1b0a3a0c3d807631ed4a17881e97d4",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/requirements"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "resources",
                "sha": "8889b958d294948dccfdbd3425c1d7fbd151e82b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/resources"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "setup.cfg",
                "sha": "708fb4ce33fcf55bd448bc2234f38c1891ae0356",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/blob/main/setup.cfg"
                    }
                },
                "size": 427
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "2e69551b8f1b221f2fc645e52c6baf020bf5d5e8",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/blob/main/setup.py"
                    }
                },
                "size": 4756
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "e407c55ce4eceaa05819cf2f8c72e1594c9b7656",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/tests"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "tools",
                "sha": "13fcb350fc0a57cadc88c4f52bfd1bfc1d8228b0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kikacaty/RAP_Benchmark/tree/main/tools"
                    }
                },
                "num_files": 16
            }
        ]
    },
    "authors": [
        {
            "name": "Enze Xie",
            "email": "xieenze@hku.hk",
            "github_id": "xieenze"
        },
        {
            "name": "Yulong Cao",
            "email": "yulongc@umich.edu",
            "github_id": "kikacaty"
        },
        {
            "name": "Zhiding Yu",
            "email": "scutchrisding@gmail.com",
            "github_id": "Chrisding"
        },
        {
            "name": "ddonatien",
            "github_id": "ddonatien"
        },
        {
            "name": "xiaochaowei",
            "email": "xiaocw@umich.edu",
            "github_id": "xiaochaowei"
        },
        {
            "name": "Cyril Zakka",
            "github_id": "cyrilzakka"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/kikacaty/RAP_Benchmark",
            "stars": 0,
            "issues": true,
            "readme": "[![NVIDIA Source Code License](https://img.shields.io/badge/license-NSCL-blue.svg)](https://github.com/NVlabs/SegFormer/blob/master/LICENSE)\n![Python 3.8](https://img.shields.io/badge/python-3.8-green.svg)\n\n# SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n\n<!-- ![image](resources/image.png) -->\n<div align=\"center\">\n  <img src=\"./resources/image.png\" height=\"400\">\n</div>\n<p align=\"center\">\n  Figure 1: Performance of SegFormer-B0 to SegFormer-B5.\n</p>\n\n### [Project page](https://github.com/NVlabs/SegFormer) | [Paper](https://arxiv.org/abs/2105.15203) | [Demo (Youtube)](https://www.youtube.com/watch?v=J0MoRQzZe8U) | [Demo (Bilibili)](https://www.bilibili.com/video/BV1MV41147Ko/)\n\nSegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.<br>\n[Enze Xie](https://xieenze.github.io/), [Wenhai Wang](https://whai362.github.io/), [Zhiding Yu](https://chrisding.github.io/), [Anima Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/), [Jose M. Alvarez](https://rsu.data61.csiro.au/people/jalvarez/), and [Ping Luo](http://luoping.me/).<br>\nTechnical Report 2021.\n\nThis repository contains the official Pytorch implementation of training & evaluation code and the pretrained models for [SegFormer](https://arxiv.org/abs/2105.15203).\n\nSegFormer is a simple, efficient and powerful semantic segmentation method, as shown in Figure 1.\n\nWe use [MMSegmentation v0.13.0](https://github.com/open-mmlab/mmsegmentation/tree/v0.13.0) as the codebase.\n\n\ud83d\udd25\ud83d\udd25 SegFormer is on [MMSegmentation](https://github.com/open-mmlab/mmsegmentation/tree/master/configs/segformer). \ud83d\udd25\ud83d\udd25 \n\n\n## Installation\n\nFor install and data preparation, please refer to the guidelines in [MMSegmentation v0.13.0](https://github.com/open-mmlab/mmsegmentation/tree/v0.13.0).\n\nOther requirements:\n```pip install timm==0.3.2```\n\nAn example (works for me): ```CUDA 10.1``` and  ```pytorch 1.7.1``` \n\n```\npip install torchvision==0.8.2\npip install timm==0.3.2\npip install mmcv-full==1.2.7\npip install opencv-python==4.5.1.48\ncd SegFormer && pip install -e . --user\n```\n\n## Evaluation\n\nDownload [trained weights](https://drive.google.com/drive/folders/1GAku0G0iR9DsBxCbfENWMJ27c5lYUeQA?usp=sharing).\n\nExample: evaluate ```SegFormer-B1``` on ```ADE20K```:\n\n```\n# Single-gpu testing\npython tools/test.py local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py /path/to/checkpoint_file\n\n# Multi-gpu testing\n./tools/dist_test.sh local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py /path/to/checkpoint_file <GPU_NUM>\n\n# Multi-gpu, multi-scale testing\ntools/dist_test.sh local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py /path/to/checkpoint_file <GPU_NUM> --aug-test\n```\n\n## Training\n\nDownload [weights](https://drive.google.com/drive/folders/1b7bwrInTW4VLEm27YawHOAMSMikga2Ia?usp=sharing) pretrained on ImageNet-1K, and put them in a folder ```pretrained/```.\n\nExample: train ```SegFormer-B1``` on ```ADE20K```:\n\n```\n# Single-gpu training\npython tools/train.py local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py \n\n# Multi-gpu training\n./tools/dist_train.sh local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py <GPU_NUM>\n```\n\n## Visualize\n\nHere is a demo script to test a single image. More details refer to [MMSegmentation's Doc](https://mmsegmentation.readthedocs.io/en/latest/get_started.html).\n\n```shell\npython demo/image_demo.py ${IMAGE_FILE} ${CONFIG_FILE} ${CHECKPOINT_FILE} [--device ${DEVICE_NAME}] [--palette-thr ${PALETTE}]\n```\n\nExample: visualize ```SegFormer-B1``` on ```CityScapes```: \n\n```shell\npython demo/image_demo.py demo/demo.png local_configs/segformer/B1/segformer.b1.512x512.ade.160k.py \\\n/path/to/checkpoint_file --device cuda:0 --palette cityscapes\n```\n\n\n\n\n\n## License\nPlease check the LICENSE file. SegFormer may be used non-commercially, meaning for research or \nevaluation purposes only. For business inquiries, please contact \n[researchinquiries@nvidia.com](mailto:researchinquiries@nvidia.com).\n\n\n## Citation\n```\n@article{xie2021segformer,\n  title={SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers},\n  author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},\n  journal={arXiv preprint arXiv:2105.15203},\n  year={2021}\n}\n```\n",
            "readme_url": "https://github.com/kikacaty/RAP_Benchmark",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
            "arxiv": "2105.15203",
            "year": 2021,
            "url": "http://arxiv.org/abs/2105.15203v3",
            "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation\nframework which unifies Transformers with lightweight multilayer perception\n(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a\nnovel hierarchically structured Transformer encoder which outputs multiscale\nfeatures. It does not need positional encoding, thereby avoiding the\ninterpolation of positional codes which leads to decreased performance when the\ntesting resolution differs from training. 2) SegFormer avoids complex decoders.\nThe proposed MLP decoder aggregates information from different layers, and thus\ncombining both local attention and global attention to render powerful\nrepresentations. We show that this simple and lightweight design is the key to\nefficient segmentation on Transformers. We scale our approach up to obtain a\nseries of models from SegFormer-B0 to SegFormer-B5, reaching significantly\nbetter performance and efficiency than previous counterparts. For example,\nSegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x\nsmaller and 2.2% better than the previous best method. Our best model,\nSegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows\nexcellent zero-shot robustness on Cityscapes-C. Code will be released at:\ngithub.com/NVlabs/SegFormer.",
            "authors": [
                "Enze Xie",
                "Wenhai Wang",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M. Alvarez",
                "Ping Luo"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "Caltech"
            },
            {
                "name": "ADE20K"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.7948446750849532,
        "task": "Semantic Segmentation",
        "task_prob": 0.9681621821693577
    }
}