{
    "visibility": {
        "visibility": "public"
    },
    "name": "Deep-Learning-Lab",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "KrishnaTarun",
                "owner_type": "User",
                "name": "Deep-Learning-Lab",
                "url": "https://github.com/KrishnaTarun/Deep-Learning-Lab",
                "stars": 1,
                "pushed_at": "2019-06-04 12:43:51+00:00",
                "created_at": "2019-06-03 01:10:05+00:00",
                "language": "TeX",
                "description": "Lab assignment for Deep Learning course.",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "498ab2a29fb11b09de27d3c8b85d1ba8695b080d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/KrishnaTarun/Deep-Learning-Lab/blob/master/.gitignore"
                    }
                },
                "size": 276
            },
            {
                "type": "code",
                "name": "assignment_1",
                "sha": "1824566d1d0fcb10c24489e4f08766253e767f32",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/KrishnaTarun/Deep-Learning-Lab/tree/master/assignment_1"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "assignment_2",
                "sha": "79896f1154f80d0d0d11fd78338fb292e6295c45",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/KrishnaTarun/Deep-Learning-Lab/tree/master/assignment_2"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "assignment_3",
                "sha": "39086f544810e62ea1762eb70058a84ed6017c7b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/KrishnaTarun/Deep-Learning-Lab/tree/master/assignment_3"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "poster_presentation",
                "sha": "b2d53e55decd091266a3e054abd023154e06c56d",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/KrishnaTarun/Deep-Learning-Lab/tree/master/poster_presentation"
                    }
                },
                "num_files": 1
            }
        ]
    },
    "authors": [
        {
            "name": "Tarun Krishna",
            "github_id": "KrishnaTarun"
        }
    ],
    "tags": [],
    "description": "Lab assignment for Deep Learning course.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/KrishnaTarun/Deep-Learning-Lab",
            "stars": 1,
            "issues": true,
            "readme": "# Deep-Learning-Lab\n\n## Description\n\nCode for the labs of the [Deep Learning course](https://uvadlc.github.io/) offered in MSc. in Artificial Intelligence at the University of Amsterdam.\n\n### Assignment 1 - MLPs, CNNs and Backpropagation\n<p align=\"justify\">\nWe explore image classification with two neural network architectures: multi-layer perceptrons (MLP) and convolutional neural networks (CNN). To gain an in-depth understanding we first focus on a basic implementation of a MLP in numpy and Pytorch. Apart from that we also learn to implement custom operations in PyTorch by re-implementing a batch-normalization layer. \n</p>\n\n#### Problems and Solutions\n- [Main Problem statement](assignment_1/assignment_1.pdf), [Python code](assignment_1/code) and [report](assignment_1/Report_1.pdf)\n\n### Assignment 2 - Recurrent Neural Networks\n\n<p align=\"justify\">\nWe explore the long-term dependency modelling capabilities of Recurrent Neural Networks (RNNs) and Long Short-Term Networks (LSTMs). In the first part of the assignment we experiment on a simple palindrome task. By comparing the perfomnace of Vanilla RNN with LSTMs as the LSTM is more capable of learning long-term dependencies than the vanilla RNN. In the second part of the assignment (Part3) we use LSTMs as Genrative model to predict the next character in the the sentence.\n</p>\n\n#### Problems and Solutions\n- [Main Problem statement](assignment_2/assignment_2.pdf), [Python code part1](assignment_2/part1), [Python code part3](assignment_2/part3) and [report](assignment_2/Report_2.pdf)\n\n### Assignment 3 - Deep Generative Models\n<p align=\"justify\">\nWe explore deep generative modelling with two\u2014currently most popular\u2014deep generative models, namely [Variational Auto Encoders](https://arxiv.org/abs/1312.6114) and [Generative Adversarial Networks](https://papers.nips.cc/paper/5423-generative-adversarial-nets). \n</p>\n\n#### Problems and Solutions\n- [Main Problem statement](assignment_3/assignment_3.pdf), [Python code](assignment_3/code) and [report](assignment_3/Report_3.pdf)\n\n#### Results\n<p align=\"center\">\n  <img src=\"assignment_3/code/results/manifold.png\" width=\"500\" /><br />\n  <i>Manifold learned by the VAE</i>\n  <br />\n  <br />\n  <img src=\"assignment_3/code/interpolation/interpolated_5.png\" width=\"500\" /><br />\n  <i>Interpolation results from GAN </i>\n</p>\n\n### Poster Presentations\nWe presented [Attention is all you Need](https://arxiv.org/abs/1706.03762) paper by Vaswani  et al. This was done in collaboration with [Vikrant Yadav](https://www.linkedin.com/in/vikrant-yadav-98134496/) and [Dhruba Pujary](https://www.linkedin.com/in/dhruba-pujary-894a2253/).\n\nThe poster can be found [here](poster_presentation/poster.pdf).\n\n",
            "readme_url": "https://github.com/KrishnaTarun/Deep-Learning-Lab",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Attention Is All You Need",
            "arxiv": "1706.03762",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.03762v5",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ]
        },
        {
            "title": "Auto-Encoding Variational Bayes",
            "arxiv": "1312.6114",
            "year": 2013,
            "url": "http://arxiv.org/abs/1312.6114v10",
            "abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.",
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9975711837456056,
        "task": "Machine Translation",
        "task_prob": 0.949477526793262
    }
}