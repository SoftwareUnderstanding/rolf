{
    "visibility": {
        "visibility": "public"
    },
    "name": "References !!!",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "ZhouYuxuanYX",
                "owner_type": "User",
                "name": "Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting",
                "url": "https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting",
                "stars": 44,
                "pushed_at": "2019-08-08 11:10:29+00:00",
                "created_at": "2018-10-06 10:03:24+00:00",
                "language": "Python",
                "description": "Sequence to Sequence Model based on Wavenet instead of LSTM implemented in Keras",
                "frameworks": [
                    "Keras"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "894a44cc066a027465cd26d634948d56d13af9af",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting/blob/master/.gitignore"
                    }
                },
                "size": 1203
            },
            {
                "type": "code",
                "name": "LSTM.py",
                "sha": "c22128433ef452bdc5597d1090b33c9db1f8c4df",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting/blob/master/LSTM.py"
                    }
                },
                "size": 3988
            },
            {
                "type": "code",
                "name": "Preprocessing.py",
                "sha": "e5e7b6219952a3a74becd1457fd8ba9feeb1d02d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting/blob/master/Preprocessing.py"
                    }
                },
                "size": 2710
            },
            {
                "type": "code",
                "name": "Wavenet.py",
                "sha": "795c9dd929e7a973d5149a35e6ca37931be9e4cf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting/blob/master/Wavenet.py"
                    }
                },
                "size": 11448
            },
            {
                "type": "code",
                "name": "figures",
                "sha": "edc4f324e889b35a77ab9a8a595e076649821512",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting/tree/master/figures"
                    }
                },
                "num_files": 10
            }
        ]
    },
    "authors": [
        {
            "name": "Yuxuan Zhou",
            "github_id": "ZhouYuxuanYX"
        }
    ],
    "tags": [],
    "description": "Sequence to Sequence Model based on Wavenet instead of LSTM implemented in Keras",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting",
            "stars": 44,
            "issues": true,
            "readme": "# References !!!\nSorry for not having stated the references in the beginning. I initially used the github for self-practice.\n\nhttps://github.com/JEddy92/TimeSeries_Seq2Seq\n\nOord, Aaron van den, et al. \"Wavenet: A generative model for raw audio.\" arXiv preprint arXiv:1609.03499 (2016).\n\n# Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting\nSequence to Sequence Model based on Wavenet instead of LSTM implemented in Keras\n\n# Web Traffic Forecasting\nTo download the data and know more about the competition, see [here](https://www.kaggle.com/c/web-traffic-time-series-forecasting/kernels?sortBy=voteCount&group=everyone&pageSize=20&competitionId=6768)\n\n## Competition Goal\nThe training dataset consists of approximately 145k time series.  Each of these time series represents a number of daily views of a different Wikipedia article, starting from July 1st, 2015 up until September 10th, 2017. The goal is to forecast the daily views between September 13th, 2017 and November 13th, 2017 for each article in the dataset.\n\nThe evaluation metric for the competition is symmetric mean absolute percentage error (SMAPE), but here we simply adopt mean absolute error(MAE) as loss function.\n\n\n## Introduction to Wavenet\nThe model architecture is similar to WaveNet, consisting of a stack of dilated causal convolutions, as demonstrated in the [diagram](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) below. For more details, see van den Oord's [paper](https://arxiv.org/abs/1609.03499).\n\n<p align=\"center\">\n  <img src=\"figures/wavenet.gif\">\n\n</p>\n\n**Causal Convolution**:\n\nThe figure below shows a causal structure, which guarantees that the current time step is only influenced by the previous time steps. Then an expression of the conditional probability could be established. That is to say, we assume that the current value is conditioned on the previous values in a time sequence. \n\n\n<p align=\"center\">\n  <img src=\"figures/WaveNet_causalconv.png\">\n\n</p>\n\n**Dilated Convolution**:\n\nBut as can be seen, the reception field is quite small with a limited number of stacks, and it results in poor performance handling long-term dependencies. So the idea of dilated convolution is employed. In a dilated convolution layer, filters are not applied to inputs in a simple sequential manner, but instead skip a constant dilation rate inputs in between each of the inputs they process, as in the WaveNet diagram below. By increasing the dilation rate multiplicatively at each layer (e.g. 1, 2, 4, 8, \u2026), we can achieve the exponential relationship between layer depth and receptive field size that we desire. The figure below ilustrates the effect of dilation.\n\n<p align=\"center\">\n  <img src=\"figures/WaveNet_dilatedconv.png\">\n\n</p>\n\n## Introduction to Sequence-to-Sequence Model\n\n**RNN based seq2seq model**:\n\nA seq2seq model is mainly used in NLP tasks such as machine translation and often based on LSTM or GRU structure. It has encoder, decoder and intermediate step as its main components, mapping an arbitrarily long input sequence to an arbitrarily long output sequence with an intermediate encoded state.:\n\n<p align=\"center\">\n  <img src=\"figures/seq2seq.png\">\n\n</p>\n\nIn comparison to fully connected feed forward neural networks, recurrent neural networks has no longer the requirement a fixed-sized input and considers naturally the relation between previous and current time steps. In addition, LSTM or GRU are advanced RNN structures, which increase the ability of capturing long-term dependencies, by forcing a approximately constant back-propagation error flow during training.\n\nHowever, due to the recurrent calculation for each time step, parrellelization is impossible for training theses networks. And it's a big disadvantage in the big data era. Even the input time range for a LSTM can not be arbitrary long in reality, and it is in fact severly limited by the training mechanism of RNN.\n\n**Wavenet based approach**:\n\nWith Wavenet, the training procedure for all the time steps in the input can be parrellelized. We just let the output sequence be one time step ahead of the input sequence, and at every time step of the output, the value is only influenced by the previous steps in the input.\n\nAs for the inference stage, it yields every time only the prediction one step ahead as in the LSTM approach. But we don't need to define a distinct model for inferencing here. In each Iteration, the last point of the output sequence is selected as the prediction one step ahead of the previous iteration, and it is in turn concatenated to the input sequence, in order to predict one step further in the future. \n\n# About this Project\n\nInpired from the core ideas of Wavenet: dilated causal convolution, a simpler version of it is implemented in Keras in my Project, disregarding the residual blocks used in the original paper, which is mainly employed to make deep neural networks easier to train. And this is not problem here for my project.\n\nAnd there are some crucial factors affecting the model performance:\n\n## Kernel Size: \n\nConvolutional Neural Networks are able to extract local features, which might be shared globally. And the kernel size of a convolutional filter represents our belief on these low level local features of a particular kind of data.\n\nIn the context of time series data, correlations between data points could be a major consideration for choosing the kernel size. Consider the following two extreme cases:\n* if the data point at each time step is uncorrelated with each other, then a kernel size of 1 might be sufficent\n* if the data points within for example 5 time steps show a strong correlation, a kernel size of 5 should be at least tested.\n\n\n",
            "readme_url": "https://github.com/ZhouYuxuanYX/Wavenet-in-Keras-for-Kaggle-Competition-Web-Traffic-Time-Series-Forecasting",
            "frameworks": [
                "Keras"
            ]
        }
    ],
    "references": [
        {
            "title": "WaveNet: A Generative Model for Raw Audio",
            "arxiv": "1609.03499",
            "year": 2016,
            "url": "http://arxiv.org/abs/1609.03499v2",
            "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition.",
            "authors": [
                "Aaron van den Oord",
                "Sander Dieleman",
                "Heiga Zen",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Alex Graves",
                "Nal Kalchbrenner",
                "Andrew Senior",
                "Koray Kavukcuoglu"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "Wikipedia"
            }
        ]
    },
    "domain": {
        "domain_type": "Speech",
        "domain_prob": 0.9776689967348474
    }
}