{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Deep learning networks",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "osmr",
                "owner_type": "User",
                "name": "imgclsmob",
                "url": "https://github.com/osmr/imgclsmob",
                "stars": 2511,
                "pushed_at": "2021-12-14 10:22:55+00:00",
                "created_at": "2018-07-09 12:57:46+00:00",
                "language": "Python",
                "description": "Sandbox for training deep learning networks",
                "license": "MIT License",
                "frameworks": [
                    "Keras",
                    "MXNet",
                    "PyTorch",
                    "TensorFlow",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "02b5a079b0f1b71492eb82c1cd3367a701fa6db0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/.gitignore"
                    }
                },
                "size": 1404
            },
            {
                "type": "code",
                "name": ".travis.yml",
                "sha": "08e9ab4d25422a854a7a1e27c7de9e3baec9d77c",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/.travis.yml"
                    }
                },
                "size": 1146
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "f976a662a79c330c6926a2ee8f74bdd1f60a64ab",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/LICENSE"
                    }
                },
                "size": 1074
            },
            {
                "type": "code",
                "name": "__init__.py",
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/__init__.py"
                    }
                },
                "size": 0
            },
            {
                "type": "code",
                "name": "chainer_",
                "sha": "dc67a1c6f20565e2aab74f0117b13019f50921dd",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/chainer_"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "common",
                "sha": "f6821ea98262a5b063c4fd4d65fae6aec4d8fa3b",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/common"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "convert_models.py",
                "sha": "d3b743be02b1e3c57c11c482699a1440c12daa44",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/convert_models.py"
                    }
                },
                "size": 87933
            },
            {
                "type": "code",
                "name": "deploy",
                "sha": "41772db296d95d44f132cfae378ed088f600b4fb",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/deploy"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "eval_ch.py",
                "sha": "35369f905a8580ac1935e94e3c8d69a75fc644b6",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/eval_ch.py"
                    }
                },
                "size": 9650
            },
            {
                "type": "code",
                "name": "eval_gl.py",
                "sha": "96d5387c111e6bf962cffcc5c9a2e0b8a878009f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/eval_gl.py"
                    }
                },
                "size": 11941
            },
            {
                "type": "code",
                "name": "eval_gl_det.py",
                "sha": "2264654b77da3188ce7d25a35e13ecaa6e7e3ae1",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/eval_gl_det.py"
                    }
                },
                "size": 11410
            },
            {
                "type": "code",
                "name": "eval_ke.py",
                "sha": "f71801af019ea004db2031fbf73a7074a38968cc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/eval_ke.py"
                    }
                },
                "size": 6665
            },
            {
                "type": "code",
                "name": "eval_pt.py",
                "sha": "d5f3a713cd0f3a82b5c360f5812de7d9071a3475",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/eval_pt.py"
                    }
                },
                "size": 13989
            },
            {
                "type": "code",
                "name": "eval_tf.py",
                "sha": "bf397eaba1ef9c78fb9a531a11034123fb1047ab",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/eval_tf.py"
                    }
                },
                "size": 5824
            },
            {
                "type": "code",
                "name": "eval_tf2.py",
                "sha": "659dc077184fa456564d512b2886ebbb4f128af5",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/eval_tf2.py"
                    }
                },
                "size": 9076
            },
            {
                "type": "code",
                "name": "examples",
                "sha": "1009c56d2f7368a5580a705cc88bb104f665c636",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/examples"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "gluon",
                "sha": "7e25ac19b58d70ea0992cc94129c0abf4e6b082e",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/gluon"
                    }
                },
                "num_files": 15
            },
            {
                "type": "code",
                "name": "keras_",
                "sha": "57f275e44c3d42e98ccfb1ca6266b9bb59e1626f",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/keras_"
                    }
                },
                "num_files": 7
            },
            {
                "type": "code",
                "name": "load_model.py",
                "sha": "44ec1a8151424d1a3f52b2ad9eab3964f93e60b0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/load_model.py"
                    }
                },
                "size": 1326
            },
            {
                "type": "code",
                "name": "other",
                "sha": "ef63d39341726eac21cd7702cf8447bffb9fb6d0",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/other"
                    }
                },
                "num_files": 17
            },
            {
                "type": "code",
                "name": "prep_model.py",
                "sha": "cd886885297f51e6ddfc16026f6b397099230b38",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/prep_model.py"
                    }
                },
                "size": 9068
            },
            {
                "type": "code",
                "name": "pytorch",
                "sha": "25ac283d7500a0e7f6695d01468488eba4661b96",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/pytorch"
                    }
                },
                "num_files": 11
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "be63c54205ef63538f81d87827e7a149232b1ab3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/requirements.txt"
                    }
                },
                "size": 284
            },
            {
                "type": "code",
                "name": "sotabench.py",
                "sha": "2978c785b88c734c1b9054ef623666faf39c1dbb",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/sotabench.py"
                    }
                },
                "size": 1645
            },
            {
                "type": "code",
                "name": "tensorflow2",
                "sha": "8e53982d64ef0027c2595626af0510e7165843c2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/tensorflow2"
                    }
                },
                "num_files": 10
            },
            {
                "type": "code",
                "name": "tensorflow_",
                "sha": "6b782860b9de0fe24787f613714f7b38dc975a49",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/tensorflow_"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "1fa9fe5e5f9db59bcd10e3b3b7cc5c5af36260ba",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/tree/master/tests"
                    }
                },
                "num_files": 18
            },
            {
                "type": "code",
                "name": "train_ch.py",
                "sha": "db1f8e32c5b63d51beb74c264007905a1dde2fa4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/train_ch.py"
                    }
                },
                "size": 9406
            },
            {
                "type": "code",
                "name": "train_gl.py",
                "sha": "6a273f10362b744491e59bf992cd03b89f44cef3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/train_gl.py"
                    }
                },
                "size": 29174
            },
            {
                "type": "code",
                "name": "train_gl_mealv2.py",
                "sha": "e60fc5c57d648e8c42f589ef7b43b49a03a0831a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/train_gl_mealv2.py"
                    }
                },
                "size": 34563
            },
            {
                "type": "code",
                "name": "train_ke.py",
                "sha": "f76805a96e705e45927b2d3845a2e85d47c4adb3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/train_ke.py"
                    }
                },
                "size": 8801
            },
            {
                "type": "code",
                "name": "train_pt.py",
                "sha": "f74a21622409ae9fba21c08542aa3faea1cac0d7",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/train_pt.py"
                    }
                },
                "size": 21667
            },
            {
                "type": "code",
                "name": "train_tf.py",
                "sha": "a4b97a60b324a45719828d23e68c7eee1594bedd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/train_tf.py"
                    }
                },
                "size": 7858
            },
            {
                "type": "code",
                "name": "train_tf2.py",
                "sha": "65c4c979ca3b054d63745b84ec2eff25d8539e34",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/osmr/imgclsmob/blob/master/train_tf2.py"
                    }
                },
                "size": 8479
            }
        ]
    },
    "authors": [
        {
            "name": "Oleg S\u00e9mery",
            "email": "osemery@gmail.com",
            "github_id": "osmr"
        },
        {
            "name": "ruro",
            "email": "ruro.ruro@ya.ru",
            "github_id": "RuRo"
        },
        {
            "name": "ShoofLLC",
            "github_id": "ShoofLLC"
        }
    ],
    "tags": [
        "machine-learning",
        "deep-learning",
        "mxnet",
        "gluon",
        "pytorch",
        "classification",
        "imagenet",
        "neural-network",
        "image-classification",
        "chainer",
        "keras",
        "tensorflow",
        "pretrained-models",
        "cifar",
        "segmentation",
        "tensorflow2",
        "semantic-segmentation",
        "human-pose-estimation",
        "3d-face-reconstruction"
    ],
    "description": "Sandbox for training deep learning networks",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/osmr/imgclsmob",
            "stars": 2511,
            "issues": true,
            "readme": "# Deep learning networks\n\n[![Build Status](https://travis-ci.org/osmr/imgclsmob.svg?branch=master)](https://travis-ci.org/osmr/imgclsmob)\n[![GitHub License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![Python Version](https://img.shields.io/badge/python-2.7%2C3.6%2C3.7-lightgrey.svg)](https://github.com/osmr/imgclsmob)\n\nThis repo is used to research convolutional networks primarily for computer vision tasks. For this purpose, the repo\ncontains (re)implementations of various classification, segmentation, detection, and pose estimation models and scripts\nfor training/evaluating/converting.\n\nThe following frameworks are used:\n- MXNet/Gluon ([info](https://mxnet.apache.org)),\n- PyTorch ([info](https://pytorch.org)),\n- Chainer ([info](https://chainer.org)),\n- Keras ([info](https://keras.io)),\n- TensorFlow 1.x/2.x ([info](https://www.tensorflow.org)).\n\nFor each supported framework, there is a PIP-package containing pure models without auxiliary scripts. List of packages:\n- [gluoncv2](https://pypi.org/project/gluoncv2) for Gluon,\n- [pytorchcv](https://pypi.org/project/pytorchcv) for PyTorch,\n- [chainercv2](https://pypi.org/project/chainercv2) for Chainer,\n- [kerascv](https://pypi.org/project/kerascv) for Keras,\n- [tensorflowcv](https://pypi.org/project/tensorflowcv) for TensorFlow 1.x,\n- [tf2cv](https://pypi.org/project/tf2cv) for TensorFlow 2.x.\n\nCurrently, models are mostly implemented on Gluon and then ported to other frameworks. Some models are pretrained on\n[ImageNet-1K](http://www.image-net.org), [CIFAR-10/100](https://www.cs.toronto.edu/~kriz/cifar.html),\n[SVHN](http://ufldl.stanford.edu/housenumbers), [CUB-200-2011](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html),\n[Pascal VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012), [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K),\n[Cityscapes](https://www.cityscapes-dataset.com), and [COCO](http://cocodataset.org) datasets. All pretrained weights\nare loaded automatically during use. See examples of such automatic loading of weights in the corresponding sections of\nthe documentation dedicated to a particular package:\n- [Gluon models](gluon/README.md),\n- [PyTorch models](pytorch/README.md),\n- [Chainer models](chainer_/README.md),\n- [Keras models](keras_/README.md),\n- [TensorFlow 1.x models](tensorflow_/README.md),\n- [TensorFlow 2.x models](tensorflow2/README.md).\n\n## Installation\n\nTo use training/evaluating scripts as well as all models, you need to clone the repository and install dependencies:\n```\ngit clone git@github.com:osmr/imgclsmob.git\npip install -r requirements.txt\n```\n\n## Table of implemented classification models\n\nSome remarks:\n- `Repo` is an author repository, if it exists.\n- `a`, `b`, `c`, `d`, and `e` means the implementation of a model for ImageNet-1K, CIFAR-10, CIFAR-100, SVHN, and CUB-200-2011, respectively.\n- `A`, `B`, `C`, `D`, and `E` means having a pre-trained model for corresponding datasets.\n\n| Model | [Gluon](gluon/README.md) | [PyTorch](pytorch/README.md) | [Chainer](chainer_/README.md) | [Keras](keras_/README.md) | [TF](tensorflow_/README.md) | [TF2](tensorflow2/README.md) | Paper | Repo | Year |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| AlexNet | A | A | A | A | A | A | [link](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) | [link](https://code.google.com/archive/p/cuda-convnet2) | 2012 |\n| ZFNet | A | A | A | A | A | A | [link](https://arxiv.org/abs/1311.2901) | - | 2013 |\n| VGG | A | A | A | A | A | A | [link](https://arxiv.org/abs/1409.1556) | - | 2014 |\n| BN-VGG | A | A | A | A | A | A | [link](https://arxiv.org/abs/1409.1556) | - | 2015 |\n| BN-Inception | A | A | A | - | - | A | [link](https://arxiv.org/abs/1502.03167) | - | 2015 |\n| ResNet | ABCDE | ABCDE | ABCDE | A | A | ABCDE | [link](https://arxiv.org/abs/1512.03385) | [link](https://github.com/KaimingHe/deep-residual-networks) | 2015 |\n| PreResNet | ABCD | ABCD | ABCD | A | A | ABCD | [link](https://arxiv.org/abs/1603.05027) | [link](https://github.com/facebook/fb.resnet.torch) | 2016 |\n| ResNeXt | ABCD | ABCD | ABCD | A | A | ABCD | [link](http://arxiv.org/abs/1611.05431) | [link](https://github.com/facebookresearch/ResNeXt) | 2016 |\n| SENet | A | A | A | A | A | A | [link](https://arxiv.org/abs/1709.01507) | [link](https://github.com/hujie-frank/SENet) | 2017 |\n| SE-ResNet | ABCDE | ABCDE | ABCDE | A | A | ABCDE | [link](https://arxiv.org/abs/1709.01507) | [link](https://github.com/hujie-frank/SENet) | 2017 |\n| SE-PreResNet | ABCD | ABCD | ABCD | A | A | ABCD | [link](https://arxiv.org/abs/1709.01507) | [link](https://github.com/hujie-frank/SENet) | 2017 |\n| SE-ResNeXt | A | A | A | A | A | A | [link](https://arxiv.org/abs/1709.01507) | [link](https://github.com/hujie-frank/SENet) | 2017 |\n| ResNeSt(A) | A | A | A | - | - | A | [link](https://arxiv.org/abs/2004.08955) | [link](https://github.com/zhanghang1989/ResNeSt) | 2020 |\n| IBN-ResNet | A | A | - | - | - | A | [link](https://arxiv.org/abs/1807.09441) | [link](https://github.com/XingangPan/IBN-Net) | 2018 |\n| IBN-ResNeXt | A | A | - | - | - | A | [link](https://arxiv.org/abs/1807.09441) | [link](https://github.com/XingangPan/IBN-Net) | 2018 |\n| IBN-DenseNet | A | A | - | - | - | A | [link](https://arxiv.org/abs/1807.09441) | [link](https://github.com/XingangPan/IBN-Net) | 2018 |\n| AirNet | A | A | A | - | - | A | [link](https://ieeexplore.ieee.org/document/8510896) | [link](https://github.com/soeaver/AirNet-PyTorch) | 2018 |\n| AirNeXt | A | A | A | - | - | A | [link](https://ieeexplore.ieee.org/document/8510896) | [link](https://github.com/soeaver/AirNet-PyTorch) | 2018 |\n| BAM-ResNet | A | A | A | - | - | A | [link](https://arxiv.org/abs/1807.06514) | [link](https://github.com/Jongchan/attention-module) | 2018 |\n| CBAM-ResNet | A | A | A | - | - | A | [link](https://arxiv.org/abs/1807.06521) | [link](https://github.com/Jongchan/attention-module) | 2018 |\n| ResAttNet | a | a | a | - | - | - | [link](https://arxiv.org/abs/1704.06904) | [link](https://github.com/fwang91/residual-attention-network) | 2017 |\n| SKNet | a | a | a | - | - | - | [link](https://arxiv.org/abs/1903.06586) | [link](https://github.com/implus/SKNet) | 2019 |\n| SCNet | A | A | A | - | - | A | [link](http://mftp.mmcheng.net/Papers/20cvprSCNet.pdf) | [link](https://github.com/MCG-NKU/SCNet) | 2020 |\n| RegNet | A | A | A | - | - | A | [link](https://arxiv.org/abs/2003.13678) | [link](https://github.com/facebookresearch/pycls) | 2020 |\n| DIA-ResNet | aBCD | aBCD | aBCD | - | - | - | [link](https://arxiv.org/abs/1905.10671) | [link](https://github.com/gbup-group/DIANet) | 2019 |\n| DIA-PreResNet | aBCD | aBCD | aBCD | - | - | - | [link](https://arxiv.org/abs/1905.10671) | [link](https://github.com/gbup-group/DIANet) | 2019 |\n| PyramidNet | ABCD | ABCD | ABCD | - | - | ABCD | [link](https://arxiv.org/abs/1610.02915) | [link](https://github.com/jhkim89/PyramidNet) | 2016 |\n| DiracNetV2 | A | A | A | - | - | A | [link](https://arxiv.org/abs/1706.00388) | [link](https://github.com/szagoruyko/diracnets) | 2017 |\n| ShaResNet | a | a | a | - | - | - | [link](https://arxiv.org/abs/1702.08782) | [link](https://github.com/aboulch/sharesnet) | 2017 |\n| CRU-Net | A | - | - | - | - | - | [link](https://www.ijcai.org/proceedings/2018/88) | [link](https://github.com/cypw/CRU-Net) | 2018 |\n| DenseNet | ABCD | ABCD | ABCD | A | A | ABCD | [link](https://arxiv.org/abs/1608.06993) | [link](https://github.com/liuzhuang13/DenseNet) | 2016 |\n| CondenseNet | A | A | A | - | - | - | [link](https://arxiv.org/abs/1711.09224) | [link](https://github.com/ShichenLiu/CondenseNet) | 2017 |\n| SparseNet | a | a | a | - | - | - | [link](https://arxiv.org/abs/1801.05895) | [link](https://github.com/Lyken17/SparseNet) | 2018 |\n| PeleeNet | A | A | A | - | - | A | [link](https://arxiv.org/abs/1804.06882) | [link](https://github.com/Robert-JunWang/Pelee) | 2018 |\n| Oct-ResNet | abcd | a | a | - | - | - | [link](https://arxiv.org/abs/1904.05049) | - | 2019 |\n| Res2Net | a | - | - | - | - | - | [link](https://arxiv.org/abs/1904.01169) | - | 2019 |\n| WRN | ABCD | ABCD | ABCD | - | - | a | [link](https://arxiv.org/abs/1605.07146) | [link](https://github.com/szagoruyko/wide-residual-networks) | 2016 |\n| WRN-1bit | BCD | BCD | BCD | - | - | - | [link](https://arxiv.org/abs/1802.08530) | [link](https://github.com/McDonnell-Lab/1-bit-per-weight) | 2018 |\n| DRN-C | A | A | A | - | - | A | [link](https://arxiv.org/abs/1705.09914) | [link](https://github.com/fyu/drn) | 2017 |\n| DRN-D | A | A | A | - | - | A | [link](https://arxiv.org/abs/1705.09914) | [link](https://github.com/fyu/drn) | 2017 |\n| DPN | A | A | A | - | - | A | [link](https://arxiv.org/abs/1707.01629) | [link](https://github.com/cypw/DPNs) | 2017 |\n| DarkNet Ref | A | A | A | A | A | A | [link](https://github.com/pjreddie/darknet) | [link](https://github.com/pjreddie/darknet) | - |\n| DarkNet Tiny | A | A | A | A | A | A | [link](https://github.com/pjreddie/darknet) | [link](https://github.com/pjreddie/darknet) | - |\n| DarkNet-19 | a | a | a | a | a | a | [link](https://github.com/pjreddie/darknet) | [link](https://github.com/pjreddie/darknet) | - |\n| DarkNet-53 | A | A | A | A | A | A | [link](https://arxiv.org/abs/1804.02767) | [link](https://github.com/pjreddie/darknet) | 2018 |\n| ChannelNet | a | a | a | - | a | - | [link](https://arxiv.org/abs/1809.01330) | [link](https://github.com/HongyangGao/ChannelNets) | 2018 |\n| iSQRT-COV-ResNet | a | a | - | - | - | - | [link](https://arxiv.org/abs/1712.01034) | [link](https://github.com/jiangtaoxie/fast-MPN-COV) | 2017 |\n| RevNet | - | a | - | - | - | - | [link](https://arxiv.org/abs/1707.04585) | [link](https://github.com/renmengye/revnet-public) | 2017 |\n| i-RevNet | A | A | A | - | - | - | [link](https://arxiv.org/abs/1802.07088) | [link](https://github.com/jhjacobsen/pytorch-i-revnet) | 2018 |\n| BagNet | A | A | A | - | - | A | [link](https://openreview.net/pdf?id=SkfMWhAqYQ) | [link](https://github.com/wielandbrendel/bag-of-local-features-models) | 2019 |\n| DLA | A | A | A | - | - | A | [link](https://arxiv.org/abs/1707.06484) | [link](https://github.com/ucbdrive/dla) | 2017 |\n| MSDNet | a | ab | - | - | - | - | [link](https://arxiv.org/abs/1703.09844) | [link](https://github.com/gaohuang/MSDNet) | 2017 |\n| FishNet | A | A | A | - | - | - | [link](http://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf) | [link](https://github.com/kevin-ssy/FishNet) | 2018 |\n| ESPNetv2 | A | A | A | - | - | - | [link](https://arxiv.org/abs/1811.11431) | [link](https://github.com/sacmehta/ESPNetv2) | 2018 |\n| DiCENet | A | A | A | - | - | A | [link](https://arxiv.org/abs/1906.03516) | [link](https://github.com/sacmehta/EdgeNets) | 2019 |\n| HRNet | A | A | A | - | - | A | [link](https://arxiv.org/abs/1908.07919) | [link](https://github.com/HRNet/HRNet-Image-Classification) | 2019 |\n| VoVNet | A | A | A | - | - | A | [link](https://arxiv.org/abs/1904.09730) | [link](https://github.com/stigma0617/VoVNet.pytorch) | 2019 |\n| SelecSLS | A | A | A | - | - | A | [link](https://arxiv.org/abs/1907.00837) | [link](https://github.com/mehtadushy/SelecSLS-Pytorch) | 2019 |\n| HarDNet | A | A | A | - | - | A | [link](https://arxiv.org/abs/1909.00948) | [link](https://github.com/PingoLH/Pytorch-HarDNet) | 2019 |\n| X-DenseNet | aBCD | aBCD | aBCD | - | - | - | [link](https://arxiv.org/abs/1711.08757) | [link](https://github.com/DrImpossible/Deep-Expander-Networks) | 2017 |\n| SqueezeNet | A | A | A | A | A | A | [link](https://arxiv.org/abs/1602.07360) | [link](https://github.com/DeepScale/SqueezeNet) | 2016 |\n| SqueezeResNet | A | A | A | A | A | A | [link](https://arxiv.org/abs/1602.07360) | - | 2016 |\n| SqueezeNext | A | A | A | A | A | A | [link](https://arxiv.org/abs/1803.10615) | [link](https://github.com/amirgholami/SqueezeNext) | 2018 |\n| ShuffleNet | A | A | A | A | A | A | [link](https://arxiv.org/abs/1707.01083) | - | 2017 |\n| ShuffleNetV2 | A | A | A | A | A | A | [link](https://arxiv.org/abs/1807.11164) | - | 2018 |\n| MENet | A | A | A | A | A | A | [link](https://arxiv.org/abs/1803.09127) | [link](https://github.com/clavichord93/MENet) | 2018 |\n| MobileNet | AE | AE | AE | A | A | AE | [link](https://arxiv.org/abs/1704.04861) | [link](https://github.com/tensorflow/models) | 2017 |\n| FD-MobileNet | A | A | A | A | A | A | [link](https://arxiv.org/abs/1802.03750) | [link](https://github.com/clavichord93/FD-MobileNet) | 2018 |\n| MobileNetV2 | A | A | A | A | A | A | [link](https://arxiv.org/abs/1801.04381) | [link](https://github.com/tensorflow/models) | 2018 |\n| MobileNetV3 | A | A | A | A | - | A | [link](https://arxiv.org/abs/1905.02244) | [link](https://github.com/tensorflow/models) | 2019 |\n| IGCV3 | A | A | A | A | A | A | [link](https://arxiv.org/abs/1806.00178) | [link](https://github.com/homles11/IGCV3) | 2018 |\n| GhostNet | a | a | a | - | - | a | [link](https://arxiv.org/abs/1911.11907) | [link](https://github.com/iamhankai/ghostnet) | 2019 |\n| MnasNet | A | A | A | A | A | A | [link](https://arxiv.org/abs/1807.11626) | - | 2018 |\n| DARTS | A | A | A | - | - | - | [link](https://arxiv.org/abs/1806.09055) | [link](https://github.com/quark0/darts) | 2018 |\n| ProxylessNAS | AE | AE | AE | - | - | AE | [link](https://arxiv.org/abs/1812.00332) | [link](https://github.com/mit-han-lab/ProxylessNAS) | 2018 |\n| FBNet-C | A | A | A | - | - | A | [link](https://arxiv.org/abs/1812.03443) | - | 2018 |\n| Xception | A | A | A | - | - | A | [link](https://arxiv.org/abs/1610.02357) | [link](https://github.com/fchollet/deep-learning-models) | 2016 |\n| InceptionV3 | A | A | A | - | - | A | [link](https://arxiv.org/abs/1512.00567) | [link](https://github.com/tensorflow/models) | 2015 |\n| InceptionV4 | A | A | A | - | - | A | [link](https://arxiv.org/abs/1602.07261) | [link](https://github.com/tensorflow/models) | 2016 |\n| InceptionResNetV1 | A | A | A | - | - | A | [link](https://arxiv.org/abs/1602.07261) | [link](https://github.com/tensorflow/models) | 2016 |\n| InceptionResNetV2 | A | A | A | - | - | A | [link](https://arxiv.org/abs/1602.07261) | [link](https://github.com/tensorflow/models) | 2016 |\n| PolyNet | A | A | A | - | - | A | [link](https://arxiv.org/abs/1611.05725) | [link](https://github.com/open-mmlab/polynet) | 2016 |\n| NASNet-Large | A | A | A | - | - | A | [link](https://arxiv.org/abs/1707.07012) | [link](https://github.com/tensorflow/models) | 2017 |\n| NASNet-Mobile | A | A | A | - | - | A | [link](https://arxiv.org/abs/1707.07012) | [link](https://github.com/tensorflow/models) | 2017 |\n| PNASNet-Large | A | A | A | - | - | A | [link](https://arxiv.org/abs/1712.00559) | [link](https://github.com/tensorflow/models) | 2017 |\n| SPNASNet | A | A | A | - | - | A | [link](https://arxiv.org/abs/1904.02877) | [link](https://github.com/dstamoulis/single-path-nas) | 2019 |\n| EfficientNet | A | A | A | A | - | A | [link](https://arxiv.org/abs/1905.11946) | [link](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet) | 2019 |\n| MixNet | A | A | A | - | - | A | [link](https://arxiv.org/abs/1907.09595) | [link](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet) | 2019 |\n| NIN | BCD | BCD | BCD | - | - | - | [link](https://arxiv.org/abs/1312.4400) | [link](https://gist.github.com/mavenlin/e56253735ef32c3c296d) | 2013 |\n| RoR-3 | BCD | BCD | BCD | - | - | - | [link](https://arxiv.org/abs/1608.02908) | - | 2016 |\n| RiR | BCD | BCD | BCD | - | - | - | [link](https://arxiv.org/abs/1603.08029) | - | 2016 |\n| ResDrop-ResNet | bcd | bcd | bcd | - | - | - | [link](https://arxiv.org/abs/1603.09382) | [link](https://github.com/yueatsprograms/Stochastic_Depth) | 2016 |\n| Shake-Shake-ResNet | BCD | BCD | BCD | - | - | - | [link](https://arxiv.org/abs/1705.07485) | [link](https://github.com/xgastaldi/shake-shake) | 2017 |\n| ShakeDrop-ResNet | bcd | bcd | bcd | - | - | - | [link](https://arxiv.org/abs/1802.02375) | - | 2018 |\n| FractalNet | bc | bc | - | - | - | - | [link](https://arxiv.org/abs/1605.07648) | [link](https://github.com/gustavla/fractalnet) | 2016 |\n| NTS-Net | E | E | E | - | - | - | [link](https://arxiv.org/abs/1809.00287) | [link](https://github.com/yangze0930/NTS-Net) | 2018 |\n\n## Table of implemented segmentation models\n\nSome remarks:\n- `a/A` corresponds to Pascal VOC2012.\n- `b/B` corresponds to ADE20K.\n- `c/C` corresponds to Cityscapes.\n- `d/D` corresponds to COCO.\n- `e/E` corresponds to CelebAMask-HQ.\n\n| Model | [Gluon](gluon/README.md) | [PyTorch](pytorch/README.md) | [Chainer](chainer_/README.md) | [Keras](keras_/README.md) | [TF](tensorflow_/README.md)  | [TF2](tensorflow_/README.md) | Paper | Repo | Year |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| PSPNet | ABCD | ABCD | ABCD | - | - | ABCD | [link](https://arxiv.org/abs/1612.01105) | - | 2016 |\n| DeepLabv3 | ABcD | ABcD | ABcD | - | - | ABcD | [link](https://arxiv.org/abs/1706.05587) | - | 2017 |\n| FCN-8s(d) | ABcD | ABcD | ABcD | - | - | ABcD | [link](https://arxiv.org/abs/1411.4038) | - | 2014 |\n| ICNet | C | C | C | - | - | C | [link](https://arxiv.org/abs/1704.08545) | [link](https://github.com/hszhao/ICNet) | 2017 |\n| SINet | C | C | C | - | - | c | [link](https://arxiv.org/abs/1911.09099) | [link](https://github.com/clovaai/c3_sinet) | 2019 |\n| BiSeNet | e | e | e | - | - | e | [link](https://arxiv.org/abs/1808.00897) | - | 2018 |\n| DANet | C | C | C | - | - | C | [link](https://arxiv.org/abs/1809.02983) | [link](https://github.com/junfu1115/DANet) | 2018 |\n| Fast-SCNN | C | C | C | - | - | C | [link](https://arxiv.org/abs/1902.04502) | - | 2019 |\n| CGNet | c | c | c | - | - | c | [link](https://arxiv.org/abs/1811.08201) | [link](https://github.com/wutianyiRosun/CGNet) | 2018 |\n| DABNet | c | c | c | - | - | c | [link](https://arxiv.org/abs/1907.11357) | [link](https://github.com/Reagan1311/DABNet) | 2019 |\n| FPENet | c | c | c | - | - | c | [link](https://arxiv.org/abs/1909.08599) | - | 2019 |\n| ContextNet | - | c | - | - | - | - | [link](https://arxiv.org/abs/1805.04554) | - | 2018 |\n| LEDNet | c | c | c | - | - | c | [link](https://arxiv.org/abs/1905.02423) | - | 2019 |\n| ESNet | - | c | - | - | - | - | [link](https://arxiv.org/abs/1906.09826) | - | 2019 |\n| EDANet | - | c | - | - | - | - | [link](https://arxiv.org/abs/1809.06323) | [link](https://github.com/shaoyuanlo/EDANet) | 2018 |\n| ENet | - | c | - | - | - | - | [link](https://arxiv.org/abs/1606.02147) | - | 2016 |\n| ERFNet | - | c | - | - | - | - | [link](http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera17tits.pdf) | - | 2017 |\n| LinkNet | - | c | - | - | - | - | [link](https://arxiv.org/abs/1707.03718) | - | 2017 |\n| SegNet | - | c | - | - | - | - | [link](https://arxiv.org/abs/1511.00561) | - | 2015 |\n| U-Net | - | c | - | - | - | - | [link](https://arxiv.org/abs/1505.04597) | - | 2015 |\n| SQNet | - | c | - | - | - | - | [link](https://openreview.net/pdf?id=S1uHiFyyg) | - | 2016 |\n\n## Table of implemented object detection models\n\nSome remarks:\n- `a/A` corresponds to COCO.\n\n| Model | [Gluon](gluon/README.md) | [PyTorch](pytorch/README.md) | [Chainer](chainer_/README.md) | [Keras](keras_/README.md) | [TF](tensorflow_/README.md)  | [TF2](tensorflow2/README.md) | Paper | Repo | Year |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| CenterNet | a | a | a | - | - | a | [link](https://arxiv.org/abs/1904.07850) | [link](https://github.com/xingyizhou/CenterNet) | 2019 |\n\n## Table of implemented human pose estimation models\n\nSome remarks:\n- `a/A` corresponds to COCO.\n\n| Model | [Gluon](gluon/README.md) | [PyTorch](pytorch/README.md) | [Chainer](chainer_/README.md) | [Keras](keras_/README.md) | [TF](tensorflow_/README.md)  | [TF2](tensorflow2/README.md) | Paper | Repo | Year |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| AlphaPose | A | A | A | - | - | A | [link](https://arxiv.org/abs/1612.00137) | [link](https://github.com/MVIG-SJTU/AlphaPose) | 2016 |\n| SimplePose | A | A | A | - | - | A | [link](https://arxiv.org/abs/1804.06208) | [link](https://github.com/microsoft/human-pose-estimation.pytorch) | 2018 |\n| SimplePose(Mobile) | A | A | A | - | - | A | [link](https://arxiv.org/abs/1804.06208) | - | 2018 |\n| Lightweight OpenPose | A | A | A | - | - | A | [link](https://arxiv.org/abs/1811.12004) | [link](https://github.com/Daniil-Osokin/lightweight-human-pose-estimation-3d-demo.pytorch) | 2018 |\n| IBPPose | A | A | A | - | - | A | [link](https://arxiv.org/abs/1911.10529) | [link](https://github.com/jialee93/Improved-Body-Parts) | 2019 |\n\n## Table of implemented automatic speech recognition models\n\nSome remarks:\n- `a/A` corresponds to LibriSpeech.\n- `b/B` corresponds to Mozilla Common Voice.\n\n| Model | [Gluon](gluon/README.md) | [PyTorch](pytorch/README.md) | [Chainer](chainer_/README.md) | [Keras](keras_/README.md) | [TF](tensorflow_/README.md)  | [TF2](tensorflow2/README.md) | Paper | Repo | Year |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Jasper DR | AB | AB | ab | - | - | ab | [link](https://arxiv.org/abs/1904.03288) | [link](https://github.com/NVIDIA/NeMo) | 2019 |\n| QuartzNet | AB | AB | ab | - | - | ab | [link](https://arxiv.org/abs/1910.10261) | [link](https://github.com/NVIDIA/NeMo) | 2019 |\n",
            "readme_url": "https://github.com/osmr/imgclsmob",
            "frameworks": [
                "Keras",
                "MXNet",
                "PyTorch",
                "TensorFlow",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Learning Transferable Architectures for Scalable Image Recognition",
            "arxiv": "1707.07012",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.07012v4",
            "abstract": "Developing neural network image classification models often requires\nsignificant architecture engineering. In this paper, we study a method to learn\nthe model architectures directly on the dataset of interest. As this approach\nis expensive when the dataset is large, we propose to search for an\narchitectural building block on a small dataset and then transfer the block to\na larger dataset. The key contribution of this work is the design of a new\nsearch space (the \"NASNet search space\") which enables transferability. In our\nexperiments, we search for the best convolutional layer (or \"cell\") on the\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\ntogether more copies of this cell, each with their own parameters to design a\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\nnew regularization technique called ScheduledDropPath that significantly\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\nreduction of 28% in computational demand from the previous state-of-the-art\nmodel. When evaluated at different levels of computational cost, accuracies of\nNASNets exceed those of the state-of-the-art human-designed models. For\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\n3.1% better than equivalently-sized, state-of-the-art models for mobile\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\ndataset.",
            "authors": [
                "Barret Zoph",
                "Vijay Vasudevan",
                "Jonathon Shlens",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Dual Attention Network for Scene Segmentation",
            "arxiv": "1809.02983",
            "year": 2018,
            "url": "http://arxiv.org/abs/1809.02983v4",
            "abstract": "In this paper, we address the scene segmentation task by capturing rich\ncontextual dependencies based on the selfattention mechanism. Unlike previous\nworks that capture contexts by multi-scale features fusion, we propose a Dual\nAttention Networks (DANet) to adaptively integrate local features with their\nglobal dependencies. Specifically, we append two types of attention modules on\ntop of traditional dilated FCN, which model the semantic interdependencies in\nspatial and channel dimensions respectively. The position attention module\nselectively aggregates the features at each position by a weighted sum of the\nfeatures at all positions. Similar features would be related to each other\nregardless of their distances. Meanwhile, the channel attention module\nselectively emphasizes interdependent channel maps by integrating associated\nfeatures among all channel maps. We sum the outputs of the two attention\nmodules to further improve feature representation which contributes to more\nprecise segmentation results. We achieve new state-of-the-art segmentation\nperformance on three challenging scene segmentation datasets, i.e., Cityscapes,\nPASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5%\non Cityscapes test set is achieved without using coarse data. We make the code\nand trained model publicly available at https://github.com/junfu1115/DANet",
            "authors": [
                "Jun Fu",
                "Jing Liu",
                "Haijie Tian",
                "Yong Li",
                "Yongjun Bao",
                "Zhiwei Fang",
                "Hanqing Lu"
            ]
        },
        {
            "title": "Aggregated Residual Transformations for Deep Neural Networks",
            "arxiv": "1611.05431",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.05431v2",
            "abstract": "We present a simple, highly modularized network architecture for image\nclassification. Our network is constructed by repeating a building block that\naggregates a set of transformations with the same topology. Our simple design\nresults in a homogeneous, multi-branch architecture that has only a few\nhyper-parameters to set. This strategy exposes a new dimension, which we call\n\"cardinality\" (the size of the set of transformations), as an essential factor\nin addition to the dimensions of depth and width. On the ImageNet-1K dataset,\nwe empirically show that even under the restricted condition of maintaining\ncomplexity, increasing cardinality is able to improve classification accuracy.\nMoreover, increasing cardinality is more effective than going deeper or wider\nwhen we increase the capacity. Our models, named ResNeXt, are the foundations\nof our entry to the ILSVRC 2016 classification task in which we secured 2nd\nplace. We further investigate ResNeXt on an ImageNet-5K set and the COCO\ndetection set, also showing better results than its ResNet counterpart. The\ncode and models are publicly available online.",
            "authors": [
                "Saining Xie",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ]
        },
        {
            "title": "Fully Convolutional Networks for Semantic Segmentation",
            "arxiv": "1411.4038",
            "year": 2014,
            "url": "http://arxiv.org/abs/1411.4038v2",
            "abstract": "Convolutional networks are powerful visual models that yield hierarchies of\nfeatures. We show that convolutional networks by themselves, trained\nend-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic\nsegmentation. Our key insight is to build \"fully convolutional\" networks that\ntake input of arbitrary size and produce correspondingly-sized output with\nefficient inference and learning. We define and detail the space of fully\nconvolutional networks, explain their application to spatially dense prediction\ntasks, and draw connections to prior models. We adapt contemporary\nclassification networks (AlexNet, the VGG net, and GoogLeNet) into fully\nconvolutional networks and transfer their learned representations by\nfine-tuning to the segmentation task. We then define a novel architecture that\ncombines semantic information from a deep, coarse layer with appearance\ninformation from a shallow, fine layer to produce accurate and detailed\nsegmentations. Our fully convolutional network achieves state-of-the-art\nsegmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012),\nNYUDv2, and SIFT Flow, while inference takes one third of a second for a\ntypical image.",
            "authors": [
                "Jonathan Long",
                "Evan Shelhamer",
                "Trevor Darrell"
            ]
        },
        {
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
            "arxiv": "1801.04381",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.04381v4",
            "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that\nimproves the state of the art performance of mobile models on multiple tasks\nand benchmarks as well as across a spectrum of different model sizes. We also\ndescribe efficient ways of applying these mobile models to object detection in\na novel framework we call SSDLite. Additionally, we demonstrate how to build\nmobile semantic segmentation models through a reduced form of DeepLabv3 which\nwe call Mobile DeepLabv3.\n  The MobileNetV2 architecture is based on an inverted residual structure where\nthe input and output of the residual block are thin bottleneck layers opposite\nto traditional residual models which use expanded representations in the input\nan MobileNetV2 uses lightweight depthwise convolutions to filter features in\nthe intermediate expansion layer. Additionally, we find that it is important to\nremove non-linearities in the narrow layers in order to maintain\nrepresentational power. We demonstrate that this improves performance and\nprovide an intuition that led to this design. Finally, our approach allows\ndecoupling of the input/output domains from the expressiveness of the\ntransformation, which provides a convenient framework for further analysis. We\nmeasure our performance on Imagenet classification, COCO object detection, VOC\nimage segmentation. We evaluate the trade-offs between accuracy, and number of\noperations measured by multiply-adds (MAdd), as well as the number of\nparameters",
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ]
        },
        {
            "title": "ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation",
            "arxiv": "1606.02147",
            "year": 2016,
            "url": "http://arxiv.org/abs/1606.02147v1",
            "abstract": "The ability to perform pixel-wise semantic segmentation in real-time is of\nparamount importance in mobile applications. Recent deep neural networks aimed\nat this task have the disadvantage of requiring a large number of floating\npoint operations and have long run-times that hinder their usability. In this\npaper, we propose a novel deep neural network architecture named ENet\n(efficient neural network), created specifically for tasks requiring low\nlatency operation. ENet is up to 18$\\times$ faster, requires 75$\\times$ less\nFLOPs, has 79$\\times$ less parameters, and provides similar or better accuracy\nto existing models. We have tested it on CamVid, Cityscapes and SUN datasets\nand report on comparisons with existing state-of-the-art methods, and the\ntrade-offs between accuracy and processing time of a network. We present\nperformance measurements of the proposed architecture on embedded systems and\nsuggest possible software improvements that could make ENet even faster.",
            "authors": [
                "Adam Paszke",
                "Abhishek Chaurasia",
                "Sangpil Kim",
                "Eugenio Culurciello"
            ]
        },
        {
            "title": "Rethinking the Inception Architecture for Computer Vision",
            "arxiv": "1512.00567",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.00567v3",
            "abstract": "Convolutional networks are at the core of most state-of-the-art computer\nvision solutions for a wide variety of tasks. Since 2014 very deep\nconvolutional networks started to become mainstream, yielding substantial gains\nin various benchmarks. Although increased model size and computational cost\ntend to translate to immediate quality gains for most tasks (as long as enough\nlabeled data is provided for training), computational efficiency and low\nparameter count are still enabling factors for various use cases such as mobile\nvision and big-data scenarios. Here we explore ways to scale up networks in\nways that aim at utilizing the added computation as efficiently as possible by\nsuitably factorized convolutions and aggressive regularization. We benchmark\nour methods on the ILSVRC 2012 classification challenge validation set\ndemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%\ntop-5 error for single frame evaluation using a network with a computational\ncost of 5 billion multiply-adds per inference and with using less than 25\nmillion parameters. With an ensemble of 4 models and multi-crop evaluation, we\nreport 3.5% top-5 error on the validation set (3.6% error on the test set) and\n17.3% top-1 error on the validation set.",
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jonathon Shlens",
                "Zbigniew Wojna"
            ]
        },
        {
            "title": "RMPE: Regional Multi-person Pose Estimation",
            "arxiv": "1612.00137",
            "year": 2016,
            "url": "http://arxiv.org/abs/1612.00137v5",
            "abstract": "Multi-person pose estimation in the wild is challenging. Although\nstate-of-the-art human detectors have demonstrated good performance, small\nerrors in localization and recognition are inevitable. These errors can cause\nfailures for a single-person pose estimator (SPPE), especially for methods that\nsolely depend on human detection results. In this paper, we propose a novel\nregional multi-person pose estimation (RMPE) framework to facilitate pose\nestimation in the presence of inaccurate human bounding boxes. Our framework\nconsists of three components: Symmetric Spatial Transformer Network (SSTN),\nParametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals\nGenerator (PGPG). Our method is able to handle inaccurate bounding boxes and\nredundant detections, allowing it to achieve a 17% increase in mAP over the\nstate-of-the-art methods on the MPII (multi person) dataset.Our model and\nsource codes are publicly available.",
            "authors": [
                "Hao-Shu Fang",
                "Shuqin Xie",
                "Yu-Wing Tai",
                "Cewu Lu"
            ]
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "arxiv": "1512.03385",
            "year": 2015,
            "url": "http://arxiv.org/abs/1512.03385v1",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "Resnet in Resnet: Generalizing Residual Architectures",
            "arxiv": "1603.08029",
            "year": 2016,
            "url": "http://arxiv.org/abs/1603.08029v1",
            "abstract": "Residual networks (ResNets) have recently achieved state-of-the-art on\nchallenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep\ndual-stream architecture that generalizes ResNets and standard CNNs and is\neasily implemented with no computational overhead. RiR consistently improves\nperformance over ResNets, outperforms architectures with similar amounts of\naugmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.",
            "authors": [
                "Sasha Targ",
                "Diogo Almeida",
                "Kevin Lyman"
            ]
        },
        {
            "title": "FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
            "arxiv": "1812.03443",
            "year": 2018,
            "url": "http://arxiv.org/abs/1812.03443v3",
            "abstract": "Designing accurate and efficient ConvNets for mobile devices is challenging\nbecause the design space is combinatorially large. Due to this, previous neural\narchitecture search (NAS) methods are computationally expensive. ConvNet\narchitecture optimality depends on factors such as input resolution and target\ndevices. However, existing approaches are too expensive for case-by-case\nredesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP\ncount does not always reflect actual latency. To address these, we propose a\ndifferentiable neural architecture search (DNAS) framework that uses\ngradient-based methods to optimize ConvNet architectures, avoiding enumerating\nand training individual architectures separately as in previous methods.\nFBNets, a family of models discovered by DNAS surpass state-of-the-art models\nboth designed manually and generated automatically. FBNet-B achieves 74.1%\ntop-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8\nphone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3 with similar accuracy.\nDespite higher accuracy and lower latency than MnasNet, we estimate FBNet-B's\nsearch cost is 420x smaller than MnasNet's, at only 216 GPU-hours. Searched for\ndifferent resolutions and channel sizes, FBNets achieve 1.5% to 6.4% higher\naccuracy than MobileNetV2. The smallest FBNet achieves 50.2% accuracy and 2.9\nms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized\nFBNet, the iPhone-X-optimized model achieves a 1.4x speedup on an iPhone X.",
            "authors": [
                "Bichen Wu",
                "Xiaoliang Dai",
                "Peizhao Zhang",
                "Yanghan Wang",
                "Fei Sun",
                "Yiming Wu",
                "Yuandong Tian",
                "Peter Vajda",
                "Yangqing Jia",
                "Kurt Keutzer"
            ]
        },
        {
            "title": "Merging and Evolution: Improving Convolutional Neural Networks for Mobile Applications",
            "arxiv": "1803.09127",
            "year": 2018,
            "url": "http://arxiv.org/abs/1803.09127v1",
            "abstract": "Compact neural networks are inclined to exploit \"sparsely-connected\"\nconvolutions such as depthwise convolution and group convolution for employment\nin mobile applications. Compared with standard \"fully-connected\" convolutions,\nthese convolutions are more computationally economical. However,\n\"sparsely-connected\" convolutions block the inter-group information exchange,\nwhich induces severe performance degradation. To address this issue, we present\ntwo novel operations named merging and evolution to leverage the inter-group\ninformation. Our key idea is encoding the inter-group information with a narrow\nfeature map, then combining the generated features with the original network\nfor better representation. Taking advantage of the proposed operations, we then\nintroduce the Merging-and-Evolution (ME) module, an architectural unit\nspecifically designed for compact networks. Finally, we propose a family of\ncompact neural networks called MENet based on ME modules. Extensive experiments\non ILSVRC 2012 dataset and PASCAL VOC 2007 dataset demonstrate that MENet\nconsistently outperforms other state-of-the-art compact networks under\ndifferent computational budgets. For instance, under the computational budget\nof 140 MFLOPs, MENet surpasses ShuffleNet by 1% and MobileNet by 1.95% on\nILSVRC 2012 top-1 accuracy, while by 2.3% and 4.1% on PASCAL VOC 2007 mAP,\nrespectively.",
            "authors": [
                "Zheng Qin",
                "Zhaoning Zhang",
                "Shiqing Zhang",
                "Hao Yu",
                "Yuxing Peng"
            ]
        },
        {
            "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
            "arxiv": "1610.02357",
            "year": 2016,
            "url": "http://arxiv.org/abs/1610.02357v3",
            "abstract": "We present an interpretation of Inception modules in convolutional neural\nnetworks as being an intermediate step in-between regular convolution and the\ndepthwise separable convolution operation (a depthwise convolution followed by\na pointwise convolution). In this light, a depthwise separable convolution can\nbe understood as an Inception module with a maximally large number of towers.\nThis observation leads us to propose a novel deep convolutional neural network\narchitecture inspired by Inception, where Inception modules have been replaced\nwith depthwise separable convolutions. We show that this architecture, dubbed\nXception, slightly outperforms Inception V3 on the ImageNet dataset (which\nInception V3 was designed for), and significantly outperforms Inception V3 on a\nlarger image classification dataset comprising 350 million images and 17,000\nclasses. Since the Xception architecture has the same number of parameters as\nInception V3, the performance gains are not due to increased capacity but\nrather to a more efficient use of model parameters.",
            "authors": [
                "Fran\u00e7ois Chollet"
            ]
        },
        {
            "title": "Objects as Points",
            "arxiv": "1904.07850",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.07850v2",
            "abstract": "Detection identifies objects as axis-aligned boxes in an image. Most\nsuccessful object detectors enumerate a nearly exhaustive list of potential\nobject locations and classify each. This is wasteful, inefficient, and requires\nadditional post-processing. In this paper, we take a different approach. We\nmodel an object as a single point --- the center point of its bounding box. Our\ndetector uses keypoint estimation to find center points and regresses to all\nother object properties, such as size, 3D location, orientation, and even pose.\nOur center point based approach, CenterNet, is end-to-end differentiable,\nsimpler, faster, and more accurate than corresponding bounding box based\ndetectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO\ndataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with\nmulti-scale testing at 1.4 FPS. We use the same approach to estimate 3D\nbounding box in the KITTI benchmark and human pose on the COCO keypoint\ndataset. Our method performs competitively with sophisticated multi-stage\nmethods and runs in real-time.",
            "authors": [
                "Xingyi Zhou",
                "Dequan Wang",
                "Philipp Kr\u00e4henb\u00fchl"
            ]
        },
        {
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "arxiv": "1502.03167",
            "year": 2015,
            "url": "http://arxiv.org/abs/1502.03167v3",
            "abstract": "Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters.",
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ]
        },
        {
            "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
            "arxiv": "1707.01083",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.01083v2",
            "abstract": "We introduce an extremely computation-efficient CNN architecture named\nShuffleNet, which is designed specially for mobile devices with very limited\ncomputing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new\noperations, pointwise group convolution and channel shuffle, to greatly reduce\ncomputation cost while maintaining accuracy. Experiments on ImageNet\nclassification and MS COCO object detection demonstrate the superior\nperformance of ShuffleNet over other structures, e.g. lower top-1 error\n(absolute 7.8%) than recent MobileNet on ImageNet classification task, under\nthe computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet\nachieves ~13x actual speedup over AlexNet while maintaining comparable\naccuracy.",
            "authors": [
                "Xiangyu Zhang",
                "Xinyu Zhou",
                "Mengxiao Lin",
                "Jian Sun"
            ]
        },
        {
            "title": "Wide Residual Networks",
            "arxiv": "1605.07146",
            "year": 2016,
            "url": "http://arxiv.org/abs/1605.07146v4",
            "abstract": "Deep residual networks were shown to be able to scale up to thousands of\nlayers and still have improving performance. However, each fraction of a\npercent of improved accuracy costs nearly doubling the number of layers, and so\ntraining very deep residual networks has a problem of diminishing feature\nreuse, which makes these networks very slow to train. To tackle these problems,\nin this paper we conduct a detailed experimental study on the architecture of\nResNet blocks, based on which we propose a novel architecture where we decrease\ndepth and increase width of residual networks. We call the resulting network\nstructures wide residual networks (WRNs) and show that these are far superior\nover their commonly used thin and very deep counterparts. For example, we\ndemonstrate that even a simple 16-layer-deep wide residual network outperforms\nin accuracy and efficiency all previous deep residual networks, including\nthousand-layer-deep networks, achieving new state-of-the-art results on CIFAR,\nSVHN, COCO, and significant improvements on ImageNet. Our code and models are\navailable at https://github.com/szagoruyko/wide-residual-networks",
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ]
        },
        {
            "title": "Sparsely Aggregated Convolutional Networks",
            "arxiv": "1801.05895",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.05895v3",
            "abstract": "We explore a key architectural aspect of deep convolutional neural networks:\nthe pattern of internal skip connections used to aggregate outputs of earlier\nlayers for consumption by deeper layers. Such aggregation is critical to\nfacilitate training of very deep networks in an end-to-end manner. This is a\nprimary reason for the widespread adoption of residual networks, which\naggregate outputs via cumulative summation. While subsequent works investigate\nalternative aggregation operations (e.g. concatenation), we focus on an\northogonal question: which outputs to aggregate at a particular point in the\nnetwork. We propose a new internal connection structure which aggregates only a\nsparse set of previous outputs at any given depth. Our experiments demonstrate\nthis simple design change offers superior performance with fewer parameters and\nlower computational requirements. Moreover, we show that sparse aggregation\nallows networks to scale more robustly to 1000+ layers, thereby opening future\navenues for training long-running visual processes.",
            "authors": [
                "Ligeng Zhu",
                "Ruizhi Deng",
                "Michael Maire",
                "Zhiwei Deng",
                "Greg Mori",
                "Ping Tan"
            ]
        },
        {
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "arxiv": "1505.04597",
            "year": 2015,
            "url": "http://arxiv.org/abs/1505.04597v1",
            "abstract": "There is large consent that successful training of deep networks requires\nmany thousand annotated training samples. In this paper, we present a network\nand training strategy that relies on the strong use of data augmentation to use\nthe available annotated samples more efficiently. The architecture consists of\na contracting path to capture context and a symmetric expanding path that\nenables precise localization. We show that such a network can be trained\nend-to-end from very few images and outperforms the prior best method (a\nsliding-window convolutional network) on the ISBI challenge for segmentation of\nneuronal structures in electron microscopic stacks. Using the same network\ntrained on transmitted light microscopy images (phase contrast and DIC) we won\nthe ISBI cell tracking challenge 2015 in these categories by a large margin.\nMoreover, the network is fast. Segmentation of a 512x512 image takes less than\na second on a recent GPU. The full implementation (based on Caffe) and the\ntrained networks are available at\nhttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ]
        },
        {
            "title": "Densely Connected Convolutional Networks",
            "arxiv": "1608.06993",
            "year": 2016,
            "url": "http://arxiv.org/abs/1608.06993v5",
            "abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        },
        {
            "title": "CondenseNet: An Efficient DenseNet using Learned Group Convolutions",
            "arxiv": "1711.09224",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.09224v2",
            "abstract": "Deep neural networks are increasingly used on mobile devices, where\ncomputational resources are limited. In this paper we develop CondenseNet, a\nnovel network architecture with unprecedented efficiency. It combines dense\nconnectivity with a novel module called learned group convolution. The dense\nconnectivity facilitates feature re-use in the network, whereas learned group\nconvolutions remove connections between layers for which this feature re-use is\nsuperfluous. At test time, our model can be implemented using standard group\nconvolutions, allowing for efficient computation in practice. Our experiments\nshow that CondenseNets are far more efficient than state-of-the-art compact\nconvolutional networks such as MobileNets and ShuffleNets.",
            "authors": [
                "Gao Huang",
                "Shichen Liu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        },
        {
            "title": "HarDNet: A Low Memory Traffic Network",
            "arxiv": "1909.00948",
            "year": 2019,
            "url": "http://arxiv.org/abs/1909.00948v1",
            "abstract": "State-of-the-art neural network architectures such as ResNet, MobileNet, and\nDenseNet have achieved outstanding accuracy over low MACs and small model size\ncounterparts. However, these metrics might not be accurate for predicting the\ninference time. We suggest that memory traffic for accessing intermediate\nfeature maps can be a factor dominating the inference latency, especially in\nsuch tasks as real-time object detection and semantic segmentation of\nhigh-resolution video. We propose a Harmonic Densely Connected Network to\nachieve high efficiency in terms of both low MACs and memory traffic. The new\nnetwork achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared\nwith FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG,\nrespectively. We use tools including Nvidia profiler and ARM Scale-Sim to\nmeasure the memory traffic and verify that the inference latency is indeed\nproportional to the memory traffic consumption and the proposed network\nconsumes low memory traffic. We conclude that one should take memory traffic\ninto consideration when designing neural network architectures for\nhigh-resolution applications at the edge.",
            "authors": [
                "Ping Chao",
                "Chao-Yang Kao",
                "Yu-Shan Ruan",
                "Chien-Hsiang Huang",
                "Youn-Long Lin"
            ]
        },
        {
            "title": "Deep Expander Networks: Efficient Deep Networks from Graph Theory",
            "arxiv": "1711.08757",
            "year": 2017,
            "url": "http://arxiv.org/abs/1711.08757v3",
            "abstract": "Efficient CNN designs like ResNets and DenseNet were proposed to improve\naccuracy vs efficiency trade-offs. They essentially increased the connectivity,\nallowing efficient information flow across layers. Inspired by these\ntechniques, we propose to model connections between filters of a CNN using\ngraphs which are simultaneously sparse and well connected. Sparsity results in\nefficiency while well connectedness can preserve the expressive power of the\nCNNs. We use a well-studied class of graphs from theoretical computer science\nthat satisfies these properties known as Expander graphs. Expander graphs are\nused to model connections between filters in CNNs to design networks called\nX-Nets. We present two guarantees on the connectivity of X-Nets: Each node\ninfluences every node in a layer in logarithmic steps, and the number of paths\nbetween two sets of nodes is proportional to the product of their sizes. We\nalso propose efficient training and inference algorithms, making it possible to\ntrain deeper and wider X-Nets effectively.\n  Expander based models give a 4% improvement in accuracy on MobileNet over\ngrouped convolutions, a popular technique, which has the same sparsity but\nworse connectivity. X-Nets give better performance trade-offs than the original\nResNet and DenseNet-BC architectures. We achieve model sizes comparable to\nstate-of-the-art pruning techniques using our simple architecture design,\nwithout any pruning. We hope that this work motivates other approaches to\nutilize results from graph theory to develop efficient network architectures.",
            "authors": [
                "Ameya Prabhu",
                "Girish Varma",
                "Anoop Namboodiri"
            ]
        },
        {
            "title": "ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation",
            "arxiv": "1906.09826",
            "year": 2019,
            "url": "http://arxiv.org/abs/1906.09826v1",
            "abstract": "The recent years have witnessed great advances for semantic segmentation\nusing deep convolutional neural networks (DCNNs). However, a large number of\nconvolutional layers and feature channels lead to semantic segmentation as a\ncomputationally heavy task, which is disadvantage to the scenario with limited\nresources. In this paper, we design an efficient symmetric network, called\n(ESNet), to address this problem. The whole network has nearly symmetric\narchitecture, which is mainly composed of a series of factorized convolution\nunit (FCU) and its parallel counterparts (PFCU). On one hand, the FCU adopts a\nwidely-used 1D factorized convolution in residual layers. On the other hand,\nthe parallel version employs a transform-split-transform-merge strategy in the\ndesignment of residual module, where the split branch adopts dilated\nconvolutions with different rate to enlarge receptive field. Our model has\nnearly 1.6M parameters, and is able to be performed over 62 FPS on a single GTX\n1080Ti GPU. The experiments demonstrate that our approach achieves\nstate-of-the-art results in terms of speed and accuracy trade-off for real-time\nsemantic segmentation on CityScapes dataset.",
            "authors": [
                "Yu Wang",
                "Quan Zhou",
                "Xiaofu Wu"
            ]
        },
        {
            "title": "Rethinking Atrous Convolution for Semantic Image Segmentation",
            "arxiv": "1706.05587",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.05587v3",
            "abstract": "In this work, we revisit atrous convolution, a powerful tool to explicitly\nadjust filter's field-of-view as well as control the resolution of feature\nresponses computed by Deep Convolutional Neural Networks, in the application of\nsemantic image segmentation. To handle the problem of segmenting objects at\nmultiple scales, we design modules which employ atrous convolution in cascade\nor in parallel to capture multi-scale context by adopting multiple atrous\nrates. Furthermore, we propose to augment our previously proposed Atrous\nSpatial Pyramid Pooling module, which probes convolutional features at multiple\nscales, with image-level features encoding global context and further boost\nperformance. We also elaborate on implementation details and share our\nexperience on training our system. The proposed `DeepLabv3' system\nsignificantly improves over our previous DeepLab versions without DenseCRF\npost-processing and attains comparable performance with other state-of-art\nmodels on the PASCAL VOC 2012 semantic image segmentation benchmark.",
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ]
        },
        {
            "title": "Deep Networks with Stochastic Depth",
            "arxiv": "1603.09382",
            "year": 2016,
            "url": "http://arxiv.org/abs/1603.09382v3",
            "abstract": "Very deep convolutional networks with hundreds of layers have led to\nsignificant reductions in error on competitive benchmarks. Although the\nunmatched expressiveness of the many layers can be highly desirable at test\ntime, training very deep networks comes with its own set of challenges. The\ngradients can vanish, the forward flow often diminishes, and the training time\ncan be painfully slow. To address these problems, we propose stochastic depth,\na training procedure that enables the seemingly contradictory setup to train\nshort networks and use deep networks at test time. We start with very deep\nnetworks but during training, for each mini-batch, randomly drop a subset of\nlayers and bypass them with the identity function. This simple approach\ncomplements the recent success of residual networks. It reduces training time\nsubstantially and improves the test error significantly on almost all data sets\nthat we used for evaluation. With stochastic depth we can increase the depth of\nresidual networks even beyond 1200 layers and still yield meaningful\nimprovements in test error (4.91% on CIFAR-10).",
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Weinberger"
            ]
        },
        {
            "title": "Identity Mappings in Deep Residual Networks",
            "arxiv": "1603.05027",
            "year": 2016,
            "url": "http://arxiv.org/abs/1603.05027v3",
            "abstract": "Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers",
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ]
        },
        {
            "title": "DiracNets: Training Very Deep Neural Networks Without Skip-Connections",
            "arxiv": "1706.00388",
            "year": 2017,
            "url": "http://arxiv.org/abs/1706.00388v2",
            "abstract": "Deep neural networks with skip-connections, such as ResNet, show excellent\nperformance in various image classification benchmarks. It is though observed\nthat the initial motivation behind them - training deeper networks - does not\nactually hold true, and the benefits come from increased capacity, rather than\nfrom depth. Motivated by this, and inspired from ResNet, we propose a simple\nDirac weight parameterization, which allows us to train very deep plain\nnetworks without explicit skip-connections, and achieve nearly the same\nperformance. This parameterization has a minor computational cost at training\ntime and no cost at all at inference, as both Dirac parameterization and batch\nnormalization can be folded into convolutional filters, so that network becomes\na simple chain of convolution-ReLU pairs. We are able to match ResNet-1001\naccuracy on CIFAR-10 with 28-layer wider plain DiracNet, and closely match\nResNets on ImageNet. Our parameterization also mostly eliminates the need of\ncareful initialization in residual and non-residual networks. The code and\nmodels for our experiments are available at\nhttps://github.com/szagoruyko/diracnets",
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ]
        },
        {
            "title": "Deep Pyramidal Residual Networks",
            "arxiv": "1610.02915",
            "year": 2016,
            "url": "http://arxiv.org/abs/1610.02915v4",
            "abstract": "Deep convolutional neural networks (DCNNs) have shown remarkable performance\nin image classification tasks in recent years. Generally, deep neural network\narchitectures are stacks consisting of a large number of convolutional layers,\nand they perform downsampling along the spatial dimension via pooling to reduce\nmemory usage. Concurrently, the feature map dimension (i.e., the number of\nchannels) is sharply increased at downsampling locations, which is essential to\nensure effective performance because it increases the diversity of high-level\nattributes. This also applies to residual networks and is very closely related\nto their performance. In this research, instead of sharply increasing the\nfeature map dimension at units that perform downsampling, we gradually increase\nthe feature map dimension at all units to involve as many locations as\npossible. This design, which is discussed in depth together with our new\ninsights, has proven to be an effective means of improving generalization\nability. Furthermore, we propose a novel residual unit capable of further\nimproving the classification accuracy with our new network architecture.\nExperiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown\nthat our network architecture has superior generalization ability compared to\nthe original residual networks. Code is available at\nhttps://github.com/jhkim89/PyramidNet}",
            "authors": [
                "Dongyoon Han",
                "Jiwhan Kim",
                "Junmo Kim"
            ]
        },
        {
            "title": "Dilated Residual Networks",
            "arxiv": "1705.09914",
            "year": 2017,
            "url": "http://arxiv.org/abs/1705.09914v1",
            "abstract": "Convolutional networks for image classification progressively reduce\nresolution until the image is represented by tiny feature maps in which the\nspatial structure of the scene is no longer discernible. Such loss of spatial\nacuity can limit image classification accuracy and complicate the transfer of\nthe model to downstream applications that require detailed scene understanding.\nThese problems can be alleviated by dilation, which increases the resolution of\noutput feature maps without reducing the receptive field of individual neurons.\nWe show that dilated residual networks (DRNs) outperform their non-dilated\ncounterparts in image classification without increasing the model's depth or\ncomplexity. We then study gridding artifacts introduced by dilation, develop an\napproach to removing these artifacts (`degridding'), and show that this further\nincreases the performance of DRNs. In addition, we show that the accuracy\nadvantage of DRNs is further magnified in downstream applications such as\nobject localization and semantic segmentation.",
            "authors": [
                "Fisher Yu",
                "Vladlen Koltun",
                "Thomas Funkhouser"
            ]
        },
        {
            "title": "QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions",
            "arxiv": "1910.10261",
            "year": 2019,
            "url": "http://arxiv.org/abs/1910.10261v1",
            "abstract": "We propose a new end-to-end neural acoustic model for automatic speech\nrecognition. The model is composed of multiple blocks with residual connections\nbetween them. Each block consists of one or more modules with 1D time-channel\nseparable convolutional layers, batch normalization, and ReLU layers. It is\ntrained with CTC loss. The proposed network achieves near state-of-the-art\naccuracy on LibriSpeech and Wall Street Journal, while having fewer parameters\nthan all competing models. We also demonstrate that this model can be\neffectively fine-tuned on new datasets.",
            "authors": [
                "Samuel Kriman",
                "Stanislav Beliaev",
                "Boris Ginsburg",
                "Jocelyn Huang",
                "Oleksii Kuchaiev",
                "Vitaly Lavrukhin",
                "Ryan Leary",
                "Jason Li",
                "Yang Zhang"
            ]
        },
        {
            "title": "Learning to Navigate for Fine-grained Classification",
            "arxiv": "1809.00287",
            "year": 2018,
            "url": "http://arxiv.org/abs/1809.00287v1",
            "abstract": "Fine-grained classification is challenging due to the difficulty of finding\ndiscriminative features. Finding those subtle traits that fully characterize\nthe object is not straightforward. To handle this circumstance, we propose a\nnovel self-supervision mechanism to effectively localize informative regions\nwithout the need of bounding-box/part annotations. Our model, termed NTS-Net\nfor Navigator-Teacher-Scrutinizer Network, consists of a Navigator agent, a\nTeacher agent and a Scrutinizer agent. In consideration of intrinsic\nconsistency between informativeness of the regions and their probability being\nground-truth class, we design a novel training paradigm, which enables\nNavigator to detect most informative regions under the guidance from Teacher.\nAfter that, the Scrutinizer scrutinizes the proposed regions from Navigator and\nmakes predictions. Our model can be viewed as a multi-agent cooperation,\nwherein agents benefit from each other, and make progress together. NTS-Net can\nbe trained end-to-end, while provides accurate fine-grained classification\npredictions as well as highly informative regions during inference. We achieve\nstate-of-the-art performance in extensive benchmark datasets.",
            "authors": [
                "Ze Yang",
                "Tiange Luo",
                "Dong Wang",
                "Zhiqiang Hu",
                "Jun Gao",
                "Liwei Wang"
            ]
        },
        {
            "title": "MixConv: Mixed Depthwise Convolutional Kernels",
            "arxiv": "1907.09595",
            "year": 2019,
            "url": "http://arxiv.org/abs/1907.09595v3",
            "abstract": "Depthwise convolution is becoming increasingly popular in modern efficient\nConvNets, but its kernel size is often overlooked. In this paper, we\nsystematically study the impact of different kernel sizes, and observe that\ncombining the benefits of multiple kernel sizes can lead to better accuracy and\nefficiency. Based on this observation, we propose a new mixed depthwise\nconvolution (MixConv), which naturally mixes up multiple kernel sizes in a\nsingle convolution. As a simple drop-in replacement of vanilla depthwise\nconvolution, our MixConv improves the accuracy and efficiency for existing\nMobileNets on both ImageNet classification and COCO object detection. To\ndemonstrate the effectiveness of MixConv, we integrate it into AutoML search\nspace and develop a new family of models, named as MixNets, which outperform\nprevious mobile models including MobileNetV2 [20] (ImageNet top-1 accuracy\n+4.2%), ShuffleNetV2 [16] (+3.5%), MnasNet [26] (+1.3%), ProxylessNAS [2]\n(+2.2%), and FBNet [27] (+2.0%). In particular, our MixNet-L achieves a new\nstate-of-the-art 78.9% ImageNet top-1 accuracy under typical mobile settings\n(<600M FLOPS). Code is at https://github.com/\ntensorflow/tpu/tree/master/models/official/mnasnet/mixnet",
            "authors": [
                "Mingxing Tan",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Visualizing and Understanding Convolutional Networks",
            "arxiv": "1311.2901",
            "year": 2013,
            "url": "http://arxiv.org/abs/1311.2901v3",
            "abstract": "Large Convolutional Network models have recently demonstrated impressive\nclassification performance on the ImageNet benchmark. However there is no clear\nunderstanding of why they perform so well, or how they might be improved. In\nthis paper we address both issues. We introduce a novel visualization technique\nthat gives insight into the function of intermediate feature layers and the\noperation of the classifier. We also perform an ablation study to discover the\nperformance contribution from different model layers. This enables us to find\nmodel architectures that outperform Krizhevsky \\etal on the ImageNet\nclassification benchmark. We show our ImageNet model generalizes well to other\ndatasets: when the softmax classifier is retrained, it convincingly beats the\ncurrent state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
            "authors": [
                "Matthew D Zeiler",
                "Rob Fergus"
            ]
        },
        {
            "title": "Residual Attention Network for Image Classification",
            "arxiv": "1704.06904",
            "year": 2017,
            "url": "http://arxiv.org/abs/1704.06904v1",
            "abstract": "In this work, we propose \"Residual Attention Network\", a convolutional neural\nnetwork using attention mechanism which can incorporate with state-of-art feed\nforward network architecture in an end-to-end training fashion. Our Residual\nAttention Network is built by stacking Attention Modules which generate\nattention-aware features. The attention-aware features from different modules\nchange adaptively as layers going deeper. Inside each Attention Module,\nbottom-up top-down feedforward structure is used to unfold the feedforward and\nfeedback attention process into a single feedforward process. Importantly, we\npropose attention residual learning to train very deep Residual Attention\nNetworks which can be easily scaled up to hundreds of layers. Extensive\nanalyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the\neffectiveness of every module mentioned above. Our Residual Attention Network\nachieves state-of-the-art object recognition performance on three benchmark\ndatasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and\nImageNet (4.8% single model and single crop, top-5 error). Note that, our\nmethod achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69%\nforward FLOPs comparing to ResNet-200. The experiment also demonstrates that\nour network is robust against noisy labels.",
            "authors": [
                "Fei Wang",
                "Mengqing Jiang",
                "Chen Qian",
                "Shuo Yang",
                "Cheng Li",
                "Honggang Zhang",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ]
        },
        {
            "title": "Pelee: A Real-Time Object Detection System on Mobile Devices",
            "arxiv": "1804.06882",
            "year": 2018,
            "url": "http://arxiv.org/abs/1804.06882v3",
            "abstract": "An increasing need of running Convolutional Neural Network (CNN) models on\nmobile devices with limited computing power and memory resource encourages\nstudies on efficient model design. A number of efficient architectures have\nbeen proposed in recent years, for example, MobileNet, ShuffleNet, and\nMobileNetV2. However, all these models are heavily dependent on depthwise\nseparable convolution which lacks efficient implementation in most deep\nlearning frameworks. In this study, we propose an efficient architecture named\nPeleeNet, which is built with conventional convolution instead. On ImageNet\nILSVRC 2012 dataset, our proposed PeleeNet achieves a higher accuracy and over\n1.8 times faster speed than MobileNet and MobileNetV2 on NVIDIA TX2. Meanwhile,\nPeleeNet is only 66% of the model size of MobileNet. We then propose a\nreal-time object detection system by combining PeleeNet with Single Shot\nMultiBox Detector (SSD) method and optimizing the architecture for fast speed.\nOur proposed detection system2, named Pelee, achieves 76.4% mAP (mean average\nprecision) on PASCAL VOC2007 and 22.4 mAP on MS COCO dataset at the speed of\n23.6 FPS on iPhone 8 and 125 FPS on NVIDIA TX2. The result on COCO outperforms\nYOLOv2 in consideration of a higher precision, 13.6 times lower computational\ncost and 11.3 times smaller model size.",
            "authors": [
                "Robert J. Wang",
                "Xiang Li",
                "Charles X. Ling"
            ]
        },
        {
            "title": "Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net",
            "arxiv": "1807.09441",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.09441v3",
            "abstract": "Convolutional neural networks (CNNs) have achieved great successes in many\ncomputer vision problems. Unlike existing works that designed CNN architectures\nto improve performance on a single task of a single domain and not\ngeneralizable, we present IBN-Net, a novel convolutional architecture, which\nremarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as\nwell as its generalization capacity on another domain (e.g. GTA5) without\nfinetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch\nNormalization (BN) as building blocks, and can be wrapped into many advanced\ndeep networks to improve their performances. This work has three key\ncontributions. (1) By delving into IN and BN, we disclose that IN learns\nfeatures that are invariant to appearance changes, such as colors, styles, and\nvirtuality/reality, while BN is essential for preserving content related\ninformation. (2) IBN-Net can be applied to many advanced deep architectures,\nsuch as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their\nperformance without increasing computational cost. (3) When applying the\ntrained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves\ncomparable improvements as domain adaptation methods, even without using data\nfrom the target domain. With IBN-Net, we won the 1st place on the WAD 2018\nChallenge Drivable Area track, with an mIoU of 86.18%.",
            "authors": [
                "Xingang Pan",
                "Ping Luo",
                "Jianping Shi",
                "Xiaoou Tang"
            ]
        },
        {
            "title": "DIANet: Dense-and-Implicit Attention Network",
            "arxiv": "1905.10671",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.10671v2",
            "abstract": "Attention networks have successfully boosted the performance in various\nvision problems. Previous works lay emphasis on designing a new attention\nmodule and individually plug them into the networks. Our paper proposes a\nnovel-and-simple framework that shares an attention module throughout different\nnetwork layers to encourage the integration of layer-wise information and this\nparameter-sharing module is referred as Dense-and-Implicit-Attention (DIA)\nunit. Many choices of modules can be used in the DIA unit. Since Long Short\nTerm Memory (LSTM) has a capacity of capturing long-distance dependency, we\nfocus on the case when the DIA unit is the modified LSTM (refer as DIA-LSTM).\nExperiments on benchmark datasets show that the DIA-LSTM unit is capable of\nemphasizing layer-wise feature interrelation and leads to significant\nimprovement of image classification accuracy. We further empirically show that\nthe DIA-LSTM has a strong regularization ability on stabilizing the training of\ndeep networks by the experiments with the removal of skip connections or Batch\nNormalization in the whole residual network. The code is released at\nhttps://github.com/gbup-group/DIANet.",
            "authors": [
                "Zhongzhan Huang",
                "Senwei Liang",
                "Mingfu Liang",
                "Haizhao Yang"
            ]
        },
        {
            "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation",
            "arxiv": "1511.00561",
            "year": 2015,
            "url": "http://arxiv.org/abs/1511.00561v3",
            "abstract": "We present a novel and practical deep fully convolutional neural network\narchitecture for semantic pixel-wise segmentation termed SegNet. This core\ntrainable segmentation engine consists of an encoder network, a corresponding\ndecoder network followed by a pixel-wise classification layer. The architecture\nof the encoder network is topologically identical to the 13 convolutional\nlayers in the VGG16 network. The role of the decoder network is to map the low\nresolution encoder feature maps to full input resolution feature maps for\npixel-wise classification. The novelty of SegNet lies is in the manner in which\nthe decoder upsamples its lower resolution input feature map(s). Specifically,\nthe decoder uses pooling indices computed in the max-pooling step of the\ncorresponding encoder to perform non-linear upsampling. This eliminates the\nneed for learning to upsample. The upsampled maps are sparse and are then\nconvolved with trainable filters to produce dense feature maps. We compare our\nproposed architecture with the widely adopted FCN and also with the well known\nDeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory\nversus accuracy trade-off involved in achieving good segmentation performance.\n  SegNet was primarily motivated by scene understanding applications. Hence, it\nis designed to be efficient both in terms of memory and computational time\nduring inference. It is also significantly smaller in the number of trainable\nparameters than other competing architectures. We also performed a controlled\nbenchmark of SegNet and other architectures on both road scenes and SUN RGB-D\nindoor scene segmentation tasks. We show that SegNet provides good performance\nwith competitive inference time and more efficient inference memory-wise as\ncompared to other architectures. We also provide a Caffe implementation of\nSegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.",
            "authors": [
                "Vijay Badrinarayanan",
                "Alex Kendall",
                "Roberto Cipolla"
            ]
        },
        {
            "title": "LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation",
            "arxiv": "1905.02423",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.02423v3",
            "abstract": "The extensive computational burden limits the usage of CNNs in mobile devices\nfor dense estimation tasks. In this paper, we present a lightweight network to\naddress this problem,namely LEDNet, which employs an asymmetric encoder-decoder\narchitecture for the task of real-time semantic segmentation.More specifically,\nthe encoder adopts a ResNet as backbone network, where two new operations,\nchannel split and shuffle, are utilized in each residual block to greatly\nreduce computation cost while maintaining higher segmentation accuracy. On the\nother hand, an attention pyramid network (APN) is employed in the decoder to\nfurther lighten the entire network complexity. Our model has less than 1M\nparameters,and is able to run at over 71 FPS in a single GTX 1080Ti GPU. The\ncomprehensive experiments demonstrate that our approach achieves\nstate-of-the-art results in terms of speed and accuracy trade-off on CityScapes\ndataset.",
            "authors": [
                "Yu Wang",
                "Quan Zhou",
                "Jia Liu",
                "Jian Xiong",
                "Guangwei Gao",
                "Xiaofu Wu",
                "Longin Jan Latecki"
            ]
        },
        {
            "title": "Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours",
            "arxiv": "1904.02877",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.02877v1",
            "abstract": "Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the runtime constraint of a mobile\ndevice? Neural architecture search (NAS) has revolutionized the design of\nhardware-efficient ConvNets by automating this process. However, the NAS\nproblem remains challenging due to the combinatorially large design space,\ncausing a significant searching time (at least 200 GPU-hours). To alleviate\nthis complexity, we propose Single-Path NAS, a novel differentiable NAS method\nfor designing hardware-efficient ConvNets in less than 4 hours. Our\ncontributions are as follows: 1. Single-path search space: Compared to previous\ndifferentiable NAS methods, Single-Path NAS uses one single-path\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters, hence drastically decreasing the number of\ntrainable parameters and the search cost down to few epochs. 2.\nHardware-efficient ImageNet classification: Single-Path NAS achieves 74.96%\ntop-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is\nstate-of-the-art accuracy compared to NAS methods with similar constraints\n(<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30\nTPU-hours), which is up to 5,000x faster compared to prior work. 4.\nReproducibility: Unlike all recent mobile-efficient NAS methods which only\nrelease pretrained models, we open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.",
            "authors": [
                "Dimitrios Stamoulis",
                "Ruizhou Ding",
                "Di Wang",
                "Dimitrios Lymberopoulos",
                "Bodhi Priyantha",
                "Jie Liu",
                "Diana Marculescu"
            ]
        },
        {
            "title": "Progressive Neural Architecture Search",
            "arxiv": "1712.00559",
            "year": 2017,
            "url": "http://arxiv.org/abs/1712.00559v3",
            "abstract": "We propose a new method for learning the structure of convolutional neural\nnetworks (CNNs) that is more efficient than recent state-of-the-art methods\nbased on reinforcement learning and evolutionary algorithms. Our approach uses\na sequential model-based optimization (SMBO) strategy, in which we search for\nstructures in order of increasing complexity, while simultaneously learning a\nsurrogate model to guide the search through structure space. Direct comparison\nunder the same search space shows that our method is up to 5 times more\nefficient than the RL method of Zoph et al. (2018) in terms of number of models\nevaluated, and 8 times faster in terms of total compute. The structures we\ndiscover in this way achieve state of the art classification accuracies on\nCIFAR-10 and ImageNet.",
            "authors": [
                "Chenxi Liu",
                "Barret Zoph",
                "Maxim Neumann",
                "Jonathon Shlens",
                "Wei Hua",
                "Li-Jia Li",
                "Li Fei-Fei",
                "Alan Yuille",
                "Jonathan Huang",
                "Kevin Murphy"
            ]
        },
        {
            "title": "Shake-Shake regularization",
            "arxiv": "1705.07485",
            "year": 2017,
            "url": "http://arxiv.org/abs/1705.07485v2",
            "abstract": "The method introduced in this paper aims at helping deep learning\npractitioners faced with an overfit problem. The idea is to replace, in a\nmulti-branch network, the standard summation of parallel branches with a\nstochastic affine combination. Applied to 3-branch residual networks,\nshake-shake regularization improves on the best single shot published results\non CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%.\nExperiments on architectures without skip connections or Batch Normalization\nshow encouraging results and open the door to a large set of applications. Code\nis available at https://github.com/xgastaldi/shake-shake",
            "authors": [
                "Xavier Gastaldi"
            ]
        },
        {
            "title": "Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation",
            "arxiv": "1911.10529",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.10529v1",
            "abstract": "We rethink a well-know bottom-up approach for multi-person pose estimation\nand propose an improved one. The improved approach surpasses the baseline\nsignificantly thanks to (1) an intuitional yet more sensible representation,\nwhich we refer to as body parts to encode the connection information between\nkeypoints, (2) an improved stacked hourglass network with attention mechanisms,\n(3) a novel focal L2 loss which is dedicated to hard keypoint and keypoint\nassociation (body part) mining, and (4) a robust greedy keypoint assignment\nalgorithm for grouping the detected keypoints into individual poses. Our\napproach not only works straightforwardly but also outperforms the baseline by\nabout 15% in average precision and is comparable to the state of the art on the\nMS-COCO test-dev dataset. The code and pre-trained models are publicly\navailable online.",
            "authors": [
                "Jia Li",
                "Wen Su",
                "Zengfu Wang"
            ]
        },
        {
            "title": "An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection",
            "arxiv": "1904.09730",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.09730v1",
            "abstract": "As DenseNet conserves intermediate features with diverse receptive fields by\naggregating them with dense connection, it shows good performance on the object\ndetection task. Although feature reuse enables DenseNet to produce strong\nfeatures with a small number of model parameters and FLOPs, the detector with\nDenseNet backbone shows rather slow speed and low energy efficiency. We find\nthe linearly increasing input channel by dense connection leads to heavy memory\naccess cost, which causes computation overhead and more energy consumption. To\nsolve the inefficiency of DenseNet, we propose an energy and computation\nefficient architecture called VoVNet comprised of One-Shot Aggregation (OSA).\nThe OSA not only adopts the strength of DenseNet that represents diversified\nfeatures with multi receptive fields but also overcomes the inefficiency of\ndense connection by aggregating all features only once in the last feature\nmaps. To validate the effectiveness of VoVNet as a backbone network, we design\nboth lightweight and large-scale VoVNet and apply them to one-stage and\ntwo-stage object detectors. Our VoVNet based detectors outperform DenseNet\nbased ones with 2x faster speed and the energy consumptions are reduced by 1.6x\n- 4.1x. In addition to DenseNet, VoVNet also outperforms widely used ResNet\nbackbone with faster speed and better energy efficiency. In particular, the\nsmall object detection performance has been significantly improved over\nDenseNet and ResNet.",
            "authors": [
                "Youngwan Lee",
                "Joong-won Hwang",
                "Sangrok Lee",
                "Yuseok Bae",
                "Jongyoul Park"
            ]
        },
        {
            "title": "ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network",
            "arxiv": "1811.11431",
            "year": 2018,
            "url": "http://arxiv.org/abs/1811.11431v3",
            "abstract": "We introduce a light-weight, power efficient, and general purpose\nconvolutional neural network, ESPNetv2, for modeling visual and sequential\ndata. Our network uses group point-wise and depth-wise dilated separable\nconvolutions to learn representations from a large effective receptive field\nwith fewer FLOPs and parameters. The performance of our network is evaluated on\nfour different tasks: (1) object classification, (2) semantic segmentation, (3)\nobject detection, and (4) language modeling. Experiments on these tasks,\nincluding image classification on the ImageNet and language modeling on the\nPenTree bank dataset, demonstrate the superior performance of our method over\nthe state-of-the-art methods. Our network outperforms ESPNet by 4-5% and has\n2-4x fewer FLOPs on the PASCAL VOC and the Cityscapes dataset. Compared to\nYOLOv2 on the MS-COCO object detection, ESPNetv2 delivers 4.4% higher accuracy\nwith 6x fewer FLOPs. Our experiments show that ESPNetv2 is much more power\nefficient than existing state-of-the-art efficient methods including\nShuffleNets and MobileNets. Our code is open-source and available at\nhttps://github.com/sacmehta/ESPNetv2",
            "authors": [
                "Sachin Mehta",
                "Mohammad Rastegari",
                "Linda Shapiro",
                "Hannaneh Hajishirzi"
            ]
        },
        {
            "title": "XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera",
            "arxiv": "1907.00837",
            "year": 2019,
            "url": "http://arxiv.org/abs/1907.00837v2",
            "abstract": "We present a real-time approach for multi-person 3D motion capture at over 30\nfps using a single RGB camera. It operates successfully in generic scenes which\nmay contain occlusions by objects and by other people. Our method operates in\nsubsequent stages. The first stage is a convolutional neural network (CNN) that\nestimates 2D and 3D pose features along with identity assignments for all\nvisible joints of all individuals.We contribute a new architecture for this\nCNN, called SelecSLS Net, that uses novel selective long and short range skip\nconnections to improve the information flow allowing for a drastically faster\nnetwork without compromising accuracy. In the second stage, a fully connected\nneural network turns the possibly partial (on account of occlusion) 2Dpose and\n3Dpose features for each subject into a complete 3Dpose estimate per\nindividual. The third stage applies space-time skeletal model fitting to the\npredicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose,\nand enforce temporal coherence. Our method returns the full skeletal pose in\njoint angles for each subject. This is a further key distinction from previous\nwork that do not produce joint angle results of a coherent skeleton in real\ntime for multi-person scenes. The proposed system runs on consumer hardware at\na previously unseen speed of more than 30 fps given 512x320 images as input\nwhile achieving state-of-the-art accuracy, which we will demonstrate on a range\nof challenging real-world scenes.",
            "authors": [
                "Dushyant Mehta",
                "Oleksandr Sotnychenko",
                "Franziska Mueller",
                "Weipeng Xu",
                "Mohamed Elgharib",
                "Pascal Fua",
                "Hans-Peter Seidel",
                "Helge Rhodin",
                "Gerard Pons-Moll",
                "Christian Theobalt"
            ]
        },
        {
            "title": "FD-MobileNet: Improved MobileNet with a Fast Downsampling Strategy",
            "arxiv": "1802.03750",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.03750v1",
            "abstract": "We present Fast-Downsampling MobileNet (FD-MobileNet), an efficient and\naccurate network for very limited computational budgets (e.g., 10-140 MFLOPs).\nOur key idea is applying an aggressive downsampling strategy to MobileNet\nframework. In FD-MobileNet, we perform 32$\\times$ downsampling within 12\nlayers, only half the layers in the original MobileNet. This design brings\nthree advantages: (i) It remarkably reduces the computational cost. (ii) It\nincreases the information capacity and achieves significant performance\nimprovements. (iii) It is engineering-friendly and provides fast actual\ninference speed. Experiments on ILSVRC 2012 and PASCAL VOC 2007 datasets\ndemonstrate that FD-MobileNet consistently outperforms MobileNet and achieves\ncomparable results with ShuffleNet under different computational budgets, for\ninstance, surpassing MobileNet by 5.5% on the ILSVRC 2012 top-1 accuracy and\n3.6% on the VOC 2007 mAP under a complexity of 12 MFLOPs. On an ARM-based\ndevice, FD-MobileNet achieves 1.11$\\times$ inference speedup over MobileNet and\n1.82$\\times$ over ShuffleNet under the same complexity.",
            "authors": [
                "Zheng Qin",
                "Zhaoning Zhang",
                "Xiaotao Chen",
                "Yuxing Peng"
            ]
        },
        {
            "title": "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks",
            "arxiv": "1611.05725",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.05725v2",
            "abstract": "A number of studies have shown that increasing the depth or width of\nconvolutional networks is a rewarding approach to improve the performance of\nimage recognition. In our study, however, we observed difficulties along both\ndirections. On one hand, the pursuit for very deep networks is met with a\ndiminishing return and increased training difficulty; on the other hand,\nwidening a network would result in a quadratic growth in both computational\ncost and memory demand. These difficulties motivate us to explore structural\ndiversity in designing deep networks, a new dimension beyond just depth and\nwidth. Specifically, we present a new family of modules, namely the\nPolyInception, which can be flexibly inserted in isolation or in a composition\nas replacements of different parts of a network. Choosing PolyInception modules\nwith the guidance of architectural efficiency can improve the expressive power\nwhile preserving comparable computational cost. The Very Deep PolyNet, designed\nfollowing this direction, demonstrates substantial improvements over the\nstate-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2,\nit reduces the top-5 validation error on single crops from 4.9% to 4.25%, and\nthat on multi-crops from 3.7% to 3.45%.",
            "authors": [
                "Xingcheng Zhang",
                "Zhizhong Li",
                "Chen Change Loy",
                "Dahua Lin"
            ]
        },
        {
            "title": "Jasper: An End-to-End Convolutional Neural Acoustic Model",
            "arxiv": "1904.03288",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.03288v3",
            "abstract": "In this paper, we report state-of-the-art results on LibriSpeech among\nend-to-end speech recognition models without any external training data. Our\nmodel, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout,\nand residual connections. To improve training, we further introduce a new\nlayer-wise optimizer called NovoGrad. Through experiments, we demonstrate that\nthe proposed deep architecture performs as well or better than more complex\nchoices. Our deepest Jasper variant uses 54 convolutional layers. With this\narchitecture, we achieve 2.95% WER using a beam-search decoder with an external\nneural language model and 3.86% WER with a greedy decoder on LibriSpeech\ntest-clean. We also report competitive results on the Wall Street Journal and\nthe Hub5'00 conversational evaluation datasets.",
            "authors": [
                "Jason Li",
                "Vitaly Lavrukhin",
                "Boris Ginsburg",
                "Ryan Leary",
                "Oleksii Kuchaiev",
                "Jonathan M. Cohen",
                "Huyen Nguyen",
                "Ravi Teja Gadde"
            ]
        },
        {
            "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design",
            "arxiv": "1807.11164",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.11164v1",
            "abstract": "Currently, the neural network architecture design is mostly guided by the\n\\emph{indirect} metric of computation complexity, i.e., FLOPs. However, the\n\\emph{direct} metric, e.g., speed, also depends on the other factors such as\nmemory access cost and platform characterics. Thus, this work proposes to\nevaluate the direct metric on the target platform, beyond only considering\nFLOPs. Based on a series of controlled experiments, this work derives several\npractical \\emph{guidelines} for efficient network design. Accordingly, a new\narchitecture is presented, called \\emph{ShuffleNet V2}. Comprehensive ablation\nexperiments verify that our model is the state-of-the-art in terms of speed and\naccuracy tradeoff.",
            "authors": [
                "Ningning Ma",
                "Xiangyu Zhang",
                "Hai-Tao Zheng",
                "Jian Sun"
            ]
        },
        {
            "title": "FractalNet: Ultra-Deep Neural Networks without Residuals",
            "arxiv": "1605.07648",
            "year": 2016,
            "url": "http://arxiv.org/abs/1605.07648v4",
            "abstract": "We introduce a design strategy for neural network macro-architecture based on\nself-similarity. Repeated application of a simple expansion rule generates deep\nnetworks whose structural layouts are precisely truncated fractals. These\nnetworks contain interacting subpaths of different lengths, but do not include\nany pass-through or residual connections; every internal signal is transformed\nby a filter and nonlinearity before being seen by subsequent layers. In\nexperiments, fractal networks match the excellent performance of standard\nresidual networks on both CIFAR and ImageNet classification tasks, thereby\ndemonstrating that residual representations may not be fundamental to the\nsuccess of extremely deep convolutional neural networks. Rather, the key may be\nthe ability to transition, during training, from effectively shallow to deep.\nWe note similarities with student-teacher behavior and develop drop-path, a\nnatural extension of dropout, to regularize co-adaptation of subpaths in\nfractal architectures. Such regularization allows extraction of\nhigh-performance fixed-depth subnetworks. Additionally, fractal networks\nexhibit an anytime property: shallow subnetworks provide a quick answer, while\ndeeper subnetworks, with higher latency, provide a more accurate answer.",
            "authors": [
                "Gustav Larsson",
                "Michael Maire",
                "Gregory Shakhnarovich"
            ]
        },
        {
            "title": "DARTS: Differentiable Architecture Search",
            "arxiv": "1806.09055",
            "year": 2018,
            "url": "http://arxiv.org/abs/1806.09055v2",
            "abstract": "This paper addresses the scalability challenge of architecture search by\nformulating the task in a differentiable manner. Unlike conventional approaches\nof applying evolution or reinforcement learning over a discrete and\nnon-differentiable search space, our method is based on the continuous\nrelaxation of the architecture representation, allowing efficient search of the\narchitecture using gradient descent. Extensive experiments on CIFAR-10,\nImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in\ndiscovering high-performance convolutional architectures for image\nclassification and recurrent architectures for language modeling, while being\norders of magnitude faster than state-of-the-art non-differentiable techniques.\nOur implementation has been made publicly available to facilitate further\nresearch on efficient architecture search algorithms.",
            "authors": [
                "Hanxiao Liu",
                "Karen Simonyan",
                "Yiming Yang"
            ]
        },
        {
            "title": "Pyramid Scene Parsing Network",
            "arxiv": "1612.01105",
            "year": 2016,
            "url": "http://arxiv.org/abs/1612.01105v2",
            "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse\nscenes. In this paper, we exploit the capability of global context information\nby different-region-based context aggregation through our pyramid pooling\nmodule together with the proposed pyramid scene parsing network (PSPNet). Our\nglobal prior representation is effective to produce good quality results on the\nscene parsing task, while PSPNet provides a superior framework for pixel-level\nprediction tasks. The proposed approach achieves state-of-the-art performance\non various datasets. It came first in ImageNet scene parsing challenge 2016,\nPASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new\nrecord of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on\nCityscapes.",
            "authors": [
                "Hengshuang Zhao",
                "Jianping Shi",
                "Xiaojuan Qi",
                "Xiaogang Wang",
                "Jiaya Jia"
            ]
        },
        {
            "title": "ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions",
            "arxiv": "1809.01330",
            "year": 2018,
            "url": "http://arxiv.org/abs/1809.01330v1",
            "abstract": "Convolutional neural networks (CNNs) have shown great capability of solving\nvarious artificial intelligence tasks. However, the increasing model size has\nraised challenges in employing them in resource-limited applications. In this\nwork, we propose to compress deep models by using channel-wise convolutions,\nwhich re- place dense connections among feature maps with sparse ones in CNNs.\nBased on this novel operation, we build light-weight CNNs known as ChannelNets.\nChannel- Nets use three instances of channel-wise convolutions; namely group\nchannel-wise convolutions, depth-wise separable channel-wise convolutions, and\nthe convolu- tional classification layer. Compared to prior CNNs designed for\nmobile devices, ChannelNets achieve a significant reduction in terms of the\nnumber of parameters and computational cost without loss in accuracy. Notably,\nour work represents the first attempt to compress the fully-connected\nclassification layer, which usually accounts for about 25% of total parameters\nin compact CNNs. Experimental results on the ImageNet dataset demonstrate that\nChannelNets achieve consistently better performance compared to prior methods.",
            "authors": [
                "Hongyang Gao",
                "Zhengyang Wang",
                "Shuiwang Ji"
            ]
        },
        {
            "title": "CBAM: Convolutional Block Attention Module",
            "arxiv": "1807.06521",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.06521v2",
            "abstract": "We propose Convolutional Block Attention Module (CBAM), a simple yet\neffective attention module for feed-forward convolutional neural networks.\nGiven an intermediate feature map, our module sequentially infers attention\nmaps along two separate dimensions, channel and spatial, then the attention\nmaps are multiplied to the input feature map for adaptive feature refinement.\nBecause CBAM is a lightweight and general module, it can be integrated into any\nCNN architectures seamlessly with negligible overheads and is end-to-end\ntrainable along with base CNNs. We validate our CBAM through extensive\nexperiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets.\nOur experiments show consistent improvements in classification and detection\nperformances with various models, demonstrating the wide applicability of CBAM.\nThe code and models will be publicly available.",
            "authors": [
                "Sanghyun Woo",
                "Jongchan Park",
                "Joon-Young Lee",
                "In So Kweon"
            ]
        },
        {
            "title": "Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution",
            "arxiv": "1904.05049",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.05049v3",
            "abstract": "In natural images, information is conveyed at different frequencies where\nhigher frequencies are usually encoded with fine details and lower frequencies\nare usually encoded with global structures. Similarly, the output feature maps\nof a convolution layer can also be seen as a mixture of information at\ndifferent frequencies. In this work, we propose to factorize the mixed feature\nmaps by their frequencies, and design a novel Octave Convolution (OctConv)\noperation to store and process feature maps that vary spatially \"slower\" at a\nlower spatial resolution reducing both memory and computation cost. Unlike\nexisting multi-scale methods, OctConv is formulated as a single, generic,\nplug-and-play convolutional unit that can be used as a direct replacement of\n(vanilla) convolutions without any adjustments in the network architecture. It\nis also orthogonal and complementary to methods that suggest better topologies\nor reduce channel-wise redundancy like group or depth-wise convolutions. We\nexperimentally show that by simply replacing convolutions with OctConv, we can\nconsistently boost accuracy for both image and video recognition tasks, while\nreducing memory and computational cost. An OctConv-equipped ResNet-152 can\nachieve 82.9% top-1 classification accuracy on ImageNet with merely 22.2\nGFLOPs.",
            "authors": [
                "Yunpeng Chen",
                "Haoqi Fan",
                "Bing Xu",
                "Zhicheng Yan",
                "Yannis Kalantidis",
                "Marcus Rohrbach",
                "Shuicheng Yan",
                "Jiashi Feng"
            ]
        },
        {
            "title": "ResNeSt: Split-Attention Networks",
            "arxiv": "2004.08955",
            "year": 2020,
            "url": "http://arxiv.org/abs/2004.08955v2",
            "abstract": "It is well known that featuremap attention and multi-path representation are\nimportant for visual recognition. In this paper, we present a modularized\narchitecture, which applies the channel-wise attention on different network\nbranches to leverage their success in capturing cross-feature interactions and\nlearning diverse representations. Our design results in a simple and unified\ncomputation block, which can be parameterized using only a few variables. Our\nmodel, named ResNeSt, outperforms EfficientNet in accuracy and latency\ntrade-off on image classification. In addition, ResNeSt has achieved superior\ntransfer learning results on several public benchmarks serving as the backbone,\nand has been adopted by the winning entries of COCO-LVIS challenge. The source\ncode for complete system and pretrained models are publicly available.",
            "authors": [
                "Hang Zhang",
                "Chongruo Wu",
                "Zhongyue Zhang",
                "Yi Zhu",
                "Haibin Lin",
                "Zhi Zhang",
                "Yue Sun",
                "Tong He",
                "Jonas Mueller",
                "R. Manmatha",
                "Mu Li",
                "Alexander Smola"
            ]
        },
        {
            "title": "Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose",
            "arxiv": "1811.12004",
            "year": 2018,
            "url": "http://arxiv.org/abs/1811.12004v1",
            "abstract": "In this work we adapt multi-person pose estimation architecture to use it on\nedge devices. We follow the bottom-up approach from OpenPose, the winner of\nCOCO 2016 Keypoints Challenge, because of its decent quality and robustness to\nnumber of people inside the frame. With proposed network design and optimized\npost-processing code the full solution runs at 28 frames per second (fps) on\nIntel$\\unicode{xAE}$ NUC 6i7KYB mini PC and 26 fps on Core$^{TM}$ i7-6850K CPU.\nThe network model has 4.1M parameters and 9 billions floating-point operations\n(GFLOPs) complexity, which is just ~15% of the baseline 2-stage OpenPose with\nalmost the same quality. The code and model are available as a part of\nIntel$\\unicode{xAE}$ OpenVINO$^{TM}$ Toolkit.",
            "authors": [
                "Daniil Osokin"
            ]
        },
        {
            "title": "Simple Baselines for Human Pose Estimation and Tracking",
            "arxiv": "1804.06208",
            "year": 2018,
            "url": "http://arxiv.org/abs/1804.06208v2",
            "abstract": "There has been significant progress on pose estimation and increasing\ninterests on pose tracking in recent years. At the same time, the overall\nalgorithm and system complexity increases as well, making the algorithm\nanalysis and comparison more difficult. This work provides simple and effective\nbaseline methods. They are helpful for inspiring and evaluating new ideas for\nthe field. State-of-the-art results are achieved on challenging benchmarks. The\ncode will be available at https://github.com/leoxiaobin/pose.pytorch.",
            "authors": [
                "Bin Xiao",
                "Haiping Wu",
                "Yichen Wei"
            ]
        },
        {
            "title": "ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time",
            "arxiv": "1805.04554",
            "year": 2018,
            "url": "http://arxiv.org/abs/1805.04554v4",
            "abstract": "Modern deep learning architectures produce highly accurate results on many\nchallenging semantic segmentation datasets. State-of-the-art methods are,\nhowever, not directly transferable to real-time applications or embedded\ndevices, since naive adaptation of such systems to reduce computational cost\n(speed, memory and energy) causes a significant drop in accuracy. We propose\nContextNet, a new deep neural network architecture which builds on factorized\nconvolution, network compression and pyramid representation to produce\ncompetitive semantic segmentation in real-time with low memory requirement.\nContextNet combines a deep network branch at low resolution that captures\nglobal context information efficiently with a shallow branch that focuses on\nhigh-resolution segmentation details. We analyse our network in a thorough\nablation study and present results on the Cityscapes dataset, achieving 66.1%\naccuracy at 18.3 frames per second at full (1024x2048) resolution (41.9 fps\nwith pipelined computations for streamed data).",
            "authors": [
                "Rudra P K Poudel",
                "Ujwal Bonde",
                "Stephan Liwicki",
                "Christopher Zach"
            ]
        },
        {
            "title": "BAM: Bottleneck Attention Module",
            "arxiv": "1807.06514",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.06514v2",
            "abstract": "Recent advances in deep neural networks have been developed via architecture\nsearch for stronger representational power. In this work, we focus on the\neffect of attention in general deep neural networks. We propose a simple and\neffective attention module, named Bottleneck Attention Module (BAM), that can\nbe integrated with any feed-forward convolutional neural networks. Our module\ninfers an attention map along two separate pathways, channel and spatial. We\nplace our module at each bottleneck of models where the downsampling of feature\nmaps occurs. Our module constructs a hierarchical attention at bottlenecks with\na number of parameters and it is trainable in an end-to-end manner jointly with\nany feed-forward models. We validate our BAM through extensive experiments on\nCIFAR-100, ImageNet-1K, VOC 2007 and MS COCO benchmarks. Our experiments show\nconsistent improvement in classification and detection performances with\nvarious models, demonstrating the wide applicability of BAM. The code and\nmodels will be publicly available.",
            "authors": [
                "Jongchan Park",
                "Sanghyun Woo",
                "Joon-Young Lee",
                "In So Kweon"
            ]
        },
        {
            "title": "Multi-Scale Dense Networks for Resource Efficient Image Classification",
            "arxiv": "1703.09844",
            "year": 2017,
            "url": "http://arxiv.org/abs/1703.09844v5",
            "abstract": "In this paper we investigate image classification with computational resource\nlimits at test time. Two such settings are: 1. anytime classification, where\nthe network's prediction for a test example is progressively updated,\nfacilitating the output of a prediction at any time; and 2. budgeted batch\nclassification, where a fixed amount of computation is available to classify a\nset of examples that can be spent unevenly across \"easier\" and \"harder\" inputs.\nIn contrast to most prior work, such as the popular Viola and Jones algorithm,\nour approach is based on convolutional neural networks. We train multiple\nclassifiers with varying resource demands, which we adaptively apply during\ntest time. To maximally re-use computation between the classifiers, we\nincorporate them as early-exits into a single deep convolutional neural network\nand inter-connect them with dense connectivity. To facilitate high quality\nclassification early on, we use a two-dimensional multi-scale network\narchitecture that maintains coarse and fine level features all-throughout the\nnetwork. Experiments on three image-classification tasks demonstrate that our\nframework substantially improves the existing state-of-the-art in both\nsettings.",
            "authors": [
                "Gao Huang",
                "Danlu Chen",
                "Tianhong Li",
                "Felix Wu",
                "Laurens van der Maaten",
                "Kilian Q. Weinberger"
            ]
        },
        {
            "title": "SINet: Extreme Lightweight Portrait Segmentation Networks with Spatial Squeeze Modules and Information Blocking Decoder",
            "arxiv": "1911.09099",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.09099v4",
            "abstract": "Designing a lightweight and robust portrait segmentation algorithm is an\nimportant task for a wide range of face applications. However, the problem has\nbeen considered as a subset of the object segmentation problem and less handled\nin the semantic segmentation field. Obviously, portrait segmentation has its\nunique requirements. First, because the portrait segmentation is performed in\nthe middle of a whole process of many real-world applications, it requires\nextremely lightweight models. Second, there has not been any public datasets in\nthis domain that contain a sufficient number of images with unbiased\nstatistics. To solve the first problem, we introduce the new extremely\nlightweight portrait segmentation model SINet, containing an information\nblocking decoder and spatial squeeze modules. The information blocking decoder\nuses confidence estimates to recover local spatial information without spoiling\nglobal consistency. The spatial squeeze module uses multiple receptive fields\nto cope with various sizes of consistency in the image. To tackle the second\nproblem, we propose a simple method to create additional portrait segmentation\ndata which can improve accuracy on the EG1800 dataset. In our qualitative and\nquantitative analysis on the EG1800 dataset, we show that our method\noutperforms various existing lightweight segmentation models. Our method\nreduces the number of parameters from 2.1M to 86.9K (around 95.9% reduction),\nwhile maintaining the accuracy under an 1% margin from the state-of-the-art\nportrait segmentation method. We also show our model is successfully executed\non a real mobile device with 100.6 FPS. In addition, we demonstrate that our\nmethod can be used for general semantic segmentation on the Cityscapes dataset.\nThe code and dataset are available in\nhttps://github.com/HYOJINPARK/ExtPortraitSeg .",
            "authors": [
                "Hyojin Park",
                "Lars Lowe Sj\u00f6sund",
                "YoungJoon Yoo",
                "Nicolas Monet",
                "Jihwan Bang",
                "Nojun Kwak"
            ]
        },
        {
            "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "arxiv": "1905.11946",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.11946v5",
            "abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed\nresource budget, and then scaled up for better accuracy if more resources are\navailable. In this paper, we systematically study model scaling and identify\nthat carefully balancing network depth, width, and resolution can lead to\nbetter performance. Based on this observation, we propose a new scaling method\nthat uniformly scales all dimensions of depth/width/resolution using a simple\nyet highly effective compound coefficient. We demonstrate the effectiveness of\nthis method on scaling up MobileNets and ResNet.\n  To go even further, we use neural architecture search to design a new\nbaseline network and scale it up to obtain a family of models, called\nEfficientNets, which achieve much better accuracy and efficiency than previous\nConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3%\ntop-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on\ninference than the best existing ConvNet. Our EfficientNets also transfer well\nand achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%),\nand 3 other transfer learning datasets, with an order of magnitude fewer\nparameters. Source code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.",
            "authors": [
                "Mingxing Tan",
                "Quoc V. Le"
            ]
        },
        {
            "title": "Res2Net: A New Multi-scale Backbone Architecture",
            "arxiv": "1904.01169",
            "year": 2019,
            "url": "http://arxiv.org/abs/1904.01169v3",
            "abstract": "Representing features at multiple scales is of great importance for numerous\nvision tasks. Recent advances in backbone convolutional neural networks (CNNs)\ncontinually demonstrate stronger multi-scale representation ability, leading to\nconsistent performance gains on a wide range of applications. However, most\nexisting methods represent the multi-scale features in a layer-wise manner. In\nthis paper, we propose a novel building block for CNNs, namely Res2Net, by\nconstructing hierarchical residual-like connections within one single residual\nblock. The Res2Net represents multi-scale features at a granular level and\nincreases the range of receptive fields for each network layer. The proposed\nRes2Net block can be plugged into the state-of-the-art backbone CNN models,\ne.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these\nmodels and demonstrate consistent performance gains over baseline models on\nwidely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies\nand experimental results on representative computer vision tasks, i.e., object\ndetection, class activation mapping, and salient object detection, further\nverify the superiority of the Res2Net over the state-of-the-art baseline\nmethods. The source code and trained models are available on\nhttps://mmcheng.net/res2net/.",
            "authors": [
                "Shang-Hua Gao",
                "Ming-Ming Cheng",
                "Kai Zhao",
                "Xin-Yu Zhang",
                "Ming-Hsuan Yang",
                "Philip Torr"
            ]
        },
        {
            "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
            "arxiv": "1807.11626",
            "year": 2018,
            "url": "http://arxiv.org/abs/1807.11626v3",
            "abstract": "Designing convolutional neural networks (CNN) for mobile devices is\nchallenging because mobile models need to be small and fast, yet still\naccurate. Although significant efforts have been dedicated to design and\nimprove mobile CNNs on all dimensions, it is very difficult to manually balance\nthese trade-offs when there are so many architectural possibilities to\nconsider. In this paper, we propose an automated mobile neural architecture\nsearch (MNAS) approach, which explicitly incorporate model latency into the\nmain objective so that the search can identify a model that achieves a good\ntrade-off between accuracy and latency. Unlike previous work, where latency is\nconsidered via another, often inaccurate proxy (e.g., FLOPS), our approach\ndirectly measures real-world inference latency by executing the model on mobile\nphones. To further strike the right balance between flexibility and search\nspace size, we propose a novel factorized hierarchical search space that\nencourages layer diversity throughout the network. Experimental results show\nthat our approach consistently outperforms state-of-the-art mobile CNN models\nacross multiple vision tasks. On the ImageNet classification task, our MnasNet\nachieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x\nfaster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than\nNASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP\nquality than MobileNets for COCO object detection. Code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/mnasnet",
            "authors": [
                "Mingxing Tan",
                "Bo Chen",
                "Ruoming Pang",
                "Vijay Vasudevan",
                "Mark Sandler",
                "Andrew Howard",
                "Quoc V. Le"
            ]
        },
        {
            "title": "i-RevNet: Deep Invertible Networks",
            "arxiv": "1802.07088",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.07088v1",
            "abstract": "It is widely believed that the success of deep convolutional networks is\nbased on progressively discarding uninformative variability about the input\nwith respect to the problem at hand. This is supported empirically by the\ndifficulty of recovering images from their hidden representations, in most\ncommonly used network architectures. In this paper we show via a one-to-one\nmapping that this loss of information is not a necessary condition to learn\nrepresentations that generalize well on complicated problems, such as ImageNet.\nVia a cascade of homeomorphic layers, we build the i-RevNet, a network that can\nbe fully inverted up to the final projection onto the classes, i.e. no\ninformation is discarded. Building an invertible architecture is difficult, for\none, because the local inversion is ill-conditioned, we overcome this by\nproviding an explicit inverse. An analysis of i-RevNets learned representations\nsuggests an alternative explanation for the success of deep networks by a\nprogressive contraction and linear separation with depth. To shed light on the\nnature of the model learned by the i-RevNet we reconstruct linear\ninterpolations between natural image representations.",
            "authors": [
                "J\u00f6rn-Henrik Jacobsen",
                "Arnold Smeulders",
                "Edouard Oyallon"
            ]
        },
        {
            "title": "ICNet for Real-Time Semantic Segmentation on High-Resolution Images",
            "arxiv": "1704.08545",
            "year": 2017,
            "url": "http://arxiv.org/abs/1704.08545v2",
            "abstract": "We focus on the challenging task of real-time semantic segmentation in this\npaper. It finds many practical applications and yet is with fundamental\ndifficulty of reducing a large portion of computation for pixel-wise label\ninference. We propose an image cascade network (ICNet) that incorporates\nmulti-resolution branches under proper label guidance to address this\nchallenge. We provide in-depth analysis of our framework and introduce the\ncascade feature fusion unit to quickly achieve high-quality segmentation. Our\nsystem yields real-time inference on a single GPU card with decent quality\nresults evaluated on challenging datasets like Cityscapes, CamVid and\nCOCO-Stuff.",
            "authors": [
                "Hengshuang Zhao",
                "Xiaojuan Qi",
                "Xiaoyong Shen",
                "Jianping Shi",
                "Jiaya Jia"
            ]
        },
        {
            "title": "CGNet: A Light-weight Context Guided Network for Semantic Segmentation",
            "arxiv": "1811.08201",
            "year": 2018,
            "url": "http://arxiv.org/abs/1811.08201v2",
            "abstract": "The demand of applying semantic segmentation model on mobile devices has been\nincreasing rapidly. Current state-of-the-art networks have enormous amount of\nparameters hence unsuitable for mobile devices, while other small memory\nfootprint models follow the spirit of classification network and ignore the\ninherent characteristic of semantic segmentation. To tackle this problem, we\npropose a novel Context Guided Network (CGNet), which is a light-weight and\nefficient network for semantic segmentation. We first propose the Context\nGuided (CG) block, which learns the joint feature of both local feature and\nsurrounding context, and further improves the joint feature with the global\ncontext. Based on the CG block, we develop CGNet which captures contextual\ninformation in all stages of the network and is specially tailored for\nincreasing segmentation accuracy. CGNet is also elaborately designed to reduce\nthe number of parameters and save memory footprint. Under an equivalent number\nof parameters, the proposed CGNet significantly outperforms existing\nsegmentation networks. Extensive experiments on Cityscapes and CamVid datasets\nverify the effectiveness of the proposed approach. Specifically, without any\npost-processing and multi-scale testing, the proposed CGNet achieves 64.8% mean\nIoU on Cityscapes with less than 0.5 M parameters. The source code for the\ncomplete system can be found at https://github.com/wutianyiRosun/CGNet.",
            "authors": [
                "Tianyi Wu",
                "Sheng Tang",
                "Rui Zhang",
                "Yongdong Zhang"
            ]
        },
        {
            "title": "Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization",
            "arxiv": "1712.01034",
            "year": 2017,
            "url": "http://arxiv.org/abs/1712.01034v2",
            "abstract": "Global covariance pooling in convolutional neural networks has achieved\nimpressive improvement over the classical first-order pooling. Recent works\nhave shown matrix square root normalization plays a central role in achieving\nstate-of-the-art performance. However, existing methods depend heavily on\neigendecomposition (EIG) or singular value decomposition (SVD), suffering from\ninefficient training due to limited support of EIG and SVD on GPU. Towards\naddressing this problem, we propose an iterative matrix square root\nnormalization method for fast end-to-end training of global covariance pooling\nnetworks. At the core of our method is a meta-layer designed with loop-embedded\ndirected graph structure. The meta-layer consists of three consecutive\nnonlinear structured layers, which perform pre-normalization, coupled matrix\niteration and post-compensation, respectively. Our method is much faster than\nEIG or SVD based ones, since it involves only matrix multiplications, suitable\nfor parallel implementation on GPU. Moreover, the proposed network with ResNet\narchitecture can converge in much less epochs, further accelerating network\ntraining. On large-scale ImageNet, we achieve competitive performance superior\nto existing counterparts. By finetuning our models pre-trained on ImageNet, we\nestablish state-of-the-art results on three challenging fine-grained\nbenchmarks. The source code and network models will be available at\nhttp://www.peihuali.org/iSQRT-COV",
            "authors": [
                "Peihua Li",
                "Jiangtao Xie",
                "Qilong Wang",
                "Zilin Gao"
            ]
        },
        {
            "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
            "arxiv": "1602.07360",
            "year": 2016,
            "url": "http://arxiv.org/abs/1602.07360v4",
            "abstract": "Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet",
            "authors": [
                "Forrest N. Iandola",
                "Song Han",
                "Matthew W. Moskewicz",
                "Khalid Ashraf",
                "William J. Dally",
                "Kurt Keutzer"
            ]
        },
        {
            "title": "Network In Network",
            "arxiv": "1312.4400",
            "year": 2013,
            "url": "http://arxiv.org/abs/1312.4400v3",
            "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN)\nto enhance model discriminability for local patches within the receptive field.\nThe conventional convolutional layer uses linear filters followed by a\nnonlinear activation function to scan the input. Instead, we build micro neural\nnetworks with more complex structures to abstract the data within the receptive\nfield. We instantiate the micro neural network with a multilayer perceptron,\nwhich is a potent function approximator. The feature maps are obtained by\nsliding the micro networks over the input in a similar manner as CNN; they are\nthen fed into the next layer. Deep NIN can be implemented by stacking mutiple\nof the above described structure. With enhanced local modeling via the micro\nnetwork, we are able to utilize global average pooling over feature maps in the\nclassification layer, which is easier to interpret and less prone to\noverfitting than traditional fully connected layers. We demonstrated the\nstate-of-the-art classification performances with NIN on CIFAR-10 and\nCIFAR-100, and reasonable performances on SVHN and MNIST datasets.",
            "authors": [
                "Min Lin",
                "Qiang Chen",
                "Shuicheng Yan"
            ]
        },
        {
            "title": "GhostNet: More Features from Cheap Operations",
            "arxiv": "1911.11907",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.11907v2",
            "abstract": "Deploying convolutional neural networks (CNNs) on embedded devices is\ndifficult due to the limited memory and computation resources. The redundancy\nin feature maps is an important characteristic of those successful CNNs, but\nhas rarely been investigated in neural architecture design. This paper proposes\na novel Ghost module to generate more feature maps from cheap operations. Based\non a set of intrinsic feature maps, we apply a series of linear transformations\nwith cheap cost to generate many ghost feature maps that could fully reveal\ninformation underlying intrinsic features. The proposed Ghost module can be\ntaken as a plug-and-play component to upgrade existing convolutional neural\nnetworks. Ghost bottlenecks are designed to stack Ghost modules, and then the\nlightweight GhostNet can be easily established. Experiments conducted on\nbenchmarks demonstrate that the proposed Ghost module is an impressive\nalternative of convolution layers in baseline models, and our GhostNet can\nachieve higher recognition performance (e.g. $75.7\\%$ top-1 accuracy) than\nMobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012\nclassification dataset. Code is available at\nhttps://github.com/huawei-noah/ghostnet",
            "authors": [
                "Kai Han",
                "Yunhe Wang",
                "Qi Tian",
                "Jianyuan Guo",
                "Chunjing Xu",
                "Chang Xu"
            ]
        },
        {
            "title": "SqueezeNext: Hardware-Aware Neural Network Design",
            "arxiv": "1803.10615",
            "year": 2018,
            "url": "http://arxiv.org/abs/1803.10615v2",
            "abstract": "One of the main barriers for deploying neural networks on embedded systems\nhas been large memory and power consumption of existing neural networks. In\nthis work, we introduce SqueezeNext, a new family of neural network\narchitectures whose design was guided by considering previous architectures\nsuch as SqueezeNet, as well as by simulation results on a neural network\naccelerator. This new network is able to match AlexNet's accuracy on the\nImageNet benchmark with $112\\times$ fewer parameters, and one of its deeper\nvariants is able to achieve VGG-19 accuracy with only 4.4 Million parameters,\n($31\\times$ smaller than VGG-19). SqueezeNext also achieves better top-5\nclassification accuracy with $1.3\\times$ fewer parameters as compared to\nMobileNet, but avoids using depthwise-separable convolutions that are\ninefficient on some mobile processor platforms. This wide range of accuracy\ngives the user the ability to make speed-accuracy tradeoffs, depending on the\navailable resources on the target hardware. Using hardware simulation results\nfor power and inference speed on an embedded system has guided us to design\nvariations of the baseline model that are $2.59\\times$/$8.26\\times$ faster and\n$2.25\\times$/$7.5\\times$ more energy efficient as compared to\nSqueezeNet/AlexNet without any accuracy degradation.",
            "authors": [
                "Amir Gholami",
                "Kiseok Kwon",
                "Bichen Wu",
                "Zizheng Tai",
                "Xiangyu Yue",
                "Peter Jin",
                "Sicheng Zhao",
                "Kurt Keutzer"
            ]
        },
        {
            "title": "Deep High-Resolution Representation Learning for Visual Recognition",
            "arxiv": "1908.07919",
            "year": 2019,
            "url": "http://arxiv.org/abs/1908.07919v2",
            "abstract": "High-resolution representations are essential for position-sensitive vision\nproblems, such as human pose estimation, semantic segmentation, and object\ndetection. Existing state-of-the-art frameworks first encode the input image as\na low-resolution representation through a subnetwork that is formed by\nconnecting high-to-low resolution convolutions \\emph{in series} (e.g., ResNet,\nVGGNet), and then recover the high-resolution representation from the encoded\nlow-resolution representation. Instead, our proposed network, named as\nHigh-Resolution Network (HRNet), maintains high-resolution representations\nthrough the whole process. There are two key characteristics: (i) Connect the\nhigh-to-low resolution convolution streams \\emph{in parallel}; (ii) Repeatedly\nexchange the information across resolutions. The benefit is that the resulting\nrepresentation is semantically richer and spatially more precise. We show the\nsuperiority of the proposed HRNet in a wide range of applications, including\nhuman pose estimation, semantic segmentation, and object detection, suggesting\nthat the HRNet is a stronger backbone for computer vision problems. All the\ncodes are available at~{\\url{https://github.com/HRNet}}.",
            "authors": [
                "Jingdong Wang",
                "Ke Sun",
                "Tianheng Cheng",
                "Borui Jiang",
                "Chaorui Deng",
                "Yang Zhao",
                "Dong Liu",
                "Yadong Mu",
                "Mingkui Tan",
                "Xinggang Wang",
                "Wenyu Liu",
                "Bin Xiao"
            ]
        },
        {
            "title": "YOLOv3: An Incremental Improvement",
            "arxiv": "1804.02767",
            "year": 2018,
            "url": "http://arxiv.org/abs/1804.02767v1",
            "abstract": "We present some updates to YOLO! We made a bunch of little design changes to\nmake it better. We also trained this new network that's pretty swell. It's a\nlittle bigger than last time but more accurate. It's still fast though, don't\nworry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but\nthree times faster. When we look at the old .5 IOU mAP detection metric YOLOv3\nis quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5\nmAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always,\nall the code is online at https://pjreddie.com/yolo/",
            "authors": [
                "Joseph Redmon",
                "Ali Farhadi"
            ]
        },
        {
            "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
            "arxiv": "1812.00332",
            "year": 2018,
            "url": "http://arxiv.org/abs/1812.00332v2",
            "abstract": "Neural architecture search (NAS) has a great impact by automatically\ndesigning effective neural network architectures. However, the prohibitive\ncomputational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours)\nmakes it difficult to \\emph{directly} search the architectures on large-scale\ntasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via\na continuous representation of network architecture but suffers from the high\nGPU memory consumption issue (grow linearly w.r.t. candidate set size). As a\nresult, they need to utilize~\\emph{proxy} tasks, such as training on a smaller\ndataset, or learning with only a few blocks, or training just for a few epochs.\nThese architectures optimized on proxy tasks are not guaranteed to be optimal\non the target task. In this paper, we present \\emph{ProxylessNAS} that can\n\\emph{directly} learn the architectures for large-scale target tasks and target\nhardware platforms. We address the high memory consumption issue of\ndifferentiable NAS and reduce the computational cost (GPU hours and GPU memory)\nto the same level of regular training while still allowing a large candidate\nset. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of\ndirectness and specialization. On CIFAR-10, our model achieves 2.08\\% test\nerror with only 5.7M parameters, better than the previous state-of-the-art\narchitecture AmoebaNet-B, while using 6$\\times$ fewer parameters. On ImageNet,\nour model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being\n1.2$\\times$ faster with measured GPU latency. We also apply ProxylessNAS to\nspecialize neural architectures for hardware with direct hardware metrics (e.g.\nlatency) and provide insights for efficient CNN architecture design.",
            "authors": [
                "Han Cai",
                "Ligeng Zhu",
                "Song Han"
            ]
        },
        {
            "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
            "arxiv": "1704.04861",
            "year": 2017,
            "url": "http://arxiv.org/abs/1704.04861v1",
            "abstract": "We present a class of efficient models called MobileNets for mobile and\nembedded vision applications. MobileNets are based on a streamlined\narchitecture that uses depth-wise separable convolutions to build light weight\ndeep neural networks. We introduce two simple global hyper-parameters that\nefficiently trade off between latency and accuracy. These hyper-parameters\nallow the model builder to choose the right sized model for their application\nbased on the constraints of the problem. We present extensive experiments on\nresource and accuracy tradeoffs and show strong performance compared to other\npopular models on ImageNet classification. We then demonstrate the\neffectiveness of MobileNets across a wide range of applications and use cases\nincluding object detection, finegrain classification, face attributes and large\nscale geo-localization.",
            "authors": [
                "Andrew G. Howard",
                "Menglong Zhu",
                "Bo Chen",
                "Dmitry Kalenichenko",
                "Weijun Wang",
                "Tobias Weyand",
                "Marco Andreetto",
                "Hartwig Adam"
            ]
        },
        {
            "title": "The Reversible Residual Network: Backpropagation Without Storing Activations",
            "arxiv": "1707.04585",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.04585v1",
            "abstract": "Deep residual networks (ResNets) have significantly pushed forward the\nstate-of-the-art on image classification, increasing in performance as networks\ngrow both deeper and wider. However, memory consumption becomes a bottleneck,\nas one needs to store the activations in order to calculate gradients using\nbackpropagation. We present the Reversible Residual Network (RevNet), a variant\nof ResNets where each layer's activations can be reconstructed exactly from the\nnext layer's. Therefore, the activations for most layers need not be stored in\nmemory during backpropagation. We demonstrate the effectiveness of RevNets on\nCIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification\naccuracy to equally-sized ResNets, even though the activation storage\nrequirements are independent of depth.",
            "authors": [
                "Aidan N. Gomez",
                "Mengye Ren",
                "Raquel Urtasun",
                "Roger B. Grosse"
            ]
        },
        {
            "title": "Searching for MobileNetV3",
            "arxiv": "1905.02244",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.02244v5",
            "abstract": "We present the next generation of MobileNets based on a combination of\ncomplementary search techniques as well as a novel architecture design.\nMobileNetV3 is tuned to mobile phone CPUs through a combination of\nhardware-aware network architecture search (NAS) complemented by the NetAdapt\nalgorithm and then subsequently improved through novel architecture advances.\nThis paper starts the exploration of how automated search algorithms and\nnetwork design can work together to harness complementary approaches improving\nthe overall state of the art. Through this process we create two new MobileNet\nmodels for release: MobileNetV3-Large and MobileNetV3-Small which are targeted\nfor high and low resource use cases. These models are then adapted and applied\nto the tasks of object detection and semantic segmentation. For the task of\nsemantic segmentation (or any dense pixel prediction), we propose a new\nefficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling\n(LR-ASPP). We achieve new state of the art results for mobile classification,\ndetection and segmentation. MobileNetV3-Large is 3.2\\% more accurate on\nImageNet classification while reducing latency by 15\\% compared to MobileNetV2.\nMobileNetV3-Small is 4.6\\% more accurate while reducing latency by 5\\% compared\nto MobileNetV2. MobileNetV3-Large detection is 25\\% faster at roughly the same\naccuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\\%\nfaster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.",
            "authors": [
                "Andrew Howard",
                "Mark Sandler",
                "Grace Chu",
                "Liang-Chieh Chen",
                "Bo Chen",
                "Mingxing Tan",
                "Weijun Wang",
                "Yukun Zhu",
                "Ruoming Pang",
                "Vijay Vasudevan",
                "Quoc V. Le",
                "Hartwig Adam"
            ]
        },
        {
            "title": "BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation",
            "arxiv": "1808.00897",
            "year": 2018,
            "url": "http://arxiv.org/abs/1808.00897v1",
            "abstract": "Semantic segmentation requires both rich spatial information and sizeable\nreceptive field. However, modern approaches usually compromise spatial\nresolution to achieve real-time inference speed, which leads to poor\nperformance. In this paper, we address this dilemma with a novel Bilateral\nSegmentation Network (BiSeNet). We first design a Spatial Path with a small\nstride to preserve the spatial information and generate high-resolution\nfeatures. Meanwhile, a Context Path with a fast downsampling strategy is\nemployed to obtain sufficient receptive field. On top of the two paths, we\nintroduce a new Feature Fusion Module to combine features efficiently. The\nproposed architecture makes a right balance between the speed and segmentation\nperformance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a\n2048x1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with\nspeed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster\nthan the existing methods with comparable performance.",
            "authors": [
                "Changqian Yu",
                "Jingbo Wang",
                "Chao Peng",
                "Changxin Gao",
                "Gang Yu",
                "Nong Sang"
            ]
        },
        {
            "title": "ShakeDrop Regularization for Deep Residual Learning",
            "arxiv": "1802.02375",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.02375v3",
            "abstract": "Overfitting is a crucial problem in deep neural networks, even in the latest\nnetwork architectures. In this paper, to relieve the overfitting effect of\nResNet and its improvements (i.e., Wide ResNet, PyramidNet, and ResNeXt), we\npropose a new regularization method called ShakeDrop regularization. ShakeDrop\nis inspired by Shake-Shake, which is an effective regularization method, but\ncan be applied to ResNeXt only. ShakeDrop is more effective than Shake-Shake\nand can be applied not only to ResNeXt but also ResNet, Wide ResNet, and\nPyramidNet. An important key is to achieve stability of training. Because\neffective regularization often causes unstable training, we introduce a\ntraining stabilizer, which is an unusual use of an existing regularizer.\nThrough experiments under various conditions, we demonstrate the conditions\nunder which ShakeDrop works well.",
            "authors": [
                "Yoshihiro Yamada",
                "Masakazu Iwamura",
                "Takuya Akiba",
                "Koichi Kise"
            ]
        },
        {
            "title": "Feature Pyramid Encoding Network for Real-time Semantic Segmentation",
            "arxiv": "1909.08599",
            "year": 2019,
            "url": "http://arxiv.org/abs/1909.08599v1",
            "abstract": "Although current deep learning methods have achieved impressive results for\nsemantic segmentation, they incur high computational costs and have a huge\nnumber of parameters. For real-time applications, inference speed and memory\nusage are two important factors. To address the challenge, we propose a\nlightweight feature pyramid encoding network (FPENet) to make a good trade-off\nbetween accuracy and speed. Specifically, we use a feature pyramid encoding\nblock to encode multi-scale contextual features with depthwise dilated\nconvolutions in all stages of the encoder. A mutual embedding upsample module\nis introduced in the decoder to aggregate the high-level semantic features and\nlow-level spatial details efficiently. The proposed network outperforms\nexisting real-time methods with fewer parameters and improved inference speed\non the Cityscapes and CamVid benchmark datasets. Specifically, FPENet achieves\n68.0\\% mean IoU on the Cityscapes test set with only 0.4M parameters and 102\nFPS speed on an NVIDIA TITAN V GPU.",
            "authors": [
                "Mengyu Liu",
                "Hujun Yin"
            ]
        },
        {
            "title": "Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic Segmentation",
            "arxiv": "1809.06323",
            "year": 2018,
            "url": "http://arxiv.org/abs/1809.06323v3",
            "abstract": "Real-time semantic segmentation plays an important role in practical\napplications such as self-driving and robots. Most semantic segmentation\nresearch focuses on improving estimation accuracy with little consideration on\nefficiency. Several previous studies that emphasize high-speed inference often\nfail to produce high-accuracy segmentation results. In this paper, we propose a\nnovel convolutional network named Efficient Dense modules with Asymmetric\nconvolution (EDANet), which employs an asymmetric convolution structure and\nincorporates dilated convolution and dense connectivity to achieve high\nefficiency at low computational cost and model size. EDANet is 2.7 times faster\nthan the existing fast segmentation network, ICNet, while it achieves a similar\nmIoU score without any additional context module, post-processing scheme, and\npretrained model. We evaluate EDANet on Cityscapes and CamVid datasets, and\ncompare it with the other state-of-art systems. Our network can run with the\nhigh-resolution inputs at the speed of 108 FPS on one GTX 1080Ti.",
            "authors": [
                "Shao-Yuan Lo",
                "Hsueh-Ming Hang",
                "Sheng-Wei Chan",
                "Jing-Jhih Lin"
            ]
        },
        {
            "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
            "arxiv": "1602.07261",
            "year": 2016,
            "url": "http://arxiv.org/abs/1602.07261v2",
            "abstract": "Very deep convolutional networks have been central to the largest advances in\nimage recognition performance in recent years. One example is the Inception\narchitecture that has been shown to achieve very good performance at relatively\nlow computational cost. Recently, the introduction of residual connections in\nconjunction with a more traditional architecture has yielded state-of-the-art\nperformance in the 2015 ILSVRC challenge; its performance was similar to the\nlatest generation Inception-v3 network. This raises the question of whether\nthere are any benefit in combining the Inception architecture with residual\nconnections. Here we give clear empirical evidence that training with residual\nconnections accelerates the training of Inception networks significantly. There\nis also some evidence of residual Inception networks outperforming similarly\nexpensive Inception networks without residual connections by a thin margin. We\nalso present several new streamlined architectures for both residual and\nnon-residual Inception networks. These variations improve the single-frame\nrecognition performance on the ILSVRC 2012 classification task significantly.\nWe further demonstrate how proper activation scaling stabilizes the training of\nvery wide residual Inception networks. With an ensemble of three residual and\none Inception-v4, we achieve 3.08 percent top-5 error on the test set of the\nImageNet classification (CLS) challenge",
            "authors": [
                "Christian Szegedy",
                "Sergey Ioffe",
                "Vincent Vanhoucke",
                "Alex Alemi"
            ]
        },
        {
            "title": "Squeeze-and-Excitation Networks",
            "arxiv": "1709.01507",
            "year": 2017,
            "url": "http://arxiv.org/abs/1709.01507v4",
            "abstract": "The central building block of convolutional neural networks (CNNs) is the\nconvolution operator, which enables networks to construct informative features\nby fusing both spatial and channel-wise information within local receptive\nfields at each layer. A broad range of prior research has investigated the\nspatial component of this relationship, seeking to strengthen the\nrepresentational power of a CNN by enhancing the quality of spatial encodings\nthroughout its feature hierarchy. In this work, we focus instead on the channel\nrelationship and propose a novel architectural unit, which we term the\n\"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise\nfeature responses by explicitly modelling interdependencies between channels.\nWe show that these blocks can be stacked together to form SENet architectures\nthat generalise extremely effectively across different datasets. We further\ndemonstrate that SE blocks bring significant improvements in performance for\nexisting state-of-the-art CNNs at slight additional computational cost.\nSqueeze-and-Excitation Networks formed the foundation of our ILSVRC 2017\nclassification submission which won first place and reduced the top-5 error to\n2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%.\nModels and code are available at https://github.com/hujie-frank/SENet.",
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Samuel Albanie",
                "Gang Sun",
                "Enhua Wu"
            ]
        },
        {
            "title": "DiCENet: Dimension-wise Convolutions for Efficient Networks",
            "arxiv": "1906.03516",
            "year": 2019,
            "url": "http://arxiv.org/abs/1906.03516v3",
            "abstract": "We introduce a novel and generic convolutional unit, DiCE unit, that is built\nusing dimension-wise convolutions and dimension-wise fusion. The dimension-wise\nconvolutions apply light-weight convolutional filtering across each dimension\nof the input tensor while dimension-wise fusion efficiently combines these\ndimension-wise representations; allowing the DiCE unit to efficiently encode\nspatial and channel-wise information contained in the input tensor. The DiCE\nunit is simple and can be seamlessly integrated with any architecture to\nimprove its efficiency and performance. Compared to depth-wise separable\nconvolutions, the DiCE unit shows significant improvements across different\narchitectures. When DiCE units are stacked to build the DiCENet model, we\nobserve significant improvements over state-of-the-art models across various\ncomputer vision tasks including image classification, object detection, and\nsemantic segmentation. On the ImageNet dataset, the DiCENet delivers 2-4%\nhigher accuracy than state-of-the-art manually designed models (e.g.,\nMobileNetv2 and ShuffleNetv2). Also, DiCENet generalizes better to tasks (e.g.,\nobject detection) that are often used in resource-constrained devices in\ncomparison to state-of-the-art separable convolution-based efficient networks,\nincluding neural search-based methods (e.g., MobileNetv3 and MixNet. Our source\ncode in PyTorch is open-source and is available at\nhttps://github.com/sacmehta/EdgeNets/",
            "authors": [
                "Sachin Mehta",
                "Hannaneh Hajishirzi",
                "Mohammad Rastegari"
            ]
        },
        {
            "title": "Residual Networks of Residual Networks: Multilevel Residual Networks",
            "arxiv": "1608.02908",
            "year": 2016,
            "url": "http://arxiv.org/abs/1608.02908v2",
            "abstract": "A residual-networks family with hundreds or even thousands of layers\ndominates major image recognition tasks, but building a network by simply\nstacking residual blocks inevitably limits its optimization ability. This paper\nproposes a novel residual-network architecture, Residual networks of Residual\nnetworks (RoR), to dig the optimization ability of residual networks. RoR\nsubstitutes optimizing residual mapping of residual mapping for optimizing\noriginal residual mapping. In particular, RoR adds level-wise shortcut\nconnections upon original residual networks to promote the learning capability\nof residual networks. More importantly, RoR can be applied to various kinds of\nresidual networks (ResNets, Pre-ResNets and WRN) and significantly boost their\nperformance. Our experiments demonstrate the effectiveness and versatility of\nRoR, where it achieves the best performance in all residual-network-like\nstructures. Our RoR-3-WRN58-4+SD models achieve new state-of-the-art results on\nCIFAR-10, CIFAR-100 and SVHN, with test errors 3.77%, 19.73% and 1.59%,\nrespectively. RoR-3 models also achieve state-of-the-art results compared to\nResNets on ImageNet data set.",
            "authors": [
                "Ke Zhang",
                "Miao Sun",
                "Tony X. Han",
                "Xingfang Yuan",
                "Liru Guo",
                "Tao Liu"
            ]
        },
        {
            "title": "Selective Kernel Networks",
            "arxiv": "1903.06586",
            "year": 2019,
            "url": "http://arxiv.org/abs/1903.06586v2",
            "abstract": "In standard Convolutional Neural Networks (CNNs), the receptive fields of\nartificial neurons in each layer are designed to share the same size. It is\nwell-known in the neuroscience community that the receptive field size of\nvisual cortical neurons are modulated by the stimulus, which has been rarely\nconsidered in constructing CNNs. We propose a dynamic selection mechanism in\nCNNs that allows each neuron to adaptively adjust its receptive field size\nbased on multiple scales of input information. A building block called\nSelective Kernel (SK) unit is designed, in which multiple branches with\ndifferent kernel sizes are fused using softmax attention that is guided by the\ninformation in these branches. Different attentions on these branches yield\ndifferent sizes of the effective receptive fields of neurons in the fusion\nlayer. Multiple SK units are stacked to a deep network termed Selective Kernel\nNetworks (SKNets). On the ImageNet and CIFAR benchmarks, we empirically show\nthat SKNet outperforms the existing state-of-the-art architectures with lower\nmodel complexity. Detailed analyses show that the neurons in SKNet can capture\ntarget objects with different scales, which verifies the capability of neurons\nfor adaptively adjusting their receptive field sizes according to the input.\nThe code and models are available at https://github.com/implus/SKNet.",
            "authors": [
                "Xiang Li",
                "Wenhai Wang",
                "Xiaolin Hu",
                "Jian Yang"
            ]
        },
        {
            "title": "DABNet: Depth-wise Asymmetric Bottleneck for Real-time Semantic Segmentation",
            "arxiv": "1907.11357",
            "year": 2019,
            "url": "http://arxiv.org/abs/1907.11357v2",
            "abstract": "As a pixel-level prediction task, semantic segmentation needs large\ncomputational cost with enormous parameters to obtain high performance.\nRecently, due to the increasing demand for autonomous systems and robots, it is\nsignificant to make a tradeoff between accuracy and inference speed. In this\npaper, we propose a novel Depthwise Asymmetric Bottleneck (DAB) module to\naddress this dilemma, which efficiently adopts depth-wise asymmetric\nconvolution and dilated convolution to build a bottleneck structure. Based on\nthe DAB module, we design a Depth-wise Asymmetric Bottleneck Network (DABNet)\nespecially for real-time semantic segmentation, which creates sufficient\nreceptive field and densely utilizes the contextual information. Experiments on\nCityscapes and CamVid datasets demonstrate that the proposed DABNet achieves a\nbalance between speed and precision. Specifically, without any pretrained model\nand postprocessing, it achieves 70.1% Mean IoU on the Cityscapes test dataset\nwith only 0.76 million parameters and a speed of 104 FPS on a single GTX 1080Ti\ncard.",
            "authors": [
                "Gen Li",
                "Inyoung Yun",
                "Jonghyun Kim",
                "Joongkyu Kim"
            ]
        },
        {
            "title": "Designing Network Design Spaces",
            "arxiv": "2003.13678",
            "year": 2020,
            "url": "http://arxiv.org/abs/2003.13678v1",
            "abstract": "In this work, we present a new network design paradigm. Our goal is to help\nadvance the understanding of network design and discover design principles that\ngeneralize across settings. Instead of focusing on designing individual network\ninstances, we design network design spaces that parametrize populations of\nnetworks. The overall process is analogous to classic manual design of\nnetworks, but elevated to the design space level. Using our methodology we\nexplore the structure aspect of network design and arrive at a low-dimensional\ndesign space consisting of simple, regular networks that we call RegNet. The\ncore insight of the RegNet parametrization is surprisingly simple: widths and\ndepths of good networks can be explained by a quantized linear function. We\nanalyze the RegNet design space and arrive at interesting findings that do not\nmatch the current practice of network design. The RegNet design space provides\nsimple and fast networks that work well across a wide range of flop regimes.\nUnder comparable training settings and flops, the RegNet models outperform the\npopular EfficientNet models while being up to 5x faster on GPUs.",
            "authors": [
                "Ilija Radosavovic",
                "Raj Prateek Kosaraju",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ]
        },
        {
            "title": "LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation",
            "arxiv": "1707.03718",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.03718v1",
            "abstract": "Pixel-wise semantic segmentation for visual scene understanding not only\nneeds to be accurate, but also efficient in order to find any use in real-time\napplication. Existing algorithms even though are accurate but they do not focus\non utilizing the parameters of neural network efficiently. As a result they are\nhuge in terms of parameters and number of operations; hence slow too. In this\npaper, we propose a novel deep neural network architecture which allows it to\nlearn without any significant increase in number of parameters. Our network\nuses only 11.5 million parameters and 21.2 GFLOPs for processing an image of\nresolution 3x640x360. It gives state-of-the-art performance on CamVid and\ncomparable results on Cityscapes dataset. We also compare our networks\nprocessing time on NVIDIA GPU and embedded system device with existing\nstate-of-the-art architectures for different image resolutions.",
            "authors": [
                "Abhishek Chaurasia",
                "Eugenio Culurciello"
            ]
        },
        {
            "title": "Training wide residual networks for deployment using a single bit for each weight",
            "arxiv": "1802.08530",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.08530v1",
            "abstract": "For fast and energy-efficient deployment of trained deep neural networks on\nresource-constrained embedded hardware, each learned weight parameter should\nideally be represented and stored using a single bit. Error-rates usually\nincrease when this requirement is imposed. Here, we report large improvements\nin error rates on multiple datasets, for deep convolutional neural networks\ndeployed with 1-bit-per-weight. Using wide residual networks as our main\nbaseline, our approach simplifies existing methods that binarize weights by\napplying the sign function in training; we apply scaling factors for each layer\nwith constant unlearned values equal to the layer-specific standard deviations\nused for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with\n1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve\nerror rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We\nalso considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test\nresults of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error\nrates halve previously reported values, and are within about 1% of our\nerror-rates for the same network with full-precision weights. For networks that\noverfit, we also show significant improvements in error rate by not learning\nbatch normalization scale and offset parameters. This applies to both full\nprecision and 1-bit-per-weight networks. Using a warm-restart learning-rate\nschedule, we found that training for 1-bit-per-weight is just as fast as\nfull-precision networks, with better accuracy than standard schedules, and\nachieved about 98%-99% of peak performance in just 62 training epochs for\nCIFAR-10/100. For full training code and trained models in MATLAB, Keras and\nPyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .",
            "authors": [
                "Mark D. McDonnell"
            ]
        },
        {
            "title": "Dual Path Networks",
            "arxiv": "1707.01629",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.01629v2",
            "abstract": "In this work, we present a simple, highly efficient and modularized Dual Path\nNetwork (DPN) for image classification which presents a new topology of\nconnection paths internally. By revealing the equivalence of the\nstate-of-the-art Residual Network (ResNet) and Densely Convolutional Network\n(DenseNet) within the HORNN framework, we find that ResNet enables feature\nre-usage while DenseNet enables new features exploration which are both\nimportant for learning good representations. To enjoy the benefits from both\npath topologies, our proposed Dual Path Network shares common features while\nmaintaining the flexibility to explore new features through dual path\narchitectures. Extensive experiments on three benchmark datasets, ImagNet-1k,\nPlaces365 and PASCAL VOC, clearly demonstrate superior performance of the\nproposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset,\na shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model\nsize, 25% less computational cost and 8% lower memory consumption, and a deeper\nDPN (DPN-131) further pushes the state-of-the-art single model performance with\nabout 2 times faster training speed. Experiments on the Places365 large-scale\nscene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation\ndataset also demonstrate its consistently better performance than DenseNet,\nResNet and the latest ResNeXt model over various applications.",
            "authors": [
                "Yunpeng Chen",
                "Jianan Li",
                "Huaxin Xiao",
                "Xiaojie Jin",
                "Shuicheng Yan",
                "Jiashi Feng"
            ]
        },
        {
            "title": "Fast-SCNN: Fast Semantic Segmentation Network",
            "arxiv": "1902.04502",
            "year": 2019,
            "url": "http://arxiv.org/abs/1902.04502v1",
            "abstract": "The encoder-decoder framework is state-of-the-art for offline semantic image\nsegmentation. Since the rise in autonomous systems, real-time computation is\nincreasingly desirable. In this paper, we introduce fast segmentation\nconvolutional neural network (Fast-SCNN), an above real-time semantic\nsegmentation model on high resolution image data (1024x2048px) suited to\nefficient computation on embedded devices with low memory. Building on existing\ntwo-branch methods for fast segmentation, we introduce our `learning to\ndownsample' module which computes low-level features for multiple resolution\nbranches simultaneously. Our network combines spatial detail at high resolution\nwith deep features extracted at lower resolution, yielding an accuracy of 68.0%\nmean intersection over union at 123.5 frames per second on Cityscapes. We also\nshow that large scale pre-training is unnecessary. We thoroughly validate our\nmetric in experiments with ImageNet pre-training and the coarse labeled data of\nCityscapes. Finally, we show even faster computation with competitive results\non subsampled inputs, without any network modifications.",
            "authors": [
                "Rudra P K Poudel",
                "Stephan Liwicki",
                "Roberto Cipolla"
            ]
        },
        {
            "title": "Deep Layer Aggregation",
            "arxiv": "1707.06484",
            "year": 2017,
            "url": "http://arxiv.org/abs/1707.06484v3",
            "abstract": "Visual recognition requires rich representations that span levels from low to\nhigh, scales from small to large, and resolutions from fine to coarse. Even\nwith the depth of features in a convolutional network, a layer in isolation is\nnot enough: compounding and aggregating these representations improves\ninference of what and where. Architectural efforts are exploring many\ndimensions for network backbones, designing deeper or wider architectures, but\nhow to best aggregate layers and blocks across a network deserves further\nattention. Although skip connections have been incorporated to combine layers,\nthese connections have been \"shallow\" themselves, and only fuse by simple,\none-step operations. We augment standard architectures with deeper aggregation\nto better fuse information across layers. Our deep layer aggregation structures\niteratively and hierarchically merge the feature hierarchy to make networks\nwith better accuracy and fewer parameters. Experiments across architectures and\ntasks show that deep layer aggregation improves recognition and resolution\ncompared to existing branching and merging schemes. The code is at\nhttps://github.com/ucbdrive/dla.",
            "authors": [
                "Fisher Yu",
                "Dequan Wang",
                "Evan Shelhamer",
                "Trevor Darrell"
            ]
        },
        {
            "title": "IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks",
            "arxiv": "1806.00178",
            "year": 2018,
            "url": "http://arxiv.org/abs/1806.00178v2",
            "abstract": "In this paper, we are interested in building lightweight and efficient\nconvolutional neural networks. Inspired by the success of two design patterns,\ncomposition of structured sparse kernels, e.g., interleaved group convolutions\n(IGC), and composition of low-rank kernels, e.g., bottle-neck modules, we study\nthe combination of such two design patterns, using the composition of\nstructured sparse low-rank kernels, to form a convolutional kernel. Rather than\nintroducing a complementary condition over channels, we introduce a loose\ncomplementary condition, which is formulated by imposing the complementary\ncondition over super-channels, to guide the design for generating a dense\nconvolutional kernel. The resulting network is called IGCV3. We empirically\ndemonstrate that the combination of low-rank and sparse kernels boosts the\nperformance and the superiority of our proposed approach to the\nstate-of-the-arts, IGCV2 and MobileNetV2 over image classification on CIFAR and\nImageNet and object detection on COCO.",
            "authors": [
                "Ke Sun",
                "Mingjie Li",
                "Dong Liu",
                "Jingdong Wang"
            ]
        },
        {
            "title": "ShaResNet: reducing residual network parameter number by sharing weights",
            "arxiv": "1702.08782",
            "year": 2017,
            "url": "http://arxiv.org/abs/1702.08782v2",
            "abstract": "Deep Residual Networks have reached the state of the art in many image\nprocessing tasks such image classification. However, the cost for a gain in\naccuracy in terms of depth and memory is prohibitive as it requires a higher\nnumber of residual blocks, up to double the initial value. To tackle this\nproblem, we propose in this paper a way to reduce the redundant information of\nthe networks. We share the weights of convolutional layers between residual\nblocks operating at the same spatial scale. The signal flows multiple times in\nthe same convolutional layer. The resulting architecture, called ShaResNet,\ncontains block specific layers and shared layers. These ShaResNet are trained\nexactly in the same fashion as the commonly used residual networks. We show, on\nthe one hand, that they are almost as efficient as their sequential\ncounterparts while involving less parameters, and on the other hand that they\nare more efficient than a residual network with the same number of parameters.\nFor example, a 152-layer-deep residual network can be reduced to 106\nconvolutional layers, i.e. a parameter gain of 39\\%, while loosing less than\n0.2\\% accuracy on ImageNet.",
            "authors": [
                "Alexandre Boulch"
            ]
        },
        {
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "arxiv": "1409.1556",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.1556v6",
            "abstract": "In this work we investigate the effect of the convolutional network depth on\nits accuracy in the large-scale image recognition setting. Our main\ncontribution is a thorough evaluation of networks of increasing depth using an\narchitecture with very small (3x3) convolution filters, which shows that a\nsignificant improvement on the prior-art configurations can be achieved by\npushing the depth to 16-19 weight layers. These findings were the basis of our\nImageNet Challenge 2014 submission, where our team secured the first and the\nsecond places in the localisation and classification tracks respectively. We\nalso show that our representations generalise well to other datasets, where\nthey achieve state-of-the-art results. We have made our two best-performing\nConvNet models publicly available to facilitate further research on the use of\ndeep visual representations in computer vision.",
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "Librispeech"
            },
            {
                "name": "SVHN"
            },
            {
                "name": "Cityscapes"
            },
            {
                "name": "Caltech"
            },
            {
                "name": "CIFAR-100"
            },
            {
                "name": "CIFAR-10"
            },
            {
                "name": "CUB-200-2011"
            },
            {
                "name": "ADE20K"
            },
            {
                "name": "COCO"
            },
            {
                "name": "CUB"
            },
            {
                "name": "MNIST"
            },
            {
                "name": "ImageNet Detection"
            },
            {
                "name": "ILSVRC 2016"
            },
            {
                "name": "Penn Treebank"
            },
            {
                "name": "PASCAL VOC 2012"
            },
            {
                "name": "WikiText-2"
            },
            {
                "name": "ILSVRC 2015"
            },
            {
                "name": "PASCAL VOC 2007"
            },
            {
                "name": "MS-COCO"
            },
            {
                "name": "LibriSpeech test-clean"
            },
            {
                "name": "Caltech-101"
            },
            {
                "name": "PASCAL Context"
            },
            {
                "name": "OCCLUSION"
            },
            {
                "name": "CamVid"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.999999982647504,
        "task": "Image Classification",
        "task_prob": 0.8215583088311644
    }
}