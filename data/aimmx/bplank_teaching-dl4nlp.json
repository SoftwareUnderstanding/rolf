{
    "visibility": {
        "visibility": "public"
    },
    "name": "teaching-dl4nlp material",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "bplank",
                "owner_type": "User",
                "name": "teaching-dl4nlp",
                "url": "https://github.com/bplank/teaching-dl4nlp",
                "stars": 4,
                "pushed_at": "2021-06-10 15:51:40+00:00",
                "created_at": "2021-04-24 16:00:51+00:00",
                "language": null,
                "frameworks": []
            },
            {
                "type": "code",
                "name": "athNLP-Lec3-BPlank-version2021.key.zip",
                "sha": "8e7d444776cb1d4e21f4f6ac035de2de662a6e53",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bplank/teaching-dl4nlp/blob/main/athNLP-Lec3-BPlank-version2021.key.zip"
                    }
                },
                "size": 33508405
            },
            {
                "type": "code",
                "name": "overview-concepts.png",
                "sha": "4842b13e393cec326daf325d5da633fec5b8aa4d",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/bplank/teaching-dl4nlp/blob/main/overview-concepts.png"
                    }
                },
                "size": 177308
            }
        ]
    },
    "authors": [
        {
            "name": "Barbara Plank",
            "email": "bplank@gmail.com",
            "github_id": "bplank"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/bplank/teaching-dl4nlp",
            "stars": 4,
            "issues": true,
            "readme": "# teaching-dl4nlp material\n\n## AthNLP 2019\n\nSlides from a 3 hour lecture on \"encoder-decoders\" at AthNLP 2019 summer school.\n\nContent:\n\n<img src=\"overview-concepts.png\" width=500>\n\n\n## Earlier notebooks\n\n### University of Malta 2019\n* Notebook slides from a week-long course, which cover a subpart of the material covered above, complemented with exercises and code (Keras).\n* Source: https://github.com/bplank/2019-ma-notebook\n\n### University of Groningen 2017\n* Notebook slides from a quadrisemester long course \"[Language Technology Project](https://www.rug.nl/ocasys/rug/vak/show?code=LIX025M05)\" with code (Keras)\n* Source: https://github.com/bplank/ltp-notebooks-2017\n\n* *Course description:* \tIn this project-based course, students are going to work on a natural language processing challenge. A challenge is a practical task in NLP related to a particular research topic. The task usually comes with data-sets for training, as well as a precise evaluation metric. The intention is that students familiarize themselves with a selected research topic, implement a solution and present the outcome critically in a term paper. The course is a mix of lectures, exercises and project-based work. The focus of this class is on deep learning (neural networks) for NLP.\n* *Learning outcomes:* After completing this course, students are able to tackle a challenge in Natural Language Processing, implement a solution, and critically assess their solution in the light of recent research papers in Natural Language Processing.\n\n\n## Key references:\n\n* [Jurafsky & Martin book](https://web.stanford.edu/~jurafsky/slp3/)\n* [Goldberg's primer](https://arxiv.org/abs/1510.00726)\n* [Kim 2014](https://arxiv.org/abs/1408.5882)\n* [Peters et al., 2018](https://arxiv.org/abs/1802.05365)\n* [Luong et al., 2015](https://arxiv.org/abs/1508.04025)\n",
            "readme_url": "https://github.com/bplank/teaching-dl4nlp",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "A Primer on Neural Network Models for Natural Language Processing",
            "arxiv": "1510.00726",
            "year": 2015,
            "url": "http://arxiv.org/abs/1510.00726v1",
            "abstract": "Over the past few years, neural networks have re-emerged as powerful\nmachine-learning models, yielding state-of-the-art results in fields such as\nimage recognition and speech processing. More recently, neural network models\nstarted to be applied also to textual natural language signals, again with very\npromising results. This tutorial surveys neural network models from the\nperspective of natural language processing research, in an attempt to bring\nnatural-language researchers up to speed with the neural techniques. The\ntutorial covers input encoding for natural language tasks, feed-forward\nnetworks, convolutional networks, recurrent networks and recursive networks, as\nwell as the computation graph abstraction for automatic gradient computation.",
            "authors": [
                "Yoav Goldberg"
            ]
        },
        {
            "title": "Deep contextualized word representations",
            "arxiv": "1802.05365",
            "year": 2018,
            "url": "http://arxiv.org/abs/1802.05365v2",
            "abstract": "We introduce a new type of deep contextualized word representation that\nmodels both (1) complex characteristics of word use (e.g., syntax and\nsemantics), and (2) how these uses vary across linguistic contexts (i.e., to\nmodel polysemy). Our word vectors are learned functions of the internal states\nof a deep bidirectional language model (biLM), which is pre-trained on a large\ntext corpus. We show that these representations can be easily added to existing\nmodels and significantly improve the state of the art across six challenging\nNLP problems, including question answering, textual entailment and sentiment\nanalysis. We also present an analysis showing that exposing the deep internals\nof the pre-trained network is crucial, allowing downstream models to mix\ndifferent types of semi-supervision signals.",
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer"
            ]
        },
        {
            "title": "Convolutional Neural Networks for Sentence Classification",
            "arxiv": "1408.5882",
            "year": 2014,
            "url": "http://arxiv.org/abs/1408.5882v2",
            "abstract": "We report on a series of experiments with convolutional neural networks (CNN)\ntrained on top of pre-trained word vectors for sentence-level classification\ntasks. We show that a simple CNN with little hyperparameter tuning and static\nvectors achieves excellent results on multiple benchmarks. Learning\ntask-specific vectors through fine-tuning offers further gains in performance.\nWe additionally propose a simple modification to the architecture to allow for\nthe use of both task-specific and static vectors. The CNN models discussed\nherein improve upon the state of the art on 4 out of 7 tasks, which include\nsentiment analysis and question classification.",
            "authors": [
                "Yoon Kim"
            ]
        },
        {
            "title": "Effective Approaches to Attention-based Neural Machine Translation",
            "arxiv": "1508.04025",
            "year": 2015,
            "url": "http://arxiv.org/abs/1508.04025v5",
            "abstract": "An attentional mechanism has lately been used to improve neural machine\ntranslation (NMT) by selectively focusing on parts of the source sentence\nduring translation. However, there has been little work exploring useful\narchitectures for attention-based NMT. This paper examines two simple and\neffective classes of attentional mechanism: a global approach which always\nattends to all source words and a local one that only looks at a subset of\nsource words at a time. We demonstrate the effectiveness of both approaches\nover the WMT translation tasks between English and German in both directions.\nWith local attention, we achieve a significant gain of 5.0 BLEU points over\nnon-attentional systems which already incorporate known techniques such as\ndropout. Our ensemble model using different attention architectures has\nestablished a new state-of-the-art result in the WMT'15 English to German\ntranslation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over\nthe existing best system backed by NMT and an n-gram reranker.",
            "authors": [
                "Minh-Thang Luong",
                "Hieu Pham",
                "Christopher D. Manning"
            ]
        },
        {
            "title": "Jurafsky & Martin book",
            "url": "https://web.stanford.edu/~jurafsky/slp3/"
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9999993325308366,
        "task": "Machine Translation",
        "task_prob": 0.9535784752083362
    }
}