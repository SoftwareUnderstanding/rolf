{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Adaptive Attention Span in Computer Vision",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "JoeRoussy",
                "owner_type": "User",
                "name": "adaptive-attention-in-cv",
                "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv",
                "stars": 28,
                "pushed_at": "2022-03-12 00:23:16+00:00",
                "created_at": "2020-02-29 17:20:57+00:00",
                "language": "Python",
                "description": "Implementation for our paper exploring a novel 2D adaptive attention span kernel in computer vision.",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "0e181292ec9dfc00a95568dfec6ebc2d610bb8fd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/.gitignore"
                    }
                },
                "size": 91
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "06b3a4a9a27030eead9805ed35468b82e1d42435",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/LICENSE"
                    }
                },
                "size": 1067
            },
            {
                "type": "code",
                "name": "attention.py",
                "sha": "dcc22feae518f6e60e9eb8cea28a952e3d6031b0",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/attention.py"
                    }
                },
                "size": 12683
            },
            {
                "type": "code",
                "name": "attention_augmented_conv.py",
                "sha": "e80cba4e4f8cf97697ae00c4c8b283bd709a9917",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/attention_augmented_conv.py"
                    }
                },
                "size": 6142
            },
            {
                "type": "code",
                "name": "config.py",
                "sha": "cc5945541a65aeab30bac8d9ae734f3d755e9b16",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/config.py"
                    }
                },
                "size": 4922
            },
            {
                "type": "code",
                "name": "file_writer.py",
                "sha": "661117e646ce2a992920f01a3935334f17787fcd",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/file_writer.py"
                    }
                },
                "size": 7787
            },
            {
                "type": "code",
                "name": "flop_count.py",
                "sha": "a7f0175bb922f0abf4dcd1d5099f09da8dfab211",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/flop_count.py"
                    }
                },
                "size": 8881
            },
            {
                "type": "code",
                "name": "main.py",
                "sha": "4b15cbeacbc9b24e546afad2a0c17ff482bf6471",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/main.py"
                    }
                },
                "size": 9606
            },
            {
                "type": "code",
                "name": "model.py",
                "sha": "f1827e8d6ad123fa4db614bc4accf95cad35306a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/model.py"
                    }
                },
                "size": 6757
            },
            {
                "type": "code",
                "name": "preprocess.py",
                "sha": "6dbcd70d06382dcb7b8240639b3e1b0ae41de084",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/preprocess.py"
                    }
                },
                "size": 7041
            },
            {
                "type": "code",
                "name": "process_tiny_image_net.py",
                "sha": "ef0f991d3e671416ec209cbe130eaa2f0fe3a889",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/process_tiny_image_net.py"
                    }
                },
                "size": 1603
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "f9def81186ba1cb9c9bb9dc633321f925d6b9ef4",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv/blob/master/requirements.txt"
                    }
                },
                "size": 100
            }
        ]
    },
    "authors": [
        {
            "name": "jerrodparker20",
            "github_id": "jerrodparker20"
        },
        {
            "name": "Shakti Kumar",
            "github_id": "shaktikshri"
        },
        {
            "name": "Joe Roussy",
            "github_id": "JoeRoussy"
        }
    ],
    "tags": [],
    "description": "Implementation for our paper exploring a novel 2D adaptive attention span kernel in computer vision.",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/JoeRoussy/adaptive-attention-in-cv",
            "stars": 28,
            "issues": true,
            "readme": "# Adaptive Attention Span in Computer Vision\n\nOfficial implementation of [Adaptive Attention Span in Computer Vision](https://arxiv.org/abs/2004.08708).\n\nIn this work we first try replicating results from [Stand-Alone Self-Attention in Vision Models](https://arxiv.org/abs/1906.05909).\n\nNext we propose a novel method based on the [Adaptive Attention Span](https://arxiv.org/abs/1905.07799) for learning a local self attention kernel size.\nWe compare this with [Local Attention](https://arxiv.org/abs/1906.05909) kernels as well as convolution kernels on CIFAR100.\nOur codes for Adaptive Attention Span in 2D is originally inspired from [FAIR's implementation](https://github.com/facebookresearch/adaptive-span/blob/master/adaptive_span.py).\nCode for self-attention in convolutions is loosely based on [this repo](https://github.com/leaderj1001/Stand-Alone-Self-Attention) by [leaderj1001](https://github.com/leaderj1001).\n\n\n### Steps to replicate\n1. Clone this repository\n2. Get the requirements ```pip install -r requirements.txt```\n\nExecution notes:\n* Our Adaptive implementation takes 3, 6 and 11 hours for small, medium and large models respectively on 2 P100 GPUs for 100 epochs on CIFAR100.\n* Some important flags are,\n    * To run on GPU, use the flag ```--cuda True```, otherwise do not use this option.\n    * Use flags ```--smallest_version True``` to run the smallest version. ```--small_version True``` to run the medium model and no flags to use the large model\n    * A description of each of the small, medium and large is given in Appendix A.3 of our [paper](https://arxiv.org/abs/2004.08708)\n* For more details on other flags, see the file config.py which has descriptions for each.\n\n### Snippets\nBest performing medium adaptive attention span model on CIFAR100:\n```\npython main.py --all_attention True --eta_min 0 --warmup_epochs 10 \\\n--lr 0.05 --batch-size 50 --small_version True --cuda True \\\n--num-workers 2 --xpid best_adaptive_medium --groups 4 \\\n--attention_kernel 5 --epochs 100 --dataset CIFAR100 --weight-decay 0.0005 \\\n--adaptive_span True --R 2 --span_penalty 0.01\n```\n\nBest performing medium local attention model on CIFAR100:\n```\npython main.py --all_attention True --eta_min 0 --warmup_epochs 10 \\\n--lr 0.05 --batch-size 50 --small_version True --cuda True \\\n--num-workers 2 --xpid best_local_medium --groups 4 \\\n--attention_kernel 5 --epochs 100  --dataset CIFAR100 --weight-decay 0.0005\n```\n\nBest performing medium CNN model on CIFAR100:\n```\npython main.py --eta_min 0 --warmup_epochs 10 --lr 0.2 --batch-size 50 \\\n--small_version True --cuda True --num-workers 2 --T_max 100 --xpid best_cnn_medium \\\n--dataset CIFAR100 --force_cosine_annealing True --weight-decay 0.0001\n```\n\n### Reference\nIf you find this repository useful, do cite it with \n```\n@misc{parker2020adaptive,\n    title={Adaptive Attention Span in Computer Vision},\n    author={Jerrod Parker and Shakti Kumar and Joe Roussy},\n    year={2020},\n    eprint={2004.08708},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n",
            "readme_url": "https://github.com/JoeRoussy/adaptive-attention-in-cv",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Stand-Alone Self-Attention in Vision Models",
            "arxiv": "1906.05909",
            "year": 2019,
            "url": "http://arxiv.org/abs/1906.05909v1",
            "abstract": "Convolutions are a fundamental building block of modern computer vision\nsystems. Recent approaches have argued for going beyond convolutions in order\nto capture long-range dependencies. These efforts focus on augmenting\nconvolutional models with content-based interactions, such as self-attention\nand non-local means, to achieve gains on a number of vision tasks. The natural\nquestion that arises is whether attention can be a stand-alone primitive for\nvision models instead of serving as just an augmentation on top of\nconvolutions. In developing and testing a pure self-attention vision model, we\nverify that self-attention can indeed be an effective stand-alone layer. A\nsimple procedure of replacing all instances of spatial convolutions with a form\nof self-attention applied to ResNet model produces a fully self-attentional\nmodel that outperforms the baseline on ImageNet classification with 12% fewer\nFLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention\nmodel matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and\n34% fewer parameters. Detailed ablation studies demonstrate that self-attention\nis especially impactful when used in later layers. These results establish that\nstand-alone self-attention is an important addition to the vision\npractitioner's toolbox.",
            "authors": [
                "Prajit Ramachandran",
                "Niki Parmar",
                "Ashish Vaswani",
                "Irwan Bello",
                "Anselm Levskaya",
                "Jonathon Shlens"
            ]
        },
        {
            "title": "Adaptive Attention Span in Transformers",
            "arxiv": "1905.07799",
            "year": 2019,
            "url": "http://arxiv.org/abs/1905.07799v2",
            "abstract": "We propose a novel self-attention mechanism that can learn its optimal\nattention span. This allows us to extend significantly the maximum context size\nused in Transformer, while maintaining control over their memory footprint and\ncomputational time. We show the effectiveness of our approach on the task of\ncharacter level language modeling, where we achieve state-of-the-art\nperformances on text8 and enwiki8 by using a maximum context of 8k characters.",
            "authors": [
                "Sainbayar Sukhbaatar",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Armand Joulin"
            ]
        },
        {
            "title": "Adaptive Attention Span in Computer Vision",
            "arxiv": "2004.08708",
            "year": 2020,
            "url": "http://arxiv.org/abs/2004.08708v1",
            "abstract": "Recent developments in Transformers for language modeling have opened new\nareas of research in computer vision. Results from late 2019 showed vast\nperformance increases in both object detection and recognition when\nconvolutions are replaced by local self-attention kernels. Models using local\nself-attention kernels were also shown to have less parameters and FLOPS\ncompared to equivalent architectures that only use convolutions. In this work\nwe propose a novel method for learning the local self-attention kernel size. We\nthen compare its performance to fixed-size local attention and convolution\nkernels. The code for all our experiments and models is available at\nhttps://github.com/JoeRoussy/adaptive-attention-in-cv",
            "authors": [
                "Jerrod Parker",
                "Shakti Kumar",
                "Joe Roussy"
            ]
        },
        {
            "primaryclass": "cs.CV",
            "archiveprefix": "arXiv",
            "eprint": "2004.08708",
            "year": "2020",
            "author": [
                "Parker, Jerrod",
                "Kumar, Shakti",
                "Roussy, Joe"
            ],
            "title": "Adaptive Attention Span in Computer Vision",
            "ENTRYTYPE": "misc",
            "ID": "parker2020adaptive",
            "authors": [
                "Parker, Jerrod",
                "Kumar, Shakti",
                "Roussy, Joe"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.90470659453744,
        "task": "Machine Translation",
        "task_prob": 0.9306471359766763
    },
    "training": {
        "datasets": [
            {
                "name": "ImageNet"
            },
            {
                "name": "enwiki8"
            },
            {
                "name": "Text8"
            },
            {
                "name": "COCO"
            }
        ]
    }
}