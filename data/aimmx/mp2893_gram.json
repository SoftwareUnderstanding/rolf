{
    "visibility": {
        "visibility": "public",
        "license": "BSD 3-Clause \"New\" or \"Revised\" License"
    },
    "name": "GRAM",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "mp2893",
                "owner_type": "User",
                "name": "gram",
                "url": "https://github.com/mp2893/gram",
                "stars": 205,
                "pushed_at": "2017-06-25 02:58:36+00:00",
                "created_at": "2016-11-04 01:01:38+00:00",
                "language": "Python",
                "description": "Graph-based Attention Model",
                "license": "BSD 3-Clause \"New\" or \"Revised\" License",
                "frameworks": [
                    "Theano"
                ]
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "e14139326d9d39d137c5776f463f68dfcd2da0bf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mp2893/gram/blob/master/LICENSE"
                    }
                },
                "size": 1468
            },
            {
                "type": "code",
                "name": "build_trees.py",
                "sha": "ce7e3cf96fdec7a2993c1480e787675a990328e2",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mp2893/gram/blob/master/build_trees.py"
                    }
                },
                "size": 5878
            },
            {
                "type": "code",
                "name": "create_glove_comap.py",
                "sha": "e295ae71c69600f77539140340764d9e43c038de",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mp2893/gram/blob/master/create_glove_comap.py"
                    }
                },
                "size": 1120
            },
            {
                "type": "code",
                "name": "glove.py",
                "sha": "9eeeae7d7bd5dfdc5d056415929e645329ce479b",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mp2893/gram/blob/master/glove.py"
                    }
                },
                "size": 5403
            },
            {
                "type": "code",
                "name": "gram.py",
                "sha": "a2118b51d9533e499ae47add8334d9c7e7a59e40",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mp2893/gram/blob/master/gram.py"
                    }
                },
                "size": 17678
            },
            {
                "type": "code",
                "name": "process_mimic.py",
                "sha": "d85d3e7df1b6c6becf36545ebce918f28c2f11fe",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/mp2893/gram/blob/master/process_mimic.py"
                    }
                },
                "size": 5505
            }
        ]
    },
    "authors": [
        {
            "name": "Edward Choi",
            "email": "edwardchoi@kaist.ac.kr",
            "github_id": "mp2893"
        }
    ],
    "tags": [],
    "description": "Graph-based Attention Model",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/mp2893/gram",
            "stars": 205,
            "issues": true,
            "readme": "GRAM\n=========================================\n\nGRAM is a prediction framework that can use the domain knowledge in the form of directed acyclic graph (DAG).\nDomain knowedge is incorporated in the training process using the [attention mechanism](https://arxiv.org/abs/1409.0473). \nBy introducing well established knoweldge into the training process, we can learn high quality representations of medical concepts that lead to more accurate predictions. \nThe prediction task could take any form such as static prediction, sequence classification, or sequential prediction.\n\n**t-SNE scatterplot of medical concepts trained with the combination of RNN and Multi-level Clincial Classification Software for ICD9** (The color of the dots represent the most general description of ICD9 diagnosis codes)\n![tsne](http://www.cc.gatech.edu/~echoi48/images/gram_tsne.png \"t-SNE scatterplot of medical concepts trained with the combination of RNN and Multi-level Clincial Classification Software for ICD9\")\n\n#### Relevant Publications\n\nGRAM implements the algorithm introduced in the following [paper](https://arxiv.org/abs/1611.07012):\n\n\tGRAM: Graph-based Attention Model for Healthcare Representation Learning\n\tEdward Choi, Mohammad Taha Bahadori, Le Song, Walter F. Stewart, Jimeng Sun  \n\tKnowledge Discovery and Data Mining (KDD) 2017\n\n#### Code Description\n\nThe current code trains an RNN ([Gated Recurrent Units](https://arxiv.org/abs/1406.1078)) to predict, at each timestep (i.e. visit), the diagnosis codes occurring in the next visit.\nThis is denoted as *Sequential Diagnoses Prediction* in the paper. \nIn the future, we will relases another version for making a single prediction for the entire visit sequence. (e.g. Predict the onset of heart failure given the visit record)\n\nNote that the current code uses [Multi-level Clinical Classification Software for ICD-9-CM](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp) as the domain knowledge.\nWe will release the one that uses ICD9 Diagnosis Hierarchy in the future.\n\t\n#### Running GRAM\n\n**STEP 1: Installation**  \n\n1. Install [python](https://www.python.org/), [Theano](http://deeplearning.net/software/theano/index.html). We use Python 2.7, Theano 0.8.2. Theano can be easily installed in Ubuntu as suggested [here](http://deeplearning.net/software/theano/install_ubuntu.html#install-ubuntu)\n\n2. If you plan to use GPU computation, install [CUDA](https://developer.nvidia.com/cuda-downloads)\n\n3. Download/clone the GRAM code  \n\n**STEP 2: Fastest way to test GRAM with MIMIC-III**  \n\nThis step describes how to run, with minimum number of steps, GRAM for predicting future diagnosis codes using MIMIC-III. \n\n0. You will first need to request access for [MIMIC-III](https://mimic.physionet.org/gettingstarted/access/), a publicly avaiable electronic health records collected from ICU patients over 11 years. \n\n1. You can use \"process_mimic.py\" to process MIMIC-III dataset and generate a suitable training dataset for GRAM. \nPlace the script to the same location where the MIMIC-III CSV files are located, and run the script. \nInstructions are described inside the script. \n\n2. Use \"build_trees.py\" to build files that contain the ancestor information of each medical code. \nThis requires \"ccs_multi_dx_tool_2015.csv\" (Multi-level CCS for ICD9), which can be downloaded from \n[here](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/Multi_Level_CCS_2015.zip).\nRunning this script will re-map integer codes assigned to all medical codes.\nTherefore you also need the \".seqs\" file and the \".types\" file created by process_mimc.py.\nThe execution command is `python build_trees.py ccs_multi_dx_tool_2015.csv <seqs file> <types file> <output path>`. \nThis will build five files that have \".level#.pk\" as the suffix.\nThis will replace the old \".seqs\" and \".types\" files with the correct ones.\n(Tian Bai, a PhD student from Temple University found out there was a problem with the re-mapping issue, which is now fixed. Thanks Tian!)\n\n3. Run GRAM using the \".seqs\" file generated by build_trees.py. \nThe \".seqs\" file contains the sequence of visits for each patient. Each visit consists of multiple diagnosis codes.\nInstead of using the same \".seqs\" file as both the training feature and the training label, \nwe recommend using \".3digitICD9.seqs\" file, which is also generated by process_mimic.py, as the training label for better performance and eaiser analysis.\nThe command is `python gram.py <seqs file> <3digitICD9.seqs file> <tree file prefix> <output path>`. \n\n**STEP 3: How to pretrain the code embedding**\n\nFor sequential diagnoses prediction, it is very effective to pretrain the code embeddings with some co-occurrence based algorithm such as [word2vec](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality) or [GloVe](http://nlp.stanford.edu/projects/glove/)\nIn the paper, we use GloVe for its speed, but either algorithm should be fine.\nHere we release codes to pretrain the code embeddings with GloVe.\n\n1. Use \"create_glove_comap.py\" with \".seqs\" file, which is generated by build_trees.py. (Note that you must run build_trees.py first before training the code embedding)\nThe execution command is `python create_glove_comap.py <seqs file> <tree file prefix> <output path>`.\nThis will create a file that contains the co-occurrence information of codes and ancestors.\n\n2. Use \"glove.py\" on the co-occurrence file generated by create_glove_comap.py.\nThe execution command is `python glovepy <co-occurrence file> <tree file prefix> <output path>`.\nThe embedding dimension is set to 128. If you change this, be careful to use the same value when training GRAM.\n\n3. Use the pretrained embeddings when you train GRAM.\nThe command is `python gram.py <seqs file> <3digitICD9.seqs file> <tree file prefix> <output path> --embed_file <embedding path> --embed_size <embedding dimension>`.\nAs mentioned above, be sure to set the correct embedding dimension.\n\n**STEP 4: How to prepare your own dataset**\n\n1. GRAM's training dataset needs to be a Python Pickled list of list of list. Each list corresponds to patients, visits, and medical codes (e.g. diagnosis codes, medication codes, procedure codes, etc.)\nFirst, medical codes need to be converted to an integer. Then a single visit can be seen as a list of integers. Then a patient can be seen as a list of visits.\nFor example, [5,8,15] means the patient was assigned with code 5, 8, and 15 at a certain visit.\nIf a patient made two visits [1,2,3] and [4,5,6,7], it can be converted to a list of list [[1,2,3], [4,5,6,7]].\nMultiple patients can be represented as [[[1,2,3], [4,5,6,7]], [[2,4], [8,3,1], [3]]], which means there are two patients where the first patient made two visits and the second patient made three visits.\nThis list of list of list needs to be pickled using cPickle. We will refer to this file as the \"visit file\".\n\n2. The label dataset (let us call this \"label file\") needs to have the same format as the \"visit file\".\nThe important thing is, time steps of both \"label file\" and \"visit file\" need to match. DO NOT train GRAM with labels that is one time step ahead of the visits. It is tempting since GRAM predicts the labels of the next visit. But it is internally taken care of.\nYou can use the \"visit file\" as the \"label file\" if you want GRAM to predict the exact codes. \nOr you can use a grouped codes as the \"label file\" if you are okay with reasonable predictions and want to save time. \nFor example, ICD9 diagnosis codes can be grouped into 283 categories by using [CCS](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp) groupers. \nWe STRONGLY recommend that you do this, because the number of medical codes can be as high as tens of thousands, \nwhich can cause not only low predictive performance but also memory issues. (The high-end GPUs typically have only 12GB of VRAM)\n\n3. Use the \"build_trees.py\" to create ancestor information, using the \"visit file\". You will also need a mapping file between the actual medical code names (e.g. \"419.10\") and the integer codes. Please refer to Step 2 to learn how to use \"build_trees.py\" script.\n\n**STEP 5: Hyper-parameter tuning used in the paper**\n\nThis [document](http://www.cc.gatech.edu/~echoi48/docs/gram_hyperparamters.pdf) provides the details regarding how we conducted the hyper-parameter tuning for all models used in the paper.\n",
            "readme_url": "https://github.com/mp2893/gram",
            "frameworks": [
                "Theano"
            ]
        }
    ],
    "references": [
        {
            "title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning",
            "arxiv": "1611.07012",
            "year": 2016,
            "url": "http://arxiv.org/abs/1611.07012v3",
            "abstract": "Deep learning methods exhibit promising performance for predictive modeling\nin healthcare, but two important challenges remain: -Data insufficiency:Often\nin healthcare predictive modeling, the sample size is insufficient for deep\nlearning methods to achieve satisfactory results. -Interpretation:The\nrepresentations learned by deep learning methods should align with medical\nknowledge. To address these challenges, we propose a GRaph-based Attention\nModel, GRAM that supplements electronic health records (EHR) with hierarchical\ninformation inherent to medical ontologies. Based on the data volume and the\nontology structure, GRAM represents a medical concept as a combination of its\nancestors in the ontology via an attention mechanism. We compared predictive\nperformance (i.e. accuracy, data needs, interpretability) of GRAM to various\nmethods including the recurrent neural network (RNN) in two sequential\ndiagnoses prediction tasks and one heart failure prediction task. Compared to\nthe basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely\nobserved in the training data and 3% improved area under the ROC curve for\npredicting heart failure using an order of magnitude less training data.\nAdditionally, unlike other methods, the medical concept representations learned\nby GRAM are well aligned with the medical ontology. Finally, GRAM exhibits\nintuitive attention behaviors by adaptively generalizing to higher level\nconcepts when facing data insufficiency at the lower level concepts.",
            "authors": [
                "Edward Choi",
                "Mohammad Taha Bahadori",
                "Le Song",
                "Walter F. Stewart",
                "Jimeng Sun"
            ]
        },
        {
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
            "arxiv": "1409.0473",
            "year": 2014,
            "url": "http://arxiv.org/abs/1409.0473v7",
            "abstract": "Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.",
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ]
        },
        {
            "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "arxiv": "1406.1078",
            "year": 2014,
            "url": "http://arxiv.org/abs/1406.1078v3",
            "abstract": "In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases.",
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merrienboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "MIMIC-III"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9996740738627287,
        "task": "Machine Translation",
        "task_prob": 0.9844447935795183
    }
}