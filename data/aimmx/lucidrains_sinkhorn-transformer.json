{
    "visibility": {
        "visibility": "public",
        "license": "MIT License"
    },
    "name": "Sinkhorn Transformer",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "lucidrains",
                "owner_type": "User",
                "name": "sinkhorn-transformer",
                "url": "https://github.com/lucidrains/sinkhorn-transformer",
                "stars": 200,
                "pushed_at": "2021-08-10 02:49:49+00:00",
                "created_at": "2020-04-03 17:38:29+00:00",
                "language": "Python",
                "description": "Sinkhorn Transformer - Practical implementation of Sparse Sinkhorn Attention",
                "license": "MIT License",
                "frameworks": [
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".github",
                "sha": "cbe5bf1f7bae378a49898909d4b61ac0412dee35",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/sinkhorn-transformer/tree/master/.github"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "b6e47617de110dea7ca47e087ff1347cc2646eda",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/sinkhorn-transformer/blob/master/.gitignore"
                    }
                },
                "size": 1799
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "cad43213602892aadc2290cca7afa74cc9f6af64",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/sinkhorn-transformer/blob/master/LICENSE"
                    }
                },
                "size": 1066
            },
            {
                "type": "code",
                "name": "divine.png",
                "sha": "b76aaa569d055ae07b58fcb282c1955210ff0c4f",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/sinkhorn-transformer/blob/master/divine.png"
                    }
                },
                "size": 37996
            },
            {
                "type": "code",
                "name": "examples",
                "sha": "bddd331a7a01b3b0ccb1af2a312b63f7cd9a7cc2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/sinkhorn-transformer/tree/master/examples"
                    }
                },
                "num_files": 3
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "de3ff055e5392b26e084aa6d82667c6fad409dd3",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/sinkhorn-transformer/blob/master/setup.py"
                    }
                },
                "size": 845
            },
            {
                "type": "code",
                "name": "sinkhorn.png",
                "sha": "f604dfbccd4bb4e27dfd6cba45dd7d1f03396abf",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/sinkhorn-transformer/blob/master/sinkhorn.png"
                    }
                },
                "size": 154803
            },
            {
                "type": "code",
                "name": "sinkhorn_transformer",
                "sha": "e20ef53ca597b46551f621498159eaf2074410d8",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/sinkhorn-transformer/tree/master/sinkhorn_transformer"
                    }
                },
                "num_files": 5
            },
            {
                "type": "code",
                "name": "sortcut.png",
                "sha": "5029ce5ac4ee4c7573ba2df5d12ec9a05be9eb90",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/lucidrains/sinkhorn-transformer/blob/master/sortcut.png"
                    }
                },
                "size": 161160
            }
        ]
    },
    "authors": [
        {
            "name": "Phil Wang",
            "email": "lucidrains@gmail.com",
            "github_id": "lucidrains"
        },
        {
            "name": "Danil",
            "email": "blizda@outlook.com",
            "github_id": "blizda"
        },
        {
            "name": "Peter",
            "email": "tatp22@gmail.com",
            "github_id": "tatp22"
        }
    ],
    "tags": [
        "artificial-intelligence",
        "deep-learning",
        "attention-mechanism",
        "transformers",
        "pytorch"
    ],
    "description": "Sinkhorn Transformer - Practical implementation of Sparse Sinkhorn Attention",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/lucidrains/sinkhorn-transformer",
            "stars": 200,
            "issues": true,
            "readme": "## Sinkhorn Transformer\n\n[![PyPI version](https://badge.fury.io/py/sinkhorn-transformer.svg)](https://badge.fury.io/py/sinkhorn-transformer)\n\n<img src=\"./sinkhorn.png\" width=\"500\">\n\n<img src=\"./sortcut.png\" width=\"500\">\n\n---\n\nThis is a reproduction of the work outlined in <a href=\"https://arxiv.org/abs/2002.11296\">Sparse Sinkhorn Attention</a>, with additional enhancements.\n\nIt includes a parameterized sorting network, using sinkhorn normalization to sample a permutation matrix that matches the most relevant buckets of keys to the buckets of queries.\n\nThis work also brings in reversible networks and feed forward chunking (concepts introduced from <a href=\"https://openreview.net/forum?id=rkgNKkHtvB\">Reformer</a>) to bring about further memory savings.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Eej8U4pP5ldZOz3tHwpoBFgmQqLhQLUq) 204k tokens (demonstration purposes)\n\n## Install\n\n```bash\n$ pip install sinkhorn_transformer\n```\n\n## Use\n\nA Sinkhorn Transformer based language model\n\n```python\nimport torch\nfrom sinkhorn_transformer import SinkhornTransformerLM\n\nmodel = SinkhornTransformerLM(\n    num_tokens = 20000,\n    dim = 1024,\n    heads = 8,\n    depth = 12,\n    max_seq_len = 8192,\n    bucket_size = 128,        # size of the buckets\n    causal = False,           # auto-regressive or not\n    n_sortcut = 2,            # use sortcut to reduce memory complexity to linear\n    n_top_buckets = 2,        # sort specified number of key/value buckets to one query bucket. paper is at 1, defaults to 2\n    ff_chunks = 10,           # feedforward chunking, from Reformer paper\n    reversible = True,        # make network reversible, from Reformer paper\n    emb_dropout = 0.1,        # embedding dropout\n    ff_dropout = 0.1,         # feedforward dropout\n    attn_dropout = 0.1,       # post attention dropout\n    attn_layer_dropout = 0.1, # post attention layer dropout\n    layer_dropout = 0.1,      # add layer dropout, from 'Reducing Transformer Depth on Demand' paper\n    weight_tie = True,        # tie layer parameters, from Albert paper\n    emb_dim = 128,            # embedding factorization, from Albert paper\n    dim_head = 64,            # be able to fix the dimension of each head, making it independent of the embedding dimension and the number of heads\n    ff_glu = True,            # use GLU in feedforward, from paper 'GLU Variants Improve Transformer'\n    n_local_attn_heads = 2,   # replace N heads with local attention, suggested to work well from Routing Transformer paper\n    pkm_layers = (4,7),       # specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best\n    pkm_num_keys = 128,       # defaults to 128, but can be increased to 256 or 512 as memory allows\n)\n\nx = torch.randint(0, 20000, (1, 2048))\nmodel(x) # (1, 2048, 20000)\n```\n\nA plain Sinkhorn Transformer, layers of sinkhorn attention\n\n```python\nimport torch\nfrom sinkhorn_transformer import SinkhornTransformer\n\nmodel = SinkhornTransformer(\n    dim = 1024,\n    heads = 8,\n    depth = 12,\n    bucket_size = 128\n)\n\nx = torch.randn(1, 2048, 1024)\nmodel(x) # (1, 2048, 1024)\n```\n\nSinkhorn Encoder / Decoder Transformer\n\n```python\nimport torch\nfrom sinkhorn_transformer import SinkhornTransformerLM\n\nDE_SEQ_LEN = 4096\nEN_SEQ_LEN = 4096\n\nenc = SinkhornTransformerLM(\n    num_tokens = 20000,\n    dim = 512,\n    depth = 6,\n    heads = 8,\n    bucket_size = 128,\n    max_seq_len = DE_SEQ_LEN,\n    reversible = True,\n    return_embeddings = True\n).cuda()\n\ndec = SinkhornTransformerLM(\n    num_tokens = 20000,\n    dim = 512,\n    depth = 6,\n    causal = True,\n    bucket_size = 128,\n    max_seq_len = EN_SEQ_LEN,\n    receives_context = True,\n    context_bucket_size = 128,  # context key / values can be bucketed differently\n    reversible = True\n).cuda()\n\nx = torch.randint(0, 20000, (1, DE_SEQ_LEN)).cuda()\ny = torch.randint(0, 20000, (1, EN_SEQ_LEN)).cuda()\n\nx_mask = torch.ones_like(x).bool().cuda()\ny_mask = torch.ones_like(y).bool().cuda()\n\ncontext = enc(x, input_mask=x_mask)\ndec(y, context=context, input_mask=y_mask, context_mask=x_mask) # (1, 4096, 20000)\n```\n\n## Autopadder\n\nBy default the model will complain if given an input that is not a multiple of the bucket size. To avoid having to make the same padding calculations each time, you can use the helper `Autopadder` class. It will take care of the `input_mask` for you as well, if given. Contextual key/values and mask are supported as well.\n\n```python\nimport torch\nfrom sinkhorn_transformer import SinkhornTransformerLM\nfrom sinkhorn_transformer import Autopadder\n\nmodel = SinkhornTransformerLM(\n    num_tokens = 20000,\n    dim = 1024,\n    heads = 8,\n    depth = 12,\n    max_seq_len = 2048,\n    bucket_size = 128,\n    causal = True\n)\n\nmodel = Autopadder(model, pad_left=True) # autopadder will fetch the bucket size and autopad input\n\nx = torch.randint(0, 20000, (1, 1117)) # odd sequence length\nmodel(x) # (1, 1117, 20000)\n```\n\n## Sinkhorn\n\nThis repository has diverged from the paper and is now using attention in place of the original sorting net + gumbel sinkhorn sampling. I have not found a noticeable difference in performance yet, and the new scheme allows me to generalize the network to flexible sequence lengths. If you would like to try Sinkhorn, please use the following settings, which only works for non-causal networks.\n\n```python\nimport torch\nfrom sinkhorn_transformer import SinkhornTransformerLM\n\nmodel = SinkhornTransformerLM(\n    num_tokens = 20000,\n    dim = 1024,\n    heads = 8,\n    depth = 12,\n    bucket_size = 128,\n    max_seq_len = 8192,\n    use_simple_sort_net = True, # turn off attention sort net\n    sinkhorn_iter = 7,          # number of sinkhorn iterations - default is set at reported best in paper\n    n_sortcut = 2,              # use sortcut to reduce complexity to linear time\n    temperature = 0.75,         # gumbel temperature - default is set at reported best in paper\n    non_permutative = False,    # allow buckets of keys to be sorted to queries more than once\n)\n\nx = torch.randint(0, 20000, (1, 8192))\nmodel(x) # (1, 8192, 20000)\n```\n\n## Product Key Memory\n\nTo see the benefits of using PKM, the learning rate of the values must be set higher than the rest of the parameters. (Recommended to be `1e-2`)\n\nYou can follow the instructions here to set it correctly https://github.com/lucidrains/product-key-memory#learning-rates\n\n## Issues\n\n### Decoding and sequence lengths\n\nSinkhorn, when trained on fixed length sequences, seems to have trouble decoding sequences from scratch, mainly due to the fact that the sorting net has trouble generalizing when the buckets are partially filled with padding tokens.\n\nFortunately, I think I have found a simple solution. During training, for causal networks, randomly truncate the sequences and force the sorting net to generalize. I have provided a flag (`randomly_truncate_sequence`) for the `AutoregressiveWrapper` instance to make this easy.\n\n\n```python\nimport torch\nfrom sinkhorn_transformer import SinkhornTransformerLM, AutoregressiveWrapper\n\nmodel = SinkhornTransformerLM(\n    num_tokens = 20000,\n    dim = 1024,\n    heads = 8,\n    depth = 12,\n    bucket_size = 75,\n    max_seq_len = 8192,\n    causal = True\n)\n\nmodel = AutoregressiveWrapper(model)\n\nx = torch.randint(0, 20000, (1, 8192))\nloss = model(x, return_loss = True, randomly_truncate_sequence = True) # (1, 8192, 20000)\n```\n\nI am open to suggestions if someone has found a better solution.\n\n### Causal sorting net\n\nThere is a potential problem with the causal sorting network, where the decision of which key/value buckets of the past sorts to a bucket is dependent only on the first token and not the rest (due to the bucketing scheme and preventing leakage of future to past).\n\nI have attempted to alleviate this problem by rotating half the heads to the left by bucket size - 1, thereby promoting the last token to be first. This is also the reason why the `AutoregressiveWrapper` defaults to left padding during training, to always make sure that the last token in the sequence have a say in what to retrieve.\n\nIf anyone has found a cleaner solution, please let me know in the issues.\n\n## Alternatives\n\n1. Routing Transformer - https://github.com/lucidrains/routing-transformer\n2. Reformer - https://github.com/lucidrains/reformer-pytorch\n\n## Citations\n\n```bibtex\n@misc{tay2020sparse,\n    title   = {Sparse Sinkhorn Attention},\n    author  = {Yi Tay and Dara Bahri and Liu Yang and Donald Metzler and Da-Cheng Juan},\n    year    = {2020},\n    url.    = {https://arxiv.org/abs/2002.11296}\n}\n```\n\n```bibtex\n@inproceedings{kitaev2020reformer,\n    title       = {Reformer: The Efficient Transformer},\n    author      = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},\n    booktitle   = {International Conference on Learning Representations},\n    year        = {2020},\n    url         = {https://openreview.net/forum?id=rkgNKkHtvB}\n}\n```\n\n```bibtex\n@misc{lan2019albert,\n    title       = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\n    author      = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\n    year        = {2019},\n    url         = {https://arxiv.org/abs/1909.11942}\n}\n```\n\n```bibtex\n@misc{shazeer2020glu,\n    title   = {GLU Variants Improve Transformer},\n    author  = {Noam Shazeer},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2002.05202}\n}\n```\n\n```bibtex\n@misc{roy*2020efficient,\n    title   = {Efficient Content-Based Sparse Attention with Routing Transformers},\n    author  = {Aurko Roy* and Mohammad Taghi Saffar* and David Grangier and Ashish Vaswani},\n    year    = {2020},\n    url     = {https://openreview.net/forum?id=B1gjs6EtDr}\n}\n```\n\n```bibtex\n@inproceedings{fan2020reducing,\n    title     ={Reducing Transformer Depth on Demand with Structured Dropout},\n    author    ={Angela Fan and Edouard Grave and Armand Joulin},\n    booktitle ={International Conference on Learning Representations},\n    year      ={2020},\n    url       ={https://openreview.net/forum?id=SylO2yStDr}\n}\n```\n\n```bibtex\n@misc{lample2019large,\n    title   = {Large Memory Layers with Product Keys},\n    author  = {Guillaume Lample and Alexandre Sablayrolles and Marc'Aurelio Ranzato and Ludovic Denoyer and Herv\u00e9 J\u00e9gou},\n    year    = {2019},\n    eprint  = {1907.05242},\n    archivePrefix = {arXiv}\n}\n```\n\n```bibtex\n@misc{bhojanapalli2020lowrank,\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\n    year    = {2020},\n    eprint  = {2002.07028}\n}\n```\n\n<img src=\"./divine.png\"></img>",
            "readme_url": "https://github.com/lucidrains/sinkhorn-transformer",
            "frameworks": [
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Sparse Sinkhorn Attention",
            "arxiv": "2002.11296",
            "year": 2020,
            "url": "http://arxiv.org/abs/2002.11296v1",
            "abstract": "We propose Sparse Sinkhorn Attention, a new efficient and sparse method for\nlearning to attend. Our method is based on differentiable sorting of internal\nrepresentations. Concretely, we introduce a meta sorting network that learns to\ngenerate latent permutations over sequences. Given sorted sequences, we are\nthen able to compute quasi-global attention with only local windows, improving\nthe memory efficiency of the attention module. To this end, we propose new\nalgorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a\ndynamic sequence truncation method for tailoring Sinkhorn Attention for\nencoding and/or decoding purposes. Via extensive experiments on algorithmic\nseq2seq sorting, language modeling, pixel-wise image generation, document\nclassification and natural language inference, we demonstrate that our memory\nefficient Sinkhorn Attention method is competitive with vanilla attention and\nconsistently outperforms recently proposed efficient Transformer models such as\nSparse Transformers.",
            "authors": [
                "Yi Tay",
                "Dara Bahri",
                "Liu Yang",
                "Donald Metzler",
                "Da-Cheng Juan"
            ]
        },
        {
            "title": "GLU Variants Improve Transformer",
            "arxiv": "2002.05202",
            "year": 2020,
            "url": "http://arxiv.org/abs/2002.05202v1",
            "abstract": "Gated Linear Units (arXiv:1612.08083) consist of the component-wise product\nof two linear projections, one of which is first passed through a sigmoid\nfunction. Variations on GLU are possible, using different nonlinear (or even\nlinear) functions in place of sigmoid. We test these variants in the\nfeed-forward sublayers of the Transformer (arXiv:1706.03762)\nsequence-to-sequence model, and find that some of them yield quality\nimprovements over the typically-used ReLU or GELU activations.",
            "authors": [
                "Noam Shazeer"
            ]
        },
        {
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
            "arxiv": "1909.11942",
            "year": 2019,
            "url": "http://arxiv.org/abs/1909.11942v6",
            "abstract": "Increasing model size when pretraining natural language representations often\nresults in improved performance on downstream tasks. However, at some point\nfurther model increases become harder due to GPU/TPU memory limitations and\nlonger training times. To address these problems, we present two\nparameter-reduction techniques to lower memory consumption and increase the\ntraining speed of BERT. Comprehensive empirical evidence shows that our\nproposed methods lead to models that scale much better compared to the original\nBERT. We also use a self-supervised loss that focuses on modeling\ninter-sentence coherence, and show it consistently helps downstream tasks with\nmulti-sentence inputs. As a result, our best model establishes new\nstate-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having\nfewer parameters compared to BERT-large. The code and the pretrained models are\navailable at https://github.com/google-research/ALBERT.",
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ]
        },
        {
            "url.": "https://arxiv.org/abs/2002.11296",
            "year": "2020",
            "author": [
                "Tay, Yi",
                "Bahri, Dara",
                "Yang, Liu",
                "Metzler, Donald",
                "Juan, Da-Cheng"
            ],
            "title": "Sparse Sinkhorn Attention",
            "ENTRYTYPE": "misc",
            "ID": "tay2020sparse",
            "authors": [
                "Tay, Yi",
                "Bahri, Dara",
                "Yang, Liu",
                "Metzler, Donald",
                "Juan, Da-Cheng"
            ]
        },
        {
            "url": "https://openreview.net/forum?id=rkgNKkHtvB",
            "year": "2020",
            "booktitle": "International Conference on Learning Representations",
            "author": [
                "Kitaev, Nikita",
                "Kaiser, Lukasz",
                "Levskaya, Anselm"
            ],
            "title": "Reformer: The Efficient Transformer",
            "ENTRYTYPE": "inproceedings",
            "ID": "kitaev2020reformer",
            "authors": [
                "Kitaev, Nikita",
                "Kaiser, Lukasz",
                "Levskaya, Anselm"
            ]
        },
        {
            "url": "https://arxiv.org/abs/1909.11942",
            "year": "2019",
            "author": [
                "Lan, Zhenzhong",
                "Chen, Mingda",
                "Goodman, Sebastian",
                "Gimpel, Kevin",
                "Sharma, Piyush",
                "Soricut, Radu"
            ],
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
            "ENTRYTYPE": "misc",
            "ID": "lan2019albert",
            "authors": [
                "Lan, Zhenzhong",
                "Chen, Mingda",
                "Goodman, Sebastian",
                "Gimpel, Kevin",
                "Sharma, Piyush",
                "Soricut, Radu"
            ]
        },
        {
            "url": "https://arxiv.org/abs/2002.05202",
            "year": "2020",
            "author": [
                "Shazeer, Noam"
            ],
            "title": "GLU Variants Improve Transformer",
            "ENTRYTYPE": "misc",
            "ID": "shazeer2020glu",
            "authors": [
                "Shazeer, Noam"
            ]
        },
        {
            "url": "https://openreview.net/forum?id=B1gjs6EtDr",
            "year": "2020",
            "author": [
                "Roy*, Aurko",
                "Saffar*, Mohammad Taghi",
                "Grangier, David",
                "Vaswani, Ashish"
            ],
            "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
            "ENTRYTYPE": "misc",
            "ID": "roy*2020efficient",
            "authors": [
                "Roy*, Aurko",
                "Saffar*, Mohammad Taghi",
                "Grangier, David",
                "Vaswani, Ashish"
            ]
        },
        {
            "url": "https://openreview.net/forum?id=SylO2yStDr",
            "year": "2020",
            "booktitle": "International Conference on Learning Representations",
            "author": [
                "Fan, Angela",
                "Grave, Edouard",
                "Joulin, Armand"
            ],
            "title": "Reducing Transformer Depth on Demand with Structured Dropout",
            "ENTRYTYPE": "inproceedings",
            "ID": "fan2020reducing",
            "authors": [
                "Fan, Angela",
                "Grave, Edouard",
                "Joulin, Armand"
            ]
        },
        {
            "archiveprefix": "arXiv",
            "eprint": "1907.05242",
            "year": "2019",
            "author": [
                "Lample, Guillaume",
                "Sablayrolles, Alexandre",
                "Ranzato, Marc'Aurelio",
                "Denoyer, Ludovic",
                "J\u00e9gou, Herv\u00e9"
            ],
            "title": "Large Memory Layers with Product Keys",
            "ENTRYTYPE": "misc",
            "ID": "lample2019large",
            "authors": [
                "Lample, Guillaume",
                "Sablayrolles, Alexandre",
                "Ranzato, Marc'Aurelio",
                "Denoyer, Ludovic",
                "J\u00e9gou, Herv\u00e9"
            ]
        },
        {
            "eprint": "2002.07028",
            "year": "2020",
            "author": [
                "Bhojanapalli, Srinadh",
                "Yun, Chulhee",
                "Rawat, Ankit Singh",
                "Reddi, Sashank J.",
                "Kumar, Sanjiv"
            ],
            "title": "Low-Rank Bottleneck in Multi-head Attention Models",
            "ENTRYTYPE": "misc",
            "ID": "bhojanapalli2020lowrank",
            "authors": [
                "Bhojanapalli, Srinadh",
                "Yun, Chulhee",
                "Rawat, Ankit Singh",
                "Reddi, Sashank J.",
                "Kumar, Sanjiv"
            ]
        }
    ],
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9933906392043214,
        "task": "Machine Translation",
        "task_prob": 0.9439049495455176
    },
    "training": {
        "datasets": [
            {
                "name": "SQuAD"
            },
            {
                "name": "GLUE"
            },
            {
                "name": "RACE"
            }
        ]
    }
}