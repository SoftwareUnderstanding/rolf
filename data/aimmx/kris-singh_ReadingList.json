{
    "visibility": {
        "visibility": "public"
    },
    "name": "Papers To Read",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "kris-singh",
                "owner_type": "User",
                "name": "ReadingList",
                "url": "https://github.com/kris-singh/ReadingList",
                "stars": 0,
                "pushed_at": "2019-01-24 12:04:12+00:00",
                "created_at": "2018-12-11 06:36:22+00:00",
                "language": "HTML",
                "frameworks": []
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "c6de4e5393d79972e11039be8af17b1d65271d60",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kris-singh/ReadingList/blob/master/.gitignore"
                    }
                },
                "size": 18
            },
            {
                "type": "code",
                "name": "PaperSummaries",
                "sha": "d760a800261a62312bcdc329cf43ce4977044449",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kris-singh/ReadingList/tree/master/PaperSummaries"
                    }
                },
                "num_files": 4
            },
            {
                "type": "code",
                "name": "README.org",
                "sha": "60fd1d6628d8cae23edbfa9540eb47289114c70a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/kris-singh/ReadingList/blob/master/README.org"
                    }
                },
                "size": 3296
            }
        ]
    },
    "authors": [
        {
            "name": "Kris Singh",
            "email": "krishnakant.singh@suiit.ac.in",
            "github_id": "kris-singh"
        }
    ],
    "tags": [],
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/kris-singh/ReadingList",
            "stars": 0,
            "issues": true,
            "readme": "### Papers To Read\n\n1. BREAKING THE SOFTMAX BOTTLENECK: A HIGH-RANK RNN LANGUAGE MODEL\n2. Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition\n3. TARMAC: TARGETED MULTI-AGENT COMMUNICATION\n4. Towards Understanding Linear Word Analogies\n5. Understanding the impact of entropy on policy optimization\n6. Do explanations make VQA models more predictable to a human?\n7. How agents see things: On visual representations in an emergent language game\n8. Semantic Parsing for Task Oriented Dialog using Hierarchical Representations\n9. [https://openreview.net/pdf?id=ryQu7f-RZ] AmsGrad\n\n\n### Projects Roadmap\n\n1. Visdial Research \n\t> RNN in Pytorch\n\t> HRED in Pytorch\n\t> LMFUSION in Pytorch\n\t> Topical Hred Augmenting Neural Response Generation with Context-Aware Topical Attention\n\t> A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\n\t> Implemention of Co-Operating Games\n\t> GuessWhat?! Visual object discovery through multi-modal dialogue\n\n\n2. Presentation for PRML group\n\t> Reading the VI paper\n\t> Reading the VAE Paper \n\t\t0. [https://arxiv.org/abs/1312.6114] Original Paper\n\t\t1. [https://arxiv.org/abs/1606.05908] Tutorial\n\t\t2. Implementation in Pytorch(Ipython notebook prefered for tutorial)\n\t> VARIATIONAL INFERENCE: FOUNDATIONS AND INNOVATIONS Tutorial\n\n3. Fashion Image Tagging \n\t> Reading the Google open images []\n\t> Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Tags Riken Noisy Fashion)\n\t> Masking: A New Perspective of Noisy Supervision (Tags Riken Noisy Fashion)\n\t>\n\n4. Autograd & Computational Graph \n\n\n\n### Mostly Cuda  Learning\n\n0. Efficient Large-scale Approximate Nearest Neighbor Search on OpenCL FPGA\n1. Billion-scale similarity search with GPUs FAIR (Tags CUDA FAISS LOPQ Image Search)\n2. Locally Optimized Product Quantization for Approximate Nearest Neighbor Search (Tags CUDA LOPQ Image Search)\n3. Sparse Tensor tutorial (Tags SparseTensor CUDA OPENAI )\n\t> [Blog](https://blog.openai.com/block-sparse-gpu-kernels/)\n\t> [Implementation](https://github.com/openai/blocksparse)\n\t> [Pytorch](https://github.com/pytorch/pytorch/issues/9674)\n\n\n\n## RL Resources\n\n1. https://spinningup.openai.com/en/latest/spinningup/keypapers.html\n\n\n## *Papers to Implement\n\n1. Poincare\u0301 Embeddings for Learning Hierarchical Representations\n2. A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 \u2013 LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY\n3. https://github.com/rusty1s/pytorch_geometric.git\n\n\n\n\n## Blogs to Read\n1. Hovrod\n2. [https://towardsdatascience.com/how-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb] Gated Conv Neural Network\n3. [https://www.fast.ai/2018/07/02/adam-weight-decay/] Super Convergence\n4. [http://www.phontron.com/class/nn4nlp2017/schedule.html] NLP course\n5. [https://stats.stackexchange.com/questions/281240/why-is-the-cost-function-of-neural-networks-non-convex] Why loss function is convex still the loss i s surface is not convex\n",
            "readme_url": "https://github.com/kris-singh/ReadingList",
            "frameworks": []
        }
    ],
    "references": [
        {
            "title": "Auto-Encoding Variational Bayes",
            "arxiv": "1312.6114",
            "year": 2013,
            "url": "http://arxiv.org/abs/1312.6114v10",
            "abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.",
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ]
        },
        {
            "title": "Tutorial on Variational Autoencoders",
            "arxiv": "1606.05908",
            "year": 2016,
            "url": "http://arxiv.org/abs/1606.05908v3",
            "abstract": "In just three years, Variational Autoencoders (VAEs) have emerged as one of\nthe most popular approaches to unsupervised learning of complicated\ndistributions. VAEs are appealing because they are built on top of standard\nfunction approximators (neural networks), and can be trained with stochastic\ngradient descent. VAEs have already shown promise in generating many kinds of\ncomplicated data, including handwritten digits, faces, house numbers, CIFAR\nimages, physical models of scenes, segmentation, and predicting the future from\nstatic images. This tutorial introduces the intuitions behind VAEs, explains\nthe mathematics behind them, and describes some empirical behavior. No prior\nknowledge of variational Bayesian methods is assumed.",
            "authors": [
                "Carl Doersch"
            ]
        }
    ],
    "domain": {
        "domain_type": "Unknown"
    }
}