{
    "visibility": {
        "visibility": "public",
        "license": "Apache License 2.0"
    },
    "name": "ulangel",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "uchange",
                "owner_type": "Organization",
                "name": "ulangel",
                "url": "https://github.com/uchange/ulangel",
                "stars": 0,
                "pushed_at": "2020-03-05 13:23:52+00:00",
                "created_at": "2019-12-19 14:22:46+00:00",
                "language": "Python",
                "description": "Python NLP text classification library based on LSTM neural network ULMFiT",
                "license": "Apache License 2.0",
                "frameworks": [
                    "scikit-learn",
                    "PyTorch"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "6a0812a4dae929ecff650200de401f4e0e4d8b61",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uchange/ulangel/blob/master/.gitignore"
                    }
                },
                "size": 1214
            },
            {
                "type": "code",
                "name": "CHANGELOG.md",
                "sha": "3ad8dd69673afb6fd44a882f880a6102dc42001a",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uchange/ulangel/blob/master/CHANGELOG.md"
                    }
                },
                "size": 1299
            },
            {
                "type": "code",
                "name": "LICENSE",
                "sha": "261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uchange/ulangel/blob/master/LICENSE"
                    }
                },
                "size": 11357
            },
            {
                "type": "code",
                "name": "doc",
                "sha": "6911b2a0b650fe150351890184d3e1053e583861",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uchange/ulangel/tree/master/doc"
                    }
                },
                "num_files": 6
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "32c973ef8ecc4cffdf7ddac714c78c346f155184",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uchange/ulangel/blob/master/requirements.txt"
                    }
                },
                "size": 59
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "5e7a5e39bf5ca328f10d36d9d306db7f653929dc",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uchange/ulangel/blob/master/setup.py"
                    }
                },
                "size": 2126
            },
            {
                "type": "code",
                "name": "ulangel",
                "sha": "193dccf5fb26cb103122ab5dbc924f318036e9b9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/uchange/ulangel/tree/master/ulangel"
                    }
                },
                "num_files": 4
            }
        ]
    },
    "authors": [
        {
            "name": "GuanguanUchange",
            "github_id": "GuanguanUchange"
        },
        {
            "name": "Marc Debureaux",
            "email": "marc@debnet.fr",
            "github_id": "debnet"
        }
    ],
    "tags": [],
    "description": "Python NLP text classification library based on LSTM neural network ULMFiT",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/uchange/ulangel",
            "stars": 0,
            "issues": true,
            "readme": "# ulangel\n\n## Background\nUlangel is a python library for NLP text classification. The idea comes from the article of Jeremy Howard et al. \"Universal Language Model Fine-tuning for Text Classification\" https://arxiv.org/pdf/1801.06146.pdf. The original codes are from the fastai library (https://github.com/fastai/course-v3). We use its NLP part as a source of reference and modify some codes to adapt to our use case. The name `ulangel` comes from `u`niversal `lang`uage mod`el`. It also means the fruit of the research department of company U (a french parisian startup), so called `l'ange de U` (the angel of U). U aimes to describe the ecosystem established by corporates as well as startups by our product `Motherbase`. In Motherbase, we have a large quantity of texts concerning company descriptions, communications where we apply this library `ulangel` to do the Natural Language Processing.\n\nThis is a LSTM based neural network. To classify the text, we train at first a language model and then fine-tune it into a classifier. That means the whole system is composed of two parts: language model and the text classifier.\n`The language model` is trained to predict the next word based on the input text. Its structure is shown below: ![Language model strucutre](doc/language_model_diagram.jpg)\nIt is supposed to treat only texts, because other features won't help to predict the next word.\n\n`The classifier` is adapted from the language model: it keeps all layers except the decoder and then adds a pooling layer and a full connected linear neural network in order to classify.\nDifferent from the language model input, there are two kinds of inputs that this library is able to deal with for the text classification:\n* `Text only mode`: This input mode means that the input consists of only integers of the text, for exemple: [45, 30, ..., 183]. The classifier structure is shown in the figure below: ![Classifier text only mode](doc/classifier_text_only.jpg)\n* `Text plus mode`: This input mode means that the input consists of not only integers of the text, but also other features for the classification problem, for exemple: [[45, 30, ..., 183], True, 2, 2019, ...]. The list [16, 8, 9, 261, ...] is integers of the text as in the `text only mode`. `True` can be a boolean to tell if this text contains a country name, `2` can be the number of presence of the country name, `2019` can be the publication year of this text. You can also add as many features as you want. All features after the integer list can be no matter what you want, as long as they are useful for your classification problem. The classifier structure is shown in the figure below: ![Classifier text plus mode](doc/classifier_text_plus.jpg)\n\n\nIn this library you will find all structures needed to process texts, to pack data, to construct your lstm with dropouts, and some evaluation fuctions, optimizers to train your neural network.\n\nAll classes and methodes are seperated in three different parts of this library:\n* `ulangel.data`: Preparation of the text classification data, including the text preparation (such as text cleaning) and the data formating (such as creating dataset, creating databunch, padding all texts to have the same length in the same batch, etc.).\n* `ulangel.rnn`: Create recurrent neural network structures, such as connection dropouts, activation dropouts, LSTM for language model, encoder, etc.\n* `ulangel.utils`: Some tools for training, such as callbacks, optimizers, evaluations functions, etc.\n\n\n## Install\n\n  ```\n  pip install ulangel\n  ```\n\n## Usage\n\n### ulangel.data\nThis part is for the data preparation. There are two main groups of functionalities: `ulangel.data.text_processor` for the text cleaning, and `ulangel.data.data_packer` for the data gathering.\n\n#### ulangel.data.text_processor\nIn this part, there are methods to clean the text, including:\n1. Replace HTML special characters and emoji\n2. Replace word repetitions and add `xxwrep` ahead: word word word -> xxwrep 3 word\n3. Replace character repetitions and add `xxrep` ahead: cccc -> xxrep 4 c\n4. Add spaces around /,@,#,:\n5. Remove multiple spaces and keep just one\n6. Replace tokens with all letters in capitals by their lower case and add `xxup` ahead: GOOD JOB -> xxup good xxup job\n7. Replace tokens with the first letter in capital by their lower caser and add `xxmaj` ahead: We -> xxmaj we\n\nThe method `ulangel.data.text_processor.text_proc` calls all methods above to clean the text, tokenize it, and add `xbos` at the beginning and `xfld` at the end of the text. Here is a notebook to show standard text processing steps, including text cleaning and text numeralization: [text_processor](doc/word_embedding_text_processor.ipynb)\n\n\n#### ulangel.data.data_packer\nThere are three types of data objects in our training and validation systems. The default input data are numpy.ndarray objects.\n* Dataset: Divide(or/and shuffle) input data into batches. Each dataset item is a tuple of x and its corresponding y.\n* Dataloader: Here we use the pytorch dataloader, to get dataset item in the way defined by the sampler.\n* Databunch: Gathering the training and validation dataloader as one data object and will be given to the learner to train the neural network.\n\n\n##### Dataset\n* `ulangel.data.data_packer.LanguageModelDataset`: For a language model, the input is a bptt-length text and the output is also a text as long as the input with just one word shifted. For a text [w0, w1, w2, w3, w4, ...] (wi is the corresponding integer of a word, in the dictionary of your own text corpus.) If bptt = 4, the input i0 is [w0, w1, w2, w3], then the output o0 will be [w1, w2, w3, w4]. The input and the output are generated from the same text, with the help of the class LanguageModelDataset.\n```python\n  import numpy as np\n  from ulangel.data.data_packer import LanguageModelDataset\n  trn_lm = np.load(your_path/'your_file.npy', allow_pickle=True)\n  trn_lm_ds = LanguageModelDataset(data=trn_lm, bs=2, bptt=4, shuffle=False)\n  # print an item of dataset: (x, y)\n  next(iter(trn_lm_ds))\n  >>> (tensor([1.1000e+01, 5.0000e+00, 2.0000e+00, 1.0000e+01]),\n  tensor([5.0000e+00, 2.0000e+00, 1.0000e+01, 4.0000e+00]))\n```\n\n* `ulangel.data.data_packer.TextClassificationDataset`: For a text classifier, its dataset is a little bit different. The input is still the same, but the output is an integer representing the corresponding class label. In this case, we use TextClassificationDataset which inherits the pytorch dataset `torch.utils.data.Dataset`.\n```python\n  import numpy as np\n  from ulangel.data.data_packer import TextClassificationDataset\n  trn_ids = np.load(your_path/'your_text_file.npy', allow_pickle=True)\n  trn_label = np.load(your_path/'your_label_file.npy', allow_pickle=True)\n  trn_clas_ds = TextClassificationDataset(x=trn_ids, y=trn_labels)\n  # print an item of dataset: (x, y)\n  next(iter(trn_clas_ds))\n  >>> (array([   11,     5,     2,    10,     4,     7,     5,     2,     9,\n              4]), 2)\n```\n\n##### Dataloader\nIn this library, we use the pytorch dataloader, but with our own sampler. For the language model, batches are generated by concatenating all texts so they all have the same length. We can use directly the dataloader to pack data.\n```python\n  from torch.utils.data import DataLoader\n  trn_lm_dl = DataLoader(trn_lm_ds, batch_size=2)\n  # print an item of dataloader: a batch of dataset\n  next(iter(trn_lm_dl))\n  >>> [tensor([[11.,  5.,  2., 10.],\n           [12., 11.,  5.,  2.]]), tensor([[ 5.,  2., 10.,  4.],\n           [11.,  5.,  2., 10.]])]\n```\n\nHowever, for the text classification, we can not concatenate texts together, because each text has its own class. It doesn't make sense to mix texts to form equilong texts in the batch. In order to train the neural network in an efficient way and at the same time to keep the randomness, we have two different samplers for the training and the validation data. Additionally, for texts in each batch, they should have the same length, so we use a collate function to pad those short texts.\nFor the Classification data, we use the dataloader in this way:\n```python\n  from ulangel.data.data_packer import pad_collate_textonly\n  trn_clas_dl = DataLoader(trn_clas_ds, batch_size=2, sampler=trn_sampler, collate_fn=pad_collate_textonly)\n  val_clas_dl = DataLoader(val_clas_ds, batch_size=2, sampler=val_sampler, collate_fn=pad_collate_textonly)\n```\nHow to create samplers and collat_fn will be explained below.\n\n###### Sampler\nSampler is an index generator. It returns a list of indexes, which corresponding item is sorted by the attribute key. In this library, TrainingSampler and ValidationSampler inherit the pytorch sampler `torch.utils.data.Sampler`.\n\n* `ulangel.data.data_packer.TrainingSampler`: TrainingSampler is a sampler for the training data. It sorts the data in the way defined by the given key, the longest at the first, the shortest at the end, random in the middle.\n```python\n  from ulangel.data.data_packer import TrainingSampler\n  trn_sampler = TrainingSampler(data_source=trn_clas_ds.x, key=lambda t: len(trn_clas_ds.x[t]), bs=2)\n```\nIn this exemple, the data source is the x of the training dataset (texts), the key is the length of each text.\n\n* `ulangel.data.data_packer.ValidationSampler`: It sorts the data in the way defined by the given key, in an ascending or a descending way. It is different from the TrainingSampler, there is no randomness, the validation texts will be sorted from the longest to the shortest.\n```python\n  from ulangel.data.data_packer import ValidationSampler\n  val_sampler = ValidationSampler(val_clas_ds.x, key=lambda t: len(val_clas_ds.x[t]))\n```\nIn this exemple, the data source is the x of the validation dataset (texts), the key is the length of each text.\n\n###### Collate Function\nCollate function can be used to manipulate your input data. In this library, our collate function: `pad_collate` is to pad all texts with padding index pad_idx to have the same length for one whole batch. This pad_collate function is inbuild, we just need to import, so that we can use it in the dataloader. It exists for two different input modes.\n* `ulangel.data.data_packer.pad_collate_textonly`:\n```python\n  from ulangel.data.data_packer import pad_collate_textonly\n```\n\n* `ulangel.data.data_packer.pad_collate_textplus`: is the textplus version.\n```python\n  from ulangel.data.data_packer import pad_collate_textplus\n```\n\n##### Databunch\n* `ulangel.data.data_packer.DataBunch`: Databunch packs your training dataloader and validation dataloader together into a databunch object, so that your can give it to your learner (which will be explained later in README)\n```python\n  from ulangel.data.data_packer import DataBunch\n  language_model_data = DataBunch(trn_lm_dl, val_lm_dl)\n```\n\n### ulangel.rnn\nIn this part, there are two main blocks to build a neural network: dropouts and some special neural network structures for our transfer learning.\n\n#### ulangel.rnn.dropouts\nThe pytorch dropout are dropouts to zero out some activations with probability p. In the article of Stephen Merity et al. \"Regularizing and Optimizing LSTM Language Models\" https://arxiv.org/pdf/1708.02182.pdf they propose to apply dropouts not only on activations but also on connections. Using pytorch dropouts is not enough. Therefore, we create three different dropout classes:\n\n* `ulangel.rnn.dropouts.ActivationDropout`: as its name, this is a dropout to zero out activations in the layer of the neural network.\n* `ulangel.rnn.dropouts.ConnectionWeightDropout`: as its name, this is a dropout to zero out connections (weights) between layers.\n* `ulangel.rnn.dropouts.EmbeddingDropout`: this is a dropout class to zero out embedding activations.\n\nThese three dropout classes will be used in the AWD_LSTM to build the LSTM for the language model training. Of course, you can also import them to build your own neural network.\n\n#### ulangel.rnn.nn_block\nIn this part, we have some structures to build a language model and a text classifier.\n\n* `ulangel.rnn.nn_block.AWD_LSTM`: is the class to build the language model except the decoder layer. This is the commom part for the language model and the text classifier. It is a LSTM neural network inheriting `torch.nn.Module` proposed by Stephen Merity et al. in the article \"Regularizing and Optimizing LSTM Language Models\" https://arxiv.org/pdf/1708.02182.pdf. Because we use the pretrained language model wikitext-103 from this article, to finetune our own language model on our corpus, we need to keep the same values for some hyperparameters as wikitext-103: embedding_size = 400, number_of_hidden_activation = 1150.\n```python\n  from ulangel.rnn.nn_block import AWD_LSTM\n  # define hyperparameters\n  class LmArg:\n      def __init__(self):\n          self.number_of_tokens = 10000\n          self.embedding_size = 400\n          self.pad_token = 1\n          self.embedding_dropout = 0.05\n          self.number_of_hidden_activation = 1150\n          self.number_of_layers = 3\n          self.activation_dropout = 0.3\n          self.input_activation_dropout = 0.65\n          self.embedding_activation_dropout = 0.1\n          self.connection_hh_dropout = 0.5\n          self.decoder_activation_dropout = 0.4\n          self.bptt = 16\n  encode_args = LmArg()\n  # build the LSTM neural network\n  lstm_enc = AWD_LSTM(\n      vocab_sz=encode_args.nomber_of_tokens,\n      emb_sz=encode_args.embedding_size,\n      n_hid=encode_args.number_of_hidden_activation,\n      n_layers=encode_args.number_of_layers,\n      pad_token=encode_args.pad_token,\n      hidden_p=encode_args.activation_dropout,\n      input_p=encode_args.input_activation_dropout,\n      embed_p=encode_args.embedding_activation_dropout,\n      weight_p=encode_args.connection_hh_dropout)\n  lstm_enc\n  >>> AWD_LSTM(\n    (emb): Embedding(10000, 400, padding_idx=1)\n    (emb_dp): EmbeddingDropout(\n      (emb): Embedding(10000, 400, padding_idx=1)\n    )\n    (rnns): ModuleList(\n      (0): ConnectionWeightDropout(\n        (module): LSTM(400, 1150, batch_first=True)\n      )\n      (1): ConnectionWeightDropout(\n        (module): LSTM(1150, 1150, batch_first=True)\n      )\n      (2): ConnectionWeightDropout(\n        (module): LSTM(1150, 400, batch_first=True)\n      )\n    )\n    (input_dp): ActivationDropout()\n    (hidden_dps): ModuleList(\n      (0): ActivationDropout()\n      (1): ActivationDropout()\n      (2): ActivationDropout()\n    )\n  )\n```\n\n* `ulangel.rnn.nn_block.LinearDecoder`: This is a decoder inheriting `torch.nn.Module`, the inverse of an encoder, to transfer the last hidden layer (embedding vector) into its corresponding integer representation of the work, so that we can find comprehensive words for human.\n```python\n  from ulangel.rnn.nn_block import LinearDecoder\n  decoder = LinearDecoder(\n      encode_args.number_of_tokens,\n      encode_args.embedding_size,\n      encode_args.decoder_activation_dropout,\n      tie_encoder=lstm_enc.emb,\n      bias=True\n  )\n  decoder\n  >>>LinearDecoder(\n    (output_dp): ActivationDropout()\n    (decoder): Linear(in_features=400, out_features=10000, bias=True)\n  )\n```\n\n* `ulangel.rnn.nn_block.SequentialRNN`: This class inherits the pytorch class `torch.nn.Sequential`, to connect different neural networks, and allows to reset all parameters of substructures with a reset methode (ex: AWD_LSTM)\n```python\n  from ulangel.rnn.nn_block import SequentialRNN\n  language_model = SequentialRNN(lstm_enc, decoder)\n  language_model.modules\n  >>>\n  <bound method Module.modules of SequentialRNN(\n    (0): AWD_LSTM(\n      (emb): Embedding(10000, 400, padding_idx=1)\n      (emb_dp): EmbeddingDropout(\n        (emb): Embedding(10000, 400, padding_idx=1)\n      )\n      (rnns): ModuleList(\n        (0): ConnectionWeightDropout(\n          (module): LSTM(400, 1150, batch_first=True)\n        )\n        (1): ConnectionWeightDropout(\n          (module): LSTM(1150, 1150, batch_first=True)\n        )\n        (2): ConnectionWeightDropout(\n          (module): LSTM(1150, 400, batch_first=True)\n        )\n      )\n      (input_dp): ActivationDropout()\n      (hidden_dps): ModuleList(\n        (0): ActivationDropout()\n        (1): ActivationDropout()\n        (2): ActivationDropout()\n      )\n    )\n    (1): LinearDecoder(\n      (output_dp): ActivationDropout()\n      (decoder): Linear(in_features=400, out_features=10000, bias=True)\n    )\n  )>\n```\n\nFor the classification data, there are two types of input data: `text only mode` and `text plus mode` as mentioned in the Background. Therefore all structures concerning classifier are made in two versions for these two different modes of input data.\n\n* `ulangel.rnn.nn_block.TextOnlySentenceEncoder`: it is a class similar to `ulangel.rnn.nn_block.AWD_LSTM`, but the difference is when the input text length exceeds the value of bptt (we define to train the language model), it divides the text into serval bptt-length sequences at the input and concatenates the results back to one text at the output.\n```python\n  from ulangel.rnn.nn_block import TextOnlySentenceEncoder\n  sent_enc = TextOnlySentenceEncoder(lstm_enc, encode_args.bptt)\n  sent_enc\n  >>>TextOnlySentenceEncoder(\n    (module): AWD_LSTM(\n      (emb): Embedding(10000, 400, padding_idx=1)\n      (emb_dp): EmbeddingDropout(\n        (emb): Embedding(10000, 400, padding_idx=1)\n      )\n      (rnns): ModuleList(\n        (0): ConnectionWeightDropout(\n          (module): LSTM(400, 1150, batch_first=True)\n        )\n        (1): ConnectionWeightDropout(\n          (module): LSTM(1150, 1150, batch_first=True)\n        )\n        (2): ConnectionWeightDropout(\n          (module): LSTM(1150, 400, batch_first=True)\n        )\n      )\n      (input_dp): ActivationDropout()\n      (hidden_dps): ModuleList(\n        (0): ActivationDropout()\n        (1): ActivationDropout()\n        (2): ActivationDropout()\n      )\n    )\n  )\n```\n\n* `ulangel.rnn.nn_block.TextPlusSentenceEncoder`: is the text plus version of the `SentenceEncoder`\n```python\n  from ulangel.rnn.nn_block import TextPlusSentenceEncoder\n  sent_enc = TextPlusSentenceEncoder(lstm_enc, encode_args.bptt)\n  sent_enc\n  >>>TextPlusSentenceEncoder(\n    (module): AWD_LSTM(\n      (emb): Embedding(10000, 400, padding_idx=1)\n      (emb_dp): EmbeddingDropout(\n        (emb): Embedding(10000, 400, padding_idx=1)\n      )\n      (rnns): ModuleList(\n        (0): ConnectionWeightDropout(\n          (module): LSTM(400, 1150, batch_first=True)\n        )\n        (1): ConnectionWeightDropout(\n          (module): LSTM(1150, 1150, batch_first=True)\n        )\n        (2): ConnectionWeightDropout(\n          (module): LSTM(1150, 400, batch_first=True)\n        )\n      )\n      (input_dp): ActivationDropout()\n      (hidden_dps): ModuleList(\n        (0): ActivationDropout()\n        (1): ActivationDropout()\n        (2): ActivationDropout()\n      )\n    )\n  )\n```\n\n* `ulangel.rnn.nn_block.TextOnlyPoolingLinearClassifier`: different from the language model, we don't need the decoder to read the output. We want to classify the input text. So at the output of the `ulangel.rnn.nn_block.AWD_LSTM`, we do some pooling to pick the last sequence of the LSTM's output, the max pooling of the LSTM's output, the average pooling of the LSTM's output. We concatenate these three sequences, as input of a linear full connected neural net classifier. The last layer's number of activations should be the same as the number of classes in your classification problem.\n```python\n  from ulangel.rnn.nn_block import TextOnlyPoolingLinearClassifier\n  pool_clas = TextOnlyPoolingLinearClassifier(\n      layers=[3*encode_args.emsize, 100, 4], # define the number of activations for each layer\n      drops=[0.2, 0.1])\n  pool_clas\n  >>>TextOnlyPoolingLinearClassifier(\n    (layers): Sequential(\n      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Dropout(p=0.2, inplace=False)\n      (2): Linear(in_features=1200, out_features=100, bias=True)\n      (3): ReLU(inplace=True)\n      (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): Dropout(p=0.1, inplace=False)\n      (6): Linear(in_features=100, out_features=4, bias=True)\n      (7): ReLU(inplace=True)\n    )\n  )\n```\n\n* `ulangel.rnn.nn_block.TextPlusPoolingLinearClassifier`: is the text plus version. The difference from the text only mode is that text plus mode pooling linear classifier has another group of layers. This supplemental group of layers takes nonverbal features into account.\n```python\n  from ulangel.rnn.nn_block import TextPlusPoolingLinearClassifier\n  pool_clas = TextPlusPoolingLinearClassifier(\n      layers1=[3*encode_args.emsize, 100, 4], # structure of text only classification\n      drops1=[0.2, 0.1],\n      layers2=[9, 4], # structure that you want with other features\n      drops2=[0.1])\n  pool_clas\n  >>>TextPlusPoolingLinearClassifier(\n      (layers1): Sequential(\n          (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1): Dropout(p=0.2, inplace=False)\n          (2): Linear(in_features=1200, out_features=100, bias=True)\n          (3): ReLU(inplace=True)\n          (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): Dropout(p=0.1, inplace=False)\n          (6): Linear(in_features=100, out_features=4, bias=True)\n          (7): ReLU(inplace=True)\n      )\n      (layers2): Sequential(\n          (0): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1): Dropout(p=0.1, inplace=False)\n          (2): Linear(in_features=9, out_features=4, bias=True)\n          (3): ReLU(inplace=True)\n      )\n)\n```\n\nTo build the complete classifier, we use the `ulangel.rnn.nn_block.SequentialRNN` to connect these two classes, here is an exemple for the text only mode:\n```python\n  classifier = SequentialRNN(sent_enc, pool_clas)\n  classifier\n  >>>SequentialRNN(\n    (0): TextOnlySentenceEncoder(\n      (module): AWD_LSTM(\n        (emb): Embedding(10000, 400, padding_idx=1)\n        (emb_dp): EmbeddingDropout(\n          (emb): Embedding(10000, 400, padding_idx=1)\n        )\n        (rnns): ModuleList(\n          (0): ConnectionWeightDropout(\n            (module): LSTM(400, 1150, batch_first=True)\n          )\n          (1): ConnectionWeightDropout(\n            (module): LSTM(1150, 1150, batch_first=True)\n          )\n          (2): ConnectionWeightDropout(\n            (module): LSTM(1150, 400, batch_first=True)\n          )\n        )\n        (input_dp): ActivationDropout()\n        (hidden_dps): ModuleList(\n          (0): ActivationDropout()\n          (1): ActivationDropout()\n          (2): ActivationDropout()\n        )\n      )\n    )\n    (1): TextOnlyPoolingLinearClassifier(\n      (layers): Sequential(\n        (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Dropout(p=0.2, inplace=False)\n        (2): Linear(in_features=1200, out_features=100, bias=True)\n        (3): ReLU(inplace=True)\n        (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): Dropout(p=0.1, inplace=False)\n        (6): Linear(in_features=100, out_features=4, bias=True)\n        (7): ReLU(inplace=True)\n      )\n    )\n  )\n```\nFor the text plus mode it will be the same. Just need to be careful that sentence encoder and pooling linear classifier should be in the same mode (an TextOnlySentenceEncoder should not be followed by a TextPlusPoolingLinearClassifier).\n\n### ulangel.utils\nIn this part, there are some tools for the training of the neural network.\n\n#### Callbacks\nCallbacks are triggers during the training. Calling callbacks can make intermediate computation or do the setting.\n\n* `ulangel.utils.callbacks.TrainEvalCallback`: setting if the model is in the training mode or in the validation mode. During the training mode, update the progressing and the number of iteration.\n\n* `ulangel.utils.callbacks.TextOnlyCudaCallback`: putting the model and the variables on cuda.\n\n* `ulangel.utils.callbacks.TextPlusCudaCallback`: is the textplus version to put the model and the variables on cuda.\n\n* `ulangel.utils.callbacks.Recorder`: recording the loss value and the learning rate of every batch, plot the variation of these two values if the methode (recorder.plot_lr() / recorder.plot_loss() / recorder.plot()) is called.\n\n* `ulangel.utils.callbacks.LR_Find`: giving the minimum and the maximum of learning rate and the maximum number of iteration, change linearlly the learning rate (from the minimum value to the maximum value) at every batch. Combine with the Recorder, we can see the evaluation of loss so that we can find an appropriate learning rate for the training. Warning: if there is LR_Find in the callback list, the model is running to go through all learning rates, but not to train the model.\n\n* `ulangel.utils.callbacks.RNNTrainer`: recording the prediction result, raw_output (without applying dropouts) and output (after applying dropouts) after every prediction. If needed, it can also add AR or/and TAR regularization to the loss to avoid overfitting.\n\n* `ulangel.utils.callbacks.ParamScheduler`: allowing to schedule any hyperparameter during the training, such as learning rate, momentum, weight decay, etc. It takes the hyperparameter's name and its schedule function sched_func. Here we use a combined schedule function combine_scheds, combing two different parts of a cosine function, to have a learning rate low at the beginning and at the end, high in the middle.\n```python\n  from ulangel.utils.callbacks import combine_scheds, ParamScheduler, sched_cos\n  lr = 1e-3\n  sched_cos1 = sched_cos(start=lr/10, end=lr*2)\n  sched_cos2 = sched_cos(start=lr*2, end=lr/100)\n  # pcts means the percentages taken by the following functions in scheds. In the exemple below means the sched combines the first 0.3 of sched_cos1 and the last 0.7 of sched_cos2.\n  sched = combine_scheds(pcts=[0.3, 0.7], scheds=[sched_cos1, sched_cos2])\n```\n The scheduled learing rate defined above looks like this:\n ![scheduled learning rate](doc/learning_rate_scheduler.png)\n\n\nFor the training process, it's up to the user to choose callbacks to make a callback list. Here it's an exemple:\n```python\n  from ulangel.utils.callbacks import TrainEvalCallback, TextOnlyCudaCallback, Recorder, RNNTrainer\n  cbs = [TextOnlyCudaCallback(), TrainEvalCallback(), Recorder(), RNNTrainer(alpha=2., beta=1.), ParamScheduler('lr', sched)]\n```\n\n\n#### Stats\nStats contains all classes and functions to compute statistics of the model's performance. There are two classes and some methods.\n\n* metrics: A metric function takes the outputs of your model, and the target values as inputs, and you can define your own way to evaluate your model's performance by writing your own computation in the function. Functions `ulangel.utils.stats.accuracy_flat` (calculate the accuracy for the language model) and `ulangel.utils.stats.accuracy` (calculate the accuracy for the classifier) are two inbuild metrics that we provide.\nWarning: `ulangel.utils.stats.cross_entropy_flat` is not a metric. It is a loss function, but it is similar to the `accuracy_flat` metric, so we put them at the same place.\n\n* `ulangel.utils.stats.AvgStats`: calculate loss and statistics defined by input metrics. This class puts the loss value and other performance statistics defined by metrics together into a list. It also has methods to update and print all these performance statistics when called.\n\n* `ulangel.utils.stats.AvgStatsCallback`: Actually the class `AvgStatsCallback` is also a callback, it uses AvgStats to calculate all performance statistics after every batch, and print these statistics after every epoch.\n\nWe can add AvgStatsCallback into the callback list, so that we can know the neural network performs after every epoch.\n\n```python\n  from ulangel.utils.stats import AvgStatsCallback, accuracy, accuracy_flat\n  # for a language model\n  cbs_languagemodel = [TextOnlyCudaCallback(), TrainEvalCallback(), AvgStatsCallback([accuracy_flat]), Recorder(), RNNTrainer(alpha=2., beta=1.), ParamScheduler('lr', sched)]\n\n  # for a classifier\n  cbs_classifier = [TextOnlyCudaCallback(), TrainEvalCallback(), AvgStatsCallback([accuracy]), Recorder(), ParamScheduler('lr', sched_clas)]\n```\n\n\n#### Optimizer\n* optimizers: `ulangel.utils.optimizer.Optimizer` is a class that decides the way to update all parameters of the model by steppers. `ulangel.utils.optimizer.StatefulOptimizer` is an optimizer with state. It inherits the class `Optimizer` and adds an attribute `state` in order to track the history of updates. As we know, when we use an optimizer with momentum, we need to know the last update value to calculate the current one. In this case, we use `StatefulOptimizer` in this library.\n\n* stepper: functions defining how to update the parameters or the gradient of the parameters. It depends on the current values. In the library we provide several steppers: `ulangel.utils.optimizer.sgd_step` (stochastic gradient descent stepper), `ulangel.utils.optimizer.weight_decay` (weight decay stepper), `ulangel.utils.optimizer.adam_step` (adam stepper). You can also program your own stepper.\n\n* stateupdater: define how to initialize and update state (for exemple, how to update momentum). In the library we provide some inbuild stateupdaters, all of them inherit the class `ulangel.utils.optimizer.StateUpdater`: `ulangel.utils.optimizer.AverageGrad`(momentum created by averaging the gradient), `ulangel.utils.optimizer.AverageSqrGrad`(momentum created by averaging the square of the gradient), `ulangel.utils.optimizer.StepCount`(step increment).\n\nIn ulangel, we provide two inbuild optimizer: `ulangel.utils.optimizer.sgd_opt` (stochastic gradient descent optimizer) and `ulangel.utils.optimizer.adam_opt` (adam optimizer). Optimizer is an input of the object of the class leaner. We will show you how to use optimizer in the learner part.\n\nIf you want to, you can also write your own stepper, your own stateupdater, to build your own optimizer. Here is an exemple to build an optimizer with momentum.\n\n```python\n  from ulangel.utils.optimizer import StatefulOptimizer, StateUpdater, StepCount\n\n  def your_stepper(p, lr, *args, **kwargs):\n      p = your_stepper_function(p, lr)\n      return p\n\n  def your_stateupdater(Stat):\n      def __init__(self):\n          your initialization values\n\n      def init_state(self, p):\n          return {\"your_state_name\": your_state_initialization_function(p)}\n\n      def update(self, p, state, *args, **kwargs):\n          state[\"your_state_name\"] = your_state_update_function(p, *args)\n          return state\n\n  def your_optimizer(xtra_step=None, **kwargs):\n      return partial(\n          StatefulOptimizer,\n          steppers=[your_stepper] + listify(xtra_step),\n          stateupdaters=[your_stateupdater()],\n          **kwargs\n      )\n```\n\n\n#### ulangel.utils.learner\nThis part includes the class `Learner` and some methods to freeze or unfreeze layers in order to train just a part of or the whole neural network.\n\n##### Learner\n* The class `ulangel.utils.learner.Learner`: It is a class that takes the RNN model, data for training, the loss function, the optimizer, the learning rate and callbacks that you need. The method `Learner.fit(epochs=number of epochs that you want to train)` executes all processes in order to train the model. Here is an exemple to build the langage model learner:\n```python\n  from ulangel.utils.learner import Learner\n  language_model_learner = Learner(\n      model=language_model,\n      data=language_model_data,\n      loss_func=cross_entropy_flat,\n      opt_func=adam_opt(),\n      lr=1e-5,\n      cbs=cbs_languagemodel)\n  # load the pretrained model\n  wgts = torch.load('your_pretrained_model.h5')\n  # some key corresponding may be necessary\n  dict_new = language_model_learner.model.state_dict().copy()\n  dict_new['key1'] = wgts['key1_pretrained']\n  dict_new['key2'] = wgts['key2_pretrained']\n\n  language_model_learner.model.load_state_dict(dict_new)\n  language_model_learner.fit(2)\n  >>>\n  0\n  train: [loss value 1 for training set, tensor(metric value 1 for training set, device='cuda:0')]\n  valid: [loss value 1 for validation set, tensor(metric value 1 for validation set, device='cuda:0')]\n  1\n  train: [loss value 2 for training set, tensor(metric value 1 for training set, device='cuda:0')]\n  valid: [loss value 2 for validation set, tensor(metric value 1 for validation set, device='cuda:0')]\n\n  # save your model if you are satisfied with its performance\n  torch.save(language_model_learner.model.state_dict(), 'your_model_path.pkl')\n```\n\n##### Methods to freeze of unfreeze layers\n* `ulangel.utils.learner.freeze_all` is a method that sets `requires_grad` of all parameters of the neural network as `Fasle`.\n* `ulangel.utils.learner.unfreeze_all` is a method that sets `requires_grad` of all parameters of the neural network as `True`.\n* `ulangel.utils.learner.freeze_upto` freezes first n layers of the neural network with `requires_grad` as `False` and `requires_grad` of the rest of layers as 'True'. It's useful when you want to train just the last few layers of a neural network.\nHere is the notebook to show how to use these methods: [freeze_to.ipynb](doc/freeze_to.ipynb)\n\n## Software Requirements\nPython 3.6\ntorch 1.3.1\ntorchvision 0.4.2\n\n## Related efforts\n* `Regularizing and Optimizing LSTM Language Models` by Stephen Merity et al\n  Article: https://arxiv.org/pdf/1708.02182.pdf\n  Github: https://github.com/salesforce/awd-lstm-lm\n\n* `Universal Language Model Fine-tuning for Text Classification` by Jeremy Howard et al\n  Article: https://arxiv.org/pdf/1801.06146.pdf\n  Github: https://github.com/fastai/course-v3\n",
            "readme_url": "https://github.com/uchange/ulangel",
            "frameworks": [
                "scikit-learn",
                "PyTorch"
            ]
        }
    ],
    "references": [
        {
            "title": "Universal Language Model Fine-tuning for Text Classification",
            "arxiv": "1801.06146",
            "year": 2018,
            "url": "http://arxiv.org/abs/1801.06146v5",
            "abstract": "Inductive transfer learning has greatly impacted computer vision, but\nexisting approaches in NLP still require task-specific modifications and\ntraining from scratch. We propose Universal Language Model Fine-tuning\n(ULMFiT), an effective transfer learning method that can be applied to any task\nin NLP, and introduce techniques that are key for fine-tuning a language model.\nOur method significantly outperforms the state-of-the-art on six text\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\nFurthermore, with only 100 labeled examples, it matches the performance of\ntraining from scratch on 100x more data. We open-source our pretrained models\nand code.",
            "authors": [
                "Jeremy Howard",
                "Sebastian Ruder"
            ]
        },
        {
            "title": "Regularizing and Optimizing LSTM Language Models",
            "arxiv": "1708.02182",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02182v1",
            "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.",
            "authors": [
                "Stephen Merity",
                "Nitish Shirish Keskar",
                "Richard Socher"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "WikiText-103"
            },
            {
                "name": "Company*"
            },
            {
                "name": "Penn Treebank"
            },
            {
                "name": "WikiText-2"
            }
        ]
    },
    "domain": {
        "domain_type": "Natural Language Processing",
        "domain_prob": 0.9998775769311171,
        "task": "Language Modelling",
        "task_prob": 0.48092777845381746
    }
}