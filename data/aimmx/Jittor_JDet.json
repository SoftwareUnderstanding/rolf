{
    "visibility": {
        "visibility": "public"
    },
    "name": "JDet",
    "definition": {
        "code": [
            {
                "type": "repo",
                "repo_type": "github",
                "owner": "Jittor",
                "owner_type": "User",
                "name": "JDet",
                "url": "https://github.com/Jittor/JDet",
                "stars": 83,
                "pushed_at": "2022-03-19 13:44:26+00:00",
                "created_at": "2021-08-04 09:13:15+00:00",
                "language": "Python",
                "description": "JDet is an object detection benchmark based on Jittor. Mainly focus on aerial image object detection (oriented object detection).",
                "frameworks": [
                    "TensorFlow"
                ]
            },
            {
                "type": "code",
                "name": ".gitignore",
                "sha": "ad4cbb46426c5f16204cb4610e1722112da63eee",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Jittor/JDet/blob/master/.gitignore"
                    }
                },
                "size": 445
            },
            {
                "type": "code",
                "name": "configs",
                "sha": "5f7a81d1020d3a8ec25365fc812d97846375a5fc",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Jittor/JDet/tree/master/configs"
                    }
                },
                "num_files": 12
            },
            {
                "type": "code",
                "name": "docs",
                "sha": "93c9318ca11b08673a1a0acf6bb756fd4e910525",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Jittor/JDet/tree/master/docs"
                    }
                },
                "num_files": 8
            },
            {
                "type": "code",
                "name": "projects",
                "sha": "249b6b4d303c8c68bbed7723fe8600f1e7972bf2",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Jittor/JDet/tree/master/projects"
                    }
                },
                "num_files": 9
            },
            {
                "type": "code",
                "name": "python",
                "sha": "2149eb50388ee96913838470c74e96f85bd22310",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Jittor/JDet/tree/master/python"
                    }
                },
                "num_files": 1
            },
            {
                "type": "code",
                "name": "requirements.txt",
                "sha": "96c8b0154e27e633e74054c64fceadfdb550c137",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Jittor/JDet/blob/master/requirements.txt"
                    }
                },
                "size": 106
            },
            {
                "type": "code",
                "name": "setup.py",
                "sha": "d06c67df278c3de0a965047837017fe241500a96",
                "filetype": "file",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Jittor/JDet/blob/master/setup.py"
                    }
                },
                "size": 912
            },
            {
                "type": "code",
                "name": "tests",
                "sha": "ecb383c06b6130c39aa26ee50e5507d607ccd926",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Jittor/JDet/tree/master/tests"
                    }
                },
                "num_files": 2
            },
            {
                "type": "code",
                "name": "tools",
                "sha": "cf14ffa759251d2a30aaa4ca8ff71475ab98f2d9",
                "filetype": "dir",
                "connection": {
                    "name": "github_url",
                    "source": {
                        "url": "https://github.com/Jittor/JDet/tree/master/tools"
                    }
                },
                "num_files": 4
            }
        ]
    },
    "authors": [
        {
            "name": "yang guo ye",
            "email": "498731903@qq.com",
            "github_id": "cxjyxxme"
        },
        {
            "name": "Xiang-Li Li",
            "email": "1905692338@qq.com",
            "github_id": "li-xl"
        },
        {
            "name": "CzzzzH",
            "email": "282492021@qq.com",
            "github_id": "CzzzzH"
        },
        {
            "name": "zhouwy19",
            "github_id": "zhouwy19"
        },
        {
            "name": "514flowey",
            "github_id": "514flowey"
        },
        {
            "name": "Gword",
            "github_id": "Gword"
        },
        {
            "name": "Yi_Zhang",
            "email": "yi.zhang.4096@gmail.com",
            "github_id": "uyzhang"
        },
        {
            "name": "Jittor",
            "github_id": "Jittor"
        },
        {
            "name": "GT-Zhang",
            "github_id": "GT-ZhangAcer"
        }
    ],
    "tags": [
        "deep-learning",
        "object-detection",
        "oriented-object-detection",
        "aerial-image-detection",
        "jittor"
    ],
    "description": "JDet is an object detection benchmark based on Jittor. Mainly focus on aerial image object detection (oriented object detection).",
    "extraction": [
        {
            "type": "github",
            "url": "https://github.com/Jittor/JDet",
            "stars": 83,
            "issues": true,
            "readme": "# JDet\n## Introduction\nJDet is an object detection benchmark based on [Jittor](https://github.com/Jittor/jittor), and mainly focus on aerial image object detection (oriented object detection). \n\n<!-- **Features**\n- Automatic compilation. Our framwork is based on Jittor, which means we don't need to Manual compilation for these code with CUDA and C++.\n-  -->\n\n<!-- Framework details are avaliable in the [framework.md](docs/framework.md) -->\n## Install\nJDet environment requirements:\n\n* System: **Linux**(e.g. Ubuntu/CentOS/Arch), **macOS**, or **Windows Subsystem of Linux (WSL)**\n* Python version >= 3.7\n* CPU compiler (require at least one of the following)\n    * g++ (>=5.4.0)\n    * clang (>=8.0)\n* GPU compiler (optional)\n    * nvcc (>=10.0 for g++ or >=10.2 for clang)\n* GPU library: cudnn-dev (recommend tar file installation, [reference link](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-tar))\n\n**Step 1: Install the requirements**\n```shell\ngit clone https://github.com/Jittor/JDet\ncd JDet\npython -m pip install -r requirements.txt\n```\nIf you have any installation problems for Jittor, please refer to [Jittor](https://github.com/Jittor/jittor)\n\n**Step 2: Install JDet**\n \n```shell\ncd JDet\n# suggest this \npython setup.py develop\n# or\npython setup.py install\n```\nIf you don't have permission for install,please add ```--user```.\n\nOr use ```PYTHONPATH```: \nYou can add ```export PYTHONPATH=$PYTHONPATH:{you_own_path}/JDet/python``` into ```.bashrc```, and run\n```shell\nsource .bashrc\n```\n\n## Getting Started\n\n### Datasets\nThe following datasets are supported in JDet, please check the corresponding document before use. \n\nDOTA1.0/DOTA1.5/DOTA2.0 Dataset: [dota.md](docs/dota.md).\n\nFAIR Dataset: [fair.md](docs/fair.md)\n\nSSDD/SSDD+: [ssdd.md](docs/ssdd.md)\n\nYou can also build your own dataset by convert your datas to DOTA format.\n### Config\nJDet defines the used model, dataset and training/testing method by `config-file`, please check the [config.md](docs/config.md) to learn how it works.\n### Train\n```shell\npython tools/run_net.py --config-file=configs/s2anet_r50_fpn_1x_dota.py --task=train\n```\n\n### Test\nIf you want to test the downloaded trained models, please set ```resume_path={you_checkpointspath}``` in the last line of the config file.\n```shell\npython tools/run_net.py --config-file=configs/s2anet_r50_fpn_1x_dota.py --task=test\n```\n### Test on images / Visualization\nYou can test and visualize results on your own image sets by:\n```shell\npython tools/run_net.py --config-file=configs/s2anet_r50_fpn_1x_dota.py --task=vis_test\n```\nYou can choose the visualization style you prefer, for more details about visualization, please refer to [visualization.md](docs/visualization.md).\n<img src=\"https://github.com/Jittor/JDet/blob/visualization/docs/images/vis2.jpg?raw=true\" alt=\"Visualization\" width=\"800\"/>\n\n### Build a New Project\nIn this section, we will introduce how to build a new project(model) with JDet.\nWe need to install JDet first, and build a new project by:\n```sh\nmkdir $PROJECT_PATH$\ncd $PROJECT_PATH$\ncp $JDet_PATH$/tools/run_net.py ./\nmkdir configs\n```\nThen we can build and edit `configs/base.py` like `$JDet_PATH$/configs/retinanet.py`.\nIf we need to use a new layer, we can define this layer at `$PROJECT_PATH$/layers.py` and import `layers.py` in `$PROJECT_PATH$/run_net.py`, then we can use this layer in config files.\nThen we can train/test this model by:\n```sh\npython run_net.py --config-file=configs/base.py --task=train\npython run_net.py --config-file=configs/base.py --task=test\n```\n\n## Models\n\n|    Models     | Dataset| Sub_Image_Size/Overlap |Train Aug | Test Aug | Optim | Lr schd | mAP    | Paper | Config     | Download   |\n| :-----------: | :-----: |:-----:|:-----:| :-----: | :-----:| :-----:| :----: |:--------:|:--------: | :--------: |\n| S2ANet-R50-FPN | DOTA1.0|1024/200| flip|-|  SGD   |   1x    | 74.11   | [arxiv](https://arxiv.org/abs/2008.09397)| [config](configs/s2anet/s2anet_r50_fpn_1x_dota.py) | [model](https://cloud.tsinghua.edu.cn/d/918bcbf7a10a40fb8dee/files/?p=%2Fmodels%2Fs2anet_r50_fpn_1x_dota_bs2_steplr_3%2Fckpt_12.pkl&dl=1) |\n| S2ANet-R50-FPN | DOTA1.0| 1024/200| flip+ra90+bc|-|  SGD   |   1x    | 76.40   | [arxiv](https://arxiv.org/abs/2008.09397)| [config](projects/s2anet/configs/s2anet_r50_fpn_1x_dota_rotate_balance.py) | [model](https://cloud.tsinghua.edu.cn/d/918bcbf7a10a40fb8dee/files/?p=%2Fmodels%2Fs2anet_r50_fpn_1x_dota_rotate_balance%2Fckpt_12.pkl&dl=1) |\n| S2ANet-R50-FPN | DOTA1.0|1024/200| flip+ra90+bc+ms |ms|  SGD   |   1x    | 79.72   | [arxiv](https://arxiv.org/abs/2008.09397)| [config](projects/s2anet/configs/s2anet_r50_fpn_1x_dota_rotate_balance_ms.py) | [model](https://cloud.tsinghua.edu.cn/d/918bcbf7a10a40fb8dee/files/?p=%2Fmodels%2Fs2anet_r50_fpn_1x_dota_rotate_balance_ms%2Fckpt_12.pkl&dl=1) |\n| S2ANet-R101-FPN |DOTA1.0|1024/200|Flip|-|  SGD   |   1x    | 74.28   | [arxiv](https://arxiv.org/abs/2008.09397)| [config](projects/s2anet/configs/s2anet_r101_fpn_1x_dota_bs2.py) | [model](https://cloud.tsinghua.edu.cn/d/918bcbf7a10a40fb8dee/files/?p=%2Fmodels%2Fs2anet_r101_fpn_1x_dota_without_torch_pretrained%2Fckpt_12.pkl&dl=1) |\n| Gliding-R50-FPN |DOTA1.0|1024/200|Flip|-|  SGD   |   1x    | 72.93  | [arxiv](https://arxiv.org/abs/1911.09358)| [config](projects/gliding/configs/gliding_r50_fpn_1x_dota_with_flip.py) | [model](https://cloud.tsinghua.edu.cn/f/ebeefa1edaf84a4d8a2a/?dl=1) |\n| Gliding-R50-FPN |DOTA1.0|1024/200|Flip+ra90+bc|-|  SGD   |   1x    | 74.93   | [arxiv](https://arxiv.org/abs/1911.09358)| [config](projects/gliding/configs/gliding_r50_fpn_1x_dota_with_flip_rotate_balance_cate.py) | [model](https://cloud.tsinghua.edu.cn/f/395ecd3ddaf44bb58ac9/?dl=1) |\n| RetinaNet-R50-FPN |DOTA1.0|600/150|-|-|  SGD   |   -    | 62.503   | [arxiv](https://arxiv.org/abs/1708.02002)| [config](configs/retinanet_r50v1d_fpn_dota.py) | [model](https://cloud.tsinghua.edu.cn/f/f12bb566d4be43bfbdc7/) [pretrained](https://cloud.tsinghua.edu.cn/f/6b5db5fdd5304a5abf19/) |\n| FasterRCNN-R50-FPN |DOTA1.0|1024/200|Flip|-|  SGD   |   1x    | 69.631   | [arxiv](https://arxiv.org/abs/1506.01497)| [config](configs/faster_rcnn_obb_r50_fpn_1x_dota.py) | [model](https://cloud.tsinghua.edu.cn/f/29197095057348d0a392/?dl=1) |\n| RoITransformer-R50-FPN |DOTA1.0|1024/200|Flip|-|  SGD   |   1x    | 73.842   | [arxiv](https://arxiv.org/abs/1812.00155)| [config](configs/faster_rcnn_RoITrans_r50_fpn_1x_dota.py) | [model](https://cloud.tsinghua.edu.cn/f/55fe6380928f4a6582f8/?dl=1) |\n| FCOS-R50-FPN | DOTA1.0|1024/200| flip|-|  SGD   |   1x    | 70.40   | [ICCV19](https://openaccess.thecvf.com/content_ICCV_2019/papers/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.pdf)| [config](configs/fcos_obb_r50_fpn_1x_dota.py) | [model](https://cloud.tsinghua.edu.cn/d/918bcbf7a10a40fb8dee/files/?p=%2Fmodels%2Ffcos_r50%2Fckpt_12.pkl&dl=1) |\n| OrientedRCNN-R50-FPN |DOTA1.0|1024/200|Flip|-|  SGD   |   1x    | 75.62  | [ICCV21](https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_Oriented_R-CNN_for_Object_Detection_ICCV_2021_paper.pdf)| [config](configs/oriented_rcnn_r50_fpn_1x_dota_with_flip.py) | [model](https://cloud.tsinghua.edu.cn/f/a50517f7b8e840949d3f/?dl=1) |\n\n\n**Notice**:\n\n1. ms: multiscale \n2. flip: random flip\n3. ra: rotate aug\n4. ra90: rotate aug with angle 90,180,270\n5. 1x : 12 epochs\n6. bc: balance category\n7. mAP: mean Average Precision on DOTA1.0 test set\n\n### Plan of Models\n<b>:heavy_check_mark:Supported  :clock3:Doing :heavy_plus_sign:TODO</b>\n\n- :heavy_check_mark: S2ANet\n- :heavy_check_mark: Gliding\n- :heavy_check_mark: RetinaNet\n- :heavy_check_mark: Faster R-CNN\n- :heavy_check_mark: SSD\n- :heavy_check_mark: ROI Transformer\n- :heavy_check_mark: fcos\n- :heavy_check_mark: Oriented R-CNN\n- :clock3: ReDet\n- :clock3: YOLOv5\n- :clock3: R3Det\n- :clock3: Cascade R-CNN\n- :heavy_plus_sign: CSL\n- :heavy_plus_sign: DCL\n- :heavy_plus_sign: GWD\n- :heavy_plus_sign: KLD\n- :heavy_plus_sign: Double Head OBB\n- :heavy_plus_sign: Oriented Reppoints\n- :heavy_plus_sign: Guided Anchoring\n- :heavy_plus_sign: ...\n\n### Plan of Datasets\n<b>:heavy_check_mark:Supported  :clock3:Doing :heavy_plus_sign:TODO</b>\n\n- :heavy_check_mark: DOTA1.0\n- :heavy_check_mark: DOTA1.5\n- :heavy_check_mark: DOTA2.0\n- :heavy_check_mark: SSDD\n- :heavy_check_mark: SSDD+\n- :heavy_check_mark: FAIR\n- :heavy_check_mark: COCO\n- :heavy_plus_sign: LS-SSDD\n- :heavy_plus_sign: DIOR-R\n- :heavy_plus_sign: HRSC2016\n- :heavy_plus_sign: ICDAR2015\n- :heavy_plus_sign: ICDAR2017 MLT\n- :heavy_plus_sign: UCAS-AOD\n- :heavy_plus_sign: FDDB\n- :heavy_plus_sign: OHD-SJTU\n- :heavy_plus_sign: MSRA-TD500\n- :heavy_plus_sign: Total-Text\n- :heavy_plus_sign: ...\n\n## Contact Us\n\n\nWebsite: http://cg.cs.tsinghua.edu.cn/jittor/\n\nEmail: jittor@qq.com\n\nFile an issue: https://github.com/Jittor/jittor/issues\n\nQQ Group: 761222083\n\n\n<img src=\"https://cg.cs.tsinghua.edu.cn/jittor/images/news/2020-12-8-21-19-1_2_2/fig4.png\" width=\"200\"/>\n\n## The Team\n\n\nJDet is currently maintained by the [Tsinghua CSCG Group](https://cg.cs.tsinghua.edu.cn/). If you are also interested in JDet and want to improve it, Please join us!\n\n\n## Citation\n\n\n```\n@article{hu2020jittor,\n  title={Jittor: a novel deep learning framework with meta-operators and unified graph execution},\n  author={Hu, Shi-Min and Liang, Dun and Yang, Guo-Ye and Yang, Guo-Wei and Zhou, Wen-Yang},\n  journal={Science China Information Sciences},\n  volume={63},\n  number={222103},\n  pages={1--21},\n  year={2020}\n}\n```\n\n## Reference\n1. [Jittor](https://github.com/Jittor/jittor)\n2. [Detectron2](https://github.com/facebookresearch/detectron2)\n3. [mmdetection](https://github.com/open-mmlab/mmdetection)\n4. [maskrcnn_benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)\n5. [RotationDetection](https://github.com/yangxue0827/RotationDetection)\n6. [s2anet](https://github.com/csuhan/s2anet)\n7. [gliding_vertex](https://github.com/MingtaoFu/gliding_vertex)\n8. [oriented_rcnn](https://github.com/jbwang1997/OBBDetection/tree/master/configs/obb/oriented_rcnn)\n9. [r3det](https://github.com/SJTU-Thinklab-Det/r3det-on-mmdetection)\n10. [AerialDetection](https://github.com/dingjiansw101/AerialDetection)\n11. [DOTA_devkit](https://github.com/CAPTAIN-WHU/DOTA_devkit)\n12. [OBBDetection](https://github.com/jbwang1997/OBBDetection)\n\n\n",
            "readme_url": "https://github.com/Jittor/JDet",
            "frameworks": [
                "TensorFlow"
            ]
        }
    ],
    "references": [
        {
            "title": "Align Deep Features for Oriented Object Detection",
            "arxiv": "2008.09397",
            "year": 2020,
            "url": "http://arxiv.org/abs/2008.09397v3",
            "abstract": "The past decade has witnessed significant progress on detecting objects in\naerial images that are often distributed with large scale variations and\narbitrary orientations. However most of existing methods rely on heuristically\ndefined anchors with different scales, angles and aspect ratios and usually\nsuffer from severe misalignment between anchor boxes and axis-aligned\nconvolutional features, which leads to the common inconsistency between the\nclassification score and localization accuracy. To address this issue, we\npropose a Single-shot Alignment Network (S$^2$A-Net) consisting of two modules:\na Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The\nFAM can generate high-quality anchors with an Anchor Refinement Network and\nadaptively align the convolutional features according to the anchor boxes with\na novel Alignment Convolution. The ODM first adopts active rotating filters to\nencode the orientation information and then produces orientation-sensitive and\norientation-invariant features to alleviate the inconsistency between\nclassification score and localization accuracy. Besides, we further explore the\napproach to detect objects in large-size images, which leads to a better\ntrade-off between speed and accuracy. Extensive experiments demonstrate that\nour method can achieve state-of-the-art performance on two commonly used aerial\nobjects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The\ncode is available at https://github.com/csuhan/s2anet.",
            "authors": [
                "Jiaming Han",
                "Jian Ding",
                "Jie Li",
                "Gui-Song Xia"
            ]
        },
        {
            "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
            "arxiv": "1506.01497",
            "year": 2015,
            "url": "http://arxiv.org/abs/1506.01497v3",
            "abstract": "State-of-the-art object detection networks depend on region proposal\nalgorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN\nhave reduced the running time of these detection networks, exposing region\nproposal computation as a bottleneck. In this work, we introduce a Region\nProposal Network (RPN) that shares full-image convolutional features with the\ndetection network, thus enabling nearly cost-free region proposals. An RPN is a\nfully convolutional network that simultaneously predicts object bounds and\nobjectness scores at each position. The RPN is trained end-to-end to generate\nhigh-quality region proposals, which are used by Fast R-CNN for detection. We\nfurther merge RPN and Fast R-CNN into a single network by sharing their\nconvolutional features---using the recently popular terminology of neural\nnetworks with 'attention' mechanisms, the RPN component tells the unified\nnetwork where to look. For the very deep VGG-16 model, our detection system has\na frame rate of 5fps (including all steps) on a GPU, while achieving\nstate-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS\nCOCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015\ncompetitions, Faster R-CNN and RPN are the foundations of the 1st-place winning\nentries in several tracks. Code has been made publicly available.",
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ]
        },
        {
            "title": "Learning RoI Transformer for Detecting Oriented Objects in Aerial Images",
            "arxiv": "1812.00155",
            "year": 2018,
            "url": "http://arxiv.org/abs/1812.00155v1",
            "abstract": "Object detection in aerial images is an active yet challenging task in\ncomputer vision because of the birdview perspective, the highly complex\nbackgrounds, and the variant appearances of objects. Especially when detecting\ndensely packed objects in aerial images, methods relying on horizontal\nproposals for common object detection often introduce mismatches between the\nRegion of Interests (RoIs) and objects. This leads to the common misalignment\nbetween the final object classification confidence and localization accuracy.\nAlthough rotated anchors have been used to tackle this problem, the design of\nthem always multiplies the number of anchors and dramatically increases the\ncomputational complexity. In this paper, we propose a RoI Transformer to\naddress these problems. More precisely, to improve the quality of region\nproposals, we first designed a Rotated RoI (RRoI) learner to transform a\nHorizontal Region of Interest (HRoI) into a Rotated Region of Interest (RRoI).\nBased on the RRoIs, we then proposed a Rotated Position Sensitive RoI Align\n(RPS-RoI-Align) module to extract rotation-invariant features from them for\nboosting subsequent classification and regression. Our RoI Transformer is with\nlight weight and can be easily embedded into detectors for oriented object\ndetection. A simple implementation of the RoI Transformer has achieved\nstate-of-the-art performances on two common and challenging aerial datasets,\ni.e., DOTA and HRSC2016, with a neglectable reduction to detection speed. Our\nRoI Transformer exceeds the deformable Position Sensitive RoI pooling when\noriented bounding-box annotations are available. Extensive experiments have\nalso validated the flexibility and effectiveness of our RoI Transformer. The\nresults demonstrate that it can be easily integrated with other detector\narchitectures and significantly improve the performances.",
            "authors": [
                "Jian Ding",
                "Nan Xue",
                "Yang Long",
                "Gui-Song Xia",
                "Qikai Lu"
            ]
        },
        {
            "title": "Gliding vertex on the horizontal bounding box for multi-oriented object detection",
            "arxiv": "1911.09358",
            "year": 2019,
            "url": "http://arxiv.org/abs/1911.09358v2",
            "abstract": "Object detection has recently experienced substantial progress. Yet, the\nwidely adopted horizontal bounding box representation is not appropriate for\nubiquitous oriented objects such as objects in aerial images and scene texts.\nIn this paper, we propose a simple yet effective framework to detect\nmulti-oriented objects. Instead of directly regressing the four vertices, we\nglide the vertex of the horizontal bounding box on each corresponding side to\naccurately describe a multi-oriented object. Specifically, We regress four\nlength ratios characterizing the relative gliding offset on each corresponding\nside. This may facilitate the offset learning and avoid the confusion issue of\nsequential label points for oriented objects. To further remedy the confusion\nissue for nearly horizontal objects, we also introduce an obliquity factor\nbased on area ratio between the object and its horizontal bounding box, guiding\nthe selection of horizontal or oriented detection for each object. We add these\nfive extra target variables to the regression head of faster R-CNN, which\nrequires ignorable extra computation time. Extensive experimental results\ndemonstrate that without bells and whistles, the proposed method achieves\nsuperior performances on multiple multi-oriented object detection benchmarks\nincluding object detection in aerial images, scene text detection, pedestrian\ndetection in fisheye images.",
            "authors": [
                "Yongchao Xu",
                "Mingtao Fu",
                "Qimeng Wang",
                "Yukang Wang",
                "Kai Chen",
                "Gui-Song Xia",
                "Xiang Bai"
            ]
        },
        {
            "title": "Focal Loss for Dense Object Detection",
            "arxiv": "1708.02002",
            "year": 2017,
            "url": "http://arxiv.org/abs/1708.02002v2",
            "abstract": "The highest accuracy object detectors to date are based on a two-stage\napproach popularized by R-CNN, where a classifier is applied to a sparse set of\ncandidate object locations. In contrast, one-stage detectors that are applied\nover a regular, dense sampling of possible object locations have the potential\nto be faster and simpler, but have trailed the accuracy of two-stage detectors\nthus far. In this paper, we investigate why this is the case. We discover that\nthe extreme foreground-background class imbalance encountered during training\nof dense detectors is the central cause. We propose to address this class\nimbalance by reshaping the standard cross entropy loss such that it\ndown-weights the loss assigned to well-classified examples. Our novel Focal\nLoss focuses training on a sparse set of hard examples and prevents the vast\nnumber of easy negatives from overwhelming the detector during training. To\nevaluate the effectiveness of our loss, we design and train a simple dense\ndetector we call RetinaNet. Our results show that when trained with the focal\nloss, RetinaNet is able to match the speed of previous one-stage detectors\nwhile surpassing the accuracy of all existing state-of-the-art two-stage\ndetectors. Code is at: https://github.com/facebookresearch/Detectron.",
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ]
        },
        {
            "year": "2020",
            "pages": "1--21",
            "number": "222103",
            "volume": "63",
            "journal": "Science China Information Sciences",
            "author": [
                "Hu, Shi-Min",
                "Liang, Dun",
                "Yang, Guo-Ye",
                "Yang, Guo-Wei",
                "Zhou, Wen-Yang"
            ],
            "title": "Jittor: a novel deep learning framework with meta-operators and unified graph execution",
            "ENTRYTYPE": "article",
            "ID": "hu2020jittor",
            "authors": [
                "Hu, Shi-Min",
                "Liang, Dun",
                "Yang, Guo-Ye",
                "Yang, Guo-Wei",
                "Zhou, Wen-Yang"
            ]
        }
    ],
    "training": {
        "datasets": [
            {
                "name": "FDDB"
            },
            {
                "name": "COCO"
            },
            {
                "name": "MSRA"
            },
            {
                "name": "PASCAL VOC 2007"
            }
        ]
    },
    "domain": {
        "domain_type": "Computer Vision",
        "domain_prob": 0.9884615271722225,
        "task": "Object Detection",
        "task_prob": 0.9666316776645092
    }
}